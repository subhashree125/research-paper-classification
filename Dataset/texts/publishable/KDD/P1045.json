{
  "ABSTRACT": "Graph Prompt Learning (GPL) bridges significant disparities be-tween pretraining and downstream applications to alleviate theknowledge transfer bottleneck in real-world graph learning. WhileGPL offers superior effectiveness in graph knowledge transfer andcomputational efficiency, the security risks posed by backdoor poi-soning effects embedded in pretrained models remain largely un-explored. Our study provides a comprehensive analysis of GPLsvulnerability to backdoor attacks. We introduce CrossBA, the firstcross-context backdoor attack against GPL, which manipulatesonly the pretraining phase without requiring knowledge of down-stream applications. Our investigation reveals both theoreticallyand empirically that tuning trigger graphs, combined with prompttransformations, can seamlessly transfer the backdoor threat frompretrained encoders to downstream applications. Through exten-sive experiments involving 3 representative GPL methods across 5distinct cross-context scenarios and 5 benchmark datasets of nodeand graph classification tasks, we demonstrate that CrossBA con-sistently achieves high attack success rates while preserving thefunctionality of downstream applications over clean input. We alsoexplore potential countermeasures against CrossBA and concludethat current defenses are insufficient to mitigate CrossBA. Our studyhighlights the persistent backdoor threats to GPL systems, raisingtrustworthiness concerns in the practices of GPL techniques.",
  "Corresponding author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior specific permission and/or afee. Request permissions from , August 25-29, 2024, Barcelona, Spain 2024 ACM.ACM ISBN 978-x-xxxx-xxxx-x/YY/MM ACM Reference Format:Xiaoting Lyu, Yufei Han, Wei Wang, Hangwei Qian, Ivor Tsang, and Xian-gliang Zhang. 2024. Cross-Context Backdoor Attacks against Graph PromptLearning. In KDD24: SIGKDD Conference on Knowledge Discovery and DataMining, August 25-29, 2024, Barcelona, Spain. ACM, New York, NY, USA,16 pages.",
  "INTRODUCTION": "Real-world graph learning tasks pose challenges in generalizationand knowledge transfer when deploying pretrained graph neuralnetworks (GNNs) to downstream applications divergent from thepretraining stage. For instance, a GNN pretrained on social net-works may be utilized in recommendation systems, while encodersdesigned for link prediction might be repurposed for node or graphclassification tasks. The substantial differences between pretrainingand downstream applications, including variations in problem do-mains, semantic space, and learning objectives , presentobstacles for transferring the graph knowledge in pretrained GNNmodels to diverse downstream applications. In response, GraphPrompt Learning (GPL) has emerged as a promising so-lution for such graph learning tasks requiring the generalization ofgraph knowledge across various application contexts (abbreviatedas cross-context graph learning). Inspired by prompt learning inLarge Language Models (LLMs) , GPL involves training GNNencoders initially on unannotated pretext graph data and then tailor-ing prompts for downstream applications to guide these encoders.This approach effectively bridges the gap between pretraining anddownstream tasks without altering the GNNs parameters, therebyavoiding resource-intensive data annotation and model retrainingwhile facilitating robust generalization of pretrained GNN encodersacross diverse downstream applications.While GPL facilitates knowledge transfer across diverse graphlearning tasks in cross-context scenarios, it also exposes down-stream applications to the risk of inheriting backdoors embedded inpretrained models. Attackers can implant backdoors into pretrainedGNN encoders, leading to downstream models built on these en-coders inheriting the backdoor poisoning effects and misclassifyingbackdoored inputs to attacker-desired target labels. Such backdoorvulnerabilities have been identified in promptlearning in Natural Language Processing (NLP), which involves us-ing rare words as triggers and associating them with specific targetclasses or output embeddings. However, adapting these NLP-based",
  "KDD24, August 25-29, 2024, Barcelona, SpainXiaoting Lyu, Yufei Han, Wei Wang, Hangwei Qian, Ivor Tsang, and Xiangliang Zhang": "Qingqing Ge, Zeyuan Zhao, Yiding Liu, Anfeng Cheng, Xiang Li, ShuaiqiangWang, and Dawei Yin. 2023. Enhancing Graph Neural Networks with Structure-Based Prompt. CoRR abs/2310.17394 (2023). arXiv:2310.17394 Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, andGraham Neubig. 2023. Pre-train, Prompt, and Predict: A Systematic Survey ofPrompting Methods in Natural Language Processing. ACM Comput. Surv. 55, 9(2023), 195:1195:35. Zemin Liu, Xingtong Yu, Yuan Fang, and Xinming Zhang. 2023. GraphPrompt:Unifying Pre-Training and Downstream Tasks for Graph Neural Networks. InProceedings of the ACM Web Conference 2023, WWW 2023, Austin, TX, USA, 30April 2023 - 4 May 2023. ACM, 417428. Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I. Jordan. 2015. LearningTransferable Features with Deep Adaptation Networks. In Proceedings of the 32ndInternational Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July2015 (JMLR, Vol. 37). JMLR.org, 97105. Yihong Ma, Ning Yan, Jiayu Li, Masood S. Mortazavi, and Nitesh V. Chawla. 2023.HetGPT: Harnessing the Power of Prompt Tuning in Pre-Trained HeterogeneousGraph Neural Networks. CoRR abs/2310.15318 (2023). arXiv:2310.15318 Kai Mei, Zheng Li, Zhenting Wang, Yang Zhang, and Shiqing Ma. 2023. NOTABLE:Transferable Backdoor Attacks Against Prompt-based NLP Models. In Proceed-ings of the 61st Annual Meeting of the Association for Computational Linguistics(Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023. Associationfor Computational Linguistics, 1555115565. Anna Rogers, Olga Kovaleva, Matthew Downey, and Anna Rumshisky. 2020.Getting Closer to AI Complete Question Answering: A Set of Prerequisite RealTasks. Proceedings of the AAAI Conference on Artificial Intelligence 34, 05 (Apr.2020), 87228731.",
  "RELATED WORKS": "Graph Prompt Learning. Prompt learning, initially successful inNLP , has been extended to graph data, tailoring prompts fordownstream tasks to guide the pretrained model to perform effec-tively without altering its parameters. Prompts in graph learningmanifest in two forms: prompt as tokens and prompt as graphs. Two representative methods of these GPLmethods are GraphPrompt and ProG . Both unify pre-training and downstream tasks into a common template but dif-fer in prompts. ProG uses learnable, graph-structured variables asprompts attached to the input graph, while GraphPrompt embedsprompt tokens as learnable vectors into the hidden layers of theGNN model, enhancing the Readout operation.Backdoor Attacks. Backdoor attacks on GNNs have gained atten-tion recently . In these attacks, the backdooredGNN model predicts an attacker-chosen label for any testing inputembedded with triggers. Notably, employs the Erdos-Rnyi(ER) model to generate subgraphs as triggers, introduces anadaptive trigger generator enhancing attack effectiveness, de-signs triggers that preserve the labels of backdoored samples, and proposes unnoticeable graph backdoor attacks to bypass de-fense mechanisms. However, in cross-context GPL scenarios, tradi-tional graph backdoor attacks are inapplicable, as the attacker onlycontrols the pretraining process with unlabeled data. GCBA ,targeting GCL, manipulates the victim GNN encoder to associate atrigger with the target classs embedding. Yet, GCBA necessitatesknowledge of the target class in downstream tasks, impractical incross-context GPL scenarios. Our study explores the feasibility ofbackdoor attacks in cross-context GPL scenarios, where attackerscan only use unlabeled data to pretrain GNN encoders. Importantly,attackers cannot access or interfere with downstream applications.",
  "PRELIMINARIES": "We focus on the workflow of cross-context graph prompt learning. Attackers pretrain the GNN encoder using self-supervisedlearning on unlabeled graph data. Downstream users then learn theprompts with few-shot training samples based on the pretrainedencoder. Relevant concepts and definitions are introduced below.GNN encoder. GNNs have become a predominant approach forlearning graph embeddings. Typically, GNNs utilize a neighborhoodaggregation strategy, wherein the encoder iteratively updates anodes embedding by aggregating embeddings from its neighborsthrough message passing. Formally, at the-th layer, the embeddingof node is given by:",
  "= AGGREGATE1, { N( ) }, () (1)": "where N () is the set of first-order neighbors of node in thegraph , and the AGGREGATE function combines neighborhoodnode embeddings to update the node embedding. The final nodeembedding of is denoted as = (, ), where denotes theparameters of the encoder . The graph embedding () is thenobtained through a Readout function that aggregates node embed-dings from the entire graph. The objective functions of pretraininginclude various self-supervised tasks such as GraphCL and linkprediction , which enable the model to capture rich structuraland feature-based graph patterns. Our study focuses on inductive learning, where the GNN encoders input is the inductive graph ofa given node, including the node itself and its k-hop neighbors.Graph prompt learning. This study is involved in two sophisti-cated GPL methods: ProG and GraphPrompt . Both ProGand GraphPrompt unify pretraining and downstream tasks intograph-level tasks, introducing learnable prompts to guide thesetasks. In ProG, the prompt graph is denoted as = (,), where = {1, 2, . . . , | |} represents the set of || nodes, each character- ized by a token vector p R1. The set = {( , )| , }defines the prompt graphs topology structure. A prompted graph isobtained by inserting the prompt graph into the input graph.The parameters for both the prompt graph and the answering func-tion of downstream tasks are optimized through few-shot learning.ProG-Meta enhances ProG by incorporating meta-learning tech-niques. GraphPrompt introduces prompts within the hidden layersof the GNN model to assist the graph pooling operation. Giventhe node set = {1, 2, . . . , } in with each nodes embedding , and a learnable prompt vector pj for the downstream task ,the prompt-assisted readout operation for graph is defined asa reweighed readout function, Readout({pj | }), where represents element-wise multiplication. The prompt vector pjupdates its parameters by gradient descent to minimize the graphsimilarity loss.Cross-context few-shot learning. Cross-context few-shot learningis designed to facilitate rapid adaptation of models to new tasks indiverse application contexts using limited labeled examples. Theinherent data heterogeneity in this paradigm results in disparities inboth semantic space and data distribution between pretraining anddownstream contexts. We summarize the cross-context scenariosstudied in previous works and delineate five cross-context scenarios to cover different levels of such disparities: Cross-task: This setting reflects the divergence in the goal of graphlearning tasks. The pretraining may be conducted for classifica-tion of an entire graph, such as ENZYMES, while the downstreamtask involves classification of nodes, like CiteSeer. Cross-domain: The pretraining and downstream data originatefrom distinct domains but share the same task type. For instance,pretraining could be conducted on commercial product networks(e.g., Amazon), while downstream tasks involve academic citationnetworks (e.g., Cora), both focusing on node classification tasks. Cross-dataset: This setting involves different datasets within thesame domain. For instance, the pretraining is on dataset likeCora, while the downstream task involves a different dataset likeCiteSeer. Both are academic citation networks but different. Cross-class: Both the pretraining and downstream datasets stemfrom the same data source, such as Cora. However, they focuson different classes, i.e., pretraining and downstream tasks havedifferent class distributions. . Cross-distribution: In this scenario, both the pretraining and down-stream datasets are sourced from the same data origin, sharingidentical features and label spaces. However, they present dis-tinctly different data distributions.",
  "THREAT MODELS": "Attackers goal. Attackers aim to mislead downstream models builtupon the attacker-crafted GNN encoder to classify backdoored in-puts as the target class. Simultaneously, the downstream modelshould behave normally for clean inputs. Specifically, in cross-context scenarios, attackers build the backdoored GNN encoder atthe pretraining phase, which memorizes the association betweenbackdoored inputs and the attacker-desired output embedding. Thisbackdoored GNN encoder is then adapted for downstream appli-cations using the GPL methods. To ensure the transferability ofbackdoors in cross-context GPL, the attack should be organizedwith the following properties: 1) Context-agnostic: The backdoorattack should remain consistently effective across various down-stream contexts. 2) Prompt-agnostic: The attack should be adaptableto different designs of graph prompt learning methods used bydownstream users. 3) Stealthy: The backdoored GNN model shouldcontribute close classification accuracy to backdoor-free modelson clean data of downstream applications. Furthermore, the at-tack remains effective with the defense mechanisms deployed bydownstream users.Attackers capability. We assume that the attacker possesses com-plete control over the pretraining process. The attacker can injectbackdoor triggers into the unlabeled pretraining data and accessthe training methods for the backdoored GNN encoder. However,the attacker is incapable of accessing or manipulating the labeleddata and the GPL training process utilized by downstream users.Attackers knowledge. The attacker has full knowledge of thepretraining phase, including the dataset, the architecture of thepretrained GNN encoder, and the training method. On the contrary,the attacker lacks any information regarding the GPL trainingprocess conducted by downstream users, including specifics aboutthe downstream datasets utilized and the GPL methods applied.",
  "CROSS-CONTEXT BACKDOOR ATTACK5.1Overview of CrossBA": "provides an overview of CrossBA. In our attack scenario,attackers train the backdoored GNN encoder using unlabeled graphdata and self-supervised methods like graph contrastive learning.CrossBA aims to inject embedding collisions between backdoored graphs and the trigger graph into the backdoored GNN encoder,while maintaining a distinct embedding for the backdoored graphcompared to its clean counterpart. This backdoor poisoning effectcauses the backdoored graph to be associated with a target em-bedding distant from the clean input, leading to misclassificationin downstream models. Additionally, CrossBA also optimizes thestandard contrastive learning objective over clean graphs to ensurerobust classification performance.Formally, the trigger graph serves as the backdoor signal is =(, ), with = {1, . . . , } representing the set of triggernodes. Each trigger node has a feature vector x R1, matching the input graphs node feature dimension. = {(, )|, } defines the links of the trigger graph. To integrate the triggergraph into the input graph for generating the backdooredgraph, the attacker randomly selects a node in as the anchor node, linking it to a specific node in the trigger graph. We limit thetrigger graph to a 3-node and fully connected graph, significantlysmaller than graphs in both pretraining and downstream datasets.",
  "Cross-Context Backdoor Attacks against Graph Prompt LearningKDD24, August 25-29, 2024, Barcelona, Spain": "of the clean graph . feat() denotes the feature vector of a node. is the backdoor-free GNN encoder trained with clean graphdata. = () denotes the embedding of the clean graph .sim(, ) is the similarity between the embeddings and . isthe temperature parameter. The parameters , , and balance theweight of the loss terms. The main learning loss Lclr defines a contrastive learning objec-tive of the main task as in . The goal is to train a GNN encodercapable of producing distinctive embeddings for clean graph data.Specifically, we maximize the similarity between the embeddingsof the graph and its augmented counterpart + . Simultaneously,we maximize the dissimilarity between the embedding of any othergraph and that of . The backdoor learning loss Lbdk defines the backdoor tasksobjective. To address the attackers lack of knowledge about down-stream datasets, CrossBA utilizes the embedding of the trigger graphas the target, and induces the backdoor mapping into the GNN en-coder by colliding the embeddings of backdoored graphs with thatof the trigger graph. By jointly tuning the backdoored GNN encoderand the trigger graph, we ensure that the embeddings of backdooredgraphs resemble the target embedding while being different fromclean graphs embeddings. Consequently, when applied to down-stream applications, backdoored graphs will be misclassified to thetarget class associated with the target embedding.Compared to manually specifying a static backdoor mapping(with a fixed trigger graph and target embedding), our design offersseveral advantages. First, by jointly tuning the target embeddingand trigger graph, we align the loss landscapes of the main task andthe backdoor task within the fixed GNN encoder. This minimizesthe backdoor learning loss while preserving the utility of the back-doored GNN encoder on clean graph data. Second, fine-tuning thetrigger graph, as supported by our theoretical analysis in .4, reduces the disparity in backdoor task performance betweenpretraining and downstream applications, thereby enhancing back-door transferability. Lastly, we parameterize the tuning of the targetembedding by optimizing the trigger graph, ensuring it remainswithin the embedding space of the graphs in . Directly optimizingthe target embedding as an independent variable may lead to ex-treme values outside the span of graphs in , making them proneto detection by downstream anomaly detection methods. The embedding alignment loss Lsim is designed to ensure thatbackdoors do not impair the GNN encoders ability to generatediscriminative embeddings for clean graph data. The attacker canbuild a clean GNN encoder using clean pretraining data as areference model. By aligning the output embeddings of withthose of on the same clean inputs, the backdoored GNN encoder can perform similarly to the clean encoder on clean data. The node feature affinity loss Lsim is designed to enhance thestealthiness of CrossBA to evade anomaly detection-based sanitarychecks over node features. Since adjacent nodes in a graph typicallyshare similar features, downstream users can check the node featureconsistency to identify abnormal nodes with significantly deviatedfeature values from their neighbors . To circumvent suchdefenses, we optimize the node features of the trigger graph by",
  "Alternating Optimization for CrossBA": "Algorithm 1 in Appendix A outlines the procedural flow of theCrossBA attack. Initially, attackers employ self-supervised methods,such as GraphCL , to train a clean GNN encoder by mini-mizing Lclr. The trigger injection is then conducted by optimizingthe attack objective function in Eq. 2 in alternating order. Duringeach attack round, attackers first freeze the GNN encoder andoptimize the node features of the trigger graph . Subsequently,attackers optimize the backdoored GNN encoder based on theoptimized trigger graph and the clean GNN encoder . Tuning trigger graph. After injecting the trigger graph intothe input graph , the attacker optimizes the node features of ,potentially optimizing the target embedding, by minimizing thebackdoor learning loss Lbdk and the node feature affinity loss Lsimwith respect to the fixed GNN encoder . For simplicity, theupdate of the trigger node features in one step is given by:",
  "= 1 1(Lbdk + Lsim)(3)": "Tuning backdoored GNN encoder. Upon completing trigger opti-mization, the attacker connects the optimized trigger graph tothe anchor node in , recreating backdoored graphs. The optimiza-tion aims to maximize the similarity between the embeddings ofbackdoored graphs and the trigger graph, as well as the similarity ofthe clean graphs embeddings between and . For simplicity,the GNN encoder parameters are updated accordingly:",
  "Attack Feasibility of CrossBA": "In this section, we explore the feasibility of the proposed CrossBAattack against cross-context GPL. To simplify the analysis, we adoptthe prompt graph setting from , where the prompt graph is attached to the input graph . We define and as the distri-butions for the pretraining and downstream graph datasets, respec-tively. Downstream users create the prompted graph by .The encoder outputs the embedding of the prompted graph as( ) = ( ). Theorem 1. Assuming a -Lipschitz continuous GNN encoder with -node input graphs, and the node feature matrix ofa graph = (, ) has a bounded Frobenius norm, i.e., | | . Upon freezing the GNN encoder , the backdoor learning lossLbdk in Eq.2 is upper-bounded by the main learning loss Lclr at thepretraining stage, as given in Eq. 5:",
  ")1/2+ 1": "(7)where + are augmented trigger graphs with randomly added orremoved links from . and are the graphs sampled from thepretraining and downstream data distribution and , respectively.The RKHS kernel function (, ) measures the similarity between twograph embeddings. , 0, and 1 are constants for the GNN encoderscomplexity and the optimal learning loss of an ideal encoder.",
  "ing dataset and the prompted graphs from the downstream dataset": "By combining Proposition 1, Eq.6, and Eq.7, we discover that em-ploying prompts in the cross-context GPL presents both advantagesand disadvantages. On one hand, augmenting downstream graphdata from with the prompt enhances the adaptability ofpretrained GNN encoders to downstream tasks by alleviating distri-butional disparities between and within the GNN encodersembedding space. This results in a well-trained GNN encoder capa-ble of generating discriminative embeddings for new tasks. How-ever, on the other hand, as indicated by Eq.7, incorporating theprompt graph into the backdoored graphs from inadvertentlyfacilitates backdoor attack transferability by reducing the upperbound of the backdoor learning loss in downstream tasks.Observation 2. Tuning the trigger graph enhances the trans-ferability of backdoor poisoning effects while concurrentlypreserving the utility of backdoored GNN models.",
  "of the trigger graph and the backdoored graphs in pretraining": "In CrossBA, we propose optimizing the trigger graph witha fixed GNN encoder during the pretraining stage to minimizethe backdoor learning loss. This process, as indicated by Eq.5 andProposition 2, improves the alignment between the embeddings of+ and , narrowing the disparity between the main learn-ing loss and the backdoor learning loss on pretraining data. Thisensures that given a well-trained GNN encoder, the trigger tuningmodule reduces the backdoor learning loss without compromising the performance of the main task. Furthermore, as shown in Eq.7,tuning the trigger graph during pretraining, along with the promptgraph, further facilitates backdoor attack transferability by lower-ing the upper bound of the backdoor learning loss in downstreamtasks, leading to misclassification with backdoored input.The theoretical investigation elucidates the feasibility of deliver-ing cross-context backdoor attacks following the design of CrossBA,providing a response to RQ1. Moreover, Observation 1, which ad-dresses RQ2, reveals the dual nature of knowledge transfer withinGPLs prompt learning. While prompt learning enhances down-stream models with the pretrained models expertise, it also posesthe risk of backdoor transfer. Our findings underscore substantialconcerns regarding the trustworthiness of GPL methods. Proofs ofTheorem 1 and Propositions 1 and 2 are provided in Appendix B.",
  "EXPERIMENTAL EVALUATION6.1Experimental Settings": "Datasets and the Backbone GNN Models. We utilize 5 bench-mark datasets for evaluation: CiteSeer , Cora , Amazon-Computers , Amazon-Photo , and ENZYMES . CiteSeer,Cora, Amazon-Computers, and Amazon-Photo are utilized for nodeclassification tasks, while ENZYMES is employed for graph classifi-cation tasks. Furthermore, following the setting in , we definegraph classification tasks using the node classification datasets. InGPL systems, we employ two advanced GNN models: Graph Atten-tion Network (GAT) and Graph Transformer (GT) . Detailsabout datasets and GNN models can be found in Appendix C.GPL Methods. We evaluate the attack performance against main-stream GPL methods applicable to both node and graph classifica-tion tasks, categorized into two types : Prompt as Graphs andPrompt as Tokens. For the former branch, we choose ProG andProG-Meta , formulating prompts as subgraph patterns. For thelatter, we target GraphPrompt , which considers prompts astokens added to the Readout module of GNNs.Baseline Attacks. GCBA emerges as the most relevant back-door attack for our study, considering the threat model. While notexplicitly tailored for GPL, GCBA aims to inject backdoor poisoningnoise into a GNN encoder trained via GCL. In GCBA, the attackergathers graph data of the target class in downstream applicationsand utilizes the GCBA-crafting method to inject the backdoor intothe GNN encoder. However, GCBA does not directly apply to cross-context GPL scenarios as it necessitates access to downstream appli-cations. We introduce two variants of GCBA adapted to our threatmodel: GCBA_R and GCBA_M. In both variants, the attacker ini-tially clusters the embeddings of clean data collected during thepretraining stage, utilizing the backdoor-free GNN encoder. Sub-sequently, GCBA_R randomly selects the embedding at the centerof a cluster as the target embedding, while GCBA_M chooses thecluster center embedding farthest from other clusters as the targetembedding. Further details can be found in Appendix D.Evaluation Metrics. We employ 3 metrics to evaluate attack ef-fectiveness: (1) Attack Success Rate (ASR) , representing theaccuracy with which a backdoored downstream model classifiesbackdoored inputs to the target class designated by the target em-bedding, (2) Accuracy of the Main Task (ACC), measuring the clas-sification accuracy of backdoored downstream models over clean",
  "ProGMeta": "GATGCBA_R0.50(+0.34)0.890.50(+0.39)0.030.50(+0.38)0.000.50(+0.26)0.00GCBA_M0.50(+0.34)0.000.50(+0.39)0.000.50(+0.38)1.000.50(+0.26)0.00CrossBA0.80(+0.04)1.000.88(+0.01)1.000.90(-0.02)1.000.87(-0.11)1.00 GTGCBA_R0.91(+0.02)1.000.83(+0.07)0.520.84(+0.08)1.000.96(+0.01)1.00GCBA_M0.50(+0.43)0.000.50(+0.40)1.000.50(+0.42)0.000.50(+0.47)0.00CrossBA0.93(-0.00)1.000.90(-0.00)1.000.94(-0.02)1.000.95(+0.02)1.00 Summary. The results reveal a common vulnerability of CrossBAto diverse GPL techniques. This aligns with the theoretical foun-dations presented in .4, highlighting the intrinsic riskswithin the GPL framework that the prompt learning module of GPLfacilitates the transfer of backdoors to downstream applications.(2) Potential Countermeasures. We evaluate the resilience ofCrossBA against potential countermeasures of downstream users.Given the absence of specific defenses tailored for cross-contextGPL, we adapt PruneG , originally designed to mitigate adver-sarial attacks on GNNs. Other defense methods, such as RandSample and GNNGuard , are not suitable for cross-context GPL scenarios. This is because both RandSample and GNNGuard en-tail training GNN encoders, which is impractical in cross-contextGPL. Therefore, we exclusively employ PruneG in our evaluation.PruneG is a preprocessing technique that eliminates edges betweennodes with dissimilar features, removing components with fewerconnected nodes. illustrates the effectiveness of differentattack methods against PruneG across 5 cross-context scenarios.CrossBA consistently outperforms the baselines, achieving highASR values exceeding 0.70 against PruneG. In cross-domain sce-narios, CrossBA achieves ASRs above 0.90, while the ASRs of thebaselines remain below 0.50. in Appendix G demonstrates",
  "Experimental Results": "(1) Attack Performance in Cross-Context GPL Scenarios. Weevaluate the feasibility of backdoor attacks on node and graph clas-sification tasks across 5 cross-context scenarios using 3 mainstreamGPL methods, addressing 3 research questions. Tables 13 and 56present the ACC and ASR of all attack methods in cross-distribution,cross-class, cross-domain, cross-dataset, and cross-task scenarios,respectively. Due to space limitations and consistent trends acrossdifferent scenarios, we report results for cross-distribution, cross-class, and cross-domain scenarios here and defer those for cross-dataset and cross-task scenarios to Appendix F. The highest ASRand ACC values, and the lowest AD values achieved among all theattacks, including our CrossBA, are highlighted in bold.Superior attack performance of CrossBA across differentcross-context scenarios (RQ1 and RQ3). The results reveal thatCrossBA consistently outperforms the baseline methods across 5cross-context GPL scenarios, achieving the highest ASR valueswhile maintaining comparable ACC levels to those of backdoor-free models. Specifically, CrossBA achieves ASR values exceeding0.87 in all scenarios, with only a negligible difference in ACC com-pared to the backdoor-free models, at most 0.06 lower. In contrast,the baseline attacks fail to achieve comparable ASR to CrossBAacross various scenarios while maintaining ACC simultaneously. For example, the 2 baseline methods never achieve ASR above 0.32against the GT model trained by GraphPrompt in all 5 scenarios, andtheir ASR values against GAT models trained by all 3 GPL methodson CiteSeer are below 0.48 in cross-dataset scenarios. Additionally,when the baselines achieve comparable ASR to CrossBA, they suffera significant drop in ACC. For instance, in cross-class scenariosagainst ProG, the baseline attacks achieve an ASR of about 0.99 onCora, similar to CrossBA, but with an ACC at least 0.27 lower. Simi-larly, in cross-distribution scenarios against the GT model trainedby ProG, the baseline attacks achieve an ASR of about 0.96, closeto CrossBA, but with ACC values almost half of CrossBAs.Summary. CrossBA effectively generalizes the backdoor poison-ing effects to diverse cross-context scenarios, outperforming twobaseline methods, while maintaining the utility of GNN encoders fordownstream tasks. This observation confirms the theoretical prin-ciples outlined in .4, highlighting the efficacy of CrossBAin enhancing backdoor transferability in various cross-context sce-narios while preserving model utility in downstream tasks.Consistent attack performance of CrossBA against differ-ent GPL methods (RQ2). CrossBA demonstrates robust attackeffectiveness across various GPL methods, achieving high ASRwhile preserving the usability of backdoored models in downstreamtasks. Specifically, against all 3 GPL methods, CrossBA consistentlyachieves ASR values above 0.90 in cross-class, cross-distribution,cross-dataset, and cross-domain scenarios. In cross-task scenarios,CrossBA achieves ASRs exceeding 0.87. In contrast, baseline attacksfail to deliver consistent attack performance against different GPLmethods. For instance, in cross-class scenarios, both baseline meth-ods show ASR values below 0.05 against GAT models trained by allthree GPL methods on CiteSeer, indicating a complete failure of theattack. The baseline methods achieve ASR values over 0.99 againstProG on Cora but only reach ASRs of 0.07 for GraphPrompt.",
  "Attack Success Rate": "(a) Cross Distribution The number of trigger nodes 0.5 0.6 0.7 0.8 0.9 1.0 (b) Cross Class The number of trigger nodes 0.5 0.6 0.7 0.8 0.9 1.0 (c) Cross Dataset The number of trigger nodes 0.5 0.6 0.7 0.8 0.9 1.0 (d) Cross Domain The number of trigger nodes 0.5 0.6 0.7 0.8 0.9 1.0 (e) Cross Task Node: ProG w/o Prune Node: ProG w/ Prune Node: GraphPrompt w/o Prune Node: GraphPrompt w/ Prune Graph: ProG w/o Prune Graph: ProG w/ Prune Graph: GraphPrompt w/o Prune Graph: GraphPrompt w/ Prune : Impact of the number of trigger nodes on attack performance of CrossBA against ProG and GraphPrompt based onCiteSeer in 5 cross-context scenarios. \"Node\" represents the node classification task, and \"Graph\" denotes the graph classificationtask.",
  ": ASR of backdoor attacks against PruneG in 5 cross-context scenarios. \"-N\" denotes the node classification task, while\"-G\" represents the graph classification task": "that CrossBA ensures trigger node features closely resemble those ofclean nodes, achieving indistinguishable similarity for trigger edges,affirming its stealthiness. More details are provided in Appendix G.(3) Ablation Study. We conduct ablation studies on CrossBA to eval-uate the significance of its components. Three variants of CrossBAare considered: (1) CrossBA without trigger optimization, usinga fixed trigger graph and target embedding; (2) CrossBA withoutembedding alignment; (3) CrossBA without node feature affinity.The attack performance of CrossBA and its variants against 2 GPLmethods with PruneG across 5 cross-context scenarios is presentedin in Appendix H. The results show that removing anycomponent significantly reduces ASRs. The ASR value of CrossBAremains the highest one compared to the variants. These findingsaffirm the necessity of integrating trigger graph optimization, em-bedding alignment-based regularization, and node feature affinityconstraint together to achieve successful cross-context backdoorattacks against GPL.(4) Impact of Prompt Tokens. in Appendix I illustrateshow the number of prompt tokens affects the attack performance ofCrossBA against ProG across 5 cross-context scenarios. The resultsunveil that the effectiveness of CrossBA remains robust, exhibitinglittle impact from variations in the number of prompt tokens.(5) Impact of Trigger Nodes. in Appendix J illustratesthe impact of the number of trigger nodes on CrossBAs attackperformance against ProG and GraphPrompt across five differ-ent scenarios. The results demonstrate that CrossBA consistentlyachieves ASR values exceeding 0.80, regardless of the number oftrigger nodes, highlighting the attacks robustness and efficiency.",
  "CONCLUSION AND FUTURE WORK": "In this study, we conduct theoretical and empirical investigationsto assess the feasibility of backdoor attacks in cross-context graphprompt learning. Our findings reveal that optimizing trigger graphs,coupled with prompt transformations, significantly enhances back-door transferability. We introduce CrossBA, the first cross-contextbackdoor attack tailored for GPL, and evaluate its performanceacross five cross-context scenarios encompassing node and graphclassification tasks. Additionally, we explore potential defensesagainst such attacks. Our results demonstrate that CrossBA seam-lessly embeds backdoors into various downstream models acrossdiverse cross-context GPL scenarios without compromising maintask performance, even under defense deployment. However, we acknowledge that our study does not address backdoor attack trans-ferability in textual attribute graphs. As a result, we aim to investi-gate the transferability of backdoors within textual attribute graphsin the future.",
  "Taoran Fang, Yunchao Zhang, Yang Yang, and Chunping Wang. 2022. Prompt Tun-ing for Graph Neural Networks. CoRR abs/2209.15240 (2022). arXiv:2209.15240": "Taoran Fang, Yunchao Mercer Zhang, Yang Yang, Chunping Wang, and Lei CHEN.2023. Universal Prompt Tuning for Graph Neural Networks. In Thirty-seventhConference on Neural Information Processing Systems. Tianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making Pre-trained LanguageModels Better Few-shot Learners. In Proceedings of the 59th Annual Meeting ofthe Association for Computational Linguistics and the 11th International JointConference on Natural Language Processing, ACL/IJCNLP 2021. Association forComputational Linguistics, 38163830.",
  "Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and StephanGnnemann. 2018. Pitfalls of graph neural network evaluation. arXiv preprintarXiv:1811.05868 (2018)": "Yu Sheng, Rong Chen, Guanyu Cai, and Li Kuang. 2021. Backdoor Attack ofGraph Neural Networks Based on Subgraph Trigger. In Collaborative Computing:Networking, Applications and Worksharing - 17th EAI International Conference, Col-laborateCom 2021, Virtual Event, October 16-18, 2021, Proceedings, Part II (LectureNotes of the Institute for Computer Sciences, Social Informatics and Telecommuni-cations Engineering, Vol. 407). Springer, 276296. Yunsheng Shi, Zhengjie Huang, Shikun Feng, Hui Zhong, Wenjin Wang, and YuSun. 2020. Masked label prediction: Unified message passing model for semi-supervised classification. arXiv preprint arXiv:2009.03509 (2020).",
  "Zhaohan Xi, Ren Pang, Shouling Ji, and Ting Wang. 2021. Graph Backdoor.In 30th USENIX Security Symposium, USENIX Security 2021, August 11-13, 2021.USENIX Association, 15231540": "Jing Xu and Stjepan Picek. 2022. Poster: Clean-label Backdoor Attack on GraphNeural Networks. In Proceedings of the 2022 ACM SIGSAC Conference on Computerand Communications Security, CCS 2022, Los Angeles, CA, USA, November 7-11,2022. ACM, 34913493. Kaidi Xu, Hongge Chen, Sijia Liu, Pin-Yu Chen, Tsui-Wei Weng, Mingyi Hong,and Xue Lin. 2019. Topology Attack and Defense for Graph Neural Networks:An Optimization Perspective. In Proceedings of the Twenty-Eighth InternationalJoint Conference on Artificial Intelligence, IJCAI 2019, Macao, China, August 10-16,2019. ijcai.org, 39613967.",
  "Hongwei Yao, Jian Lou, and Zhan Qin. 2023. PoisonPrompt: Backdoor Attack onPrompt-based Large Language Models. arXiv:2310.12439 [cs.CL]": "Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, andYang Shen. 2020. Graph contrastive learning with augmentations. Advances inneural information processing systems 33 (2020), 58125823. Hangfan Zhang, Jinghui Chen, Lu Lin, Jinyuan Jia, and Dinghao Wu. 2023. GraphContrastive Backdoor Attacks. In International Conference on Machine Learn-ing, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA (Proceedings of MachineLearning Research, Vol. 202). PMLR, 4088840910. Qiannan Zhang, Shichao Pei, Qiang Yang, Chuxu Zhang, Nitesh V. Chawla, andXiangliang Zhang. 2023. Cross-Domain Few-Shot Graph Classification with aReinforced Task Coordinator. In Thirty-Seventh AAAI Conference on ArtificialIntelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications ofArtificial Intelligence, IAAI 2023, Thirteenth Symposium on Educational Advancesin Artificial Intelligence, EAAI 2023, Washington, DC, USA, February 7-14, 2023.AAAI Press, 48934901. Xiang Zhang and Marinka Zitnik. 2020. GNNGUARD: defending graph neuralnetworks against adversarial attacks. In Proceedings of the 34th InternationalConference on Neural Information Processing Systems (Vancouver, BC, Canada)(NIPS20). Curran Associates Inc., Red Hook, NY, USA, Article 777, 13 pages. Zaixi Zhang, Jinyuan Jia, Binghui Wang, and Neil Zhenqiang Gong. 2021. Back-door Attacks to Graph Neural Networks. In SACMAT 21: The 26th ACM Sympo-sium on Access Control Models and Technologies, Virtual Event, Spain, June 16-18,2021. ACM, 1526.",
  ": end while": "work for joint tuning of the trigger graph and a backdoored GNNencoder. Let denote the number of training samples, representthe feature dimensionality, be the number of training rounds forthe clean encoder, and reflect the time complexity of optimizingthe trigger. Initially, the clean GNN encoder is trained for rounds,with each round having a time complexity of (2), where the 2 term arises from pairwise feature interactions. Given the rounds,the initial training phase has a complexity of (2). Duringthe attack phase, the trigger is optimized using all samples withcomplexity (). Subsequently, the optimizer is employed to fine-tune the backdoored GNN encoder. The overall time complexityfor the attack process thus becomes (2 + ( +2)), where represents the number of attack rounds.",
  "anchor() = ( (() (anchor) 2)(8)": "where controls the smoothness of the kernel. anchor produces a softdecision output normalized between 0 and 1. A higher/lower outputfrom anchor indicates that has a more/less similar embedding tothat of anchor. With slight reformulations, the loss functions of the main learn-ing task Lclr and the backdoor learning task Lbdk in Eq.2 can beunified as a cross-entropy loss of the anchored classifier. This canbe expressed as:",
  "( {neganchor}) (1 anchor ())(9)": "where {posanchor} and {neganchor} are the sets of graphs forming posi-tive and negative pairs with the given anchor graph anchor.L simplifies to Lclr in Eq.2 if 1) {posanchor} denotes the set ofaugmented graphs derived from the trigger graph anchor = by adding or removing links randomly from ; and 2) neganchorrepresents any graph sampled from distribution that is not in theset of augmented graphs.Similarly, L simplifies to Lbdk in Eq.2 when 1) posanchor is definedas with ; and 2) neganchor follows the same setting asin Lclr, being any graph from other than .In this sense, the main learning task aims to minimize L, withthe distribution of input graphs as +anchor , where +anchordenotes the distribution of the augmented graphs of anchor. Incontrast, the backdoor learning task minimizes L over a differentdistribution of input graphs as anchor .Proof to Theorem 1. For any anchored classifier , we treat+anchor and anchor as the data distributions of thesource and target problem domain. Additionally, within the an-chored classifier, we designate the graph encoder as the back-doored encoder , which is trained during the pretraining stage.Following Theorem 1 in , the backdoor learning loss during thepretraining phase is upper bounded, as shown in Eq.10:",
  "(10)": "where represents a constant that encapsulates the model com-plexity of and the minimum achievable loss with the idealparameters for . (, ) is an RKHS kernel function measuring thesimilarity between graph embeddings produced by .Furthermore, for the simplicity of analysis, we assume the en-coder is a GIN model defined by: () = ( + (1 + )). denotes the adjacency matrix corresponding to the input graph. The matrix is a feature matrix, where each row correspondsto the features of node of . is a multi-layer encoder with aLipschitz constant denoted by . The first two terms are upperbounded independently of the kernel choice.",
  "where is the graph adjacency matrix of": "Building on Proposition 3, we can deduce the analytical form offeat( ) by assuming the encoder employs a GIN architecture.We follow the assumption originally established in the proof toProposition 3, the GIN structure has a linear layer for node fea-ture extraction and a sum readout function. Therefore, the nodeembedding produced by can be given as:",
  "Deg + (1 + )(16)": "wherefeat( ) is the-th element offeat( ). The injected promptgraph has nodes in total. We assume the node feature vec-tors in and the prompt graph share the same dimension.feat() thus denotes the -th node feature element of the node of the prompt graph. is the adjacency matrix of the promptgraph . Deg and are the total degree of all of the nodes andthe number of graph nodes of .In summary, embeddings of a prompted graph , ofthe downstream application context can be represented as below.Let (,) are the nodes of ,. feat( (,)) is the additionalperturbation over the node features of ,. feat() denotes thenode features of the prompt graph .",
  "SUM((, + (1 + )) (feat( (, )) + feat( (, ))) ) 22(19)": "where 2 is the -2 norm. Therefore, for any set of the graphs{,} and {, } for pretraining and the downstream learningtasks, there exists an optimal feat() solving the optimizationproblem in Eq.19.Proof of Proposition 2. Given a graph as the trigger graph inCrossBA, an augmented graph + is derived by adding / removinglinks from . The backdoored graph is generated bylinking the trigger graph to an anchor node in an input graph. Proposition 4. Proposition 4 in . We perform a link trans-formation over a graph , i.e. adding / removing links from a graph containing nodes, to derive an augmented graph +. For a frozengraph encoder , we assume each node of an input graph has avdim-dimensional feature vector. There hence exists a node-wise fea-ture perturbation feat( ) vdim applied to the node features of (noted as feat( )), which satisfies:",
  "CDATASETS AND GNN MODELS": "CiteSeer comprises 3,312 scientific publications categorizedinto 6 classes, connected by 4,732 citation links. Cora consistsof 2,708 scientific publications, each assigned to one of 7 categorieswithin a citation network with 5,429 links. Amazon-Computersand Amazon-Photo are subgraphs of the Amazon co-purchasegraph, where nodes represent products, and edges indicate frequentco-purchases. Amazon-Computers has 13,752 nodes across 10 cat-egories and 491,722 edges, while Amazon-Photo comprises 7,650nodes from 8 categories and 238,162 edges. The ENZYMES dataset contains 600 enzymes from the BRENDA enzyme database, clas-sified into 6 EC enzyme categories. For graph classification datasetsderived from node classification datasets, we follow the methodol-ogy proposed in , involving edge and subgraph sampling fromthe original data. Detailed statistics are presented in , wherethe last column indicates the type of downstream task for eachdataset: \"N\" for node classification and \"G\" for graph classification.GAT and GT are two advanced GNN models. GATemploys neighborhood aggregation for node embedding learningand distinguishes itself by assigning varied weights to neighboringnodes, thereby modifying their influence in the aggregation process.GT integrates the processing capabilities for graph-structured datawith the self-attention mechanisms of Transformer networks, effec-tively capturing complex relationships and feature dependenciesbetween nodes in a graph.",
  "DBASELINE ATTACKS": "GCBA is the most relevant backdoor attack method to ourstudy, given the threat model definition. Unlike CrossBA, whichtargets the GPL framework, GCBA aims to inject backdoor poi-soning noise into a GNN encoder trained using Graph ContrastiveLearning (GCL). The attacker in GCBA can collect the graph data ofthe target class in the downstream applications. The attack is thenformulated to maximize the similarity between the embeddings ofbackdoored graph data and those of clean graph data belonging tothe target class in the downstream task, as well as the similaritybetween embeddings of clean data from both the backdoored andclean GNN encoders. However, GCBA is not directly applicable tocross-context GPL scenarios as it relies on access to downstream application data. To facilitate a fair comparison, we introduce twovariants of GCBA, namely GCBA_R and GCBA_M. In both vari-ants, the attacker first clusters the embeddings of clean graph datacollected during pretraining using the backdoor-free GNN encoder.Then, GCBA_R randomly selects an embedding centered arounda cluster in the embedding space as the target embedding, whileGCBA_M selects the cluster center furthest away from other classcenters as the target embedding. Subsequently, both variants followthe same workflow as GCBA to execute the attack.Compared to CrossBA, GCBA differs in two significant aspects inits algorithmic design. Firstly, while CrossBA optimizes the targetembedding during training of the backdoored GNN encoder (asshown in Eq. 2), GCBA uses a fixed target embedding derived fromthe downstream datasets target class. Secondly, CrossBA includesa node feature affinity constraint in its attack objective, aiding incircumventing countermeasures that inspect node feature outliers,such as PruneG. However, GCBA permits arbitrary node features inthe injected trigger graph. Consequently, CrossBA presents a morestealthy attack compared to GCBA, particularly against counter-measures deployed by downstream users.",
  "EIMPLEMENTATION DETAILS": "We implement CrossBA using PyTorch and execute it on an NVIDIA3090 GPU. For both GAT and GT, we adopt a two-layer graph neuralnetwork structure with a hidden dimension set to 100. Followingthe methodology of ProG , we utilize Singular Value Decomposi-tion (SVD) to reduce the initial feature dimension of the data to 100.The number of prompt nodes used in ProG and ProG-Meta is setto 15. The self-supervised learning method employed by ProG andProG-Meta is GraphCL . Consistent with the original paperssettings, we employ the Adam optimizer for ProG and ProG-Meta.On most evaluated datasets, the learning rate for GT is set to 0.001,and for GAT, it is set to 0.0001. Similarly, following the settings ofGraphPrompt , we use the AdamW optimizer with a unifiedlearning rate of 0.01 for all test instances. The self-supervised learn-ing method used by GraphPrompt is the link prediction method .Additionally, for downstream tasks, we employ 200-shot learningfor ProG and ProG-Meta, and 100-shot learning for GraphPrompt.In this study, we investigate five distinct cross-context scenarios.To address cross-distribution, cross-dataset, and cross-domain sce-narios, we adopt the methodology of ProG , which leveragesclustering methods to divide the entire graph into 200 distinct sub-graphs for creating the pretraining dataset. For downstream tasks,we follow ProGs approach to construct induced graph datasets tai-lored for both node and graph classification tasks. In cross-class sce-narios, we initially divide the label space into pretraining classes anddownstream classes. Subsequently, we partition the entire graphinto two disjoint subgraphs according to these classes, which arethen used to form induced graph datasets for pretraining and down-stream tasks, respectively. For the cross-task scenario, we utilizeeither the real graph classification dataset ENZYMES or the gen-erated induced graph dataset for graph classification tasks as thepretraining dataset. The induced graph dataset for node classifica-tion tasks is employed as our downstream dataset.Regarding the backdoor attack setting, for all the attack meth-ods, the trigger graph is designed to consist of only three nodes,",
  ": Kernel density estimation of node feature similarityon CiteSeer": "significantly fewer than the number of nodes in the input graph.During pretraining, the attacker randomly selects a node in theinput graph as the anchor node. We set = 0.5 for both the baselineattack methods and CrossBA. Additionally, for CrossBA, we set and to 0.05 across the majority of datasets. The Adam optimizeris used for optimizing the trigger graph and the backdoored GNNencoder. Specifically, for CrossBA, we set to 0.01 and to 0.0001.For GCBA_R and GCBA_M, following the setting in , we set to 0.0015 and to 0.001.",
  "FATTACK RESULTS IN CROSS-DATASET ANDCROSS-TASK SCENARIOS": "highlights the attack performance of various backdoorattacks in cross-dataset scenarios, where Cora is used as the pre-training dataset for CiteSeer and CiteSeer-Graph, while Computersserves as the pretraining dataset for Photo and Photo-Graph. Theresults underscore the superior attack capabilities of CrossBA acrossdiverse downstream applications. CrossBA consistently achievesASR values surpassing 0.83 in all tested cases, with a maximum de-crease of only 0.06 in ACC compared to the clean model. In contrast,baseline methods encounter challenges in transferring backdoors todownstream models without significantly compromising the maintasks performance.In cross-task scenarios, the GNN encoder model is pretrained onENZYMES, consisting of actual molecular graphs. The downstreamnode classification tasks involve CiteSeer and Cora. From the re-sults in , we find that CrossBA effectively injects backdoorsinto downstream models pretrained on the graph classificationtask while maintaining high ACC values in node classificationtasks. Conversely, the baseline methods often face challenges intransferring backdoors to downstream node classification tasks.For example, when targeting GraphPrompt, the baseline methodsstruggle to achieve an ASR higher than 0.41. In contrast, CrossBAconsistently achieves ASR values surpassing 0.97.",
  "GATTACK AGAINST PRUNEG": "We explore potential countermeasures against the CrossBA attack.Given the lack of specific defenses for cross-context GPL scenarios,we adapt defenses from other scenarios to mitigate the CrossBAattack. Real-world graphs, such as social networks, typically exhibithomophily, where nodes with similar features are connected byedges. However, trigger graph injection can disrupt this homophily.Therefore, a category of defense methods exists that improves GNNrobustness by pruning edges connecting nodes with low feature similarity . We extend the PruneG defense to align with ourthreat model, where the downstream user acts as the defender.Given a graph, the defender calculates the similarity of node fea-tures connected by edges and prunes those edges with similaritybelow the threshold, removing the component with fewer nodesthat the edge connects. shows the PruneG defenses effectiveness against back-door attacks in both node and graph classification tasks across5 cross-context scenarios and 2 GPL methods. Cora is employedfor pretraining in cross-dataset scenarios, Photo serves as the pre-training dataset in cross-domain scenarios, and the induced graphdataset for the graph classification task is used as the pretrainingdataset in cross-task scenarios. All the baseline methods exhibitpoor attack performance facing the PruneG defense. In contrast,CrossBA successfully evades detection, achieving high ASR values.For example, in cross-domain scenarios, CrossBA attains ASR val-ues above 0.90 on all the tested cases, while the baselines ASRvalues remain below 0.50. In CrossBA, the optimization of triggernode features is constrained to closely resemble those of the cleananchor nodes. illustrates the kernel density of node fea-ture similarity values, highlighting that trigger nodes created bythe baseline attacks exhibit minimal similarity to anchor nodesfeatures, making them more easily detected. Conversely, CrossBAachieves indistinguishable similarity for trigger edges compared toclean edges, confirming its stealthiness.",
  "HABLATION STUDY": "In our ablation studies on CrossBA to assess the significance ofits components, we consider three variants: (1) CrossBA withouttrigger optimization, employing a fixed trigger graph and targetembedding; (2) CrossBA without embedding alignment ( = 0); (3)CrossBA without node feature affinity ( = 0). illustratesthe attack performance of CrossBA and its variants against two GPLmethods with the Prune defense, encompassing five cross-contextscenarios. Cora is used for pretraining in cross-dataset scenarios,Photo serves as the pretraining dataset in cross-domain settings,and the induced graph dataset for the graph classification task isemployed in cross-task scenarios.Comparing CrossBA to its three variants, we observe that CrossBAconsistently maintains stable attack performance across variouscross-context GPL scenarios, even in the presence of defense mech-anisms. Each component of CrossBA is crucial for achieving highASR values. For instance, in cross-task scenarios involving Graph-prompt, the removal of any component significantly lowers ASRvalues to below 0.20. Conversely, CrossBA attains an ASR of 0.90,underscoring the significance of each component to the attackssuccess.",
  "IIMPACT OF PROMPT TOKENS": "illustrates the impact of the number of prompt tokens onthe attack performance of CrossBA against ProG in both node andgraph classification tasks on CiteSeer across five cross-context sce-narios. In cross-dataset scenarios, Cora is employed for pretraining,while Photo serves as the pretraining dataset for cross-domain set-tings. In cross-task scenarios, the generated induced graph datasetfor the graph classification task is utilized for pretraining.",
  "Node": "GraphPrompt 0.0 0.2 0.4 0.6 0.8 1.0(e) Cross Task 0.00.20.40.60.81.0 0.0 0.2 0.4 0.6 0.8 1.0 w/o embedding alignmentw/o node feature affinityw/o trigger optimizationCrossBA : Ablation study of CrossBA against 2 GPL methods with Prune on CiteSeer across 5 cross-context scenarios. \"Node\"represents the node classification task, and \"Graph\" denotes the graph classification task. The number of prompt tokens 0.5 0.6 0.7 0.8 0.9 1.0",
  "JIMPACT OF TRIGGER NODES": "illustrates the impact of varying the number of triggernodes on the attack performance of CrossBA against ProG andGraphPrompt in both node and graph classification tasks on Cite-Seer across five cross-context scenarios. In cross-dataset scenarios,the pretraining is conducted using Cora, while Photo serves as thepretraining dataset for cross-domain settings. In cross-task scenar-ios, the generated induced graph dataset for the graph classification"
}