{
  "ABSTRACT": "Over the past few years, Federated Learning (FL) has become anemerging machine learning technique to tackle data privacy chal-lenges through collaborative training. In the Federated Learningalgorithm, the clients submit a locally trained model, and the serveraggregates these parameters until convergence. Despite significantefforts that have been made to FL in fields like computer vision,audio, and natural language processing, the FL applications uti-lizing multimodal data streams remain largely unexplored. It isknown that multimodal learning has broad real-world applicationsin emotion recognition, healthcare, multimedia, and social media,while user privacy persists as a critical concern. Specifically, thereare no existing FL benchmarks targeting multimodal applicationsor related tasks. In order to facilitate the research in multimodalFL, we introduce FedMultimodal, the first FL benchmark for mul-timodal learning covering five representative multimodal applica-tions from ten commonly used datasets with a total of eight uniquemodalities. FedMultimodal offers a systematic FL pipeline, enablingend-to-end modeling framework ranging from data partition andfeature extraction to FL benchmark algorithms and model eval-uation. Unlike existing FL benchmarks, FedMultimodal providesa standardized approach to assess the robustness of FL againstthree common data corruptions in real-life multimodal applica-tions: missing modalities, missing labels, and erroneous labels. Wehope that FedMultimodal can accelerate numerous future researchdirections, including designing multimodal FL algorithms towardextreme data heterogeneity, robustness multimodal FL, and efficientmultimodal FL. The datasets and benchmark results can be accessedat:",
  "Federated Learning, Multimodal Learning, Multimodal Benchmark": "ACM Reference Format:Tiantian Feng, Digbalay Bose, Tuo Zhang, Rajat Hebbar, Anil Ramakrishna,Rahul Gupta, Mi Zhang, Salman Avestimehr, and Shrikanth Narayanan.2023. FedMultimodal: A Benchmark For Multimodal Federated Learning. InProceedings of the 29th ACM SIGKDD Conference on Knowledge Discoveryand Data Mining (KDD 23), August 610, 2023, Long Beach, CA, USA. ACM,New York, NY, USA, 11 pages.",
  "INTRODUCTION": "With rapid advances in machine learning (ML) in the pastdecade, modern mobile devices and wearable sensors haverevolutionized applications and services in industries ranging fromentertainment and transportation to healthcare and defense, signifi-cantly changing how people live, work, and interact with each other.These intelligent sensing devices, equipped with sensors of multiplemodalities, can capture diverse information about a user, includingbut not limited to physiological, emotional, and rich spatiotemporalcontextual information . These data records aretypically transmitted to remote servers for centralized training ofthe ML models. However, the collection of human-centered dataraises significant concerns about compromising user privacy due toassociation with sensitive environments and contexts, ranging fromhomes, workplaces, and business meetings to hospitals and schools. Therefore, it is critical to ensure that modern ML systems canprotect user privacy by preventing any unauthorized access to data.In response to this, ML practitioners have developed FederatedLearning as an alternative paradigm to build models, without theneed to transfer user data from the edge devices . Unlike cen-tralized training, models are trained locally using locally stored",
  "FedMultimodal (Ours)": "data, and updated parameters are transmitted to the server insteadof raw data. FL allows clients to train a model collaboratively with-out sharing their local data, making it one of the most emergingprivacy-enhancing learning algorithms in ML research .Previous works in FL have primarily focused on designing robustand efficient algorithms for federated model training. FedAvg was the earliest FL optimization algorithm to train the model inthe distribution mechanism. In FedAvg, each client executes localmodel updates before submitting the updates to the server. Eventhough FedAvg offers possibilities for deploying FL in the wild,it often encounters slow convergence as a consequence of gradi-ent drifting from data heterogeneity. As such, researchers haveproposed algorithms such as stochastic Controlled Averaging Algo-rithm (SCAFFOLD) and FedProx to minimize the impact ofgradient drift for heterogeneous data. For example, SCAFFOLD ac-celerates the training speed through control variates which preventthe client gradients from drifting away from the global optima. Sim-ilarly, introduced adaptive optimization algorithms, FedOpt,that allow server optimization through momentum.To facilitate FL research in more diverse problem domains, anumber of FL benchmarks have been developed in the past fewyears. For example, LEAF was the earliest FL benchmark whichincludes multiple FL training tasks on 5 datasets covering variouscomputer vision and NLP tasks. FedML , besides providing anopen-source library and a platform for federated learning deploy-ment, it includes multiple FL benchmarks on computer vision andhealth , data mining , IoT , and NLP . More recently, announced a multi-domain FL benchmark called FedScale. Fed-Scale included implementations with 20 realistic FL datasets mainlyin computer vision and natural language processing applications. introduced an FL simulation tool named FLUTE, which coversthe application of CV, NLP, and audio tasks. Meanwhile, pre-sented an audio-centric federated learning framework, FedAudio,which focused on speech emotion recognition, keyword spotting,and audio event classification. Further, FLamby is a recentlyproposed FL benchmark for a wide range of healthcare applicationssuch as identifying lung nodules and predicting death risks. Feder-atedScope also incorporates various benchmarks for federatedlearning in CV, NLP, and data mining . Existing Multimodal Federated Learning Works: While exist-ing FL benchmarks largely focus on unimodal applications suchas computer vision (CV), natural language processing (NLP), andspeech recognition, a significant number of real-world applicationsare associated with multimodal data streams. As listed in , was one of the earliest to investigate FL using multi-sensory data. They proposed a self-supervised learning approach calledScalogram-signal Correspondence Learning (SSCL) to learn robustmulti-modal representations in FL. More recently, designed amulti-modal FL framework named MMFed using the cross-attentionmechanism. Moreover, proposed an FL framework called FedM-Split that targeted the issue of missing modalities in the multimodalsetup. CreamFL provides a multi-modal FL framework usingcontrastive representation-level ensemble to learn a larger servermodel from heterogeneous clients across multi-modalities. How-ever, existing multimodal FL frameworks perform their evaluationusing their defined experimental setups, thus making it challengingfor researchers to compare their methods with existing state-of-the-art fairly and effectively.",
  "Our Contributions: In this work, we introduce FedMultimodal, aFL benchmark for multimodal applications. We summarize our keycontributions as follows:": "FedMultimodal includes ten representative datasets coveringfive diverse application scenarios emotion recognition, mul-timodal action recognition, human activity recognition, health-care, and social media that are well aligned with FL. We presentsystematic benchmark results on the above datasets to facilitateresearchers to fairly compare their algorithms. To help the community accurately compare performance and en-sure reproducibility, FedMultimodal presents an open-sourceend-to-end FL simulation framework and includes capa-bilities to perform data partitioning, feature processing, andmultimodal training. FedMultimodal offers support for severalpopular FL optimizers including FedAvg , FedProx , Fe-dRS , SCAFFOLD , and FedOpt , and provide flex-ibility that allows users to customized the trainers on the in-cluded datasets. The source codes and user guides are availableat In addition to ensuring accessibility and reproducibility, thebenchmark provides a robustness assessment module thatallows researchers to simulate challenges uniquely tied to multi-modal FL applications in real-world scenarios. As listed in ,previous works on multimodal FL provide limited assessmentsof the robustness under real-world settings. Specifically, Fed-Multimodal emulates missing modalities, missing labels, anderroneous labels on top of the provided datasets to simulate sce-narios when deploying FL systems in real-world settings. This isa crucial difference and a unique contribution of FedMultimodalcompared to existing FL literature.",
  "MULTIMODAL DATASETS AND TASKS": "provides an overview of the 10 datasets included in FedMul-timodal. These 10 multimodal datasets cover five diverse tasks Emotion Recognition, Multimedia Action Recognition, Human Ac-tivity Recognition, Healthcare, and Social Media classification. Oneimportant reason we select these 10 datasets is that they are publiclyavailable, thus ensuring ease of accessibility and reproducibility. Inthis section, we provide a brief overview of each included datasetand the corresponding tasks.",
  "Emotion Recognition (ER)": "Emotion recognition (ER) has broad applicability of ER in virtualassistant-based tasks, human behavior analysis, and AI-assistededucation, making it a valuable research topic in FL . Fed-Multimodal benchmark incorporates two widely used datasets inthis category: MELD and CREMA-D. MELD is a multiparty dialog dataset containing over 9k utter-ances with audio and transcripts data from the Friends TV series.Due to the imbalanced label distribution in the dataset, we keep 4emotions with the most samples i.e., neutral, happy, sad, and angry.",
  "Multimodal Action Recognition (MAR)": "The task of MAR consists of classifying a video into action cate-gories based on underlying visual and audio modalities. In FedMul-timodal, we include two well-known MAR testbeds: UCF101 andMoments in Time (MiT). UCF101 dataset consists of 13,320 web videos with 101 sport-based action labels. However, data associated with only 51 labelsare presented with video and audio information, resulting in lessthan 7,000 videos for the experiments. The duration of the videosranges from several seconds to over 20 seconds. We subsample thevideo at the frame rate of 1Hz to reduce the computation overhead. Moments in Time (MiT) is a large-scale MAR ( 1 million) dataset with short (3 seconds) videos with overall list of 339 actionlabels. It is worth noting that MiT is a challenging dataset, withstate-of-the-art top-1 accuracy close to 35% . Given the inherentdifficulty of this task, we tackle the easier classification problemby creating partitions of data with fewer distinct labels. We createtwo sub-datasets, MiT10 and MiT51, from the original MiT dataset.MiT10 and MiT51 contain videos of the 10 and the 51 most frequentlabels. Similar to the UCF101 setting, we subsample the video every10 frames to accommodate the computing constraints in FL.",
  ". FedMultimodal provides the implementation on two HARdatasets: UCI-HAR and KU-HAR. In our experiments, we treat theaccelerometer and gyroscope data as two different modalities": "UCI-HAR dataset consists of smartphone sensors (Accelerome-ter and Gyroscope) data from 30 subjects (19-48 yrs old) performingsix daily activities: walking, walking upstairs, walking downstairs,sitting, standing, laying. The participants wear smartphones ontheir waists during the collection phase. The accelerometer andgyroscope data are sampled at 50Hz. KU-HAR is a recent human activity recognition dataset col-lected with a group of 90 participants (75 male and 15 female) on18 different activities. Instead of evaluating the 18 activities, wedecided to keep 6 activities existing in the UCI-HAR dataset whileadding jumping and running activities.",
  "Healthcare": "Healthcare ML applications have made immense progress in a rangeof domains e.g., heart-disease classification over the last decade.FedMultimodal explores the problem of ECG classification basedon the PTB-XL dataset. PTB-XL includes over 20,000 clin-ical 12-lead ECG recordings from 18,885 patients for a multi-labelclassification task. There are 5 classes describing ECG diagnosis,including normal ECG, myocardial infarction, ST/T change, con-duction disturbance, and hypertrophy. As suggested by , weuse the ECG data provided at the sampling frequency of 100 Hz.We separate the readings from electrodes I, II, III, AVL, AVR, AVF,and V1-V6 as two modalities suggested by .",
  "Social Media (SM)": "Social media has become an increasingly important tool duringdisasters and emergencies for people to track the latest updates inthe area, especially the impact (e.g., property damage, injury, anddeath) of the disaster, as well as urgent needs for help. However,the widespread adoption of social media has also drawn significantconcerns about spreading misinformation, thus urging the need toidentify and mitigate this misleading and harmful content. To accel-erate FL research in this domain, FedMultimodal incorporates twosocial-media-based multimodal datasets related to hateful contentand crisis information classification.",
  "END-TO-END MULTIMODAL FEDERATEDLEARNING FRAMEWORK": "To benchmark the performance of the multimodal datasets de-scribed in as well as to support future research in thearea of multimodal federated learning, we have built an end-to-endmultimodal federated learning research framework. 1 il-lustrates the overall architecture of the framework. As shown, ourframework covers the complete pipeline of multimodal federatedlearning, which includes six key components: (1) non-IID data par-titioning, (2) feature processing, (3) multimodal models, (4) fusionschemes, (5) federated optimizers, and (6) real-world noise factoremulator. In particular, one key difference between FedMultimodaland existing multimodal FL literature is that FedMultimodal takesthe real-world noise factors into consideration and examines modelrobustness to three real-world noise factors: missing modalities,missing labels, and erroneous labels. In this section, we describeeach of the six key components in detail.",
  "Non-IID Data Partitioning": "The non-IID data partitioning is a fundamental step in emulating FLexperiments. The first partition scheme is through the unique clientidentifier. For example, speech-related datasets, like CREMA-D andMELD, comprise speech-text or speech-visual data organized byspeaker IDs. Hence, it is natural to use speaker IDs to partition theclient data in FL, creating authentic non-IID data distributions. Sim-ilarly, we consider partitions in datasets like KU-HAR and PTB-XLcomprise data with based on participant IDs and clinical site IDs,respectively. On the other hand, other multimodal datasets usedin this paper, including MAR and SM datasets, do not have suchrealistic client partitions thus requiring ML practitioners to synthe-size non-IID data distributions. Following prior works, we partitionthese datasets using Dirichlet distribution with {0.1, 5.0} tocontrol the level of data heterogeneity, where = 0.1 and = 5.0represents high heterogeneity and low heterogeneity, respectively.Although the original UCI-HAR datasets consist of data partitionedby participants, each participant performed the same amount ofactivities from each category, making the label distribution IID inUCI-HAR. Hence, we increase the heterogeneity of data distributionby dividing each participants data using the Dirichlet distribution.",
  ": The architecture of the basic model": "Mainly, the selected feature needs to align with the computationcapabilities available on the edge computing devices. For exam-ple, it is unrealistic to assume that edge devices could load and runlarge transformer-based models for inference purposes withoutsacrificing system performance. Hence, we focus on implement-ing mobile-friendly feature extraction pipelines in FedMultimodalwhich are listed below, targeting swift computation, efficient stor-age, and ease of deployment. Visual: For the visual data, our benchmark supports MobileNetV2 and MobileViT as the embedding network to extractlatent presentations. The complete MobileNetV2 and MobileViThave 4.3M and 2.7M parameters, respectively, making them prac-tical visual feature backbones in FL. Due to space constraints, wereport benchmark results with MobileNetV2 in this paper. Text: FedMultimodal integrates both MobileBERT and Dis-tillBERT to extract representations from textual data. Mo-bileBERT uses a bottleneck structure to reduce the parameter sizefrom 340M to 25M when compared to BERT , while Distill-BERT applies a knowledge distillation process that decreases theBERT model to 66M parameters. We decide to benchmark withthe MobileBERT feature backbone given the page constraints.",
  "Multimodal Models": "Model Design Principles. Compared to remote servers, edgecomputing nodes are more appropriate for lightweight computingtasks due to constraints in computation resources, storage capa-bilities, battery capacities, and communication bandwidths. Whendesigning ML models for resource-constrained devices, a signifi-cant design consideration is to reduce the number of parametersin edge ML models, thus reducing memory and execution latency.Such models can either be the backbone feature extraction mod-els or application-specific prediction models. One major designprinciple of FedMultimodal is to study lightweight but effectivesolutions for multimodal FL learning instead of training modelswith multi-million parameters.",
  "UCI-HARKU-HAR10%10%0.050.05200200PTB-XL25%0.05200Hateful-MemesCrisisMMD25%10%0.050.05200200": "Model Architecture. With this design principle in mind, we con-struct ML models mainly based on the 1D Conv-RNN/MLP architec-ture. Even though the transformer-based model has achieved SOTAperformance in diverse applications, these models typically includemillions or even billions of parameters, making them impracticalto use in FL settings as a result of massive computations, memoryusage, and battery consumption during back-propagation .An example model architecture is presented in . Specifi-cally, the multimodal model in FedMultimodal includes an encoder,a modality-fusion block, and a downstream classifier. The encoderpart follows either Conv+RNN architecture or RNN-only archi-tecture. The encoder that adopts Conv+RNN architecture takesthe input of audio, accelerometer, gyroscope, and ECG informa-tion, otherwise uses RNN-only architecture. Following encodermodules, FedMultimodal uses a late-fusion mechanism to combinemodality-specific representations into a multimodal representation.The multimodal representation is then fed through 2 dense layersfor downstream predictions.",
  "Fusion Schemes": "In this work, we present two basic fusion approaches: concatenation-based fusion and attention-based fusion. In the concatenation-basedfusion, the average pooling operation is first performed on theGRU output. After that, we concatenate the pooling embeddings toform the multimodal embedding. On the other hand, the attention-based fusion concatenates the temporal output from each modalitywithout the average pooling step. We apply an attention mecha-nism similar to hierarchical attention . Given the concatenatedmultimodal data , the attention procedures are as follows:",
  "=": "The concatenated multimodal data is first fed through a one-layer MLP to get representation . We then use a context vector toobtain a normalized importance score through a softmax function.After that, we compute the final multimodal embedding as aweighted sum of based on the weights . Here, we can furtherimplement a multi-head attention mechanism by having multiple .We would also stress that this attention mechanism is lightweight,thus making it realistic to deploy on a variety of edge devices. Inaddition, the attention mechanism allows us to mask the missing",
  "Federated Optimizers": "First, most existing FL training algorithms are validated in uni-modal settings, and their efficacy on multimodal tasks remainsunexplored. As a result, FedMultimodal is suited to several popularFL algorithms, including FedAvg , FedProx , FedRS ,and FedOpt . Particularly, FedOpt holds state-of-the-art perfor-mance across multiple unimodal applications . One objectiveof FedMultimodal is to provide comprehensive evaluations acrossvarious FL algorithms.",
  "Real-world Noise Factor Emulator": "Prior literature (see ) on multimodal FL provides little orno assessment of their robustness in real-life settings. In orderto provide a comprehensive evaluation of multimodal FL modelstoward safe and robust deployment, FedMultimodal enables theemulation of missing modalities, missing labels, and erroneouslabels for real-world multimodal FL.Missing Modality. In practice, data sources, whether they aremicrophones, cameras, mobile hardware, or medical electrodes, areprone to data imperfections or complete data losses (e.g., missingmodalities) caused by firmware malfunctions, network disconnec-tions, or sensor damages . Hence, it is critical to design anemulator module to synthesize the cases of missing modalitiesfor some clients. FedMultimodal provides the simulations of miss-ing modalities as suggested by , where the availability of eachmodality follows a Bernoulli distribution. We set an equal missingrate for each modality in the following experiments.Missing Labels. Not only can the multimodal FL encounter data im-perfection challenges, it can also suffer from missing label problems.Surprisingly, most prior works have made the ideal assumption that the data stored on edge devices are fully annotated with ground-truth labels. However, in a more realistic real-world FL setting, weargue that only a portion of the data can come with labels. As such,FedMultimodal allows the missing label simulation to assess therisk of decreased robustness.Erroneous Labels. In addition to missing labels, real-world FLimplementations encounter label noise as a result of bias, skilldifferences, and labeling errors from the annotators. Inspired by, we apply a label error generation process described in .In summary, the erroneous labels are generated using a transitionmatrix , where , = P( = | = ) indicates the chance ofground-truth label being erroneous annotated with label .",
  "EXPERIMENTS AND DISCUSSION4.1Experimental Details": "Setup. We adopt the RNN-only model architecture to the video andtext modalities while utilizing the Conv-RNN model architecturein other modalities. Specifically, the model with the convolutionalmodule consists of 3 convolution layers with the number of filtersin {16, 32, 64} and the filter kernel size of 5 5. Moreover, we set thehidden layer size of RNN as 128. We choose ReLU as the activationfunction and the dropout rate as 0.2. The number of attention headsis 6 in all experiments. We fixed the batch size for all datasets to 16and the local epoch to 1 for all experiments.Additionally, we set the training epochs as 200 for all datasetsexcept the MiT sub-datasets. However, the total training epoch is300 in MiT10 and MiT51 as these 2 datasets contain more data thanthe other datasets. Hyperparameter details such as the learning andclient sampling rates for each dataset are listed in . Due tothe limited number of clients in the Hateful Memes dataset andPTB-xl datasets, we apply a higher client sample rate in these 2datasets. In the FedOpt algorithm, we search the server learningrate in a range from 103 to 2.5 103. Meanwhile, the proximalterm ranges from 102 to 100 in the FedProx algorithm.",
  ": Relative performance changes under different missing modality rates": "Evaluation Metrics. We follow established practices from theliterature while conducting evaluations on each dataset. Specifi-cally, evaluation metrics (e.g., F1) and validation protocols (e.g.,predefined splits) are two fundamental components to ensure com-parability with past (and future) works. With datasets that providea pre-defined partition for training/validation/testing, we repeat theexperiments 5 times using different seeds. We perform 5-fold cross-validation on datasets without such pre-defined experimentingrules. We provide details about our evaluation methods in .",
  "Overall Performance": "We first report the comparisons between two fusion mechanisms(attention-based fusion and concatenation-based fusion) in. From the results, we can find that the attention-based fu-sion mechanism outperforms concatenation-based fusion in themajority of the datasets. Specifically, the attention-based fusionmechanism leads to better performances in most high data hetero-geneity conditions, but it underperforms the concatenation-basedfusion in two synthetic datasets with = 5.0 (low data heterogene-ity). Moreover, we observe that the FedOpt algorithm consistentlyyields better baselines compared to other FL algorithms with afew exceptions in low data heterogeneity conditions. However, wewould like to highlight that, in practice, FedOpt requires additionalhyperparameter tuning on the server learning rate to reach the bestperformance. Overall, these results imply that the fusion mecha-nism is a critical factor impacting multimodal model performancein data heterogeneous FL. Moreover, HAR tasks are associated with the highest perfor-mance scores, suggesting the simplicity of this learning task. Incontrast, classification results on the Hateful Memes dataset andCrisisMMD dataset imply that social media classification is a chal-lenging task using FL, with the best model performance on theCrisisMMD dataset below 30%. A plausible explanation is that thepre-trained models that we rely on are not generalized to socialmedia data, generating image and textual features that are unrep-resentative of downstream learning models. On the other hand,performances on the MiT51 dataset demonstrate similar findingspointed out from , validating that MiT is a challenging dataset.However, reducing the number of labels indeed simplifies the pre-dicting task, resulting in moderate model performance on MiT10.",
  "Uni-modality vs. Multi-modalities": "One fundamental research question centering around multimodallearning is its performance compared to unimodal models. For ex-ample, the previous multimodal benchmark , with an emphasison centralized learning setup, demonstrates that unimodal learn-ing could yield similar performance to multimodal models withfewer parameters. Similar to MultiBench, FedMultimodal providesthe unimodal FL to compare with multimodal FL baselines. Wesummarize the benchmark comparisons between unimodal FL andmultimodal FL in . The comparisons use datasets with nat-ural partitions or high data heterogeneity partitions. Overall, weobserve that unimodal learning provides competitive performancecompared with the multimodal FL benchmarks, complying withcentralized benchmark results reported in . Nevertheless, in",
  "Impact of Missing Modalities": "As described in earlier sections, a unique challenge associated withmultimodal learning is dealing with scenarios of missing modali-ties . In this section, we benchmark our selected datasetswith different rates of missing modalities. In this experiment, weassume that the availability of each modality follows a Bernoullidistribution with a missing rate of . Following the experimentprotocol presented by , we set a uniform missing rate of foreach modality, where {0.1, 0.2, 0.3, 0.4, 0.5}. As described in themultimodal model section, attention-based fusion allows us to trainthe model even with missing data through masking. To train themodel with the missing entries, we fill the missing data with 0 while masking out the corresponding data points in calculatingattention scores.We present the relative model performance changes at differentmissing modality rates in . From the graph, we find that therelative performance changes with missing rates below 30% are notsubstantial, suggesting that a small amount of missing modalitiesin deployment does not impact the final model performance in mul-timodal FL. Furthermore, we observe that the model performancestarts to decline substantially at the missing rate of 50%. Surpris-ingly, we observe that half of the models have relative performancedecreases that are under 10%, suggesting that the provided baselinemodels that use attention-based fusion still learn useful informa-tion in these cases. However, we find that the missing modalityintroduces a significantly larger impact on CrisisMMD data and the",
  "Impact of Missing Labels": "Missing labels is a widely presented challenge in FL. In this section,we perform evaluations of missing label conditions using the Fed-Multimodal benchmark. Similar to the missing modality experiment,we assume that the availability of each label follows a Bernoulli dis-tribution with a missing rate . We apply the FedMultimodal bench-mark to emulate the missing label rate {0.1, 0.2, 0.3, 0.4, 0.5}.The goal of this experiment is to quantify the impact of missinglabels on the overall performance of benchmarks, hence we do notintegrate any mitigation methodologies, such as semi-supervisedlearning or self-supervised learning in our experiments.Results on relative model performance changes at different labelmissing ratios using the FedMultimodal framework are presentedin . Overall, we can observe that missing labels have areduced impact on the model performance when compared to themissing modality scenario. For instance, the model performancesuffers less than 10% relative performance decreases in the majorityof the datasets with the exception of KU-HAR datasets. When themissing label ratio is below 50%, we observe minor performancedecreases in all datasets. Surprisingly, we identify that CrisisMMDyields worse performance at 30% than at 10% missing label ratio. Weconjecture that the reason behind this result might be attributed tothe data labeling quality of the Hateful-Memes dataset. For example,determining whether the content is hateful or not can be verysubjective, and such subjectiveness could hurt labeling quality.",
  "Impact of Erroneous Labels": "Besides missing modalities and missing labels, erroneous labelsfrequently exist in FL . In this section, we report our benchmarkperformance with erroneous labels. Similar to previous experiments,we search the erroneous label ratio {0.1, 0.2, 0.3, 0.4, 0.5}, where represents the amount of data with erroneous labels. Similar tothe experiment setup in , our benchmark defines the sparsityof erroneous label transition matrix Q as 0.4. The error sparsityspecifies the possible number of unique labels that one labelcan be wrongly annotated with, with a small sparsity error ratecorresponding to a larger .The complete results of the relative model performance changesat different levels of erroneous label ratios are shown in .Compared to the missing modalities experiment, the erroneouslabel condition leads to substantially larger performance decreases.For example, more than half of the datasets have the relative per-formance decreases above 10% at the erroneous label ratio of 30%.Moreover, a 20% performance drop can be identified from thesedatasets when the erroneous label ratio reaches 50%. To compare theimpact of data corruption conditions in FL, we plot the relative per-formance changes with different data corruption conditions at thedata corrupted ratio of 30% in . We can observe that perfor-mances of multimodal FL are more susceptible to label noises thanmissing modalities or missing labels. Based on these observations,our future benchmark directions also include implementations ofbackdoor attacks and mitigation in FedMultimodal.",
  "LIMITATIONS AND FUTURE WORK": "Scale of Datasets and Models. The dataset selection criteria ofFedMultimodal ensures that the chosen datasets are representativeand diversified across different dimensions such as application sce-narios, data size, and number of clients. In addition, FedMultimodalonly includes ML models that align with the use cases of FL, takinginto account the computational limitations of edge devices. Weacknowledge that FedMultimodal currently does not cover severalpromising multimodal applications, such as medical image analy-sis, autonomous driving, and virtual reality, and the range of thesupported models is limited. We will continuously update FedMul-timodal to support new tasks such as Ego4D , as well as newerfeature extraction models.Scale of Modality Fusion Schemes. Currently, FedMultimodalincludes two basic approaches for modality fusion: concatenationand attention. Modality fusion under FL remains an open problem, and our objective is to draw attention to the need for developingmore advanced modality fusion schemes under FL .Data Heterogeneity. As discussed in previous sections, address-ing the data heterogeneity challenge is critical in FL. While manyFL studies focus their experiments on the unimodal setup, thereis a lack of extensive research on tackling data heterogeneity inmultimodal FL. To address this gap, the FedMulitmodal benchmarkprovides opportunities to facilitate fundamental research in thisdirection. In the future, it is of further interest to explore knowledge-transfer learning approaches as suggested in , withinthe context of multimodal FL.Label Scarcity. One major challenge for FL is the lack of qualitativelabels. FedMultimodal enables researchers to efficiently performexperiments on multimodal FL with missing labels by providing theability to emulate experimental conditions with missing labels. Wehope FedMultimodal brings unique benefits for ML practitioners todevelop self-supervised learning and semi-supervisedlearning algorithms under multimodal FL.Privacy Leakage. While sharing model updates is considered tobe more private than sharing raw data, recent works have revealedthat FL can still be susceptible to privacy attacks. These attacksinclude (but are not limited to) membership inference attacks ,reconstruction attacks , attribute inference attacks andlabel inference attacks . Consequently, an emerging researchdirection for expanding FedMultimodal is to explore the privacyleakages in multimodal FL. Apart from identifying privacy risks as-sociated with multimodal FL, it is also crucial to investigate privacy-enhancing techniques, such as differential privacy andsecure aggregation as promising areas of research within thescope of FedMultimodal to mitigate privacy attacks.",
  "CONCLUSION": "In this paper, we presented a new framework for multimodal fed-erated learning, named FedMultimodal, which enables federatedlearning in multimodal applications. We further established a re-producible benchmark of results for 5 multimodal FL applicationscovering 10 datasets for future comparisons. We also benchmarkedresults on model robustness to missing modalities, missing labels,and noisy labels in each of these tasks.",
  "Firoj Alam, Ferda Ofli, and Muhammad Imran. 2018. Crisismmd: Multimodaltwitter datasets from natural disasters. In Twelfth international AAAI conferenceon web and social media": "Erick A Perez Alday, Annie Gu, Amit J Shah, Chad Robichaux, An-Kwok IanWong, Chengyu Liu, Feifei Liu, Ali Bahrami Rad, Andoni Elola, Salman Seyedi,et al. 2020. Classification of 12-lead ecgs: the physionet/computing in cardiologychallenge 2020. Physiological measurement 41, 12 (2020), 124003. Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra Perez, and Jorge LuisReyes Ortiz. 2013. A public domain dataset for human activity recognitionusing smartphones. In Proceedings of the 21th international European symposiumon artificial neural networks, computational intelligence and machine learning.437442. Burin Becerik-Gerber, Gale M. Lucas, Ashrant Aryal, Mohamad Awada, MarioBergs, Sarah Billington, Olga Boric-Lubecke, Ali Ghahramani, Arsalan Hey-darian, Christoph Helscher, Farrokh Jazizadeh, Azam Khan, Jared Langevin,Ruying Liu, Frederick Marks, Matthew Louis Mauriello, Elizabeth L. Murnane,Haeyoung Noh, Marco Pritoni, Shawn C Roll, Davide Schaumann, Mir HasanSeyedrezaei, John Ellor Taylor, Jie Zhao, and Runhe Zhu. 2022. The field of humanbuilding interaction for convergent research and innovation for intelligent builtenvironments. Scientific Reports 12 (2022). Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H BrendanMcMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth. 2017. Prac-tical secure aggregation for privacy-preserving machine learning. In proceedingsof the 2017 ACM SIGSAC Conference on Computer and Communications Security.11751191. Brandon M Booth, Tiantian Feng, Abhishek Jangalwa, and Shrikanth S Narayanan.2019. Toward robust interpretable human movement pattern analysis in a work-place setting. In ICASSP 2019-2019 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP). IEEE, 76307634. Brandon M Booth, Karel Mundnich, Tiantian Feng, Amrutha Nadarajan, Tiago HFalk, Jennifer L Villatte, Emilio Ferrara, and Shrikanth Narayanan. 2019. Multi-modal human and environmental sensing for longitudinal behavioral studies innaturalistic settings: Framework for sensor selection, deployment, and manage-ment. Journal of medical Internet research 21, 8 (2019), e12832. Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Konen`y,H Brendan McMahan, Virginia Smith, and Ameet Talwalkar. 2018. Leaf: Abenchmark for federated settings. arXiv preprint arXiv:1812.01097 (2018). Houwei Cao, David G Cooper, Michael K Keutmann, Ruben C Gur, Ani Nenkova,and Ragini Verma. 2014. Crema-d: Crowd-sourced emotional multimodal actorsdataset. IEEE transactions on affective computing 5, 4 (2014), 377390. Jiayi Chen and Aidong Zhang. 2022. FedMSplit: Correlation-Adaptive FederatedMulti-Task Learning across Multimodal Split Networks. In Proceedings of the 28thACM SIGKDD Conference on Knowledge Discovery and Data Mining. 8796.",
  "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding.ArXiv abs/1810.04805 (2019)": "Dimitrios Dimitriadis, Mirian Hipolito Garcia, Daniel Madrigal Diaz, AndreManoel, and Robert Sim. 2022. Flute: A scalable, extensible framework for high-performance federated learning simulations. arXiv preprint arXiv:2203.13789(2022). Nanqing Dong and Irina Voiculescu. 2021. Federated contrastive learning for de-centralized unlabeled medical images. In Medical Image Computing and ComputerAssisted InterventionMICCAI 2021: 24th International Conference, Strasbourg,France, September 27October 1, 2021, Proceedings, Part III 24. Springer, 378387.",
  "Cynthia Dwork. 2006. Differential privacy. In Automata, Languages and Program-ming: 33rd International Colloquium, ICALP 2006, Venice, Italy, July 10-14, 2006,Proceedings, Part II 33. Springer, 112": "Tiantian Feng, Brandon M Booth, Brooke Baldwin-Rodrguez, Felipe Osorno, andShrikanth Narayanan. 2021. A multimodal analysis of physical activity, sleep,and work shift in nurses with wearable sensor data. Scientific reports 11, 1 (2021),8693. Tiantian Feng, Hanieh Hashemi, Rajat Hebbar, Murali Annavaram, andShrikanth S Narayanan. 2021. Attribute inference attack of speech emotionrecognition in federated learning settings. arXiv preprint arXiv:2112.13416 (2021). Tiantian Feng, Rajat Hebbar, Nicholas Mehlman, Xuan Shi, Aditya Kommineni,and Shrikanth Narayanan. 2023. A Review of Speech-centric Trustworthy Ma-chine Learning: Privacy, Safety, and Fairness. APSIPA Transactions on Signal andInformation Processing 12, 3 (2023). Tiantian Feng and Shrikanth Narayanan. 2019. Imputing missing data in large-scale multivariate biomedical wearable recordings using bidirectional recurrentneural networks with temporal activation regularization. In 2019 41st Annual",
  "International Conference of the IEEE Engineering in Medicine and Biology Society(EMBC). IEEE, 25292534": "Tiantian Feng and Shrikanth Narayanan. 2022. Semi-FedSER: Semi-supervisedLearning for Speech Emotion Recognition On Federated Learning using Multi-view Pseudo-Labeling. arXiv preprint arXiv:2203.08810 (2022). Tiantian Feng and Shrikanth S Narayanan. 2019. Discovering optimal variable-length time series motifs in large-scale wearable recordings of human bio-behavioral signals. In ICASSP 2019-2019 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP). IEEE, 76157619. Tiantian Feng, Raghuveer Peri, and Shrikanth Narayanan. 2022. User-LevelDifferential Privacy against Attribute Inference Attack of Speech Emotion Recog-nition on Federated Learning. In Proc. Interspeech 2022. 50555059. Chong Fu, Xuhong Zhang, Shouling Ji, Jinyin Chen, Jingzheng Wu, ShanqingGuo, Jun Zhou, Alex X Liu, and Ting Wang. 2022. Label inference attacks againstvertical federated learning. In 31st USENIX Security Symposium (USENIX Security22). 13971414.",
  "Jiahui Geng, Yongli Mou, Feifei Li, Qing Li, Oya Beyan, Stefan Decker, andChunming Rong. 2021. Towards General Deep Leakage in Federated Learning.arXiv preprint arXiv:2110.09074 (2021)": "Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, AntoninoFurnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al.2022. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1899519012. Chaoyang He, Keshav Balasubramanian, Emir Ceyani, Yu Rong, Peilin Zhao,Junzhou Huang, Murali Annavaram, and Salman Avestimehr. 2021. FedGraphNN:A Federated Learning System and Benchmark for Graph Neural Networks. ArXivabs/2104.07145 (2021). Chaoyang He, Songze Li, Jinhyun So, Mi Zhang, Hongyi Wang, Xiaoyang Wang,Praneeth Vepakomma, Abhishek Singh, Hang Qiu, Li Shen, Peilin Zhao, YanKang, Yang Liu, Ramesh Raskar, Qiang Yang, Murali Annavaram, and SalmanAvestimehr. 2020. FedML: A Research Library and Benchmark for FederatedMachine Learning. arXiv preprint arXiv:2007.13518 (2020). Chaoyang He, Alay Dilipbhai Shah, Zhenheng Tang, Di Fan1Adarshan Naiy-nar Sivashunmugam, Keerti Bhogaraju, Mita Shimpi, Li Shen, Xiaowen Chu,Mahdi Soltanolkotabi, and Salman Avestimehr. 2021. Fedcv: a federated learningframework for diverse computer vision tasks. arXiv preprint arXiv:2111.11066(2021). Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, WeijunWang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017. Mobilenets:Efficient convolutional neural networks for mobile vision applications. arXivpreprint arXiv:1704.04861 (2017). Sohei Itahara, Takayuki Nishio, Yusuke Koda, Masahiro Morikura, and KojiYamamoto. 2021.Distillation-based semi-supervised federated learning forcommunication-efficient collaborative training with non-iid private data. IEEETransactions on Mobile Computing 22, 1 (2021), 191205. Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman,and Joao Carreira. 2021. Perceiver: General perception with iterative attention.In International conference on machine learning. PMLR, 46514664. Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurlien Bellet, Mehdi Ben-nis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode,Rachel Cummings, et al. 2021. Advances and open problems in federated learning.Foundations and Trends in Machine Learning 14, 12 (2021), 1210.",
  "Yan Kang, Yang Liu, and Xinle Liang. 2022. Fedcvt: Semi-supervised verticalfederated learning with cross-view training. ACM Transactions on IntelligentSystems and Technology (TIST) 13, 4 (2022), 116": "Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebas-tian Stich, and Ananda Theertha Suresh. 2020. Scaffold: Stochastic controlledaveraging for federated learning. In International Conference on Machine Learning.PMLR, 51325143. Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, AmanpreetSingh, Pratik Ringshia, and Davide Testuggine. 2020. The hateful memes chal-lenge: Detecting hate speech in multimodal memes. Advances in Neural Informa-tion Processing Systems 33 (2020), 26112624. Jakub Konen`y, H Brendan McMahan, Felix X Yu, Peter Richtrik,Ananda Theertha Suresh, and Dave Bacon. 2016. Federated learning: Strategiesfor improving communication efficiency. arXiv preprint arXiv:1610.05492 (2016). Fan Lai, Yinwei Dai, Xiangfeng Zhu, Harsha V Madhyastha, and Mosharaf Chowd-hury. 2021. FedScale: Benchmarking model and system performance of federatedlearning. In Proceedings of the First Workshop on Systems Challenges in Reliableand Secure Federated Learning. 13.",
  "FedMultimodal:A Benchmark For Multimodal Federated LearningKDD 23, August 610, 2023, Long Beach, CA, USA": "Xin-Chun Li and De-Chuan Zhan. 2021. Fedrs: Federated learning with restrictedsoftmax for label distribution non-iid data. In Proceedings of the 27th ACM SIGKDDConference on Knowledge Discovery & Data Mining. 9951005. Paul Pu Liang, Yiwei Lyu, Xiang Fan, Zetian Wu, Yun Cheng, Jason Wu, LeslieChen, Peter Wu, Michelle A Lee, Yuke Zhu, et al. 2021.Multibench: Mul-tiscale benchmarks for multimodal representation learning.arXiv preprintarXiv:2107.07502 (2021). Bill Yuchen Lin, Chaoyang He, Zihang Zeng, Hulin Wang, Yufen Huang, MahdiSoltanolkotabi, Xiang Ren, and Salman Avestimehr. 2021. Fednlp: Benchmarkingfederated learning methods for natural language processing tasks. arXiv preprintarXiv:2104.08815 (2021). Tao Lin, Lingjing Kong, Sebastian U Stich, and Martin Jaggi. 2020. Ensembledistillation for robust model fusion in federated learning. Advances in NeuralInformation Processing Systems 33 (2020), 23512363.",
  "Sachin Mehta and Mohammad Rastegari. 2021. Mobilevit: light-weight, general-purpose, and mobile-friendly vision transformer. arXiv preprint arXiv:2110.02178(2021)": "Luca Melis, Congzheng Song, Emiliano De Cristofaro, and Vitaly Shmatikov.2019. Exploiting unintended feature leakage in collaborative learning. In 2019IEEE Symposium on Security and Privacy (SP). IEEE, 691706. Fatemehsadat Mireshghallah, Mohammadkazem Taram, Praneeth Vepakomma,Abhishek Singh, Ramesh Raskar, and Hadi Esmaeilzadeh. 2020. Privacy in deeplearning: A survey. arXiv preprint arXiv:2004.12254 (2020). Mathew Monfort, Alex Andonian, Bolei Zhou, Kandan Ramakrishnan, Sarah AdelBargal, Tom Yan, Lisa Brown, Quanfu Fan, Dan Gutfreund, Carl Vondrick, et al.2019. Moments in time dataset: one million videos for event understanding. IEEEtransactions on pattern analysis and machine intelligence 42, 2 (2019), 502508.",
  "Curtis Northcutt, Lu Jiang, and Isaac Chuang. 2021. Confident learning: Esti-mating uncertainty in dataset labels. Journal of Artificial Intelligence Research 70(2021), 13731411": "Alexandros Pantelopoulos and Nikolaos G Bourbakis. 2009. A survey on wearablesensor-based systems for health monitoring and prognosis. IEEE Transactionson Systems, Man, and Cybernetics, Part C (Applications and Reviews) 40, 1 (2009),112. Srinivas Parthasarathy and Shiva Sundaram. 2020. Training strategies to handlemissing modalities for audio-visual expression recognition. In Companion Publi-cation of the 2020 International Conference on Multimodal Interaction. 400404. Shyamal Patel, Hyung Park, Paolo Bonato, Leighton Chan, and Mary Rodgers.2012. A review of wearable sensors and systems with application in rehabilitation.Journal of neuroengineering and rehabilitation 9, 1 (2012), 117. Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, ErikCambria, and Rada Mihalcea. 2018. Meld: A multimodal multi-party dataset foremotion recognition in conversations. arXiv preprint arXiv:1810.02508 (2018). Andrew Raij, Animikh Ghosh, Santosh Kumar, and Mani Srivastava. 2011. Pri-vacy risks emerging from the adoption of innocuous wearable sensors in themobile environment. In Proceedings of the SIGCHI Conference on Human Factorsin Computing Systems. 1120. Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush,Jakub Konen`y, Sanjiv Kumar, and H Brendan McMahan. 2020. Adaptive feder-ated optimization. arXiv preprint arXiv:2003.00295 (2020).",
  "Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and DennyZhou. 2020. Mobilebert: a compact task-agnostic bert for resource-limited devices.arXiv preprint arXiv:2004.02984 (2020)": "Jean Ogier du Terrail, Samy-Safwan Ayed, Edwige Cyffers, Felix Grimberg,Chaoyang He, Regis Loeb, Paul Mangold, Tanguy Marchand, Othmane Marfoq,Erum Mushtaq, et al. 2022. FLamby: Datasets and Benchmarks for Cross-Silo Fed-erated Learning in Realistic Healthcare Settings. arXiv preprint arXiv:2210.04620(2022). Vasileios Tsouvalas, Tanir Ozcelebi, and Nirvana Meratnia. 2022.Privacy-preserving Speech Emotion Recognition through Semi-Supervised FederatedLearning. In 2022 IEEE International Conference on Pervasive Computing and Com-munications Workshops and other Affiliated Events (PerCom Workshops). IEEE,359364. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin. 2017. Attention is Allyou Need. In Advances in Neural Information Processing Systems, I. Guyon, U. VonLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.),Vol. 30. Curran Associates, Inc. Patrick Wagner, Nils Strodthoff, Ralf-Dieter Bousseljot, Dieter Kreiseler, Fatima ILunze, Wojciech Samek, and Tobias Schaeffter. 2020. PTB-XL, a large publiclyavailable electrocardiography dataset. Scientific data 7, 1 (2020), 115.",
  "Meng Wang, Weijie Fu, Xiangnan He, Shijie Hao, and Xindong Wu. 2020. Asurvey on large-scale machine learning. IEEE Transactions on Knowledge andData Engineering (2020)": "Zhen Wang, Weirui Kuang, Yuexiang Xie, Liuyi Yao, Yaliang Li, Bolin Ding, andJingren Zhou. 2022. FederatedScope-GNN: Towards a Unified, Comprehensiveand Efficient Package for Federated Graph Learning. Proceedings of the 28th ACMSIGKDD Conference on Knowledge Discovery and Data Mining (2022). Kang Wei, Jun Li, Ming Ding, Chuan Ma, Howard H Yang, Farhad Farokhi, Shi Jin,Tony QS Quek, and H Vincent Poor. 2020. Federated learning with differentialprivacy: Algorithms and performance analysis. IEEE Transactions on InformationForensics and Security 15 (2020), 34543469. Yuexiang Xie, Zhen Wang, Daoyuan Chen, Dawei Gao, Liuyi Yao, Weirui Kuang,Yaliang Li, Bolin Ding, and Jingren Zhou. 2022. FederatedScope: A Compre-hensive and Flexible Federated Learning Platform via Message Passing. ArXivabs/2204.05011 (2022).",
  "Qiying Yu, Yimu Wang, Ke Xu, Yang Liu, and Jingjing Liu. 2023. MultimodalFederated Learning via Contrastive Representation Ensemble. In InternationalConference on Learning Representations": "Fengda Zhang, Kun Kuang, Zhaoyang You, Tao Shen, Jun Xiao, Yin Zhang, ChaoWu, Yueting Zhuang, and Xiaolin Li. 2020. Federated unsupervised representationlearning. arXiv preprint arXiv:2010.08982 (2020). Tuo Zhang, Tiantian Feng, Samiul Alam, Sunwoo Lee, Mi Zhang, Shrikanth SNarayanan, and Salman Avestimehr. 2022. FedAudio: A Federated LearningBenchmark for Audio Tasks. arXiv preprint arXiv:2210.15707 (2022). Tuo Zhang, Lei Gao, Chaoyang He, Mi Zhang, Bhaskar Krishnamachari, andSalman Avestimehr. 2021. Federated Learning for the Internet of Things: Ap-plications, Challenges, and Opportunities. IEEE Internet of Things Magazine 5(2021), 2429. Zhengming Zhang, Yaoqing Yang, Zhewei Yao, Yujun Yan, Joseph E Gonza-lez, Kannan Ramchandran, and Michael W Mahoney. 2021. Improving semi-supervised federated learning by reducing the gradient diversity of models. In2021 IEEE International Conference on Big Data (Big Data). IEEE, 12141225."
}