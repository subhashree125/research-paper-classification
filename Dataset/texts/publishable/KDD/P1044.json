{
  "ABSTRACT": "Collaborative fairness stands as an essential element in federatedlearning to encourage client participation by equitably distribut-ing rewards based on individual contributions. Existing methodsprimarily focus on adjusting gradient allocations among clients toachieve collaborative fairness. However, they frequently overlookcrucial factors such as maintaining consistency across local mod-els and catering to the diverse requirements of high-contributingclients. This oversight inevitably decreases both fairness and modelaccuracy in practice. To address these issues, we propose FedSAC,a novel Federated learning framework with dynamic SubmodelAllocation for Collaborative fairness, backed by a theoretical con-vergence guarantee. First, we present the concept of \"bounded collab-orative fairness (BCF)\", which ensures fairness by tailoring rewardsto individual clients based on their contributions. Second, to im-plement the BCF, we design a submodel allocation module with atheoretical guarantee of fairness. This module incentivizes high-contributing clients with high-performance submodels containinga diverse range of crucial neurons, thereby preserving consistencyacross local models. Third, we further develop a dynamic aggre-gation module to adaptively aggregate submodels, ensuring the",
  "Both authors contributed equally to this researchCorresponding Author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from acronym XX, June 0305, 2018, Woodstock, NY 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-1-4503-XXXX-X/18/06 equitable treatment of low-frequency neurons and consequently en-hancing overall model accuracy. Extensive experiments conductedon three public benchmarks demonstrate that FedSAC outperformsall baseline methods in both fairness and model accuracy. We seethis work as a significant step towards incentivizing broader clientparticipation in federated learning. The source code is available at",
  "federated Learning, collaborative fairness, privacy": "ACM Reference Format:Zihui Wang, Zheng Wang, Lingjuan Lyu, Pengzhao Peng, Zhicheng Yang,Chenglu Wen, Rongshan Yu, Cheng Wang, and Xiaoliang Fan. 2018. Fed-SAC: Dynamic Submodel Allocation for Collaborative Fairness in FederatedLearning. In Proceedings of Make sure to enter the correct conference title fromyour rights confirmation emai (Conference acronym XX). ACM, New York,NY, USA, 14 pages.",
  "INTRODUCTION": "Federated Learning (FL) empowers multiple data owners to collec-tively train a global model while preserving the privacy of theirindividual training data . Early FL frameworks usually distributed the same model to all clients without consider-ing their distinct contributions to the model performance, resultingin unfairness to high-contributing clients. Collaborative fairness(CF) stands as an essential element in federated learning to mo-tivate client engagement by ensuring impartial reward distributiontied directly to individual contributions.",
  "Training step": ": Problem illustration of collaborative fairness inFL. (a) Conventional gradients-based methods will result inpoor fairness and model accuracy. For example, it is unfairthat obtained models of 2 and 1 are equivalentneglecting the inferior contribution (1) of 1. Plus, theinconsistency in local models results in that obtained modelsof2 and3 are worse than expected ( ). (b) Our pro-posed FedSAC allocates sufficient submodels to each clientby ensuring a comprehensive balance between fairness andmodel accuracy. For example, FedSAC ensures that obtainedmodels of all clients (i.e., 1, 2 and 3) are inaccordance with their contributions respectively. In addition,FedSAC guarantees the alignment of and of 2 dur-ing the training process, thereby enabling all three clients toobtain their expected models ( ). More recently, several gradient-based methods were proposedto enhance CF (i.e., rewarding clients with correspond-ing model quality according to their contributions) in FL. They dis-tribute a larger quantity of gradients to higher-contributing clientsthan the lower ones as rewards and quantify the degree of fairnessby Pearson Correlation Coefficient . However, for achieving CF,existing gradient-based methods have two major limitations. Onone hand, the conventional definition of CF doesnt adequatelydistinguish in reward distribution among clients, resulting in apersistent unfairness for high-contributing clients. In (a),suppose the contributions of three clients are = , and theirrewards are = [99, 99.2, 99.3] corresponding. Through the defi-nition of CF by CGSV , the fairness is calculated as =98.97, butthere exists an underlying unfairness towards 2 and 3because 1 with an inferior contribution is over-rewarded. Onthe other hand, conventional gradient-based methods are ineffective because the inconsistency of local models updated byvariable gradients might lead to significant degradation of overallmodel performance. In (a), the local models of clients inround (1,,2,, and3,) exhibit notable differences (i.e., the largerthe circle, the higher the accuracy). Consequently, the gradientsuploaded by individual clients may not be the optimal for others, cre-ating a misalignment between obtained rewards ,(i.e., the",
  "each client": "To address the aforementioned challenges, we propose a novelFederated learning framework with dynamic Submodel Allocationfor bounded Collaborative fairness (FedSAC), supported by a the-ory of convergence while achieving competitive model accuracy.First, our approach introduces the concept of \"bounded collabora-tive fairness (BCF) (refer to 1)\", which ensures fairnessby integrating a differentiated range of rewards allocated to eachclient. Second, the submodel allocation module with a theoreticalfairness guarantee, is designed to assign relevant submodels (i.e.,results of the aggregated model dropout) to individual clients basedon their contributions. Specifically, these submodels encompassa diverse array of essential neurons for effective training. Third,the dynamic aggregation module is implemented as a weight re-alignment mechanism by treating low-frequency neurons equally,which further improves the overall performance of the global model.Extensive experiments on three public benchmarks show that theproposed FedSAC outperforms all baseline methods in terms ofcollaborative fairness and model accuracy.The contributions of this work are summarized: We propose FedSAC, a novel federated learning frameworkwith a convergence guarantee, introducing a new conceptof bounded collaborative fairness (BCF). To the best of ourknowledge, this is the first approach that allocates submodelsequitably for collaborative fairness in FL. We implement the concept of BCF through two modules.First, submodel allocation module prioritizes high-contributingclients by rewarding them with high-performance submodelsunder a theoretical guarantee. Second, dynamic aggregationmodule merges submodels by paying equitable attention tolow-frequency neurons to be aggregated.",
  "RELATED WORKS": "Recent research has shown that distributing different rewards basedon clients contributions can significantly impact the FL systems . The incentive mechanisms can motivate clients to contributehigh-quality data and promote collaboration . We outlinethree types of rewards that can be adopted to achieve CF in FL.Money-based reward. Several studies focus on the mechanismthat rewards clients monetary based on their contributions. proposes a reputation-based and reverse auction theory mechanismto reward clients with a limited budget. shows a scheme thatdynamically allocates budgets to clients in a context-aware man-ner by jointly maximizing the collective utility. While monetaryrewards can be a natural and effective way to incentivize clientsin FL, there exist challenges in maintaining a balance between thevalue of model quality and money .Data-based reward. Early studies have explored the fairnessof rewarding different data sizes based on their contributions. evaluates clients contributions by aggregating the training data,and reward them with the corresponding models. trains agenerative model through the local data of all clients and providesmore synthetic data to those datasets closely aligned with the realdata distribution. However, most of existing data-based reward",
  "FedSAC: Dynamic Submodel Allocation for Collaborative Fairnessin Federated LearningConference acronym XX, June 0305, 2018, Woodstock, NY": "Han Yu, Zelei Liu, Yang Liu, Tianjian Chen, Mingshu Cong, Xi Weng, DusitNiyato, and Qiang Yang. 2020. A fairness-aware incentive scheme for federatedlearning. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society.393399. Hao Yu, Sen Yang, and Shenghuo Zhu. 2019. Parallel restarted SGD with fasterconvergence and less communication: Demystifying why model averaging worksfor deep learning. In Proceedings of the AAAI Conference on Artificial Intelligence,Vol. 33. 56935700. Ruichi Yu, Ang Li, Chun-Fu Chen, Jui-Hsin Lai, Vlad I Morariu, Xintong Han,Mingfei Gao, Ching-Yung Lin, and Larry S Davis. 2018. Nisp: Pruning networksusing neuron importance score propagation. In Proceedings of the IEEE conferenceon computer vision and pattern recognition. 91949203. Yaodong Yu, Alexander Wei, Sai Praneeth Karimireddy, Yi Ma, and MichaelJordan. 2022. TCT: Convexifying federated learning using bootstrapped neuraltangent kernels. Advances in Neural Information Processing Systems 35 (2022),3088230897. Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald,Nghia Hoang, and Yasaman Khazaeni. 2019. Bayesian nonparametric federatedlearning of neural networks. In International Conference on Machine Learning.PMLR, 72527261.",
  "PRELIMINARY": "FL system consists of a server and multiple clients, aiming to mini-mize the weighted average of all clients local objectives by opti-mizing a global model . First, the server broadcasts amodel to the clients at random. Second, after training several roundslocally, the server aggregates these different trained models intoa new global model. Finally, the aggregated model will be sent tothe clients for further local training. The aforementioned process isrepeated multiple times until the global model converges .In this setup, the goal of FL framework is defined as:",
  ", = =1 . () is the loss on client using model parameters , i.e., () = 1": "(, ), where represents the local dataset of client , and denotes the data sizeof . To achieve this goal as effectively as possible, FedAvg samples a subset of clients uniformly, 0 < , to train theglobal model and aggregate the locally trained models by utilizingthe data size ratio as the weight of client . Although FedAvg isproven to be effective in minimizing the objective successfully, itmay be unfair to high-quality clients since the system distributes thesame rewards to all clients regardless of their contributions .",
  "Problem Formulation": "The standard FL framework allocates the same model to all clientsregardless of their contributions , dampening themotivation of high-quality clients to join FL . Collabora-tive fairness in FL aims to reward high-contributing clients withhigh-quality models. The existing works assess the fair-ness with the Pearson Correlation Coefficient, (;), where and represent the contributions and rewards of clients, respec-tively. However, the definition simply considers the relationshipbetween the contributions and rewards of clients, which may leadto insufficient incentives for high-contributing clients. For examplein (a), suppose the contributions of 1, 2, and3 are = and their rewards are = [99, 99.2, 99.3]correspondingly. Through the definition of CF, the fairness is calcu-lated as =98.97, but there exists an underlying unfairness towards",
  "contributing client. BCF could ensure 1 < 1 < (1+3 )": "2and thenquantitative fairness with (;). The rationale behind the formulain 1 aims to amplify significant distinctions in rewards.The formulas left side ensures that clients rewards exceed theircontributions, while the right side prevents excessive rewards forclients with low contributions. Definition 1 (Bounded Collaborative Fairness). Thecontributions () and the rewards () of clients are calculated by theperformance of their standalone models (train without collaboration)and the final models obtained after collaboration, respectively. Basedon clients obtained rewards < < (+ ())",
  "THE PROPOSED FEDSAC": "In this section, we will introduce the details of proposed FedSAC, amethod that ensures both BCF and consistency in local models foreach client. The architecture of FedSAC is shown in . Thepseudo codes for FedSAC are provided in Algorithm 1. First, weintroduce the submodel allocation module in .1. Second,we present the dynamic aggregation module in .2. Third,we proposed the fairness guarantee theory in .3 to provethat this submodel allocation strategy can achieve collaborativefairness. Fourth, we conducted a convergence analysis on FedSACand demonstrated its convergence in .5. In addition, weanalyzed the time complexity and communication costs of FedSACin .5. Finally, we discussed limitations in .6.",
  "Submodel Allocation Module": "A naive approach achieving bounded collaborative fairness involvesallocating distinct submodels to each client based on their respec-tive contributions . Unlike previous works such as , thereare two primary motivations behind achieving BCF through sub-model allocation. First, submodels with appropriate pruning maynot match the performance of the global model, enabling clients toreceive diverse submodels according to their contributions. Second,despite being subsets of the global model, these submodels exhibitstrong mutual validity, meaning that the submodels uploaded byone client are effective for others, facilitating the training of theglobal model. However, it is still challenging to achieve BCF throughsubmodel-based methods. For one thing, it is crucial to ensure thatthe majority of neurons are adequately trained to guarantee theoptimal performance of the global model. For another, the perfor-mance of allocated submodels should align with their respectivecontributions.To address the aforementioned two challenges, we design atwo-step approach for submodel allocation module. First, we evalu-ate the importance of each neuron within the model to determinetheir contributions respectively (neuron importance evaluation in.1.1). Second, we construct submodels for each client with",
  "Validation": ": The overall framework of FedSAC that achieves bounded collaborative fairness by maintaining consistency acrosslocal models. FedSAC consists of two module: 1) submodel allocation module conducts neuron importance evaluation andsubmodel construction to reward high-contributing clients with high-performance submodels, thus ensuring consistency inlocal models; 2) dynamic aggregation module treats those low-frequency neurons equally, which further refines the performanceof the global model.",
  "varying performances based on their contributions, ensuring a di-verse array of important neurons is included within each submodel(submodel construction in .1.2)": "4.1.1Neuron Importance Evaluation. Each neuron within themodel holds a unique contribution . Our intuitionis that the constructed submodels can yield varied performances.Inspired by Taylor-FO , we calculate the neuron importancein the model by measuring the change in loss upon their removal.For instance, a greater increase in loss indicates a more significantcontribution by the removed neuron to the model. In ,neurons depicted in a redder shade represent a higher contributionto the model. Essentially, the training objective is to minimize thecross-entropy loss :",
  "=1 (,),(2)": "where denotes the sample, represents the model, and (,)is the loss function of the classification tasks.The neurons in the model have a multitude of model parameters,each of which contributes to the overall performance of the model.The importance of a neuron of the model can be calculatedthrough the loss increased by removing it:",
  "= (,|= 0) (,),(3)": "where denotes the validation set, which is constructed by evenlyselecting 10% of the data from the original training samples ,represents that the parameters of the -th neuron in the modelare all set to 0.To simplify the construction of submodels, we normalize thesum of neuron scores, which represent their importance in the",
  "= 100,(4)": "where represents all neurons of the model. To reduce the trainingtime of the framework, we measure the importance of neurons inthe model by Eq. (3) and Eq. (4) every 10 epochs. All these operationsallow us to efficiently assess the importance of each neuron whilelimiting excessive computation demands. 4.1.2Submodel Construction. In pursuit of fairness, we employa dynamic allocation system for submodels with varying perfor-mances, leveraging clients reputations derived from their contribu-tions. Our approach incorporates a pruning mechanism tailored toclients contributions, simplifying the extraction of submodels withdifferent performance levels from the global model. In this scheme,the client s reputation is expressed as:",
  "() 100,(6)": "where represents client s contribution, is a hyper-parameter.The reputations of clients are directly proportional to their contri-butions. The design rationale for Eq. (5) and Eq. (6) is to calculatethe clients reputations (), which facilitates the allocation of theirsubmodels fairly. More specifically, our pruning method beginswith the most important neuron, ensuring that submodels for low-contribution clients possess a higher parameter count, which isbeneficial for training the global model. These actions serve a dualpurpose: promoting collaborative fairness while maximizing theoverall performance of the global model. Submodel is constructed",
  "),(7)": "where (, ) represents the submodel when = , denotes client s reputation, denotesthe set importance for different neurons. represents the positionsof all neurons in the model, arranged in ascending order from theleast to the most important. This design choice aims to maximize theinclusion of neurons in each submodel, thereby enhancing the per-formance of the corresponding local model updates. Subsequently,this quantity will be utilized in Eq. (8) to generate the submodelsmask.",
  "Dynamic Aggregation Module": "Next, the server aggregates the locally trained submodels and allo-cates distinct submodels to clients in the subsequent round. Recentsubmodel-based methods have aimed to allocate varied sub-models containing numerous neurons to clients. However, theseapproaches might pose a potential risk of compromising overallmodel performance when integrating low-frequency neurons intothe global model. Consequently, employing a direct aggregationmethod such as FedAvg for all neurons becomes inequitable.Instead of simply averaging the uploaded submodels, our objec-tive is to optimize the utilization of all neurons within the model.With the sizes of submodels varying across clients, it becomes es-sential to treat the contribution of each neuron individually duringaggregation. To ensure fair treatment of low-frequency neurons,we integrate the frequency of submodel parameter aggregations asweights to dynamically aggregating local models:",
  ",(9)": "where (,) denotes submodel s mask (same shape asthe submodel ), denotes the aggregated model, denotes thetotal number of clients. It sets the components of both and at the same position to 1 and 0 for the rest. The role of the maskfunction (,) is to calculate the frequency of each modelparameter selected by the global model in round . Later,the mask function (,) will be utilized in Eq. (9) to treatthose low-frequency parameters equally by suppressing the weightof high-frequency parameters in the aggregation, which makeseach parameter play a fair role during the aggregation phase. Forexample, the more the frequency of a selected parameter in round ,the smaller the weight of the parameter to be aggregated in round + 1.",
  "Fairness Guarantee": "In .1, we delved into the fundamental concept underpin-ning our definition of fairness. This concept centers on rewardinghigh-contributing clients with high-performance submodels, wherea submodels improved performance correlates with the number ofneurons it contains. Consequently, this approach leads to a trainingloss (i.e., model accuracy) that more closely aligns with the aggre-gated model. Its important to note that the submodel acquired by client is determined based on its reputation across the entiretraining process up to iteration .Our primary result ensures a notion of fairness under specificconditions concerning the loss function . If client holds a higherreputation than client ( ), and the submodel obtained byclient encompasses the submodel obtained by client ( ). Then, the submodel obtained by client will exhibit closeralignment with the aggregated model in round . Letting :=|| ||, its evident that . Consequently, the submodel obtained by client will yield a smaller loss function () comparedto client in round .",
  "|| ||2.(11)": "Theorem 1 (Fairness in Training Loss). Assume Assumptions 1and 2 hold, FedSAC can guarantee collaborative fairness by rewardinghigh-contributing clients obtaining high-performance models. For-mally speaking, let := || ||. Suppose that is close to astationary point of for +, and () is both -smooth and-strongly convex with . For all , in round t, if ,it follows that , , and therefore ( ) ().",
  "(17)": "where the first inequality is derived from the Cauchy-Schwarz, thesecond inequality is by substituting the aforementioned upper limit(refer to Eq. (16)), and the last inequality (line 1209) emerges fromtaking the absolute values of two negative values (refer to Eq. (15)).Finally, given that |1| |2| and 2 0, we derive 1+2 0.Therefore, it follows that 1+2 0, which subsequentlyimplies ( ) ().",
  "Convergence Analysis": "In this section, we delve into the convergence analysis of theproposed FedSAC. To guarantee convergence to the global opti-mum, we make the assumption that each neuron in the aggregatedmodel is equally allocated over rounds. Consequently, the antici-pated weight of the allocated submodel contracts towards theaggregate model , i.e., +1= . Here, (0 1) denotesthe long-term expectation of the size ratio between the submodel and the aggregate model obtained in multiple iterations. At thisstage, Eq. (9) can be expressed as the aggregation of each submodel",
  "where {1, 2, ...,N} and {1, 2, ..., 1}": "Assumption 5. Each neuron in the aggregation model is assignedthe same number of times after rounds. Therefore, the expectedweight of the allocated submodel is a contraction of the aggregatemodel , i.e., +1= . Here, (0 1) denotes the long-term expectation of the size ratio between the submodel and theaggregate model obtained in multiple iterations.",
  "Complexity and Communication CostAnalysis": "We further analyze the time complexity and communicationcosts of FedSAC as follows.For the time complexity, the primary computational demandin FedSAC stems from evaluating neuron importance, as definedin Eq. (3) and Eq. (4)). The time complexity for this evaluation is(), where denotes the total number of neurons across thehidden layers of the global model.For the communication cost, FedSAC mitigates the introduc-tion of additional communication overhead by conducting neuronimportance evaluation solely on the server. This approach effec-tively eliminates the necessity for client-server communication,thereby enhancing overall efficiency. Moreover, it displays a com-munication complexity of O(d*m) per round, as outlined in ,where m1 denotes the average ratio of submodel parameters tothe global model. As a result, FedSAC showcases lower communi-cation complexity compared to all baseline methods in cross-siloFL scenarios . More details about the communication cost ex-periments are put in Appendix E.",
  "Limitations": "In and , we conduct extensive experiments on var-ious datasets and observe that FedSAC could exhibit a distinctadvantage over all baseline methods in terms of both fairness andmodel accuracy. Nevertheless, the sufficient evaluation of neuronimportance (.1.1) within the submodel allocation moduleimposes an additional computational burden. This problem may beamplified for large models. Despite this challenge, we hold a strongconviction that the substantial enhancements in both fairness andaccuracy achieved through by FedSAC clearly affirm its superiorityover baseline methods.",
  "EXPERIMENTS": "In this section, we conduct comprehensive experiments to answerthe following research questions:RQ1. How does the fairness of our FedSAC compare to variousstate-of-the-art methods?RQ2. How does the predictive model performance achieved byour proposed method compare with the state-of-the-art methodson different datasets?RQ3. How do different components (i.e., submodel allocationmodule and dynamic aggregation module) affect the results?",
  "Experimental Settings": "Datasets and Models. We evaluate the performance of FedSACon three commonly used public datasets in collaborative fairness,including Fashion MNIST , CIFAR10 , and SVHN . Fol-lowing , we employ a feedforward neural network withtwo hidden layers for all datasets.Data splits. We construct five heterogeneous scenarios by vary-ing the size and the class numbers of the dataset. For imbalanceddataset sizes (POW) , we randomly divide the total datasetinto various data sizes for each client by using a power law. ForCIFAR10, we partition the data set of size 20000 among 10 clients.The clients with more extensive data sizes are expected to achievebetter prediction performance. For imbalanced class numbers(CLA) , we change the number of classes and keep themhave the same amount of data. For CIFAR10 with 5 clients, clients 1,2, 3, 4, 5 own local training data with 1, 3, 5, 7, 10 classes respectively.For imbalanced data size and class numbers (DIR), we provideclients with various data sizes and classes by the Dirichlet distribu-tion function . Specifically, we sample () andassign a percentage of the data of class to client , where ()is the Dirichlet distribution with a parameter . More details onthe varying numbers of clients used in the experiment are put inAppendix C.Baselines. We compare FedSAC with the following methods:(1) FedAvg distributes the same model to all clients in eachFL iteration. In this case, Pearson Correlation Coefficient () in.1 is uncomputable. To address this and create a person-alized model for each client, we follow CFFL and CGSV ,which enables clients to train for an additional epoch at the end ofFL algorithm. (2) q-FFL enables the reweighting of loss acrossdifferent clients by the q-parameterized weights, thus reducing thevariance in the accuracy distribution and achieving a fairer distri-bution of accuracy. (3) CFFL allocates more gradients to higherreputation client, and the reputation is calculated by the local accu-racy and data sizes (or label diversity). (4) CGSV assigns moregradients to clients whose local model gradients is more similarto the global gradients. (5) FedAVE assigns more gradients toclients whose data distribution information is more similar to theideal dataset. (6) Standalone trains local models alone withoutcollaboration. Particularly, to evaluate more fairly, we make allalgorithms distribute rewards based on client contributions ratherthan the calculated reputations.Hyper-Parameters. We tune all hyper-parameters in datasetsby using grid search with FedAvg and subsequently apply theoptimal parameters obtained from the validation dataset. The batch size is = 64 for SVHN and = 32 for both FashionMNIST andCIFAR10. The optimal parameters for SVHN, Cifar10, and Fashion-MNIST of six scenarios are = {15, 20}, = {0.05, 0.1}, = {3, 5,10}; = {15, 20}, = {0.03, 0.05}, = {3, 5, 10}, and = 20, = {0.03,0.05}, = {1, 10, 20, 25}, respectively. For comparison, we selectthe best fairness achieved by each method. The effects of hyper-parameter on FedSAC are detailed in Appendix D (the smaller is, the higher the accuracy achieved by FedSAC). More detailsabout the hyper-parameters are put in Appendix F.Implementation. All experiments are run on a 64 GB-RAMUbuntu 18.04.6 server with Intel(R) Xeon(R) CPU E5-2630 v4 @2.20GHz and 1 NVidia(R) 2080Ti GPUs.",
  "Experimental Results": "Fairness (RQ1). To evaluate the FedSAC fairness, we comparedit with a few baselines on three datasets. shows the fair-ness metrics according to 1. Standalone trains localmodels alone without collaboration, which represents the clientscontributions. indicates that our proposed dynamic sub-model allocation mechanism achieves a fairness score above 95.73%on all datasets, while the FedAvg performs poorly with the lowestfairness score of -19.83%. On three datasets, the fairness of algo-rithms (i.e., CFFL, CGSV, and FedAVE) exceeds 73.65% for the POWand CLA scene. In these scenarios, the clients contribution variesgreatly and is mainly related to the amount of data or diverse labelsof data. For the DIR scene, the data distribution among clients is sig-nificantly uneven, resulting in a high degree of non-iid settings andclients with relatively similar contributions. Consequently, CFFL,CGSV, and FedAVE show low fairness, as the rewards received byclients tend to be indistinguishable. In particular, in DIR (1.0) ofCIFAR10, our method outperforms CFFL, CGSV and FedAVE by69.24%, 35.85%, and 40.71%, respectively. demonstrates that the proposed FedSAC outperformsthe state-of-the-art approaches in fairness, and validated the ef-fectiveness of our method: high-contributing clients obtain high-performance models. shows the comparison results ofoverall performance to achieve bounded collaborative fairness withstate-of-the-art methods in CIFAR10 (left), SVHN (middle), andFashion MNIST (right). Obviously, FedSAC outperforms all base-lines in terms of fairness.Predictive performance (RQ2). To effectively assess the pre-dictive performance of algorithms, we present our highest test ac-curacies in comparison with all baseline methods in . Theseresults demonstrate the ability of the algorithms to reward high-contributing clients with high-performance. First, comparing theaccuracy of FedSAC with Standalone (i.e., contribution) reveals thatFedSAC significantly outperforms Standalone. Second, among thePOW scene, FedSAC achieves the highest performance in CIFAR10,SVHN and Fashion MNIST with accuracies of 48.61%, 74.84%, and87.88%, respectively. Third, for the CLA scene on three datasets, thehighest accuracy is obtained by FedSAC, surpassing FedAvg by atleast 0.19%. In addition, in the extremely non-iid setting (e.g., DIR(1.0) of SVHN), our method outperforms CFFL, CGSV, and FedAVEby 2.17%, 5.06%, and 4.62%, respectively. Finally, for the DIR(2.0),and DIR (3.0) scenes, FedSAC achieves comparable performanceto baseline methods in terms of accuracy. Specially, the notably",
  ": Ablation studies on FedSAC for fairness on three public benchmarks. A higher denotes better fairness": "poor accuracy of q-FFL appears attributed to its mechanism that of-fers the same reward to all clients, without adapting these rewardsbased on individual client contributions. illustrates the dis-tributions of contributions and allocated rewards under scenes (i.e., DIR (1.0)) in CIFAR10 comparing FedSAC against baseline methods.It demonstrates that FedSAC not only guarantees BCF but alsoenables clients to receive rewards that exceed their contributions(i.e., Standalone). In short, FedSAC outperforms all baselines in",
  ": Comparison results of test accuracy using the datapartition of DIR (1.0) with state-of-the-art methods in CI-FAR10 (up) and SVHN (down). Results of other scenes are inAppendix D": "terms of accuracy. More results under different scenarios (i.e., CLAscene) on CIFAR10 and SVHN are presented in Appendix B. illustrates the changes in clients test accuracy as thenumber of communication rounds increases in the POW, and CLAdata partition of CIFAR10 and SVHN. Owing to the varying datasizes and diversity of labels owned by clients in FL, their contri-butions to the system exhibit significant differences. As shownin , our proposed FedSAC, underpinned by a theoreticalguarantee, aims to reward high-contributing clients with high-performance submodels by maintaining consistency in local mod-els. As a result, each client will converge to a different model andachieve varying levels of performance.Ablation study (RQ3). To evaluate the effectiveness of twoproposed modules in FedSAC, a series of ablation experiments arecarried out on three public benchmarks with 10 clients, as shownin and 4. The operation of eliminating neuron importance,denoted as / , aims to treat all neurons equally andallocate submodels based on their contributions. /",
  ": The test accuracy achieved by clients during train-ing for CIFAR10 (left) and SVHN (right) in each round, underthe setting of POW and CLA": "denotes removing the dynamic aggregation module, which usesthe traditional FedAvg aggregation method to train. The effective-ness of the submodel allocation module is demonstrated in ,indicating that this module can reward high-contribution clientsto obtain high-performance models. In particular, our method hassignificantly improved the fairness measure by 88.21% on the DIR(1.0) scene of the CIFAR10 dataset. shows the results ofthe proposed dynamic aggregation module, which implies that thismodule can effectively aggregate submodels with different sizes,thereby further improving the overall performance of the localmodels. Thus, the ablation study demonstrates that the two de-signed modules in FedSAC are crucial and significant in enhancingbounded collaborative fairness.In summary, all experimental results show that both fairnessand model accuracy are of significance for bounded collaborativefairness, and our FedSAC outperforms all baseline methods in bothfairness and model accuracy.",
  "CONCLUSION": "In this work, we introduce a novel FL framework named FedSACthat allocates submodels based on their contributions, thereby en-suring bounded collaborative fairness and attaining superior localaccuracy while maintaining the consistency in local models. Ourmethod ensures that high-contributing clients can be rewardedwith high-performance submodels, which in turn enhances theoverall model accuracy. The experiments on three datasets showthat FedSAC exhibits a distinct advantage over baseline methods interms of fairness and accuracy. In the future, we aim to investigatethe implementation of FedSAC on large models.",
  "Conference acronym XX, June 0305, 2018, Woodstock, NYZihui Wang, Zheng Wang, Lingjuan Lyu, Pengzhao Peng, Zhicheng Yang, Chenglu Wen, Rongshan Yu, Cheng Wang, and Xiaoliang Fan": "Chen Chen et al. 2022. Gear: a margin-based federated adversarial training ap-proach. In International Workshop on Trustable, Verifiable, and Auditable FederatedLearning in Conjunction with AAAI, Vol. 2022. Dengsheng Chen, Jie Hu, Vince Junkai Tan, Xiaoming Wei, and Enhua Wu. 2023.Elastic Aggregation for Federated Optimization. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition (CVPR). 1218712197. Jean Ogier Du Terrail, Samy-Safwan Ayed, Edwige Cyffers, Felix Grimberg,Chaoyang He, Regis Loeb, Paul Mangold, Tanguy Marchand, Othmane Marfoq,Erum Mushtaq, et al. 2022. FLamby: Datasets and Benchmarks for Cross-SiloFederated Learning in Realistic Healthcare Settings. In NeurIPS 2022-Thirty-sixthConference on Neural Information Processing Systems. Liang Gao, Huazhu Fu, Li Li, Yingwen Chen, Ming Xu, and Cheng-Zhong Xu.2022. FedDC: Federated Learning With Non-IID Data via Local Drift Decouplingand Correction. In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition (CVPR). 1011210121. Liang Gao, Huazhu Fu, Li Li, Yingwen Chen, Ming Xu, and Cheng-Zhong Xu.2022. Feddc: Federated learning with non-iid data via local drift decoupling andcorrection. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition. 1011210121. Junyuan Hong, Haotao Wang, Zhangyang Wang, and Jiayu Zhou. 2022. EfficientSplit-Mix Federated Learning for On-Demand and In-Situ Customization. In InProceedings of the International Conference on Learning Representations. Samuel Horvath, Stefanos Laskaridis, Mario Almeida, Ilias Leontiadis, StylianosVenieris, and Nicholas Lane. 2021. Fjord: Fair and accurate federated learning un-der heterogeneous targets with ordered dropout. Advances in Neural InformationProcessing Systems 34 (2021), 1287612889. Wenke Huang, Mang Ye, and Bo Du. 2022. Learn From Others and Be Yourself inHeterogeneous Federated Learning. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR). 1014310153. Yutao Huang, Lingyang Chu, Zirui Zhou, Lanjun Wang, Jiangchuan Liu, Jian Pei,and Yong Zhang. 2021. Personalized cross-silo federated learning on non-iid data.In Proceedings of the AAAI conference on artificial intelligence, Vol. 35. 78657873. Ekaterina Khramtsova, Christian Hammerschmidt, Sofian Lagraa, and Radu State.2020. Federated learning for cyber security: SOC collaboration for malicious URLdetection. In 2020 IEEE 40th International Conference on Distributed ComputingSystems (ICDCS). IEEE, 13161321.",
  "Yann LeCun, Lon Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradient-based learning applied to document recognition. Proc. IEEE 86, 11 (1998), 22782324": "Ang Li, Jingwei Sun, Xiao Zeng, Mi Zhang, Hai Li, and Yiran Chen. 2021. Fed-mask: Joint computation and communication-efficient personalized federatedlearning via heterogeneous masking. In Proceedings of the 19th ACM Conferenceon Embedded Networked Sensor Systems. 4255. Bo Li, Mikkel N. Schmidt, Tommy S. Alstrm, and Sebastian U. Stich. 2023. Onthe Effectiveness of Partial Variance Reduction in Federated Learning WithHeterogeneous Data. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR). 39643973. Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar,and Virginia Smith. 2020. Federated optimization in heterogeneous networks.Proceedings of Machine learning and systems 2 (2020), 429450.",
  "Lingjuan Lyu, Xinyi Xu, Qian Wang, and Han Yu. 2020. Collaborative fairness infederated learning. In Federated Learning. Springer, 189204": "Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, andBlaise Aguera y Arcas. 2017. Communication-efficient learning of deep net-works from decentralized data. In Artificial Intelligence and Statistics. PMLR,12731282. Matias Mendieta, Taojiannan Yang, Pu Wang, Minwoo Lee, Zhengming Ding,and Chen Chen. 2022. Local Learning Matters: Rethinking Data Heterogeneityin Federated Learning. In Proceedings of the IEEE/CVF Conference on Computer",
  "Vision and Pattern Recognition (CVPR). 83978406": "Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz. 2019.Importance estimation for neural network pruning. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition. 1126411272. Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. 2017.Pruning Convolutional Neural Networks for Resource Efficient Inference. In InProceedings of the International Conference on Learning Representations. Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and An-drew Y Ng. 2011. Reading digits in natural images with unsupervised featurelearning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning2011 (2011). Jean Ogier du Terrail, Samy-Safwan Ayed, Edwige Cyffers, Felix Grimberg,Chaoyang He, Regis Loeb, Paul Mangold, Tanguy Marchand, Othmane Mar-foq, Erum Mushtaq, et al. 2022. FLamby: Datasets and Benchmarks for Cross-SiloFederated Learning in Realistic Healthcare Settings. Advances in Neural Informa-tion Processing Systems 35 (2022), 53155334. Zhen Qin, Shuiguang Deng, Mingyu Zhao, and Xueqiang Yan. 2023. FedAPEN:Personalized Cross-silo Federated Learning with Adaptability to Statistical Het-erogeneity. In Proceedings of the 29th ACM SIGKDD Conference on KnowledgeDiscovery and Data Mining. 19541964.",
  "Sebastian U Stich, Jean-Baptiste Cordonnier, and Martin Jaggi. 2018. SparsifiedSGD with memory. Advances in Neural Information Processing Systems 31 (2018)": "Benyuan Sun, Hongxing Huo, Yi Yang, and Bo Bai. 2021. Partialfed: Cross-domainpersonalized federated learning via partial initialization. Advances in NeuralInformation Processing Systems 34 (2021), 2330923320. Yue Tan, Yixin Liu, Guodong Long, Jing Jiang, Qinghua Lu, and Chengqi Zhang.2023. Federated learning on non-iid graphs via structural knowledge sharing. InProceedings of the AAAI conference on artificial intelligence, Vol. 37. 99539961. Sebastian Shenghong Tay, Xinyi Xu, Chuan Sheng Foo, and Bryan Kian HsiangLow. 2022. Incentivizing collaboration in machine learning via synthetic datarewards. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 36.94489456. Md Palash Uddin, Yong Xiang, Xuequan Lu, John Yearwood, and Longxiang Gao.2020. Mutual information driven federated learning. IEEE Transactions on Paralleland Distributed Systems 32, 7 (2020), 15261538.",
  "Tianhao Wang, Johannes Rausch, Ce Zhang, Ruoxi Jia, and Dawn Song. 2020. Aprincipled approach to data valuation for federated learning. Federated Learning:Privacy and Incentive (2020), 153167": "Yangyang Wang, Xiao Zhang, Mingyi Li, Tian Lan, Huashan Chen, Hui Xiong,Xiuzhen Cheng, and Dongxiao Yu. 2023. Theoretical Convergence GuaranteedResource-Adaptive Federated Learning with Mixed Heterogeneity. In Proceedingsof the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.24442455. Zihui Wang, Zhaopeng Peng, Xiaoliang Fan, Zheng Wang, Shangbin Wu, Rong-shan Yu, Peizhen Yang, Chuanpan Zheng, and Cheng Wang. 2024. FedAVE:Adaptive data value evaluation framework for collaborative fairness in federatedlearning. Neurocomputing (2024), 127227.",
  "Xinyi Xu and Lingjuan Lyu. 2020. A reputation mechanism is all you need:Collaborative fairness and adversarial robustness in federated learning. arXivpreprint arXiv:2011.10464 (2020)": "Xinyi Xu, Lingjuan Lyu, Xingjun Ma, Chenglin Miao, Chuan Sheng Foo, andBryan Kian Hsiang Low. 2021. Gradient driven rewards to guarantee fairnessin collaborative machine learning. Advances in Neural Information ProcessingSystems 34 (2021), 1610416117. Yuan-Yi Xu, Ci-Siang Lin, and Yu-Chiang Frank Wang. 2023. Bias-EliminatingAugmentation Learning for Debiased Federated Learning. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2044220452. Gang Yan, Hao Wang, Xu Yuan, and Jian Li. 2023. Criticalfl: A critical learningperiods augmented client selection framework for efficient federated learning.In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery andData Mining. 28982907.",
  "( = 20)46.53(24.10)46.98(28.41)43.50(20.17)46.03(25.70)": ": The maximum test accuracy (%) achieved by FedSACacross different , given a fairness threshold of > 95%,on CIFAR10. Values in the middle brackets represent theminimum test accuracy (%) among 10 clients. Existing methods allocate rewards to clients lack sufficient differ-entiation, resulting in an ongoing unfairness for high-contributingclients. For example in SVHN-CLA ( (d)), the contributions(i.e., Standalone) of 3 and 10 differ significantly. How-ever, CFFL and CGSV do not exhibit a substantial difference in therewards assigned to them. In addition, FedSAC effectively differen-tiates the rewards it received, thereby ensuring the collaborativefairness in FL.",
  "D. THE IMPACT OF ON THE EXPERIMENT": "In , we present the performance of FedSAC with differentvalues of on each scene of CIFAR10. The experiments demon-strate that as increases, the maximum test accuracy will graduallydecrease. This is because the size of the submodels downloaded byclients increases as decreases. When is small, the submodels oflow-contribution clients contain more neurons, enabling effectivetraining to enhance all local model performance.",
  "E. THE COMMUNICATION COST EXPERIMENTS": "The most works on collaborative fairness require full clients infor-mation , which will inevitably introduce large communi-cation overhead and computation cost. In addition, this communica-tion and computation overhead of full sampling problem is tolerablefor most cases in cross-silo FL scenarios , such as healthcare andfinance, because there are only dozens of clients in cross-silo FLscenarios. In the , the results show that our FedSAC demon-strates less communication costs than all the baseline methods inthree datasets across all settings. In addition, our FedSAC does notintroduce additional communication, because the evaluation of neu-ron importance is conducted on the server without communicatingwith clients, effectively preventing any communication overhead.In the , FedSAC exhibits a communication complexity ofO(d*m) per round , where m<=1 represents the average ratioof the parameters of the submodel compared to the global model.",
  "F. DETAILS ON HYPER-PARAMETERS": "For each dataset, the local data of each client was partitioned intotraining and validation sets. Then, we tuned each dataset hyper-parameters by using grid search with FedAvg. Subsequently, weapplied the optimal parameters obtained from the validation dataset.The optimized hyper-parameters for scenarios (i.e., POW, CLA,DIR(1.0), DIR(2.0), and DIR(3.0)) are shown in ."
}