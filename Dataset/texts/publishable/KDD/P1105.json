{
  "ABSTRACT": "Large Language Models (LLMs) have revolutionized the fields ofcomputer vision (CV) and natural language processing (NLP). Oneof the most notable advancements of LLMs is that a single model istrained on vast and diverse datasets spanning multiple domains aparadigm we term All in One. This methodology empowers LLMswith super generalization capabilities, facilitating an encompassingcomprehension of varied data distributions. Leveraging these capa-bilities, a single LLM demonstrates remarkable versatility across avariety of domains a paradigm we term One for All. However,applying this idea to the graph field remains a formidable challenge,with cross-domain pretraining often resulting in negative transfer.This issue is particularly important in few-shot learning scenarios,where the paucity of training data necessitates the incorporationof external knowledge sources. In response to this challenge, wepropose a novel approach called Graph COordinators for PrEtrain-ing (GCOPE), that harnesses the underlying commonalities acrossdiverse graph datasets to enhance few-shot learning. Our novelmethodology involves a unification framework that amalgamatesdisparate graph datasets during the pretraining phase to distill andtransfer meaningful knowledge to target tasks. Extensive exper-iments across multiple graph datasets demonstrate the superior",
  "Equal Contribution.Corresponding author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08 efficacy of our approach. By successfully leveraging the synergisticpotential of multiple graph datasets for pretraining, our work standsas a pioneering contribution to the realm of graph foundationalmodel. Code available at",
  "pretraining; prompt tuning; graph neural networks": "ACM Reference Format:Haihong Zhao, Aochuan Chen, Xiangguo Sun, Hong Cheng, and Jia Li.2024. All in One and One for All: A Simple yet Effective Method towardsCross-domain Graph Pretraining. In Proceedings of the 30th ACM SIGKDDConference on Knowledge Discovery and Data Mining (KDD 24), August2529, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.",
  "INTRODUCTION": "Recently, artificial general intelligence (AGI) has achieved remark-able advancement in the realms of computer vision (CV) and natural language processing (NLP) . One of the key in-novations for these models is that they pre-train one foundationmodel through various contexts, making the model absorb andsynthesize knowledge across diverse domains (a.k.a all in one) todeliver robust, context-aware responses. Their ability to generalizeand apply learned knowledge to a wide spectrum of downstreamdomains (a.k.a one for all) is a testament to the success of theirpre-training strategies, which effectively capture and utilize thenuances across different domains.Compared with NLP and CV areas, graph data is non-linear andmore general. For example, a sentence can be seen as a graph path",
  "KDD 24, August 2529, 2024, Barcelona, SpainHaihong Zhao, Aochuan Chen, Xiangguo Sun, Hong Cheng & Jia Li": "Fan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. 2019. Infograph: Un-supervised and semi-supervised graph-level representation learning via mutualinformation maximization. arXiv preprint arXiv:1908.01000 (2019). Ke Sun, Zhouchen Lin, and Zhanxing Zhu. 2020. Multi-stage self-supervisedlearning for graph convolutional networks on graphs with few labeled nodes. InProceedings of the AAAI conference on artificial intelligence, Vol. 34. 58925899. Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, and Jihong Guan. 2023. All in One:Multi-Task Prompting for Graph Neural Networks. In Proceedings of the 26thACM SIGKDD international conference on knowledge discovery & data mining(KDD23). 21202131.",
  "MOTIVATION": "WisconsinTexasCornellChameleonSquirrel Improvement (%) From PubmedFrom Photos : Negative transfer phenomenon in the single-sourcecross-domain transfer setting. Sources (Pubmed and Photos)are two homophilic datasets. Targets (Wisconsin, Texas, Cor-nell, Chameleon, and Squirrel) are five heterophilic graphs. Unlike images, words, or sentences, which often share extensiveunderlying semantics across datasets, graph data are more abstract,making it more challenging to discern low-level common semantics.Therefore, the negative transfer phenomenon is commonlyfound in the field of graph learning. Here, we focus on the rep-resentative C-way-K-shot setting. We validate this phenomenon",
  ": Overview of our proposed GCOPE method. The left part is our pretraining stage and the right part transferring stage": "through extensive experimentation, examining two main scenarios:the transfer of knowledge from a single source dataset, as well asfrom multiple source datasets.Observations: As depicted in , transferring from a singlesource dataset does indeed negatively affect the target task, con-firming our analysis of the distinctiveness of two graph datasets.In order to overcome this obstacle, it is necessary to expand thescope of the source dataset so that it can offer valuable insightsfor the downstream task. However, a brutal combination of sourcedatasets still failed to enhance the performance of the target task.We detail this in section 4.2.Further Analysis: The first reason causing this negative transferacross domains is that the structural patterns are different, espe-cially reflected in homophilic and heterophilic datasets. Graph datais characterized by its complex network of connections, wherenodes are not isolated but part of an interconnected structure. Therelationships among nodes, represented by edges, facilitate the flowof influence and information, making each node both a product anda contributor to its environment. This interconnectivity is centralto graph data, resulting in systems where the collective propertiessurpass those of individual components. Nonetheless, this char-acteristic presents challenges for machine learning models thatattempt to learn from multiple graph datasets simultaneously. Eachdataset is akin to its own distinct ecosystem, governed by its uniquetopology and rules. Within a given graph, nodes and edges are at-tuned to specific patterns and linkages that are relevant withinthat context. When merging different graphs for joint training, theinherent disparity of each graphs structure becomes a barrier, asthe datasets naturally exist in isolation from one another and thusthe information flow is blocked.The second reason is the lack of feature alignment across thesedatasets. Graph attributes are heterogeneous and context-dependent,representing a wide range of abstract concepts and connections.Unlike textual or visual data, which have a common referenceframework, graph attributes are highly varied and specific to theirdomain. Consequently, aligning features from different graphs is adaunting task, as there is no straightforward method to reconcilethe disparate languages of each dataset into a unified representationfor machine learning models to process.Objectives: In this paper, we focus on the above two challenges,the disparity and isolation of graph datasets and the difficulty inaligning their diverse features. We denote our pretraining datasets",
  "V () denote the sets of nodes and edges, respectively. Each G()": "is associated with feature matrix () R|V () | and adjacencymatrix () R|V () ||V () |. Our objective is to train a GNN ()parameterized by , which is capable of encoding knowledge trans-ferable to downstream tasks. The downstream dataset is representedas G( ) = (V ( ), E ( )) with the feature matrix ( ) and adajcencymatrix ( ). To address this question, we carefully design a gen-eral pretraining scheme that is independent of datasets, networkarchitectures and downstream tasks. 3METHOD3.1Overview of Our FrameworkIn this section, we introduce a cohesive approach that enables thesimultaneous pretraining of a graph model on multiple datasets.We utilize established pretraining objectives, namely GraphCL and SimGRACE , to guide the learning process. Additionally,we implement novel techniques specifically designed to overcomethe challenges highlighted in . A visual representation ofour methodology is provided in .3.2Aligning Graphs by CoordinatorsDifferent graphs usually have different features and structural pat-terns. Here we propose a two-phase graph alignment approach. Thefirst step is to make the feature dimensions all the same in format.Then we seek to further learn a latent data alignment strategy bycoordinators, which can reformulate graphs w.r.t their structuralpatterns and semantic patterns.3.2.1Feature Projection. During the pretraining phase of our GNNmodel, we first present a projecting module to align feature dimen-sion, which is described by:",
  "() = Proj( ()) R|V () |P,(1)": "where Proj() denotes a certain projection operation andP denotesthe predefined projected dimension. Without loss of generality, weprovide two widely used methods, singular value decomposition(SVD) and attention mechanism, as two representative projectionoperations in this paper. However, it is worth mentioning that themere projection of features onto a common plane does not sufficeto address the alignment challenge; additional alignment endeavorsare indispensable.",
  "|V () | < +11|V () |0otherwise.(3)": "We treat the coordinator representation as learnable parameters.We opt for a flexible approach so that they can evolve organicallyalongside the GNN training, adapting dynamically to the data. Thisensures they continually refine their attributes to effectively serveas conduits of evolving graph embeddings.(iii). Generate Graph Batches for Efficient Training: Moreover,by harnessing the interconnectivity facilitated by coordinators, weunlock the potential for joint sampling of nodes originating from di-verse data graphs. This innovative strategy empowers the trainingprocess through aggregated batches, thereby promoting the natu-ral alignment of features across datasets. Exposing the model to aplethora of features within a single learning iteration prompts it toseek out unified representations, effectively synthesizing insightsfrom disparate sources into cohesive and comprehensive representa-tions. This unified approach not only enhances the models abilityto capture the underlying structure of the data but also fostersrobust generalization across domains. 3.2.3Why It Works? Kindly note that our proposed coordinatorshave the same theory support as graph prompts. The main differ-ence is that general graph prompts are used for downstream taskswhile we use coordinators during pretraining. Evidenced by ,prompts are demonstrated to be tantamount to graph transforma-tions (e.g., node, edge, graph removing or adding, and some nodefeature operations). In parallel, our coordinators can be construedas a form of graph transformation. Given a GNN denoted as ()and a prompt , it has been established that:",
  "( + , ) = (( , )) + (4)": "where represents an error bound associated with the GNN() and the prompt . Here, () signifies a graph transformationoperation. Given that the learnable features embedded within ourcoordinators can be interpreted as the prompt , they facilitateequivalent graph transformations for the originally isolated graphs.The learned equivalent transformation effectively harmonizes thesegraphs (e.g., homophilic and heterophilic graphs), enabling thediscovery of inherent commonalities among them.Additionally, we would like to facilitate a better understandingof our frameworks design principles and the functions of its vari-ous components through Interdisciplinary Teaching, which is acutting-edge approach in the field of education:Interdisciplinary Teaching aims to enhance the comprehensive-ness and depth of learning by integrating knowledge and methodsfrom different subjects, similar to how GCOPE integrates diversegraph datasets. It addresses the complexity of real-world prob-lems (like diverse and complex structures, attributes, and structure-attribute patterns), requiring a multidisciplinary perspective forsolutions and encouraging students to apply cross-domain knowl-edge. This approach leads to a more thorough understanding andhigher-level thinking skills, mostly like the transferrable pretrainedgraph representations. The integration strategy in education, whichhighlights core concepts and fosters interaction among disciplines,resembles the coordinators in GCOPE. This helps students see thebridges between subjects, nurturing flexible and innovative think-ing, like transferring to new downstream datasets, especially thefew-shot scenarios. 3.3Pretraining on Multi-domain GraphsOur approach is very flexible and compatible with many pretrainingapproaches. Here, we present a general pretraining framework thatcan extract high-quality embeddings at both the node and graphlevels. Prior research has predominantly focused on pretrainingwithin the same data domains as the downstream task . Withinthis context, GraphCL and SimGRACE have been particu-larly noteworthy for their effectiveness in generating granular nodeembeddings as well as holistic graph embeddings. GraphCL em-ploys graph data augmentation to generate positive pairs, whereasSimGRACE perturbs the GNN to achieve this objective. Given thestrengths of these methods, we chose them as our basic pretrainingstrategy.Next, we move on to the crucial task of preserving the integrityof information from each graph. To this end, we introduce an aux-iliary feature reconstruction loss. This loss is quantified throughthe mean squared error (MSE) metric, which assesses the discrep-ancy between the original node feature vector (after projection)",
  "All in One and One for All: A Simple yet Effective Method towards Cross-domain Graph PretrainingKDD 24, August 2529, 2024, Barcelona, Spain": "knowledge from the pretrained model to new tasks. While not guar-anteed to outperform finetuning in every scenario, graph promptsexcel in terms of efficiency due to their minimal parameter foot-print. This shift towards graph prompts highlights the ongoingexploration of efficient and effective knowledge transfer withinthe realm of GNNs. Further research is warranted to refine promptconstruction strategies and assess their broader applicability acrossdiverse downstream tasks.6CONCLUSION This study delves into the intricate phenomenon of negative transferwithin the field of graph learning, shedding light on its complexitiesthrough a rigorous analysis. In response, we introduce an innovativepretraining approach named GCOPE, devised to effectively mitigatenegative transfer effects. GCOPE leverages coordinators to seam-lessly amalgamate disparate graphs, establishing interconnectionsand aligning their features. Our thorough experimentation, con-ducted across a spectrum of homophilic and heterophilic datasets,vividly demonstrates the efficacy of our proposed method.Despite the notable success achieved by GCOPE, it is prudentto acknowledge the potential limitations stemming from the non-parametric nature of SVD. This aspect may constrain the methodsgeneralizability across diverse datasets. As such, our future researchendeavors will be directed towards devising a more robust learning-based feature projection pipeline, which naturally understands andaligns features of graphs from a variety of domains. This Research of Li was supported by NSFC Grant No. 62206067 andGuangzhou-HKUST(GZ) Joint Funding Scheme 2023A03J0673. Theresearch of Cheng was supported in part by project #MMT-p2-23of the Shun Hing Institute of Advanced Engineering, The ChineseUniversity of Hong Kong and by grants from the Research GrantCouncil of the Hong Kong Special Administrative Region, China(No. CUHK 14217622). Deyu Bo, Xiao Wang, Chuan Shi, and Huawei Shen. 2021. Beyond low-frequencyinformation in graph convolutional networks. In Proceedings of the AAAI Confer-ence on Artificial Intelligence, Vol. 35. 39503957. Aochuan Chen, Yuguang Yao, Pin-Yu Chen, Yihua Zhang, and Sijia Liu. 2023.Understanding and improving visual prompting: A label-mapping perspective. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.1913319143.",
  "exp(sim((PS( , , )),(NS( , , ))/)+ 2(5)": "where is the concatenated feature matrix of all pretraining datasetsafter projection (Equation 1), is the adjacency matrix after beingconnected by the coordinators (Equation 2), PS, NS respectivelydenote the positive sampling and negative sampling, sim denotes asimilarity metric (e.g., cosine similarity), , are different graphaugmentations, and is the reconstruction loss coefficient thatcontrols how much attention is paid to the reconstruction task.",
  "Applying Knowledge to Downstream Data": "Our pretraining methodology is inherently flexible and agnostic todownstream task constraints, making it compatible with variousadaptation techniques. For instance, the recent introduction ofprompting in the graph field has attracted considerable interestdue to its remarkable efficiency and effectiveness . To show thesuperior generalization ability of our GCOPE method, we exploreits transferability within both the conventional finetuning paradigmand the nascent graph prompt framework.Drawing inspiration from the methodology presented by , wecast downstream tasks into a graph-level framework. As delineatedin their work, the maximal retention of pretraining knowledge isobserved when downstream tasks are aligned within the same taskspace as the pretraining task. Given our pretraining is conductedat the graph level, we correspondingly transform all downstreamtasks to this level by constructing induced subgraphs. The mainalgorithm is presented in Algorithm 1.",
  "Complexity Analysis": "Thanks to the simplicity of our pretraining framework, the increasein parameter complexity is only marginal. The only additionalparameters introduced are the features of the coordinators, with acomplexity of O(A), which scales linearly with the number ofpretraining datasets. In practical settings, scenarios characterized byan excessively abundant availability of pretraining datasets are rare.Assuming GNN we employ comprises layers with a maximumlayer width of , and let = =1 |V () | and = =1 |E () |. Itis worth noting that the time complexity of a typical graph model,such as Graph Convolutional Network (GCN), is O(2 + +) . With the incorporation of coordinators, the revised timecomplexity becomes O(( + )2 + ( + + ) + ( + )).The additional time complexity is O(2 + ( + ) + ).Given that , the supplementary time cost scales nearlylinearly with the number of original nodes.",
  "RQ5. Which projection operation is more compatible for CCOPE?": "4.1Experimental SetupDatasets. For comprehensive comparisons, we conduct experi-ments on ten real-world datasets. We choose five homophily datasetsin experiments, including Cora , Citeseer , Pubmed ,Computers and Photos datasets , and five heterophilic datasets,including three sub-datasets derived from the WebKB (Cornell,Texas, and Wisconsin) and two page-page networks (Chameleonand Squirrel) extracted from Wikipedia . summarizesthe details, where () is a metric that represents the degreesof homophily and heterophily. The values of () are directlydrawn from , indicating that the first five datasets are highlyhomophilic and the latter five are highly heterophilic . Dif-ferent degrees of homophily and heterophily indicate different se-mantics in graphs. For more detailed information on these datasets,please check in Appendix.",
  "IMP (%)12.57%4.58%13.76%6.65%6.08%16.87%37.66%14.29%29.62%11.97%6.01%25.36%3.25%1.06%16.39%": "Baselines. We compare our proposed method with the followingbaselines, categorized into three groups and accompanied by con-cise descriptions. (1) Supervised methods: These methods train aGNN model on a downstream task and infer results directly. In thiswork, four famous GNN models are adopted, including GCN ,GAT , BWGNN , and FAGCN . We choose GCN andFAGCN as the backbones for our proposed GCOPE method sinceFAGCN is tailored to address both homophilic and heterophilicgraphs and GCN, a widely used GNN model, is the basis ofFAGCN. (2) Isolated Pretraining (IP) with finetuning: Thesemethods initially utilize multiple cross-domain datasets as sourcedatasets, which are combined in an isolated manner to pretraina GNN model in a self-supervised fashion (e.g. GraphCL orSimGRACE ). Here, isolated denotes that the datasets areamalgamated into a batch object without establishing inter-datasetconnections, resulting in an adjacency matrix composed of distinctblocks. Subsequently, the pretrained model undergoes finetuningfor a new downstream task. (3) Graph Coordinators for Pre-training (GCOPE) with transferring: With learnable coordina-tors, our proposed GCOPE method aims to unify the isolated sourcedatasets into one large-scale dataset with inter-dataset connectionsfor pretraining and then transfer the pretrained GNN model to adownstream task via finetuning or prompting . Metrics and Implementations. We choose three widely usedmetrics to evaluate the node classification task , in-cluding classification accuracy (Acc), mean AUC-ROC (AUC), andmean F1-score (F1). We leverage a 10-fold partition strategy to splitthe 10 real-world datasets, nine datasets as cross-domain sourcedatasets for pretraining and the rest one dataset as the target datasetfor transferring. To unify features of multiple cross-domain sourcedatasets onto one single plane, we leverage SVD to reduce the ini-tial features to 100 dimensions. We fix the number of graph neurallayers at 2, with a hidden dimension of 100 for GCN, FAGCN, andGAT models. Additionally, we adopt the identical hyperparametersas specified in for BWGNN. For all networks, we apply theAdam optimizer and the learning rate is assigned as 0.0001. In termsof GCOPE, we introduce one coordinator for each source datasetand assign 0.2 as the default reconstruction weight. Finally, in thetransferring stage, we adopt node classification as the downstreamtask. For fair comparisons, we apply the C-way-K-shot learningsetting, same as , for each target dataset to build the trainingdata and then split the rest of the data randomly in 1:9 for vali-dating and testing. In this paper, we report average results on alldownstream tasks. More implementation details are shown in Ap-pendix A, in which we also analyze the performance of GCOPEwith more coordinators and dynamical edges between coordinators.",
  "Cross-domain Transfer Performance withFew-shot Learning Settings (RQ1)": "We compare our proposed GCOPE method with all the baselines onthe downstream node classification tasks under the C-way-1-shotsetting. We repeat the evaluation 5 times and report the averageresults and standard deviations in and . The resultsof supervised methods are the benchmark to help validate whetherthe pretrained GNN models effectively transfer to the downstreamtasks. As is known, pretraining GNNs aims to transfer prior knowl-edge from upstream tasks to improve the performance of GNNs ondownstream tasks, especially in few-shot scenarios. However, weobserve that most IP with finetuning methods struggle to achievebetter performance compared with supervised methods, embody-ing the negative transfer phenomenon. This is due to the apparentdifferences in distribution across cross-domain datasets. Under theIP strategy, each graph sample only contains information from oneof the nine distributions, making it challenging for GNN modelsto fit the nine independent distributions into a unified one andlearn general graph representations effectively. In contrast, our pro-posed GCOPE with finetuning methods significantly outperformsalmost all baselines, achieving positive transfer. This is becausecoordinators connect all the source datasets together, unifying nineindependent distributions into one cohesive joint distribution. Inthe pretraining stage, all the training samples are drawn from thisjoint distribution, facilitating the sharing of information acrosscross-domain datasets. Consequently, the GNN model is able to bal-ance the shared information and learn better graph representationsfor transferring knowledge on the downstream task. The reportedimprovements range from 3.25% up to 37.66% in terms of nodeclassification accuracy. Kindly note that our few-shot experimentsettings are different from the classic few-shot node classificationsettings that are actually designed for the meta-learning task. Addi-tionally, we also evaluate our proposed GCOPE on C-way-3-shotand C-way-5-shot settings, shown in Table A1 and Table A2.4.3Interconnectivity Analysis (RQ2)In this section, we investigate the impact of edges between differentcoordinators on the effectiveness of our proposed GCOPE frame-work. Specifically, we compare two variants: GCOPE/w, whichincludes inter-coordinator edges, and GCOPE/wo, which excludesthem. Both variants share the same hyperparameters, with FAGCNas the backbone model and pretraining based on GraphCL. Addi-tionally, the datasets utilized for pretraining and downstream tasksare partitioned using the 10-fold strategy. presents the nodeclassification accuracy (meanstd). Our experimental findings un-derscore the essential and effective role of inter-coordinator edgesin GCOPE.4.4Reconstruction Analysis (RQ3)We conduct a comparison of downstream node classification per-formance on Citeseer between our proposed GCOPE method withvarying reconstruction loss coefficient values , and a supervisedmethod to assess the efficacy of the reconstruction module. Specifi-cally, FAGCN serves as the backbone model for both GCOPE andthe supervised method. For GCOPE, GraphCL is utilized as the pre-training strategy. All other hyperparameters between GCOPE andthe supervised method are maintained consistent. depictsthe experimental results, focusing on Acc, AUC, and F1 metrics.",
  "Photos0.6037.030.6329.02Squirrel0.2177.000.2257.00": "Our experimental findings yield the following observations:First, GCOPE without reconstruction ( = 0.0) outperforms thesupervised pretraining, highlighting the effectiveness of the intro-duced coordinators. Second, GCOPE with reconstruction ( > 0.0)achieves optimal performance when is set to 0.2, surpassing boththe supervised pretraining and GCOPE ( = 0.0). This improve-ment is attributed to the reconstruction modules ability to aligngraph features across datasets, facilitating more effective learningof shared information by GNNs from cross-domain source datasets.Third, as exceeds 0.2, performance begins to deteriorate, ulti-mately falling short of both the supervised pretraining and GCOPE( = 0.0). This decline can be attributed to excessively large values,which cause the model to overly prioritize reconstruction at theexpense of its primary pretraining task. In summary, the inclusionof the reconstruction module with relatively small values provesessential for our proposed GCOPE method.4.5Transferring by Graph Prompt (RQ4)In addition to finetuning, we explore the feasibility of leveragingthe graph prompt technique to transfer upstream cross-domainknowledge learned by GCOPE. Specifically, we adopt the imple-mentation of ProG , a widely used graph prompt method, whichinvolves freezing the parameters of pretrained GNNs and incorpo-rating graph prompts in the downstream node classification task.Subsequently, we evaluate the performance of GCOPE with ProGon four downstream datasets, comprising two homophilic and twoheterophilic datasets. The experimental results are presented in Ta-ble 5. For comparison, we include results of the supervised method,IP with finetuning, and GCOPE with finetuning.Based on our experimental observations, we draw the followingconclusions: In comparison to the supervised and IP with finetun-ing methods, both GCOPE with finetuning and GCOPE with ProGexhibit superior performance. Notably, GCOPE with ProG achievespositive transfer with minimal tunable parameters in the down-stream node classification task. While the performance of GCOPEwith ProG is slightly lower than that of GCOPE with finetuning, thedisparity between the two methods is significantly narrower thanthat between GCOPE with ProG and the supervised method. Basedon the aforementioned analysis, we can assert that our proposedGCOPE framework can effectively benefit downstream prompts.4.6Compatibility Analysis of ProjectionOperation (RQ5)To study the compatibility of various projection operations, weevaluate our proposed GCOPE method with SVD or attention mech-anism across ten cross-domain real-world datasets. Specifically, weset FAGCN as the backbone, leverage GraphCL for pretraining, andfinally transfer the pretrained GNNs to the downstream C-way",
  "Photos0.4857.040.6329.02Squirrel0.2210.000.2257.00": "1-shot node classification tasks. For GCOPE with attention mecha-nism, we equip each dataset with a data-specific attention moduleto project their respective features into a shared plane, regardlessof their position in the pretraining and finetuning pipeline. Wereport the node classification accuracy (meanstd ) in . Wecan easily observe that GCOPE with attention mechanism signifi-cantly underperforms the ones with SVD. This could be attributedto the fact that the utilized samples are insufficient to train attentionmodules effectively. Each attention module has 32, 792, 288 tunedparameters, while SVD is a non-parametric mathematical method.Therefore, we ensure that SVD is more compatible under few-shotlearning scenarios. For effectiveness and simplicity, we set SVD asthe default projection operation of GCOPE in this work.",
  "RELATED WORK": "Graph Pretraining. In the machine learning (ML) research field,pretraining is widely acknowledged for its ability to leverage ex-isting data to train a feature extractor with robust generalizationcapabilities . Specifically within the graph do-main, pretraining methods can be categorized into three maintypes: generation-based, auxiliary property-based, and contrast-based methods . Generation-based methods utilize feature or structure reconstruction as the loss function to extract embed-dings with strong generalization properties (e.g., GAE , Graph-MAE , and GraphMAE2 ). Auxiliary property-based meth-ods introduce new attributive or structural properties as supervi-sion signals, such as clustering pseudo labels utilized by M3S .Contrast-based methods define positive and negative embeddingpairs and aim to bring positive pairs closer while pushing negativepairs apart. Among these three categories, contrast-based methodshave garnered the most popularity and achieved notable successes. However, despite the success of thesemethods across various graph tasks, none have managed to achievethe objective of pretraining on multiple graph datasets belongingto different domains. Consequently, the efficacy of pretraining re-mains constrained by the size and diversity of the source data. Thisremains an open question in the graph pretraining field.Graph Transfer Learning. Recent years have witnessed signifi-cant advancements in GNNs. However, adapting pre-trained GNNsto diverse downstream tasks efficiently remains a challenge. Tradi-tionally, finetuning, as presented in , has dominated thisfield. This approach leverages a pretrained GNN as a foundation,finetuning either the final layer or the entire model for the specifictask. Finetuning has consistently achieved state-of-the-art (SOTA)performance.However, recent exploration has led to the emergence of graphprompts as a compelling alternative, drawing inspira-tion from the NLP communitys prompting paradigm .As detailed in , a graph prompt comprises three key elements:prompt tokens, which contain the prompt content in the form ofvectors; token structures, which depicts how the tokens are con-nected; inserting patterns, which fuses the graph prompt with thetarget data. By carefully crafting these components, often throughlearning-based approaches, graph prompts can effectively transfer",
  "Taoran Fang, Yunchao Zhang, Yang Yang, and Chunping Wang. 2022. Prompttuning for graph neural networks. arXiv preprint arXiv:2209.15240 (2022)": "Jean-Bastien Grill, Florian Strub, Florent Altch, Corentin Tallec, PierreRichemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, ZhaohanGuo, Mohammad Gheshlaghi Azar, et al. 2020. Bootstrap your own latent-a newapproach to self-supervised learning. Advances in neural information processingsystems 33 (2020), 2127121284. Russell Alan Hart, Linlin Yu, Yifei Lou, and Feng Chen. 2023. Improvements onUncertainty Quantification for Node Classification via Distance Based Regular-ization. In Thirty-seventh Conference on Neural Information Processing Systems.",
  "Kaveh Hassani and Amir Hosein Khasahmadi. 2020. Contrastive multi-viewrepresentation learning on graphs. In International conference on machine learning.PMLR, 41164126": "Zhenyu Hou, Yufei He, Yukuo Cen, Xiao Liu, Yuxiao Dong, Evgeny Kharlamov,and Jie Tang. 2023. GraphMAE2: A Decoding-Enhanced Masked Self-SupervisedGraph Learner. In Proceedings of the ACM Web Conference 2023. 737746. Zhenyu Hou, Xiao Liu, Yukuo Cen, Yuxiao Dong, Hongxia Yang, Chunjie Wang,and Jie Tang. 2022. Graphmae: Self-supervised masked graph autoencoders. InProceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery andData Mining. 594604. Jongwon Jeong, Hoyeop Lee, Hyui Geon Yoon, Beomyoung Lee, Junhee Heo,Geonsoo Kim, and Jin Seon Kim. 2024. iGraphMix: Input Graph Mixup Methodfor Node Classification. In The Twelfth International Conference on LearningRepresentations.",
  "Yuhan Li, Peisong Wang, Zhixun Li, Jeffrey Xu Yu, and Jia Li. 2024. ZeroG:Investigating Cross-dataset Zero-shot Transferability in Graphs. arXiv preprintarXiv:2402.11235 (2024)": "Yuhan Li, Jian Wu, Zhiwei Yu, Brje F Karlsso, Wei Shen, Manabu Okumura,and Chin-Yew Lin. 2023. Unlocking Science: Novel Dataset and Benchmark forCross-Modality Scientific Information Extraction. arXiv preprint arXiv:2311.08189(2023). Hao Liu, Jiarui Feng, Lecheng Kong, Ningyue Liang, Dacheng Tao, Yixin Chen,and Muhan Zhang. 2023. One for all: Towards training one graph model for allclassification tasks. arXiv preprint arXiv:2310.00149 (2023). Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, andGraham Neubig. 2023. Pre-train, prompt, and predict: A systematic survey ofprompting methods in natural language processing. Comput. Surveys 55, 9 (2023),135. Yang Liu, Jiashun Cheng, Haihong Zhao, Tingyang Xu, Peilin Zhao, Fugee Tsung,Jia Li, and Yu Rong. 2024. SEGNO: Generalizing Equivariant Graph NeuralNetworks with Physical Inductive Biases. In The Twelfth International Conferenceon Learning Representations. Yixin Liu, Ming Jin, Shirui Pan, Chuan Zhou, Yu Zheng, Feng Xia, and S Yu Philip.2022. Graph self-supervised learning: A survey. IEEE Transactions on Knowledgeand Data Engineering 35, 6 (2022), 58795900.",
  "Zemin Liu, Xingtong Yu, Yuan Fang, and Xinming Zhang. 2023. Graphprompt:Unifying pre-training and downstream tasks for graph neural networks. In Pro-ceedings of the ACM Web Conference 2023. 417428": "Sitao Luan, Chenqing Hua, Minkai Xu, Qincheng Lu, Jiaqi Zhu, Xiao-Wen Chang,Jie Fu, Jure Leskovec, and Doina Precup. 2023. When Do Graph Neural NetworksHelp with Node Classification? Investigating the Homophily Principle on NodeDistinguishability. In Thirty-seventh Conference on Neural Information ProcessingSystems. Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel.2015. Image-based recommendations on styles and substitutes. In Proceedingsof the 38th international ACM SIGIR conference on research and development ininformation retrieval. 4352. Galileo Namata, Ben London, Lise Getoor, Bert Huang, and U Edu. 2012. Query-driven active surveying for collective classification. In 10th international workshopon mining and learning with graphs, Vol. 8. 1.",
  "Siheng Xiong, Ali Payani, Ramana Kompella, and Faramarz Fekri. 2024. Largelanguage models can learn temporal reasoning. arXiv preprint arXiv:2401.06853(2024)": "Zhe Xu, Yuzhong Chen, Qinghai Zhou, Yuhang Wu, Menghai Pan, Hao Yang,and Hanghang Tong. 2023. Node classification beyond homophily: Towards ageneral solution. In Proceedings of the 29th ACM SIGKDD Conference on KnowledgeDiscovery and Data Mining. 28622873. Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, andYang Shen. 2020. Graph contrastive learning with augmentations. Advances inneural information processing systems 33 (2020), 58125823.",
  "Wen Zhang, Lingfei Deng, Lei Zhang, and Dongrui Wu. 2022. A survey onnegative transfer. IEEE/CAA Journal of Automatica Sinica 10, 2 (2022), 305329": "Yihua Zhang, Yimeng Zhang, Aochuan Chen, Jinghan Jia, Jiancheng Liu, GaowenLiu, Mingyi Hong, Shiyu Chang, and Sijia Liu. 2023. Selectivity Drives Produc-tivity: Efficient Dataset Pruning for Enhanced Transfer Learning. arXiv preprintarXiv:2310.08782 (2023). Haihong Zhao, Bo Yang, Jiaxu Cui, Qianli Xing, Jiaxing Shen, Fujin Zhu, andJiannong Cao. 2023. Effective fault scenario identification for communicationnetworks via knowledge-enhanced graph neural networks. IEEE Transactions onMobile Computing (2023). Haihong Zhao, Chenyi Zi, Yang Liu, Chen Zhang, Yan Zhou, and Jia Li. 2024.Weakly Supervised Anomaly Detection via Knowledge-Data Alignment. In Pro-ceedings of the ACM on Web Conference 2024. 40834094.",
  "Code available at Amore detailed information on related datasets is as follows:": "Citation network: Cora and Citeseer datasets consist of adiverse collection of computer science publications, where eachnode is characterized by a bag-of-words representation of papersand a categorical label indicating the paper topic. Pubmed dataset comprises articles related to diabetes from the PubMed data-base. Each node in this dataset is represented by an attribute",
  "vector containing TF/IDF-weighted word frequencies, accompa-nied by a label specifying the particular type of diabetes discussedin the publication": "Amazon network: Computers and Photos datasets aretwo networks illustrating co-purchase relationships sourced fromAmazon. In these networks, each node represents a product, andan edge indicates frequent co-purchases between two products.Additionally, each node is associated with a bag-of-words repre-sentation of product reviews and is labeled with its respectivecategory. WebKB: Cornell, Texas, and Wisconsin are three subdatasetsderived from the WebKB dataset , compiled from multipleuniversities web pages. Each node within these datasets repre-sents a web page, with edges denoting hyperlinks between pages.The node features are represented as bag-of-words representa-tions of the web pages. Additionally, the web pages are manuallycategorized into five distinct labels: student, project, course, staff,and faculty. Wikipedia network: Chameleon and Squirrel datasets con-sist of two page-page networks extracted from Wikipedia, fo-cusing on specific topics. In these networks, nodes representindividual web pages, while edges signify links between pages.Node attributes are defined as sets of informative nouns extractedfrom the pages. Moreover, each node is labeled based on the av-erage monthly traffic received by the respective web page.",
  "Prompting--19,398": "A.3Cross-domain Transfer Performance withother C-way-K-shot settingsWe use Table A1, Table A2, Table A3, and Table A4 to show thecross-domain transfer learning performance with 3/5-shot learningscenarios respectively. From the experimental results, we observethat our proposed GCOPE maintains its superiority over other base-line methods, even though the improvements modestly decreasefrom 1-shot to 5-shot scenarios.A.4Quantitative Analysis of GraphCoordinatorsWe conduct additional experiments to investigate the quantitativeanalysis of graph coordinators, shown by Table A5 and Table A6.Experimental results show that, despite slight fluctuations in per-formance across different datasets, GCOPE with varying numbersof graph coordinators generally outperforms both the Supervisedand IP methods.",
  "A.5Dynamical Study of inter-coordinator edges": "We additionally study the impact of dynamical inter-coordinatoredges on the performance of GCOPE, and report the transfer learn-ing results in Table A7. According to the experimental results, weobserve that GCOPE/d outperforms GCOPE/f on the Wisconsin and Texas, performs better than the supervised method on the Cora butis inferior to GCOPE/f, and exhibits negative transfer on Citeseer.In comparison, GCOPE/f demonstrates positive transfer across alldatasets.",
  "IP0.6063.040.8356.010.5555.070.7425.030.7034.030.6141.090.2588.040.6262.040.2442.040.2443.000.5530.010.1875.010.2223.000.5307.000.1740.02": "GCOPE-10.6579.030.8531.010.5649.000.7125.020.6693.020.6300.030.4013.050.6897.010.3160.020.2886.000.5898.000.2320.000.2257.000.5257.000.1885.01GCOPE-30.6217.000.8267.000.5397.010.7675.040.7005.030.5834.050.5675.030.7334.010.4506.020.2895.000.5785.000.2205.000.2199.010.5274.010.1815.02GCOPE-50.6353.060.8327.030.5692.050.7262.040.6913.020.5468.040.6550.020.8350.000.5047.020.2810.000.5654.000.2113.000.2129.000.5183.000.2029.00 Table A7: Cross-domain transfer learning performance (meanstd Acc/AUC/F1) on two homophilic and two heterophilicdatasets (C-way-1-shot) of GCOPE with full or dynamical inter-coordinator edges. /f means that the inter-coordinator edgesin GCOPE are fully connected to each other. /d represents that inter-dataset edges in GCOPE are dynamically connected bycomputing the similarity between them."
}