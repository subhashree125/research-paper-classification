{
  "Amazon-M2: A Multilingual Multi-locale ShoppingSession Dataset for Recommendation and TextGeneration": "Wei Jin12, Haitao Mao3, Zheng Li1, Haoming Jiang1, Chen Luo1,Hongzhi Wen3, Haoyu Han3, Hanqing Lu1, Zhengyang Wang1, Ruirui Li1,Zhen Li1, Monica Cheng1, Rahul Goutam1, Haiyang Zhang1, Karthik Subbian1,Suhang Wang4, Yizhou Sun5, Jiliang Tang3, Bing Yin1, and Xianfeng Tang1 1 Amazon.com2 Emory University3 Michigan State University4 The Pennsylvania State University5 University of California, Los Angeles{joewjin,amzzhe,jhaoming,zhengywa,ruirul,luhanqin,cheluo}@amazon.com,{amzzhn,chengxc,rgoutam,hhaiz,ksubbian,alexbyin,xianft}@amazon.com, ,, , {haitaoma,wenhongz,hanhaoy1,tangjili}@msu.edu",
  "Abstract": "Modeling customer shopping intentions is a crucial task for e-commerce, as itdirectly impacts user experience and engagement. Thus, accurately understandingcustomer preferences is essential for providing personalized recommendations.Session-based recommendation, which utilizes customer session data to predicttheir next interaction, has become increasingly popular. However, existing sessiondatasets have limitations in terms of item attributes, user diversity, and dataset scale.As a result, they cannot comprehensively capture the spectrum of user behaviorsand preferences. To bridge this gap, we present the Amazon Multilingual Multi-locale Shopping Session Dataset, namely Amazon-M2. It is the first multilingualdataset consisting of millions of user sessions from six different locales, wherethe major languages of products are English, German, Japanese, French, Italian,and Spanish. Remarkably, the dataset can help us enhance personalization andunderstanding of user preferences, which can benefit various existing tasks as wellas enable new tasks. To test the potential of the dataset, we introduce three tasks inthis work: (1) next-product recommendation, (2) next-product recommendationwith domain shifts, and (3) next-product title generation. With the above tasks, webenchmark a range of algorithms on our proposed dataset, drawing new insights forfurther research and practice. In addition, based on the proposed dataset and tasks,we hosted a competition in the KDD CUP 20232 and have attracted thousands ofusers and submissions. The winning solutions and the associated workshop can beaccessed at our website",
  "arXiv:2307.09688v2 [cs.IR] 19 Oct 2023": "which often has a dominant impact on users next behavior. Besides, many recommendation algo-rithms require access to user profiles , which can be incomplete or even missing in real-worldsituations especially when users are browsing in an incognito mode. In these cases, only the mostrecent user interactions in the current session can be utilized for understanding their preferences. Con-sequently, the session-based recommendation has emerged as an effective solution for modeling usersshort-term interest, focusing on a users most recent interactions within the current session to predictthe next product. Over the past few years, the session-based recommendation has gained significantattention and has prompted the development of numerous models . A critical ingredient for evaluating the efficacy of these methods is the session dataset. Whilenumerous session datasets have been carefully curated to meet the requirementsof modeling user intent and are extensively employed for evaluating session-based recommendersystems, they have several drawbacks. First, existing datasets only provide limited product attributes,resulting in incomplete product information and obscuring studies that leverage attribute informationto advance the recommendation. Second, the user diversity within these datasets is limited and maynot adequately represent the diversity of user-profiles and behaviors. Consequently, it can result inbiased or less accurate recommendations, as the models may not capture the full range of customerpreferences. Third, the dataset scale, particularly in terms of the product set, is limited, which fallsshort of reflecting real-world recommendation scenarios with vast product and user bases. To break the aforementioned limitations, we introduce the Amazon Multilingual Multi-Locale Shop-ping Session Dataset, namely Amazon-M2, a large dataset of anonymized user sessions with theirinteracted products collected from multiple language sources at Amazon. Specifically, the datasetcontains samples constructed from real user session data, where each sample contains a list ofuser-engaged products in chronological order. In addition, we provide a table of product attributes,which contains all the interacted products with their associated attributes such as title, brand, color,etc. Modeling such session data can help us better understand customers shopping intentions, whichis also the main focus of e-commerce. Particularly, the proposed dataset exhibits the followingcharacteristics that make it unique from existing session datasets. (a) Rich semantic attributes: Amazon-M2 includes rich product attributes (categorical, textual, andnumerical attributes) as product features including title, price, brand, description, etc. Theseattributes provide a great opportunity to accurately comprehend the users interests. To our bestknowledge, it is the first session dataset to provide textual features.",
  "(b) Large scale: Amazon-M2 is a large-scale dataset with millions of user sessions and products,while existing datasets only contain tens of thousands of products": "(c) Multiple locales: Amazon-M2 collected data from diverse sources, i.e., six different localesincluding the United Kingdom, Japan, Italian, Spanish, French, and Germany. Thus, it provides adiverse range of user behaviors and preferences, which can facilitate the design of less biasedand more accurate recommendation algorithms. (d) Multiple languages: Given the included locales, Amazon-M2 is special for its multilingualproperty. Particularly, six different languages (English, Japanese, Italian, Spanish, French, andGerman) are provided. It enables us to leverage recent advances such as language models to model different languages in user sessions. By utilizing this dataset, we can perform diverse downstream tasks for evaluating relevant algorithmsin recommendation and text generation. Here, we focus on three different tasks, consisting of (1)next-product recommendation, (2) next-product recommendation with domain shifts, and (3) next-product title generation. The first task is the classic session-based recommendation which requiresmodels to predict the ID of the next product, where the training dataset and test dataset are from thesame domain. The second task is similar to the first task but requires the models to pre-train onthe large dataset from large locales and transfer the knowledge to make predictions on downstreamdatasets from different domains (i.e., underrepresented locales). The third task is a novel taskproposed by us, which asks models to predict the title of the next product which has never beenshown in the training set. Based on these tasks, we benchmark representative baselines along withsimple heuristic methods. Our empirical observations suggest that the representative baselines fail tooutperform simple heuristic methods in certain evaluation metrics in these new settings. Therefore,we believe that Amazon-M2 can inspire novel solutions for session-based recommendation and enablenew opportunities for tasks that revolve around large language models and recommender systems.",
  "Dataset Description": "Before we elaborate on the details of the proposed Amazon-M2 dataset, we first introduce session-based recommendation. Given a user session, session-based recommendation aims to provide arecommendation on the product that the user might interact with at the next time step. As shown ina, each session is represented as a chronological list of products that the user has interactedwith. Specifically, we use S = {s1, s2, . . . , sn} to denote the session dataset containing n sessions,where each session is represented by s = {e1, e2, . . . , et} with et indicating the product interacted bythe user at time step t. In addition, let V = {v1, v2, , vm} denote a dictionary of unique productsthat appeared in the sessions, and each product is associated with some attributes. Designed for session-based recommendation, Amazon-M2 is a large-scale dataset composed ofcustomer shopping sessions with interacted products. Specifically, the dataset consists of twocomponents: (1) user sessions where each session is a list of product IDs interacted by the currentuser (a), and (2) a table of products with each row representing the attributes of one product(b). Particularly, the user sessions come from six different locales, i.e., the United Kingdom(UK), Japan (JP), German (DE), Spain (ES), Italian (IT), and France (FR). Given its multi-localenature, the dataset is also multilingual: the textual attributes (e.g., title and description) of the productsin the user sessions are in multiple languages, namely, English, Italian, French, Germany, and Spanish.Based on this dataset, we construct the training/test dataset for each task. A summary of our sessiondataset is given in . It includes the number of sessions, the number of interactions, the numberof products, and the average session length for six different locales. We can find that UK, DE, and JPhave approximately 10 times the number of sessions/products compared to ES, FR, and IT. Moredetails about the collection process of the dataset can be found in Appendix B. Comparison with Existing Datasets. We summarize the differences between existing sessiondatasets (especially from session-based recommendation) and Amazon-M2 in . First of all,Amazon-M2 is the first dataset to provide textural information while other datasets majorly focuson the product ID information or categorical attributes. Without comprehensive product attributes,the recommendation models may struggle to capture the nuanced preferences of customers. Second,existing datasets only provide sessions from a single locale (or country) which limits their userdiversity. Consequently, it may lead to biased or less accurate recommendations, as the models maynot capture the full range of customer preferences. By contrast, our proposed Amazon-M2 is collectedfrom multiple locales and is multilingual in nature. Third, our proposed Amazon-M2 provides a largenumber of user sessions and is on a much larger product scale, which can better reflect real-worldrecommendation scenarios with huge product bases.",
  "Task Description": "The primary goal of this dataset is to inspire new recommendation strategies and simultaneouslyidentify interesting products that can be used to improve the customer shopping experience. Weintroduce the following three different tasks using our shopping session dataset. (a) Task 1. Next-product recommendation. This task focuses on traditional session-based recom-mendation task, aiming to identify the next product of interest within a user session. Given a usersession, the goal of this task is to predict the next product that the user will interact with. Notethat the training/test data are from the same distribution of large locales (JP, UK, and DE).",
  "(b) Product attributes": ": An illustration of the proposed Amazon-M2 dataset. (a) A user session contains a list ofprevious products that the user has interacted with and the next product that the user is going tointeract with. The user sessions come from multiple locales such as UK, ES, JP, etc. It is import tonote that one product can appear in multiple locales. (b) The product attributes are publicly availableand can be accessed on Amazon.com. Users can find information about a specific product by itsASIN number. (b) Task 2. Next-product recommendation with domain shifts. This task is similar to Task 1 butadvocates a novel setting of transfer-learning: practitioners are required to perform pretrainingon a large pretraining dataset (user sessions from JP, UK, and DE) and then finetune and makepredictions on the downstream datasets of underrepresented locales (user sessions from ES,IT, and FR). Thus, there is a domain shift between the pretraining dataset and the downstreamdataset, which requires the model to transfer knowledge from large locales to facilitate therecommendation for underrepresented locales. This can address the challenge of data scarcityin these languages and improve the accuracy and relevance of the recommendations. Morediscussions on distribution shift can be found in . (c) Task 3. Next-product title generation. This is a brand-new task designed for session datasetswith textual attributes, and it aims to predict the title of the next product that the user willinteract with within the current session. Notably, the task is challenging as the products in thetest set do not appear in the training set. The generated titles can be used to improve cold-start recommendation and search functionality within the e-commerce platform. By accuratelypredicting the title of the next product, users can more easily find and discover items of interest,leading to improved user satisfaction and engagement. The tasks described above span across the fields of recommender systems, transfer learning, and natu-ral language processing. By providing a challenging dataset, this work can facilitate the developmentof these relevant areas and enable the evaluation of machine learning models in realistic scenarios.",
  "Dataset Analysis": "In this section, we offer a comprehensive analysis of the Amazon-M2 dataset to uncover valuableinsights. Our analysis covers several essential perspectives: long-tail phenomenon, product overlapbetween locales, session lengths, repeat pattern, and collaborative filtering pattern. Correspondingcodes can be found here. Long-tail phenomenon is a significant challenge in the session recommendation domain. Itrefers to the situation where only a handful of products enjoy high popularity, while the majority ofproducts receive only a limited number of interactions. To investigate the presence of the long-tailphenomenon in Amazon-M2 dataset, we analyze the distribution of product frequencies, as depicted",
  "(f) Distribution of 10th high-est SKNN score": ": Data analysis on Amazon-M2. (a)(c)(e) illustrate the long-tail phenomenon of productfrequency, session length distribution, and the number of repeat products in a single session. (b)shows product overlap ratio between locales. (d) illustrates the proportion of sessions with repeatpatterns in different locales. (f) shows most sessions can find relevant products of high SKNN scores. in a. The results clearly demonstrate the existence of a long-tail distribution, where the headof the distribution represents popular items and the tail consists of less popular ones. Furthermore,we observe that the long-tail phenomenon is also evident within each individual locale. For detailedexperimental results regarding this phenomenon in each locale, please refer to Appendix B.2. Thelong-tail distribution makes it difficult to effectively recommend less popular products, as a smallnumber of popular items dominate the recommendations. Product overlap ratio between locales is the proportion of the same products shared by differentlocales. A large number of overlapping products indicates a better transferability potential whentransferring the knowledge from one locale to the other. For example, cross-domain recommendationalgorithms like can then be successfully applied, which directly transfers the learned embeddingof the overlapping products from popular locales to the underrepresented locales. We then examineproduct overlap between locales in Amazon-M2 with the product overlap ratio. It is calculated as|NaNb|",
  "|Na|, where Na and Nb correspond to the products set of locale a and b, respectively. In b": "we use a heatmap to show the overlap ratio, where x and y axes stand for locale a and b, respectively.From the figure, we make several observations: (1) For the products in the three large locales, i.e., UK,DE, and JP, there are not many overlapping products, except the one between UK and DE locales; (2) Considering the product overlap ratio between large locales and underrepresented locales, i.e.,ES, FR, and IT, we can see a large product overlapping, indicating products in the underrepresenteddomain also appear in the large locales. Particularly, the overlap ratio between small locales and DEcan reach around 0.4. Thus, it has the potential to facilitate knowledge transfer from large locales andareas to underrepresented regions. Notably, despite the existence of overlapping products between different locales, there still remains alarge proportion of distinguished products in each locale, indicating the difficulty of transferabilitywith distribution shift. Moreover, the multilingual property, where the product textual descriptionfrom different locales is in different languages, also induces to distribution shift issue. Such amultilingual issue is a long-established topic in the NLP domain. For instance, point outmorphology disparity, tokenization differences, and negative transfer in the multilingual scenario,leading to distribution shift. Session length is an important factor in the session recommendation domain. Typically, a longersession length may lead to the interest shift challenge problem with difficulties in capturingmultiple user interests in one single session. Most existing algorithms show a betterperformance on the shorter sessions while failing down on the longer ones. As shown in c.We can observe that the session length also exhibits a long-tail distribution: most sessions are shortwhile only few sessions are with a length larger than 100. Repeat pattern is also an important user pattern, which refers to the phenomenonthat a user repeatedly engages the same products multiple times in a single session. The presence ofrepeat patterns in recommender systems can potentially result in the system offering more familiarproducts that match users existing preferences, which may lead to a less diverse and potentiallyless satisfying user experience. On the other hand, the repeat pattern is also an important propertyutilized in the graph-based session recommendation algorithms . Typically, thosegraph-based algorithms construct a session graph where each node represents a product and eachedge indicates two products interacted by the user consecutively. Complicated session graphs withdifferent structure patterns can be built when sessions exhibit evident repeat patterns. In d, wereport the proportion of sessions with repeat patterns for the six locales and we can observe that thereare around 35% sessions with repeat patterns across different locales. Furthermore, we examine thenumber of repeat products in those sessions with repeat patterns and report results on the distributionof repeated products in e. We make two observations: (1) the number of repeated productsvaries on different sessions; and (2) the number of repeated products in a session also follows thelong-tail distribution where most sessions only appear with a few repeated products. Collaborative filtering pattern. Collaborative filtering is a widely used technique that generatesrecommended products based on the behavior of other similar users. It is generally utilized asan important data argumentation technique to alleviate the data sparsity issue, especially for shortsessions . Since Amazon-M2 encompasses a much larger product set than existing datasets,we investigate whether collaborative filtering techniques can potentially operate in this challengingdata environment. Specifically, we utilize the session collaborative filtering algorithm, Session-KNN (SKNN) , to identify sessions that are similar to the target users current session. Thesimilarity score of SKNN can be calculated in the following steps. First, for a particular sessions, we first determines a set of its most similar sessions N(s) S with the cosine similaritysim(s, sj) = |s sj|/",
  "|s||sj|. Second, the score of a candidate product e in similar sessions N(s)is then calculated by: score(e, s) =": "nN (s) sim(s, n)In(e), where the indicator function In(e) is1 if n contains item e. Third, for each session s, we choose the 10th highest score(e, s) to indicatethe retrieval quality of candidate products. In f, we show the distribution of the 10th highestSKNN scores for all sessions. A notable observation is that the majority of sessions exhibit a highSKNN score, hovering around 1. This finding suggests that for most sessions, it is possible to retrieveat least 10 similar products to augment the session data.",
  "STAMP captures users current interest and general interests based on the last-click productand whole session, respectively": "SRGNN is the first to employ GNN layer to capture user interest in the current session. CORE ensures that sessions and items are in the same representation space via encoding thesession embedding as a linear combination of item embeddings. MGS incorporates product attribute information to construct a mirror graph, aiming to learnbetter preference understanding via combining session graph and mirror graph. Notably, MGS canonly adapt categorical attributes. Therefore, we discretize the price attribute as the input feature. In addition, we include a simple yet effective method, Popularity, by simply recommending all usersthe most popular products. We utilize Mean Reciprocal Rank@K (MRR@100) and Recall@100 toevaluate various recommendation algorithms. More results on NDCG@100 metric can be found inAppendix C. Corresponding codes can be found here. Results & Observations. The experiment results across different locales can be found in .We can observe that the popularity heuristic generally outperforms all the deep models with respectto both MRR and Recall. The only exception is that CORE achieves better performance on Recall.On one hand, the success of the popularity heuristic indicates that the product popularity is a strongbias for this dataset. On the other hand, it indicates that the large product set in Amazon-M2poses great challenges in developing effective recommendation algorithms. Thus, more advancedrecommendation strategies are needed for handling the challenging Amazon-M2 dataset.",
  "Task 2. Next-product Recommendation with Domain Shifts": "The purpose of Task 2 is to provide next-product recommendations for underrepresented locales,specifically ES, IT, and FR. To evaluate the baseline methods, we consider two training paradigms.(1) Supervised training, which involves directly training models on the training data on ES, IT, andFR and then testing them on their test data. The goal of this paradigm is to evaluate the modelsefficacy when no other data is available. (2) Pretraining & finetuning, which begins with pretrainingthe model using data from large locales, i.e., JP, UK, and DE, and then finetuning this pretrainedmodel on the training data from ES, IT, and FR. This approach tests the models ability to transferknowledge from one source to another. In both paradigms, we incorporate the same baseline modelsused in Task 1, which include GRU4REC++ NARM, STAMP, SRGNN, and CORE. Additionally, wecompare these methods with the popularity heuristic. Results & Observations. The results across different locales can be found in . From theresults, we have the following observations: (1) In the context of supervised training, the popularityheuristic outperforms all baselines in terms of MRR on underrepresented locales, which is consistentwith observations from Task 1. Interestingly, despite this, most deep learning models surpass thepopularity heuristic when considering Recall. This suggests a promising potential for deep learningmodels: they are more likely to provide the correct recommendations among the retrieved candidateset while the rankings are not high. (2) Finetuning most methods tends to enhance the performancein terms of both MRR and Recall compared to supervised training alone. This demonstrates thatthe knowledge of large locales can be transferred to underrepresented locales despite the existenceof domain shifts. Importantly, it highlights the potential of adopting pretraining to enhance theperformance on locales with limited data in the Amazon-M2 dataset. Ablation Study. Notably, the aforementioned methods utilized random initialization as the productembeddings. To enhance the models ability to transfer knowledge across different locales, we caninitialize product embeddings with embeddings from textual attributes. Specifically, we introduceSRGNNF and GRU4RecF to enable SRGNN and GRU4Rec++ to leverage the textual attributes (i.e.,product title), respectively. To obtain the text embedding, we opt for pre-trained Multilingual SentenceBERT to generate additional item embeddings from the item titles. As shown in , thisstraightforward approach has unintended effects in our ablation study. This could be due to the factthat the pre-training data for Multilingual Sentence BERT does not align with the text of product titles.Therefore, how to effectively utilize textual features remains an open question. We look forward tomore research concerning the integration of language models with session recommendation.",
  "Task 3. Next-product Title Generation": "In this task, the goal is to generate the title of the next product of interest for the user, which is a textgeneration task. Therefore, we adopt the bilingual evaluation understudy (BLEU) score, a classicmetric in natural language generation, as the evaluation metric. For the same reason, in this task, weexplore the effectiveness of directly applying language models as a baseline recommendation model.We fine-tune a well-known multilingual pre-trained language model, mT5 , using a generativeobjective defined on our dataset. Specifically, the language model receives the titles of the mostrecent K products and is trained to predict the title of the next product. We compare the performanceby varying K from 1 to 3, while also investigating the impact of parameter size. Additionally, weoffer a simple yet effective baseline approach, namely Last Product Title, in which the title for thenext product is predicted as that of the last product. We randomly select 10% of training data forvalidation, and report the BLEU scores of different methods on validation and test set in .",
  "mT5-small, K = 10.24990.2265mT5-small, K = 20.24010.2176mT5-small, K = 30.23660.2142mT5-base, K = 10.24770.2251Last Product Title0.25000.2677": "Results & Observations. The results in demonstratethat for the mT5 model, extending the session history length(K) does not contribute to a performance boost. Furthermore,the simple heuristic, Last Product Title, surpasses the perfor-mance of the finetuned language model. This observationhighlights two issues: (1) The last product bears significantimportance, as the users next interest is often highly corre-lated with the most recent product. This correlation becomesparticularly evident when the product title serves as the prediction target. For instance, the nextproduct title can have a large overlap with the previous product (e.g., same product type or brand). (2)The mT5 model did not function as effectively as expected, potentially due to a mismatch betweenthe pre-training and downstream texts. Thus, even an escalation in the models size (from mT5-smallto mT5-base) did not result in a performance gain. This suggests the necessity for a domain-specificlanguage model to leverage text information effectively. Moreover, we illustrate some examples ofgenerated results compared with the ground truth title in to intuitively indicate the qualityof the generated examples. We note that the generated titles look generally good, nonetheless, thegenerated ones still lack details, especially numbers,e.g., (180x200cm) in example 2. In addition,we anticipate that larger language models such as GPT-3/GPT-4 could achieve superiorperformance in this task, given their exposure to a more diverse corpus during pre-training. Wereserve these explorations for future work.",
  "Discussion": "In this section, we discuss the new opportunities that the proposed Amazon-M2 dataset can introduceto various research topics in academic study and industrial applications. Due to the space limit, weonly list five topics in this section while leaving others in Appendix B.4. Pre-Training & Transfer Learning. Our dataset comprises session data from six locales withdifferent languages, three of which have more data than the remaining three. This property presents aunique opportunity to investigate pre-training and transfer learning techniques for recommendationalgorithms , as demonstrated in Task 2. Due to the inherent sparsity of session data ,building accurate models that can capture users preferences and interests based on their limitedinteractions can be challenging, particularly when the number of sessions is small. One solutionto address the data sparsity problem is to transfer knowledge from other domains. Our dataset isadvantageous in this regard because it provides data from sources (locales/languages), which enablesthe transfer of user interactions from other locales, potentially alleviating the data sparsity issue. Large Language Models in Recommendation. The rich textual features in Amazon-M2 giveresearchers a chance for leveraging large language models (LLMs) in providingrecommendations. For example, there has been a growing emphasis on evaluating the capabilityof ChatGPT in the context of recommender systems . Theproposed Amazon-M2 presents an excellent opportunity for researchers to explore and experimentwith their ideas involving LLMs and recommendation. Text-to-Text Generation. Our dataset presents ample opportunities for exploring natural languageprocessing tasks, including text-to-text generation. In Task 3, we introduced a novel task that involvespredicting the title of the next product, which requires the recommender system to generate textualresults, rather than simply predicting a single product ID. This new task resembles to QuestionAnswering and allows us to leverage advanced text-to-text generation techniques , such as thepopular GPT models , which have demonstrated significant success in this area. Cross-Lingual Entity Alignment. Entity alignment is a task that aims to find equivalent entitiesacross different sources . Our dataset provides a good resource for evaluating variousentity alignment algorithms, as it contains entities (products) from different locales and languages.Specifically, the dataset can be used to test the performance of entity alignment algorithms incross-lingual settings, where the entities to be aligned are expressed in different languages. Graph Neural Networks. As powerful learning tools for graph data, graph neural networks(GNNs) have tremendously facilitated a wide range of graph-related applications includ-ing recommendation , computation biology , and drug discovery . Our datasetprovides rich user-item interaction data which can be naturally represented as graph-structureddata. Thus, it is a good tool for evaluating various GNNs and rethinking their development in thescenarios of recommendation and transfer learning. In addition, the abundant textual and structuraldata in Amazon-M2 provides a valuable testing platform for the recent surge of evaluating LLMs forgraph-related tasks . Item Cold-Start Problem. The item cold-start problem is a well-known challenge inrecommender systems, arising when a new item is introduced into the system, and there is insufficientdata available to provide accurate recommendations. However, our dataset provides rich itemsattributes including detailed textual descriptions, which offers the potential to obtain excellentsemantic embeddings for newly added items, even in the absence of user interactions. This allowsfor the development of a more effective recommender system that places greater emphasis onthe semantic information of the items, rather than solely relying on the users past interactions.Therefore, by leveraging this dataset, we can overcome the cold-start problem and deliver betterdiverse recommendations, enhancing the user experience. Data Imputation. Research on deep learning requires large amounts of complete data, but obtainingsuch data is almost impossible in the real world due to various reasons such as damages to devices,data collection failures, and lost records. Data imputation is a technique used to fill in missingvalues in the data, which is crucial for data analysis and model development. Our dataset providesample opportunities for data imputation, as it contains entities with various attributes. By exploringdifferent imputation methods and evaluating their performance on our dataset, we can identify themost effective approach for our specific needs.",
  "Conclusion": "This paper presents the Amazon-M2 dataset, which aims to facilitate the development of recom-mendation strategies that can capture language and location differences. Amazon-M2 provides richsemantic attributes including textual features, encompasses a large number of sessions and products,and covers multiple locales and languages. These qualities grant machine learning researchers andpractitioners considerable flexibility to explore the data comprehensively and evaluate their modelsthoroughly. In this paper, we introduce the details of the dataset and provide a systematic analysisof its properties. Furthermore, through empirical studies, we highlight the limitations of existingsession-based recommendation algorithms and emphasize the immense potential for developing newalgorithms with Amazon-M2. We firmly believe that this novel dataset provides unique contributionsto the pertinent fields of recommender systems, transfer learning, and large language models. Thedetailed discussion on broader impact and limitation can be found in Appendix D.",
  "Paul Resnick and Hal R Varian. Recommender systems. Communications of the ACM, 40(3):5658, 1997": "Wayne Xin Zhao, Shanlei Mu, Yupeng Hou, Zihan Lin, Yushuo Chen, Xingyu Pan, KaiyuanLi, Yujie Lu, Hui Wang, Changxin Tian, Yingqian Min, Zhichao Feng, Xinyan Fan, Xu Chen,Pengfei Wang, Wendi Ji, Yaliang Li, Xiaoling Wang, and Ji-Rong Wen. Recbole: Towardsa unified, comprehensive and efficient framework for recommendation algorithms. In CIKM,pages 46534664. ACM, 2021. Wenqi Fan, Xiaorui Liu, Wei Jin, Xiangyu Zhao, Jiliang Tang, and Qing Li. Graph trendfiltering networks for recommendation. In Proceedings of the 45th International ACM SIGIRConference on Research and Development in Information Retrieval, pages 112121, 2022.",
  "Paul Covington, Jay Adams, and Emre Sargin. Deep neural networks for youtube recommenda-tions. In Proceedings of the 10th ACM conference on recommender systems, pages 191198,2016": "Wayne Xin Zhao, Sui Li, Yulan He, Liwei Wang, Ji-Rong Wen, and Xiaoming Li. Explor-ing demographic information in social media for product recommendation. Knowledge andInformation Systems, 49:6189, 2016. Xin Wayne Zhao, Yanwei Guo, Yulan He, Han Jiang, Yuexin Wu, and Xiaoming Li. We knowwhat you want to buy: a demographic-based system for product recommendation on microblogs.In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discoveryand data mining, pages 19351944, 2014.",
  "Balzs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. Session-basedrecommendations with recurrent neural networks. arXiv preprint arXiv:1511.06939, 2015": "Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, and Tieniu Tan. Session-basedrecommendation with graph neural networks. In Proceedings of the AAAI conference onartificial intelligence, volume 33, pages 346353, 2019. Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, ZhongyuanWang, and Ji-Rong Wen. S3-rec: Self-supervised learning for sequential recommendation withmutual information maximization. In Proceedings of the 29th ACM international conference oninformation & knowledge management, pages 18931902, 2020. Jing Li, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Tao Lian, and Jun Ma. Neural attentivesession-based recommendation. In Proceedings of the 2017 ACM on Conference on Informationand Knowledge Management, pages 14191428, 2017. Qiao Liu, Yifu Zeng, Refuoe Mokhosi, and Haibin Zhang. Stamp: short-term attention/memorypriority model for session-based recommendation. In Proceedings of the 24th ACM SIGKDDinternational conference on knowledge discovery & data mining, pages 18311839, 2018. Feng Yu, Yanqiao Zhu, Qiang Liu, Shu Wu, Liang Wang, and Tieniu Tan. Tagnn: Targetattentive graph neural networks for session-based recommendation. In Proceedings of the 43rdinternational ACM SIGIR conference on research and development in information retrieval,pages 19211924, 2020. David Ben-Shimon, Alexander Tsikinovsky, Michael Friedmann, Bracha Shapira, Lior Rokach,and Johannes Hoerle. Recsys challenge 2015 and the yoochoose dataset. In Proceedings of the9th ACM Conference on Recommender Systems, pages 357358, 2015.",
  "Tianchi. Ijcai-15 repeat buyers prediction dataset, 2018": "Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, BingYin, and Xia Hu. Harnessing the power of llms in practice: A survey on chatgpt and beyond.arXiv preprint arXiv:2304.13712, 2023. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min,Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXivpreprint arXiv:2303.18223, 2023. Ce Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing Wang, Kai Zhang, Cheng Ji, QibenYan, Lifang He, et al. A comprehensive survey on pretrained foundation models: A historyfrom bert to chatgpt. arXiv preprint arXiv:2302.09419, 2023.",
  "Siyi Liu and Yujia Zheng. Long-tail session-based recommendation. In Proceedings of the 14thACM Conference on Recommender Systems, pages 509514, 2020": "Chen Gao, Xiangning Chen, Fuli Feng, Kai Zhao, Xiangnan He, Yong Li, and Depeng Jin.Cross-domain recommendation without sharing user-relevant data. In The world wide webconference, pages 491502, 2019. Antoine Nzeyimana and Andre Niyongabo Rubungo. Kinyabert: a morphology-aware kin-yarwanda language model. In Proceedings of the 60th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers), pages 53475363, 2022. Phillip Rust, Jonas Pfeiffer, Ivan Vulic, Sebastian Ruder, and Iryna Gurevych. How good is yourtokenizer? on the monolingual performance of multilingual language models. In Proceedingsof the 59th Annual Meeting of the Association for Computational Linguistics and the 11thInternational Joint Conference on Natural Language Processing (Volume 1: Long Papers),pages 31183135, 2021. Yu-Hsiang Lin, Chian-Yu Chen, Jean Lee, Zirui Li, Yuyan Zhang, Mengzhou Xia, ShrutiRijhwani, Junxian He, Zhisong Zhang, Xuezhe Ma, et al. Choosing transfer languages forcross-lingual learning. In Proceedings of the 57th Annual Meeting of the Association forComputational Linguistics, pages 31253135, 2019.",
  "Yujia Zheng, Siyi Liu, Zekun Li, and Shu Wu. Cold-start sequential recommendation via metalearner. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages47064713, 2021": "Meirui Wang, Pengjie Ren, Lei Mei, Zhumin Chen, Jun Ma, and Maarten De Rijke.Acollaborative session-based recommendation approach with parallel memory modules. InProceedings of the 42nd international ACM SIGIR conference on research and development ininformation retrieval, pages 345354, 2019. Balzs Hidasi and Alexandros Karatzoglou. Recurrent neural networks with top-k gains forsession-based recommendations. In Proceedings of the 27th ACM international conference oninformation and knowledge management, pages 843852, 2018. Yupeng Hou, Binbin Hu, Zhiqiang Zhang, and Wayne Xin Zhao. Core: simple and effectivesession-based recommendation within consistent representation space. In Proceedings of the45th international ACM SIGIR conference on research and development in information retrieval,pages 17961801, 2022. Siqi Lai, Erli Meng, Fan Zhang, Chenliang Li, Bin Wang, and Aixin Sun. An attribute-driven mirror graph network for session-based recommendation. In Proceedings of the 45thInternational ACM SIGIR Conference on Research and Development in Information Retrieval,pages 16741683, 2022. Nils Reimers and Iryna Gurevych. Making monolingual sentence embeddings multilingualusing knowledge distillation. In Proceedings of the 2020 Conference on Empirical Methods inNatural Language Processing. Association for Computational Linguistics, 11 2020. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant,Aditya Barua, and Colin Raffel. mt5: A massively multilingual pre-trained text-to-text trans-former. arXiv preprint arXiv:2010.11934, 2020. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models tofollow instructions with human feedback. Advances in Neural Information Processing Systems,35:2773027744, 2022.",
  "Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving languageunderstanding by generative pre-training. 2018": "Yupeng Hou, Shanlei Mu, Wayne Xin Zhao, Yaliang Li, Bolin Ding, and Ji-Rong Wen. Towardsuniversal sequence representation learning for recommender systems. In Proceedings of the28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 585593,2022. Chenglin Li, Mingjun Zhao, Huanming Zhang, Chenyun Yu, Lei Cheng, Guoqiang Shu, BeibeiKong, and Di Niu. Recguru: Adversarial learning of generalized user representations forcross-domain recommendation. In Proceedings of the fifteenth ACM international conferenceon web search and data mining, pages 571581, 2022.",
  "OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023": "Wenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Jiliang Tang, andQing Li. Recommender systems in the era of large language models (llms). arXiv preprintarXiv:2307.02046, 2023. Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley, andWayne Xin Zhao. Large language models are zero-shot rankers for recommender systems.arXiv preprint arXiv:2305.08845, 2023. Xiaolei Wang, Xinyu Tang, Wayne Xin Zhao, Jingyuan Wang, and Ji-Rong Wen. Rethinkingthe evaluation for conversational recommendation in the era of large language models. arXivpreprint arXiv:2305.13112, 2023.",
  "Wenjie Wang, Xinyu Lin, Fuli Feng, Xiangnan He, and Tat-Seng Chua. Generative recommen-dation: Towards next-generation recommender paradigm. arXiv preprint arXiv:2304.03516,2023": "Jinming Li, Wentao Zhang, Tian Wang, Guanglei Xiong, Alan Lu, and Gerard Medioni. Gpt4rec:A generative framework for personalized recommendation and user interests interpretation.arXiv preprint arXiv:2304.03879, 2023. Junjie Zhang, Ruobing Xie, Yupeng Hou, Wayne Xin Zhao, Leyu Lin, and Ji-Rong Wen.Recommendation as instruction following: A large language model empowered recommendationapproach. arXiv preprint arXiv:2305.07001, 2023. Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. Tallrec: Aneffective and efficient tuning framework to align large language model with recommendation.arXiv preprint arXiv:2305.00447, 2023. Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen, Chuan Qin,Chen Zhu, Hengshu Zhu, Qi Liu, et al. A survey on large language models for recommendation.arXiv preprint arXiv:2305.19860, 2023.",
  "Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and YoshuaBengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017": "Hongzhi Wen, Jiayuan Ding, Wei Jin, Yiqi Wang, Yuying Xie, and Jiliang Tang. Graph neuralnetworks for multimodal single-cell data integration. In Proceedings of the 28th ACM SIGKDDConference on Knowledge Discovery and Data Mining, pages 41534163, 2022. Jiayuan Ding, Hongzhi Wen, Wenzhuo Tang, Renming Liu, Zhaoheng Li, Julian Venegas,Runze Su, Dylan Molho, Wei Jin, Wangyang Zuo, et al. Dance: A deep learning library andbenchmark for single-cell analysis. bioRxiv, pages 202210, 2022. David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel,Aln Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learningmolecular fingerprints. Advances in neural information processing systems, 28, 2015. Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang Wang,Dawei Yin, Wenqi Fan, Hui Liu, et al. Exploring the potential of large language models (llms)in learning on graphs. arXiv preprint arXiv:2307.03393, 2023.",
  "Jiawei Zhang. Graph-toolformer: To empower llms with graph reasoning ability via promptaugmented by chatgpt. arXiv preprint arXiv:2304.11116, 2023": "Hoyeop Lee, Jinbae Im, Seongwon Jang, Hyunsouk Cho, and Sehee Chung. Melu: Meta-learneduser preference estimator for cold-start recommendation. In Proceedings of the 25th ACMSIGKDD International Conference on Knowledge Discovery & Data Mining, pages 10731082,2019. Stef Van Buuren. Flexible imputation of missing data. CRC press, 2018. Wayne Xin Zhao, Shanlei Mu, Yupeng Hou, Zihan Lin, Yushuo Chen, Xingyu Pan, KaiyuanLi, Yujie Lu, Hui Wang, Changxin Tian, et al. Recbole: Towards a unified, comprehensiveand efficient framework for recommendation algorithms. In proceedings of the 30th acminternational conference on information & knowledge management, pages 46534664, 2021.",
  "AExperimental Setup": "Data Splits. Following the setting in our KDDCUP competition3, the Amazon-M2 dataset containsthree splits: training, phase-1 test and phase-2 test. For the purpose of model training and selection,we further split the original training set into 90% sessions for development (used for training), and10% sessions for validation. Note that the numbers in Tables 3, 4, and 5 of the main content arefor validation performance. Without specific mention, the test set mentioned in the main contentindicates the phase-1 test. Due to the page limitation of main content, we defer the performances onthe phase-2 test set to the appendix. Hyperparameter Settings. The hyper-parameters of all the models are tuned based on the perfor-mance of the validation set. For Task 1 and Task 2, we follow the suggested hyper-parameter rangeto search for the optimal settings provided by Recbole toolkits . By default, we only use theproduct ID to train the models since most of the popular session-based recommendation baselinesare ID-based methods. We leave the exploration of other rich attributes such as price, brand, anddescription as future work. Specifically, the search ranges for different models are outlined below:",
  "GRU4RECF: learning_rate: [0.01,0.001,0.0001], num_layers: . SRGNNF: learning_rate: [0.01,0.001,0.0001], step: . MGS: learning_rate: [0.01,0.001,0.0001]": "For Task 3, we adopted the code example provided by HuggingFace4 which supports text-to-textgeneration. Moreover, we tune the following hyperparameters in mT5: weight_decay in the range of{0, 1e-8}, learning_rate in the range of {2e-5, 2e-4}, num_beams in the range of {1, 5}. Additionally,we set the training batch size to 12, and the number of training epochs to 10.",
  "Hardware and Software Configurations. We perform experiments on one server with 8 NVIDIARTX A6000 (48 GB) and 128 AMD EPYC 7513 32-Core Processor @ 3.4 GHZ. The operatingsystem is Ubuntu 20.04.1": "Metric details:Mean Reciprocal Rank(MRR)@K is a metric used in information retrieval andrecommendation systems to measure the effectiveness of a model in providing relevant results. MRRis computed with the following two steps: (1) calculate the reciprocal rank. The reciprocal rank is theinverse of the position at which the first relevant item appears in the list of recommendations. If norelevant item is found in the list, the reciprocal rank is considered 0. (2) average of the reciprocalranks of the first relevant item for each session.",
  "where Rank(t) is the rank of the ground truth on the top K result ranking list of test session t, and ifthere is no ground truth on the top K ranking list, then we would set1": "Rank(t) = 0. MRR values rangefrom 0 to 1, with higher values indicating better performance. A perfect MRR score of 1 means thatthe model always places the first relevant item at the top of the recommendation list. An MRR scoreof 0 implies that no relevant items were found in the list of recommendations for any of the queries orusers.",
  "where N denotes the number of test sessions. nhit is the number of test sessions with the next productin the top K of the ranked list": "Normalized Discounted Cumulative Gain(NDCG@K) quantifies the effectiveness of ranked listsby considering both the relevance of items and their positions within the list up to a specified depth K.This metric facilitates fair comparisons of ranking algorithms across different datasets and scenarios,as it normalizes the Discounted Cumulative Gain (DCG) score to a standardized range, typicallybetween 0 and 1.",
  "B.1Dataset Collection": "The Amazon-M2 dataset is a collection of anonymous user session data and product data from theAmazon platform. Each session represents a list of products that a user interacted with during a30-minute active window. Note that the product list in each session is arranged in chronologicalorder, with each product represented by its ASIN number. Users can search for Amazon productsusing their ASIN numbers5 and obtain the corresponding product attributes. We include the followingproduct attributes:",
  "Locale: Locale refers to the specific geographical or regional settings and preferences that determinehow information is presented to users on Amazons platform": "Title: The Title attribute represents the name or title given to a product, book, or creative work. Itprovides a concise and identifiable name that customers can use to search for or refer to the item. Brand: The Brand attribute represents the manufacturer or company that produces the product. Itprovides information about the brand reputation and can influence a customers purchasing decisionbased on brand loyalty or recognition.",
  "Color Text: Color Text describes the color or color variation of the product. It provides informationabout the products appearance and helps customers choose items that match their color preferences": "Author: Author refers to the individual or individuals who have written a book or authored writtencontent. It helps customers identify the creator of the work and plays a significant role in bookpurchasing decisions. Bullet Description (desc): Bullet Description is a concise and brief description of the products keyfeatures, benefits, or selling points. It highlights the most important information about the item in aclear and easily scannable format. The dataset spans a period of 3 weeks, with the first 2 weeks designated as the training set and theremaining week as the test set. To enhance evaluation, the test set is further randomly divided intotwo equal subsets, i.e., phase-1 test and phase-2 test.",
  "B.4Extended Discussion": "Item Cold-Start Problem. The item cold-start problem is a well-known challenge inrecommender systems, arising when a new item is introduced into the system, and there is insufficientdata available to provide accurate recommendations. However, our dataset provides rich itemsattributes including detailed textual descriptions, which offers the potential to obtain excellentsemantic embeddings for newly added items, even in the absence of user interactions. This allowsfor the development of a more effective recommender system that places greater emphasis onthe semantic information of the items, rather than solely relying on the users past interactions.Therefore, by leveraging this dataset, we can overcome the cold-start problem and deliver betterdiverse recommendations, enhancing the user experience. Data Imputation. Research on deep learning requires large amounts of complete data, but obtainingsuch data is almost impossible in the real world due to various reasons such as damages to devices,data collection failures, and lost records. Data imputation is a technique used to fill in missingvalues in the data, which is crucial for data analysis and model development. Our dataset providesample opportunities for data imputation, as it contains entities with various attributes. By exploringdifferent imputation methods and evaluating their performance on our dataset, we can identify themost effective approach for our specific needs. 0510 15 20 25 30 35 #Repeated products #Session",
  "(f) UK": ": The number of repeat items w.r.t. locales where the x-axis corresponds to the number ofrepeat items, the y-axis indicates to the number of session with the corresponding number of repeatitems. Notably, we exclude those sessions with no repeat patterns. A clear long-tail phenomenon canbe found, where only a few sessions show many repeat items.",
  "C.2Task 1. Next-product Recommendation": "In this subsection, we provide the model performance comparison on the phase-1 test and phase-2test in and , respectively. We can have similar observations as we made in .1:the popularity heuristic generally outperforms the deep models with respect to both MRR and Recall,with the only exception that CORE achieves better performance on Recall. This suggests thatthe popularity heuristic is a strong baseline and the challenging Amazon-M2 dataset requires newrecommendation strategies to handle. We believe that it is potentially helpful to design strategies thatcan effectively utilize the available product attributes.",
  "C.3Task 2. Next-product Recommendation with Domain Shifts": "We report the mode performances on the phase-1 test and phase-2 test in and ,respectively. Note that we omit the supervised training results since we have already identifiedthat finetuning can significantly improve it. From the tables, we arrive at a similar observation aspresented in .2 that the finetuned deep models generally outperform the popularity heuristicin Recall but underperform it in MRR. This illustrates that the deep models have the capability toretrieve a substantial number of pertinent products, but they fall short in appropriately ranking them.As a result, there is a need to enhance these deep models further in order to optimize their rankingefficacy.",
  "C.4Task 3. Next-product Title Prediction": "We expand to include the results on phase-2 test and the full results are shown in .From the table, we have the same observations as we made in .3: (1) Extending the sessionhistory length (K) does not contribute to a performance boost, and (2) The simple heuristic of LastProduct Title outperforms all other baselines. It calls for tailored designs of language models for thischallenging task.",
  "DLimitation & Broader Impact": "The release of the Amazon-M2 dataset brings several potential broader impacts and research op-portunities in the field of session-based recommendation and language modeling. It provides thepotential for research in the session recommendation domain to access the rich semantic attributesand knowledge from multiple locales, enabling better recommendation systems for diverse userpopulations. While the Amazon-M2 dataset offers significant research potential, it is crucial to consider the certainlimitations associated with its use. Despite efforts that have been made to include diverse userbehaviors and preferences with multiple locales and languages, it may not capture the full linguisticand cultural diversity of all regions. Moreover, the dataset can be only collected within the Amazonplatform, which may not fully capture the diversity of user behaviors in other domains or platforms,leading to a potentially biased conclusion and may not hold true in different contexts. We also carefully consider the broader impact from various perspectives such as fairness, security, andharm to people. There may be one potential fairness issue, Amazon-2M may inadvertently reinforcebiases if specific demographics are disproportionately represented or underrepresented in differentregions. Such issue may happen as we promote diversity with sessions from more locales. Whilethere might be fairness concerns, we assert that the Amazon-M2 dataset underscores the need forprogress in fair recommendation development and serves as an effective platform for assessing fairrecommendation approaches."
}