{
  "ABSTRACT": "Prompting methods have shown impressive performance in a vari-ety of text mining tasks and applications, especially few-shot ones.Despite the promising prospects, the performance of promptingmodel largely depends on the design of prompt template and ver-balizer. In this work, we propose MetricPrompt, which eases verbal-izer design difficulty by reformulating few-shot text classificationtask into text pair relevance estimation task. MetricPrompt adoptsprompting model as the relevance metric, further bridging the gapbetween Pre-trained Language Models (PLM) pre-training objec-tive and text classification task, making possible PLMs smoothadaption. Taking a training sample and a query one simultane-ously, MetricPrompt captures cross-sample relevance informationfor accurate relevance estimation. We conduct experiments on threewidely used text classification datasets across four few-shot settings.Results show that MetricPrompt outperforms manual verbalizerand other automatic verbalizer design methods across all few-shotsettings, achieving new state-of-the-art (SOTA) performance.",
  "Corresponding author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 23, August 0610, 2023, Long Beach, CA 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0103-0/23/08...$15.00",
  "INTRODUCTION": "Since unstructured text data takes up over 80% information inour society, text mining is believed to have significant commer-cial value . Text classification is regarded as a fundamentaland essential task in text mining, and the related techniques areused in various kinds of text mining applications, such as informa-tion retrieval , sentiment analysis , recommendationsystem , knowledge management , document summariza-tion , etc. Recently proposed pre-trained language models (PLMs)achieve satisfactory text classification performance under data-richsetting , but these models Few-Shot Learning (FSL)ability still lags far behind human intelligence .Prompting methods are proposed to better utilize PLMs generalknowledge by aligning downstream tasks to its pre-training objec-tive. Prompting method inserts sample text into prompt template toform prompted text. Prompting model takes as input the promptedtext, and the result is obtained by projecting the models outputwords to corresponding labels. Label mapping is conducted by ver-balizer, which serves as a critical part of the prompting model anddetermines its performance.Although achieving promising results in a wide variety of textmining tasks, prompting methods are very susceptible to sub-optimal",
  ": A comparison between vanilla fine-tuning and prompt tuning with MLM for text classification": "verbalizer design . However, designing a proper verbalizer re-quires deep understanding of both downstream task and PLMsinner mechanism. In practice, this procedure can be extremely timeand resource-consuming. To this end, automatic verbalizer designmethods are proposed to ease the difficulty. These algorithms canbe classified into discrete verbalizer design and soft verbalizer de-sign methods. Discrete verbalizer design methods, such asAVS , LM-BFF and AutoPrompt , search each labelscorresponding answer words in PLMs vocabulary to build theverbalizer. Soft verbalizer design methods like WARP and Pro-toVerb search for proper verbalizer parameters in an infinitecontinuous space and thus achieve better performance.Despite the promising prospects of soft verbalizer, current meth-ods performance is still far from satisfactory. The main reasonis that these methods optimization and inference formulationsare distinct from PLMs pre-training objective. As illustrated inFig 1, WARP and ProtoVerb introduce task-specific labelembeddings and use PLMs inner representation to make predic-tions. Although adopting prompting method, both methods forcePLM to adapt to a distinct task formulation from its pre-trainingobjective which only operates on output word probabilities. Whatsworse, these label embeddings have to be trained from scratch indownstream tasks, leading to severe over-fitting problem. As aresult, PLM cannot adapt to downstream tasks smoothly.To tackle the above issues, we propose MetricPrompt, whichfrees human labor from task-specific verbalizer design by refor-mulating few-shot text classification task into text pair relevanceestimation task. We re-organize training data into text pairs, andadopt prompting model to learn the relevance metric. The learnedmetric is then used to evaluate each query samples relevance withtraining samples, and the classification result is obtained by pool-ing the estimated relevance scores. As shown in Fig 1, an explicittask-specific verbalizer is no longer required in our method. Fol-lowing PLMs pre-training objective, MetricPrompt only operateswith PLMs output word probabilities, and thus enabling smoothadaption to downstream tasks. To produce accurate relevance esti-mations, MetricPrompt takes text pairs as input and aid estimationaccuracy with cross-sample relevance information. Experiments on three widely used few-shot text classification datasets across fourfew-shot settings indicate that MetricPrompt achieves the highestfew-shot text classification accuracy over previous SOTA verbalizerdesign baselines.We summarize the contribution of this paper as below:(1) We propose a novel prompting method MetricPrompt, whicheases task-specific verbalizer design difficulty by reformulatingfew-shot classification task into relevance estimation problem andlearning the relevance metric with prompting model.(2) We conduct experiments on three widely used few-shot textclassification datasets with four few-shot settings, and results showthat MetricPrompt outperforms all automatic verbalizer baselinesand even manual verbalizer which requires heavy human labor intask-specific verbalizer design.(3) We provide analysis to demonstrate the extensibility androbustness of MetricPrompt, and explain its performance variancewhen equipped with different pooling methods.All code and data will be publicly available at",
  "PRELIMINARIES AND RELATED WORK2.1Prompting methods": "Traditionally, PLM is adapted to downstream tasks via vanilla fine-tuning. As shown in Fig 2, fine-tuning pools PLMs last layer hid-den states to form a sentence representation, and makes predic-tions with a task-specific classification head. Fine-tuning is effectivegiven sufficient training data, but its performance degrades signifi-cantly under few-shot scenario. The newly initialized task-specifichead is prone to over-fitting problem, and the gap between down-stream task formulation and the pre-training objective hindersPLMs smooth adaption.Prompting methods are proposed to reformulate downstreamtasks to enable PLMs smooth adaption to new tasks. A promptingmodels pipeline is illustrated in Fig 2. Denoting () as the prompt-ing function which fills the input text x into a prompt template,prompting model takes the prompted text and produces output",
  "((x);) = ( ((x);)).(2)": "First proposed by Radford et al., prompting methods have beenused in a variety of text mining tasks, such as text classification , text generation , named entity recognition , knowledge probing , etc. Prompting model alleviatesthe over-fitting problem under few-shot scenario significantly . However, prompting models performance relies largelyon the selection of prompt template and verbalizer . Althougha number of works focus on the design of prompt ,less are proposed to ease verbalizer design difficulty. Verbalizermaps prompting models output words to classification results di-rectly, and therefore influences prompting models performancesignificantly . In this work, we seek to free human labor fromverbalizer design with less performance loss.",
  "Verbalizer Design Methods": "Current verbalizer design methods can be classified into manual ver-balizer design, discrete verbalizer design and soft verbalizer design.A carefully hand-crafted verbalizer can achieve highly competitiveperformance in various text mining tasks . However, design-ing such a well-performing verbalizer relies heavily on humansaccurate understanding of the target task and requires heavy trial-and-error work. Discrete verbalizer design methods frees humanlabor from tedious and time-consuming verbalizer design process.LM-BFF generates proper answer words with large PLMs suchas T5 . AutoPrompt and AVS initialize a verbalizerand optimize it to meet a predefined criteria iteratively. PETAL searches for answer words that maximize the likelihood of the train-ing data. These methods reduce human labor required by verbalizerdesign significantly, but their performance lags far behind carefullyhand-crafted ones . To achieve better performance, soft verbal-izer design methods render answer words from a fixed vocabularyas differentiable label embeddings represented in an infinite contin-uous space. Expanding the possibilities of verbalizer design space, these methods achieve better results than discrete verbalizer designalgorithms .In this work, we ease task-specific verbalizer design difficultyby reformulating text classification task into text pair relevanceestimation task. In this way, no additional task-specific parameteris introduced, and the over-fitting problem is alleviated. Adoptingprompting model as the relevance metric, PLM can adapt to newtasks more smoothly. A recent work also views text classificationtask as natural language inference problem , but it focuseson zero-shot scenario and hand-craft each labels description. Incomparison, our method tackle few-shot text classification tasksinstead of zero-shot ones, and does not require human labor intask-specific verbalizer design.",
  "(, )D D{((x, x ), y)},(3)": "where (, ) is MetricPromts prompting function. It takes twopieces of sample text and produces a filled prompt template withthe given text. We use x [SEP] A news of [MASK] topic: x \" asthe prompt template. y = I(y = y ) indicates whether the pairof text are of the same class.Similarly, we build query data for MetricPrompt as follows:",
  "Optimization": "MetricPrompt differs from previous soft verbalizer design methodsfor we do not introduce task specific label embeddings, but insteadreformulate few-shot text classification task as a text pair relevanceestimation task to let loose the need of task-specific verbalizer. Byreformulating the task, MetricPrompts optimization coincides withPLMs pre-training objectives.Let (;) be an MLM model parameterized by and (;)be its output word probability over the vocabulary at [MASK] po-sition. We define the optimization objective of MetricPrompt asfollows:",
  "(5)": "where () stands for a probability distribution over label categories.The corresponding position of the input samples label is set as1 while others are set to be 0. () represents a predefined task-general meta verbalizer, which projects output word probability (;) to a binomial distribution (;). We task this metaverbalizer to aggregate logits at {relevant, similar, consistent}to be the predicted logit of label 1, while logits at {irrelevant, in-consistent, different} are aggregated to be the logit of label 0.1 L is the loss function of MetricPrompt, which is defined as thecross-entropy loss between the probability distributions producedby the meta verbalizer () and the ground truth distribution.MetricPrompt formulates few-shot text classification tasks op-timization objective to be a generalized MLM task, which is the",
  "Inference": "After optimization, the prompting model serves as a relevance met-ric during inference. As illustrated in Fig 4, we take an originalquery sample colored in black and pair it with all training sam-ples colored differently to form inference samples. Given an originaltraining sample , MetricPrompt computes its relevance score with as follows:",
  "MetricPrompt: Prompting Model as a Relevance Metric for Few-shot Text ClassificationKDD 23, August 0610, 2023, Long Beach, CA": "We set as half the size of training set. When several labels appearin D for the same number of times, we select the sample obtain-ing the highest relevance score from these labels correspondingsamples, and classify to its class.MetricPrompt renders prompting model as a relevance metric,and classify query samples according to its relevance with trainingsamples of the few-shot task. Prompting model takes two pieces ofsample text at once. Thus, MetricPrompt is able to use cross-sampleinformation to estimate relevance and make predictions accurately.",
  "More Efficient Inference": "The cost of the inference procedure is relatively high because of thepairing strategy. To further improve the efficiency of MetricPrompt,we propose to use pivot samples to reduce the time complexity ofthe inference stage of MetricPrompt.We propose to use the optimized prompting model to calculatethe representativeness of each training sample. For a training sam-ple labeled with , we use to denote its representativeness,which is calculated as follows:",
  "|{ | D D }| .(11)": "represents the relevance score between samples and. Basedon this representativeness metric, we select the top samples withthe highest representativeness scores from the training samplescorresponding to each label. These samples are marked as pivotsamples. During inference, we only pair each test sample with eachlabels pivot samples and compute their relevance scores to makeclassification prediction.Pivot samples reduce the time complexity of MetricPromptsinference process significantly. Assume a few-shot text classifica-tion task with labels and samples per label. Without introduc-ing pivot samples, each test sample needs to be paired with training samples and compute relevance scores, resulting into atime complexity of ( ). In contrast, prompting methods withhuman-designed or automatically designed verbalizer calculate thedot product similarity between the feature representations of testsamples extracted by the pre-trained model and the feature rep-resentations of each label. Since the total number of labels is ,the time complexity of these methods is only (). By introduc-ing pivot samples to improve the inference process, MetricPromptonly needs to estimate the relevance between each test sample andthe pivot samples of each label, reducing the time complexity to( ). Because is a pre-defined constant, the time complexityof MetricPrompts inference process accelerated with pivot samplesis (), which is consistent with other commonly used promptingmethods. We set the number of pivot samples per label to 2 inthe experiments.",
  "Datasets": "We conduct experiments with three widely used text classificationtasks, which contain numerous classes and hence require extensiveresources to build a proper manual verbalizer. We adopt AGs News,Yahoo Answers topics and DBPedia as our text classifica-tion tasks. The statistics of the datasets are given in .Since the average text length of the three datasets is short, wetruncate all sample text to 120 tokens for better efficiency with littlesemantic meaning loss.",
  "Few-shot experiment settings": "We conduct experiments under 2, 4, 8 and 16-shot settings, wherecorresponding number of training samples are sampled from eachdatasets training set randomly. Models performance are observedto fluctuate largely when the training set is sampled with differentrandom seeds. Hence, we sample 10 training sets for each datasetand each few-shot setting to alleviate the influence of randomnessin training set selection. All experiment results are reported as theaverage value of the models performance on the 10 training sets.",
  "We implement MetricPrompt with PyTorch and Hugging-face framework, and baselines are implemented with Open-Prompt toolkit . We introduce Kernl to accelerate the inferenceprocedure.2": "We adopt BERT-base-uncased as the backbone model for bothMetricPrompt and all baseline models for fair comparison. Modelparameters are optimized with AdamW optimizer , and thelearning rate is set as 1e-5. We set total training steps proportionallyto the size of training set, and the number of training epochs isadjusted accordingly. The size of training set varies across datasetsand shot numbers, and the specific number of training epochs isgiven in .",
  ": Prompt templates and task-specific verbalizers used by MetricPrompt and other baselines. - means no task-specificverbalizer is required": "Manual Verbalizer (ManualVerb) uses hand-crafted verbalizerto map PLMs output word to classification labels.Automatic Verbalize Search (AVS) is a search-based ver-balizer design method. It initializes the verbalizer with randomwords, and improves answer words iteratively.Soft Verbalizer (SoftVerb) represents each label with a train-able embedding. In the original work WARP, the prompt templateis also represented as trainable embeddings. In this work, however,we follow Cui et al. to use manual template for fair comparison.Prototypical Verbalizer (ProtoVerb) also represents classifi-cation labels as soft embeddings and samples as features encoded byPLM. ProtoVerb adopts prototypical contrastive learning loss instead of vanilla cross-entropy loss to optimize model parameters.We hand-craft a task-general prompt template and verbalizer forMetricPrompt. For other baselines, we make minor modificationsto the default template and verbalizer used in OpenPrompt to unifythe input format. An overview of prompt template and verbalizerused for our experiments is given in .",
  "Main Results": "We conduct experiments on three text classification datasets withdifferent text styles under four few-shot settings. Experiment resultsfor 2 and 4-shot settings are listed in , while 8 and 16-shotsexperiment results are shown in . MetricPrompt outperformsprevious SOTA automatic verbalizer design method ProtoVerb by alarge margin, improving 2-shot accuracy by 5.88 (11.91% ), 4-shotaccuracy by 11.92 (19.59% ), 8-shot accuracy by 6.80 (9.45% ) and16-shot accuracy by 1.56 (1.96% ). MetricPrompts performanceeven surpasses that of ManualVerb under all few-shot settingswithout any human labor involved in task-specific verbalizer design.Meanwhile, we have following observations:(1) MetricPrompt is the only prompting method outperformingManualVerb without any human labor involved in task-specificverbalizer design. MetricPrompt benefits from paired input text, which enables the model to use cross-sample information to makeup the lack of extra human knowledge and trial-and-error work.(2) MetricPrompt achieves the highest score over automatic ver-balizer design methods. Compared with AVS, MetricPrompt doesnot restrict each classs representation to several sub-optimal wordsfrom the vocabulary, but instead represent it with correspondingtraining samples, leading to more accurate semantic representation.For the comparison with SoftVerb and ProtoVerb, we attribute Met-ricPrompts leading performance to the smaller gap between itstask formulation and PLMs pre-training objective. Unlike SoftVerband ProtoVerb, MetricPrompt does not operate on PLMs inner rep-resentations, but functions with only the output word probabilitydistribution at [MASK] position, enabling PLM to adapt to few-shottext classification task more smoothly. Moreover, MetricPromptdoes not introduce task-specific parameters to be trained fromscratch, avoiding over-fitting problem under few-shot scenarios.(3) MetricPrompt achieves comparable results with mean pool-ing and max pooling, but a performance drop is witnessed withKNN pooling. We ascribe the performance gap to the incompati-bility between KNN pooling and the non-uniform distribution ofMetricPrompts predicted relevance scores. We further discuss thisphenomenon in .3.",
  "+Yahoo71.0073.9979.57+Yahoo53.0376.4189.47+DBPedia32.7743.6353.78": ": The performance of MetricPrompt and ProtoVerb with additional OOD training data. Underlined number indicates thebest result under the same few-shot and OOD setting. Bold number represents the best result on the few-shot task. be improved. We adopt mean pooling and simply mix up trainingsamples of the original dataset and OOD ones to train the model.For ProtoVerb baseline, we first train the model on OOD trainingdata, and then re-initialize prototype parameters for the trainingon the original dataset.As shown in , MetricPrompt achieves much higher accu-racy when boosted by OOD training data. Compared with previousSOTA baseline ProtoVerb, MetricPrompt obtains better predictionaccuracy in 17 out of 18 few-shot and OOD data settings (underlinednumbers in the table), showing remarkable extensibility with OODsamples. Although OOD data improves the models performanceunder most settings, AGs News training set fail to aid DBPedia4-shot and Yahoo 4-shot performance. We ascribe this failure to the abundance of training samples under these settings. DBPediaand Yahoo contains 4096 and 1600 training samples under 4-shotsetting, which is sufficient for the model to adapt to the relevanceestimation task. OOD data serves more as noises and thereforeharms the models performance.It is worth noticing that MetricPrompts performance improve-ment under 1-shot setting is abnormally high. MetricPrompt un-derperforms ProtoVerb on the three datasets without OOD data,because MetricPrompt only takes two identical pieces of text aspositive sample under 1-shot setting, leading to severe over-fittingproblem. The model is optimized to only produce high relevancescore when given two identical pieces of text. Diversified OOD data",
  "Robustness against Noisy Samples": "Noisy samples harm few-shot text classification models perfor-mance severely due to the lack of supervision signal. In this section,we evaluate MetricPrompts robustness against noisy samples onAGs News dataset. Following Cui et al., we conduct experimentsunder 8 and 16-shot settings for training stability. We replace 1, 2and 4 training samples labels randomly to introduce noises, andevaluate MetricPrompts performance when equipped with mean,max and KNN pooling. We compare our method with previousSOTA baseline ProtoVerb, which shows the best robustness againstnoisy samples among automatic verbalizer design methods . Theperformance drop caused by noisy samples is displayed in .Compared with ProtoVerb, MetricPrompt suffers from less per-formance drop and achieves higher classification accuracy withmean and max pooling, while KNN pooling leads to worse perfor-mance. We attribute the large performance drop of KNN poolingto its susceptibility to the variance of each classs training sam-ple number, which is introduced by noisy samples. We provide adetailed analysis in .3.",
  "Comparison across Pooling Methods": "In this part, we analyze different pooling methods with clean andnoised training data to explain their performance gap.Firstly, we focus on clean data scenario without noisy samples.We choose AGs News 2-shot setting and compute the average rele-vance score of each query samples most relevant training sample,second most relevant training sample, etc. As shown in Fig 5, thedistribution of relevance scores is highly non-uniform. The highestrelevance score is much larger than others, so the max relevancescore serves as a decisive factor for MetricPrompt with mean pool-ing. Therefore, mean pooling shows similar behavior with maxpooling. KNN pooling, however, adopts voting strategy which ig-nores score value information, leading to deteriorated performance.We then analyze MetricPrompts performance when noisy sam-ples are introduced. We first categorize classes of AGs News dataset8-shot training sets according to the number of their correspond-ing training samples. Then we collect the statistics of the averagepredicted query sample number for each type of class and showthem in . KNN pooling shows significantly stronger prefer-ence to classes with more training samples than mean pooling andmax pooling do. Since the distribution of MetricPrompts relevancescore except the top ones is relatively even, samples from classes 1st2nd3rd4th5th6th7th8th Training sample rank 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Relevance score",
  ": Average relevance scores between each query sam-ple and all training samples under AGs News 2-shot setting.The scores are sorted and shifted to non-negative region": "with large training set are more likely to become the majority ofKNN poolings top relevant samples. Without considering rele-vance score value, a large drop in KNN poolings performance iswitnessed. On the contrary, mean pooling and max pooling takeeach training samples relevance score value into consideration, sothe influence of training sample number is mitigated. As a result,they suffer from smaller performance drops than KNN pooling do.",
  "Influence of Pivot Sample Number": "In this part, we investigate the influence of pivot sample numbersto the performance of MetricPrompt. We conduct experiments onthe three datasets across four few-shot settings with pivot samplenumber set as 1, 2 and 4, the performance of MetricPrompt underdifferent few-shot settings are displayed and .As shown in the tables, the performance of MetricPrompt corre-lates with the number of pivot samples positively. As the numberof pivot samples increase, MetricPrompt captures each labels se-mantic meaning more accurately and therefore achieves betterperformance. It is worth noticing that even if only one pivot sampleis selected for each class, MetricPrompt still outperforms ProtoVerbunder the four few-shot settings. The selection of pivot samplenumber serves as a trade-off between classification accuracy andefficiency. In real world applications, MetricPrompt can adapt tovarying scenarios with different requirements by adjusting thenumber of pivot samples.",
  "Class": "# Predicted query sample KNN pooling / 4 noisy samples : Average query sample number classified to classes with 7, 8 and 9 training samples under AGs News 8-shot setting. #Predicted query sample indicates the average number of query samples predicted to the class. KNN pooling shows strongerpreference to 9-sample classes than classes with fewer training samples.",
  "CONCLUSIONS AND FUTURE WORK": "In this work, we propose MetricPrompt, which frees human laborfrom task-specific verbalizer design by reformulating few-shot textclassification task into a text pair relevance estimation problem.MetricPrompt prompts query-training text pair to fulfill relevanceestimation, which coincides with PLMs pre-training objective andthus enables smooth adaption to downstream tasks. Taking a pair ofsample text simultaneously, MetricPrompt introduces cross-sampleinformation for better accuracy. Experiments on three widely usedtext classification datasets under four few-shot settings indicateMetricPrompts leading performance over previous SOTA baselines.Our analysis further demonstrates MetircPrompts promising ex-tensibility and robustness, and explains its performance variancewith different pooling methods and pivot sample numbers.Although MetricPrompt achieves satisfactory few-shot text clas-sification performance in our experiments, its performance with large language models remains to be explored. Current large lan-guage models achieve impressive performance in few-shot textclassification tasks with prompting methods, but they still sufferfrom prompting methods susceptibility to the design of verbaliz-ers. When given classes which are difficult to be described withseveral words, these models performance deteriorates significantly.The proposed MetricPrompt is not bounded with specific backbonemodel, and can be easily generalized to large language models. Welook forward to using MetricPrompt to ease human effort from ver-balizer design and further improve the few-shot text classificationaccuracy of large language models.",
  "Charu C Aggarwal and Charu C Aggarwal. 2016. Content-based recommendersystems. Recommender systems: The textbook (2016), 139166": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, BenjaminChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, IlyaSutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.In Advances in Neural Information Processing Systems 33: Annual Conference onNeural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,virtual, Hugo Larochelle, MarcAurelio Ranzato, Raia Hadsell, Maria-FlorinaBalcan, and Hsuan-Tien Lin (Eds.). Ziqiang Cao, Wenjie Li, Sujian Li, and Furu Wei. 2017. Improving Multi-DocumentSummarization via Text Classification. In Proceedings of the Thirty-First AAAIConference on Artificial Intelligence, February 4-9, 2017, San Francisco, California,USA, Satinder P. Singh and Shaul Markovitch (Eds.). AAAI Press, 30533059. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de OliveiraPinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman,et al. 2021. Evaluating large language models trained on code. ArXiv preprintabs/2107.03374 (2021).",
  "Thomas Cover and Peter Hart. 1967. Nearest neighbor pattern classification.IEEE transactions on information theory 13, 1 (1967), 2127": "Ganqu Cui, Shengding Hu, Ning Ding, Longtao Huang, and Zhiyuan Liu. 2022.Prototypical Verbalizer for Prompt-based Few-shot Tuning. In Proceedings of the60th Annual Meeting of the Association for Computational Linguistics (Volume 1:Long Papers). Association for Computational Linguistics, Dublin, Ireland, 70147024. Leyang Cui, Yu Wu, Jian Liu, Sen Yang, and Yue Zhang. 2021. Template-BasedNamed Entity Recognition Using BART. In Findings of the Association for Compu-tational Linguistics: ACL-IJCNLP 2021. Association for Computational Linguistics,Online, 18351845. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding. InProceedings of the 2019 Conference of the North American Chapter of the Associationfor Computational Linguistics: Human Language Technologies, Volume 1 (Long andShort Papers). Association for Computational Linguistics, Minneapolis, Minnesota,41714186. Ning Ding, Shengding Hu, Weilin Zhao, Yulin Chen, Zhiyuan Liu, Haitao Zheng,and Maosong Sun. 2022. OpenPrompt: An Open-source Framework for Prompt-learning. In Proceedings of the 60th Annual Meeting of the Association for Compu-tational Linguistics: System Demonstrations. Association for Computational Lin-guistics, Dublin, Ireland, 105113. Sanjay K Dwivedi and Chandrakala Arya. 2016. Automatic text classificationin information retrieval: A survey. In Proceedings of the second internationalconference on information and communication technology for competitive strategies.16. Tianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making Pre-trained LanguageModels Better Few-shot Learners. In Proceedings of the 59th Annual Meeting ofthe Association for Computational Linguistics and the 11th International JointConference on Natural Language Processing (Volume 1: Long Papers). Associationfor Computational Linguistics, Online, 38163830. Karen Hambardzumyan, Hrant Khachatrian, and Jonathan May. 2021. WARP:Word-level Adversarial ReProgramming. In Proceedings of the 59th AnnualMeeting of the Association for Computational Linguistics and the 11th Interna-tional Joint Conference on Natural Language Processing (Volume 1: Long Pa-pers). Association for Computational Linguistics, Online, 49214933.",
  "Vandana Korde and C Namrata Mahender. 2012. TEXT CLASSIFICATION ANDCLASSIFIERS: A SURVEY. International Journal of Artificial Intelligence & Appli-cations 3, 2 (2012), 85": "Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas,Pablo N Mendes, Sebastian Hellmann, Mohamed Morsey, Patrick Van Kleef, SrenAuer, et al. 2015. Dbpediaa large-scale, multilingual knowledge base extractedfrom wikipedia. Semantic web (2015). Junnan Li, Pan Zhou, Caiming Xiong, and Steven C. H. Hoi. 2021. PrototypicalContrastive Learning of Unsupervised Representations. In 9th International Con-ference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7,2021. OpenReview.net. Xiang Lisa Li and Percy Liang. 2021. Prefix-Tuning: Optimizing ContinuousPrompts for Generation. In Proceedings of the 59th Annual Meeting of the Associ-ation for Computational Linguistics and the 11th International Joint Conferenceon Natural Language Processing (Volume 1: Long Papers). Association for Com-putational Linguistics, Online, 45824597.",
  "Bo Pang, Lillian Lee, et al. 2008. Opinion mining and sentiment analysis. Foun-dations and Trends in information retrieval 2, 12 (2008), 1135": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gre-gory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga,Alban Desmaison, Andreas Kpf, Edward Yang, Zachary DeVito, Martin Rai-son, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai,and Soumith Chintala. 2019. PyTorch: An Imperative Style, High-PerformanceDeep Learning Library. In Advances in Neural Information Processing Systems32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS2019, December 8-14, 2019, Vancouver, BC, Canada, Hanna M. Wallach, HugoLarochelle, Alina Beygelzimer, Florence dAlch-Buc, Emily B. Fox, and RomanGarnett (Eds.). 80248035. Fabio Petroni, Tim Rocktschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin,Yuxiang Wu, and Alexander Miller. 2019. Language Models as Knowledge Bases?.In Proceedings of the 2019 Conference on Empirical Methods in Natural LanguageProcessing and the 9th International Joint Conference on Natural Language Pro-cessing (EMNLP-IJCNLP). Association for Computational Linguistics, Hong Kong,China, 24632473. Flor Miriam Plaza-del Arco, Mara-Teresa Martn-Valdivia, and Roman Klinger.2022. Natural Language Inference Prompts for Zero-shot Emotion Classificationin Text across Corpora. In Proceedings of the 29th International Conference on Com-putational Linguistics. International Committee on Computational Linguistics,Gyeongju, Republic of Korea, 68056817.",
  "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and IlyaSutskever. 2019. Language models are unsupervised multitask learners. OpenAIblog 1, 8 (2019), 9": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the Limitsof Transfer Learning with a Unified Text-to-Text Transformer. Journal of MachineLearning Research 21 (2020), 167. Timo Schick, Helmut Schmid, and Hinrich Schtze. 2020. Automatically Iden-tifying Words That Can Serve as Labels for Few-Shot Text Classification. InProceedings of the 28th International Conference on Computational Linguistics. In-ternational Committee on Computational Linguistics, Barcelona, Spain (Online),55695578.",
  "Timo Schick and Hinrich Schtze. 2020. Few-shot text generation with pattern-exploiting training. ArXiv preprint abs/2012.11926 (2020)": "Timo Schick and Hinrich Schtze. 2021. Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference. In Proceedings of the16th Conference of the European Chapter of the Association for ComputationalLinguistics: Main Volume. Association for Computational Linguistics, Online,255269. Timo Schick and Hinrich Schtze. 2021. Its Not Just Size That Matters: Small Lan-guage Models Are Also Few-Shot Learners. In Proceedings of the 2021 Conferenceof the North American Chapter of the Association for Computational Linguistics: Hu-man Language Technologies. Association for Computational Linguistics, Online,23392352.",
  "KL Sumathy and M Chidambaram. 2013. Text mining: concepts, applications,tools and issues-an overview. International Journal of Computer Applications 80,4 (2013)": "Alon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. 2020. oLMpics-On What Language Model Pre-training Captures. Transactions of the Associationfor Computational Linguistics 8 (2020), 743758. Derek Tam, Rakesh R. Menon, Mohit Bansal, Shashank Srivastava, and ColinRaffel. 2021. Improving and Simplifying Pattern Exploiting Training. In Proceed-ings of the 2021 Conference on Empirical Methods in Natural Language Processing.Association for Computational Linguistics, Online and Punta Cana, DominicanRepublic, 49804991.",
  "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe": "Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu,Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,and Alexander Rush. 2020. Transformers: State-of-the-Art Natural LanguageProcessing. In Proceedings of the 2020 Conference on Empirical Methods in NaturalLanguage Processing: System Demonstrations. Association for ComputationalLinguistics, Online, 3845. Ningyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi Tan,Fei Huang, and Huajun Chen. 2021. Differentiable prompt makes pre-trainedlanguage models better few-shot learners. ArXiv preprint abs/2108.13161 (2021). Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015. Character-level Con-volutional Networks for Text Classification. In Advances in Neural Informa-tion Processing Systems 28: Annual Conference on Neural Information Process-ing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, CorinnaCortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and RomanGarnett (Eds.). 649657."
}