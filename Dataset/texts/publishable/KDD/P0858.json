{
  "Abstract": "Developing effective path representations has become increasinglyessential across various fields within intelligent transportation. Al-though pre-trained path representation learning models have shownimproved performance, they predominantly focus on the topologicalstructures from single modality data, i.e., road networks, overlookingthe geometric and contextual features associated with path-relatedimages, e.g., remote sensing images. Similar to human understand-ing, integrating information from multiple modalities can provide amore comprehensive view, enhancing both representation accuracyand generalization. However, variations in information granularityimpede the semantic alignment of road network-based paths (roadpaths) and image-based paths (image paths), while the heterogeneityof multi-modal data poses substantial challenges for effective fu-sion and utilization. In this paper, we propose a novel Multi-modal,Multi-granularity Path Representation Learning Framework (MM-Path), which can learn a generic path representation by integratingmodalities from both road paths and image paths. To enhance thealignment of multi-modal data, we develop a multi-granularity align-ment strategy that systematically associates nodes, road sub-paths,and road paths with their corresponding image patches, ensuringthe synchronization of both detailed local information and broaderglobal contexts. To address the heterogeneity of multi-modal dataeffectively, we introduce a graph-based cross-modal residual fusioncomponent designed to comprehensively fuse information acrossdifferent modalities and granularities. Finally, we conduct extensive",
  "*Corresponding authors": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from acronym XX, June 0305, 2025, Woodstock, NY 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-1-4503-XXXX-X/18/06 experiments on two large-scale real-world datasets under two down-stream tasks, validating the effectiveness of the proposed MM-Path.This is an extended version of the paper accepted by KDD 2025. Thecode is available at:",
  "Introduction": "Understanding paths and developing effective path representationsare increasingly essential, offering invaluable insights for diversefields such as intelligent navigation , route rec-ommendation , urban planning , and urbanemergency management . Recent studies focus on developingpre-trained path representation learning models, which have demon-strated outstanding generalization capabilities . Thesemodels efficiently produce generic path representations in an un-supervised manner. With simple fine-tuning and little labeled data,they are adaptable to diverse downstream tasks such as travel timeestimation and path ranking score estimation. Consequently, theysignificantly improve computational efficiency by reducing bothlabeled data and runtime.Paths have different modalities that provide richer, more diverseinformation. For example, while paths derived from road networks(road paths for short) elucidate topological relationships among road",
  ": A path in different modalities": "segments in paths, remote sensing images of paths (image paths forshort) provide insights into geometric features and broader environ-mental contexts (see ). Integrating these modalities enrichespath representations with varied perspectives, thereby improvingaccuracy and enhancing generalization capabilities. However, cur-rent path representation learning models primarily rely on single-modality data from road networks, which fails to capture the deep,comprehensive context essential for a complete understanding ofpaths. This calls for developing a multi-modal pre-trained path rep-resentation learning model. Nonetheless, constructing such a modelfaces several challenges:Information granularity discrepancies between road pathsand image paths significantly hinder cross-modal semantic align-ment. Effective cross-modal alignment, which ensures semanticconsistency and complementarity among various modalities, is cru-cial for constructing multi-modal models . However, the dis-crepancies in information granularity between road paths and imagepaths are substantial. As depicted in , road paths typicallyfocus on detailed topological structures and delineate road connec-tivity, while the image paths capture global environmental contextson a large scale, reflecting the functional attributes of correspondingregions. It is worth noting that images may include extensive regionsthat show low relevance to the road paths, such as the dark regions in (c). Current image-text multi-modal models typically align individual images with textual sequences. However,such single-granularity and coarse alignment methods introducenoise, which are not suitable for the precise alignment required forpaths. Additionally, as shown in (a), roads have differentgranularities in nature, including intersections, road segments, andsub-roads. Fully understanding paths at different granularities canprovide insights from micro to macro levels, mitigating the neg-ative effects caused by the differences in information granularityacross modalities. Although some studies have exploredmulti-granularity in single-modal data, they have not adequatelyaddressed the requirements for multi-granularity analysis in multi-modal contexts. Thus, it is crucial to refine multi-granularity dataprocessing and develop multi-granularity methods for cross-modalalignment.The inherent heterogeneity of road paths and image pathsposes a significant challenge during feature fusion. The differ-ences in data structure and information granularity between roadpaths and image paths extend to their learning methods. Road path representation learning typically focuses on connectivity and reacha-bility between roads and intersections, as well as analyzing graphstructures . Conversely, image learning methods thatare able to learn image paths prioritize object recognition and fea-ture extraction, aiming for a broad understanding of image con-tent . These disparate learning methods lead to road pathsand image paths mapped to different embedding spaces, resultingin feature dimensions with similar semantics containing entirelydifferent information. Simple fusion methods like early fusion (i.e.,integrating multiple modalities before or during the feature extrac-tion stage) and late fusion (i.e., keeping each modality independentlyprocessed until the final fusion stage) may result in information lossand increased bias, and fail to capture subtle correlations betweenroad paths and image paths . Therefore, a multi-modal fusionmethod that can capture the relationships among entities in differentmodalities and ensuring effective data fusion, is critically needed.To address these challenges, we propose a Multi-modal, Multi-granularity Path Representation Learning Framework, namely MM-Path, for learning generic path representations.To address the first challenge, we propose a multi-granularityalignment component. This component systematically associatesintersections, road sub-paths, and entire road paths with their cor-responding image information to capture details accurately at afiner granularity as well as maintaining global correspondence at acoarser granularity. Specifically, we divide the image of the entireinterested region into small fixed-size images, collect the small fixed-size images along each path, and arrange the collected images intoan image path (i.e., image sequence). We employ modal-specifictokenizers to generate the initial embeddings for road paths andimage paths, respectively. Subsequently, these initial embeddingsare fed into the powerful Transformer architecture to learn complexencoded embeddings for each modality at three granularities. Finally,a multi-granularity alignment loss function is employed to ensure thealignment of road and image encoded embeddings across differentgranularities.To address the second challenge, we introduce a graph-basedcross-modal residual fusion component, which is designed to ef-fectively fuse cross-modal features while incorporating spatial con-textual information. Specifically, we link the encoded embeddingsof each modality with the initial embeddings of the other modalityto create road and image residual embeddings, respectively, withthe purpose of fusing cross-modal features from different stages.We then build a cross-modal adjacency matrix for each path basedon spatial correspondences and contextual information. This ma-trix guides the GCN to iteratively fuse the residual embeddingsof each modality separately, thus obtaining road and image fusedembeddings. Finally, we apply contrastive loss to ensure the con-sistency of the fused embeddings across the two modalities. As thefinal representation effectively integrates cross-stage features of thetwo modalities with spatial context information, this componentnot only achieves deep multi-modal fusion but also enhances thecomprehensive utilization of information.The contributions of this work are delineated as follows:",
  "of our knowledge, MM-Path is the first model that leveragesroad network data and remote sensing images to learn genericpath representations": "We model and align the multi-modal path information usinga fine-to-coarse multi-granularity alignment strategy. Thisstrategy effectively captures both intricate local details andthe broader global context of the path. We introduce a graph-based cross-modal residual fusion com-ponent. This component utilizes a cross-modal GCN to fullyintegrate information from different modalities while main-taining the consistency of dual modalities.",
  "Preliminaries2.1Basic Conception": "Path. A path is a sequence of continuous junctions, which can beobserved from the road network view and the image view.Road network. A road network is denoted as G = (V, E), whereV and E represent a set of nodes and edges, respectively. Node V is a road intersection or a road end. Edge E denotes aroad segment connecting two nodes.Road paths. We define the sequence of nodes on a road network for apath as a road path R() = 1, 2, ..., |R() |, where each elementrepresents a node, and |R()| represents the length of the road pathR(). It is noted that there must be an edge E connecting anyadjacent nodes in the road path.Image paths. Given an interested region, we partition the regioninto fixed-size segments to generate a set of images, M, consistingof disjoint, fixed-size remote sensing images. Each image withinthis set is denoted as R , where represents the numberof channels, and (,) denotes the resolution. Subsequently, givenroad path R(), the image path (i.e., image sequence of the path)M() is formed by selecting a series of image that correspondto specific latitudes and longitudes along the nodes in the road path.For example, as shown in the upper part of , consider theroad path R() = 1, ..., 8, where nodes 1, 2, and 3 are locatedin image 1, nodes 4, 5 and 6 in image 2, and nodes 7 and 8in image 3. This results in the image path M() = 1,2,3. [cls][sep][sep]",
  ": An example of image path processing": "Road Sub-paths. Given a road path R() and an image path M(),the nodes of R() located in the same image belong to a road sub-path.Taken as an example, the road path R() = 1, . . . , 8has three road sub-paths: 1 = 1, 2, 3, 2 = 4, 5, 6 and 3 =7, 8.",
  "Overall Framework": "Different from existing methods that are limited to data from a singlemodality, MM-Path leverages data from both road networks andimages for pre-training, providing a more comprehensive perspective.MM-Path comprises two main components: the multi-granularityalignment component and the graph-based cross-modal residualfusion component.The multi-granularity alignment component is designed to con-centrate on path-related information while capturing fine-graineddetails and coarse-grained global context. Initially, we convert theimage of interested region into fixed-sized image sequences to obtainimage paths. Subsequently, We establish road path and image pathencoding branches to process the two modalities. In each branch,a modal-specific tokenizer generates initial embeddings for eachmodality at three granularities: node/patch, road sub-path/image, androad path/image path. These initial embeddings are then processedby road and image transformers to produce road and image encodedembeddings, respectively, which are also generated at the same threegranularities. A multi-granularity loss function is utilized to syn-chronize the semantic information of road encoded embeddings andimage encoded embeddings, and to capture their interrelations atdifferent granularities, from fine to coarse.The graph-based cross-modal residual fusion component is de-signed to effectively fuse cross-modal heterogeneous data. A cross-modal residual connection merges the encoded embedding from eachbranch with the initial embedding from the other branch, generat-ing road and image residual embeddings. This connection considerscross-modal features at different stages, promoting deep cross-modalfeature fusion. Subsequently, we construct a cross-modal adjacencymatrix for each path based on spatial correspondences and contex-tual information. This matrix, embedded within a GCN, guides thefusion of the two modalities for each branch. Consequently, a fused",
  "We model the road paths and image paths using Transformer ar-chitecture, respectively. We then construct a multi-granularity lossfunction to ensure alignment between these two modalities": "3.2.1Input Representations. Due to information granularitydiscrepancies between road paths and image paths, direct alignmentis often disturbed by irrelevant information. To solve this problem,we use a sequence of fixed-size images, instead of a single imagecommonly used in traditional image-text multi-modal methods , to model image paths. This procedure preserves thescale and shape features of the images by avoiding distortions causedby inconsistencies in image sizes. Furthermore, as these fixed-sizeimages can be utilized across different paths, the storage of images isreduced. Then, we utilize specialized tokenizers to separately encodethe data of each modality into a unified format.The patch tokenizer segments each image within an image pathinto a series of patches to extract fine-grained semantic informa-tion. Specifically, as shown in , an image R is reshaped into a sequence of 2/2 (e.g., 16) patches, where represents the number of channels, (,) denotes the resolutionof fixed-size image, and (,) defines the resolution per patch.After patching, we concatenate the patch sequences from all im-ages within a image path to form a unified patch sequence. Then,we place a special [cls] token at the beginning of the patch se-quence. As the [cls] token captures the global information of theentire sequence , it can be regarded as a representation of theentire image path. Special [sep] tokens are placed at the end of each image to delineate local information of each image. For ex-ample, the token sequence of image path M() = 1,2,3 is[cls,(1)1 , . . . ,(16)1, sep, . . . , sep, m(1)3 , . . . ,(16)3, sep], where()R denotes the -th patch of the -th image.Each patch ()is then projected into a patch embedding m()R, which can be initialized using pre-trained ResNet50 . Theimage initial embeddings are computed by summing the patch em-beddings with the image position embeddings Timage R1, resulting in H(0) = [mcls, m(1)1 , ..., m( 2/2)|M() |, msep] + Timage. Here,mcls and msep are the image initial embeddings of the [cls] and[sep] tokens, respectively. 1 denotes the length of the patch tokensequence, and represents the dimension of the embeddings.The modeling for a road path is similar, starting with a [cls] to-ken at the beginning and placing [sep] tokens at the end of eachroad sub-path. For instance, a road path R() comprising threeroad sub-paths1 = 1, 2, 3, 2 = 4, 5, 6, and 3 = 7, 8generates the node token sequence [cls, 1, 2, 3, sep, 4, 5, 6, sep, 7,8, sep]. Then, the node tokenizer linearly projects each node fromthe road path R() to a node embedding v R, initialized usingNode2vec . We also integrate standard learnable road positionembeddings Troad R2 with the node embeddings to generatethe road initial embeddings P(0) = [vcls, v1, ..., v|R() |, vsep]+Troad,where vcls and vsep represent the road initial embeddings of the [cls]and [sep] tokens, respectively. 2 denotes the length of the road pathtoken sequence.",
  "P() = Road-Transformer(P(1)),(3)": "where = 1, ...,, P() R2 is the output of the -th layer. Ina brief, the road encoded embeddings P() are represented as P =[pcls, p1, . . . , p|R()|, psep|M()|]. Here, p, psep, pcls R denotethe encoded embeddings of the -th node, the sub-path , and theentire road path R(), respectively. |R()| represents the numberof nodes in road path R(), and |M()| indicates the number ofimages in image path M(), which also corresponds to the numberof road sub-paths.To better capture the complex dependencies in a path, similarto masked language modeling task (MLM) , we use a maskednode modeling task as a self-supervised task. The intuition behindthis is that the information density of individual pixels or patchesin an image is relatively low compared to the topological structureinformation relevant to the path. Therefore, image masking tasksare not deemed essential. To this end, we propose to employ nodemasking tasks. In particular, we randomly mask the nodes in roadpaths (Ref. as to the gray triangle in ) , and then use asoftmax classifier to predict the node tokens corresponding to themasked nodes. The loss function for training is defined as follows:",
  "where P represents the training sets of all paths, D is randomlymasked positions of road path, and maskis the node that is maskedaccording to D": "3.2.4Modalities Aligning. The encoded embeddings from eachbranch capture the hidden semantic information within their re-spective modality, including fine-grained node/patch embeddings,medium-grained road sub-path/image embeddings, and coarse-grainedentire road path/image path embeddings. We aim for embeddingswith similar semantics across modalities to be proximate within theembedding space. Additionally, we seek detailed alignment betweenthe two modalities while maintaining global correspondence. Ac-cordingly, we design a loss function that operates at three distinctlevels of granularityfine, medium and coarsecorresponding tonode/patch, road sub-path/image, and entire road path/image path,respectively.Fine granularity. Since each patch may contain more than onenode, the encoded embeddings of a node and the corresponding patch(Ref. as to the dark yellow triangle the dark green rectangle withyellow borders in ) should maintain directional consistency.To precisely capture the semantic information of fine-grained paths,we minimize the cosine distance between the encoded embeddings of",
  "psep hsep ),(6)": "where psep and hsep (Ref. as to the dark yellow rectangle and thedark green triangle with blue borders in ) are the encodedembeddings of road sub-path and the corresponding image ,respectively.Coarse granularity. Due to the unique correspondence betweenroad path and the corresponding image path, a clearer distinctionis necessary. Therefore, We construct a contrastive loss functionfor coarse-grained data. Considering a batch of road path-imagepath pairs B, the objective of this contrastive learning loss is toaccurately identify the matched pairs among the |B| |B| possiblecombinations. Within a training batch, there are |B|2 |B| negativepairs. The contrastive learning loss function can be formulated as:",
  "NegB exp(sim(pNegcls , hcls)/)),(7)": "where pcls and hcls (Ref. as to the dark yellow rectangle and thedark green triangle with dark blue borders in ) correspondto the encoded embeddings of the entire road path and image path ina road path-image path pairs. Neg and Neg are the negative roadpath and image path in the batch set B, respectively. is a learnedtemperature parameter. sim(pcls, hcls) returns the Euclidean distancebetween pcls and hcls.Finally, the multi-granularity loss can be formulated as Lmulti =Lfine + Lmedium + Lcoarse.",
  "Graph-based Cross-modal Residual Fusion": "In this framework, each path is represented through two distinctmodalities, providing complementary perspectives. To effectivelyleverage these modalities, we propose a graph-based cross-modalresidual fusion component. 3.3.1Cross-modal Residual Connection. To facilitate com-prehensive information exchange between modalities, we introducecross-modal residual connections that effectively concatenate em-beddings across different stages and modalities. These connectionsenable direct propagation of gradients to earlier layers, thereby en-hancing stability and improving training efficiency. Specifically, weconcatenate the road initial embeddings P(0) with the image en-coded embeddings H, and the image initial embeddings H(0) with",
  ": An example of multi-modal graph construction": "the road encoded embeddings P. The resulting image residual embed-dings and road residual embeddings are defined as U = P(0) H andQ = PH(0), respectively. Here, U, Q R(1+2), and denotesthe concatenation operation. 3.3.2Graph-based Fusion. Although traditional attention mech-anisms proficiently identify correlations among entities, they oftenfail to incorporate contextual information concurrently. To addressthis limitation, we utilize graph neural networks , which incorpo-rate contextual information into the learning process by representingit as graph structures. Leveraging this capability, we introduce agraph-based fusion method to enhance the accuracy of informationunderstanding across different modalities.Initially, we construct a specialized cross-modal directed graphfor each path. This graph treats all tokens, including [cls] and [sep]tokens from both modalities, as entities. These entities are connectedvia three types of relationships: intra-modal context, cross-modalcorrespondence, and cross-modal context. The intra-modal contextfocuses on interactions within a single modality, facilitating a deepunderstanding of its specific information. Cross-modal correspon-dence aids in comprehending and learning the spatial correspondencebetween different modalities. Cross-modal context addresses indi-rect relationships between different modal entities, which enhancesthe models ability to interpret complex scenes. Collectively, lever-aging these relationships significantly boosts the models capacityto handle multi-modal data effectively. demonstrates the construction of the graph. Taking node4 as an example, node 4 is connected by directed edges from fiveentities: adjacent context nodes 3 and 5, its corresponding patchL(4), and their respective patches L(3) and L(5). The patch L(4)(i.e., ()) is connected by directed edges from nine entities, includ-ing 3, 4, 5, L(3), L(5), and four geographically adjacent image",
  "), where": "denotes thenumber of patches per row of an image. Additionally, the [sep] to-kens are connected by their context [sep] tokens, corresponding [sep]tokens from another modality, and cross-modal context tokens. The[cls] token, encapsulating more global information, is connected byall [sep] tokens and corresponding [cls] token from another modality.Then, we construct an adjacency matrix A R(1+2)(1+2) for each path to capture the comprehensive relation within themulti-modal data. Given the effectiveness of GCNs in transferringand fusing information across entities within a graph structure, weemploy a GCN to derive the updated embeddings for both branches.",
  ",(9)": "where W1, W2, W3, W4 R are weight matrices, and is thedegree matrix of A. The augmented adjacency matrix A = A + I,where I is a modified identity matrix with all diagonal elements setto 1, except for those corresponding to patches without relationshipto any nodes (Ref. as to the dark green rectangle with a white borderin ). This modification aims to exclude patches that arerelatively unrelated to the path, thereby preventing the introductionof noise into the model.After iterative graph convolution operations, the embeddings ofeach entity within the graph are updated. We perform average pool-ing on U and Q to aggregate the updated embeddings, respectively.The fused embedding for each branch is then obtained by:",
  "where y, z R denote the image fused embedding and the roadfused embedding, respectively": "3.3.3Cross-modal Constrastive Loss. The image fused embed-ding y and the road fused embedding z encapsulate features acrossmultiple modalities of the same path, reflecting an inherent similar-ity. Therefore, we implement a quadruplet loss function to ensurethat the difference between y and z is smaller than the differenceswith the fused embeddings of other paths. For negative samples, werandomly sample the image fused embedding yN and the road fusedembedding zN from the batch. The loss function is defined as:",
  "L = maskLmask + multiLmulti + fuseLfuse,(13)": "where mask, multi and fuse are the weights assigned to Lmask,Lmulti, and Lfuse.After pre-training, we combine the image fused embedding y withthe road fused embedding z into a generic path embedding x = y||z,achieving a more robust and generalized representation. The genericpath embedding is then fine-tuned using interchangeable linear layertask heads, enabling the model to adapt to a variety of downstreamtasks effectively.",
  "Experiments4.1Experimental Setups": "4.1.1Datasets. We utilize the road networks, GPS datasets, andremote sensing image datasets of two cities: Aalborg, Denmark, andXian, China. The road networks are sourced from OpenStreetMap1,while the remote sensing image datasets are acquired from GoogleEarth Engine. Employing an existing tool , we map-matchall GPS records to road networks to generate the path datasets andhistorical trajectory datasets. The details of the datasets are shownin .",
  "Number of images950133AVG number of nodes per image7.9653.01AVG image number per image path6.286.87": "4.1.2Implementation Details. All experiments are conductedusing PyTorch on Python 3.8 and executed on an NVIDIATesla-A800 GPU. Each fixed-size image is 500 500 pixels, witheach pixel corresponding to 2 meters on the earth. In other words, animage covers a 1km 1km region. We segment each image into 16patches and set the embedding dimension to 64. Both the Image-Transformer and Road-Transformer comprise five layers. To enhancethe pre-training efficiency, we initialize our Road-Transformer withthe pre-trained LightPath . The mask ratio is set at 15%. Theweights mask, fuse, and multi are uniformly set to 1. Trainingproceeds for up to 60 epochs with a learning rate of 0.02. The linearlayer task head includes two fully connected layers, with dimensionsof 32 and 1, respectively. Training MM-Path on the Aalborg andXian datasets takes 78 and 161 minutes, respectively. Since trainingis conducted offline, the runtime is acceptable. 4.1.3Downstream Tasks and Metrics. Path Travel Time Esti-mation: We calculate the average travel time (in seconds) for eachpath based on historical trajectories. The accuracy of travel timeestimations is evaluated using three metrics: Mean Absolute Error(MAE), Mean Absolute Relative Error (MARE), and Mean AbsolutePercentage Error (MAPE). Path Ranking Score Estimation (PathRanking): Each path is assigned a ranking score ranging from 0to 1, derived from historical trajectories by following existing stud-ies . We evaluate the effectiveness of path ranking using",
  "The multi-modal methods are:": "CLIP : This is a classic pre-trained multi-modal model.For each path, we use a single rectangular image for the im-age modality and replace the original text sequence with anode sequence. After pre-training, we concatenate the repre-sentations of the two modalities and use them as input to thelinear layer task head.",
  "START+image: This is a multi-modal variant of START,processed similarly to LightPath+image": "For all methods, we standardize the embedding dimensionality() to 50. All parameters are set according to the specifications in theoriginal papers. All baselines are fine-tuned using a linear layer taskhead. The output of this task head serves as the prediction result.For all methods, we initially pre-train using unlabeled trainingdata (e.g., 30K unlabeled Aalborg dataset and 160K unlabeled Xiandataset). Subsequently, we use a smaller volume of labeled data(e.g., 10K labeled Aalborg dataset and 40K labeled Xian dataset)for task-specific fine-tuning. Validation and evaluation processes areconducted on separate validation dataset (e.g., 5K Aalborg datasetand 20K Xian dataset) and test dataset (e.g., 10K Aalborg datasetand 40K Xian dataset), respectively.",
  "Improvement*6.819%8.511%7.943%7.826%8.984%8.614%6.312%8.531%9.222%6.780%12.719%12.213%": "Improvement* quantify the enhancements achieved by MM-Pathover the best single-modal and multi-modal baselines, respectively.Overall, MM-Path outperforms all baselines on these tasks acrossboth datasets, demonstrating its superiority. Specifically, we canmake the following observations: The graph representation learningmethod Node2vec significantly underperforms compared to MM-Path, primarily due to its focus solely on the topological informationof nodes while overlooking the sequential information of paths.Single-modal models like PIM, LightPath, and TracjCL show im-proved performance over Node2vec, indicating the importance ofcapturing sequential correlations within paths. Among the single-modal models, START achieves the best performance. It adeptlyintegrate sequential path information with spatio-temporal transitionrelationships derived from historical trajectory data. However, asa single-modal model, its capabilities are inherently constrained.As a multi-modal model, CLIP exhibits the weakest performance.Designed primarily for general corpora, it focuses on single, coarse-grained image representations, which often introduce noise into pathmodeling. Consequently, CLIP struggles to effectively capture com-plex spatial information and correspondences, making it unsuitablefor modeling paths. USPM performs poorly because it analyzes in-dividual streets using images and road networks, rather than paths(i.e., street sequences). As a result, it fails to effectively mine thesequential relationships present in the two modalities. The variantsLightPath+image and START+image perform comparably to theirsingle-modal models (i.e., LightPath and START), suggesting thatmerely concatenating two modalities does not effectively enhancemulti-modal fusion. Having adapted JGRM to integrate image pathsand road paths, JGRM outperforms other multi-modal baselines. It isspecifically designed for multi-modal integration and excels at merg-ing information from various sources. However, JGRMs limitationsin handling multi-modal information of varying granularities and itslack of use of cross-modal context information to guide the fusionprocess make its performance less optimal compared to MM-Path.",
  "Ablation Study. We design eight variants of MM-Path toverify the necessity of the components of our model: (1) MM-Path-y:": "This model utilizes the fused embedding y (cf. Eq. 10) as a genericrepresentation of the path. (2) MM-Path-z: This variant leveragesthe fused embedding z (cf. Eq. 11) as a generic representation of thepath. (3) w/o alignment: This version excludes the multi-granularityloss. (4) w/o fusion: This variant substitutes the graph-based residualfusion component with average pooling of the encoded embeddingsfrom both modalities. (5) w/o GCN: This model replaces the GCNin the graph-based cross-modal residual fusion component with across-attention mechanism. (6) w/o fine, (7) w/o medium, and (8)w/o coarse: These variants omit the fine-grained, medium-grained,and coarse-grained loss, respectively.The results are summarized in Tables 3 and 4. We can observethat MM-Path w/o alignment shows poor performance, which isattributed to its reliance solely on multi-modal data fusion withoutconsidering multi-granularity alignment. The variants, w/o fine, w/omedium, and w/o coarse, outperform w/o alignment but still worsethan the full MM-Path, demonstrating the importance of multiplegranularity alignments. MM-Path w/o fusion also exhibits poor per-formance, while MM-Path w/o GCN performs slightly worse thanMM-Path. These results indicate that complex fusion methods withcross-modal context information enhance path understanding. BothMM-Path-y and MM-Path-z demonstrate comparable performancein travel time estimation and path ranking. This indicates that dif-ferent modal perspectives contribute valuable insights for variousdownstream tasks. The overall performance of MM-Path surpassesall variants. This result implies that each of the proposed componentssignificantly enhances the models effectiveness. This conclusivelyvalidates that MM-Path optimally utilizes all designed components. 4.2.3Effect of Pre-training. In this section, we evaluate the ef-fect of pre-training. We vary the size of labeled data used for fine-tuning, and compare the performance of the proposed MM-Path(Pre-trained) with its variant that lacks pre-training (No Pre-trained). shows the performance of travel time estimation and pathranking. We observe that the performance of both models improveswith an increase in labeled data size, and the pre-trained model con-sistently outperforms the no pre-trained model. This illustrates that",
  "MM-Path187.4520.19323.6440.1650.2570.294": "the pre-trained model, equipped with extensive cross-modal contextinformation, requires less labeled data and achieves superior perfor-mance compared to the model without pre-training. These findingssuggest that MM-Path can effectively serve as a pre-training modelto enhance supervised learning methods. 4.2.4Parameter Sensitivity. We explore the impact of imagegranularity size on the models performance. For uniform segmenta-tion, each 500 500 pixel image is segmented into 11, 22, 44,55, and 1010 patches, respectively. The performance and testingruntime for each granularity size are detailed in .We observe that a larger number of patches also incurs longerinference runtime for each path. Moreover, the models performanceimproves as the number of patches increases from 11 to 44,suggesting that finer granularity enhances the capture of detailed fea-tures. However, performance begins to decline with further increasesto 55 and 1010 patches. This decrease is due to the limited contextfeatures extracted by excessively fine granularity, which negativelyimpacts path understanding.Then, we explore the model scalability in terms of road pathlength. We evaluate the performance of all models on paths withvarying numbers of nodes. Specifically, paths with fewer than 50nodes are classified as short paths, while those with more than 50nodes are classified as long paths. The performance of all models onshort and long paths is detailed in Tables 5 and 6, respectively. Theexperimental results show that MM-Path outperforms other modelsacross both path lengths, demonstrating its superiority. 4.2.5Case Study. We inspect a pair of representative paths inAalborg to demonstrate the superiority of MM-Path. The road pathsand image paths are visualized in . The travel time estimationresults of MM-Path and the superior baselines are shown in .Two paths in exhibit a similar structure on the roadnetwork, both having a node degree sequence of 3, 2, 3, 3, 3. Suchsingle-modal data might suggest that these paths have comparabletravel times. However, the visual information from their imagesdiffers significantly. Specifically, path 1 traverses a roundabout andruns along a trunk road, where paths typically allow for faster travelspeeds. In contrast, path 2 is located on an ordinary road near resi-dential buildings, typically associated with slower speeds.As shown in , the travel time estimates from TrajCL,START, and START+image suggest a shorter travel time for path 2",
  "Improvement*6.732%12.318%14.599%11.842%10.499%11.873%5.527%18.103%10.447%5.504%5.283%7.836%": "and a longer travel time for path 1, which are contrary to the groundtruth. This illustrates that a single modality provides limited infor-mation, and the simple concatenation of multi-modal data in theSTART+image model fails to effectively extract image information.In contrast, the results from the multi-modal model JGRM align withthe relative magnitudes of the actual travel times, but its estimatesexhibit large deviations. Meanwhile, MM-Path demonstrates supe-rior travel time estimation performance compared to other models,indicating its effective fusion and utilization of image information.",
  "MM-Path: Multi-modal, Multi-granularity Path Representation LearningExtended VersionConference acronym XX, June 0305, 2025, Woodstock, NY": "developing path representation learning models that do not rely onlabeled training data, showing robust generalization across multipledownstream tasks . For instance, Jiang et al. intro-duce a self-supervised trajectory representation learning frameworkthat includes tasks such as span-masked trajectory recovery and tra-jectory contrastive learning to leverage temporal patterns and travelsemantics effectively. Yang et al. aim to minimize resource con-sumption and enhance model scalability by developing LightPath, alightweight and scalable framework designed to conserve resourceswhile maintaining accuracy. Additionally, Ma et al. proposea representation learning framework that integrates GPS and routemodeling based on self-supervised technology, further expandingthe fields methodologies.Recent advancements in Large Language Models (LLMs) have facilitated the development of general spatio-temporalprediction models . For example, Unist mapsspatio-temporal data onto grids and provides generic predictionsacross various scenarios using elaborated masking strategies andspatio-temporal knowledge-guided prompts. However, since eachpath exhibits geographical continuity and cannot be discretized intoa single region, these models are not well-suited for path modeling.Despite recent advancements, current path representation learningmodels overlook the potential contributions of images in path un-derstanding, which can provide valuable insights into the geometricfeatures and contextual environmental information from a globalperspective.",
  "Multi-modal Pre-trained Models": "Multi-modal pre-trained models aim to enhance target representa-tion by leveraging diverse modalities, including text, images, andaudio. Some methodologies converge the information from variousmodalities into a unified latent space . For example, Wang etal. propose an unsupervised multi-modal method that encodesvisual, textual, and geospatial data of urban neighborhoods into aunified vector space using multiple triplet loss functions. Radfordet al. develop CLIP, a large-scale multi-modal model that eval-uates the congruence of image-text pairs through cosine similarity.Some research opts to map each modality into distinct embeddingspaces, while enforcing fusion on the representations . Forinstance, Pramanick et al. achieve robust video-text represen-tation by embedding cross-modal fusion within the core video andlanguage structures, thereby facilitating various downstream tasksand reducing fine-tuning requirements. Bao et al. present VLMO,a unified vision-language pre-trained model that jointly trains adual encoder and a fusion encoder within a modular Transformernetwork.Nevertheless, these models are predominantly trained on generalcorpora. The characteristics of road paths and image paths, whichinclude complex correspondences and spatial topological relation-ships, are distinct from those found in conventional multi-modaldatasets. Consequently, existing multi-modal models exhibit limitedgeneralizability to this specialized domain.",
  "In this paper, we propose a Multi-modal Multi-granularity Path Rep-resentation Learning Framework (MM-Path), which is the first work": "that integrate data from road networks and remote sensing imagesinto generic path representation learning. Initially, we model the roadpaths and image paths separately, and implement a multi-granularityalignment strategy to ensure the synchronization of both detailedlocal information and broader global context. Furthermore, we de-velop a graph-based cross-modal residual fusion component thateffectively fuses information from both modalities while preservingthe semantic consistency between modalities. MM-Path outperformsall baselines on two real-world datasets across two downstream tasks,demonstrating its superiority.In the future, we plan to further investigate the capability of multi-modal models for generic path representing learning, with particularfocus on few-shot and zero-shot learning scenarios.",
  "Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. 2021. BEiT: BERT pre-training of image Transformers. In ICLR": "Hangbo Bao, Wenhui Wang, Li Dong, Qiang Liu, Owais Khan Mohammed,Kriti Aggarwal, Subhojit Som, Songhao Piao, and Furu Wei. 2022. Vlmo: Uni-fied vision-language pre-training with mixture-of-modality-experts. NeurIPS 35(2022), 3289732912. Sean Bin Yang, Chenjuan Guo, Jilin Hu, Bin Yang, Jian Tang, and Christian S.Jensen. 2022. Weakly-supervised temporal path representation learning withcontrastive curriculum learning. In ICDE. 28732885.",
  "Ziqiao Liu, Hao Miao, Yan Zhao, Chenxi Liu, Kai Zheng, and Huan Li. 2024.LightTR: A lightweight framework for federated trajectory recovery.arXivpreprint arXiv:2405.03409 (2024)": "Yandi Lun, Hao Miao, Jiaxing Shen, Renzhi Wang, Xiang Wang, and SenzhangWang. 2024. Resisting tul attack: balancing data privacy and utility on trajectoryvia collaborative adversarial learning. GeoInformatica 28, 3 (2024), 381401. Zhipeng Ma, Zheyan Tu, Xinhai Chen, Yan Zhang, Deguo Xia, Guyue Zhou, YilunChen, Yu Zheng, and Jiangtao Gong. 2024. More than routing: Joint GPS androute modeling for refine trajectory representation learning. In WWW. 30643075.",
  "Paul Newson and John Krumm. 2009. Hidden markov map matching throughnoise and sparseness. In SIGSPATIAL. 336343": "Zhicheng Pan, Yihang Wang, Yingying Zhang, Sean Bin Yang, Yunyao Cheng,Peng Chen, Chenjuan Guo, Qingsong Wen, Xiduo Tian, Yunliang Dou, et al. 2023.Magicscaler: Uncertainty-aware, predictive autoscaling. PVLDB 16, 12 (2023),38083821. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gre-gory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al.2019. Pytorch: An imperative style, high-performance deep learning library.",
  "Simon Aagaard Pedersen, Bin Yang, Christian S. Jensen, and Jesper Mller. 2023.Stochastic routing with arrival windows. ACM Trans. Spatial Algorithms Syst. 9,4 (2023), 30:130:48": "Shraman Pramanick, Yale Song, Sayan Nag, Kevin Qinghong Lin, Hardik Shah,Mike Zheng Shou, Rama Chellappa, and Pengchuan Zhang. 2023. EgoVLPv2:Egocentric video-language pre-training with fusion in the backbone. In ICCV.52855297. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,et al. 2021. Learning transferable visual models from natural language supervision.In ICML. 87488763.",
  "Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Jason Wei, Xuezhi Wang,Hyung Won Chung, Dara Bahri, Tal Schuster, Steven Zheng, et al. 2022. UL2:Unifying language learning paradigms. In ICLR": "Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kul-shreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.2022.Lamda: Language models for dialog applications.arXiv preprintarXiv:2201.08239 (2022). Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu,Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, et al.2023. Image as a foreign language: Beit pretraining for vision and vision-languagetasks. In CVPR. 1917519186."
}