{
  "Abstract": "Learning interpretable models has become a major focus of ma-chine learning research, given the increasing prominence of ma-chine learning in socially important decision-making. Among in-terpretable models, rule lists are among the best-known and easilyinterpretable ones. However, finding optimal rule lists is compu-tationally challenging, and current approaches are impractical forlarge datasets.We present a novel and scalable approach to learn nearly opti-mal rule lists from large datasets. Our algorithm uses sampling toefficiently obtain an approximation of the optimal rule list with rig-orous guarantees on the quality of the approximation. In particular,our algorithm guarantees to find a rule list with accuracy very closeto the optimal rule list when a rule list with high accuracy exists.Our algorithm builds on the VC-dimension of rule lists, for whichwe prove novel upper and lower bounds. Our experimental evalua-tion on large datasets shows that our algorithm identifies nearlyoptimal rule lists with a speed-up up to two orders of magnitudeover state-of-the-art exact approaches. Moreover, our algorithm isas fast as, and sometimes faster than, recent heuristic approaches,while reporting higher quality rule lists. In addition, the rules re-ported by our algorithm are more similar to the rules in the optimalrule list than the rules from heuristic approaches.",
  "Interpretability is one of the characteristics of machine learningmodels that has become a major topic of research due to the ever": "KDD 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s).This is the authors version of the work. It is posted here for your personal use. Notfor redistribution. The definitive Version of Record was published in Proceedings ofthe 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 24),August 2529, 2024, Barcelona, Spain, increasing impact of machine learning models in socially importantdecision-making . The goal is to build models that are easilyunderstood by humans, while achieving high predictive power, incontrast to black-box models (e.g., large deep-learning models) thatare highly predictive but are not transparent nor interpretable.Rule lists , and more in general rule-based models such asdecision trees, are among the best-known and easily interpretablemodels. A rule list is a sequence of rules, and the prediction fromsuch a model is obtained by applying the first rule in the list whosecondition is satisfied for the given input.Rule-based models often display predictive power comparableto black-box models in several critical applications . While ad-vanced techniques have been recently developed to speed-up thediscovery of optimal rule lists , it remains a computa-tionally challenging task, in particular for large datasets where dif-ferentiable models (such as neural networks) can be easily learnedwith gradient descent and its variants.Fast heuristic methods for rule lists and other rule-based modelshave been developed , but they do not provide guarantees interms of the accuracy of the rule list they report. In several appli-cations identifying an optimal, or close to optimal rule, is crucial,since the goal is to find models that are not only interpretable,but also have high predictive power and can therefore be used asinterpretable alternatives to black-box models.A natural solution to speed up the training on a large dataset,and to obtain an approximation of the best rule list, is to reduce thesize of the training set, e.g., by only considering a small randomsample of the dataset. On the other hand, it is clear that there isan intrinsic trade-off between the accuracy of this approximationand the size of the random sample, i.e., the computational costincurred by the learning algorithm. This accuracy depends directlyon the fluctuations of the losses estimated on the sample w.r.t. thelosses measured on the entire dataset: a sample too small can beanalyzed quickly but may provide inaccurate estimates. Moreover, itis possible that the best rule list, learned from a random sample, maybe qualitatively different, thus not representative, of the optimalsolution computed on the whole dataset.Contributions. To address these challenges, we precisely quantifythe maximum deviation between the accuracies of rule lists thatare estimated on the random sample w.r.t. the respective exactcounterparts. As a consequence, we obtain sufficient sample sizesthat guarantee that an accurate rule list can be found from a smallrandom sample. We then propose SamRuLe (Sampling for Ruleslist Learning), a scalable approach to approximate optimal rule listsfrom large datasets with rigorous guarantees on the quality of theapproximation.",
  "KDD 24, August 2529, 2024, Barcelona, SpainLeonardo Pellegrina and Fabio Vandin": "SamRuLe than draws a sample S of (R, D) instances takenuniformly at random from D, and outputs = arg minR (, S)by finding the optimal rule list on the sample S. Note that SamRuLecan leverage any exact algorithm, such as CORELS , to find from S.In the following sections, we prove that SamRuLe pro-vides an (,)-approximation of the optimal rule list =arg minR (, S) with probability 1 . Our proof lever-age novel bounds on the VC-dimension of rule lists, that we presentin .1, while the approximation guarantees from SamRuLeare proved in .2.",
  "Related Works": "We now discuss the works most related to ours. We focus on meth-ods to learn rule lists, in particular sparse rule lists. Sparsity isa crucial requirement for the interpretability of models. For anoverview on the topic of interpretability in machine learning, wepoint the interested reader to the survey from Rudin et al. .Finding optimal sparse rule lists is NP-hard, and several methodshave been proposed to learn sparse rule lists. Such methods fallunder two categories: exact methods, that guarantee to find theoptimal rule list (i.e., with highest accuracy), and heuristic methods,that are faster but provide no guarantees on the accuracy of thereported rule list w.r.t. the optimal.In the category of exact methods, Angelino et al. proposeCORELS, a branch-and-bound algorithm that achieves several or-ders of magnitude speedup in time and a significant reduction ofmemory consumption, by leveraging algorithmic bounds, efficientdata structures, and computational reuse. Rudin and Ertekin develop a mathematical programming approach to building rulelists. Okajima and Sadamasa define a continuous relaxed ver-sion of a rule list, and propose an algorithm that optimizes rule listsbased on such continuous relaxed versions. Yu et al. proposea SAT-based approach to find optimal rule sets and lists. Whilehighly optimized, exact methods such as the aforementioned onesonly apply to problems of moderate size, and do not scale to largedatasets.In the category of heuristic methods, Rivest proposed greedysplitting techniques, also used in subsequent heuristic algorithmsfor learning decision trees . RIPPER builds rule sets in agreedy fashion, and a similar greedy strategy has been used for find-ing sets of robust rules in terms of minimum description length .Other works considered different problems and classification mod-els, such as probabilistic rules for multiclass classification , and,more recently, rule lists with preferred variables , sets of locallyoptimal rules , and multi-label rule sets . Yang et al. uses, instead, a Bayesian approach with Monte-Carlo search to explorethe rule list solution space. All such approaches focus on improvingefficiency with respect to exact approaches, but do not provideguarantees on the quality of the reported rule lists with respect tothe optimal one. In several challenging instances, the gap with theoptimal solution may be substantial, resulting in a predictive modelwith suboptimal performance.A different line of research, related to the identification of almost-optimal models, is provided by the exploration of the Rashomonset, that is the set of almost-optimal models for a machine learningproblem . Works in this area have focused on the explo-ration or full enumeration of the Rashomon set, including the caseof rule-based models , for example to study variable impor-tance in well-performing models . While some of our theoreticalresults may be useful for such applications as well, our focus isnot the exploration or enumeration of all almost optimal models,but rather on efficiently finding one nearly optimal model whileproviding guarantees on its quality.Our algorithm builds on novel upper bounds to the VC-dimension of rule lists, a fundamental combinatorial measure ofthe complexity of classification models. We combine these boundswith sharp concentration inequalities to bound the maximum devi-ation between the prediction accuracy of rule lists from a randomsample w.r.t. the entire dataset. Our VC-dimension upper boundsgeneralize previous results that can be derived from rule-basedclassification models , as they provide a more granular depen-dence on the rule list parameters compared to known results (see.1 for a detailed discussion). We establish the tightness ofour analysis by proving almost matching lower bounds, that signif-icantly improves over the best known results from Yildiz . TheVC-dimension has been used to develop generalization bounds andsampling algorithms for other data mining tasks, including associa-tion rules mining and sequential pattern mining . To thebest of our knowledge, our work is the first to use VC-dimensionin designing efficient sampling approaches for learning rule lists.",
  "Preliminaries": "We define a dataset D as a collection D = {(1,1), . . . , (,)} oftraining instances, where each training instance is a pair (,) Dcomposed by a set of features, and a categorical label . Wedenote with the -th feature, for 1 . For simplicity, in thiswork we will focus on datasets with binary features, {0, 1}for 1 , and binary labels {0, 1}, but most of our resultsextend to the general case. We use D to denote the collection ofthe features of each training instance, ignoring the labels, that isD = {1,2, . . . ,}.We define a rule list = [1,2, . . . ,,] of length || = as asequence of 0 rules , 1 , plus a default rule . Eachrule is defined as a pair = (, ) where is a condition on asubset of the features of the dataset, and {0, 1} is a predictionof the label. Similarly to previous rule-based models , weonly consider conditions composed by conjunctions of monotonevariables of the type = = 1, i.e., we do not consider thenegation of binary features (e.g., = 0). Note that negationscan be included easily by considering additional binary features.The default rule = (, ) is composed by the condition that",
  "Scalable Rule Lists Learning with SamplingKDD 24, August 2529, 2024, Barcelona, Spain": "k SamRuLe (z=1)SamRuLe (z=2)SamRuLe (z=3) k 2 104 3 104 4 104 Number of samples Number of samples (mushroom) k Running time (s) Running time (mushroom) 1.01.52.02.53.03.54.04.55.0 k Rule list loss Rule list loss (mushroom) k 2 104 3 104 4 104 6 104 Number of samples Number of samples (phishing) k Running time (s) Running time (phishing) k 8 102 9 102 Rule list loss Rule list loss (phishing)",
  "(, )D1 [(,) ] + ||": "The use of the regularized loss is common practice in rule listlearning , allowing to prefer shorter (i.e., simpler) rule lists overlonger ones when their accuracy is similar, as quantified by theparameter .Define R as the set of rule lists with length , where eachrule in is given by the conjunction of at most terms. The rulelist learning problem is to identify a rule such that",
  "= arg minR{(, D)}": "The values of and are usually fairly small, since large valueswould compromise the interpretability of the rule list . Finding is NP-hard, and to develop efficient algorithms one has to resortto approximations.Our goal is to compute, for given accuracy parameters , (0, 1], a rule list that provides an (,)-approximation of theoptimal rule list , defined as follows.",
  "( , D) (, D) + max{(, D),}": "Our definition of (,)-approximation is motivated by the factthat the optimal loss (, D) is unknown a priori, and often ex-tremely expensive to compute or even estimate from large datasets.Therefore, we need to design approximation guarantees that aresharp in all situations, i.e., for the all possible values of the optimalloss. The (,)-approximation of Definition 3.1 allows to interpo-late between an additive approximation, with small absolute error, when the optimal loss (, D) is small (i.e., ), and a relativeapproximation (with parameter ) when the optimal loss (, D) is large (i.e., > ). This flexible design avoids statistical bottlenecksthat are incurred when the loss of the best model is large, a wellknown issue in statistical learning theory . Taking this effectinto account allows SamRuLe to be extremely efficient in terms ofsample sizes.As we will formalize in , the main goal of SamRuLe isto compute an (,)-approximation of the optimal rule list from arandomly drawn samples with high probability.",
  "SamRuLe Algorithm": "As introduced in previous sections, the main idea of our approachis to compute an approximation of the best rule list performingthe analysis of a small random sample, instead of processing alarge dataset, which may be extremely expensive and unfeasible inpractice. In this section we formally define our algorithm.Define a sample S as a collection of instances of the datasetD taken uniformly and independently at random. The regularizedloss of a rule list computed on the sample S is defined as",
  "ES[(, S)] = (, D)": "We remark, however, that the convergence of (, S) towards(, D) in expectation is not sufficient to derive rigorous approxi-mations for the best rule list; instead, it is necessary to take intoaccount the variance of the deviations, that indirectly depends thecomplexity of the class of rule list models. Therefore, our approach isbased on properly bounding the deviations |(, S) (, D)| withhigh probability, in order to guarantee that the best rule identifiedfrom the sample S is a high-quality approximation of the optimalsolution from D. To obtain a scalable method, we wish to identifythe smallest possible sample size that yields the above-mentionedguarantees. We address this challenging problem combining ad-vanced VC-dimension bounds and sharp concentration inequalities.We now describe our algorithm SamRuLe. The input to SamRuLeis: a dataset D of training instances over features; the values and , representing the maximum number of rules in a rule listand the maximum number of conditions for each rule; constants, (0, 1], that are the values defining the quality of the (,)-approximation (see ); and a constant (0, 1], that definesthe required confidence, that is the probability that the output bySamRuLe is a (,)-approximation. (Note that the set R of ruleslists that can be produced in output by SamRuLe is defined by D, ,and .) Given the input, SamRuLe computes the quantity (R, D),defined as the minimum value of 1 such that it holds",
  "Complexity of Rule Lists": "In this section we provide analytical results on the complexity ofrule lists, by studying their VC-dimension . Intuitively, the VC-dimension measures the capacity of a class of predicion models ofpredicting arbitrary labels assignments with perfect accuracy. Asdiscussed in previous sections, these bounds will be instrumental toderive accurate approximation bounds and prove that the reportedrule by SamRuLe is an (,)-approximation.We start by providing the main concepts related to VC-dimension(see for a more in depth introduction).Given a dataset D = {(1,1), . . . , (,)}, remember that D ={1, . . . ,}. Given the set R of rule lists with length , whereeach condition contains conjunctions with at most terms, foreach rule list R we define the projection of on the datasetD as (, D) = { D : (,) = 1}. We define the rangespace associated to the dataset D and to R as (D, R), whereR = { (, D) : R} is the range set. Note that in general| R | can be smaller than |R |, since two rule lists 1 and 2 mayprovide the same predictions for all instances in a dataset D, whichimplies (1, D) = (2, D). The projection of the range set Ron a subset D D of the dataset D is defined as P( R, D) ={ (, D) : R}. If P( R, D) = 2D, i.e., the projection of R on D produces allpossible dichotomies of the elements of D, then D is shattered byR. The VC-dimension of the range space (D, R) is the cardinalityof the largest subset of D that is shattered by R. In what followswe will use (R) to denote the VC-dimension of the range space(D, R) associated to the set of rule lists R. 4.1.1Upper bounds to the VC-dimension. We now prove refinedupper bounds to the VC-dimension of rule lists. We start by provid-ing upper bounds to the growth function (the maximum cardinalityof distinct projections of a range set, see below) of rule lists R1,for which = 1 (i.e., each rule consists of a condition on exactly1 feature). From such bound we derive an upper bound to theVC-dimension of the range space associated to R1, that we willgeneralize to derive an upper bound to the VC-dimension of R forgeneral values of . Due to space constraints, some of the proofsare in Appendix.Define the growth function (R,) of the rule lists R as themaximum number of distinct projections of rule lists in R over",
  "=0(G,)": "We proceed to bound each (G,) separately, while excludingrule lists G that have the same projection, on any D, ofother rules G, for some . First, consider G0; it holds(G0,) = 2 since G0 contains 1 = [(, 1)] and 2 = [(, 0)].Then, consider G for any 1 , and let any rule list G, with = [(1, 1), . . . , (, ), (, )]. There are twopossibilities: = or . In the first case, denote the rule = [(1, 1), . . . , (1, 1), (, )] G1. It is easy to ob-serve that the projections (, D) and (, D) are equal, forany D. Therefore, any rule list G with = can be ig-nored from the upper bound to (G,) as it is already coveredby at least one element of the set G1. Then, consider a rule list = [(1, 1), . . . , (, ), (, )] G such that there exist twoindices 1 < with = . We observe that the rule = [(1, 1), . . . , (1, 1), (+1, +1) . . . , (, ), (, )] G1 is equivalent to , since (, D) = (, D) for any D.From these observations, the number of rules lists G withdistinct projections on a dataset with elements cannot exceed2 1=0 ( ), as we consider all possible ordered choices of distinct elements from the set of features {1, . . . ,}, and all 2",
  "+ 2": "The bounds we derived in Corollary 4.2 and Corollary 4.3 scalelinearly with and , and logarithmically with . This implies that,since we are interested in sparse models with small values of and, the resulting complexity will not be large. This guarantees thataccurate models will generalize well, and that the random sampleS should be representative of the dataset D. Moreover, the weakdependence of the bounds on allows SamRuLe to be extremelyeffective even on datasets with large sets of features.Rudin et al. study generalization bounds for classes of binaryclassification models built from sequences of association rules; thisgeneral model includes rule lists. Moreover, they show that theVC-dimension of the set of rule lists created using pre-mined rulesis exactly the size of the set of pre-mined rules, which is O() (seeTheorem 3 in ). However, we remark that this result only holdsfor unconstrained rule lists (e.g., without any bound on theirlength), and cannot be adapted to our setting. Our O( log(/))upper bound on the VC-dimension is more accurate, since it offers amore granular dependence on the parameters defining the rule listsearch space. More precisely, it explicitly depends on the maximumnumber of rules in the list, the number of features , and themaximum number of conditions in each rule, while the boundO() from is not sensible to such constraints (as it assumes = ). Since in most cases , our upper bounds are ordersof magnitude smaller. 4.1.2Lower bounds to the VC-dimension. In this section we provealmost matching lower bounds to the VC-dimension of rule lists,confirming the tightness of our analysis. Our first result involves alower bound to (R1).",
  "Approximation guarantees": "In this section we prove sharp approximation bounds for the es-timated losses (, S) of rules on a random sample S w.r.t. thelosses (, D) on the dataset D, and derive sufficient sample sizesto compute (,)-approximations for the optimal rule list . Dueto space constraints, we provide most of the proofs in the Appendix.We define as = ln(2/) + 2, a complexity parameter of ourbounds derived from the results of .1 (Corollary 4.3).",
  "SamRuLe provides an (,)-approximation of with probability 1": "Proof. Our proof is based on obtaining an upper bound to( , D), where is the rule list reported by SamRuLe, that is afunction of the optimal loss (, D), and then showing that it issufficiently small to guarantee an (,)-approximation. First, weobserve that (, D) ( , D), as is one of the rule listswith minimum loss. Therefore, from the first set of inequalities ofTheorem 4.6 setting = , we have the upper bound to (, D)",
  ".(2)": "We now prove that, since is chosen as the rule list with minimumloss on the sample, its estimated loss ( , S) should be concentratedtowards (, D). We have ( , S) (, S), as is one ofthe rule list with minimum loss in S. Using the last inequality ofTheorem 4.6 it holds",
  "(,) , 1.(5)": "From the definition of (R, D), we know that (,) holdsfor all (R, D), therefore (5) is verified for = . By takingthe derivative w.r.t. of both sides of (5), it is simple to check thatthe derivative of the r.h.s. is constant, while the derivative of thel.h.s. is monotonically decreasing with . Therefore, (5) also holdsfor all < 1, obtaining the statement.",
  "); moreover, it upper bounds(R, D), as (6) provides a value of that make (1) true (notnecessarily the minimum), obtaining the statement": "Interestingly, the sample size (R, D) is completely indepen-dent of the size of the dataset D: it only depends on the parameters = ln(2/) + 2 of the rule lists search space, and the desiredapproximation accuracy , and confidence . This characteristicallows SamRuLe to be applied to massive datasets, of arbitrarilylarge size .",
  "Experiments": "This section presents the results of our experiments. The maingoal of our experimental evaluation is to test the scalability ofSamRuLe in analyzing large datasets compared to exact approaches(.1). To do so, we measure the number of samples used bySamRuLe to obtain an accurate approximation of the best rule, itsrunning time, and the accuracy of the reported rule list comparedto the optimal solution. Then, we also compare SamRuLe withstate-of-the-art heuristics for rule list training (.2), as suchmethods, while not offering theoretical guarantees, may still provideaccurate rule lists in practice. Finally, we quantify the performanceof SamRuLe under several settings of the rule list parameters and (.3).Datasets. We tested SamRuLe on 8 benchmark datasets fromUCI1. We binarized the datasets containing countinous featuresby considering 4 thresholds at equally spaced quantiles: for eachcountinous feature and each threshold , we created two binaryfeatures and < . The statistics of the resulting binary",
  "Comparison to exact method": "In this section we describe the experimental comparison betweenSamRuLe and CORELS, the state-of-the-art method to identify theoptimal rule list with minimum loss. Our main goal is to eval-uate the scalability of SamRuLe in terms of number of samplesand running time required to obtain an accurate approximation ofthe best rule. Furthermore, we evaluate, both quantitatively and",
  "a9a325611241004adult325611751004bank411881521004higgs1100000053113ijcnn191701351008mushroom81241182005phishing11050691008susy500000017915": "qualitatively, the accuracy of the rule list found by SamRuLe w.r.t.the optimal solution returned by CORELS. We ran both methods onall datasets, setting = 1 and as in . For SamRuLe, we varythe parameters and as described at the beginning of . shows the results for these experiments. In .(a)and (b) we compare the running time of SamRuLe with the timeneeded by CORELS on the adult and bank datasets. The resultsfor other datasets are very similar, and shown in (in theAppendix). From these results, we can immediately conclude thatSamRuLe requires a small fraction of the time needed by CORELS,with an improvement of up to 2 orders of magnitude. The reasonfor this significant speedup is that SamRuLe searches for the rulewith minimum loss on a small sample S, which is all cases ordersof magnitude smaller than the size of original dataset D. We showthe number of samples (R, D) used by SamRuLe for all datasetsand for all parameters in (in the Appendix).Then, we evaluated the quality of the solutions returned by Sam-RuLe with the optimal rule list computed by CORELS. Figures 2.(c)and (d) show the average deviations |( , D) (, D)| of theloss of the rule list found by SamRuLe w.r.t. the optimal rule list found by CORELS on the dataset D. To verify the validity ofSamRuLes theoretical guarantees, the plots also show the loss ofthe optimal solution (, D) found by CORELS (purple horizontalline), and upper error bars (+std) for the average deviations (seeFigures 6 and 7 in the Appendix for the plots for all datasets withboth bars). From these results, we observe that the rule lists re-ported by SamRuLe are extremely accurate in terms of predictionaccuracy, since the deviations |( , D) (, D)| are orders ofmagnitude smaller than (, D), and smaller than guaranteed byour theoretical analysis. This confirms that SamRuLe outputs ex-tremely accurate rule lists, even when trained on random samplesthat are orders of magnitude smaller than the entire dataset. Further-more, it is likely that the guarantees of the (,)-approximationshold for samples smaller than what guaranteed by our analysis;this leaves significant opportunities for further improvements ofour algorithm. Then, we quantify the deviations between the loss( , S) of estimated on the sample w.r.t. to the loss ( , D) onthe dataset, i.e., the approximation error incurred by SamRuLe dueto analyzing S instead of the entire dataset D. In we showthe average loss approximation error |( , D) ( , S)|, which is",
  "(d)": ": Performance and accuracy comparison between SamRuLe and CORELS on adult and bank datasets, for differentvalues of and . (a)-(b): running times of CORELS and SamRuLe. (c)-(d): average deviations |( , D) (, D)| of the loss ofthe rule list found by SamRuLe with the optimal rule list found by CORELS (purple horizontal line drawn at = (, D)).The deviation plots only show upper errors bars at +std to improve readability. See Figures 6 and 7 in the Appendix for theplots for all datasets and with std bars.",
  "(b)": ": (a): optimal rule computed by CORELS on the adult dataset ((, D) = 0.176) to predict high income (the label 1denotes 50). (b): set of rule lists computed by SamRuLe over 10 runs. SamRuLe identified the optimal rule and slightvariations 1 and 2 that differ in the second rule of the list: they predict a lower outcome using the age ( 1) and the per-weekwork hours features ( 2) with respective loss ( 1, D) = 0.1763 and ( 2, D) = 0.1775. also extremely small (i.e., 1 to 2 orders of magnitude smaller than( , D)), confirming that the conclusions that can be drawn fromthe sample using SamRuLe, e.g., from the estimated loss ( , S), arevery close to the corresponding exact ones. Finally, we comparedthe logical conditions in the rule lists returned by SamRuLe withthe ones in the best rule list computed by CORELS. Our goal is toverify that the insights gained from the approximated predictionmodels from SamRuLe were similar to the optimal ones, i.e., thatSamRuLe allows a qualitative interpretation of the reported rulelist that was stable over the different experimental runs and similarto what obtainable from the exact analysis. reports theoptimal rule list computed by CORELS on the dataset adult (a),and the set of rule lists computed by SamRuLe (b) over all runs.Interestingly, we observe that the approximations from SamRuLeeither match the optimal solution, or are very similar to it. In fact,all rules reported by SamRuLe either share the same conditionsfound in the optimal rule list, or replace one of the feature withalternative reasonable insights (e.g., predicting a lower income foryoung individuals and limited weekly working hours). In general,we found the solutions reported by SamRuLe to be extremely stableand similar to the respective optimal solutions also for all otherdataset.From these observations we conclude that SamRuLe computesextremely accurate rule lists using a fraction of the resources neededby exact approaches, therefore scaling effectively to large datasets.",
  "Comparison to heuristic methods": "In this set of experiments, we compare SamRuLe with two state-of-the-art heuristic methods SBRL and RIPPER. For SamRuLe we fix = 0.025 and = 0.5. We ran SBRL for 104 iterations using defaultparameters (as suggested by ). Also for these experiments,for each dataset we fix = 1 and to the values in .We show the results of these experiments in ..(a) shows the running times for the three methods. Weobserve that in all but one case, RIPPER is the slowest method.On two datasets (ijcnn1 and higgs) we stopped it as it could notcomplete after more than 12 hours, while requiring a very largememory footprint (e.g., more than 400 GB of memory for higgs).Regarding SBRL and SamRuLe, both methods are fast, e.g., requiringalways less than 18 minutes. More precisely, SamRuLe is faster on 3datasets, up to 3 orders of magnitude for the susy dataset. In other2 datasets, the running times of the two methods are comparable,while for phishing and adult SBRL is faster by a factor at most 4. Thisexperiment confirms that SamRuLe is very practical, requiring alower or comparable amount of resources of state-of-the-art scalableheuristics..(b) compares all methods in terms of accuracy. The plotsshow the values of the loss ( , D) for the rule list reportedby all runs of the methods. We may observe that RIPPER is theworst approach, as it always reports rule lists with the highest loss",
  ": Comparison in terms of running time (a) and accuracy (b) between SamRuLe, SBRL, and RIPPER. (c): rule computedby SBRL on adult with loss (, D) = 0.238 over all 10 runs": "(results for ijcnn1 and higgs are not shown as RIPPER could notcomplete in reasonable time, as discussed before), while SamRuLealways provides a rule list with the smallest loss. Regarding SBRL,we observe that it reports a rule list with the same, or almost thesame, loss of SamRuLe on 4 datasets, while it provides suboptimalsolutions for other cases, with losses up to 30% higher than Sam-RuLe. This suggests that, while SBRL scales to large datasets, itoften provides solutions that are sensibly less accurate than theoptimal one. Instead, as discussed previously, SamRuLe outputs arule list with guaranteed gap with the optimal solution, and alwaysvery close to it in practice. We remark that providing a suboptimalsolution may also impact the interpretation for the predictions, e.g.,missing relevant factors for the model. In fact, we report the rulelist computed by SBRL on adult (see .(c)) for all the 10 runs:such rule list does not involve the capital-gain feature, a key condi-tion for the optimal solution to predict high income (.(a)),thus obtaining a higher loss (0.238). In contrast, this feature wasalways found in the rule lists reported by SamRuLe (.(b)),which have loss always very close to the optimal (0.176).Overall, compared to RIPPER and SBRL, SamRuLe outputs anequally accurate solution in less time, or a sensibly better rule listusing comparable resources. We conclude that SamRuLe providesan excelled combination of scalability and high accuracy for rulelist learning from large datasets, achieving a better trade-off thanstate-of-the-art heuristic methods with no theoretical guarantees.",
  "Impact of rule list parameters": "In this final set of experiments, we evaluate the impact to the per-formance of SamRuLe of the rule list search space parameters and. We focus on the datasets mushroom and phishing, as the resultsfor other datasets were similar. We test all values of 1 3 and1 5, fixing = 0.025 and = 0.5, measuring the number ofsamples required by SamRuLe, its running time, and the accuracyof the rule lists provided in output.We show these results in . When increasing and ,the number of samples (R, D) considered by SamRuLe growsfollowing the expect trend (R, D) O(( + log(1/))/(2))proved in our analysis (Theorem 4.8), resulting in sample sizes that are always a small fraction of the size of the dataset. Consequently,the running time of SamRuLe increases roughly linearly with thesample size, remaining practical for all settings. Regarding theaccuracy of the rule lists, we observed the parameter to have thelargest impact on the loss, that remains fairly stable w.r.t. .These results demonstrate that SamRuLe is applicable to complexanalysis involving larger values of and while scaling to largedatasets, that are in most cases out of reach of exact approaches.",
  "Conclusions": "We introduced SamRuLe, a novel and scalable algorithm to findnearly optimal rule lists. SamRuLe uses sampling to scale to largedatasets, and provides rigorous guarantees on the quality of therule lists it reports. Our approach builds on the VC-dimension ofrule lists, for which we proved novel upper and lower bounds. Ourexperimental evaluation shows that SamRuLe enables learninghighly accurate rule lists on large datasets, is up to two orders ofmagnitude faster than state-of-the-art exact approaches, and is asfast as, and sometimes faster than, recent heuristic approaches,while reporting higher quality rule lists.Our work opens several interesting directions for future research,including the use of sampling to scale approaches for learning otherrule-based models while providing rigorous guarantees on the qual-ity of the learned model. Moreover, the efficient computation of ad-vanced data-dependent complexity measures, such as Rademacheraverages , may be useful to obtain even sharper approxi-mation guarantees for our problem. This work was supported by the National Center for HPC, BigData, and Quantum Computing, project CN00000013, and by thePRIN Project n. 2022TS4Y3N - EXPAND: scalable algorithms forEXPloratory Analyses of heterogeneous and dynamic NetworkedData, funded by the Italian Ministry of University and Research(MUR), and by the project BRAINTEASER (Bringing Artificial In-telligence home for a better care of amyotrophic lateral sclerosisand multiple sclerosis), funded by European Unions Horizon 2020(grant agreement No. GA101017598).",
  "J. Ross Quinlan. 1993. C4.5: programs for machine learning. Morgan KaufmannPublishers Inc., San Francisco, CA, USA": "Matteo Riondato and Eli Upfal. 2014. Efficient Discovery of Association Rules andFrequent Itemsets through Sampling with Tight Performance Guarantees. ACMTrans. Knowl. Disc. from Data 8, 4 (2014), 20. Ronald L Rivest. 1987. Learning decision lists. Machine learning 2 (1987), 229246. Cynthia Rudin, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia Semenova, andChudi Zhong. 2022. Interpretable machine learning: Fundamental principles and10 grand challenges. Statistics Surveys 16, none (2022), 1 85.",
  "A.1Proofs of .1": "Proof of Corollary 4.3. Given a dataset D with features, wecreate a new dataset D built as follows. Let = {1 = 1, 2 =1, . . . , = 1} be the set of all possible conditions on the binary features of D; for any non-empty subset with|| , we add to D the binary feature that is equal to 1 forall training instances of D such that is true. Equivalently,the feature values of are obtained from the logical AND of theevaluations of the conditions in . It follows that the total numberof features of D is = =1.We now observe that the set P( R, D) of projections of rule lists in R on D is contained in the set P( R1, D) of projections of rulesin R1 on D, since we can replace each rule with a conjunctionswith terms, with 1 , in any rule list R on D by a rulewith a single condition on one of the features of D, obtaining anequivalent rule list from R1. Therefore, from Corollary 4.2 applied",
  "and the statement follows": "Proof of Theorem 4.4. To show the lower bound, we build adataset that is shattered by the rangeset defined by R1. Let theintegers , such that 1 and = 2 1, and define the binarymatrix {0, 1} such that each column is a distinct binaryvector with at least one element equal to 1, i.e., that the set ofcolumns has cardinality 2 1. Equivalently, the columns of represent all the distinct non-empty subsets of a set of items. Forinstance, for = 3 we have = 7 and 3 as",
  "We observe that the range set R1 shatters D(,). In fact, let anybinary vector {0, 1}, and, for , define {0, 1}": "as a copy of where all the elements with indices ( 1) or +1 are set to 0. Therefore, for all the indices [( 1)+1,]it holds = . Let be the -th row of D(,), for all .Define the function : {0, 1} such that () is the index of the column of D(,) that is equal to . Note that suchcolumn corresponds to one of the columns of the matrix .We prove that there is at least one rule list R1 whose predic-tions [(,1), (,2), . . . , (,)] form a binary vector equalto . We define as follows: let be the subset of indicessuch that if and only if contains at least one entry equalto 1. We define as a rule list of length = | |, composed bythe list [ : ] and the default rule = (, 0). Each rule ,corresponding to the index , is = (, 1) where the condition is defined as follows: we take the condition () = 1, i.e., usingthe column of the dataset D(,) equal to . We observe that, forany binary vector {0, 1}, it holds",
  "such that . Then () log2": "Proof of Theorem 4.5. For , 1, denote with the classof -term monotone -DNF over the features of the dataset. Wenote that, for any formula , we can build an equivalent rulelist that, for any {0, 1}, provides the same predictions. Todo so, let = =1 where and each is a conjunctionwith at most monotone terms. It is easy to observe that the rulelist = [(1, 1), (2, 1), . . . , (, 1), (, 0)] is equivalent to . Thismapping implies that R, therefore () (R).From Lemma A.1, we observe that the VC-dimension of is",
  "(, D) = (, D) ||.(8)": "Note that (, D) and (, S) are the non-regularized variantsof the loss functions (, D) and (, S). It easy to show thatES [(, S)] = (, D) (, D), and that (, S) is an aver-age of binary random variables with expectation (, D). Froman application of the Chernoff bound (Theorem A.2) to the randomvariable = (, S) with ES [] = (, D), we have that, forany 0 < < 1,",
  "holds for all R with probability 1 /2, since ln() + ln 2": "(following analogous derivations for the proof of Corol-lary 4.2). The first set of inequalities is obtained from (10) afteradding || on both sides, from the fact that (, D) (, D),after solving the quadratic inequality + w.r.t. , and afterstraightforward computations.We now prove the last inequality and the statement. Let be an arbitrary rule list with (, D) = minR (, D). Notethat is fixed and independent of the choice of S. We applythe Chernoff bound to the random variable = (, S) withES [] = (, D), obtaining for any 0 < < 1",
  ": Number of samples (R, D) used by SamRuLe varying and for all datasets. is set as in and = 1": "Average deviations Average deviations (susy) SamRuLe ( =0.25)SamRuLe ( =0.5)SamRuLe ( =1.0)CORELS Running time (s) Running time (a9a) Running time (s) Running time (adult) Running time (s) Running time (bank) Running time (s) Running time (higgs) Running time (s) Running time (ijcnn1) Running time (s) Running time (mushroom) Running time (s) Running time (phishing) Running time (s) Running time (susy)",
  ": Average deviations |( , D) (, D)| over all runs for different values of and and for all datasets. The purplehorizontal line is drawn at = (, D) (the loss of the optimal rule list found by CORELS)": "Loss approximation error Average loss approximation error (susy) SamRuLe ( =0.25)SamRuLe ( =0.5)SamRuLe ( =1.0)exact loss Loss approximation error Average loss approximation error (a9a) Loss approximation error Average loss approximation error (adult) Loss approximation error Average loss approximation error (bank) Loss approximation error Average loss approximation error (higgs) Loss approximation error Average loss approximation error (ijcnn1) Loss approximation error Average loss approximation error (mushroom) Loss approximation error Average loss approximation error (phishing) Loss approximation error Average loss approximation error (susy) : Average loss approximation error |( , D) ( , S)| over all runs for different values of and and for all datasets.The purple horizontal line is drawn at = ( , D) (the loss on the dataset for the rule list found by SamRuLe)."
}