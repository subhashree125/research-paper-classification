{
  "ABSTRACT": "Existing domain generalization (DG) methods for cross-persongeneralization tasks often face challenges in capturing intra- andinter-domain style diversity, resulting in domain gaps with the tar-get domain. In this study, we explore a novel perspective to tacklethis problem, a process conceptualized as domain padding. This pro-posal aims to enrich the domain diversity by synthesizing intra- andinter-domain style data while maintaining robustness to class labels.We instantiate this concept using a conditional diffusion model andintroduce a style-fused sampling strategy to enhance data genera-tion diversity. In contrast to traditional condition-guided sampling,our style-fused sampling strategy allows for the flexible use of oneor more random styles to guide data synthesis. This feature presentsa notable advancement: it allows for the maximum utilization ofpossible permutations and combinations among existing styles togenerate a broad spectrum of new style instances. Empirical evalu-ations on a broad range of datasets demonstrate that our generateddata achieves remarkable diversity within the domain space. Bothintra- and inter-domain generated data have proven to be signifi-cant and valuable, contributing to varying degrees of performanceenhancements. Notably, our approach outperforms state-of-the-artDG methods in all human activity recognition tasks.",
  "Corresponding author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08...$15.00",
  "Human Activity Recognition; Domain Generalization; DiffusionModel; Domain Padding": "ACM Reference Format:Junru Zhang, Lang Feng, Zhidan Liu, Yuhan Wu, Yang He, Yabo Dong,and Duanqing Xu. 2024. Diverse Intra- and Inter-Domain Activity StyleFusion for Cross-Person Generalization in Activity Recognition. In Proceed-ings of the 30th ACM SIGKDD Conference on Knowledge Discovery and DataMining (KDD 24), August 2529, 2024, Barcelona, Spain. ACM, New York,NY, USA, 14 pages.",
  "INTRODUCTION": "Human activity recognition (HAR) is a crucial application of timeseries data collected from wearable devices like smartphones andsmartwatches, garnering substantial attention in recent years . Deep learning (DL) techniques have proven effective in timeseries classification (TSC) for HAR tasks . However, acommon assumption underpinning these models is that training andtest data distributions are identically and independently distributed(i.i.d.) , a condition that does not often hold up in real life due toindividual differences in activity styles influenced by factors such asage and gender . For instance, sensor data distributions candiverge significantly between younger and older individuals due tovariations in walking speed and frequency, leading to challenges inachieving cross-person generalization with standard DL models.Domain generalization (DG) seeks to address this issue .Approaches such as domain-invariant anddomain-specific methods are designed to extract ro-bust inter-domain and intra-domain features that can withstanddata distribution shifts across various domains. However, their ef-fectiveness is reliant on the diversity and breadth of the trainingdata . The challenge arises in HAR tasks, where the collectedtraining data is often small-scale and lacks the necessary diversitydue to resource constraints on edge devices . This inherentdiversity scarcity in source domain training data can lead to overfit-ting to local and narrow inter- or intra-domain features, resultingin poor generalization to new, unseen domains. As shown in",
  "Padding": ": T-SNE visualization of time-series features extractedby various methods across three domains in HAR. Existingrepresentation learning methods result in domain gaps as inboth (a) and (b), covering a small portion of target domain(red circles). Standard data augmentation (DA) leads to aug-mented data (stars), with source domains (orange/blue circles)remaining in close proximity to each other and failing to fillgaps. Our method (d) creates a comprehensive feature spaceby padding domain gaps via the idea of (e). (a) and (b), the learned features lack required intra- or inter-domainfeature robustness, thereby impeding their generalization to targetdomains (red circles). One promising solution is to enrich trainingdistributions by data generation. Recent research has focusedon enhancing training data richness through standard data aug-mentation like rotation and scaling; however, it primarily enhancesintra-domain diversity and falls short of addressing inter-domainvariability. As shown in (c), the augmented data (stars) forsource domains (orange and blue circles) tends to cluster tightly,yet fails to generate the necessary inter-domain data. The targetdomain (red circles) thus cannot be comprehensively represented.In this work, we focus on the generation of highly diverse datadistributions to address the issue of limited domain diversity inHAR. We explore a novel perspective to tackle this problem. Asdepicted in (d), the core idea involves enabling synthetic data(stars) to fill the empty spaces within and across source domainswhile maintaining robustness to class labels, a process we concep-tualize as domain padding. For instance, as illustrated in (e),we can combine multiple walking styles of an elderly man and ayoung man to create a novel inter-domain style or merge multiplewalking styles of a young man to generate a new intra-domain style.Compared to existing DG methods, our domain padding holds greatpotential to generate a more extensive range of unknown style dis-tributions. This enables TSC models to comprehensively explore awide array of intra- and inter-domain variations, contributing toenhanced generalization in HAR scenarios.We instantiate our concept using conditional diffusion proba-bilistic models . To generate samples with instance-leveldiversity, we first design a contrastive learning pipeline . It aimsto extract the activity style representations of the available data inthe source domains while preserving their robustness for classifica-tion tasks. The resulting style representation, denoted as , can beinterpreted as a [class] activity performed in [] style. We then pro-pose a novel style-fused sampling strategy for the diffusion model to achieve domain padding requirements. This involves randomlycombining one or multiple style representations of training sampleswithin the same class. Styles in each combination are then utilizedto jointly guide the diffusion to generate novel activity samples thatfuse the styles. This innovation presents a notable advancement:the randomness of the combination (whether originating from dif-ferent or the same domains) ensures diversity in both inter-domainand intra-domain, thereby achieving the domain padding, as shownin (d) and (e). Moreover, it allows for the maximum utilizationof possible permutations and combinations among existing stylesto generate a broad spectrum of new style instances. Hence, weterm our approach as Diversified Intra- and Inter-domain distribu-tions via activity Style-fused Diffusion modeling (DI2SDiff). Wesummarize our main contributions as follows: We explore a pivotal challenge hampering the effectiveness ofcurrent DG methods in HAR: diversity scarcity of source do-main features. In response, we introduce the concept of domainpadding, offering a fresh perspective for enhancing domain di-versity and ultimately improving DG models performance.",
  "We propose to use activity style features as conditions to guidethe diffusion process, extending the information available at theinstance-level beyond mere class labels": "We propose a novel style-fused sampling strategy, which canflexibly fuse one or more style conditions to generate new, unseensamples. This strategy achieves data synthesis diversity bothwithin and across domains, enabling DI2SDiff to instantiate theconcept of domain padding. We conduct extensive empirical evaluations of DI2SDiff across aboard of HAR tasks. Our findings reveal that it markedly diversi-fies the intra- and inter-domain distribution without introducingclass label noise. Leveraging these high-quality samples, DI2SDiffoutperforms existing solutions, achieving state-of-the-art resultsacross all cross-person activity recognition tasks.",
  "RELATED WORK": "Human activity recognition (HAR) uses wearable sensors forrecognizing activities in healthcare and human-computer interac-tion . The complexity of daily activities, varying among indi-viduals with different personal styles, makes recognition challeng-ing . With the rise of deep learning, deep neural networkshave been increasingly utilized to extract informative features fromactivity signals . For example, DeepConvLSTM in-corporates convolutional and LSTM units for multimodal wearablesensors . MultitaskLSTM extracts features using shared weights,then classifies activities and estimates intensity separately .Domain generalization (DG) aims to improve model perfor-mance across different domains. Early works focused on utilizing multiple source domains and enforcing do-main alignment constraints to extract robust features. For example,DANN employed adversarial training to accomplish this task,but it requires target data during training. To recall more beneficialfeatures, several methods such as mDSDI havebeen proposed to preserve domain-specific features. Another lineof research in DG focuses on data augmentation techniques to explore more robust patterns for improvedgeneralization, such as generating adversarial examples .",
  "Diverse Intra- and Inter-Domain Activity Style Fusion for Cross-Person Generalization in Activity RecognitionKDD 24, August 2529, 2024, Barcelona, Spain": "Given the practical significance of DG learning for HAR tasks,researchers have turned their focus to studyingDG problems in this field. For instance, Wilson et al. proposedan adversarial approach to learn domain-invariant features, whichrequires labeled data in the target domain during training. Qianet al. improved variational autoencoder (VAE) framework to disentangle domain-agnostic and domain-specific features auto-matically, but domain labels are required. DDLearn is a recentadvanced approach that enriches feature diversity by contrastingaugmented views but is limited to standard augmentation tech-niques that only enrich intra-domain features.Diffusion models have showcased their remarkable potentialin generating diverse and high-quality samples in various domains,like computer vision , natural language processing , anddecision-making . Furthermore, classifier-free guidance models have achieved impressive outcomes in multimodal modeling,with wide applications in tasks such as text-to-image synthesis and text-to-motion . Considering the potential non-stationarydistribution of time-series data , we propose harnessing thepower of diffusion models to generate diverse data in HAR tasks,and thereby enhancing the models generalization ability. Intrigu-ingly, diffusion models have received limited attention in HARtasks. A recent survey on time-series diffusion models indi-cates that although some successful attempts have been made toapply diffusion models to time-series tasks like interpolation and forecasting , comprehensive investigations in time-seriesgeneration tasks are still lacking. Our study not only establishesdiffusion models for time-series generation but also guides the dif-fusion model to produce diverse samples, effectively addressing thechallenges of DG in HAR tasks. Our work thus presents a noveland challenging contribution to the field.",
  "PRELIMINARIES3.1Problem Statement": "In cross-person activity recognition , a domain is characterizedby a joint probability distribution , across the product spaceof time-series instances X, and the corresponding label space Y.Each instance X R represents the values of each time seriesobtained from sensors, where is the dimensionality of features,and is the temporal length of the series. Moreover, each instanceX corresponds to an activity class label {1, 2, . . . ,}, indicat-ing the specific activity category performed by the subjects, with denoting the total number of activity categories. The domain gen-eralization challenge lies in the nonintersection and domain differ-ences between the training and testing sets. Typically, the trainingset = {(X,)}=1 is collected from the labeled source-domainsubjects, where represents the number of training instances.Importantly, is often small in cross-person activity recognitionscenarios, presenting the small-scale challenge. On the other hand,the test set = {(X,)}=1 consists of instances obtainedfrom the unseen target-domain subjects and satisfies the condi-tion = . In addition, source and target domains havedifferent joint probability distributions while sharing the identi-cal feature space and class label space, i.e., (X,) (X,),and X = X, Y = Y. The primary objective is to leverage theavailable data in to train a TSC model : X Y capable of effectively generalizing to an inaccessible, unseen test domain ,without any prior exposure to target domain data or domain labelsduring training. This task is inherently more challenging than con-ventional transfer learning settings due to the disparatedistributions across source and target domains, compounded bythe small-scale settings of the training data.",
  "Diffusion Probabilistic Model": "Diffusion model involves training a model distribution ()to closely approximate the target ground-truth data distribution(). It assumes distribution () as a Markov chain of Gauss-ian transitions: (0) = ( ) =1 (1|)1:, where1, . . . , denote the latent variables with the same dimensionalityas original (noiseless) data 0. ( ) N (0, I) is the Gaussianprior. (1|) is the trainable reverse process given by",
  "L() := E0,U,N(0,I)|| (,)||2,(3)": "where U is the uniform distribution and the noise predictor (,),parameterized with a deep neural network, aims to estimate thenoise at time given . As (,) is determined by (,),the target (1|) can be consequently derived.Sampling procedure. Given a well-trained , the data gen-eration procedure begins with a Gaussian noise N (0, I)and proceeds by iteratively denoising for = , . . . , 1 through (1|), culminating in the generation of the new data 0.",
  "DOMAIN PADDING": "A major obstacle to achieving domain generalization in HAR tasksis the limited data diversity of the source domain. This presentsrepresentation learning methods from extracting robust featuresto distribution shifts across domains. Moreover, data augmenta-tion also shows insufficient data richness within the domain space,particularly the inter-domain distribution.In response, our work aims to achieve highly diverse data gen-eration to enrich the training distributions. We propose a novelperspective, which we refer to as domain padding. The core idea isto achieve the richness of the domain space by padding the distri-butional gaps within and between source domains, as demonstratedin (d). To ensure the generation of high-quality, diverse datathat can effectively augment the training datasets for HAR models,domain padding adheres to two key criteria:",
  "KDD 24, August 2529, 2024, Barcelona, SpainJunru Zhang, et al": "the channel dimension, resulting in a tensor that is used as thecondition for each residual block in the UNet.Training details. Our diffusion training settings primarily ad-here to the guidelines outlined in . The batch size is 64, witha learning rate of 2 104 using the Adam optimizer. The diffusionstep is set to = 100. We choose the probability of dropping theconditioning information to be 0.5.",
  "METHODOLOGY": "We implement domain padding using conditional diffusion mod-els given their highly-expressive generative capabilities .The iterative denoising process of diffusion models makes themexceptionally suited for flexible conditioning mechanisms. In thisframework, given the original dataset , we generate a new sam-ple 0 X using conditional information Xcond to guide thegeneration process. The ensemble of all generated data constitutesa synthetic dataset, denoted as = {( Xi,)} =1, with denotingthe total count of generated samples. Next, we use 0 to denote anexample of the synthetic samples. The generation objective is toestimate the conditional data distribution ( |). This allows us togenerate a synthetic sample 0 given a specific constraint . Theconditional diffusion process can be described by:",
  "( | 1,), ( 1| ,).(4)": "Sequentially performing enables the generation of new samplesto capture the attributes of . However, realizing domain paddingis not a trivial task due to a key aspect: how to guide the diffusionmodel to generate diverse activity samples meeting two criteria ofdomain padding.In the following, we introduce the DI2SDiff framework, designedto enable diffusion to achieve domain padding. In 5.1, we presenta contrastive learning pipeline that extracts style features to serveas conditions for the diffusion model. Given a style condition, weemploy classifier-free guidance to generate new samples thatmeet the first criterion in 5.2. For the second criterion, we constructa diverse style combination space for the condition space Xcond",
  "Activity Style Condition": "Conditional diffusion models are typically guided by label or textprompts that provide task-specific knowledge, such as create a[cartoonish] [cat] image . However, the generation ofinstance-level time-series data introduces distinct challenges. It isdifficult to capture the complex patterns solely through label or textprompts due to the inherently high-dimensional and non-stationarynature . To address this issue, we propose the development of astyle conditioner using a contrastive learning approach . This ap-proach has demonstrated robustness in extracting representationsfrom unlabeled time-series data. The transformed data can retainthe distinctive characteristics of the original data while preserving the semantic information of the classes. Thus, it is well-suited forextracting robust instance-level representation, termed as style,which can serve as conditions to guide diffusion models.Delving into specifics, the contrastive learning pipeline consistsof a feature encoder and Transformer on the available trainingdata. The objective is to maximize the similarity between differentcontexts of the same sample and minimize the similarity betweencontexts of different samples. Once the module is trained, we utilizeit as style conditioner denoted as style. When extracting the stylefrom the original data X, the style conditioner produces a stylevector = style(X) R, where denotes the length of thevector. Consequently, each activity style condition can be inter-preted as a [] activity performed in [] style, where denotesthe class of the original data. This approach takes an importantstep towards the first criteria of domain padding due to the preser-vation of class semantics. The aggregation of all context vectorsfrom training instances constitutes a set S = {}=1, which canbe further divided into class-specific subsets corresponding to classes. Each subset contains style vectors pertaining to a specificclass, expressed as S = {S1 S2 S}. In Appendix A, weprovide the details of the contrastive learning approach .",
  "Synthesizing with Classifier-Free Guidance": "To control the generation of time-series samples, we can leveragethe style in S to guide the conditional sampling process ( 1| ,)presented in Eq. (4). To this end, we adopt the classifier-free guid-ance , which has proven to be effective in generating data withspecific characteristics. In this framework, the training process ismodified to learn a conditional ( ,,) and an unconditional ( ,, ), where symbolizes the absence of the condition . Theloss function is formulated as follows:",
  "L() := E0X,N(0,I ),U,S ( ,,)2,(5)": "where condition is one style feature in S derived from the pre-trained conditioner, and it is randomly dropped during the training.During the sampling phase, a sequence of samples , . . . , 0is generated starting from N (0, I). For each timestep , themodel refines the process of denoising 1 based on throughthe following operation:",
  "= ( ,, ) + ( ,,) ( ,, ),(6)": "where is a scalar hyperparameter that controls alignment betweenthe guidance signal and the sample . Through the iterativeapplication of Eq. (6), the diffusion model is capable of samplingnew time-series samples that conform to specific styles S. It isworth noting that the styles 1, . . . , are robust to the class labels,the generation process guarantees the first criterion of domainpadding: each generated sample belongs to a known class underthe guidance of a single condition.",
  "Pad the gap": ": Illustration of the diffusion within DI2SDiff. It contains a style conditioner to produce styles and a conditionaldiffusion for data generation. Suppose we have three original walking samples: X1, X2, and X3, where X1 is from a differentdomain while X2 and X3 come from the same domain. (a) The style conditioner generates style features from the original data.The style features are randomly combined to build the condition space, in which the combination of inter-domain styles isindicated by blue brackets and the combination of intra-domain styles is indicated by grey brackets. (b) During training, thediffusion retrieves each data sample with one style for the forward process. (c) During sampling, the diffusion receives noiseand a style combination, e.g., , for the reverse process. (d) The generated sample X is used to diversify the data space. diffusion to generate new data that satisfy any number and combi-nation of styles (conditions), rather than just one. By doing so, thegenerated data fuse diverse inter and intra-domain styles, effectivelymeeting the second criterion of domain padding.Random style combination. Random style combination entailsthe ensemble of one or multiple style features under a unified classto establish a new diffusion sampling condition. Importantly, theensemble styles must belong to the same class to preserve classconsistency. For each class label , we randomly select any numberof style features from the class-specific set S and combine themin all possible ways. This will end up with 2 1 different stylecombinations (excluding the empty set), where = |S | is thenumber of styles in S. Mathematically, the collection of all possiblestyle combinations for class can be expressed by the power setP(S) of S:",
  "P(S) = {D |D S, D }.(7)": "For instance in (c), three styles in S = {1,2,3} resultsin 7 different combinations1. This operation is replicated acrossall classes 1, . . . ,, integrating them into a comprehensive stylecombination set D = {P(S1) P(S)}. The randomnessin selecting style combinations can significantly foster diversitywithin and between domains, and maximize the exploitation ofexisting styles to generate highly diverse condition space Xcond.Style-fused sampling. Subsequently, our efforts are directedtowards empowering the diffusion model to fuse multiple styles dur-ing the data generation conditioned on a specific style combinationD D. Assuming the diffusion has learned the data distributions{ ( ,,)}=1 through Eq. (5), sampling from the composed datadistribution ( 0|D) for any given style combination D D is",
  "( ,,) ( ,, ).(8)": "The derivation of Eq. (8) is provided in Appendix B. This indicatesthat while the diffusion training process primarily focuses on anindividual style, we can flexibly combine these styles during sam-pling. For instance, consider the combination of D = {1,3} in (c) and (d). Each element represents a style associated withthe [walking] activity. Eq. (8) can generate new samples with the[walking] label possesses unique characteristics that fuse thesetwo styles. This is critical for inter and intra-domain diversity indomain padding: diffusion can flexibly incorporate class-specificinstance-level styles from different or the same domains to gener-ate new samples with novel domain distribution. Moreover, giventhe existence of sub-domains within each domain, our diffusionmodel is capable of synthesizing novel domains, even from sam-pling instances within the same domain (we verify this later in theexperiments).",
  "Workflow of DI2SDiff": "Finally, we elaborate on the comprehensive workflow of our ap-proach, which we refer to as Diversified Intra- and Inter-domain dis-tributions via activity Style-fused Diffusion modeling (DI2SDiff).Architectural design. The diffsuion model :X N Xcond X is built upon a UNet architecture with repeatedconvolutional residual blocks. To accommodate the characteristicsof time series input, we adapt 2D convolution to 1D temporal convo-lution. The model incorporates a timestep embedding module anda condition embedding module, each of which is a multi-layer per-ceptron (MLP). The condition embedding module is used to encodeeach activity style S, and in the unconditional case = , we",
  "EXPERIMENTS": "In this section, we conduct a comprehensive evaluation of DI2SDiffacross various cross-person activity recognition tasks to demon-strate (1) its ability to achieve domain padding and significantlydiversify the domain space; (2) its outstanding performance in do-main generalization; (3) a detailed ablation and sensitivity analysis;and (4) its versatility in boosting existing DG baselines.",
  "Experimental Setup": "Datasets. We assess our method on three widely used HAR datasets:UCI Daily and Sports Dataset (DSADS) , PAMAP2 dataset and USC-HAD dataset . We follow the same experimental set-tings in that provided a generalizable cross-person scenario.Specifically, the subjects are organized into separate groups forleave-one-out validation. We assign the data of one group as thetarget domain and utilize the remaining subjects data as the sourcedomain. Each subject is treated as an independent task.Baselines. We compare our approach with a wide range ofclosely related, strong baselines adapted to TSC tasks. We first selectMixup , RSC , SimCLR , Fish , and DDLearn , given",
  "(a) DSADS(b) PAMAP2(c) USC-HAD": ": T-SNE visualization of DSADS, PAMAP2, and USC-HAR datasets. Each method generates the same amount ofsynthetic data. Each domain category is represented by acolor, and the target domain is represented by a red dot. Theoriginal and synthetic data are represented by shapes dotsand crosses, respectively. Best viewed in color and zoom in. their outstanding performance in most recent study . Notably,DDLearn is ranked as the top-performing method. We alsoinclude TS-TCC for its remarkable generalization performancein self-supervised learning. Additionally, we incorporate DANN and mDSDI , which are designed to address domain-invariant anddomain-specific feature learning, respectively. In our analysis, thestandard data augmentation (DA) techniques are identical tothose employed in , such as scaling and jittering.Architecture and implementation. For fairness, we adopt thesame feature extractor as described in , which consists of twoblocks for DSADS and PAMAP2, and three blocks for USC-HAD.Each block includes a convolution layer, a pooling layer, and abatch normalization layer. All baselines, except TS-TCC , employ",
  "USC-HAD": "20%67.2270.1767.2267.4259.0662.1574.2577.3684.0040%75.3077.3169.1673.5461.5268.8575.3280.7284.9760%78.1477.5971.3876.0968.7176.7577.8480.8887.5380%79.7678.6571.9977.2168.5277.7278.9182.4989.25100%81.2779.4172.1478.9272.0578.5979.1582.5191.13Avg76.3476.6270.3874.6465.9772.8177.0980.8087.38 DDLearn , which utilizes data augmentation, underscoring theimportance of training data diversity in enhancing generalizationin HAR. In contrast, our DI2SDiff, leveraging advanced synthesisof highly diverse data across both intra- and inter-domain space,markedly surpasses all baseline methods in every task. In addition,we observe that all baselines, including DDLearn, demonstrate poorperformance on the USC-HAD dataset. As we discussed in (c), this decline is due to the presence of sub-domains within thesource domain, which poses a highly challenging problem in DG.Nevertheless, DI2SDiff adeptly addresses this issue by integratinginstance-level style fusion, thereby synthesizing new data distribu-tions between the sub-domains. As a result, our approach achievesoutstanding performance, outperforming the second-best methodby a clear margin (6.64%) in USC-HAD.Data proportion analysis. In Tab. 2, we assess DI2SDiffs per-formance over a range of data volumes by adjusting the propor-tion of training data from 20% to 100%. The results demonstrateDI2SDiffs consistent superiority over the baseline methods acrossvarious proportions of training data. This highlights the abilityof our approach to efficiently generate informative samples fromvarying amounts of available data and effectively learn from them.As the size of the training sample increases, the advantage of ourmethod becomes more pronounced. For instance, as we increasethe size from 20% to 100% of USC-HAD, the accuracy improvementgrows from 6.64% to 8.62% compared to the second-best baseline(DDLearn). This is because the number of style combinations in-creases exponentially (2 1) with larger training data volumes,as shown in Eq. (7). Hence, enlarging the training dataset can pro-vide significant diversity enhancement of data synthesis, leadingto more substantial gains in the models generalization ability.",
  "Domain Padding and Diversity Evaluation": "In this part, we demonstrate whether our approach can effectivelydiversify the domain space and generate diverse samples that meetdomain padding criteria. To this end, we adopt T-SNE to visual-ize the latent feature space in terms of class and domain dimensions.(1) Class-Preserved Generation. Firstly, we evaluate the classconsistency of synthetic data, i.e., the first criterion of domainpadding. We employ a class feature extractor, trained with classlabels, to map both original and synthetic data into a class-specificlatent space. The results of single-condition guidance (|D | = 1)and multiple-condition guidance (|D | > 1) are shown in .It can be observed that all synthetic samples (crosses) are closelyclustered around their corresponding original instances and classes(dots). This clustering indicates that our method effectively main-tains class information, avoiding the introduction of class noise; im-portantly, this holds true under both single and multiple-conditionguidance. Moreover, the use of multiple-condition guidances ap-pears to enhance class discriminability more than single-conditionguidance in . This enhancement is likely because more guid-ance signals provide more robust class semantics (akin to ensemblelearning), therefore resulting in a better class alignment.(2) Intra- and Inter-Domain Diversity. Next, we evaluatethe intra- and inter-domain diversity of the synthetic data, i.e., thesecond criterion of domain padding. We train the domain featureextractor on source domains with domain labels. We then mapsource and target data into a domain-specific latent space, andcompare the synthetic data from the standard DA method, single-condition guidance (|D | = 1), and multiple-condition guidance(|D | > 1). The results are shown in .The findings reveal that the standard DA method generatestightly clustered samples (crosses) around the original data (dots),falling short of diversifying the domain space, particularly the inter-domain space. Our single-condition guidance method offers a par-tial solution and generates sparse data between different domains thanks to diffusions probabilistic nature. However, relying solelyon a single-style guidance approach has limitations for domainpadding. The introduction of our style combinations in Eq. (7)makes a substantial improvement: the multiple-condition guidanceexcels in padding the distributional gaps both within and betweensource domains, as demonstrated in .Moreover, as indicated in , through multiple-conditionguidance, the synthetic samples (crosses) closely resemble the targetdomain data (red dots) and demonstrate less dependence on thespecific characteristics of source domains. This demonstrates thatthe fusion of multiple style features creates a new style. This isof importance for the domain generalization in TSC. Given thesignificant differences in individual styles and the small-scale natureof source domain data, our style-fused sampling demonstrates greatpotential to simulate various new and unseen distributions, fromwhich the TSC model can better adapt to the target domains.In addition, we can observe in (c) that the USC-HADdataset presents an additional challenge of intra-domain gaps dueto its fragmented and sparsely distributed source domains withdistinct sub-domains. These gaps contribute to an increased distri-bution shift, posing difficulties for existing DG methods to performeffectively (We show their results in Tab. 1 in later). Through ran-dom instance-level style fusion, our approach effectively addressesthis sub-domain challenge, enabling the synthesis of new data dis-tribution within sub-domains. As a result, our method can yieldexceptional performance on the complex tasks like USC-HAD.",
  "Generalization Performance": "Now we conduct a series of experiments to evaluate the generaliza-tion performance of DI2SDiff against other strong DG baselines.Overall performance. Tab. 1 presents a comparative analysisof the classification accuracies achieved by all DG methods acrossthree datasets, each task of which comprises 20% of the trainingdata. As we can see, representation learning baselines that focussolely on learning domain-invariant features, such as DANN ,exhibit suboptimal performance due to the limited diversity of thetraining data in HAR. The method mDSDI , on the other hand,achieves improved performance by additionally learning domain-specific features. However, it does not match the performance of",
  "Ablation and Sensitivity Analysis": "In this section, we perform an ablation study that focuses on themain step of DI2SDiff, i.e., generating diverse time-series data viadiffusion for data augmentation. We keep the number of syntheticsamples and the training strategy of TSC models the same forall variants. Additionally, we conduct a sensitivity analysis thatfocuses on its two hyperparameters:, which controls the maximum",
  ": Hyperparameter sensitivity analysis on and": "number of style features in each style combination, and , whichcontrols the volume of synthetic data.Ablation on diffusion model. Our findings, as presented in, indicate that the standard DA method and class label guid-ance2 falter in performance. The failure of the class label guidancesampling suggests that using class labels alone, without instance-related information, cannot generate high-quality data. In contrast,the diffusion model that utilizes a single style feature as a conditionachieves better performance, suggesting that leveraging represen-tation features for instance-specific sampling can boost the qualityof generated data. Moreover, the incorporation of style-fused sam-pling can further improve generalization by producing sampleswith distinct features.Hyperparameter sensitive analysis. We analyze the sensi-tivity of our hyperparameters by varying one parameter whilemaintaining the others constant. As shown in , increasing thecomplexity of style combinations () and the volume of syntheticdata () generally leads to performance improvement. It becomesnon-sensitive when the value is too large. We find that an valueof 5 for the DSADS and PAMAP2 and an value of 10 for the USC-HAD sufficiently ensure a diverse range of styles. values of 1 or 2strike an effective balance between accuracy and training overheadfor all three datasets. By flexibly tuning these hyperparameters, wecan achieve even greater performance improvements for the TSCmodel across various tasks while meeting specific needs.",
  "Benifits to other DG baselines": "We demonstrate the versatility of our approach in boosting theperformance of existing DG baselines. The results are shown in . By incorporating our synthetic data into the training datasetsof baselines, we consistently observe performance improvementsacross the board, including DANN , mDSDI 3, and DDLearn4. This demonstrates the versatility of integrating our method 2This involves directly using the class labels, rather than the style features, as thecondition to guide diffusion.3In mDSDI, our synthetic data is treated as a new domain.4In DDLearn, our synthetic data is treated as a new augmentation method.",
  "CONCLUSION": "In this paper, we tackle the key issue of DG in cross-person ac-tivity recognition, i.e., the limited diversity in source domain. Weintroduce a novel concept called domain padding and proposeDI2SDiff to realize this concept. Our approach generates highlydiverse inter- and intra-domain data distributions by utilizing ran-dom style fusion. Through extensive experimental analyses, wedemonstrate that our generated samples effectively pad domaingaps. By leveraging these new samples, our DI2SDiff outperformsadvanced DG methods in all HAR tasks. A notable advantage ofour work is its efficient generation of diverse data from a limitednumber of labeled samples. This potential enables DI2SDiff to pro-vide data-driven solutions to various models, thereby reducing thedependence on costly human data collection.",
  "Andreas Bulling, Ulf Blanke, and Bernt Schiele. 2014. A tutorial on human activityrecognition using body-worn inertial sensors. ACM Computing Surveys (CSUR)46, 3 (2014), 133": "Kaixuan Chen, Dalin Zhang, Lina Yao, Bin Guo, Zhiwen Yu, and Yunhao Liu.2021. Deep learning for sensor-based human activity recognition: Overview,challenges, and opportunities. ACM Computing Surveys (CSUR) 54, 4 (2021), 140. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. Asimple framework for contrastive learning of visual representations. In Interna-tional conference on machine learning. PMLR, 15971607. Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, Chee KeongKwoh, Xiaoli Li, and Cuntai Guan. 2021. Time-series representation learning viatemporal and contextual contrasting. arXiv preprint arXiv:2106.14112 (2021). Sarah Erfani, Mahsa Baktashmotlagh, Masud Moshtaghi, Xuan Nguyen, Christo-pher Leckie, James Bailey, and Rao Kotagiri. 2016. Robust domain generali-sation by enforcing distribution invariance. In Proceedings of the Twenty-FifthInternational Joint Conference on Artificial Intelligence (IJCAI-16). AAAI Press,14551461.",
  "Lang Feng, Pengjie Gu, Bo An, and Gang Pan. 2024. Resisting Stochastic Risksin Diffusion Planners with the Trajectory Aggregation Tree. arXiv preprintarXiv:2405.17879 (2024)": "Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, HugoLarochelle, Franois Laviolette, Mario Marchand, and Victor Lempitsky. 2016.Domain-adversarial training of neural networks. The journal of machine learningresearch 17, 1 (2016), 20962030. Rui Gong, Wen Li, Yuhua Chen, and Luc Van Gool. 2019. Dlow: Domain flowfor adaptation and generalization. In Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition. 24772486.",
  "Jonathan Ho and Tim Salimans. 2022. Classifier-free diffusion guidance. arXivpreprint arXiv:2207.12598 (2022)": "Biwei Huang, Kun Zhang, Jiji Zhang, Joseph Ramsey, Ruben Sanchez-Romero,Clark Glymour, and Bernhard Schlkopf. 2020. Causal discovery from heteroge-neous/nonstationary data. The Journal of Machine Learning Research 21, 1 (2020),34823534. Zeyi Huang, Haohan Wang, Eric P Xing, and Dong Huang. 2020. Self-challengingimproves cross-domain generalization. In Computer VisionECCV 2020: 16thEuropean Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part II 16.Springer, 124140. Wenchao Jiang and Zhaozheng Yin. 2015. Human activity recognition usingwearable sensors by deep convolutional neural networks. In Proceedings of the23rd ACM international conference on Multimedia. 13071310.",
  "Oscar D Lara and Miguel A Labrador. 2012. A survey on human activity recog-nition using wearable sensors. IEEE communications surveys & tutorials 15, 3(2012), 11921209": "Lei Li, Ke Gao, Juan Cao, Ziyao Huang, Yepeng Weng, Xiaoyue Mi, ZhengzeYu, Xiaoya Li, and Boyang Xia. 2021. Progressive domain expansion networkfor single domain generalization. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition. 224233. Xiang Li, John Thickstun, Ishaan Gulrajani, Percy S Liang, and Tatsunori BHashimoto. 2022. Diffusion-lm improves controllable text generation. In Advancesin Neural Information Processing Systems, Vol. 35. 43284343.",
  "Lequan Lin, Zhengkun Li, Ruikun Li, Xuliang Li, and Junbin Gao. 2023. Diffusionmodels for time-series applications: a survey. Frontiers of Information Technology& Electronic Engineering (2023), 123": "Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B Tenenbaum. 2022.Compositional visual generation with composable diffusion models. In EuropeanConference on Computer Vision. Springer, 423439. Wang Lu, Jindong Wang, Xinwei Sun, Yiqiang Chen, and Xing Xie. 2022. Out-of-distribution Representation Learning for Time Series Classification. In TheEleventh International Conference on Learning Representations. Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte,and Luc Van Gool. 2022. Repaint: Inpainting using denoising diffusion proba-bilistic models. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition. 1146111471.",
  "Calvin Luo. 2022. Understanding diffusion models: A unified perspective. arXivpreprint arXiv:2208.11970 (2022)": "Massimiliano Mancini, Samuel Rota Bulo, Barbara Caputo, and Elisa Ricci. 2018.Best sources forward: domain generalization through source-specific nets. In 201825th IEEE international conference on image processing (ICIP). IEEE, 13531357. Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien HuuNguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, and Dan Roth. 2023. Recentadvances in natural language processing via large pre-trained language models:A survey. Comput. Surveys 56, 2 (2023), 140.",
  "Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.Journal of machine learning research 9, 11 (2008)": "Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John C Duchi, Vittorio Murino,and Silvio Savarese. 2018. Generalizing to unseen domains via adversarial dataaugmentation. Advances in neural information processing systems 31 (2018). Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu,Yiqiang Chen, Wenjun Zeng, and Philip Yu. 2022. Generalizing to unseen domains:A survey on domain generalization. IEEE Transactions on Knowledge and DataEngineering (2022). Shujun Wang, Lequan Yu, Kang Li, Xin Yang, Chi-Wing Fu, and Pheng-Ann Heng.2020. Dofe: Domain-oriented feature embedding for generalizable fundus imagesegmentation on unseen datasets. IEEE Transactions on Medical Imaging 39, 12(2020), 42374248. Yucheng Wang, Yuecong Xu, Jianfei Yang, Zhenghua Chen, Min Wu, Xiaoli Li,and Lihua Xie. 2023. Sensor alignment for multivariate time-series unsuperviseddomain adaptation. In Proceedings of the AAAI Conference on Artificial Intelligence,Vol. 37. 1025310261.",
  "Huatao Xu, Pengfei Zhou, Rui Tan, and Mo Li. 2023. Practically Adopting HumanActivity Recognition. In Proceedings of the 29th Annual International Conferenceon Mobile Computing and Networking. 115": "Huatao Xu, Pengfei Zhou, Rui Tan, Mo Li, and Guobin Shen. 2021. Limu-bert:Unleashing the potential of unlabeled data for imu sensing applications. InProceedings of the 19th ACM Conference on Embedded Networked Sensor Systems.220233. Keyulu Xu, Mozhi Zhang, Jingling Li, Simon S Du, Ken-ichi Kawarabayashi, andStefanie Jegelka. 2020. How neural networks extrapolate: From feedforward tograph neural networks. arXiv preprint arXiv:2009.11848 (2020). Minghao Xu, Jian Zhang, Bingbing Ni, Teng Li, Chengjie Wang, Qi Tian, andWenjun Zhang. 2020. Adversarial domain adaptation with domain mixup. InProceedings of the AAAI conference on artificial intelligence, Vol. 34. 65026509.",
  "Zhenlin Xu, Deyi Liu, Junlin Yang, Colin Raffel, and Marc Niethammer. 2020.Robust and generalizable visual representation learning via random convolutions.arXiv preprint arXiv:2007.13003 (2020)": "Jianbo Yang, Minh Nhut Nguyen, Phyo Phyo San, Xiaoli Li, and Shonali Kr-ishnaswamy. 2015. Deep convolutional neural networks on multichannel timeseries for human activity recognition.. In Ijcai, Vol. 15. Buenos Aires, Argentina,39954001. Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao,Wentao Zhang, Bin Cui, and Ming-Hsuan Yang. 2023. Diffusion models: Acomprehensive survey of methods and applications. Comput. Surveys 56, 4 (2023),139. Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun Chen, Liunian Li,Xiyang Dai, Lijuan Wang, Lu Yuan, Jenq-Neng Hwang, and Jianfeng Gao. 2022.Glipv2: Unifying localization and vision-language understanding. Advances inNeural Information Processing Systems 35 (2022), 3606736080. Junru Zhang, Lang Feng, Yang He, Yuhan Wu, and Yabo Dong. 2023. TemporalConvolutional Explorer Helps Understand 1D-CNNs Learning Behavior in TimeSeries Classification from Frequency Domain. In Proceedings of the 32nd ACMInternational Conference on Information and Knowledge Management. 33513360. Mi Zhang and Alexander A Sawchuk. 2012. USC-HAD: A daily activity datasetfor ubiquitous activity recognition using wearable sensors. In Proceedings of the2012 ACM conference on ubiquitous computing. 10361043. Shibo Zhang, Yaxuan Li, Shen Zhang, Farzad Shahabi, Stephen Xia, Yu Deng,and Nabil Alshurafa. 2022. Deep learning in human activity recognition withwearable sensors: A review on advances. Sensors 22, 4 (2022), 1476. Yi-Fan Zhang, Jindong Wang, Jian Liang, Zhang Zhang, Baosheng Yu, LiangWang, Dacheng Tao, and Xing Xie. 2023. Domain-Specific Risk Minimization forDomain Generalization. In Proceedings of the 29th ACM SIGKDD Conference onKnowledge Discovery and Data Mining. 34093421. Rui Zhao, Ruqiang Yan, Zhenghua Chen, Kezhi Mao, Peng Wang, and Robert XGao. 2019. Deep learning and its applications to machine health monitoring.Mechanical Systems and Signal Processing 115 (2019), 213237. Guangtao Zheng, Mengdi Huai, and Aidong Zhang. 2024. AdvST: Revisiting DataAugmentations for Single Domain Generalization. In Proceedings of the AAAIConference on Artificial Intelligence, Vol. 38. 2183221840.",
  "ADETAILS OF STYLE CONDITIONER": "Our style conditioner is adapted from the contrastive module calledTime-Series representation learning framework via Temporal andContextual Contrasting (TS-TCC), proposed in . TS-TCC hasdemonstrated robust representation learning capability for the HARtask, resulting in each output containing both class-maintainedinformation and unique features of the corresponding time-seriesinstance. This makes it highly suitable for extracting distinctiveactivity styles from each activity sample. We implemented themodel using the official code ( Here is a brief introduction to the model and how to adjustit for our tasks.Architecture. TS-TCC consists of two components: a featureencoder denoted as enc and a Transformer denoted as trans. Thefeature encoder enc is a 3-block convolutional architecture. Eachblock comprises a convolutional layer, a batch normalization layer,and a ReLU activation function. The Transformer trans primarilyconsists of successive blocks of multi-headed attention (MHA) fol-lowed by an MLP block. The MHA block employs 8 attention heads,and the MLP block is composed of two fully-connected layers witha non-linearity ReLU function and dropout in between. The Trans-former stacks layers to generate the final features, where istypically set to 4.Contrastive learning pipeline . TS-TCC uses strong andweak data augmentation techniques to enable contrastive learningof the feature encoder and Transformer. These generate two viewsfor temporal and contextual contrasting, which minimize distanceand pull views closer together. Thus, the self-supervised loss com-bines the temporal and contextual contrastive losses to encouragediscriminative representations.Adaptation for our style conditioner. We adjust the inputchannel to match the input channel of our training data and setthe kernel size to 9. Other components remain consistent with theexperimental settings used in the HAR dataset. We maintain theoriginal training settings, such as setting the epoch to 40 and usinga Adam optimizer with a learning rate of 3e-4. Once the moduleis trained, the trained encoder and Transformer are combined toform our style conditioner. When extracting the style from thecorresponding original data X, the style conditioner produces therepresentation = (X) and then outputs the correspondingcontext vector = () R , where is its length.",
  "log( |) ( ,,).(9)": "Hence, single style conditional data distribution {( |)}=1 canbe modeled with a singular denoising model { ( ,,)}=1 thatconditions on the respective style .To integrate multiple styles in a specific sytle combination D D, we aim is to model data distribution ( |D) = ( |{}=1).Here, for convenience, we use to denote the number of stylesin D and use {}=1 to denote all styles in D. We assume that{}=1 are conditionally independent given . Thus, it can be",
  "CDETAILS OF DIFFUSION MODEL": "We present a 1D UNet implementation that includes key compo-nents such as style embedding layer. During training, the modeltakes in a 1D time-series sample, an activity style vector, and atimestep to produce noise of the same dimension as the input. Dur-ing sampling, the model uses noise, concatenated activity stylevectors, and a timestep to generate a new time-series sample. Ourdiffusion model operates according to these specifications.",
  "C.1Architecture and Training Details": "Architecture. The model begins with an initialization convolutionlayer, followed by a series of downsampling blocks. Each down-sampling block comprises two residual blocks and an attentionlayer, executed using a 1D convolutional layer with a kernel sizeof 3. The output of each downsampling block is saved in a list,which is later used in the upsampling process. After the downsam-pling blocks, the model has a middle block consisting of a residualblock and an attention layer. The upsampling blocks are then ap-plied in reverse order, with each block consisting of two residualblocks and an attention layer. The upsampling operation is per-formed using a transposed convolutional layer with a kernel sizeof 3. In addition to the convolutional layers and residual blocks,the model also includes a time embedding layer, which consistsof a sinusoidal positional embedding and two linear layers witha channel size of 256. We borrow the code for the 1D UNet from Differ-ently, we add a style embedding layer, which consists of a linearlayer with a channel size of 100 and a linear layer with a channelsize of 64. Both of these embedding layers are concatenated along",
  "C.2Sampling Procedure": "Once our diffusion is trained, we can use the style-fused samplingstrategy to diversify the condition space for our task. During theselection of style sets for generating samples, we specify the ratio of new samples to original samples, as well as the maximumnumber of styles that can be fused for each new sample and thenumber of fused styles is determined by a random variable thatfollows a specified distribution.For example, if we have 1000 training samples and = 1 and = 5, we will generate 1000 training samples. For each new sample,we can constrain it to be fused with up to 5 styles, and the numberof fused styles is determined by a random variable that follows aspecified distribution. The distribution used to determine the num-ber of fused styles can be customized based on the specific task anddataset. For example, we can use a uniform distribution to ensurean equal probability of fusing any number of styles, or we can usea Poisson distribution to favor fewer fused styles. In our code, wehave set a default probability distribution for the number of fusedstyles. For example, when = 5 , we set the probability distributionfor mixing from 1 to 5 styles to {0.2, 0.2, 0.2, 0.2, 0.2}, with a totalsum of 1. This means that each new sample can be fused with upto 5 styles, and the number of fused styles is determined randomlybased on this distribution. However, this probability distributioncan be flexibly adjusted to potentially achieve better results. By ad-justing these hyperparameters, we can control the number of fusedstyles for each new sample and explore different combinations ofstyles. This could help us generate more diverse and representativesamples for our task.",
  "C.3Implementation of Generation in TSCmodels": "To leverage parallel computing for training time series classification(TSC) models, we simultaneously process a batch of training datacontaining samples and generate new samples. Withineach batch, we select an appropriate number of style sets for eachclass in a class-balanced manner and aggregate all stylesets as conditional inputs. Next, we use our diffusion model togenerate new samples. Once this batch generation processis complete, we utilize the generated samples to train the featureextractor, while the generated data with its class labels are stored.This generation process is repeated for each batch of training data,requiring execution only once. The stored data will be directly usedto train the feature extractor in subsequent epochs without theneed for data regeneration.Budget. Overall, our approach offers a cost-effective solutionto the challenges associated with human activity data collection in HAR scenarios. For instance, the cost of a three-axis accelerom-eter typically ranges from several tens of dollars, and collectinghuman activity data for 30,000 samples of various activities cantake several weeks and cost approximately $1,000 per participant.Additionally, manual annotation of the data in subsequent stagescan lead to additional expenses. In contrast, our diffusion modelonly uses a GPU like RTX 3090 to create over 30,000 labeled activitysamples in just one hour 5 . These generated samples can providesignificant performance gains for various baselines. Importantly,our method only requires a one-time expansion without the needfor re-generation. Moreover, the generated samples may simulatenew and unseen users, making the trained deep learning modelsmore likely to be effectively deployed on new edge devices forreal-world applications.",
  "DDIVERSITY LEARNING STRATEGYD.1Details of training TSC models": "After generating synthetic dataset , the data space expands to = { } = {(X, )}+ =16. To enhance the diversity oflearned features from , we present a simple yet effective diversitylearning strategy that is adapted from the representation learningmethod proposed in .Our approach involves differentiatingbetween the origin of each sample, whether it is \"synthetic\" or\"original,\" and its corresponding class label. Since our augmenteddata is highly diverse, we adopt a simplified learning objective thatremoves complex computations, such as measuring the distance andsimilarity between synthetic and original data. Instead, we dependentirely on the cross-entropy loss, where each loss is determinedby different classification criteria. To this end, we employ a multi-objective method that consists of three sequential steps to train theclassifier on .Specifically, there are three fundamental components of a TSCmodel: the feature extractor , the projection layer proj*, and theclassifier *. The function () maps the inputs to their respec-tive representations and is updated throughout all three steps. Thefunction proj* () is a fully connected layer that maps the repre-sentations to vectors of length . The function * () is a classifierresponsible for predicting the designed label. The superscripts (*)indicate that these components are utilized in different steps. Thethree steps are then described as follows:(i) Class-origin feature learning. To learn more detailed repre-sentations, we label each sample based on its origin and class. Theoriginal data is labeled as o = 1 and the augmented data as o = 0. We then combine the origin and class labels to create new labels,represented as co= (c + o ) N. By using the classifierco : R R2, we train the model to predict these new labelsusing cross-entropy, which can be expressed as:",
  "=1oprojo (X),o.(11)": "Here, o : R R2 serves as an origin classifier to distinguishwhether the input features originate from a synthetic or an originalsample.(iii) Class-specific feature learning. The feature extractorfinally undergoes a training process on to correctly predictthe class labels. This step allows the model to separate clustersbelonging to different classes. We employ a class classifier c :R R by minimizing the following loss:",
  "=1cprojc (X),c.(12)": "Overall.During the training process, as Algorithm 1 shows, the threesteps are iteratively repeated in each epoch until termination. Inthe inference phase, as Algorithm 2 shows, we only use the trainedfeature extractor , projection layer projc and the class classifierc from step (iii). They are stacked together to classify the inputtime-series samples in the test dataset .",
  "EDETAILS OF EXPERIMENTAL SETUPE.1Datasets": "To ensure fairness and reproducibility, we conduct our experimentsby the same experimental setup as that provided in to comparethe performance of our method on HAR tasks. Our datasets for thisstudy include DSADS 7, PAMAP2 8, and USC-HAD 9. We followthe same processing steps involving the domain split and randomlychoose remaning data used for each dataset, as detailed in theofficial code 10. The corresponding processing information is asfollows:Dataset information and pre-processing. The information de-tails of the three HAR datasets are presented in . To segmentthe data, we employ a sliding window approach. For DSADS, thewindow duration is set to 5 seconds as per , while for PAMAP2,it is set to 5.12 seconds . Similarly, for USC-HAD, a 5-secondwindow is utilized with a 50% overlap between consecutive win-dows. Given the sampling rates of each dataset (25Hz for DSADS,100Hz for PAMAP2, and 100Hz for USC-HAD), the window lengthsare calculated to be 125 readings, 512 readings, and 500 readings, re-spectively. We normalize the data using normalization and reshapethe 1D time series sample into a 2D format with a height of 1. Eachbatch is structured as (,,,), where represents the mini-batchsize, denotes the number of channels corresponding to the totalaxes of sensors, signifies the height, and denotes the windowlength.Domain split. We implement leave-one-out-validation by divid-ing subjects into multiple groups. In this approach, we designateone group of subjects data as the target domain, while the remain-ing subjects data serve as the source domain. Each group can beconsidered as an individual task. For DSADS and PAMAP2, we di-vide the 8 subjects into 4 groups. As for USC-HAD, we divide the 14subjects into 5 groups, with groups 0-3 comprising 3 subjects each,and the last group consisting of 2 subjects. Subsequently, we parti-tion the data within each group into training, validation, and testsets, maintaining a ratio of 6:2:2. To assess the impact of trainingdata size on model performance, we conduct experiments where werandomly sample 20% to 100% of the training data with incrementsof 20%. This allows us to simulate the small-scale setting. Duringtesting, we evaluate the trained model on the test set of the targetdomain.",
  "TS-TCC : As a recent self-supervised method, it leverages asmall set of labeled time-series data for model generalization": "DDLearn : A latest DG method that leverages standard dataaugmentation baselines, designed for low-resource scenarios.In our experiments, we use the same data augmentation techniques as those used in to ensure a fair comparison. These tech-niques include rotation, permutation, time-warping, scaling, magni-tude warping, jittering, and random sampling. For example, jitteringinvolves applying different types of noise to the samples, whichincreases the diversity of data magnitude. Scaling, on the otherhand, rescales the samples to different magnitudes.",
  "E.3Architecture": "All DG baselines use the same network architecture for featureextraction, except for TS-TCC . Specifically, we adopt an archi-tecture consisting of a feature extractor, a projection layer, and aclassifier, as described in . The feature extractor is composedof two or three Conv1D layers, depending on the dataset beingused. For DSADS and PAMAP2, we use two Conv1D layers with akernel size of 9, while for USC-HAD, we use three Conv1D layerswith a kernel size of 6. Each Conv1D layer is followed by a ReLU activation function and a maxpool1d operation. The output is thenconnected to a projection layer, which is a fully-connected layerwith output feature dimensions of 64 for DSADS and PAMAP2,and 128 for USC-HAD. Finally, we employ a fully-connected layeras the classifier, which takes the extracted features as input andoutputs -dimensional logits. By applying a softmax operation,we obtain prediction probabilities for each class, which sum up to1. In our diversity learning strategy, we use different projectionlayers and classifiers at different training steps, while keeping theoverall architecture of the model the same. Specifically, we usedifferent projection layers with the same architecture for differ-ent steps, and classifiers with the same input channel but output(2 )-dimensional, 2-dimensional, and -dimensional logits forspecific classification goals. During the inference phase, we utilizeonly a trained feature extractor, a projection layer, and a classifier.Since TS-TCC is designed for contrastive learning, we retain itsarchitecture and make slight modifications to adapt it to the inputdata channels and lengths for use in HAR tasks.",
  "E.4Training Details": "All methods in our experiments use PyTorch. We utilize Adamoptimization with a scheduler. The batch size is fixed at 64. Theexperiments are conducted on one GeForce RTX 3090 Ti GPU. Toensure its performance, we fine-tune the training configurationsfor each baseline. Furthermore, the results for Mixup , RSC ,SimCLR , Fish , and DDLearn reported in Tables 1 and2 are obtained from the paper ."
}