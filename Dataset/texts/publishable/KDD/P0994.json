{
  "ABSTRACT": "Graph representation learning (GRL) is to encode graph elementsinto informative vector representations, which can be used in down-stream tasks for analyzing graph-structured data and has seen ex-tensive applications in various domains. However, the majority ofextant studies on GRL are geared towards generating node represen-tations, which cannot be readily employed to perform edge-basedanalytics tasks in edge-attributed bipartite graphs (EABGs) that per-vade the real world, e.g., spam review detection in customer-productreviews and identifying fraudulent transactions in user-merchantnetworks. Compared to node-wise GRL, learning edge representa-tions (ERL) on such graphs is challenging due to the need to incor-porate the structure and attribute semantics from the perspectiveof edges while considering the separate influence of two heteroge-neous node sets U and V in bipartite graphs. To our knowledge,despite its importance, limited research has been devoted to thisfrontier, and existing workarounds all suffer from sub-par results.Motivated by this, this paper designs EAGLE, an effective ERLmethod for EABGs. Building on an in-depth and rigorous theoreticalanalysis, we propose the factorized feature propagation (FFP) schemefor edge representations with adequate incorporation of long-rangedependencies of edges/features without incurring tremendous com-putation overheads. We further ameliorate FFP as a dual-view FFPby taking into account the influences from nodes in U and V sever-ally in ERL. Extensive experiments on 5 real datasets showcase theeffectiveness of the proposed EAGLE models in semi-supervisededge classification tasks. In particular, EAGLE can attain a consider-able gain of at most 38.11% in AP and 1.86% in AUC when comparedto the best baselines.",
  "graph representation learning, edge classification, attributed graph,bipartite graph": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for third-party components of this work must be honored.For all other uses, contact the owner/author(s).KDD 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s).ACM ISBN 979-8-4007-0490-1/24/08 ACM Reference Format:Hewen Wang, Renchi Yang, and Xiaokui Xiao. 2024. Effective Edge-wiseRepresentation Learning in Edge-Attributed Bipartite Graphs. In Proceedingsof the 30th ACM SIGKDD Conference on Knowledge Discovery and DataMining (KDD 24), August 2529, 2024, Barcelona, Spain. ACM, New York,NY, USA, 11 pages.",
  "INTRODUCTION": "Edge-attributed bipartite graphs (EABGs) (a.k.a. attributed interac-tion graphs ) are an expressive data structure used to model theinteractive behaviors between two sets of objects U and V wherethe behaviors are characterized by rich attributes. Practical exam-ples of EABGs include reviews from users/reviewers on movies,businesses, products, and papers; transactions between users andmerchants; and disease-protein associations.In real life, EABGs have seen widespread use in detecting spamreviews in e-commerce , malicious incidents in telecommunica-tion networks , fraudulent transactions/accounts in finance or E-payment systems , abusive behaviors in online retail web-sites , insider threats from audit events , and others .The majority of such applications can be framed as edge-basedprediction or classification tasks in EABGs.In recent years, graph representation learning (e.g., graph neuralnetworks and network embedding) has emerged as a popular andpowerful technique for graph analytics and has seen fruitful successin various domains . In a nutshell, GRL seeks to map graphelements in the input graph into feature representations (a.k.a. em-beddings), based on which we can perform downstream predictionor classification tasks. However, to our knowledge, most of theexisting GRL models, e.g., GCN , GraphSAGE and GAT, are devised for learning node-wise representations in node-attributed graphs, and edge-wise representation learning (ERL), espe-cially on EABGs, is as of yet under-explored. A common treatmentfor obtaining edge representations is to directly apply the canonicalnode-wise GRL models to generate node em-beddings, followed by concatenating them as the embeddings of thecorresponding edges. Despite its simplicity, this methodology fallsshort of not only the accurate preservation of the graph topologyfrom the perspective of edges (demanding an effective combina-tion of node embeddings) but also the incorporation of the edgeattributes in EABGs, thereby resulting in compromised embeddingquality. Another category of workarounds is to simply transformthe original EABGs into node-attributed unipartite graphs by con-verting the edges into nodes and connecting them if they sharecommon endpoints in the input EABGs. In doing so, the node-wiseGRL techniques can be naturally adopted on such projected graphsfor deriving edge representations. Unfortunately, aside from infor-mation loss of the bipartite structure in the input EABG G by the",
  "KDD 24, August 2529, 2024, Barcelona, SpainTrovato and Tobin, et al": "for the edge embeddings. For Amazon, DBLP, and Google, the AUCscores of all our methods increase as increases (excluding thesolution from power iteration), as larger embedding dimensionscan contain more graph structural information. For AMiner, AUCreaches the maximum when = 8 for EAGLE (DV-FFP) usingthe sum aggregator. For MAG, AUC reaches the maximum forEAGLE (DV-FFP) using the max aggregator when = 128 and thendecreases.In , we report how AUC scores vary for different . Bytuning , we observe that EAGLE (DV-FFP) with the max aggrega-tor can achieve the best performance on these datasets comparedwith other methods. In particular, on Amazon, AMiner, DBLP, andGoogle, EAGLE (DV-FFP) with the max aggregator performs bestwith value between 0.3 and 0.6. On MAG, EAGLE (DV-FFP) withthe max aggregator shows a decreasing trend as increases. As men-tioned in .1, balances the importance between the edgerepresentations derived from edge features and graph structures.This suggests on MAG, our methods can achieve improvements bya careful trade-off between edge attributes and graph structures.",
  "PRELIMINARIES": "Throughout this paper, sets are denoted by calligraphic letters, e.g.,V, and |V| is used to denote the cardinality of the set V. Matrices(resp. vectors) are written in bold uppercase (resp. lowercase) letters,e.g., M (resp. x). The superscript M is used to symbolize the trans-pose of matrix M. M[] (M[:,]) is used to represent the -th row(resp. column) of matrix M. Accordingly, M denotes the (, )-thentry in matrix M. For each vector M[], we use M[] to represent",
  ": An Example EABG": "exemplifies an EABG G in online retail platforms (e.g.,Amazon and eBay) with 5 users 1-5 in U, 6 products 1-6 in V,and the user-product interactions in E. Each interaction (i.e., edge)is associated with a review (i.e., edge attributes) from the user onthe product.For each node U (resp. V), E (resp. E ) symbolizesthe set of edges incident to (resp. ). We use DU R|U||U|",
  "R(|U|+|V|)(|U|+|V|) is the diagonal node degree": "matrix of G. Further, we denote by EU R| E||U| and EV R| E||V| the edge-node indicator matrices for node sets U and V,respectively. More precisely, for each edge E and its two endpoints (), (), we have EU [, ()] = EV [, ()] = 1. For othernodes U \\ () and V \\ (), EU = EV = 0.On the basis of DU, EU and DV, EV, we define edge-wise tran-sition matrix PU and PV as follows:",
  "Problem Formulation": "We formalize the edge representation learning (ERL) in EABGs asfollows. Given an EABG G = (U V, E, X), the task of ERL aimsto build a model : E Z R| E| ( |E|), which transformseach edge E into a length- vector Z[] as its feature represen-tation. Such a feature representation Z[] should capture the richsemantics underlying both the bipartite graph structures and edgeattributes. In this paper, we focus on the edge classification task,and thus, the edge representations are learned in a semi-supervisedfashion by plugging the loss function for classifying edges into themodel .",
  "THE EAGLE MODEL": "As illustrated in , we have developed two ERL models forEABG, i.e., EAGLE with FFP and dual-view FFP, both of whichinvolve two key steps: -truncated singular value decomposition(SVD) and feature propagation.In this section, we focus on introducing our base EAGLE model,i.e., EAGLE with FFP. .1 first presents the objective oflearning the edge-wise representations, while .2 then offersan in-depth analysis of the solution to the optimization objective. In.3, we elaborate on the feature propagation mechanism forcomputing the edge representations, followed by the loss functionfor the model training in .4.",
  "Representation Learning Objective": "Inspired by the numeric optimization analysis of generalized graphneural network models in recent studies , we formulatethe ERL in EABGs as an optimization problem with considerationof the lopsided nature of bipartite graphs.More concretely, EAGLE aims at achieving two goals: (i) the edgerepresentations Z close to the input edge feature matrix; and (ii)representations of edges that are incident to the same nodes shouldbe similar. The former corresponds to a fitting term in the followingequation:O = Z (X)2,(2) where (X) R|E| represents a non-linear transformationfeatures of the input edge attribute matrix X using an MLP ()parameterized by a learnable weight matrix R (includinga nonlinear activation function ReLU operation and dropout op-eration), while the latter is a graph structure-based regularizationterm O defined by",
  "EV.(6)": "Lemma 3.1 offers a simple yet elegant way (i.e., Eq. (5)) to calcu-late the optimal edge representations Z to the optimization objectivein Eq. (4). However, Eq. (5) requires summing up an infinite seriesof matrix multiplications, which is infeasible in practice, especiallyfor large EABGs. A remedy is to compute an approximate versionZ by summing up at most + 1 terms with a small integer :",
  "Lemma 3.2. Given P in Eq. (6), P = PU + (1 ) PV": "First, by Lemma 3.2, P is a linear combination of PU and PV.Recall that both PU and PV are non-negative doubly stochastic,which further connotes that P is non-negative doubly stochasticand can be regarded as a reversible Markov chain. Let be itsmixing time. Using its doubly stochastic property and the Conver-gence Theorem in , when > , P (X) converges to astationary distribution , wherein [] is a constant vector, i.e.,1 (X)[]1. Thus, Z in Eq. (5) can be broken down into twoparts1:",
  "+ .(8)": "Intuitively, since each row in is a constant vector, is notan informative representation matrix. As such, Eq. (8) implies that ifwe pick a large ( ) for Z, constant vectors mightjeopardize the representation quality of Z in Eq. (7), especially ongraphs with small mixing times, resulting in degraded performance.On the other hand, a small ( 1) for Z fails to enablean adequate preservation of the topological semantics underlyingthe input EABGs.",
  ": The overall framework of EAGLE": "Lemma 3.3 provides a lower bound for , which is propor-tional to the inverse of 1 22. As per the empirical data on realEABGs (see .2) from , 2 is notably approaching 1,rendering extremely large (over thousands), as a consequenceof the unique characteristics of bipartite graph structures. We canconclude that the part in Z (Eq. (8)) is insignificant. Addi-tionally, based on .2 in , given any integer and anyedge E,",
  "(P (X)[]) 42 ((X)[]),": "where stands for the variance computed w.r.t. the sta-tionary distribution []. Since 2 is almost 1, the above equa-tion manifests that even for a very large , the difference betweenP (X)[] and the stationary distribution [] can be as signif-icant as that of the input feature vector (X)[]. That is to say,P (X)[] with large still encompasses rich and informativefeatures, and thus, computing Z via Eq. (7) leads to compromisedrepresentation quality.",
  "Y = sigmoid((Z)) R| E|||(11)": "where || is the number of classes and is parameterized bya learnable weight matrix R||, followed by a nonlinearactivation function ReLU operation and a dropout operation. Insum, the trainable parameters of EAGLE are only the weight matrix R in the feature transformation layer (X) and the weightmatrix R|| of the output layer in Eq. (11).",
  "EAGLE WITH DUAL-VIEW FFP": "Recall that in .2, the edge-wise transition matrix P canbe equivalently converted into P = PU + (1 ) PV (Lemma3.2). In turn, the edge representations Z in Eq. (5) are essentiallyobtained through a linear combination of the features propagatedbetween edges via their connections using two heterogeneous nodesets U and V as intermediaries, which tends to yield sub-optimalrepresentation effectiveness. Further, such a linear combinationrelies on a manually selected parameter to balance the importanceof features w.r.t. these two views, which requires re-calculating the-truncated SVD of ED 1 2 (see Eq. (6)) from scratch to create Q (Eq.(10)) once was changed, leading to significant computation effort.To mitigate the foregoing issues, in EAGLE, we develop dual-viewfactorized feature propagation (referred to as DV-FFP) for learningenhanced edge representations. The basic idea is to create twointermediate edge representations, ZU and ZV, by utilizing theassociations between edges from the views of U and V severally,and then coalesce them into the final edge representations Z.In the sequel, .1 elaborates on the details of DV-FFP,followed by a theoretical analysis in .2",
  "(12)": "In Eq. (12), U (X) (resp. V (X)) corresponds to the initial edgefeatures used for the generation of ZU (resp. ZV), which is trans-formed from the input edge attribute vectors X through an MLPnetwork parameterized by weight matrix U (resp. V).In analogy to FFP in .3, DV-FFP adopts a low-dimensionalmatrix approximation trick to approximate (1 ) =0 PUand (1 ) =0 PV, while sidestepping the explicit construc-tion of these two |E| |E| dense matrices. Specifically, DV-FFP",
  "Analysis": "In the rest of this section, we theoretically analyze the optimizationobjective of learning ZU, ZV as in Eq. (12), the approximationaccuracy guarantees of QU and QV, as well as the computationalexpense of DV-FFP, respectively.Optimization Objective. Recall that P = PU + (1 ) PV byLemma 3.2. If we set to 1 and 0, P in Eq. (6) turns into PU andPV, respectively. Accordingly, Z defined in Eq. (5) becomes ZUand ZV, if we replace (X) by U (X) and V (X), respectively.Since Lemma 3.1 indicates that Z in Eq. (5) is the closed solution tothe objective in Eq. (4), when = 1 or = 0, ZU and ZV definedin Eq. (12) are thus the closed form solutions to the problems thatminimize the following objectives:",
  "EXPERIMENTS": "In this section, we empirically study the effectiveness of our pro-posed EAGLE models on real-world datasets in terms of edge classifi-cation. All experiments are conducted on a Linux machine poweredby 4 AMD EPYC 7313 CPUs with 500GB RAM, and 1 NVIDIA RTXA5000 GPU with 24GB memory. The code and all datasets are avail-able at for reproducibility.",
  "Baselines and Hyperparameters": "We compare our proposed solutions against 9 competitors in termsof edge classification accuracy. The first category of baseline modelsconsists of node-wise representation learning methods, includingGCN , GraphSAGE , SGC , DGI , GAT , andGATv2 . We initialize the embeddings of edge endpoints as themean average of their connected edge attributes. Then, we applythese node-wise representation learning methods to update thenode embeddings for the edge endpoints. Finally, we concatenatethe embeddings of edge endpoints along with edge attributes togenerate the corresponding edge embeddings. The second categoryof baseline models consists of edge-wise representation learningmethods, including GEBE and AttrE2Vec . Additionally, weinclude a fully connected neural network (FC) to transform edgeattributes without considering any network structure information.For DGI, GEBE, and AttrE2Vec, we collect the source codes fromthe respective authors and adopt the parameter settings suggestedin their papers to generate edge representations before feedingthem to MLPs (multi-layer perceptrons) for classification. For GCN,GraphSAGE, SGC, GAT, and GATv2, we utilize the standard imple-mentations provided in the well-known DGL2 library and follow athree-layer neural network architecture, including two GNN layersand one linear layer, with ReLU as activation functions betweenlayers. Besides, we set the dropout rate to 0.5 and the maximumnumber of training epochs to 300, and employ the Adam opti-mizer for optimization with a learning rate of 0.001. All themethods are implemented in Python. In our solutions (i.e., EAGLE(FFP) and EAGLE (DV-FFP)), unless otherwise specified, we set thehyperparameter and to be 0.5, in Eq. (14) to be 0.5, and dimen-sion to be 256. The edge representations are then input to MLPclassifiers to obtain the final edge labels. We report the AP/AUCon the test datasets using the model selected with the best AUCachieved on the cross-validation datasets.",
  "We use 5 real-world bipartite network datasets in the experiments.The Amazon dataset contains user reviews for movies and TV": "shows, where the edges represent the reviews written by users onthe products, which are associated with labels representing usersratings on these products. The Google dataset containsreview information of business entities on Google Maps in Hawaii,United States, where the edges are reviews written by users onthe business entity IDs. Similarly, the edge labels represent usersratings on the business entities. AMiner , MAG andDBLP datasets are 3 citation networks, in which nodes repre-sent scholars and their publication venues of a paper. The edgesrepresent the paper abstracts written for that paper. For AMiner,edge labels correspond to the keywords for the papers. For DBLPand MAG, edge labels correspond to the field of study for the papers.We select the most frequent 10 labels as targets to be predicted. Toobtain initial edge features from text for these datasets, we applythe Sentence-BERT model to encode text into 768-dimensionalvectors. For each dataset, we use breadth-first search (BFS) to sam-ple a smaller subset. Then we randomly split all edges into training,cross-validation and test sets with an 8 : 1 : 1 ratio. The propertiesand scales of the datasets used in our experiments are summarizedin .",
  "Edge Classification": "presents the edge classification performance of all meth-ods on five datasets. Overall, our proposed methods consistentlyoutperform all competitors on all five datasets. On review datasetslike Amazon and Google, our method (EAGLE (DV-FFP)) using themax aggregator achieves approximately 0.9%-3.1% improvement inAP and approximately 0.4%-0.7% improvement in AUC comparedto the best competitors. On citation network datasets like AMinerusing keywords as edge labels, our method (EAGLE (FFP)) achievesaround 177.5% improvement in AP and around 1.5% improvement inAUC compared to the best competitors. Note that most of the base-lines cannot achieve high AP (below 0.2), due to the difficulties ofclassifying keywords in AMiner, as the number of keyword labels inthe original dataset is much higher than the number of labels in theother datasets. On citation network datasets like DBLP and MAGusing the field of study as edge labels, our method (EAGLE (DV-FFP)) using the max aggregator achieves approximately 7.1%-15.0%improvement in AP and approximately 1.2%-1.9% improvement inAUC compared to the best competitors. It is worth noting that fordatasets like Amazon and AMiner, FC performs the best among thecompetitors. This indicates the difficulties of capturing the struc-tural similarities in these graph datasets. However, our methodscan still successfully generate better edge representations on thesedatasets. Another observation is that EAGLE (DV-FFP) outperformsEAGLE (FFP) and other competitors on four datasets out of five.This suggests the importance of introducing intermediate edge rep-resentation independently from the views of heterogeneous nodesets ZU and ZV.",
  ": Varying": "In , we report how AUC scores vary for different forEAGLE (FFP) and different for EAGLE (DV-FFP). For Amazon,Google, and MAG, EAGLE (DV-FFP) with the max aggregator con-sistently performs the best across different . For AMiner, the AUCscores reach the maximum when = 0.7 and then decrease forEAGLE (DV-FFP) with max aggregator. For DBLP, the EAGLE (FFP) performs the best when = 0.2, but when 0.3, EAGLE (DV-FFP)with max aggregator becomes the best among all our methods. Wecan also observe that on DBLP, Google, and MAG, EAGLE (FFP) ismore sensitive to the change of compared with EAGLE (DV-FFP).In , we report how AUC scores vary for different SVDdimensions . in refers to using power iteration to solve",
  "Node-wise Representation Learning": "Node-wise GRL refers to the process of generating embeddings forthe nodes of a graph. Conventional approaches for addressing thistask involve methods based on matrix factorization and randomwalk. In matrix factorization-based methods, such as HOPE ,AROPE , PRONE , NRP , PANE , and SketchNE ,a proximity-based matrix P is initially created for the graph, whereeach element P denotes the proximity measure between nodes and . Subsequently, a dimension reduction technique is employedto derive lower-dimensional representations for the nodes. In ran-dom walk-based methods, such as Deepwalk , LINE , andnode2vec , the process begins with the generation of randomwalks for each node to capture the underlying graph structures.Subsequently, the co-occurrence in these random walks is employedto assess node similarities and generate node embedding vectors.Another line of research lies in graph neural networks (GNNs).The major categories of GNNs, for example, GCN , Graph-SAGE , SGC , DGI , GAT , and GATv2 , adoptideas from convolutional neural networks for modeling graph-structured data. GNNs aggregate local neighborhood informationto get contextual representation for graph nodes and have shownpromising results in this area. To consider the effect of edge at-tributes, some new GNN models are proposed to incorporate themduring the training process. EGAT proposes edge-featuredgraph attention layers that can accept node and edge features as in-puts and handle them spontaneously within the models. GERI constructs a heterogeneous graph using the attribute informationand applies random walk with a modified heterogeneous skip-gram to learn node embeddings. EEGNN proposes a frame-work called edge-enhanced graph neural network that uses thestructural information extracted from a Bayesian nonparametricmodel for graphs to consider the effect of self-loops and multiple edges between two nodes and improve the performance of variousdeep message-passing GNNs. EGNN uses multi-dimensionalnonnegative-valued edge features represented as a tensor and ap-plies GCN/GAT to exploit multiple attributes associated with eachedge. GraphBEAN applies autoencoders on bipartite graphswith both node and edge attributes to obtain node embeddings fornode and edge level anomaly detection.",
  "Edge-wise Representation Learning": "Edge-wise GRL refers to the process of generating embeddings foredges of a graph. uses random walks to sample a series ofedge sequences to generate edge embeddings and apply clusteringalgorithms for community detection. Edge2Vec uses deep auto-encoders and skip-gram models to generate edge embeddings thatpreserve both the local and global structure information of edges forbiomedical knowledge discovery. AttrE2Vec generates randomwalks starting from a node and uses aggregation functions to aggre-gate node/edge features in the random walks and obtain node/edgerepresentations. Then, it uses auto-encoders and self-attention net-works with feature reconstruction loss and graph structural lossto build edge embeddings in an unsupervised manner. usesmatrix factorization and feature aggregation to generate edge rep-resentation vectors based on the graph structure surrounding edgesand edge attributes, which can encode high-order proximities ofedges and edge attribute information into low-dimensional vectors.CEN-DGCNN introduces a deep graph convolutional neuralnetwork that integrates node and edge features, preventing over-smoothing. It captures non-local structural features and refineshigh-order node features by considering long-distance dependen-cies and multi-dimensional edge features. DoubleFA proposesto use top-k Personalized PageRank to conduct proximal featureaggregation and anomaly feature aggregation using edge featuresfor edge anomaly detection.",
  "Bipartite Graph Representation Learning": "For a comprehensive review of existing bipartite graph representa-tion learning methods, we suggest readers check . BiANE employs auto-encoders to model inter-partition and intra-partitionproximity using attribute proximity and structure proximity througha latent correlation training approach. Cascade-BGNN utilizescustomized inter-domain message passing and intra-domain align-ment with adversarial learning for message aggregation across andwithin graph partitions. BiGI utilizes GCN to generate initialnode embeddings and applies a subgraph-level attention mecha-nism to maximize the mutual information between local and globalnode representations. DualHGCN transforms the multiplexbipartite network into two sets of homogeneous hypergraphs anduses spectral hypergraph convolutional operators to capture infor-mation within and across domains. GEBE proposes proximitymatrices derived from the edge weight matrix and applies matrixfactorization to capture multi-hop similarity/proximity betweenhomogeneous/heterogeneous nodes. AnchorGNN proposes aglobal-local learning framework that leverages an anchor-basedmessage-passing schema to generate node embeddings for largebipartite graphs.",
  "CONCLUSIONS": "In this work, we introduce the problem of ERL on EABGs andpropose EAGLE models to address this problem. Building on anin-depth theoretical analysis of extending the feature propagationparadigm in GNNs to ERL on EABGs, we design the FFP schemethat is able to effectively capture long-range dependencies betweenedges for generating high-quality edge representations withoutentailing vast computational costs. On the basis of FFP, we proposethe dual-view FFP by leveraging the semantics of two sets of het-erogeneous nodes in the input bipartite graphs to enhance edgerepresentations. The effectiveness of our proposed EAGLE mod-els is validated by our extensive experiments comparing EAGLEagainst nine baselines over five real datasets. This research is supported by the Ministry of Education, Singapore,under its MOE AcRF TIER 3 Grant (MOE-MOET32022-0001). Anyopinions, findings and conclusions or recommendations expressedin this material are those of the author(s) and do not reflect the viewsof the Ministry of Education, Singapore. Renchi Yang is supportedby the NSFC Young Scientists Fund (No. 62302414) and the HongKong RGC ECS grant (No. 22202623).",
  "Eq. (8) naturally follows": "Proof of Lemma 3.3. We first prove that22 is the second largesteigenvalue of P. By Eq. (6), P = (ED1/2) (ED1/2), which in-dicates that the eigenvalues of P are the squared singular valuesof ED1/2 . Since singular values are non-negative, the sec-ond largest eigenvalue of P is 22. According to the fact of P is areversible Markov chain and Theorem 12.5 in , satisfies 1",
  "According to , the definition of P in Eq. (6) (i.e., P =ED1/2 (ED1/2)) implies that the singular values of ED1/2 arethe square roots of the eigenvalues of P, and the left singular vectorsof ED 1": "2 are the eigenvectors of P. In particular, due to the non-negativity of singular values, the -th largest eigenvalue of P isequal to 2 where denotes the -th largest singular value ofED1/2.Recall that P is doubly stochastic, meaning that P is a symmetricmatrix. Using Theorem 4.1 in , we can derive that the singularvalues of P are the absolute values of the corresponding eigenvalues,and the left singular vectors of P are equal to the eigenvectors ofP. Since all the eigenvalues of P are non-negative, its -th largesteigenvalue is equal to the -th largest singular value of P.Combining the above two conclusions, we can extrapolate thatthe -th largest singular value of P is equal to 2, and the leftsingular vectors of ED1/2 are the left singular vectors of P.",
  "Shaked Brody, Uri Alon, and Eran Yahav. 2022. How Attentive are Graph Atten-tion Networks?. In ICLR. arXiv:2105.14491": "Jiangxia Cao, Xixun Lin, Shu Guo, Luchen Liu, Tingwen Liu, and Bin Wang.2021. Bipartite Graph Embedding via Mutual Information Maximization. InProceedings of the 14th ACM International Conference on Web Search and DataMining (WSDM 21). Association for Computing Machinery, New York, NY, USA,635643. Rizal Fathony, Jenn Ng, and Jia Chen. 2023. Interaction-Focused Anomaly Detec-tion on Bipartite Node-and-Edge-Attributed Graphs. In 2023 International JointConference on Neural Networks (IJCNN). IEEE, 110. Zheng Gao, Gang Fu, Chunping Ouyang, Satoshi Tsutsui, Xiaozhong Liu, JeremyYang, Christopher Gessner, Brian Foote, David Wild, Ying Ding, et al. 2019.edge2vec: Representation learning using edge semantics for biomedical knowl-edge discovery. BMC bioinformatics 20, 1 (2019), 115. Mathieu Garchery and Michael Granitzer. 2020. Adsage: Anomaly detectionin sequences of attributed graph edges applied to insider threat detection atfine-grained level. arXiv preprint arXiv:2007.06985 (2020).",
  "Keke Huang, Jing Tang, Juncheng Liu, Renchi Yang, and Xiaokui Xiao. 2023.Node-wise diffusion for scalable graph learning. In Proceedings of the ACM WebConference 2023. 17231733": "Wentao Huang, Yuchen Li, Yuan Fang, Ju Fan, and Hongxia Yang. 2020. BiANE:Bipartite Attributed Network Embedding. SIGIR 2020 - Proceedings of the 43rdInternational ACM SIGIR Conference on Research and Development in InformationRetrieval 10, 20 (2020), 149158. Jaehyeong Jo, Jinheon Baek, Seul Lee, Dongki Kim, Minki Kang, and Sung JuHwang. 2021. Edge representation learning with hypergraphs. Advances inNeural Information Processing Systems 34 (2021), 75347546.",
  "Effective Edge-wise Representation Learning in Edge-Attributed Bipartite GraphsKDD 24, August 2529, 2024, Barcelona, Spain": "Jiacheng Li, Jingbo Shang, and Julian McAuley. 2022. UCTopic: UnsupervisedContrastive Learning for Phrase Representations and Topic Mining. Proceedingsof the Annual Meeting of the Association for Computational Linguistics 1 (2022),61596169. arXiv:2202.13469 Suxue Li, Haixia Zhang, Dalei Wu, Chuanting Zhang, and Dongfeng Yuan. 2018.Edge representation learning for community detection in large scale informationnetworks. In Mobility Analytics for Spatio-Temporal and Social Data: First Inter-national Workshop, MATES 2017, Munich, Germany, September 1, 2017, RevisedSelected Papers 1. Springer, 5472. Yiming Li, Siyue Xie, Xiaxin Liu, Qiu Fang Ying, Wing Cheong Lau, Dah MingChiu, Shou Zhi Chen, et al. 2021. Temporal graph representation learning fordetecting anomalies in e-payment systems. In 2021 International Conference onData Mining Workshops (ICDMW). IEEE, 983990. Yirui Liu, Xinghao Qiao, Liying Wang, and Jessica Lam. 2023. EEGNN: EdgeEnhanced Graph Neural Network with a Bayesian Nonparametric Graph Model.In International Conference on Artificial Intelligence and Statistics. PMLR, 21322146. Yao Ma, Xiaorui Liu, Tong Zhao, Yozen Liu, Jiliang Tang, and Neil Shah. 2021. Aunified view on graph neural networks as graph signal denoising. In Proceedings ofthe 30th ACM International Conference on Information & Knowledge Management.12021211.",
  "Petar Velikovi, William Fedus, William L Hamilton, Pietro Li, Yoshua Bengio,and R Devon Hjelm. 2018. Deep Graph Infomax. In ICLR": "Andrew Z Wang, Rex Ying, Pan Li, Nikhil Rao, Karthik Subbian, and Jure Leskovec.2021. Bipartite dynamic representations for abuse detection. In Proceedings of the27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining. 36383648. Hewen Wang, Renchi Yang, Keke Huang, and Xiaokui Xiao. 2023. Efficient andEffective Edge-wise Graph Representation Learning. In Proceedings of the 29thACM SIGKDD Conference on Knowledge Discovery and Data Mining. 23262336. Hewen Wang, Renchi Yang, and Jieming Shi. 2023. Anomaly Detection in Finan-cial Transactions Via Graph-Based Feature Aggregations. In Big Data Analyticsand Knowledge Discovery: 25th International Conference, DaWaK 2023, Penang,Malaysia, August 2830, 2023, Proceedings. Springer-Verlag, Berlin, Heidelberg,6479.",
  "Xueyi Wu, Yuanyuan Xu, Wenjie Zhang, and Ying Zhang. 2023. Billion-ScaleBipartite Graph Embedding: A Global-Local Induced Approach. Proc. VLDBEndow. 17, 2 (2023), 175183": "Yulin Wu, Xiangting Hou, Wen Jun Tan, Zengxiang Li, and Wentong Cai. 2017.Efficient parallel simulation over social contact network with skewed degreedistribution. In Proceedings of the 2017 ACM SIGSIM Conference on Principles ofAdvanced Discrete Simulation. 6575. Zhanghao Wu, Paras Jain, Matthew Wright, Azalia Mirhoseini, Joseph E Gonzalez,and Ion Stoica. 2021. Representing long-range context for graph neural networkswith global attention. Advances in Neural Information Processing Systems 34(2021), 1326613279. Hansheng Xue, Luwei Yang, Vaibhav Rajan, Wen Jiang, Yi Wei, and Yu Lin. 2021.Multiplex Bipartite Network Embedding Using Dual Hypergraph ConvolutionalNetworks. In Proceedings of the Web Conference 2021 (WWW 21). Association forComputing Machinery, New York, NY, USA, 16491660. An Yan, Zhankui He, Jiacheng Li, Tianyang Zhang, and Julian McAuley. 2023.Personalized Showcases: Generating Multi-Modal Explanations for Recommen-dations. SIGIR 2023 - Proceedings of the 46th International ACM SIGIR Conferenceon Research and Development in Information Retrieval 5, 23 (2023), 22512255. arXiv:2207.00422 Hongqiang Yan, Yan Jiang, and Guannan Liu. 2018. Telecomm fraud detectionvia attributed bipartite network. In 2018 15th International Conference on ServiceSystems and Service Management (ICSSSM). IEEE, 16. Liang Yang, Chuan Wang, Junhua Gu, Xiaochun Cao, and Bingxin Niu. 2021. Whydo attributes propagate in graph convolutional neural networks?. In Proceedingsof the AAAI Conference on Artificial Intelligence, Vol. 35. 45904598."
}