{
  "ABSTRACT": "Link prediction is a fundamental task in graph learning, inherentlyshaped by the topology of the graph. While traditional heuristicsare grounded in graph topology, they encounter challenges in gen-eralizing across diverse graphs. Recent research efforts have aimedto leverage the potential of heuristics, yet a unified formulationaccommodating both local and global heuristics remains undiscov-ered. Drawing insights from the fact that both local and globalheuristics can be represented by adjacency matrix multiplications,we propose a unified matrix formulation to accommodate and gener-alize various heuristics. We further propose the Heuristic LearningGraph Neural Network (HL-GNN) to efficiently implement the for-mulation. HL-GNN adopts intra-layer propagation and inter-layerconnections, allowing it to reach a depth of around 20 layers withlower time complexity than GCN. Extensive experiments on thePlanetoid, Amazon, and OGB datasets underscore the effective-ness and efficiency of HL-GNN. It outperforms existing methodsby a large margin in prediction performance. Additionally, HL-GNN is several orders of magnitude faster than heuristic-inspiredmethods while requiring only a few trainable parameters. Thecase study further demonstrates that the generalized heuristics andlearned weights are highly interpretable. The code is available at",
  "Correspondence is to Q. Yao at": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain. 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08 ACM Reference Format:Juzheng Zhang, Lanning Wei, Zhen Xu, and Quanming Yao. 2024. HeuristicLearning with Graph Neural Networks: A Unified Framework for LinkPrediction. In Proceedings of the 30th ACM SIGKDD Conference on KnowledgeDiscovery and Data Mining (KDD 24), August 2529, 2024, Barcelona, Spain.ACM, New York, NY, USA, 15 pages.",
  "INTRODUCTION": "Link prediction stands as a cornerstone in the domain of graphmachine learning, facilitating diverse applications from knowledgegraph reasoning to drug interaction prediction andrecommender systems . While its significance is unquestionable,research in this area has not reached the same depth as that fornode or graph classification .In graph machine learning, two fundamental sources of informa-tion play a pivotal role: node features and graph topology .Link prediction task is inherently driven by graph topology . Heuristics, which derive exclusively from graph topology,naturally align with link prediction. The appeal of heuristics lies intheir simplicity and independence of learning. While heuristics arecrafted from human intuition and insights, they can be broadly cate-gorized into two types: local heuristics, which focus on neighboringnodes, and global heuristics, which focus on global paths .Effective link prediction benefits from both local and globaltopological information . For instance, in a triangular network,each pair of nodes shares two common neighbors, making localheuristics effective. Conversely, in a hexagonal network, whereonly length-5 paths connect each node pair, global heuristics mayyield better results. Hence, the adaptive integration of multi-rangetopological information from both local and global heuristics isessential for accurate predictions.While heuristics prove effective in link prediction tasks, theyinherently capture specific topology patterns, posing challenges intheir generalization to diverse graphs . Moreover, heuris-tics are unable to leverage node features, limiting their efficacy onattributed graphs . To make heuristics more universal and gen-eral, recent research efforts have been directed toward establishingformulations for heuristics and learning heuristics from these for-mulations. Notable examples include SEAL , NBFNet , andNeo-GNN . SEALs -decaying framework and NBFNets pathformulation are tailored for global heuristics, while Neo-GNNsMLP framework is tailored for local ones. To obtain multi-rangetopological information, a unified formulation that accommodates",
  "KDD 24, August 2529, 2024, Barcelona, Spain.Juzheng Zhang, Lanning Wei, Zhen Xu, and Quanming Yao": "both local and global heuristics is necessary, yet it remains undis-covered.Our motivation for constructing the unified formulation stemsfrom the observation that both local and global heuristics can beexpressed through adjacency matrix multiplications. Therefore, weunify local and global heuristics into a matrix formulation, enablingthe accommodation and generalization of various local and globalheuristics. In contrast to previous works that construct formula-tions based on abstract functions such as SEAL , NBFNet ,and Neo-GNN , our unified formulation is developed throughdirect matrix operations. This unified formulation ensures rigorousequivalence to numerous local and global heuristics under specificconfigurations.To learn generalized heuristics and acquire multi-range informa-tion, we propose the Heuristic Learning Graph Neural Network(HL-GNN) to efficiently implement the formulation. HL-GNN incor-porates intra-layer propagation and inter-layer connections whileexcluding transformation and activation functions. This enablesHL-GNN to effectively reach a depth of around 20 layers, whileonly requiring the training of a global GNN with a time complexityeven lower than GCN. The adaptive weights in HL-GNN facilitatethe integration of multi-range topological information, and governthe trade-off between node features and topological information.Our comprehensive experiments, conducted on the Planetoid,Amazon, and OGB datasets, confirm the effectiveness and efficiencyof our proposed HL-GNN. It consistently achieves state-of-the-artperformance across numerous benchmarks, maintains excellentscalability, and stands out as the most parameter-efficient methodamong existing GNN methods. Furthermore, it demonstrates su-perior speed, surpassing existing heuristic-inspired methods byseveral orders of magnitude. HL-GNN is highly interpretable, asevidenced by the generalized heuristics and learned weights onreal-world datasets as well as synthetic datasets.Our contributions can be summarized as follows: We unify local and global heuristics into a matrix formulation,facilitating the accommodation and generalization of heuristics.We demonstrate that numerous traditional heuristics alignwith our formulation under specific configurations. We propose HL-GNN to efficiently implement the formulation,capable of reaching a depth of around 20 layers with lower timecomplexity than GCN. HL-GNN can adaptively balance thetrade-off between node features and topological information. Comprehensive experiments demonstrate that HL-GNN out-performs existing methods in terms of performance and effi-ciency. The interpretability of HL-GNN is highlighted throughthe analysis of generalized heuristics and learned weights.",
  "RELATED WORKS2.1Graph Neural Networks": "Graph Neural Networks (GNNs) have emerged as a powerful para-digm for learning node representations by exploiting neural net-works to manipulate both node features and graph topology. Thesenetworks employ a message-passing mechanism, with notable ex-amples including Graph Convolutional Network (GCN) , Graph-SAGE , and Graph Attention Network (GAT) . Through iter-ative message propagation, GNNs enable each node representation to accumulate information from its neighboring nodes, therebyfacilitating downstream tasks.While GNNs have emerged as potent solutions for node andgraph classification , they sometimes fall short in linkprediction scenarios compared to traditional heuristics like Com-mon Neighbors (CN) or the Resource Allocation Index (RA) .The primary issue lies in the inherent intertwining of node fea-tures and graph topology during the message-passing process inconventional GNNs. This entanglement causes node features tointerfere with graph topology, impeding the effective extraction oftopological information for link prediction tasks.Although in principle an arbitrary number of GNN layers canbe stacked, practical GNNs are usually shallow, typically consist-ing of 2-3 layers, as conventional GNNs often experience a sharpperformance drop after just 2 or 3 layers. A widely accepted explana-tion for this performance degradation with increasing depth is theover-smoothing issue , which refers to node representationsbecoming non-discriminative when going deep. While the adap-tive integration of both local and global topological informationis essential for link prediction, conventional GNNs usually can-not penetrate beyond 3 layers, restricting the extraction of globaltopological information.",
  "Link Prediction": "Link prediction predicts the likelihood of a link forming betweentwo nodes in a graph. The problem of link prediction has tra-ditionally been addressed by heuristic methods. These methodsare primarily concerned with quantifying the similarity betweentwo nodes based on the graph topology. Heuristic methods can bebroadly categorized into two groups: local and global .Local heuristics can be further divided into entirety-based heuris-tics and individual-based heuristics. Entirety-based heuristics, likeCommon Neighbors (CN) and the Local Leicht-Holme-NewmanIndex (LLHN) , consider the cumulative count of common neigh-bors. In contrast, individual-based heuristics, exemplified by theResource Allocation Index (RA) , focus on nodes within thecommon neighborhood and incorporate detailed topological infor-mation such as the degree of each node.Global heuristics, on the other hand, leverage the entire graphtopology. Methods such as the Katz Index (KI) and the GlobalLeicht-Holme-Newman Index (GLHN) consider all possiblepaths between node pairs. The Random Walk with Restart (RWR) assesses the similarity between two nodes based on random walkprobabilities. Some global heuristics are tailored to specific pathlengths, like the Local Path Index (LPI) and the Local RandomWalks (LRW) .Traditional heuristic methods are manually designed and showlimitations on complex real-world graphs, prompting a shift towardlearning-based approaches. Embedding methods, including MatrixFactorization , DeepWalk , LINE , and Node2vec ,factorize network representations into low-dimensional node em-beddings. However, embedding methods face limitations due totheir inability to leverage node features on attributed graphs.Recent advancements have focused on enhancing GNNs withvaluable topological information. Subgraph GNNs like SEAL ,GraIL , and SUREL explicitly encode subgraphs around",
  "Heuristic Learning with Graph Neural Networks: A Unified Framework for Link PredictionKDD 24, August 2529, 2024, Barcelona, Spain": "node pairs. However, they require the running of a subgraph GNNwith the labeling trick for each link during training and inference.Taking a different perspective, models like NBFNet and RED-GNN adopt source-specific message passing, drawing inspira-tion from global heuristics. However, they require training a globalGNN for each source node. Some methods opt for a single globalGNN to improve scalability and efficiency. Neo-GNN uses twoMLPs, while SEG uses a GCN layer and an MLP to approxi-mate a heuristic function. BUDDY develops a novel GNN thatpasses subgraph sketches as messages. However, these methodsprimarily focus on local topological information and struggle tocapture global topological information. In contrast, the proposedHL-GNN can capture long-range information up to 20 hops whileonly requiring the training of a global GNN. Further details aboutthe comparison of HL-GNN with existing methods are provided in.2.",
  "UNIFIED HEURISTIC FORMULATION": "Let G = (V, E) denote a graph, with nodes V and edges E. In thiswork, we consider undirected and unweighted graphs. We define|V| = as the number of nodes and |E| = as the number ofedges. Node features are characterized by the node feature matrix R , where indicates the number of features. The graphtopology is encapsulated by the adjacency matrix {0, 1} .The matrix = + represents the adjacency matrix with self-loops, where = 1 signifies an edge between nodes and . Thenode degree with self-loops is given by = , with the diago-nal degree matrix with self-loops denoted as = diag( 1, , ).The set represents the 1-hop neighbors of node , encompassingnode itself.We introduce a set of normalized adjacency matrices, detailed in. This set comprises the symmetrically normalized matrixsym, the row-stochastic normalized matrix rs, and the column-stochastic normalized matrix cs, which encompass diverse nor-malization techniques (left multiplication, right multiplication, orboth) applied to the adjacency matrix. Next, we define the propaga-tion operator A to offer a choice among different types of adjacencymatrices:",
  "Definition 3.1. (Propagation operator). The propagation operatorA R is defined as A { , sym, rs, cs}. The expressionsfor the adjacency matrices , sym, rs, cs are detailed in": "The propagation operator encapsulates the prevalent propaga-tion mechanisms commonly employed in GNNs. By substituting theadjacency matrix with the propagation operator A, we can opt forvarious propagation mechanisms that deliver diverse information.Given that heuristics are fundamentally influenced by graphtopology, it is possible to express various heuristics using adjacencymatrices. The (, ) entry of the 2-order adjacency matrix multi-plication denotes the count of common neighbors for nodes and. The (, ) entry of the -order adjacency matrix multiplicationsdenotes the number of length- paths between nodes and . Hence,by employing distinct orders of adjacency matrix multiplications,we can extract varying insights from neighbors or paths. Followingthis intuition, we can express diverse heuristics in matrix form.We provide a concise summary of heuristics, their mathematical",
  ",(1)": "where A() { , sym, rs, cs} for 1 represent thepropagation operators, and A(0) = . The coefficients () for0 modulate the weights of different orders of matrix mul-tiplications, and is the maximum order. Numerous traditionalheuristics align with our formulation under specific configurations. showcases a selection of traditional heuristics and illus-trates their alignment with our formulation through propagationoperators A() and weights (). We assert the formulations abil-ity to accommodate heuristics in Proposition 3.1. The proof forProposition 3.1 can be found in Appendix A.1.",
  "Proposition 3.1. Our formulation can accommodate a broadspectrum of local and global heuristics with propagation operatorsA() for 1 , weight parameters () for 0 , andmaximum order": "Unlike previous methods that exclusively cater to either localor global heuristics, our formulation seamlessly integrates both as-pects, presenting a unified solution. In contrast to prior works rely-ing on abstract functions for heuristic approximation ,our formulation is developed through direct matrix operations. Thisformulation facilitates rigorous equivalence to numerous local andglobal heuristics under specific configurations. It is crucial to notethat our heuristic formulation does not aim to accommodate all pos-sible heuristics. Instead, it aims to distill the critical characteristicsof heuristics, with a specific focus on extracting common neighborsfrom local heuristics and global paths from global heuristics.Existing heuristics are primarily handcrafted and may not beoptimal for real-world graphs. Leveraging the propagation opera-tors, weight parameters and maximum order offers the potential tolearn generalized, possibly more effective heuristics, which we willdiscuss in .5.1 and Appendix D.",
  "HEURISTIC LEARNING GRAPH NEURALNETWORK (HL-GNN)4.1Heuristic Learning Graph Neural Network": "4.1.1Motivation. Direct matrix multiplication serves as a straight-forward method to implement the heuristic formulation in Equa-tion (1). However, it comes with high computational and mem-ory costs. The time complexity of direct matrix multiplication isO(2 3) and the space complexity is O( 2), where denotesthe number of nodes. This is attributed to executing up to -ordermatrix multiplications for times. The significant time and spacecomplexities present two major challenges of ensuring scalabilityand maintaining depth: Scalability. To be scalable, the model must effectively handle largegraphs. Datasets like OGB are substantially larger than those likePlanetoid, making the value of a considerable strain on thetime and space complexities. Depth. To effectively integrate global heuristics into the heuristicformulation, the value of must be sufficiently large to encapsu-late messages from distant nodes. However, increasing furtherstrains the time and space complexities.",
  "Consequently, there is a pressing need to mitigate the burdens ofboth time and space complexities": "4.1.2Architecture. The construction and computation of ma-trices impose significant computational and memory demands. Onepotential technique is initializing A(0) = instead of A(0) = .This approach effectively reduces the feature dimensionality from to , resulting in substantial time and space savings. Moreover,since heuristics cannot leverage node features on attributed graphs,initializing with A(0) = allows the heuristic formulation toutilize node features. Even if node features are of low quality orcompletely absent, we can still train embeddings for each node.Therefore, can represent either raw node features or learnablenode embeddings.Further, we exploit the sparsity of the graph and employ a GraphNeural Network to compute the heuristic formulation. We proposethe efficient and scalable Heuristic Learning Graph Neural Network",
  "=0 () (),(2)": "with () representing the learnable weight of the -th layer, and representing the models depth. An illustration of our proposed HL-GNN is provided in . We do not impose constraints on (),allowing them to take positive or negative values. Adaptive weights () facilitate the integration of multi-range topological informationand govern the trade-off between node features and topologicalinformation. Given the discrete nature of the propagation operatorsA() { , sym, rs, cs}, they obstruct the back-propagationprocess, necessitating their relaxation to a continuous form:",
  "A() = ()1rs + ()2cs + ()3sym,for1 ,(3)": "where ()1 , ()2 , ()3are layer-specific learnable weights harmo-nizing three propagation mechanisms. The continuous relaxationof the propagation operators enables gradient back-propagation,thereby allowing the model to be trained end-to-end. We exclude theadjacency matrix in Equation (3) to ensure that the eigenvalues ofA() fall within the range . Moreover, we apply a softmax func-tion to (), where softmax( ()) = exp( ())/3=1 exp( ()) for = 1, 2, 3. Controlling the eigenvalues of A() helps prevent numer-ical instabilities as well as issues related to exploding gradients orvanishing gradients.HL-GNN employs intra-layer propagation and inter-layer con-nections as described in Equation (2). The salient trait is its elimina-tion of representation transformation and non-linear activation ateach layer, requiring only a few trainable parameters. We assert therelationship between the learned representations and the heuris-tic formulation in Proposition 4.1. The proof for Proposition 4.1can be found in Appendix A.2.",
  ": Illustration of the proposed Heuristic LearningGraph Neural Network (HL-GNN). Every rounded rectanglesymbolizes a left multiplication operation": "According to Proposition 4.1, the learned representations uti-lize heuristics as weights to combine features from all nodes. Theheuristic formulation can be effectively distilled through themessage-passing process in HL-GNN. Consequently, HL-GNN hasthe ability to accommodate and generalize both local and globalheuristics. Our method can be viewed as topological augmentation,employing the topological information embedded in to enhanceraw node features .For sparse graphs, the time complexity of HL-GNN is O(),where is the number of edges. The space complexity of HL-GNNis O(). On a large graph, typically containing millions of nodes,HL-GNN leads to remarkable time and space savings ten andfive orders of magnitude, respectively compared to direct matrixmultiplication. 4.1.3Training. After acquiring the node representations, we em-ploy a predictor to compute the likelihood for each link by = ( ), where is a feed-forward neural network, and represent the representations of node and respectively, and thesymbol denotes the element-wise product.Many methods categorize link prediction as a binary classifica-tion problem and conventionally employ the cross-entropy lossfunction. However, this might not always be the suitable strategy.Standard evaluation procedures in link prediction do not label pos-itive pairs as 1 and negative pairs as 0. The primary objective isto rank positive pairs higher than negative pairs, aligning withthe maximization of the Area Under the Curve (AUC). In light ofthis, we adopt the AUC loss as described in , ensuring it alignsconceptually with the evaluation procedure:",
  "HL-GNNLocal / Global20 hopsO( )": "comparisons is provided in . HL-GNN excels at accommodat-ing and generalizing a wide range of both local and global heuristics.In contrast, SEAL focuses on subgraphs to learn local heuristics,while NBFNet concentrates on paths to learn global heuristics.Neo-GNN leverages two MLPs for local heuristic learning, andBUDDY uses subgraph sketches to represent local heuristics.Notably, most of these methods are limited to topological informa-tion within a 3-hop range. In contrast, HL-GNN can reach a depthof approximately 20 layers, providing a broader information range.Adaptive weights in HL-GNN enable the integration of both localand global topological information.HL-GNN has a time complexity of O(), which is the lowestamong the compared methods. Unlike conventional GNNs, HL-GNN solely utilizes propagation mechanisms and omits transforma-tion and activation functions. SEAL requires running a subgraphGNN with the labeling trick for each link, and NBFNet requiresrunning a global GNN for each source node during training and in-ference. In contrast, HL-GNN only requires running a single globalGNN during training and inference. Furthermore, HL-GNN avoidsthe need to extract topological information from common neigh-bors and subgraph sketches, as required by Neo-GNN and BUDDY,respectively. A detailed time complexity analysis of each method isincluded in Appendix E.",
  "EXPERIMENTS5.1Experiment Setup": "5.1.1Datasets. We utilize nine datasets from three sources: Plan-etoid , Amazon , and OGB . The Planetoid datasets in-clude Cora, Citeseer, and Pubmed. The Amazon datasets includePhoto and Computers. The OGB datasets include ogbl-collab,ogbl-ddi, ogbl-ppa, and ogbl-citation2. Dataset statistics canbe found in Appendix B.1. 5.1.2Baselines. We compare our model against a diverse set ofbaseline methods, including heuristics like CN , RA , KI ,and RWR , traditional embedding-based methods such as MF ,Node2vec , and DeepWalk , as well as conventional GNNslike GCN and GAT . Additionally, we benchmark HL-GNNagainst heuristic-inspired GNN methods like SEAL , NBFNet ,Neo-GNN , and BUDDY . This comprehensive comparisonenable us to assess the performance and effectiveness of the pro-posed HL-GNN.",
  "CoraCiteseerPubmedPhotoComputerscollabddippacitation2Hits@100Hits@100Hits@100AUCAUCHits@50Hits@20Hits@100MRR": "CN33.920.4629.790.9023.130.1596.730.0096.150.0056.440.0017.730.0027.650.0051.470.00RA41.070.4833.560.1727.030.3597.200.0096.820.0064.000.0027.600.0049.330.0051.980.00KI42.340.3935.620.3330.910.6997.450.0097.050.0059.790.0021.230.0024.310.0047.830.00RWR42.570.5636.780.5829.770.4597.510.0096.980.0060.060.0022.010.0022.160.0045.760.00 MF64.671.4365.191.4746.941.2797.920.3797.560.6638.860.2913.684.7532.290.9451.864.43Node2vec68.432.6569.343.0451.881.5598.370.3398.210.3948.880.5423.262.0922.260.8861.410.11DeepWalk70.342.9672.052.5654.911.2598.830.2398.450.4550.370.3426.426.1035.120.7955.581.75 GCN66.791.6567.082.9453.021.3998.610.1598.550.2747.141.4537.075.0718.671.3284.740.21GAT60.783.1762.942.4546.291.7398.420.1998.470.3255.781.3954.125.4319.941.6986.330.54SEAL81.711.3083.892.1575.541.3298.850.0498.700.1864.740.4330.563.8648.803.1687.670.32 NBFNet71.652.2774.071.7558.731.9998.290.3598.030.54OOM4.000.58OOMOOMNeo-GNN80.421.3184.672.1673.931.1998.740.5598.270.7962.130.5863.573.5249.130.6087.260.84BUDDY88.000.4492.930.2774.100.7899.050.2198.690.3465.940.5878.511.3649.850.2087.560.11HL-GNN94.221.6494.311.5188.150.3899.110.0798.820.2168.110.5480.273.9856.770.8489.430.83 5.1.3Experimental settings. In accordance with previous works , we randomly sample 5% and 10% of the links for validationand test sets on non-OGB datasets. We sample the same numberof non-edge node pairs as negative links. For the OGB datasets,we follow their official train/validation/test splits. Following theconvention in previous works , we use Hits@100 as theevaluation metric for the Planetoid datasets, and we use AUC forthe Amazon datasets. For the OGB datasets, we use their officialevaluation metrics, such as Hits@50 for ogbl-collab, Hits@20for ogbl-ddi, Hits@100 for ogbl-ppa, and Mean Reciprocal Rank(MRR) for ogbl-citation2 .We include a linear layer as preprocessing before HL-GNN toalign the dimension of node features with the hidden channels ofHL-GNN. We also leverage node embeddings on the OGB datasetsto enhance the node representations. For the ogbl-collab dataset,we follow OGBs guidelines and use the validation set for training.We evaluate HL-GNN over 10 runs without fixing the random seed.More details about the experiments are provided in Appendix B.2.",
  "Main Results": "As shown in , HL-GNN consistently outperforms all thebaselines on all of the datasets, highlighting its effectiveness androbustness for link prediction tasks. reports the averagedresults with standard deviations. Notably, HL-GNN achieves a re-markable gain of 7.0% and 16.7% in Hits@100 compared to thesecond-best method on the Planetoid datasets Cora and Pubmed,respectively. Moreover, our HL-GNN demonstrates its ability tohandle large-scale graphs effectively, as evidenced by its superiorperformance on the OGB datasets. Specifically, HL-GNN achievesa gain of 13.9% in Hits@100 on ogbl-ppa, and achieves 68.11%Hits@50 on ogbl-collab and 89.43% MRR on ogbl-citation2.Even when node features are absent or of low quality, HL-GNNmaintains consistent performance by learning embeddings for each node, as demonstrated on datasets like ogbl-ddi, which lack nodefeatures.HL-GNN outperforms all listed heuristics, indicating its capacityto generalize heuristics and integrate them with node features. Ac-cording to , local heuristics like CN and RA perform betterthan global heuristics on the OGB datasets, while global heuristicslike KI and RWR perform better on the Planetoid and Amazondatasets. This underscores the importance of establishing a unifiedformulation that accommodates both local and global heuristics.Notably, we can use the configuration in to recover theheuristic RA from HL-GNN without training, achieving a perfor-mance of 49.33% Hits@100 on ogbl-ppa. This result serves as acompelling lower bound for HL-GNNs performance on ogbl-ppa.HL-GNN significantly outperforms conventional GNNs like GCNand GAT across all datasets. Additionally, HL-GNN also surpassesexisting heuristic-inspired GNN methods, including SEAL, NBFNet,Neo-GNN, and BUDDY, suggesting that integrating informationfrom multiple ranges is beneficial for link prediction tasks.",
  "Ablation Studies": "5.3.1Different information ranges. The adaptive weights () inHL-GNN facilitate the integration of multi-range information, en-compassing both local and global topological information. To inves-tigate the impact of information ranges, we conduct experimentsisolating either local or global information. We train a GNN variantusing skip-connections of the first 3 layers as the output, exclu-sively considering local topological information. Similarly, we trainanother GNN variant using the final-layer output with GNN depth 5 to exclusively consider global topological information. Fig-ure 2 demonstrates that HL-GNN consistently outperforms GNNvariants focusing solely on local or global topological information.This underscores HL-GNNs efficacy in adaptively combining bothtypes of information.",
  ": Test performance on the Planetoid and OGB datasetswith different GNN depths": "5.3.2Sufficient model depths. In HL-GNN, achieving sufficientmodel depth is crucial for learning global heuristics and capturinglong-range dependencies. Our model can effectively reach a depthof around 20 layers without performance deterioration, as shown in. In contrast, conventional GNNs often experience a sharpperformance drop after just 2 or 3 layers. For the Planetoid datasetsCora and Pubmed, shallow models yield poor performance, likelydue to the absence of global topological information. Conversely,for the OGB datasets ogbl-collab and ogbl-ddi, deeper models(exceeding 15 layers) result in decreased performance, possiblydue to the introduction of non-essential global information, whichdilutes the crucial local information needed for accurate predictions.",
  "Efficiency Analysis": "5.4.1Time efficiency. Our HL-GNN demonstrates exceptional timeefficiency with the lowest time complexity, as indicated in .The wall time for a single training epoch is provided in .Although HL-GNN generally has a larger depth compared toconventional GNNs, its experimental wall time per training epochis comparable to models like GCN and GAT. In practice, HL-GNNrequires slightly more time than GCN or GAT due to its increaseddepth. However, HL-GNN is several orders of magnitude fasterthan heuristic-inspired GNN methods such as SEAL, NBFNet, andNeo-GNN, thanks to its avoidance of running multiple GNNs andtime-consuming manipulations like applying the labeling trick.",
  "HL-GNN433k1.01M194k99k1.22M": "preprocessing step and the MLP predictor. compares thenumber of parameters in HL-GNN with other GNN methods, clearlyhighlighting HL-GNNs superior parameter efficiency. Our modelstands out as the most parameter-efficient among the listed con-ventional GNNs and heuristic-inspired GNN methods.While conventional GNNs excel in efficiency but may lack inperformance, and heuristic-inspired GNN methods are effectivebut time and parameter-intensive, HL-GNN strikes a balance. Itconsistently achieves top-tier prediction performance on numerouslink prediction benchmarks, maintains excellent scalability and timeefficiency, and stands out as the most parameter-efficient method.",
  "Interpretability Analysis": "5.5.1Generalized heuristics and learned weights. Leveraging thecapabilities of the unified formulation, we can derive generalizedheuristics by analyzing the learned parameters of HL-GNN. Thegeneralized heuristics and learned weights () provide insightsinto the graph-structured data. The learned weights () are visuallydepicted in . Due to space constraints, the formulas for thegeneralized heuristics are provided in Appendix C.2.For the Cora and Citeseer datasets, the learned weights mono-tonically decrease, indicating that the graph filter serves as a low-pass filter. The weight (0) has the largest magnitude, suggestingthat crucial information is primarily contained in node features.Local topological information from nearby neighbors plays a majorrole, while global topological information from distant nodes servesas a complementary factor. Conversely, for the ogbl-collab and",
  ": Learned weights () with = 20 for the Coraand Citeseer datasets, and = 15 for the ogbl-collab andogbl-ddi datasets": "ogbl-ddi datasets, the weights do not monotonically increase ordecrease. Instead, they experience a significant change, especially inthe first 5 layers, indicating that the graph filter serves as a high-passfilter. The weight (2) has the largest magnitude, suggesting thatcrucial information lies in local topology rather than node features.Moreover, for large values of on the ogbl-collab and ogbl-ddidatasets, the weights () become negative, suggesting that globaltopological information from distant nodes compensates for exces-sive information from nearby neighbors. The learnable weights ()",
  "govern the trade-off between node features and topological informa-tion, enabling the adaptive integration of multi-range topologicalinformation": "5.5.2Leveraging generalized heuristics. With the generalized heuris-tic for each dataset, there is no need to train a GNN and an MLPpredictor from scratch. Instead, we can simply follow the general-ized heuristic and train an MLP predictor only, which is significantlymore efficient than training from scratch. The performance of train-ing the MLP alone is comparable to training from scratch, but itconverges more quickly. The training time for training from scratchversus training only the MLP is shown in , while the perfor-mance details are provided in Appendix C.3. The slight decrease inperformance can likely be attributed to the fact that, when trainingthe GNN and MLP together, the gradients flow through both blocks,allowing them to adapt to each other. In contrast, training the MLPalone limits its ability to capture complex interactions between thetwo blocks.",
  ": Learned weights () with = 20 for the synthetictriangular and hexagonal networks": "nodes forming a triangle. As each pair of nodes shares two commonneighbors, we anticipate that the learned heuristic would resemble alocal heuristic focusing on 2-hop information. The learned weightsare presented in , with (2) having the largest magnitude,corresponding to a local heuristic.The hexagonal network also comprises 1000 nodes, with everysix nodes forming a hexagon. Here, we expect the learned heuris-tic to resemble a global heuristic focusing on 5-hop information.As shown in , the weight (5) has the largest magnitude,corresponding to a global heuristic. In both cases, HL-GNN demon-strates its ability to adaptively learn the most effective heuristicbased on the specific topology. This also emphasizes the importanceof developing a formulation that can effectively accommodate bothlocal and global heuristics.",
  "CONCLUSION": "We introduce a unified formulation that accommodates and gen-eralizes both local and global heuristics using propagation oper-ators and weight parameters. Additionally, we propose HL-GNN,which efficiently implements this formulation. HL-GNN combinesintra-layer propagation and inter-layer connections, allowing theintegration of multi-range topological information. Experimentsdemonstrate that HL-GNN achieves state-of-the-art performanceand efficiency. This study is confined to undirected graphs; fordirected graphs, we preprocess them by converting them into undi-rected graphs. Extensions to multi-relational graphs, such as knowl-edge graphs, are left for future work. This work is supported by National Key Research and DevelopmentProgram of China (under Grant No.2023YFB2903904), NationalNatural Science Foundation of China (under Grant No.92270106)and Beijing Natural Science Foundation (under Grant No.4242039).",
  "Yaqing Wang, Zaifei Yang, and Quanming Yao. 2024. Accurate and interpretabledrug-drug interaction prediction enabled by knowledge subgraph learning. Com-munications Medicine 4, 1 (2024), 59": "Lanning Wei, Huan Zhao, Quanming Yao, and Zhiqiang He. 2021. Pooling archi-tecture search for graph classification. In Proceedings of the 30th ACM InternationalConference on Information & Knowledge Management. 20912100. Qitian Wu, Wentao Zhao, Zenan Li, David P Wipf, and Junchi Yan. 2022. Node-former: A scalable graph structure learning transformer for node classification.Advances in Neural Information Processing Systems 35 (2022), 2738727401. Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichiKawarabayashi, and Stefanie Jegelka. 2018. Representation learning on graphswith jumping knowledge networks. In International conference on machine learn-ing. PMLR, 54535462. Haoteng Yin, Muhan Zhang, Yanbang Wang, Jianguo Wang, and Pan Li. 2022. Al-gorithm and system co-design for efficient subgraph-based graph representationlearning. arXiv preprint arXiv:2202.13538 (2022). Seongjun Yun, Seoyoon Kim, Junhyun Lee, Jaewoo Kang, and Hyunwoo J Kim.2021. Neo-gnns: Neighborhood overlap-aware graph neural networks for linkprediction. Advances in Neural Information Processing Systems 34 (2021), 1368313694.",
  "conforms to the formulation when A() = rs for 1, () =(1 ) for 0, =": "Proof. Random Walk with Restart (RWR) calculates the station-ary distribution of a random walker starting at , who iterativelymoves to a random neighbor of its current position with probability or returns to with probability 1. Let denote the probabilityvector of reaching any node starting a random walk from node .Let cs be the transition matrix and ( cs) = ( 1) =1 if",
  "BADDITIONAL EXPERIMENTAL DETAILSB.1Detailed Information about Datasets": "We employ a diverse set of benchmark datasets to comprehensivelyevaluate the link prediction performance of HL-GNN. Each datasetcaptures distinct aspects of relationships and presents unique chal-lenges for link prediction. We utilize nine datasets from threesources: Planetoid , Amazon , and OGB . The Plane-toid datasets include Cora, Citeseer, and Pubmed. The Amazondatasets include Photo and Computers. The OGB datasets includeogbl-collab, ogbl-ddi, ogbl-ppa, and ogbl-citation2. The sta-tistics of each dataset are shown in .We introduce each dataset briefly: Cora: This dataset represents a citation network. Nodes in thegraph correspond to documents, and edges denote citationsbetween documents. Each document is associated with a bag-of-words representation. Citeseer: Similar to Cora, Citeseer is also a citation network.Nodes represent scientific articles, and edges signify citations.The node features include bag-of-words representations andinformation about the publication. Pubmed: Another citation network, Pubmed comprises nodescorresponding to scientific publications, and edges representcitations. Node features consist of binary indicators of wordoccurrences in the documents.",
  "ogbl-ddi: ogbl-ddi stands for drug-drug interaction, and thisnetwork models interactions between drugs. Edges indicateinteractions, and the dataset contains information about drugstructures": "ogbl-ppa: This dataset represents a protein-protein associa-tion network. Nodes correspond to proteins, and edges indicateassociations based on various biological evidence. Node fea-tures include protein sequences and structural information. ogbl-citation2: This dataset is a citation network that fo-cuses on the dynamics of citations. Nodes represent papers,and edges indicate citation relationships. Node features consistof paper metadata.These datasets are widely used for evaluating link predictionmethods, each presenting unique challenges based on the nature ofthe relationships captured in the respective graphs.",
  "B.2Detailed Information about Experiments": "In our experiments, we employ a set of hyperparameters for trainingHL-GNN across various datasets. The hyperparameter details areoutlined in . All experiments can be performed using anA100 (80GB) GPU. Hyperparameters are tuned via random searchusing Weights and Biases, involving 50 runs. The hyperparametersyielding the highest validation accuracy were selected, and resultsare reported on the test set.The number of layers in HL-GNN is set to 20 for the Planetoidand Amazon datasets, and set to 15 for the OGB datasets. We keepthe hidden dimensionality of HL-GNN the same as the raw nodefeatures on the Planetoid and Amazon datasets. Additionally, weleverage node embeddings on the OGB datasets to enhance thenode representations. The dimensions of node embeddings are setto 256, 512, 256, and 64 for ogbl-collab, ogbl-ddi, ogbl-ppa, andogbl-citation2, respectively.Inspired by heuristics KI and RWR presented in , weexplore different initialization strategies for (). We adopt KI ini-tialization as () = and RWR initialization as () = (1 ).We consistently adopt KI initialization for the OGB datasets andRWR initialization for the Planetoid and Amazon datasets (exceptfor Pubmed, which utilizes KI initialization). We do not impose con-straints on (), allowing them to take positive or negative values.The MLP utilized in HL-GNN serves as the predictor and com-prises 3 layers for most Planetoid and Amazon datasets, and com-prises 2 layers for the OGB datasets. We employ the Adam optimizerwith a fixed learning rate of 0.001 for all datasets. Training is carriedout for a specified number of epochs, ranging from 100 to 800, de-pending on the dataset. Dropout regularization is optimized among{0.3, 0.4, 0.5, 0.6, 0.7}.",
  "CADDITIONAL EXPERIMENTSC.1More Ablation Studies": "C.1.1Different initialization strategies. We investigate the impactof various initial values for () on the final performance in thissection. Inspired by heuristics KI and RWR in , we exploredifferent initialization strategies, including: KI initialization: () = . RWR initialization: () = (1 ). Random initialization: () N (0, 1). Uniform initialization: () = 1/. Reverse-KI initialization: () = . Final-layer initialization: () = 1, with others set to 0.The results of these different initialization strategies are pre-sented in , with performance comparisons on four datasets.Notably, the KI and RWR initialization strategies outperform the",
  ": Ablation study on different initialization strategiesacross various datasets": "others, underscoring the importance of emphasizing short-rangedependencies, given their direct topological relevance.It is noteworthy that even without extensive tuning, KI initial-ization at = 0.5 for the OGB datasets and RWR initializationat = 0.2 for the Planetoid and Amazon datasets consistentlydemonstrate strong performance. In practical applications, we con-sistently adopt KI initialization for the OGB datasets and RWRinitialization for the Planetoid and Amazon datasets (except forPubmed, which utilizes KI initialization) in our experiments. Theinitialization strategies for each dataset are summarized in . C.1.2Introduction of transformation and activation functions. Toinvestigate if the introduction of transformation and activationfunctions could potentially improve the performance of HL-GNN,we conduct additional experiments on these two functions. Theresults are summarized in .The results reveal that the introduction of transformation andactivation functions do not lead to an improvement in performance.This outcome can be attributed to the disturbance of learned heuris-tics by non-linearity, preventing them from retaining the form ofmatrix multiplications, which is the foundation of our formulation.Furthermore, the inclusion of transformation matrices significantlyincreases the number of parameters, posing challenges to the learn-ing process. Consequently, HL-GNN with transformation is unableto maintain the depth of around 20 layers. The optimal performanceis achieved with a limited depth of less than 5 layers. This limitationcontributes to the observed inferior performance of HL-GNN (w/tran.) and HL-GNN (w/ ReLU act. & tran.).",
  "C.2Interpretability of Generalized Heuristics": "In this section, we present the learned heuristic formulation to exhibit the interpretability of generalized heuristics. Its worthnoting that each individual heuristic for a link (, ) can be directlyextracted from the (, ) entry of . For clarity, we simplify Equa-tion (3) by omitting () and utilizing A() = sym. For conciseness,we provide the first ten weights below.For Cora,",
  "With a generalized heuristic for each dataset, there is no need totrain a GNN and predictor from scratch. Instead, we can simply": "follow the generalized heuristic and train an MLP predictor only,which is significantly more efficient than training from scratch. Weutilize a pretrained preprocessing block, which is a linear layer thattransforms the dimension of node features into hidden channels ofthe GNN. We also train node embeddings on the OGB datasets toenhance the node representations.The performance for training from scratch and training the MLPonly is presented in . Training the MLP only yields com-parable performance to training from scratch, but with quickerconvergence. This highlights the efficacy of the learned generalizedheuristics, allowing us to achieve comparable results efficiently bytraining a simple MLP. This significantly improves efficiency andconserves computational resources.",
  "ETIME COMPLEXITY ANALYSIS": "GCN . The propagation cost is O() and the transformationcost with weight matrices () is O(2) if we do not modifyfeature dimensionality with (). And the total cost is O( ( +)).GAT . Denote the number of attention heads as and the av-erage degree of nodes as . Attention computation cost is O(2).The cost of computing one nodes representation is O(22).For attention heads and layers, the overall time complexity isO(22).SEAL . Denote and as the average number of edgesand vertices in the subgraphs. The complexity in constructing theenclosing subgraph is at most O(3), and the cost of computingshortest path is dependent on the algorithm, and the most efficient Dijkstras algorithm is O( 2). The cost of the subgraph GNN isO(). The algorithm need to be done for each edge, and theoverall cost is O(( 2 + )).NBFNet . INDICATOR function takes O() cost, MESSAGEfunction takes O((+)) cost, and AGGREGATE function takesO(2) cost. The algorithm need to be done for each source node,and the total cost is O( ( + )).Neo-GNN . Denote the high-order matrix power as andthe average degree of nodes as . Node structural feature computa-tional cost is O(2), computing high-order matrix power up to is O( 2), computing output feature is O(). And Neo-GNNalso needs a conventional GNN which needs a O(( + 2))cost. The total cost is O( + 2).BUDDY . Denote the cost of hash operations as . The pre-processing cost of BUDDY is O(( + )). A link probability iscomputed by (i) extracting ( + 2) structural features, which costsO(2); (ii) An MLP on structural and node features, which costsO(2 + 2). The cost for computing probabilities for all links isO(( + 2)). The total cost is O(( + 2)).",
  "FLIMITATION": "Our heuristic formulation does not aim to encompass all possibleheuristics. A successful formulation generalizes heuristic common-alities and applies them for effective link prediction. Our formu-lation distills the critical characteristics of heuristics, specificallyfocusing on extracting common neighbors from local heuristics andglobal paths from global heuristics.Upon careful consideration of the comprehensive survey ,our formulation may not contain certain heuristics with normal-ization operators like union, log, minimum, and maximum in thedenominator. For example, heuristics like Jaccard and Adamic-Adar,due to their use of union and log operators in the denominator, can-not be directly represented as matrix multiplications. However, ourformulation introduces normalization through the three propaga-tion mechanisms and layer-specific learnable weights to adaptivelycontrol the degree of normalization, potentially mitigating the needfor additional operators."
}