{
  "ABSTRACT": "Automated Machine Learning (AutoML) techniques have recentlybeen introduced to design Collaborative Filtering (CF) models ina data-specific manner. However, existing works either search ar-chitectures or hyperparameters while ignoring the fact they areintrinsically related and should be considered together. This moti-vates us to consider a joint hyperparameter and architecture searchmethod to design CF models. However, this is not easy becauseof the large search space and high evaluation cost. To solve thesechallenges, we reduce the space by screening out usefulness hy-perparameter choices through a comprehensive understanding ofindividual hyperparameters. Next, we propose a two-stage searchalgorithm to find proper configurations from the reduced space. Inthe first stage, we leverage knowledge from subsampled datasets toreduce evaluation costs; in the second stage, we efficiently fine-tunetop candidate models on the whole dataset. Extensive experimentson real-world datasets show better performance can be achievedcompared with both hand-designed and previous searched models.Besides, ablation and case studies demonstrate the effectiveness ofour search framework.",
  "Corresponding author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 23, August 610, 2023, Long Beach, CA, USA. 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0103-0/23/08...$15.00",
  "Recommendation System; Collaborative Filtering; Automated Ma-chine Learning": "ACM Reference Format:Yan Wen, Chen Gao, Lingling Yi, Liwei Qiu, Yaqing Wang, and Yong Li.2023. Efficient and Joint Hyperparameter and Architecture Search for Col-laborative Filtering. In Proceedings of the 29th ACM SIGKDD Conference onKnowledge Discovery and Data Mining (KDD 23), August 610, 2023, LongBeach, CA, USA. ACM, New York, NY, USA, 12 pages.",
  "INTRODUCTION": "Collaborative Filtering (CF) is the most widely used approach forRecommender Systems , aiming at calculating thesimilarity of users and items to recommend new items to potentialusers. They mainly use Neural Networks to build models for usersand items, simulating the interaction procedure and predict the pref-erences of users for items. Recent works also built CF models basedon Graph Neural Networks (GNNs) . While CF mod-els may have different performance on different scenes , recentworks have begun to apply Automated Machine Learning(AutoML) to search data-specific CF models. Previous works, in-cluding SIF , AutoCF and , applied Neural ArchitectureSearch (NAS) on CF tasks. They split the architectures of CF modelsinto several parts and searched each part on architecture space.However, most of these methods focus on NAS in architecturespace, only considering hyperparameters as fixed settings and there-fore omitting the dependencies among them. A CF model can bedecided by a given architecture and a configuration of hyperparam-eters. Especially in the task of searching best CF models, hyper-parameter choice can affect search efficiency and the evaluationperformance an architecture can receive on a given dataset. Recentmethods mainly focus on model search, ignoring the important roleof hyperparameters. For instance, SIF focuses on interaction func-tion, while it uses grid search on hyperparameters space. AutoCF",
  "KDD 23, August 610, 2023, Long Beach, CA, USA.Yan Wen et al": "does not use hyperparameter tuning on each architecture, whichmay make the searched model sub-optimal since proper hyperpa-rameters vary for different architectures. includes GNN modelsin the architecture space, but the search and evaluation cost forarchitectures may be high.We find that these works only focus on either hyperparametersor fixed parts in CF architecture, neglecting the relation betweenarchitecture and hyperparameters. If architecture cannot be eval-uated with proper hyperparameters, a sub-optimal model may besearched, possibly causing a reduction in performance. To summa-rize, there exists a strong dependency between hyperparametersand architectures. That is, the hyperparameters of a model are basedon the design of architectures, and the choices of hyperparametersalso affect the best performance an architecture may approach. Wesuppose that hyperparameters can be adaptively changed when thearchitecture change in CF tasks, so we consider that the CF searchproblem can be defined on a joint space of hyperparameters andarchitectures. Therefore, the CF problem can be modeled as a jointsearch problem on architecture and hyperparameter space. Whilehyperparameters and architectures both influences the cost andefficiency of CF search task, there exist challenges for joint searchproblems: (1) Since the joint search space is designed to includeboth hyperparameters and architectures, the joint search problemhas a large search space, which may make it more difficult to findthe proper configuration of hyperparameters and architectures; (2)In a joint search problem, since getting better performance on agiven architecture requires determining its hyperparameters, theevaluation cost may be more expensive.We propose a general framework, which can optimize CF ar-chitectures and their hyperparameters at the same time in searchprocedure. The framework of our method is shown in b,consisting of two stages. Prior to searching hyperparameters andarchitectures, we have a full understanding of the search spaceand reduce hyperparameter space to improve efficiency. Specifi-cally, we reduced the hyperparameter space by their performanceranking on CF tasks from different datasets. We also propose afrequency-based sampling strategy on the user-item matrix for fastevaluation. In first stage, we search and evaluate models on reducedspace and subsampled datasets, and we jointly search architectureand hyperparameters with a surrogate model. In second stage, wepropose a knowledge transfer-based evaluation strategy to leveragesurrogate model to larger datasets. Then we evaluate the modelwith transferred knowledge and jointly search the hyperparametersand architectures to find the best choice in original dataset.Overall, we make the following important contributions:",
  "We propose an approach that can jointly search CF architecturesand hyperparameters to get performance from different datasets": "We propose a two-stage search algorithm to efficiently optimizethe problem. The algorithm is based on a full understanding ofsearch space and transfer ability between datasets. It can jointlyupdate CF architectures and hyperparameters and transfer knowl-edge from small datasets to large datasets. Extensive experiments on real-world datasets demonstrate thatour proposed approach can efficiently search configurations indesigned space. Furthermore, results of ablation and case studyshow the superiority of our method.",
  "RELATED WORK2.1Automated Machine Learning (AutoML)": "Automated Machine Learning (AutoML) refers to a type ofmethod that can learn models adaptively to various tasks. Recently,AutoML has achieved great success in designing the state-of-the-art model for various applications such as image classification andsegmentation , natural language modeling , andknowledge graph embedding .AutoML can be used in mainly two fields: Neural ArchitectureSearch (NAS) and Hyperparameter Optimization (HPO). NAS splits architectures into several components andsearches for each part of the architecture to achieve the wholepart. DARTS uses gradient descent on continuous relaxationof the architecture representation, while NASP improvesDARTS by including proximal gradient descent on architectures. HPO , usually tuning the hyperparameters of a givenarchitecture, always plays an important role in finding the besthyperparameters for the task. Random Search is a frequentlyused method in HPO for finding proper hyperparameters. Algo-rithms for HP search based on model have been developed foracceleration , including Bayesian Optimization (BO) methodslike Hyperopt ,BOHB and BORE , etc. Recent studies on AutoML have shown that incorporating HPOinto the NAS process can lead to better performance and a moreeffective exploration of the search space. For example, a study onResNet showed that considering the NAS process as HPO canlead to improved results. Other works, such as AutoHAS , haveexplored the idea of considering hyperparameters as a choice in thearchitecture space. FEATHERS has focused on the joint searchproblem in Federated Learning. ST-NAS uses weight-sharingNAS on architecture space and consider HP as part of architectureencoding. These studies demonstrate the potential of joint search onhyperparameters and architectures in improving the performanceand efficiency of machine learning models.",
  "Collaborative Filtering (CF)": "2.2.1Classical CF Models. Collaborative Filtering (CF) is themost fundamental solution for Recommender Systems (RecSys).CF models are usually designed to learn user preferences based onthe history of user-item interaction. Matrix Factorization (MF) generates IDs of users and items, using a high-dimensional vectorto represent the location of users and items features. The innerproduct is used as interaction function, calculating the similar-ity of user/item vectors. The MF-based method has been demon-strated effective in SVD++ and FISM . NCF appliedneural networks to building CF models, using a fused model withMF and multi-layer perceptron (MLP) as an interaction function,taking user/item embeddings as input, and inferring preferencescores. JNCF extended NCF by using user/item history to re-place user/item ID as the input encoding.Recently, the user-item interaction matrix can also be consideredas a bipartite graph, thus Graph Neural Networks (GNNs) arealso applied to solve CF tasks for their ability to capture the high-order relationship between users and items . They considerboth users and items as nodes and the interaction of users and",
  "Efficient and Joint Hyperparameter and Architecture Search for Collaborative FilteringKDD 23, August 610, 2023, Long Beach, CA, USA": "found in Table A1 in Appendix. We also compare our method withconventional method in Figure A1.To briefly summarize our algorithm: In the first stage (Lines3-14), we sample several subgraphs with frequency-based methods,and preserve = 0.2 of interactions from the original rating matrix.Based our understanding of hyperparameters in .1, weselect architecture and its hyperparameters in our reduced hyper-parameter space.We use a surrogate model to find proper architecture and hyper-parameters to improve search efficiency, noted as (,), is theparameter of the model. After we get evaluations of architectureand hyperparameters, we update parameters of . In our frame-work, we choose BORE as a surrogate and Random Forest (RF)as the regressor to modulate the relation between search compo-nents and performance. Details of the BORE+RF search algorithm(RFSurrogate) is shown in Algorithm A1 in Appendix.We first transfer the parameters of trained in the first stageby collected configurations and performance. Then, in the secondstage (Lines 15-22), we select architectures and hyperparametersand evaluate them in the original dataset. Besides, we increasethe embedding dimension to reach better performance. Similarly,the configurations and performance are recorded for the surrogatemodel (RF+BORE), which can give us the next proper configuration.Finally, after two-stage learning on the subsampled and originaldatasets, we get the final output of architecture and hyperparame-ters as the best choice of architecture and its hyperparameters.",
  "Prediction FunctionSUM, VEC, MLP": "as well as frequently utilized hyperparameters in the learning stagewithin H. Since this space contains various types of components,including continuous hyperparameters and categorical architec-tures, it is essential to appropriately encode the joint search spacefor effective exploration. Secondly, considering the dependencybetween hyperparameters and architectures, the search strategyshould be robust and efficient, meeting the accuracy and efficiencyrequirements of real-world applications. Compared to previous Au-toML works on CF tasks, our method is the first to consider a jointsearch on hyperparameters and architectures on CF tasks.",
  "Architecture Space: A": "In this paper, the general architecture of CF models can be separatedinto four parts : Input Features, Feature Embedding, InteractionFunction, and Prediction Function. Based on the frequently usedoperations, we build the architecture space A, illustrated in . Input Features The input features of CF architectures come fromoriginal data: user-item rating matrix. We can apply the interac-tions between users and items and map them to high-dimensionvectors. There are two manners for encoding users and items asinput features: one-hot encoding (ID) and multi-hot encoding (H).As for one-hot encoding (ID), we consider the reorganized id ofboth users and items as input. Since the number of users and itemsis different, we should maintain two matrices when we generatethese features. As for multi-hot encoding (H), we can consider theinteraction of users and items. A user vector can be encoded inseveral places by its historical interactions with different items. Theitems can be encoded in the same way. Feature Embedding The function of feature embedding mapsthe input encoding with high dimension into vectors with lowerdimension. According to designs in .2, we can elaboratethe embedding manners in two categories: Neural Networks based(NN-based) embeddings and Graph Neural Networks based (Graph-based) embeddings. The embedding function is related to the size ofinput features. As for NN-based methods, a frequently used methodof calculation is Mat, mainly consists of a lookup-table in ID level,and mean pooling on both users/items sides. We can also use amulti-layer perceptron (MLP) for each user and item side, helpingconvert multi-hot interactions into low-dimensional vectors. Asfor Graph-based methods, the recent advances in GNNs use more",
  "complex graph neural networks to aggregate the neighborhoods,such as GraphSAGE , HadamardGNN , and SGC etc": "Interaction Function The interaction function calculates therelevance between a given user and an item. In this operation, theoutput is a vector affected by the embeddings of the user and item.The inner product is frequently used in many research works onCF tasks. We split the inner product and consider an element-wiseproduct as an essential operation, which is noted as multiply. Incoding level, we can also use minus, max, min and concat. They helpus join users and items with different element-wise calculations. Prediction Function This operation stage helps turn the outputof the interaction function into an inference of similarity. As for theoutput vector of a specific interaction, a simple way is to use summa-tion on the output vector. Thus, multiply+SUM can be consideredthe inner product in this way. Besides, we use a weight vector withlearnable parameters, noted as VEC. Multi-layer perceptron (MLP)can also be used for more complex prediction on similarity.",
  "Hyperparameter Space: H": "Besides the model architecture, the hyperparameter (HP) settingalso plays an essential role in determining the performance of theCF model. The used components for hyperparameter space areillustrated in the first column of .The CF model, like any machine learning model, consists ofstandard hyperparameters such as learning rate and batch size.Specifically, excessively high learning rates can hinder convergence,while overly low values result in slow optimization. The choice ofbatch size is also a trade-off between efficiency and effectiveness.In addition, CF model has specific and essential hyperparameters,which may not be so sensitive in other machine learning models.Embedding dimension for users and items influence the represen-tative ability of CF models. Besides, the embedding size determinesthe models capacity to store all information of users and items. In general, too-large embedding dimension leads to over-fitting,and too-small embedding dimension cannot fit the complex user-item interaction data. The regularization term is always adopted toaddress the over-fitting problem.",
  "SEARCH STRATEGY": "As mentioned in , joint search on hyperparameters andarchitectures have two challenges in designing a search strategyeffectively and efficiently: the large search space and the high eval-uation costs on large network datasets. Previous research works onmodel design with joint search space of hyperparameters and archi-tectures such as consider the search problem in the mannerof a. They considered the search component in space jointlyand used different search algorithms to make the proper choice.The search procedure can be costly on a large search space anddataset. Therefore, addressing the challenges of a vast search spaceand the costly evaluation process is crucial.The first challenge means the joint search space of H and Adescribed in is large for accurate search. Thus, we needto reduce the search space. In practice, we choose to screen Hchoices by comparing relative ranking in controlled variable ex-periments, which is explained in .1. The second challengemeans the evaluation time cost in Equation (2) is high. A significantvalidation time will lower the search efficiency. Thus, we apply asampling method on datasets by interaction frequency, elaboratedin .2.In comparison to conventional joint search problem in a,we design a two-stage search algorithm in b. We use Ran-dom Forest (RF) Regressor, a surrogate model, to improve the effi-ciency of the search algorithm. To transfer the knowledge, includingthe relation modeling between hyperparameters, architectures, andevaluated performance, we learn the surrogate models parameters in the first stage, and we use as initialization of RF model in the",
  "Screening Hyperparameter Choices": "We screen the hyperparameter (HP) choices from H to H with twotechniques. First, we shrink the HP space by comparing relativeperformance ranking of a special HP while fixing the others. Af-ter we get the ranking distribution of different HPs, we find theperformance distribution among different choices of a HP, thus wemay find the proper range or shrunk set of a given HP. Second, wedecouple the HP space by calculating the consistency of differentHP. If the consistency of a HP is high, that means the performancecan change positively or negatively by only alternating this HP.Thus, this HP can be tuning separately neglecting its relation withother HPs in HP set. 4.1.1Shrink the hyperparameter space. The screening method onhyperparameter space is based on analysis of ranking distributionon the performance with fixed value for a certain HP and randomchoices for other HPs and architectures. In this part, we denotea selection of hyperparameter H as a vector, noted as =(1),(2), . . . ,(). For instance, (1) means optimizer, and (2) means learning rate.To obtain the ranking distribution of a certain HP (), we startwith a controlled variable experiment. We vary () in discretevalues as the third column in , and we vary other HPs inoriginal range as the second column. Specifically, given as adiscrete set of HP (), we choose a value and we can obtainthe ranking rank(h, ) of the anchor HP h H by fixing otherHPs except the -th HP. embedding dim learning rate optimizer batch size weight decay 0.0 0.1 0.2 0.3 0.4 0.5 0.6 SRCC 0.4874 0.2445 0.03040.01210.0102",
  ": Consistency of each hyperparameters": "To ensure a fair evaluation of different architecture, we traversethe architecture space and calculate rank of performance with dif-ferent configurations, then we can get the distribution of a type ofHP. The relative performance ranking with different HPs is shownas violin plots in . In this figure, we can get H throughthe distribution of different HP values. We learn that the properchoice for optimizer can be shrunk to Adam and Adagrad; Properrange for learning rate is (1e-5, 1e-2); Proper range for embeddingdimension can be reduced to ; And we can fix weight decayin our experiments. We demonstrate the conclusion in the fourthcolumn in . 4.1.2Decouple the hyperparameter space. To decouple the searchspace, we consider the consistency of the ranking of hyperparam-eters when only alternating a given hyperparameter. For the -thelement () of H, we can change different values for (),and then we can decouple the search procedure of the -th hyper-parameter with others. We use Spearman Rank-order CorrelationCoefficient (SRCC) to show the consistency of various types of HPs,which is definited in Equation (3).",
  "|H | (|H |2 1).(3)": "where |H | means the number of anchor hyperparameters in H.SRCC demonstrates the matching rate of rankings for anchor hy-perparameters in H with respect to () = 2 and () = 2.The SRCC of the-th HP is evaluated by the average of SRCC(1, 2)among different pairs of , as is shown in Equation (4).",
  "(1,2) SRCC(1, 2).(4)": "The SRCC results is demonstrated in , we can directly findthat the important hyperparameter with higher SRCC has a morelinear relationship with its values. Since high embedding dimensionmodel is time-costly during training and evaluation, we decide touse lower dimension in the first stage to reduce validation time, andthen raise them on the original dataset to get better performance.To summarize, we shrink the range of HP search space and findthe consistency of different HPs. The shrunk space shown in help us search more accurately, and the consistency analysis onperformance ranking help us find the dependency between differentHPs, thus we can tune HP with high consistency separately.",
  "Evaluating Architecture with Sampling": "To evaluate architecture more efficiently, we collect performanceinformation evaluated from subgraphs, since the subgraph canapproximate the properties of whole graph . In this part, weintroduce our frequency-based sampling method, and then we showthe transfer ability of subgraphs by testing the consistency of archi-tectures performance from subsampled dataset to origin dataset. 4.2.1Matrix sampling method. Since dataset for CF based on in-teraction of records, our sampling method is based on items ap-pearance frequency. That is, we can subsample the original datasetwhen we preserve part of the bipartite graph, and the relative per-formance on smaller datasets should have a similar consistency (i.e.ranking distribution of performance on Sval and Sval).The matrix subsample algorithm is demonstrated in Algorithm 1.First, we set the number of user-item interactions to be preservedfirst, which can be controlled by a sample ratio , (0, 1). We cal-culate the interactions for each item, and then we preserve the itemwith a higher frequency. The items can be chosen in a fixed list (i.e.topk, Line 6-7 in Algorithm 1), or the interaction frequency countof items can be normalized to a probability, then different itemshave corresponding possibility to be preserved (i.e. distribute,Line 8-10 in Algorithm 1). 4.2.2Transfer ability of architecture on subsampling matrix. To en-sure that the relative performance ranking on subsampled datasetsis similar to that on original datasets, we need to test the consis-tency of architecture ranking on different datasets. We evaluate thetransfer ability among from subgraph to whole graph by SRCC.For a given value of , we choose to select a sample set of ar-chitecture from A. Then we evaluate them on a subsampleddataset S and origin dataset S. The relative rank of on Sand S is noted as rank(, S) and rank(, S).",
  "| | (| |2 1).(5)": "We can choose different subsampled dataset S to get averageconsistency. As is demonstrated in , sample ratio withhigher SRCC has better transfer ability among graphs with samplemode topk, and the proper sample ratio should be in [0.2, 1).To summarize, through the sampling method, the evaluationcost will be reduced, and thus the search efficiency is improved.Since the ranking distribution among subgraphs is similar to thatof the original dataset, we can transfer the evaluation modelingfrom small to large dataset.",
  "Datasets. We use MovieLens-100K, MovieLens-1M, Yelp, andAmazon-Book for CF tasks. The detailed statistics and preprocessstage of datasets are shown in Table A2 in Appendix A.4": "5.1.2Evaluation metrics. As for evaluation metrics, we choose twowidely used metrics for CF tasks, Recall@ and NDCG@. Accord-ing to recent works , we set the length for recommendedcandidates as 20. We use Recall@20 and NDCG@20 as validation.As for loss function, we use BPR loss , the state-of-the-art lossfunction for optimizing recommendation models. 5.1.3Baselines for Comparison. Since we encode both hyperpa-rameters and architectures, we can use previous search algorithmson hyperparameters and extend them on a joint searchspace. The details of search algorithms can be found in Appen-dix A.5.1.",
  "Performance Comparison (RQ1)": "For CF tasks, we compare our results with NN-based CF modelsand Graph-based CF models. Besides, we also compare our searchalgorithm with other search algorithms designed for CF models. Thesearch space is based on analysis of hyperparameter understandingin .1. We report the performance on four datasets in .We summarize the following observation: We find that in our experiment, our CF model trained by searchedhyperparameters and architectures can achieve better perfor-mance than the classical CF models. Some single models alsoperform well due to their special design. For example, LightGCNperforms well on ML-100K and Yelp, while NGCF also performswell on ML-1M. Since our search algorithm has included vari-ous operations used in CF architectures, the overall architecturespace can cover the architectures of classical models. Compared to NAS method on CF models, our method worksbetter than SIF for considering multiple operations in differentstages of architecture. Our methods also outperform AutoCF by3.10% to 12.1%. The reason for that is we have an extended jointsearch space for both hyperparameters and architectures. Besides,our choice for hyperparameters is shrunk to a proper range toimprove search efficiency and performance.",
  "Algorithm Efficiency (RQ2)": "We compare the different search algorithms mentioned in Sec-tion 5.1.3. The search results are shown in a on datasetML-100K and b on dataset ML-1M. We plot our results bythe output of the search algorithm. As is demonstrated in band 5a, search algorithms of BO perform better than random search,since the BO method considers a Gaussian Process surrogate modelfor simulating the relationship between performance output andhyperparameters and architectures. We find that BORE+RF out-performs other search strategies in efficiency. The reason is thatthe surrogate model RF can better classify one-hot encoding ofarchitectures. Besides we also compare different time curves forBORE+RF in single-stage and two-stage evaluations. Since our two-stage algorithm uses subsampling method on rating matrix, andensure the consistency between subgraph and whole graph, we canachieve better performance and higher search efficiency.",
  "Plausibility of screening hyperparameters. To further validatethe effectiveness of our design of screening hyperparameter choices,": "we choose hyperparameters on the origin space, shrunk space, anddecoupled space. The search time curve is shown in c.We demonstrate that screening hyperparameter choices can helpimprove performance on CF tasks and search efficiency. Accordingto our results, performance on H is better than the one on the originspace H. The reason is that the shrunk hyperparameter space hasa more significant possibility of including better HP choices for CFarchitectures to achieve better performance. The search efficiencyis improved since the choice of hyperparameters is reduced, andproper hyperparameters can be found more quickly in a smallersearch space. We also find that the search efficiency reduces whenwe increase the batch size and embedding dimension when wesearch on the decoupled search space. While increasing batch sizeand embedding dimension help improve final performance, the costevaluation is higher, which may reduce search efficiency. 5.4.2Choice of Sampling Ratio. To find the impact of changingsampling ratio, we search with different sampling ratios on differentdatasets in the same controlled search time. The evaluation results(Recall@20) for experiments on different sampling ratio settingsare listed in .According to the table, smaller sampling ratios have lower per-formance. The reason is that the user-item matrix sampled by lowsample ratio may not capture the consistency in the origin matrix,as the consistency results shown in .2. While sampleddataset generated by higher sample ratio has more considerableconsistency with the original dataset, the results in a limited timemay not be better. The reason is that too much time on evaluationin the first stage may have fewer results for the surrogate model",
  "ML-100K0.22590.23353.36%ML-1M0.13090.13583.74%Yelp0.06430.06724.51%Amazon-Book0.03540.03591.41%": "to learn the connection between performance and configurationsof hyperparameters and architectures. Thus, the first stage has atrade-off between sample ratio and time cost, and we choose 20%as our sample ratio in our experiments. 5.4.3Tuning on Hyperparameters. To show the effectiveness ofour design on joint search, we propose to apply our joint searchmethod to previous research work AutoCF . The results areshown in . We apply our search strategy to AutoCF to includehyperparameters in our search space, noted as AutoCF(HP). Wefind that hyperparameter searched on shrunk space can performbetter than the one that uses architecture space and random searchon different datasets.",
  "Case Study (RQ4)": "In this part, we mainly focus on our search strategys search resultsof different architectures. The search results with top performanceshare some similarities, while different architecture operations havedifferent performances.According to search results, we present the top models for eachtask on all datasets in . It is easy to find that interactionhistory-based encoding features may have better performance andmore powerful representative ability. Besides, the embedding func-tion of SGC has stronger representative ability since it can capture the high-order relationship. Both SGC and HadanardGCN collect in-fomation from different layers, simply designed SGC have strongerability. As for the interaction function, we find the classical element-wise multiply can receive strong performance; The predictionfunction of learnable vector and MLP can capture more powerfuland complex interaction than SUM. We can find that these top mod-els of each task have similar implementations, but there also havesome differences among different datasets. One top architecture ona given dataset may not get the best performance on another one.In summary, the proper architectures for different datasets maynot be the same, but these top results may share some same opera-tions in architecture structures. The result and analysis can helphuman experts to design more powerful CF architectures.",
  "CONCLUSION AND FUTURE WORK": "In this work, we consider a joint search problem on hyperparame-ters and architectures for Collaborative Filtering models. We pro-pose a search framework based on a search space consisting offrequently used hyperparameters and operations for architectures.We make a complete understanding of hyperparameter space toscreen choices of hyperparameters, We propose a two-stage searchalgorithm to find proper hyperparameters and architectures config-urations efficiently. We design a surrogate model that can jointlyupdate CF architectures and hyperparameters and can be trans-ferred from small to large datasets. We do experiments on severaldatasets, including comparison on different models, search algo-rithms and ablation study.For future work, we find it important to model CF models basedon Knowledge Graphs as a search problem. With additional entitiesfor items and users, deep relationships can be mined for betterperformance. An extended search framework can be built on largernetwork settings. Models on extensive recommendation tasks andother data mining tasks can also be considered as a search problem. This work is partially supported by the National Key Research andDevelopment Program of China under 2021ZD0110303, the NationalNatural Science Foundation of China under 62272262, 61972223,U1936217, and U20B2060, and the Fellowship of China PostdoctoralScience Foundation under 2021TQ0027 and 2022M710006.",
  "J. Bergstra and Y. Bengio. 2012. Random search for hyper-parameter optimization.JMLR 13, Feb (2012), 281305": "James Bergstra, Dan Yamins, David D Cox, et al. 2013. Hyperopt: A pythonlibrary for optimizing the hyperparameters of machine learning algorithms. InProceedings of the 12th Python in science conference, Vol. 13. Citeseer, 20. Jinhang Cai, Yimin Ou, Xiu Li, and Haoqian Wang. 2021. ST-NAS: Efficient Opti-mization of Joint Neural Architecture and Hyperparameter. In Neural InformationProcessing, Teddy Mantoro, Minho Lee, Media Anugerah Ayu, Kok Wai Wong,and Achmad Nizar Hidayanto (Eds.). Springer International Publishing, Cham,274281.",
  "Liam Li and Ameet Talwalkar. 2019. Random search and reproducibility forneural architecture search. arXiv preprint arXiv:1902.07638 (2019)": "Bin Liu, Chenxu Zhu, Guilin Li, Weinan Zhang, Jincai Lai, Ruiming Tang, Xi-uqiang He, Zhenguo Li, and Yong Yu. 2020. AutoFIS: Automatic Feature Interac-tion Selection in Factorization Models for Click-Through Rate Prediction. (2020). Chenxi Liu, Liang-Chieh Chen, Florian Schroff, Hartwig Adam, Wei Hua, Alan LYuille, and Li Fei-Fei. 2019. Auto-deeplab: Hierarchical neural architecture searchfor semantic image segmentation. In Proceedings of the IEEE Conference on Com-puter Vision and Pattern Recognition. 8292.",
  "Quanming Yao, Ju Xu, Wei-Wei Tu, and Zhanxing Zhu. 2019. Differentiable NeuralArchitecture Search via Proximal Iterations. arXiv preprint arXiv:1905.13577(2019)": "Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton,and Jure Leskovec. 2018. Graph convolutional neural networks for web-scalerecommender systems. In Proceedings of the 24th ACM SIGKDD InternationalConference on Knowledge Discovery & Data Mining (KDD). ACM, 974983. Arber Zela, Aaron Klein, Stefan Falkner, and Frank Hutter. 2018. Towards au-tomated deep learning: Efficient joint neural architecture and hyperparametersearch. arXiv preprint arXiv:1807.06906 (2018).",
  "A.2Search Algorithms": "A.2.1Surrogate Model Design. We demonstrate our search algo-rithm with surrogate model in Algorithm A1.We design our search algorithm with BORE and Random Forest(RF) regressor. In the first stage, we train the surrogate model with, update parameters of surrogate model. The output of BORE+RFcan help give an inference between (0, 1), and we choose the leastone as output. After the first stage, we save the parameters of thissurrogate model, and transfer it to second stage, which we do ex-periment on larger datasets. The configuration of hyperparametersand architectures and the performance on larger dataset can alsoupdate the parameters of surrogate model. With the knowledge welearn on the first stage, the surrogate model can better choose theproper architecture and hyperparameter for CF tasks. A.2.2Fair Comparison. Compared with hyperparameters, the choiceof architectures can affect the performance of CF models more. Thus,to compare different experiment settings fairly, we use the averageof top5 configurations instead for the search time curve, noted astop5avg. A.2.3Search Procedure. We show the comparison of conventionaljoint search method and our method in Figure A1. In Figure A1, con-ventional one-stage method search components separately, whileour method search hyperparameters on a shrunk space. Besides,the evaluation time is lower in the first stage in our method.",
  "A.3Discussion": "In this section, we discuss about the method we have chosen, andthe difference with previous works on joint search problems.The first is the reason for screening in the hyperparameters spacerather than the architecture space. The search space of hyperpa-rameters mainly consists of components of continuous values withinfinite choices. And screening range of hyperparameters isproven effective on deep neural networks and graph-based models.",
  ": return ,": "Architecture space components are typically categorized, and eachone is necessary in some manner and should not be ignored. Further-more, we believe that it is unnecessary to reduce the architecturespace after conducting a fair ranking of different architectures.Our work is the first on CF tasks, which is different from previousresearch studies on joint search. mainly focuses on a jointsearch problem on typical neural networks. Another study on thejoint search problem, AutoHAS , focuses on model search withweight sharing. FEATHERS focuses on a joint search problemon Federate Learning, an aspect of Reinforcement Learning. In ST-NAS , the sub-models are candidates of a designed super-net,and they sample sub-ST-models from the super-net and weights areupdate using training loss while updating HPs with the validationloss.",
  "Amazon-Book 4 This book-rating dataset is collected from usersuploaded review and rating records on Amazon": "A.4.2Preprocess. Since the origin dataset in Table A2 is too largefor training, we reduce them in frequency order. We use 10-coreselect on Yelp and 50-core select on Amazon-Book. -core meanswe choose users and items that appear more than times in thewhole record history. After we select the dataset, we split the datainto training, validation and test sets. We shuffle each set when westart a new evaluation task. A.4.3Dataset sampling. Some papers studying long-tail recom-mendation or self-supervised recommendation may dis-cuss the impact of data sparsity. We can sample the subgraph ac-cording to the popularity , mainly user-side and item-side, andthe long-tail effect on the item-side is more severe. Thus we sam-ple the rating matrix based on the item frequency. The more theitem appears in rating records, the more likely it is reserved in thesubsampled matrix.",
  "BOHB . BOHB is a method consisting of both Bayesian Opti-mization (BO) and HyperBand (HB), helping modulate functionswith a lower time budget": "BORE . BORE is a BO method considering expected im-provement as a binary classification problem. It can be combinedwith regressors, including Random Forest (RF), Multi-Layer Per-ceptron (MLP), and Gaussian Process (GP). We choose RF as aregressor in our experiments. A.5.2Hardware Environment. We implement models with PyTorch1.12 and run experiments on a 64-core Ubuntu 20.04 server withNVIDIA GeForce RTX 3090 GPU with 24 GB memories each. Ittakes 3.5-4 hours to search on a dataset with one million records."
}