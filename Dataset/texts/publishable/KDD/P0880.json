{
  "Abstract": "Spatio-temporal forecasting is a critical component of various smartcity applications, such as transportation optimization, energy man-agement, and socio-economic analysis. Recently, several automatedspatio-temporal forecasting methods have been proposed to au-tomatically search the optimal neural network architecture forcapturing complex spatio-temporal dependencies. However, theexisting automated approaches suffer from expensive neural ar-chitecture search overhead, which hinders their practical use andthe further exploration of diverse spatio-temporal operators in afiner granularity. In this paper, we propose AutoSTF, a decoupledautomatic neural architecture search framework for cost-effectiveautomated spatio-temporal forecasting. From the efficiency per-spective, we first decouple the mixed search space into temporalspace and spatial space and respectively devise representation com-pression and parameter-sharing schemes to mitigate the parameterexplosion. The decoupled spatio-temporal search not only expe-dites the model optimization process but also leaves new roomfor more effective spatio-temporal dependency modeling. Fromthe effectiveness perspective, we propose a multi-patch transfermodule to jointly capture multi-granularity temporal dependen-cies and extend the spatial search space to enable finer-grainedlayer-wise spatial dependency search. Extensive experiments oneight datasets demonstrate the superiority of AutoSTF in terms ofboth accuracy and efficiency. Specifically, our proposed methodachieves up to 13.48 speed-up compared to state-of-the-art auto-matic spatio-temporal forecasting methods while maintaining the",
  "Corresponding author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 25, Toronto, ON, Canada 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-1245-6/25/08",
  "Introduction": "Spatio-temporal forecasting is the process of predicting future statesthat depend on both spatial and temporal contexts . Forexample, in transportation systems, the future traffic flow and speedcan be estimated by simultaneously learning the spatio-temporaldependencies of historical traffic conditions generated by geo-distributed roadside sensors . Accurate spatio-temporalforecasting plays a pivotal role in various smart city applications,such as human mobility modeling , demand-supply rebal-ancing , and urban anomaly event detection .In the past decade, extensive efforts have been made to capturespatio-temporal dependencies by leveraging advanced deep learn-ing techniques . To name a few, D2STGNN incorpo-rates Graph Neural Network (GNN) and Recurrent Neural Network(RNN) to model the diffusion process of traffic dynamics, whileSTEP devise pre-training to encode temporal patterns intosegment-level representations. As another example, METRO isa versatile framework that utilizes a multi-scale temporal graphneural network to model dynamic and cross-scale variable corre-lations simultaneously. Despite adopting various neural network",
  "MAE": ": The training time and forecasting accuracy (meansquared error) comparison of manually-designed and auto-mated spatio-temporal forecasting models on the METR-LAdataset. Our proposed method (AutoSTF) achieves the bestforecasting accuracy while taking much less training timecompared with all existing automated models. blocks that have proven effective in preserving spatio-temporal de-pendencies, it requires substantial domain knowledge and extensiveexpert efforts to design an optimal model architecture .As an emerging trend, recently a few automated spatio-temporalmethods have been proposed to determine the optimal neural archi-tecture . In general, such methods automatically searchthe model architecture by exploiting a pre-defined spatial and tem-poral operator space in an end-to-end manner. However, the combi-natorial nature of diverse spatio-temporal operators leads to a vastnumber of potential model architectures, making the search processcomputationally expensive and time-consuming. As depicted in Fig-ure 1, existing automated spatio-temporal forecasting methods takean order of magnitude longer than manually-designed methodswhile yielding marginal prediction accuracy improvements. The ex-tensive computational overhead of automated approaches not onlyprevents their practical use in real-world tasks, but also hindersfurther optimization of the forecasting accuracy. It is imperativeto develop an efficient and effective automated spatio-temporalforecasting method. In fact, after analyzing the performance of au-tomated methods on various spatio-temporal forecasting datasets,we identify two aspects that were previously overlooked by existingapproaches, which provide great potential for us to improve the effi-ciency and effectiveness of automated spatio-temporal forecasting.We detail our research insights below.Mixed spatio-temporal search space. Existing automated modelssearch for the optimal neural architecture within a unified searchspace, which combines various temporal and spatial operators. Froman efficiency perspective, such a mixed space entangles the searchprocess for spatial and temporal correlation modeling blocks, re-sulting in exponential search complexity. However, current spatio-temporal forecasting models typically capture spatio-temporal cor-relations separately, e.g., using GNNs to capture spatial dependencyand RNNs for temporal dependency . Searching for a forecast-ing model in a mixed search space may lead to high computationalredundancy and potentially suboptimal neural architecture dueto insufficient exploration. Thus, we propose a decoupled spatio-temporal search framework wherein the spatial dependency blockand temporal dependency block are determined separately. More-over, we devise a representation compression scheme to distill key temporal knowledge and introduce a parameter-sharing schemeto reduce the optimization overhead of the searched spatial de-pendency network. By decoupling the mixed search space andminimizing the parameter size, the search process can be signifi-cantly accelerated, allowing for more flexible exploration in boththe parameter optimization and advanced search space design.Coarse-grained spatio-temporal correlation search. Limited bycomputational inefficiency, current automated forecasting methodstypically leverage spatio-temporal dependencies at a coarse gran-ularity. In the temporal domain, previous works have commonlyapplied the same spatial operator to all previous time steps, disre-garding the possibility that the correlation may vary across differenttime steps. Furthermore, existing studies have adopted an identicalmessage-passing scheme in different GNN layers, which overlooksdistinct spatial dependencies in different multi-hop neighbors. Wedetail the effectiveness bottleneck with more empirical evidence inSection A.3. In this paper, we propose a multi-patch transfer modulethat divides temporal representations into different patches so thattime-varying pair-wise dependency can be preserved in differentpatches. Besides, we introduce the spatial adjacency matrix as a newclass of spatial operators and search the adjacency for each layerto strengthen the spatial dependency modeling. By incorporatingfine-grained temporal and spatial dependencies, the accuracy ofspatio-temporal forecasting can be further improved.Along these lines, in this paper, we develop AutoSTF, a cost-effective decoupled Automated Spatio-Temporal Forecasting frame-work. Our main contributions are summarized as follows. From theefficiency perspective, we propose a decoupled automated spatio-temporal forecasting framework to reduce the neural architecturesearch overhead. By incorporating representation compression andparameter sharing schemes, AutoSTF not only expedites the modeloptimization process but also leaves new room for more effectivespatio-temporal forecasting. From the effectiveness perspective, wepropose a multi-patch transfer module and layer-wise message-passing spatial operators to respectively capture fine-grained tem-poral and spatial dependencies, thereby enhancing the forecastingaccuracy. We conduct extensive experiments on eight datasets fromdifferent application domains to demonstrate that our proposedframework outperforms state-of-the-art automated spatio-temporalforecasting models in terms of both efficiency and effectiveness.",
  "Preliminaries": "Definition 2.1 (Spatial Graph). We denote the spatial graph asG = (V, E), where |V| = is the set of nodes (sensors), indicatingeach node in V corresponds to a time series; is the numberof nodes; and E denotes the set of edges that represent spatialcorrelations between different nodes. A R is the adjacencymatrix of spatial graph G. In this work, the adjacency matrix A can be predefined, e.g.,based on the node distances, or adaptive by a data-driven approach,e.g., based on time series or node embedding. The time series onthe spatial graph can be denoted as graph signal matrix.",
  "oaAqopsYypjucume2Kubn7pSpNDglxBncprghzq/zs2s1qa3d9JbZ+KvNKzZ8zw3w5u5JQ248nOcw6CxUa5sl7eONkvVvXzU1jBKtZpnjuo4gCHqJP3BR7wiCfn2Llybp27j1RnJNcs49ty7t8BP3iUmA=</latexit>T0": "Z0S0JuS0sUSaWLKSwnr01wTz42zZn/z7hlPfbcb+vWKyRW4ZzYv3SDzP/qdC0KXWyaGiTVlBhGV8etS26om/ufqpKkUNCnMYdiqeEuVEO+uwaTWZq171lJv5qMjWr9zm5njTt6QB176P8ydorlRr69W1vdXK1rYdRELWMQyzXMDW9hFHQ3yvsQDHvHk7DvXzq1z95HqFKxmHl+Wc/8OPRWUlw=</latexit>S0 : AutoSTF framework. (a) shows the overview framework of AutoSTF. (b) depicts the Embedding Layers, which consistof raw time series embedding, node embedding, and time embedding. (c) illustrates the Temporal Search Module, which searchesfor the optimal temporal operator within the Temporal-DAG to model complex temporal dependencies. (d) describes theMulti-patch Transfer, which segments the embedding into several patches along the temporal feature axis and compresseseach into a dense semantic representation. (e) presents the Spatial Search Module, tasked with searching for the optimal spatialoperator and integrating it with the preceding temporal dependencies to uncover fine-grained spatial-temporal correlations.The two-headed arrow denotes the search operation. The single-headed arrow denotes the operator, and different colorsrepresent different operators. Spatio-temporal Forecasting. We consider both single-stepand multi-step spatio-temporal forecasting. Given G, X, and thehistorical time steps , the goal of multi-step forecasting is to predictthe value at all future time steps:",
  "( Y+1, Y+2, , Y+) F (H+1, H+2, H),(1)": "where H = (G, X), and Y denotes the predictive value at timestep ; F denotes the forecasting model. While single-step forecast-ing aims to predict the value of -th future time step.Problem Statement. In this work, we aim to design an effi-cient and effective automated spatio-temporal forecasting modelF . We try to search for an optimal neural architecture thatcontains both architecture parameters and model parameters .The objective function is as follows:",
  "Embedding Layers": "Here we introduce the three kinds of embeddings preprocessedin the Embedding Layers, including time series embedding, nodeembedding, and time embedding.The original time series X R is processed througha linear layer to obtain an initial latent representation: Z(0) =XW + , where Z(0) R is time series embedding, andW denotes the learnable matrix and is the hidden dimension.Node embedding aims to identify and encode the spatial locationsof different sensors and can be expressed as E R . Timeembedding is designed to map the inherent time feature that can",
  "KDD 25, August 37, 2025, Toronto, ON, CanadaTengfei Lyu, Weijia Zhang, Jinliang Deng, and Hao Liu": "be extracted from raw time series. E R and E R denote the embedding of day-of-week and time-of-day,respectively. denotes the time slots in a day, and denotesthe day-of-week.We concatenate these three embeddings and feed it into the fullyconnected layers, as shown E = ( [E, E , E ]),where denotes the fully connected layers and E R rep-resents the corresponding embedding, which will be fully utilizedin subsequent processes.",
  "Temporal Search Module": "In this section, we first introduce the temporal search space andtemporal-DAG, then explain how to address the temporal search.Temporal search space. The temporal search operator shouldprioritize two key factors: the capacity to accurately model bothshort-term and long-term temporal dependencies and the efficiencyof temporal operators . Based on these findings, we select theGated Dilated Causal Convolution (GDCC) and informer to construct the temporal search space. Furthermore, to enhance theflexibility of temporal search, we also incorporate two commonlyused operators, namely and , into our temporal searchspace, similar to the works in . We define the temporalsearch space as OT = {, ,, }.Temporal-DAG. In line with previous works , wehave also employed a directed acyclic graph (DAG) to facilitate thesearch for various combinations of temporal operators, as shownin (c). The temporal-DAG has T nodes, and each nodedenotes a latent representation. T0 = Z(0) represents the outputfrom the Embedding Layer and serves as the input for this temporal-DAG. The edge in temporal-DAG denotes the search operation and will search for an optimal temporal operator in the temporalsearch space. Specifically, for the edge between node T0 and T1,we have |OT| choice to determine a temporal operator. The latentrepresentation of T can only be transmitted to T using the selectedoperator, if < . In other words, the latent representation of thecurrent node is enhanced by its previous nodes in the temporal-DAG. For example, in (c), the latent representation of nodeT3 is aggregated with the information from nodes T0, T1, and T2.Parameterizing temporal-DAG. Following DARTS framework, we perform the temporal search within a temporal-DAG, asshown in (c). Each operator in the temporal search space iscapable of transforming a latent representation (e.g., T0) into a newone (e.g., T1) in the temporal-DAG. We denote as the architectureparameters of temporal-DAG and introduce the search operationbetween node T and T with () R| OT |. Specifically, theweight of the operator OT between node T and T is formulateas follow:",
  "Multi-patch Transfer Module": "In this module, we segment the output embedding of the temporalsearch into several patches along the temporal feature axis andcompress each patch into a dense semantic representation. Imple-menting these strategies can explore the finer-grained spatial andtemporal dependencies in subsequential spatial search, and decreaseredundant computations in the temporal feature dimension.Specifically, after obtaining the temporal search embeddingT R from the temporal search, we divide the temporal feature( the dimension of T) into several patches to explore the finer-grained spatio-temporal dependencies and effective compress thetemporal feature for spatial search. As illustrated in , weinitially divide the temporal embedding T into patches:",
  "xit>OS": ": The spatial search in a Spatial-DAG. S denote thelatent representation. The search operation aims to iden-tify an optimal spatial operator between any two nodes (e.g.,S1 S2). Once identified, this operator transfers the latentrepresentation from S1 to S2. is then used as the input embedding for the spatial search. Fi-nally, the output of multi-patch transfer can be denoted as P ={S1,S2, S }.The multi-patch transfer module serves as the bridge betweentemporal search and spatial search. It involves segmenting the tem-poral feature and conducting the spatial search for each patch toinvestigate fine-grained spatio-temporal dependencies. It convertsthe temporal search embedding T from R to R ,resulting in a significant reduction in computation time (refer to Ta-ble 7). In addition, it combines the embedding for the input ofeach spatial search, which preserves the temporal information andimproves the accuracy of modeling spatio-temporal dependencies.",
  "Spatial Search Module": "To model fine-grained spatio-temporal dependencies for each patchembedding S, we design a new spatial search space to covera wide range of spatial modeling paradigms and automaticallyoptimize messages-passing across different layers, leading to moreaccurate forecasting. is the detailed framework of thespatial search module.Spatial search space. After analyzing existing literature (re-fer to Section A.3), we classify the adjacency matrix into threecategories: fully fixed, semi-adaptive, and fully adaptive (see ta-ble 8). To enhance the flexibility and capacity of exploring spatio-temporal dependencies, we also incorporate and into the spatial search space. We define the spatial search spaceas OS = { , , , , }, where ,, and denote fully fixed, semi-adaptive,and fully adaptive matrix used in GNN layer, respectively. . This operator employs a distance-based matrix formessage-passing aggregation and is the predefined adjacency ma-trix, fixed in both the training and inference phases. The formalformulation is as follows:",
  "=0A ,(8)": "where A is the distance matrix and is the finite steps. is the learnable weights and is the input of operator.. This operator utilizes an adaptive matrix that is learnedfrom node embedding. We refer to as a semi-adaptive matrix because the adjacency matrix in this operator is trainableduring the training phase but remains fixed during the inferencephase. The adaptive matrix has a strong ability to model spatialdependencies and is commonly used in many spatio-temporal fore-casting models . As is common in many models, werandomly generate two sets of node embeddings 1 R and2 R that are represented by learnable weights. We thencompute the adaptive matrix using these two node embeddings:A = ( (12 )). We use this adaptive matrix totackle the massage-passing aggregation of the information for theirneighborhood. The formal formulation is as follows:",
  "=0A,(9)": "where is learnable weights and is the input of this operator.. Attentions are utilized in many state-of-the-art spatio-temporal forecasting models. This operator utilizes multi-head at-tention to compute a fully adaptive matrix based on the input timeseries. Notably, this fully adaptive matrix in operator isautomatically generated during the training and inference phases.We briefly introduce the multi-attention mechanism, which is aconcatenation of self-attention heads. Specifically, given attentionhead , the learnable matrices , , R",
  "() = ( ( 1 + 1)) 2 + 2.(13)": "Drawing inspiration from , we also adopt a grouping strategyin the operator. The input of this operator is initiallypartitioned into several groups, and Equation 12 is subsequentlyapplied to each group, which can enhance computational efficiency.Spatial-DAG. To explore the fine-grained temporal and spatialdependencies (refer to ), we construct M spatial-DAGs foreach patch embedding (e.g., S1) in the P. In each spatial-DAG,spatio-temporal dependencies are explored by automatically select-ing the optimal message-passing aggregation in different layers. Itis important to note that in order to address the issue of parameterexplosion, the network parameters of spatial operators on the sametype edge (e.g., the edge between S0 and S1) are shared amongdifferent spatial-DAGs. However, the architecture parameters on different spatial-DAG are trained separately. In other words,optimal spatial operators can be automatically selected for differentspatial-DAGs.Parameterizing spatial-DAG. To provide a clearer explanation,we consider the parameterizing process of one spatial-DAG for one",
  "OS ( ())(S),(14)": "where S0 = S server as the input of this spatial-DAG. S is thenumber of nodes in this spatial-DAG, and , S denotes the outputembedding of this -th spatial-DAG.Finally, using the same method, all the patch embeddings canbe calculated to obtain the output of each spatial-DAG, denoted asS = {1S, ,S, ,S }. We sum the outputs of each spatial-DAG to obtain the final output of the spatial search module, theformulation is as follows:",
  "Output Layer and Search Strategy": "In the output layer, we concatenate the skip embedding as thefinal representation: = T S, where denotes theconcatenation operation. Then, the final presentation is fed intothe linear layers to make the spatio-temporal forecasting.In our AutoSTF, there are mainly two types of parameters: ar-chitecture parameters, such as weights of candidate operators, andnetwork parameters, such as internal parameters of spatial andtemporal operators. We denote the architecture parameters and net-work weight parameters as = {, } and , respectively, and allcomputations are differentiable. Therefore, we can employ a bi-levelgradient-based optimization algorithm like DARTS , which is agradient-based neural network architecture search algorithm.:",
  "Experiments": "In this section, we conducted experiments on both multi- and single-step spatio-temporal forecasting to evaluate AutoSTF. We introducethe experimental settings, overall results of AutoSTF, efficiencystudy, and ablation study. Due to page limitations, we have providedthe parameter sensitivity analysis and case study in AppendicesB.5 and B.6, respectively. In Appendix B.7, we include a detailedfigure to demonstrate the significant improvements of our modelcompared to other automated models. We analyze the algorithmiccomplexity of AutoSTF in Appendix B.8. In addition, we conducteda comprehensive analysis and extensive experiments of decoupledthe search space in Appendices C, which involved comparing the",
  "Experimental Settings": "In this subsection, we describe the datasets used, the metrics em-ployed in our experiments, and the baseline comparisons we haveconducted. Detailed information on hyper-parameters and imple-mentation specifics can be found in Appendix B.4.Datasets. We performed experiments on eight benchmark datasetsto evaluate both multi-step and single-step forecasting. For multi-step forecasting, we utilized traffic speed datasets (METR-LA andPEMS-BAY) provided by Li et al. and traffic flow datasets(PEMS03, PEMS04, PEMS07, and PEMS08) released by Song et al.. As for single-step forecasting, we employed solar-energy andelectricity datasets made available by Lai et al. . These datasetscan be categorized as traffic speed, traffic flow, solar-energy, andelectricity. We provide the detailed statistics of the datasets in Ap-pendix B.1.Evaluation Metrics. We followed previous works toevaluate the models performance in multi- and single-step forecast-ing. Specifically, Mean Absolute Error (MAE), Root Mean SquaredError (RMSE), and Mean Absolute Percentage Error (MAPE) wereutilized to assess the accuracy of multi-step forecasting. For single-step forecasting, we utilized Root Relative Squared Error (RRSE)and Empirical Correlation Coefficient (CORR) as evaluation metrics.Lower values of MAE, RMSE, MAPE, and RRSE are indicative ofhigher accuracy, while larger CORR values denote higher accuracy.More details are stated in Appendix B.3.Baselines. We compare AutoSTF with 14 manual-designed meth-ods, including DCRNN , STGCN , Graph WaveNet ,AGCRN , MTGNN , STID , STHODE , DeepSTUQ ,",
  "Overall Results of AutoSTF": "Firstly, we present the main experiments for both multi-step (referto and 3) and single-step (see ) spatio-temporal fore-casting. We adopt the convention of highlighting the best accuracyin bold and underlining the second best accuracy.Multi-step forecasting. As demonstrated in Tables 2 and 3,AutoSTF outperforms all baseline models on all multi-step fore-casting tasks across all evaluation metrics. In summary, the resultsdemonstrate that AutoSTF can produce highly competitive archi-tectures adept at capturing spatio-temporal dependencies, leadingto enhanced forecasting accuracy.In particular, compared with the most advanced manual-designedbaseline LightCTS, our model achieved significant improvementson all datasets. For example, our model resulted in a (2.05%, 1.32%,0.76%) and (4.23%, 3.11%, 2.31%) reduction in MAE for long-term (60min), medium-term (30 min), and short-term (15 min) forecastingon METR-LA and PEMS-BAY datasets, respectively. Furthermore,our model demonstrated enhanced performance on other trafficflow datasets compared to LightCTS, achieving a reduction in MAE(2.18%), RMSE (0.93%), and MAPE (1.72%) for PEMS04, as well as adecrease in the same metrics for PEMS08, with MAE (3.83%), RMSE(1.36%), and MAPE (3.08%). Two crucial insights can be drawn fromour observations. Firstly, the experimental outcomes reveal thatAutoSTF is capable of automatically generating highly competitiveneural architectures that surpass human-designed models in perfor-mance. secondly, no hand-designed model consistently outperformsothers across various traffic flow datasets. This implies that differ-ent datasets require distinct model architectures. Nonetheless, ourAutoSTF can adapt to these requirements by automatically design-ing optimal neural architectures for different traffic flow datasets,ultimately yielding the most accurate forecasting results.Moreover, when compared to the automated models, our pro-posed AutoSTF demonstrates significant advancements across alldatasets in terms of multi-step forecasting performance. Specifically,compared with AutoCTS on long-term forecasting, our AutoSTFresults in a (3.46%, 0.70%, 2.34%), (4.23%, 1.65%, 2.98%), (3.92%, 1.91%,2.40%), and (5.06%, 1.99%, 3.89%) improvement on the METR-LA,",
  "AutoSTF0.07320.94970.09350.9279": "PEMS-BAY, PEMS04, and PEMS08 datasets, respectively, under theMAE, RMSE, and MAPE metrics. We can draw two key observationsas follows: Firstly, compared to other automated models, our pro-posed AutoSTF achieves superior performance on both traffic speedand traffic flow datasets, demonstrating the effectiveness of ourframework in accurately capturing spatio-temporal dependencies.Secondly, the experimental results also indicate that fine-grainedspatial search and adaptive selection of the optimal message-passingaggregation in different layers can significantly improve accuracy.Single-step forecasting. In this experiment, we compared Au-toSTF with other automated spatio-temporal forecasting modelsand presented the results in for the single-step forecast-ing task. Consistent with prior research , we present thesingle-step forecasting results for the 3rd and 24th future time steps.It should be noted that AutoSTG utilizes a predefined adjacencymatrix for its spatial graph convolution operators. However, sincethis matrix is not available in single-step spatio-temporal datasets,the spatial graph convolution operators can be excluded from Au-toSTGs search space when running it on such datasets.Overall, AutoSTF outperformed the other models and achievedstate-of-the-art results on the Solar-Energy and Electricity datasetsfor almost all forecasting horizons. Notably, AutoSTF demonstrateda significant improvement in the CORR metric and outperformed allother models on these datasets. The improvements achieved by Au-toSTF on different datasets highlight the importance of automatedsolutions in identifying specific and optimal models.",
  "Efficiency Study": "For efficiency, in this study, we place significant emphasis on opti-mizing the search efficiency. We selected AutoCTS and AutoSTG,which are among the most advanced automated spatio-temporalforecasting models, as baselines for comparing the search time andmemory consumption in the search phase. It is important to notethat the experimental setup for AutoCTS+ is significantly dif-ferent from that of AutoSTF, AutoCTS, and AutoSTG, as it involvestraining an Architecture-Hyperparameter Comparator using a largevolume of samples to predict the final neural architecture. There-fore, to ensure an accurate comparison, we excluded AutoCTS+from the efficiency evaluation. The efficiency experiments wereconducted on a Linux Ubuntu server equipped with 2 V100 GPUs.Figures 5 and 6 present a comparison of the average search timeper epoch and the memory consumption across different datasets,respectively. Our AutoSTF consistently demonstrates the best per-formance in terms of search time across all datasets, highlightingthe efficiency of our proposed framework. From the results, we candraw two key insights as follows. Firstly, AutoCTS takes thousandsof seconds to complete an epoch search on most datasets, resultingin excessive computational overhead and presenting significantchallenges. In contrast, AutoSTF only takes hundreds of secondsto complete the one epoch search, significantly improving compu-tational efficiency. This demonstrates that decoupling the mixedsearch space, reducing redundant temporal features and parameter-sharing schemes for spatial search can effectively enhance com-putational efficiency. Secondly, regarding memory computations,",
  "Ablation Study": "To investigate the contribution of each key module to improvingthe performance and efficiency of the search phase of the proposedAutoSTF model, we performed an ablation study on three variantsof AutoSTF. The three variants are described as follows: (1) \"w/o TS\"refers to the model variant that omits the Temporal Search module,instead utilizing a Linear layer as its replacement. (2) \"w/o MPT\" isthe model variant that removed the Multi-Patch Transfer module,which means this model variant only contains a temporal-DAGand a spatial-DAG. (3) \"w/o SS\" is the model variant that removedthe Spatial Search module. We present the experimental results in and provide a discussion of the findings. demonstrates that AutoSTF outperforms its variants,each of which disregards a different component of AutoSTF. Specif-ically, AutoSTF achieves higher accuracy compared to w/o TS andw/o SS, which highlights the effectiveness that AutoSTF achievedby decoupling the mixed search space into temporal and spatialspace. Furthermore, AutoSTF significantly outperforms the w/oMPT variant, reinforcing the effectiveness of multi-patch trans-fer in improving accuracy by incorporating temporal informationto model finer-grained spatio-temporal dependencies. In terms ofsearch time, it can be concluded that the temporal search is the mosttime-consuming part. This can be attributed to the inclusion of theinformer as the temporal operator in the temporal search, whichis a particularly computationally demanding model. Furthermore,it should be noted that the search time of the w/o MPT variant islower than that of AutoSTF. This is because when the MPT moduleis removed, the temporal search embedding is not split into multiplepatches. As a result, there is only one spatial-DAG to explore thespatio-temporal correlation, leading to a reduced search time.",
  "AutoSTF: Decoupled Neural Architecture Search for Cost-Effective Automated Spatio-Temporal ForecastingKDD 25, August 37, 2025, Toronto, ON, Canada": "temporal dependencies within time series and spatial correlationsacross different locations for spatio-temporal forecasting. For tem-poral modeling, techniques such as Gated Recurrent Units (GRU),Long Short-Term Memory (LSTM) , and Temporal Convolu-tional Networks (TCN) are commonly employed to extractinformation from time series data. In addition, various Graph Neu-ral Network approaches have been proposed for modeling spatialdependencies using different adjacency matrices, including pre-defined and adaptive matrices . Graph WaveNet , asproposed by Wu et al., utilizes both predefined and adaptive matri-ces to capture spatio-temporal dependencies. Furthermore, severalstudies have proposed methods to model dynamic spatial dependen-cies . These methods primarily focus on learning thehidden relationships between nodes using dynamic node features,which are generally characterized by a combination of real-timetraffic attributes. For example, ESG presents a flexible graphneural network that models multi-scale time series interactionsto simultaneously capture pairwise correlations and temporal de-pendencies. However, manually-designed models necessitate sub-stantial domain-specific expertise, as well as extensive parameteradjustments.The automated models. Recently, there has been a growingtrend toward developing automated methods for designing efficientmodel architectures, aiming to enhance the accuracy of forecastingpredictions . The existing automated time series fore-casting models aim to search the optimal architecture in the mixedsearch space for modeling the spatio-temporal dependencies. Forinstance, AutoSTG employs an adaptive matrix learned fromnode features using meta-learning and explores spatio-temporaldependencies within a mixed search space. AutoCTS employsmicro and macro search strategies to identify optimal blocks froma mixed search space and determine the topology among heteroge-neous blocks, ultimately constructing a novel architecture. Addi-tionally, a recent study, AutoCTS+ , introduces a unified searchstrategy that incorporates both operators and hyperparameters,making it the first work to include hyperparameters in the searchspace. However, the current automated approaches face challengesdue to the high computational cost of neural architecture search,which limits their practical application and the exploration of awider range of spatio-temporal operators at in finer granularity.",
  "Conclusion": "In this paper, we propose AutoSTF, a novel decoupled neural ar-chitecture search framework for cost-effective automated spatio-temporal forecasting. By decoupling the mixed search space intotemporal and spatial space, we design representation compressionand parameter-sharing schemes to mitigate the parameter explosionin our framework. The decoupled spatio-temporal search not onlyexpedites the model optimization process but also leaves new roomfor more effective spatio-temporal dependency modeling. In addi-tion, in the multi-patch transfer module, we aim to jointly capturemulti-granularity temporal dependencies and extend the spatialsearch space to enable finer-grained layer-wise spatial dependencysearch. In spatial search, we design layer-wise message-passingspatial operators to capture the spatio-temporal dependencies byautomatically selecting the optimal adjacency matrix in different layers, thereby enhancing forecasting accuracy. Comprehensiveexperiments conducted on eight datasets highlight the superiorperformance of AutoSTF in terms of both accuracy and efficiency.In future work, focusing on improving the search algorithm, ex-panding the search space, and scaling the model to handle largedatasets presents a valuable and meaningful challenge. Addition-ally, developing a method to automatically determine the optimalnumber of patches for different datasets is another meaningful task. Lei Bai, Lina Yao, Can Li, Xianzhi Wang, and Can Wang. 2020. Adaptive graphconvolutional recurrent network for traffic forecasting. Advances in NeuralInformation Processing Systems 33 (2020), 1780417815. Weiqi Chen, Ling Chen, Yu Xie, Wei Cao, Yusong Gao, and Xiaojie Feng. 2020.Multi-range attentive bicomponent graph convolutional network for traffic fore-casting. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34.35293536. Razvan-Gabriel Cirstea, Bin Yang, Chenjuan Guo, Tung Kieu, and Shirui Pan.2022. Towards spatio-temporal aware traffic time series forecasting. In IEEE 38thInternational Conference on Data Engineering. 29002913. Yue Cui, Kai Zheng, Dingshan Cui, Jiandong Xie, Liwei Deng, Feiteng Huang,and Xiaofang Zhou. 2021. METRO: a generic graph neural network frameworkfor multivariate time series forecasting. Proceedings of the VLDB Endowment 15,2 (2021), 224236. Zulong Diao, Xin Wang, Dafang Zhang, Yingru Liu, Kun Xie, and Shaoyao He.2019. Dynamic spatial-temporal graph convolutional neural networks for trafficforecasting. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33.890897. Yuchen Fang, Yanjun Qin, Haiyong Luo, Fang Zhao, Bingbing Xu, Liang Zeng,and Chenxing Wang. 2023. When spatio-temporal meet wavelets: Disentangledtraffic forecasting via efficient spectral graph attention networks. In 2023 IEEE39th International Conference on Data Engineering. 517529. Ziquan Fang, Lu Pan, Lu Chen, Yuntao Du, and Yunjun Gao. 2021. MDTP: Amulti-source deep traffic prediction framework over spatio-temporal trajectorydata. Proceedings of the VLDB Endowment 14, 8 (2021), 12891297. Shengnan Guo, Youfang Lin, Ning Feng, Chao Song, and Huaiyu Wan. 2019.Attention based spatial-temporal graph convolutional networks for traffic flowforecasting. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33.922929. Liangzhe Han, Bowen Du, Leilei Sun, Yanjie Fu, Yisheng Lv, and Hui Xiong. 2021.Dynamic and Multi-faceted Spatio-temporal Deep Learning for Traffic SpeedForecasting. In Proceedings of the 27th ACM SIGKDD Conference on KnowledgeDiscovery & Data Mining. 547555. Liangzhe Han, Bowen Du, Leilei Sun, Yanjie Fu, Yisheng Lv, and Hui Xiong.2021. Dynamic and multi-faceted spatio-temporal deep learning for traffic speedforecasting. In Proceedings of the 27th ACM SIGKDD Conference on KnowledgeDiscovery & Data Mining. 547555. Rongzhou Huang, Chuyin Huang, Yubao Liu, Genan Dai, and Weiyang Kong.2020. LSGCN: Long Short-Term Traffic Prediction with Graph ConvolutionalNetworks. In Proceedings of the 29th International Joint Conference on ArtificialIntelligence, Vol. 7. 23552361. Renhe Jiang, Zhaonan Wang, Jiawei Yong, Puneet Jeph, Quanjun Chen, Ya-sumasa Kobayashi, Xuan Song, Shintaro Fukushima, and Toyotaro Suzumura.2023. Spatio-temporal meta-graph learning for traffic forecasting. In Proceedingsof the AAAI Conference on Artificial Intelligence, Vol. 37. 80788086. Guangyin Jin, Fuxian Li, Jinlei Zhang, Mudan Wang, and Jincai Huang. 2022.Automated dilated spatio-temporal synchronous graph modeling for traffic pre-diction. IEEE Transactions on Intelligent Transportation Systems 24, 8 (2022),88208830. Songyu Ke, Zheyi Pan, Tianfu He, Yuxuan Liang, Junbo Zhang, and Yu Zheng.2023. AutoSTG+: An automatic framework to discover the optimal network forspatio-temporal graph prediction. Artificial Intelligence 318 (2023), 103899. Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. 2018. Modelinglong-and short-term temporal patterns with deep neural networks. In The 41stinternational ACM SIGIR Conference on Research & Development in InformationRetrieval. 95104. Zhichen Lai, Dalin Zhang, Huan Li, Christian S Jensen, Hua Lu, and Yan Zhao.2023. LightCTS: A Lightweight Framework for Correlated Time Series Forecast-ing. Proceedings of the ACM on Management of Data 1, 2 (2023), 126. Fuxian Li, Jie Feng, Huan Yan, Guangyin Jin, Fan Yang, Funing Sun, Depeng Jin,and Yong Li. 2023. Dynamic graph convolutional recurrent network for trafficprediction: Benchmark and solution. ACM Transactions on Knowledge Discoveryfrom Data 17, 1 (2023), 121.",
  "Mengzhang Li and Zhanxing Zhu. 2021. Spatial-temporal fusion graph neuralnetworks for traffic flow forecasting. In Proceedings of the AAAI Conference onArtificial Intelligence, Vol. 35. 41894196": "Ting Li, Junbo Zhang, Kainan Bao, Yuxuan Liang, Yexin Li, and Yu Zheng. 2020.Autost: Efficient neural architecture search for spatio-temporal prediction. InProceedings of the 26th ACM SIGKDD International Conference on KnowledgeDiscovery & Data Mining. 794802. Wei Li, Xi Zhan, Xin Liu, Lei Zhang, Yu Pan, and Zhisong Pan. 2023. SAST-GCN: A Self-Adaptive Spatio-Temporal Graph Convolutional Network for TrafficPrediction. ISPRS International Journal of Geo-Information 12, 8 (2023), 346.",
  "Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. 2017. Diffusion convolu-tional recurrent neural network: Data-driven traffic forecasting. InternationalConference on Learning Representations": "Zhonghang Li, Chao Huang, Lianghao Xia, Yong Xu, and Jian Pei. 2022. Spatial-temporal hypergraph self-supervised learning for crime prediction. In IEEE 38thInternational Conference on Data Engineering. 29842996. Yuxuan Liang, Songyu Ke, Junbo Zhang, Xiuwen Yi, and Yu Zheng. 2018. Ge-oman: Multi-level attention networks for geo-sensory time series prediction..In Proceedings of the 27th International Joint Conference on Artificial Intelligence.34283434. Yuxuan Liang, Kun Ouyang, Yiwei Wang, Zheyi Pan, Yifang Yin, HongyangChen, Junbo Zhang, Yu Zheng, David S Rosenblum, and Roger Zimmermann.2022. Mixed-order relation-aware recurrent neural networks for spatio-temporalforecasting. IEEE Transactions on Knowledge and Data Engineering 35, 9 (2022),92549268. Haitao Lin, Zhangyang Gao, Yongjie Xu, Lirong Wu, Ling Li, and Stan Z Li. 2022.Conditional local convolution for spatio-temporal meteorological forecasting. InProceedings of the AAAI Conference on Artificial Intelligence, Vol. 36. 74707478. Shuai Ling, Zhe Yu, Shaosheng Cao, Haipeng Zhang, and Simon Hu. 2023. STHAN:Transportation demand forecasting with compound spatio-temporal relation-ships. ACM Transactions on Knowledge Discovery from Data 17, 4 (2023), 123.",
  "Fan LIU, Hao Liu, and Wenzhao Jiang. 2022. Practical Adversarial Attacks onSpatiotemporal Traffic Forecasting Models. In Advances in Neural InformationProcessing Systems, Vol. 35. 1903519047": "Hangchen Liu, Zheng Dong, Renhe Jiang, Jiewen Deng, Jinliang Deng, Quan-jun Chen, and Xuan Song. 2023. Spatio-temporal adaptive embedding makesvanilla transformer sota for traffic forecasting. In Proceedings of the 32nd ACMinternational conference on information and knowledge management. 41254129. Hao Liu, Jindong Han, Yanjie Fu, Jingbo Zhou, Xinjiang Lu, and Hui Xiong. 2020.Multi-modal transportation recommendation with unified route representationlearning. Proceedings of the VLDB Endowment 14, 3 (2020), 342350.",
  "Hanxiao Liu, Karen Simonyan, and Yiming Yang. 2019. Darts: Differentiablearchitecture search. In International Conference on Learning Representations": "Zheyi Pan, Songyu Ke, Xiaodu Yang, Yuxuan Liang, Yong Yu, Junbo Zhang,and Yu Zheng. 2021. AutoSTG: Neural Architecture Search for Predictions ofSpatio-Temporal Graph. In Proceedings of the Web Conference. 18461855. Zheyi Pan, Yuxuan Liang, Weifeng Wang, Yong Yu, Yu Zheng, and Junbo Zhang.2019. Urban traffic prediction from spatio-temporal data using deep meta learning.In Proceedings of the 25th ACM SIGKDD International Conference on KnowledgeDiscovery & Data Mining. 17201730. Cheonbok Park, Chunggi Lee, Hyojin Bahng, Yunwon Tae, Seungmin Jin, KihwanKim, Sungahn Ko, and Jaegul Choo. 2020. ST-GRAT: A novel spatio-temporalgraph attention networks for accurately forecasting dynamically changing roadspeed. In Proceedings of the 29th ACM International Conference on Information &Knowledge Management. 12151224. Weizhu Qian, Dalin Zhang, Yan Zhao, Kai Zheng, and JQ James. 2023. Uncer-tainty quantification for traffic forecasting: A unified approach. In 2023 IEEE 39thInternational Conference on Data Engineering (ICDE). IEEE, 9921004. Hao Qu, Yongshun Gong, Meng Chen, Junbo Zhang, Yu Zheng, and Yilong Yin.2022. Forecasting fine-grained urban flows via spatio-temporal contrastive self-supervision. IEEE Transactions on Knowledge and Data Engineering 35, 8 (2022),80088023. Zezhi Shao, Zhao Zhang, Fei Wang, Wei Wei, and Yongjun Xu. 2022. Spatial-temporal identity: A simple yet effective baseline for multivariate time seriesforecasting. In Proceedings of the 31st ACM International Conference on Information& Knowledge Management. 44544458. Zezhi Shao, Zhao Zhang, Fei Wang, and Yongjun Xu. 2022. Pre-training enhancedspatial-temporal graph neural network for multivariate time series forecasting.In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery &Data Mining. 15671577. Zezhi Shao, Zhao Zhang, Wei Wei, Fei Wang, Yongjun Xu, Xin Cao, and Chris-tian S Jensen. 2022. Decoupled dynamic spatial-temporal graph neural networkfor traffic forecasting. Proceedings of the VLDB Endowment 15, 11 (2022), 27332746. Chao Song, Youfang Lin, Shengnan Guo, and Huaiyu Wan. 2020.Spatial-temporal synchronous graph convolutional networks: A new framework forspatial-temporal network data forecasting. In Proceedings of the AAAI Conferenceon Artificial Intelligence, Vol. 34. 914921.",
  "Xinle Wu, Dalin Zhang, Chenjuan Guo, Chaoyang He, Bin Yang, and Christian SJensen. 2021. AutoCTS: Automated correlated time series forecasting. Proceedingsof the VLDB Endowment 15, 4 (2021), 971983": "Xinle Wu, Dalin Zhang, Miao Zhang, Chenjuan Guo, Bin Yang, and Christian SJensen. 2023. AutoCTS+: Joint Neural Architecture and Hyperparameter Searchfor Correlated Time Series Forecasting. Proceedings of the ACM on Managementof Data 1, 1 (2023), 126. Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, Xiaojun Chang, and ChengqiZhang. 2020. Connecting the dots: Multivariate time series forecasting with graphneural networks. In Proceedings of the 26th ACM SIGKDD International Conferenceon Knowledge Discovery & Data Mining. 753763. Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, and Chengqi Zhang. 2019.Graph wavenet for deep spatial-temporal graph modeling. In Proceedings of the28th International Joint Conference on Artificial Intelligence. 19071913.",
  "Chengzhi Yao, Zhi Li, and Junbo Wang. 2023. Spatio-Temporal Hypergraph NeuralODE Network for Traffic Forecasting. In 2023 IEEE International Conference onData Mining. IEEE, 14991504": "Junchen Ye, Zihan Liu, Bowen Du, Leilei Sun, Weimiao Li, Yanjie Fu, and HuiXiong. 2022. Learning the evolutionary and multi-scale graph structure formultivariate time series forecasting. In Proceedings of the 28th ACM SIGKDDConference on Knowledge Discovery & Data Mining. 22962306. Bing Yu, Haoteng Yin, and Zhanxing Zhu. 2018. Spatio-temporal graph convolu-tional networks: a deep learning framework for traffic forecasting. In Proceedingsof the 27th International Joint Conference on Artificial Intelligence. 36343640.",
  "Fisher Yu and Vladlen Koltun. 2016. Multi-scale context aggregation by dilatedconvolutions. In International Conference on Learning Representations": "Qi Zhang, Jianlong Chang, Gaofeng Meng, Shiming Xiang, and Chunhong Pan.2020. Spatio-temporal graph structure learning for traffic forecasting. In Proceed-ings of the AAAI Conference on Artificial Intelligence, Vol. 34. 11771185. Weijia Zhang, Hao Liu, Yanchi Liu, Jingbo Zhou, and Hui Xiong. 2020. Semi-supervised hierarchical recurrent graph neural network for city-wide parkingavailability prediction. In Proceedings of the AAAI Conference on Artificial Intelli-gence, Vol. 34. 11861193.",
  "Yunhao Zhang and Junchi Yan. 2022. Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting. In InternationalConference on Learning Representations": "Zeyang Zhang, Xin Wang, Ziwei Zhang, Haoyang Li, Zhou Qin, and WenwuZhu. 2022. Dynamic graph neural networks under spatio-temporal distributionshift. Advances in Neural Information Processing Systems 35 (2022), 60746089. Zijian Zhang, Xiangyu Zhao, Hao Miao, Chunxu Zhang, Hongwei Zhao, andJunbo Zhang. 2023. Autostl: Automated spatio-temporal multi-task learning. InProceedings of the AAAI Conference on Artificial Intelligence, Vol. 37. 49024910. Chuanpan Zheng, Xiaoliang Fan, Shirui Pan, Haibing Jin, Zhaopeng Peng, Zong-han Wu, Cheng Wang, and S Yu Philip. 2023. Spatio-temporal joint graph con-volutional networks for traffic forecasting. IEEE Transactions on Knowledge andData Engineering (2023). Chuanpan Zheng, Xiaoliang Fan, Cheng Wang, and Jianzhong Qi. 2020. Gman: Agraph multi-attention network for traffic prediction. In Proceedings of the AAAIConference on Artificial Intelligence, Vol. 34. 12341241. Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,and Wancai Zhang. 2021. Informer: Beyond efficient transformer for long se-quence time-series forecasting. In Proceedings of the AAAI Conference on ArtificialIntelligence, Vol. 35. 1110611115.",
  "A.1Automated Spatio-temporal Models": "General Framework. In order to gain a deeper understanding ofthe potential of automated spatio-temporal forecasting models, weinvestigate several representative works . We firstprovide an overview of the general automated model, as shownin , which primarily comprises three key components, i.e.,the embedding layer, search module, and output layer. Firstly, anembedding layer is responsible for converting raw time series datainto latent representations or fusing various embeddings, such aspositional, spatial, and temporal embeddings, as described by Au-toST . Secondly, the search module is the key component thatmay comprise several spatio-temporal blocks. Each block exploresspatio-temporal dependencies by automatically examining the opti-mal combination of operators within a mixed search space. Finally,an output layer integrates the latent representations derived fromthe preceding layer, resulting in forecasting.Mixed Search Space. We summarize the search space of spatio-temporal blocks in existing representative models .By following previous studies , we classify frequently usedoperators for automated spatio-temporal forecasting and analyzethe time and space complexity of each operator, as shown in .Specifically, within the field of automated spatio-temporal forecast-ing, we categorize temporal operators into two groups. (1) TheConvolutional Neural Network (CNN) family, such as the TemporalConvolutional Network (TCN), employs dilated causal convolutionsto process spatio-temporal data. (2) The Transformer family, whichemploys self-attention mechanisms to facilitate the weights among",
  ": An example of the different traffic patterns on thePEMS04 dataset. Spatial dependencies may vary based on thetime intervals of varying granularity": "input time steps, enhancing the representation ability of temporalinformation over extended sequences. Similarly, we classify spatialoperators into two families. (1) The Graph Neural Network (GNN)family, which leverages predefined or adaptive adjacency matriceslearned from a data-driven method. (2) The Transformer family,which applies attention mechanisms across different time series tocapture high-order spatial dependencies.",
  "A.2The Efficiency Bottleneck": "To investigate the efficiency bottleneck of temporal and spatial oper-ators in the mixed search space, we analyze run time and parametersvia a case study. Specifically, we select two state-of-the-art auto-mated spatio-temporal forecasting models, namely AutoSTG and AutoCTS . We conduct the experiment with the PEMS-BAYdataset and use the settings reported by the original papers, andthe results are shown in . We calculate the run time andparameters of each operator and then count the proportions oftemporal and space operators in mixed search space. As reported,",
  "FAAttention MatrixGeoMAN , LightCTS": "the spatial operator typically has a larger parameter size and con-sumes more computational resources than the temporal operator.Such observations lead to two potential optimization directions:reducing computational redundancies and reducing the number ofparameters involved in spatial operators.Firstly, it is evident that temporal information is crucial for mod-eling dependencies within each time series but may be redundantwhen exploring dependencies across different locations . Thus,the first critical insight is that decoupling the mixed search spaceinto temporal space and spatial space and devising a representationcompression scheme to distill the key temporal information for spa-tial search can speed up the overall computations. As presented in, the time and space complexities of all temporal and spatialoperators are positively correlated with the numbers of sensors ,historical time steps , and hidden dimension . However, isdecided by the raw time series data and should be left as a constantin the spatio-temporal forecasting model. Directly reducing caninevitably degrade forecasting accuracy, as validated by previousstudies . Fortunately, recent studies prove also can notbe directly reduced during the temporal modeling phase but canbe compressed in the spatial modeling phase without influencingthe forecasting performance. Thus, it is intuitive to devise a rep-resentation compression scheme to accelerate the overall neuralarchitecture search.Secondly, existing automated forecasting models involve a sub-stantial number of parameters in the spatial search process (asshown in ), inducing considerable computation overhead.To tackle this issue, we propose to share parameters among thesame type of spatial blocks to reduce the spatial search overhead.In this paper, we investigate decoupling the mixed spatio-temporalsearch space and respectively devise representation compressionand parameter-sharing schemes to optimize the neural architecturesearch efficiency.",
  "A.3The Effectiveness Bottleneck": "From the effectiveness perspective, existing automated models ex-plore spatio-temporal dependencies in a coarse-grained manner,leading to unsatisfied prediction accuracy. As shown in , thetraffic patterns of three geo-distributed sensors are vastly differentand exhibit diverged correlations. Specifically, Sensor 108 experi-ences high traffic flow throughout the day, Sensor 7 has the mostsignificant traffic flow during morning peak hours, and Sensor 6 has",
  ") ( )": "the largest traffic flow during evening peak hours. However, it hasstrong correlations in a finer granularity. For instance, the trafficflow in Patch 1 exhibits an increasing trend, while the traffic flowin Patch 4 shows a decreasing trend. Thus, fine-grained explorationof spatial search can be beneficial in enhancing performance.In addition, existing automated methods have employed the samemessage-passing scheme across different GNN layers, which failsto consider the unique spatial dependencies present in differentmulti-hop neighbors. Recent advances have proposed diverse adja-cency matrices for spatial dependency modeling and demonstratedthe effectiveness of different adjacency matrices in different con-texts . As summarized in , we categorize the commonlyused adjacency matrices in three categories : (1) the fully fixed(FF) matrix that is the predefined adjacency matrix and fixed inboth the training and inferencing phase; (2) the semi-adaptive (SA)matrix that the parameters of the adjacency matrix are learnablewith the training phase, and fixed in the inferencing phase; (3) thefully adaptive (FA) matrix that was learned from data-driven meth-ods and adaptive in both the training and inferencing phase. In fact,each adjacency matrix category can be regarded as a new class ofspatial operators. It is intuitive to incorporate varying adjacencymatrix operators in different layers so as to encompass more spatialsemantics for more effective forecasting. In this work, we constructan extended spatial search space that includes the distance ma-trix, adaptive matrix, and attention matrix operators. Based on thenew spatial space, we propose a more flexible message-passingscheme to automatically select the most suitable adjacency matrixin different layers for better forecasting accuracy.",
  "BSupplementary of ExperimentB.1Datasets": "We conducted experiments on eight benchmark datasets for bothmulti- and single-step forecasting. These datasets can be categorizedas traffic speed, traffic flow, solar-energy, and electricity.Multi-step forecasting. The METR-LA and PEMS-BAY datasetsare both traffic speed datasets released by Li et al. . The PEMS03,PEMS04, PEMS07, and PEMS08 datasets are all traffic flow collectedfrom the Caltrans Performance Measurement System (PeMS) andreleased by Song et al. .Single-step forecasting. The solar-energy dataset captures the10-minute variations of 137 PV plants across Alabama. The elec-tricity dataset reflects the hourly electricity consumption of 321clients from 2012 to 2014. These two datasets are pre-processed andreleased by Lai et al. .",
  "STHODE : It employs a spatial-temporal hypergraphcoupled with ODE networks to enhance traffic predictionby capturing high-order dependencies in road networks andtraffic dynamics": "DeepSTUQ : This model employs a spatio-temporal modelfor traffic datas complex correlations and uses two sub-networks for aleatoric uncertainty, while integrating MonteCarlo dropout and Adaptive Weight Averaging for epistemicuncertainty. STJGCN : It offers accurate traffic forecasting by con-structing dynamic spatio-temporal joint graphs and apply-ing dilated graph convolutions with a multi-range attentionmechanism to capture dependencies across multiple timesteps and ranges.",
  "B.3Evaluation Metrics": "We followed previous works and adopted differentmetrics to evaluate the models performance in multi- and single-step forecasting. Specifically, Mean Absolute Error (MAE), RootMean Squared Error (RMSE), and Mean Absolute Percentage Error(MAPE) were utilized to assess the accuracy of multi-step forecast-ing. For single-step forecasting, we utilized Root Relative SquaredError (RRSE) and Empirical Correlation Coefficient (CORR) as eval-uation metrics. Lower values of MAE, RMSE, MAPE, and RRSE areindicative of higher accuracy, while larger CORR values denotehigher accuracy.",
  "B.4Implementation Details": "The experiments were conducted on three different server config-urations. The first configuration consisted of three Linux Centosservers, each equipped with 4 RTX 3090 GPUs. The second config-uration included one Linux Ubuntu server with 2 V100 GPUs, andthe third configuration included one Linux Ubuntu server with 2A800 GPUs.Following the previous works , we use MAE as theloss function to train the model and use Adam with a learning rateof 0.001 for both the architecture parameters and the networkweights . In addition, we use a weight decay of 0.0001 as the opti-mizer, and the batch size is set to 64. In the temporal and spatialDAG, we set the number of nodes as 4. In order to ensure repro-ducibility, we have provided the detailed settings for each datasetin our source code.",
  "B.5Parameter Sensitivity Analysis": "In this experiment, we evaluate the impact of key hyperparametersin AutoSTF, including T, which represents the number of nodesin the temporal-DAG, S, which represents the number of nodesin the spatial-DAG, and , which represents the number of patchesin the multi-patch transfer module. We set the default values forT, S, and to 4, 4, and 2, respectively. For instance, when = 2, we divide the temporal embedding T into two patches.We vary T and S among {2, 3, 4, 5, 6}, and among {1, 2, 3, 4,6}, while keeping the remaining hyperparameters at default values.",
  ",367": "We present the results in Figures 9 and , where the utilizeddatasets represent different types of data. Specifically, METR-LAcorresponds to traffic speed data, while PEMS04 corresponds totraffic flow data.As shown in , AutoSTF achieves the best accuracy underT = 4 and S = 4. Reducing T or S may result in insufficientexpressiveness of the model, leading to a reduction in the accuracyof the model. Increasing T or S augments the complexity of thetemporal and spatial search space, which may lead to overfittingproblems. As a result, it may slightly degrade the accuracy. Addi-tionally, we present the results for different numbers of patches in. It can be observed that the impact of patches with vary-ing granularities varies significantly across different datasets. Forinstance, in traffic flow datasets, the fluctuations in traffic flow atdifferent time points might be more pronounced. Therefore, examin-ing spatio-temporal correlations with a more fine-grained approachcan enhance the accuracy of the model. For the PEMS04 dataset,using a larger value of can lead to an improvement in the modelsaccuracy. On the other hand, in traffic speed datasets, the averagespeed of vehicles tends to change less over a short period of time.Thus, employing fewer patches to explore spatio-temporal depen-dencies may yield more accurate results. For the METR-LA dataset,a smaller value of achieves the best performance compared tolarger values of .",
  "B.6Case Study": "Due to space limitations, we only show the neural architecturepredicted by AutoSTF for the PEMS04 dataset in . Fromthe results, we can observe that in the temporal-DAG, the model au-tomatically selects three and three modules to ex-plore the temporal dependencies. Furthermore, in the spatial search,our model has three spatial-DAGs. Based on the results, we canconclude that our model is capable of automatically designing ap-propriate message-passing aggregations for different spatial-DAGsin order to examine spatio-temporal dependencies. This demon-strates the effectiveness of decoupling the mixed search space andperforming fine-grained spatial search for different patches. Ulti-mately, this approach results in the development of models thatexcel in both efficiency and accuracy.Additionally, we also conduct a case study to demonstrate ourmodels ability to capture and model spatio-temporal dependencies",
  ": The visualization of the forecasting performanceon the PEMS04 and PEMS08 datasets": "accurately, shown in . We randomly select two sensorsthat are interconnected in PEMS04 and PEMS08 dataset, respec-tively. We can observe from that each two sensor showsa significant correlation, indicating that the traffic flow of one sen-sor correlates with the other sensor. For example, Sensor 290 inPEMS04 shows a strong spatio-temporal correlation with Sensor110, when traffic flow is increased at Sensor 110, Sensor 290 willalso increase. They exhibit a similar pattern, indicating a strongspatio-temporal correlation. Based on , it is evident thatour AutoSTF model accurately predicts the traffic flow of Sensor(110, 290) and (51, 73), showcasing remarkable accuracy in fore-casting similar patterns. This demonstrates that AutoSTF can alsoeffectively preserve the spatio-temporal correlation.",
  "B.7The Improvement of AutoSTF": "To more clearly compare the performance improvements of Au-toSTF and other NAS-based spatio-temporal forecasting models,we have calculated their performance improvements relative to Au-toSTG based on the MAE on METR-LA and PEMS-BAY datasets. Asobserved in , it is evident that AutoSTF exhibits significantimprovements in all different time-scale predictions compared toother automated spatio-temporal forecasting models.",
  "B.8Complexity Analysis": "In this section, we analyze the algorithmic complexity of AutoSTF.In AutoSTF, we compute the maximum space and time complexitythat occurs during the neural architecture search process. AssumingAutoSTF comprises one temporal-DAG and spatial-DAGs, eachcontaining edges (search operation), where is the number ofpatches. In the temporal search module, we hypothesize that eachsearch operation selects an (the most time-consumingoperator in our search space), with the space complexity of the being() and the time complexity being(), where is the sequence length. Thus, the total space complexityduring the temporal search phase is () , and the total timecomplexity is () . In the spatial search module, the mosttime-consuming operator is , which is based on the spatialtransformer architecture. It has a space complexity of ( 2) and atime complexity of ( 2), where is the number of nodes. Con-sequently, the overall space complexity in the spatial search phaseamounts to ( 2), and the overall time complexity reaches( 2). We can determine that the space and time complexi-ties of the neural architecture search of AutoSTF are respectively( + 2) and ( + 2).",
  "C.1The accuracy and efficiency of AutoSTFcompared with AutoSTF-mixed": "We conducted experiments to validate the decoupled search space,as shown in and . In our model variant, AutoSTF-mixed, we created a mixed search space that combines both tempo-ral and spatial operators. We utilized two directed acyclic graphs(DAGs) to simulate the forward flow during the search and trainingphases of a neural network. Within these two DAGs, we searchedfor the optimal neural architecture for the AutoSTF-mixed model.We evaluated the forecasting accuracy and average search time perepoch to assess the performance of the AutoSTF-mixed model. Aspresented in and , it is surprising to observe thatAutoSTF-mixed did not achieve the highest forecasting accuracyand efficiency compared to AutoSTF. This outcome can be attrib-uted to the presence of numerous operators in the mixed searchspace. The weights assigned to operators by the DARTS algorithmmay deviate significantly from their actual operator, resulting in asubstantial performance gap (see AutoCTS). Conversely, searchingwithin both temporal and spatial search spaces allows for moreprecise identification of the optimal operator, thereby enhancingthe models performance.",
  "(f) PEMS08": ": Visualization of AutoSTF and other classic manual-design models (DCRNN, GraphWaveNet, and MTGNN). Thevisualization illustrates the superior forecasting performance of AutoSTF compared to traditional models that utilize spa-tiotemporal correlation for improved prediction. This also demonstrates the capability of AutoSTF to preserve spatiotemporalcorrelation. the suitable operator based on the results of the temporal search.This selection process is optimized through backpropagation, ensur-ing the entire model framework is optimized holistically, therebypreserving the spatio-temporal correlation. In addition, we design amore comprehensive spatial operator in spatial search space, includ-ing the fully fixed matrix, semi-adaptive matrix, and fully adaptivematrix. These operators can be adaptively chosen at different layersof the GNN, which can extract various complex spatiotemporalfeatures.To comprehensively evaluate the ability of AutoSTF to preservespatio-temporal correlation, we compared AutoSTF with traditionalmethods such as MTGNN, GraphWaveNet, and DCRNN, which also claim to improve prediction performance by capturing spatio-temporal correlation. For visualization purposes, we randomly se-lected a sensor from different datasets and displayed the resultsin . It is evident from that AutoSTF outper-forms these classic spatio-temporal forecasting methods. This alsodemonstrates, from various perspectives, that AutoSTF has the abil-ity to preserve spatio-temporal correlation, leading to enhancedprediction performance.Overall, based on the detailed analysis and experiments pre-sented above, our model is capable of preserving spatiotemporalcorrelations even after decoupling the spatiotemporal search intoseparate spatial and temporal searches."
}