{
  "ABSTRACT": "Sentiment analysis and emotion detection are important researchtopics in natural language processing (NLP) and benefit many down-stream tasks. With the widespread application of large languagemodels (LLMs), researchers have started exploring the applicationof LLMs based on instruction-tuning in the field of sentiment analy-sis. However, these models only focus on single aspects of affectiveclassification tasks (e.g. sentimental polarity or categorical emo-tions), and overlook the regression tasks (e.g. sentiment strengthor emotion intensity), which leads to poor performance in down-stream tasks. The main reason is the lack of comprehensive affec-tive instruction tuning datasets and evaluation benchmarks, whichcover various affective classification and regression tasks. Moreover,although emotional information is useful for downstream tasks,existing downstream datasets lack high-quality and comprehen-sive affective annotations. In this paper, we propose EmoLLMs, thefirst series of open-sourced instruction-following LLMs for compre-hensive affective analysis based on fine-tuning various LLMs withinstruction data, the first multi-task affective analysis instructiondataset (AAID) with 234K data samples based on 3 classificationtasks and 2 regression tasks to support LLM instruction tuning,and a comprehensive affective evaluation benchmark (AEB) with 8regression tasks and 6 classification tasks from various sources anddomains to test the generalization ability of LLMs. We propose aseries of EmoLLMs by fine-tuning LLMs with AAID to solve variousaffective instruction tasks. We compare our models with a varietyof LLMs and sentiment analysis tools on AEB, where our modelsoutperform all other open-sourced LLMs and sentiment analysis",
  "Corresponding authorQianqian is now affiliated with Yale University. The work was done while she was atThe University of Manchester": "Permission to make digital or hard copies of part or all of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for third-party components of this work must be honored.For all other uses, contact the owner/author(s).KDD 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s).ACM ISBN 979-8-4007-0490-1/24/08 tools, and surpass ChatGPT and GPT-4 in most tasks, which showsthat the series of EmoLLMs achieve the ChatGPT-level and GPT-4-level generalization capabilities on affective analysis tasks, anddemonstrates our models can be used as affective annotation tools.This project is available at",
  "Sentiment analysis, emotion detection, large language models, af-fective instruction dataset, affective evaluation benchmark": "ACM Reference Format:Zhiwei Liu, Kailai Yang, Qianqian Xie, Tianlin Zhang, and Sophia Anani-adou. 2024. EmoLLMs: A Series of Emotional Large Language Models andAnnotation Tools for Comprehensive Affective Analysis. In Proceedings ofthe 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining(KDD 24), August 2529, 2024, Barcelona, Spain. ACM, New York, NY, USA,10 pages.",
  "INTRODUCTION": "Emotions and sentiments play a crucial role in shaping our lives.Our words and actions serve as indicators of our emotional states. Leveraging natural language processing (NLP) techniquessuch as Emotion Detection (ED) and Sentiment Analysis (SA), wecan delve into the analysis of human interactions, enabling usto comprehend peoples emotional responses toward particularsubjects . Specifically, SA tasks typically involve predictingthe polarity (usually positive, negative, or neutral), along withthe strength of this tone , and emotion detection tasks ofteninvolve classifying data into fine-grained emotion categories (e.g.Ekman , Plutchik ) or predicting the intensity of emotions. These affective information are proven as useful features formany downstream tasks, including mental health analysis ,misinformation detection , and empathetic dialogue systems.",
  "KDD 24, August 2529, 2024, Barcelona, SpainZhiwei Liu, Kailai Yang, Qianqian Xie, Tianlin Zhang, & Sophia Ananiadou": "Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, ChelseaVoss, Alec Radford, Dario Amodei, and Paul F Christiano. 2020. Learning tosummarize with human feedback. Advances in Neural Information ProcessingSystems 33 (2020), 30083021. Mike Thelwall, Kevan Buckley, Georgios Paltoglou, Di Cai, and Arvid Kappas.2010. Sentiment strength detection in short informal text. Journal of the Americansociety for information science and technology 61, 12 (2010), 25442558. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-AnneLachaux, Timothe Lacroix, Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, et al. 2023. Llama: Open and efficient foundation language models. arXivpreprint arXiv:2302.13971 (2023). Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-mine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-ale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXivpreprint arXiv:2307.09288 (2023). Cheng Wen, Xianghui Sun, Shuaijiang Zhao, Xiaoquan Fang, Liangyu Chen, andWei Zou. 2023. ChatHome: Development and Evaluation of a Domain-SpecificLanguage Model for Home Renovation. arXiv preprint arXiv:2307.15290 (2023). BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, ElliePavlick, Suzana Ili, Daniel Hesslow, Roman Castagn, Alexandra Sasha Luccioni,Franois Yvon, et al. 2022. Bloom: A 176b-parameter open-access multilinguallanguage model. arXiv preprint arXiv:2211.05100 (2022). Hongliang Xie, Shi Feng, Daling Wang, and Yifei Zhang. 2018. A novel atten-tion based CNN model for emotion intensity prediction. In Natural LanguageProcessing and Chinese Computing: 7th CCF International Conference, NLPCC 2018,Hohhot, China, August 2630, 2018, Proceedings, Part I 7. Springer, 365377. Qianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao Lai, Min Peng, AlejandroLopez-Lira, and Jimin Huang. 2023. PIXIU: A Large Language Model, InstructionData and Evaluation Benchmark for Finance. arXiv preprint arXiv:2306.05443(2023). Kailai Yang, Tianlin Zhang, Ziyan Kuang, Qianqian Xie, and Sophia Ananiadou.2023. Mentalllama: Interpretable mental health analysis on social media withlarge language models. arXiv preprint arXiv:2309.13567 (2023). Zhewei Yao, Reza Yazdani Aminabadi, Olatunji Ruwase, Samyam Rajbhandari,Xiaoxia Wu, Ammar Ahmad Awan, Jeff Rasley, Minjia Zhang, Conglong Li,Connor Holmes, et al. 2023. DeepSpeed-Chat: Easy, Fast and Affordable RLHFTraining of ChatGPT-like Models at All Scales. arXiv preprint arXiv:2308.01320(2023).",
  "We introduce a series of EmoLLMs, the first open-sourceinstruction following LLMs for comprehensive affective anal-ysis": "We compare EmoLLMs with other LLMs on AEB. Addition-ally, we conduct a comprehensive analysis of the affectiveanalysis capabilities of ChatGPT and GPT-4. Our modelsachieve SOTA performance on the AEB dataset comparedto other open-sourced LLMs and present ChatGPT-level andGPT-4-level generalization capabilities, establishing theirpotential as effective tools for affective annotation.The structure of this paper is as follows: introduces therelated work about sentiment analysis models and open-sourcedLLMs. introduces the proposed method. Specifically, Sec-tion 3.1 introduce the task definition. .2 and .3present the construction process of AAID and AEB respectively..4 introduces the training process of EmoLLMs. presents the experiment results on AEB and analyses the perfor-mance of each model. concludes this paper by summariz-ing our findings. discusses the real-world applications ofEmoLLMs, limitations, and future work.",
  "RELATED WORK2.1Affective Analysis Model": "There have been various affective analysis tools proposed, such asVADER , and TextBlob. Although these tools are convenient touse, their effectiveness in sentiment analysis is not ideal . Inrecent years, many studies have focused on fine-tuning PLMs to en-hance their capabilities in the field of sentiment analysis. Bello et al. combine BERT with other deep learning models (e.g. CNN, RNN,LSTM) to improve the ability of the model in short and simple textsentiment analysis. Liao et al. propose a multi-task model basedon RoBERTa for aspect-category sentiment analysis. Yin et al. propose the SentiBERT model, which focuses on the field of senti-ment analysis. SentiBERT integrates a recursive constituency treebased on BERT to better capture compositional sentiment semantics.Recently, numerous studies have embarked on investigating theutilization of LLMs in sentiment analysis, resulting in remarkableperformance gains in sentiment analysis tasks. Zhang et al. propose a retrieval-augmented LLM for financial sentiment analy-sis, which utilizes additional background information from externalsources and outperforms LLM baselines by 15% and 48%. Similarly,Lei et al. also use a simple yet effective retrieval module toenhance the emotion recognition capability of LLM in dialogue.Zhang et al. develop a context and emotion knowledge-tunedLLM, namely DialogueLLM, obtained by fine-tuning LLM with mul-timodal (i.e., texts and videos) emotional dialogues, which achieved",
  "Open Sourced Large Language Models": "Although ChatGPT and GPT-4 have shown excellent performancein various fields, their closed-source availability affects the progressof scientific research. Therefore, numerous studies are dedicatedto democratizing LLMs, such as the LLaMA series , OPTseries , BLOOM series , and Falcon . Based on the open-source LLMs, many efforts have been made to develop modelswith instruction-following capabilities like ChatGPT by training onextensive instruction-tuning datasets (e.g. Alpaca2 and the Vicuna3).Recently, there has been a lot of domain-specific work aimed atimproving the performance of LLM in specific domains by trainingon domain-specific instruction datasets. Such as FinMA in thefinance domain, MentalLLaMA in the mental health domain,TimeLlaMA used for temporal reasoning, and ExTES-LLaMA in emotional support chatbots. Our work is the first open-sourced LLM series for comprehensive multitask affective analysis.",
  "METHODS": "The goal of this work is to evaluate and enhance the comprehensiveand complex affective analysis capabilities of LLMs. To achieve thisobjective, we build the first affective analysis instruction dataset(AAID) to support LLMs tuning for comprehensive affective anal-ysis tasks. We propose EmoLLMs, a series of emotional LLMs byfine-tuning LLMs based on AAID. Furthermore, we construct acomprehensive affective evaluation benchmark to test the general-ization ability of LLMs.",
  "Task Definition": "Similar to in handling mental health analysis tasks, we alsoapproach affective analysis as a generative task, where a generativemodel (i.e., an autoregressive language model (|) parameter-ized by pre-trained weights ) is employed as the foundation, whichis unlike previous discriminative and regression models. This modelis capable of simultaneously addressing affective analysis tasks,such as sentiment polarity and strength prediction, emotion clas-sification and intensity prediction. Each task t is represented bya subset of training context-target pairs: = ( , )=1,2,... ,where is a token sequence containing the task description, targettext, and query, and is another sequence containing the queryanswer (i.e., classification result or regression result). All subsets arecombined into a training dataset: . The model is optimized basedon this merged data, aiming to maximize the conditional languagemodeling objective to enhance the accuracy of predictions.",
  "EI-reg, EI-ocanger1701/38817010/3880Twitterfear2252/38922520/3890Twitterjoy1616/29016160/2900Twittersadness1533/39715330/3970TwitterV-reg, V-oc1181/44911810/4490TwitterE-c6838/88668380/8860Twitter": "3.2.1Raw Data. SemEval 2018 Task1 contains five subtasks: 1.emotion intensity regression (EI-reg), 2. ordinal classification ofemotion intensity (EI-oc), 3. valence (sentiment) regression (V-reg), 4. ordinal classification of valence (sentiment) (V-oc), and 5.emotion classification (E-c).EI-reg: Given a tweet and an emotion E (anger, fear, joy, sadness),determine the intensity of E that best represents the mental state ofthe tweetera real-valued score between 0 (least E) and 1 (most E);EI-oc: Given a tweet and an emotion E (anger, fear, joy, sadness),classify the tweet into one of four ordinal classes (0: no E can beinferred. 1: low amount of E can be inferred. 2: moderate amount ofE can be inferred. 3: high amount of E can be inferred) of intensityof E that best represents the mental state of the tweeter;V-reg: Given a tweet, determine the intensity of sentiment orvalence (V) that best represents the mental state of the tweeterareal-valued score between 0 (most negative) and 1 (most positive);V-oc: Given a tweet, classify it into one of seven ordinal classes(from -3: very negative to 3: very positive), corresponding to var-ious levels of positive and negative sentiment intensity, that bestrepresents the mental state of the tweeter;E-c: Given a tweet, classify it as neutral or no emotion or asone, or more, of eleven given emotions (anger, anticipation, disgust,fear, joy, love, optimism, pessimism, sadness, surprise, trust) thatbest represent the mental state of the tweeter. 3.2.2AAID: Affective Analysis Instruction Dataset. We constructthe instruction dataset based on the raw data. Due to the limitedquantity of the original dataset, we utilize 10 different task instruc-tions for each task to augment the training set and validation set.The data statistics are presented in . Specifically, we buildinstruction-tuning samples based on some templates. de-scribes the specific instruction templates for each task, and provides corresponding examples (Taking EmoLLaMA as the ex-ample and each task selects one [task prompt] as an example). [taskprompt] describes the instructions for each specific task. The wordTweet can be adjusted based on the actual task. The [input text]refers to the content of the raw data. The final [output] shouldbe adjusted based on the specific task to provide sentiment clas-sification, sentiment strength, emotion classification, or emotionintensity.",
  "AEB: Affective Evaluation BenchmarkBuilding": "We first collect the test data from SemEval-2018 Task 1: Affect inTweets. To test the robustness of our model, a random instructionfrom the ten instructions used in train augment is selected foreach instance in the test set. We also collect additional sentimentanalysis or emotion detection datasets from various sources anddomains to test the generalizability of our model. We construct theAEB following the template format provided in . shows the task prompt example for each dataset. Except for thefour datasets from VADER, all other datasets utilize the originaltest dataset. shows the statistic details.Datasets used in Valence Aware Dictionary for sEntimentReasoning (VADER) : There are four datasets from different social media platforms with sentiment intensity (Valence) scoreswithin : V-Amazon (Amazon reviews snippets), V-Movies(Movies reviews snippets, collected from rotten.tomatoes.com), V-NYT (New York Times editorial snippets), V-Tweet (Tweets). Werandomly sampled 1000 instances from each dataset for generaliz-ability testing.EmoBank : This dataset was collected from News, blogs,fictions, letters etc. and contains three dimensions, which weremanually annotated with emotion according to the psychologicalValence-Arousal-Dominance scheme with scores within .Stanford Sentiment Treebank (SST) : It is collected frommovie reviews, which is the first corpus with fully labeled parsetrees, allowing for a comprehensive analysis of the composition-ality of sentiment in language. In SST4, each sentence is assigneda floating-point label that indicates the degree of positive senti-ment, ranging from 0.0 to 1.0. while in SST55, each sentence isannotated with five labels: very positive, positive, neutral, negative,very negative.Target Dependent Twitter Sentiment Classification (TDT): It is a Twitter sentiment classification dataset collected frompost comments for the celebrities, products, and companies, whichis annotated manually with three labels (negative, neutral, positive).To facilitate our generalizability testing, we restored the entities thatwere masked in the original data, creating a standard sentence-levelsentiment analysis dataset.GoEmotion : It is a multi-label classification dataset col-lected from Reddit comments, which consists of 28 emotion labels,including the neutral. However, the original dataset with 28 emotionlabels is imbalanced. To mitigate this issue, we select the \"Ekman\"",
  "DatasetTask prompt": "EI-regAssign a numerical value between 0 (least E) and 1 (most E) to represent the intensity of emotion E expressed in the tweet.EI-ocCategorize the tweet into an intensity level of the specified emotion E, representing the mental state of the tweeter. 0: no E can be inferred. 1: low amount of E can be inferred. 2: moderateamount of E can be inferred. 3: high amount of E can be inferred.V-regEvaluate the valence intensity of the tweeters mental state based on the tweet, assigning it a real-valued score from 0 (most negative) to 1 (most positive).V-ocCategorize the tweet into an ordinal class that best characterizes the tweeters mental state, considering various degrees of positive and negative sentiment intensity. 3: very positive mentalstate can be inferred. 2: moderately positive mental state can be inferred. 1: slightly positive mental state can be inferred. 0: neutral or mixed mental state can be inferred. -1: slightly negativemental state can be inferred. -2: moderately negative mental state can be inferred. -3: very negative mental state can be inferred.E-cCategorize the tweets emotional tone as either neutral or no emotion or identify the presence of one or more of the given emotions (anger, anticipation, disgust, fear, joy, love, optimism,pessimism, sadness, surprise, trust).V-A,V-M,V-NYT, V-TCalculate the sentiment intensity or valence score of the text, which should be a real number between -4 (extremely negative) and 4 (extremely positive). SSTCalculate the sentiment score of the text, which should be a real number between 0 (extremely negative) and 1 (extremely positive).EmobankDetermine the valence/arousal/dominance intensity of the writers mental state on a scale of 1 (most negative) to 5 (most positive).GoEmotionCategorize the texts emotional expression, classifying it as either neutral or as one or more of the specified emotions (anger, disgust, fear, joy, sadness, surprise) that reflect the writersstate of mind.SST5Classify the text into one of five classes of sentiment that best represents the mental state of the text. 0: very negative, 1: negative, 2: neutral, 3: positive, 4: very positive.TDTClassify the text into one of three classes of sentiment that best represents the mental state of the text. -1: negative, 0: neutral, 1: positive. option from the dataset provided by the authors, which consists of7 emotion labels, including the neutral.Since the first five datasets are collected from the same sources asthe AAID, the remaining data comes from different platforms andsources, we divide AEB into two parts for comparison. The formeris referred to as AEB-1, used to test the training effectiveness of themodels. The latter is called AEB-2, which is suitable for testing thegeneralization ability of models.",
  "EmoLLMs": "We build EmoLLMs by fine-tuning various LLMs based on AAID. Wetrain three EmoLLaMA models based on LLaMA2 : EmoLLaMA-7B, EmoLLaMA-chat-7B, EmoLLaMA-chat-13B by fine-tuning LLaMA2-7B, LLaMA2-chat-7B, LLaMA2-chat-13B, where LLaMA2-chat-7Band LLaMA2-chat-13B are the first open-source LLMs tuned withreinforcement learning from human feedback (RLHF) . Wealso train EmoOPT, and EmoBLOOM based on OPT-13B andBLOOM-7B ) respectively. All models are trained for threeepochs based on AdamW optimizer , utilizing early stoppingtechniques to prevent overfitting, and leveraging DeepSpeed to reduce memory usage. We set the batch size to 256. Theinitial learning rate is set to 1e-6 with a warm-up ratio of 5%, and themaximum model input length is set to 2048. All models are trainedon two Nvidia Tesla A100 GPUs, each with 80GB of memory.",
  "EVALUATION4.1Base Models": "PLMs: Sentiment analysis and emotion detection are typically re-garded as classification tasks, while intensity prediction is consid-ered a regression task. We select some commonly used PLMs asbaseline models, which can only fine-tuned on a single task, includ-ing BERT, RoBERTa, and one domain-specific pre-trained model (i.e. SentiBERT ). We add a fully connected neural layer to eachmodel, which is used for classification or regression. For EI-reg andV-reg tasks, we utilize the mean squared error (MSE) loss function.For EI-oc and V-oc tasks, we use cross-entropy loss. For multi-labeltask E-c, we adopt binary cross-entropy with logits loss.Zero-shot/few-shot methods (LLMs without fine-tuning):With the emergence of LLMs, zero-shot and few-shot learning havebecome effective approaches for solving numerous tasks. We se-lect Falcon-7b-instruct , Vicuna-13b-v1.56, LLaMA2-chat-7Band LLaMA2-chat-13B to perform zero-shot prompting on the in-struction dataset. In addition, we employ zero-shot and few-shotprompting methodologies with the closed-source LLM ChatGPT(gpt-3.5-turbo) and GPT-4 (gpt-4-1106-preview). We select at leastone piece of data for each emotion category or label category toserve as few-shot prompts.Emotion-based instruction-tuning methods: In addition tothe EmoLLMs series models, we also fine-tuned BART , T5 using the same instructional dataset as baseline models to furtherevaluate the effectiveness of our models.",
  "Evaluation Methods": "For AEB-1, we use the official evaluation metric7, Pearson corre-lation coefficient (pcc), as the evaluation metric for EI-reg, EI-oc,V-reg, and V-oc and use accuracy, micro-F1 (mi-F1), macro-F1 (ma-F1) for E-c. Additionally, the official evaluation also incorporatessecondary evaluation metrics. For the regression tasks, they alsouse pearson correlation for a subset of the test set that includesonly those tweets with intensity score greater or equal to 0.5. Forthe ordinal classification tasks, they also use pearson correlationfor a subset of the test set that includes only those tweets with",
  "Results": "4.3.1Results on AEB-1. The evaluation results on AEB-1 are shownin (The results of open-sourced models are the average offive runs). The first line is the score of the top 1 on the SemEval-2018Task1 leaderboard.Comparison between EmoLLMs and PLMs, Zero-shot/few-shot methods: presents the results on AEB-1 of severaldifferent kind of methods. For EmoLLMs, we chose EmoLLaMA-chat-13B, which shows the best overall performance, to comparewith other categories. The results in show that EmoLLaMA-chat-13B outperforms all other LLMs and surpasses the top ranking8 in the first four tasks of AEB-1. For the complex tasks EI-reg and EI-oc, EmoLLaMA-chat-13B shows high improvement compared to top1, with respective increases of 3.2% (EmoLLaMA:0.831, top1:0.799)and 6.8% (EmoLLaMA:0.763, top1:0.695). For independent raw taskfine-tuning methods, although these PLMs are trained on extensivedatasets and fine-tuned separately for each task, the results do notsurpass the original top 1 scores. The findings demonstrate thatgeneral PLMs are more prone to overlooking important informationcompared to LLMs when dealing with affective regression tasks andfine-grained sentiment classification tasks. For zero-shot/few-shotmethods, we can observe that this category of methods performspoorly compared to other fine-tuning approaches, especially in theEI-reg and EI-oc tasks. This indicates that the LLMs without fine-tuning struggle to handle the issue of emotion intensity effectively(We also test BART, T5, OPT and BLOOM in zero-shot and few-shotmethods, but their response is highly irrelevant).Comparison between EmoLLMs: We can observe from Ta-ble 5 that EmoLLMs all perform well compared with LLMs with-out fine-tuning. EmoT5 performs the best on the emotion classi-fication task E-c (ma-F1) (EmoT5:0.568, EmoLLaMA:0.545), but itdoes not perform as well as other models on regression tasks (e.g.EI-reg(ave): EmoT5:0.783, EmoLLaMA:0.831). Although EmoOPTslightly outperforms EmoLLaMA in a few regression tasks (e.g.V-reg: EmoOPT:0.887, EmoLLaMA:0.886), it still lags behind EmoL-LaMA in most tasks.In conclusion, our proposed instruction-tuning strategy for sen-timent analysis tasks outperforms PLMs and all LLMs without fine-tuning, achieving the best comprehensive performance. Comparedto other instruction-tuned EmoLLMs, EmoLLaMA demonstrates amore comprehensive and integrated capability in affective analysis. 8Seernet achieved first position in the first four tasks of SemEval-2018 Task1during the competition phase. It is based on traditional machine learning methods,which perform comprehensive data pre-processing and apply the stacking techniqueto ensemble multiple ML methods (e.g. XG Boost, Random Forest). 4.3.2Results on AEB-2. In order to evaluate the generalizabilityof EmoLLMs, we execute experiments on the AEB-2 that are notincluded in the training process (Detailed descriptions can be foundin ). All models apply zero-shot method. We compare theseries of EmoLLMs with ChatGPT, GPT4, several open-source LLMs(i.e. LLaMA2-chat, Falcon, and Vicuna) and several sentiment anal-ysis tools (i.e. VADER, TextBlob). presents the experimentresults (The results of open-sourced models are the average of fiveruns). For EmoLLMs, it is worth noting that, since we use labelsranging from 0 to 1 when fine-tuning the model on the regressiondataset, we also use the range of 0 to 1 for predictions during thegeneralization testing of regression tasks. Afterward, we map thesepredictions to the corresponding range of the data.Comparision between EmoLLMs and LLMs without fine-tuning: presents the results on AEB-2 of several differentkind of methods. We still choose EmoLLaMA as the representativefor EmoLLMs. From , we can see that EmoLLaMA seriesoutperform ChatGPT, GPT-4, and LLMs without fine-tuning inmost regression tasks. In the first four regression tasks, EmoLLaMAovertakes GPT-4 by over 10%. Although EmoLLaMA performs lesswell than ChatGPT and GPT-4 in SST and Emobank-Arousal, thedifference is less than 5%. For classification tasks, EmoLLaMA se-ries performs better than ChatGPT and GPT-4 in the TDT task.In the GoEmotion, the performance of EmoLLaMA is within a 5%difference compared to ChatGPT and GPT-4. In SST5 tasks, GPT-4performs exceptionally well (acc:0.543, ma-F1:0.504), as we can seethat ChatGPT, GPT-4, both outperform other models in SST5 andSST tasks. The possible reason is that the SST dataset is popular,and LLMs have been exposed to similar corpora during pre-training,which enables them to perform better using zero-shot methods.Comparision between EmoLLMs: shows that all in-struction tuning LLMs perform well on AEB-2 and have good trans-ferability except EmoBART and EmoT5. EmoBART and EmoT5 per-form similarly to their performance on the AEB-1 dataset, showingpoor performance in regression tasks. Interestingly, EmoLLaMA-chat-7B performs the best in most tasks of the AEB-2 and evenoutperforms EmoLLaMA-chat-13B in most regression tasks. Onepossible reason is that models with a larger number of parameterstend to overfit during fine-tuning, which can subsequently affecttheir general performance ability.It is worth noting that, in AEB-2 dataset, only TDT and V-Tweetare sourced from Twitter, while the others are collected from dif-ferent platforms and domains. Although EmoLLMs training datais only sourced from Twitter, it performs well on other platformsand domains, which demonstrates its excellent transferability. Theresults also show that the performance of the current sentimentanalysis tools (i.e. VADER, TextBlob) is significantly inferior to thatof EmoLLMs. Overall, the experiment results on AEB-2 illustrateEmoLLMs series achieves ChatGPT-level and GPT-4-level generalcapabilities (especially EmoLLaMA) and can be used as emotionannotation tools. 4.3.3Analysis of Chatgpt and GPT-4 . On the AEB-1 dataset, shows that GPT-4 and GPT-4-FS perform best in zero-shot/few-shot methods, followed by ChatGPT and ChatGPT-FS. This illus-trates the current open-sourced LLMs still have a big gap withChatGPT and GPT-4 in complex tasks (e.g. gaps between GPT-4",
  "EmoLLMs: A Series of Emotional Large Language Models and Annotation Tools for Comprehensive Affective AnalysisKDD 24, August 2529, 2024, Barcelona, Spain": "of textual content. Thus, when applied to the real world, there maybe some biases. Additionally, current EmoLLMs are limited to Eng-lish text content and lack content from other languages and modal-ities. In the future, we will introduce more datasets from differentplatforms, domains, modalities, and languages into instruction-tuning data to further enhance the capabilities of EmoLLMs. The code in this project is based on BELLE code . TheEmoLLaMA picture in was generated by PIXLR9. Thiswork is supported by the computational shared facility at the Uni-versity of Manchester and the scholar award from the Departmentof Computer Science at the University of Manchester. This work issupported by the project JPNP20006 from New Energy and Indus-trial Technology Development Organization (NEDO), the Centrefor Digital Trust and Society at the University of Manchester, andthe Manchester-Melbourne-Toronto Research Fund. Md Shad Akhtar, Asif Ekbal, and Erik Cambria. 2020. How intense are you?Predicting intensities of emotions and sentiments using stacked ensemble [appli-cation notes]. IEEE Computational Intelligence Magazine 15, 1 (2020), 6475. Hashir Ali, Ehtesham Hashmi, Sule Yayilgan Yildirim, and Sarang Shaikh. 2024.Analyzing amazon products sentiment: a comparative study of machine and deeplearning, and transformer-based techniques. Electronics 13, 7 (2024), 1305.",
  "Scores": "EmoLLaMA-chat-13BEmoLLaMA-chat-7BGPT-4ChatGPTVicuna : Comparison between EmoLLMs and LLMs without fine-tuning on AEB-2. The evaluation score for the first six tasks(regression tasks) is the pcc. The last three tasks (classification tasks) utilize the macro-F1 score. To sum up, there is still a certain gap between the current open-source LLMs and ChatGPT, GPT-4 in affective analysis tasks. Cur-rently, we can only surpass ChatGPT and GPT-4 by fine-tuning onspecific tasks.",
  "CONCLUSION": "In this paper, we propose EmoLLMs, a series of comprehensiveaffective analysis models and annotation tools. We also constructa multi-task affective analysis instruction dataset (AAID) and anaffective evaluation benchmark (AEB). We conduct a comprehensiveanalysis of the performance of EmoLLMs, as well as a variety ofLLMs on the AEB benchmark. The results indicate that EmoLLMsperform exceptionally well in both affective analysis regressiontasks and classification tasks, achieving SOTA compared to the otheropen-sourced LLMs, and EmoLLMs exhibit strong transferability, asit has achieved the generalization capabilities of ChatGPT and GPT-4 in various unseen affective analysis tasks. The results also showthat there is still a certain gap between the current open-sourcedLLMs and ChatGPT, GPT-4 in specific domains. An ideal solutionto address the issue is the instruction-tuning strategy employed in",
  "DISCUSSIONS": "Real-World Applications. EmoLLMs can provide high-qualityand multiple emotional information automatically, which can beused for various practical applications. For example, (1) Misinforma-tion detection: Rumors or fake news often convey specific emotions.Affective features can help verify misinformation . (2) Health-care (e.g. mental health): The severity of depressive symptoms isclosely related to emotions. The main reason is that individualswith depressive symptoms often struggle to regulate their emo-tions, leading to a decrease in emotional complexity. Therefore,emotional information is useful for diagnosing mental disorders. (3) Customer service (e.g. online shopping): Conducting senti-ment analysis on product reviews provides valuable insights intoproduct and service quality as well as customer experience .Limitations and Future Work. Most of the publicly availabledatasets are from the internet and social media, which have differentexpression forms, text formats, and styles compared to other types",
  "Sven Buechel and Udo Hahn. 2022. Emobank: Studying the impact of annotationperspective and representation format on dimensional emotion analysis. arXivpreprint arXiv:2205.01996 (2022)": "Jireh Yi-Le Chan, Khean Thye Bea, Steven Mun Hong Leow, Seuk Wai Phoong,and Wai Khuen Cheng. 2023. State of the art: a review of sentiment analysisbased on sequential transfer learning. Artificial Intelligence Review 56, 1 (2023),749780. Israel Cohen, Yiteng Huang, Jingdong Chen, Jacob Benesty, Jacob Benesty, Jing-dong Chen, Yiteng Huang, and Israel Cohen. 2009. Pearson correlation coefficient.Noise reduction in speech processing (2009), 14.",
  "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:Pre-training of deep bidirectional transformers for language understanding. arXivpreprint arXiv:1810.04805 (2018)": "Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, andNoah Smith. 2020. Fine-tuning pretrained language models: Weight initializations,data orders, and early stopping. arXiv preprint arXiv:2002.06305 (2020). Diwen Dong, Fuqiang Lin, Guowei Li, and Bo Liu. 2022. Sentiment-Aware FakeNews Detection on Social Media with Hypergraph Attention Networks. In 2022IEEE International Conference on Systems, Man, and Cybernetics (SMC). IEEE,21742180. Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming Zhou, and Ke Xu. 2014. Adap-tive recursive neural network for target-dependent twitter sentiment classifica-tion. In Proceedings of the 52nd annual meeting of the association for computationallinguistics (volume 2: Short papers). 4954. Venkatesh Duppada, Royal Jain, and Sushant Hiray. 2018. SeerNet at SemEval-2018 Task 1: Domain Adaptation for Affect in Tweets. In Proceedings of the 12thInternational Workshop on Semantic Evaluation. 1823.",
  "Lu He, Tingjue Yin, and Kai Zheng. 2022. They May Not Work! An evaluationof eleven sentiment analysis tools on seven social media datasets. Journal ofBiomedical Informatics 132 (2022), 104142": "Amal Htait and Leif Azzopardi. 2021. Sentiment intensity prediction using neuralword embeddings. In Proceedings of the 2021 ACM SIGIR International Conferenceon Theory of Information Retrieval. 93102. Clayton Hutto and Eric Gilbert. 2014. Vader: A parsimonious rule-based modelfor sentiment analysis of social media text. In Proceedings of the internationalAAAI conference on web and social media, Vol. 8. 216225. Clayton Hutto and Eric Gilbert. 2014. Vader: A parsimonious rule-based modelfor sentiment analysis of social media text. In Proceedings of the internationalAAAI conference on web and social media, Vol. 8. 216225. Yunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang Niu, Lei Zhang, BaochangMa, and Xiangang Li. 2023. Exploring the impact of instruction data scalingon large language models: An empirical study on real-world use cases. arXivpreprint arXiv:2303.14742 (2023). Shanglin Lei, Guanting Dong, Xiaoping Wang, Keheng Wang, and Sirui Wang.2023. Instructerc: Reforming emotion recognition in conversation with a retrievalmulti-task llms framework. arXiv preprint arXiv:2309.11911 (2023). Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, AbdelrahmanMohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoisingsequence-to-sequence pre-training for natural language generation, translation,and comprehension. arXiv preprint arXiv:1910.13461 (2019).",
  "Yukun Ma, Khanh Linh Nguyen, Frank Z Xing, and Erik Cambria. 2020. A surveyon empathetic dialogue systems. Information Fusion 64 (2020), 5070": "Saif Mohammad, Felipe Bravo-Marquez, Mohammad Salameh, and SvetlanaKiritchenko. 2018. Semeval-2018 task 1: Affect in tweets. In Proceedings of the12th international workshop on semantic evaluation. 117. Saif Mohammad and Svetlana Kiritchenko. 2018. Understanding Emotions: ADataset of Tweets to Study Interactions between Affect Categories. In Proceedingsof the Eleventh International Conference on Language Resources and Evaluation(LREC 2018). European Language Resources Association (ELRA), Miyazaki, Japan. Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru,Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,and Julien Launay. 2023. The RefinedWeb dataset for Falcon LLM: outperformingcurated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116(2023).",
  "Robert Plutchik. 1980. A general psychoevolutionary theory of emotion. InTheories of emotion. Elsevier, 333": "Syed Arbaaz Qureshi, Gael Dias, Mohammed Hasanuzzaman, and Sriparna Saha.2020. Improving depression level estimation by concurrently learning emotionintensity. IEEE Computational Intelligence Magazine 15, 3 (2020), 4759. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits oftransfer learning with a unified text-to-text transformer. The Journal of MachineLearning Research 21, 1 (2020), 54855551. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deep-speed: System optimizations enable training deep learning models with over100 billion parameters. In Proceedings of the 26th ACM SIGKDD InternationalConference on Knowledge Discovery & Data Mining. 35053506. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning,Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semanticcompositionality over a sentiment treebank. In Proceedings of the 2013 conferenceon empirical methods in natural language processing. 16311642."
}