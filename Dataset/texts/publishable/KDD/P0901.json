{
  "Abstract": "Causality is vital for understanding true cause-and-effect relation-ships between variables within predictive models, rather than rely-ing on mere correlations, making them highly relevant in the fieldof Explainable AI. In an automated decision-making scenario, wecan analyze the underlying data-generation process using causalinference methods, which enable us to explain a models decision bymanipulating features and creating counterfactual examples. Thesecounterfactuals explore hypothetical scenarios where a minimalnumber of factors are altered, providing end-users with valuable in-formation on how to change their situation. However, interpretinga set of multiple counterfactuals can be challenging for end-userswho are not used to analyze raw data records. In our work, we pro-pose a novel multiple-step pipeline that uses counterfactuals andgenerates natural language explanations of actions that will leadto a change of outcome in classifiers of tabular data using LLMs.This pipeline was thought to guide the LLM through smaller tasksthat mimic human reasoning when explaining a decision based oncounterfactual cases. We conducted various experiments using apublic dataset and proposed a method of closed-loop evaluationto assess the coherence of the final explanation with the counter-factuals as well as the quality of the content. Results show to bepromising, although further experiments with other data sets aswell as human evaluation should be carried out.The code produced for this work can be found in the followingrepository:",
  "Introduction": "Machine learning systems are becoming increasingly influential insociety, especially in critical decision-making areas such as health-care, education, and justice. However, many of these models func-tion as \"black boxes\" , where inputs are transformed into outputswithout transparency into the decision-making process, leading toissues with trust and interpretability for users. This gave birth tothe research field of XAI (explainable Artificial Intelligence), wheremultiple techniques for extracting explanations for model predic-tions have been proposed. In this work we focus on contrastiveexplanations through sets of multiple counterfactual examples ,which are variants of user data with minimal changes in order toflip the prediction.When a user is presented with a single counterfactual, extractingconclusions from it is straightforward. Nevertheless, this may notreflect the whole scope of possible actions the user can take. On theother hand, as we increase the number of counterfactuals, drawingconclusions from them becomes difficult for people who are nottrained on a data analytic thinking . The objective of this workis to evaluate the use of LLMs in generating clear, plain-languageexplanations for end users without the need for individual caseanalysis. We guide the LLM through the explanation generation Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 2024 Workshop, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-1-4503-XXXX-X/18/06",
  "Implementation": "Our work aims to bridge the gap between the ML models deci-sion and the end user understanding and accepting or contestingthat decision. To do that, we would like to generate a clear andactionable explanation for a user who receives a negative outcome,like being denied a loan, using LLMs to automatize the process.Due to the limitations of these models when tackling problemsthat require planning and multiple step reasoning , we brokedown the problem into smaller ones, mimicking human reasoning. We hope that by doing so, the LLM will be capable of correctlyidentifying the causes that led to a change of class and synthesisethem into a clear explanation which gives advice to the user onhow to change to their desired output. In we can see ascheme of the different steps taken to generate the final text.",
  "Counterfactual Generation": "One of the most natural approaches to answering a \"why\" questionis to use a contrastive explanation. Humans generate a contrastiveexplanation by comparing the actual outcome with a hypotheticalalternative scenario. The process begins with understanding theevent or decision that occurred within a causal context. Then, weconsider what could have happened differently by imagining ascenario where a key cause is altered. Finally, we focus on thespecific factors or causes that lead to a different outcome in thealternative scenario.We will simulate alternative scenarios by generating counterfac-tuals that will flip the decision of the classifier. In our case, we used the DiCE framework , which is among the most popular, how-ever, other frameworks could be used. DiCE utilizes user data andthe ML model to generate counterfactuals through perturbations,approaching the problem as an optimization task. The optimizationfocuses on maximizing validity (examples that effectively changeclass), proximity (examples that are as close as possible to the orig-inal), and diversity (examples with different causes changing tooffer various routes). Additionally, users can restrict the featuresto be changed in order to avoid some unfeasible counterfactuals(like changing race), and post-hoc processes are applied to enhancesparsity (changing the minimum number of features possible). Bydoing this, the obtained set of counterfactuals can provide the userwith diverse, feasible paths to the desired outcome, provided theyare understood properly. In an example of an individual ofthe adult dataset predicted to earn less than 50k $ and five counter-factual examples predicted to earn above that threshold are shown.",
  "Extracting causes": "After generating the counterfactual examples, our goal is to identifythe primary causes deduced from them. To achieve this, we fedthe LLM the set of counterfactual examples as well as the originaluser data, and asked it to return a list of the main reasons why theuser was classified in one way and not the other. This process canbe viewed as employing abductive reasoning to generate possiblecauses. In we can see the prompt used and the outputproduced by the LLM with the main causes that led to a change ofoutcome for the example of .",
  "Cause Evaluation and Selection": "Once a list is generated, we need to verify its correctness andidentify the most relevant causes to the explanation. To achievethis, we use a program-aided prompt to instruct the LLM togenerate Python code that counts how many times a cause wasresponsible (or partially responsible) of flipping the outcome bycounting the number of counterfactual examples that containedthe proposed change. By generating and executing this code, wecan externally confirm the LLMs accuracy and rank each of themain reasons in the generated list. Although a cause appearingin all counterfactuals does not necessarily imply it is essential, itindicates higher relevance.In order for the LLM to generate this code we feed it the originaluser data, the set of counterfactual examples, the output with themain causes of the previous step and some information about thecolumns and possible values of the dataset in order to make thecode generation more reliable. This can be seen in as well",
  ": Prompt used to extract main causes of the shift inclassification and LLM response": "User Prompt: Im providing a negative outcome from aML-system that predicts whether a person will earn more than 50k$ a year and your task is to extract the most important observedrules based on a set of counterfactual cases. User Data Negative outcome <We insert here a Python dataframe containing original user data> Positive counterfactual outcome <We insert here a Python dataframe containing the generatedcounterfactual examples> Rules <List of Rules>",
  ": Prompt used to generate code that counts how manycounterfactuals contain each cause, code produced by theLLM and results of code execution": "User Prompt: Im providing a negative outcome from aML-system, a set of counterfactual cases that flip the decision ofthe system and the main rules inferred from the counterfactuals.You should generate python code to count how many of thecounterfactuals are consistent with the rule. The code shouldcreate a df with the counterfactuals provided and then check foreach rule how many of them follow the rules. Order the rules.Finally, you should print the results. User Data Negative outcome <We insert here a Python dataframe containing original user data> Positive counterfactual outcome <We insert here a Python dataframe containing the generatedcounterfactual examples> Rules <We insert here a list with the generated rules> Dataset info The following info about the dataset is available:<We insert here public information about the dataset> Code import pandas as pd# complete this code",
  "importpandasas pd": "# Definethedataf o rtheDataFramedata ={ ' age ' : ,' workclass ' :[ ' Self Employed ' ] ,' education ' :[ ' Bachelors ' ] ,' m a r i t a l _ s t a t u s ' :[ ' Married ' ] ,' occupation ' : [ ' P r o f e s s i o n a l ' ] ,' race ' :[ ' White ' ] ,'gender ' :[ ' Male ' ] ,' hours_per_week ' : ,' income ' : }",
  "Generating explanation": "Finally, we leverage all the information that has been produced andfeed it to the LLM. We ask it to generate a final explanation in plainlanguage that emphasizes actions that can be taken by the user inorder to change to the desired class. This is were the LLM is mostuseful, since we assume that in its training it will have captured thecorrect form of human interaction, and that by crafting the correctprompt and using the previous information, the explanation willsatisfy the final user.In the prompt used and the final textual explantion gen-erated by the LLM are shown.",
  "Evaluation3.1What makes a good explanation?": "Before delving into how to evaluate the process of the LLM, itis important to introduce some concepts on what makes a goodexplanation from the social sciences perspective , which willhelp us create a better evaluation. Contrastive Explanations: People are usually not just interestedin why something happened, but why it happened instead of some-thing else. Counterfactual examples are contrastive by nature, whichmakes them a perfect starting point for building an explanation. Biased Selection in Explanations: People dont usually expect tobe presented with the whole causal chain that leads to an eventhappening. We often choose only one or two possible reasons out ofmany when explaining something, and these choices can be biased.This selective process is influenced by cognitive biases, meaningthat we might prefer certain explanations over others, not becausethey are more accurate, but because they align better with ourway of thinking. Some of these biases are selecting simpler andmore general causes, selecting more recent and abnormal events ascauses, or using necessary causes over sufficient ones. The latteris especially relevant in our work, because if a cause appears inall counterfactuals it has more chances of being necessary andtherefore we should pay attention to it. Social Dynamics of Explanations: Explanations are social, func-tioning as a knowledge transfer mechanism within interactionsor conversations. A person who receives an explanation will notbe satisfied if it is not aligned with his or hers prior beliefs and ifit is not given in the correct form. We assume that the LLM hascaptured this common knowledge in its training and that it will becapable of writing the explanation in a correct manner given theright instructions.",
  "Closed Loop Evaluation": "As it is the case for most of LLM applications, evaluating how \"good\"these explanations are is not an easy task. It would be ideal to have agroup of people manually analyzing the outputs of the LLM, but thisvery time consuming and expensive. Because of this, we decidedto make a \"closed loop\" evaluation of LLMs understanding of thecounterfactuals and causes of the change. We did so by asking the LLM to generate a counterfactual example at the end of thereasoning, using only the original case, information of the datasetand the final explanation. If the LLM is capable of generating acounterfactual example from the explanation it means that theimportant information was correctly synthesized in it. Additionally,we checked some metrics to evaluate the quality of the explanation.To obtain all of these metrics, a final prompt was used to createa table with all the information and save it in a csv file. Afterextracting and executing the code generated by the LLM we canapply external operations on the table . The different metricsevaluated are the following: Validity: The final example produced by the LLM mustbelong to the desired class. This will indicate that the ex-planation contained the necessary information in order toproduce the flip in classification. Consistency with causes: The example we generate mustbe consistent with the identified causes that were reasonedin the previous steps, so we will check how many of themit follows. As we mentioned, people prefer simpler explana-tions and only the main causes, so the ratio of causes used inthe explanation and the use of the top ranked causes couldbe seen as a proxy of explanation quality.",
  "Novelty: Since the dataset we are using is quite well knownand available on the internet, we should check that the LLMis not taking samples it has memorized from the originaldataset": "Feasibility: We would also like to check if the example givencould be applied in the real world. Since the perturbationmethod can change the selected variables freely, it can pro-duce some examples that may not be applicable in the realworld, for example, suggesting to change to a job for whichwe are not qualified without changing our education. Thisstep is optional and can only be done if one has access to thecausal graph . It will be left as future work.",
  "Experiments and Results": "As in the illustrative example used to explain the method, in ourexperiments we worked with the \"Adult\" dataset . A RandomForest classifier was trained, to predict whether an individual willearn more than 50k$ a year and we selected 100 cases in which thesystem predicted that an individual would earn less than 50k$ peryear. Explanations were generated with our proposed pipeline usingGPT-4o as the LLM and DiCE allowing all features to vary freelyto generate the counterfactuals. We evaluated the explanations withthe method method mentioned in the previous section. We variedthe size of the sets of counterfactuals between 1, 3 and 5 to observethe effects of adding more counterfactuals to the causes identified,and how the LLM selected them. Furthermore, we started by tryingtwo different prompting techniques, Zero-Shot and Few-Shot ,to see if there were improvements in the results.Since generating the counterfactuals is an optimization problemwith random initial conditions, the generated counterfactuals canbe different each time the algorithm is called. On top of this, theresponses of the LLM are also stochastic , and vary in each",
  ": Prompt used to generate an explanation of the counterfactuals and LLM response": "User prompt: A negative outcome from a ML-system was provided to several systems that explain why that case is negative analyzingcounterfactuals, generating rules and evaluating them. The results of the system are the following.System 1: Rules By generating counterfactual, we obtained the following rules:<We insert here a list with the generated rules> Results We have checked that the rules are followed by counterfactual:<We insert here code execution results> Explanation <Explanation generated by the LLM> System 2: Rules By generating counterfactual, we obtained the following rules:<We insert here a list with the generated rules> Results We have checked that the rules are followed by counterfactual:<We insert here code execution results> Explanation <Explanation generated by the LLM> System 3: Rules By generating counterfactual, we obtained the following rules:<We insert here a list with the generated rules> Results We have checked that the rules are followed by counterfactual:<We insert here code execution results> Explanation <Explanation generated by the LLM> Given this information, provide an explanation to the user in plain language so that he/she can improve their chances of changing class. Itshould be as clear as possible and call to action. Consider that the higher amount of counterfactuals that follow the rule, the more importantthat rule is.",
  "GPT-4o answer: Based on the analysis of your current situation and the observed patterns in the data, here are some clear steps you cantake to improve your chances of earning more than $50K a year:": "1. Pursue Higher Education. The most influential factor is education. Individuals with higher education levels such as Professional School,Bachelors, and Doctorate degrees are more likely to earn a higher income. Currently, you have a \"School\" level of education. Investing infurther education could significantly improve your economic prospects. Consider enrolling in higher education programs or professionalcourses to enhance your qualifications. 2. Explore Different Occupations. Another key factor is the type of occupation. Individuals in \"Professional\" roles tend to earn higher incomescompared to those in \"Blue-Collar\" jobs. If possible, seek opportunities or training that could transition you into a more professional rolewithin your field or a new industry that values your skills. 3. Consider Marital Status Impact. While this factor is less controllable, data shows that individuals who are \"Divorced\" have higher incomeseven with the same education and occupation. This might reflect a variety of socio-economic factors. While changing your marital status isnot a practical recommendation, its important to be aware of how life circumstances can impact income. 4. Evaluate Workclass Opportunities. Being \"Self-Employed\" is another observed factor that can lead to higher income compared to being inthe \"Private\" workclass. If you have entrepreneurial skills or a business idea, exploring self-employment might be a viable path to increasingyour income. 5. Hours Per Week is Less Critical. Interestingly, the total number of hours you work per week seems to be less influential compared toeducation and occupation. While working more hours can help, focusing on improving your education and transitioning to a higher-incomeoccupation will likely have a greater impact on your income level.",
  "(1) Education: Research higher education programs or professional courses that align with your career goals.(2)": "generation. We used Tree of Thought (ToT) to take advantageof this and try to create more diversity in the explanations. Weused both of the prompting techniques previously mentioned andcreated three different explanations and sets of identified causesof the change starting from different sets of 5 counterfactuals. Wethen leveraged all the information to create a final explanation and hoped that this would add diversity to the causes and lead to abetter final explanation. In Appendix B the prompts for this methodare shown.The results of the experiments are shown in .Validity was over 70% in all the experiments we did, showcasingthe method proposed is able to generate explanations that would",
  "Mean Values1 CF3 CFs5 CFs1 CF3 CFs5 CFs5 CFs": "Validity75%73%79%86%84%93%72%Causes Identified2.983.183.712.023.184.319.61Causes used78%73%77%85%64%75%44%1st Cause used91%82%90%92%74%95%70%2nd Cause used79%78%88%78%61%71%55%3rd Cause used68%64%69%67%53%71%55%In the data14%10%15%22%23%27%19% contain the necessary information to generate a counterfactual ex-ample that flips the decision of the classifier. Furthermore, we cansee for the different prompt types that by using larger sets of coun-terfactual examples the validity improves. When more examplesare given, more reasons for the given classification are found, andtherefore a larger amount of different paths to obtain the desiredoutcome can be observed. This could be a cause of the increase invalidity.The best results in terms of validity were obtained when using aZero-shot strategy. However, this does not mean that the explana-tion is better in terms of quality, and a human evaluation shouldbe made to determine this. When using Tree of Thought, largersets of causes were extracted, which is what we expected, althoughvalidity decreased with respect to the other experiments. Again,this does not mean that the quality is worse.In all the cases, the final counterfactual generated by the LLM isconsistent with the deduced causes, which indicates that they arecontained in the explanation. Furthermore, we can see that not allthe causes are applied to the example, which is what we expect ifa good explanation was given. In addition to this, we see that thecauses selected are consistent with the rank we made, and the mostrelevant cause is selected in most of the cases, having the highestpercentage of selection followed by the second and then the third.The percentage of generated examples present in the datasetis below 30% in all the experiments. This could be coincidences,since the dataset is large and there is not a lot of variability inthe features which are usually changed like education level andprofession. However, we cannot be certain about this and furtherexperiments should be carried out with other less known data sets.",
  "Conclusions": "In our work we present a method to produce automatic explana-tions of a set of counterfactuals using LLMs. We have seen that theLLM is capable of correctly extracting possible causes that will leadto a flip of the decision of a classifier by analyzing these counterfac-tual examples, select the main reasons according to their relevanceand generate a final explanation in natural language that can beinterpreted by the final user. We have also observed that increasingthe amount of counterfactuals increases the amount of times theLLM is capable of generating a counterfactual of the opposite classusing only the natural language explanation, indicating better in-formation given. Furthermore, using larger sets of counterfactuals and prompting techniques like Tree of Thought can lead to morediversity of causes in the final explanation. Overall, our methodshows to be a promising option to complement sets of counterfac-tual examples in order to make them more understandable for thefinal user. However, further experiments should be made with otherdatasets, and with human evaluators in order to assess the qualityif the natural language explanations.",
  "Barry Becker and Ronny Kohavi. 1996. Adult. UCI Machine Learning Repository.DOI:": "Clara Bove, Marie-Jeanne Lesot, Charles Albert Tijus, and Marcin Detyniecki.2023. Investigating the Intelligibility of Plural Counterfactual Examples forNon-Expert Users: an Explanation User Interface Proposition and User Study.In Proceedings of the 28th International Conference on Intelligent User Interfaces (,Sydney, NSW, Australia,) (IUI 23). Association for Computing Machinery, NewYork, NY, USA, 188203. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter,Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, BenjaminChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, IlyaSutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.In Advances in Neural Information Processing Systems, H. Larochelle, M. Ran-zato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates,Inc., 18771901. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang,Jamie Callan, and Graham Neubig. 2023. PAL: program-aided language models. InProceedings of the 40th International Conference on Machine Learning (, Honolulu,Hawaii, USA,) (ICML23). JMLR.org, Article 435, 36 pages. Yair Gat, Nitay Calderon, Amir Feder, Alexander Chapanin, Amit Sharma, andRoi Reichart. 2023. Faithful Explanations of Black-box NLP Models Using LLM-generated Counterfactuals. arXiv:2310.00603 [cs.CL] Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, FoscaGiannotti, and Dino Pedreschi. 2018. A Survey of Methods for Explaining BlackBox Models. ACM Comput. Surv. 51, 5, Article 93 (aug 2018), 42 pages. R. Thomas McCoy, Shunyu Yao, Dan Friedman, Matthew Hardy, and Thomas L.Griffiths. 2023. Embers of Autoregression: Understanding Large Language ModelsThrough the Problem They are Trained to Solve. arXiv:2309.13638 [cs.CL]",
  "Kaya Stechly, Karthik Valmeekam, and Subbarao Kambhampati. 2024. On the Self-Verification Limitations of Large Language Models on Reasoning and PlanningTasks. arXiv:2402.08115 [cs.AI]": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin. 2017. Attention is Allyou Need. In Advances in Neural Information Processing Systems, I. Guyon, U. VonLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.),Vol. 30. Curran Associates, Inc. Zilong Wang, Hao Zhang, Chun-Liang Li, Julian Martin Eisenschlos, VincentPerot, Zifeng Wang, Lesly Miculicich, Yasuhisa Fujii, Jingbo Shang, Chen-Yu Lee,and Tomas Pfister. 2024. Chain-of-Table: Evolving Tables in the Reasoning Chainfor Table Understanding. arXiv:2401.04398 [cs.CL] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, andKarthik Narasimhan. 2023. Tree of Thoughts: Deliberate Problem Solving withLarge Language Models. In Advances in Neural Information Processing Systems,A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (Eds.), Vol. 36.Curran Associates, Inc., 1180911822.",
  "AClosed Loop Evaluation Prompts and Pipeline": "In this section we will show the prompts and the different inter-mediate results obtained in order to better illustrate the processof closed loop evaluation of the natural language explanation. Wewill start by asking the LLM to generate a final counterfactual byusing only the original user data, the textual explanation in orderto know which changes it can make, and finally some informationabout the name of the columns and the possible values they cantake, so it creates an example that could belong to the dataset. This example will be created by the LLM with python code, and it willbe saved into a temporary csv file. We can see the prompt, and theoutput of the LLM in .Once the example is created by executing the code externally,we will prompt the LLM in order to compare the example generatedfrom the textual explanation to the previously identified causes andtheir importance. In this part we will feed the LLM the causes weidentified, the results of the code that ranked them according tohow many of the counterfactuals contained that cause, original userdata and information about the dataset. The result of this processwill be a csv with the list of causes indicating their importance andif they are represented in the final counterfactual. In anexample of the process is given. Finally, we will analyze the tablecreated by the LLM to extract the final metrics like in .",
  "BTree of Thought Prompts": "In this section we will explain more thoroughly how we used theTree of Thought strategy to generate explanations more diversein the causes. In we show the prompt used to generatethe explanation with a Tree of Thought strategy. We generate theexplanation and rules three times, twice using a Zero-Shot andonce using a One-shot prompt. In the prompt we can see how weleverage the three executions in order to generate a more diverseexplanation.",
  ": Prompt used to generate a final counterfactual example by using the original data, the explanation and informationabout the dataset. Code produced by the LLM and results from the execution": "User prompt: A person has been classified in the negative class of ML-system. The data is the following: User Data Negative outcome <We insert here a Python dataframe containing original user data> Explanation The following explanation was given in order to try and change the class.<We insert here the provided explanation> Dataset info The following info about the dataset is available:<We insert here public information about the dataset> Example Given this information, provide an example in the format of a pandas dataframe that would be in the positive class. Complete the code belowand note that it is very important to use the name temp_csv.csv, since later processes rely on it.import pandas as pddf = pd.DataFrame(...) #complete this linedf.to_csv(temp_csv.csv, index = False)",
  ": Prompt used to count how many of the causes are present in the final example and give the results in table format.Code produced by LLM and results": "User prompt: Im providing a negative outcome from a ML-system. A counterfactual example in the format of a single row dataframe wascreated in temp_csv from the rules that are also provided. Give some code to check the number of rules followed by the example. The resultmust be given in the format of a dataframe and saved as a csv. The dataframe must have columns Rule with the text of the rule,Importance with the number of counterfactuals follow each rule, and In explanation (1 or 0) depending if the final example follows theexplanation or not. It is very important to save the csv as evaluation.csv.- User Data Negative outcome <We insert here a Python dataframe containing data from > Rules By generating counterfactual, we obtained the following rules:<We insert here a list with the generated rules> Results We have checked that the rules are followed by counterfactual:<We insert here code execution results> Dataset info The following info about the dataset is available:<We insert here public information about the dataset> Code import pandas as pddf = pd.read_csv(temp_csv.csv)# Complete code# Save to csvdf_final.to_csv(evaluation.csv, index = False)",
  "# Read exampledf = pd . read_csv ( ' temp_csv . csv ' )": "# Definethedataf o rtheDataFramedata = {' Rule ' :[' Highereducation( Prof school ,Bachelors ,Doctorate )l e a d stohigherincome . ' ,' M a r i t a ls t a t u sbeingDivorcedcanleadtohigherincome . ' ,' Type ofworkclassbeingSelf Employed cani n f l u e n c eincome . ' ,' OccupationbeingP r o f e s s i o n a ll e a d stohigherincome . ' ,' Hours per weekl e s si n f l u e n t i a lint h i scase . '] ,' Importance ' : ,#C o u n t e r f a c t u a l sfollowingeachr u l e' Inexplanation ' :#I n i t i a lvalues ,w i l lupdatebased on theexample}",
  "# CreatetheDataFramed f _ f i n a l= pd . DataFrame ( data )": "# Checki ftheexamplef o l l o w seachr u l ei fdf [ ' education ' ] . i l o c in[ ' Prof school ' ,' Bachelors ' ,' Doctorate ' ] :d f _ f i n a l . at [ 0 ,' Inexplanation ' ]= 1i fdf [ ' m a r i t a l _ s t a t u s ' ] . i l o c ==' Divorced ' :d f _ f i n a l . at [ 1 ,' Inexplanation ' ]= 1i fdf [ ' workclass ' ] . i l o c ==' Self Employed ' :d f _ f i n a l . at [ 2 ,' Inexplanation ' ]= 1i fdf [ ' occupation ' ] . i l o c ==' P r o f e s s i o n a l ' :d f _ f i n a l . at [ 3 ,' Inexplanation ' ]= 1i fdf [ ' hours_per_week ' ] . i l o c < 4 0 :# Assumptionbased on theprovidedexampled f _ f i n a l . at [ 4 ,' Inexplanation ' ]= 1"
}