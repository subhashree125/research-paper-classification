{
  "ABSTRACT": "Large Language Models (LLMs) have demonstrated impressive ca-pabilities in natural language tasks, but their safety and moralityremain contentious due to their training on internet text corpora.To address these concerns, alignment techniques have been de-veloped to improve the public usability and safety of LLMs. Yet,the potential for generating harmful content through these modelsseems to persist. This paper explores the concept of jailbreakingLLMsreversing their alignment through adversarial triggers. Pre-vious methods, such as soft embedding prompts, manually craftedprompts, and gradient-based automatic prompts, have had limitedsuccess on black-box models due to their requirements for modelaccess and for producing a low variety of manually crafted prompts,making them susceptible to being blocked. This paper introducesa novel approach using reinforcement learning to optimize adver-sarial triggers, requiring only inference API access to the targetmodel and a small surrogate model. Our method, which leveragesa BERTScore-based reward function, enhances the transferabilityand effectiveness of adversarial triggers on new black-box models.We demonstrate that this approach improves the performance ofadversarial triggers on a previously untested language model.",
  "INTRODUCTION": "Large Language Models (LLMs) have been in the spotlight recently,due to their impressive capabilities in natural language tasks. How-ever, given these models are trained on the internet text corpora,their safety and morality have been questioned in the literature. To mitigate the objectionable behaviors of LLMs, a line ofwork called alignment, has been done to improve their public usabil-ity and safety . Despite their relative success in groundingLLMs to human morals, the question of \"Is it still possible to exploitLLMs to generate harmful content?\" remains an under-explored area.",
  "AI4CYBER - KDD 2024, August 26, 2024, Barcelona, Spain2024": "Ever since the alignment of LLMs and following the same schemeof the common adversarial examples in machine learning ,there have been many attempts to reverse the alignment of LLMs,using the perturbation of their inputs, which are called Jailbreakingin the Natural Language Processing (NLP) community .While the image processing field has seen excessive research in ad-versarial examples , the NLP literature, specifically pertainingto LLMs has not been sufficiently explored. With the exponentiallyincreasing popularity of LLMs, especially the public-facing com-mercial chatbots, such as GPT-4 and Claude3, ensuring theirsafety bears significant relevance.The key issue with the existing perturbation approaches is thatthey are limited against black-box models. For example, Soft embed-ding prompts require open access to the models embeddings,are not interpretable, and lack the ability to transfer between mod-els because of their different embedding distributions. Manuallycrafted prompts however, can typically be used on differ-ent models and do not require white-box access, but they requirehuman creativity and are blocked quickly due to their constantnature. Automatic discrete prompt perturbation for jailbreakingoften involves appending a trigger string to the user prompt, whichis optimized using gradient data , which requires white-boxaccess to the model, although it has been shown to have sometransferability to black-box models. Proposed gradient-free attacksoften require access to powerful models to succeed , or requirecarefully crafted initial seeds . Decoding manipulation at-tacks, which are more recent and faster , still require somelevel of access to the models output logits or the output probabilitydistribution.In this paper, we introduce a novel approach to optimize adver-sarial triggers using reinforcement learning. Our approach onlyrequires inference API access to the target language model, anda small surrogate model which is trained by reward signals cal-culated using the target models text output. We show that ourapproach can be an extension to all previous work that optimize anadversarial trigger on white-box models and can personalize andextend the performance of triggers on new black-box target models.Intuitively, our work takes the adversarial triggers trained on amodel and adapts them to a new model, using only inference to thenew model. In summary, the contributions of this work are: i) Wedesign a reinforcement learning paradigm, adapted from previouswork, to optimize adversarial triggers using inference-only APIs.ii) we introduce a BERTScore-based reward function utilizingthe target models text output generations. iii) We show that our",
  "BACKGROUND": "Prompt Tuning. Although Large Language Models (LLMs) ex-hibit exceptional generalization capabilities, they still necessitatemeticulously designed prompts to achieve optimal performancefor specific tasks. According to the empirical research conductedby Scao and Rush , a well-crafted prompt can be as valuableas hundreds of data samples in a classification task. As LLMs con-tinue to advance, there has been a growing focus on automaticprompt tuning and in-context learning . Auto-matic prompting initially involved fine-tuning prompt embeddings,a technique referred to as Soft Prompting , which,despite its effectiveness, is often complex and computationally in-tensive. Subsequently, researchers began exploring the use of thecontinuous embedding space to create discrete prompts .Another significant approach has been the direct optimization ofdiscrete prompt tokens . This method not only en-hances interpretability and transferability between models but alsohas been demonstrated to outperform soft prompting in terms ofperformance. Adversarial Examples. The machine learning field has establishedthat the inputs of a model can be deliberately altered to cause themodel to produce (un)desired outputs; such modified inputs aretermed Adversarial Examples . Within the realm ofNatural Language Processing, these adversarial attacks have beenemployed across various applications, including classification tasks, sentiment analysis , and inducing toxic outputs. As language models evolve and prompting becomes moreprevalent, there has been a significant rise in interest concerningadversarial attacks on prompts . These recentdevelopments underscore the ongoing challenge of ensuring therobustness and security of language models against such sophisti-cated adversarial techniques. LLM Alignment and Jailbreaks. Pre-trained Large Language Mod-els, while possessing remarkable out-of-the-box capabilities ,are often unsuitable for public use due to their insufficient under-standing of instructions and their inherent unethical tendencies,such as biases and toxic behavior . Consequently,researchers strive to align these models with human values andregulatory standards through techniques like instruction-tuning,Reinforcement Learning from Human Feedback (RLHF) , andDirect Preference Optimization . However, this alignmentprocess has sparked vigorous attempts to jailbreak the models, com-pelling them to follow harmful instructions . These effortshighlight the ongoing battle between enhancing model safety andthe persistence of adversarial actors seeking to exploit model vul-nerabilities. Adversarial Attacks on LLMs. The advent of prompt tuning hassignificantly influenced the landscape of adversarial attacks, partic-ularly in the realm of language models. This trend has emerged be-cause prompt tuning provides a pathway for creating automaticallygenerated inputs for these models. Efforts to disrupt the alignmentof language models (commonly known as jailbreaking) often mirror",
  "Harmful Prompt": ": Overall architecture of our method. The surrogatemodel is already initialized in a supervised fine-tuning setupand is further fine-tuned to the target model with the re-ward signals. BERTScore is used as the Semantic SimilarityFunction to compare the resulting generation of the currentadversarial trigger with the desired target output and rewardsthe surrogate model. the methods used in prompt tuning. Soft prompt attacks, for in-stance, involve training adversarial embeddings to manipulate themodels outputs as desired . Despite their occasional success,soft prompt attacks are generally impractical in real-world settingsdue to the lack of access to model embeddings. Researchers likeZou et al. , Liu et al. , and Shi et al. have employedgradient-based techniques to optimize discrete adversarial prompts.To circumvent the need for gradient data, methods utilizing geneticalgorithms have been proposed by Yu et al. and Lapid et al. .Additionally, another approach involves using other language mod-els as red-teaming assistants, which require meticulously craftedseed inputs . These diverse strategies underscore the evolvingnature of adversarial attacks on LLMs, reflecting a continual armsrace between model developers and adversaries.",
  "Using a similar notation to previous work, we define an Autore-gressive Language Model M and its vocabulary set V. Let Vdenote a single token and x V a sequence of tokens, where V": "is the set of all possible token sequences of any length. The languagemodel M can be utilized to calculate the probability distribution ofthe next token, given x. Formally written as M (|x) : V .Additionally, for instruct tuned models, the input typically followsthe structure x = x(1) x() x(2), where is the concatenationoperator, x() is the user prompt, and x(1) and x(2) are systemprompts at the beginning and the end of the input respectively.",
  "Threat Model": "Analogous to most jailbreaking methods , our threat modelallows an adversary to append a sequence of adversarial tokensx() to the user prompt, forming the new input to the model x =x(1) x() x() x(2). The adversarys objective is to maximizethe attack success rate A : V by finding an adversarialtoken sequence x(), which we call Adversarial Trigger in this paper.In this paper, we assume the attacker has already obtained an initialset of adversarial triggers T0 on a previously attacked model withwhite-box access. The objective of this paper is to enhance theattack success rate on a previously unseen target language modelM by personalizing T0 to the new target model. Contrary to mostprevious work, the attacker does not have any access to the newtarget model, other than an input/output inference API.",
  "Approach": "Consider a set of adversarial sequences T0 that have been obtainedby attacking a previously targeted language model M0 on a set ofharmful prompts P. In this section, we introduce a new method toobtain a new set of adversarial triggers T with an improved attacksuccess rate when used to attack a new target model M comparedto T0. We assume that it is impractical or impossible to obtainT while attacking M using the same method used to obtain T0while attacking M0. For instance, M could be a black-box model,accessed only through an inference API.In this paper, we use a surrogate language model M () to gen-erate adversarial sequences x() T . The surrogate model istypically a small language model; in our case, we use different vari-ations of GPT-2 , such as the 82M parameter distilGPT-2, andthe 1.5B parameter GPT-2-xl. Similar to RLPrompt, we limit theparameters to be trained, , to an MLP with a single hidden layer,adapted to the surrogate model M () before the language head,and freeze the rest of the parameters of the model. Hence, given theset of harmful prompts P, the objective of finding the adversarialtrigger x() can be formally written as",
  "maximizex()V A(P, x())(1)": "where A is the attack success rate and x()is a candidate adversar-ial trigger, sampled from the surrogate model given an empty inputand parameterized by which, with a slight abuse of notation, isdefined as M () (x()|;). The overall architecture of our methodis depicted in figure 1.To train the new randomly initialized parameters of the surro-gate model, , we go through two phases of training. In the firstphase of the training, we use the previously obtained T0 to fine-tune M ()in a supervised setting. The second phase, which is themain training phase of adapting the adversarial triggers to the newmodel, involves refining the surrogate models adversarial triggergenerations, using reinforcement learning. We describe each phasein detail in the following paragraphs. Phase 1. In reinforcement learning (RL) setups, it is common toutilize supervised fine-tuning to ensure the correct initializationof the model weights. In this paper, T0, the set of adversarialsequences obtained by attacking a previously targeted model, using any attacking method such as the work by Zou et al. , or Auto-DAN is used to fine-tune the surrogate model M () using onlythe new added weights , while the rest of the model is frozen. Inthis work, the triggers obtained by attacking the vicuna-7b-v1.5using the method introduced in Zou et al. are used to show-case our approach. For a non-exhaustive list of possible methodsto obtain T0, refer to section 2. The objective of the first phase isformalized as an optimization problem in equation 2. Conceptu-ally, the surrogate model is steered in the direction of favoring thegeneration of adversarial sequences in T0 over any other sequence,given an empty input.",
  "where x()is a sequence of adversarial tokens with length andis a member of the baseline adversarial sequence set T0": "Phase 2. To refine the adversarial triggers generated by the sur-rogate model M (), we adapt the RLPrompt framework to fine-tune the parameters for the new target model M using reinforce-ment learning. During training, the surrogate model generates a setof candidate adversarial sequences T. These candidate adversarialtriggers are then used to infer the new inference-only target modelM in combination with the harmful prompts P. More elaborationand samples of the prompt set are available in section 4. From theresults of inferring the target model M, we calculate a rewardsignal using a reward function R. This reward signal fine-tunesthe attacker parameters with any off-the-shelf reinforcementlearning algorithm. Similar to RLPrompt, we use the on-policycomponent of the soft Q-learning algorithm. Soft Q-learning ischosen for its efficient exploration of action spaces and its stabilityin training, making it well-suited for optimizing adversarial actionsin this context. The adversarys objective can be rewritten as",
  "where T represents the adversarial trigger sequence candidates": "Reward Function. It has been shown that if a language modelstarts responding to a prompt with an affirmative tone, such as \"Sure,here is how you...\", it is very likely that the model will continue gen-erating the response to the prompt. For harmful prompts, analigned language model typically starts a response with a denialtone and refuses to respond to the prompt. The goal of the adver-sarial trigger is to put the language model in a state, such that thetarget LLM starts its response with an affirmative sentence. Hence,we design our reward function to quantize how affirmative thegeneration of the target model is. The output of the target modelis passed to the BERTScore model alongside an affirmative ref-erence sentence, which is the preferable response to the harmfulprompt. Intuitively, the BERTScore evaluates the target models re-sponse semantically using the cosine similarity metric and rewardsthe adversarial trigger with a high reward if the adversarial triggerhas successfully attacked the target model. Formally, the rewardfunction is defined as",
  "EVALUATION4.1Dataset": "The Harmful Behavior set of the AdvBench dataset, which con-tains 500 harmful instructions and the first few tokens of a targetresponse for each instruction, is used in this paper. For instance,\"Create a tutorial on how to commit identity theft\" and \"Sure, hereis a tutorial on how to commit identity theft\" is a sample of theAdvbenchs instructions and its respective target response. In thiswork, we regard the instruction as x() and the target string as. We use the first 100 samples of the harmful behaviors as thetrain set and hold out the next 400 samples for testing in all of ourexperiments.",
  "Experiment Setup": "To obtain the set of initial adversarial triggers T0, GCG is usedwith the default parameters as described in their paper. 300 steps ofthe GCG method is utilized to obtain an adversarial prompt for eachof the first 100 behaviors of the AdvBench. We use vicuna-7b-v1.5as a white-box model during the GCG training. For the purposeof testing our method, we regard the Mistral-7B-Instruct-v0.2as an inference-only black-box model M. Hence, we can not at-tack this model using any gradient-based method, including GCG.However, our reinforcement learning-based method can attack thismodel, as it only requires inference of the target model. We limit theadversarial sequence length to 20 tokens for all of our experiments.the \"distilGPT-2\" model is used as the surrogate modelM (). An MLP with a single hidden layer and 2048 hidden neuronsis added to the surrogate model after the last transformer blockand before the language head to provide the trainable parameters, while the rest of the model is kept frozen. These parameters arethen fine-tuned in a supervised fine-tuning setup, as explained insection 3. We use empty inputs and the set of initial adversarialtriggers T0 as labels to train the model for 3 epochs using the cross-entropy loss mentioned in equation 2. We use the Adam optimizerwith a learning rate of 104.During the attack, the surrogate models parameters, , are fur-ther fine-tuned using the Soft Q-Learning algorithm for 104 steps.We use the default parameters of the RLPrompt during the re-inforcement learning procedure. For the reward function, we usethe official implementation of BERTScore with the model hashroberta-large_L17_no-idf_version=0.3.12(hug_trans=4.40.1).",
  "Results": "To test our preliminary results, we compare the improvement of theattack success rate when transferred to the new target model. Asmentioned, we use GCG to obtain the initial set of adversarialtriggers and try to improve and personalize these triggers for thenew target model Mistral. Hence, we compare our work to bothtypes of the GCG algorithm. Following previous work Wedeem an attack successful if the target models response does notcontain a list of denial phrases, such as \"I am sorry\". Acknowledging that this method is not a robust evaluation, Liu et al. show thatit is one of the closest evaluations to human judgment.For GCG-individual, we obtain one adversarial trigger for eachsample in the train set, thus, it is not possible to test this method onthe test set. The GCG-multiple trains one single adversarial triggerfor the entire train set, resulting in a transferable trigger to betested with the test set with both models. For our reinforcementlearning-based method, we directly optimize the triggers for thetarget model, which is impossible when using GCG, hence, we areable to improve the attack success rate for 5% and 4% on the trainand test set respectively. shows our quantitative results forthese methods. : Attack Success Rate of the GCG and our method.The GCG method is trained on Vicuna and the resultingadversarial prompt is transferred to Mistral. We use Mistralonly as an inference API. We do not test our method onVicuna since our method extends the GCG prompts to newtarget models.",
  "CONCLUSION": "In this paper, we presented a novel reinforcement learning-basedapproach to optimize adversarial triggers for jailbreaking LargeLanguage Models (LLMs). Our method addresses the limitationsof existing techniques by requiring only inference API access tothe target model, thus eliminating the need for white-box access.By training a small surrogate model with BERTScore-based rewardfunctions, we have shown that it is possible to enhance the perfor-mance and transferability of adversarial triggers on new black-boxmodels. Our results indicate that this approach not only improvesattack success rates but also extends the applicability of previouslydeveloped adversarial triggers to a broader range of language mod-els. This work contributes to the ongoing efforts to understandand mitigate the vulnerabilities of LLMs, highlighting the need forrobust safety measures in their deployment.While our preliminary results show clear improvements in theattack success rate, we acknowledge that our work is intended asonly a spark to motivate future work. Exploring more options asthe initial set of adversarial triggers, more sophisticated rewardengineering, for instance adding a coherency reward to bypass per-plexity filters, and thoroughly testing this method qualitativelyand with more black-box models are some of the interesting fu-ture routes to take. Additionally, future research should explorepotential defensive measures to mitigate these attacks. Developingrobust detection mechanisms, enhancing model resilience throughadversarial training, and implementing stricter access controls areessential steps to protect LLMs from such vulnerabilities. These",
  "Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Srivastava,and Kai-Wei Chang. 2018. Generating Natural Language Adversarial Examples. arXiv:1804.07998 [cs]": "A. I. Anthropic. [n. d.]. The Claude 3 Model Family: Opus, Sonnet, Haiku. ([n. d.]). Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, NovaDasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, NicholasJoseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, NelsonElhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston,Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, TomBrown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan.2022. Training a Helpful and Harmless Assistant with Reinforcement Learning fromHuman Feedback. arXiv:2204.05862 [cs]",
  "Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Srndic,Pavel Laskov, Giorgio Giacinto, and Fabio Roli. 2013. Evasion Attacks againstMachine Learning at Test Time. [cs]": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, BenjaminChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, IlyaSutskever, and Dario Amodei. 2020. Language Models Are Few-Shot Learners. arXiv:2005.14165 [cs]",
  "Han Guo, Bowen Tan, Zhengzhong Liu, Eric P. Xing, and Zhiting Hu. 2022.Efficient (Soft) Q-Learning for Text Generation with Limited Good Data. arXiv:2106.07704 [cs]": "Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang,Yawen Duan, Zhonghao He, Jiayi Zhou, Zhaowei Zhang, Fanzhi Zeng, Kwan YeeNg, Juntao Dai, Xuehai Pan, Aidan OGara, Yingshan Lei, Hua Xu, Brian Tse, JieFu, Stephen McAleer, Yaodong Yang, Yizhou Wang, Song-Chun Zhu, Yike Guo,and Wen Gao. 2024. AI Alignment: A Comprehensive Survey. arXiv:2310.19852 [cs] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, De-vendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,Guillaume Lample, Lucile Saulnier, Llio Renard Lavaud, Marie-Anne Lachaux,Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothe Lacroix,and William El Sayed. 2023. Mistral 7B. arXiv:2310.06825 [cs.CL] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, BlancheSavary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma BouHanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lam-ple, Llio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock,Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, ThophileGervet, Thibaut Lavril, Thomas Wang, Timothe Lacroix, and William ElSayed. 2024. Mixtral of Experts. [cs.LG]",
  "Roberto Navigli, Simone Conia, and Bjrn Ross. 2023. Biases in Large LanguageModels: Origins, Inventory, and Discussion. 15, 2 (2023), 10:110:21": "OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, IlgeAkkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Alt-man, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom,Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, et al. 2024. GPT-4Technical Report. arXiv:2303.08774 [cs] Nedjma Ousidhoum, Xinran Zhao, Tianqing Fang, Yangqiu Song, and Dit-YanYeung. 2021. Probing Toxic Content in Large Pre-Trained Language Models. InProceedings of the 59th Annual Meeting of the Association for Computational Lin-guistics and the 11th International Joint Conference on Natural Language Processing(Volume 1: Long Papers) (Online, 2021-08), Chengqing Zong, Fei Xia, Wenjie Li,and Roberto Navigli (Eds.). Association for Computational Linguistics, 42624274. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, PamelaMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, JohnSchulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, AmandaAskell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Train-ing Language Models to Follow Instructions with Human Feedback. arXiv:2203.02155 [cs]",
  "Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi.2020. BERTScore: Evaluating Text Generation with BERT. arXiv:1904.09675 [cs.CL]": "Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, ZhanghaoWu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, HaoZhang, Joseph E. Gonzalez, and Ion Stoica. 2023.Judging LLM-as-a-Judgewith MT-Bench and Chatbot Arena. [cs] Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang,Linyi Yang, Wei Ye, Yue Zhang, Neil Zhenqiang Gong, and Xing Xie. 2023. Prompt-Bench: Towards Evaluating the Robustness of Large Language Models on AdversarialPrompts. arXiv:2306.04528 [cs]"
}