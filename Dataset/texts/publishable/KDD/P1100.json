{
  "Abstract": "In this paper, we tackle the complex task of analyzing televiseddebates, with a focus on a prime time news debate show from India.Previous methods, which often relied solely on text, fall short incapturing the multimodal essence of these debates . To addressthis gap, we introduce a comprehensive automated toolkit thatemploys advanced computer vision and speech-to-text techniquesfor large-scale multimedia analysis. Utilizing state-of-the-art com-puter vision algorithms and speech-to-text methods, we transcribe,diarize, and analyze thousands of YouTube videos of a prime-timetelevision debate show in India. These debates are a central part ofIndian media but have been criticized for compromised journalisticintegrity and excessive dramatization . Our toolkit providesconcrete metrics to assess bias and incivility, capturing a compre-hensive multimedia perspective that includes text, audio utterances,and video frames. Our findings reveal significant biases in topicselection and panelist representation, along with alarming levelsof incivility. This work offers a scalable, automated approach forfuture research in multimedia analysis, with profound implicationsfor the quality of public discourse and democratic debate. To cat-alyze further research in this area, we also release the code, datasetcollected and supplemental pdf1.",
  "Equal contribution.Corresponding author1": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08",
  "Multimodal analysis; video analysis; television; Bias detection; In-civil speech": "ACM Reference Format:Anmol Agarwal, Pratyush Priyadarshi, Shiven Sinha, Shrey Gupta, HitkulJangra, Ponnurangam Kumaraguru, and Kiran Garimella. 2024. TelevisionDiscourse Decoded: Comprehensive Multimodal Analytics at Scale. In Pro-ceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery andData Mining (KDD 24), August 2529, 2024, Barcelona, Spain. ACM, NewYork, NY, USA, 12 pages.",
  "Introduction": "Television debates are a cornerstone of public discourse, servingas platforms for the exchange of ideas and viewpoints. In India,prime-time debates are viewed by millions and have a substantialimpact on shaping public opinion . However, these debates haverecently undergone scrutiny for compromised journalistic integrityand increasing incivility . Understanding the nuances at scalein these debates is important, yet a formidable task due to themultimedia nature of the content, which blends text, audio, & video.Automated methods to analyze such content have largely beenabsent or inadequate, often focusing only on textual aspects .These naive approaches are insufficient for two reasons: the sheerscale of televised debates available for analysis, and the intricatemultimedia elements that must be considered to provide a completepicture. Previous attempts at solving this problem either employtext-based analytics that miss out on contextual cues or rely onsmall-scale, manual coding that lacks scalability .One of the most intriguing yet challenging aspects of analyzingnews debates lies in their multimodal nature, which combines text,",
  "KDD 24, August 2529, 2024, Barcelona, SpainAnmol Agarwal et al": "spoken, who is on-screen, and the tone of voice used. Overall, ourresearch builds on recent advancements in various domains ofAI. We leverage state-of-the-art models in image processing fortasks such as face and gender recognition, utilize speech processingalgorithms to identify instances of shouting, and employ speech-to-text models to capture the spoken content. We aim to provide aholistic, multi-modal analysis that can serve as a robust foundationfor future studies in video analytics.",
  "Background and Related Work2.1Bias and Incivility in Indian media": "India, the worlds largest democracy, has recently experienced adecline in press freedom, currently ranking 161 out of 180 countriesas per Reporters Without Borders . This decline has been partlyattributed to the acquisition of media outlets by influential figureswho maintain close ties with political leaders. Such ownershipstructures have led to seemingly evident biases in media reporting,with a majority of TV channels noticeably supporting the politicalparty in power. Given the critical role of media in a democraticsetup, it becomes imperative to analyze and quantify this bias, atask that some previous work has approached qualitatively.Since the Channels inception, it has been the most-watchedEnglish news channel in India, commanding an average viewershipof 40% . Known for its sensationalist approach to news reporting,the Channel has often been criticized for displaying a pro-Hindu,pro-nationalist, and pro-government bias . One of the channelspopular prime time debate show, henceforth, referred as the Showepitomizes this tendency. The show attracts over five million dailyviewers and is characterized by its nationalistic tone. It often targetsthose who appear to oppose the governments viewpoint.Despite its status as the most-watched news TV show in India,the program has moved away from the traditional format of a bal-anced news debate. Instead, it now often features a heightenedlevel of dramatization, impassioned language, and overlapping di-alogue . This sensational approach appears to resonate withviewers .While there is a substantial body of qualitative work addressingbias, factual inaccuracies, and the dramatization of news in Indianmedia , our research contributes by offering quantitativeevidence. Notably, some channels, including the Channel, haveeven acknowledged their tendencies to sensationalize news. Ourstudy enriches this dialogue by supplying empirical data on thenature and framing of the content presented in such debate shows.",
  "models": ": Pipeline overview: Branch (a) details the process for identifying gender from facial data in videos and extracting hash-tags from debate screens; Branch (b) outlines the audio cleaning and speaker diarization procedures, followed by transcriptionof utterances into text; Branch (c) illustrates the semi-automated annotation system that leverages YouTube metadata & LLMsto streamline the categorization of videos into categories, thereby reducing human annotation workload.",
  "Analysis of TV News and Media": "In the realm of analysis of TV news and media, multiple avenues ofresearch have emerged that address the intricate problem of mediabias, the influence of media on public perception, and the roleof technological platforms in shaping or amplifying these biases.One stream of work delves into detecting subtle biases in onlinenews by examining gatekeeping, coverage, and statement bias,using unsupervised methods on a geographically diverse set ofnews sources . This line of research intersects with another thatundertakes a comparative framing analysis of terrorism coverage inUS and UK newspapers, revealing differing national focuses, eithermilitaristic or diplomatic, that guide news stories framing .While these studies examine traditional media forms, a morerecent shift towards social media as a news outlet is apparent in theresearch literature. For example, some researchers employ scalablemethodologies that leverage social medias advertiser interfacesto infer the ideological slant of thousands of news outlets. Thismethod provides granularity, capturing demographic biases thatgo beyond political leanings, and results in deployable systems fortransparency . This complements work on newspaper endorse-ments influence on voting behavior, highlighting source credibilityas a key factor in endorsement effectiveness.Interestingly, research has also been conducted in the Indian con-text, where media bias in policy coverage has been systematicallyquantified. This work reveals biases in topic selection and represen-tation of different social classes and political parties. Notably, socialmedia platforms seem to echo rather than mitigate these biases, aninsight that aligns with the earlier observations on the role of so-cial media in amplifying traditional media biases . Collectively,these studies illuminate the evolving landscape of news and media analysis, showcasing the need for comprehensive, multifaceted ap-proaches. They underline the significance of understanding boththe subtleties in traditional media framing and the influential roleof social media platforms.",
  "Multimodal Analysis Tools": "Video analysis has become an increasingly significant area of re-search, particularly as social media platforms transition towardsvideo-centric content. The rise of short video services like Tik-Tok underscores the growing importance of video in the digitalage. Advances in computer vision technology have reached a stagewhere real-world applications are not just feasible but increas-ingly sophisticated. Problems such as video summarization and keyframe extraction have been addressed, offering novel solutions andmethodologies .Earlier works faced challenges in transcribing large volumesof audio data284,000 hours of radiodue to the limitations in tran-scription models at the time. The current models for transcriptionhave improved considerably showcasing a rapid evolution of thefield. Videos present a complex interplay of multiple modalities,including visuals, text, and audio. While each can be analyzed in-dependently, their true power lies in how they interact. Renoustet al. explored this by using deep neural networks for facedetection and text counting metrics to measure politicians screentime. Their work demonstrated the capability of AI techniques inanalyzing large video datasets, offering insights into complex socialdynamics. The GDELT Project provides web-based interfacesfor analyzing caption text and other on-screen elements but lacksin-depth labelling related to voice tone or content being discussed.Our work fills these gaps by analyzing a comparable datasetof videos and enriches it by labelling content related to what is",
  "Data Collection & Processing": "Our primary dataset comprises 2,087 hours of debate footage from3,000 videos. Initially, we used the YouTube Data API2 to extractmetadata from the official playlist of the the Show as of December2022. This provided us with 3,151 unique videos dating back toMay 2017. Out of these, we filtered out 67 videos because theywere too short/long (i.e. their duration was less than 10 minutes orexceeded 4 hours) and filtered out an additional 84 videos becausethe annotators couldnt agree on their categories. We were finallyleft with 3,000 videos corresponding to over 2,087 hours of videocontent. The metadata fetched using the YouTube Data API for eachvideo contains the title, URL, description, and a list of tags chosenby the channel3 associated with the video.",
  "Categorizing the Videos": "To categorize the 3,000 videos in our dataset, we manually cre-ated categories. Initially, using a framework from a prior study weadopted 18 categories . Each coder independently assessed asubset of videos, relying on metadata such as titles, descriptions,hashtags, and tags for initial categorization. If a video did not fitinto the existing categories, a new category was proposed and dis-cussed among coders for potential inclusion. This iterative processcontinued until a consensus was reached on the categories.Recognizing that a video could span multiple topics, we imple-mented a two-tiered coding system comprising major and minorcategories. Each video was assigned to one major category (e.g.,sports, religion, international affairs) while potentially belongingto multiple minor ones, allowing for emergent sub-themes (e.g.,Russia-Ukraine crisis, SSR case). To automate video categoriza-tion, we used the \"tags\" present in the YouTube metadata. The tagswere then mapped to categories. For example, a video with tag \"bud-get 2019\" was labelled Economy as the major category. However,after this initial categorization, we were left with 830 videos thatcould not be mapped to a category due to the absence of tags orthe presence of generic tags. We then used GPT-4 to map theseremaining videos to the categories based on the videos title.The final step involved human refinement to correct any poten-tial errors from the automated labelling. Two annotators indepen-dently examined and refined the labels, and their agreement wasmeasured using the Fleiss kappa statistic, which was computed tobe 0.933, indicating excellent agreement. By incorporating LLMsand tags-metadata we reduced the number of categories that can bemapped to a video to a smaller subset thereby significantly reducingthe time taken in the human annotation. This hybrid approach ofautomated and human annotation in our pipeline allowed for an",
  "#snippet.tags[]": "efficient and comprehensive categorization of the videos.In a minority of the cases with disagreements (110 cases), both theannotators discussed among themselves and resolved most of thedisagreements. There was no clear agreement on 84 videos whichwere removed from further analysis, leaving us with 3, 000 videos.A complete breakdown of major and minor categories is availablein Appendix D. The majority of the videos fall into five dominantcategories: Politics, Religion, COVID Lockdowns, International Af-fairs, and Crime & Justice, collectively accounting for 66% of thetotal dataset. A mapping from these categories to their respectivetags and examples of the annotation process can be found in thesupplemental pdf. Our semi-automated pipeline has been illustratedin branch (c) of .",
  "Transcription and Speaker Diarization": "To analyze the content of the debates, it was essential to determineboth what was said and who said it. Audio transcription convertsspeech in an audio file into written text, but debates involve multiplespeakers in multi-turn interactions. Therefore, before transcription,we performed speaker diarizationa process that partitions anaudio stream into segments and attributes them to specific speak-ers . This allowed us to transcribe individual speaker segments,resulting in a conversation-format transcription for each video.We executed two key pre-processing steps to enhance the qualityof the diarization results. First, we removed segments devoid ofspeech, such as interstitials and speaker transitions, using the VoiceActivity Detection feature from the Pyannote toolkit . Thisremoval improved subsequent diarization accuracy. Second, wefiltered out overlapping speech segments to avoid performancedegradation in speaker clustering during diarization, accomplishedusing the same Pyannote model .After these pre-processing steps, we employed the Pyannotediarization module to partition the audio into homogeneous seg-ments, each assigned to a specific speaker . For transcription,we leveraged OpenAIs Whisper speech-to-text model , notablefor its robust performance on diverse accents and technical lan-guage. Whisper has demonstrated near-human-level accuracy inchallenging noisy settings . Combining Whispers transcrip-tion capabilities with Pyannotes audio segmentation and speakerdiarization enabled us to transcribe and accurately attribute speech(and the corresponding transcribed text) to individual speakers.Our qualitative analysis revealed certain limitations in the Pyan-note models overlap detection. Specifically, the model only con-sidered speech overlapping if all audio segments were incoherent.If one speakers voice dominated others, the model did not recog-nize the speech as overlapping. This issue resulted in scenarioswhere multiple speakers are active, but not identified as overlap-ping. Additionally, the transcription quality for overlapped speechwas suboptimal, likely because Whispers training data primar-ily focuses on transcribing a single speaker while treating othervoices as background noise.4 Due to these overlap detection limita-tions, we encountered spurious speakersartifacts that appearedto be individual speakers but were actually combinations of multi-ple voices. Such spurious speakers also emerged when the debate",
  "Face and Gender Detection": "Gender identification from video frames, as shown in Branch (a) of, entailed extracting and analyzing facial data. For facialrecognition in our study, we employed the DeepFace library ,specifically utilizing the RetinaFace detector coupled with the VGG-Face model . From a given video, we sampled one frame every3 seconds and extracted all faces from it. One challenge we en-countered was the presence of spurious faces, in advertisements orimages unrelated to the debate. To address this, we implementeda filtering mechanism based on the size of the face in the frameand the confidence scores provided by the model. Its importantto acknowledge that our study operates within the limitation ofrecognizing gender in binary terms, although we recognize thatgender is not a binary construct. In a small-scale experiment tovalidate the performance of this model, we annotated all the faceson 2,500 randomly sampled frames across our dataset and foundthe classifier to have a precision of 0.91 and a recall of 0.994 formales, and a precision of 0.975 and a recall of 0.81 for females.",
  "Extracting Panelist Names from Transcripts": "To study the individuals appearing in the debates, we extracted thenames of panelists from the transcripts. Traditional approaches likeNamed-Entity Recognition (NER) on the transcripts did not performwell for three main reasons: (i) NER captured names of peoplementioned in the debate but not actually panelists, (ii) multiplevariations were used to refer to the same person (e.g., [GeneralGD Bakshi, General Bakshi, Major General GD Bakshi]), and (iii)transcription errors led to inconsistent spellings of the same name(e.g., Atiqur Rahman, Atiq-ur-Rehman Sahab, Atiku Rehman). Toaddress these issues, we adopted Metas open-sourced LLaMA-213B model for this task .When the transcript of an entire video exceeded the modelscontext length, we chunked the transcript into parts and took theunion of names extracted from each chunk to identify potentialpanelists for the video. The prompt used for name extraction can befound in the supplemental pdf. The names returned by this approachwere not completely clean, so we performed fuzzy matching andclustered similar names using a combination of Partial Token SortRatio and metaphone-based matching.Using these techniques, we curated a list of 265 panelists, cover-ing 91.7% of the videos and 50% of all appearances. We focused onfrequently invited guests rather than full coverage due to the longtail distribution of debate participants. To validate our pipeline, oneauthor manually identified panelists in 50 videos and comparedthem to our pipelines results, achieving a precision of 0.901 andrecall of 0.730.Next, we identified and coded the occupation of the panelists intocategories such as TV-related, academics, activist, advocate, analyst,author, civil servant, consultant, doctor, film-related, journalist, politician, religious leader, social leader, and spokesperson. We alsocoded their affiliations (e.g., political party support).From the initial set of 285 people identified, 20 were removed asfalse positives. We only marked individuals who were part of someorganization (e.g., Bombay High Court, Samajwadi Party, DMK,BJP, All India Trinamool Congress, the Channel, Congress) andmarked None for others.",
  "What is discussed in the debates?4.1Bias in Transcripts": "Existing literature supports the notion that the show exhibitsa pro-government stance. Our categorization, summarized in Ap-pendix D, aligns with this perspective, revealing a significant 3-to-1ratio in favor of narratives that support the ruling party. However,unlike previous works, this paper zeroes in further on the contentof the show to showcase a political tilt, if any. To achieve this, wework with the transcripts and adopt a methodology akin to thosein , utilizing language models to identify potentially biasedattributive/contextual tokens.Specifically, we train a classifier to determine if a sentence inthe transcript pertains to the ruling party or the opposition. Thisclassifier is based on a fine-tuned BERT-Base-Uncased model ,equipped with a classification head.For classifier training, we select sentences from the transcriptsthat explicitly reference the ruling party or the opposition, usingspecific keywords such as names of parties or leaders (Appendix D).We exclude sentences that mention both to prevent ambiguity. Toensure the model focuses on the context rather than the keywords,we mask the specific keywords, replacing person names with <PER>and party names with <PARTY>.Given BERTs shortcomings in handling negations , we ex-clude sentences containing negation keywords such as not,wontetc. Our final dataset comprises 16, 444 sentences about the Oppo-sition and 14, 865 about the Ruling Party, divided into 80% training,10% validation, and 10% test sets. The model is fine-tuned for 30epochs with a batch size of 32, using the AdamW optimizer at a2e5 learning rate.To make the models decision-making process more interpretable,we use integrated gradients , a technique that effectively deter-mines the influence of individual tokens on the models predictions.This approach helps us pinpoint the tokens that significantly swaythe models judgment in classifying sentences as pertaining to theRuling Party or the Opposition, in line with Ding et al. .Our classifier achieved an accuracy of 85.72%. For a nuanced un-derstanding, we sorted the words in each category by their averageattribution scores across all sentences. After excluding stopwords,infrequently occurring words (less than 50 times), and generic termsto minimize noise, a qualitative analysis of these highly-attributabletokens reveals a distinct bias against the Opposition, while favour-ing the Ruling Party. The complete list can be found in .Below, we provide examples to illustrate this qualitatively: Ruling Party related tokens:(i) Election-centric Narratives: Tokens like vote victory and powersuggest a focus on the electoral successes of the ruling party.(ii) Veneration of Leadership: Terms like Modi wave, Modi factor,",
  "president (0.122) investigation (0.121) corruption (0.116) communal (0.101)chinese (0.092)xi-jinping * (0.088)failed (0.087)": "and respectful suffixes like ji (as in Modiji) paint a picture ofreverence around the party leadership. The term developmentoften co-occurs, framing the ruling party as a catalyst for progress.(iii) Defensive and Counter-Narratives: Surprisingly, words like ha-tred appear in the context of disputing the notion that animositytowards ruling party is justified. Other tokens like Trump andPakistan indicate international validation or emphasize a toughstance on national security.Opposition related tokens:(i) Dynastic Politics: Usage of words like dynasty, and familial ref-erences like mother-son-sister aim to cast the main Oppositionparty in a light suggestive of nepotism.(ii) Name-Calling and Stereotypes: Phrases like Rahul Baba, VadraCongress, and references to lobby paint the main Opposition partywith connotations of naivety & questionable ethics, or disloyalty.(iii) Allegations and Scandals: Terms like Rafale, China, and Jin-ping are mentioned in contexts that suggest improper or unpa-triotic conduct by the main Opposition party. Words like fake,shame, and lie reinforce a narrative of dishonesty and ineptitude. We also find similar bias in hashtags used for the show. To fetchthe hashtags displayed on the screen, we sampled a frame every30 seconds and extracted text using EasyOCR . The text corre-sponding to the hashtags was extracted using a regular expression.We see a clear pattern in how the hashtags are chosen: while crit-icisms of the ruling party tend to be issue-specific and nuanced,criticisms of the Opposition are likely to be sweeping and deroga-tory, contributing to a narrative that could potentially influencepublic perception. In debates critical of the ruling party, the hash-tags tend to be issue-centric rather than party-centric. For example,hashtags like #WillYogiSackMLA, and #YogiWakeUp focus on in-dividual incidents or politicians and dont necessarily indict theruling party as a whole. On the contrary, hashtags targeting theOpposition often portray them as either against the country oras disorganized and ineffective. Examples include #CongInsults-Democracy and #RahulMocksForces, where the use of Cong (anabbreviation for the main opposition party) implies that the en-tire party is undermining democratic values or the armed forces.",
  ": Fraction of panelists invited from the ruling partyvs. the opposition. Pro-ruling-party panelists appear morethan the opposition in almost all categories": "Further, hashtags like #MamataLosesGrip or #MayaDumpsCongindicate that the opposition parties are fractious and unreliable.The full list of hashtags used in our analysis is shown in AppendixD.By analyzing the affiliations of panelists, whose names wereextracted from the transcripts, we observe a discernible bias in theselection process for the shows panelists. As illustrated in , there is a disproportionate tendency to invite spokespeople orsupporters of the ruling party across various categories.",
  "Gender Bias": "provides a temporal analysis of the gender distributionof faces visible during the debate videos, spanning a period of sixyears. The data unambiguously shows that females are consistentlyunderrepresented when compared to their male counterparts. Thistrend is not isolated to specific periods but is persistent across theentire datasets history.We further delved into the issue by examining the representationof females in debates across various categories. Figures 4a and 4b",
  ": Average number of faces observed when a frame israndomly sampled from a video in the given month. Femaleguests are consistently underrepresented compared to theirmale counterparts": "highlight the top 5 and bottom 5 categories in terms of femalerepresentation, respectively. The data corroborates the presence ofsystemic gender bias. Notably, there are no categories where femalesconstitute the majority. Although Bollywood-related debates arean outlier, having nearly 40% of the panelists as women, in othercategories, female presence is alarmingly sparse. For instance, incritical and often polarizing topics like the Citizenship AmendmentAct (CAA) or the Kashmir issue, women make up only about 20% ofthe panelists. This under representation becomes even more starkin debates about the Pulwama terror attack, where women occupya mere 5% of the screentime.In addition to presence, we assessed the screen space allocatedto each gender by measuring the average size of visible faces insquare pixels. Our findings show that, on average, male faces occupy3, 798.51 sq pixels, while female faces are allotted only 2, 424.87 sqpixels. This discrepancy is not an isolated occurrence but a consis-tent pattern over time, as illustrated in Appendix D. The limitedscreen space for women, even when they are present, underscoresthe bias.Our comprehensive dataset of 3,000 videos reveals that womenaccount for a mere 7.5% of the total screen time, which diminishesto 7.2% in political debates. This underrepresentation is stark whencompared to the presence of women in Indian politics, where fe-males make up 14.32% of Parliament members, and around 25% ofthe internet population in India.As we will discuss in , there is a correlation betweencategories with lower female representation and higher levels ofincivility. This correlation raises concerns about the quality ofdiscourse and suggests that the gender imbalance may contribute toa more hostile debate environment. It also challenges the inclusivityof media channels in reflecting diverse viewpoints, especially onmatters of national and societal significance.",
  "Incivility in the Debates": "Indian television debates, particularly the one under study, are of-ten marked by high levels of incivility and excessive dramatization,characteristics that can both entertain and polarize the audience.While these traits contribute to the shows popularity, they raiseserious questions about the quality of public discourse and demo-cratic debate in the country. In this section, we aim to quantify these elements of incivility using three carefully chosen metrics: (1)speech overlap, (2) use of foul language, and (3) instances of shouting.Speech overlap acts as a proxy for conversational decorum, withexcessive overlap often indicative of a lack of respect for differingopinions. The use of foul language, operationalized through detect-ing hateful language using Googles Perspective API , directlyreflects the tone and content of the debate, revealing any under-lying animosities or prejudices. Lastly, the frequency of shoutingby the panelists offers insights into the emotional intensity of thedebate, potentially correlating with heightened levels of aggressionor antagonism. Collectively, these metrics provide a comprehensivelens to quantify and understand incivility in the complex setting ofIndian TV debates.",
  "Overlapping Speech and Toxicity": "The debates often elicit an emotional response from the panelistswhich either results in (1) panelists speaking over each other or (2)using foul speech to attack others opinions .To identify overlapping speech, we follow the procedure out-lined in .2. Figures 4c and 4d show the top and bottom 5categories which are significantly over or under the mean respec-tively. They indicate a pronounced pattern of overlap in specificcategories of debates, with particularly elevated levels observed indiscussions revolving around contentious issues like the CitizenshipAmendment Act (CAA), Kashmir, Politics, and Pulwama-Balakotevents , as well as Religion. It is striking to note that in debateson the Pulwama terror attack, the CAA, and Kashmir, over 20% ofthe discourse features overlapping speech. This suggests that thesehighly contentious issues are divisive and incite a breakdown inconversational decorum. Conversely, we find markedly lower levelsof incivility in debates related to International Affairs, COVID-19,the TRP Scam related to the Channel, Sports, and Bollywood. Wenext turn our attention to the prevalence of toxic speech, specificallythe use of foul language, in prime-time news debates. Contrary towhat one might expect from a mainstream platform, the presence oftoxic speech is not an aberration but rather an unsettling norm. Toquantitatively measure toxicity, we employ the Perspective API ,which assesses text across multiple dimensions including toxicity,identity attack, insult, profanity, severe toxicity, and threat. Ouranalysis, detailed in e, shows that an average of over 1%of the duration across videos in our dataset contain some form offoul language. While this percentage may seem relatively low, itgains significance when considering the shows mass viewership,often in the millions. Most strikingly, the categories registering thehighest toxicity levels are those discussing sensitive topics like Pak-istan, Kashmir and terrorist attacks. These topics require the mostthoughtful and nuanced discussion, yet they have been reduced toshouting matches and verbal attacks.Elevated levels of incivility (captured through overlap speechand toxic speech) are not just isolated events but indicative of abroader trend that compromises the quality of public discourse.When panelists choose disruption over dialogue, they contributeto a media environment where aggressive and confrontational be-haviour becomes the norm rather than the exception.",
  "(f)": ": Confidence Intervals. (a) Top-5 categories with more females than average. (b) Bottom-5 categories with less femalesthan average. (c) Fraction of the total duration of videos exhibiting overlapped speech for the top-5 categories, significantlyexceeding the datasets mean. The highest-ranking category has 20% of video duration overlapping speech. (d) Fraction ofthe total duration of videos with overlapping speech for the bottom-5 categories, significantly below the datasets mean. (e)Fraction of the total duration of videos with toxic speech in the top-5 most toxic categories. (f) Fraction of the total duration ofvideos with most shouting in the top-5 categories. Generalizability: Though the current study focuses on Indian TVdebates, our pipeline is adaptable to other content on the web, specifi-cally to debate shows in English. To demonstrate its generalizabil-ity and establish baselines, we applied our pipeline to four Eng-lish debate/panel-based shows: The Debate Show (France 24), ThePledge Debates (Sky News, UK), Morning Joe (MSNBC, US), and USPresidential Debates (2008-2020). Our analysis () comparedoverlapping speech and toxicity in these shows and found that theshows on the Channel have a statistically significantly higherincivility ( < 0.01) than all these shows. Refer to Appendix A forthe statistics and details on data collection for other debates.",
  "Detecting Shouted Speech": "To capture incivility holistically, it is imperative to not just studywhat is said but how it was said. Shouting is another form of in-civility used to overpower others opinions in a debate. Shoutingdetection in human speech is an established area of research .The Indian Broadcast News Debate (IBND) corpus containsnews debates from the Channel with annotations for shouted vs.normal speech. We used only the data corresponding to debatesheld on the Channel since all our inferences will be performed onsamples from the same domain. Using the raw audio from videosin our dataset, we extract 26 MFCCs5 per frame per audio file, witha frame size of 25ms and a gap of 10ms. On a per-audio level, weperform standard-scaling of these features and group frames into 1",
  "Mel Frequency Cepstral Coefficients (MFCCs) of a signal are features which conciselydescribe the overall shape of an audio spectral wave": "second blocks. Inferences for shouting detection are performed ona per-second level.We use a Convolutional Neural Network (CNN) to perform in-ference on per-second samples. The CNN consists of four blocks.Each block contains a convolutional layer with a ReLU activationfunction, a max pooling layer for down-sampling, and a dropoutlayer for regularization and ends with a fully connected layer witha sigmoid activation function for binary classification. The CNNwas compiled with the Adam optimization algorithm and binarycross-entropy as the loss function. We tested our approach on theIBND dataset with an 80/20 train-test split, ensuring no data leakageby dividing on a per-audio basis. The model achieved 85% accuracyand, with a high precision of 0.862, was deemed reliable for broaderapplication. A majority voting system for continuous shouting fur-ther minimized false positives. The lower recall of 0.71 suggeststhat shouting instances may be underreported. Manual checks ofrandomly sampled shouting instances found no false positives. Forvalidation of the classifiers performance, see Appendix B.f shows the average percentage of time shouting occursin each video, focusing on the top five categories. The completeplot for all categories is included in supplemental pdf. Shoutingoccupies 9% of the video duration on average, suggesting a notabledeparture from civil discourse. Categories like Kashmir, Religion,and Crime & Justice are especially prone to high levels of shouting,corroborating the findings in Figures 4c, 4d, and 4e. This level ofshouting, particularly in sensitive topics, underscores the emotion-ally charged nature of these debates. It raises questions about theefficacy of such discourse in fostering meaningful dialogue.",
  "Discussion": "Our research employs a comprehensive toolkit, integrating state-of-the-art open-source tools in computer vision, speech processing,and NLP, to analyze large quantities of video content. We applythis toolkit to a case study involving one of Indias most-watchedprime-time television debate shows, which garners over five milliondaily viewers. The show has received critique for its emphasis onstrong nationalistic sentiments and its approach towards minoritycommunities. By making our code public, we aim to encouragefurther research and analysis in diverse contexts.Our analysis uncovers significant bias and incivility within thedebates, including a notable underrepresentation of women anda bias towards the ruling party. While there has been anecdotalevidence suggesting such biases, our research quantifies these bi-ases. The act of delegitimizing opposition voices has far-reaching implications for the democratic discourse. Our analysis suggeststhat the use of sensationalism and dramatization may be a deliber-ate tactic rather than merely a byproduct of the shows popularity.Around 10% of the debate time involves shouting, highlighting anenvironment that is antithetical to civil discourse.Televisions significant influence on public opinion is concerningwhen coupled with the biases weve identified . This becomeseven more alarming considering that opposition coalitions havestarted boycotting certain television hosts based on similar criti-cisms , potentially furthering polarization. When millions relyon such a low-quality platform for political insights, the spreadof biased information undermines democratic processes and couldlead to a misinformed electorate. The high ratings of such showsdespite their evident flaws introduce a complex paradox. It chal-lenges the simplistic notion that the media merely reflects publicopinion, suggesting that it may play a role in shaping/distorting it.Overall, our findings offer more than an academic contribution;they signal an urgent call to action. They serve as a critical resourcefor researchers studying media ethics, democratic governance, andsocietal polarization. Our work raises complex questions about theethical responsibilities of media in a democracy and the influenceof media on public opinion. These issues warrant investigation andshould be of concern to policymakers, civil society organizations,and the public at large.Limitations: (i) Scope: Our study is limited to a single prime-timenews debate show and may not apply to more informal content likeTikTok videos, which have highly variable discourse quality andnature. (ii) Manual Annotation: The need for manual annotationin categorizing videos and identifying panelists limits scalabilityand could introduce bias. (iii) Technical Constraints: Our work isconstrained by the accuracy and potential biases of the classifiers,with the risk of compounded errors throughout the pipeline stages.Ethics Statement: While our toolkit makes large video datasetsmore tractable for analysis, the potential for misuse is present;for example, the ability to index and search entire video archivescould pose significant privacy risks. As with any tool, the ethicalimplications of its application should be carefully considered ac-cording to the use case. Considering that politicians and politicalanalysts are public figures, and taking into account the significanceof research in comprehending the language employed in politicaldebates and its consequences, we believe our work conforms toacceptable standards of privacy .Future Work. This study merely scratches the surface of whatcan be achieved with automated, large-scale analysis of televiseddebates. We have not fully utilized diarization data due to cluster-ing challenges. While speech embeddings have been tested, theyneed refinement for practical use. Future work could use diariza-tion for deeper analyses like anchor bias or systemic media biasOverall, while our study has limitations, it offers a pioneering ap-proach to multimedia content analysis, setting the stage for morecomprehensive, automated methods in the future.",
  "SK Hussain. 2020. The Dirty Game Pro-Hindutva TV Channels And Their An-chors Play old.indiatomorrow.net. [Accessed 13-10-2023]": "Jungseock Joo, Erik Bucy, and Claudia Seidel. 2019. Automated Coding of Tele-vised Leader Displays: Detecting Nonverbal Political Behavior With ComputerVision and Deep Learning. International Journal of Communication 13 (2019). Aditya Khandelwal and Suraj Sawant. 2020. NegBERT: A Transfer Learning Ap-proach for Negation Detection and Scope Resolution. In Proceedings of the TwelfthLanguage Resources and Evaluation Conference. European Language Resources As-sociation, Marseille, France, 57395748. Lev Konstantinovskiy, Oliver Price, Mevan Babakar, and Arkaitz Zubiaga. 2021.Toward automated factchecking: Developing an annotation schema and bench-mark for consistent automated claim detection. Digital threats: research andpractice 2, 2 (2021), 116.",
  "Raksha Kumar. 2023.How Indian TV news became a theatre of aggres-sion fanning the flames of populism reutersinstitute.politics.ox.ac.uk": "Alyssa Lees, Vinh Q Tran, Yi Tay, Jeffrey Sorensen, Jai Gupta, Donald Metzler, andLucy Vasserman. 2022. A new generation of perspective api: Efficient multilingualcharacter-level transformers. In Proceedings of the 28th ACM SIGKDD Conferenceon Knowledge Discovery and Data Mining. 31973207. Bo Li, Dongseong Hwang, Zhouyuan Huo, Junwen Bai, Guru Prakash, Tara NSainath, Khe Chai Sim, Yu Zhang, Wei Han, Trevor Strohman, et al. 2023. Efficientdomain adaptation for speech foundation models. In ICASSP 2023-2023 IEEEInternational Conference on Acoustics, Speech and Signal Processing (ICASSP).IEEE, 15. Tianming Liu, Hong-Jiang Zhang, and Feihu Qi. 2003. A novel video key-frame-extraction algorithm based on perceived motion energy model. IEEE transactionson circuits and systems for video technology 13, 10 (2003), 10061013. Naveen Mishra. 2018. Broadcast Media, Mediated Noise, and Discursive Violence-High Decibel TV Debates and the Interrupted Public Sphere. KOME: An Interna-tional Journal of Pure Communication Inquiry 6, 1 (2018), 113.",
  "Shriphani Palakodety, Ashiqur R. KhudaBukhsh, and Jaime G. Carbonell. 2020.Mining Insights from Large-Scale Corpora Using Fine-Tuned Language Models.In European Conference on Artificial Intelligence": "Zizi Papacharissi and Maria de Fatima Oliveira. 2008. News frames terrorism: Acomparative analysis of frames employed in terrorism coverage in US and UKnewspapers. The international journal of press/politics 13, 1 (2008), 5274. Tae Jin Park, Naoyuki Kanda, Dimitrios Dimitriadis, Kyu J Han, Shinji Watanabe,and Shrikanth Narayanan. 2022. A review of speaker diarization: Recent advanceswith deep learning. Computer Speech & Language 72 (2022), 101317. Jouni Pohjalainen, Tuomo Raitio, Santeri Yrttiaho, and Paavo Alku. 2013. De-tection of shouted speech in noise: Human and machine. The Journal of theAcoustical Society of America 133, 4 (2013), 23772389.",
  "Benjamin Renoust, Duy-Dinh Le, and ShinIchi Satoh. 2016. Visual analyticsof political networks from face-tracking of news video. IEEE Transactions onMultimedia 18, 11 (2016), 21842195": "Filipe Ribeiro, Lucas Henrique, Fabricio Benevenuto, Abhijnan Chakraborty,Juhi Kulshrestha, Mahmoudreza Babaei, and Krishna Gummadi. 2018. Mediabias monitor: Quantifying biases of social media news outlets at large-scale. InProceedings of the International AAAI Conference on Web and Social Media, Vol. 12. rkcosmos. 2020. GitHub - JaidedAI/EasyOCR: Ready-to-use OCR with 80+ sup-ported languages and all popular writing scripts including Latin, Chinese, Ara-bic, Devanagari, Cyrillic and etc. github.com. [Accessed 13-10-2023]. Diego Saez-Trumper, Carlos Castillo, and Mounia Lalmas. 2013. Social medianews communities: gatekeeping, coverage, and statement bias. In Proceedings ofthe 22nd ACM international conference on Information & Knowledge Management.16791684. Parul Saini, Krishan Kumar, Shamal Kashid, Ashray Saini, and Alok Negi. 2023.Video summarization using deep learning techniques: a detailed analysis andinvestigation. Artif. Intell. Rev. 56, 11 (2023), 1234712385. Anirban Sen, Debanjan Ghatak, Gurjeet Khanuja, Kumari Rekha, Mehak Gupta,Sanket Dhakate, Kartikeya Sharma, and Aaditeshwar Seth. 2022. Analysis ofmedia bias in policy discourse in india. In ACM SIGCAS/SIGCHI Conference onComputing and Sustainable Societies (COMPASS). 5777.",
  "India Today. [n. d.]. INDIA bloc to boycott shows of 14 TV journalists, mediapanel condemns move indiatoday.in. [Accessed 12-10-2023]": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-mine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-ale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucu-rull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, CynthiaGao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, et al. 2023. Llama 2:Open Foundation and Fine-Tuned Chat Models. arXiv:2307.09288 [cs.CL]",
  "The Pledge Debates (hosted on Sky News in the UK): Focusedon full-length debate videos from their YouTube channel,specifically selecting those exceeding 20 minutes in duration": "Morning Joe (hosted on MSNBC in the US): Curated a setof videos from their YouTube playlist, including only thoselonger than 30 minutes to concentrate on complete episodesof the main show, resulting in 403 videos for our study. US Presidential Debates (from 2008-2020): Compiled 38 de-bate videos, encompassing the main presidential and vice-presidential debates from 2008-2012, as well as intra-partycandidate-nomination debates.We conducted a two-tailed t-test with a 95% confidence intervalto compare the overlap speech in debates from the Channel withthe other debates mentioned. We found that the overlap speech inthe Show is statistically greater than in all the other TV debateslisted above. Detailed results can be found in a. Similarly,when analyzing toxicity levels, we determined that the Show ex-hibits statistically higher toxicity compared to debates from France24, the US Presidential Elections, and Morning Joe. For a compre-hensive breakdown, refer to b.",
  "BValidation Experiments": "Validation for classification of speech into shouted and non-shouted categories: We manually annotated 50 audio samplesfrom our dataset, classifying them as shouted or non-shouted. Ourclassifier achieved a precision of 0.91 and a recall of 0.75. The IndianBroadcast News Debate (IBND) dataset , which includes debatesfrom the Channel with shout annotations, showed our classifierhad a precision of 0.86 and a recall of 0.71 on 62,375 test samples.Given the IBND dataset shares our datasets domain, these resultssuggest comparable performance on our data.",
  ": Panelist co-occurrence network": "Using panelists information we coded in .4, we createda co-occurrence network between the panelists. If two panelistsappeared together in a debate, they were connected by an edge. Wefound that such a network () was clustered along categories and occupations of the panelists, indicating that the show invitesspecific panelists based on topics of discussion. The five commu-nities were automatically identified using the Louvain method forcommunity detection. (1) Orange: Found occupation like Advocate,civil servants but not film related occupation: Not related to Bol-lywood internal disputes (2) Blue: All religious/social leaders andacademic people: Something related to religion (3) Pink: All TV andfilm related people: related to Bollywood (4) Yellow: Army relatedpersonal, activists: related to border disputes/army (5) Green: Onlypolitician, spokesperson and analyst: Any general political debate",
  "Farmers Protest Issue": "Number of speakers Dataset unique shouter count meanUnique shouter count meanUnique speaker count mean : Average count of panelists engaged in shouting (inred) compared to the total panelist count (in blue) for top 5categories with the highest incidence of shouting. The dataindicates that 50% of panelists in these categories participatein shouting behavior. We look at the number of people participating in the shouting. Bymatching the shouting segments with the diarized text, we identifythe speakers who shouted. We wanted to understand whether thedebates are being derailed by a small group of people or manypanelists have to engage in such behavior to have their voicesheard. shows the top five categories ordered by the averagenumber of panelists engaging in shouting along with the number ofspeakers on average in each category. We find that, roughly half ofthe participants engage in shouting. It is also important to note thatthese categories with the highest number of shouting panelists arevery different from the results we found in the rest of the figuresdocumenting incivility (Figures 4c, 4d, 4e, and 4f)."
}