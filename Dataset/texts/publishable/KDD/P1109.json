{
  "Abstract": "Denial of service attacks pose a threat in constant growth. This is mainly due to their tendency to gain in sophistication, ease of implementation, ob- fuscation and the recent improvements in occultation of fingerprints. On the other hand, progress towards self-organizing networks, and the different tech- niques involved in their development, such as software-defined networking, network-function virtualization, artificial intelligence or cloud computing, fa- cilitates the design of new defensive strategies, more complete, consistent and able to adapt the defensive deployment to the current status of the network. In order to contribute to their development, in this paper, the use of artifi- cial immune systems to mitigate denial of service attacks is proposed. The approach is based on building networks of distributed sensors suited to the requirements of the monitored environment. These components are capable of identifying threats and reacting according to the behavior of the biolog- ical defense mechanisms in human beings. It is accomplished by emulating the different immune reactions, the establishment of quarantine areas and the construction of immune memory. For their assessment, experiments with public domain datasets (KDD99, CAIDA07 and CAIDA08) and simula- tions on various network configurations based on traffic samples gathered by the University Complutense of Madrid and flooding attacks generated by the",
  "By definition, Denial of Service (DoS) has the objective of disabling com-": "puter systems or networks. The DoS attacks with origin in multiple sources are referred as Distributed Denial of Service (DDoS) attacks. In recent years, the number of incidents related with these threats reported by the various organizations for cyber defense shows an alarming growth. According to the European Network and Information Security Agency (ENISA), between 2013 and 2014 an increase of 70% was observed (Marinos, L. and Sfakianakis, A. (2016)). In addition, they pose a threat that has begun to be used in or- der to achieve other objectives. These include disguising activities in rela- tionship with malware spreading, concealment of fraudulent money transfers (DoS (2016d)) or compromising anonymous networks, such as Tor or Freenet (Jansen et al. (2014)). This growth is attributed to various reasons: the first of them is that DDoS attacks are usually triggered by previously infected systems, which in most of the cases are part of botnets. The botnets have been adapted to be resilient against the classical detection schemes, thus allowing the construction and maintenance of larger collections of zombies and increasing their difficulty to be identified. (DoS (2016f)). Another im- portant reason is that attackers are able to take advantage of amplifying elements, in this way enhancing their potential to be harmful. To do this, they exploit vulnerabilities in protocol implementations on the intermediate network devices, particularly at DNS, NTP and SNMP. On the other hand, as the European Police Office (Europol) warns (DoS (2016g)), the DDoS is becoming increasingly linked with the organized crime. Rent botnets for execution of these attacks is a very profitable business on the black market, often supplied as Crimeware-as-a-Service (CaaS). Finally, offenders with lack of formation have a wide variety of tools for easily configuration and deploy- ment of flooding attacks. The black market also offers technical support, a situation that expands the range of user profiles which are able to attack with success.",
  "To serve this cause, a strategy for detection and mitigation of DDoS": "flooding attacks is proposed. Therein the deployment of a sensor network that integrates an Artificial Immune System (AIS) inspired by the biological defense mechanisms of human beings is introduced. Unlike similar proposals, conventional bio-inspired methods for pattern recognition were not applied. Instead a combination of strategies for DDoS detection based on the study of variations of the entropy on the network traffic by thresholding, with the adaptation of the biological immune reactions is proposed. This makes it possible to apply real-time countermeasures, building an immune memory and establishment of quarantine areas, all in accordance with the current state of the protected network. In view of this, it is important to highlight the two major contributions of this paper: firstly, a new method for detect- ing DDoS that is able to forecast anomalies on the entropy of the traffic analyzed, and thereby recognition of flooding attacks is introduced. This is performed by representation of the entropy in time series and the definition of prediction intervals. On the other hand, a strategy for management of immune agents that implement the previously described detection system is proposed. Within this, the decisions are made as to when and how they will act and in what level of restriction, all this depending on the status of the network and orchestrated by an artificial immune approach.",
  "The paper is divided into seven sections, and the first of them is the present": "introduction. The background necessary for a better understanding of the approach is described in section II. The proposed AIS is introduced in section III. The novel DDoS detection method implemented in the vari- ous agents of the AIS is detailed in section IV. Experiments, datasets and methodology are described in section V. Results are discussed in section VI. Finally, conclusions and future work are presented in section VII.",
  "According to (Wei et al. (2013)), there are two types of traffic injection": "able to compromise a system or network by flooding. The first one is based on the constant and continuous generation of large volumes of information, and is well known as high rate flooding. This is a method which is usually very visible that easily overflows the computing capacity of the victim. On the other hand, the victim may be compromised by less noisy attacks, which are able to exploit vulnerabilities in the various communication protocols. They are known as low rate flooding attacks, using as a typical example, the attacks with On/Off patterns addressed against the TCP (Tang et al. (2014)). In both cases, the malicious traffic may be sent to the victim in a direct or reflected way (Bhuyan et al. (2015); Anagnostopoulos et al. (2013)). Most efforts of the community to deal with denial of service assume these behaviors, and from them different methods for detection, mitigation and identification of sources are proposed. Their most important features and evaluation schemes are described below.",
  "The total or partial reduction of the damage inflicted by the attacks is": "defined as mitigation. To do this, it is common to use honeypots (Heckman et al. (2013)), puzzles that recognize non-human users (Zhu et al. (2014)), bandwidth enlargement (Khanna et al. (2012)), filtering or the adoption of security protocols such as IPsec (DoS (2016g)). As can be observed, the set of mitigation actions may also contain prevention strategies. These are characterized by not having direct dependence on the attack detection.",
  "To find the compromised systems from which the malicious traffic is origi-": "nated is referred to as the identification of their sources. Ideally, its objective is to track the cybercriminal. However, given the administrative difficulties that this process involves (different Internet Service Providers (ISPs), prox- ies, data privacy legislations, etc.), and the recent advances on footprint occultation, this goal is often very hard to carry out successfully. Conse- quently, many of the proposals in the literature just focus on getting as close as possible to the attacker, in order to sanitize the largest amount of regions within the protected network. On the identification of sources, the packet traceback is the most frequent approach. In (Alenezi and Reed (2014)) this issue is discussed, a lot of current approaches are collected, and a new scheme for uniform tracking is proposed. The Passive IP Traceback (PIT) that by- passes the deployment difficulties of the conventional IP traceback techniques by investigation of ICMP error messages is proposed in (Yao et al. (2015)). Finally, in (Jeong and Lee (2014); Yao et al. (2015)) the influence of the characteristics of the network topology on the effectiveness of the strategies for packet marking is studied.",
  "All living beings have developed multiple immune mechanisms, empha-": "sizing among them defenses of vertebrate species due to their sophistication. Many types of proteins, cells, organs and tissues form part of these systems, and they are related through an elaborate and dynamic network. As part of this more complex immune response, the human immune system, over time, adapts to recognize specific antigens, which is called adaptive immunity. The defense mechanisms compose the innate immunity, and usually are the first line of protection.",
  "On the other hand, the adaptive immunity presents specificity, i.e., after": "learning how to identify and reject an antigen, the knowledge gained allows it to react more firmly against the intruder, by generating new and stronger agents; but they can only act for mitigating the threat for which they were created. The most important cells involved in this process are lymphocytes and presenting cells, and the most important adaptive immune responses are humoral and cellular. In both of them take part agents responsible for",
  "Humoral response. The antibodies detect and eliminate the threats": "by swallowing them. The remains of this process are captured by Th lymphocytes, and these stimulate the Tb lymphocytes to generate an even greater amount of antibodies specialized in recognizing the threat. Antigens never seen before are identified by the antibodies that have suffered small mutations in their construction process, allowing them to recognize different antigenic determinants.",
  "The adaptation of biological defenses towards information security is usu-": "ally performed by deployment of multi-agent systems (Ou (2012)). In pio- neering approaches, such as (King et al. (2001); Harmer et al. (2002)), the main guidelines for the emulation of the activities carried out by immune cells were introduced. They often apply some of the four classical bio-inspired al- gorithms: negative selection, clonal selection, artificial immune networks and Danger Theory (DT), which are briefly described below.",
  "Negative selection is the process by which the immune agents learn to": "distinguish antigens from the cells of the organism itself. In (Zhaowen et al. (2012)) there is a good example of its application for detection of anomalies. But as the authors suggest, it poses a methodology that tends to generate high false positive rates, a situation that often leads to its complementation by clonal selection algorithms (Ligeiro (2014)). These are based on the as- sumption that every lymphocyte at its growth stage must be able to react",
  "The immune networks stem to the idea of extending clonal selection to": "networks. They are commonly implemented as a channel of interaction be- tween the different actors of AIS. In (Seresht and Azmi (2014)) there is an example of an immune network for connecting different agents that perform negative selection. In (Yang et al. (2011)) a similar deployment is adopted, but this time involving agents that apply Danger Theory.",
  "The Danger Theory takes into account the latest advances in medical": "research. Consequently and unlike their predecessors, it rejects the idea that organisms have the capacity to distinguish between own cells and antigens. Instead it postulated that the triggering of immune reactions is originated by warning signals sent from tissues in direct contact with the threat. Because of its novelty, it is one of the most common algorithms in the bibliography of recent years. In (Aickelin et al. (2003)) the bases for its adaptation to intrusion detection are defined. DT is applied to recognition of enumeration attacks in (Greensmith et al. (2010)), and it is also considered for intrusion detection based on studying the system calls of the protected environment in (Azmi and Pishgoo (2013)).",
  "Despite the predominance of these methods in the bibliography, not all": "AIS are based on such specific processes of the biological immune systems. Some approaches imitate the global behavior of the innate and adaptive immune responses on the vertebrate beings, without following predefined algorithms. This makes it easier to combine the defensive strategies used in each field, with the ideas provided by biological immune systems, by this way reaching solutions that best fit the real use cases. A good example of this is proposed in (Boukerche et al. (2007)), where this idea is implemented to detect anomalies in network traffic. To do this, six main classes of agents are considered, among which are distributed sensing, communication and reaction tasks. Their adaptive immune response involves the cloning of a greater number of antibody agents in the threatened regions, by this way increasing their presence on the protected environment. In (Chen (2010)) antibodies are mobile agents that swarm the network looking for indicators of damage. Another example is (Visconti and Tahayori (2011)), wherein when the immune agents identify a potentially harmful incidence, the adaptive",
  "As can be observed, these premises meet an important part of the needs of": "the current networks. However there are other aspects that have been set aside, highlighting among them the fight against the various evasion strate- gies. These contain methods for disguising the source of the attacks or hinder the tracking of the flooding path (Fast-Flux, Domain Generation Algorithms (DGA), exploitation of anonymous networks, etc.) (Zargar et al. (2013); Al- Duwairi and Al-Hammouri (2014). On the other hand, there are algorithms designed for misleading the detection system and thus do not triggering coun- termeasures, such as those proposed in (Ozcelik and Brooks (2015)). Their consideration involves adding a lot of complexity to the proposal, and be- cause of this, it is out of the scope of the paper. Another important aspect that is also delegated to future work is to facilitate the interoperability with security protocols and data protection policies. We understand that analyz- ing obfuscated headers also means an important increment on its complexity, not being recommended for a first approach. Bearing this in mind, the ar- chitecture, behavior and properties of the proposal are described in depth below.",
  "The distribution and cooperation of the various components of this ar-": "chitecture allows performing innate and adaptive responses to attacks. The deployment of an orchestrator on a different data plane from the protected network connections provides autonomy, prevents that the traffic generated by the AIS penalize QoS, and facilitates the adoption of security protocols, thus reducing the risk of packet poisoning or Man-in-the-Middle attacks. But despite its obvious benefits, it is optional, because in certain use cases it is not possible to have much control of the protected environment (law restric- tions, privacy, etc.). In this situation, the immune agents may have sufficient autonomy to do without it, which involves: being in direct contact with agents over which it has influence, saving a log with the performed actions and determining whether it belongs to a region in quarantine or not (and the consequences that this entails). Usually these actions can be carried out easily, as for example by the implementation of tunneling between agents for their communication, the use of counters to determine the quarantine period, etc. but not with the global overview of the protected network and the advantages that the orchestrator provides.",
  "As in biological systems, the innate immunity on the approach is the first": "line of the defense strategy. It aims to identify and mitigate new threats and protect H detectors of disablement by flooding. The process of innate immunity requires maintaining activated DH agents along the protected net- work. These sensors monitor the entire traffic flowing through them looking for suspicious anomalies. Therefore, detected attacks must present certain evident characteristics related to considerable fluctuations in the analyzed traffic distribution. Once a threat is identified, the mitigation measures con- sist mainly on the adoption of directives that restrict the communications with nodes, ports or services involved in the attack vector. We are aware that more sophisticated mitigation actions could be implemented, but their decision and development are delegated to future studies in order to facilitate a better understanding of this first approach.",
  "The innate response provides quick and efficient countermeasures, re-": "quiring no communication with the orchestrator prior to their launch. By recognition and elimination of pathogens before they enter into the system, the proposal innate response emulates the behavior of the immune system of human beings. This is because it acts in the same way as the various exter- nal physical barriers or cells, and without specificity. In addition, it should be noted how agents involved in this task act coincide with those of most conventional Intrusion Prevention Systems (IPS), i.e. IDS with the ability to apply basic countermeasures.",
  "The adaptive response is the next defensive step in the proposal. It is trig-": "gered every time a DH agent recognizes a new threat, which implies that they must hold at least a short memory capable of storing their latest decisions. In this context, determining when an attack is Non-Seen-Before (NSB) im- plies it is not presence in the immune memory. Because of this, that memory has a very important role in the AIS decision-making. Determination of how the immune memory will be implemented is not usually a trivial problem. With this purpose, the two more intuitive schemes to take into account are those centralized or distributed. When the immune memory is centralized, is sustained by the orchestrator. In this case this component is responsible for determining whether an incidence is NSB, considering information provided by all the sensors. However, if it is distributed, each detector disposes only the information gathered for itself, or by a group of close sensors. Hence",
  "Once the adaptive reaction is released, the DH that identified the attack": "sends activation signals to the DA agents in close proximity. The scope of these signals can be determined in different ways. It may affect the surround- ing neighbors, nodes on a certain region or network devices in specific routes previously established by the orchestrator. This also enables the application of Artificial Intelligence and Data Mining in order to research their optimal propagation, giving many possibilities for future works. For simplicity the first of the previously mentioned alternatives was implemented.",
  "Then the activated DA agents analyze traffic flowing through them. Un-": "like DH, their detection engines increase restrictiveness in proportion to the flood of the attack, usually acting much more stricter than DH. In this way it is prevented that the division of the attack flow reach the victim by al- ternative routes, assuming that when it is split, it becomes less noisy and hence more difficult to be detected. In order to prevent this measure result- ing in a substantial increase in the false positive rate, specificity is taken into account. To ensure specificity, they are only able to apply countermeasures against the threat that has activated them. Therefore, they can only take action against several attacks if they have been activated to mitigate each of them. Because of all of this, and as in nature, the artificial adaptive immune response involves the increase of the amount of effectives able to react against a certain triggering attack.",
  "At the end, the deployed countermeasures are effective for a certain period": "of time. While the threat persists, the immune response remains activated. If it is no longer visible, a quarantine period is activated. The quarantine is interrupted only upon detection of replicas of the intrusion (implying back to the previous state), or when the countdown expires. The network segments covered by a set of DA agents active against a specific threat and coordinated by the same DH sensor, are their quarantine region.",
  "An example of the behavior of the proposed AIS when dealing with DDoS": "flooding attacks is described in . It is part of the situation shown in a, where S is the source of the attack, T is the target, and each Ni is an intermediate node located at i. In the case that the sensor DH is unable to recognize the threat, it is propagated along the protected network via different routes and according to the load balancing policies, as shown in b. But if the attack is successfully detected, the innate immune response is initiated. Thus the traffic from S is discarded, slowing the advance of the intrusion, as shown in c. As a legitimate flow trying to reach its destination when some network incident block its route, the malicious traffic will try to reach the victim for alternative paths. Fortunately, when the threat was detected the adaptive immune response was also triggered. As shown in d, this has led to the activation of the neighboring DA agents. Consequently, the attack reaching the victim through the new connections is prevented.",
  "Despite its simplicity, this scheme has a large number of advantages": "Firstly, it increases the defensive measures when a threat is recognized, pro- portional to the risk level. In addition, the specificity makes it only occur on specific connections, without affecting other traffic routes and reducing the impact of the false positives. Furthermore, the presence of two types of agents allows it to strategically adapt the defensive network to the monitor- ing environment. In this way the most powerful computers may act as DH, and the rest assume the role of DA. Note that a single node can assume both roles, or even deploy their own virtual network including several of them. On the other hand, the restriction level is self-regulated through deactivation of agents when the quarantine time passes, thus saving resources.",
  "The monitored traffic is analyzed by studying the entropy of its distribu-": "tion. In particular, entropy fluctuations are analyzed looking for anomalous behaviors. The decision to use entropy variations over other detection meth- ods proposed in the bibliography is that, as demonstrated in (Ozcelik and Brooks (2015)), is much more effective. This is principally because its accu- racy depends less than those of the others on how the protected network is used. The traditional entropy was first adapted to the information theory by Shannon in 1948 (Shannon (1948)). It was considered as a measure of fluctuations on qualitative variables, and it is often defined as the degree of unpredictability on their behavior. Given the qualitative variable X, the finite set {x1, x2, , xn} and their probabilities p1, p2, , pn, the Shannon entropy was described by the following expression:",
  "(1)": "where loga blogbx = logax. In addition, if the variable is deterministic then H(x) = 0 must be satisfied. This means that all the pi probabilities are 0, except for one, which has value 1. There are different generalizations of this entropy, adapted to the different use cases. The AIS applies that proposed by R`enyi. This decision was made considering studies like (Bhuyan et al. (2015)), where its effectiveness stands out from the rest of variations when applied on the detection of DDoS flooding attacks. R`enyi entropy is defined as follows:",
  "count that H(X)N series may experiment changes in trend and seasonality": "over time. Additionally, the forecasting methods to consider should be ef- fective with few observations, and able to run efficiently in real time. For this reasons, to model the network behavior the triple exponential smooth- ing proposed by Holt-Winters has been chosen. This election is supported by publications like (Gardner and Dannenbring (1980); Groff (1973)), where it is shown that considering the trend and seasonality of series in which they are unrepresentative, leads to insignificant prediction errors. Furthermore, the time required to calculate their forecasts is considerably lower than in the autoregressive models. Note that in order to avoid confusion between the range on R`enyi entropy and the parameter on Holt-Winters, henceforth H(X) is summarized as H(X), and the coming alpha symbols will refer only to the forecasting adjustment.",
  "N": "where Bt is the base estimation at t, the estimation of the trend is Tt and the estimation of the seasonal factor is St. The forecasting parameters , and fall in the range 0 < , , < 1, and facilitate the adjustment of the smoothing. The prediction Ht+1 is usually calculated by additive or multiplicative operations. This approach considers the additive version, since it is assumed that the seasonal pattern of the series is independent of its trend. Consequently the forecast is calculated as follows:",
  "Another important aspect to keep in mind is the initialization method of": "B0, T0 and S0 estimators. It is assumed that when no trend or seasonality is expected on the time series, the initialization of estimators based on the latest observations is preferable over the use of global measures. The imple- mented method is described in (Makridakis et al. (1998)), which has proven to behave particularly well in similar use cases. Namely, the last twenty-four observations are considered. The calculations performed are the following:",
  "var(Et) (14)": "where Et is the prediction error in t and p0 is the prediction of the last obser- vation. The prediction error is given by the difference between the forecast and the t observation. The variance V ar(Et) is calculated considering the prediction error of the last t observations. In addition, the thresholds include a parameter K, from which it is possible to adjust the sensitivity of the detec- tor. In the case of DH agents, the default value Z = is assigned to K, thus relating the thresholds with the normal distribution of the series. Note that this is not a wrong decision considering publications as (Makridakis et al. (1998)), where it has shown that when the time series does not approach the normal distribution, the error is unrepresentative. Moreover the margin rate of both intervals is in the order 100(1 ).",
  "As an example, in , the time series associated with the entropy of the": "monitored traffic in part of one of the experiment and its prediction are shown. Legitimate traffic passes through the sensor until the observation at t = 56; then the injection of a large volume of traffic is observed. In , the prediction intervals of the sensor under the same circumstances are shown. When the attack is launched, both thresholds are exceeded. Then the agent reports of the incidence and drops the malicious packets, so the entropy back to their original values.",
  "In the implementation of the sensors, the observations are delimited by": "a fixed number of packets whose order of arrival is consecutive. A common alternative to this is their delimitation by considering time intervals. Both pose advantages and disadvantages, with the first choice being more efficient in the analysis of collections of previously captured traces, as is the case of most of the datasets analyzed (Ozcelik and Brooks (2015)). In addition, a sliding window of size N that gathers the observations involved in the calculation of the entropy was applied. This boundedness is important to ensure that the implemented algorithms are computable, avoiding the case where N . The proposal has been evaluated in two stages. Firstly, the accuracy of the various agents when dealing with DDoS flooding attacks was measured. On the other hand, several features related to the effectiveness of the deployment of the AIS were studied. The following describes each of them in detail.",
  "Given the controversy relating to the evaluation methods of the effec-": "tiveness of intrusion detection system for identification of DDoS attacks, the scheme proposed in (Kumar and Selvakumar (2013)) was applied. This involves the use of two well-known datasets: KDD99 (DoS (2016e)) and CAIDA07 (DoS (2016b)); and in the generation of flooding attacks with the tool DDoSIM (DoS (2016c)) in a real use case. The following describes the principal characteristics of each test and their application.",
  "The KDD99 (DoS (2016e)) is one of the most referenced methodologies": "in the bibliography, and according to (Bhatia et al. (2014)), possibly the only one that presents a dataset with reliable labeling. It was created in 1999, under the KDD Cup competition, and from captures of traffic provided by DARPA98. The competition task was to build a network intrusion detec- tor, a predictive model capable of distinguishing between bad connections, called intrusions or attacks, and good normal connections. It provides 41 different features of legitimate and malicious traffic samples. The attacks fall into four main categories: DoS (Denial of Service, e.g. syn flood ), R2L (unauthorized access from a remote machine, e.g. guessing password ), U2R (unauthorized access to local superuser (root) privileges, e.g., buffer overflow attacks) and probing (surveillance and other probing, e.g., port scanning). Originally it separates a subset of the datasets for the training stages of ex- pert intrusion detection systems, and the rest for their evaluation. Since the proposed system requires no training, all samples have been applied in the evaluation process, with the exception of the first observations, necessary for initialization of the predictive models. It is noteworthy that the antiq- uity of the dataset and the discovery of irregularities in its content have led to its discrediting. An important part of the research community considers KDD99 unrepresentative, mainly due to lack of heterogeneity in comparison with current networks, old class of intrusions, errors when data gathering, etc. As is discussed in (Viswanathan et al. (2013)), this leads to the mistake of consider that the experimental results are scalable to real monitoring en- vironments. But despite this, it remains one of the most used methodologies, mainly due to the administrative difficulties associated with the publication of new datasets and the fact that it was implemented in most of the previous proposals.",
  "The CAIDA07 dataset (DoS (2016b)) provided samples of traffic traces": "containing DDoS flooding attacks (mainly ICMP, SYN and HTTP) moni- tored in August 2007. They are divided into files with extension pcap and spaced at time intervals of five minutes. As described in their documenta- tion, after the capture, most of the non-malicious contents were removed. Therefore it provides a good battery of tests to evaluate the effectiveness of the detection systems when analyzing flooding attacks, thus allowing their hit rates to be calculated. However, it is also necessary to determine their",
  "This test scenario is a real use case. It combines the study of the habitual": "traffic on the subnet corresponding with the Faculty of Computer Science of the Complutense University of Madrid (UCM), with the analysis of flooding attacks injected by the tool DDoSIM (DoS (2016c)). The generated attacks act at application layer, and are based on the massive send of different HTTP and TCP requests. During its course, DDoSIM simulates the behavior of various zombie computers using random assignment of IP addresses, which are able to log into the victim servers. Once established, it proceeds to the flooding of requests. The captured traffic has been divided into two groups: legitimate and malicious. Both contain samples with traces of 40,000 packets in format pcap. The first one is applied to calculate the false positive rate of the approach. The other is to determine the hit rate.",
  "To evaluate the effectiveness of the deployment of the AIS, a simulator ca-": "pable of generating traffic distributions and different networks with different locations of DH and DA has been implemented. This is because none of the functional standards for the evaluation of similar systems provides a complete knowledge of the organization of various networks. In the generation of new networks, several parameters had been taken into account: number of nodes, legitimate traffic volume, branching component, and cyclic component. The last two determine the number of connections associated with each node and the number of cycles in the network when it is plotted as a graph of finite dimensions. On the other hand a tool for simulating flooding attacks has been developed. Given a network built by the previously described scheme,",
  "In a summary of the tasks involved in the creation of each of the": "networks in the experiments is shown. In a first step, the network topology and the distribution of agents are defined. The network is built according to the previously described parameters, and is represented by a graph where the vertices are its nodes, and the edges are its connections. Then the origin and the destination of the attacks are defined. The location of immune agents is defined by greedy graph coloring (Galinier and Hertz (2006)), where the two most frequent calculated colors represent the class of actors. Thus the amount of DA sensors dependent of each DH is regulated. The second level defined how the traffic is generated. The legitimate communications are randomly decided by taking into account the simulation parameters. The injection of traffic is carried out by the tool hping3. In the case of clean traffic, various protocols (FTP, HTTP, ICMP, etc.) and actions (transfer of files, requests, session maintenance, etc.) are performed. With all of this, a script that allows the deployment of the network in a virtualized environment is",
  "At the evaluation of the effectiveness of the artificial immune agents when": "detecting threats, the adjustment parameters of the detectors are the variable that defines the order of the traffic entropy, and the number of packets per observation. Variations on the first of them have a similar result with the three benchmarks: When higher is the value, the higher the level of restriction of the sensor. In such a way that when > 3, the deployment of the AIS is counterproductive because the false positive rate becomes excessively high (exceeding 20%). In general terms, the smaller the value of , the lower the rate of false positives. For this reason it was considered = 1 along the experimentation. Bearing this in mind, the following describes and discusses the results obtained by analyzing KDD99, CAIDA07/08 and DDoSIM injection against habitual UCM traffic.",
  "In the results obtained by analyzing the collection KDD99 are": "shown. The X axis displays the number of packets per observation, whereas the Y axis indicates the True Positive Rate (TPR) and False Positive Rate (FPR). The hit rate has remained nearly unchanged in all the tests. However, the amount of errors in analyzing legitimate traffic is especially sensitive to the position in X; with fewer packets per sample, the system behaves more restrictively. Its ability in detection ceases to depend on it from 2,000 observations, at which it operates in saturation mode. This reason has led to study in more detail the two class of flooding attacks contained in more than",
  "In the results obtained in the experimentation with UCM traffic and": "DDoSIM are shown. They remain the behavior of the previous tests, reaching saturation in 2,000 packets per observation. However, the accuracy obtained is considerably worse: the hit rate is 92.3% and the false positive rate is 8.3%. The difference precision achieved demonstrates that the good results obtained when applying the evaluation functional standards are not scalable to real networks. This is because the homogeneity of the traffic analyzed is much higher, according to the current usage models. But despite the high false positive rate, the quality of service of the protected environment will not be affected. The proportional increase in the rigor with which act the agents of the adaptive immune response and the specificity of the approach, allow that most of the legitimate traffic involved in the emission of false positives reach their destination by alternative ways; this will be unusual when dealing with malicious traffic.",
  "The experimentation also emphasizes another important feature of the": "proposed method: most of the identified attacks have triggered alerts with proximity to the beginning and end of the malicious flooding. These are the observations where variations of entropy differ most from the legitimate traf- fic. When the attack is constant (as example, due to high rate attacks), the entropy tends to stabilize again, becoming invisible to the detector. Such be- havior is illustrated more clearly in . It shows the impact on the traffic entropy of a distributed attack generated by DDoSIM against the UCM net- work. The X axis displays the observations and the Y axis indicates the value of the normalized entropy. The attack starts from the observation 60. En- tropy anomalies are particularly visible on the following 15-20 observations, where the forecast exceeds the prediction thresholds, and thus the sensor re- ports an incidence. If mitigation measures (such as the packet drop applied in ) are not adopted, the entropy is stabilized, albeit with much higher values. Most likely the attack will not be visible again until completion. At that time 15-20 observations reveal the descent of the entropy to their usual",
  "In the results when DH and DA are triggered in different place-": "ments are shown. Y axis indicates the TPR/FPR and the X axis the location of the agents. The latter is indicated by a value in range 0 to 1, which de- termines the distance in the path between source and victim, where 0 is the location of the source, and 1 the location of the victim.",
  "When new attacks are launched, the average TPR is 0.85 and the FPR": "is 0.072. There is a trend: near the ends, the TPR value approaches 1. However, at intermediate points the accuracy is reduced by up TPR is 0.70. This occurs in the points closest to the equidistant location between the ends. When the DA agents are activated by the immune adaptive response, the trend is repeated. Nevertheless the average TPR was increased by 7%. The greatest improvement is observed at intermediate distances. The worst TPR value is 0.82, which implies the improvement of 12%. The adaptive immunity response has low impact on the FPR, i.e., it increased 0.6% at the worst case.",
  "In view of these results, and assuming that most of the conventional": "IPS behave similarly to the innate response, it can be stated that the pro- posed AIS poses a significant improvement in the cases where they find more difficulties to operate adequately. This occurs at the nodes above halfway between the attacker and the victim, mainly because the malicious traffic can spread over a larger number of alternative paths; near the ends they tend to converge (close to the victim) or diverge (close to the attacker).",
  "In the accuracy of the system depending on the flooding potency": "is observed. The Y axis details the TPR/FPR and the X axis indicates the power of the attacks. The last is represented in values in the range of 0 to 1, which correspond to the percentage of the bandwidth that aims to occupy, with 0 being no traffic injection and 1 means the complete saturation of the connections. As shown, when the attack is more powerful than 0.5, the accuracy in both responses is similar and close to 1. Therefore, changes are irrelevant. However, when less noisy attacks, they are more difficult to be detected. These cases are where the adaptive response improves the performance of the system. At the best case, the TPR has increased by 26.5% in the power range 0.3 to 0.4. In summary, the higher the power of the attacks, the greater the noise caused, and therefore the threats are easier to detect. When attacks are less visible, a more significant improvement of the proposed AIS on the conventional IPS is observed.",
  "In the detection capability in function of the volume of legiti- mate": "traffic flowing through the networks is shown. The X axis details the TPR/FPF, and the Y axis indicated the legitimate traffic volume. The last is indicated with values in range 0 to 1. As it was done in the above test, X represents the saturation level of the network. Results are very similar to those in . When the traffic density is low, the proposed strategy is very accurate, as is the case of the conventional schemes. This is because the attacks are much more visible, taking a larger share of bandwidth. However,",
  "In the rate of attacks that have not reached their destinations,": "depending on the number of nodes in their paths, is observed. When acting solely the innate response, 81.4% of the threats had been blocked at the worst case. However, the adaptive response was able to prevent 95.5% of them, i.e., has improved accuracy by 14.1%. From the figure it also follows that the",
  "The system has been evaluated in two stages. Firstly, the accuracy of the": "detection methods was measured taking into account the functional stan- dards KDD99, CAIDA07 and recent traffic of the UCM network compro- mised by the tool DDoSIM. The results obtained were satisfactory, empow- ering their collaboratively deployment. On the other hand, their efficiency as AIS has been evaluated. This has entailed its implementation on differ- ent virtualized networks and the assessment of their effectiveness based on different parameters. Regardless of the criteria from which the behavior of the AIS has been evaluated (location of the immune agents, flooding power, network congestion or mitigation), the adaptive response has always shown more effectiveness than the innate response. This results in a significant improvement in their ability to detect and mitigate attacks, without penal- ization in their error rates when processing legitimate traffic. The innate response behaves in the same way as the conventional IPS, so in the adap- tive reactions it is possible to study the raw effectiveness of the approach over the conventional mitigation schemes. Bearing these in mind, it is possible to confirm that the emulation of the biological immune responses is a very good way to enhance the classical countermeasures against DoS attacks."
}