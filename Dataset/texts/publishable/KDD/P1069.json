{
  "ABSTRACT": "Time Series Representation Learning (TSRL) focuses on generatinginformative representations for various Time Series (TS) model-ing tasks. Traditional Self-Supervised Learning (SSL) methods inTSRL fall into four main categories: reconstructive, adversarial,contrastive, and predictive, each with a common challenge of sensi-tivity to noise and intricate data nuances. Recently, diffusion-basedmethods have shown advanced generative capabilities. However,they primarily target specific application scenarios like imputationand forecasting, leaving a gap in leveraging diffusion models forgeneric TSRL. Our work, Time Series Diffusion Embedding (TSDE),bridges this gap as the first diffusion-based SSL TSRL approach.TSDE segments TS data into observed and masked parts using anImputation-Interpolation-Forecasting (IIF) mask. It applies a train-able embedding function, featuring dual-orthogonal Transformerencoders with a crossover mechanism, to the observed part. Wetrain a reverse diffusion process conditioned on the embeddings,designed to predict noise added to the masked part. Extensive ex-periments demonstrate TSDEs superiority in imputation, interpo-lation, forecasting, anomaly detection, classification, and clustering.We also conduct an ablation study, present embedding visualiza-tions, and compare inference speed, further substantiating TSDEsefficiency and validity in learning representations of TS data.",
  "Zineb Senane and Lele Cao contributed equally as first authors. For correspondence,please reach out to either of them. The source code and models for reproductionpurposes are available at": "Permission to make digital or hard copies of part or all of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for third-party components of this work must be honored.For all other uses, contact the owner/author(s).KDD 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s).ACM ISBN 979-8-4007-0490-1/24/08.",
  "INTRODUCTION": "Time Series (TS) data is a sequence of data points collected at regulartime intervals. It is prevalent in various real-world applications,such as understanding human behavioral patterns , conductingin-depth financial market analyses , predicting meteorologicalphenomena , and enhancing healthcare diagnostics . Inthis work, we focus on Multivariate TS (MTS) data, which refersto a TS with multiple variables or features recorded at each timepoint, where these variables may have inter-dependencies. This isin contrast to Univariate TS (UTS), which only involves a singlevariable. It should be noted that Multiple TS (Multi-TS) differs fromMTS as it pertains to the simultaneous monitoring of several UTSs,each operating independently without any interrelation amongthem. While this paper primarily concentrates on MTS data, ourmethodology and insights are also applicable to UTS and Multi-TS,ensuring the versatility and broad applicability of our approach.To effectively extract and interpret valuable information fromintricate raw MTS data, the field of Time Series RepresentationLearning (TSRL) has become increasingly pivotal. TSRL focuses onlearning latent representations that encapsulate critical informationwithin the time series, thereby uncovering the intrinsic dynamicsof the associated systems or phenomena . Furthermore, the",
  "KDD 24, August 2529, 2024, Barcelona, SpainZineb Senane and Lele Cao et al": "0.5 1.0 1.5 2.0 2.5 value x100 1.0 1.2 1.5 x100 0.4 0.6 0.8 1.0 x100 0.5 1.0 1.5 2.0 2.5 x100TSDE5%-95% quantilesMissingObserved 0.8 0.9 1.0 1.1 1.2 value x100 3.4 3.6 3.8 4.0 x10 3.0 4.0 x100 0.4 0.6 0.8 x1000 1.2 1.4 1.6 1.8 value x10 1.0 2.0 x1000 2.0 3.0 4.0 x100 1.0 1.5 2.0 x1000 2.0 2.5 3.0 3.5 4.0 value x1000 0.8 1.0 1.2 1.4 1.6 x100 0.4 0.6 0.8 x1000 0.4 0.6 0.8 1.0 x1000 0.8 1.0 1.2 1.4 value x100 0.6 0.8 1.0 1.2 x100 0.5 1.0 1.5 2.0 x1000 0.6 0.8 1.0 1.2 x100 0.4 0.6 0.8 1.0 value x100 0.6 0.8 1.0 1.2 x1000 1.0 1.5 2.0 2.5 3.0 x100 0.5 1.0 1.5 2.0 x1000 0.4 0.6 0.8 1.0 value x100 3.0 4.0 5.0 6.0 x100 4.0 6.0 8.0 x10 0.8 1.0 1.2 1.4 x1000 3.0 4.0 5.0 value x10 3.0 4.0 x10 1.0 1.5 2.0 x100 3.0 4.0 5.0 6.0 x10 2.0 3.0 4.0 value x10 0.8 1.0 1.2 1.4 1.6 x1000 2.0 3.0 4.0 5.0 x100 0.6 0.8 1.0 1.2 1.4 x1000 time 0.5 1.0 1.5 value x100000 time 1.8 2.0 x10000 time 0.8 1.0 1.2 1.4 x100 time 0.4 0.6 0.8 1.0 x1000 : Prediction visualization for Electricity dataset, interpolation task. The blue line and the red dashed line indicate themedian of the generated samples. The red shade represents 5%-95% quantiles for the missing values.",
  "RELATED WORK": "This research addresses the problem of TSRL using a SSL approach.Inspired by the taxonomies adopted by , we structureour review of SSL-based TSRL around four primary methodologies:reconstructive, adversarial, contrastive, and predictive methods. 1A pretext task in SSL is a self-generated learning challenge designed to facilitate theextraction of informative representations for downstream tasks, encompassing variousmethods such as transformation prediction, masked prediction, instance discrimination,and clustering, tailored to the specific data modality involved . Reconstructive methods focus on minimizing the discrepancybetween original and reconstructed MTS data, mostly using anencoder-decoder Neural Network (NN) architecture to emphasizesalient features and filter out noise, thereby training the NN to learnmeaningful representations . Recent mainstream methods inthis category predominantly employ Convolutional NN (CNN) , Recurrent NN (RNN) or Transformer as theirarchitectural backbone. In this category, deep clustering standsout by simultaneously optimizing clustering and reconstructionobjectives. It has been implemented through various clusteringalgorithms, including -means , Gaussian Mixture Model(GMM) , and spectral clustering . Reconstructive meth-ods might face limitations in addressing long-term dependenciesand adequately representing complex features such as seasonality,trends, and noise in extensive, high-dimensional datasets.Adversarial methods utilize Generative Adversarial Network(GAN) to learn TS representations by differentiating between realand generated data . These methods often integrate ad-vanced NN architectures or autoregressive models to effectivelycapture temporal dependencies and generate realistic TS data. Forinstance, TimeGAN combines GANs with autoregressive mod-els for temporal dynamics replication, while RGAN uses RNNto enhance the realism of generated TS. Furthermore, approacheslike TimeVAE and DIVERSIFY innovate in data generation,with the former tailoring outputs to user-specified distributions andlatter employing adversarial strategies to maximize distributional di-versity in generated TS data. However, the intricate training processof GANs, potential for mode collapse, and reliance on high-qualitydatasets are notable drawbacks of adversarial methods, potentiallygenerating inconsistent or abnormal samples .Contrastive methods distinguish themselves by optimizingself-discrimination tasks, contrasting positive samples with similarcharacteristics against negative samples with different ones .These methods learn representations by generating augmentedviews of TS data and leveraging the inherent similarities and vari-ations within the data . They include instance-level mod-els that treat each sample independently, usingdata augmentations to form positive and negative pairs. Prototype-level models , on the other hand, break this indepen-dence by clustering semantically similar samples, thereby captur-ing higher-level semantic information. Additionally, temporal-levelmodels address TS-specific challenges by focusing onscale-invariant representations at individual timestamps, enhanc-ing the understanding of complex temporal dynamics. However,a common disadvantage across these contrastive methods is theirpotential to overlook higher-level semantic information, especiallywhen not integrating explicit semantic labels, leading to the gener-ation of potentially misleading negative samples.Predictive methods excel in capturing shared information fromTS data by maximizing mutual information from various data slicesor augmented views. These methods, like TST , wave2vec ,CaSS and SAITS , focus on predicting future, missing, orcontextual information, thereby bypassing the need for full inputreconstruction. Most recent advancements in this category, suchas TEMPO and TimeGPT , leverage LLM (Large LanguageModel) architectures to effectively decompose and predict complexTS components. TimeGPT, in particular, stands out as a foundation",
  "Self-Supervised Learning of Time Series Representation via Diffusion Process and Imputation-Interpolation-Forecasting MaskKDD 24, August 2529, 2024, Barcelona, Spain": "0.5 1.0 value x100 0.8 1.0 1.2 x100 0.4 0.6 0.8 1.0 x100 0.5 1.0 1.5 2.0 x100TSDE5%-95% quantilesMissingObserved 0.2 0.4 0.6 0.8 value x100 3.4 3.6 3.8 4.0 x10 3.0 4.0 5.0 6.0 7.0 x100 0.4 0.6 0.8 1.0 x1000 1.2 1.4 1.6 1.8 2.0 value x10 2.0 4.0 6.0 x1000 2.0 3.0 4.0 x100 1.0 1.5 2.0 x1000 2.0 2.5 3.0 value x1000 0.8 1.0 1.2 1.4 1.6 x100 0.4 0.6 0.8 x1000 0.4 0.6 0.8 1.0 x1000 0.8 1.0 1.2 1.5 1.8 value x100 0.6 0.8 1.0 1.2 x100 0.5 1.0 1.5 2.0 x1000 0.6 0.8 1.0 1.2 x100 0.4 0.6 0.8 1.0 value x100 0.6 0.8 1.0 1.2 1.4 x1000 1.0 1.5 2.0 2.5 3.0 x100 0.5 1.0 1.5 2.0 2.5 x1000 0.4 0.6 0.8 1.0 1.2 value x100 3.0 4.0 5.0 x100 0.4 0.6 0.8 1.0 x100 0.8 1.0 1.2 1.4 x1000 2.0 3.0 4.0 5.0 value x10 2.0 4.0 6.0 x10 1.0 1.5 2.0 x100 3.0 4.0 5.0 6.0 x10 2.0 3.0 4.0 value x10 0.8 1.0 1.2 x1000 2.0 3.0 4.0 5.0 x100 0.6 0.8 1.0 1.2 x1000 time 0.5 1.0 1.5 value x100000 time 1.8 2.0 x10000 time 0.6 0.8 1.0 1.2 1.4 x100 time 0.4 0.6 0.8 1.0 1.2 x1000 : Prediction visualization for Electricity dataset, forecasting task. The blue line and the red dashed line indicate themedian of the generated samples. The red shade represents 5%-95% quantiles for the forecasted future values.",
  "Imputation-Interpolation-Forecasting Mask": "The reverse process of unconditional diffusion facilitates the genera-tion of MTS from noise. However, our objective is to create general-purpose embeddings for unlabeled MTS, which can be leveragedin many popular downstream tasks such as imputation, interpo-lation, and forecasting. Consequently, we propose an Imputation-Interpolation-Forecasting (IIF) mask strategy, producing a pseudoobservation mask mIIF = {IIF1:,1:} {0, 1} where IIF, =1 if ,in Equation (1) is observable, and IIF, =0 otherwise. Algorithm 1details the implementation and combination of imputation, interpo-lation, and forecasting masks2. During training, given any originalMTS x0, we extract the observed (xobs0 ) and masked (xmsk0) segmentsbyxobs0:= x0 mIIF andxmsk0:= x0 (m mIIF) ,(10)where represents element-wise product; and m = {1:,1:} {0, 1} is a mask with zeros indicating originally missing valuesin x0. We now reformulate our self-supervised learning objective togenerate the masked version of MTS, denoted as xmsk0, from a cor-rupted input xmsk, through a diffusion process, conditioned on 2The imputation mask simulates random missing values; the interpolation mask mimicsthe MTS interpolation tasks by masking all values at a randomly selected timestamp;and the forecasting mask assumes all values post a specified timestamp unknown.",
  "Output": "? sumL128K16KL KLKL16 KL16 KL16 KL33 KL11128 KL64 KL128KL Conditional Reverse Diffusion (denoising):Embedding Function/Block: Z= KL160 f (x 0 ) obs ?(m -m )IIFf (x 0 )) obs ?(x t , t| msk?? f (x 0 ) obs ? Rsidual layer 1 Rsidual layer N SplitSplit KL160 (L160)K(K160)L ConcatConcat (L160)K(K160)L KL160KL160 KL KL160 KL160 KL64 KL64 KL128 KL64 KL64 KL64 KL64 KL64 stimesfeat expand expand x 0obs~x 0obs~ sdiff(t)",
  "Embedding Function": "The left part of illustrates the architectural design of theembedding function f (xobs0). This figure highlights that the func-tion not only processes the input xobs0 , but also incorporates ad-ditional side information (namely, time embedding stime(), fea-ture embedding sfeat(), and the mask mIIF) into its computations.Consequently, the notation f (xobs0) is succinctly used to repre-sent the more extensive formulation f (xobs0 , stime, sfeat, mIIF), whichaccounts for all the inputs processed by the function. To obtain128-dimensional stime(), we largely follow :",
  "xobs0= (((xobs0)), stime, sfeat),(18)": "where (), () and () represent concatenation,ReLu activation, and 11 convolution operation respectively.To accurately capture the inherent temporal dependencies andfeature correlations in MTS data, thereby enabling clearer data in-terpretation and a customizable, modular design, we devise separatetemporal and feature embedding functions: g ( xobs0) and h ( xobs0),parameterized by and respectively. Inspired by , both thetemporal g () and feature g () encoders are simply implementedas a one-layer Transformer encoder that takes an input tensorshaped 160, as shown in . Specifically, the temporalencoder operates on tensors shaped 1160, representing a fea-ture across all timestamps; and the feature encoder handles tensors",
  "return mIIF;": "shaped 1160, representing a feature vector corresponding to atime stamp.To integrate temporal and feature embeddings in varying orderswithout adding to the models trainable parameters, we have de-veloped a crossover mechanism. This mechanism is depicted bythe red and blue arrows in . It facilitates the generationof g (h ( xobs0)) and h (g ( xobs0)), which are subsequently trans-formed and concatenated along with mIIF, resulting in the finalembedding Z = f (xobs0) :=",
  "The Overall Architecture": "provides a comprehensive depiction of the various com-ponents within the TSDE architecture. The process begins by ap-plying the IIF mask mIIF to partition the input MTS into observable(xobs0 ) and masked (xmsk0) segments. The entire architecture primarilyconsists of two key elements: (1) an embedding function f (xobs0)thoroughly introduced in .5; and (2) a conditional reversediffusion module, illustrated on the right side of .The conditional reverse diffusion, introduced in .3, func-tions as a noise predictor, effectively implementing (xmsk, |f (xobs0)).During the -th training step, as outlined in Algorithm 2, the sam-pled diffusion step is first transformed into a 128-dimensionalvector, denoted as sdiff() :=sin(100463 ), . . . , sin(1063463 ), cos(100463 ), . . . , cos(1063463 ). (20) Subsequently, the MTS embedding Z, along with sdiff() and xmsk0,are input into a residual block composed of residual layers. Theoutputs of these layers are aggregated (summation), processedthrough some transformations, and combined with xmsk. This re-sults in (xmsk, |f (xobs0)) (mmIIF), which is then utilized tocompute the loss L(, ), as formulated in Equation (16).",
  "return and ;": "and classification as demonstrated in .2, 4.3, and 4.4, re-spectively. (2) When combined with the trained conditional reversediffusion process, the model is capable of predicting missing val-ues (for imputation and interpolation) as well as future values (forforecasting) in MTS data. In the second scenario, a notable increasein speed can be achieved compared to the existing diffusion-basedmethods such as those in . This efficiency, confirmed in Sec-tion 4.1.5, stems from simplifying the conditional reverse diffusion(the right block of , i.e., ) to use only Conv11 operators.This streamlining significantly accelerates the =50 steps reversediffusion process.",
  "Imputation, Interpolation and Forecasting": "4.1.1Imputation. We carry out imputation experiments on Phys-ioNet3 and PM2.54 . TSDE is benchmarked against severalstate-of-the-art TS imputation models. These include BRITS , adeterministic method using bi-directional RNN for correlation cap-ture; V-RIN , employing variational-recurrent networks withfeature and temporal correlations for uncertainty-based imputa-tion; GP-VAE , integrating Gaussian Processes with VAEs; andCSDI , the top-performing model among the diffusion-basedTS imputation models. The model performance is evaluated us-ing continuous ranked probability score (CRPS) to assess the fitof predicted outcomes with original data distributions, and twodeterministic metrics mean absolute error (MAE) and the rootmean square error (RMSE). Deterministic metrics are calculatedusing the median across all samples, and CRPS value is reported as 3PhysioNet, a healthcare dataset with 4,000 records of 35 variables over 48 hours, isprocessed and hourly sampled as , leading to 80% missing rate. For testing, werandomly mask 10%, 50%, and 90% of observed values to create ground-truth scenarios.On this dataset, we pretrain TSDE for 2,000 epochs, followed by a 200-epoch finetuningwith an imputation mask.4PM2.5, an air quality dataset, features hourly readings from 36 Beijing stations over 12months with artificially generated missing patterns. Adapting , each series spans36 consecutive timestamps. On this dataset, we pretrain for 1,500 epochs and finetunefor 100 epochs using a history mask as detailed in Algorithm 5.",
  "Interpolation": "Latent ODE 0.700(0.002)0.522(0.002)0.799(0.012)0.676(0.003)0.506(0.003)0.783(0.012)0.761(0.010)0.578(0.009)0.865(0.017)* Results reproduced using GP-VAEmTANs 0.526(0.004)0.389(0.003)0.749(0.037)0.567(0.003)0.422(0.003)0.721(0.014)0.689(0.015)0.533(0.005)0.836(0.018)original implementation available atCSDI 0.380(0.002)0.362(0.001)0.722(0.043)0.418(0.001)0.394(0.002)0.700(0.013)0.556(0.003)0.518(0.003)0.839(0.009) 0.331(0.001) 0.597(0.002) 0.403(0.001) 0.371(0.001) 0.657(0.001) 0.517(0.001) 0.476(0.001) 0.775(0.001)We report the mean and standardTSDE+ft0.374(0.001)0.338(0.001)0.610(0.003)0.421(0.001)0.385(0.001)0.677(0.003)0.570(0.004)0.522(0.006)0.821(0.006)deviation of three runs. the normalized average score for all missing values distributions(approximated with 100 samples).The imputation results, as detailed in the upper part of ,highlight TSDEs superior performance over almost all metrics,outperforming all baselines. Notably, the pretraining-only variant(i.e., TSDE) excels on the PhysioNet dataset, underpinning its ro-bustness and enhanced generalization capability, even without theneed of any imputation-specific finetuning. For the PM2.5 dataset,finetuning TSDE (i.e., TSDE+ft) yields improved outcomes, likelyattributable to its capability to adapt to the datasets structuredmissing value patterns. Overall, TSDEs improvement in CRPSby 4.2%-6.5% over CSDI, a leading diffusion-based TS imputationmodel, signifies a notable advancement in the field. For a qualitativeillustration of imputation results, refer to (a). 4.1.2Interpolation. For interpolation analysis, we utilized the samePhysioNet dataset , adopting the processing methods from . Ground truth scenarios were created by masking all valuesat randomly selected timestamps, sampled at rates of 10%, 50% and90%. TSDE is pretrained for 2,000 epochs, and then further fine-tuned using an interpolation-only mask for another 200 epochs. Inour benchmarking, TSDE is compared against three TS interpola-tion methods: (1) Latent ODE , an RNN-based model leveragingODE (ordinary differential equation) for dynamic, continuous andirregular TS handling; (2) mTANs , utilizing time embeddingsand attention mechanisms, noted for its strong performance in ir-regular TS interpolation; and (3) CSDI which has also reportedcompetitive result in interpolation tasks.The results in the lower section of demonstrate TSDEsexceptional performance in interpolation, outperforming CSDI by3.6%-7.0% in CRPS, 5.8%-8.6% in MAE, and 6.1%-17.3% in RMSE.These findings highlight TSDEs adeptness in managing irregulartimestamp gaps, a likely factor behind the observation that fine-tuning does not enhance the pretraining-only TSDEs performance.Comparatively, while CSDI also operates on a similar diffusionmodel backbone, TSDEs edge lies in its unique embedding learningability via IIF masking, adeptly capturing intricate TS characteris-tics and dynamics for improved results. A qualitative illustration ofinterpolation results can be found in (b). 4.1.3Forecasting. We conducted two sets of benchmarking exper-iments. The first was a benchmarking for probabilistic multivariatetime series forecasting. We employ five real-world datasets: (1)Electricity, tracking hourly consumption across 370 customers; (2)Solar, detailing photovoltaic production at 137 Alabama stations; (3)Taxi, recording half-hourly traffic from 1,214 New York locations;(4) Traffic, covering hourly occupancy rates of 963 San Franciscocar lanes; and (5) Wiki, monitoring daily views of 2,000 Wikipediapages. Adapting the practices from , each dataset is con-verted into a series of multivariate sequences, with 1 historicaltimestamps followed by 2 timestamps for forecasting. Trainingdata apply a rolling window approach with a stride of 1, whilevalidation and testing data employ a stride of 2, ensuring distinct,non-overlapping series for evaluation. Specific 1 and 2 values areoutlined in . For evaluation metrics, we use CRPS and MSE,supplemented by CRPS-Sum, as introduced in . CRPS-Sum iscomputed by summing across different features, capturing the jointimpact of feature distributions. As of benchmarking baselines, weinclude several state-of-the-art probabilistic MTS forecasting models:GP-copula , TransMAF and TLAE . Additionally, inthe realm of diffusion-based methods, we include CSDI andTimeGrad .For the second benchmarking, which is a deterministic bench-marking including recent baselines in the time series library ,we conducted five experiments for each baseline following thesame setting in with history-prediction window lengths of{8-8, 16-16, 32-32, 96-96, 96-192} on the Electricity dataset. We re-port the averaged performance in terms of MAE and MSE. Wecompared TSDE with the following baselines: TimesNet , ETS-former , LightTS , DLinear , FEDformer , Non-stationary Transformer , Autoformer , Pyraformer ,Informer , Reformer , and PatchTST .The forecasting results, as detailed in , showcase TSDEsrobust performance, especially when finetuned with a forecastingmask. Its effectiveness is notable when compared to CSDI, whichis the most closely related method, sharing a diffusion backbone.TSDE particularly excels in the Electricity, Taxi, and Wiki datasets,especially as evaluated by the CRPS metric. However, it is importantto note a discrepancy in the Solar dataset performance between",
  "* We replace the linear Transformers in CSDI with the Pytorch TransformerEncoder . We take the training MTS dataset and split it into training, validation and testing sets": "TSDE/CSDI and other baselines, likely due to a data split issue: theactual test set, per the source code, is identical to the training set,which contradicts the details reported in the corresponding paper. demonstrates that TSDE outperforms the recent baselinesin terms of average MSE and MAE, highlighting its robustness andsuperiority compared to recent methods. The detailed results foreach window length are available in the appendix, . For aqualitative illustration, refer to (c). 4.1.4Ablation Study. In an ablation study on TSDE across impu-tation, interpolation, and forecasting, evaluated on PhysioNet (10%missing ratio) and Electricity datasets, two configurations weretested: one without crossover, and another without IIF mask (re-placed by an imputation mask detailed in Algorithm 4). underscores the positive contribution of the crossover mechanismacross all three tasks. The impact of IIF masking, while less pro-nounced for imputation and interpolation, becomes noticeable inthe forecasting task. This can be attributed to the random PhysioNetmissing values, which are distributed fundamentally differentlyfrom a typical forecasting scenario. Thus, IIF strategy is importantfor TSDE to gain a generalization ability across various settings.",
  "Imputation(MAE/CRPS)Interpolation(MAE/CRPS)Forecasting(CRPS-sum/CRPS)": "w/o crossover0.252(0.001)/0.274(0.001) 0.339(0.000)/0.373(0.000)0.021(0.001)/0.046(0.001)w/o IIF mask0.207(0.001)/0.225(0.001) 0.330(0.001)/0.364(0.001)0.028(0.004)/0.053(0.003)TSDE0.208(0.001)/0.226(0.002) 0.331(0.001)/0.365(0.001) 0.020(0.001)/0.043(0.000) 4.1.5Inference Efficiency. Similar to CSDI , TSDE performsinference by gradual denoising from the last diffusion step =50to the initial step =1, to approximate the true data distribution ofmissing or future values for imputation/interpolation/forecastingtasks. Typically, this iterative process can become computationallyexpensive. TSDE achieves a substantial acceleration in this processas illustrated in , where TSDE is ten times faster than CSDIunder the same experimental setup. This is primarily owing to itsglobally shared, efficient dual-orthogonal Transformer encoderswith a crossover mechanism, merely requiring approximately aquarter of the parameters used by CSDI for MTS encoding.",
  "Anomaly Detection": "For anomaly detection, we adopt an unsupervised approach us-ing reconstruction error as the anomaly criterion, aligning with. We evaluate TSDE on five benchmark datasets: SMD ,MSL , SMAP , SWaT and PSM . Once TSDE is pre-trained, a projection layer, designed to reconstruct MTS from TSDEembeddings, is finetuned by minimizing MSE reconstruction loss.Our anomaly detection experiments align with TimesNet ,utilizing preprocessed datasets from . Following their method,we segment datasets into non-overlapping MTS instances of 100timestamps each, labeling timestamps as anomalous based on a MSEthreshold. This threshold is set according to the anomaly propor-tion in the validation dataset, ensuring consistency with baselineanomaly ratios for a fair comparison.In this task, TSDE is benchmarked against an extensive setof baselines featuring diverse backbones, including a) Frozen pre-trained LLM-based models: GPT4TS ; b) Task-agnostic founda-tion models: TimesNet ; c) MLP (multi-layer perceptron) based",
  "(a) Imputation on PhysioNet": "0.4 0.5 1.0 1.5 2.0 x1000 0.6 0.8 1.0 1.2 x100 1.0 1.5 2.0 2.5 3.0 x100 0.5 1.0 1.5 2.0 x1000 4.0 6.0 8.0 x10 0.8 1.0 1.2 1.4 x1000 1.0 1.5 2.0 x100 3.0 4.0 5.0 6.0 x10 2.0 3.0 4.0 5.0 x100 0.6 0.8 1.0 1.2 1.4 x1000 me time 0.8 1.0 1.2 1.4 x100",
  "(b) Interpolation on Electricity": "0.6 0.5 0.6 0.6 0.8 1.0 1.2 1.4 x1000 1.0 1.5 2.0 2.5 3.0 x100 0.5 1.0 1.5 2.0 2.5 x1000 3.0 4.0 5.0 x100 0.4 0.6 0.8 1.0 x100 0.8 1.0 1.2 1.4 x1000 2.0 4.0 6.0 x10 1.0 1.5 2.0 x100 3.0 4.0 5.0 6.0 x10 0.8 1.0 1.2 x1000 2.0 3.0 4.0 5.0 x100 0.6 0.8 1.0 1.2 x1000 ime time 1.8 2.0 x10000 time 0.6 0.8 1.0 1.2 1.4 x100",
  "Logistic Regression (LR) on imputed PhysioNet data.* Train Random Forest (RF) on imputed PhysioNet data": "models: LightTS and DLinear ; and finally d) Transformer-based models: Transformer , Reformer , Informer , Aut-oformer , Pyraformer , LogSparse Transformer , FED-former , Non-stationary Transformer , ETSformer ,PatchTST and Anomaly Transformer . The results in Ta-ble 6 reveal that TSDEs anomaly detection performance surpassesnearly all baselines, with less than a 1% F1 score difference fromGPT4TS. Notably, while TSDE doesnt outperform GPT4TS, itsimportant to consider that GPT4TS benefits from a pretrained LLM(GPT-2) with about 1.5 billion model parameters. TSDE, in contrast,relies on just two single-layer Transformer encoders (<0.3 millionparameters), demonstrating its competitive edge despite havingsignificantly fewer model parameters.",
  "Classification": "To further inspect the discriminative power of the pretrained TSDEembedding, we utilize the labeled PhysioNet dataset to evaluateTSDEs performance on a binary classification downstream task.This dataset, marked by in-hospital mortality labels for each pa-tient, features MTS with over 80% missing values. To address this,we pretrain TSDE for 2,000 epochs to impute the raw MTS. Subse-quently, we train a simple MLP for 40 epochs to perform mortalityclassification. Given the imbalanced nature of PhysioNet labels, we assess our models efficacy with AUROC as in . We bench-mark TSDE against a diverse range of established MTS classificationmethods, categorized into 3 groups with a total of 12 methods: (1)heuristic methods: mean/forward imputation , (2) GP/VAEbased models: GP , VAE , HI-VAE , GP-VAE , and(3) RNN based models: GRUI-GAN , GRU-D , M-RNN and BRITS variants .As shown in , TSDE surpasses all existing baselines and ison par with the state-of-the-art BRITS baseline. It is worth notingthat BRITS achieves that performance by employing a sophisticatedmulti-task learning mechanism tailored for classification tasks. Incontrast, our method achieves top-tier results by simply finetun-ing a simple MLP. TSDEs remarkable performance, especially inchallenging classification scenarios with significant class imbal-ance (10% positive classes), highlights its ability to learn genericembeddings well-suited for downstream MTS classification tasks.",
  "(c) Embed imputed MTS": ": Clustering of (a) raw MTS, (b) TSDE embedding of rawMTS, and (c) TSDE embedding of TSDE-imputed MTS. Marker shapesdenote ground-truth binary labels; colors indicate DBSCAN clusters after UMAP dimension reduction. As shown in , the clustering quality is assessed acrossthree data types: (a) raw MTS, (b) TSDE embeddings of raw MTS,and (c) TSDE embeddings of TSDE-imputed MTS. The ground truthbinary labels are indicated using two distinct marker shapes: circlesand triangles. When using raw MTS as seen in 3(a), the clustersare unfavourably intertwined, with data points from both classesintermingling. However, the TSDE embeddings, whether derivedfrom raw or imputed MTS, exhibit substantially improved clusterdifferentiation. These embeddings enable more precise alignmentwith ground truth classifications, implying the capability of TSDEin capturing data nuances. Furthermore, the negligible performancedisparity between Figures 3(b) and 3(c) suggests that TSDE embed-dings can be directly used for MTS clustering without the need ofimputation. This consistency is likely because our encoders pro-ficiently encapsulate missing data traits, seamlessly integratingthese subtleties into the embeddings. To provide a quantitativeassessment of clustering, given the presence of labels, we calcu-late RI (rand index) and ARI (adjusted RI) . These metricsare reported on top of each setup in . Notably, the RI andARI values align with the qualitative observations discussed earlier,further substantiating our findings.",
  "Embedding Visualization": "To substantiate the representational efficacy of TSDE embeddings,we undertake a visualization experiment on synthetic MTS data, asshowcased in . The data comprises three distinct UTS: (a) aconsistently ascending trend, (b) a cyclical seasonal signal, and (c) awhite noise component. Each UTS embedding has two dimensions( 33); for a lucid depiction, we cluster the second dimension bytreating it as 33 samples each of length , and visualize the centroidof these clusters. Intriguingly, the embeddings, which were pre-trained on the entire synthetic MTS, vividly encapsulate the jointencoding effects of all series. The trends embedding delineates the series progression, evident from the gradual color saturationchanges, embodying the steady evolution. The seasonal signalsembedding mirrors its inherent cyclicity, with color oscillationsreflecting its periodic nature. Finally, the noise components em-beddings exhibit sporadic color band patterns (with subtle tracesof seasonal patterns), capturing the inherent randomness. (a) Trend (b) Seasonal (c) Noise",
  "CONCLUSION": "In this paper, we propose TSDE, a novel SSL framework for TSRL.TSDE, the first of its kind, effectively harnesses a diffusion process,conditioned on an innovative dual-orthogonal Transformer encoderarchitecture with a crossover mechanism, and employs a uniqueIIF mask strategy. Our comprehensive experiments across diverseTS analysis tasks, including imputation, interpolation, forecast-ing, anomaly detection, classification, and clustering, demonstrateTSDEs superior performance compared to state-of-the-art models.Specifically, TSDE shows remarkable results in handling MTS datawith high missing rates and various complexities, thus validatingits effectiveness in capturing the intricate MTS dynamics. Moreover,TSDE not only significantly accelerates inference speed but alsoshowcases its versatile embeddings through qualitative visualiza-tions, encapsulating key MTS characteristics. This positions TSDEas a robust, efficient, and versatile advancement in MTS representa-tion learning, suitable for a wide range of MTS tasks. Future workwill focus on several key directions to address the limitation ofslower inference for IIF tasks. Particularly, we will explore simpli-fying TSDEs architecture with a simple MLP without the need forthe diffusion block, enabling the pretrained TSDE to execute IIFtasks independently. We would like to thank Yang Song (OpenAI & Stanford University)for his help to connect us with CSDI authors and the insightfuldiscussions regarding time series representation learning.R. Tu would like to acknowledge the support of Gustav EjeHenter, Hedvig Kjellstrm and the Wallenberg AI, AutonomousSystems and Software Program (WASP). R. Tu was also funded bythe Industrial Strategic Technology Development Program (grantno. 20023495) from MOTIE, Korea.",
  "Juan Lopez Alcaraz and Nils Strodthoff. 2023. Diffusion-based Time SeriesImputation and Forecasting with Structured State Space Models. Transactionson Machine Learning Research (2023)": "Defu Cao, Furong Jia, Sercan O Arik, Tomas Pfister, Yixiang Zheng, Wen Ye,and Yan Liu. 2023. TEMPO: Prompt-based Generative Pre-trained Transformerfor Time Series Forecasting. arXiv preprint arXiv:2310.04948 (2023). Lele Cao, Sahar Asadi, Wenfei Zhu, Christian Schmidli, and Michael Sjberg.2020. Simple, scalable, and stable variational deep clustering. In Joint EuropeanConference on Machine Learning and Knowledge Discovery in Databases. Springer,108124. Lele Cao, Sonja Horn, Vilhelm von Ehrenheim, Richard Anselmo Stahl, and Hen-rik Landgren. 2022. Simulation-informed revenue extrapolation with confidenceestimate for scaleup companies using scarce time-series data. In Proceedings ofthe 31st ACM international conference on information & knowledge management.29542963. Wei Cao, Dong Wang, Jian Li, Hao Zhou, Lei Li, and Yitan Li. 2018. BRITS:Bidirectional Recurrent Imputation for Time Series. In Advances in NeuralInformation Processing Systems, S. Bengio, H. Wallach, H. Larochelle, K. Grauman,N. Cesa-Bianchi, and R. Garnett (Eds.), Vol. 31. Curran Associates, Inc. Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. 2018.Deep clustering for unsupervised learning of visual features. In Proceedings ofthe European conference on computer vision (ECCV). 132149. Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, andArmand Joulin. 2020. Unsupervised learning of visual features by contrastingcluster assignments. Advances in neural information processing systems 33 (2020),99129924. Sravan Kumar Challa, Akhilesh Kumar, and Vijay Bhaskar Semwal. 2022. Amultibranch CNN-BiLSTM model for human activity recognition using wearablesensor data. The Visual Computer 38, 12 (2022), 40954109.",
  "Xinlei Chen, Zhuang Liu, Saining Xie, and Kaiming He. 2024. Deconstruct-ing Denoising Diffusion Models for Self-Supervised Learning. arXiv preprintarXiv:2401.14404 (2024)": "Yijiang Chen, Xiangdong Zhou, Zhen Xing, Zhidan Liu, and Minyang Xu. 2022.CaSS: A Channel-Aware Self-supervised Representation Learning Frameworkfor Multivariate Time Series Classification. In International Conference on Data-base Systems for Advanced Applications. Springer, 375390. Ranak Roy Chowdhury, Xiyuan Zhang, Jingbo Shang, Rajesh K Gupta, and DezhiHong. 2022. Tarnet: Task-aware reconstruction for time-series transformer. InProceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery andData Mining. 212220.",
  "Cristbal Esteban, Stephanie L Hyland, and Gunnar Rtsch. 2017. Real-valued(medical) time series generation with recurrent conditional GANs. arXiv preprintarXiv:1706.02633 (2017)": "Martin Ester, Hans-Peter Kriegel, Jrg Sander, Xiaowei Xu, et al. 1996. A density-based algorithm for discovering clusters in large spatial databases with noise.In Proceedings of ACM SIGKDD Conference on Knowledge Discovery and DataMining, Vol. 96. 226231. Vincent Fortuin, Dmitry Baranchuk, Gunnar Raetsch, and Stephan Mandt. 2020.GP-VAE: Deep Probabilistic Time Series Imputation. In Proceedings of the TwentyThird International Conference on Artificial Intelligence and Statistics (Proceedingsof Machine Learning Research, Vol. 108), Silvia Chiappa and Roberto Calandra(Eds.). PMLR, 16511661.",
  "Lawrence Hubert and Phipps Arabie. 1985. Comparing partitions. Journal ofClassification 2, 1 (1985), 193218": "Kyle Hundman, Valentino Constantinou, Christopher Laporte, Ian Colwell, andTom Soderstrom. 2018. Detecting Spacecraft Anomalies Using LSTMs andNonparametric Dynamic Thresholding. In Proceedings of the 24th ACM SIGKDDInternational Conference on Knowledge Discovery & Data Mining (London, UnitedKingdom) (KDD 18). Association for Computing Machinery, New York, NY,USA, 387395. Zhuxi Jiang, Yin Zheng, Huachun Tan, Bangsheng Tang, and Hanning Zhou.2017. Variational deep embedding: an unsupervised and generative approach toclustering. In Proceedings of the 26th International Joint Conference on ArtificialIntelligence. 19651972.",
  "Roderick J A Little and Donald B Rubin. 1986. Statistical analysis with missingdata. John Wiley & Sons, Inc., USA": "Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X. Liu, andSchahram Dustdar. 2022. Pyraformer: Low-Complexity Pyramidal Attention forLong-Range Time Series Modeling and Forecasting. In International Conferenceon Learning Representations. Xiao Liu, Fanjin Zhang, Zhenyu Hou, Li Mian, Zhaoyu Wang, Jing Zhang,and Jie Tang. 2021. Self-supervised learning: Generative or contrastive. IEEEtransactions on knowledge and data engineering 35, 1 (2021), 857876. Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long. 2022. Non-stationaryTransformers: Exploring the Stationarity in Time Series Forecasting. In Advancesin Neural Information Processing Systems, S. Koyejo, S. Mohamed, A. Agarwal,D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates, Inc., 98819893. Wang Lu, Jindong Wang, Xinwei Sun, Yiqiang Chen, and Xing Xie. 2022. Out-of-distribution Representation Learning for Time Series Classification. In TheEleventh International Conference on Learning Representations. Yonghong Luo, Xiangrui Cai, Ying ZHANG, Jun Xu, and Yuan xiaojie. 2018.Multivariate Time Series Imputation with Generative Adversarial Networks.In Advances in Neural Information Processing Systems, S. Bengio, H. Wallach,H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (Eds.), Vol. 31.Curran Associates, Inc.",
  "file/96b9bff013acedfb1d140579e2fbeb63-Paper.pdf": "Liantao Ma, Chaohe Zhang, Yasha Wang, Wenjie Ruan, Jiangtao Wang, WenTang, Xinyu Ma, Xin Gao, and Junyi Gao. 2020. Concare: Personalized clinicalfeature embedding via capturing the healthcare context. In Proceedings of theAAAI Conference on Artificial Intelligence, Vol. 34. 833840. Pankaj Malhotra, Vishnu TV, Lovekesh Vig, Puneet Agarwal, and GautamShroff. 2017. TimeNet: Pre-trained deep recurrent neural network for timeseries classification. In Proceedings of the European Symposium on ArtificialNeural Networks, Computational Intelligence and Machine Learning. 607612. Aditya P. Mathur and Nils Ole Tippenhauer. 2016. SWaT: a water treatmenttestbed for research and training on ICS security. In 2016 International Workshopon Cyber-physical Systems for Smart Water Networks (CySWater). 3136.",
  "Leland McInnes, John Healy, Nathaniel Saul, and Lukas Groberger. 2018. UMAP:Uniform Manifold Approximation and Projection. Journal of Open Source Soft-ware 3, 29 (2018), 861": "Mehran Mehralian and Babak Karasfi. 2018. RDCGAN: Unsupervised repre-sentation learning with regularized deep convolutional generative adversarialnetworks. In 2018 9th conference on artificial intelligence and robotics and 2ndAsia-pacific international symposium. IEEE, 3138. Qianwen Meng, Hangwei Qian, Yong Liu, Lizhen Cui, Yonghui Xu, and ZhiqiShen. 2023. MHCCL: masked hierarchical cluster-wise contrastive learningfor multivariate time series. In Proceedings of the AAAI Conference on ArtificialIntelligence, Vol. 37. 91539161.",
  "Alfredo Nazabal, Pablo M. Olmos, Zoubin Ghahramani, and Isabel Valera. 2020.Handling Incomplete Heterogeneous Data using VAEs. arXiv:1807.03653 [cs.LG]": "Nam Nguyen and Brian Quanz. 2021. Temporal latent auto-encoder: A methodfor probabilistic multivariate time series forecasting. In Proceedings of the AAAIConference on Artificial Intelligence, Vol. 35. 91179125. Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. 2023.A Time Series is Worth 64 Words: Long-term Forecasting with Transformers. InThe Eleventh International Conference on Learning Representations. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gre-gory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga,Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison,Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, andSoumith Chintala. 2019. PyTorch: An Imperative Style, High-Performance DeepLearning Library. In Advances in Neural Information Processing Systems, H. Wal-lach, H. Larochelle, A. Beygelzimer, F. d'Alch-Buc, E. Fox, and R. Garnett (Eds.),Vol. 32. Curran Associates, Inc. F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M.Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D.Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn:Machine Learning in Python. Journal of Machine Learning Research 12 (2011),28252830.",
  "Kashif Rasul, Calvin Seward, Ingmar Schuster, and Roland Vollgraf. 2021. Au-toregressive Denoising Diffusion Models for Multivariate Probabilistic TimeSeries Forecasting. arXiv:2101.12072": "Kashif Rasul, Abdul-Saboor Sheikh, Ingmar Schuster, Urs M Bergmann, andRoland Vollgraf. 2021. Multivariate Probabilistic Time Series Forecasting viaConditioned Normalizing Flows. In International Conference on Learning Repre-sentations. Yulia Rubanova, Ricky T. Q. Chen, and David K Duvenaud. 2019. Latent OrdinaryDifferential Equations for Irregularly-Sampled Time Series. In Advances inNeural Information Processing Systems, H. Wallach, H. Larochelle, A. Beygelzimer,F. d'Alch-Buc, E. Fox, and R. Garnett (Eds.), Vol. 32. Curran Associates, Inc.",
  "Satya Narayan Shukla and Benjamin Marlin. 2021. Multi-Time Attention Net-works for Irregularly Sampled Time Series. In International Conference on Learn-ing Representations": "I Silva, G Moody, DJ Scott, LA Celi, and RG Mark. 2012. Predicting In-HospitalMortality of ICU Patients: The PhysioNet/Computing in Cardiology Challenge2012. Computing in Cardiology 39 (2012), 245248. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli.2015. Deep unsupervised learning using nonequilibrium thermodynamics. InInternational conference on machine learning. PMLR, 22562265. Wei Song, Lu Liu, Minghao Liu, Wenxiang Wang, Xiao Wang, and Yu Song.2020. Representation learning with deconvolution for multivariate time seriesclassification and visualization. In Data Science: 6th International Conference ofPioneering Computer Scientists, Engineers and Educators, ICPCSEE 2020, Taiyuan,China, September 18-21, 2020, Proceedings, Part I 6. Springer, 310326.",
  "Yang Song and Stefano Ermon. 2020. Improved techniques for training score-based generative models. Advances in neural information processing systems 33(2020), 1243812448": "Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Ste-fano Ermon, and Ben Poole. 2021. Score-Based Generative Modeling throughStochastic Differential Equations. In International Conference on Learning Repre-sentations. Ya Su, Youjian Zhao, Chenhao Niu, Rong Liu, Wei Sun, and Dan Pei. 2019.Robust Anomaly Detection for Multivariate Time Series through StochasticRecurrent Neural Network. In Proceedings of the 25th ACM SIGKDD InternationalConference on Knowledge Discovery & Data Mining (Anchorage, AK, USA) (KDD19). Association for Computing Machinery, New York, NY, USA, 28282837.",
  "Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven Hoi. 2023.ETSformer: Exponential Smoothing Transformers for Time-series Forecasting": "Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and MingshengLong. 2023. TimesNet: Temporal 2D-Variation Modeling for General Time SeriesAnalysis. In The Eleventh International Conference on Learning Representations. Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. 2021. Autoformer:Decomposition Transformers with Auto-Correlation for Long-Term Series Fore-casting. In Advances in Neural Information Processing Systems, M. Ranzato,A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (Eds.), Vol. 34.Curran Associates, Inc., 2241922430.",
  "Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. 2023. Are TransformersEffective for Time Series Forecasting? Proceedings of the AAAI Conference onArtificial Intelligence": "George Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty,and Carsten Eickhoff. 2021. A transformer-based framework for multivariatetime series representation learning. In Proceedings of the 27th ACM SIGKDDconference on knowledge discovery & data mining. 21142124. Dejiao Zhang, Feng Nan, Xiaokai Wei, Shang-Wen Li, Henghui Zhu, KathleenMckeown, Ramesh Nallapati, Andrew O Arnold, and Bing Xiang. 2021. Support-ing Clustering with Contrastive Learning. In Proceedings of the 2021 Conference",
  "of the North American Chapter of the Association for Computational Linguistics:Human Language Technologies. 54195430": "Kexin Zhang and Yong Liu. 2021. Unsupervised Feature Learning with DataAugmentation for Control Valve Stiction Detection. In 2021 IEEE 10th DataDriven Control and Learning Systems Conference (DDCLS). IEEE, 13851390. Kexin Zhang, Qingsong Wen, Chaoli Zhang, Rongyao Cai, Ming Jin, YongLiu, James Zhang, Yuxuan Liang, Guansong Pang, Dongjin Song, et al. 2023.Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress, andProspects. arXiv preprint arXiv:2306.10125 (2023). Tianping Zhang, Yizhuo Zhang, Wei Cao, Jiang Bian, Xiaohan Yi, Shun Zheng,and Jian Li. 2022. Less Is More: Fast Multivariate Time Series Forecasting withLight Sampling-oriented MLP Structures. arXiv:2207.01186 [cs.LG] Wenrui Zhang, Ling Yang, Shijia Geng, and Shenda Hong. 2023. Self-supervisedtime series representation learning via cross reconstruction transformer. IEEETransactions on Neural Networks and Learning Systems (2023). Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,and Wancai Zhang. 2021. Informer: Beyond efficient transformer for long se-quence time-series forecasting. In Proceedings of the AAAI conference on artificialintelligence, Vol. 35. 1110611115. Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin.2022. FEDformer: Frequency Enhanced Decomposed Transformer for Long-term Series Forecasting. In Proceedings of the 39th International Conference onMachine Learning (Proceedings of Machine Learning Research, Vol. 162), KamalikaChaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and SivanSabato (Eds.). PMLR, 2726827286. Tian Zhou, Peisong Niu, Xue Wang, Liang Sun, and Rong Jin. 2023. One FitsAll: Power General Time Series Analysis by Pretrained LM. In Thirty-seventhConference on Neural Information Processing Systems. Yanqiao Zhu, Yichen Xu, Qiang Liu, and Shu Wu. 2021. An Empirical Study ofGraph Contrastive Learning. In Thirty-fifth Conference on Neural InformationProcessing Systems Datasets and Benchmarks Track (Round 2).",
  "PhysioNet35483,9972,7993997992,000200PM2.53636-4,842709821,500100": "A.1.1PhysioNet. The PhysioNet dataset, part of the PhysioNetChallenge 2012 , is a rich repository of clinical time series dataderived from intensive care unit (ICU) patients. This comprehensivedataset encompasses a total of 12,000 patients records, each com-prising 42 recorded vital variables over 48 hours with an in-hospitaldeath label indicating the survival of the patient. Patients in thisdataset are categorized into three groups: training set A, open testset B, and hidden test set C, each containing 4,000 patients. Aligningwith the baselines established in previous works and evaluating onthe PhysioNet benchmark , we use specifically set A ofthe dataset, and covert it to an hourly granularity, culminating ina format of 48 time steps for each series. Furthermore, we select asubset of variables (vital signals) from this dataset, 35 out of 42, tofacilitate a rigorous and meaningful benchmarking of our proposedmodel against the backdrop of existing research baselines.A notable characteristic of the PhysioNet dataset is its sparsityand significant proportion of missing values, 80%, posing a uniquechallenge for data imputation and analysis techniques. Moreover,the classification labels are unbalanced with only around 10% pos-itive class labels. To prepare the data for our model, we adopt asystematic segmentation of the available data samples by splittingthem into 5 folds. PhysioNet set A is systematically segmentedinto 20% test, 10% validation, and 70% training sets, similar to .As the vital signs (features) have different scales, we use the nor-malization process in , resulting in features with zero mean andunit variance. To simulate ground-truth testing data for imputationtask, we randomly mask a percentage (10%, 50% and 90%) of theobserved values in each MTS. Conversely, for the interpolationtask, a percentage of timestamps is randomly chosen, and all valuescorresponding to these selected timestamps are masked. A.1.2Air Quality PM2.5. The air quality dataset collected from36 monitoring stations in Beijing, offers hourly PM2.5 readings from2014/05/01 to 2015/04/30. With each station representing a distinctfeature, there are 36 features in total, highlighting the datasetsspatial correlation. By setting 36 consecutive time steps as onetime series, the temporal correlation will be captured as well. In anadded layer of complexity, more intentional missing patterns wereintroduced (13% missing values), hinting a structured absence andcorrelations of the actual data . For the experiments, data fromthe 3rd, 6th, 9th, and 12th months of the calendar year will be usedfor testing as in . Data from the remaining months will beallocated for training, with one month designated as the validation set. During training and validation, a sliding window of size 36 witha stride of 1 is used to generate sequences for each calendar monthin the respective set. During testing, a sliding window of the samesize is employed, but with the stride set to the sequence length. Ifthe length of monthly data in the test set was not divisible by 36,we allow the final sequence to overlap with its predecessor andexcluded the results for the overlapping sections from aggregation.",
  "A.2Forecasting Datasets": "The forecasting datasets exhibit diverse dimensionality, featuring arange of hundreds to thousands of features. Preprocessed by Salinaset al. , these datasets are available as JSON files, pre-split intotraining and testing sets. Moreover, accompanying metadata detailsthe JSON file contents and specifies the value frequency for eachseries. Each line corresponds to one variable, and the target valuesare the measurements of that variable. The specific processingdetails for each dataset can be found in . The objective of aforecasting task is to predict 2 future timestamps using a sequenceof 1 preceding timestamps. We employ a sliding window (of size) technique, where = 1 + 2 to create MTS ready for the TSDEmodel. The specific dimensions for 1 and 2 are outlined in .",
  "Dataset TrainValTesttrain val test pre ft seed step": "SMD38 100 566,724 141,681 708,420 5,667 1,416 7,084 250 30 0.542100MSL55 100 44,65311,66473,729446116737100 20142100SMAP25 100 108,146 27,037 427,617 1,081270 4,276 250 30142100SWaT51 100 396,000 99,000 449,919 3,960990 4,499 200 30142100PSM25 100 105,984 26,49787,8411,0592648785010142100 A.2.1Electricity. The electricity dataset5 provides hourly elec-tricity consumption of 370 customers. The training set features asingle, long, and high-dimensional MTS, covering 370 customersacross 5,833 timestamps. The testing set comprises 7 MTS, eachwith identical dimensions. While the customers are not necessarilyidentical across the different MTS, the time series for each customerbelong to the same distribution, exhibiting a similar range of values.The training and validation sequences are created from the trainingset, while the testing sequences are derived from the test set. Weuse overlapping window with stride of 1 to create training MTSinstances, and a sliding window with a stride equal to the predictionlength for evaluation. This guarantees that each evaluation in thevalidation and test sets is based on unique data segments.",
  "A.3.1Server Machine Dataset (SMD). Introduced by Su et al. ,the SMD is derived from a large Internet companys server data,": "spanning a duration of 5 weeks. This dataset encompasses 38 di-mensions, capturing a wide range of metrics, like CPU load, net-work usage and memory usage, that are crucial for understandingserver performance and anomalies. The observations in SMD areall equally-spaced 1 minute apart. The comprehensive nature ofthe SMD allows for a detailed analysis of server behavior undervarious conditions. There are about 4.16% anomalous timestampsin the test set. A.3.2Pooled Server Metrics (PSM). Collected by Abdulaal et al. ,the PSM dataset aggregates data from multiple application servernodes at eBay. With 26 dimensions, it provides a rich set of metricsthat are instrumental in monitoring and detecting anomalies withineBays server infrastructure. Similar to SMD, the features describeserver machine metrics such as CPU utilization and memory. Thetraining dataset spans 13 weeks, with an additional eight weeksdesignated for testing. Both the training and testing datasets containanomalies. However, labels indicating these anomalies have beenprovided solely for the testing set after being meticulously craftedby engineers and domain experts. There are about 27.8% anomaliesin the test set. A.3.3Mars Science Laboratory (MSL). MSL, a public contributionfrom NASA , includes telemetry anomaly data derived from theIncident Surprise Anomaly (ISA) reports of spacecraft monitoringsystems. It embodies 55 dimensions in the context of space explo-ration, where precision and reliability are paramount. This datasethas about 10.5% anomalies in the test set. A.3.4Soil Moisture Active Passive (SMAP). SMAP, similar to SMDis provided by NASA , includes telemetry anomaly data fromthe SMAP satellite with 25 dimension. This dataset contains 12.8%of timepoints that are anomalous in the test set. A.3.5Secure Water Treatment (SWaT). The SWaT dataset, providedby Mathur and Tippenhauer , is a testbed for attacks againstindustrial facilities. It includes data from 51 sensors over continuousoperations in water treatment systems. The SWaT dataset allowsfor an in-depth exploration of attacks detection in environmentswhere the stakes for security are exceptionally high. In this dataset,anomalies account for 12.1% of the total timestamps.",
  "BMETRICS": "In this section, we will elaborate all the metrics (with formula)that are used in our downstream tasks evaluation. In the scenariowhere missing/future values need to be predicted, i.e., imputation,interpolation and forecasting, we introduce a set of mathemati-cal notations to precisely describe how the adopted metrics arecomputed. Let x represent the MTS with ground truth values. Tofacilitate evaluation, we define two masks as visualized in : mgt marks the values that are originally available with 1s, in-cluding the ones used for evaluation, and the values missing inthe original data with 0. It serves to identify the original groundtruth observations within the data. m represents the values that are known or observable to themodel, excluding the masked values set aside for evaluation.This approach of selective masking enables an assessment of themodels performance on specific data segments.",
  ": Visualization of a MTS input x, illustrating theformation of binary-valued evaluation masks mgt and m": "Once all the missing values are imputed, we focus solely on thevalues that were specifically masked for evaluation. These are iden-tified by the mask meval = mgt m. Given the probabilisitic natureof our model, we approximate the predicted distribution with 100samples and denote it as F. For deterministic evaluation, ratherthan relying on the entire distribution, we utilize the median of allsamples, serving as a deterministic estimation (denoted by x) of themissing values.Assuming our dataset has MTS instances, let X R bethe set of all MTS x, and X R be the set of all predictions x.For the metrics formulas in the upcoming sections, we will averagetheir value over all samples.",
  "B.1CRPS": "The CRPS (Continuous Ranked Probability Score) is a metric usedto quantify the accuracy of probabilistic predictions. It comparesan observed outcome to the predicted distribution, and measurestheir compatibility. It is calculated by evaluating the cumulativedistribution function (CDF) of the predicted probabilities and theCDF of the observed outcomes. The mean squared difference be-tween these two CDF over all possible thresholds gives the CRPSvalue. A lower CRPS indicates a more accurate model, meaningthe predicted probabilities are closer to the observed outcomes.Formally, CRPS is defined as follows:",
  "where refers to the tick value. With these notations, ( + 1) = 1": "CRPS is a popular metric for probabilistic tasks, where under-standing the uncertainty and range of possible outcomes is as cru-cial as predicting the most likely outcome as opposed to determin-istic tasks. In our experiments, we evaluate the performance on thevalues that are masked and imputed by TSDE, denoted byeval,= 1.We formulate the CRPS score obtained for a value , as:",
  "B.6AUROC": "AUROC (Area Under Receiving Operating Characteristic Curve) isa fundamental metric for assessing binary classification models. Itrepresents the models ability to differentiate between the classes(positive/negative) over various threshold levels, especially whendealing with imbalanced datasets. By plotting the True Positive Rate(TPR) against the False Positive Rate (FPR), it illustrates the modelsdiscrimination power. The AUROC measures the area under thiscurve and scales between 0 and 1, with a higher value indicatingsuperior model performance. In our evaluation, we leverage AUROCmetric as implemented in the scikit-learn library.",
  "B.10RI": "RI (Rand Index) is a clustering metric used to evaluate similaritybetween two clustering assignments, regardless of the absoluteassociated labels. It is calculated by examining the agreement anddisagreement of pairings within the clustering assignments relativeto the true labels. RI takes into account all sample pairs, countingthose that are consistently grouped in the same or different clustersin both the clustering assignments and the true labels. This includesthe number of agreeing pairs assigned to the same cluster and thosecorrectly separated into distinct clusters.",
  "B.11Adjusted RI (ARI)": "The raw RI is adjusted to account for chance grouping and ensurethat random labeling will have a very low score (close to 0) indepen-dently of cluster assignments. The ARI is a more accurate clusteringsimilarity metric, with values ranging from -1 to 1. Values closeto 1 suggest a more accurate cluster assignment. Negative valuesindicate a clustering result that is worse than a random assignment.",
  "CIMPLEMENTATION": "In this section we delve into the architecture components and thelayers that underpin our framework implementation. Our archi-tecture is built on two key elements: an embedding function forprocessing observed segments and a denoising block for imputingthe missing or the masked values, both implemented as PytorchNN modules. Below, we provide more details about the the type oflayers and the design of these components. summarizesthe main layers introduced in the subsequent sections.",
  "C.1Embedding Function": "The embedding function stands as the pivotal component of TSDE,enabling us to adeptly handle a wide range of tasks by using thegenerated embeddings. This block combines four inputs and usethem to transform the MTS into useful and generic representations:(1) Time embedding stime: We use 128-dimensional fixed embed-ding for each timestamp following to encapsulatetemporal information. Refer to (17) for a detailed formulation. (2) Feature embedding sfeat: We leverage a Pytorch embeddinglayer to derive 16-demensional embeddings for each feature ID.These weights of the embedding layer are trained, enabling themodel to better understand and incorporate the non-temporaldependencies in the MTS embedding.",
  "(4) xobs0: the observed values in the MTS, obtained by applying mIIF": "to raw MTS x0, as formulated in (10).The core of the embedding block invloves the Transfromer archi-tecture. Our model utilizes dual-orthogonal Transformer encoderswith a crossover mechanism, each realized with a single-layer Trans-formerEncoder as implemented in Pytorch. This setup includesa multihead (=8) attention layer and incorporates fully connectedlayers and layer normalization. The input to the transformer en-coders is a concatenation of stime, sfeat and xobs0. To align with theinput requirement of multihead attention, xobs0is first projectedinto a matrix of shape 16, ensuring that the last dimensionof the concatenated tensor is divisible by 8.The outputs of the two encoders, are projected to lower dimen-sional space and concatenated along with the mIIF mask, resultingin a refined embedding of the MTS (the observed segment).",
  "C.2Denoising Block": "At diffusion step , the denoising block (i.e., conditional reversediffusion block) receives a set of inputs that are used for the condi-tioned denoising. These inputs include the MTS embeddings fromthe embedding function, the masked and noisy segment of the orig-inal MTS at the -th diffusion step, the diffusion step embedding,and a mask delineating the locations of added noise correspondingto the optimizable areas. To prepare xmskfor the denoising block,we derive it from xmsk0by",
  ")2.(43)": "The minimum noise level and the maximum noise level are set to1 = 0.0001 and = 0.5 respectively as in .The denoising block is composed mainly of 1 1 layers,implemented using the Linear layer in Pytorch. Conditioned on theembeddings generated by the embedding block, the denoising blockplays a dual role that extends beyond mere denoising. It encourages the embedding function to learn robust and meaningful representa-tion of the MTS and updates this block weights during the training.Furthermore, in the context of filling in missing values, the denois-ing block plays a pivotal role in imputing missing values by fillingthem first with random values and then denoising them iterativelystarting from diffusion step ==50 down to step t=1. During in-ference, the goal is to impute all the missing values (not only themasked ones for evaluation), hence xmsk0= (1 m) x0. Arandom noise is then injected into xmsk0to obtain xmsk; and TSDEdenoise it resulting in imputed MTS, as detailed in Algorithm 3.Implementation-wise, we use a 4-layer residual module (we set theresidual channels to 64), composed mainly of Conv11 layers.",
  "To tailor the model for specific missing values patterns, we finetuneTSDE by employing task-specific masks": "C.3.1Imputation mask. We randomly mask a ratio of the ob-served values to simulate missing data. The value of is sampledfrom the range [0.1,0.9] to cover different missing cases, yet keep-ing some observed values for conditionning the denoising process.This mask is useful when the missing values do not intricate somespecific patterns within the data.",
  "return mimp;": "C.3.2History mask. The history mask combines the imputationmask or random masking strategy with a strategic approach ex-ploiting the structured missing patterns observed in the data forimproved imputation. This is achieved by intersecting the observedindices of a given sample with the missing values from anotherrandomly chosen sample to create a history mask. Then, based ona sampled probability, we decide to either retain the history maskor apply the imputation mask, thereby effectively addressing thestructured missing patterns.",
  "C.4Projection Head for Anomaly Detection": "To enable anomaly detection with TSDE framework, we employ adedicated projection layer to reconstruct the MTS from their em-beddings. This reconstruction helps identifying deviations fromnormal patterns, which signify anomalies. The core of this projec-tion is a Linear layer, implemented in Pytorch, designed to projectthe reshaped embeddings, excluding the mask mIIF, from a shapeof ( 32, ) back to (, ). The finetuning of this projection headoptimizes a MSE loss. This training objective is instrumental inguiding the projection layer to minimize the discrepancy betweenreconstructed and original MTS if there are no outliers. In ourimplementation, we used the Pytorch built-in MSE loss.",
  "C.5Classification": "For the classification task, we integrated a classifier head composedof MLP layer. The classifier is designed to transform the embeddingsinto class probabilities. 1st layer: consists of a fully connected layer followed by a SiLuactivation function and a dropout layer. The linear layer projectthe flattened embeddings from 33 to 256, the SiLu functionintroduce non-linearity and the dropout layer randomly omitssome of the layer outputs to prevent overfitting. 2nd layer: Similar to the first layer, with a fully connected layerconnecting 256 nodes to 256 nodes followed by SiLu and dropout.It allows for increasing the model capacity and thus further ex-tracting complex relationships from the embeddings and intro-duce more non-linear operation within the network.",
  "D.1Imputation": "D.1.1Imputation on PhysioNet. For the PhysioNet dataset, weset the batch size to 16. The model is pretrained for 2,000 epochs,followed by a finetuning phase of 200 epochs. During finetuning, weupdate all model weights, including the embeddings and denoisingdiffusion blocks, using the imputation mask (cf. Algorithm 4). TheAdam optimizer, available in Pytorch, is employed throughout thepretraining and finetuning, starting with a learning rate of 0.001.This rate is scheduled to decay to 0.0001 at the 1, 500th epoch andfurther reduced to 0.00001 at 1, 800th epoch during pretraining.Similarly, during finetuning, the learning rate is reduced at the150th and 180th epochs. This decay aligns precisely with a scheduleddecay at 75% and 90% of the training total epochs, respectively forboth pretraining and finetuning as in . D.1.2Imputation on PM2.5. We maintain the same batch size of 16.The pretraining phase is shortened to 1,500 epochs, with finetuningfurther reduced to 100 epochs, reflecting the specific requirementsand complexity of this dataset. Finetuning for this dataset entailsan update of all model weights by employing a history mask (cf. Al-gorithm 5) tailored for the type of this dataset, having structuredmissing patterns, as opposed to IIF masking and imputation mask.We follow the same optimization setup using Adam optimizer witha starting learning rate of 0.001, systematically decaying to 0.0001and 0.00001 at the 75% and 90% training completion respectively,during both pretraining and finetuning.",
  "D.3Forecasting": "D.3.1Probabilistic Multivariate Forecasting. For forecasting, ourexperimental setup mirrors the imputation framework, using simi-lar model hyperparameters and the same optimizer. However, toaccommodate the high dimensionality in the forecasting MTS, weadjust the batch size to 8. Finetuning is carried out using the fore-casting mask, as delineated in Algorithm 7, masking all the future",
  "D.4Anomaly Detection": "For the anomaly detection we use the same pretraining setup withsimilar hyperparameters except for batch size and number of epochs.We set the batch size to 32, and adjust the total number of epochsfor each dataset accordingly. We use a seed of 42 for all the anomalydetection experiments. Once TSDE is pretrained, we freeze all themodel weights and finetune only the projection head weights. Forfinetuning, we used Adam optimizer in Pytorch and we set thelearning rate to 0.0001 and the weight decay parameter to 106. D.4.1Anomaly threshold setting. Given the impracticality of ac-cessing the size of the test subset in real-world scenarios, we adopta pragmatic approach by fixing the anomaly detection threshold(). This threshold is determined to ensure that the anomaly scores for r time points within the validation set exceed , thereby clas-sifying them as anomalies. This strategic threshold setting main-tains consistency with the anomaly ratios observed in baselinemethodologies. In our experiments we used the same anomalyratios employed by the baselines, as outlined in . D.4.2Adjustement strategy. TSDE, following , incorpo-rates a nuanced adjustment strategy for the detection of anomalieswithin successive abnormal segments. This strategy is predicated onthe premise that the detection of a single time point as anomalouswithin a continuous abnormal segment warrants the classificationof all time points within this segment as anomalies. This approach isderived from practical observations in real-world scenarios, wherethe identification of an anomaly triggers an alert, thereby bringingthe entire abnormal segment to attention.",
  "D.5Classification": "To evaluate TSDE on the classification task, we perform two runs. Inthe first run, we pretrain the TSDE model specifically for the impu-tation task on the PhysioNet data with 10% missing data ratio. Thispretraining follows the experimental setup detailed in Section D.1.1.Upon completing the pretraining phase, we proceed with inferenceon the training, validation, and test datasets, and save the medianof the generated samples along with their respective labels. Subse-quently, we freeze the weights of the embedding block and proceedto finetune only the classifier head of the pretrained model. Thisfinetuning phase spans 40 epochs, during which the embeddingblock is fed with the imputed MTS. The primary training objectiveis minimizing the cross-entropy loss as implemented in PyTorch.To achieve this, we utilize the Adam optimizer, setting the learningrate to 0.0001 and applying a weight decay of 106.",
  "D.6Clustering": "The clustering is tested under three settings: (1) raw MTS, (2) embed-ding of imputed MTS and (3) embedding of raw MTS. To generatethe MTS embeddings, we employ the same pretrained model as usedin the classification task. Each of the three aforementioned tensorsis then projected into 2-dimensional space using UMAP with theJaccard distance metric. For the clustering in this 2-dimensionalprojected space, we utilize the DBSCAN algorithm, as implementedin the scikit-learn library.",
  "FKEY FINDINGS AND LIMITATIONS": "This work explored the integration of diffusion models and trans-former encoders for TSRL. Our results indicate that conditioningthe diffusion model on learned embeddings improves performanceacross tasks such as imputation, interpolation, and forecasting. Ad-ditionally, the embedding block within TSDE proves highly effectivein handling sparse data by leveraging the SSL task of imputing miss-ing values. The use of IIF masking facilitated the models robustnessto sparse data, and handling of different missingness scenarios.Despite these advancements, the TSDE model has few limita-tions. The iterative nature of the denoising diffusion probablistic",
  "means that there are some mismatches between our input-output setting and their papers. We adopt their official codes and only changethe length of input and output sequences for a fair comparison": "model can lead to slower inference, presenting a trade-off betweenenhanced quality and efficiency in real-world applications. Incor-porating the SSL pretext task of IIF masking requires additionaltraining epochs, which can be a constraint for rapid model deploy-ment and retraining. Furthermore, while TSDE outperforms othermethods, there remains a gap in perfectly matching the groundtruth in highly noisy scenarios. Addressing these limitations will"
}