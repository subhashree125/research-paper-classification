{
  "Abstract": "In the rapidly evolving domain of Recommender Systems (RecSys),new algorithms frequently claim state-of-the-art performance basedon evaluations over a limited set of arbitrarily selected datasets.However, this approach may fail to holistically reflect their effec-tiveness due to the significant impact of dataset characteristicson algorithm performance. Addressing this deficiency, this paperintroduces a novel benchmarking methodology to facilitate a fairand robust comparison of RecSys algorithms, thereby advancingevaluation practices. By utilizing a diverse set of 30 open datasets,including two introduced in this work, and evaluating 11 collabo-rative filtering algorithms across 9 metrics, we critically examinethe influence of dataset characteristics on algorithm performance.We further investigate the feasibility of aggregating outcomes frommultiple datasets into a unified ranking. Through rigorous experi-mental analysis, we validate the reliability of our methodology un-der the variability of datasets, offering a benchmarking strategy thatbalances quality and computational demands. This methodologyenables a fair yet effective means of evaluating RecSys algorithms,providing valuable guidance for future research endeavors.",
  "Information systems Recommender systems": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08",
  "Recommender Systems; Evaluation; Benchmarking; Datasets; DataCharacteristics": "ACM Reference Format:Valeriy Shevchenko, Nikita Belousov, Alexey Vasilev, Vladimir Zholobov,Artyom Sosedka, Natalia Semenova, Anna Volodkevich, Andrey Savchenko,and Alexey Zaytsev. 2024. From Variability to Stability: Advancing RecSysBenchmarking Practices. In Proceedings of the 30th ACM SIGKDD Conferenceon Knowledge Discovery and Data Mining (KDD 24), August 2529, 2024,Barcelona, Spain. ACM, New York, NY, USA, 12 pages.",
  "Introduction": "Recommender systems have become the backbone of personalizinguser experiences across diverse online platforms. By suggestingmovies, recommending products, and curating news feeds ,RecSys is a key machine learning technology widely used in manyapplications. Their impact drives ongoing development in bothacademia and industry, resulting in the introduction of numerousRecSys algorithms each year .With this ongoing expansion, there is a growing need for toolsthat enable reproducible evaluation, allowing researchers to as-sess new methods alongside well-established baselines .While several frameworks excel in conducting a rigorousevaluation of RecSys algorithms on a specific dataset, selecting thebest-performing models across multiple problems remains challeng-ing. Results vary significantly based on the considered dataset, andwhat works well in one context may perform poorly in another .This variability often results in inconsistent conclusions from evalu-ation studies, highlighting the importance of comparing algorithmsacross datasets with various data characteristics. On the other hand,extensive evaluation with dozens of datasets uses large amountsof computational resources and harms both the environment",
  "KDD 24, August 2529, 2024, Barcelona, SpainValeriy Shevchenko, et al": "Additional experiments. In Appendix B, we explore the stabilityof a ranking provided by aggregations after the inclusion of a newmethod slightly superior or inferior to an existing one. There, allmethods, except for Mean ranks, demonstrate stability under ad-versarial perturbations. Moreover, our analysis delves into specificintrinsic properties of a ranking system, such as Pareto efficiencywhen using a small number of datasets. An aggregation method isconsidered Pareto efficient if it outperforms another method for allmetrics. The results of all tests are presented in .",
  "Related work": "RecSys evaluation. Recommender systems continue to be a dy-namic research area. Traditional techniques like Neighborhood-based models and Matrix Factorization remain reliablebaselines. However, incorporating Deep Neural Networks has no-tably advanced RecSys, significantly enriching the domain .This variety leads to the development of open-source libraries andtools to address diverse application needs. Among the notewor-thy ones, DeepRec , Implicit , LightFM , NeuRec ,RecBole , RecPack and Replay offer realizations ofpopular recommendation algorithms.",
  "To guarantee the reproducibility of our results, all code and datasets employed inexperiments are available in the GitHub repository and": "Offline evaluation remains essential in RecSys research as itprovides a reliable and cost-effective approach to assess algorithmperformance. It is particularly suitable for researchers who aredeveloping new models. As a part of offline evaluation, the variety inthe field leads to the need for rigorous and reproducible evaluationmethodologies. Notable studies such as Elliot , Recbole , andDaisyRec introduced comprehensive evaluation frameworksin both reproducing and benchmarking recommendation models.These frameworks offer a rich array of options for pre-filtering, datasplitting, evaluation metrics, and hyperparameter tuning across abroad spectrum of popular recommender models. Notably, Elliotuniquely provides statistical tests for robustly analyzing the finalresults, adding a layer to the evaluation process.RecSys datasets. Dozens of public datasets from diverse domainsare available for constructing and evaluating recommender systems.The research shows that most studies utilize, on average, 2.9datasets, with dataset selection and preprocessing affecting eval-uation outcomes significantly. Different data filtering techniquescan change data characteristics, leading to varied performancerankings. Deldjoo et al. investigated how data propertiesimpact recommendation accuracy, fairness, and vulnerability toshilling attacks, highlighting the importance of data understand-ing in enhancing system performance. The paper emphasizedthe crucial role of dataset diversity in RecSys algorithm evalua-tions, showing that dataset choice significantly affects evaluationconclusions. These findings collectively underscore the need forconsidering dataset variability in future research to enhance thereliability of recommender system evaluations.Aggregating methods. When introducing a novel machinelearning approach, it is important to rigorously compare its per-formance against existing methods across a comprehensive setof relevant tasks to determine its standing relative to the currentstate-of-the-art. However, drawing conclusions about the superioralgorithm based on the outcomes from multi-dataset benchmarkscan be challenging.Various techniques have been developed to yield concise sum-maries to address this challenge. One basic method involves meanaggregation, assuming uniformity across task metrics . How-ever, this can lead to biases when metrics vary significantly .The Dolan-Mor performance profiles, initially developed for op-timization algorithm benchmarks , have gained traction forevaluating machine learning algorithm efficacy across diverse prob-lems . Unlike mean aggregation, Dolan-Mor curves considerthe distribution of performance values, offering insight into howfrequently and significantly an algorithm excels. Similarly, the Crit-ical Difference (CD) diagram is frequently used to comparealgorithms across multiple tasks. This method of presenting resultshas become broadly accepted , providing both groupwise andpairwise comparisons. Groupwise comparisons are achieved byordering all methods based on the mean rank of relative perfor-mance on each task. Pairwise comparisons are based on a criticaldifference in this mean rank or, later , the Wilcoxon signed-ranktest with multiple test corrections.VOTENRANK is another framework proposed for rank-ing systems in multitask benchmarks rooted in the principles of",
  "From Variability to Stability: Advancing RecSys Benchmarking PracticesKDD 24, August 2529, 2024, Barcelona, Spain": "Once evaluation metrics are collected, we should define a methodto rank algorithms using performance scores. Our pipeline useswell-established methods to aggregate performance to a single rankscore over multiple datasets. These aggregations are adopted fromgeneral Machine learning practice and reused for our problem ofRecSys methods ranking.The list of aggregators includes arithmetic, geometric, and har-monic mean aggregations of a quality metric, CD diagrams emphasizing mean ranks; Dolan-Mor (performance) curves featuring AUC values, and algorithms inspired by the social choicetheory, specifically the Copeland, and MinMax rules, proposed foraggregation of results over various NLP tasks .",
  "Methodology": "Our aim is to present a robust and efficient benchmarking method-ology tailored for the RecSys domain. We align our experimentalsetup with online evaluation, replicating real-time recommendationscenarios while ensuring the reproducibility of our benchmarkingresults.To achieve our goal, we collect a diverse set of open-sourcedatasets and establish a robust pipeline that incorporates predefinedprocedural steps. Additionally, we integrate 11 RecSys algorithmsfrom various open-source libraries and repositories. This pipelineserves a dual purpose: it streamlines the evaluation process andenhances the comparability of results across different algorithmsand datasets. The pipeline scheme is shown in .The models evaluated in this study are collaborative filteringmethods that use only user-item interaction data. While industrialscenarios often include rich user and item features, we focusedexclusively on interaction data. This approach allows for a straight-forward comparison of algorithms, including those introduced re-cently, across a diverse dataset spectrum. Consequently, our choiceenabled us to compare numerous algorithms and datasets, makingour comparison the most extensive in the current literature.",
  "Datasets and Preprocessing": "In our benchmarking process, we use 30 public datasets, each withtimestamps, spanning seven diverse domains. These datasets covermany business areas, including e-commerce, social networks, andentertainment. Alongside the utilization of the 28 established publicdatasets, we introduce two new ones, namely Zvuk and MegaMar-ket, with their details provided in the Appendix A. This diversity issummarized in .",
  ": Dataset distribution by domains, # is the numberof datasets in a domain. The newly introduced Zvuk andMegaMarket expand the most data-scarce domains": "Implicit feedback-based recommendation systems are increas-ingly prevalent, primarily due to the frequent absence of explicitrating information in many applications. Therefore, datasets thatinitially include item ratings are usually transformed into binarysignals, an approach we have also implemented in our evalua-tion. . We have introduced a dataset-specific threshold pa-rameter, denoted as , to filter out interactions falling below thisthreshold. Such interactions are considered negative feedback andare thus removed from the datasets. For more on determining theoptimal value for individual datasets, refer to Appendix A.1.In their initial state, the datasets exhibit a highly sparse nature,characterized by a substantial proportion of users interacting witha limited number of items, often fewer than five. As part of theevaluation process, preprocessing steps are applied to filter outinactive users and items. Most researchers either adopt a 5- or10-filter/core preprocessing . -filter and -core filteringtechniques differ. The former simultaneously filters items and usersin a single pass, while the latter employs iterative filtering untilall users and items have a minimum of interactions. We adoptthe 5-filter 3 methodology, prioritizing item filtering before user",
  "Protocol Setting": "Maximize NDCG@10 metrics Aggregatingmetrics on all datasets to rank models: - CD diagrams based on Mean Ranks - Performance curves - ... ... Mean rank leaderboard #1. EASE #2. LightGCN #3. MultiVAE ... Performance curves : Benchmarking methodology for ranking algorithms. Our main innovations are the curated list of datasets that enablethe option of comparison of pairs of models and aggregation strategies that provide principled ranking of approaches w.r.t.various criteria.",
  "Recommendation Models": "Current recommendation frameworks enable the streamlined in-tegration of widely-used baseline models and recently proposedmodels. We have leveraged existing implementations of well-knownalgorithms and developed an evaluation pipeline. This pipeline en-compasses dataset filtering, data splitting, metrics computation,and hyperparameter optimization. The frameworks we have usedinclude Implicit , LightFM , RecBole , and Replay .Reflecting on recent relevant research in benchmarking ,we have selected the following categories of algorithms for ouranalysis:",
  "Evaluation Settings": "Data splitting. A guiding principle for splitting data into train-ing and test subsets is to resemble the deployment conditionsclosely . In the top-N recommendation paradigm, the primarychallenge is to infer user preferences from past interactions to pre-dict future ones. Given this, the training data should chronologicallyprecede the test data, acting as the \"history\" followed by the \"future\"at a designated time. This approach helps in mitigating the risk ofdata leakage . Therefore, we adopt the global temporal splittingstrategy with an 80/10/10 training, validation, and test set ratiosfollowing . After splitting, we exclude cold-start usersand items with no record in the training set.Negative Sampling. In the context of Recommender Systemsevaluation, negative sampling involves prediction and evaluationfor only a limited set of non-relevant items and known relevant items instead of full item list scoring. These non-relevant itemsare chosen from a candidate item pool. Although sampling strate-gies like the Uniform Sampler have been used to avoid biasesin evaluating RecSys algorithms and to boost computational ef-ficiency , studies have questioned their reliability .Consequently, our evaluation involves testing on all unobserveditems.Evaluation Metrics. The precise interpretation of popular qual-ity metrics in the field lacks a consensus, and it is often observedthat the more complex a metric is, the greater the scope for vary-ing interpretations . Therefore, offering a detailed evaluationprotocol for reproducibility and clarity in the assessment is crucial.In light of this, our approach is meticulous: we precisely defineeach metric and accurately compute them within our establishedpipeline.To evaluate the performance of our models, we employ a spec-trum of standard quality metrics, such as Precision@k, Recall@k,nDCG@k, MAP@k, HitRate@k, and MRR@k. Our evaluation scopeextends further, incorporating beyond-accuracy objective metricsthat provide a more comprehensive view of model effectiveness.These include Coverage@k, Diversity@k, and Novelty@k .Hyperparameter Tuning. Hyperparameter optimization is cru-cial for achieving optimal performance of machine learning algo-rithms and ensuring reliable benchmarking. The paper high-lights that most RecSys baselines can attain between 90 95% oftheir maximum performance within the initial 40 iterations whenusing Bayesian optimization. Leveraging this insight, we utilize theOptuna framework and apply the Tree of Parzen Estimators(TPE) algorithm for hyperparameter tuning. In alignment with priorresearch , we conduct hyper-parameter optimization withrespect to nDCG@10 for each baseline on each dataset.After determining the optimal hyperparameters, we executea final training on the consolidated training and validation sets.This procedure ensures that all available interactions up to the testtimestamp are incorporated, including the most recent ones.",
  "EXPERIMENTS AND RESULTS4.1Metrics": "Our experiments begin with the collection of performance met-rics to evaluate 11 recommendation algorithms across 30 datasets.These metrics include User Preference Accuracy, Ranking Quality,and Beyond-Accuracy metrics. Following the data collection, weperform an initial analysis of the accumulated results, focusing onSpearmans correlation for each dataset and subsequently comput-ing the average correlation scores. These results are consolidatedinto a correlation heatmap, illustrating the relationships among allpairs of metrics, as shown in .",
  "Cov": "RecallNDCG MRRMap HitPrecision DivNovCov 10.9 0.85 0.86 0.92 0.96 0.04-0.03 -0.1 0.910.95 0.97 0.9 0.9400.01-0.08 0.85 0.9510.94 0.89 0.8900.01 -0.1 0.86 0.97 0.9410.85 0.91-0.010-0.09 0.92 0.9 0.89 0.8510.93 0.05-0.01-0.08 0.96 0.94 0.89 0.91 0.9310.02-0.04-0.12 0.0400-0.010.05 0.0210.29 0.29 -0.030.01 0.010-0.01-0.040.2910.63 -0.1 -0.08 -0.1 -0.09-0.08-0.120.29 0.631 0.0 0.2 0.4 0.6 0.8 1.0",
  ": Spearman correlation between metrics for = 10.Darker blue indicates stronger correlations": "The structure of our correlation matrix is similar to the smaller-scale experiment conducted by Zhao et al. . The heatmap re-veals that the Accuracy and Ranking metrics located in the topleft corner (, , , , , ) exhibithigh correlations with each other, surpassing 0.8. In contrast, theBeyond-Accuracy metrics positioned at the bottom demonstrateweak correlations with the rest. This distinction can be attributedto the different goals of these metrics, as Beyond-Accuracy metricsdo not straightforwardly describe recommendation quality.Moreover, our findings indicate that nDCG has the highest corre-lation ( 0.9) with accuracy and ranking metrics. This observationreinforces the usage of in benchmarking and during hyper-parameter optimization as an optimization objective. Consequently,",
  ",": "where is the index of the curve that corresponds to a RecSysmodel, is the problem index, is the number of datasets, is ahyperparameter limiting the values of , is the metric value thatcorresponds to a RecSys model and the problem , and is themetric value that corresponds to a RecSys model and the problem. The DM curve for a specific reports the number of problemsfor which the model performs no more than times worse thanthe best model for that problem (e.g., (1) represents the share ofproblems where the -th algorithm is the best).In the case of aggregation, we take the area under the DM curvedivided by the sum of all areas. In our experiments, we fixed = 3;below, we show that ranking remains stable across a wide range of values.",
  "(4) Repeat the previous steps using the remaining methods": "Social Choice Theory. The last two aggregating approaches con-sidered, Copeland and Minimax, are majority-relation-based rules.A majority relation for two methods , ( ) holds, if has a higher metric value than . Copeland method definesthe aggregation score () as () = |()| | ()|, where() = {| }, () = {| }. The Minimax usesa score (,), representing the number of datasets for whichmethod has a higher score than . The aggregated score isgiven by () = max (,). 4.2.2Comparison of RecSys algorithms. One of our objectives is topresent interpretable results that facilitate swift visual comparisonsof the performance of RecSys algorithms across multiple datasets.We present these visual comparisons using DM performance pro-files and a CD diagram. The DM profiles are in . Further, weuse the presented DM AUC to rank the algorithms. The CD diagramcan be found in . In addition to the traditional CD diagramthat includes the pairwise Wilcoxon test, we have introduced theBayesian Signed-Rank test, indicated by dashed horizontal greylines. We exclude the concept of ROPE from our analysis becauseit requires homogeneity among the set of metrics, which is notapplicable in RecSys. This inhomogeneity also leads to the absenceof statistical significance in the CD. While we use a large numberof datasets, due to their diversity, the ranks of approaches change alot.",
  ": Performance profiles for the comparison of RecSysalgorithms. The higher the curve, the better the performanceof the algorithm. We also provide AUCs for each approach": "However, our findings indicate that EASE emerges as the winnerfor both options. There is no distinct second-place algorithm, asareas under the performance profiles for LightGCN and MultiVAEare almost identical. Finally, all methods perform significantly betterthan a random approach and are mostly superior to the MostPopbaseline. Leaderboard for different aggregations. Different aggregation meth-ods yield distinct rankings for the approaches considered. Withthe set of 30 datasets, the ranks presented in consistentlyidentify EASE as the top-performing approach. For the subsequenttop positions, we have a pair of candidates for most aggregations:MultiVAE and LightGCN.",
  "EASE": "nDCG@10 : The Critical Difference diagram for the comparisonof RecSys algorithms. The numbers represent the mean ranksof methods over all datasets. Thick horizontal lines representa non-significance based on the Wilcoxon-Holmes test, whiledashed horizontal lines represent non-significance accordingto the Bayesian Signed-Rank test. 4.2.3Comparison of reliability of aggregations. To ensure a reli-able aggregation method for benchmarking, it should demonstratestability under various perturbations, including adversarial ones.We conduct an analysis to assess the robustness of the presentedaggregation methods.First, we determine rankings based on 30 datasets across allmethods, establishing them as our reference benchmarks. Next, weexamine the sensitivity of these rankings to the following modifi-cations of the input matrix of quality metrics: (1) Inclusion or exclusion of a dataset.(2) Introduction or removal of a RecSys algorithm.(3) Incorporation or exclusion of a slightly superior/inferiormethod to a particular algorithm, exploring all possible per-mutations.",
  "(4) Adjustments in the hyperparameters of an aggregation method": "Change of the set of used datasets. We measure the correlationbetween the final rankings and the references using the Spearmancorrelation coefficient .The results for the case of dropping datasets are presented in Fig-ure 5. All aggregations exhibit a relatively stable behaviour, exceptfor Minimax, which shows a low after dropping 15 datasets. Onthe contrary, the best-performing method is Geom. mean, Harm.mean and DM LBO, with their average metric values being lessinfluenced by specific datasets. Overall, different aggregation tech-niques tend to produce similar rankings if the number of datasetsis large enough.Furthermore, we explore the case when we use only five datasetsto calculate ranks. We randomly sampled 100 pairs of subsets of sizefive and calculated Spearmans correlation between aggregationsfor each pair of sets of datasets. The results are in . Aggre-gations are less stable in this case. Moreover, MA, Harm. mean andMinimax methods have Spearmans correlation of less than 0.8. Asin the case of dropping datasets, the outlier is Minimax model withlow . The Mean ranks and Copeland methods perform the best inthis scenario, demonstrating equivalent values. Elimination of RecSys methods. In this scenario, we computeSpearmans correlation between the results for all methods and theresults with the exclusion of some methods.The results are presented in . We observe that mostaggregating methods exhibit relatively stable behaviour, except",
  "RankingDM AUC DM LBO Mean ranks MA Geom. mean Harm. mean Copeland Minimax position": "1EASE: 0.121EASE: 1EASE: 2.933EASE: 0.069EASE: 0.042EASE: 0.023EASE: 10.0EASE: -0.02LightGCN: 0.111LightGCN: 2MultiVAE: 4.107LightGCL: 0.065LightGCN: 0.038LightGCN: 0.021MultiVAE: 8.0SLIM: -21.03MultiVAE: 0.111LightGCL: 3LightGCN: 4.363LightGCN: 0.064LightGCL: 0.038ALS: 0.02LightGCN: 6.0MultiVAE: -22.04LightGCL: 0.11MultiVAE: 4SLIM: 5.247MultiVAE: 0.061MultiVAE: 0.038LightGCL: 0.02SLIM: 3.0LightGCN: -22.05ALS: 0.106ALS: 5ALS: 5.32LightFM: 0.059ALS: 0.035MultiVAE: 0.02ALS: 2.0LightGCL: -23.06ItemKNN: 0.1ItemKNN: 6LightGCL: 5.5SLIM: 0.058LightFM: 0.034ItemKNN: 0.018LightGCL: 0.0ALS: -24.07LightFM: 0.1LightFM: 7LightFM: 5.707BPR: 0.057ItemKNN: 0.033LightFM: 0.017LightFM: -1.0BPR: -25.08SLIM: 0.093BPR: 8ItemKNN: 6.25ALS: 0.057BPR: 0.03BPR: 0.014ItemKNN: -4.0ItemKNN: -26.09BPR: 0.088SLIM: 9BPR: 6.793ItemKNN: 0.056SLIM: 0.025MostPop: 0.006BPR: -6.0LightFM: -26.010MostPop: 0.058MostPop: 10MostPop: 9.067MostPop: 0.041MostPop: 0.017SLIM: 0.003MostPop: -8.0MostPop: -29.011Random: 0.003Random: 11Random: 10.8Random: 0.007Random: 0.001Random: 0.0Random: -10.0Random: -30.0",
  "for Minimax. As the number of discarded methods increases, thelikelihood of changing the best method also increases. This notablyimpacts the Mean ranks, DM AUC and Copeland methods": "Changing the hyperparameters of the aggregating method. In thepaper, only the DM AUC and DM LBO aggregating methods haveadjustable hyperparameters ( - the maximum value of the ratioof the best metric value and the one under consideration, the right Number of methods 0.75 0.80 0.85 0.90 0.95 1.00 Spearman's DM AUCMean ranksDM LBOGeom. meanMAHarm. meanMinimaxCopeland",
  ": Stability of aggregations with respect to the numberof used methods": "boundary of the X-axis in the performance profiles). We calculatethe Spearman correlation between the case when = 3 and thecase when can take any value. The results presented in cdemonstrate that can influence the rankings, underscoring thesignificance of determining and fixing the optimal value. The rank-ing remains stable over a wide range of values, with DM LBOshowing more robustness compared to the pure DM AUC.",
  "Dataset Characteristics": "In addition to the performance benchmark, our study explores theconnection between specific dataset characteristics and recommen-dation quality. We utilize user-item interaction matrix propertiesfrom . These properties serve as problem characteristics andencompass various aspects, including the size, shape, and densityof the dataset (SpaceSize, Shape, Density), as well as counts of users,items, and interactions (Nu, Ni, Nr). We also consider interaction fre-quencies per user and item (Rpu, Rpi), Gini coefficients that describeinteraction distribution among users and items (Giniu, Ginii), andstatistics related to popularity bias and long-tail items (APB, StPB,SkPB, KuPB, LTavg, LTstd, LTsk, LTku) . These characteristics ofthe 30 selected datasets exhibit a wide range of variability.To establish a connection between these data characteristicsand our primary quality metric, @10, we employed threedistinct measures: Pearson product-moment correlation, Spearmanrank an order correlation, and Mutual information a nonlinearalternative. The obtained values are in . SpaceSize",
  ": Pearson, Spearman correlations, and Mutual infor-mation between data characteristics and RecSys algorithmsperformance": "Datasets exhibiting higher levels of popularity bias APB andDensity tend to simplify the prediction tasks for recommendermodels. Conversely, datasets exhibit long-tailed item distributions,increased item diversity, and pronounced Popularity Bias, present-ing a greater challenge for recommender models. Furthermore, themoderate mutual information values emphasize the practical impactof these characteristics on the performance of the models.",
  "Optimizing Dataset Selection forBenchmarking": "Using 30 public datasets provides diverse evaluation characteris-tics but is not computationally efficient. To decrease the use ofdatasets while preserving variability, it would be practical to se-lect datasets that may belong to the same group. We employ theKMeans approach to split datasets into multiple clusters, using datacharacteristics from the previous section as feature representations.Principal datasets selection. To decrease time consumptionand minimize the degradation of benchmarking, we can run it only for a limited number of datasets, carefully selecting them. We useseveral approaches for selection: Random, KMeans, A-optimality,and D-optimality approaches.In Random, we uniformly at random select a subset of datasetsfrom the set. The KMeans identifies core datasets as the closest onesto cluster centers for clusters being selected in the space of datacharacteristics . Two additional baselines are A-optimality andD-optimality. They constitute two fundamental criteria focusedon obtaining the lowest possible error of a model that predictsperformance and the error for parameters estimation of a model .Technical details are provided in Appendix C.By selecting six datasets per method and calculating Spearmanscorrelation across 500 simulations, our results indicate superiorperformance of the KMeans, as shown in .",
  ": Spearman correlation among metrics across six se-lected datasets compared to the entire set of 30 datasets": "Clustering datasets using their characteristics With theclustering approach described above, we have generated rankingsand metrics for different clusters as illustrated in , and thespecific datasets for each cluster are listed in .Cluster 1 consists of six datasets characterized by a high num-ber of items relative to users. For example, the Amazon TV datasetincludes approximately 50K users, 216K items, and 2M in-teractions. Cluster 2 also includes six datasets, each marked bymoderate characteristic values and relative sparsity. For instance,the Reddit dataset comprises 4K users and items each, witharound 70K interactions. Cluster 3 is smaller, containing just 3datasets, where the number of users significantly exceeds the num-ber of items. This pattern suggests a different interaction dynamiccompared to other clusters. Cluster 4 includes 9 datasets with amoderate number of items and users. Typically, the number of itemssurpasses the number of users in these datasets, which have fewer",
  "Random": "2.17 (0.0136) 3.0 (0.0585) 2.0 (0.1244) 3.44 (0.0298) 4.33 (0.1044) 1.33 (0.0789) 3.17 (0.0131) 5.0 (0.0556) 3.33 (0.1228) 5.0 (0.0268) 4.67 (0.1056) 6.0 (0.0526) 4.17 (0.0115) 3.67 (0.0575) 4.67 (0.118) 3.67 (0.0279) 6.33 (0.0796) 3.0 (0.0619) 3.67 (0.0121) 8.0 (0.0496) 5.33 (0.1149) 5.67 (0.0259) 3.67 (0.1683) 7.0 (0.0483) 5.0 (0.0108) 4.83 (0.0553) 6.0 (0.1153) 5.56 (0.0248) 5.67 (0.0824) 4.0 (0.0612) 5.17 (0.0096) 5.17 (0.0551) 7.33 (0.1092) 6.44 (0.0236) 8.0 (0.0676) 5.67 (0.0587) 6.0 (0.0091) 5.5 (0.0541) 4.33 (0.1195) 6.89 (0.0233) 4.33 (0.0921) 4.33 (0.0556) 8.33 (0.0015) 3.33 (0.0585) 3.67 (0.1219) 4.89 (0.0268) 5.0 (0.087) 5.0 (0.0686) 8.17 (0.0059) 7.17 (0.0465) 8.33 (0.1024) 5.67 (0.0228) 4.67 (0.1591) 8.67 (0.0464) 9.17 (0.0025) 9.33 (0.0413) 10.0 (0.082) 7.78 (0.0125) 10.33 (0.0577) 10.0 (0.0256) 11.0(0.0) 11.0 (0.0018) 11.0 (0.0024) 11.0 (0.0003) 9.0 (0.0318) 11.0 (0.0004)",
  ": Ranks and Geom. mean (in brackets) aggregated@10 on different clusters of datasets": "interactions (generally 1M, with one exception). Moreover, theSkPB and KuPB are highest in this cluster, meaning items have animbalanced probability distribution. For instance, the Retail datasetfeatures about 32K users, 52K items, and 342K interactions,with SkPB 2 and KuPB 6. Cluster 5 contains only 3 datasets,which are the smallest in terms of user and item counts but are themost densely populated, with densities ( 0.1). For example, theKuairec small dataset has about 1400 users, 3100 items, and ahigh density of 0.8. This cluster shows the lowest SkPB, indicatinga more uniform item usage across users. Cluster 6 consists of thelargest datasets, where both the user and item counts typically 100, and interactions 20, denoting sparse interactions. Aprominent dataset in this cluster is MegaMarket, with 184K users, 167K items, and 25M interactions. Note that the characteristicsof the datasets and the clusters identified herein are derived fromthe preprocessed versions of these datasets.Cluster Analysis Summary. The EASE consistently performswell across most clusters, with LightGCL only outperforming itin Cluster 5. Other approaches exhibit less stability. For example,LightGCL has an average rank of 8 for Cluster 2, while SLIM has thesecond-best average rank for it. This variability is partly due to thedifferences in cluster complexity, as evidenced by EASEs fluctuat-ing geometric mean from 0.0136 to 0.1244. These results underlinethe importance of using datasets with similar characteristics foreffective offline evaluation for business use cases.",
  "RecSys Performance Variability overDatasets": "Drawing upon the insights from the benchmarks, we conclude themain part of the article with an analysis of how different algorithmsperform in relation to specific dataset characteristics.ItemKNN: Despite its interpretability and suitability as a base-line, ItemKNN shows limited effectiveness, excelling only in datasetswith specific characteristics such as low Shape values or moder-ate Density, including Movielens 1M, GoodReads, and Amazon MI.These findings align with challenges of sparsity and scalabilitytypical of neighborhood-based models . Matrix Factorization Methods (LightFM, ALS, BPR): Thesealgorithms vary in performance depending on dataset characteris-tics. Our analysis identifies that sparsity remains a significant limi-tation . While these methods serve as reliable baselines, theirefficacy is more pronounced in smaller datasets, like Foursquare,where the data structure may not be extensively sparse. LightFMtends to perform less efficiently in terms of computation time inlarger datasets. ALS demonstrates more robust performance acrossvarious scales, making it a versatile choice for both moderatelysized and large datasets.Linear Models (SLIM, EASE): These models show consistentlyhigh performance across a variety of datasets. EASE, in particular,excels in datasets with extensive user interaction data, support-ing findings from . However, the significant computationaldemands of EASE, including both time and memory resources, maylimit its use in settings with restricted computational capabilities.Neural and Graph-Based Models: Demonstrating superiorperformance, especially in dense or highly connected environments,MultiVAE and graph-based method LightGCN are particularly ef-fective. MultiVAE performs well in datasets with moderate userinteractions, benefiting from Bayesian priors that help managethe inherent uncertainty in sparse data. Graph-based models excelin datasets with high connectivity, such as social networks (e.g.,Gowalla, Yelp), where the relational structure can be fully exploitedto enhance recommendation quality.",
  "Conclusions": "Our paper introduces a novel benchmarking system for recom-mender systems. It integrates a rigorous pipeline that leveragesmultiple datasets, hyperparameter tuning, and validation strategy,as well as an aggregation procedure for metrics across differentdatasets. Our approach is interpretable and robust, working fordistinct metrics used for RecSys evaluation. Among the consideredmethods, EASE is a clear winner with respect to all considered ag-gregation strategies. Other methods show inferior performance onaverage while being interesting for particular subdomains identifiedby our clustering scheme.Further research provides deeper insight related to the stabilityand efficiency of ranking. Due to the usage of 30 datasets, two ofwhich are open-sourced in this study, the results are robust in di-verse considered scenarios. Via our clustering procedure, we obtaina collection of 6 datasets that also provides a consistent ranking,achieving efficiency and reliability simultaneously. Additional ex-periments confirm the stability of our benchmark with respect toreducing the number of considered datasets, methods, and adver-sarial manipulation of the list of methods. Overall, our researchoffers a streamlined guide and valuable datasets for advancing rec-ommender system studies that can be used both by practitionersduring the selection of a method and researchers during the evalu-ation of a novel idea.",
  "Vito Walter Anelli, Alejandro Bellogn, Tommaso Di Noia, Dietmar Jannach,and Claudio Pomo. 2022. Top-N recommendation algorithms: A quest for thestate-of-the-art. In ACM UMAP. 121131": "Vito Walter Anelli, Alejandro Bellogn, Antonio Ferrara, Daniele Malitesta, Fe-lice Antonio Merra, Claudio Pomo, Francesco Maria Donini, and TommasoDi Noia. 2021. Elliot: A comprehensive and rigorous framework for reproduciblerecommender systems evaluation. In ACM SIGIR. 24052414. Vito Walter Anelli, Tommaso Di Noia, Eugenio Di Sciascio, Claudio Pomo, andAzzurra Ragone. 2019. On the discriminative power of hyper-parameters incross-validation and how to choose them. In ACM RecSys. 447451.",
  "Eunjoon Cho, Seth A Myers, and Jure Leskovec. 2011. Friendship and mobility:user movement in location-based social networks. In ACM SIGKDD. 10821090": "Pierre Jean A Colombo, Chlo Clavel, and Pablo Piantanida. 2022. Infolm: A newmetric to evaluate summarization & data2text generation. In AAAI Conferenceon Artificial Intelligence, Vol. 36. 1055410562. Jacek Dbrowski, Barbara Rychalska, Micha Daniluk, Dominika Basaj, KonradGouchowski, Piotr Bbel, Andrzej Michaowski, and Adam Jakubowski. 2021. Anefficient manifold density estimator for all recommendation systems. In ICONIP.Springer, 323337. Yashar Deldjoo, Alejandro Bellogin, and Tommaso Di Noia. 2021. Explaining rec-ommender systems fairness and accuracy through the lens of data characteristics.Information Processing & Management 58, 5 (2021), 102662.",
  "G0ohard. 2019. Rekko Dataset competition. Accessed: 2024-13-6": "Chongming Gao, Shijun Li, Wenqiang Lei, Jiawei Chen, Biao Li, Peng Jiang, Xiang-nan He, Jiaxin Mao, and Tat-Seng Chua. 2022. KuaiRec: A Fully-Observed Datasetand Insights for Evaluating Recommender Systems. In ACM CIKM. 540550. Pieter Gijsbers, Marcos LP Bueno, Stefan Coors, Erin LeDell, Sbastien Poirier,Janek Thomas, Bernd Bischl, and Joaquin Vanschoren. 2024. AMLB: an AutoMLbenchmark. Journal of Machine Learning Research 25, 101 (2024), 165.",
  "Matthew Richardson, Rakesh Agrawal, and Pedro Domingos. 2003. Trust man-agement for the semantic web. In ISWC. Springer, 351368": "Mark Rofin, Vladislav Mikhailov, Mikhail Florinsky, Andrey Kravchenko, TatianaShavrina, Elena Tutubalina, Daniel Karabekyan, and Ekaterina Artemova. 2023.VotenRank: Revision of Benchmarking with Social Choice Theory. In EACL.670686. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, SeanMa, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al.2015. Imagenet large scale visual recognition challenge. International journal ofcomputer vision 115 (2015), 211252.",
  "Aixin Sun. 2023. Take a Fresh Look at Recommender Systems from an EvaluationStandpoint. In ACM SIGIR. 26292638": "Zhu Sun, Hui Fang, Jie Yang, Xinghua Qu, Hongyang Liu, Di Yu, Yew-Soon Ong,and Jie Zhang. 2022. DaisyRec 2.0: Benchmarking Recommendation for RigorousEvaluation. IEEE Transactions on Pattern Analysis and Machine Intelligence (2022). Zhu Sun, Di Yu, Hui Fang, Jie Yang, Xinghua Qu, Jie Zhang, and Cong Geng. 2020.Are we evaluating rigorously? benchmarking recommendation for reproducibleevaluation and fair comparison. In ACM RecSys. 2332.",
  "Mengting Wan, Rishabh Misra, Ndapa Nakashole, and Julian J. McAuley. 2019.Fine-Grained Spoiler Detection from Large-Scale Review Corpora. In ACL. 26052610": "Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael,Felix Hill, Omer Levy, and Samuel Bowman. 2019. Superglue: A stickier bench-mark for general-purpose language understanding systems. NeurIPS 32 (2019). Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and SamuelBowman. 2018. GLUE: A Multi-Task Benchmark and Analysis Platform forNatural Language Understanding. In Proceedings of the 2018 EMNLP WorkshopBlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. 353355.",
  "Shuai Zhang, Lina Yao, Aixin Sun, and Yi Tay. 2019. Deep learning based rec-ommender system: A survey and new perspectives. ACM CSUR 52, 1 (2019),138": "Wen Zhang, Yuhang Du, Taketoshi Yoshida, and Ye Yang. 2019. DeepRec: Adeep neural network approach to recommendation with item embedding andweighted loss function. Information Sciences 470 (2019), 121140. Wayne Xin Zhao, Yupeng Hou, Xingyu Pan, Chen Yang, Zeyu Zhang, Zihan Lin,Jingsen Zhang, Shuqing Bian, Jiakai Tang, Wenqi Sun, et al. 2022. RecBole 2.0:towards a more up-to-date recommendation library. In ACM CIKM. 47224726. Wayne Xin Zhao, Zihan Lin, Zhichao Feng, Pengfei Wang, and Ji-Rong Wen. 2022.A revisiting study of appropriate offline evaluation for top-N recommendationalgorithms. ACM Transactions on Information Systems 41, 2 (2022), 141. Wayne Xin Zhao, Shanlei Mu, Yupeng Hou, Zihan Lin, Yushuo Chen, Xingyu Pan,Kaiyuan Li, Yujie Lu, Hui Wang, Changxin Tian, et al. 2021. Recbole: Towards aunified, comprehensive and efficient framework for recommendation algorithms.In ACM CIKM. 46534664.",
  "ANew Datasets": "We introduce two new datasets named MegaMarket and Zvuk.Their size exceeds the sizes of many publicly accessible datasets.Additionally, these datasets are from fields that are less commonlyrepresented in existing research.MegaMarket documents user interactions, tracking events likeviews, favorites, additions to carts, and purchases over a five-monthperiod from January 15 to May 15, 2023. It comprises a total of196,644,020 events involving 3,562,321 items across 10,001 distinctcategories, contributed by 2,730,776 unique users.Content:",
  "A.1Preprocessing": "We focus on collaborative filtering, converting datasets into implicitfeedback through threshold binarization. For datasets with ratingsfrom 0 to 5, we use a threshold of 3.5 for positive feedback. Fordatasets with weights from 0 to 1, such as percentage-based data,we use a threshold of 0.3. Other datasets use thresholds based onthe drop ratio.For datasets like MegaMarket, which track user behavior viaevents, preprocessing is required to handle repeated user-item pairs.We preprocess the data in the following manner:",
  "BAdditional ExperimentsB.1Reliability Tests: Incorporation orExclusion of Marginally Different Methods": "We examine the impact of introducing a model similar to existingones, using a parameter to adjust the metrics of a chosen model.We set |1 | 0.15. For example, the case when = 1. meansadding the new model equal to the selected one.Adjusting makes the new model perform slightly better orworse than the chosen model. Results, shown in a, indi-cate most aggregation methods are stable, except for Mean ranksand Copeland. The Minimax method is excluded due to poorSpearman correlation.",
  "B.2Reliability tests: Adding a New Best Method": "We analyze the impact of introducing a new best model. We identifythe best metric values across all datasets and utilize an arbitraryvalue (following similar conditions as in Section B.1). We restrictthe parameter as follows: . For instance, when = 1, itsignifies the addition of a new model with metrics equal to thecurrent best metrics.b shows the Minimax method is unstable for any ,and DM AUC becomes unstable as 4. This behavior can beattributed to the direct dependence of the best metric values onthe parameter . Conversely, all other aggregation methods remainentirely stable in this scenario.",
  "B.3Sensitivity to Hyperparameters ofAggregations": "The results for varying the hyperparameters case are presentedin c. The vertical dotted line mean the case when = 3.We see that the DM AUC aggregating method is more stable thanthe DM LBO method. Instability of the DM AUC can be interpretedby the proximity of the curves of the methods (in , forexample, LightFM AUC and MultiVAE AUC curves look visuallysimilar to each other). If the more methods have similar perfomancecurves, then the more the Spearman correlation value decreases (in",
  "CChoosing the Optimal Subset of Datasets": "In the KMeans clustering process, we first standardize the data toensure uniformity. Next, we apply Principal Component Analysis(PCA) to reduce the number of dimensions while preserving asmuch variance as possible, addressing the problem of correlatedfeatures. We then use the Isolation Forest method to detect andremove outliers, decreasing the dataset size from 30 to 25 observa-tions. With the data now prepared, we move on to the clusteringstage, employing the K-means algorithm. During clustering, weevaluate the Silhouette Coefficient and Davies-Bouldin Scores todetermine the optimal number of clusters, which we found to be 6.Finally, we select datasets that are closest to the cluster centres forfurther analysis.The next two approaches assume that resulting metrics can bepredicted with the linear regression method:"
}