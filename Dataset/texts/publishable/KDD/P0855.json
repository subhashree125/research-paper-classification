{
  "ABSTRACT": "Machine learning systems are notoriously prone to biased predic-tions about certain demographic groups, leading to algorithmicfairness issues. Due to privacy concerns and data quality problems,some demographic information may not be available in the trainingdata and the complex interaction of different demographics canlead to a lot of unknown minority subpopulations, which all limitthe applicability of group fairness. Many existing works on fairnesswithout demographics assume the correlation between groups andfeatures. However, we argue that the model gradients are also valu-able for fairness without demographics. In this paper, we show thatthe correlation between gradients and groups can help identify andimprove group fairness. With an adversarial weighting architecture,we construct a graph where samples with similar gradients are con-nected and learn the weights of different samples from it. Unlikethe surrogate grouping methods that cluster groups from featuresand labels as proxy sensitive attribute, our method leverages thegraph structure as a soft grouping mechanism, which is much morerobust to noises. The results show that our method is robust tonoise and can improve fairness significantly without decreasingthe overall accuracy too much.",
  "To whom correspondence should be addressed": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 25, August 37, 2025, Toronto, ON, Canada. 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-1245-6/25/08...$15.00 ACM Reference Format:Yingtao Luo, Zhixun Li, Qiang Liu, and Jun Zhu. 2025. Fairness withoutDemographics through Learning Graph of Gradients. In Proceedings of the31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.1(KDD 25), August 37, 2025, Toronto, ON, Canada. ACM, New York, NY, USA,9 pages.",
  "INTRODUCTION": "Fairness in machine learning has become an urgent concern, as ma-chine learning systems can be biased against certain demographicgroups, which contributes to the socioeconomic disparities in manyareas such as healthcare , finance , etc. For example, whenlearning the default risk of financial loan applicants, due to certainbiases, the model prediction can be inaccurate for some protectedgroups, such as minorities. To deal with this issue, some approaches follow the Rawlsian Max-Min fairness to minimizethe worst-case errors over all groups. Some other methods mini-mize the prediction accuracy gap between different demographicgroups . Most of these existing methods require sensitiveattributes, such as race, gender, etc., to identify which group is dis-criminated against by machine learning models. However, due toprivacy concerns, these sensitive attributes are often not accessibleto algorithm developers. For example, certain regulations like theHIPAA privacy rule have set up safeguards, which now limit notonly direct access but also indirect methods that infer sensitiveattributes, as these can lead to re-identification risks and other pri-vacy concerns. An additional challenge arises when dealing withnumerous demographic attributes. Defining the intricate interplayamong various demographic factors for minority subgroups before-hand can be complex, and the potential protected groups increaseexponentially as the number of sensitive attributes grows. This, inturn, escalates the difficulty of identifying the most disadvantagedgroup. Consequently, it is crucial to advocate for machine learningfairness that does not rely on demographic information.To ensure fairness without demographics, many existing meth-ods with proxy sensitive attributes assume thecorrelation between sensitive attributes (groups) and non-sensitiveattributes (features), perform clustering to obtain surrogate groups,and enforce group fairness. The problem with these methods, how-ever, is the difficulty to guarantee a large overlap of the proxy",
  "KDD 25, August 37, 2025, Toronto, ON, Canada.Yingtao Luo, Zhixun Li, Qiang Liu, and Jun Zhu": "For simplicity, we assume = + , where is the linear coef-ficient and is the noise term representing the part in that isuncorrelated to . For simplicity, we can assume (, 2).Similarly, we have = + where is the linear coefficient and (, 2). The two noise terms and are unknown andstatistically independent. We assume that follows the standardnormal distribution after preprocessing, thus = 0, 2 = 1.Note that we can rearrange the given equalities as follows: =",
  "RELATED WORK2.1Group fairness for classification": "Group fairness is a concept that aims to ensure that the outcomes ofan algorithm are equitable across different subpopulations definedby sensitive attributes, such as race, gender, etc. To alleviate thegroup disparity , Equal Opportunity hopes that the true pos-itive rates should be the same for all subpopulations, and PredictiveEquality requires the equality of false positive rates. Prepro-cessing methods ensure that the data used for training isunbiased and representative of different subgroups by re-sampling,feature selection, etc. In-processing methods regu-larize the training process with fair constraints, sample reweighting,and adversarial training. Post-processing methods focuson adjusting the model prediction after training by threshold adjust-ment, calibration, etc., which are usually very efficient. However, toguarantee group fairness, the availability of sensitive informationis a necessity. Some papers also address fairness concernsby using techniques that are robust to noisy or shifting sensitiveattributes. Others propose methods to address fairness concernsin complex data structures . While certain policiesrestrict versatility, we still need to count on fairness without demo-graphic methods.",
  "Fairness without demographics": "In light of the challenges to discovering the worst-off groups dueto both the regulatory limitations and the complex interaction ofmany demographic variables , increasing methods are proposedin recent years to achieve fairness without demographics. Somemethods follow the Rawlsian Max-Min fairness to minimizethe empirical risk of the group with the least utility. For example,Distributionally Robust Optimization (DRO) proposes to use2-divergence to discover and minimize the worst-case distributionrepeatedly, which essentially only focuses on the learning of theworst-off group. Adversarial Reweighted Learning (ARL) usesan adversary network to generate sample weights that maximizethe empirical risk and performs weighted learning for the learnermodel. Based on the concept of computational identifiability, ARLhypothesizes that it can learn demographic information from datafeatures and labels. Surrogate grouping methods are also pro-posed to minimize the correlation between data features and modelprediction, or directly predict surrogate demographic groups and then perform group fairness algorithms . Some debias-ing methods also propose to identify the group disparities basedon clustering information and upsample the minority groups tobalance the distribution .",
  "THEORETICAL ANALYSIS3.1Problem Formulation": "Consider data (,,) with samples, where represents the non-sensitive features, represents the labels, and represents the sen-sitive attributes that determine the protected groups. Then, given, we need to predict without the knowledge of while satisfyingcertain fairness criteria with respect to . For example, the disparateimpact requires that the model prediction is independent ofsensitive attributes, the equalized odd requires the independence",
  "Definition of gradient": "Gradients provide both the training information about the learnermodel that indicates the algorithmic bias and the data informationthat indicates the data bias, therefore can help achieve fairness.First, there is a correlation between non-sensitive features andsensitive attributes . For example, the prevalence of diseases isoften affected by factors such as diets, which are affected by de-mographics such as religions and cultures. Second, as long as themodel prediction accuracy is unfair across different groups, ()is also correlated to sensitive attributes . Nevertheless, we do nothave true demographics as the label, so we do not know as anestimated function of and () a priori. Therefore, we proposeto use gradients to represent demographic groups.We consider a neural network parametrized by as (;) = ,where = ( , ). Thus, we have = (1, ...,) R asthe weight of the last layer where denotes the dimensionalityof the last latent representation. is the weight of all previouslayers. (;) = ( (; )), where () = /=1 . Thelast-layer gradient w.r.t. the cross entropy loss is calculated as",
  ", = () | | = ,(5)": "which is the multiplication of the latent representation of the non-sensitive feature and the prediction error (i.e., alternative to modelaccuracy ) of the label class. Here denotes the true value ofthe -th class in the label. We show that the undirected gradient toestimate the sensitive attributes is more accurate by both theoreticalanalysis and experimental verification.",
  "Relationship between gradients andsensitive demographics": "In this section, we discuss and theoretically analyze the effective-ness of gradients to represent sensitive demographics. We firstgenerally demonstrate, through the lens of information theory andas articulated in Theorem 3.1, that the distribution of gradients that combines the information of both input features and modelerror more closely aligns with sensitive demographic attributesas compared to input features, if input features are not perfect so-lutions for identifying demographics. Further, we explore underthe condition of linear relationships, as outlined in Lemma 1, thatmodel gradients are more effective than input features in a largercorrelation between input features and model prediction error. Theorem 3.1. The mutual dependence between gradients (thecombination of input features and model error) and sensitive attributesis larger than the mutual dependence between input features andsensitive attributes. If we denote sensitive demographics as s, modelprediction error as U, and input features as x, I(xU|s) > I(x|s).",
  "(|) = () (|).(7)": "Since is influenced by, or is a function of, and (different inputfeatures and demographics groups lead to different model error), wehave ( ;|) > 0. This means that introduces variations in that by itself cannot account for. This condition holds because and are not perfectly dependent on each other, that is, ( ;|) > 0.Therefore, we have ( |) ( |) > 0.Subtracting ( |) and (|), we have",
  "> 0.(8)": "Then, we demonstrate that, as a special case of Theorem 3.1, thegradient of the last layer of a deep neural network can representsensitive demographics better than input features alone. We firstintroduce Lemma 1. Based on Lemma 1, we show by Proposition3.2 that the last-layer gradient of a deep learning model is sufficientfor the estimation of demographic groups. Intuitively, the last layercan capture high-level patterns in the data to make predictions,which is enough for representing sensitive demographics. Here,Pearson correlation in Lemma 1 is used as an example to illustrateour point. Specific nonlinear correlation is much harder to measureand analyze. Without further assumptions or knowledge about thedata, there could be many possible nonlinear relationships (e.g.,logarithmic, polynomial, exponential) and nonlinear correlationmeasurements (e.g., Hilbert-Schmidt Independence Criterion, Mu-tual Information, Maximal Information Coefficient).",
  ".(18)": "From observation, we find that when 2 + 2 = 0, (, ) = (,) = (,) = 1, regardless of the value of ,, , .Since we regard as a constant that is not subject to change, wecan conclude that 2 and 2 can directly determine the correlation.We have the correlation between and as",
  "(,) increasesin (, )": "Proposition 3.2. The last-layer gradient of the deep learning pre-diction model can have a stronger correlation to sensitive attributesthan non-sensitive input features. If we denote input features as ,model prediction error as , last-layer representation as , and sensi-tive attribute classes as , we have (,) > (,). Proof. We can assume that the correlation between the last-layer representation and the label is larger than the correlation be-tween the input features and the label, i.e., (,) > (,).This assumption is very likely to hold in practice because the pur-pose of a neural network is to learn a representation to make iteasier to predict the label. As long as the neural network is effec-tively learning representations, this assumption holds. Accordingto Lemma 1, we have",
  "=1((;),).(30)": "where denotes the number of samples. The optimal hypothesis that maximizes the utility is replaced by the model parameters minimizing the loss function, and the selection of demographicgroup that minimizes the utility is replaced by the reweightingof each demographic group with a learning weight . When thedemographic groups are unknown to us, people learn to predict as a function of and , or simply assign weights to each samplebased on features and labels.The main difference between predicting and directly predicting is whether a group partition is enforced. The former surrogategrouping methods mandate identifying the demographic groupsand then assigning weights to different groups. The latter ones, suchas the Adversarial Reweighted Learning (ARL) approach, directlyassign weight generated by an adversary model to each sample.Since noisy data is an outlier to the true data distribution, it isdifficult for a model to learn it correctly. To maximize the loss, theadversary will increase its sample weight to be unreasonably large.When generating weight for each sample instead of consideringgroups, this problem will be enlarged. In light of this issue, we pro-pose to leverage the advantage of graph learning, which allows theinteractions among connected neighbors to alleviate the problemswith noisy labels. By learning from a graph, the weight genera-tion of every sample will be dependent on the interdependenciesbetween others with similar demographics. Assuming that mostsamples are not outliers, each outlier should be connected to somesamples to avoid assigning large weights for outliers. Based onthis notion, we propose to construct a graph where each sample isconnected to the KK nearest samples. In this paper, we propose a novel learning framework to addressthe deficiencies of existing approaches. The overall framework isshown in . Upon the observation that hard boundariesfor group partition can be intractable with too many unknowndemographics, we propose a method that mimics the groupingeffect to alleviate the noises but does not specify boundaries. Indetail, our method can reformulate Eq. 7 as follows",
  "(,,) = =1 ,(34)": "where = + . Here, = (0) R is the embeddingof gradients, with (0) R. = (1) R is the dataembedding, with (1) R where denotes the number ofdimensions of . We use to represent the weight for sample, which is normalized according to Eq. 11. On the other hand, R is the adjacency matrix for constructing the Graph ofGradients (GoG), which is calculated as follows for a certain entry,",
  "(,) = ( (2)) R1,(36)": "which is a one-layer graph convolutional network that takes eachsamples data as input and outputs the sample weight. Therefore, = [ (0), (1), (2)]. To generate the weights, each sample,which is a node in the graph, aggregates the information of similarsamples with K-nearest gradients. Here, (2) R 1 allows thegraph network to learn the importance of neighboring samples foraggregation, and denotes the activation function. In practice, weimplement the above equation via its improved version, such as",
  "(2)) R1,(37)": "where = + where is the identity matrix and is the diagonalnode degree matrix of . The learning of GoG aims at enforcing agrouping mechanism based on the prior from the learner, withoutknowing the actual demographic groups a priori. When performingreweighting to minimize group disparity, samples that share similargradients are connected and can interact with each other.The overall algorithm is detailed in Algorithm 1. The overallcomplexity is linear. Therefore, it does not significantly increasethe time complexity of the original learner model, which is alsoof an O(n) complexity. The K-nearest neighbor algorithm costs(), where is the dimensionality of the input sequence, isthe batch size, is the number of data samples. Because for eachbatch, the complexity is (2), and there are / batches in total.Then, the one-layer GCN has the complexity of ( + ) intotal, where || = is the number of edges in the graph (accordingto the KNN algorithm), and is the dimensionality of the outputfor each node. Note that, only is a scalable variable of interest thatgrows with more data samples. Therefore, in total, the complexityof GoG is () + ( + ), which is overall of O(n).",
  "Experimental Setup": "We randomly divide each dataset by samples into the training,validation, and testing sets in a 0.75:0.1:0.15 ratio. We tune all thehyperparameters to obtain the optimal evaluation on the validationset for each model. The range of learning rate is {1e-2, 3e-3, 1e-3},batch size is {32, 64, 128}, hidden dimension is {16, 32, 64}, dropoutrate is {0,1, 0.5}, K is {3, 10, 30}. The training will stop if the accuracyof the worst group validation metrics does not increase in 20 epochs,and the test performance will be recorded. All results are averagedunder five random seeds.We evaluate our method on three diverse datasets. COMPAS isa small-sized tabular dataset, BNP is a larger-sized tabular datasetwith 20x more samples, and MIMIC-III is a large-scale sequentialdataset with 100x more samples with noise and sparsity due tomissing values. All datasets are highly imbalanced. COMPAS Dataset The Correctional Offender ManagementProfiling for Alternative Sanctions Dataset2 is a public crim-inology dataset containing the risk of recidivism. There are7214 samples and 52 attributes. We use an MLP as the base-line. Sex, age, and race are protected attributes. BNP Dataset The BNP Paribas Cardif Claims ManagementDataset 3 is a public credit assessment dataset from BNPParibas Cardif. There are 23 Categorical and 108 continuousattributes with 114321 data samples. We use a simple MLPas the baseline. Based on the 2 statistics, v22, v56, v79, andv113 are selected as protected attributes. MIMIC-III Dataset The Medical Information Mart for In-tensive Care database4 has patients who stayed in thecritical care units of the Beth Israel Deaconess Medical Cen-ter between 2001 and 2012. There are 53423 patients and651048 diagnosis codes. The goal is to predict future di-agnoses for multi-class classification. We follow the datapipeline in and use LSTM as the baseline. Sex, age,and race are protected attributes.We compare our method that uses the graph of gradients with thefollowing fairness-without-demographics models that use featuresfor weight generation or clustering.",
  "FairCB: A fair algorithm that performs cluster-basedoversampling that identifies the subgroups and removes classbias in data": "FairRF: A fair algorithm that minimizes the correlationbetween data features and model predictions with impor-tance weighting.We evaluate fairness based on the following metrics. We areaware of the many possible fairness metrics for evaluation, such asdisparate imapct. However, first, we are only interested in improv-ing the model performance across protected groups instead of the",
  "DatasetApproachW. Acc()E. Odds()O. Acc()": "COMPASBaseline41.050.22%42.102.47%64.320.35%COMPASDRO44.200.42%31.662.98%56.130.45%COMPASARL44.320.39%31.893.36%56.240.41%COMPASFairCB43.460.14%33.731.53%57.320.11%COMPASFairRF44.240.18%31.541.88%56.670.21%COMPASGoG44.850.37%29.132.53%57.150.43% BNPBaseline48.250.20%47.730.84%73.280.32%BNPDRO49.100.24%39.470.98%68.620.42%BNPARL48.920.20%39.141.36%68.800.38%BNPFairCB48.000.31%42.561.55%68.350.14%BNPFairRF48.790.24%40.891.68%68.960.31%BNPGoG50.270.21%33.601.47%69.550.51% MIMIC-IIIBaseline19.320.20%29.281.45%24.640.29%MIMIC-IIIDRO19.680.14%25.752.19%22.620.22%MIMIC-IIIARL19.570.10%25.141.90%22.800.18%MIMIC-IIIFairCB19.080.38%29.961.95%23.010.47%MIMIC-IIIFairRF19.590.14%26.051.70%22.960.21%MIMIC-IIIGoG20.460.29%18.991.72%23.250.31% results; second, in medical datasets, as pointed out by clinicians ,since different patients do have different genotypes and phenotypes,it can be wrong to assume different patients to have similar prob-abilities for a certain disease. Therefore, we only use worst-caseaccuracy and Equalized odds in our experiments to avoid possibleconcerns. We also report the overall model performances to showthe trade-off between fairness and performance. Worst-Group AUC/Accuracy: For COMPAS and BNP, weadopt the Area Under the Curve (AUC) for the worst sub-group. For MIMIC-III, we adopt the top-20 accuracy of theworst subpopulation. We abbreviate it as W. Acc. Equalized Odds: We use the equalized odds (E. Odds), whichrequires the probability of instances with any two protectedattributes, being assigned to an outcome are equal, giventhe label. In particular, for MIMIC-III, we cluster labels intoeighteen diagnosis categories based on the ICD-9 code cate-gories5 to calculate E. Odds. We calculate the total differenceof all groups as",
  "MIMIC-IIIGoG20.460.29% 18.991.72% 23.250.31%MIMIC-III-Graph19.980.25%24.270.56%22.820.27%MIMIC-III-Grad20.310.21%20.080.42%23.140.26%": "the W.Acc and E. Odds outperforming other fairness algorithms.Furthermore, while all fairness algorithms result in reduced accu-racy, our algorithm surpasses them in overall accuracy, indicating asuperior balance between fairness and accuracy. GoG demonstratesto work well with different deep learning baselines, such as MLPfor COMPAS and BNP and LSTM for MIMIC-III in our cases.Notably, we observe that our methods improvements are moresignificant in complex datasets. MIMIC-III and BNP datasets containmore complex structures, more demographic subgroups, attributes,and data samples than the COMPAS dataset and are likely to havemore outliers, as mentioned in their sources. In particular, the noisesin data can damage the fairness performance severely.In addition, we provide an empirical time cost comparison. Withan A100 80G graphical card, on MIMIC-III dataset, running thelearner costs 2.7 seconds per epoch, while running the GoG costs6.4 seconds per epoch; on the COMPAS dataset, it is 0.6 secondsagainst 1.6 seconds; on BNP dataset, it is 1.5 seconds against 3.3seconds. The additional training time increases at most by twice,and the inference speed is the same, since the weighting providedby GoG only happens during training.",
  "Robustness Study": "To assess our models robustness to noise, we create noisy datasetsby altering the labels of 10% of samples in the original datasets,treating these false labels as true during evaluation. Samples withwrong labels are harder to learn due to the deviation from the datadistribution, which forces the fairness algorithm to focus blindlyon the samples that cannot improve the model fairness for realsamples. We train various models on the noisy datasets to evaluatefairness, with results shown in Figure. 2. Some baselines can occa-sionally perform even worse than the baseline, which shows thatnoisy labels can severely damage existing fairness algorithms. Ourmodels fairness metrics do not degrade as quickly as other fairnessalgorithms under noise, showing great robustness.",
  "MIMIC-IIILast20.460.29% 18.991.72% 23.250.31%MIMIC-IIIFirst20.390.28%19.201.58%23.160.30%": "when a particular part is removed. The ablation results, shown in, indicate that both components contribute positively, asthe model without graph or gradients still outperforms baselines.Furthermore, we observe that graph learning (-Grad) plays a morecritical role, particularly for the more complex BNP and MIMIC-IIIdatasets. GoG contributes to the improved trade-off between fair-ness and accuracy. In addition, we also conduct a hyperparametersensitivity study for in , which shows that does notaffect performance. When we use first-layer gradient instead of thelast-year gradient, as shown in , the performance decreasesslightly. This demonstrates the effectiveness of last-layer gradient.",
  "shows that while it is almost impossible to separate peo-ple by race based on input features shown in a, we find": "a) Distribution of input features by t-SNEb) Distribution of last-layer gradient by t-SNE : The distribution of input features and last-layergradients upon different race groups for MIMIC-III dataset(Purple: White, Yellow: BLACK, Green: HISPANIC, Otherraces are not considered for convenience). that the least populated subpopulation group, colored by green, isdistributed closely on the graph of last-layer gradient shown in Fig-ure 3b. Last-layer gradient is distributed more evenly in the space,which helps the classification of groups. This case study supportsour claim that gradient is a better proxy for true demographics.",
  "CONCLUSION": "In this study, we address the fairness concerns in machine learningalgorithms, focusing on the challenges posed by the complex inter-play of demographic variables and regulatory constraints, whichoften render demographic information unknown. We present anovel approach to tackle the limitations of existing methods. Wedemonstrate that model gradient can better identify unknown de-mographics and propose the graph of gradients by connecting eachsample to its K-nearest neighbors to identify demographic groupsand generate sample weights. Experimental results reveal that ourmethod significantly enhances the algorithmic fairness, exhibitingnotable robustness to noise that can be common in the real world.Since GoG depends on the gradients of learner, there might be alimitation if the learner cannot effectively capture the data pattern,e.g., data is not big enough and model is not strong enough.",
  "Yushun Dong, Ninghao Liu, Brian Jalaian, and Jundong Li. 2022. Edits: Modelingand mitigating data bias for graph neural networks. In Proceedings of the ACMweb conference 2022. 12591269": "Yushun Dong, Song Wang, Jing Ma, Ninghao Liu, and Jundong Li. 2023. Inter-preting unfairness in graph neural networks via training node attribution. InProceedings of the AAAI Conference on Artificial Intelligence, Vol. 37. 74417449. Mengnan Du, Subhabrata Mukherjee, Guanchu Wang, Ruixiang Tang, AhmedAwadallah, and Xia Hu. 2021. Fairness via representation neutralization. Advancesin Neural Information Processing Systems 34 (2021), 1209112103. Milena A Gianfrancesco, Suzanne Tamang, Jinoos Yazdany, and Gabriela Schma-juk. 2018. Potential biases in machine learning algorithms using electronic healthrecord data. JAMA internal medicine 178, 11 (2018), 15441547. Stephen Giguere, Blossom Metevier, Bruno Castro da Silva, Yuriy Brun, PhilipThomas, and Scott Niekum. 2022. Fairness guarantees under demographic shift.In International Conference on Learning Representations.",
  "Nayeong Kim, Sehyun Hwang, Sungsoo Ahn, Jaesik Park, and Suha Kwak. 2022.Learning Debiased Classifier with Biased Committee. In Advances in NeuralInformation Processing Systems": "Preethi Lahoti, Alex Beutel, Jilin Chen, Kang Lee, Flavien Prost, Nithum Thain,Xuezhi Wang, and Ed Chi. 2020. Fairness without demographics through adver-sarially reweighted learning. Advances in neural information processing systems33 (2020), 728740. Zhixun Li, Yushun Dong, Qiang Liu, and Jeffrey Xu Yu. 2024. Rethinking fairgraph neural networks from re-balancing. In Proceedings of the 30th ACM SIGKDDConference on Knowledge Discovery and Data Mining. 17361745.",
  "Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. 2019.Distributionally Robust Neural Networks. In International Conference on LearningRepresentations": "Changjian Shui, Gezheng Xu, Qi Chen, Jiaqi Li, Charles X Ling, Tal Arbel, BoyuWang, and Christian Gagn. 2022. On learning fairness and accuracy on multiplesubgroups. Advances in Neural Information Processing Systems 35 (2022), 3412134135. Yu Wang, Yuying Zhao, Yushun Dong, Huiyuan Chen, Jundong Li, and TylerDerr. 2022. Improving fairness in graph neural networks via mitigating sensitiveattribute leakage. In Proceedings of the 28th ACM SIGKDD conference on knowledgediscovery and data mining. 19381948. Shen Yan, Hsien-te Kao, and Emilio Ferrara. 2020. Fair class balancing: Enhancingmodel fairness without observing sensitive attributes. In Proceedings of the 29thACM International Conference on Information & Knowledge Management. 17151724. Tianxiang Zhao, Enyan Dai, Kai Shu, and Suhang Wang. 2022. Towards FairClassifiers Without Sensitive Attributes: Exploring Biases in Related Features.In Proceedings of the Fifteenth ACM International Conference on Web Search andData Mining. 14331442."
}