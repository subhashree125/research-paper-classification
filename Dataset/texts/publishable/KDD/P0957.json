{
  "ABSTRACT": "Tabular datasets play a crucial role in various applications. Thus,developing efficient, effective, and widely compatible predictionalgorithms for tabular data is important. Currently, two prominentmodel types, Gradient Boosted Decision Trees (GBDTs) and DeepNeural Networks (DNNs), have demonstrated performance advan-tages on distinct tabular prediction tasks. However, selecting aneffective model for a specific tabular dataset is challenging, often de-manding time-consuming hyperparameter tuning. To address thismodel selection dilemma, this paper proposes a new framework thatamalgamates the advantages of both GBDTs and DNNs, resulting ina DNN algorithm that is as efficient as GBDTs and is competitivelyeffective regardless of dataset preferences for GBDTs or DNNs. Ouridea is rooted in an observation that deep learning (DL) offers alarger parameter space that can represent a well-performing GBDTmodel, yet the current back-propagation optimizer struggles toefficiently discover such optimal functionality. On the other hand,during GBDT development, hard tree pruning, entropy-driven fea-ture gate, and model ensemble have proved to be more adaptableto tabular data. By combining these key components, we present aTree-hybrid simple MLP (T-MLP). In our framework, a tensorized,rapidly trained GBDT feature gate, a DNN architecture pruningapproach, as well as a vanilla back-propagation optimizer collabora-tively train a randomly initialized MLP model. Comprehensive ex-periments show that T-MLP is competitive with extensively tunedDNNs and GBDTs in their dominating tabular benchmarks (88datasets) respectively, all achieved with compact model storageand significantly reduced training duration. The codes and fullexperiment results are available at",
  "The corresponding author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08",
  "classification and regression, tabular data, green AI, AutoML": "ACM Reference Format:Jiahuan Yan, Jintai Chen, Qianxing Wang, Danny Z. Chen, and Jian Wu.2024. Team up GBDTs and DNNs: Advancing Efficient and Effective TabularPrediction with Tree-hybrid MLPs. In Proceedings of the 30th ACM SIGKDDConference on Knowledge Discovery and Data Mining (KDD 24), August2529, 2024, Barcelona, Spain. ACM, New York, NY, USA, 14 pages.",
  "INTRODUCTION": "Tabular data are a ubiquitous and dominating data structure in var-ious machine learning applications (e.g., click-through rate (CTR)prediction and financial risk detection ). Current prevalenttabular prediction (i.e., classification and regression) models can begenerally categorized into two main types: (1) Gradient BoostedDecision Trees (GBDTs) , a kind of classical non-deep-learning approach that has been extensively verified as test-of-timesolutions ; (2) Deep Neural Networks (DNNs), on whichcontinuous endeavors apply deep learning (DL) techniques fromcomputer vision (CV) and natural language processing (NLP) todevelop tabular learning methods such as meticulous architectureengineering and pre-training . Withrecent developments of bespoke tabular DNNs, increasing stud-ies reported their better comparability and even superior-ity to GBDTs, especially in complex data scenarios ,while classical thinking believes that GBDTs still completely sur-pass DNNs in typical tabular tasks , both evaluated withdifferent benchmarks and baselines, implying respective tabulardata proficiency of these two model types.For DNNs, their inherent high-dimensional feature spaces andsmooth back-propagation optimization gain tremendous success onunstructured data and capability of mining subtle featureinteractions . Besides, leveraging DNNs transfer-ability, recent popular tabular Transformers can be further im-proved by costly pre-training , like their counterparts in",
  "KDD 24, August 2529, 2024, Barcelona, SpainJiahuan Yan, Jintai Chen, Qianxing Wang, Danny Z. Chen, & Jian Wu": ": The average values (standard deviations) of all themethod ranks on TabBen (four dataset types). Num. andCat. denote numerical datasets (all features are numeri-cal) and categorical datasets (some features are categorical),respectively. Classif. and Reg. denote classification andregression tasks. Num. Reg. group includes only results ofregression on numerical datasets (similar notations are forthe others). || is the dataset number in each group. Baselinetest results are obtained based on the best validation resultsduring 400 iterations of HPT (according to the TabBen paperand repository). Detailed results are given in the Appendix.",
  "Comprehensive experiments on 88 datasets from 4 bench-marks, covering DNN- and GBDT-favored ones, show that a": ": Comparison of model cost-effectiveness on small andlarge datasets across popular tabular DNNs. and denotethe amounts of features and samples, is the parameternumber, and denotes the overhead of total training timeagainst the proposed T-MLP. We reuse performances andparameter sizes of the best model configurations in the FT-Transformer benchmark. is evaluated on an NVIDIA A100PCIe 40GB (see Sec. 4.1). Based on the fixed architecture andtraining configurations, T-MLP achieves stable model sizeand cheap training duration cost regardless of the data scale.",
  "T-MLP0.731.00.8640.751.08.768": "single T-MLP is competitive with advanced or pre-trainedDNNs, and T-MLP ensemble can even consistently outper-form them and is competitive with extensively tuned state-of-the-art GBDTs, all achieved with a compact model sizeand significantly reduced training duration. We develop an open-source Python package with APIs ofbenchmark loading, uniform baseline invocation (DNNs, GB-DTs, T-MLP), DNN pruning, and other advanced functionsas a developmental tool for the tabular learning community.",
  "RELATED WORK2.1Model Frameworks for Tabular Prediction": "In the past two decades, classical non-deep-learning methods have been prevalent for tabular prediction applications, espe-cially GBDTs due to their efficiency and robustness intypical tabular tasks . Because of the universal success of DNNson unstructured data and the development of computation devices,there is an increasing effort in applying DNNs to such tasks. Theearly tabular DNNs aimed to be comparable with GBDTs by emulat-ing the ensemble tree frameworks (e.g., NODE , Net-DNF ,and TabNet ), but they neglected the advantages of DNNs for au-tomatic feature fusion and interaction. Hence, more recent attemptsleveraged DNNs superiority, as they transferred successful neuralarchitectures (e.g., AutoInt , FT-Transformer ), proposedbespoke designs (e.g., T2G-Former ), or adopted pre-training(e.g., SAINT , TransTab ), reporting competitive or evensurpassing results compared to conventionally dominating GB-DTs in specific data scenarios . Contemporary surveys demonstrated that GBDTs and DNNs are two prevailing types offrameworks in current tabular learning research.",
  "largemodelcapacity": ": Our proposed T-MLP vs. existing tabular prediction approaches: GBDTs and DNNs. (a) GBDTs are classical non-deep-learning models for tabular prediction. (b) DNNs are emerging promising methods especially for large-scale, complex,cross-table scenarios. (c) T-MLP is a hybrid framework that integrates the strengths of both GBDTs and DNNs, accomplished viaGBDT feature gate tensorization, MLP framework pruning, simple block ensemble, and end-to-end back-propagation. It yieldscompetitive results on both DNN- and GBDT-favored datasets, with a rapid development process and compact model size. gMLP , MAXIM , and other vision MLPs , achiev-ing comparable or even superior results to their CNN or Trans-former counterparts with reduced capacity or FLOPs. This pure-MLP trend is also arising in NLP and other real-world appli-cations . Another lightweight scheme is model compression,where pruning is a predominant approach used to trim down largelanguage models from various granularity . Inthe tabular prediction field, there are a few pure-MLP studies, butall focusing on regularization or numerical embedding rather than the DNN architecture itself. Besides, model compressionof tabular DNNs has not yet been explored. We introduce relatedtechniques to make our T-MLP more compact and effective.",
  "TREE-HYBRID SIMPLE MLP": "We first review some preliminaries of typical GBDTs inferenceprocess and feature encoding techniques in current Transformer-based tabular DNNs. Next, we elaborate on the detailed designs ofseveral key components of T-MLP, including the GBDT feature gatefor sample-specific feature selection, the pure-MLP basic block, andGBDT-inspired fine-grained pruning for sparse MLPs. Finally, weprovide a discussion of the T-MLP workflow.",
  "Preliminaries": "Problem Statement. Given a tabular dataset with input features R and targets R , the tabular prediction task is tofind an optimal solution : R R that minimizes theempirical difference between the predictions and the targets .Here in current practice, the common choice of is either tradi-tional GBDTs (e.g., XGBoost , CatBoost , LightGBM ) ortabular DNNs (e.g., TabNet , FT-Transformer , SAINT ,T2G-Former ). A typical difference metric is accuracy or AUCscore for classification tasks, and is the root of mean squared error(RMSE) for regression. Definition 3.1: GBDT Feature Frequency. Given a GBDT modelwith decision trees (e.g., CART ), the GBDT feature frequencyof a sample denotes the number of times each feature is accessedby this GBDT on the sample. Specifically, the process of GBDT inference on a sample R provides times a single decision treeprediction () = CART() (), {1, 2, . . . , }. For each decisiontree prediction, there exists a sample-specific decision path fromits root to one of the leaf nodes, forming a used feature list thatincludes features involved in this prediction action. We denote thisaccessed feature list of the -th decision tree as a binary vector () {0, 1} , in which 0 indicates that the corresponding featureof this sample is not used by the -th decision, and 1 indicates thatit is accessed. Consequently, we can represent the GBDT featurefrequency of the sample with the sum of the decision trees binaryvectors, as:",
  "where represents the exploitation level of each feature in theGBDT, suggesting the feature preference of the GBDT model onthis sample": "Feature Tokenizer. Inspired by the classical language models(e.g., BERT ), recent dominating Transformer-based tabularmodels adopted distributed feature representation by embedding tabular values into vector spaces and treating thevalues as unordered word vectors. Such Transformer modelsuse feature tokenizer to process tabular features as follows:Each tabular scalar value is mapped to a vector R witha feature-specific linear projection, where is the feature hid-den dimension. For numerical (continuous) values, the projectionweights are multiplied with the value magnitudes. Given 1 nu-merical features and 2 categorical features, the feature tokenizeroutputs feature embedding R(1+1+2) by stacking pro-jected features (and an extra [CLS] token embedding), i.e., =",
  ":, if training :, if inference, {1, 2, . . . ,} .(4)": "The extra [CLS] embedding is omitted in this subsection fornotation brevity; in implementation, it is directly concatenated tothe head of the gated . In Eq. (3), is the normalized GBDT featurefrequency that represents the access probabilities of each feature inthe -tree GBDT, and is a binary feature mask sampled with theprobabilities . To incorporate the GBDTs feature preference intothe DNN framework, in Eq. (4), we use sparse feature masks fromreal GBDT feature access probabilities to perform hard feature selec-tion during training, and use the soft probabilities during inferencefor deterministic prediction. GFG assists in filtering out unneces-sary features according to the GBDTs feature preference, ensuringan oracle selection behavior compared to previous differential treemodels in learning feature masks with neural networks.Since the original GBDT library (we uniformly use XGBoostin this work) has no APIs for efficiently fetching sample-specificGBDT feature frequency in Eq. (2) and the used backend is incom-patible with common DL libraries (e.g., PyTorch), to integrate theGFG module into the parallel DNN framework, we tensorize thebehavior of Eq. (2). Technically, we are inspired by the principle ofthe Microsoft Hummingbird compiling tools1 and extract routingmatrices, a series of parameter matrices that contain informationof each decision trees node adjacency and threshold values, fromthe XGBoost model. Based on the extracted routing matrices, fea-ture access frequency can be simply acquired through alternatingtensor multiplication and comparison on input features , and thesubmodule of Eq. (2) is initialized with these parameter matrices.In the actual implementation, we just rapidly train an XGBoostwith uniform default hyperparameters provided in (regardlessof its performance) to initialize and freeze the submodule of Eq. (2)during the T-MLP initialization step. Other trainable parametersare randomly initialized. Since there are a large number of deci-sion trees to vote the feature preference in a GBDT model, slighthyperparameter modification will not change the overall featurepreference trend, and a lightly-trained default XGBoost is alwaysusable enough to guide greedy feature selection. To further speedup the processes in Eqs. (2)-(3), we cache the normalized feature",
  "Pure MLP Basic Block": "To explore the capability of pure-MLP architecture and keep ourtabular model compact, we take inspiration from vision MLPs.We observe that a key factor of their success is the attention-likeinteraction realized by linear projection and soft gating on fea-tures . Thus, we employ the spatial gating unit (SGU)proposed in , and formulate a simplified pure-MLP block, as:",
  "SGU() = 3LayerNorm(:,:) :,: .(6)": "The block is similar to a single feed-forward neural network (FFN)in the Transformer with an extra SGU (Eq. (6)) for feature-level in-teraction. The main parameters are located in two transformations,i.e., 1 R2 and 2 R in Eq. (5), where correspondsto the FFN intermediate dimension size. In Eq. (6), R 2 denotes the input features of SGU, and3 R is a feature-leveltransformation to emulate attention operation. Since in most cases, the model size is determined by 1 and 2, and iscomparable to the FFN size. All the bias vectors are omitted fornotation brevity.Analogous to vision data, we treat tabular features and featureembeddings as image pixels and channels. But completely differ-ent from vision MLPs, T-MLP is a hybrid framework tailored foreconomical tabular prediction that performs competitively againsttabular Transformers and GBDTs with significantly reduced run-time costs. On most uncomplicated datasets, using only one basicblock in T-MLP is enough. In comparison, previous vision MLPstudies emphasized architecture engineering and often demandeddozens of blocks in order to be comparable to vision Transformers.",
  "Sparsity with User-controllable Pruning": "Inspired by the pre-pruning of GBDTs that controls model complex-ity and promotes generalization with user-defined hyperparameters(e.g., maximum tree depth, minimum samples per leaf), we designa similar mechanism for T-MLP by leveraging the predominantmodel compression approach, i.e., DNN pruning , whichis widely used in NLP research to trim down over-parameterizedlanguage models while maintaining the original reliability .Specifically, we introduce two fine-grained variables h {0, 1} and in {0, 1} to mask parameters from hidden dimension andintermediate dimension, respectively. As the previous FFN prun-ing in language models , the T-MLP pruning operation can beattained by simply applying the mask variables to the weight matri-ces, i.e., substituting 1 and 2 with diag(h)1 and diag(in)2in Eq. (5). We use the classical 0 regularization reparametrized withhard concrete distributions , and adopt a Lagrangian multiplierobjective to achieve the controllable sparsity as in .Although early attempts of tabular DNNs have considered sparsestructures, for example, TabNet and NODE built learn-able sparse feature masks, and more recently TabCaps andT2G-Former designed sparse feature interaction, there are twoessential differences: (1) existing tabular DNNs only consideredsparsity on the feature dimension, while T-MLP introduces sparsity",
  "Team up GBDTs and DNNs: Advancing Efficient and Effective Tabular Prediction with Tree-hybrid MLPsKDD 24, August 2529, 2024, Barcelona, Spain": "NVIDIA A100 PCIe 40GB. All the hyperparameter spaces and iter-ation numbers of the baselines follow the settings in the originalpapers to emulate the tuning process of each baseline. For T-MLP,we use fixed hyperparameters as the model is trained only once.The XGBoost used for T-MLPs GBDT Feature Gate is in defaultconfiguration as in . In experiments, each single T-MLP uses onebasic block for most datasets if without special specification. Weuniformly use a learning rate of 1e-4 for a single T-MLP and learn-ing rates of 1e-4, 5e-4, and 1e-3 for the three branches in the T-MLPensemble (group T-MLP(3)). We reuse the same data splits as inthe original benchmarks. The baseline performances are inheritedfrom the reported benchmark results, and the baseline capacitiesare calculated based on the best model configurations provided inthe corresponding paper repositories. Detailed information of theruntime environment and hyperparameters is given in Appendix C. Compared Methods. On the four benchmarks, we compare ourT-MLP (the single-model and 3-model-ensemble versions) with: (1)known non-pre-trained DNNs: MLP, ResNet, SNN , GrowNet ,TabNet , NODE , AutoInt , DCNv2 , TabTransformer ,DANets , FT-Transformer (FT-T) , and T2G-Former (T2G) ;(2) pre-trained DNN: SAINT ; (3) GBDT models: XGBoost ,CatBoost , LightGBM , GradientBoostingTree (GBT), Hist-GradientBoostingTree (HistGBT), and other traditional non-deepmachine learning methods like RandomForest (RF) . For otherunmentioned baselines, please refer to Appendix B. In the experi-ment tables below, T-MLP denotes a single T-MLP and T-MLP(3)denotes the ensemble version with three branches.",
  "Overall Workflow and Efficient Ensemble": "The overall T-MLP workflow is as follows: During the training stage,the input tabular features are embedded with the feature tokenizerand discretely selected by the sampled feature mask in Eq. (3);then, they are processed by a single pruned basic block in Eq. (5),and the pruning parameter masks h and in are sampled withreparameterization on the 0 regularization; the final prediction ismade with the [CLS] token feature using a normal prediction headas in other tabular Transformers, as:",
  "= FC(ReLU(LayerNorm( ()[CLS],:))),": "where FC denotes a fully connected layer. We use the cross entropyloss for classification and the mean squared error loss for regressionas in previous tabular DNNs. The whole framework is optimizedwith back-propagation. After training, the parameter masks aredirectly applied to 1 and 2 by accordingly dropping the prunedhidden and intermediate dimensions. In the inference stage, theinput features are softly selected by the normalized GBDT featurefrequency in Eq. (3), and processed by the simplified basic block.Since the T-MLP architecture is compact and computation-friendlywith low runtime cost, we further provide an efficient ensemble ver-sion by simultaneously training three branches with the sharedGBDT feature gate from the same initialization point with threefixed learning rates. This produces three different sparse MLPs,inspired by the model soups ensemble method . The final en-semble prediction is the average result of the three branches as ina bagging ensemble model. Since the ensemble learning processcan be implemented by simultaneous training and inference withmulti-processing programming (e.g., RandomForest ), the train-ing duration is not tripled but determined by the slowest convergingbranch.",
  "EXPERIMENTS": "In this section, we first compare our T-MLP with advanced DNNsand classical GBDTs on their dominating benchmarks (including 88datasets for different task types) and analyze from the perspectiveof cost-effectiveness. Next, we conduct ablation and comparisonexperiments with multi-facet analysis to evaluate the key designsthat make T-MLP effective. Besides, we compare the optimized",
  "Datasets. We use four recent high-quality tabular benchmarks(FT-Transformer2 (FT-T, 11 datasets) , T2G-Former3 (T2G, 12datasets) , SAINT4 (26 datasets) , and Tabular Benchmark5": "(TabBen, 39 datasets) ), considering their elaborated results onextensive baselines and datasets. The FT-T and T2G benchmarksare representative of large-scale tabular datasets, whose sizes varyfrom 10K to 1,000K and include various DNN baselines. The SAINTbenchmark is gathered from the OpenML repository6, and is dom-inated by the pre-trained DNN SAINT, containing balanced tasktypes and diverse GBDTs. TabBen is based on typical tabular datasettings that constrain dataset properties, e.g., the data scale (amaximum data volume of 10K) and the feature number (not high-dimension) , and the datasets are categorized into several typeswith combinations of task types and feature characteristics. No-tably, on TabBen, GBDTs achieve overwhelming victory, surpassingcommonly-used DNNs. Each benchmark represents a specific frame-work preference. Since several benchmarks have adjusted datasetarrangements in their current repositories (e.g., some datasets wereremoved and some were added), to faithfully follow and reuse theresults, we only retain the datasets reported in the published origi-nal papers. We provide detailed benchmark statistical informationin and discuss benchmark characteristics in Appendix A.",
  "T2GSAINTTabBen": ": The winning rates of GBDTs and DNNs on threebenchmarks, which represent the proportion of each frame-work achieving the best performance in the benchmarks. Itexhibits varying framework preferences among the datasetsused in different tabular prediction works. Implementation Details. We implement our T-MLP model andPython package using PyTorch on Python 3.10. Since the reportedbaseline training durations on the original benchmarks are esti-mated under different runtime environments and using differentevaluation codes, and do not consider hyperparameter tuning (HPT)budgets, for uniform comparison of training costs, we encapsulatethe experimental baselines with the same sklearn-style APIs asT-MLP in our built package, and conduct all the experiments on",
  "FT-T 344006525ACCACCRMSET2G 354037222ACCACCRMSESAINT 971010312169AUCACCRMSETabBen 150242370052ACCN/AR-Squared": ": Cost-effectiveness comparison on the FT-T benchmark. Classification datasets and regression datasets are evaluatedusing the accuracy and RMSE metrics, respectively. Rank denotes the average values (standard deviations) of all the methodsacross the datasets. represents the average overhead of the used training time against T-MLP, and compares only theduration before achieving the best validation scores. All the training durations are estimated with the original hyperparametersearch settings. denotes the average parameter number of the best model configuration provided by the FT-T repository.TabNet is not compared considering its different backend (Tensorflow) in the evaluation. The top performances are marked inbold, and the second best ones are underlined (similar marks are used in the subsequent tables).",
  "T-MLP0.4470.8640.3860.7280.7290.9560.89778.7680.9680.7560.7473.1 (0.9)1.001.000.79T-MLP(3) 0.438 0.867 0.386 0.732 0.730 0.9600.89788.732 0.9690.755 0.745 1.7 (0.8)1.051.082.37": ": Cost-effectiveness comparison on the T2G benchmark with similar notations as in . The baseline performancesand configurations are also reused from the T2G repository. According to the T2G paper, for the extremely large dataset Year,FT-T and T2G use 50-iteration hyperparameter tuning (HPT), DANet-28 follows its default hyperparameters, and the otherbaseline results are acquired with 100-iteration HPT.",
  "GE CH EY CA HO AD OT HE JA HI FB YE Rank (M)": "XGBoost0.684 0.859 0.725 0.436 3.169 0.873 0.825 0.375 0.719 0.724 5.359 8.8504.3 (3.1)32.7842.88N/AMLP0.586 0.858 0.611 0.499 3.173 0.854 0.810 0.384 0.720 0.720 5.943 8.8498.3 (1.9)13.7311.450.64SNN0.647 0.857 0.616 0.498 3.207 0.854 0.812 0.372 0.719 0.722 5.892 8.9018.3 (1.5)22.7412.540.82TabNet0.600 0.850 0.621 0.513 3.252 0.848 0.791 0.379 0.723 0.720 6.559 8.916 10.2 (2.4)N/AN/AN/ADANet-28 0.616 0.851 0.605 0.524 3.236 0.850 0.810 0.355 0.707 0.715 6.167 8.914 10.6 (2.0)N/AN/AN/ANODE0.539 0.859 0.655 0.463 3.216 0.858 0.804 0.353 0.728 0.725 5.698 8.7777.0 (3.0) 329.79 288.21 16.95AutoInt0.583 0.855 0.611 0.472 3.147 0.857 0.801 0.373 0.721 0.725 5.852 8.8628.1 (2.0)68.3055.520.06DCNv20.557 0.857 0.614 0.489 3.172 0.855 0.802 0.386 0.716 0.722 5.847 8.8828.4 (2.0)24.4021.632.30FT-T0.613 0.861 0.708 0.460 3.124 0.857 0.813 0.391 0.732 0.731 6.079 8.8524.7 (2.6)64.6850.902.22T2G0.656 0.863 0.782 0.455 3.138 0.860 0.819 0.391 0.737 0.734 5.701 8.8513.1 (1.7)88.9387.041.19",
  "In to , the baseline results are based on heavy HPT,and are obtained from respectively reported benchmarks. All theT-MLP results are based on default hyperparameters": "Comparison with Advanced DNNs. Tables 3 and 4 report de-tailed performances and runtime costs on the FT-T and T2G bench-marks for comparison of our T-MLP versions and bespoke tabularDNNs . The baseline results in these tables are based on 50(for complicated models on large datasets, e.g., FT-Transformer onthe Year dataset) or 100 (the other cases) iterations of HPT exceptspecial models (default NODE for the datasets with large class num-bers and default DANets for all datasets). An overall trend that onemay observe is that the single T-MLP is able to achieve competitiveresults as the state-of-the-art DNNs on each benchmark, and a sim-ple ensemble of three T-MLPs (i.e., T-MLP(3)) exhibits even betterperformances with significantly reduced training costs. Specifically,benefiting from fixed hyperparameters and simple structures, thesingle T-MLP achieves obvious speedup and reduces training du-rations by orders of magnitude compared to the powerful DNNs,and is also more training-friendly than XGBoost, a representativeGBDT that highly relies on heavy HPT. Besides, we observe onlyabout 10% training duration increase in T-MLP ensemble since weadopt multiprocessing programming to simultaneously train thethree T-MLPs (see Sec. 3.5) and thus the training time dependson the slowest converging sub-model. In the implementation de-tails (Sec. 4.1), the single T-MLP uses the smallest learning rate inthe three sub-models, and hence the convergence time of T-MLP : The average values (standard deviations) of all themethod ranks on the SAINT benchmark of three task types.|| is the dataset number in each group. Notably, all the base-line results are based on HPT, and SAINT variants need fur-ther training budgets on pre-training and data augmentation.More detailed results are given in the Appendix.",
  "T-MLP(3)3.9 (1.9)2.9 (2.5)5.0 (2.9)": "ensemble often approximates that of the single T-MLP. From theperspective of model storage, as expected, the size of the singleT-MLP is comparable to the average level of naive MLPs across thedatasets and its size variation is stable (see ), since the blocknumber, hidden dimension size, and sparsity rate are all fixed. InSec. 4.3, we will further analyze the impact of model sparsity andtheoretical complexity of the model parameters. Comparison with Pre-trained DNNs. reports the meansand standard deviations of model ranks on the SAINT benchmark .Surprisingly, we find that the simple pure MLP-based T-MLP outper-forms Transformer-based SAINT variants (SAINT-s and SAINT-i)and is comparable with SAINT on all the three task types. It isworth noting that SAINT and its variants adopt complicated inter-sample attention and self-supervised pre-training along with HPTon parameters of the training process. Moreover, T-MLP ensembleeven achieves stable results that are competitive to tuned GBDTs(i.e., XGBoost, CatBoost, LightGBM) and surpasses the pre-trainedSAINT on classification tasks. Since the detailed HPT conditions(i.e., iteration times, HPT methods, parameter sampling distribu-tions) are not reported, we do not estimate specific training costs. Comparison with Extensively Tuned GBDTs. comparesT-MLP on the typically GPDTs-dominating benchmark TabBen ,on which GBDT frameworks completely outperform various DNNsacross all types of datasets. Results of each baseline on TabBen areobtained with around 400 iterations of heavy HPT, almost repre-senting the ultimate performances with unlimited computation re-sources and budgets. As expected, when extensively tuned XGBoostis available, the single T-MLP is eclipsed, but it is still competitive tothe other ensemble tree models (i.e., RF, GBT, HistGBT) and superiorto the compared DNNs. Further, we find that T-MLP ensemble is",
  "T-MLP3.2 (1.6)4.3 (1.9)3.5 (2.3)3.6 (1.4)T-MLP(3)2.1 (1.4)2.7 (1.5)3.0 (1.3)1.8 (0.7)": "able to be comparable to the ultimate XGBoost in all the four datasettypes with similar rank stability, serving as a candidate for a tunedXGBoost alternative. More significantly, in the experiments, eachT-MLP (or T-MLP ensemble) employs a tensorized XGBoost trainedin default configuration (see implementation details in Sec. 4.1),and all the other hyperparameters are fixed; thus T-MLP and itsensemble have potential capability of a higher performance ceilingby HPT or selecting other GBDTs as the feature gate.In summary, we empirically show the strong potential of ourhybrid framework to achieve flexible and generalized data adapt-ability with various tabular preferences (tabular data preferringadvanced DNNs, pre-training, or GBDTs). Based on the impressiveeconomical performance-cost trade-off and friendly training pro-cess, T-MLP can serve as a promising tabular model frameworkfor real-world applications, especially under limited computationbudgets.",
  "What Makes T-MLP Cost-effective?": "reports ablation and comparison experimental results of T-MLP on several classification and regression datasets (i.e., CaliforniaHousing (CA) , Adult (AD) , Higgs (HI) , and Year (YE) )in various data scales (given in parentheses). Main Ablations. The top four rows in report the impact oftwo key designs in a single T-MLP. An overall observation is thatboth the structure sparsity and GBDT feature gate (FG) contribute toperformance enhancement of T-MLP. From the perspective of dataprocessing, GBDT FG brings local sparsity through sample-specificfeature selection, and the sparse MLP structure offers global sparsityshared by all samples. Interestingly, we find that the impact of GBDTFG is more profound on the CA dataset. A possible explanationis that the feature amount of CA (8 features) is relatively small : Main ablation and comparison on classical tables invarious task types and data scales. The top 4 rows: ablationson key designs in the T-MLP framework. The bottom 2 rows:results of T-MLP with neural network feature gate (NN FG).",
  "T-MLP (NN FG)0.45590.8520.7188.925w/o sparsity0.45570.8400.7138.936": "compared to the others (14, 28, and 90 features in AD, HI, and YE,respectively) and the average feature importance may be relativelylarge; thus, the CA results are more likely to be affected by featureselection. For the datasets with larger feature amounts, selectingeffective features is likely to be more difficult. Greedy Feature Selection. We notice a recent attempt on sample-specific sparsity for biomedical tables using a gating network; itwas originally designed for low-sample-size tabular settings andhelped prediction interpretability in the biomedical domain .We use its code and build a T-MLP version by substituting GBDTFG with the neural network feature gate (NN FG) for comparison.The bottom two rows of report the results. As expected, onthe smallest dataset CA, NN FG can boost performance by learningto select informative features, but such a feature gating strategyconsistently hurts the performance as data scales increase. This maybe due to (1) large datasets demand more complicated structuresto learn the meticulous feature selection, (2) the discrete nature ofthe selection behavior is incompatible with smooth optimizationpatterns of neural networks, and (3) DNNs confirmation bias may mislead the learning process, i.e., NN FG will be ill-informedonce the subsequent neural network captures wrong patterns. Incontrast, GBDT FG always selects features greedily as real GBDTs,which is conservative and generally reasonable. Besides, the com-plicated sub-tree structures are more complete for the selectionaction. Sparsity Promotes Tabular DNNs. plots performance vari-ations on two classification/regression tasks with respect to T-MLPsparsity. Different from the pruning techniques in NLP that aim totrim down model sizes while maintaining the ability of the originalmodels, we find that suitable model sparsity often promotes tabu-lar prediction, but both excessive and insufficient sparsity cannotachieve the best results. The results empirically indicate that, com-pared to DNN pruning in large pre-trained models for unstructureddata, in the tabular data domain, the pruning has the capabilityto promote non-large tabular DNNs as GBDTs beneficial sparsestructures achieved by tree pre-pruning, and the hidden dimensionin tabular DNNs is commonly over-parameterized.",
  "Credit-g Bioresponse": ": Decision boundary visualization of FT-Transformer(FT-T), XGBoost, and a single-block T-MLP on the Biore-sponse and Credit-g datasets, using two most important fea-tures. Different colors represent distinct categories, while thevarying shades of colors indicate the predicted probabilities. these three methods. The two most important features are selectedby mutual information (estimated with the Scikit Learn package).Different from common DNNs and GBDTs, T-MLP exhibits a novelintermediate pattern that combines characteristics from both DNNsand GBDTs. Compared to DNNs, T-MLP yields grid-like boundarieswhose edges are often orthogonal to feature surfaces as GBDTs,and the complexity is essentially simplified with pruned sparsearchitectures. Besides, T-MLP is able to capture tree-model-like sub-patterns (see T-MLP on Credit-g), while DNNs manage only mainpatterns. Hence, DNNs are overfit-sensitive due to their relativelyirregular boundaries and neglecting of fine-grained sub-patterns.Compared to GBDTs with jagged boundaries and excessively splitsub-patterns, T-MLP holds very smooth vertices at the intersectionof boundaries (see T-MLP on Credit-g). Notably, T-MLP can decideconditional split points like GBDT feature splitting (orthogonaledges at feature surfaces) through a smooth process (see T-MLPboundary edges on Bioresponse, from top to bottom, in whichthe split point on the horizontal feature is conditionally changedwith respect to the vertical feature in a smooth manner, while XG-Boost is hard to attain such dynamical split points). Overall, T-MLP",
  "CONCLUSIONS": "In this paper, we proposed T-MLP, a novel hybrid framework attain-ing the advantages of both GBDTs and DNNs to address the modelselection dilemma in tabular prediction tasks. We combined a ten-sorized GBDT feature gate, DNN pruning techniques, and a vanillaback-propagation optimizer to develop a simple yet efficient andwidely effective MLP model. Experiments on diverse benchmarksshowed that, with significantly reduced runtime costs, T-MLP hasthe generalized adaptability to achieve considerably competitiveresults regardless of dataset-specific framework preferences. Weexpect that our T-MLP will serve as a practical method for econom-ical tabular prediction as well as in broad applications, and helpadvance research on hybrid tabular models.",
  "ABENCHMARK CHARACTERISTICS": "We provide detailed dataset statistical information of each bench-mark in . These benchmarks exhibit broad data diversity indata scales and task types. From the FT-T benchmark to TabBen, theoverall data volume is gradually reduced. We additionally visualizethe respective winning rates of GBDT and DNN frameworks in, indicating varying framework preferences among the datasetcollections used in different tabular prediction tasks. FT-T does notinclude GBDT baselines in its main benchmark, but has the mostextremely large datasets. Overall, the FT-T benchmark is the ex-tremely large-scale data collection (in both data volume and featurewidth), the T2G benchmark is a large one, the SAINT benchmarkcontains diverse data scales, and TabBen focuses on middle-sizetypical tables.",
  "C.2Hyperparameters of T-MLP": "In the main experiments, we uniformly set the hidden size to1024, the intermediate size to 676 (2/3 of the hidden size), thesparsity rate to 0.33, and the residual dropout rate to 0.1, with threebasic blocks for multi-class classification or extremely large binaryclassification datasets, and one block for the others. The learningrate of the single T-MLP is 1e-4, and the learning rates of the threebranches in T-MLP ensemble are 1e-4, 5e-4, and 1e-3, respectively."
}