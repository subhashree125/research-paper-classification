{
  "ABSTRACT": "Subspace clustering seeks to identify subspaces that segment a setof data points into ( ) groups, which has emerged as apowerful tool for analyzing data from various domains, especiallyimages and videos. Recently, several studies have demonstratedthe great potential of subspace clustering models for partitioningvertices in attributed graphs, referred to as SCAG. However, theseworks either demand significant computational overhead for con-structing the self-expressive matrix, or fail to incorporate graphtopology and attribute data into the subspace clustering frameworkeffectively, and thus, compromise result quality.Motivated by this, this paper presents two effective and efficientalgorithms, S2CAG and M-S2CAG, for SCAG computation. Particu-larly, S2CAG obtains superb performance through three major con-tributions. First, we formulate a new objective function for SCAGwith a refined representation model for vertices and two non-trivialconstraints. On top of that, an efficient linear-time optimizationsolver is developed based on our theoretically grounded problemtransformation and well-thought-out adaptive strategy. We thenconduct an in-depth analysis to disclose the theoretical connectionof S2CAG to conductance minimization, which further inspiresthe design of M-S2CAG that maximizes the modularity. Our ex-tensive experiments, comparing S2CAG and M-S2CAG against 17competitors over 8 benchmark datasets, exhibit that our solutionsoutperform all baselines in terms of clustering quality measuredagainst the ground truth while delivering high efficiency.",
  "attributed graph, subspace clustering, conductance, modularity": "ACM Reference Format:Xiaoyang Lin, Renchi Yang, Haoran Zheng, and Xiangyu Ke. 2025. SpectralSubspace Clustering for Attributed Graphs. In Proceedings of the 31th ACMSIGKDD Conference on Knowledge Discovery and Data Mining (KDD 25), Permission to make digital or hard copies of part or all of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for third-party components of this work must be honored.For all other uses, contact the owner/author(s).KDD 25, August 37, 2025, Toronto, Canada 2025 Copyright held by the owner/author(s).ACM ISBN 978-1-4503-XXXX-X/18/06.",
  "INTRODUCTION": "Attributed graphs are an omnipresent data structure used to modelthe interplay between entities characterized by rich attributes,such as social networks, citation graphs, World Wide Web, andtransportation grids. Clustering such graphs, i.e., partition verticestherein into disjoint groups, has gained massive attention over thepast decade , due to its encouraging performance by lever-aging the complementary nature of graph topology and attributes,as well as its extensive use in community detection , Bioin-formatics , anomaly identification , online advertisingand recommendation , etc.This problem remains tenaciously challenging as it requiresjoint modeling of graph structures and nodal attributes that presentheterogeneity, complex semantics, and noises. State-of-the-art so-lutions are built upon deep learning techniques, especiallygraph convolutional neural networks , which first aggregates at-tributes of neighbors in the graph before mapping attribute vectorsto low-dimensional feature representations of vertices for cluster-ing. Notwithstanding promising results reported, these works relyon a strong assumption that neighboring vertices should have highattribute homogeneity, rendering them vulnerable to attributednetworks contaminated with irrelevant and noisy data . Inaddition, this category of methods suffers from poor scalability anddemands tremendous computational resources for model training,especially on large graphs.Subspace clustering (SC) is a fundamental technique in datamining used for analyzing high-dimensional data including images,text, gene expression data, and so on . Intrinsically, it simul-taneously clusters the data into multiple subspaces and identifiesa low-dimensional subspace fitting each group of points, therebyeradicating the adverse impacts of irrelevant and noisy dimensions(i.e., features) . More precisely, the linchpin of SC is to constructa self-expressive matrix (SEM) that models the affinity of all datapoints such that each data point can be written as a linear combi-nation of others. Subsequent research further bolsters theSC performance by working in tandem with multi-view data wherethe data set is represented by multiple distinct feature sets.Inspired by its success, in recent years, several attempts [28, 46, 63] have been made towards extending the principle of SC to attrib-uted graphs for enhanced clustering quality, given that an attributedgraph embodies two sets of features (i.e., structures and attributes).For simplicity, we refer to this line of research as SCAG (short forsubspace clustering for attributed graphs) henceforth. Among these",
  "KDD 25, August 37, 2025, Toronto, CanadaLin et al": "Lemma A.1 provides upper and lower bounds for the sum ofentries in each row of , which are dependent on the maximumsand minimums of and . In what follows, we show that thevariables in { | V} and { | V} are dispersed in a narrowvalue range with low variances, making V Vnearly identical.",
  "RELATED WORK2.1Subspace Clustering": "Foundational subspace clustering algorithms like Sparse SubspaceClustering (SSC) , Low-Rank Representation (LRR) , andMultiple Subspace Representation (MSR) focus on learning anaffinity matrix, which is then employed in spectral clustering todiscern underlying data structures.However, the traditional methods often grapple with high compu-tational complexity. In response, a suite of low-rank sparse methodshas been proposed , which simultaneously reducecomputational load and improve algorithmic efficiency and scalabil-ity. Further enhancements in subspace clustering algorithms havebeen achieved through methods that emphasize local structure-preserving and kernel techniques . For an in-depthexploration, we kindly refer to surveys . Furthermore, anumber of studies have successfully incorporated deep learning into the traditional subspace clustering paradigm .These approaches leverage end-to-end training to learn proximitiesin latent subspaces, ultimately producing a representative coeffi-cient matrix that is critical for subsequent clustering processes.",
  "Attributed Graph Clustering": "Extensive studies have identified various methods for address-ing attribute graph clustering (AGC) challenges. These includeapproaches grounded in edge-weight-based ,distance-based and probabilistic-model-based meth-ods . For detailed insights, readers could refer torelated reviews .Recently, common practice involves integrating structural con-nectivity into vertex attributes to derive vertex embeddings , which are then used to perform clustering via estab-lished techniques such as KMeans. Lai et al. conducted a com-prehensive assessment of all existing deep attribute graph clustering(DAGC) methods for AGC, which capture boththe topological and attribute information of graphs, integrating thisfused information to facilitate the learning of vertex embeddings.To augment the proficiency of vertex representation learning, cer-tain models embedded in the DAGC framework have embracedgraph attention mechanisms , coupled with ad-vanced graph contrastive learning methodologies .Beyond the enhancement of vertex representations to achieve supe-rior clustering performance, certain models incorporatethe outcomes of vertex clustering into deep learning frameworksto optimize the cluster distribution.Owing to the high efficiency demonstrated by subspace clus-tering algorithms, a growing body of research hasbeen applying these algorithms to AGC. These methodologies per-form SC algorithms on graphs that integrate both topological andattribute information. Among these, SAGSC stands out asa state-of-the-art algorithm, characterized by its scalability andefficiency in SC. Nevertheless, the previously mentioned method-ologies are constrained by their incapacity to thoroughly exploitthe graphs topological framework and nodal attribute data, con-comitant with an absence of low-complexity SC underpinned bystringent mathematical principles.",
  "PROBLEM FORMULATION3.1Notations and Terminology": "Throughout this paper, sets are symbolized by calligraphic letters,e.g., V. Matrices (resp. vectors) are denoted as bold uppercase (resp.lowercase) letters, e.g., (resp. ). The transpose and inverse ofmatrix are denoted by and 1, respectively. The -th row(resp. column) of is represented by (resp. ,). Accordingly,, stands for the (, )-th entry of . stands for the Frobe-nius norm of matrix . We use to denote the identity matrix andits size is obvious from context. We refer to the left (resp. right)singular vectors of that correspond to its -largest singular val-ues as top- left (resp. right) singular vectors. The eigenvectors of corresponding to its largest eigenvalue in absolute value arereferred to as the -largest eigenvectors of .An attributed graph is defined as G = (V, E, ), composed of aset V = {1, 2, . . . , } of vertices (a.k.a. nodes), a set E V V",
  "Spectral Subspace Clustering for Attributed GraphsKDD 25, August 37, 2025, Toronto, Canada": "of edges, and an attribute matrix . Each vertex Vis characterized by a length- attribute vector and each edge(, ) E connects two vertices , V. For each vertex, we denote by N = { V|(, ) E} the set of directneighbors of , and by () := |N | the degree of . We use {0, 1} to represent the adjacency matrix (with self-loops) ofG, where , = 1 if (, ) E or = , and 0 otherwise. In thispaper, we consider undirected graphs, and hence, is symmetric.The diagonal degree matrix of G is symbolized by , wherein thediagonal entry , = () +1. The normalized adjacency matrix and transition matrix of G are defined as 1",
  "Subspace Clustering": "Let R be a data matrix for distinct data samples whereeach data sample {1, 2, . . . ,} is represented by a -dimensionalfeature vector . Subspace clustering aims to group data samplesinto disjoint clusters {C1, C2, . . . , C}, which is based on the as-sumption that data samples lie in a union of subspaces . Self-Expression Model is the most widely adopted objective for-mulation for subspace clustering. In this model, each data sampleis assumed to be expressed as a linear combination of other datasamples in the same subspace:",
  "minR 2 + (),(5)": "where R is known as the self-expressive matrix (SEM) (a.k.a.coefficient matrix). The first term is to reconstruct via and ,while the regularization term () is introduced to impose con-straints rendering meet certain structures or averting trivial so-lutions, e.g., . Popular constraints include sparsity constraint and low-rank representation (LRR) . The former minimizes thevector 1 norm of to induce sparsity, whereas LRR minimizesthe rank of such that captures the global correlation of thedata samples . In simpler terms, with the low-rank constraint,correlations between data samples are strengthened within clustersbut weakened across clusters. Besides, by virtue of the low-ranksetting, we can extract dominant patterns/features in the data whilefiltering out minor deviations, and hence, improve the robustnessto noise and outliers. The resulting SEM is then used to form anaffinity matrix +",
  "To extend subspace clustering to attributed graphs, a simple andstraightforward idea is to employ the vertex representations obtained via GLS in Eq. (3) as the data feature matrix": "Normalized Smoothed Representations. We argue that the di-rect adoption of for subspace clustering is problematic. First, in (Eq. (3)) is restricted within the range (0, 1) to avoid negativeor zero values due to the existence of 1 . Although 1 canbe removed, we still cannot assign large values to as it leadsto weighty coefficients that might overwhelm the entries in and other terms. Thereby, is constrained to monotonicallydecrease as increases, limiting the capacity of in capturingthe topological semantics in various graphs. Moreover, each entry, , V merely considers the degrees of endpoints and and overlooks the structures of other adjacent vertices of or, which is apt to cause biased attribute aggregation in Eq. (4).Similar issues arise on , where attribute vectors of vertices fall ondifferent scales. In response, we propose to calculate the normalizedsmoothed representations (NSR) of vertices in G as via",
  "minR 2 + + 2,(7)": "where the nuclear norm is an approximation of ()and the regularizer 2 is introduced to prevent overcorre-lation between vertices.The above objective poses two formidable technical challenges.First and foremost, the resulting SEM is a dense matrix whosematerialization consumes (2) space storage, which is prohibitivefor large graphs. Using such a dense matrix for the rear-mountedspectral clustering further yields an exorbitant expense of (2)time. On top of that, the complex constraints and objectives inEq. (7) lead to numerous iterations till convergence towards itsoptimization, each of which takes a quadratic time of (2) due tothe high density of .",
  "S2CAG APPROACH": "To cope with the above-said issues, this section presents a practicalalgorithm S2CAG for SCAG that runs in time linear to the sizeof G. .1 transforms our optimization objective in Eq. (7)into a simple truncated SVD of based on a rigorous theoreticalanalysis. .2 delineates how S2CAG implements the SVDand clustering in an adaptive fashion for higher efficiency. In Sec-tion 4.3, we establish the theoretical connections between S2CAGand minimizing the conductance of clusters.",
  "High-level Idea": "Note that the ultimate goal of SCAG is to partition V into disjointclusters rather than computing the intermediate . In light of this,we capitalize on the following theoretical analyses to bypass theexplicit construction of and streamline the SCAG computation. Orthogonal Procrustes Transformation. Instead of optimizingEq. (7) directly, we resort to an approximate solution by relaxing itsconstraints as follows. First of all, recall that minimizing the nuclearnorm is to reduce the rank of as much as possible. However,the spectral clustering stage in SCAG requires computing the eigenvectors that correspond to the largest non-zero eigenvalues of or its Laplacian for clustering. Intuitively, an SEM with a rank < has solely non-zero eigenvalues, which is incompetent forproducing meaningful clusters. Simply, we enforce the rank of to be as a trade-off. With Lemma 4.11, our optimization objectivein Eq. (7) is transformed into an orthogonal Procrustes problem with a -rank constraint over by letting = = and = .",
  "Algorithm": "The pseudo-code of S2CAG is illustrated in Algo. 1. S2CAG em-ploys a fast rounding algorithm, SNEM , to fulfil the secondgoal, i.e., derive {C1, C2, . . . , C} from (Line 14), whose runtimeis merely (). The critical task thus lies on the computation of .A naive approach proceeds as follows. First, we construct the NSRusing power iterations , dubbed as PowerMethod2. It takesas input , , order , and decay factor , and outputs definedin Eq. (6) using () time per iteration. Next, we conduct therandomized SVD of to get , which can be done in ()time. However, the cost () of constructing is significant ondense attributed graphs associated with large attribute sets. In suchcases, a better treatment is to integrate the computation of intothe process of the randomized SVD algorithm, so as to sidestep theexplicit construction of .S2CAG combines the aforementioned two ways in an adaptivefashion for optimal efficiency. More concretely, before entering thecore procedure, S2CAG estimates the runtime costs of the naive and",
  "nave(G,,, ) = + 2( + 1) ( + ) + 3( + )2(9)": "therein, respectively, based on G, the number of iterations, order, and the number of clusters. For the sake of space, we defer thecomplexity analysis to Appendix A.2.If nave(G,,,) integr(G,,,), Algo. 1 follows the naiveway remarked earlier (Lines 2-3), and otherwise integrates the com-putations of and (Lines 5-13). To be specific, it first generates a ( + ) standard Gaussian random matrix at Line 5, where ( 2) stands for the oversampling parameter used in randomizedSVD for proper conditioning. Next, we begin an iterative processto form () by calling PowerMethod with , or, as inputs alternately at Lines 6-10.Afterwards, S2CAG constructs an orthonormal matrix througha QR factorization of , and feeds and into PowerMethod toget a matrix that builds by (Lines 11-12). Lastly, wecalculate at Line 13 via",
  "are the approximate top-( + ) left singular vectors of": "By Theorem 4.4, derived in S2CAG contain the approximatetop-( +) left singular vectors of . Particularly, S2CAG selects thesecond to ( + 1)-th columns, i.e., ,2:+1, as for clustering. Thereason is that is close to a scaled stochastic matrix, rendering ,1 approximate 1/ that is trivial for clustering. For the interestof space, we refer readers to Appendix A.3 for related evidence.",
  "Lemma 4.5. = arg maxR() subject to =": "Since is an optimal solution to the trace maximization problemin Lemma 4.5, the optimal VCA that S2CAG aims to derivealso maximizes () where is required to be a VCAdefined in Eq. (8). Put another way, the objective of S2CAG isequivalent to optimizing the problem of max (). Lemma 4.6. Let G be an undirected weighted graph with vertexset V and adjacency matrix , wherein each entry , stands forthe weight of edge (, ) G. If there exists a scalar such that is a stochastic matrix,",
  "(C1, C2, . . . , C ) = =1 C, V\\C,|C | .(11)": "Let G be an affinity graph constructed on the vertex set V andevery edge (, ) V V is associated with a weight computedby . Accordingly, is the weighted adjacency matrix of G,which is approximately stochastic as pinpointed in the precedingsection. Lemma 4.6 implies that S2CAG is to partition vertices inG into clusters {C1, C2, . . . , C} such that their total conductance(i.e., the overall connectivity of vertices across clusters over G)defined in Eq. (11) is minimized.",
  "Algorithm and Analysis": "Algo. 2 presents the pseudo-code of M-S2CAG, which begins bytaking an additional parameter compared to Algo. 1. Initially,M-S2CAG obtains NSR via PowerMethod, based on which it cal-culates and as in the context of Eq. (13) (Lines 1-2). At Lines 3-4,we create an orthogonal matrix through a QR decompositionof a standard Gaussian matrix R generated randomly. Sub-sequently, M-S2CAG performs subspace iterations to update",
  "EXPERIMENTS": "This section experimentally evaluates S2CAG and M-S2CAG against17 competitors regarding clustering quality and efficiency on 8real attributed graphs. All experiments are conducted on a Linuxmachine with an NVIDIA Ampere A100 GPU (80GB memory), AMDEPYC 7513 CPUs (2.6 GHz), and 1TB RAM. Due to space constraint,we defer the clustering visualizations to Appendix B.",
  "Experimental Setup": "Datasets. summarizes the statistics of datasets we experi-ment with. CiteSeer, PubMed , Cora , ACM, DBLP , andArXiv are academic citation networks, in which ground-truthclusters represent subjects or fields of study of publications. Photois a segment of the Amazon product co-purchase graph , wherecluster labels correspond to product categories. Wiki is a refer-ence network of Wikipedia documents. Evaluation Criteria. Following previous works ,we adopt three widely-used metrics: Clustering Accuracy (ACC),Normalized Mutual Information (NMI), and Adjusted Rand Index(ARI) to assess the clustering quality in the presence of ground-truth cluster labels. ACC and NMI scores range from 0 to 1.0, whilstARI ranges from 0.5 to 1.0. For all of them, higher values indicatebetter clustering performance. Baselines, Implementations, and Parameter Settings. We care-fully select 17 competing methods from four categories for com-parison including one metric clustering method KMeans ; fivesubspace clustering methods: K-FSC , LSR , SSC-OMP ,EDESC , SAGSC ; four spectral methods: SC , MinCut-Pool , DMoN , DGCluster ; and seven GRL-based methods:",
  "KMeans 68.10.237.30.231.870.559.20.130.00.126.60.129.81.839.80.510.50.417.00.222.60.17.30.111.2": "K-FSC 42.00.013.20.010.30.055.40.116.50.015.70.023.90.029.80.011.00.017.30.015.50.06.10.013.6LSR 30.90.21.10.10.30.138.80.00.40.00.60.01.80.00.10.00.00.06.30.00.50.00.00.017.5SSC-OMP 29.50.10.20.10.10.060.30.021.50.018.90.013.80.222.60.16.30.1---14.5EDESC 44.50.112.80.112.30.147.80.03.80.03.10.010.50.015.20.02.90.0---15.3SAGSC 93.10.178.00.283.30.371.10.032.90.034.10.041.30.953.90.924.41.347.81.846.90.638.20.73.8 SC 30.00.01.40.00.10.053.80.010.80.07.20.010.20.516.31.02.00.2---15.0MiniCutPool 87.70.171.80.074.10.061.50.028.30.025.40.031.00.049.50.019.10.0---11.6DMoN 46.30.657.30.138.40.521.70.022.60.09.50.023.30.042.70.016.40.037.50.038.60.027.40.013.0DGCluster 92.10.175.20.080.90.041.40.034.70.024.40.038.10.053.40.028.00.033.90.043.20.029.20.07.2 GCC 90.91.173.60.078.42.270.82.232.34.133.30.040.20.054.10.026.00.041.20.047.10.035.80.06.2GIC 87.40.068.10.071.70.065.10.026.40.024.60.030.60.050.20.020.00.012.40.018.40.06.40.08.3SSGC 87.10.067.50.071.60.070.70.032.54.533.30.037.00.053.60.026.00.028.90.032.00.020.00.07.8SDCN 79.40.158.30.058.70.163.00.023.40.022.20.011.50.021.10.04.20.026.40.017.80.010.50.011.9DCRN 93.00.077.80.083.50.069.00.034.20.032.10.039.40.053.30.026.50.0---6.5DAEGC 89.80.071.20.076.10.065.20.025.80.024.60.032.20.049.90.021.30.0---9.1Dink-Net 87.30.167.10.069.70.066.10.025.70.025.90.032.90.039.30.015.80.020.50.017.60.07.10.011.7 S2CAG (SNEM)93.50.078.80.084.30.075.30.036.50.041.90.044.70.053.60.033.10.046.90.046.10.038.70.02.0M-S2CAG (SNEM)93.50.078.90.084.30.075.50.036.80.042.30.044.40.053.90.031.60.050.00.047.20.040.50.01.7 GCC , GIC , SSGC , SDCN , DCRN , DAEGC, Dink-Net . Amid them, metric clustering and subspaceclustering methods are solely applied to attribute matrices, exceptSAGSC, which is a SCAG solution using hand-crafted features fromgraph structures and attributes. Spectral methods produce clustersby optimizing conductance- or modularity-like metrics on the orig-inal graph, while GRL-based approaches apply KMeans to vertexrepresentations learned by various neural network models.For most competitors, we reproduce results using source codescollected from authors and parameter settings prescribed whenpossible. Unless otherwise specified, we set in M-S2CAG to 0.9on CiteSeer and Wiki and 1.0 on others. As for the numbers ofiterations in S2CAG and M-S2CAG, we follow the default settingsin randomized SVD and subspace iterations. We run grid searchesfor remaining parameters (i.e., and ) and report the best results.More details regarding parameter setup are in Appendix B. Thedatasets and our code are available at",
  "Performance Evaluation": "6.2.1Clustering Quality. This set of experiments reports theACC, NMI, and ARI scores achieved by S2CAG, M-S2CAG, andall baselines over 8 datasets. We conduct 5 trials and report theaveraged values and standard deviation over the trials. A methodis omitted if it fails to report the results within one day or incursout-of-memory errors.Tables 2 and 3 show the ACC, NMI, ARI scores, and the averageperformance rankings. The best and second-best results are high-lighted in blue and darker shades indicate better clustering. Thebest baselines are underlined. From the last columns of Tables 2and 3, we can see that M-S2CAG and S2CAG are ranked the highestand second highest in terms of overall clustering quality amongall evaluated methods, respectively, whereas the best performerSAGSC in baselines is another SCAG approach. This observa-tion validates the effectiveness of subspace clustering for attributedgraph clustering tasks.Specifically, on small and medium-sized attributed graphs, S2CAGand M-S2CAG outperform all competing methods, with conspicu-ous improvements pertinent to almost all metrics. For instance, ourproposed methods improve the best baselines by significant mar-gins of 8%, 4.9%, 4.3% in ACC, NMI, and ARI on Wiki, and 4.4%, 2.1%,8.2% on PubMed, respectively. This superiority is still pronouncedon datasets with millions of edges, i.e., DBLP and ArXiv, where wecan observe that M-S2CAG is able to give a performance gain of0.4%, 2.2% for ACC and 1.0%, 2.3% for ARI, respectively. Moreover,on all datasets except a small one Wiki, it can be observed thatM-S2CAG yields comparable and often superior performance toS2CAG. Overall, M-S2CAG improves S2CAG in generalization androbustness by maximizing modularity. 6.2.2Efficiency. depicts the running times required byS2CAG, M-S2CAG, and other competitive baselines (ranked top 7in Tables 2 and 3) for clustering on all datasets. The-axis representsthe running time (seconds) in log scale. The reported runtime valuesexclude the costs for input (loading datasets) and output (savingclustering results).From (a) and 1(b), we can observe that S2CAG is able togain 183 and 13.6 speedup on CiteSeer and Wiki when comparedto the best baselines DCRN and DGCluster, respectively. On therest of the datasets except DBLP, S2CAG and M-S2CAG achievecomparable efficiency to the state of the art SAGSC and are amongthe fastest methods. Over the DBLP graph with 2.5 million edges,our solutions take at least 7.2 seconds to terminate and SAGSCconsumes 2.5 seconds. But recall that in , S2CAG and M-S2CAG outperform SAGSC by a considerable gain of 0.4%, 0.8%,1.0% in ACC, NMI, and ARI, respectively.In summary, S2CAG and M-S2CAG consistently deliver superiorresults for clustering on various attributed graphs while offeringhigh empirical efficiency. The empirical observation corroboratesthe efficacy of our novel objective function in .4 and algo-rithmic designs developed in Sections 4 and .",
  "the three largest datasets PubMed, DBLP, and ArXiv, respectively,by varying each parameter while fixing others as in .1": "Varying . illustrates the ACC scores obtained by S2CAG andM-S2CAG on PubMed, DBLP, and ArXiv when is varied from 0.2to 2.2, 0.1 to 1.2, and 0.5 to 3, respectively. It can be observed that onall tested datasets, both S2CAG and M-S2CAG see a drastic uptickin ACC and reach a plateau afterward when increasing . Specif-ically, when is beyond 1.4, 0.9, and 2.0, the ACC scores remainalmost invariant on PubMed, DBLP, and ArXiv, respectively. Theseobservations reveal that large (especially over 1.0) is conducivefor clustering, as it can amplify the feature patterns of far-reachingneighbors in Eq. (6), which are restrained in standard vertex repre-sentations calculated in Eq. (3) by limiting within (0, 1). Varying . plots the ACC scores of S2CAG and M-S2CAGwhen varying order from 50 to 200, 1 to 40, and 5 to 35, on PubMed,DBLP, and ArXiv, respectively. Observe that S2CAG and M-S2CAGexperience a rapid improvement in ACC with increasing in thebeginning. Subsequently, the clustering quality of both of them re-mains relatively stable on PubMed and DBLP, but witnesses a sharpperformance decline on ArXiv when exceeds 30. This phenome-non is attributed to the over-smoothing and over-squashing issues caused by large orders . However, on PubMed, our methodsrequire an order greater than 100 to attain satisfactory perfor-mance since vertices inside it are poorly connected. Varying . As shown in , M-S2CAG achieves the best ACCwhen is around 1.0, which is consistent with the original definitionof modularity discussed in .1. Moreover, it can be observedfrom that M-S2CAG is highly sensitive to , whose perfor-mance is considerably inferior when is only slightly smaller orgreater than 1.0. The reason is that the affinity values ,",
  "CONCLUSION": "In this paper, we present two effective and scalable solutions, S2CAGand M-S2CAG, for SCAG. Under the hood, our proposed methodsinclude (i) a new optimization objective built on an optimized repre-sentation model and non-trivial constraints, (ii) fast and theoretically-grounded optimization solvers, and (iii) careful theoretical analysesinvestigating the rationale underlying S2CAG and M-S2CAG. Ourthorough evaluation results manifest the efficacy of our techniquesin addressing the limitations of existing works for vertex clusteringover attributed graph datasets of varied volumes. Renchi Yang is supported by the NSFC Young Scientists Fund (No.62302414) and Hong Kong RGC ECS grant (No. 22202623). XiangyuKe is supported by the Ningbo Yongjiang Talent Introduction Pro-gramme (2022A-237-G) and Zhejiang Provinces Lingyan R&DProject under Grant No. 2024C01259. Esra Akbas and Peixiang Zhao. 2017. Attributed Graph Clustering: an Attribute-aware Graph Embedding Approach. Proceedings of the IEEE/ACM InternationalConference on Advances in Social Networks Analysis and Mining (2017).",
  "Uri Alon and Eran Yahav. 2021. On the Bottleneck of Graph Neural Networksand its Practical Implications. In International Conference on Learning Represen-tations": "Aritra Bhowmick, Mert Kosan, Zexi Huang, Ambuj Singh, and Sourav Medya.2024. DGCLUSTER: A Neural Framework for Attributed Graph Clustering viaModularity Maximization. In Proceedings of the AAAI Conference on ArtificialIntelligence, Vol. 38. 1106911077. Filippo Maria Bianchi, Daniele Grattarola, and Cesare Alippi. 2020. SpectralClustering with Graph Neural Networks for Graph Pooling. In Proceedings ofthe 37th international conference on Machine learning. ACM, 27292738.",
  "Ccile Bothorel, Juan David Cruz, Matteo Magnani, and Barbora Micenkova.2015. Clustering attributed graphs: models, measures and methods. NetworkScience 3, 3 (2015), 408444": "David Buterez, Ioana Bica, Ifrah Tariq, Helena Andrs-Terr, and Pietro Li.2022. CellVGAE: an unsupervised scRNA-seq analysis workflow with graphattention networks. Bioinformatics 38, 5 (2022), 12771286. Jinyu Cai, Jicong Fan, Wenzhong Guo, Shiping Wang, Yunhe Zhang, and ZhaoZhang. 2022. Efficient Deep Embedded Subspace Clustering. 2022 IEEE/CVFConference on Computer Vision and Pattern Recognition (CVPR) (2022), 2130.",
  "Symposium on Intelligent Data Analysis": "Ganqu Cui, Jie Zhou, Cheng Yang, and Zhiyuan Liu. 2020. Adaptive GraphEncoder for Attributed Graph Embedding. Proceedings of the 26th ACM SIGKDDInternational Conference on Knowledge Discovery & Data Mining (2020). Shifei Ding, Benyu Wu, Ling Ding, Xiao Xu, Lili Guo, Hongmei Liao, andXindong Wu. 2024. Towards Faster Deep Graph Clustering via Efficient GraphAuto-Encoder. ACM Transactions on Knowledge Discovery from Data (2024). Xiaowen Dong, Dorina Thanou, Pascal Frossard, and Pierre Vandergheynst.2016. Learning Laplacian matrix in smooth graph signal representations. IEEETransactions on Signal Processing 64, 23 (2016), 61606173. Amani HB Eissa, Mohamed E El-Sharkawi, and Hoda MO Mokhtar. 2018. To-wards recommendation using interest-based communities in attributed socialnetworks. In Companion Proceedings of the The Web Conference 2018. 12351242.",
  "Johannes Gasteiger, Aleksandar Bojchevski, and Stephan Gnnemann. 2018.Predict then Propagate: Graph Neural Networks meet Personalized PageRank.In International Conference on Learning Representations": "Stephan Gnnemann, Ines Frber, Sebastian Raubach, and Thomas Seidl. 2013.Spectral subspace clustering for graphs with feature vectors. In 2013 IEEE 13thInternational Conference on Data Mining. IEEE, 231240. Nathan Halko, Per-Gunnar Martinsson, and Joel A. Tropp. 2009. Finding Struc-ture with Randomness: Probabilistic Algorithms for Constructing ApproximateMatrix Decompositions. SIAM Rev. 53 (2009), 217288.",
  "Roger A Horn and Charles R Johnson. 2012. Matrix analysis. Cambridgeuniversity press": "Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, BowenLiu, Michele Catasta, and Jure Leskovec. 2020. Open Graph Benchmark: Datasetsfor Machine Learning on Graphs. arXiv preprint arXiv:2005.00687 (2020). Guangyu Huo, Yong Zhang, Junbin Gao, Boyue Wang, Yongli Hu, and Baocai Yin.2021. CaEGCN: Cross-Attention Fusion Based Enhanced Graph ConvolutionalNetwork for Clustering. IEEE Transactions on Knowledge and Data Engineering35 (2021), 34713483.",
  "Yue Liu, Ke Liang, Jun Xia, Sihang Zhou, Xihong Yang, Xinwang Liu, and Stan ZLi. 2023. Dink-net: Neural clustering on large graphs. In International Conferenceon Machine Learning. PMLR, 2179421812": "Yue Liu, Wenxuan Tu, Sihang Zhou, Xinwang Liu, Linxuan Song, Xihong Yang,and En Zhu. 2022. Deep Graph Clustering via Dual Correlation Reduction. InProceedings of the AAAI Conference on Artificial Intelligence, Vol. 36. 76037611. Yue Liu, Jun Xia, Sihang Zhou, Xihong Yang, Ke Liang, Chenchen Fan, YanZhuang, Stan Z Li, Xinwang Liu, and Kunlun He. 2022. A Survey of Deep GraphClustering: Taxonomy, Challenge, Application, and Open Resource. arXivpreprint arXiv:2211.12875 (2022).",
  "Dijun Luo, Feiping Nie, C. Ding, and Heng Huang. 2011. Multi-Subspace Repre-sentation and Discovery. In ECML/PKDD": "Xuexiong Luo, Jia Wu, Amin Beheshti, Jian Yang, Xiankun Zhang, Yuan Wang,and Shan Xue. 2022. Comga: Community-aware attributed graph anomalydetection. In Proceedings of the Fifteenth ACM International Conference on WebSearch and Data Mining. 657665. Yao Ma, Xiaorui Liu, Tong Zhao, Yozen Liu, Jiliang Tang, and Neil Shah. 2020. AUnified View on Graph Neural Networks as Graph Signal Denoising. Proceedingsof the International Conference on Information & Knowledge Management (2020). Zhengrui Ma, Zhao Kang, Guangchun Luo, Ling Tian, and Wenyu Chen. 2020.Towards clustering-friendly representations: Subspace clustering via graphfiltering. In Proceedings of the ACM international conference on multimedia. 30813089.",
  "Yu-Xiang Wang, Huan Xu, and Chenlei Leng. 2013. Provable subspace clustering:When LRR meets SSC. Advances in Neural Information Processing Systems (2013)": "Lai Wei, Zhengwei Chen, Jun Yin, Changming Zhu, Rigui Zhou, and Jin Liu.2023. Adaptive graph convolutional subspace clustering. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition. 62626271. Hui Xia, Shu shu Shao, Chun qiang Hu, Rui Zhang, Tie Qiu, and Fu Xiao.2023. Robust Clustering Model Based on Attention Mechanism and GraphConvolutional Network. IEEE Transactions on Knowledge and Data Engineering35 (2023), 52035215.",
  "Wang Xiao, Ji Houye, Shi Chuan, Wang Bai, Cui Peng, Yu P., and Ye Yanfang.2019. Heterogeneous Graph Attention Network. WWW (2019)": "Zhiqiang Xu, Yiping Ke, Yi Wang, Hong Cheng, and James Cheng. 2012. Amodel-based approach to attributed graph clustering. Proceedings of the 2012ACM SIGMOD International Conference on Management of Data (2012). Cheng Yang, Zhiyuan Liu, Deli Zhao, Maosong Sun, and Edward Y. Chang. 2015.Network representation learning with rich text information. In Proceedings of the24th International Conference on Artificial Intelligence (Buenos Aires, Argentina)(IJCAI15). AAAI Press, 21112117.",
  "Hugo Zanghi, Stevenn Volant, and Christophe Ambroise. 2009. Clustering basedon random graph model embedding vertex features. Pattern Recognit. Lett. 31(2009), 830836": "Changqing Zhang, Huazhu Fu, Qinghua Hu, Xiaochun Cao, Yuan Xie, DachengTao, and Dong Xu. 2018. Generalized latent multi-view subspace clustering.IEEE transactions on pattern analysis and machine intelligence 42, 1 (2018), 8699. Han Zhao, Xu Yang, Zhenru Wang, Erkun Yang, and Cheng Deng. 2021. GraphDebiased Contrastive Learning with Joint Representation Clustering. In Inter-national Joint Conference on Artificial Intelligence.",
  "nave(G,,,) = + 2( + 1) ( + ) + 3( + )2": "The asymptotic complexity can be simplified as ( + )since the oversampling parameter can be regarded as a constant.The major cost of the integrated method lies at Lines 6-13 ofAlgo. 1. As per Section A.1, for each of the iterations, both Lines7 and 8 incur ( + ) operations when executing PowerMethod,while Lines 7 and 9 need ( + ) operations for computing and . Similarly, we can analyze that the cost of Lines 10 and12 is 2 ( + ) + 2( + ). To sum up, Lines 6-9, 10, and 12together involve 2( + 1) ( + ) ( +) operations in total.Since the QR decomposition and SVD are applied to ( +) and( +) matrices, respectively, both their runtime can be boundedby (( +)2). The matrix multiplication in Eq. (10) also requires(( +)2) operations. Overall, the estimated computational costof the integrated method can be calculated by",
  "for each V": "Similarly, we can compute by setting = for eachvertex V, where is a sum of all row vectors in , i.e., = V . The cost is also (). Both Lines 4 and 7 apply a QRdecomposition of an matrix, requiring (2) time. Notethat Eq. (15) can be computed in () via re-ordered matrixmultiplications. Considering all the iterations, the total cost forupdating and is then(). Overall, the runtime complexityof M-S2CAG is bounded by ( + ).",
  "holds, where () = V , and () = V ,": "Our task thus turns to bound V, which is done inLemma A.3. Since is normalized, the values in { ()| V}has a little variation, namely min and max are close. Asa consequence, the row sums V V are closelyaligned, which can be corroborated by our empirical observationson each dataset from and indicates that is nearly ascaled stochastic matrix.",
  "B.1Clustering Visualization": "We visually compare the clustering results output by S2CAG andSAGSC against the ground truth on Wiki, ACM, and PubMed datasets.Specifically, for each of them, we run the well-known Fruchterman-Reingold force-directed algorithm to draw the layout of the graph,wherein vertices are colored as per their respective cluster labels(true ones or predicted ones)., 5, 7 display the visualization results of the ground truth,S2CAG and SAGSC on Wiki, ACM, and PubMed, respectively. Themajor differences between the predicted results and the groundtruth are highlighted in the rectangles. It can be observed thatcompared to SAGSC, the vertices in the highlighted areas are col-ored (i.e., assign cluster labels to vertices) in a way more similar tothe ground truth by S2CAG, which is consistent with the fact thatS2CAG enjoys higher clustering accuracy. For example, in ,in the three rectangles, most of the vertices are mistakenly coloredin green by SAGSC, whereas our S2CAG can color them in redand blue correctly. Accordingly, as reported in , on PubMed,S2CAG outperforms SAGSC remarkably by a substantial margin of4.2%, 3.6%, and 7.8% for ACC, NMI, and ARI, respectively.",
  "Proof of Lemma 4.2": "Theorem C.1 ( ). Let be an real symmetric matrix.Let columns in , , and be the left singular vectors, right singularvectors, and eigenvectors, respectively. Diagonal matrices and consist of the singular values and eigenvalues of at their diagonalentries, respectively. Then, , = ,, , = , (,), and, = |, | hold for 1 , where () stands for the signfunction, i.e., () = 1 if > 0, () = 0 if = 0, and() = 1 if < 0.Let () (resp. ()) contain the top- left (resp. right) singularvectors of . Next, we prove that () = () and it is thetop- left singular vectors of . Let the columns in , , and thediagonal entries in be the left singular vectors, right singularvectors, and singular values of , respectively. Using the relationof SVD to eigendecomposition , the columns of are eigen-vectors of and the diagonal elements of are the squareroots of the eigenvalues of . In other words, 2 is theeigendecomposition of . Further, according to Theorem C.1, is equal to the eigenvectors of , and thus, = .Note that singular values in are non-negative. The top- sin-gular vectors in of are then exactly the -largest eigenvectorsof since its eigendecomposition is 2 . By Theorem C.1and the non-negativity of 2, the eigenvalues 2 of are thesame as its singular values and = . Accordingly, 2 is",
  "are orthonormal. Consider any column (),of () 1 .We have () () (),= () = (), ,": "where R stands for a column vector with value 1 at the -thposition and 0 elsewhere. The above equation implies that columnsof () are all eigenvectors of () () whose correspondingeigenvalues are 1.By the definition, () () is a rank- projection matrix ontothe subspace spanned by the columns of (), whose eigenvalues are either 0 or 1. Since its rank is , i.e., the sum of all eigenvaluesis , () () has eigenvectors corresponding to eigenvalue1 and other eigenvectors not in the span of correspond to theeigenvalue 0. Consequently, we can conclude that the columns of () are the -largest eigenvectors of () () and the lemmais proved.",
  "where (+)/2+1 signifies the (( + )/2 + 1)-th largest singularvalue of . In sum, the columns in are the approximate top-( +)left singular vectors of": "Proof of Lemma 4.5. Using Ky Fans trace maximization prin-ciple , the optimal solution to the following trace maximizationproblem maxR () subject to = is the -largest eigenvectors of . Since is the top- left singular vec-tors of and is a symmetric matrix, Theorem C.1 states that is also the -largest eigenvectors of . The lemma is proved.",
  "Next, suppose that matrix satisfies =": "1 .According to Lemma 4.1 and Lemma 4.2, the solution to theproblem in Eq. (16) is = () (), where () consists of thetop- left singular vectors of . Similar to the proof of Lemma 4.2,using Theorem C.1 derives that the -largest eigenvectors of are the top- left singular vectors of , namely = = ,which completes the proof."
}