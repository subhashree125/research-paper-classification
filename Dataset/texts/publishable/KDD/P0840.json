{
  "Abstract": "Knowledge graphs (KGs), which store an extensive number of re-lational facts, serve various applications. Recently, personalizedknowledge graphs (PKGs) have emerged as a solution to optimizestorage costs by customizing their content to align with users spe-cific interests within particular domains. In the real world, on onehand, user queries and their underlying interests are inherentlyevolving, requiring PKGs to adapt continuously; on the other hand,the summarization is constantly expected to be as small as possiblein terms of storage cost. However, the existing PKG summarizationmethods implicitly assume that the users interests are constantand do not shift. Furthermore, when the size constraint of PKG isextremely small, the existing methods cannot distinguish whichfacts are more of immediate interest and guarantee the utility of thesummarized PKG. To address these limitations, we propose APEX2,a highly scalable PKG summarization framework designed withrobust theoretical guarantees to excel in adaptive summarizationtasks with extremely small size constraints. To be specific, afterconstructing an initial PKG, APEX2 continuously tracks the interestshift and adjusts the previous summary. We evaluate APEX2 underan evolving query setting on benchmark KGs containing up to 12million triples, summarizing with compression ratios 0.1%. Theexperiments show that APEX outperforms state-of-the-art baselinesin terms of both query-answering accuracy and efficiency. ACM Reference Format:Zihao Li, Dongqi Fu, Mengting Ai, and Jingrui He. 2025. APEX2: Adaptiveand Extreme Summarization for Personalized Knowledge Graphs. In Pro-ceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery andData Mining V.1 (KDD 25), August 37, 2025, Toronto, ON, Canada. ACM,New York, NY, USA, 27 pages.",
  "KDD 25, August 37, 2025, Toronto, ON, Canada 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-1245-6/25/08": ": Example of Adaptive PKGs. The entire KG is storedon a cloud server, and the PKG is stored on the users device.The initial PKG is constructed based on the query \"Whoinvented Java\" at 9am. Then, this PKG adapts to the sameusers evolving queries at 3pm and 8pm. network analysis . Due to the ever-growingamount of data, encyclopedic knowledge graphs are becoming in-creasingly large and complex , such as DBpedia , Free-base , Wikidata , and YAGO . In contrast, KG users (e.g.,individual people, systems, software packages) usually do not havevery general interests but only care about a small portion of thewhole KG for certain topics. Therefore, personalized knowledgegraphs (PKGs) have recently attracted much research attention forbalancing storage cost and query-answering accuracy .In brief, a personalized knowledge graph (PKG) is extracted (sum-marized, compressed, or distilled) from a larger comprehensiveknowledge graph. For KG and an individual user, a PKG has a lim-ited size, but contains many entities/triples in the KG that the useris interested in, and can answer their personal queries appropriately.Furthermore, on the application side, each user will have a PKG.Since there might be many individual users, each PKG is expectedto store as few facts as possible, to minimize the total storage cost.As the individual KGs size has been very large, re-summarizingPKG from scratch requires many computational resources, but in",
  "KDD 25, August 37, 2025, Toronto, ON, CanadaLi et al": "With the input S and C, let V be the set of these inputs thatare consistent with the answers to all comparisons made so far.Initially, V contains all that is a permutation of S C and doesnot violate any order in S. Each new comparison (is > ?) willsplit V into two groups: the ones answering \"YES\" and the onesanswering \"NO\". With the fact that only one group will be the nextV, setting the correct answer to the larger group will guarantee|V | |V | 2.The algorithm must reduce |V| to 1 in order to get the output,and the total number of the initial V can be attained by countingthe number of ways to insert C ( distinct elements) into S (already-sorted elements). Use the formula from Stars and Bars (S contains stars, and C contains bars, permitting neighboringbars), the total number of ways is + 1 1",
  "|S|number of elements in the set S; Specifically, |G|represents number of triples in knowledge graph G": "The rest of the paper is organized as follows. In , weintroduce the problem setting and background. In , weintroduce how APEX2 dynamically and incrementally models usersevolving interests. In , we formally propose APEX2 and itsvariant APEX2-N. We theoretically analyze both APEX2 and APEX2-N from multiple aspects. In , we report the experimentalresults showing the effectiveness and efficiency of our methods. Wediscuss related works in and conclude in .Reproducibility. The code and the download instructions ofKG datasets are provided. Refer to Appendix C.4 for more details.",
  "Problem Definition": "We use calligraphic letters (e.g., A) for sets, bold capital letters formatrices (e.g., ), parenthesized superscript to denote the temporalindex (e.g., ( )), unparenthesized superscript to denote the power(e.g., ). For matrix indices, we use , to denote the entry inthe row and the column. The notation used in our proposedAPEX is summarized in .Knowledge Graph. A knowledge graph G = (E, R, T) is de-fined by an entity set E, a relation set R and a triple set T. Atriple = (,,) T is defined by entities , and theirrelationship . The undirected adjacency matrix is defined as",
  ", = 1 .. T(1)": "Personalized Knowledge Graph. A PKG P of KG G is defined byan entity set E E, a relation set R R and a triple set T T.In our problem setting, a PKG is summarized from a KG accordingto the users query log Q.Query Log. Since most real queries to KGs consist of only oneor two triples , assuming we have the full access to the wholeKG, we study simple queries with known answers. A query log Qconsists of a number of queries. Each query consists of a queryentity , query relation , and a set of answer entities A. Atriple = (,,) in the knowledge graph G is said to bean answer triple to a query q, if = = A. Aquery may have multiple answer triples. For example, for the queryWhat movie did Christopher Nolan direct, the query entity isChristopher Nolan (\"Nolan\" for short), and the query relation is",
  "APEX2: Adaptive and Extreme Summarization forPersonalized Knowledge GraphsKDD 25, August 37, 2025, Toronto, ON, Canada": ": We adopt a heat decay-inject-diffuse framework for heat diffusion. For each timestamp, first, all the heat (i.e., interest)will be decayed; then, an amount of new heat will be injected to the searched entities (marked by the dashed-line boxes); third,a global push is executed, where the heat of each entity will be partially pushed to its neighbors. by the dashed-line box in Timestamp 2. Third, global push is con-ducted and entities such as Pattinson, Averngers, Washington, Oscar,Murphy, History get pushed partial heat.For future timestamps, the decay-inject-diffuse mechanism willwork similarly as a simulation of user interest tracking.",
  "Q (+1)/Q ()1(P ( ),) .. |P ( ) | (2)": "In this paper, when the size budget 1%, we say the problembecomes adaptive and extreme PKG summarization. The small-est value that existing PKG summarization methods have explicitly used is 10%. In our experiments, 0.1%.Heat Diffusion. Heat diffusion process is a natural way to modelusers interest , as warmer nodes are considered moreimmediately interesting to the user . Once a query is performedby the user, the queried area will gain more heat, and then globally,the heat of each node will be partially pushed to its neighbors. Inthis work, we adopt a heat decay-inject-diffuse framework, with avisual example and more details provided in Appendix D.",
  "Adaptive PKG Summarization": "In the adaptive PKG summarization scenario, compared to previ-ous static PKG summarization , the major difference isthat the users interest may be shifting, manifested by the usersevolving query history. The adapting may be simply achieved byre-applying summarization methods from scratch every time theusers query log has evolved. But such a from-scratch solution lacksreal-time efficiency. A natural follow-up is: can we reuse the resultsfrom the previous summarization and further develop a real-timeframework that can evolve incrementally with the users interest?Moreover, when the storage constraint is extremely small, can weincrementally maintain a descending-order rank of the users inter-ests in each entity/relation/triple? To address these questions, wepropose our solution APEX2.The core idea of APEX2 is to maintain a real-time sparse heatstructure that stores the users interest and incrementally updatesits content, then greedily chooses the triples with the highest heatto construct the summarization, even under extremely small stor-age limitations. To adapt to the users interests, we introduce adecay factor into our framework. This factor is crucial to bothadapting effectiveness and efficiency: (i) decaying previous interests gives higher priority to recent queries, which are more likely torepresent the users current interest; (ii) for a set of elements, de-caying all of them does not affect their order and will not introduceadditional computations to the heat ranking process. controls thetrade-off between adapting to new interests and retaining relevantinformation from past queries.Systematically, our APEX2 consists of three components foradaptive PKG summarization, i.e., Dynamic Model of User Interests,Incremental Updating, and Incremental Sorting. In brief, DynamicModel of User Interests is proposed to model the user interest dynam-ically. Then based on the evolving interests modeled, IncrementalUpdating tracks the new interests. Incremental Sorting is neces-sary to construct a high-quality newly summarized PKG, underan extremely small storage constraint. Details of the three compo-nents are introduced through Subsections 3.1 3.3. Our end-to-endAPEX2 is summarized in Algorithm 4.",
  "Dynamic Model of User Interests": "In order to formulate more conveniently, we start by freezing ata specific timestamp ( as a constant), and define tostore the number of times each entity is accessed as query entity oranswer entity in the query log. If accessed as the answer to a query,the marginal value will be weighted by1 # of answers. We provide asimple example here. Suppose we have 5 entities (indexed 0, 1, 2, 3,4). The first query is (entity 0, some relation, {entity 1, entity 3}), thesecond query is (entity 2, some relation, {entity 0, entity 3}), thentotal will be a column vector (1 + 0.5, 0.5 + 0, 0 + 1, 0.5 + 0.5, 0 + 0) =(1.5, 0.5, 1, 1, 0). Assuming the users temporal query log is Q( ),we use Q( ) \\ Q(1) to denote new queries arriving at time .Additionally, Q1 = . Then,",
  "(4)": "where {1, . . . , |E|} is the index of entities.We model users interest on an entity in a heat-diffusing style.Define () to be the -hop neighbors of entity . Additionally0() = . A topic is a sub-area in the KG that has implicit innerconnections. Such connections carry semantic meanings for hu-mans (e.g., artistic, physical entities) and are modeled topologicallyby entities1. With the straightforward inspiration that entities nearthe searched ones are likely to be in the users interested topics, wemodel the users static preference for entities as",
  "is the query relation of query 0otherwise ( is unrelated to query )(10)": "where {1, . . . , |R|} is the index of relations.Then, we are ready to introduce the dynamics (from nowstarts to evolve) and decaying factor into the problem on bothentities and relations. The decaying factor controls the trade-offbetween adapting to new interests and retaining relevant infor-mation from past queries. Involving decay factor into total, , ,while a heat-diffusing style still applies, at timestamp , temporalexpressions of Eqs. 3, 6, 8 become",
  "( ) [][][] = ( ) [] ( ) []( ) [](19)": "The updating of ( ) from ( 1) is per-entry conditional. As-sume at timestamp , compared to timestamp 1, if entries, in ( ) and entry in ( ) are not updated (excluding de-cay), then ( ) [][][] = 3 ( 1) [][][]. Only entries in ( ) [][][] with any of those being updated need to be recal-culated. The number of updated entries is small as we assign much less than the diameter of G. Therefore, for each timestamp,we multiply by 3 and then do a small-scale update.",
  "Incremental Sorting": "After the users preference is updated in real-time, we can directlysort the triples by their preferences and then pick the ones with thehighest heat until the size budget is reached. However, the sortingalgorithms usually cost ( log2 ) complexity .For every new timestamp, excluding decay (which does notchange the order), only part of the triples heat will be updated andrecalculated. This indicates that it is possible to reuse the previousorder and accelerate the process to get the up-to-date order. Forthe following problem, we propose an intuitive solution namedincremental binary insertion sort, as shown in Algorithm 3.",
  "Algorithms4.1APEX2 Framework": "Our APEX2 combines user preference modeling, incremental heatupdating and incremental sorting as both solutions and optimiza-tions. As shown in Algorithm 4, in Steps 17, APEX2 first initializesthe data structures for both heat updating and incremental sorting,then performs pre-computing. Then, in Steps 814, for each latertimestamp a user inputs new queries, APEX2 performs macroscopicdecaying, incrementally updating and necessary recalculating. Af-ter that, the heat of triples gets incrementally sorted and a newPKG is constructed by picking the ones with the highest heat.The adaptability of APEX2 is ensured by the decaying operations,and we prove the effectiveness of APEX2 in Theorem 4.1. As forefficiency, intuitively, though the whole KG is available, APEX2 only accesses the entities, relations and triples of the users interest.Moreover, when the heat of a triple is decayed to a small enoughvalue, APEX2 would switch it to zero and such an out-of-interesttriple does not require any further computational resource. Thesefacts show that APEX2 is highly scalable for large databases. Weprove that the incremental time complexity of APEX2 is unre-lated to the size of KG in Theorem 4.2. Here, incremental timecomplexity means time complexity per adapting phase, whichis the total time in the adapting phase amortized by the number offor-loop iterations in Steps 8-14. In the following theorems, con-",
  "|E |+2|T ||E |+2|T | . (Proof inAppendix E.6)": "For each query = (,, A), we can decompose it into a set ofsub-queries = {(,, {}) A}. Similarly, we can decom-pose a query log into a sub-query log. Theorem 4.2 shows thetime complexity of APEX2 is only related to the connectivity of KGand the number of sub-queries the user performed. Theorem 4.2 (Time Complexity of APEX2). The incrementaltime complexity for APEX2s adapting phase is(|Q|2log2(|Q|)),where is the query log decomposed into sub-queries (each queryin has only one query entity, one query relation and one answer).",
  "APEX2 Variant: APEX2-N": "In APEX2, we model the users interest in triples in Eq. 14, wherewe assign equal weights to entities and relations. However, the usermay put more attention on entities than relations when performingqueries. For example, a user who searched what pieces of music didTaylor Swift create might be more likely to search whats TaylorSwifts music style than what pieces of music did Bill Evens createlater on. In this case, we propose APEX2-N, a variant of APEX2 that gives higher weights to entities than relations.APEX2-N only incrementally tracks and sorts the heat of entitiesbut not for relations. In other words, APEX2-N gives weight 1 toentities and 0 to relations. APEX2-N is designed mainly for adaptivesolutions of PKG summarization, and we leave the trade-off betweenweights on entities and relations to future work. Since APEX2-Nis a variation of APEX2, we summarize the detailed operations ofAPEX2-N in Algorithm 5 in the Appendix. We also give proof ofthe effectiveness and efficiency of APEX2-N as follows.",
  "And = (=0() )| E|, where R is the adjacency matrix ofG, E is the set of entity of G, and is the operator outputting thenumber of non-zero elements in a matrix. (Proof in Appendix E.3)": "Like APEX2, the time complexity of APEX2-N can also beoptimized to ( log2 ) by setting a threshold value. In thefuture, Both APEX2 and APEX2-N may be extended to the fullydynamic setting, where the KG itself can evolve. New entities canbe reserved as dummy nodes. When a new entity, relation, or tripleis added, the initial heat is zero, therefore we only need to updatethe adjacency matrix and start tracking their heat from the nexttimestamp. When a triple is deleted, we clear its heat to zero. Whenan entity or relation is removed, it means there is no triple withthat entity, and we can safely clear its heat to zero.",
  "Freebase14,541237310,116": "5.1.2Baselines. We choose several state-of-the-art methods as thebaselines. We compare sampling-based knowledge graph summa-rization algorithm (GLIMPSE ), merging-based graph summa-rization algorithm (PEGASUS ), workload-based knowledgegraph summarization algorithm (iSummary ), random walkwith restart on knowledge graph (Personalized PageRank ),together with our APEX2 and APEX2-N. Details of GLIMPSE, PE-GASUS and iSummary are provided in section B. The PPR baselinecalculates the PageRank vector personalized to total and constructsthe summarization by continuously adding the most relevant entity. 5.1.3Re-summarization Interval. By design, the baselines cannottake temporal query logs as inputs. To enable the baselines tohandle adaptive PKG summarization problem, we let them outputnew PKGs after a certain amount of timestamps. The choiceof affects the performances of baseline methods. If is small(i.e., = 1 means re-summarize every timestamp), then the re-summarization happens frequently, and the baselines will becomevery slow. If is large, then the baseline summaries are outdatedfor most timestamps. To pick a good for fair comparisons, weconduct pre-experiments in Appendix C.5 and find that = 9 is agood effectiveness-efficiency trade-off for baselines.In our design, APEX2 and APEX2-N can also take multiplequeries at one time by masking the summary updating phase(line 13-14 in Algorithm 4 and line 13-17 in Algorithm 5) for non-summary timestamps. We use to denote that they updatethe PKG every timestamp. By default, = 1, and weprovide a comprehensive study on in .6.",
  "Experimental Settings": "We show the outperformance of APEX2 and APEX2-N through auto-regressive3 style experiments. We set the default hyperparameters = 0.5, = 0.3, = 1 and PageRank restart probability to be0.85. We set the compression ratio to be 0.000001 = 0.0001% (onein a million) for YAGO and DBPedia, 0.0001 = 0.01% (one in tenthousand) for MetaQA, 0.0005 = 0.05% for Freebase.Generate user queries. To calculate the average and standarddeviation, we simulate 10 users to query the KGs. Following the",
  ": Effectiveness Comparison Under Querying Scenario": "norm set by Freebase discussed in section 3.1, we model the ab-stract concept of topic by \"queries with the same query entity\"4.To simulate a real-world querying scenario with interest shift, foreach KG, we generate 200 queries on 20 topics for each user. Eachgroup of 10 consecutive queries are in the same topic. We associateeach query with timestamp 1. For MetaQA, we first categorizethe provided queries into different topics by query entity, then ran-domly sample 20 distinct query entities. After that, we randomlychoose 10 queries on each of the 20 topics. For DBPedia, YAGO andFreebase, we synthetically generate queries by randomly choosing20 query entities in the KG. Then for each query, we randomlychoose 10 relations (with possible multiplicity) that the query en-tity has. Finally, we include all entities satisfying (query entity,chosen relation, ) T into the answer set. We study 1-hop simplequeries with known answers because in real life there is only asmall portion of complex queries , which can be decomposedinto simple queries.Query Answering Evaluation. After loading KG and the queriesof a user, we adaptively summarize the KG. (i) For GLIMPSE, PEGA-SUS, iSummary, PageRank, we construct an initial summary usingthe first query, then re-summarize after each = 9 timestamps(i.e., queries). (ii) For APEX2 and APEX2-N, they both evolve everytimestamp whenever the user performs a new query. We calculatethe F1 score of the very next query after re-summarization for allthe methods. For example, if at timestamp the summarizationmethod performs re-summarization or evolving using query + 1,then at timestamp + 1 we search the query + 2 in the new PKGand calculate the F1 score.",
  "Comparisons": "Effectiveness Comparison. We measure the F1 score of each PKGsummarization method in the auto-regressive querying scenariofor 10 users. We report the mean and standard deviation of sampledtimestamps in . First, both APEX2 and APEX2-N outperformthe existing baseline methods on the F1 score in all cases. Second,APEX2 and APEX2-N remain highly effective even if the knowledge",
  "APEX-N22.5280.5023.3050.0410.0180.0020.0240.003": "Our experiments show that both APEX2 and APEX2-N are muchfaster than re-running GLIMPSE for adaptive PKG summarization.Respectively, APEX2 and APEX2-N outperform GLIMPSE by 20to 30 and 40 to 75. If we regard ParallelPR as the optimal timecomplexity, then compared to GLIMPSE, APEX2 and APEX2-N get30 to 45 and 80 to 400 closer to the optimal. Overall, APEX2 and APEX2-N have similar efficiency performance to PageRank(walk length 1), which is a very strong baseline in time complexity.Furthermore, our APEX2 and APEX2-N have efficiency close to Par-allelPR. Moreover, our experiments show that APEX2 and APEX2-Nare more scalable because they outperform all the baseline methodsin the largest YAGO dataset. Considering APEX2-N and APEX2 together, APEX2-N has better experimental efficiency because itdoes not consider the users interest in relationships. This meansAPEX2-N could be a good choice for entity interest tracking andsuits tasks where the interest in triples is not very necessary.",
  "Ablation Study": "5.4.1Decay Ablation. In our design, decaying (i.e., a forgettingmechanism), is the key to the adaptive and extreme summariza-tion. Here, we compare the effectiveness under different levels ofdecaying to further show the importance of decaying. We run theuser querying scenario in MetaQA with varying values. Otherhyperparameters are set to be the same as .2. We use thesame query generated for MetaQA and report both APEX2 andAPEX2-Ns average F1 score of 10 users in . Larger meanslower decay level (less extent of decay). When = 1, the decayingis completely eliminated, and the PKG is full of outdated inter-ests, resulting in low F1 scores of both APEX2 and APEX2-N. Ingeneral, starting from = 0.5, the effectiveness does not change",
  ": Querying Effectiveness in Multiple Decay Levels": "much until = 0.9, and then the performance gets worse massivelyfrom = 0.9 to = 1. It turns out that when the storage space isvery limited, decaying the previous interests can pave the way forpersonalized summarization. 5.4.2Component Ablation. Aiming to accelerate the computation,after the necessary dynamic modeling of user interests, we designedincremental updating and incremental sorting. These two compo-nents serve to accelerate the computation and do not affect thecomputational results; therefore, to show that all three componentsof the APEX22 framework contribute to the overall efficiency per-formance, we can compare the mean execution time in secondsshown as follows.",
  "Hyperparameter Study": "We study the parameter sensitivity to show our models robustnessusing MetaQA dataset. From the result in , larger com-pression ratio leads to better F1 score, but after a certain value F1score does not change much. This is intuitive as the searching accu-racy increases with more triples stored in the PKG. In general, ourmethods are robust with damping factor and diffusing parameter.A larger diffusing diameter takes more time because more itemsget non-zero heat. Time per adapting phase increases quadraticallywith diffusing parameter, because the interested area grows with .",
  ": Parameter Study. From left to right: compression ratio , damping factor of neighbor , diffusing diameter": "conduct comprehensive experimental analysis on varying and report the results in Appendix C.6 due to page limitation. Inshort, on the effectiveness side, our methods, with varying of 2, 3, 6, achieved competitive query accuracy (i.e., F1 score) andstill outperformed baseline methods. On the efficiency side, thetime consumed by the varying methods is similar to that of = 1. These results suggest that our heat tracking method has arobust performance over timestamps.",
  "We provide a comparative case study to illustrate how our methodswork and that they can summarize high-quality PKGs": "5.7.1Dataset and Query. We do a case study on MetaQA datasetusing the queries of user 0 to show in which way our methods areable to adaptively summarize highly interested items into PKG. Weuse the first 33 queries from the user. Most of the interested part ofthe KG is shown in . In the first 3 groups of 10 queries, thequery entities are respectively the actor \"Stevan Riley\", the movie\"The Disappearance of Haruhi Suzumiya\" and the movie \"LOL\". 5.7.2Settings. We aim to study how APEX2, APEX2-N, GLIMPSEand PageRank deal with topic shift. Starting from the 31 query,the user asks (Chad Michael Murray, movie_to_actor, ?), i.e., \"whichmovies did Chad Michael Murray act in\" three times. The correctanswer entities are \"A Cinderella Story\", \"House of Wax\", and \"LeftBehind\". For APEX2 and APEX2-N, we use the same setting as inthe main experiments: both of them evolve every timestamp fromthe beginning. For GLIMPSE and PageRank, we give them higherprivileges that they can re-summarize each new timestamp, com-pared to per 9 timestamps in the main experiments. We use thesame hyperparameters as the main experiments. 5.7.3Results. The PKG results are shown in , 13, 14, 15.The summarized PKG at timestamp 1 have been fed query .(i) The sub-figure (a) (i.e., the PKGs after the first three groups ofqueries) of GLIMPSE and PageRank still contains all informationabout the out-of-interest topic \"Blue Blood\" and \"Stevan Riley\",which were accessed in the first 10 queries. (ii) For both APEX2 and APEX2-N, after the first query on the new topic, the threeanswer triples (Chad Michael Murray, movie_to_actor, A CinderellaStory), (Chad Michael Murray, movie_to_actor, House of Wax),and (Chad Michael Murray, movie_to_actor, Left Behind) get intothe PKG. However, after three times querying on the new topic,the PKG summarized by GLIMPSE only contains (Chad MichaelMurray, movie_to_actor, House of Wax), and the PKG summarizedby PageRank does not contain any of them. (iii) From APEX2 and APEX2-N (b) to (d), as topic on \"Chad Michael Murray\" is queriedagain and again, the connected group centered at \"Chad MichaelMurray\" grows larger. At timestamp 32 (Figure d), some of its 2-hopneighbors are included, such as (House of Wax, actor_to_movie,Nicolas Cage) in APEX2 and (A Cinderella Story, director_to_movie,Mark Rosman) in APEX2-N. The reason why the actor relationshipis included first is that, this relation is recently queried many timesand has a high relational interest. (iv) There are some triples thatare not very related to the user queries but are summarized in theGLIMPSE PKG, for example (Onibi, movie_to_language, Japanese),(john lithgow, tag_to_movie, 2010) and (Hercules, movie_to_genre,Animation). The PageRank PKG almost remains the same giventhree queries on the new topic. Compared to these, APEX2 andAPEX2-N produce PKGs that have intuitively higher quality interms of users interest.",
  "Related Work": "Graph compression or graph summarization has been a popularresearch topic in recent years. In 2018, Liu et al. wrote a survey and provided a taxonomy for graph summarization algorithms:static plain graph summarization , static labeled graphsummarization and dynamic plain graph summarization. Different techniques and metrics have been adoptedfor graph summarization. Kleindessner et al. proposes using k-center clustering for fair data summarization; SSumM greedilymerges supernodes to minimize the reconstruction error; MoSSo approximates the optimal utility by random search; GraphZIP focus on effective summarization for clique structures andcompress the graph by decomposing it into a set of cliques; NET-CONDENSE shrink the temporal networks by propagating andmerging the unimportant node and time-pairs; It is worth notingthat macroscopically deep-learning-based methods may not alwaysbe a good choice for large-scale graph summarization purposes. Thisis because a graph-scale embedding requires much larger spacethan the graph itself, which is already massive and needs to besummarized. However, partial embeddings might still be eligible.Adaptive summarization could be useful in many cases where thecomputational resource is limited, but the original graph is large.One example topic is Neural Graph Databases, which are consideredthe next step in the evolution of graph databases . NeuralGraph Databases are powered by neural query embedding meth-ods, which take large space complexity to be applied on the wholeinput graph . Adaptive summarization has the potential to fillthis gap by adaptively summarizing the whole input graph into adomain-specific partial graph personalized to the user.",
  "Conclusion": "In this paper, we propose APEX2, the first framework for adap-tive personalized knowledge graph summarization. We prove theadapting effectiveness, time complexity of APEX2 and its variantAPEX2-N. Then we find that by adopting APEX2 in a real-worldreal-time knowledge graph summarization scenario, much of thestorage space can be saved while maintaining high searching effec-tiveness. We design extensive experiments to show the superiorityof APEX2 over baseline methods.",
  "Limitations": "Our PKG summarization technique is particularly beneficial inscenarios where (1) users anticipate challenges in communicatingwith the central server and loading the entire KG, (2) are concernedabout their future query privacy and prefer not to send queriesdirectly to the server, and (3) expect a large volume of future queriesand want to reduce query response times. In cases where noneof these conditions apply, there may be no significant advantageto summarizing a PKG rather than querying the KG directly. Forexample, if a user queries the KG infrequently, direct querying mightbe a better choice since querying KG directly takes less time thansummarizing it. For instance, querying entire YAGO directly takesapproximately 1 second, although querying summarized YAGOcosts < 0.0001 second with competitive effectiveness, generating asummary takes around 6 seconds. Making the summarization forinfrequent user is interesting and challenging, we would like toexplore it in the future work.",
  "Broader Impact": "In the past decades, graphs and knowledge graphs have been serv-ing various real-world applications, from ranking , social net-work analysis , transportation , anomaly de-tection , community detection andrecommendation , to molecular biology and cli-mate sciences . Stepping into the era of large and foun-dation models , graphs have been leveraged in RetrievalAugmented Generation (RAG) , which enhances theretrieval and integration of relevant context for improving the qual-ity and relevance of generated responses, and knowledge graphpersonalization still holds immense potential for enabling moreaccurate, context-aware, and adaptive decision-making by integrat-ing domain-specific knowledge and leveraging the scalability andrepresentation power of these advanced models. Notably, recentworks on personalization and compression ofLarge Language Models (LLMs) underscore the significance of en-hancing adaptability, efficiency, and user-specific customization tobetter meet diverse application needs.",
  "Yikun Ban, Jiaru Zou, Zihao Li, Yunzhe Qi, Dongqi Fu, Jian Kang, Hanghang Tong,and Jingrui He. 2024. PageRank Bandits for Link Prediction. CoRR abs/2411.01410(2024). arXiv:2411.01410": "Caleb Belth, Xinyi Zheng, Jilles Vreeken, and Danai Koutra. 2020. What isNormal, What is Strange, and What is Missing in a Knowledge Graph: UnifiedCharacterization via Inductive Summarization. In WWW 2020. Kurt D. Bollacker, Colin Evans, Praveen K. Paritosh, Tim Sturge, and JamieTaylor. 2008. Freebase: a collaboratively created graph database for structuringhuman knowledge. In Proceedings of the ACM SIGMOD International Conferenceon Management of Data, SIGMOD 2008, Vancouver, BC, Canada, June 10-12, 2008,Jason Tsong-Li Wang (Ed.).",
  "Dongqi Fu, Dawei Zhou, Ross Maciejewski, Arie Croitoru, Marcus Boyd, and Jin-grui He. 2023. Fairness-Aware Clique-Preserving Spectral Clustering of TemporalGraphs. In WWW": "Dongqi Fu, Yada Zhu, Hanghang Tong, Kommy Weldemariam, Onkar Bhardwaj,and Jingrui He. 2024. Generating Fine-Grained Causality in Climate Time SeriesData for Forecasting and Anomaly Detection. CoRR abs/2408.04254 (2024). arXiv:2408.04254 Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi,Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang. 2023.Retrieval-Augmented Generation for Large Language Models: A Survey. CoRRabs/2312.10997 (2023). Cyril Goutte and ric Gaussier. 2005. A Probabilistic Interpretation of Precision,Recall and F-Score, with Implication for Evaluation. In Advances in Informa-tion Retrieval, 27th European Conference on IR Research, ECIR 2005, Santiago deCompostela, Spain, March 21-23, 2005, Proceedings. Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable Feature Learningfor Networks. In Proceedings of the 22nd ACM SIGKDD International Conferenceon Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17,2016. Jingrui He, Hanghang Tong, Qiaozhu Mei, and Boleslaw K. Szymanski. 2012.GenDeR: A Generic Diversified Ranking Algorithm. In Advances in Neural In-formation Processing Systems 25: 26th Annual Conference on Neural InformationProcessing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake",
  "Bowen Jin, Jinsung Yoon, Jiawei Han, and Sercan . Arik. 2024. Long-ContextLLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG.CoRRabs/2410.05983 (2024)": "Shinhwan Kang, Kyuhan Lee, and Kijung Shin. 2022. Personalized Graph Sum-marization: Formulation, Scalable Algorithms, and Applications. In 38th IEEEInternational Conference on Data Engineering, ICDE 2022, Kuala Lumpur, Malaysia,May 9-12, 2022. Matthus Kleindessner, Pranjal Awasthi, and Jamie Morgenstern. 2019. Fair k-Center Clustering for Data Summarization. In Proceedings of the 36th InternationalConference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California,USA.",
  "Jihoon Ko, Yunbum Kook, and Kijung Shin. 2020. Incremental Lossless GraphSummarization. In KDD 2020": "Danai Koutra, U Kang, Jilles Vreeken, and Christos Faloutsos. 2014. VOG: Sum-marizing and Understanding Large Graphs. In Proceedings of the 2014 SIAMInternational Conference on Data Mining, Philadelphia, Pennsylvania, USA, April24-26, 2014. Rmi Lam, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger,Meire Fortunato, Alexander Pritzel, Suman V. Ravuri, Timo Ewalds, Ferran Alet,Zach Eaton-Rosen, Weihua Hu, Alexander Merose, Stephan Hoyer, George Hol-land, Jacklynn Stott, Oriol Vinyals, Shakir Mohamed, and Peter W. Battaglia.2022. GraphCast: Learning skillful medium-range global weather forecasting.CoRR abs/2212.12794 (2022).",
  "Kyuhan Lee, Hyeonsoo Jo, Jihoon Ko, Sungsu Lim, and Kijung Shin. 2020. SSumM:Sparse Summarization of Massive Graphs. In KDD 2020": "Zihao Li, Yuyi Ao, and Jingrui He. 2024. SpherE: Expressive and InterpretableKnowledge Graph Embedding for Set Retrieval. In Proceedings of the 47th In-ternational ACM SIGIR Conference on Research and Development in InformationRetrieval, SIGIR 2024, Washington DC, USA, July 14-18, 2024, Grace Hui Yang,Hongning Wang, Sam Han, Claudia Hauff, Guido Zuccon, and Yi Zhang (Eds.).ACM, 26292634.",
  "Zihao Li, Dongqi Fu, Hengyu Liu, and Jingrui He. 2024. Hypergraphs as WeightedDirected Self-Looped Graphs: Spectral Properties, Clustering, Cheeger Inequality.arXiv preprint arXiv:2411.03331 (2024)": "Zihao Li, Dongqi Fu, Hengyu Liu, and Jingrui He. 2024. Provably ExtendingPageRank-based Local Clustering Algorithm to Weighted Directed Graphs withSelf-Loops and to Hypergraphs. arXiv preprint arXiv:2412.03008 (2024). Zihao Li, Lecheng Zheng, Bowen Jin, Dongqi Fu, Baoyu Jing, Yikun Ban, JingruiHe, and Jiawei Han. 2024. Can Graph Neural Networks Learn Language withExtremely Weak Text Supervision? arXiv preprint arXiv:2412.08174 (2024).",
  "Lihui Liu, Yuzhong Chen, Mahashweta Das, Hao Yang, and Hanghang Tong. 2023.Knowledge Graph Question Answering with Ambiguous Query. In Proceedingsof the ACM Web Conference 2023": "Lihui Liu, Boxin Du, Yi Ren Fung, Heng Ji, Jiejun Xu, and Hanghang Tong. 2021.KompaRe: A Knowledge Graph Comparative Reasoning System. In Proceedingsof the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining(Virtual Event, Singapore) (KDD 21). Association for Computing Machinery,New York, NY, USA, 33083318. Lihui Liu, Boxin Du, Jiejun Xu, Yinglong Xia, and Hanghang Tong. 2022. JointKnowledge Graph Completion and Question Answering. In Proceedings of the 28thACM SIGKDD Conference on Knowledge Discovery and Data Mining (WashingtonDC, USA) (KDD 22). Association for Computing Machinery, New York, NY, USA,10981108. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, OmerLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: ARobustly Optimized BERT Pretraining Approach. CoRR abs/1907.11692 (2019).arXiv:1907.11692",
  "James Thorne, Majid Yazdani, Marzieh Saeidi, Fabrizio Silvestri, Sebastian Riedel,and Alon Y. Levy. 2021. From Natural Language Processing to Neural Databases.Proc. VLDB Endow. (2021)": "Giannis Vassiliou, Fanouris Alevizakis, Nikolaos Papadakis, and HaridimosKondylakis. 2023. iSummary: Workload-Based, Personalized Summaries forKnowledge Graphs. In The Semantic Web - 20th International Conference, ESWC2023, Hersonissos, Crete, Greece, May 28 - June 1, 2023, Proceedings (Lecture Notes inComputer Science, Vol. 13870), Catia Pesquita, Ernesto Jimnez-Ruiz, Jamie P. Mc-Cusker, Daniel Faria, Mauro Dragoni, Anastasia Dimou, Raphal Troncy, and SvenHertling (Eds.). Springer, 192208.",
  "Quinton Yong, Mahdi Hajiabadi, Venkatesh Srinivasan, and Alex Thomo. 2021.Efficient Graph Summarization using Weighted LSH at Billion-Scale. In SIGMOD2021": "Zhichen Zeng, Boxin Du, Si Zhang, Yinglong Xia, Zhining Liu, and Hang-hang Tong. 2024.Hierarchical Multi-Marginal Optimal Transport for Net-work Alignment. In Thirty-Eighth AAAI Conference on Artificial Intelligence,AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial In-telligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artifi-cial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, Michael J.Wooldridge, Jennifer G. Dy, and Sriraam Natarajan (Eds.). AAAI Press, 1666016668. Yuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexander J. Smola, and Le Song.2018. Variational Reasoning for Question Answering With Knowledge Graph. InProceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18),New Orleans, Louisiana, USA, February 2-7, 2018.",
  "Lecheng Zheng, John R. Birge, Yifang Zhang, and Jingrui He. 2024. Towards Multi-view Graph Anomaly Detection with Similarity-Guided Contrastive Clustering.CoRR abs/2409.09770 (2024)": "Lecheng Zheng, Baoyu Jing, Zihao Li, Hanghang Tong, and Jingrui He. 2024.Heterogeneous Contrastive Learning for Foundation Models and Beyond. InProceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery andData Mining, KDD 2024, Barcelona, Spain, August 25-29, 2024, Ricardo Baeza-Yatesand Francesco Bonchi (Eds.). ACM, 66666676. Ming Zhong, Chenxin An, Weizhu Chen, Jiawei Han, and Pengcheng He. 2024.Seeking Neural Nuggets: Knowledge Transfer in Large Language Models froma Parametric Perspective. In The Twelfth International Conference on LearningRepresentations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Dawei Zhou, Kangyang Wang, Nan Cao, and Jingrui He. 2015. Rare CategoryDetection on Time-Evolving Graphs. In 2015 IEEE International Conference onData Mining, ICDM 2015, Atlantic City, NJ, USA, November 14-17, 2015, Charu C.Aggarwal, Zhi-Hua Zhou, Alexander Tuzhilin, Hui Xiong, and Xindong Wu (Eds.).IEEE Computer Society, 11351140. Dawei Zhou, Lecheng Zheng, Dongqi Fu, Jiawei Han, and Jingrui He. 2022.MentorGNN: Deriving Curriculum for Pre-Training GNNs. In Proceedings of the31st ACM International Conference on Information & Knowledge Management,Atlanta, GA, USA, October 17-21, 2022, Mohammad Al Hasan and Li Xiong (Eds.).ACM, 27212731. Dawei Zhou, Lecheng Zheng, Jiawei Han, and Jingrui He. 2020. A Data-DrivenGraph Generative Model for Temporal Interaction Networks. In KDD 20: The 26thACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event,CA, USA, August 23-27, 2020, Rajesh Gupta, Yan Liu, Jiliang Tang, and B. AdityaPrakash (Eds.). ACM, 401411. Xinwen Zhu, Zihao Li, Yuxuan Jiang, Jiazhen Xu, Jie Wang, and Xuyang Bai.2024. Real-time Vehicle-to-Vehicle Communication Based Network Cooper-ative Control System through Distributed Database and Multimodal Percep-tion: Demonstrated in Crossroads.CoRR abs/2410.17576 (2024). arXiv:2410.17576 Jiaru Zou, Mengyu Zhou, Tao Li, Shi Han, and Dongmei Zhang. 2024. PromptIn-tern: Saving Inference Costs by Internalizing Recurrent Prompt during Large Lan-guage Model Fine-tuning. In Findings of the Association for Computational Linguis-tics: EMNLP 2024, Miami, Florida, USA, November 12-16, 2024, Yaser Al-Onaizan,Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Lin-guistics, 1028810305.",
  "()1( ))(20)": "where Pr(|Q) stands for speculative preference on given thequery log, 1 is an indicator function, i.e., 1(X) = 1 X = True,and denotes the damping factor of neighbors () in the givenknowledge graph G. This equation assumes users preference ismore on the searched entity and can be generally pushed to its",
  "T 0 { Gs.t. ( |P, Q) 0}(24)": "where T 0 is the set of triples with nonzero marginal utility forany given KG G and the corresponding PKG P. means definedas. The sampling process stops when the number of triples in thesummarization reaches the restriction or bound of size. The overallGLIMPSE framework is shown in Algorithm 1. B.1.2PEGASUS. PEGASUS is a merging-based method forsummarizing personalized graphs (not specific to knowledge graphs).It determines how to merge supernodes and superedges by mini-mizing the reconstruction error, during which more considerationwill be put on a given set of targeted nodes. Formally, given a graphG = (V, E), a set of target nodes T, and a space budget , PEGA-SUS aims to find a summarized graph P = (V, E) of G that ispersonalized to T while satisfying the budget . The general logic isthat attributes (edge connectivities) near the target set T are morelikely to be retained in P than those far from the target set. Theoptimization of PEGASUS is based on the weighted reconstructionerror (T),",
  "B.2Proof of non-adaptability or corruption": "Here, we show that neither GLIMPSE or PEGASUS candeal with interest-evolving user queries very well. Our discussionshere are under the circumstance that the summarized PKG is alreadyfull. Due to the page limit, we briefly introduce the proof idea andflow. The detailed and formal proof is placed in the appendix.For GLIMPSE , the triples with higher utility will be morelikely to be included in the summarization. We show that once theusers interest has shifted from a previous topic to a new topic,GLIMPSE can adapt to new interest only if the volume of newqueries is considerably large, with respect to the volume of querieson previous interests. From this perspective, GLIMPSE is not veryagile for quick and ad-hoc new interest queries. Theorem B.1 (Non-adaptability of GLIMPSE in PKG Summa-rization). In adaptive PKG summarization setting, GLIMPSE couldnot swiftly adapt to users new interest after the previous summarizedPKG reaches the size budget. (Proof in Appendix E.4) For PEGASUS , according to Eq. 26 and Eq. 27, a historicaltarget node permanently gives a lower bound to all . In otherwords, if a node is close to a target node , node (as well as thenodes that are close to node ) will be given high weights. Then,we show that once these nodes (e.g., and its -hop neighbors) getshigh weights and are considered important, they are hard to bereplaced by the nodes of later new interests. This means that thoughthe users interest may already shift to other topics, previous topicsstill have high weights and may occupy the summary storage. Theorem B.2 (Non-adaptability of PEGASUS in PKG Sum-marization). In adaptive PKG summarization setting, PEGASUScould not effectively evolve with users new interest after the previoussummarized PKG reaches the size budget. (Proof in Appendix E.5) For iSummary , under the extremely small storage limitationin our experiment scenario, even if we set = , the summarizedPKG is still too large in terms of size. In this case, even thoughiSummary finds the shortest paths, those paths will not be used,and the PKG simply stores the explicitly searched nodes that havethe highest heat. In the extreme summarization case, iSummarybecomes a simple caching algorithm and does not have the abilityto infer the users interests. In other words, iSummary corruptsunder extremely small storage constraints since > , i.e., thenumber of seed nodes exceeds the maximum number of nodes tosummarize.",
  "precision + recall(32)": "\"Precision\" calculates the fraction of relevant instances amongthe summarized instances, while \"Recall\" is the fraction of relevantinstances that were summarized7. Taking both of them into account,F1 score provides a single number that reflects the models overallsearch performance.In our adaptive setting, we adapt historical PKG to the new query,wishing to make the adaption accurate. In this way, the very nextquery is the most valuable to test the utility of the current PKG, asfuture queries may shift again. Moreover, we report the mean andstd over multiple single next queries, showing the robustness.",
  "C.2Datasets": "YAGO3 is a huge semantic knowledge base, derived from WikipediaWordNet and GeoNames. Currently, the whole YAGO3 has knowl-edge of more than 10 million entities and contains more than 120million facts about these entities8. The whole YAGO3 is over 200GB.Due to the computational resource limitation, we use the coreYAGO3 facts that contains 4.2 million entities and 12.4 milliontriples, which is a 40% sub-YAGO3.",
  ": Cosine similarity between consecutive query topics in different datasets for one user. The cosine similarity is computedfrom Roberta text embeddings": "DBpedia was created by extracting semantically-structured in-formation from Wikipedia and other data sources. Specifically, weuse the DBpedia ontology, which is the \"heart of DBpedia\"9. TheDBpedia Ontology is a shallow, cross-domain ontology, which hasbeen manually created based on the most commonly used infoboxeswithin Wikipedia10.MetaQA consists of a movie ontology derived from the Wiki-Movies Dataset11. WikiMovies dataset, introduced by Miller etal., is constructed along with a graph-based KB consisting ofentities and relations, with the guarantee that each query can beanswered by the KB. We use the whole MetaQA dataset and the\"Vanilla\" branch of its query data.Freebase is a large collaborative knowledge base created by theFreebase community. We use FB15K237, a well-developed and com-monly used sub-knowledge-graph of the originally retired Freebasedata dump12.",
  "C.3Quality Analysis of Synthetic Queries": "To the best of our knowledge, there is no paired user query log andKG dataset, because the public user query datasets are anonymizedby the publisher and do not have user identification. Therefore,it is impossible to extract one specific users consecutive queries.Nonetheless, we conduct our experiments over our synthetic butrealistic query logs over benchmark KGs. In this section, we furthervalidate that our synthetic queries are of high quality.",
  "Resource:": "One piece of evidence is that our sampled queries follow thelogic and structure of Linked SPARQL Queries DBpedia anony-mous data dump13. In both LSQ and our queries, multiple queryrelations on the same query head appear in a batch and get an-swered sequentially. Then query relations start from another entityhead and repeat such a pipeline.We further conduct a study on the semantic of sampled topics. Asdiscussed in , for each dataset, we sample 10 users and 200queries for each user. To be specific, for each user, we uniformly ran-domly sampled 20 entities (i.e., topics) from all the entities in the KG.The user querying topics are evolving, which is exactly the reasonwhy we need adaptive personalized KG summarization. Therefore,we measure the semantic distances of each pair of consecutive top-ics. For example, sample a random user in the real-world MetaQAdataset, and we can get the query topic history as [Stevan Riley,The Disappearance of Haruhi Suzumiya, LOL, Chad Michael Mur-ray, Orson Scott Card, Grard Lanvin, menahem golan, ...All theMarbles, Operator 13, Heaven Help Us, peter pan, Mark Saltzman,Denise Dillaway, Hell Drivers, drummer, Beware of Pity, TheCruel Sea, The Great Gabbo, Takuya Kimura, The Secrets]. Fromthis log of topics, we can see the topic changes from different genremovies to different directors and even novels. Using the widely-usedlanguage model RoBERTa , we can compute the normalizedembedding vector for each pair of consecutive topics. Their co-sine similarities are [-0.2100357562303543, -0.2824622392654419,-0.18836577236652374, 0.2763717472553253, 0.2524639368057251,0.03907020390033722, -0.19497732818126678, -0.14980322122573853,-0.041029710322618484, -0.16855204105377197, 0.18958419561386108,",
  "(c) MetaQA": ": The distribution of cosine similarities between consecutive query topics in our synthetic queries. From left to right:YAGO, DBpedia, MetaQA. Our synthetic queries cover diverse topic-evolving scenarios. 0.3601977825164795, -0.278546005487442, 0.12963563203811646, -0.04322625696659088, 0.10026416927576065, 0.3284454941749573,-0.15075044333934784, -0.3128302991390228]. Such semantic dis-tances range wide and are diverse, validating that our sample userquery logs cover diverse topic-evolving scenarios and are of highquality.We also conduct the visualization for all users in every datasetto show their query topic shifts. The figures of the cosine similarityfrequency of all topic shifts for all datasets are shown in .Here, frequency means, among a total of 190 = 10 (20 1) topicchanges, i.e., number of users times topic evolves per user, howmany times the cosine similarity between consecutive topics fallsinto a specific range. In general, we can observe that the variouskinds of topics evolve, which demonstrates that our query synthesisand our adaptive summarization setting are challenging. Under sucha realistic setting, our proposed methods achieve effectiveness andefficiency outperformance than baseline methods.",
  "C.4Reproducibility": "C.4.1Code. The code for the main experiment is provided at The data of DBpedia and MetaQAis provided within our submission code, but YAGO3 is not due toits large size. You need to manually download YAGO3 follow ourinstruction if you want to play with it. Our code only supports \".gz\"Compressed Archive Folders, so you need to convert using gzip ifthe downloaded files are in other format.",
  "C.5Pre-experiments to Pick BaselineRe-summary Interval": "For the baseline methods, we vary their (re-summarization in-terval) hyperparameter. We observe that when = 9, their queryanswering accuracy does not drop much compared to = 1. Butif exceeds 9, their query answering accuracy drops significantly.An example curve for GLIMPSE on the MetaQA dataset is shownin . Therefore, we pick the Pareto-optimal = 9 for thebaselines.",
  "APEX2-N-60.6670.689": "From the results, by adjusting , we can balance betweenthe searching accuracy and the overall time consumption. By choos-ing a larger and processing multiple queries together withless granularity, APEX2 and APEX2-N can execute faster, but sacri-fice interest tracking accuracy.APEX2- and APEX2-N- are similar to APEX2 andAPEX2-N, except that the re-summarization interval is different.Because we only masked the summary updating phase in sometimestamps, at the timestamps where there is no mask, the outputPKGs are identical to those from APEX2 and APEX2-N, respec-tively. Therefore, the results and analysis from our case study still applies for APEX2- and APEX2-N- . Additionally, weconduct the decay ablation studies and hyperparameter studieson {1, 2, 3, 6} to further study these faster variants. Theresults are shown in and . From the ablation re-sults, the decaying mechanism controlled by is still crucial forAPEX2- and APEX2-N- , as setting close to 1 will de-crease their performances. From the hyperparameter study results,APEX2- and APEX2-N- demonstrate similar trendswhen adjusting compression ratio , damping factor of neighbor ,diffusing diameter . Overall, the effectiveness of APEX2-and APEX2-N- are robust to the damping factor of neighbor, diffusing diameter .",
  "C.7Handling Very Large Knowledge Graphs": "Theoretically, we give scalability proofs in Theorems 4.2 and 4.4.The complexity is further optimized to be independent of graphsize by setting an eliminating threshold. In the main experiments,we verify the scalability of our algorithms by two large KGs con-taining 10M triples. To further demonstrate the scalability of our al-gorithms, we conduct an experiment using a freebase subset of 30Mtriples with 0.001% compression ratio. The mean results are reportedin .",
  "DVisual Aid for Heat Diffusion Mechanism": "We use a heat model to simulate and track users interest fromrealistic perspectives of human interest phenomenons. (i) When ahuman searches for one thing over KG or the Internet, it means, inmost cases, he or she is interested in that thing. (ii) Human interesttransits from one thing to its related things. For example, when auser shows interest in a specific movie, such interest may extend torelated aspects such as the actors, director, or genre of the film. (iii)Over time, as humans tend to forget, their interest in a particularthing should naturally decline. From these facts and observations,",
  ": Parameter Study adjusting different . From left to right: compression ratio , damping factor of neighbor ,diffusing diameter": "we adopt a heat decay-inject-diffuse framework for heat diffusion,as shown in , which shows an illustration of users interestafter each timestamp.Initially (Timestamp 0), the user has not interacted with theKG, and the KG is all-white, i.e., we cannot infer if the user isinterested in any entity. Then, at timestamp 1, first, there is a decayover entities, resulting from all-white to still all-white. Second,the user searches (Interstellar, directed_by, ?), and gets the answertriple (Interstellar, directed_by, Nolan). Thus, we inject heat intoentities Interstellar and Nolan, as marked by the dashed-line box in Timestamp 1, . Third, there is a global push from all entitiesto its neighbors. Therefore, the entities connected to Interstellarand Nolan get partial heat.At Timestamp 2, first, there is heat decaying happening for all en-tities, and therefore entities such as Interstellar, McConaughey, Zim-mer, London change to lighter gray colors. Second, the user searches(Tenet, has_tag, ?) and gets the answer triple (Tenet, has_tag, Ac-tion). Thus, we inject heat into entities Tenet and Action, as marked",
  "EProof of TheoremsE.1Proof of Theorem 4.1": "Proof. All the sorting algorithms must output a permutation ofS C. With the fact that there is only one correct permutation thatsatisfies the sorted condition (the \"deterministic\" one), reconsiderthe sorting process to be eliminating the possible permutationsby new comparisons. Note that using the prior knowledge in thesorted S does not count toward new comparisons.",
  "We then define \"average connectivity of an area (sub knowledge graph) V = (E, R, T)\" asE ()": "| E |.Assume two areas U and V with connectivity and .Let the user initially query U for times, i.e., the query set |Q| = .For each query, E Pr(, Q) is expected to increase 2(1 + ). Then, for many queries, the total increase is 2(1 + ).Assume the users interest shifts to V and queries V for times. In the same way, the total increase is 2(1 + ). s Assume the totalquery is Q, where Q Q and Q Q.",
  "|R | , where E, is the": "average time being queried for each relation in R and E, |R| = R ( |Q). Also, E, is defined as the average time beingqueried for each entity in E. E,, E, are defined similarly.To adapt to the users interest shift, the summarized PKG should have (U, Q) < (V, Q). According to Eq. 23, it equals to",
  "E.5Proof of Theorem 3.2": "Proof. Here, we give the proof for the non-adaptability of PEGASUS .Similar to proof on in E.5 for GLIMPSE , we also assume two areas U and V with connectivity and . Let the user initially queryU for times. The query set |Q| = . The users interest then shifts to V and queries V for times.For PEGASUS here, we show that the historical search on U will permanently give high weights to some edges in U, and hence thesesummarized items in U (with high weights) are hard to get replaced by items of later interests.Consider two search queries in U search 1 and 2 (i.e., target nodes), then the edge 12 gets weight 1",
  "E.6Proof of Theorem 4.2": "Proof. First, we assume the query set and its volume |Q| = , and the users interest shifts to V and queries V for times. Then, wedenote the total query is , where and . Define E, and E, as the average time being queried for each entity in E and E,respectively; and E, and E, as the average time being queried for each relation in R and R, respectively."
}