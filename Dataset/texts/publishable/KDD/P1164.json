{
  "ABSTRACT": "Multimodal stock trading volume movement prediction with stock-related news is one of the fundamental problems in the financialarea. Existing multimodal works that train models from scratchface the problem of lacking universal knowledge when modelingfinancial news. In addition, the models ability may be limited bythe lack of domain-related knowledge due to insufficient data inthe datasets. To handle this issue, we propose the Prompt-basedMUltimodal Stock volumE prediction model (ProMUSE) to processtext and time series modalities. We use pre-trained language modelsfor better comprehension of financial news and adopt prompt learn-ing methods to leverage their capability in universal knowledge tomodel textual information. Besides, simply fusing two modalitiescan cause harm to the unimodal representations. Thus, we proposea novel cross-modality contrastive alignment while reserving theunimodal heads beside the fusion head to mitigate this problem.Extensive experiments demonstrate that our proposed ProMUSEoutperforms existing baselines. Comprehensive analyses furthervalidate the effectiveness of our architecture compared to potentialvariants and learning mechanisms. Our code will be available in",
  "multimodal learning, stock movement prediction, prompt learning": "ACM Reference Format:Ruibo Chen, Zhiyuan Zhang, Yi Liu, Ruihan Bao, Keiko Harimoto, and XuSun. 2023. Incorporating Pre-trained Model Prompting in Multimodal StockVolume Movement Prediction. In KDD 23 Workshop on Machine Learningin Finance, August, 2023, Workshop. ACM, New York, NY, USA, 9 pages. Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior specific permission and/or afee. Request permissions from 23 workshop, August, 2023, Workshop 2023 Association for Computing Machinery.ACM ISBN xxx-x-xxxx-xxxx-X/xx/xx...$xx",
  "INTRODUCTION": "Stock trading volume movement prediction is one of the fundamen-tal tasks in the financial area which has been paid much attentionto , and it has various important downstream applicationssuch as algorithmic trading and stock trading anomalydetection .Traditional researches only use historical trading data and relyheavily on feature engineering. They employ statistical models toforecast the time series like the Autoregressive Integrated MovingAverage model (ARIMA) . With the development of deeplearning techniques, Gharehchopogh et al. use linear regressionfor stock market trading volume prediction. More complicatedmodels utilizing LSTM and CNN are also applied in this field . However, these methods which only involve historical tradingdata suffer from the lack of basic stock information. Human tradersmake their decisions on multiple factors, including stock-relatednews. As a result, unimodal models using only historical tradingdata as the input may make incorrect predictions. Thus, researchersstart to introduce text information like news and tweets to bettermodel the stock movement. Typically, sentimental analysis modulesare used to reflect the market sentiment . Thesemethods usually adopt pipeline architecture and the two modalitiesare not integrated properly. Additional errors may get involved inthe prediction of sentiment. More recent works design multimodalmodels to jointly process text and time series data in order toacquire a better understanding. For example, Li et al. proposeto use the event-driven LSTM model to leverage news data. Zouand Herremans design a hybrid multimodal model using CNNand SVM as the backbone.A significant weakness of previous works is that they tend toconstruct a certain architecture and train from scratch. Currenthigh-quality financial news datasets all tend to be much smallerin size than the large-scale unlabelled corpora collected from theInternet since it is costly to write, collect and filter to get the relatednews. Thus, it is very difficult to train a robust large multimodalmodel based on them, causing inferior ability compared to pre-trained language models such as Fin-BERT and ChatGPT1. Inaddition, as the topics and contents of financial news can be broad,domain knowledge and universal knowledge are both requiredfor the textual learning process, making incorporating pre-trainedlanguage models necessary in this task.",
  ",1918]T": ": An overview of our model. Financial news and historical trading data are encoded respectively. The News Encoderemploys a frozen Financial-RoBERTa as the backbone and only the continuous prompts in all layers are tunable. The DataEncoder employs a pre-trained 6-layer transformer which will be fine-tuned during training. The fusion module utilizesnews and trading data representations to obtain an integrated prediction. The alignment loss is designed for cross-modalitycontrastive alignment and prevents damage to the unimodal representations during training. To settle the issue of lack of necessary knowledge, we proposethe Prompt-based MUltimodal Stock volumE movement predic-tion model (ProMUSE), which includes a News Encoder, a DataEncoder, and a fusion module. As illustrated in , the NewsEncoder uses a pre-trained language model as a backbone, and weuse prompt learning methods to efficiently exploit the knowledgewithin the pre-trained models. However, the direct fusion of thetwo encoders will severely damage the representation learning ofthe two modalities. Thus we propose to reserve a prediction headfor each encoder to receive unimodal supervision. Specially, weadd a cross-modality contrastive alignment loss to align the embed-ding space, alleviating the harm to their representations during thejoint training. For inference, we use the algorithmic mean of thetwo unimodal head predictions and the multimodal fusion resultthrough the fusion module. This makes it possible for our model torobustly generate outputs with even unimodal inputs.We validate our proposed ProMUSE model on the TOPIX500trading dataset and overnight financial news from Reuters.Our extensive experiments demonstrate that our proposed Pro-MUSE outperforms unimodal methods and multimodal baselineswith significant gaps. We also conduct a series of ablation studies toanalyze different modules in our model. We show that fusion-onlymethods perform worse as they lack unimodal supervision andtherefore incur damage to unimodal representations. on the otherhand, ensemble-only methods lose cross-modality contrastive align-ment. Our method implements multimodal fusion and alignmentreserving unimodal prediction heads to mitigate the harm to rep-resentation learning, thus achieving the best results. Besides, ourproposed ProMUSE can reach higher performance than methodstraining from scratch consistently under varying data sizes.Our main contributions can be summarized as follows:",
  "Task Formulation": "The overnight stock volume movement prediction task is a bi-nary classification problem: label 1/0 denotes that the volume goesup/down on the next trading day. The models classification de-cisions are based on the overnight news and the stock historicaltrading data for the past 20 days, which both serve as the modelsinput. Overnight News. Following , we only adopt news head-lines for our task, as they are more informative with a suitablelength for processing. The overnight news can be modeled as aninput sentence with tokens = {1,2, ,}. The overnightnews happened after the close of the 20th days trading market andbefore the opening of the next day.",
  "Incorporating Pre-trained Model Prompting in Multimodal Stock Volume Movement PredictionKDD 23 workshop, August, 2023, Workshop": "Stock Historical Trading Data. The overall stock historical tradingdata includes trading volumes and prices for the past 20 daysand 10 time slots for each day with a granularity of 30 minutes.The trading data of a specific time slot includes the volume ,high price , low price , open price , and close price . Theoverall stock historical trading data can be formulated as ={, | Z, Z}, where , represents thetrading data for the -th time slot in the -th day, namely , =T R5. Our goal is to predict the volume movement of the first time slot forthe 21st day 21,1, which is the first 30 minutes after the openingof the market. Following Zhao et al. , we define the movementhere as a comparison between 21,1 and the average volume in thesame time slot for the past 20 days, so the final prediction targetlabel can be formulated as:",
  "Model Architecture Overview": "Our proposed ProMUSE mainly includes two large-scale pre-trainedmodules to process the input: the News Encoder for the text modal-ity, and the Data Encoder for the time series modality. The twoencoders can effectively capture the features from the input andtransform them into corresponding vectors. They also use theirprediction head to produce unimodal losses and prediction to re-ceive unimodal supervision, which will be introduced in section 2.3and section 2.4. In section 2.5, we further utilize a fusion modulefor generating multimodal losses and outputs. The cross-modalitycontrastive alignment described in section 2.6 is designed to helpimprove unimodal representations during multimodal learning.Finally, we use the weighted sum of the two unimodal losses, mul-timodal loss, and alignment loss for the training objective in sec-tion 2.7. During inference in section 2.8, unimodal and multimodalpredictions are combined. An overview of the model architectureis shown in .",
  "The News Encoder is Financial-RoBERTa2, a 24-layer RoBERTa model pre-trained on financial text, such as financial statements,news, and earnings announcements. To avoid overfitting on the": "limited news data as well as to accelerate the training procedure, weadopt prompt learning methods and choose P-Tuning v2 as itreaches the best performance in our preliminary experiments. Thismethod inserts soft prompts in each layer of the Transformer-basedmodels. We set the prompt length to 20 and enable the reparame-terization. In our experiments, only the prompts are tunable, whileall other parameters in the language model are frozen.Given the input news including tokens, we insert a CLStoken at the beginning of the sentence: {CLS,1,2, ,}.Then we utilize the Financial-RoBERTa and P-Tuning v2 method totransform it into a series of vectors {NewsCLS ,News1,News2, ...,News}:",
  "Data Encoder": "In order to acquire high-quality representations for historical trad-ing data, we pre-train a 6-layer Transformer model. Recall thathistorical trading data = {, | Z, Z},, R5, the input time series can therefore be formulated as[CLS;1,1;1,2;1,3; ;2,1; ;20,10] R2015 in the time or-der, where the time slot number is 1 + 20 10 = 201. The DataEncoder transforms the input series into a hidden vector series[DataCLS ;Data1,1 ;Data1,2 ;Data1,3 ; ;Data2,1 ; ;Data20,10] R201Data:",
  "Cross-Modality Contrastive Alignment": "Contrastive learning is a widely used technique in the multi-modalarea. Methods like CLIP and ALPRO use cross-modalitycontrastive losses to align representations in different embeddingspaces. In our model, we implement the cross-modality contrastivealignment between overnight news and historical trading data withthe alignment loss.Only matched pairs in a batch are considered to be positive pairs.Given a batch of pair News = [News1, News2, , News], Data =[Data1, Data2, , Data] RAlign and represents the batch size,we define the similarity using dot-product following :",
  "LAlign = LN2DAlign + LD2NAlign.(17)": "Note that previous works use the cross-modality contrastive lossto boost performance on retrieval tasks such as image classifications,and some even report harm on other tasks .However, in our model, LAlign is designed to improve represen-tation learning in different modalities, and the defined similaritySim is not used for inference.In addition, embedding spaces of text and historical data can besignificantly different because they are not so strongly connectedas typical settings in image-text or video-text scenarios. Deep con-nections between text-data modalities can cause models to learnincorrect relations and fall into the trap of overfitting as shownin our analysis experiments. We find that as the relatively simplecross-modality contrastive alignment loss is applied to the outputof the encoders, it encourages multimodal fusion and alignmentand does not harm the structures of the large models.",
  "Samples74,9502,2144,072": "is used as the training set, and Jan. 1st, 2018 to Apr. 30th, 2018 asthe development set, May 1st, 2018 to Sept. 30th, 2018 as the testset. During data processing, we drop the data point where thereis a missing entry. The overnight news is collected from ReutersFinancial News3. Following , the data is filtered with RIClabels provided by Reuters, which are the possible stocks that maybe influenced by the news.In this paper, we only select the data in which both the overnightnews and the historical trading data are available for stock move-ment prediction. In addition, we filter the data where the volumemovement is not significant enough, alleviating the effect ofrandomness and minor, irrelevant news:",
  "where EMA1 is initialized as 1,1, and we use EMA20 as aprediction of 21,1. The prediction is = I (EMA20 > )": "3.2.2Models Training from Scratch. We try to train a six-layerTransformer from scratch with the hidden size set to 200 to replacethe pre-trained Financial-RoBERTa in the News Encoder. Withregard to the Data Encoder, we try multiple structures including lin-ear, one-layer LSTM , and a six-layer Transformer. The detailedstructure is similar to Zhang et al. . We also explore differentfusion or ensembling paradigms for model training from scratch.",
  "Settings and Hyperparameters": "We train every model in our experiments for 40 epochs and reportthe test accuracy as the result. The checkpoints with the best accu-racy on the development set are selected for the report. We repeatevery experiment 4 times and report the average results.In our main experiments, we adopt the AdamW optimizer ,using the learning rate as 1e-5 and the weight decay factor as 1e-3.The batch size is chosen as 32. The weights for each loss are setas = = = 1 and Align = 0.1. Align is set to 200. We usethe log value of the stock historical trading data as the input ofthe Data Encoder.For the pre-training of our Data Encoder, we run 100 epochsand save the checkpoint with the lowest development loss for themain experiments. We also use the AdamW optimizer with thelearning rate as 1e-5 and the weight decay factor as 1e-3. AdditionalExponential decay is used with a factor of 0.95. Pre-training batchsize is set for 64.",
  "Our main experimental results can be found in": "3.4.1Training from Scratch. We adopt a six-layer Transformer asan alternative to the Financial-RoBERTa model in the News Encoder.In the case of the Data Encoder, we explore linear, LSTM, and Trans-former models as substitutes for the pre-trained Transformer. Theoutcomes reveal that the available data volume is insufficient fortraining a News Encoder from scratch, while our method success-fully utilizes the knowledge of the pre-trained Financial RoBERTa.Furthermore, the linear and LSTM structures yield inferior per-formance compared to Transformer. The benefits of employing apre-trained Transformer model are also evident. 3.4.2Unimodal Models. Unimodal models perform significantlyworse than multimodal methods, as only the information of a singlemodality is fed to them, and both modalities are important in thistask. Besides, we find that prompting the Financial-RoBERTa canmore effectively leverage its knowledge and performs better thanfine-tuning. Moreover, modeling historical trading data is easierthan financial news, thus data-only methods can achieve betterperformance than new-only models.",
  "Fusion-Only and Ensemble-Only Methods. Direct multimodalfusion or ensemble can achieve improvement compared with uni-modal methods. However, fusion-only methods without unimodal": "supervision may inflict damage on unimodal representations, andensemble-only methods lack multimodal connections. Our methodsettles these problems by constructing fusion and cross-modalitycontrastive alignment as well as retaining unimodal predictions tohelp representation learning. 3.4.4Effectiveness of ProMUSE. As shows, ProMUSE canachieve the best performance. We successfully surpass the widely-used traditional EMA baseline in the financial area and exhibit ouradvantages against the models training from scratch, unimodalmodels, fusion-only methods, and ensemble-only methods.",
  "Linear72.47": "The ensemble-only method can cause degradation where accuracyfalls from 73.56 to 72.28 as cross-modality interaction is lost. Thefusion-only setting also triggers a loss in accuracy to 72.47 due toa heavier influence on unimodal representations without LAlign.Dropping unimodal heads will severely damage the performanceas both modalities are indispensable. The removal of the fusion oralignment process also breaks the connections between modalities.Note that during the inference stage, predicting ( Data) and( News) does not need much extra computation as the calculationof ( Fusion) also needs the calculation of two heads. Predictingthem for ensemble can get an improvement from 73.35 to 73.56. 4.1.2Effectiveness of Fusion Models. In this section, we furtherexplore different fusion models in . The linear fusion methodwhich is adopted in our proposal outperforms all other variants.Here Attention is described in equation 24, One-layer/Six-layerTransformer is stacked on the two encoders as discussed in thebaseline section. Fusing data into the News Encoder methods useDataCLS to generate the continuous prompts for the News Encoder.The reason is that the two encoders are pre-trained and will noteasily overfit into the knowledge of the small dataset, while thecomplicated fusion methods such as introducing another Trans-former model are trained from scratch, which create additionalparameters and can lead to incorrect connections and overfitting. 4.1.3Effectiveness of Ensemble Algorithms. We test the effective-ness of different ensemble algorithms in . In our experi-ments on ensemble-only models, we find that an average of newsprediction and data prediction perform best. In accordance with Learnable Predicted Normalized AverageProMUSE",
  "Prompt73.56": "our previous analysis of fusion models, complicated ensemble algo-rithms also tend to deteriorate the performance, because new param-eters involved in ensembling are estimated on a small dataset, whichare difficult to generalize and may cause overfitting. Therefore wechoose to calculate the algorithmic mean of ( News), ( Data) and( Fusion) for inference in our model.",
  "Why We Utilize P-Tuning v2 Paradigm": "In search of the proper paradigms for our News Encoder andData Encoder, we conduct the experiments in . The tem-plate we use for the hard prompt in our experiments is News:{1,2, ,}. The volume will go up/down. Soft prompt hererepresents implementing continuous prompts in the embeddinglayer, which are similar to Prefix-Tuning and P-Tuning .We find that P-Tuning v2 best suits the News Encoder andachieve an accuracy of 63.75, which inserts continuous promptsin every layer due to more trainable parameters and deeper layers.Fine-tuning the Data Encoder can get great improvement againstzero-shot setting, but fine-tuning the large Financial-RoBERTamodel with limited data can easily get into the overfitting problemand cause sub-optimal, with the accuracy dropping to 66.79.",
  "ProMUSE62.6967.3873.56": "set and test set are kept unchanged, and we train all models for 80epochs to compensate for the reduction of data volume.Here we compare the performance of ProMUSE with a six-layertransformer which is trained from scratch and only receives his-torical trading data input because it performs best in our previousexperiments. We drop News Encoder as the data volume contin-ues to shrink and a high-quality encoder cannot be obtained underthese circumstances. The results are shown in . We find thatProMUSE can achieve significant improvement over training fromscratch in all settings, proving that exploiting the universal knowl-edge of pre-trained language models through prompt learning isessential in this task.",
  "ProMUSE Helps Representation Learning": "In this section, we discuss why our proposal can achieve improve-ment. As shows, the introduction of multi-modal learningcan get general gains compared to unimodal models. However,simultaneously training the two modalities can harm the represen-tation learning of the individual encoders, as we see a decrease inaccuracy for news-only or data-only input scenarios.Our model uses the cross-modality contrastive alignment andmulti-modal fusion to better alleviate the damage caused in themulti-modal learning process, providing a strong constraint andregularization for the unimodal encoders to best avoid degradation.",
  "RELATED WORK5.1Pre-trained Language Models": "Since Tranformer shows great success in the natural languageprocessing (NLP) area, various pre-trained language models are pro-posed and achieve state-of-the-art performance in numerous NLPtasks. Models such as and GPT-3 exploit Transformer decoderstructure to construct unidirectional autoregressive language mod-els. Bidirectional BERT-like models are basically basedon Transformer encoders. T5 , BART , and Flan-T5 choose to adopt the encoder-decoder framework. They are trainedon corresponding pre-training tasks and large unlabelled corpora,and models become increasingly large in size to pursue bet-ter performance. Recently, Reinforcement Learning from HumanFeedback techniques are applied to ChatGPT and GPT-4 , whichgain outstanding performance.",
  "Multimodal Learning": "Existing multimodal learning methods mainly focus on image-text and video-text tasks. UNITER learns joint contextualizedrepresentations for both text and image through pretraining. ViL-BERT extends the BERT model by the co-attentional Trans-former layers for learning task-agnostic representations of imagecontent and natural language. ALIGN and CLIP constructthe dual-encoder architecture to align visual and textual representa-tions using image-text contrastive learning, and the cross-modalitycontrastive loss has become an important component in many mod-els. BLIP-2 uses a lightweight querying Transformer to learnfrom frozen image encoders and large language models.",
  "Stock Movement Prediction": "Stock movement prediction is a key research direction in the fi-nance area. Xu and Cohen present a deep generative modelto jointly learn from tweet text and price signals, and use recur-rent latent variables to process stochasticity. Li et al. design anLSTM-RGCN model for learning overnight news and the correlationbetween stocks. Chen et al. set up a dual-process meta-learningmethod to mine general patterns and stock-specific knowledge. Xieet al. analyze the zero-shot ability of ChatGPT in multimodalstock movement prediction.",
  "CONCLUSION": "In this paper, we present ProMUSE, a prompt-based multimodalstock volume movement prediction model, to fully exploit the tex-tual information in financial news and the potential of pre-trainedlanguage models with limited data. We use the News Encoder andData Encoder to process the overnight financial news and historicaltrading data respectively, and use a fusion model to generate multi-modal output. Unimodal supervision, multimodal supervision, andcross-modality contrastive alignment are used for training, whileunimodal and multimodal predictions constitute the final inferenceresult. Extensive experiments show that our method significantlyoutperforms various baselines. Comprehensive analysis testifies tothe effectiveness of different modules. Moreover, ProMUSE can helpmitigate the harm to representation learning during joint trainingof textual and time series modalities.",
  "Bipin B Ajinkya and Prem C Jain. 1989. The behavior of daily stock markettrading volume. Journal of accounting and economics 11, 4 (1989), 331359": "George EP Box and David A Pierce. 1970. Distribution of residual autocorrelationsin autoregressive-integrated moving average time series models. Journal of theAmerican statistical Association 65, 332 (1970), 15091526. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shot learners. Advances in neuralinformation processing systems 33 (2020), 18771901.",
  "Deli Chen, Yanyan Zou, Keiko Harimoto, Ruihan Bao, Xuancheng Ren, and XuSun. 2019. Incorporating fine-grained events in stock movement prediction. arXivpreprint arXiv:1910.05078 (2019)": "Kai Chen, Yi Zhou, and Fangyan Dai. 2015. A LSTM-based method for stockreturns prediction: A case study of China stock market. In 2015 IEEE internationalconference on big data (big data). IEEE, 28232824. Ruibo Chen, Wei Li, Zhiyuan Zhang, Ruihan Bao, Keiko Harimoto, and Xu Sun.2022. Stock Trading Volume Prediction with Dual-Process Meta-Learning. In JointEuropean Conference on Machine Learning and Knowledge Discovery in Databases.Springer, 137153.",
  "Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, ZheGan, Yu Cheng, and Jingjing Liu. 2019. Uniter: Learning universal image-textrepresentations. (2019)": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, GauravMishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Se-bastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways.arXiv preprint arXiv:2204.02311 (2022). Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus,Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scalinginstruction-finetuned language models. arXiv preprint arXiv:2210.11416 (2022).",
  "Sepp Hochreiter and Jrgen Schmidhuber. 1997. Long short-term memory. Neuralcomputation 9, 8 (1997), 17351780": "Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 2021. Scaling up visual and vision-language representation learning with noisy text supervision. In InternationalConference on Machine Learning. PMLR, 49044916. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, PiyushSharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised learningof language representations. arXiv preprint arXiv:1909.11942 (2019). Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, AbdelrahmanMohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoisingsequence-to-sequence pre-training for natural language generation, translation,and comprehension. arXiv preprint arXiv:1910.13461 (2019). Dongxu Li, Junnan Li, Hongdong Li, Juan Carlos Niebles, and Steven CH Hoi.2022. Align and prompt: Video-and-language pre-training with entity prompts. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.49534963. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrappinglanguage-image pre-training with frozen image encoders and large languagemodels. arXiv preprint arXiv:2301.12597 (2023). Qing Li, Jinghua Tan, Jun Wang, and Hsinchun Chen. 2021. A Multimodal Event-Driven LSTM Model for Stock Prediction Using Online News. IEEE Transactionson Knowledge and Data Engineering 33, 10 (2021), 33233337. Wei Li, Ruihan Bao, Keiko Harimoto, Deli Chen, Jingjing Xu, and Qi Su. 2021.Modeling the stock relation with graph network for overnight stock movementprediction. In Proceedings of the twenty-ninth international conference on interna-tional joint conferences on artificial intelligence. 45414547.",
  "Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuousprompts for generation. arXiv preprint arXiv:2101.00190 (2021)": "Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and JieTang. 2022. P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning AcrossScales and Tasks. In Proceedings of the 60th Annual Meeting of the Association forComputational Linguistics (Volume 2: Short Papers). Association for ComputationalLinguistics, Dublin, Ireland, 6168. Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang,and Jie Tang. 2021. P-tuning v2: Prompt tuning can be comparable to fine-tuninguniversally across scales and tasks. arXiv preprint arXiv:2110.07602 (2021).",
  "Thien Hai Nguyen, Kiyoaki Shirai, and Julien Velcin. 2015. Sentiment analysison social media for stock movement prediction. Expert Systems with Applications42, 24 (2015), 96039611": "OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL] Venkata Sasank Pagolu, Kamal Nayan Reddy, Ganapati Panda, and Babita Majhi.2016. Sentiment analysis of Twitter data for predicting stock market movements.In 2016 international conference on signal processing, communication, power andembedded system (SCOPES). IEEE, 13451350. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,et al. 2021. Learning transferable visual models from natural language supervision.In International conference on machine learning. PMLR, 87488763. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits oftransfer learning with a unified text-to-text transformer. The Journal of MachineLearning Research 21, 1 (2020), 54855551. Sreelekshmy Selvin, R Vinayakumar, EA Gopalakrishnan, Vijay Krishna Menon,and KP Soman. 2017. Stock price prediction using LSTM, RNN and CNN-slidingwindow model. In 2017 international conference on advances in computing, com-munications and informatics (icacci). IEEE, 16431647.",
  "Philip Treleaven, Michal Galas, and Vidhi Lalchand. 2013. Algorithmic tradingreview. Commun. ACM 56, 11 (2013), 7685": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin. 2017. Attention is allyou need. Advances in neural information processing systems 30 (2017). Qianqian Xie, Weiguang Han, Yanzhao Lai, Min Peng, and Jimin Huang. 2023.The Wall Street Neophyte: A Zero-Shot Analysis of ChatGPT Over MultiModalStock Movement Prediction Challenges. arXiv preprint arXiv:2304.05351 (2023). Yumo Xu and Shay B Cohen. 2018. Stock movement prediction from tweets andhistorical prices. In Proceedings of the 56th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers). 19701979."
}