{
  "ABSTRACT": "We present Catalog Phrase Grounding (CPG), a model that canassociate product textual data (title, brands) into corresponding re-gions of product images (isolated product region, brand logo region)for e-commerce vision-language applications. We use a state-of-the-art modulated multimodal transformer encoder-decoder archi-tecture unifying object detection and phrase-grounding. We trainthe model in self-supervised fashion with 2.3 million image-textpairs synthesized from an e-commerce site. The self-supervisiondata is annotated with high-confidence pseudo-labels generatedwith a combination of teacher models: a pre-trained general do-main phrase grounding model (e.g. MDETR) and a specialized logodetection model. This allows CPG, as a student model, to bene-fit from transfer knowledge from these base models combininggeneral-domain knowledge and specialized knowledge. Beyondimmediate catalog phrase grounding tasks, we can benefit fromCPG representations by incorporating them as ML features intodownstream catalog applications that require deep semantic under-standing of products. Our experiments on product-brand matching,a challenging e-commerce application, show that incorporatingCPG representations into the existing production ensemble systemleads to on average 5% recall improvement across all countriesglobally (with the largest lift of 11% in a single country) at fixed95% precision, outperforming other alternatives including a logodetection teacher model and ResNet50.",
  "Phrase Grounding, Object Detection, Transformers, MultiModalModel, Natural Language Understanding": "ACM Reference Format:Wenyi Wu, Karim Bouyarmane, and Ismail Tutar. 2023. Catalog PhraseGrounding (CPG): Grounding of Product Textual Attributes in ProductImages for e-commerce Vision-Language Applications. In Proceedings ofKDD 22: ACM SIGKDD Conference on Knowledge Discovery and Data Mining(KDD 22). ACM, New York, NY, USA, 7 pages. Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior specific permission and/or afee. Request permissions from 22, August 1418, 2021, Washington, DC, USA 2023 Association for Computing Machinery.ACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00",
  "INTRODUCTION": "Incorporating multimodal understanding of image and textual dataof products is essential for many e-commerce applications. A typicalproduct page on an e-commerce website consists of a product title inthe form of short textual description, and an image representationof the product. The image can display an isolated product image orthe image of the product in the context of use in an environment.Additional fields can be found in the product page, such as brand,dimensions, etc. An e-commerce catalog constitutes, therefore, avery rich corpus that can be used to design self-supervised vision-language tasks for the pre-training of deep learning product-vision-language understanding models.One such vision-language self-supervision task that can be craftedfrom the e-commerce catalog and that takes particular advantageof its specificities is phrase grounding. Phrase grounding con-sists in associating (or grounding) a textual phrase or part of it toa specific region of an image. The nature of e-commerce productdata allows for expressive and domain-specific multi-task phrasegrounding pre-training: grounding product title noun to isolatedproduct region in the image, grounding product brand field to brandlogo region in the product image, etc. We call the multi-task phrasegrounding of e-commerce specific entities such as product-brand-to-logo and product-noun-to-object Catalog Phrase Grounding (CPG).CPG outputs semantic rich representations that are particularlysuited for e-commerce domain-specific downstream tasks.Pre-training of the CPG model is done in a self-supervised waywith two teacher models: a pre-trained general-domain phrasegrounding model, and a specialized logo-detection model. The stu-dent model learns to combine the knowledge distilled from bothteacher models in multi-teacher multi-task learning setting. Thetasks and phrases are crafted and self-generated from the catalogcorpus, allowing to benefit from very large amount of unlabeleddata. We illustrate two products with self-generated annotations in.The learned CPG visual representations are powerful, semantic-rich, fine-grained representations of the e-commerce product im-ages. We demonstrate it with a challenging task: product-brandmatching. A brand is a complex e-commerce entity that is repre-sented by fields such as the brand name, brand logo (rarely), aswell as a set of representative sample products. Brand name aloneis not sufficient to characterize the brand entity, due to the sheernumber of brands and homonyms. Matching a product to a brandentity thus consists in using the information from the brand and therepresentative sample products and inferring whether the query",
  "RELATED WORK AND CONTRIBUTIONS": "A standard joint visual and textual understanding model is typically trained with a fixed set of visual concepts (classes) onvision-language tasks, such as visual question answering , objectdetection and phrase grounding. MDETR extended trans-former based object detection model, i.e. DETR , to a modulatedmultimodal model trained with two tasks: object detection andphrase grounding. Therefore, MDETR could be pre-trained with1.3M text-images pairs having explicit alignment between phrasesin text and objects in the image from combined pre-existing datasets,e.g. MS COCO, Flickr30k and etc.In order to further expand the visual concepts of image regionsbeyond vocabularies of pre-existing datasets, a recent line of work considers using web-scale raw image-text pairs. CLIP demonstrates that an image-level representations can be learned ef-fectively through alignment between raw image-text pairs collectedfrom the internet. Following the same idea, GLIP pre-trains aphrase grounding model with 24 million web-crawled image-textpairs. The regions of interest in images were detected by a pre-trained teacher model. It has been shown that the pre-trained GLIPlearned effectively from the broader set of raw text to generatesemantic enriched region-level visual representations. However,these models are not trained for e-commerce specific vocabularyand entities in synergy with downstream e-commerce applications.Our proposal, CPG, is a transformer encoder-decoder architec-ture based model learning semantic rich visual representationsfor e-commerce specific entities through multiple domain-specifictasks. Following the trend of using free-form text, we train the CPGmodel with 2.3M product entities synthesized from an e-commercesite in a self-supervised fashion. The bounding boxes for product-brand-to-logo grounding task are generated from a YOLO-basedlogo detection model while the product brands exist in productpage already. The bounding boxes for product-noun-to-object taskare generated by a pre-trained general domain modulated detec-tion model conditional on noun phrases parsed from free-formproduct title using a general NLP parser. : Left: logo localized by the logo detection teachermodel and product localized by the phrase-grounding teachermodels; Right: only product regions localized by the phrase-grounding teacher model. Teacher models can locate rareentities expressed in product titles. We present usage of the CPG representations as ML featuresto address one of major challenges of downstream product-brandmatching application, which is differentiating homonym brands,especially when logos are absent, as shown in . The GPG rep-resentations contain comprehensive visual-language understandingof logos, brand strings, product details for the query product entityand for all brand representative product entities. Therefore, thesimilarity between them shed light on identifying the correct brandof an input product from homonym ones, either through straight-forward logo comparison or through product region comparison inthe absence of logos.We summarize the contributions of our work as follows: We propose an efficient and scalable method to learn seman-tic rich visual representations for e-commerce products in aself-supervised fashion. We leveraged massive raw producttext data and images and applied teacher models to obtainregion-phrase alignment annotations. In this way, we ex-tended the limited general vocabulary to substantial visualconcepts expressed in the e-commerce catalog. We transfer knowledge from 2 teacher models: a logo detec-tion model and a general domain phrase grounding model byleveraging high-confidence predictions as pseudo labels forcatalog specific tasks so that CPG benefits from both general-domain knowledge and specialized catalog knowledge.",
  "MODAL ARCHITECTURE AND TRAINING3.1CPG Model Architecture": "The CPG model in this paper grounds logos and isolated productdetails to the brand and noun phrases in free-form title simultane-ously. We manually craft the caption of image by concatenating thebrand string and the product title together and inserting commain between. To facilitate understanding of the logo concept anddistinguishing noun words in brands from general vocabulary, e.g.brand Apple versus fruit apple, we prefix the brand string with : Homonym brands are independent brands associ-ated with the same name (e.g. Gerber) selling different prod-ucts. The Gerber baby food has the logos in image whileGerber tools and Gerber plumbing fixtures dont.",
  "Catalog Phrase Grounding (CPG): Grounding of Product Textual Attributes in Product ImagesKDD 22, August 1418, 2021, Washington, DC, USA": "training set are randomly split into 80% training set and 20% vali-dation set. For test, we collected 20, 000 (product, brand) pairs percountry from 9 countries globally consisting of A-F and 3 new Eng-lish countries (we will refer to them as G, H and I). We use differentsampling strategies to collect training and test samples. For training,in order to utilize auditing resources efficiently, we exclude trivialnegative pairs, the products in which are obvious generic productsirrelevant to any known brand entity. To collect test pairs, we sim-ply random sample from the catalog of an e-commerce site. Thecatalogs in different countries show different patterns. In country I,there are more products with homonym brands than the rest andtherefore, it is expected to show performance lift once we includeimage signals. We denote a pair as positive if the product is sold bythe brand and as negative if not. We collect ground truth of productand brand relationships through manual labeling.",
  "L = L + L(1)": "For contrastive alignment loss between noun phrases spannedmultiple tokens and object queries, we use the same fine-grainedcontrastive loss proposed in MDETR. We consider the maximumnumber of tokens to be , the number of the fixed set of objectqueries to be , the set of tokens representing object to be + andthe set of objects associating with token to be + . The contrastiveloss for objects is normalized by the number of positive tokens:",
  "Scalable Training with Pseudo Labels": "In order to extend general vocabulary to noun phrases in producttitles in e-commerce platform, the traditional way is to manually la-bel phrases in corpus. Furthermore, to train a unified model for logodetection and product phrase grounding, it requires manual labelsof bounding boxes which is expensive. Therefore, to scale the CPGmodel training with abundant textual concepts in e-commerce, wequery 2.3M raw product images and textual attributes from Englishcountries of an e-commerce site and use them in a self-supervisedframework. In order to generate product-noun-to-object ground-ing pseudo labels, we start with noun phrases extraction. Givenaugmented product image caption combining product brand and",
  "KDD 22, August 1418, 2021, Washington, DC, USAWu and Bouyarmane, et al": "title attributes, as described in section 3.1, we apply general Nat-ural Language Processing parser to identify all noun words intitle and include all leading adjective words prior to noun words toobtain noun phrases. We then apply a unified object detection andphrase grounding teacher model, MDETR to detects product re-gions conditioned on the extracted noun phrases. We obtained 3.2Munique noun phrases from titles with 6.1M associated bounding-boxes. To collect product-brand-to-logo grounding pseudo labels,we simply apply a logo detection model that is trained withe-commerce product images to localize logo regions in the imageif exist. Then we associate logo regions to the brand section inaugmented image captions. We obtained 92k logo region groundinglabels in total. We illustrate two sample annotations in ,the MDETR can localize rare language concepts expressed in thefree-form product title, like camera, single cylinder deadbolt, andround style knob. Finally, we train the CPG model on these twotasks simultaneously with two types of grounding labels together.Our self-supervised data augmentation is inspired by GLIPwhich applied the pre-trained GLIP model to obtain pseudo-labelsof raw web-crawled image-text pairs to finetune the same GLIPmodel. We first collect samples from an e-commerce site to scalegrounding data and especially to enrich e-commerce related se-mantic concepts which is different from general domain. We stepfurther to obtain pseudo-labels from two pre-trained models sothat as a student model, CPG model benefits from both generalknowledge transferred from MDETR and specialized brand knowl-edge transferred from the logo detection model. Because of crossattention between logos and product details, CPG model enablesproduct understanding in both the general context and the brandspecific context. Furthermore, CPG model enables comprehensivebrand understanding by unifying logo understanding and repre-sentative sample products understanding. Therefore, CPG modeloutperforms the logo detection teacher model when applying tothe product-brand matching task, which will be described later insection 4.",
  "PRODUCT-BRAND MATCHINGAPPLICATION": "We show the effectiveness of CPG representations pre-trained withpseudo labels by applying them to a downstream e-commerce appli-cation: product-brand matching. Brand is a key attribute impactingcustomers shopping decisions, which, however, is not trivial toinfer because of homonyms. Differentiating homonym brands ischallenging and can only be done if different logos and/or differentproducts sold by brands are provided. For example, we should mapaGerber knife to the right Gerber in instead of the othertwo, because we learn from the representative product that rightGerber sells gears while the middle one sells baby food and the leftone sells plumbing fixtures. For the middle Gerber, we can easilymap products correctly by identifying the logo. Therefore, in orderto map products to brands correctly, we have to comprehensivelyunderstand the brand entity consisting of multiple products andlogos (may not exist). The number of brand representative prod-ucts varies per brand because of remarkable brand scope difference.Some brands, like 3M, sell products from dozens of different cate-gories while others focus on one specific category. We formulate the product to brand matching problem as an asymmetric entity match-ing problem, which makes binary predictions regarding whether aproduct belongs to a brand. The existing state-of-the-art product-brand matching system ensembles a wide variety of features basedon product text data (details in 5.2.1). It fails to capture image fea-tures and hence is incapable of distinguishing homonym brands bylogos or other fine-grained regions in product images.We use the semantics rich CPG representations extracted fromthe model with high confidence (> 0.5) as additional ML featuresto the existing system. We can not simply concatenate CPG rep-resentations and feed to downstream task because the number ofrepresentative products per brand has a long tailed distribution.The number of object representations with high confidence alsovaries from one image to another. As a result, concatenating themand padding to the longest makes the feature vector extraordi-nary long, which increases the matching system complexity andtherefore breaks the latency requirement for real-time use cases.Another way is to truncate CPG representations to a fix number.However, it will prevent model from making informative predic-tions. For example, if Apple has only one representative product,e.g. a computer, model can not tell if Apple phone case should bemapped to it. Therefore, in order to utilize all information but stillhave a light enough system to satisfy latency requirement, we firstcompute distance-based similarity scores, i.e. Euclidean distanceand Cosine distance, between CPG representations of the inputproduct and CPG representations of each representative product.Then the summary statistics, i.e. minimum, maximum, medium andvariance, of similarity scores are fed to ensemble system. In addi-tion, we feed two boolean values to indicate whether the number oflearned CPG representations is 0 for the product and for the brand,respectively. We refer these summary statistics derived from CPGrepresentations as CPG features.",
  "EXPERIMENTATION AND RESULTS": "We first train the CPG model using augmented product image textpairs with pseudo labels. Then we evaluate the textual-attribute-semantic rich object representations learned by CPG by supplement-ing them to existing product-brand matching model. We report therelative gain of recall at 95% precision in 9 countries as we need tomeet a high precision threshold for deployment to ensure customershopping experience. We further compare the visual representa-tions learned by CPG with representations learned from two typesof vision model: the logo detection model and image-level under-standing model, i.e. ResNet50.",
  "Product-Brand Matching Dataset": "All samples for product-brand matching task are in (product, brand)pair format. Input product is a structured entities with a fixed set ofattributes, e.g. title, brand and image. The textual attributes could beinaccurate or even missing. Input brand is also a structured entitieswith varying number of representative products that share the samedata structure as the input product. We collected 50, 000 (product,brand) training pairs per country from 6 countries (we will referthem as A-F). The textual attributes in these countries are in Englishand multiple Romance languages, e.g. French, Spanish and etc. The",
  "Baseline Models": "5.2.1Existing ensemble model with Text Features. The existingproduct-brand matching model ensembles in a total of 139 featurescontaining manually crafted syntactic similarity features and MLfeatures learned from base models, e.g. brand extraction model, tex-tual attributes understanding model and etc. The brand extractionmodel tackles the challenge of missing brand attribute when brandstrings are mentioned in other textual attributes. The textual at-tributes understanding model captures semantic similarity betweenthe product and the brand despite the brand string variation, e.g.similarity between James Bond 007 Fragrances and 007 Frangrances.This existing ensemble model doesnt contain any signal from prod-uct image, which serves as the first baseline to the ensemble modelwith features derived from object representations learned fromCPG. The existing model is a country-aware model trained with alltraining samples collected from 6 countries. The performance ofthe ensemble model is evaluated in each of 9 countries. 5.2.2Ensemble model with logo features. One natural way to dis-tinguish homonym brands is to compare logos. We apply the YOLO-based logo detection teacher model to both input product imageand brand representative product images and use detected logosas features. Because the number of representative products perbrand varies, directly using detected logos leads to various inputlength. Furthermore, the detected logo region similarity could beimpacted by the original product image quality, size, angle andetc. To address these challenges, we first leverage encoded vectorfrom the last hidden layer of the logo detection model to replaceraw detected logo regions as input features. Then, to address var-ious input length challenge and have fair comparison with CPGrepresentations, we supplement the existing ensemble model withthe same set of summary statistics based on similarity betweendetected logos, as described in 4. We refer this set of features aslogo features. 5.2.3Ensemble model with ResNet50 features. Another way toleverage image information is through image-level understand-ing. We fine-tuned a ResNet50 model using product images syn-thesized from English e-commerce catalogs. The data collectionprocess is independent of CPG self-supervised training data col-lection process. The fine-tune task is to detect whether images are from duplicate products. The training samples contains similar yetdifferent products whose images differ in details as well as dupli-cate products whose images may be taken from different angles,lights and etc. Therefore, the fine-tuned ResNet50 model learnse-commerce specialized image patterns and learns to pay attentionto both local regions as well as the whole image. We use vectorsfrom last hidden layer as an image-level representations for productimage. The same set of summary statistics of Euclidean and Cosinedistances between image representations of the input product andof the brand representative products are supplemented to existingensemble model for re-training and evaluation. We refer this set offeatures as ResNet50 features",
  "Results": "We note that all results reported in this paper are in absolute terms.We evaluate model performance using recall at high precision be-cause we need to prevent incorrect brand mappings from impairingcustomers shopping experience. Therefore, we report relative gainsin terms of recall at precision 90% and recall at precision 95% anddenote lift as @90 and @95 in tables. 5.3.1Comparison with Existing Ensemble model with text features. shows the performance lift of the ensemble model with CPGfeatures over the existing ensemble model with text features onlyas described in 5.2.1 across 9 countries. From the table, we see thatleveraging similarity between object representations learned fromCPG model leads to significant performance jump on top of exist-ing ensemble model in all countries globally except one countryC. In country C, the model with CPG features performs compara-bly with the baseline model because textual attributes quality ishigh in country C where we observe less inaccurate or missingattributes. Therefore, the model with text features already containenough product information to distinguish homonym brands byunderstanding the textual attributes. As expected, we observe thelargest performance lift in country I with largest homonym pro-portion. This confirmed the conjecture that semantic rich objectrepresentations provide signals to differentiate homonym brands. 5.3.2Comparison with Ensemble model with logo features. shows the performance lift of the ensemble model with CPG fea-tures over the ensemble model with logo features as described in5.2.2 across 9 countries. We can see that, leveraging object repre-sentations learned from CPG model performs better than simplyusing detected logos from the teacher model in all countries exceptC and D where these two models perform comparably. Despite theperformance lift in country I in table 2 is lower than in table 1, westill observe positive performance lift. This indicates that leveraginglogo features can only mitigate the homonym challenge partially. Itdoesnt solve all problems because not all product images containthe logo. In fact, only 40% of product images have logo detectedwith high confidence (>0.5). For cases where the input productimage doesnt contain logos, the object representations learnedfrom CPG can provide fine-grained product image understandingto guide the correct brand mapping.",
  ": Performance gains of leveraging CPG features over leveraging features derived from the ResNet50 model fine-tunedon catalog from English countries for image matching in 9 countries": "features learned from the ResNet50 model as described in 5.2.3across 9 countries. We see that CPG features lead to higher perfor-mance gains in all countries except country C and G. In country G,both models perform comparably at precision 90% and our modelperforms better at precision 95%. All ensemble models performsimilarly in country C. The lift of leveraging CPG features on topof leveraging ResNet50 features are mainly from two sources: morefine-grained image understanding and logo specialized knowledge.In country I, where logo features lead to performance lift, we seethat CPG features outperform ResNet50 features significantly. Thisdemonstrates that CPG model effectively transfers knowledge fromthe logo detection teacher model while ResNet50 features are lackof logo specialized knowledge.",
  "CONCLUSION": "In this paper, we presented a catalog phrase grounding (CPG) modelwhich learns fine-grained visual representations of product imageconditional on structured product textual attributes. We investi-gated how to train the model with catalog-scale raw product at-tributes in a self-supervised fashion by transferring knowledgefrom both the general domain phrase grounding teacher modeland the catalog specific logo detection teacher model. Therefore,the learned representations of logos and isolated product detailsfrom the same latent space provide an integrated understandingof brands and products. The effectiveness of learned semantic richrepresentations are demonstrated by performance gains on a crucialand challenging e-commerce application: product-brand matching.We further show that integrated understanding of products andbrands makes CPG competitive with the logo detection teachermodel. Its worth future study to apply CPG representations toother e-commerce applications. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,C Lawrence Zitnick, and Devi Parikh. 2015. Vqa: Visual question answering. InProceedings of the IEEE international conference on computer vision. 24252433.",
  "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residuallearning for image recognition. In Proceedings of the IEEE conference on computervision and pattern recognition. 770778": "Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 2021. Scaling up visual and vision-language representation learning with noisy text supervision. In InternationalConference on Machine Learning. PMLR, 49044916. Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra,and Nicolas Carion. 2021. MDETR-modulated detection for end-to-end multi-modal understanding. In Proceedings of the IEEE/CVF International Conference onComputer Vision. 17801790. Gen Li, Nan Duan, Yuejian Fang, Ming Gong, and Daxin Jiang. 2020. Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training. InProceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 1133611344. Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, ChunyuanLi, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. 2021.Grounded Language-Image Pre-training. arXiv preprint arXiv:2112.03857 (2021). Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, DevaRamanan, Piotr Dollr, and C Lawrence Zitnick. 2014. Microsoft coco: Commonobjects in context. In European conference on computer vision. Springer, 740755. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, OmerLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: Arobustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692(2019)."
}