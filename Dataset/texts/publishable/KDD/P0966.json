{
  "ABSTRACT": "Time series forecasting has been an essential field in many differentapplication areas, including economic analysis, meteorology, andso forth. The majority of time series forecasting models are trainedusing the mean squared error (MSE). However, this training basedon MSE causes a limitation known as prediction delay. The predic-tion delay, which implies the ground-truth precedes the prediction,can cause serious problems in a variety of fields, e.g., finance andweather forecasting as a matter of fact, predictions succeedingground-truth observations are not practically meaningful althoughtheir MSEs can be low. This paper proposes a new perspective on tra-ditional time series forecasting tasks and introduces a new solutionto mitigate the prediction delay. We introduce a continuous-timegated recurrent unit (GRU) based on the neural ordinary differentialequation (NODE) which can supervise explicit time-derivatives. Wegeneralize the GRU architecture in a continuous-time manner andminimize the prediction delay through our time-derivative regular-ization. Our method outperforms in metrics such as MSE, DynamicTime Warping (DTW) and Time Distortion Index (TDI). In addition,we demonstrate the low prediction delay of our method in a varietyof datasets.",
  "Time-series forecasting, Prediction delay, Neural ODE": "ACM Reference Format:Sheo Yon Jhin, Seojin Kim, and Noseong Park. 2024. Addressing Predic-tion Delays in Time Series Forecasting: A Continuous GRU Approachwith Derivative Regularization. In Proceedings of the 30th ACM SIGKDDConference on Knowledge Discovery and Data Mining (KDD 24), August2529, 2024, Barcelona, Spain. ACM, New York, NY, USA, 15 pages.",
  "Time series forecasting is important in diverse domains, such asweather prediction, stock prediction, and so forth, and has several": "Permission to make digital or hard copies of part or all of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for third-party components of this work must be honored.For all other uses, contact the owner/author(s).KDD 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s).ACM ISBN 979-8-4007-0490-1/24/08",
  ": Visualization of (experimental results forGOOG stock prediction from August 21 to August 28, 2023)": "challenges . The necessity to tackle these practical chal-lenges has spurred numerous proposed studies investigating theintricacies of short- and long-term time series forecasting. Withinthis realm, a spectrum of models has been suggested, ranging fromsimple linear networks to advanced transformer-based architec-tures . Traditionally, the predominant evaluation metricsin most studies have been Mean Squared Error (MSE) or MeanAbsolute Error (MAE), with ongoing research endeavors strivingto showcase state-of-the-art outcomes through learning based onthese metrics. However, the remarkable success achieved in timeseries forecasting using MSE highlights a limitation related to theprediction delay, as illustrated in .(a). In this context, wedefine prediction delay as a phenomenon where the actual obser-vations precede the prediction in the time series forecasting task in other words, a model is trained to output an observation similar",
  "CONTIME4.7122.1890.189": "visually presents the experimental results detailed in. Notably, a prediction delay is discernible in state-of-the-art(SOTA) models, exemplified by PatchTST and DLinear , de-spite their relatively small Mean Squared Error (MSE). Conversely,CONTIME, characterized by a comparatively similar MSE, doesnot exhibit a prediction delay. This study seeks to provide a com-prehensive interpretation of time series forecasting by introducingadditional metrics, namely Temporal Distortion Index (TDI) andDynamic Time Warping (DTW), aimed at elucidating the observedphenomenon. Furthermore, underscores the significanceof prediction delay in a straightforward scenario. Investors relyingon SOTA model forecasts for GOOG stocks anticipate the upperprice limit on August 25th, 2023. However, due to a one-day delayin the forecast results, this leads to stock sales (See .(a)).In contrast, investors relying on CONTIME, free from predictiondelay, predict a stock price decline on August 25th, 2023, promptingthem to initiate stock sales on August 24th, 2023 (See .(b)).Assuming an investor trades 100 shares of stock, those relying onCONTIME stand to make a profit of approximately $6,000. Thisachieves more accurate and timely forecasts in real-world appli-cations and provides beneficial forecast results to investors. Theexample in highlights the importance of mitigating theprediction delay in time series forecasting.Beyond the financial domain, the aforementioned predictiondelay assumes a significant role in areas intricately interwovenwith daily life, such as weather forecasting. Despite discussions onthese limitations dating back to 1998 , recent state-of-the-art studies have predominantly concentrated on evaluatingthe performance of metrics like MSE, MAE, etc., in the contextof time series forecasting. As evident from , it is imperativethat a models prediction accurately captures both the shape andtemporal trends within the time series. Dynamic Time Warping(DTW) emerges as a method capable of discerning differences inshape between time series. Additionally, Temporal Distortion Index(TDI) serves as an extra metric to explore the temporal lag betweentwo sequences. The incorporation of these two metrics enables acomprehensive assessment of comparability between the respectivetime series. Consequently, our intention is to subject the model toevaluation using these novel metrics.This paper introduces an innovative approach to mitigate theprediction delay in time series forecasting. In this paper, we rede-fine GRU as differential equation that reflect past observations tothe current hidden state for processing continuously generalizingGRU. We propose a continuous-time bi-directional gated recurrentunit (GRU) network based on neural ordinary differential equation (NODE) and train it with explicit time-derivative regularizations,thus addressing the inherent prediction delay observed in varioustime series forecasting models. We extend the bi-directional GRU toefficiently capture the temporal dependencies within time-series se-quences with minimal delays. Our contributions can be summarizedas follows: (1) We propose CONtinuous GRU to address the predictiondelay in TIME series forecasting, i.e., CONTIME. By con-tinuously extending the bi-directional GRU, we present anovel architecture that facilitates the supervision of the time-derivative of observations in the continuous time domain. (2) In .2, we compute the time-derivatives of the hiddenstate h(), the reset gate r(), the update gate z(), and theupdate vector g() of GRU. We strategically employ the bi-directional GRU structure to generate more effective hiddenrepresentations for downstream task.",
  "where h() R, , represents a -dimensional vec-tor (with boldface denoting vectors and matrices). The derivativeh() def= h( )": "is approximated by the neural network (h(),; ),and solving the initial value problem yields the final value h() fromthe initial value h(0). The ODE-based neural network learns byestimating the differential values of the data function (h(),; )using ODE solvers such as the explicit Euler method, the 4th orderRunge-Kutta (RK4) method, the Dormand-Prince (DOPRI) tech-nique, and similar approaches .There also exist prominent time-series processing models basedon NODE, such as Neural Controlled Differential Equation (NCDE).NCDE, an advanced network of NODE, utilizes the RiemannStieltjesintegral, as shown in Equation (2). Unlike NODE, which employsthe Riemann integral, NCDE can continuously read () valuesover time. Thus, NCDE can overcome the limitations of NODE that",
  "(2)": "Moreover, NCDE creates a continuous path () by employinginterpolation techniques like the natural cubic spline or Hermitecubic spline. Since the natural cubic spline uses all time observationsto form a continuous path (), so in the context of time-seriesforecasting, the natural cubic spline method may not be suitable forforecasting tasks. Therefore, in this paper, we opt for the Hermitecubic spline method to generate the continuous path () . Transformer-based models: Subsequent advancements have in-troduced transformer architectures, originally devised for naturallanguage processing, to the domain of time series forecasting,thereby incorporating self-attention mechanisms. These models,utilizing self-attention, have demonstrated remarkable efficacy incapturing overarching dependencies within sequential data, leadingto the development of significant transformer-based studies suchas Autoformer and FEDformer . While Autoformer employsauto-correlation attention for periodic patterns, it falls short inseries decomposition, overly depending on a basic moving averagefor detrending, which may constrain its ability to capture intricatetrend patterns. On the other hand, FEDformer integrates theTransformer with seasonal trend decomposition, utilizing decom-position for global profiles and Transformers for detailed structures.Despite these notable accomplishments, it is crucial to acknowledgethat transformer-based architectures exhibit inefficiencies in captur-ing local dependencies and temporal information. This constrainthas spurred continuous research endeavors aimed at addressingand improving the effectiveness of transformer-based models incomprehensively capturing both global and local intricacies withintime series data. Recent state-of-the-art models: The innovative introduction ofPatchTST represents a groundbreaking approach that employspatch-based representations to enhance the capture of both localand global patterns within time series data. Building upon this,PatchTST further enhances its methodology by segmenting timeseries before utilizing a Transformer, demonstrating superior per-formance compared to existing models. Despite being rooted in the foundational Transformer architecture, innovations are focusedon transitioning from self-attention to sparse self-attention, oftenoverlooking a comprehensive global view of time series data. DLin-ear has significantly contributed to the field by exploring linearmodels for time series forecasting. In defiance of the prevalent as-sumption that only highly complex nonlinear models excel in thiscontext, DLinear has exhibited competitive performance with alinear layer, emphasizing efficiency and interoperability.In summary, the progression from Neural ODE to PatchTST andDLinear signifies an ongoing quest for more effective and efficientdeep learning models in the domain of time series forecasting. Eachmodel brings unique features, methodologies, and challenges thatchallenge prevailing assumptions, with a notable emphasis on anovel approach for model evaluation based on the MSE.",
  "Evaluation and training metrics": "In the realm of evaluating and training deep models for time seriesforecasting, prevalent approaches heavily depend on metrics suchas MAE, MSE, and their variants, including SMAPE. While thesemetrics effectively gauge overall model performance, evaluatingshape and temporal location is crucial for a more comprehensive as-sessment. Techniques like Dynamic Time Warping (DTW) areemployed to capture shape-related metrics, and Temporal Distor-tion Index (TDI) is utilized for prediction delay estimation.However, due to their non-differentiability, these evaluation metricsare unsuitable as loss functions for training deep neural networks.Addressing the challenge of optimizing non-differentiable eval-uation metrics directly, the development of surrogate losses hasbeen explored across various domains, including computer vision.Recently, alternatives to MSE have been investigated, with a fo-cus on seamless approximations of DTW to train deep neuralnetworks. Despite its effectiveness in assessing shape errors, theinherent design of DTW, i.e., the invariance to elastic distortions,overlooks crucial considerations about the temporal localization ofchanges. Le Guen and Thome attempted to train models witha loss that combines DTW and TDI to account for both the shapeand temporal distortion. and provide examples where each metric (MSE,DTW, TDI) excels in analyzing experimental results (refer to ). By examining the relationship between metric values and visual-ization results, we gain insight into the role of each metric. In , the results of DLinear in .(a) demonstrate a relatively",
  "DLinear3.810X1.409X0.084OPatchTST3.166X1.253O0.091OCONTIME2.378O1.114O0.074O": "small MSE of 0.084, yet exhibit a lack of superiority in terms of pre-diction shape and timing. This observation is further supported bythe TDI and DTW metrics. It illustrates that a good MSE score doesnot necessarily guarantee accurate time series prediction. .(b) presents predictions with a more accurate shape than .(a), but entails a prediction delay in determining the direction ofmovement. Consequently, TDI and MSE values are large comparedto smaller DTW values. This indicates that time series forecasts can-not be evaluated solely using DTW and MSE. .(c) showcasesthe prediction results of CONTIME, demonstrating excellent perfor-mance in terms of time series shape and timing, naturally leadingto small MSE values. These analyses emphasize the necessity toevaluate time series forecasts from diverse perspectives.This paper advances this approach by directly computing thegradient of sequences. To enable the instantaneous prediction ofrises and declines, we incorporate a regularization component thatutilizes time-derivatives. This strategy addresses the gap by intro-ducing time-derivative regularization to the traditional MSE loss.By decoupling the training for prediction delay and the MSE cri-terion, this paper aims to provide a robust framework for trainingdeep neural networks on real-time series data.",
  "The prediction delay in time seriesforecasting": "Time series forecasting is a crucial task spanning diverse domains,including finance and environmental science. A significant chal-lenge in this domain is prediction delay, where models may struggleto provide accurate and timely predictions. This subsection exploresexisting research addressing prediction delay in time series forecast-ing, emphasizing neural network approaches and other relevantmethodologies. In , challenges in time series forecasting usingneural networks are investigated, and strategies to mitigate fore-cast delays are proposed. The study assesses the impact of delay onprediction accuracy and explores techniques to enhance predictiontimeliness using neural network architectures. In addition, threestudies in focus on applying artificial neural networksto rainfall-runoff modeling, economic models, traffic forecasting,and so on and investigating constraints related to forecast delays.One of the studies evaluates the trade-off between hydrologicalstate representation and model evaluation, emphasizing the chal-lenges posed by delays in hydrological forecasting. Beyond theapplications like rainfall runoff or wave height predictions, thedelay phenomenon is also observed in the economic field.",
  "Causes of the prediction delay: We categorize two common causesof prediction delay in time series prediction models:": "(1) Time series data often exhibit temporal dependence, wherecurrent values are influenced by past observations. The pre-diction delay can occur if the model fails to accurately cap-ture these dependencies or experiences a delay in incorpo-rating relevant historical information. (2) The prediction delay may arise from MSE-based forecastingmodels limited ability to adjust to sudden changes in thetime series because their primary objective is to minimizethe mean square difference between predicted and actualvalues .In this paper, we propose CONTIME, a model architecture forsupervising time-derivatives to eliminate prediction delays. We adda time-derivative regularization to the main task-dependent lossfunction to effectively handle prediction delays.",
  "(h2(),;2),(3)": "where denotes initial point in time-series sample = (, ..., ) R( ) , where means the number of features. h1() = h1( ),h2() = h2( ) and h1, h2 is a fully-connected layer-basedfeature extractor. In Equation (3), we use bi-directional integral op-erations in the forward ( ) and backward ( ) directionsto generate a more useful hidden representation in long sequences.After reverse h2() to h2( ), we can write our final hiddenrepresentation h() as follows:",
  "(5)": "where W Rdim(h)dim(x) and U Rdim(h)dim(h) are weightmatrices, and b Rdim(h) is a bias vector. is a sigmoid functionand is a hyperbolic tangent function. > 0 is a delay factor note that = 1 in the original design of GRUs whereas we interpretit in a continuous manner. Since the hidden state h() is a compositefunction of r(), z(), and g(), the derivative of h() can be writtenas follows:",
  "W (h()) + b ) and h( )": "is defined by the ODE function 1. So,Equation (12) can be easily calculated by the automatic differen-tiation method. Therefore, we use the MSE loss between and to supervise the time-derivative, where := 1 inother words, we use the difference to supervise the derivative, which is reasonable since we do not know the explicit time-derivative of . Our loss function can be summarized as follows:",
  "where and are the coefficients of the two terms. Finally, we cansummarize our training algorithm in Algorithm 1": "Well-posedness: The well-posedness2 of NODE was already provedin [25, Theorem 1.3] under the mild condition of the Lipschitz con-tinuity. We show that our CONTIME is also well-posed. Almostall activations, such as ReLU, Leaky ReLU, Tanh, Sigmoid, ArcTan,and Softsign, have a Lipschitz constant of 1. Other common neuralnetwork layers, such as dropout, batch normalization, and otherpooling methods, have explicit Lipschitz constant values. Therefore,the Lipschitz continuity of h( )",
  "EXPERIMENTS": "In this section, we describe our experimental environments andresults. We conduct experiments on multivariate time series fore-casting. All experiments were conducted in the same software andhardware environments. Ubuntu 18.04 LTS, Python 3.8.0, Numpy1.22.3, Scipy 1.10.1, Matplotlib 3.6.2, PyTorch 2.0.1, CUDA 11.4,NVIDIA Driver 470.182.03 i9 CPU, and NVIDIA RTX A5000. Werepeat training and testing procedures with three different randomseeds and report their mean scores. We report standard deviationsof all 6 datasets in the arXiv version.",
  "We list all the descriptions of datasets and detailed experimentalsettings in Appendix, D, and I": "Baselines: We test the following state-of-the-art baselines to com-pare our proposed CONTIME with 6 baseline models. (1) DLin-ear is a simple linear network with time series decomposi-tion method and shows state-of-the-art performance. (2) NeuralODE (NODE ) is a continuous-time model that defines the hid-den state h() with an initial value problem (IVP). (3) Neural CDE(NCDE ) is a conceptually enhanced model of NODE based onthe theory of controlled differential equations. (4) Autoformer is a transformer-based method which uses an auto-correlation at-tention for periodic patterns. (5) FEDformer is a transformer-based method which integrates transformer with seasonal trenddecomposition, leveraging decomposition for global profiles andtransformers for detailed structures. (6) PatchTST is a timeseries forecasting technique that makes use of patch-based pro-cessing to improve the models capacity to grasp complex patternsand relationships by segmenting temporal sequences into smallerpatches. Datasets: We evaluate the performance of the proposed CON-TIME on six benchmarked datasets, including weather, exchange,and four Stock datasets (AAPL, AMZN, GOOG, MSFT). Amongthe benchmarked datasets used, weather and exchange are widelyutilized and are publicly available at . The Stock dataset hasbeen actively used in . The following is a description of the sixexperimental data sets. (1) The Stocks dataset containsstock prices of four companies (APPLE, AMAZON, Google, andMicrosoft). All four datasets measure 6 stock indicators (Open price,High price, Low price, Close price, Adj Close price, and Volume)of each company from January 17th, 2019 to January 4th, 2024.",
  "(2) Exchange contains exchange data among 8 countries . (3)Weather is data that measures 21 weather indicators, includingtemperature and humidity, every 10 minutes throughout 2020": "Evaluation metrics: As time-series forecasting is a complicatedtask, evaluating the prediction result only with MSE or MAE isinsufficient. Thus, we include DTW and TDI as additional metrics,which can be interpreted as follows, to analyze the time-seriesforecasting task from multiple perspectives: (1) DTW: We evaluate the difference of the overall shape be-tween and via DTW. In particular, the more volatilethe data is, the more emphasis is placed on using these met-rics. Small DTW values mean the overall shapes of and .However, one pitfall of DTW is that it ignores delays. (2) TDI: TDI quantifies the disparity between the optimal pathsof and . Further details on TDI can be found in Appendix F.Utilizing TDI, a metric for assessing temporal distortion, iscritical for precise predictions. Smaller TDI values indicateminimal prediction delays, aligning with the objectives ofthis paper.",
  "Experimental results": "In this subsection, we analyze the experimental results of six datasetsby dividing them into a total of three evaluation metrics (MSE, DTW,and TDI) . introduces our experimental results for time-series forecasting with 6 datasets from various fields. We also reportour time complexity and model usage in Appendix H. Stocks: The past five years of stock data for AAPL, AMZN, GOOG,and MSFT exhibit both sharp rises and sharp falls, rendering themsuitable for assessing accurate time series predictions across var-ious aspects. PatchTST demonstrates specialization in MSE andsurpasses other models based on differential equations and trans-formers. Autoformer exhibits reasonable DTW scores. Among thedifferential equation-based models, including CONTIME, the lowestTDI scores are observed. Notably, most baseline models special-ize in a single metric, such as MSE or DTW, whereas CONTIMEoutperforms across all metrics with the lowest standard deviation. Exchange: presents the experimental findings for the Ex-change dataset. Most models exhibit small MSE values; conversely,the NODE and NCDE models demonstrate superior TDI perfor-mance compared to others, indicating the efficacy of models basedon differential equations in addressing prediction delays. Signifi-cantly, our suggested model, CONTIME, performs second-best withan MSE difference of only about 0.005 when compared to DLinear.In addition, CONTIME performs fairly well in TDI, indicating itscapacity to efficiently reduce prediction delays. Weather: In , our proposed model demonstrates supe-riority over all other models across all metrics. While DLinearand PatchTST exhibit reasonable performance in MSE and DTW,CONTIME consistently outperforms them. Specifically, CONTIMEshows an average decrease of 0.168 in TDI compared to the second-best model across all prediction lengths. Furthermore, NODE andNCDE exhibit remarkable performance, reaffirming the efficiency",
  "Visualization": "provides a visualization of AAPL, AMZN, Exchange, andWeather forecasting results that prove CONTIMEs outstanding per-formance over various aspects compared to state-of-the-art (SOTA)models, such as PatchTST and DLinear. For instance, focusing onthe highlighted section in yellow in .(a), the SOTA models(blue and green line) predict the opposite of the stock price fluctua-tion due to delays in the prediction. Specifically, unlike the groundtruth, which starts to decline around August, 28th, 2023, state-of-the-art (SOTA) models fail to promptly recognize this change dueto a delay. In contrast, CONTIME (red line) accurately capturesthe actual stock price (black line) in terms of shape, and timing..(b) illustrates the visualization results for the AMZN stock.Across the entire time, CONTIME closely matches the shape ofthe ground truth and makes predictions without any noticeabledelays. Similarly, in .(c), while the SOTA models exhibitsimilarities in terms of shape, their results are delayed; comparedto the ground-truth with a high OT value on January 6, 2010, SOTAmodels predict a high OT value on January 11 due to a delay whileCONTIME predicts on time. In (d), most models exhibita shape similar to the ground-truth. Notably, in the highlightedsections of our model (indicated in yellow), the fluctuations of T(degC) are predicted in detail. Conversely, the baseline models madepredictions with a slight delay.",
  "Sensitivity analysis & ablation study": "4.4.1Ablation study on loss function. summarizes the re-sults of the ablation study for the loss functions applied to CON-TIME. Three types of loss functions are utilized. is a plainMSE loss function to compare and . uses the TDI-basedregularization proposed in to address the prediction delayproblem. This regularization minimizes TDI using Equation (28)of Appendix F. The loss function aims to reduce the computedtime-derivative explicitly. Through this ablation study, we evaluatethe effectiveness of the time-derivative regularization process inalleviating prediction delays.CONTIME (Only ), which trains with the task loss, exhibitsreasonable performance in terms of MSE. However, it becomesapparent that it does not effectively learn in other aspects such asTDI and DTW. On the other hand, and , employing differ-ent types of regularization respectively, demonstrate exceptionalperformance in terms of TDI. Though CONTIME ( + )performs well on TDI, it exhibits unstable performance in terms ofMSE or DTW. However, CONTIME ( + ) excels the othersin all three evaluation metrics, proving efficacy of our loss. Relationship between and . The experimental results in of the paper show that CONTIME () has superior TDIvalues compared to CONTIME (). The TDI loss, calculatedfrom the optimal DTW path between and , exhibits goodperformance in TDI, which is slghtly inferior to our methodology.Unlike the TDI loss, solely focusing on aligning the timing of theDTW path, our methodology improves performance on both DTW",
  "(shape) and TDI (timing) through the explicit gradient modeling ateach time": "4.4.2Sensitivity to , . In , we discern the impact of our loss on TDI, DTW, and MSE with the sensitivity curve w.r.t (varying from 0.1 to 0.001), compared to the top 3 baselines in eachmetric. Across all settings, CONTIME consistently outperformsthe baselines in terms of DTW and TDI, demonstrating the effi-cacy of our model. Regarding MSE, CONTIME exhibits reasonable performance across all settings. These results indicate stable perfor-mance of our model trained with loss, thereby leading to moreeffective elimination of prediction delays and it also shows stableperformance in DTW and MSE. 4.4.3Additional experiments on other 3 datasets. To evaluate theperformance of our model in different domains, we evaluate themodel on ILL (National Disease) and ETTh1 and ETTh2. Comparedto PatchTST and DLinear, we show slightly better performance in",
  "CONCLUSIONS": "This paper suggests yet another view on the time series forecastingresearch in other perspectives. To mitigate the prediction latencyin time series forecasting, we suggest CONTIME, a unique architec-ture that enables the explicit supervision of the time-derivative ofobservations in the continuous time domain by continuously gen-eralizing the bi-directional GRU. With this distinctive architecture,we effectively addressed the prediction delay problem, which haslong been an obstacle of time series forecasting. By applying thecontinuous bi-directional GRU and loss to naturally supervisethe time-derivative, CONTIME alleviates the prediction delay prob-lem. We quantify these phenomena by measuring not just MSE butalso TDI and DTW as evaluation metrics. As a result, we exhibitsuperior overall performance when compared to 6 state-of-the-artbaselines for 6 datasets from various fields.",
  "ACKNOWLEDGEMENTS": "This work was partly supported by the Korea Advanced Instituteof Science and Technology (KAIST) grant funded by the Korea gov-ernment (MSIT) (No. G04240001, Physics-inspired Deep Learning,10%), and Institute for Information & Communications TechnologyPlanning & Evaluation (IITP) grants funded by the Korea govern-ment (MSIT) (No. RS-2020-II201361, Artificial Intelligence GraduateSchool Program (Yonsei University),5%), and (No.RS-2022-II220113,Developing a Sustainable Collaborative Multi-modal Lifelong Learn-ing Framework,80%) and Institute for Information & Communi-cations Technology Promotion (IITP) grant funded by the Koreagovernment (MSIT) (No.RS-2019-II190075 Artificial IntelligenceGraduate School Program(KAIST), 5%)",
  "Pradnya Dixit, Shreenivas Londhe, and Yogesh Dandawate. 2015. Removing pre-diction lag in wave height forecasting using Neuro-Wavelet modeling technique.Ocean Engineering 93 (2015), 7483": "Laura Fras-Paredes, Fermn Mallor, Martn Gastn-Romeo, and Teresa Len.2017. Assessing energy forecasting inaccuracy by simultaneously consideringtemporal and absolute errors. Energy Conversion and Management 142 (2017),533546. Yang Han, Ying Tian, Liangliang Yu, and Yuning Gao. 2023. Economic systemforecasting based on temporal fusion transformers: Multi-dimensional evaluationand cross-model comparative analysis. Neurocomputing 552 (2023), 126500.",
  "Alphabet Inc. 2024. Google Stock": "Sheo Yon Jhin, Jaehoon Lee, Minju Jo, Seungji Kook, Jinsung Jeon, JihyeonHyeong, Jayoung Kim, and Noseong Park. 2022.Exit: Extrapolation andinterpolation-based neural controlled differential equations for time-series classi-fication and forecasting. In Proceedings of the ACM Web Conference 2022. 31023112. Sheo Yon Jhin, Heejoo Shin, Sujie Kim, Seoyoung Hong, Minju Jo, Solhee Park,Noseong Park, Seungbeom Lee, Hwiyoung Maeng, and Seungmin Jeon. 2023.Attentive neural controlled differential equations for time-series classificationand forecasting. Knowledge and Information Systems (2023), 131. Patrick Kidger, James Morrill, James Foster, and Terry Lyons. 2020. Neuralcontrolled differential equations for irregular time series. Advances in NeuralInformation Processing Systems 33 (2020), 66966707. Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, andJaegul Choo. 2021. Reversible instance normalization for accurate time-seriesforecasting against distribution shift. In International Conference on LearningRepresentations.",
  "Bryan Lim and Stefan Zohren. 2021. Time-series forecasting with deep learning:a survey. Philosophical Transactions of the Royal Society A 379, 2194 (2021),20200209": "Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, andSchahram Dustdar. 2021. Pyraformer: Low-complexity pyramidal attention forlong-range time series modeling and forecasting. In International conference onlearning representations. Qingqing Long, Zheng Fang, Chen Fang, Chong Chen, Pengfei Wang,and Yuanchun Zhou. 2024.Unveiling Delay Effects in Traffic Fore-casting: A Perspective from Spatial-Temporal Delay Differential Equations.arXiv:2402.01231 [cs.LG]",
  "H. Sakoe and S. Chiba. 1978. Dynamic programming algorithm optimizationfor spoken word recognition. IEEE Transactions on Acoustics, Speech, and SignalProcessing 26, 1 (1978), 4349": "Afan Galih Salman, Bayu Kanigoro, and Yaya Heryadi. 2015. Weather forecastingusing deep learning techniques. In 2015 international conference on advancedcomputer science and information systems (ICACSIS). Ieee, 281285. Loc Vallance, Bruno Charbonnier, Nicolas Paul, Stphanie Dubost, and PhilippeBlanc. 2017. Towards a standardized procedure to assess solar forecast accuracy:A new ramp and time alignment metric. Solar Energy 150 (2017), 408422.",
  "Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. 2023. Are transformerseffective for time series forecasting?. In Proceedings of the AAAI conference onartificial intelligence, Vol. 37. 1112111128": "Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,and Wancai Zhang. 2021. Informer: Beyond efficient transformer for long se-quence time-series forecasting. In Proceedings of the AAAI conference on artificialintelligence, Vol. 35. Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin.2022. Fedformer: Frequency enhanced decomposed transformer for long-termseries forecasting. In International Conference on Machine Learning. PMLR, 2726827286.",
  "In Section. 3.2, we calculated ( )": "by using Cubic Hermite splinemethod. In this section, we describe why we choose the CubicHermite spline method not the Natural cubic spline which createsthe continuous path (). There are two interpolation methodsthat create continuous path (), Natural cubic splines and CubicHermite splines. Natural cubic splines: Natural cubic splines used in Neural CDE require the entire time series to be used as a control signal. That is,a change in future time step may interfere past time steps, therebymaking interpolated result unreliable. In other words, it is an inter-polation method that cannot be used in online prediction. Cubic Hermite splines: This approach mitigates the discontinuityof linear control while maintaining the same online properties byjoining adjacent viewpoints with cubic splines that use additionaldegrees of freedom to smooth out gradient discontinuities. Thisresults in faster integration times than linear control. The maindifference from natural cubic splines is that Cubic Hermite splinessolve a single equation for each [, + 1) piece independently. Asa result, it changes more quickly than the natural cubic splineand therefore has a slower integration time than the natural cubicspline .Due to the above two differences, we believe that the Cubic Her-mite spline is more suitable for real-world time series forecasting,so we use this method to create a continuous path ().",
  "FHOW TO CALCULATE TDI": "We adopted TDI loss calculation for time-series sequence from .The calculation below is applied to each feature of data definedin Section. 3.2.We define A as a binary warping path of prediction length, i.e. A {0, 1}, with , = 1 if is associated with and otherwise 0, where , are a time point of each sequence.( ,) :=( )2,, which means measuring dissimilarityof two sequences by euclidean distance. We calculate TDI from theoptimal path matrix A of DTW as follows:",
  "Exchange:(1) DLinear : We train for 100 epochs with a learning rate in{0.01, 0.05, 0.001, 0.005, 0.0001}. Input sequence length in{60, 72, 96}": "(2) Differential-equation based models: For Neural ODE andNeural CDE, we train for 100 epochs with a learning rate in {0.01, 0.05, 0.001, 0.005, 0.0001}. Hidden size in {39, 49, 59}.Input sequence length in {60, 72, 96}. (3) Transformer-based models: For Autoformer and FEDformer,we train for 50 epochs with a learning rate in {0.01, 0.05,0.001, 0.005, 0.0001}. Input sequence length in {60, 72, 96}.Other settings follow the same experimental settings in thebaseline. (4) PatchTST: We train for 50 epochs with a learning rate in {0.01, 0.05, 0.001, 0.005, 0.0001}. Input sequence length in {60, 72, 96}. Other settings follow the same experimentalsettings in the baseline.",
  "in {0.01, 0.05, 0.001, 0.005, 0.0001}. Hidden size in {39, 49, 59}.Input sequence length in {60, 72, 96}": "(3) Transformer-based models: For Autoformer and FEDformer,we train for 50 epochs with a learning rate in {0.01, 0.05,0.001, 0.005, 0.0001}. Input sequence length in {60, 72, 96}.Other settings follow the same experimental settings in thebaseline. (4) PatchTST: We train for 50 epochs with a learning rate in {0.01, 0.05, 0.001, 0.005, 0.0001}. Input sequence length in {60, 72, 96}. Other settings follow the same experimentalsettings in the baseline.",
  "For reproducibility, we report the hyperparameters search range asfollows:": "Stocks: We train for 100 epochs with a batch size of 256. Alearning rate in {0.001, 0.005, 0.01, 0.05} are used. Coefficient of in {0.7, 0.8, 0.9, 1.0} and in {0.001, 0.005, 0.01, 0.05, 0.1}.We used rk4 as an ODE solver. Our input sequence length in{96, 104, 144}. Exchange: We train for 100 epochs with a batch size of 256. Alearning rate in {0.001, 0.005, 0.01, 0.05} are used. Coefficient of in {0.7, 0.8, 0.9, 1.0} and in {0.001, 0.005, 0.01, 0.05, 0.1}.We used rk4 as an ODE solver. Our input sequence length in{60, 72, 96}. Weather: We train for 150 epochs with a batch size of 256. Alearning rate in {0.001, 0.005, 0.01, 0.05} are used. Coefficient of in {0.7, 0.8, 0.9, 1.0} and in {0.001, 0.005, 0.01, 0.05, 0.1}.We used rk4 as an ODE solver. Our input sequence length in{60, 72, 96}.In , we showed our best hyperparameter for all 6 datasets."
}