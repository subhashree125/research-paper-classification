{
  "ABSTRACT": "In partial label learning (PLL), every sample is associated with a can-didate label set comprising the ground-truth label and several noisylabels. The conventional PLL assumes the noisy labels are randomlygenerated (instance-independent), while in practical scenarios, thenoisy labels are always instance-dependent and are highly relatedto the sample features, leading to the instance-dependent partiallabel learning (IDPLL) problem. Instance-dependent noisy label isa double-edged sword. On one side, it may promote model trainingas the noisy labels can depict the sample to some extent. On theother side, it brings high label ambiguity as the noisy labels arequite undistinguishable from the ground-truth label. To leveragethe nuances of IDPLL effectively, for the first time we create class-wise embeddings for each sample, which allow us to explore therelationship of instance-dependent noisy labels, i.e., the class-wiseembeddings in the candidate label set should have high similar-ity, while the class-wise embeddings between the candidate labelset and the non-candidate label set should have high dissimilarity.Moreover, to reduce the high label ambiguity, we introduce the con-cept of class prototypes containing global feature information todisambiguate the candidate label set. Extensive experimental com-parisons with twelve methods on six benchmark data sets, includingfour fine-grained data sets, demonstrate the effectiveness of theproposed method. The code implementation is publicly available at",
  "Corresponding Author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from , July 2017, Washington, DC, USA 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-x-xxxx-xxxx-x/YY/MM",
  "weakly supervised learning, partial label learning, instance-dependentpartial label learning": "ACM Reference Format:Fuchao Yang, Jianhong Cheng, Hui Liu, Yongqiang Dong, Yuheng Jia, and Jun-hui Hou. 2018. Mixed Blessing: Class-Wise Embedding guided Instance-Dependent Partial Label Learning. In Proceedings of ACM Conference (Con-ference17). ACM, New York, NY, USA, 10 pages. : Differences between the conventional PLL and ID-PLL, where the red label is the ground-truth label of theinstance. In PLL, the noisy labels in the candidate label setare randomly generated. However, in IDPLL, the noisy labelsin the candidate label set are instance-dependent, makingthem very similar to that of the ground-truth label, whichbrings more label ambiguity.",
  "Conference17, July 2017, Washington, DC, USAFuchao Yang, Jianhong Cheng, Hui Liu, Yongqiang Dong, Yuheng Jia, and Junhui Hou": ": Illustration of our method CEL. Our model consists of three modules: the backbone , the class-wise encoder , andthe classifier . For each sample, after processing through the backbone and class-wise encoder , each sample obtains itsclass-wise embeddings, i.e., each class corresponds to an embedding for that sample. The red line represents the process ofconstructing class prototypes based on the high-confidence class selected according to the models output. The class associativeloss (CAL) considers the relationships among each samples different class-wise embeddings. While the prototype discriminativeloss (PDL) considers the relationships between high-confidence class and class prototypes.",
  "(b) FGVC100": ": The classification accuracy curves of PLL methodPRODEN in IDPLL and PLL settings on two data setsCUB200 and FGVC100, where AVG. CLs represent the aver-age number of candidate labels of each sample. In the IDPLLsetting, the model has a faster learning speed in the earlystage of training because the instance-dependent noisy la-bels are related to the sample to some extent, which canprovide more supervision in the early stage of model train-ing. However, in the later stage of training, the performanceof PRODEN in the IDPLL setting is significantly inferior tothat in the PLL setting as instance-dependent noisy labelsbring more label ambiguity. different classes of each sample to guide the learning of class-wiseembeddings. In IDPLL, the class-wise embeddings within the can-didate label set should exhibit high similarity to each other as thecandidate labels can describe the instance to some extent. In con-trast, the class-wise embeddings between the candidate label setand the non-candidate label set should display stark differences.In this way, we can obtain class-wise embeddings that are moresuitable for IDPLL. Second, we propose a prototype discriminativeloss (PDL) by constructing class prototype for each class which con-taints global feature information to guide the label disambiguationprocess. To be specific, we select the most high-confidence labelfor each sample based on the model output, and then we ensurethe class-wise embedding of this particular high-confidence labelis aligned with its corresponding class prototype while being dis-tanced from other class prototypes, thereby enhancing the modelsdiscriminative ability. By employing the above two losses, we canobtain embeddings that are tailor-made for IDPLL and enhance themodels label disambiguation performance simultaneously, thusaddressing the mixed blessing issue in IDPLL. Extensive exper-iments on 6 benchmark data sets demonstrate the effectivenessof the proposed method when compared with 12 state-of-the-artmethods.The contributions of our work are summarized as follows: To the best of our knowledge, we are the first to introduceclass-wise embedding in IDPLL. Class-wise embedding enablethe model to explore the nuances relationships of classes ineach sample, thereby better addressing the mixed blessingproblem inherent in IDPLL.",
  "RELATED WORK2.1Partial Label Learning": "Conventional PLL methods can be roughly divided into two cat-egories based on the used information. The first category usesthe information in the feature space to guide label disambiguation. The second category leverages the informationof label space to achieve label disambiguation . Thesemethods often rely on linear models and are difficult to apply tolarge-scale data sets. Deep PLL has received wide attention in recentyears and it adopts deep neural networks to process large-scale datasets . PRODEN proposed to progressively identifythe ground-truth label during the self-training procedure. RC andCC proposed provably risk-consistent and classifier-consistentlabel disambiguation methods. LWS proposed a family of lever-aged weighted loss functions. PICO introduced the widely usedcontrastive loss to the PLL, which became the foundation fora large number of follow-up works. CR-DPLL employed con-sistency regularization to reduce the impact of noisy labels. PAPI constructed the similarity score between feature prototypesand sample embeddings, and improves the model performancesby aligning the similarity score with the model output. CROSEL used two models to cross-select trustworthy samples from thedata set for the training phase. The above methods have achievedsuperior results in the conventional PLL setting, where the noisylabels in the candidate label set are randomly generated. However,in practice, noisy labels are always instance-dependent .For example, in crowdsourced annotation tasks, the annotationsfrom many annotators constitute the candidate label set of eachsample, and the noisy labels within it are all instance-dependent.Consequently, the performances of conventional PLL methods areoften limited due to the lack of consideration of the characteristicsof IDPLL.",
  "Instance-Dependent Partial Label Learning": "IDPLL is a PLL framework that is closer to real-world scenarios.VALEN was the first work to introduce the concept of IDPLL,with a two-stage disambiguation process. The first stage involvesrecovering samples latent label distribution, and then trainingthe model using the recovered label distribution in the secondstage. ABLE proposed an ambiguity-induced positive selectioncontrastive learning framework for IDPLL to achieve label disam-biguation. NEPLL introduced a sample selection method basedon normalized entropy, intending to separate well-disambiguatedsamples and under-disambiguated samples. It also established adynamic candidate-aware thresholding for the further refinementof sample selection. POP presented a method that progres-sively purified the candidate label set and optimized the classifier. IDGP modeled the candidate label generation process for ID-PLL, simulating the ground-truth label and noisy labels generationprocesses with Categorical distribution and Bernoulli distributionrespectively. DIRK proposed a self-distillation-based label dis-ambiguation method, i.e., the training of the student model is di-rected by the output of the teacher model. It also developed a labelconfidence rectification method that meets the prior knowledgeof IDPLL. However, the aforementioned IDPLL methods did notconsider that the IDPLL setting is a mixed blessing, i.e., on thepositive side, noisy labels can describe the sample to some extentand help the model training, while on the negative side, identifyingthe ground-truth label within the candidate label set becomes morechallenging. They only focused on developing better disambigua-tion methods while ignoring the noisy label information, therebylimiting their performance.",
  "PROPOSED METHOD3.1Class-Wise Embedding": "Denote by X R the -dimensional feature space and Y ={1, 2, ...,} the corresponding label space with classes. Let D ={(, ), 1 } denote a partial label data set comprising samples, where is the candidate label set of sample and theground-truth label of sample is concealed in . The objective ofIDPLL is to induce a multi-class classifier that maps elements fromX to Y. Previous models in the realm of PLL and IDPLL typicallyconsist of two modules. The first module is the backbone respon-sible for deriving the feature map R for each sample,where , and are the height, width, and channel respectively.Afterward, is processed through average global pooling to obtainthe low dimensional embedding. The second module encompassesa linear layer that translates this low-dimensional embedding intothe final prediction.In this paper, we use a different paradigm that consists of threemodules. As shown in , the first module is as same as othermodels, i.e., a backbone , which extracts the feature map = () R of each sample through the deep neural net-work. The second module is a class-wise embedding encoder which produces class-wise embeddings = () R, where is the length of each class-wise embedding. Note, the indicates the representation of the -th class of sample . Thethird module is a group of linear layers that output the predictedprobabilities = () R.In conventional PLL and IDPLL representation methods, thefeatures of each sample are extracted as a single embedding, sothey can only consider relationships at the sample level. However,in IDPLL, the relationships between each samples candidate la-bels and non-candidate labels are valuable and worth leveraging.By employing class-wise encoder, we can represent each samplesembeddings on different labels. This allows us to explore the in-ternal relationships among different classes and fully utilize theprior knowledge of IDPLL, making it a more suitable representationmethod for IDPLL.",
  "Label Disambiguation Loss": "As aforementioned, the pivotal process in addressing PLL is labeldisambiguation, which mitigates the impact of noisy labels withinthe candidate label set. Here, we adopt a widely used deep PLLdisambiguation strategy , which constructs sample label confi-dences based on model outputs during training. Initially, the labelconfidence vector R of sample is initialized as =1| | ,if and = 0, otherwise, where | | returns the number ofcandidate labels of sample . Throughout the training, we updatethe label confidence according to the model output:",
  "Class Associative Loss based on Class-WiseEmbedding": "As previous analyzed, IDPLL is a double-edged sword. On the posi-tive side, the noisy labels in the candidate label set are very similarto the ground-truth label because they often share common featuresand they can depict the sample to some extent, which is also thefundamental reason leading to label ambiguity. Therefore, an impor-tant characteristic of IDPLL is that the labels within the candidatelabel set should be very similar, while the labels in the candidatelabel set should exhibit significant differences from the labels inthe non-candidate label set. To fully leverage this prior knowledge,we can measure the relationships among classes of each samplethrough class-wise embeddings. To be specific, the class-wise em-beddings corresponding to each label in the candidate label setshould be similar to each other, while the class-wise embeddingsbetween the candidate label set and the non-candidate label setshould display stark differences. Therefore, we can construct the",
  ",1,(4)": "where and represent the -th class-wise embedding and thecandidate label set of sample respectively. , returns the cosinesimilarity of two vectors. Note that the class-wise embeddings havebeen normalized before calculating the cosine similarity. in Eq.(3) denotes the average similarity of classes in the candidate labelset, while in Eq. (4) indicates the average similarity of classesbetween the candidate label set and the non-candidate label set.By considering the similarities of Eqs. (3) and (4) simultaneously,we can obtain the following class associative loss (CAL) function:",
  "+ 1,(5)": "where 1 is a trade-off parameter that balances two different terms.|| is the absolute value operator given (1, 1). The first termin Eq. (5) means that for each sample , the class-wise embeddingsin its candidate label set should be pulled as close as possible. In themeanwhile, the second term implies that the class-wise embeddingsbetween the candidate label set and the non-candidate label setshould be pushed as far away as possible, and in the ideal situation,their class-wise embeddings should be orthogonal, i.e. = 0. Byminimizing the loss function L, we can obtain a model that ismore suitable for the IDPLL setting.",
  "Prototype Discriminative Loss based onClass-Wise Embedding": "In IDPLL, we can distinguish the labels between the candidate labelset and the non-candidate label set easily, however, it becomesmore difficult to identify which label in the candidate label setis the ground-truth label, as the candidate labels are similar toeach other, bringing more label ambiguity. Therefore, to tacklethis negative side of IDPLL and identify the ground-truth label,we use the global information of samples to guide the modelsdisambiguation. Therefore, we first construct prototypes for eachclass,",
  "= ( + ), if = () and ,(6)": "where is the -th class prototypes and () denotesthe 2 normalization operator. To ensure the quality of the classprototypes, we only select the class with the highest model outputprobability in the candidate label set as the high-confidence labelof each sample and add the corresponding class-wise embeddinginto the class prototype.Similar to Eq. (3) and Eq. (4), we construct the similarity rela-tionships between class-wise embeddings and class prototypes asfollows:",
  "Return result": "where in Eq. (7) represents the similarity between the class-wise embedding of the class with the highest model predictionin the candidate label set and the corresponding class prototype.Meanwhile, in Eq. (8) denotes the average similarity betweenthis selected class-wise embedding and all other class prototypes.By combing Eq. (7) and Eq. (8), we have the following prototypediscriminative loss (PDL) function:",
  ",(9)": "where 2 is the trade-off parameter that balances the two differ-ent prototype level terms. By minimizing Eq. (9), we can guidethe model training through class prototypes, i.e., the most reliableclass predicted by the model should be as close as possible to thecorresponding class prototype, while this reliable class should befar away from other class prototypes, and in the most ideal case,their similarity relationship should be 0. By utilizing the globalinformation of class prototypes, we can effectively improve the dis-criminative performance of the model and select the ground-truthlabel from the candidate label set.",
  "Overall Objective": "Considering the low quality of class prototypes obtained in the earlystages of model training, it is difficult to ensure the effectiveness oflabel disambiguation. Therefore, we divide the model training intotwo stages. In the first stage, our training uses the classificationloss and class associative loss which aims to learn a model moresuitable for IDPLL. The training objective of the first stage can bewritten as follows:",
  "EXPERIMENTS4.1Experimental Setting": "4.1.1Data sets. To demonstrate the effectiveness of our method,we conducted comparisons on several challenging data sets with atleast 100 classes. To be specific, we conducted experiments on twocommon image data sets including CIFAR-100 , CIFAR-100H and four fine-grained image data sets including CUB-200-2011 , Stanford Cars , FGVC Aircraft , StanfordDogs which are more easily to cause label ambiguity. records the detailed information of all the data sets, where Avg.CLs represent the average number of candidate labels per sample.For data sets CIFAR-100 and CIFAR-100H, the image size was set to 32 32, while for fine-grained data sets, we resized the images to224 224. We employed the IDPLL noisy label generation methodproposed by VALEN to generate instance-dependent noisylabels. Note that for CIFAR-100H, noisy labels only appear in othersubclasses that belong to the same superclass as the ground-truthlabel. 4.1.2Comparing methods. To demonstrate the effectiveness ofthe proposed method, we compared our method with 12 methodsincluding 6 IDPLL methods and 6 PLL methods. IDPLL methods: (i)DIRK , a self-distillation based label disambiguation method. (ii)NEPLL , a normalized entropy guided sample selection method.(iii) POP , a method that progressive purifies candidate labelset and refines classifier. (iv) IDGP , a generation method thatmodels the candidate label generation process. (v) ABLE , acontrastive learning-based framework that uses ambiguity-inducedpositives selection method. (vi) VALEN , a label enhancementguided latent label distribution recovery method. PLL methods: (i)PICO , a method that combines PLL and contrastive learning forthe first time. (ii) CR-DPLL , a consistency regularization label",
  ": Classification accuracy curves of all methods onbenchmark data set CUB200": "disambiguation method. (iii) LWS , a method that uses leveragedweighted loss to balance candidate label set and non-candidate labelset. (iv) PRODEN , a method that progressively identifies theground-truth label during the self-training procedure. (v) RC , arisk-consistent weighting method. (vi) CC , a classifier-consistentthat uses a transition matrix. The parameters of all methods wereset according to their original papers. 4.1.3Implementation details. For fair comparisons, we employedResNet-18 as the backbone on data sets CIFAR-100 and CIFAR-100H, while using a ResNet-34 pre-trained on ImageNet asthe backbone on fine-grained data sets for all methods. We usedML-Decoder as our class-wise encoder . Stochastic gradientdescent (SGD) was used as the optimizer with a momentum of 0.9and all methods were trained for 500 epochs. We selected learningrate in {0.5, 0.1, 0.05, 0.01, 0.005, 0.001} and weight decay in {0.001,0.0005, 0.0001} respectively. Additionally, we applied the cosineannealing learning rate schedule for all methods. The batch sizesof all data sets were set to 128. We repeated the experiments threetimes under the same random seeds and recorded the mean accuracyand standard deviation. As for our method, parameters and were selected from {0.1, 0.5, 1}, while 1 and 2 were selected from{0.5, 1, 2, 5}. The was set to 250, which is half of the total trainingepochs. The length of each class-wise embedding was set to 512on all data sets. Following DIRK , we used the same weakaugmentation and strong augmentation for our method.",
  "Experimental Results": "4.2.1Classification performance. reports the classificationaccuracies of all methods on two common data sets and four fine-grained data sets. According to , our method ranks first inall benchmark data sets when compared with 6 PLL methods and6 IDPLL methods. It is worth noting that there is only minor dif-ference in classification accuracies between the previous PLL andIDPLL methods on common data sets like CIFAR-100 and CIFAR-100H. While our method improves classification accuracy from74.400.18% to 75.510.28% on the CIFAR-100 data set comparedto the best previous method. On fine-grained data sets, the perfor-mance gaps between PLL and IDPLL methods are significant. PLLmethods generally fail to achieve satisfactory accuracy because alllabels in the fine-grained data sets belong to the same superclass,which is more challenging. Consequently, IDPLL methods tailoredto this setting often outperform the conventional PLL methods.Our method CEL constantly excels on these fine-grained data sets,improving classification accuracy from 66.601.07% to 68.600.10%on the data set CUB200 and from 75.970.29% to 78.180.12% onthe data set DOGS120, compared to the best previous method. 4.2.2Significance test. reports win/tie/loss counts of ourmethod CEL against each comparing method based on the pairwiset-test at 0.05 significance level. As shown in , on commondata sets, our method CEL significantly outperforms the compar-ing methods in 83.3% (20/24). While on fine-grained data sets, ourmethod significantly outperforms the comparing methods in 95.8%(46/48). Furthermore, our method achieves superior performancethan all comparing methods except DIRK on fine-grained data sets.Considering all benchmark data sets, our method significantly out-performed the comparing methods in 91.7% (66/72), demonstratingthe effectiveness of our method. 4.2.3Classification accuracy curves. shows the classificationaccuracy curves of all methods on data set CUB200. As shown in, compared to other methods, our method CEL maintainsa relatively fast learning speed in the early stages, only slightlyslower than POP, demonstrating that our class associative loss canenhance the models learning speed. Moreover, CEL achieves thebest classification accuracy in the later stages of training, provingthat our prototype discriminative loss further improves the modelsdisambiguation performance.",
  ": Parameters sensitivity of our method CEL. (a) - (d) represent the classification accuracy of our method on benchmarkdata sets CIFAR-100 and CUB200 by varying , , 1 and 2 respectively": "a classification loss L, the second row represents our methodwith the classification loss and our proposed class associative loss,i.e., L + L, and the third row represents our method with theclassification loss, class associative loss, and our prototype discrimi-native loss, i.e., L +L +L. According to the results in , both the class associative loss L and prototype discriminativeloss L can improve the models classification performance. Tobe specific, the class associative loss improves the classificationaccuracy by an average of 1.22% on six data sets, and the prototypediscriminative loss L loss also promotes the classification accu-racy by an average of 0.71%. Therefore, incorporating both termsinto the model is the optimal choice. 4.3.2Parameters sensitivity. shows the classification accu-racy of our method CEL on benchmark data sets CIFAR-100 andCUB200 under different parameter settings. (a), (b), (c) and(d) correspond to the parameters , , 1, and 2, respectively. Tobe specific, and control the weights of the class associative lossand the prototype discriminative loss, while 1 and 2 control therelative importance of the pull close and push away componentswithin each loss. As illustrated in , when is set to 0.5, to 1,and 1 to 1, the model achieves the best classification performance.Specifically, when 2 is set to 1 and 2, our method achieves thehighest classification accuracy on data sets CUB200 and CIFAR-100, respectively. Therefore, setting , , and 1 to 0.5, 1, and 1,and choosing 2 from {1, 2} are the suggested parameters for ourmethod. 4.3.3Length of the class-wise embedding. We conduct experimentson the data sets CIFAR-100 and CUB200 to verify the impact ofdifferent lengths of class-wise embedding on model classificationperformance. To be specific, we select in {128, 256, 512, 768, 1024}.As shown in , for data set CIFAR-100, which has smaller image Length of Class-Wise Embedding l",
  ": Classification accuracy of different lengths of class-wise embedding on data sets CIFAR-100 and CUB200": "sizes (32 32), the classification accuracy of the model is higherwhen the length of the class-wise embedding is less than or equal to512. This is because excessively large embeddings can dilute impor-tant features in data sets with smaller feature dimensions, whichreduces classification performance. Conversely, for the data setCUB200, which has larger image sizes (224 224), the classificationaccuracy is higher when the length of the class-wise embedding isgreater than or equal to 512, this is because for data sets with largerfeature dimensions, excessively small embeddings can compressimportant feature information, leading to a decline in performance.Therefore, considering both cases, setting the class-wise embeddinglength to 512 is a good choice.",
  "CONCLUSION": "In this paper, we have presented a novel method named CEL to ad-dress the IDPLL problem. For the first time, we realize that IDPLL isa mixed blessing with both positive side and negative side. We, there-fore, propose to construct the class-wise embeddings to explore therelationships among the candidate labels and the non-candidatelabels. To leverage the positive side of IDPLL, we introduced theclass associative loss to learn representations that are more suitablefor IDPLL. This is achieved by leveraging the similarity amonglabels within the candidate label set and the differences betweenthe candidate label set and the non-candidate label set throughclass-wise embeddings. To mitigate the negative side of IDPLL, i.e.,identifying the ground-truth label in the candidate set becomesmore challenging, we constructed prototype discriminative loss toguide the models disambiguation process using class prototypeswhich include global sample information. Extensive experimentson both common and fine-grained data sets demonstrate that ourmethod significantly outperforms twelve state-of-the-art PLL andIDPLL methods. Moreover, compared with previous methods, ourmethod converges faster in the early stages of model training, whileproduces the highest classification accuracy in the later trainingstages.",
  "Alex Krizhevsky. 2009. Learning Multiple Layers of Features from Tiny Images.(2009)": "Jack Lanchantin, Tianlu Wang, Vicente Ordonez, and Yanjun Qi. 2021. Gen-eral Multi-Label Image Classification With Transformers. In IEEE Conference onComputer Vision and Pattern Recognition, CVPR 2021. 1647816488. Changchun Li, Ximing Li, and Jihong Ouyang. 2020. Learning with Noisy PartialLabels by Simultaneously Leveraging Global and Local Consistencies. In ACMInternational Conference on Information and Knowledge Management. 725734. Li-Ping Liu and Thomas G. Dietterich. 2012. A Conditional Multinomial MixtureModel for Superset Label Learning. In Advances in Neural Information ProcessingSystems 25: 26th Annual Conference on Neural Information Processing Systems.557565.",
  "Shilong Liu, Lei Zhang, Xiao Yang, Hang Su, and Jun Zhu. 2021. Query2Label:A Simple Transformer Way to Multi-Label Classification. CoRR abs/2107.10834(2021). arXiv:2107.10834": "Jie Luo and Francesco Orabona. 2010. Learning from Candidate Labeling Sets. InAdvances in Neural Information Processing Systems 23: 24th Annual Conference onNeural Information Processing Systems. 15041512. Jiaqi Lv, Biao Liu, Lei Feng, Ning Xu, Miao Xu, Bo An, Gang Niu, Xin Geng, andMasashi Sugiyama. 2024. On the Robustness of Average Losses for Partial-LabelLearning. IEEE Trans. Pattern Anal. Mach. Intell. 46, 5 (2024), 25692583. Jiaqi Lv, Miao Xu, Lei Feng, Gang Niu, Xin Geng, and Masashi Sugiyama. 2020.Progressive Identification of True Labels for Partial-Label Learning. In Interna-tional Conference on Machine Learning, ICML 2020. 65006510.",
  "Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew B. Blaschko, and AndreaVedaldi. 2013. Fine-Grained Visual Classification of Aircraft. CoRR abs/1306.5151(2013). arXiv:1306.5151": "Congyu Qiao, Ning Xu, and Xin Geng. 2023. Decompositional Generation Processfor Instance-Dependent Partial Label Learning. In The Eleventh InternationalConference on Learning Representations, ICLR 2023. Tal Ridnik, Gilad Sharir, Avi Ben-Cohen, Emanuel Ben Baruch, and Asaf Noy.2023. ML-Decoder: Scalable and Versatile Classification Head. In IEEE/CVF WinterConference on Applications of Computer Vision, WACV 2023. 3241. Shiyu Tian, Hongxin Wei, Yiqun Wang, and Lei Feng. 2024. CroSel: Cross Selec-tion of Confident Pseudo Labels for Partial-Label Learning. In IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition, CVPR 2024. 1947919488.",
  "Deng-Bao Wang, Min-Ling Zhang, and Li Li. 2022. Adaptive Graph GuidedDisambiguation for Partial Label Learning. IEEE Trans. Pattern Anal. Mach. Intell.44, 12 (2022), 87968811": "Haobo Wang, Ruixuan Xiao, Yixuan Li, Lei Feng, Gang Niu, Gang Chen, andJunbo Zhao. 2022. PiCO: Contrastive Label Disambiguation for Partial LabelLearning. In The Tenth International Conference on Learning Representations, ICLR2022. Xiu-Shen Wei, Yi-Zhe Song, Oisin Mac Aodha, Jianxin Wu, Yuxin Peng, JinhuiTang, Jian Yang, and Serge J. Belongie. 2022. Fine-Grained Image Analysis WithDeep Learning: A Survey. IEEE Trans. Pattern Anal. Mach. Intell. 44, 12 (2022),89278948. Hongwei Wen, Jingyi Cui, Hanyuan Hang, Jiabin Liu, Yisen Wang, and ZhouchenLin. 2021. Leveraged Weighted Loss for Partial Label Learning. In Proceedingsof the 38th International Conference on Machine Learning, ICML 2021, Vol. 139.1109111100. Dong-Dong Wu, Deng-Bao Wang, and Min-Ling Zhang. 2022. Revisiting Consis-tency Regularization for Deep Partial Label Learning. In International Conferenceon Machine Learning, ICML 2022, Vol. 162. 2421224225. Dong-Dong Wu, Deng-Bao Wang, and Min-Ling Zhang. 2024. Distilling ReliableKnowledge for Instance-Dependent Partial Label Learning. In Thirty-Eighth AAAIConference on Artificial Intelligence. 1588815896. Shiyu Xia, Jiaqi Lv, Ning Xu, and Xin Geng. 2022. Ambiguity-Induced Con-trastive Learning for Instance-Dependent Partial Label Learning. In Thirty-FirstInternational Joint Conference on Artificial Intelligence. 36153621. Shiyu Xia, Jiaqi Lv, Ning Xu, Gang Niu, and Xin Geng. 2023. Towards EffectiveVisual Representations for Partial-Label Learning. In IEEE/CVF Conference onComputer Vision and Pattern Recognition, CVPR 2023. 1558915598. Ning Xu, Biao Liu, Jiaqi Lv, Congyu Qiao, and Xin Geng. 2023. Progressive Purifi-cation for Instance-Dependent Partial Label Learning. In International Conferenceon Machine Learning, ICML 2023. 3855138565."
}