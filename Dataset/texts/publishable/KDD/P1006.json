{
  "ABSTRACT": "Graph-based fraud detection has widespread application in modernindustry scenarios, such as spam review and malicious account de-tection. While considerable efforts have been devoted to designingadequate fraud detectors, the interpretability of their results hasoften been overlooked. Previous works have attempted to generateexplanations for specific instances using post-hoc explaining meth-ods such as a GNNExplainer. However, post-hoc explanations cannot facilitate the model predictions and the computational cost ofthese methods cannot meet practical requirements, thus limitingtheir application in real-world scenarios. To address these issues,we propose SEFraud, a novel graph-based self-explainable fraud de-tection framework that simultaneously tackles fraud detection andresult in interpretability. Concretely, SEFraud first leverages cus-tomized heterogeneous graph transformer networks with learnablefeature masks and edge masks to learn expressive representationsfrom the informative heterogeneously typed transactions. A newtriplet loss is further designed to enhance the performance of masklearning. Empirical results on various datasets demonstrate theeffectiveness of SEFraud as it shows considerable advantages in",
  "Both authors contributed equally to this research and are listed alphabetically.Corresponding author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain. 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08 both the fraud detection performance and interpretability of pre-diction results. Specifically, SEFraud achieves the most significantimprovement with 8.6% on AUC and 8.5% on Recall over the sec-ond best on fraud detection, as well as an average of 10x speed-upregarding the inference time. Last but not least, SEFraud has beendeployed and offers explainable fraud detection service for thelargest bank in China, Industrial and Commercial Bank of ChinaLimited (ICBC). Results collected from the production environmentof ICBC show that SEFraud can provide accurate detection resultsand comprehensive explanations that align with the expert busi-ness understanding, confirming its efficiency and applicability inlarge-scale online services.",
  "fraud detection, interpretability, graph neural networks": "ACM Reference Format:Kaidi Li, Tianmeng Yang, Min Zhou, Jiahao Meng, Shendi Wang, YihuiWu, Boshuai Tan, Hu Song, Lujia Pan, Fan Yu, Zhenli Sheng, and YunhaiTong. 2024. SEFraud: Graph-based Self-Explainable Fraud Detection viaInterpretative Mask Learning. In Proceedings of the 30th ACM SIGKDDConference on Knowledge Discovery and Data Mining (KDD 24), August2529, 2024, Barcelona, Spain. ACM, New York, NY, USA, 10 pages.",
  "SEFraud: Graph-based Self-Explainable Fraud Detection via Interpretative Mask LearningKDD 24, August 2529, 2024, Barcelona, Spain": "Milliseconds response efficiency. Rather than retrainingthe explanation network for each new instance, the learnedfeature mask and edge mask are self-explanatory, whichallows SEFraud to generate precise and comprehensive ex-planations within milliseconds. Excellent application effect. SEFraud has been deployedand offers explainable detection service for the largest bankin China, Industrial and Commercial Bank of China Limited(ICBC). Results on the real production environment withreal graphs from ICBC prove that SEFraud can provide bothaccurate detection results and explanations that align withtheir business understanding, showing its efficiency andapplicability in practice.",
  "PRELIMINARIES AND RELATED WORK": "Graph-based fraud detection can be applied to a wide range of sce-narios, such as financial transactions , human behaviours ,and social media interactions , to uncover patterns and anom-alies that indicate fraudulent activity. In this section, we introducethe basic graph-based fraud detection task definition and review therecent developments on GNNs and their heterogeneous variants.Then, we briefly introduce works about GNNs and interpretabilityand their applications in the fraud detection scenario.",
  "Problem Formulation": "Given a graph G = (V, E, X, Y) with node set V and edge set E; X represents a -dimension feature vector of node and ; Y is the set of labels for each node in V. Generally, graph-based fraud detection can be viewed as a binary node classificationproblem on G to classify the nodes into benign ( = 0) or fraudu-lent ( = 1) groups.It is noted that if there are one type of node and various types ofrelations in G, a multi-relation graph can be defined as E = { }|with different types of relations. Considering a heterogeneousscenario that may contain different types of nodes and differenttypes of relations simultaneously, we further define a heteroge-neous graph to be associated with a node type mapping function : V A and an edge type mapping function : E R. A andR denote the sets of predefined node types and relation types, and|A| + |R| > 2.",
  "= (1, aggr(1| ())),(1)": "where 0 = , aggr() denotes a differentiable, permutation in-variant function to aggregate neighbor information, () is a trans-formation function between two propagation steps, () is theconnected neighbors of .Many previous works conduct semi-supervised node classifica-tion tasks on single-relation graphs or define multi-relation graph to tackle fraud detection tasks. The commoninsight is to design different massage passing mechanisms for ag-gregating neighborhood information. For example, GeniePath learns convolutional layers and neighbor weights using LSTM and the attention mechanism . SemiGNN applies a GNN-based hierarchical attention mechanism to detect fraudsters on Ali-pay. GraphConsis and CARE-GNN filter dissimilar neigh-bors before aggregation to discover camouflage fraudsters. PC-GNN identifies and solves the label imbalance issue by noderesampling. Heterogeneous GNNs are capable oflearning embeddings for heterogeneous graphs but still remainunderexplored for fraud detection scenarios.",
  "Explainability of GNNs": "Despite the notable achievements of graph-based methods, theirlack of transparency hinders easy comprehension of the predictions.Nevertheless, enhancing explanations and providing the modelcredibility which offers valuable guidance to business professionalsin terms of prevention and control holds significant importance,especially for applications such as fraud detection. Recently, severalmodels have been proposed to explain the predictions of GNNs,such as XGNN , GNNExpaliner , and PGExplainer . GN-NExplainer learns soft masks for edges and node features toexplain the predictions via mask optimization. PGExplainer learns approximated discrete masks for edges to explain the predic-tions via the reparameterization trick. XGNN trains a graphgenerator to provide model-level explanations and is only used forgraph classification models. Few works pay efforts to provide expla-nations for graph-based fraud detection. xFraud combines thefraud detector with an explainer using GNNExplainer and providesexplanations to facilitate further processes. However, these meth-ods can only provide post-hoc explanations. Besides, GNNExpalinerneeds to re-train the explanation network for each new instanceand thus is criticized for its low efficiency. While PGExplainer enhances both the accuracy and efficiency of explanations com-pared to GNNExplainer , it still has limitations. Specifically,PGExplainers explanations are solely from the perspective of edgeaspects, leading to a certain degree of incompleteness. This becomesparticularly problematic in scenarios that necessitate a comprehen-sive explanation encompassing both node features and relationssimultaneously. By incorporating a self-explanatory mask learningmodule into the GNN model, we aim to improve representationlearning of the GNN detector to enhance the models predictionability, as well as generate high-quality and comprehensive expla-nations with milliseconds response efficiency.",
  "THE PROPOSED METHOD": "In this section, we present the proposed Self-Explainable Fraudframework. As sketched in , SEFraud first utilizes cus-tomized heterogeneous graph transformer networks to extractmeaningful representations from diverse transaction types, whichare then incorporated by the learnable feature masks and edgemasks, generating the reweighted graph. A novel triplet loss isspecifically designed to enhance the mask learning. We detail theinformation aggregation and interpretative mask learning in Sec-tion 3.1 and 3.2, respectively. Then we explain how to aggregate",
  "\"2'": ": The architecture of SEFraud. A heterogeneous convolution layer is utilized to aggregate the hetero-graph informationand generate the feature embedding for each node. These feature embeddings, raw features, and node type encodings for eachnode are then concatenated to form the input for FNet. An edge embedding consists of the node embeddings at its two ends,and concatenates with the edge type encodings to form the the input for ENet. The learned feature masks and edge masks arefurther leveraged to reconstruct a weighted hetero-graph, which serves as the input for the GNN/Detection model. A contrastivetriplet loss is then constructed based on the output of the model for the training process.",
  "Heterogeneous Convolution Layer": "Considering various node types and edge types in real-world in-dustry scenarios, we utilize a Heterogeneous Graph Transformer(HGT) to construct our convolution layer, and aggregate het-erogeneous neighborhood information from source nodes to geta contextualized representation for target node. The process canbe decomposed into three components: Heterogeneous MutualAttention, Heterogeneous Message Passing, and Target-SpecificAggregation. Denoting the output of the layer is , we canstack multiple layers to learn the node representations as follows:",
  "Interpretative Mask Learning": "Interpretability in graph-based fraud detection tasks includes twoaspects. Firstly, influential edges offer topology explanations forcorrelations between nodes. Secondly, certain critical features pro-vide the properties of nodes to be flagged as fraudsters. By stacking layers, we can obtain the node representations of the entire graph,denoted as (). Unlike the attention weights in HGT that varyacross different layers, we aim to learn a unified, consistent featuremask and edge mask among all aggregating layers.In order to achieve this, we integrate a feature attention net-work (FNet) and an edge attention network (ENet) following theheterogeneous convolution layers. Given the heterogeneity of nodetypes and edge types, we also implement type encoding for eachnode type and edge type. Specifically, for the feature mask, weconcatenate the initial feature 0 , the aggregated feature , andthe node type encoding of each node. We then utilize a fordifferent node types to learn the node mask. For the edge mask, weconcatenate the source node and target node features along withtheir edge type encodings. We then employ an to learn theedge mask. Formally, the learned feature mask - and edgemask - are acquired by:",
  "Contrastive Triplet Loss": "With the learned feature mask and edge mask, the initial node fea-tures are re-weighted, and the influence of edges is constrainedduring the forward process of the model. Intuitively, the modelsprediction results with learned masks should align closely withthe ground truth. Conversely, if we assign negative weights to themasks, the models prediction results should diverge significantly.Denoting the positive prediction results of node as +, the nega-tive results as , and the ground truth as , we have designeda contrastive triplet loss function, denoted as L, that operateson each triplet pair < , +, >. This function penalizes thedistance between the positive pair and the negative pair with amargin :",
  "EXPERIMENTS": "For a comprehensive evaluation of SEFraud, we conduct experi-ments for both fraud detection and interpretation tasks, and com-pare it with various baselines. An ablation study is also performedto show the effectiveness of our designs. Case studies, time effi-ciency comparison, and deployment effect are further provided todemonstrate the advantages of our method in industry applicationscenarios.",
  "Fraud detection datasets": "Yelp and Amazon. The Yelp dataset includes hotel andrestaurant reviews filtered (spam) and recommended (legiti-mate) by Yelp. The nodes in the graph are reviews with threerelations: the reviews posted by the same user, the reviewsunder the same product with the same star rating, and thereviews under the same product posted in the same month.The Amazon dataset includes product reviews under theMusical Instruments category. The nodes in the graph ofthe Amazon dataset are users with 100-dimension featuresand also contain three relations: users reviewing at least onesame product, users having at least one same star ratingwithin one week, and users with top-5% mutual review TF-IDF similarities. The users with more than 80% helpful votesare benign entities and users with less than 20 helpful votesare fraudulent entities. Handcrafted features are extractedfrom prior works as the raw node features for thedatasets, respectively. ICBC. The financial fraud detection dataset, termed ICBC, isgenerated by TabSim 1, a tool based on the statistical charac-teristics of debts and customers provided by ICBC (Industrialand Commercial Bank of China), one of the largest banksin China. The nodes of ICBC data represent customers and",
  "Interpretation datasets": "BA-Shapes, Tree-Cycles and Tree-Grids. These datasetsare synthetic graphs for node-level explanation. BA-Shapesis a single graph consisting of a base Barabasi-Albert (BA)graph with 300 nodes and 80 house-structured motifs.These motifs are attached to randomly selected nodes fromthe BA graph. After that, random edges are added to perturbthe graph. Nodes in the base graph are labeled with 0; theones locating at the top/middle/bottom of the house arelabeled with 1,2,3, respectively. For the Tree-Cycles dataset,an 8-level balanced binary tree is adopted as the base graph.An 80 six-node cycle motifs are attached to randomly se-lected nodes from the base graph. Tree-Grid is constructedsimilarly to Tree-Cycles, except that 3-by-3 grid motifs areused to replace the cycle motifs. BA-2motifs. For graph level explanation, BA-2motifs adoptsthe BA graphs as base graphs. Half of the graphs are attachedwith house motifs, and the rest are attached with five-node cycle motifs. Graphs are assigned to one of 2 classesaccording to the type of attached motifs.",
  "Experimental Settings": "To make a fair comparison, we closely follow the experimentalprocedure with on fraud detection and on explanation.We use the same feature vectors, labels, and train/val/test splits forall baselines. For SEFraud, the layers of heterogenous convolution is 2, number of heads is 4, margin of contrastive triplet loss is 0.1, trade off parameter of combined loss function is 0.3.Other settings are tuned from: hidden size {8, 16, 32, 64}, learningrate {5-3, 1-2}, and L2 regularization weight {1-3, 5-4}.We implement our methods using Pytorch Geometric with theAdam optimizer .",
  "Fraud Detection Performance": "In most fraud detection scenarios, the nodes are predominantly be-nign, with the proportion of fraudulent entities being significantlylow. From the application perspective, the task of fraud detection issupposed to pay more attention to recognizing potential fraudsters.Thus, we report the ROC-AUC (AUC) and Recall metrics follow-ing to evaluate the overall performance of all methods. AUC iscomputed based on the relative ranking of prediction probabilitiesof all instances, which could eliminate the influence of imbalancedclasses. Recall measures how the model correctly identifies posi-tive instances (true positives) from all actual positive samples, thusrepresenting the ability to flag potential fraudsters. summarizes the prediction results of fraud detectionamong three datasets. Most GNNs built on homogeneous graphssuch as GCN and GeniePath, or simply aggregate information fromdifferent relations such as RGCN, show inferior performance com-pared with methods that can handle complex multi-relation graphs(CARE-GNN) or apply heterogeneous graph convolution (xFraudand SEFraud) on Yelp and Amazon. In particular, xFraud achievesremarkable improvements on these two datasets. It is worth men-tioning that GeniePath outperforms other baselines on the ICBCdataset, which may be because ICBC is more sparse, and it is criticalto explore essential neighborhoods. In a sparse and noisy scenario,GeniePath and CARE-GNN, which are capable of adaptively filter-ing neighborhood information, can even overpass xFraud.On the one hand, SEFraud tackles heterogeneity compared withgeneral GNNs. On the other hand, compared with xFraud, SEFraudcan enhance the heterogeneous convolution process by learningfeature mask and edge mask, thus consistently outperforming allbaselines on both AUC and Recall metrics. Specifically, SEFraudachieves an 8.6% improvement on AUC and an 8.5% improvementon Recall over xFraud on the Yelp dataset.We further conducted ablation studies to evaluate the effective-ness of the each component in the proposed SEFraud. Specifically,we denoted models without a specific component using w/o andre-evaluate the performance. The results presented in the bottompart of demonstrate that removing any of the componentsleads to performance degradation, confirming the efficacy of each",
  "SE-Mask97.81.699.80.196.51.395.82.7": "module in the proposal. Notably, for denser graphs like Yelp andAmazon, the edge masks has a more significant impact than thefeature masks as considerable degradation on the performance isobserved if the corresponding module is dropped. Conversely, inthe case of ICBC where edges are sparse, the node mask plays acritical role. Additionally, the triplet loss is essential for learninghigh-quality masks and exhibits a noticeable decrease on perfor-mance when removed.",
  "Interpretability": "Given that the FNet and ENet are trained alongside the detectionmodel, a higher level of model prediction accuracy demonstratesthe validity of the learned masks applied to the original graphs.The learned masks reflect the significance of the node features andedges. Therefore, they are interpretative and can directly be utilizedto elucidate the model predictions. 4.5.1Quantitative evaluation. To quantitatively verify the inter-pretability of the learned masks, we follow the setting in GNNEx-plainer and PGExplainer to construct experiments onthe synthetic datasets on node classification and graph classifica-tion tasks. Concretely, we incorporate our proposed mask learningparadigm (termed as SE-Mask) into a GCN backbone used in GN-NExplainer and PGExplainer, and leverage the learned edge masksas interpretation when GCN training is finished.The explanation problem is formalized as a binary classificationtask, where edges in the ground-truth explanation are treated aslabels, and importance weights given by the explanation method",
  "Base": ": Qualitative explanation examples comparison.Node labels are represented by their colors. Explanationsof instance in each dataset are highlighted by bold blackedges ranked by their importance weights. are viewed as prediction scores. A suitable explanation methodassigns higher weights to edges in the ground truth motifs thanoutside ones. Thus, AUC is adopted as the metric for quantitativeevaluation. Results in show that our proposed SE-Maskachieves remarkable interpretation performances compared withbaseline explanation methods in three datasets. For Tree-Cycles,SE-Mask performs inferior to PGExplainer but still overpasses GN-NExpalainer. The average lifts on the AUC of SE-Mask over GN-NExpalainer and PGExplainer are 12.8% and 3.2%, demonstratingits superior interpretability. 4.5.2Qualitative evaluation. We also choose an instance for eachdataset and visualize its explanations given by SE-Mask and base-lines in . We first compute the edge weights for each in-stance and highlight the top-K edges where K is the number of",
  ": Explanation examples from ICBC dataset": "edges in each explanation motif. For both graph classification andnode classification tasks, SE-Mask can accurately identify edgesinside the explanation motifs (i.e., the \"house,\" \"cycle,\" and \"3-by-3grid\"), while GNNExplainer and PGExplainer have confused someessential edges. These visualizations demonstrate that SE-Maskpossesses a more robust ability for explanation.To further validate the credibility of our explanations on finan-cial fraud detection, we randomly selected 100 nodes that werepredicted to be fraudsters, which were further manually checkedand confirmed by business experts of ICBC. The experts corrobo-rated that the explanations provided by our system, SEFraud, arenot only logical but also congruent with their extensive businessacumen. This validation underscores the credibility of our systemin the realm of financial fraud detection. Two detailed examples aresketched in .For node prediction t1, from the perspective of edge analysis,the most pivotal edge contributing to its classification as a fraud-ulent entity is its equity nexus with another identified fraudster,s2. When considering node features, the three most significant at-tributes include Behavioral Score, Accumulated Released Amount,and Overdue principal balance. According to the expert analysisfrom ICBC, a behavioral score of 65 is considered relatively low,indicating a high risk. Furthermore, examining the other two fea-tures reveals that t1 has already received a substantial accumu-lated released amount. However, the overdue balance generatedconstitutes a significant proportion compared to the accumulatedreleased amount. This combination of factors further substantiatesthe classification of t1 as fraudulent. In the second example, theclassification of t2 as a fraudulent entity is primarily predicatedon the Guarantor edge from S2, which is already identified as ahigh-risk fraudster. Regarding the node features, t2 has provideda mortgage as the method of credit guarantee associated with arelatively extended contract duration. However, it is noteworthythat t2 also possesses a subprime loan. The combination of thesefactors suggests that t2 may lack the financial capacity to repaythe debt. This amalgamation of circumstances further substantiatesthe classification of t2 as a fraudulent entity. These two examplesdemonstrate how the explanations provide insights into the rea-sons behind the fraudsters predictions, aiding in understandingand interpreting the models decisions.",
  "Model Analysis": "Different GNN encoders. While it is possible to use alternativeGNNs like GCN as feature encoders, ignoring the heterogeneitymay result in performance degradation. In general, the data inICBC contains heterogeneous types of information related to vari-ous nodes and relations thus we opt to construct our model using acustomized heterogeneous graph transformer , which has demon-strated its superiority in our baseline XFraud . To provide athorough analysis of model design, we also conduct experimentto evaluate different kinds of GNN based encoders. As shown in, heterogenous GNNs (i.e. HAN and HGT ) signif-icantly outperform homogenous GNNs including GCN and GAT.HGT has achieved a higher performances as well as less time con-sumption than HAN. Hyper-parameters analysis. We also conducted a hyper-parameteranalysis on the ICBC dataset. One of the key hyperparameters is ,which balances the classification loss and contrastive tripplet loss.As shown in the Tabel 6 , the performance initially improves andthen gradually declines when ranges from 0 to 0.5. This behaviorcan be attributed to the increasing weight of the triplet loss, whichenhances instance discrimination but may weaken classificationperformance beyond a certain threshold. A similar trend is observedfor another hyperparamete , which acts as a margin in the tripletloss. A larger margin encourages the model to distinguish importantedges and node features more confidently. However, excessivelylarge margins can make it challenging to reduce the triplet loss tozero, potentially harming the overall model performance.",
  "Time Comparison": "Efficiency plays a critical role in industry applications, especially forfinancial systems requiring milliseconds response speed. For eachinstance, GNNExplainer needs to re-train the explanation networkbefore generating a new explanation and thus is criticized for itslow efficiency. The explanation network in PGExplainer is sharedacross the population of instances and can be utilized to explain",
  "SE-Mask10.55ms0.96ms0.54ms0.60ms#speed-up86x678x1278x1189x": "new instances in the inductive setting. However, it also needs tocalculate the weights of the subgraph in each explaining process.In contrast, explanation weights in SE-Mask are trained with thebackbone GNN model and can be directly extracted for explaining,which only takes milliseconds. As shown in Tabel 7, we comparedthe inference time of generating a new explanation instance withthe three kinds of methods. SE-Mask achieves over 1000x speed-upon explain Tree-Cycles and Tree-Grids datasets, having a signifi-cant advantage on time efficiency. In an industry financial frauddetection system, GNNExplainer requires approximately 7 secondsto generate a reasonable explanation for a flagged fraudster. Incontrast, our proposed SE-Mask accomplishes the same task in just0.4 milliseconds.",
  "Deployment and application": "In the studys second phase, the proposed SEFraud is further de-ployed in ICBCs production environment with the Mindspore ser-vice . The models performance is measured based on ICBCsreal data, which is divided into four parts corresponding to fourmonths. Each month contains approximately 86,000 nodes and18,000 edges. The model is trained using one months data andtested its predictive performances and explanation effects usingthe following months data. The feedback received from ICBC ishighly positive and inspiring. During the real business practice, Theaverage prediction AUC of SEFraud is 97%, indicating a high levelof accuracy, with a recall rate of 0.98. Additionally, the inferencetime taken for predicting and providing explanations for a singlenode averaged just at 0.4ms. Moreover, the generated explanationsfor the predicted fraudsters are well-received by the experts fromICBC, demonstrating the effectiveness of our approach.",
  "CONCLUSION": "Current practical implementations of graph fraud detection appli-cation are limited for scenarios that require both high confidence(i.e., explainable detection) and efficiency. In this paper, we intro-duce the SEFraud, a highly efficient self-explainable fraud detectionframework. The interpretive feature mask and edge mask are inte-grated into customized heterogeneous graph transformer networksto enhance the detection models prediction ability and provide ex-planations. And a specific contrastive triplet loss is further designedto augment the mask learning. Experimental results demonstratethe advantages of SEFraud in various fraud detection tasks and it isable to provide reasonable explanations within milliseconds. Thedeployment and verification of SEFraud in ICBC, one of the largest",
  "Xinyu Fu, Jiani Zhang, Ziqiao Meng, and Irwin King. 2020. Magnn: Metap-ath aggregated graph neural network for heterogeneous graph embedding. InProceedings of The Web Conference 2020. 23312341": "Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George EDahl. 2017. Neural message passing for quantum chemistry. In Internationalconference on machine learning. PMLR, 12631272. William L Hamilton, Rex Ying, and Jure Leskovec. 2017. Inductive representationlearning on large graphs. In Proceedings of the 31st International Conference onNeural Information Processing Systems. 10251035.",
  "Thomas N Kipf and Max Welling. 2017. Semi-supervised classification with graphconvolutional networks. International Conference on Learning Representations(ICLR) (2017)": "Xiangfeng Li, Shenghua Liu, Zifeng Li, Xiaotian Han, Chuan Shi, Bryan Hooi, HeHuang, and Xueqi Cheng. 2020. Flowscope: Spotting money laundering basedon graphs. In Proceedings of the AAAI conference on artificial intelligence, Vol. 34.47314738. Junfeng Liu, Min Zhou, Shuai Ma, and Lujia Pan. 2023. MATA*: CombiningLearnable Node Matching with A* Algorithm for Approximate Graph Edit Dis-tance Computation. In Proceedings of the 32nd ACM International Conference onInformation and Knowledge Management. 15031512. Yang Liu, Xiang Ao, Zidi Qin, Jianfeng Chi, Jinghua Feng, Hao Yang, and QingHe. 2021. Pick and choose: a GNN-based imbalanced learning approach for frauddetection. In Proceedings of the web conference 2021. 31683177. Ziqi Liu, Chaochao Chen, Longfei Li, Jun Zhou, Xiaolong Li, Le Song, and YuanQi. 2019. Geniepath: Graph neural networks with adaptive receptive paths. InProceedings of the AAAI Conference on Artificial Intelligence, Vol. 33. 44244431. Ziqi Liu, Chaochao Chen, Xinxing Yang, Jun Zhou, Xiaolong Li, and Le Song.2018. Heterogeneous graph neural networks for malicious account detection. InProceedings of the 27th ACM international conference on information and knowledgemanagement. 20772085. Zhiwei Liu, Yingtong Dou, Philip S Yu, Yutong Deng, and Hao Peng. 2020. Al-leviating the inconsistency problem of applying graph neural network to frauddetection. In Proceedings of the 43rd international ACM SIGIR conference on re-search and development in information retrieval. 15691572.",
  "Jun Ma, Danqing Zhang, Yun Wang, Yan Zhang, and Alexey Pozdnoukhov. 2018.GraphRAD: a graph-based risky account detection system. In Proceedings of ACMSIGKDD conference, London, UK, Vol. 9": "Xiaoxiao Ma, Jia Wu, Shan Xue, Jian Yang, Chuan Zhou, Quan Z. Sheng, HuiXiong, and Leman Akoglu. 2021. A Comprehensive Survey on Graph Anom-aly Detection with Deep Learning. IEEE Transactions on Knowledge and DataEngineering (2021), 11. Tahereh Pourhabibi, Kok-Leong Ong, Booi H Kam, and Yee Ling Boo. 2020. Frauddetection: A systematic literature review of graph-based anomaly detectionapproaches. Decision Support Systems 133 (2020), 113303. Susie Xi Rao, Shuai Zhang, Zhichao Han, Zitao Zhang, Wei Min, Zhiyao Chen,Yinan Shan, Yang Zhao, and Ce Zhang. 2020. xFraud: explainable fraud transactiondetection. arXiv preprint arXiv:2011.12193 (2020). Shebuti Rayana and Leman Akoglu. 2015. Collective opinion spam detection:Bridging review networks and metadata. In Proceedings of the 21th acm sigkddinternational conference on knowledge discovery and data mining. 985994. Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, IvanTitov, and Max Welling. 2018. Modeling relational data with graph convolu-tional networks. In The Semantic Web: 15th International Conference, ESWC 2018,Heraklion, Crete, Greece, June 37, 2018, Proceedings 15. Springer, 593607. Xuemeng Song, Chun Wang, Changchang Sun, Shanshan Feng, Min Zhou, andLiqiang Nie. 2023. MM-FRec: Multi-Modal Enhanced Fashion Item Recommenda-tion. IEEE Transactions on Knowledge and Data Engineering (2023).",
  "Petar Velikovi, Guillem Cucurull, Arantxa Casanova, Adriana Romero, PietroLio, and Yoshua Bengio. 2017. Graph attention networks. International Conferenceon Learning Representations (ICLR) (2017)": "Daixin Wang, Jianbin Lin, Peng Cui, Quanhui Jia, Zhen Wang, Yanming Fang,Quan Yu, Jun Zhou, Shuang Yang, and Yuan Qi. 2019. A semi-supervised graphattentive network for financial fraud detection. In 2019 IEEE International Confer-ence on Data Mining (ICDM). IEEE, 598607. Jianyu Wang, Rui Wen, Chunming Wu, Yu Huang, and Jian Xiong. 2019. Fdgars:Fraudster detection via graph convolutional networks in online app reviewsystem. In Companion proceedings of the 2019 World Wide Web conference. 310316.",
  "Tianmeng Yang, Yujing Wang, Zhihan Yue, Yaming Yang, Yunhai Tong, and JingBai. 2022. Graph pointer neural networks. In Proceedings of the AAAI conferenceon artificial intelligence, Vol. 36. 88328839": "Tianmeng Yang, Min Zhou, Yujing Wang, Zhengjie Lin, Lujia Pan, Bin Cui, andYunhai Tong. 2023. Mitigating Semantic Confusion from Hostile Neighborhoodfor Graph Active Learning. arXiv preprint arXiv:2308.08823 (2023). Xiaocheng Yang, Mingyu Yan, Shirui Pan, Xiaochun Ye, and Dongrui Fan. 2023.Simple and efficient heterogeneous graph neural network. In Proceedings of theAAAI Conference on Artificial Intelligence, Vol. 37. 1081610824. Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton,and Jure Leskovec. 2018. Graph convolutional neural networks for web-scalerecommender systems. In Proceedings of the 24th ACM SIGKDD internationalconference on knowledge discovery & data mining. 974983. Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec.2019. Gnnexplainer: Generating explanations for graph neural networks. Ad-vances in neural information processing systems 32 (2019). Hao Yuan, Jiliang Tang, Xia Hu, and Shuiwang Ji. 2020. Xgnn: Towards model-level explanations of graph neural networks. In Proceedings of the 26th ACMSIGKDD International Conference on Knowledge Discovery & Data Mining. 430438. Shijie Zhang, Hongzhi Yin, Tong Chen, Quoc Viet Nguyen Hung, Zi Huang, andLizhen Cui. 2020. Gcn-based user representation learning for unifying robustrecommendation and fraudster detection. In Proceedings of the 43rd internationalACM SIGIR conference on research and development in information retrieval. 689698. Haizhong Zheng, Minhui Xue, Hao Lu, Shuang Hao, Haojin Zhu, Xiaohui Liang,and Keith Ross. 2017. Smoke screener or straight shooter: Detecting elite sybilattacks in user-review social networks. arXiv preprint arXiv:1709.06916 (2017). Qiwei Zhong, Yang Liu, Xiang Ao, Binbin Hu, Jinghua Feng, Jiayu Tang, andQing He. 2020. Financial defaulter detection on online credit payment via multi-view attributed heterogeneous information network. In Proceedings of The WebConference 2020. 785795."
}