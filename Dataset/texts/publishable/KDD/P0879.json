{
  "Abstract": "This paper describes the winning solutions of all tasks in Meta KDDCup 24 from db3 team. The challenge is to build a RAG systemfrom web sources and knowledge graphs. We are given multiplesources for each query to help us answer the question. The CRAGchallenge involves three tasks: (1) condensing information fromweb pages into accurate answers, (2) integrating structured datafrom mock knowledge graphs, and (3) selecting and integratingcritical data from extensive web pages and APIs to reflect real-worldretrieval challenges. Our solution for Task #1 is a framework of webor open-data retrieval and answering. The large language model(LLM) is tuned for better RAG performance and less hallucination.Task #2 and Task #3 solutions are based on a regularized API setfor domain questions and the API generation method using tunedLLM. Our knowledge graph API interface extracts directly relevantinformation to help LLMs answer correctly. Our solution achieves1st place on all three tasks, achieving a score of 28.4%, 42.7%, and47.8%, respectively.",
  "Introduction": "Ensuring the trustworthiness of language model (LLM) responsesis critical due to the persistent issue of hallucination, where modelsgenerate inaccurate or ungrounded answers. Studies show that GPT-4s accuracy for fast-changing facts is often below 35% . Retrieval-Augmented Generation (RAG) offers a promising solution byintegrating external information retrieval with LLMs to providegrounded answers. Despite its potential, RAG faces challenges inselecting relevant information, reducing latency, and synthesizingcomplex answers.To address these issues, Meta introduces the Meta Comprehen-sive RAG Challenge (CRAG) as a 2024 KDDCup event, aiming tobenchmark RAG systems with clear metrics and evaluation proto-cols to drive innovation and advance solutions in this field. CRAGBenchmark encompasses five domains, eight question types, vary-ing answer timelines, and a range of entity popularity, includinghead, torso, and tail facts, as well as simple and complex questionformats to test reasoning and synthesis capabilities. Each queryhas a time budget of 30 seconds, which also poses an efficiencychallenge to the candidate solutions.In detail, the CRAG challenge consists of three tasks:",
  "(3) Task #3: End-to-End RAG. 50 web pages and Mock APIsaccess per question are provided to select and integratethe most important data, reflecting real-world informationretrieval challenges": "The authors team, db3, participates in the contest and achievesfirst place in the three tasks, gaining a score of 28.4%, 42.7%, and47.8%, respectively. This paper describes the authors solution tothe three tasks. The difference between the three challenges is thatthe information sources are different. Since Task #2 and Task #3are provided with both sources, we describe the solution of thesetwo tasks together. Our code is available on GitLab 1.In the remainder of the paper, we discuss the web retrieval mod-ule and related model adjustment in Sec. 2, and we discuss theknowledge graph extraction module and related model adjustmentin Sec. 3. We conclude our work and look into future works inSec. 4.",
  ": Illustration of Task 1 framework": "We use the CharacterTextSplitter from LangChain3 to split the textinto chunks.Parent-Child Chunk Retriever. The Retriever ranks the textchunks by calculating the similarity between the query and eachtext chunk. Specifically, the retriever analyzes the keywords alongwith semantics within the query and matches them with the contentof the text chunks. We refer to the open-source retriever leader-boards for retriever selection to ensure the most effective retrieveris used. The final candidates are bge-base-en-v1.5 and bce-embedding-base_v1 , producing similar results. The submittedversion is based on bge-base-en-v1.5.Since relatively smaller chunks have better retrieval precisionand relatively larger chunks retain more information, we use theParentDocumentRetriever from LangChain to manage a parent-child chunk split. We use the relatively small child chunks to retrievefrom the question and the ParentDocumentRetriever to maintainthe inclusion relationship. The smaller child chunks, such as in-dividual sentences, are used for retrieval, while the larger parentchunks, which contain these retrieved child chunks, are typicallywhole paragraphs and are fed into the RAG system.In the contest, larger parent chunk size will result in a largercontext for LLM, leading to more inference time. So, to balance thetime issue, we select parent_chunk_size=700,child_chunk_size=200 as a baseline. Under the submission con-straints, we tune these hyperparameters, finding thatparent_chunk_size=500,1000,2000 is also acceptable.Reranker. The results from the retriever can be regarded as acoarse selection. In this challenge, we set the retriever to return_ text chunks. These chunks contain potentially relevantinformation, but not all are equally important or relevant.To more effectively utilize this information within the limitedcontext, we introduce a reranker model. The reranker model per-forms a secondary screening by more finely evaluating and rankingthe _ data chunks returned by the Retriever. Through thisdetailed screening process, we can identify the most valuable andrelevant chunks, ultimately selecting the top _. These top_ chunks will serve as the basis for further processingand answering the questions, ensuring that the our answers aremore accurate and reliable.We refer to the open-source reranker leaderboards for rerankerselection. The final candidates are bge-reranker-v2-m3 andbce-reranker-base_v1 . The results are rather similar. The sub-mitted version is based on bge-reranker-v2-m3. To fit in the LLM context and given a limited running time, we setthe recall number for the retriever _ = 50. For the numberof chunks we fed to the LLM _, we set it according tothe parent_chunk_size. For example, we take _ = 5 ifparent_chunk_size=2000, and we would take _ = 10 ifparent_chunk_size=1000.",
  "Public Data Pathway": "A regular web retriever usually suffers greatly from insufficientinformation and misinformation. For some of the stable facts, wecan gather public data to provide additional information. This infor-mation is preprocessed into a paragraph, which is combined withthe content of the web search to help the LLM answer the question.This information for different domains is constructed differently.For the movie domain, we preprocess the Oscar award information4 and the Full MovieLens5. For finance, we preprocess the currentpe-ratio, market cap and eps stats for every current stock in Amer-ica. For music, we preprocess the Grammy award information6.Due to the limited information in other domains, no preprocessingis conducted. Specifically, we serialize the entitys table/json datainto natural language format using the structure \"The [entity at-tribute] is [value of the entity attribute].\" Specifically, we serializethe entitys table/json data into natural language format using thestructure \"The [entity attribute] is [value of the entity attribute].\"For example, for a specific movie, the preprocessed format is: Thetitle is \"Rain Man.\" The director is Barry Levinson. The lead actors areDustin Hoffman and Tom Cruise. The release year is 1988...We store the preprocessed information using the correspondingentities as keys and then utilize LLM to locate the query entitiesthrough in-context learning. Specifically, we first locate the problemdomain based on Domain Prompt: [{\"role\": \"system\", \"content\": \"You are an assistant expert inmovie, sports, finance and music fields.\"},{\"role\": \"user\", \"content\": \"Please judge which category thequery belongs to, without answering the query. You can onlyand must output one word in (movie, sports, finance, music).If the question doesnt belong to movie, sports, finance, music,please answer other.Question: {query_str}Answer: \"}]. Because open domain questions often include content from otherdomains, we use \"other\" instead of \"open\". Next, we design differentprompts for various domains to query the entities. For example, formovie Entity Prompt:",
  "Winning Solution For Meta KDD Cup 24": "answer I dont know. There is no need to explain the reason-ing behind your answers. \"},{\"role\": \"user\", \"content\": Given a query about movies, returnthe title of each movie in below formats.If multiple movie names are involved, connect with &&.#Examples:Question: which movie was created first, a walk to rememberor the notebook?Answer: a walk to remember && the notebook......#Query:Question: {query_str}Answer: \"}]. Regarding how the entities in LLMs responses are linked to entitiesin the dataset, the required level of matching varies based on thecharacteristics of the question domain. Matching criteria rangefrom strict to lenient, including exact character matches, substringinclusion, or similarity comparisons using embedding models.",
  "LLM Inference Module": "According to the contest rules, the correct answer will be awarded1 point, and the incorrect answer will be penalized for 1 point. So,a significant challenge during the contest is reducing hallucina-tions and dealing with invalid questions. We will present our LLMinference module to the two challenges.Base model. We follow the contest instructions to use the LLamaseries LLM 7. Considering the limited running time, we use theLlama-3-8B-instruct model as the base model.Basic Query Prompt. We use the following basic prompt to generate answers: [{\"role\": \"system\", \"content\": \"You are a helpful and honestassistant. Please, respond concisely and truthfully in {to-ken_limit} words or less. Now is {query_time}\"},{\"role\": \"user\",\"content\": \"Context information is below.{context_str}Given the context information and using your prior knowl-edge, please provide your answer in concise style. End youranswer with a period. Answer the question in one line only.Question: {query_str}Answer: \"}] where token_limit is the limit to the answer that we want to control(as an answer longer than 75 tokens will be truncated.), query_timeis the time when the question is asked, which is crucial in real-timequestions, context_str is formed by combining data from publicretrieval and web retrievals using <doc> tokens, thentruncated based on a maximum token limit of 4000 (note that theprevious retrievers chunk size is based on characters, while here itis based on tokens), query_str is the query.Reduce Hallucination using Prompt Control. Hallucinationcan partly be controlled by prompt design. Certain instructions inthe prompt can hold the LLM from generating wrong facts. E.g., we",
  "can use the following controlled prompt to generate higher-quality answers:": "[{\"role\": \"system\", \"content\": \"You are a helpful and honestassistant. Please, respond concisely and truthfully in {to-ken_limit} words or less. Now is {query_time}\"},{\"role\": \"user\",\"content\": \"Context information is below.{context_str}Given the context information and using your prior knowl-edge, please provide your answer in concise style. Answerthe question in one line only.If the question is based on false prepositions or assumptions,output \"invalid question\". For example, Whats the name ofTaylor Swifts rap album before she transitioned to pop? (Tay-lor Swift didnt release any rap album.)If you are not sure about the question, output \"i dont know\"Question: {query_str}Answer: \"}] Using to control the generation can lead to better results,as for questions hard for the LLM to answer, it will probably avoidthe penalty. For some invalid questions, the tuned model may pointout that the questions are based on false prepositions. However,the overall results are not satisfying. So, we dont directly use thismethod to control hallucination.Reduce Hallucination from Fine-tuning. Usually, through suf-ficient supervised fine-tuning (SFT), its possible to make the LLMperform better on a particular task. So, we try fine-tuning the basemodel to reduce hallucination further in the contest.Through experiment, we find out that using to generateanswers will reduce hallucination, while in the meantime, somequestions which can be answered correctly using will beanswered wrongly with \"i dont know\". So, we hope we can leveragethe whole potential of the RAG system while hindering most of thewrong answers.Its clear that for some continuing changing facts, its impossiblefor the LLM to answer correctly if not provided with the fact inthe context_str. Our intuition is that we hope the LLM can answercorrectly for the facts contained in the context_str. For the factsthat are not contained in the context_str but in the LLMs internalknowledge, we hope the LLM can answer correctly, too. For theother queries, which are out of the RAGs capabilities, we hopethe LLM can answer \"i dont know\" honestly. As common sense,such patterns may be learned by the LLM. For example, for thecontinuing changing finance problems, the answer accuracy isclose to 0, so the LLM may learn to answer honestly \"i dont know\"for such questions.So, we follow the following steps to generate the labels for SFTto meet our intuition: 1. For the queries identified in the ground truth as invalidquestions, we set the labels to \"invalid question\", hopingthat the model can possess the capability to find questionsbased on false premises.",
  "For the queries with context_str that the LLM regardsas irrelevant to the ground truth answer, we label thesequeries with \"i dont know\", as these questions can hardlybe answered correctly": "3.2.2 For the queries with context_str that the LLM re-gards as relevant to the ground truth answer, we label thesequeries with the ground truth answer. We hope the fine-tuned LLM can have a stronger comprehension ability fromthe noisy context containing hints of the correct answer. We move the fine-tuning part after constructing the trainingdata as above. Considering limited computation resources, we useLoRA to fine-tune the base model. Using LoRA models hasanother advantage. We can fine-tune several LoRA models eachresponsible for a subtask. Since LoRA parameters are tiny comparedwith the LLM parameters, its easy to switch between the subtasks,which is time-saving. We tune the basic model for 2-3 epochs onthe training set. The tuning hyperparameters can be found in theappendix. After tuning, there are three significant improvements:",
  ". The LLM can deal with some false premise cases, savingmany points": "Inference Acceleration. We use vLLM to accelerate the inferenceprocess 9. However, though the latest vLLM library supports theLoRA framework, the graphics card driver in the test environmenthas some compatibility issues. So, we have to load each LoRA modelcompletely when we aim to switch between multiple LoRA models.Each query can share the switching time through batch inferenceto meet the time requirement. Our submission contains versionsthat use and do not use vLLM. If the compatibility issue is solved,introducing vLLM can save much time.",
  "Solution to Task #2 and Task #3": "In this section, we will first introduce the main framework of oursolution for Task #2 and Task #3. Then, we will propose our knowl-edge graph retrieval module based on a set of regularized APIs andAPI generation using a tuned LLM. The web retrieval and answergeneration part of Task #2 and Task #3 is similar to the ones insolution to Task #1, which will be presented briefly in this section. 8For long answers generated by Llama3, it appears that judging also by Llama3 wouldbe inaccurate. The answer may be more favored by its generated model. As for shortanswers, this problem is relieved greatly.9",
  "Framework for Solving Task #2 and Task #3": "We observe that the information extracted from web pages is noisierthan the information from the Mock APIs. Therefore, our frame-work separates the answer from the Mock API and web pages, andwe prioritize the results based on the Mock API results. Once theresults based on the Mock API content are not \"i dont know\", weaccept the result and output directly. The framework of Task #2and #3 is illustrated in .Notably, the web retrieval part for Task #3 is slightly differentfrom those in Task #1 and Task #2. Since Task #3 has 50 web pages,the time budget should be taken into consideration. In the contest,a page snippet is provided for each web page. We use the rerankerto find the top five related web pages based on the snippets, whichmakes the overall running time for the retriever acceptable, and thenprocess them according to the method used in Task 1. Additionally,we dont use public data in Task #2 and #3 because its basicallycovered by Mock APIs.",
  "Knowledge Graph Retrieval Module": "A Mock API knowledge graph is provided in Task #2 and Task#3. We will describe how we extract useful information from thisAPI or knowledge graph. The main idea of our knowledge graphretrieval module is to use LLMs to generate a series of API calls,which extract the specific information the query is asking about.Regularization of the API. Though the APIs provided containrich information, its quite hard to locate the exact informationwe want. The exact location of the information can be found byexecuting the code generated by the LLM. However, the code gen-eration capability of a local 7B LLM is rather limited. So we designa regularized version of API. For each query, only one (or several)APIs are generated. From the generated regularized APIs, we use aparsing system to get the generated results. The results are thenconverted to natural language to form the output of our retrievalmodule.We take the movie domain as an example. The original movieAPI consists of the following API calls:",
  "get_year_info (year) movie_list(list); oscar_awards(list)": "This actually forms a relational database behind the scenes. Thereare two relational tables: the PERSON table and the MOVIE table.For instance, the PERSON table has the columns: name, birthday,and the MOVIE table has the columns: title, release_date, origi-nal_title, original_language, budget, revenue, rating, genres, year.There are three additional tables that have foreign keys referring tothe PERSON table and MOVIE table, which records the relationshipbetween person and movies: the CAST table, the CREW table andthe OSCAR table. Each entity of the five tables can be constructedusing the API calls. So theoretically, we can use SQL language toquery information from this relational database, and convertingthe query to SQL language is a typical text2sql task , which is",
  "Hallucination": "Tuned LLM API Prompt Step 1: Retrieve on KG Step 2: Retrieve on Web same as Task 1 Q1: Whats the latest film that walt becker has directed?Q2: Which one of these came out earlier, the greater meaning of water or small town ecstasy? API for Q1: get_movie_person_crew(None,\"walt becker\", eq(job, \"Director\")); sort(None,-year)[\"movie_name\"]API for Q2:get_movie(\"greater meaning of water\")[\"release_date\"]; get_movie(\"small town ecstasy\")[\"release_date\"] Parsing content for Q1:the top 1 movies sorted by release date participated by walt becker are: 'Clifford the Big Red Dog.Parsing content for Q2:the release date of The Greater Meaning of Water is 2010-07-06', 'the release date of Small Town Ecstasy is 2002-01-01",
  ": Illustration of Task #2 and #3 framework": "well studied. However, its impossible to call the API multiple timesto form a relational database and execute SQL on it due to timebudget. So, we design a new type of regularized API that is easyto parse and execute and leverages the characteristics of relationaldatabases.For the movie domain, we set the following API:",
  "get_movie(movie_name,condition)[key_name], which isequal to SELECT key_name FROM MOVIE WHERE conditionand title=movie_name": "get_movie_person_X(movie_name,person_name,condition)[key_name], (here X=CREW, CAST or OSCAR), whichis equal to SELECT key_name FROM MOVIE, PERSON, XWHERE condition and title=movie_name, name=person_name. Using these three regularized APIs, most of the information thequery wants in this contest can be extracted with only a few APIcalls. These APIs can also be easily evaluated by the original APIsthrough a parsing system. More importantly, these APIs are easyto generate for the LLM, as essentially, only template selection andextraction of entity names have to be done by the LLM. There areno complex codes or SQL generation involved, which may lead tobetter performance.We design different APIs for the five different domains, but thedesign choices are all similar. The details of these APIs can be foundin our released codes.Details of the API System. How to make the conditions work inthe above system is a question. Answering some of the questions inthe contest involves a slight modification of the API system above.For instance, querying the latest Spielberg film actually requires thesort function. For some queries in the post-processing questions,",
  "We can use sort(condition,sort_key_name) to get a sortedlist which satisfies such condition, and the list is sortedusing the sort_key_name. If we want descending sort, wecan use -sort_key_name": "These are all simplified operators for coded functions or func-tions in SQL. Other functions include using * to represent the outputof the last query and using * in another query, which is similar tothe sublist function in SQL. This can perfectly fit the multi-hopquery scenario. Its a pity that we havent finished this part dueto limited time in this contest, and we look forward to developingsuch functions in future systems.Parser. We manually program the parser to meet the requirementsof most queries in the development set. This may affect the gen-eralization capability of this system, so using the standard SQLexecution engine may be a better choice if the relational databasesare presented. The output of the parser is converted to natural lan-guage so that even if irrelevant information is extracted, the LLMmay be aware of that and refuse to answer.Examples of New Regularized APIs. Here are three ideal exam-ples of the new regularized API in the movies domain:",
  "Whats the latest film that walt becker has directed? get_movie_person_crew(None,\"walt becker\", eq(job, \"Direc-tor\")); sort(None,-year)[\"movie_name\"]": "An example of the full pipeline can also be found in .API Generation. After setting down the rules and parser of thenew regularized API system, we move to solving the API generationproblem. We use prompts to help the LLM generate as many validAPIs as possible to help us extract information. Neglecting thesystem message, the API generation prompt _ is as follows: You are given a query about movies, and several APIs to getinformation from a database How to collect useful informa-tion from the database using the given APIs.The schema of entities is as follows:{Schema_info}The API rules are below:{API_rules}Here are some examples:{ICL_examples}Generate the answer only using the information from thequery. Please strictly follow the format in the examples andAPIs, you do not have to provide the code, only the use of APIin the examples. The only allowed format is multiple linesof get_X,sort. (sort is optional) Please complete the answeronly:Query:{query_str}Answer: where Schema_info are some descriptions about the underlyingrelational database schema, restricting the LLM to generate keynames in the relational tables, API_rules are the rules for generatingthe API described in the above subsection, query_str is the question,and ICL_examples are some in-context learning selected examplesof query and API pairs.Here, the in-context learning examples are selected manuallyiteratively. First a few examples are selected, and we generate 100examples using the LLM. Then the queries with the wrongly gener-ated examples are added to the examples. Notably, selecting relevantICL examples may be extremely effective in this scenario, as forsimilar queries, only the entity names have to be substituted. Wehavent realized this function, but we believe it may have greatresults. Details of this prompt for different domains can be foundin our source code. Here, we present some details of the prompt formovies in the Appendix.Fine-tuning for API Generation. Through experiment, we ob-serve that using _ still requires the LLM to have relativelystrong capabilities. Strong LLMs, e.g., GPT-4, perform better than the local Llama3 8B. So we hope fine-tuning the local LLM canhelp boost the performance of API Generation. For the fine-tuningground truth data, We use GPT-4 and _ to generate a firstversion of ground truth APIs for convenience. Then, we manuallylabel the ground truth APIs for higher quality. We also use LoRA tofine-tune our base Llama3 model, as we need to efficiently switchbetween different LoRA parameters under the time budget. We alsofine-tune the base model for 2-3 epochs, and the hyperparametersare also listed in the Appendix.",
  "Conclusion": "The Meta KDDCup 24 competition is a unique challenge due to thevarious types of information sources and the changing facts, whichare difficult for the LLMs. We have presented how we addressedthese challenges successfully in all three tasks of the contest. Oursolution for Task #1 is a framework of web or open-data retrievaland tuned LLM for question answering. The solution to Task #2and Task #3 is based on a regularized API set for domain questionsand the API generation method using tuned LLM. We will furtherlook into the balance of efficiency and effectiveness in RAG and arefined API parsing system for RAG to extract information fromstructured sources in the future.",
  ". The Twelfth International Conference on Learning Representations, ICLR 2024,Vienna, Austria, May 7-11, 2024. OpenReview.net": "Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024.BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity TextEmbeddings Through Self-Knowledge Distillation. arXiv:2402.03216 [cs.CL] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, SheanWang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of largelanguage models. arXiv preprint arXiv:2106.09685 (2021). Hugo Larochelle, MarcAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, andHsuan-Tien Lin (Eds.). 2020. Advances in Neural Information Processing Systems33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,December 6-12, 2020, virtual.",
  "Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. 2023.C-Pack: Packaged Resources To Advance General Chinese Embedding.arXiv:2309.07597 [cs.CL]": "Xiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla, Xiangsen Chen, SajalChoudhary, Rongze Daniel Gui, Ziran Will Jiang, Ziyu Jiang, Lingkun Kong, BrianMoran, Jiaqi Wang, Yifan Ethan Xu, An Yan, Chenyu Yang, Eting Yuan, HanwenZha, Nan Tang, Lei Chen, Nicolas Scheffer, Yue Liu, Nirav Shah, Rakesh Wanga,Anuj Kumar, Wen tau Yih, and Xin Luna Dong. 2024. CRAG ComprehensiveRAG Benchmark. arXiv:2406.04744 [cs.CL] Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, JamesMa, Irene Li, Qingning Yao, Shanelle Roman, et al. 2018. Spider: A large-scalehuman-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. arXiv preprint arXiv:1809.08887 (2018).",
  "Neglecting the system message, _ is as follows:": "INSTRUCTIONS =# Task:You are given a Question, a model Prediction, and a list ofGround Truth answers, judge whether the model Predictionmatches any answer from the list of Ground Truth answers.Follow the instructions step by step to make a judgement.1. If the model prediction matches any provided answers fromthe Ground Truth Answer list, \"Accuracy\" should be \"True\";otherwise, \"Accuracy\" should be \"False\".2. If the model prediction says that it couldnt answer thequestion or it doesnt have enough information, \"Accuracy\"should always be \"False\".3. If the Ground Truth is \"invalid question\", \"Accuracy\" is\"True\" only if the model prediction is exactly \"invalid ques-tion\".# Output:Respond with only a single JSON string with an \"Accuracy\"field which is \"True\" or \"False\".# Examples:",
  "Neglecting the system message, _ for the movie domain isas follows:": "You are given a query about movies, and several APIs to getinformation from a database How to collect useful informa-tion from the database using the given APIs.The schema of entities are as follows:Movie:- title (string): title of movie...- year (string): year of the moviePerson:- name (string): name of person- birthday (string): string of persons birthday, in the formatof \"YYYY-MM-DD\"Besides we have the concat tables for the concat of these twobasic entities:Cast Movie Person: list of cast members of the movie andtheir roles. The schema of the cast member entity is:-movie_name:name of the movie,...-year(string):the year of castingCrew Movie Person: list of crew members of the movie andtheir roles.-movie_name:name of the movie,...-year(string):the year of crewingOscar info: list of oscar awards, win or nominated, in whichthe movie was the entity. The schema for oscar award entityare:year (int): year of the oscar ceremony,...",
  "winner (bool): whether the person won the award": "The API rules are below:1.you can use cmp(key_name,value_name) to set a condi-tion, the cmp here can be neq,eq,ge,le, which represents notequal,equal, greater, lesser respectively. e.g eq(gender,male),which means the contion of gender to be male, ge(revenue,10),which means the condition of revenue greater than 10. thecondition can be a list of multiple conditions,e.g. [eq(gender,\"male\"),eq(character,\"batman\")] you can addcondition to the last parameter of get_X_info(X_key_value,condition)2.you can use get_movie(movie_name,condition)[key_name]to search movie_name for the most relevant result undersuch condition and query the key_name attribute of it. thekey names valid to use with get_movie_info is the key of themovie entities....8.By default we output the first element of one list, however ifyou want it all, you can add ALL in the front of the command,e.g. ALL, ALL get_movie_person_crew(\"batman\",\"Jack\",1997),represent get ALL Jack crews of batman movies in 1997. Youcan use [:n] to represent take the first n result of the list Here are some examples:{ICL_examples}Generate the answer only using the information from thequery. Please strictly follow the format in the examples andAPIs, you do not have to provide the code, only the use of APIin the examples. The only allowed format is multiple linesof get_X,sort. (sort is optional) Please complete the answeronly:Query:{query_str}"
}