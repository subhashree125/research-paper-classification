{
  "ABSTRACT": "Generating city-scale lane-level maps faces significant challengesdue to the intricate urban environments, such as blurred or absentlane markings. Additionally, a standard lane-level map requiresa comprehensive organization of lane groupings, encompassinglane direction, style, boundary, and topology, yet has not beenthoroughly examined in prior research. These obstacles result inlabor-intensive human annotation and high maintenance costs. Thispaper overcomes these limitations and presents an industrial-gradesolution named DuMapNet that outputs standardized, vectorizedmap elements and their topology in an end-to-end paradigm. To thisend, we propose a group-wise lane prediction (GLP) system thatoutputs vectorized results of lane groups by meticulously tailoring atransformer-based network. Meanwhile, to enhance generalizationin challenging scenarios, such as road wear and occlusions, as wellas to improve global consistency, a contextual prompts encoder(CPE) module is proposed, which leverages the predicted results ofspatial neighborhoods as contextual information. Extensive experi-ments conducted on large-scale real-world datasets demonstrate thesuperiority and effectiveness of DuMapNet. Additionally, DuMap-Net has already been deployed in production at Baidu Maps sinceJune 2023, supporting lane-level map generation tasks for over 360cities while bringing a 95% reduction in costs. This demonstrates",
  "These authors contributed equally to this work.Corresponding authors": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain. 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08",
  "Lane-Level Map Generation; End-to-End; Lane Group; Baidu Maps": "ACM Reference Format:Deguo Xia, Weiming Zhang, Xiyan Liu, Wei Zhang, Chenting Gong, JizhouHuang, Mengmeng Yang, and Diange Yang. 2024. DuMapNet: An End-to-End Vectorization System for City-Scale Lane-Level Map Generation. InProceedings of the 30th ACM SIGKDD Conference on Knowledge Discoveryand Data Mining (KDD 24), August 2529, 2024, Barcelona, Spain. ACM, NewYork, NY, USA, 10 pages.",
  "INTRODUCTION": "Lane-level map, as a crucial layer of the high-definition map, offerscritical prior information for autonomous driving, facilitating be-yond visual line of sight (BVLOS) perception, path planning, anddecision-making with globally consistent road data. Specifically,lane-level map models the real world within decimeter level. Thisindicates that they not only depict the fundamental structure andlayout of roads but also provide lane-level details, including lane linegeometry, lane marking style, as well as connection topology, etc.Building on this, lane-level navigation, a pioneering advancementin precise travel guidance, has been extensively deployed to assistpublic travel by providing detailed, high-precision map elementsand suggested routes (see ). Additionally, with its higherprecision and advanced route planning capabilities, lane-level mapwill potentially benefit a wide range of geographical tasks, such asgeo-object change detection , traffic condition prediction ,estimated time of arrival prediction (ETA prediction, a.k.a., traveltime estimation) , and road extraction at Baidu Maps.",
  "Crosswalk": ": DuMapNet introduces a learning-based methodol-ogy for lane-level map vectorization. Our proposed methodincorporates a scheme based on contextual prompts and com-ponents dedicated to group-wise lane prediction. With theseadvancements, DuMapNet achieves cost-effective generationof city-scale vectorized maps and significantly supports vari-ous applications, such as lane-level navigation in Baidu Maps. The task of lane-level map generation can be formulated as con-structing and updating the core geographic elements that meetlane-level precision, given abundant road data. These geographic el-ements primarily include open-shape elements (e.g., lane lines, stoplines, etc.) and closed-shape elements (e.g., crosswalks). Moreover,as an efficient and standardized management unit in high-definitionmap, lane group plays a crucial role in onboard navigation and au-tonomous driving. It can be defined as a set of one or more lanes ona road segment perpendicular to the direction of travel (see). Within a lane group, the number of lanes remains con-stant, and all lanes belong to the same road segment with the samedirection of travel. Given this definition, the lane group emerges asan exceptionally convenient and efficient unit for driving guidance,markedly improving vehicle interaction with urban environments.Consequently, this paper introduces a learning-based solution thatdirectly generates final standardized results in an end-to-end man-ner. This method effectively supersedes existing techniques thatrely on manual post-processing to construct lane groups.Traditional map generation solutions are often costly and labor-intensive as they require trained experts to manually annotategeographic elements. To improve efficiency with less human effort,leveraging advancements in computer vision for map generationhas become a viable approach. These algorithms can be roughlycategorized into segmentation-based methods , lanedetection-based methods , as well as vectorization-basedmethods . Specifically, segmentation-based methods aresuboptimal since they often require a series of post-processingstrategies, such as thinning and fitting, to convert mask into vec-torized map. Lane detection-based methods are usually limited interms of extensibility and flexibility regarding map element types.While the vectorization-based solutions have achieved commend-able results, they still exhibit limitations in prediction accuracy,post-processing logic and handling complex road scenarios, suchas road wear and vehicle occlusion. Moreover, such onboard ap-proaches are often constrained by computational power and local construction patterns, preventing them from meeting the precisionand global consistency required for city-scale lane-level map.To fully explore the paradigm of large-scale lane-level map gen-eration, we propose an automatic industrial-grade offboard solutiontermed DuMapNet. Given a birds-eye-view (BEV) image, DuMap-Net can unify the modeling of polyline-style and polygon-stylemap elements as a set of points. To significantly improve the pre-diction results for difficult scenarios such as road wear, occlusions,and complex intersections, as well as the connections of vectoriza-tion results among frames, we propose the contextual promptsencoder (CPE) module. By using the spatial prediction resultsof the current BEV images neighborhood as prompt information,CPE significantly enhances the geometric and category consistencyof the prediction results in a larger receptive field. To avoid theerror accumulation effect and weak generalization of traditionalmulti-stage map-making methods, and considering the require-ments for standardized map construction, we design a group-wiselane prediction (GLP) to output the vectorized results of lanegroups through mutual constraints between lane group polygonsand lane lines, without the need for complex post-processing logic.Finally, to achieve an end-to-end large-scale map generation mode,we develop the topology prediction module, which predicts thelane line topological relationships between BEV images, enablinglarge-scale map correlation. Our key contributions to both the re-search and industrial communities are as follows: Potential impact: We introduce DuMapNet, an end-to-endvectorization modeling framework, as an industrial-gradesolution for city-scale lane-level map generation. DuMapNethas been successfully deployed in production at Baidu Maps,supporting lane-level map generation for over 360 cities andrealizing a 95% reduction in costs. Novelty: DuMapNet represents a new paradigm for city-scalelane-level map generation task, achieving end-to-end predic-tions from birds-eye-view (BEV) images to vectorized resultsthat meet cartographic standards. The novelty lies in eachstage, from the unified vectorization modeling, the group-wise lane prediction system, the contextual prompts encoder,to the topology prediction module, making the lane-level mapgeneration task highly automatic and cost-effective. Technical quality: Extensive qualitative and quantitativeexperiments are performed on large-scale, real-world datasetscollected from Baidu Maps, which demonstrate the superiorityof DuMapNet. The successful deployment of DuMapNet atBaidu Maps further shows that it is a practical and robustsolution for city-scale lane-level map generation.",
  "DuMapNet2.1Preliminaries": "The task of lane-level map generation from BEV image is definedas follows: given a BEV image collected from vehicle-mountedsensors as input, the network is supposed to predict the vectorizedmap elements . Next, we will describe the data preparation andunified vectorization.Data Preparation. Different from most onboard methods thatoperate on BEV features using multi-view images as input, follow-ing , our offboard approach is built upon the BEV image",
  "Pe": ": Overall architecture of DuMapNet. DuMapNet processes the entire city-scale land area using a sliding window approach.For each local area, an image encoder is utilized to extract image features from the BEV image. Meanwhile, we propose a novelContextual Prompts Encoder (CPE) to encode the predictions of adjacent scanned areas. To achieve Group-wise Lane Prediction(GLP), we meticulously tailor key network components, including the query, decoder, and prediction heads. Consequently, thenetwork is capable of generating a vectorized map, which encompasses vectorized elements and their topology. Additionally,two auxiliary predictions are generated: the use of group polygons aids in the organization of lane groups, while foregroundsegmentation enhances lane point localization. For detailed illustrations, please refer to . produced using multi-view images, point cloud data, and vehicleposes information. The advantages primarily lie in two aspects:first, the regional global information can be fully utilized, suchas geometric smoothness constraints, semantic correlations, andglobal precision consistency; second, conducting multi-trip datacollection can alleviate the inevitable challenges such as precisionbias and dynamic occlusion. Instead of appealing to heavy labelingmanpower, we solve the large-scale annotation problem using theBaidu Map Database in an automatic fashion. Specifically, givena BEV image with resolution, with the spatial resolution4 4 for each pixel, it covers /25 meters by /25 metersregion with a certain geographic coordinate range. First, we in-dex the instance geometries, labels, and lane group IDs withinthat range from the database. Second, based on the lane groupIDs, the instance geometries are organized into a list format at thegranularity of lane groups and mapped to pixel coordinate system.Simultaneously, we compute the minimum bounding rectangle ofall instances contained within each lane group to create the grouppolygon. Finally, based on the spatial relationship between BEVimages, neighborhood IDs are added to the ground truth. For betterunderstanding, we have released a demo for reference on GitHubat Vectorization. We define a unified vectorized repre-sentation for the core geographic elements in each local land area.Formally, given a BEV image , we denote the corresponding lane-level vectorization as = {}=0, where denotes the numberof the lane groups in the local land area. Each lane group iscomposed of a set of geographic elements and element style R , where is a one-hot vector with element stylesin total. We thus denote a lane group as = {, }=0, where is the element number in a lane group. Next, the point set ofeach element instance is denoted as = {}=0, where is the",
  "Overall Architecture": "The city-scale lane-level map generation is inherently complex,requiring a comprehensive organization of lane groupings and ex-isting methods only generate partial elements of lane groupings.Meanwhile, DuMapNet is the first end-to-end solution to achievecity-scale lane-level map generation, realizing practical and effec-tive industrial gains.Specifically, illustrates the overall architecture of ourproposed DuMapNet. To obtain a city-scale lane-level vectorizedmap, DuMapNet processes the entire land area using a slidingwindow approach following a zig-zag scan sequence. The modelsinputs contain two parts: a BEV image and contextual prompts.Specifically, the BEV image is obtained by the aforementioned datapreparation process, providing abundant appearance features of thelocal land area. We employ an image encoder that contains a back-bone network and a Feature Pyramid Network (FPN) to extract BEVfeature R384384 from the BEV image . Meanwhile, to enablespatial coherent lane-level vectorized predictions over adjacent landareas, we propose to take the predicted vectorized map of adjacentscanned areas as the additional input of DuMapNet. We furthertailor a contextual prompts encoder (CPE) to realize an effectiveencoding for the predictions of adjacent scanned areas. Moreover,we devise a query combination that contains a set of hierarchicalqueries for lane line prediction, one foreground segmentation query,and a set of queries for lane group polygon prediction. In the De-coder, the proposed query combination interacts with both the BEVfeature and the contextual prompt embeddings derived from CPE.Finally, we construct multiple tasking prediction heads to facili-tate various predictions, where the predictions include vectorized",
  "Contextual Prompts Encoder (CPE)": "Inspired by the recent success of prompt-based vision models , our proposed CPE adopts a simple yet effective architectureto encode both the geometric and semantic information of thevectorization results of adjacent land areas, providing contextualcues for the vectorization of the current land area during the slidingwindow operation. Formally, we define",
  "where denotes a set of adjacent land areas. 256": "denotes the prompt embeddings that interact with the featuresof intermediate layers in the decoder. Where , , denotethe number of total predicted groups, the number of element in-stances in each group, and the number of points in each instance,respectively.The architecture of CPE is illustrated in . Specifically, weadopt a shared MLP (Multi-Layer Percseptron) network to encodethe predicted style type for each element to serve as the semanticencoding. For the geometric information, we perform a shared po-sitional encoding sub-network to the coordinates of element points.This sub-network consists of sine and cosine functions with differ-ent frequencies as well as a subsequent MLP. Finally, the geometricand semantic cues are concatenated and then fed to an MLP, gener-ating the final prompt embeddings . Furthermore, we introducea memory mechanism into CPE to realize a long-term feature de-pendence. Specifically, we adopt FIFO queue as a memory bank tostore the prompt embeddings of previous frames of local landareas. The memory bank efficiently stores prompt embeddings ofthe remaining 1 neighboring frames. These embeddings arethen aggregated using a weighted sum operation within the CPE.The weights assigned to each stored frame are learnable parametersgenerated by intermediate layer in CPE, allowing the model to adap-tively focus on relevant information. This design effectively reducesnoise in the prompted information (e.g. prompts may contain pre-diction errors) while maintaining a lightweight architecture withminimal computational overhead. The memory bank is the coremodule of the CPE and directly reflects its performance. Finally, is obtained via learning an aggregation of the stored embeddings.",
  "Group-wise Lane Prediction (GLP)": "A lane group refers to a collection of lanes that share commoncharacteristics e.g. same style or are directed towards a commongoal or destination. Practically, lane groups are essential for pathplanning and navigation as they help in understanding complexroad structures. However, predicting an accurate group-wise laneis challenging since it demands sophisticated semantic analysisand geometrical reasoning. Particularly, localizing the endpoint ofa lane instance requires the knowledge of the style and topologychanging of the other lanes in the same group. To address this,we propose to use a polygon, namely group polygon to outline the boundary of a lane group. We further introduce an auxiliarytask in the network architecture to predict the group polygons.Since all the points of the predicted map elements are located in thegroup polygons, we propose an additional point-in-polygon loss tofacilitate group-wise lane prediction.In this section, we introduce the key components regardingto group-wise lane prediction (GLP), including the query design,decoder architecture, and the prediction heads.Queries. We design a query combination to flexibly encodestructured map information and perform hierarchical bipartitematching for both map element and group polygon learning. Specif-ically, we extend the hierarchical query scheme in MapTR and customize two set of queries, namely, element queries {}=0and group polygon queries { }=0. These two types of queriesadopt the same hierarchical query scheme that efficiently encodesinstance-level and point-level information. Moreover, we introducean additional foreground-background (FG) query for the auxil-iary task of semantic segmentation.Decoder. All map elements, group polygons, and segmentationmasks are simultaneously predicted using a unified Transformerstructure. The decoder is composed of several cascaded layers, eachincorporating a self-attention module, two cross-attention mod-ules, and an intra-instance self-attention module. The initial self-attention module is designed to enable hierarchical queries to ex-change information across the entire feature space. The subsequentcross-attention module facilitates interaction between hierarchicalqueries and BEV features. To enhance prediction accuracy and spa-tial consistency, an additional cross-attention module has been in-novatively introduced, with contextual prompt embeddings servingas input keys and values to interact with hierarchical queries. Lastly,the intra-instance self-attention module allows for interactions be-tween points within the same instance, thereby improving geomet-ric smoothness. Ultimately, after processing through the decoder,the hierarchical queries are effectively encoded into group-levelquery embeddings R 256, line-level query embeddings R 256, and a foreground embedding R1 256.Predictions. For predicting lane lines and lane group polygons,we input both map element and group polygon query embeddingsinto a shared classification branch and a shared regression branchto facilitate type classification and geometric property regression,respectively. For each predicted instance, the regression branchoutputs a vector of dimension R 2, representing the normalizedcoordinates of points.Furthermore, to enhance the performance of the classificationand regression branches, thereby improving prediction accuracyand accelerating training convergence, we propose a foregroundsegmentation branch. Instead of directly utilizing the BEV featuresfor segmentation, an individual foreground query R1256 isintroduced alongside the hierarchical queries. Following process-ing by the conventional decoder network and MLP encoding, theforeground query embedding interacts with the BEV feature togenerate a segmentation map.",
  "Fe": ": Topology prediction. The topology matrix is pro-duced as an additional output of the decoder to indicate theconnections between element instances in the currentland area and element instances in the contextual landareas. vectorization of single-frame BEV images. Predicting the topolog-ical relationships between frames is indispensable for our task.To address this, we propose to directly predict a topology matrixM R which indicates the connections between element instances in current land area and element instancesin the contextual land areas.Inspired by , we formulate topology prediction as a classifi-cation task, where the topology matrix is produced as an additionaloutput of the decoder. The architecture is outlined in andspecified in . Specifically, we adopt a similar sub-networkas the CPE to encoder the predicted map elements, producing anembedding R 256 that encapsulate both the predictedcoordinates and class information of instances. Subsequently, wecalculate the correlation of and the prompt embeddings thanadopt a MLP to produce the topology matrix. During inference,lane lines and lane group polygons are aggregated based on theirgeometric relationships, facilitating the generation of coherent lanegroup configurations.",
  "End-to-End Training": "In the training stage, for each frame, we apply the hierarchicalmatching scheme in MapTR to obtain pairs of the map-elementspredictions and the ground truths, denoted as { , }. Meanwhile,we adopt the same matching scheme to obtain pairs of the predictedgroup polygons and the ground truths, denoted as { , }.Based on the matching results and the correspondence betweenmap elements and their group polygons, we employ multiple task-specific losses to train our proposed DuMapNet in an end-to-endmanner:L = L + L + L + L + L(2) where L, L denotes the loss for learning map element and topol-ogy, respectively. Besides, L denotes the loss for learning grouppolygons while L is an additional point-in-polygon loss to fa-cilitate group-wise elements organization. Finally, we introduce aforeground segmentation loss L to enhance lane point localiza-tion. , , ,, are hyperparameters that strike a balance betweendifference losses. Next, we provide detailed illustrations for eachtype of loss function.Map Elements Learning. For each map element instance, weemploy aligned Focal Loss L for style classification and an L1regression loss L1 for point localization, respectively. Here wefollow MapTR and further apply a direction loss L to align",
  "L = L + L + L(3)": "Furthermore, inspired by Stable DINO , we adapt the alignedFocal Loss to enhance the alignment between classification scoreand localization quality in L. As shown in Equation. (4), L1distance of the -th matched pairs between prediction and corre-sponding ground truth is used as a positional metric to supervisethe training probabilities of positive examples. The classificationloss is thus formulated as:",
  "= || ||1(5)": "where is the probability for the -th predicted map element. and denote the number of positive and negative elements,respectively. Moreover, as for L, we apply the same loss functionas L for learning group polygons.Topology Learning. We define the topological relationship asa 2-class classification task i.e. connected or not. As the number ofconnected ones is significantly less than the number of unconnectedones, we apply a focal loss to supervise the topological associationmatrix prediction.Group-guided Auxiliary Supervision. We leverage the grouppolygons to provide auxiliary supervision for learning high-qualitygroup-wise lane lines. Specifically, this auxiliary supervision isdesigned based on the following observations: all the points ofthe lane lines should be located inside or on the boundary of theircorresponding group polygons. We thus propose a point-in-polygonloss as:",
  "where (, ) is the closest distance from point to any edge of": "the lane group polygon . During the training stage, we adopt theground-truth group polygons to punish predicted points locatedoutside their group polygon. During the inference stage, we sim-ply employ the predicted group polygons to check map elementslocations.Segmentation-guided Auxiliary Supervision. To improvethe accuracy of vectorized predictions, we choose a combination ofBinary Cross Entropy and Dice loss to calculate the loss betweenthe predicted foreground mask and the ground truthmask generated from ground truth lane lines:",
  "KDD 24, August 2529, 2024, Barcelona, Spain.Deguo Xia et al": "DuLD, consisting of birds-eye view (BEV) images and ground truthdata from six cities: Beijing, Guangzhou, Changchun, Changzhou,Chongqing, and Leshan. These cities were chosen for their var-ied urban scales and geographic features. The dataset from Bei-jing, Guangzhou, Changchun, and Changzhou was divided into atraining set and a validation set in a 9 : 1 ratio. Meanwhile, datafrom Chongqing and Leshan were used as the test set to evaluatethe models performance. Statistically, DuLD contains 134, 524 im-ages, spanning 8, 072 kilometers, with each image at a resolutionof 1536 1536 pixels. More details can be found in . Impor-tantly, to investigate the benefits of larger-scale data, we introduceDuLD-L, a dataset with one million paired images and correspond-ing ground truths, and evaluate DuMapNets performance on thisexpanded dataset.Evaluation Metrics. We adopt recall (R) and precision (P) toassess the quality of map construction at the instance level. Theevaluation considers category consistency, endpoint distance, andoverlap to determine if a pair of lane instances from ground truthand prediction match. Moreover, category consistency and IoU( > 0.5) are used for closed-shape elements such as crosswalks.Specifically, category consistency mandates that the instances be-long to the same category. Endpoint distance requires the L2 dis-tance between the start points and end points of the instancesshould be less than 3 meters, respectively. Overlap considers theparallel distance between instances. When calculating overlap, bothprediction instances and ground truth are first divided into multi-ple segments with 1-meter intervals. Then, the projection distancebetween segments is calculated. If the proportion of segments withprojection distance smaller than the threshold = {0.5, 1} ex-ceeds the threshold = {0.5, 0.8}, the prediction instance will beconsidered as true positive (TP). In the following experiments, weuse @, = to represent the recall at precision with threshold and . Lower values of threshold and higher values of threshold signify more stringent precision requirements.Implementation Details. Our model is trained using 16 NVIDIATesla V100 GPUs, with a batch size of 16. The AdamW optimizeris employed with a weight decay of 0.01 and the initial learningrate is set to 6 104 with cosine decay. The input images havea resolution of 768 768 pixels. For our architecture, we employResNet50 and HRNet48 as backbones. The default numberof instance queries, point queries and decoder layers are 50, 50 and6, respectively. As for hyper-parameters of loss weight, we set , ,, and to 1, 1, 0.2, 0.15 and 100, respectively. The inference timeis measured on a single NVIDIA Tesla V100 GPU with batch size 1.",
  "Evaluation": "Comparison with Baselines. We compare our DuMapNet withsegmentation-based method and other vectorization-basedmethods . As shown in , vectorization-based methodsachieve better results without complex post-processing logic. Inparticular, our DuMapNet outperforms the existing state-of-the-artmethod by a large margin (+2.66% when 1,0.8 = 90%) under thesame setting of ResNet50, indicating the effectiveness of our method.Surprisingly, our method achieves a further improvement of 4.99%by replacing the backbone with HRNet48 to obtain enhanced featurerepresentation. Furthermore, our method achieves 73.28% recall on",
  "All8,072134,524-": "one million of training data, demonstrating that as the training datavolume increases, our method can achieve greater advantages.The quantitative comparisons of different evaluation thresh-olds are summarized in . From the results, we observe thatDuMapNet consistently brings significant improvements. ,0.8 = 90% as an example, DuMapNet achieves better perfor-mance with +3.00% 5.04% recall gains on DuLD, which under-scores our methods ability to achieve superior geometric precisionand maintain category consistency. Particularly, as the projectiondistance decreases, the performance of all approaches experiencesa significant decline, yet our DuMapNet shows a slighter drop, in-dicating that our method is more robust and maintains superiorperformance at higher precision levels.Ablation Studies. In this section, extensive ablation studiesare conducted to systematically evaluate the key designs of ourDuMapNet. As shown in , Group I is the baseline withouta series of designs. From Group I and II, it is proved that addingthe task-aligned supervision can bring slight improvements by fos-tering better synergy between geometric learning and categoryidentification. Further analysis between Groups II and III revealsthat incorporating intra-instance self-attention results in a 0.99improvement under 1,0.8 = 95%. The transition from Group IIIto Group IV examines the impact of adding segmentation-guidedauxiliary supervision, showing marked improvements in all metrics,especially at higher precision levels. This outcome is expected asthe segmentation branch contributes to more fine-grained pixel-level modeling, enhancing semantic understanding and refiningprediction accuracy. The results from Group V highlight the sig-nificant role of the proposed contextual prompts encoder (CPE)module, showing a notable increase of 2.74% in recall at a highaccuracy level (1,0.8 = 95%). These findings demonstrate that CPE,by leveraging spatial prediction results from the area surroundingthe current BEV image, significantly enhances the geometric andcategory consistency of predictions across a broader receptive field.Analysis of generalization. To further demonstrate the gen-eralization of our method, five cities are additionally selected asthe test set. These cities are distributed across various regions,such as Harbin in northeastern China and Xian in northwesternChina, and exhibit diverse sizes, with Shanghai being a large first-tier city, while Zhongshan is a second-tier city. Finally, a totalof 5, 000 images were collected for evaluation. The experimentalresults are presented in . On the one hand, DuMapNet out-performs the existing state-of-the-art methods on all city test sets,demonstrating the effectiveness of our approach. On the other hand,DuMapNet shows superior generalization with less fluctuation in",
  "Visualization": "Qualitative results from the DuLD dataset are presented in and . DuMapNet not only performs well in simple scenesbut also predicts high-quality vectorized map elements in complexscenarios like intersections, road wear and occlusions. As shown in , DuMapNet exhibits significant advantages in terms oflane recall, lane accuracy, and endpoint accuracy. For example, asdemonstrated in the second row of , DuMapNet preciselycaptures both the geometry and category of lane lines in occlu-sion scenes, avoiding unnecessary lane line predictions. In addition, provides a visual comparison that underscores the effective-ness of group-guided supervision in enabling accurate prediction ofendpoint positions, even in scenarios with subtle visual differences,such as the dashed line and the segment of solid line. Group-guided",
  "Map Construction": "With the development of deep learning and BEV perception ,map construction is transitioning from a labor-intensive annotationtask to a model-based dense prediction challenge. Segmentation-based methods generate rasterized map by perform-ing BEV semantic segmentation. To build vectorized maps, HDMap-Net adopts a two-stage approach of segmentation followed bypost-processing to generate vectorized instances. As the first end-to-end framework, VectorMapNet utilizes an auto-regressivedecoder to predict points sequentially. MapTR proposes a uni-fied shape modeling method based on a parallel end-to-end frame-work, which has been followed by many works . MapVR applies differentiable rasterization to vectorized outputs toperforms precise and geometry-aware supervision. MapTRv2 further introduces auxiliary one-to-many matching and auxiliarydense supervision to speedup convergence. BeMapNet adopts aparameterized paradigm and constructs map elements as piecewiseBezier curves. PivotNet utilizes a dynamic number of pivotalpoints to model map elements, preventing the loss of essential de-tails. Different from the existing works, our proposed DuMapNetleverages neighboring map elements as prompts to guide the gen-eration of map elements in the current frame, which can enhancethe spatial consistency of the map elements.",
  "Lane Detection": "Lane detection plays a critical role in detecting lane elements inroad scenes and can be considered as a subtask of map construction.LaneATT utilizes an anchor-based deep lane detection model.CondLaneNet adopts a conditional lane detection model basedon conditional convolution and row-wise formulation. GANet formulates lane detection as a keypoint estimation and associationproblem. BezierLaneNet proposes a parametric Bezier curve-based method, which can model the geometric shapes of lane lines.PersFormer utilizes a transformer-based spatial feature transfor-mation module and unify 2D and 3D lane detection simultaneously.Different from these methods that primarily focus on lane elements,our proposed DuMapNet models map elements in a unified vec-torized form, which can detect open-shape map elements such aslanes, as well as closed-shape elements like crosswalks.",
  "DISCUSSION": "Before the deployment of DuMapNet, Baidu Maps relied heavily onlabor-intensive manual annotation processes that involved segmen-tation techniques and complex post-processing logic. This approachsignificantly increased operational costs and decreased efficiency.With the introduction of DuMapNet, now operational in over 360cities, production efficiency has seen a twenty-fold improvement,leading to a remarkable 95% reduction in costs.Despite DuMapNets impressive achievements, several challeng-ing issues remain unresolved and require further investigation. Forinstance, the model struggles in scenarios with extensive staticobstructions, such as long stretches of road with parked vehicles.Such conditions disrupt performance because the lack of visibleroad surface markings compromises the effectiveness of the con-textual prompts encoder. To address this challenge, integrating multi-source data may be an effective approach. In addition, gen-erating qualified map data from low-precision sources, such ascrowdsourced data, presents an intriguing challenge that meritsdeeper exploration in future work. Currently, leveraging the hightimeliness, broad coverage, and low cost of crowdsourced data formap updates represents a more reasonable paradigm. For example,crowdsourced data can provide timely updates for elements withlower accuracy requirements, such as style changes, or for dynamicchanges like construction or temporary road closures.",
  "CONCLUSIONS": "In this paper, we present an effective industrial solution for city-scale lane-level map generation. Specifically, we reformulate thistask as a vectorization modeling task that takes birds-eye-view(BEV) images as input and outputs standardized, vectorized mapelements and their topology in an end-to-end paradigm. We pioneerorganize the lane group using a learning-based methodology andaddress it through the proposed group-wise lane prediction (GLP)system that outputs vectorized results of lane groups by applyingmutual constraints between lane group polygons and lane lines,thereby eliminating the need for intricate post-processing logic.To improve the generalization in challenging scenarios, such asroad wear and occlusions, as well as to improve the continuityof vectorization results across frames, we present the contextualprompts encoder (CPE) module, which leverages the spatial predic-tion results from the surrounding area of the current BEV image ascontextual information. Extensive experiments conducted on thecollected large-scale real-world dataset from Baidu Maps demon-strate the superiority of DuMapNet. The successful deploymentof DuMapNet at Baidu Maps has significantly improved its perfor-mance. Since its launch in June 2023, DuMapNet served over 360cities while bringing a 95% reduction in costs.",
  "Syed Ammar Abbas and Andrew Zisserman. 2019. A geometric approach to obtaina birds eye view from an image. In Proceedings of the IEEE/CVF internationalconference on computer vision workshops. 00": "Li Chen, Chonghao Sima, Yang Li, Zehan Zheng, Jiajie Xu, Xiangwei Geng,Hongyang Li, Conghui He, Jianping Shi, Yu Qiao, et al. 2022. Persformer: 3d lanedetection via perspective transformer and the openlane benchmark. In EuropeanConference on Computer Vision. Springer, 550567. Wenjie Ding, Limeng Qiao, Xi Qiu, and Chi Zhang. 2023. Pivotnet: Vectorizedpivot learning for end-to-end hd map construction. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision. 36723682. Xiaomin Fang, Jizhou Huang, Fan Wang, Lihang Liu, Yibo Sun, and HaifengWang. 2021. SSML: Self-Supervised Meta-Learner for En Route Travel TimeEstimation at Baidu Maps. In Proceedings of the 27th ACM SIGKDD Conference onKnowledge Discovery & Data Mining. 28402848. Xiaomin Fang, Jizhou Huang, Fan Wang, Lingke Zeng, Haijin Liang, and HaifengWang. 2020. ConSTGAT: Contextual Spatial-Temporal Graph Attention Networkfor Travel Time Estimation at Baidu Maps. In Proceedings of the 26th ACM SIGKDDInternational Conference on Knowledge Discovery & Data Mining. 26972705. Zhengyang Feng, Shaohua Guo, Xin Tan, Ke Xu, Min Wang, and Lizhuang Ma.2022. Rethinking efficient lane detection via curve modeling. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition. 1706217070.",
  "Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie,Bharath Hariharan, and Ser-Nam Lim. 2022. Visual prompt tuning. In Euro-pean Conference on Computer Vision. Springer, 709727": "Giseop Kim, Sunwook Choi, and Ayoung Kim. 2021. Scan context++: Structuralplace recognition robust to rotation and lateral variations in urban environments.IEEE Transactions on Robotics 38, 3 (2021), 18561874. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, LauraGustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al.2023. Segment anything. arXiv preprint arXiv:2304.02643 (2023). Nikol Krausz, Vivien Pot, Jnos Mt Lg, and rpd Barsi. 2022. Comparisonof complex traffic junction descriptions in automotive standard formats. PeriodicaPolytechnica Civil Engineering 66, 1 (2022), 282290.",
  "Qi Li, Yue Wang, Yilun Wang, and Hang Zhao. 2022. Hdmapnet: An online hdmap construction and evaluation framework. In 2022 International Conference onRobotics and Automation (ICRA). IEEE, 46284634": "Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, YuQiao, and Jifeng Dai. 2022. Bevformer: Learning birds-eye-view representa-tion from multi-camera images via spatiotemporal transformers. In Europeanconference on computer vision. Springer, 118. Bencheng Liao, Shaoyu Chen, Xinggang Wang, Tianheng Cheng, Qian Zhang,Wenyu Liu, and Chang Huang. 2022. Maptr: Structured modeling and learning foronline vectorized hd map construction. arXiv preprint arXiv:2208.14437 (2022). Bencheng Liao, Shaoyu Chen, Yunchi Zhang, Bo Jiang, Qian Zhang, Wenyu Liu,Chang Huang, and Xinggang Wang. 2023. Maptrv2: An end-to-end frameworkfor online vectorized hd map construction. arXiv preprint arXiv:2308.05736 (2023).",
  "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruc-tion tuning. arXiv preprint arXiv:2304.08485 (2023)": "Lizhe Liu, Xiaohao Chen, Siyu Zhu, and Ping Tan. 2021. Condlanenet: a top-to-down lane detection framework based on conditional convolution. In Proceedingsof the IEEE/CVF International Conference on Computer Vision. 37733782. Shilong Liu, Tianhe Ren, Jiayu Chen, Zhaoyang Zeng, Hao Zhang, Feng Li,Hongyang Li, Jun Huang, Hang Su, Jun Zhu, et al. 2023. Detection Transformerwith Stable Matching. arXiv preprint arXiv:2304.04742 (2023).",
  "Bowen Pan, Jiankai Sun, Ho Yin Tiga Leung, Alex Andonian, and Bolei Zhou.2020. Cross-view semantic segmentation for sensing surroundings. IEEE Roboticsand Automation Letters 5, 3 (2020), 48674873": "Lang Peng, Zhirong Chen, Zhangjie Fu, Pengpeng Liang, and Erkang Cheng.2023. BEVSegFormer: Birds Eye View Semantic Segmentation From ArbitraryCamera Rigs. In Proceedings of the IEEE/CVF Winter Conference on Applications ofComputer Vision. 59355943. Limeng Qiao, Wenjie Ding, Xi Qiu, and Chi Zhang. 2023. End-to-End Vector-ized HD-Map Construction With Piecewise Bezier Curve. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition. 1321813228. Limeng Qiao, Yongchao Zheng, Peng Zhang, Wenjie Ding, Xi Qiu, Xing Wei,and Chi Zhang. 2023. MachMap: End-to-End Vectorized Solution for CompactHD-Map Construction. arXiv preprint arXiv:2306.10301 (2023). NDS Open Lane Model 1.0 Release. 2019. Lucas Tabelini, Rodrigo Berriel, Thiago M Paixao, Claudine Badue, Alberto FDe Souza, and Thiago Oliveira-Santos. 2021. Keep your eyes on the lane: Real-time attention-guided lane detection. In Proceedings of the IEEE/CVF conferenceon computer vision and pattern recognition. 294302.",
  "Andrew Tao, Karan Sapra, and Bryan Catanzaro. 2020. Hierarchical multi-scaleattention for semantic segmentation. arXiv preprint arXiv:2005.10821 (2020)": "Jinsheng Wang, Yinchao Ma, Shaofei Huang, Tianrui Hui, Fei Wang, Chen Qian,and Tianzhu Zhang. 2022. A keypoint-based global association network for lanedetection. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition. 13921401. Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, YangZhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, et al. 2020. Deephigh-resolution representation learning for visual recognition. IEEE transactionson pattern analysis and machine intelligence 43, 10 (2020), 33493364. Dongming Wu, Fan Jia, Jiahao Chang, Zhuoling Li, Jianjian Sun, Chunrui Han,Shuailin Li, Yingfei Liu, Zheng Ge, and Tiancai Wang. 2023. The 1st-place Solutionfor CVPR 2023 OpenLane Topology in Autonomous Driving Challenge. arXivpreprint arXiv:2306.09590 (2023). Deguo Xia, Jizhou Huang, Jianzhong Yang, Xiyan Liu, and Haifeng Wang. 2022.DuARUS: Automatic Geo-object Change Detection with Street View Imageryfor Updating Road Database at Baidu Maps. In Proceedings of the 31st ACMInternational Conference on Information and Knowledge Management. Deguo Xia, Xiyan Liu, Wei Zhang, Hui Zhao, Chengzhou Li, Weiming Zhang,Jizhou Huang, and Haifeng Wang. 2022. DuTraffic: Live Traffic Condition Predic-tion with Trajectory Data and Street Views at Baidu Maps. In Proceedings of the31st ACM International Conference on Information and Knowledge Management. Jianzhong Yang, Xiaoqing Ye, Bin Wu, Yanlei Gu, Ziyu Wang, Deguo Xia, andJizhou Huang. 2022. DuARE: Automatic Road Extraction with Aerial Imagesand Trajectory Data at Baidu Maps. In Proceedings of the 28th ACM SIGKDDConference on Knowledge Discovery and Data Mining. 43214331. Gongjie Zhang, Jiahao Lin, Shuang Wu, Yilin Song, Zhipeng Luo, Yang Xue,Shijian Lu, and Zuoguan Wang. 2023. Online Map Vectorization for AutonomousDriving: A Rasterization Perspective. arXiv preprint arXiv:2306.10502 (2023)."
}