{
  "ABSTRACT": "Quantifying the value of data within a machine learning workflow can play a pivotal role in makingmore strategic decisions in machine learning initiatives. The existing Shapley value based frame-works for data valuation in machine learning are computationally expensive as they require con-siderable amount of repeated training of the model to obtain the Shapley value. In this paper, weintroduce an efficient data valuation framework EcoVal, to estimate the value of data for machinelearning models in a fast and practical manner. Instead of directly working with individual datasample, we determine the value of a cluster of similar data points. This value is further propagatedamongst all the member cluster points. We show that the overall value of the data can be determinedby estimating the intrinsic and extrinsic value of each data. This is enabled by formulating the per-formance of a model as a production function, a concept which is popularly used to estimate theamount of output based on factors like labor and capital in a traditional free economic market. Weprovide a formal proof of our valuation technique and elucidate the principles and mechanisms thatenable its accelerated performance. We demonstrate the real-world applicability of our method byshowcasing its effectiveness for both in-distribution and out-of-sample data. This work addressesone of the core challenges of efficient data valuation at scale in machine learning models. The codeis available at",
  "Introduction": "Data valuation is a pivotal concern in modern machine learning (ML) and data analytics, where the quality and worthof data have profound implications for decision-making, model performance, and data marketplace. Quantifying theworth of data plays an important role in data pricing and regulation compliance , removing low-value/noisydata from the training set , and incentivizing data sharing by personal data monetization . In a MLframework, the quality of data determines the effectiveness of the final model. Therefore, identifying high and lowvalue data through data valuation would yield significant benefits for a wide range of machine learning applications. Background: In recent studies, a cooperative game theory concept, Shapley value has been frequently used fordata valuation in supervised ML . It offers a desirable property of equitable reward allocation. The dataShapley and its extensions have empirically shown the effectiveness of Shapley value based valuation ina fixed dataset as well as in a particular distribution of data, allowing for out-of-time data valuation. The value of a datapoint in ML relies on its individual contribution to the models performance and its relationship with other data pointsutilized during training. The presence of similar data in the training set can dilute the significance of individual points.To account for these interactions, data Shapley methods evaluate the contribution of each point by determining how",
  "arXiv:2402.09288v5 [cs.LG] 9 Jul 2024": "its absence affects the overall performance of the model. This process usually involves repeatedly training the modelwith the selective exclusion of certain instances or subsets, thereby identifying those with the most substantial impact.The impact is measured by the observing the change in the performance score of the ML model. However, this incursa high computational cost, typically requiring model training runs in the order of O(n2) in current methodologies,where n is the total number of data points in the dataset. Motivation: While offering insightful analyses of data point significance and alleviating the issue of poor discrimi-nation of data quality in leave-one-out (LOO) error methods, existing data Shapley based frameworks suffer from a high computational cost. The need for a higher number of repeated training sessions for a model, asrequired by these methods, leads to inefficiencies in both time and resource utilization. Furthermore, this inefficiencytranslates to an increased carbon footprint due to the energy requirements of training, thereby exacerbating climatechange concerns . The development of scalable algorithms capable of handling extensive datasets is essential forpractical use of data valuation in real-world applications. Our Contribution: We adopt a two-step approach where the valuation is performed at cluster-level first and thevalue is further divided among the cluster members. The similar data points are represented through a cluster whichsignificantly reduces the total number of data points to deal with during training phase of the valuation process. Atcluster level, we can use a simple LOO error for valuation since there is minimal possibility (almost zero) of a similardatum to be found in other clusters. The difficulty however, is to divide the value at each cluster among the clustermembers. To address this issue, a novel approach is proposed based on production functions in economics. Our two-step approach aims to significantly speed up the valuation process in comparison to the Truncated Monte Carlo (TMC)Shapley. In this paper, we introduce a a novel framework based on Leave Cluster Out (LCO) and production functions for datavaluation in machine learning. The framework is computationally efficient, with theoretical and empirical verifications.The following are the key contributions of our work: Novel Framework: The intuition behind our framework is that we find a group of similar items and estimate thisclusters marginal contribution. As similar data items are bound to have similar values, we extend this principle toestimate cluster-level value through Leave Cluster Out (LCO). We introduce a production function formulation representing the relation between the data and its utility in a model.We show that this formulation can be used to estimate the value of individual data based on the value of each cluster. Computational Efficiency: We estimate the intrinsic and extrinsic value of each data point to determine the individualdata value. By checking only the marginal contribution of the representative data point of a cluster, we substantiallyreduce the overhead of creating multiple subsets containing similar data points. Our approach is scalable to largedatasets without being limited by the presence of similar data points in the dataset. Theoretical Proof: We provide a theoretical proof of our data valuation method. We also show that the valuationobtained by our method has negligible error margin when compared with the vanilla Shapley value approximationmethod. Empirical Evaluation: We conduct experiments with machine learning models on MNIST, CIFAR10, and CIFAR100.We compare the value rankings of our method with the existing state-of-the-art data valuation approaches data Shap-ley , LOO error, and Distributional Shapley and notice similar or better performance with significant speed-upin data valuation process.",
  "Related Work": "Literature Review of Shapley Value. Shapley value as formalized in establishes the axiomatic properties anddemonstrates its unique ability to fairly allocate gains from cooperation among players. This seminal contribution laidthe theoretical groundwork for subsequent developments in cooperative game theory . Shapley valuehas been extensively used for applications in economics, , management science , online advertis-ing . In machine learning, it has been utilized for addressing the challenges in pricing ML training data, featureselection, and ML explainability. proposed to employ Shapley value properties for feature selection. use Shapley value in market mechanism to price training data and match buyers to sellers data marketplace design. introduced the SHAP framework, leveraging Shapley values to provide interpretable explanations for machine learningmodels. Other works have also explored its utility in explaining black-box model predictions [30, 31, 32, ?]. Data Valuation in ML. Recently, the subfield of data valuation in ML models has attracted significant attention and theexisting works have shown promising outcomes. Data Shapley proposed to use Shapley value from cooperativegame theory for valuation of training data. KNN Shapley improved the efficiency of data Shapley by using a k-nearest neighborhood model. Distributional Shapley expanded the scope of valuation to the underlying datadistribution instead of only considering the data points. Beta Shapley relaxes the efficiency axiom in DataShapleyand reports utility of data valuation in detecting mislabeled images in the training data. Data Banzhaf proposeto estimate the Banzhaf value to improve results on noisy label detection. Several works have attempted to improvethe efficiency of the Shapley value computation through approximation techniques [?, 10]. Apart from this, otheraspects of data value has been studied in . However, approximation of Shapley value still remainsa computationally expensive process, making it difficult to adapt for large models and datasets. The main goal of thiswork is to develop an alternative efficient data valuation framework to overcome this problem. Literature Review of Production Functions. offers a detailed outline of the evolution and econometrics of theproduction function. Aggregate production functions are used in macroeconomics to represent the relationship be-tween total output of an economy (GDP) and the inputs used to produce that output. These inputs typically includecapital (K), labor (L), and sometimes other factors like technology or natural resources . The simplest pro-duction function used in economics, is the Cobb-Douglas production function introduced by . identifies allmulti-factor production functions with given elasticity of output and from given elasticity of production. Productionfunctions have been used in various domains, including health, education, and energy, to name a few . Inour study, we adopt the concept of a production function and adapt it for data valuation. This approach draws inspira-tion from foundational works and recent advancements in the field. develops a theoretical framework that appliesthe production function to the economics of data, particularly employing data as an input for training machine learningmodels. Moreover, highlights the role of data as information aimed at reducing forecast errors, which hints ata production function characterized by bounded returns to data. In our paper, we align with these perspectives andfurther the discourse by specifically focusing on the application of the production function concept in the valuation ofdata.",
  "Preliminaries": "Let an ML model M, intended for a task T, is trained on a dataset B of size m. Let U denote the performance metricand U T denote the performance obtained on task T. The overall performance U is achieved after training a sufficientnumber of epochs e. Here the sufficient number of epochs means |Ue+i+1 Ue+i| < for all i 0, where is anarbitrarily small value. It should be noted that arises due to the randomness within the learning algorithm and notfurther training. The value of a data point is denoted by .",
  "LOO(z; U, B) = U(B) U(B \\ {z}).(1)": "It struggles in differentiating data quality when similar data samples exist in the dataset. For example, if each samplehas a duplicate copy in the dataset, the LOO will return a value 0 for all of the samples. Shapley value overcomes thislimitation by checking the marginal distribution over many subsets of the dataset.",
  "Further details regarding the interpretation of the above axioms in the context of machine learning can be referredto and": "Production Function. In economics, a production function expresses the relationship between the specific quantitiesand combinations of different inputs a company uses and the amount of output it produces. Commonly used production functions include Linear, Leontief, CobbDouglas , CES, and CRESH , each varying in their assumptionsfor the input and the output. The widespread usage of the Cobb-Douglas production function is attributed to itssimplicity and adaptability. It assumes homogeneity of inputs and this principle is consistent with many machinelearning setups.",
  "P = ALxKy.(4)": "It should be noted that the Cobb-Douglas production function also supports the diminishing returns in terms of bothlabor and capital. The Law of Diminishing Returns states that as the amount of a single factor of productionis incrementally increased, the marginal output of a production process decreases. This property is analogous tohow more data points have diminishing effects on a machine learning models performance. We therefore adapt theformulation of production functions in our proposed method to efficiently distribute the value of a cluster among itsdata members.",
  "Proposed Method": "A two-stage approach is proposed for efficient data valuation. First, data points are clustered together based on sharedcharacteristics. Then, a leave cluster out (LCO) technique is applied to estimate the value of each cluster. This clustervalue is then distributed among its members to obtain the preliminary individual data valuations. In the following,we delve into the building blocks of the proposed method and discuss its properties compared to the original Shapleymethods.",
  "Vc = U(B) U(B \\ c).(5)": "The simple LOO error may provide an underestimated view of the true impact of specific data points, especiallywhen similar data points remain in the dataset even after removal. Data Shapley alleviates this issue but suffersfrom high computational cost. By organizing data points into clusters based on their similarity, we ensure that whenan entire cluster is removed, there are no closely-related points to mask the effect of its absence in other clusters.Consequently, this leads to a more precise assessment of the clusters marginal contribution, effectively approximatingits value. Furthermore, this clustering approach significantly reduces the number of model training iterations neededin comparison to Data Shapley since evaluations are conducted at the cluster level instead of for each individual datapoint. Once we have obtained cluster-level valuations, the subsequent step involves efficiently approximating thevalues of individual data points within each cluster.",
  "Value Propagation within a Cluster": "Production Function for ML. We adapt the Cobb-Douglas production function to approximate the data value forML. In this context, we can draw an analogy: the labor L corresponds to the available data points for the model;the learning capacity or the number of parameters in the model represents the capital K; and the final output is theobtained performance on the test set U T . As both data quantity and model complexity exhibit diminishing returns,the Cobb-Douglas production function can be leveraged to effectively model learning performance. Therefore, wepropose to approximate the models performance after e epochs as",
  "hT (N)o(z).(7)": "To better understand Eq. (7), let us consider f as a smooth function of x as specified in Eq. (6), i.e., U T (x, N) =Af(x)hT (N). Thus, a minor change in x leads to a change in U T , which can be approximated by Af (x)hT (N)x.This allows us to interpret the expression enclosed in square brackets of Eq. (7) as effectively serving as the derivativeof f with respect to the set S, especially when considering incremental changes to S. Also, in Eq. (7), o(z) serves as an indicator of how a single data point enhances the models overall performance andis a proxy to x discussed above. Therefore, the difference f(S + S) f(S) captures the marginal impact on themodels performance when dataset S is augmented by a new data point. Analogous to the concept of derivatives incalculus, this difference, when normalized by the contribution o(z) of the individual point, can be interpreted as therate-of-changeof f upon the addition of a new data point. This rate is contingent on both the existing dataset S andthe new data point being added. That is",
  "(z, S)(10)": "Proposition 1 (Production Function Based Valuation for ML). Let T (z) denote the intrinsic value of a datum z,i.e., T (z) is only dependent on the characteristics of z. The interaction of z with rest of the data points in B iscaptured by (z, B). From equitable properties of data valuation in , we postulate that for every datum z havingan intrinsic value T (z), the (z, B) acts as a multiplier or extrinsic factor that decreases the value of z if similardata points are present in the dataset. Similarly, it increases the data value if z is a unique datum. Then the datavaluation can be performed as below(z; U T , B) = T (z)(z, B).(11)",
  "To simplify notation, we denote T (z) with (z), and (z; U T , B) with (z; U, B) for the rest of the discussion, sinceT is invariant": "Fast Data Valuation. Based on the above setup, we propose an efficient data valuation method that also works as anefficient proxy to Distributional Shapley to predict valuation for unseen data-points in the distribution. The existingData Shapley adheres to two fundamental axioms : symmetry and efficiency. Symmetry states that for points z andz that contribute similarly to the models performance should have the same value, i.e. U(S {z}) = U(S {z})for all S B \\ {z, z}. Efficiency, on the other hand, ensures that the aggregate value of all data points aligns with theoverall performance achieved after training on the entire dataset. Proposition 2 (Fast Data Valuation of Cluster Data Members) The symmetry and efficiency properties when appliedto a specific cluster implies the data points within a cluster, characterized by similar features, will likely possess similarvalues and a clusters value can be accurately represented as the sum of its constituent data points valuations.",
  "where nc is the number of data points in cluster c. Using this cluster-level assignment of initial data value, we estimatethe actual data value based on Eq. (11) asV i = ii .(13)": "Estimating and . Assuming each cluster contains an equal number of data points, the distribution of similarand dissimilar samples encountered by each datum becomes roughly uniform. This results in a near-constant extrinsicfactor, (z, B), across all data points. Thus, the value of these data points are directly proportional to (zi). We useQi to denote the value of individual datum to differentiate it from Vi value that is initialized by the cluster value inEq. (12).",
  "zjc QjVc.(15)": "The adjustment factor i ensures that value of individual data point is adjusted not only based on its personal contri-bution (Qi) but also in proportion to the corresponding clusters overall impact (Vc). This dual consideration is crucialfor accurately reflecting the true value of each data point, balancing internal cluster contributions with the broadercontext of the clusters role within the entire dataset. Including Vc in the adjustment ensures that the recalibration ofi remains sensitive to both intra-cluster dynamics and the comparative significance of each cluster, providing a morenuanced and equitable valuation of data points across a heterogeneous dataset. Corollary 4.1.1 When all data points in a cluster are exactly the same, the adjustment factor should be equal to 1 sothat for each point in c, the value becomes Vi. But the above formulation of i yields 1 + 1/n when all the points areidentical as Vi and Vj will be equal for any i, j. Thus, we normalize i as follows",
  ".(16)": "Similar to i, we find the adjustment factor for i , i.e. i . i measures the interaction of zi with all other datapoints in B. As all data points similar to zi belong to the same cluster and i is only affected by the other membersin zis cluster. We use the distance between zi and cluster centroid as a measure to its belongingness to the cluster orsimilarity to other points in the cluster.",
  "Discussion: Comparison with Original Shapley": "Let E(z) denote the appropriate embedding from a machine learning model or the pre-final layer of a deep learningmodel for a data point z. We extend the notion of Lipschitz Stability of data Shapley introduced in to estimate thedifference in value of different data points. We use proximity of the embeddings E(z) as a proxy to the closeness inthe underlying data distribution and formalize the same in the following Theorem.",
  "Algorithm 1 EcoVal Data Valuation": "1: M(.; ): Fully Trained Model2: B: Training Dataset3: BD: Set of available points from the underlying distribution of B4: Mn(x; ) Embedding of data x obtained from the nth last layer of the model5: Let E(x) = Mn(x; )6: Let Ac be a clustering algorithm then (xi, cj) Ac(BD)xi BD where cj C is the cluster associated withxi and C is the set of all clusters 7: Find valuation at cluster level8: Vcj = U(B) U(B \\ cj) cj C9: Initialize value Vi for each cluster member xi10: Vi = Vcj/ncj, where ncj is the number of elements in cluster cj to which xi belongs11: Initialize: D []12: for cj C do13:Sample Xj = {xj1, xj2, ... xjnc} from cj",
  "Experiments": "We show the broad effectiveness of the proposed valuation framework and its general applicability to machine learningmodels through empirical evidence. We estimate the value of data in a machine learning model in MNIST, CIFAR10,and CIFAR100 datasets. We compare our method with Data Shapley and Distributional Shaply . Experiment Settings: Following the common practice in previous works, we extract the features from last layer ofa pre-trained network and apply Shapley on this embedded vector. We sample a small subset, i.e. 200 samples fromthe original training data and run the baseline methods TMC-Shapley (Data Shapley) and distributional Shapley. 2000samples are used for testing and holdout for Shapley calculation. We keep 10,000 samples which are never seen bymodel or valuation method at any point, we call this out-of-sample (OOS) set. The rest of the samples are used as datadistribution and exposed to Distributional Shapley, and our method during the clustering step and correction step. Weuse Gaussian Mixture Models (GMM) for clustering. Our proposed method works for both in-distribution and OOSsamples. As Data Shapley only works for in-distribution samples, we compare our results with Distribution Shapleyfor out-of-sample data. We use Gaussian Mixture Model (GMM) clustering with default parameters of scikit-learnsimplementation, covariance-type=full, tol=0.001, reg-covar=1e-06, max-iter=100, n-init=1, init-params=kmeans,and 30 mixtures/clusters.",
  "Comparative Analysis of the Computational Time": "The Data Shapley approximation method TMC Shapley converges in approximately 3|B| (or 3m in Eq. 2) MonteCarlo samples. Each Monte Carlo sample is a random permutation of the data points in the training set. The marginal : Computation cost in terms of number of training iterations required for the given dataset size. We compareEcoVal with TMC Shapley (also known as Data Shapley), distributional Shapley, and a lighweight version of EcoVal.Our method requires substantially lower number of training iterations for data valuation. contribution of a data point z in a given permutation is obtained as the performance difference between the modeltrained on data points before this datum, say S, and the model trained on S {z}. Each point is added sequentiallymeaning |B| training runs are required in a single Monte Carlo sample. This makes the number of training runs in theorder of O(|B|2). Distributional Shapleys time complexity is similar with T runs to get an unbiased estimate usingdifferent subsets S from the underlying data distribution. This makes the number of training runs of DistributionalShapley O(T |B|2). Our method performs clustering that takes less time than training a machine learning or deep learning model in mostreal-world scenarios. This is a one time effort, so the complexity is in the order of O(1). Estimating the value of eachcluster requires O(p) training runs. Apart from that, our method involves running Data Shapley on a curated subsetp containing an equal number of points from each cluster, this take O(p2) time. The size of this subset p is muchsmaller than |B|. The total number of training runs required is in the order of O(1) + O(p2) + O(p). We compare thecomputational cost associated with the TMC Shapley/Data Shapley with our EcoVal method. As illustrated in ,EcoVal requires significantly fewer training iterations, approximately 103 to 105, compared to 107 to 109 required bytraditional TMC Shapley. Our method reduces computational overhead by employing a curated subset approach,which involves running Data Shapley on a subset p containing an equal number of points from each cluster. Thiscurated subset approach requires computational resources in the order of O(p2), significantly less than the O(|B|2)required by the standard TMC Shapley method, where |B| represents the size of the full dataset. The size of subset pis much smaller than |B|, which explains our methods efficiency and scalability. With the increase in the dataset size,the utility of our EcoVal becomes more evident. Our method without correction is even faster with negligible lossin valuation quality.",
  "Data Point Addition and Removal Experiments": "We evaluate the data valuation methods by running the data point addition and removal experiments as proposed in .For a given model and dataset, the data points are added in the order of predicted value, i.e. from largest to lowestvalues, and the model is retrained for each addition. Similarly, another experiment is conducted where we removesamples with high values and observe the performance drop. The impact of removal and addition of high value data-points help us measure the effectiveness of data valuation techniques. We compare our results with state-of-the-artData Shapley and Distributed Data Shapley valuation methods. Removing most valued data points. We predict values of data-points using each valuation method and we measurethe drop in performance of model by removing most-valued data-points for each method. A better valuation methodshigh value data-points will result in a higher drop in performance. So, for removal of most valued points, the methodresulting in higher performance drop is a better valuation method. Adding most valued data points. This approach is vice-versa of the previous approach, we add most valued data-points into the training set and observe the increase in the performance. A higher increase on adding the top data-pointsshows better valuation method. shows the performance drop and increase upon adding and removing most : Accuracy difference with respect to the % of data points added or removed. We add or remove the highestvalued data points first and then subsequently add or remove the lesser value data, respectively. The top, middleand bottom rows show the results for MNIST, CIFAR10, CIFAR100, respectively with in-distribution valuation. TheEcoVal gives comparable or better performance when compared to Data Shapley and Distribution Data Shapley. : Data valuation on out-of-sample data (top left to bottom right: CIFAR10, CIFAR100, MNIST, respectively).Our EcoVal method outperforms Distributed Data Shapley and Random Data Removal by getting steeper performancedrop with increasing % valuable data removal. valued points, respectively. It can be observed that EcoVal performance drop is slightly less than that of Data Shapleybut significantly higher than the Distributed Data Shapley which is desired. Similar patterns can be observed in thedata addition graph also. Removing most valuable data points from out-of-sample set. The discussed earlier, EcoVal supports data valuationfor out-of-sample data as well which is supported only by Distributed Data Shapley. Therefore, we compare the OOSvaluation results between them. shows the performance drop by removing the most valuable points froman out-of-sample set of size 10, 000. It can be observed that EcoVals performance drop is very high as compared to : Effect of adding different adjustment terms (refer .2).EcoVal: the full proposed method,EcoVal_no_alpha:removal of adjustment term, EcoVal_no_beta:removal of adjustment term, Eco-Val_no_adjustment: EcoVal without any adjustment terms, i.e. the mean of the cluster value used as the data value.",
  "Effect of the Adjustment Terms": "We observe the effect of removing different adjustment terms i , i or both in the EcoVal framework and show theresults in . The overall EcoVal framework with the terms and performs the best, in general. Eliminatingone of the adjustment terms deteriorates the quality of the valuation by a small margin. Removing both correctionssignificantly impacts the quality of data valuation. This is particularly visible in the initial phase of adding the mostsignificant data points. It should be noted that eliminating i only affects the valuation quality marginally, butcompletely removes the need for model training, giving an even more efficient version of our valuation method. The performance of different variations of our method (EcoVal, EcoVal-no-, EcoVal-no-, EcoVal-no-adj) wouldvary depending on the intra-class and inter-class variations present in a dataset. In , the performance dif-ferences are not sometimes consistent. We report our observation based on the experiments with CIFAR-10 dataset.More complex datasets with larger variations may better reveal the impact of these adjustment factors, which is thefuture scope of this work.",
  "Mutual Influence of Clustering Methods and Adjustment Terms": "The effectiveness of EcoVal is intrinsically related to the success of the clustering method, as the initial valuation iscarried out at the cluster level. The adjustment terms are meant to marginally correct the data value distribution withineach cluster. The similarity within a single cluster compared to the similarity between clusters significantly influencesthe structure and application of the adjustment factors. We assume that data points within the same cluster have ahigher degree of similarity compared to points across different clusters, a standard assumption in clustering algorithmssuch as Gaussian mixture models (GMM). However, the variance in the degree of similarity across different clusters(inter-cluster dissimilarity) justifies the need for a scaling factor such as i, introduced in Equation 15. This accountsfor the relative contribution of an entire cluster to the models performance, acknowledging that some clusters maybe more pivotal due to their positioning, density, or the nature of the data points they contain. This process dependson the intra-cluster variation present in a dataset. For some dataset, a simple distribution of the value of the clusterwithout correction factors can also give a good valuation, leaving a minimal scope of correction.",
  "Conclusion": "This work presents a focused study on improving the speed of data valuation in machine learning models. We developan efficient data valuation method that is significantly fast and practical for working with large datasets. Our methodworks for both in-distribution and out-of-sample data. The proposed EcoVal data valuation framework shows compa-rable and sometimes even better results than the existing approaches for in-distribution data. For out-of-sample datapoints, our method significantly outperforms competing methods, thus establishing a new state-of-the-art. This provesour methods utility in a data market, where new data points analogous to our out-of-sample set are generated everypassing instant. Our valuation also shows negligible error margin with the vanilla Shapley value approximation. Thepoints mentioned above collectively make the proposed method a robust and scalable approach to estimate the valueof data across a variety of machine learning models. This research/project is supported by the National Research Foundation, Singapore under its Strategic CapabilityResearch Centres Funding Initiative. Any opinions, findings and conclusions or recommendations expressed in thismaterial are those of the author(s) and do not reflect the views of National Research Foundation, Singapore.",
  "Jian Pei. Data pricing from economics to data science. Association for Computing Machinery, 2020": "Siyi Tang, Amirata Ghorbani, Rikiya Yamashita, Sameer Rehman, Jared A Dunnmon, James Zou, and Daniel LRubin. Data valuation for medical imaging using shapley value and application to a large-scale chest x-raydataset. Scientific reports, 11(1):8366, 2021. Bojan Karla, David Dao, Matteo Interlandi, Bo Li, Sebastian Schelter, Wentao Wu, and Ce Zhang. Data de-bugging with shapley importance over end-to-end machine learning pipelines. arXiv preprint arXiv:2204.11131,2022. Ruoxi Jia, David Dao, Boxin Wang, Frances Ann Hubis, Nick Hynes, Nezihe Merve Grel, Bo Li, Ce Zhang,Dawn Song, and Costas J Spanos. Towards efficient data valuation based on the shapley value. In The 22ndInternational Conference on Artificial Intelligence and Statistics, pages 11671176. PMLR, 2019.",
  "LS Shapley. A value for n-person games. In Contributions to the Theory of Games (AM-28), Volume II, pages307318. Princeton University Press, 1953": "Yongchan Kwon, Manuel A Rivas, and James Zou. Efficient computation and analysis of distributional shapleyvalues. In International Conference on Artificial Intelligence and Statistics, pages 793801. PMLR, 2021. Yongchan Kwon and James Zou. Beta shapley: a unified and noise-reduced data valuation framework for ma-chine learning. In International Conference on Artificial Intelligence and Statistics, pages 87808802. PMLR,2022.",
  "Amirata Ghorbani and James Y Zou. Neuron shapley: Discovering the responsible neurons. Advances in neuralinformation processing systems, 33:59225932, 2020": "Ruoxi Jia, David Dao, Boxin Wang, Frances Ann Hubis, Nezihe Merve Gurel, Bo Li4 Ce Zhang, and CostasSpanos1 Dawn Song. Efficient task-specific data valuation for nearest neighbor algorithms. Proceedings of theVLDB Endowment, 12(11). Jiachen T. Wang and Ruoxi Jia. Data banzhaf: A robust data valuation framework for machine learning. InFrancisco Ruiz, Jennifer Dy, and Jan-Willem van de Meent, editors, Proceedings of The 26th InternationalConference on Artificial Intelligence and Statistics, volume 206 of Proceedings of Machine Learning Research,pages 63886421. PMLR, 2527 Apr 2023.",
  "A.1Theorem 3.4": "Let equal number of samples are used from each cluster to run TMC Shapley. Then the intrinsic value of a datum isindependent of proportion and bias in the data distribution. If ns such samples exist, the value is divided into these nssamples. From Shapely axioms, the data Shapley at the current stage of TMC becomes approximately i",
  ".(28)": "Intuition. Vi does not have any error as this is the difference between the performance with and without the cluster cdivided by a constant. Both the values can be directly computed from the model. Similarly, di is the distance of thedatum from the centroid of the cluster c which can be calculated without any error.",
  "zjc Qj)2 ,(29)": "c and Qi cannot be arbitrarily large as they are the average Shapely value for a cluster and change in performancedue to a data point zi. With increasing cluster size nc, the corresponding predicted value zjc Qj will increase.Thus, nc/ zjc Qj can not be very large. The R error will be low for a moderately good regression model. Thus,our method estimates the Shapely value i with negligible error.",
  "The sum of the Shapley values within a cluster equalling the leave-cluster-out value of the cluster follows logicallyfrom the axioms of the Shapley value, in particular, additivity and efficiency": "Additivity: If we treat each data point as contributing a separate game to the performance, the total Shapley value of acluster should naturally be the sum of the Shapley values of each data point within the cluster. Efficiency: This axiom ensures that the total value generated by the coalition is fully distributed among the players.If the Shapley value calculation respects this axiom, then the allocation to a cluster should match the cumulativecontribution of its members. We can consider a simple proof by induction on the number of data points in the cluster:1. Base Case: If a cluster has only one data point, then the Shapley value of the cluster is clearly equal to the Shapleyvalue of the single data point.2. Inductive Step: Assume the proposition holds for all clusters of size k. For a cluster of size k + 1, if you removeone data point, by the induction hypothesis, the sum of the Shapley values of the remaining k data points equals thevalue of these k data points. Adding the k + 1-st point, by the additivity axiom, the overall Shapley value would bethe sum of the individual Shapley values."
}