{
  "ABSTRACT": "Missing data is a pervasive issue in both scientific and engineeringtasks, especially for the modeling of spatiotemporal data. This prob-lem attracts many studies to contribute to data-driven solutions.Existing imputation solutions mainly include low-rank models anddeep learning models. The former assumes general structural pri-ors but has limited model capacity. The latter possesses salientfeatures of expressivity but lacks prior knowledge of the underly-ing spatiotemporal structures. Leveraging the strengths of both twoparadigms, we demonstrate a low rankness-induced Transformerto achieve a balance between strong inductive bias and high modelexpressivity. The exploitation of the inherent structures of spa-tiotemporal data enables our model to learn balanced signal-noiserepresentations, making it generalizable for a variety of imputationproblems. We demonstrate its superiority in terms of accuracy, effi-ciency, and versatility in heterogeneous datasets, including trafficflow, solar energy, smart meters, and air quality. Promising empiri-cal results provide strong conviction that incorporating time seriesprimitives, such as low-rankness, can substantially facilitate thedevelopment of a generalizable model to approach a wide range ofspatiotemporal imputation problems. The model implementation isavailable at:",
  "KDD 24, August 2529, 2024, Barcelona, Spain. 2024 Copyright held by the owner/author(s).ACM ISBN xx": "weather, energy supply, and sensor service time can adversely affectthe quality of monitoring data . Given these factors, data missingrates can be quite high. For example, the air quality measurements inthe Urban Air project contain about 30% invalid records due tostation malfunction. Similarly, subsets of Uber Movement data, suchas New York City, have an approximate 85% of missing records. Thisproblem encourages researchers to develop advanced models thatcan exploit limited observations to impute missing values. Extensiveresearch has contributed to data-driven methods for this purpose,especially in the field of spatiotemporal data .Generally, there are two research paradigms for imputing miss-ing data. The first paradigm uses low-rank and low-dimensionalanalytical models, such as , which assumes thedata has a well-structured matrix or tensor form. These modelsutilize the algebraic properties of the assumed structure, such aslow-rankness, low nuclear norm, and spectrum sparsity (we use theterm low-rank as a proxy), to impute missing values . Whilesimple low-rank models like matrix factorization and tensor com-pletion can effectively handle incomplete data, they may strugglewith capturing complex patterns, such as nonlinearity and nonsta-tionarity. With strong inductive bias, these models can excessivelysmooth the reconstructed data, filtering out informative signals,and generating oversmoothing reconstructions in some cases .The second paradigm uses deep learning-based imputation mod-els. These models learn the dynamics of the data-generating processand demonstrate improved performance . How-ever, despite the success of these models on various benchmarks,there are still costs and difficulties that need further attention. First,such data-intensive methods require a substantial amount of train-ing expenses due to the complex model structures in deep learning,such as probabilistic diffusion and bidirectional recurrence .This can consume significant computational memory and resources,making them less efficient for real-time deployment. Second, empir-ical loss-based learning methods, without the guidance of physics ordata structures, are prone to overfitting and perform poorly whenapplied to tasks fall outside of the distribution of training data .With the shift of focus in the field of deep imputation from RNNsand diffusion models to Transformers , Transformer-related architectures have gained significant attention due to theirpotential to provide efficient generative output and high expressiv-ity, enabling more effective imputations compared to autoregression-based models. Additionally, Transformers are considered founda-tional architectures for general time series forecasting . How-ever, the effectiveness of applying Transformers to general dataimputation tasks requires further investigation. Modern deep learn-ing techniques associated with these architectures, such as self-attention and residual connections, can unintentionally preserve",
  "Smooth": "Singular valuesSingular values Spatiotemporal matrix : (a) The distribution of singular values in spatiotemporal data is long-tailed. The existence of missing data can increaseits rank (or singular values). (b) Low-rank models can filter out informative signals and generate a smooth reconstruction,resulting in truncating too much energy in the left part of its spectrum. (c) Deep models can preserve high-frequency noise andgenerate sharp imputations, maintaining too much energy for the right part of the singular spectrum. With the generality oflow-rank models and the expressivity of deep models, ImputeFormer achieves a signal-noise balance for accurate imputation. high-frequency noise in data as informative signals . This canlead the model to learn high-rank representations that violate thenatural distribution of data. Furthermore, the existence of missingdata can introduce spurious correlations between tokens, posingchallenges to these architectures. Considering the above-mentionedconcerns, incorporating a low-rank inductive bias into the Trans-former framework seems to provide a chance to improve both theeffectiveness and efficiency in spatiotemporal imputation.In summary, matrix- and tensor-based models offer useful priorsfor spatiotemporal data, such as low-rankness and sparsity. How-ever, their ability to represent data is limited (see (b)). Onthe other hand, deep learning models, particularly Transformers,excel at learning representations but lack prior knowledge of datageneration (see (c)). As the demand for a versatile and adapt-able model that can handle various imputation problems in realityincreases, such as cross-domain datasets, different observation con-ditions, highly sparse measurements, and different input patterns,it becomes apparent that existing advanced solutions, typicallyevaluated on limited tasks with simple settings, may not be general-izable. Hence, there is a temptation to merge these two paradigmsand utilize their respective strengths to investigate an alternativeparadigm that can effectively handle complex imputation scenarios.To this end, in this paper we leverage the structural priors of low-rankness to generalize the canonical Transformer (see (d)) ingeneral spatiotemporal imputation tasks. Our approach, referred toas Imputation Transformers (ImputeFormer), imposes low-ranknessand achieves attention factorization equivalently by introducinga projected attention mechanism on the temporal dimension andan embedded attention on the spatial dimension. Additionally, wepropose a Fourier sparsity loss to regularize the solutions spectrum.By inheriting the merits of both low-rank and deep learning models,it has achieved state-of-the-art imputation performance on variousbenchmarks. Our main contributions are summarized as follows:",
  "PRELIMINARY": "Notations. This section first introduces some basic notations fol-lowing . In a continuously working sensor system with staticdetectors at some measurement positions, spatiotemporal data withcontext information can be obtained: (1) X:+ R : The ob-served data matrix containing missing values collected by all sen-sors over a time interval T = {, . . . , + }, where represents theobservation period; (2) Y:+ R : The ground truth data ma-trix used for evaluation; (3) U:+ R u: Exogenous variablesdescribe time series, such as the time of day, day of week, and weekof month information; (4) V R v: Meta information of sensors,such as detector ID and location of installation.Problem Formulation. The multivariate time series imputationproblem defines an inductive learning and inference process:",
  "Imputation": "Low-Rank Pattern in Spatial Attention Map Redundancy and Dominance in Time Series Fourier Sparsity in Both Space and Time : Low-rankness in time series and the induced ImputeFormer. (a) Redundancy in time series: PEMS08 data can bereasonably reconstructed using only five dominant patterns. (b) Low-rank spatial attention map: the singular values of themultivariate attention map show a long-tailed distribution and most of them are small values. (c) Fourier sparsity in both spaceand time axes: both the spatial and temporal signals possess a sparse Fourier spectrum, with most amplitudes close to zero.",
  "RELATED WORK": "Generally, there exist two series of studies on multivariate timeseries imputation, i.e., 1) low-dimensional/rank models and 2) deepimputation models. We particularly discuss existing Transformer-based solutions to clarify the connections between our models.Low-Dimensional/Rank Imputation. Early methods addressedthe data imputation problem by exploring statistical interpolationtools, such as MICE . Recently, low-rank matrix factorization and tensor completion have emerged as nu-merically efficient techniques for spatiotemporal imputation. To in-corporate series-related features, TRMF imposed autoregressiveregularization on the temporal manifold. TiDER decomposedthe time series into trend, seasonality and bias components underthe factorization framework. Despite being conceptually intuitiveand concise, limited capacity hinder their practical effectiveness.Deep Learning Imputation. Recent advances in neural time seriesanalysis open a new horizon to improve imputation performance.Generally, deep imputation methods learn to reconstruct the distri-bution of observed data or aggregate pointwise information pro-gressively . Representative methods include GRU-D , GRUI, BRITS , GAIN , E2GAN NAOMI , CSDI , andPriSTI . To exploit the multivariate nature of spatiotemporaldata, graph neural networks (GNNs) have been adopted to modelsensor-wise correlations for more complicated missing patterns.For example, MDGCN and GACN applied GNNs with RNNsfor traffic data imputation. IGNNK , STAR , and STCAGCN further tackle the kriging problem, which is a special data impu-tation scenario. As a SOTA model and an architectural templatefor GNN-RNNs, GRIN based on message-passing GRUs that pro-gressively performed a two-stage forward and backward recurrentmessage aggregation with predefined relational biases.Transformers for Time Series Imputation. Transformers can aggregate abundant information from arbitrary input elements,becoming a natural choice for sequential data imputation. In partic-ular, CDSA developed a cross-channel attention that utilizes cor-relations in different dimensions. SAITS combined the masked imputation task with an observed reconstruction task, and applieda diagonally-masked self-attention to hierarchically reconstructsparse data. SPIN achieved SOTA imputation performance byconducting a sparse cross-attention and a temporal self-attention onall observed spatiotemporal points. However, the high complexityof cross-attention hinders its application in larger graphs.",
  "LOW RANKNESS-INDUCED TRANSFORMER": "This section elaborates the ImputeFormer model. The major dif-ference between our model and the canonical Transformer is theintegration of low-rank factorization. Unlike GNNs, ImputeFormerdoes not require a predefined graph due to the global adaptiveinteraction between series. It also bypasses the use of intricate tem-poral techniques, such as bidirectional recurrent aggregation, sparsecross-attention, and attention masking. Furthermore, it achieveslinear complexity with respect to spatial and temporal dimensions.",
  "Architectural Overview": "The overall structure of the proposed ImputeFormer is shown in. The input embedding layer projects sparse observationsto hidden states in an additional dimension and introduces bothfixed and learnable embedding into the inputs. Following a time-and-graph template , TemporalInteraction and SpatialInteractionperform global message passing alternatively at all spatiotemporalcoordinates. Finally, a MLP readout is adopted to output the finalimputation. This process can be summarized as follows:",
  "KDD 24, August 2529, 2024, Barcelona, Spain.Tong Nie et al": "and testing datasets. A desirable characteristic is that a model candeal with different missing patterns during inference. Therefore, weconsider a challenging scenario in which a model is trained with afixed missing rate but evaluated on different scenarios with varyingmissing rates. This constructs a zero-shot transfer evaluation. It isnoteworthy in Tab. 5 that both ImputeFormer and SPIN are morerobust than other baselines in these scenarios. RNNs and vanillaTransformers can overfit the training data with a fixed data missingpattern, thereby showing inferior generalization ability.",
  "Z,(+1):+= F (Z,():+ )Z,():+,(4)": "where F () : R R represents a data-driven function atthe -th layer, such as self-attention. The rationale of this strategyis discussed in A.1.2.Time Stamp Encoding. Time stamp encoding is adopted to handlethe order-agnostic nature of Transformers . As the input seriescovers a relatively short range, we only consider the time-of-dayinformation. We adopt the sinusoidal positional encoding in to inject the time-of-day information of each time series:",
  "u =sinecosine,(5)": "where is the index of -th time-of-day point in the series, and is the day-unit time mapping. We concatenate psine and pcosineas the final time stamp encoding U:+ R 2.Node Embedding. Previous work has demonstrated the impor-tance of node identification in distinguishing different sensors forspatiotemporal forecasting . Here we also recommendthe use of learnable node embedding for imputation task. On theone hand, it benefits the adaptation of local components ingraph-based data structure. On the other hand, we highlight thatnode embedding can be treated as an abstract and low-dimensionalrepresentation of the incomplete series. To implement, we assigneach series a randomly initialized parameter e R . We thensplit the hidden dimension of the static node embedding equallyby the length of the time window as a multi-head node embeddingand unfold it to form a low-dimensional and time-varying repre-sentation: E:+ R / . Implicit interactions between nodeembedding, input data, and modular components are involved inthe end-to-end gradient descent process. Finally, the spatiotemporalinput embedding for each node can be formulated as follows:",
  "Temporal Projected Attention": "As is evident in , time series are supposed to be redundantin the time domain, that is, most of the information can be recon-structed using only a few dominant modes. However, as the hiddendimension is practically much larger than the sequence length, the attention score R R R can be a high-rankmatrix, which is both adverse and inefficient to reconstruct incom-plete hidden spaces. To address this concern, we propose a newprojected attention mechanism to impose a low-rank constrainton the attentive process and efficiently model pairwise temporalinteractions between time points in linear complexity.To utilize this structural bias, we first project the initial featuresto dense representations by attending to a low-dimensional vector.Specifically, we first randomly initialize a learnable vector that isshared by all nodes with the gradient tractable as the projectorPproj R, where < is the projected dimension. In order torepresent the temporal message in a compact form, we then projectthe hidden states Z,() R (subscripts are omitted for brevity)to the projected space by attending to the query projector:",
  "R are linear weights. In particular, since the projector Pproj isdecoupled from the spatial dimension, the resulted attention mapSoftmax(P()projWQWTKZ,(),T/": ") R can be interpreted asan indicator of how the incomplete information flow can be com-pressed into a compact representation with smaller dimension, thatis, an aggregation of available messages. More expositions on theprojector will be provided in .6.Z,()proj stores the principal temporal patterns within the inputdata. Then, we can recover the complete series with this compactrepresentation by dispersing the projected information to all otherfull-length series by using the projector as a key dictionary:",
  "Z,(+1) = LayerNorm(Z,() + FeedForward(Z,())),(9)": "where Z,(+1) R is the imputation by the -th temporalinteraction layer. Since the projector in Eqs. (7) and (8) can beobtained by end-to-end learning and is independent of the order inseries, it has the property of data-dependent model in Eq. (4). Toindicate how the above process learns the low-rank representationof temporal attention, we develop the following remark.",
  "= (QPT)(PKT)V 1 2 Q(PTP)KTV": "Recall that the projector P R can have a small projectiondimension , it can be viewed as a channel-wise matrix factorizationto reduce redundancy within each time series. The rank of the projectedattention matrix is min{, }, which is theoretically lower thanthe original rank. The projected attention guarantees expressivity bymaintaining a large hidden dimension , while at the same timeadmitting a low-rank solution using a small projection dimension .The rank-reduced temporal attention matrix exploits the low-ranknessof data in the temporal dimension, which is different from the low-rank adaptation of model parameters developed recently . The projection-reconstruction\" process in Eqs. (7) and (8) re-semble the low-rank factorization process X = UVT. Inflow in Eq.(7) controls the amount of message used to form a dense represen-tation in a lower-dimensional space. Outflow in Eq. (8) determineshow hidden states can be reconstructed using only a few projectedcoordinates. This mechanism also brings about efficiency benefits.The canonical self-attention costs O( 2) time complexity. The com-plexity of the projected attention is O(), which scales linearly(see Section A.1.3) and is efficient for longer sequences. In addition,low-rank attention preserves the dominating correlational struc-tures and eliminates spurious correlations. The cleaned correlationsallow the model to focus on the most relevant data as a reference.",
  "Spatial Embedded Attention": "The availability of observed temporal information is not sufficientfor fine-grained imputation. In some cases, specific spatial events,such as traffic congestion, can lead to non-local patterns and un-usual records. Therefore, it is reasonable to exploit the multivariaterelationships between series as a complement. A straightforwardway to address this problem is to apply a Transformer block in spa-tial dimension . Nevertheless, the three concerns discussedin .1 prevent the direct use of this technique.Consequently, we design an embedded attention as an alternativeto spatial attention. We highlight that the node embedding in Eq. (6)signifies not only the identity of series, but also a dense abstract ofeach individual. We then establish a correlation map using this low-dimensional agent. Formally, we assume that the message passinghappens on a fully connected dense graph, and the edge weightsare estimated by the pairwise correlating of node embedding:",
  "where A() R denotes the pairwise correlation score of allsensors, and Q() , K() R emb are linearly projected from the": "spatiotemporal embedding set E = [e1e2 e ] R / ,with e being the static node embedding averaging over the tempo-ral heads { : + } from Eq. (6).Given the graph representation over a period Z R ,the complexity of obtaining a full spatial attention matrix costsO( 2). To alleviate scalability concerns on large graphs, weadopt the normalization trick in to reparameterize Eq. (10).Observe that the main bottleneck in Eq. (10) happens in the mul-tiplication of two large matrix Q()and K(), we can reduce thecomplexity using the associative property of matrix multiplicationif we can decouple the softmax function. To this end, we apply thesoftmax on separate side of the Q-K matrix and approximate A as:",
  "(12)": "By computing the multiplication of 1(K())T and Z()at first,the above process admits a O(emb) time complexity, whichscales linearly with respect to the number of sensors. Since E isdecoupled from temporal information, it is robust to missing valuesand reliable to infer a correlation map for global imputation. Thefull attention has the size R ( ) R( ) R ,while the embedded attention has R emb Remb R .In this sense, the attention map in Eq. (10) act as a factorized low-rank approximation of full attention. We highlight this property bycomparing the two formulations in the following analysis.",
  "Remark (Difference between embedded attention and canon-ical self-attention). Given a hidden state Z R , theself-attention can be computed on the folded matrix Z R ()": "as SelfAtten(Z, Z, Z) = (ZWWTZT)ZW . The resulting rankof the attention matrix obeys min{,}. While the embed-ded attention has SelfAtten(E, E, Z) = (EW)(EW)TZW . Ifwe ignore the possible rank-increasing effect of softmax, the abovecalculation generates the output with rank min{, emb}. Sincethe dimension of the node embedding emb is much smaller than themodel dimension , the rank of the embedded attention map has alower bound of rank than the full attention. In addition, the modelstill has a large feedforward dimension to ensure capacity.",
  "Fourier Imputation Loss": "As imputation model is typically optimized in a self-supervisedmanner, previous studies proposed adopting accumulated and hi-erarchical loss to improve the supervision of layerwiseimputation. However, we argue that such designs are not necessaryand may generate overfitting. Instead, we propose a novel Fourierimputation loss (FIL) combined with a simple supervision loss astask-specific biases to achieve effective training and generalization.",
  "Mwhiten (X Y)1,(13)": "where Y is the label used for training.Fourier Sparsity Regularization. As discussed above, we canobtain a reasonable imputation by constraining the rank of the esti-mated spatiotemporal tensor in the time domain. However, directlyoptimizing the rank of the tensor or matrix is challenging , as itincludes some non-trivial or non-differentiable computations, suchas truncated singular value decomposition (SVD). And the SVD of a matrix costs O(min{ 2, 2}) complexity, which can be-come a bottleneck when integrated with deep models. Fortunately,we can simplify this process using the following lemma. Lemma (Eqivalence between convolution nuclear normand Fourier 1 norm ). Given a smooth or periodic time seriesx R , its circulant (convolution) matrix C(x) R reflects theTucker low-rankness, depicted by the convolutional nuclear norm.This property can be revealed by using the Discrete Fourier Transform(DFT). Let the DFT matrix be U C , then the DFT is achieved by:",
  "Therefore, we have DFT(x)0 = [1, 2, . . . , ]T0 = rank(C(x)),and DFT(x)1 = C(x)": "This lemma means that we can efficiently obtain the singularvalues through DFT. The 0 norm is exactly the matrix rank andthe 1 norm is equal to the nuclear norm C(x). Since C(x)serves as a convex surrogate of the rank of x in an approximatesense , we can equivalently achieve this goal by optimizing theFourier 1 norm. Considering the above equivalence, we can developa sparsity-constrained loss function in the frequency domain:",
  "Flatten(FFT( X, dim = ))1,(14)": "where FFT() is the Fast Fourier Transform (FFT), Flatten() :R R rearranges the tensor form and 1 is the vector 1norm. Since the spatiotemporal matrix can be regarded as a specialRGB image from a global viewpoint, it also features a sparse Fourierspectrum in the space dimension . We apply the FFT on boththe space and time axes and then flatten it into a long vector. LFILis in fact a unsupervised loss that encourages the imputed valuesto be naturally compatible with the observed values globally.",
  "L = Lrecon + LFIL,(15)": "where is a weight hyperparameter. It is worth commenting thatthe two loss functions complement each other: Lrecon promptsthe model to reconstruct the masked observations as precisely aspossible in the space-time domain and LFIL generalizes on unob-served points with regularization on the spectrum. This makesImputeFormer work effectively in highly sparse observations.",
  "EMPIRICAL EVALUATIONS": "In this section, we evaluate our model on several well-known spa-tiotemporal benchmarks, comparing it with state-of-the-art base-lines, and testing its generality on different scenarios. Then compre-hensive analysis and case studies are provided. A brief summary ofthe adopted datasets is shown in Tab. 1. Detailed descriptions of ex-perimental settings are provided in Section A.2. PyTorch implemen-tations are available at",
  "Results on Traffic Benchmarks": "The imputation results on traffic speed and volume data are givenin Tab. 2. As can be seen, ImputeFormer consistently achieves thebest performance in all traffic benchmarks. Two strong competitorsGRIN and SPIN show promising results in traffic speed data, whichalign with the results of their respective papers . However,their performance is inferior on volume datasets and is surpassedby simple baselines such as ST-Transformer and Bi-MPGRU. Com-pared to deep models, pure low-rank methods, such as matrix fac-torization and tensor completion, are less effective due to limitedcapacity. As for missing patterns, the structured block missing ismore challenging than the point missing pattern. For instance, thevanilla Transformer is competitive in the point missing case, whileit is ineffective in block missing case. Generally, ImputeFormeroutperforms others by a large margin in this tricky scenario.",
  "Results on Environmental and Energy Data": "By exploiting the underlying low-rank structures, ImputeFormercan serve as a general imputer in a variety of spatiotemporal data.To demonstrate its versatility, we perform experiments on otherspatiotemporal data, including energy and environmental data. Re-sults are given in Tab. 3. It is observed that ImputeFormer exhibitssuperiority in other spatiotemporal datasets beyond traffic data.In particular, the correlation of solar stations cannot be described",
  "% 5.8% 20.2% 5.8% 4.4% 7.5% 15.9% 7.9% 26.5% 15.0% 13.8% 21.3%": "by physical distance and can be inferred from the data. After com-paring the performance of SAITS, Transformer, ST-Transformer,and ImputeFormer, it can be concluded that direct attention compu-tations on both temporal and spatial dimensions are less beneficialthan the low-rank attention. Furthermore, the spatial correlation ofenergy production is less pronounced. Canonical attention on thespatial axis can be redundant and generate spurious correlations.The use of embedded attention in our model can alleviate this issue.",
  "Ablation Study": "To justify the rationale of model designs, we conduct ablation stud-ies on the model structure. Results are shown in Tab. 4. Severalintriguing findings can be observed: (1) After removing any of thetemporal and spatial attention modules, the performance degen-erates substantially; especially, the spatial interaction contributesto the inference of block missing patterns significantly, while thetemporal modules are crucial for point missing scenarios. (2) The in-corporation of MLP benefits little for the imputation, which validatesour argument in Section A.1.2. (3) Compared to hierarchical loss",
  "ImputeFormer11.5212.1817.351.962.172.79": "Inference with Varying Sequence Length. In reality, the imputa-tion model can face time series with different lengths and samplingfrequencies. We can adopt a well-trained model to perform infer-ence on varying sequence length. Results are shown in . Itis obvious that ImputeFormer can readily generalize to sequenceswith different lengths and more robust than other models.",
  ": Inference under different lengths of input sequencewith a single trained model (zero-shot)": "Dealing with Highly Sparse Observation. To evaluate the per-formance on highly sparse data, we further train and test modelswith lower observation rates. Results are shown in Tab. 6. Generallyspeaking, both Transformer- and RNN-based models are suscepti-ble to sparse training data. Due to the low-rank constraints on theattention matrix and loss function, our model is more robust withhighly sparse data. Since the attention map in SPIN is calculatedonly at the observed points, it is more stable than other baselines.But our model consistently achieves lower imputation errors.Random Masking in Training. Random masking strategy isused to create supervised samples for model training. Therefore,the distribution of masking samples of training data and missingobservations of testing data should be close to ensure good per-formance . However, it can be difficult to know exactly themissing patterns or missing rates in advance in many scenarios.Therefore, a proper masking strategy is of vital importance. To",
  "ImputeFormer8.899.2316.968.80": "evaluate the impact of the masking rate in training data, we furtherconsider four different masking strategies during model training:the masking rates are set to 0.25, 0.5, 0.75, and a combination of them[0.25, 0.5, 0.75] respectively. As shown in Tab. 7, most models per-form best when the masking rate is close to the missing rate in thepoint missing scenario (e.g., 25%). However, when the missing rateis unclear due to randomness within the failure generation process,such as the block missing, the combination strategy is more advan-tageous. For example, Transformers including ST-Transformer,SAITS, and ImputeFormer can benefit from this strategy. More im-portantly, such a hybrid masking method enables the model to worksuccessfully on varying observation rates during inference. 00.0050.010.050.10.2 lambda 11.25 11.50 11.75 12.00 12.25 12.50 12.75 13.00 13.25",
  "Case Studies: Interpretability": "This section studies the interpretability using data examples.Spectrum Analysis. To corroborate the hypothesis that our modelhas the merits of both deep learning and low-rank methods, weanalyze the singular value (SV) spectrum of the imputations. shows the cumulative SV distribution of different competingmodels. ImputeFormer has a close SV cumulative distribution tocomplete data, and the first 85 SVs can account for 80% of the energy.There exist two additional interesting observations: (1) deep learn-ing models without explicit low-rank modeling such as canonicalTransformers downplay the role of the first few dominant SVs; (2)pure low-rank models such as MF generate an oversmoothing resultthat too much energy is constrained to the first part of spectrum.Thus, we can ascribe the desirable performance of our model to thegood balance of significant signals and high-frequency noise.",
  ": Singular spectrum ofdata and node embedding": "Interpretations on Spatial Embedding. To illustrate the role ofnode embedding, we analyze the SV spectrum of the PEMS08 datain . The complete data show a prominent low-rank property,but the SVs of incomplete data dramatically expand. In contrast,the node embedding also displays a similar low-rank distribution,which can act as a dense surrogate for each sensor. Furthermore,we analyze the multivariate attention map obtained by correlatingthe node embedding in . It is evident that as the embedded at-tention layers become deeper, the learned attention maps approachthe actual ones. However, incomplete data produce noisy correlationswith little informative pattern. displays the t-SNE visualizationof each node embedding with two projected coordinates in PEMS08.The embeddings tend to form clusters, and different clusters areapart from others. This phenomenon is in accordance with highwaytraffic sensor systems that proximal sensors share similar readings. Attention Map at 1-st EA Layer #of Sensor #of Sensor #of Sensor Attention Map at 3-rd EA Layer #of Sensor Attention Map of Complete Data #of Sensor Attention Map of Sparse Data",
  ": Multivariate attention maps of PEMS08 data": "Interpretations on Temporal Projector. To illustrate the mecha-nism of the temporal projected attention in Eqs. (7) and (8), inflowand outflow attention maps are shown in Figs. 11. It can be seenthat these matrices quantify how the information of incompletestates flows into compact representations and then is recovered to Coordinate 1 Coordinate 2 t-SNE Visualization of Node Embedding",
  ": Singular values ofdifferent hidden states in thetemporal attention layer": "complete states. Inflows show that only a fraction of the message isdirected towards the projector, while different attention heads canprovide varying levels of information density. Meanwhile, outflowsindicate that a small number of temporal modes can reconstructuseful neural representations for imputation. This can be analogousto the low-rank reconstruction process, which serves as an inductivebias for time series with low information density. We further examinethe SV distribution of different hidden states in the last temporalattention layer. As evidenced by , after flow through theprojected attention layer, the hidden states have lower SVs than theincomplete inputs and are closer to the complete representations. Inflow at Different Attention HeadsOutflow at Different Attention Heads Projected Attention at Head 0 Projected Attention at Head 1 Projected Attention at Head 2 Projected Attention at Head 3 # of Step # of Coordinate # of Step # of Coordinate Projected Attention at Head 0 Projected Attention at Head 1 Projected Attention at Head 2 Projected Attention at Head 3 # of Coordinate# of Coordinate # of Step# of Step",
  "CONCLUSION": "This paper demonstrates a low rankness-induced Transformermodel termed ImputeFormer to address the missing spatiotemporaldata imputation problem. Taking advantage of the low-rank fac-torization, we design projected temporal attention and embeddedspatial attention to incorporate structural priors into the Trans-former model. Furthermore, a Fourier sparsity loss is developed toregularize the solutions spectrum. The evaluation results on vari-ous benchmarks indicate that ImputeFormer not only consistentlyachieves state-of-the-art imputation accuracy, but also exhibits highcomputational efficiency, generalizability across various datasets,versatility for different scenarios, and interpretability. Therefore,we believe that it has the potential to advance research on spa-tiotemporal data for general imputation tasks. Future work canadopt it to achieve time series representation learning task andexplore the multipurpose pretraining problem for time series. The work was supported by research grants from the National Nat-ural Science Foundation of China (52125208), National Key R&DPrograms of China (2022YFB2602100), the China National Postdoc-toral Program for Innovative Talents (BX20220231), the China Post-doctoral Science Foundation (2022M712409), and the Science andTechnology Commission of Shanghai Municipality (22dz1203200).",
  "Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. 2017. Diffusion convolu-tional recurrent neural network: Data-driven traffic forecasting. arXiv preprintarXiv:1707.01926 (2017)": "Wei Liang, Yuhui Li, Kun Xie, Dafang Zhang, Kuan-Ching Li, Alireza Souri, andKeqin Li. 2023. Spatial-Temporal Aware Inductive Graph Neural Network forC-ITS Data Recovery. IEEE Transactions on Intelligent Transportation Systems 24,8 (2023), 84318442. Yuebing Liang, Zhan Zhao, and Lijun Sun. 2022. Memory-augmented dynamicgraph convolution networks for traffic data imputation with diverse missing pat-terns. Transportation Research Part C: Emerging Technologies 143 (2022), 103826.",
  "Guangcan Liu and Wayne Zhang. 2022. Recovery of future data via convolutionnuclear norm minimization. IEEE Transactions on Information Theory 69, 1 (2022),650665": "Hangchen Liu, Zheng Dong, Renhe Jiang, Jiewen Deng, Jinliang Deng, Quan-jun Chen, and Xuan Song. 2023. Spatio-temporal adaptive embedding makesvanilla transformer sota for traffic forecasting. In Proceedings of the 32nd ACMInternational Conference on Information and Knowledge Management. 41254129. Ji Liu, Przemyslaw Musialski, Peter Wonka, and Jieping Ye. 2012. Tensor comple-tion for estimating missing values in visual data. IEEE transactions on patternanalysis and machine intelligence 35, 1 (2012), 208220.",
  "Yonghong Luo, Xiangrui Cai, Ying Zhang, Jun Xu, et al. 2018. Multivariate timeseries imputation with generative adversarial networks. Advances in neuralinformation processing systems 31 (2018)": "Yonghong Luo, Ying Zhang, Xiangrui Cai, and Xiaojie Yuan. 2019. E2gan: End-to-end generative adversarial network for multivariate time series imputation.In Proceedings of the 28th international joint conference on artificial intelligence.AAAI Press Palo Alto, CA, USA, 30943100. Jiawei Ma, Zheng Shou, Alireza Zareian, Hassan Mansour, Anthony Vetro, andShih-Fu Chang. 2019. CDSA: cross-dimensional self-attention for multivariate,geo-tagged time series imputation. arXiv preprint arXiv:1905.09904 (2019).",
  "Tong Nie, Guoyang Qin, Lijun Sun, Wei Ma, Yu Mei, and Jian Sun. 2023. Contex-tualizing MLP-Mixers Spatiotemporally for Urban Data Forecast at Scale. arXivpreprint arXiv:2307.01482 (2023)": "Tong Nie, Guoyang Qin, Yunpeng Wang, and Jian Sun. 2023. Correlating sparsesensing for large-scale traffic speed estimation: A Laplacian-enhanced low-ranktensor kriging approach. Transportation Research Part C: Emerging Technologies152 (2023), 104190. Tong Nie, Guoyang Qin, Yunpeng Wang, and Jian Sun. 2023. Towards better trafficvolume estimation: Jointly addressing the underdetermination and nonequilib-rium problems with correlation-adaptive GNNs. Transportation Research Part C:Emerging Technologies 157 (2023), 104402.",
  "Pratyusha Sharma, Jordan T Ash, and Dipendra Misra. 2023. The Truth is inThere: Improving Reasoning in Language Models with Layer-Selective RankReduction. arXiv preprint arXiv:2312.13558 (2023)": "Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li.2021. Efficient attention: Attention with linear complexities. In Proceedings of theIEEE/CVF winter conference on applications of computer vision. 35313539. Yusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. 2021. Csdi: Con-ditional score-based diffusion models for probabilistic time series imputation.Advances in Neural Information Processing Systems 34 (2021).",
  "Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and MingshengLong. 2022. Timesnet: Temporal 2d-variation modeling for general time seriesanalysis. arXiv preprint arXiv:2210.02186 (2022)": "Yuankai Wu, Dingyi Zhuang, Aurelie Labbe, and Lijun Sun. 2021. Inductivegraph neural networks for spatiotemporal kriging. In Proceedings of the AAAIConference on Artificial Intelligence, Vol. 35. 44784485. Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, Xiaojun Chang, and ChengqiZhang. 2020. Connecting the Dots: Multivariate Time Series Forecasting withGraph Neural Networks. In Proceedings of the 26th ACM SIGKDD InternationalConference on Knowledge Discovery and Data Mining. 753763. Yongchao Ye, Shiyao Zhang, and James JQ Yu. 2021. Spatial-temporal trafficdata imputation via graph attention convolutional network. In InternationalConference on Artificial Neural Networks. Springer, 241252.",
  "Hidden StatesHidden States": ": (a) MPGRU adopts Bi-RNNs to gather availablereadings from consecutive time points and GCNs to collectneighborhood data on predefined graphs. (b) Transformerscompute all pairwise correlations of the raw data in bothspatial and temporal axes. (c) ImputeFormer utilizes projectedattention along the temporal axis and embedded attentionbased on the node representation in the spatial axis. A.1.2Effective Input Embedding. Unlike images or languages, timeseries have low semantic densities . Therefore, many forecastingmodels flatten and abstract the input series to reduce informationredundancy . Specifically, given the input X:+ R ,each series can be processed by a MLP shared by all series: z,(0) =MLP(x), where MLP() : R R. However, we claim that thistechnique is not suitable for imputation. If we express it as follows:",
  "each time step, thus overfitting the missing patterns in the trainingdata. Therefore, we suggest not to use linear mappings on the timeaxis to account for the varying missing time points": "A.1.3Low-Rankness in Self-Attention. Wang et al. studiedthe observation that the self-attention matrix in Transformer islow-rank and proposed a linear attention. Our proposed temporalprojected attention model shares a similar idea as this work buthas different mechanisms and formulations. We indicate the differ-ences in the following exposition. Given Q, K, V R , and theprojector P R, temporal projected attention is formulated as:",
  ")FV.(18)": "As for complexity, we can compute Eq. (17) in the order:(QPT) >(PKT) > (PKT)V > (QPT)(PKT)V, which admits (4)complexity. Similarly, Eq. (18) has the same complexity. Althoughof the same complexity, our model has an explicit and symmetricformulation that brings improved model expressivity. The advan-tages are threefold: (1) explicit low-rank factorization: Linformer does not directly achieve a low-rank factorization of the atten-tion matrix. It first computes the full attention matrix QKT withsize and then compresses it to . Instead, ImputeFormerdirectly factorizes the full attention matrix from to ,,which is beneficial for dealing with redundancy and missingnessin the attention matrix; (2) pattern adaptation: Linformer sets thecompression matrix E, F R completely learnable, which isagnostic to the missing patterns of Q, K, and V. These static pa-rameters cannot account for the varying missing patterns. Instead,we obtain the factor matrix through the query QPT, whichis pattern-adaptive; (3) increased capacity: Linformer has learnable parameters, while ours has parameters, which hasa larger model capacity while having the same time complexity.",
  "A.2Reproducibility": "A.2.1Implementations. We build our model and baselines based onthe SPIN repository ( All experiments were performed on a single NVIDIARTX A6000 GPU (48 GB). For the hyperparameters of ImputeFormer,we set the hidden size to 256, the input projection size to 32, thenode embedding size to 64, the projected size to 6, the number ofattention layers to 3, and the sequence length to 24 for all data.We also keep the same training, validation and evaluation split as and report the metrics on the masked evaluation points. A.2.2Dataset Descriptions. We adopt heterogeneous spatiotempo-ral benchmark datasets to evaluate the imputation performance.Traffic Speed Data. Our experiments include two commonly usedtraffic speed datasets, named METR-LA and PEMS-BAY. METR-LA con-tains spot speed data from 207 loop sensors over a period of 4",
  ": Visualization examples of imputation for traffic speed and volume data under different missing patterns": "months from Mar 2012 to Jun 2012, located at the Los AngelesCounty highway network. PEMS-BAY records 6 months of speeddata from 325 static sensors in the San Francisco South Bay Area.Traffic Volume Data. We adopt four traffic volume data, includingPEMS03, PEMS04, PEMS07, and PEMS08. They contain the highwaytraffic volume record collected by the Caltrans Performance Mea-surement System (PeMS) and aggregated into 5-minute intervals.Energy and Environmental Data. Four energy and environmen-tal data are selected to evaluate the generality of models, including:(1) Solar: solar power production records from 137 synthetic PVfarms in Alabama state in 2006, which are sampled every 10 minutes;(2) CER-EN: smart meters measuring energy consumption from theIrish Commission for Energy Regulation Smart Metering Project. Following the setting in , we select 435 time series aggre-gated at 30 minutes for evaluation. (3) AQI: PM2.5 pollutant recordscollected by 437 air quality monitoring stations in 43 Chinese citiesfrom May 2014 to April 2015 with the aggregation interval of 1 hour.Note that AQI data contains nearly 26% missing data. (4) AQI36: asubset of AQI data which contains 36 sensors in Beijing distinct. A.2.3Experimental Settings and Baseline Methods. This sectiondescribes the detailed information on experimental setups.Missing patterns. For traffic, Solar and CER-EN, we consider twoscenarios discussed in : (1) Point missing: randomly removeobserved points with 25% probability; (2) Block missing: randomlydrop 5% of the available data and at the same time simulate a sensorfailure lasting for L U(12, 48) steps with 0.15% probability.We keep the above missing rates the same as in the previous work. In addition, we also evaluated the performance under sparserconditions. For example, the block missing with 10% probabilitycorresponds to a total missing rate of 90 95%. Note that matrixor tensor models can only handle in-sample imputation, where theobserved training data and the test data are in the same time period.However, deep models can work in out-of-sample scenarios where the training and test sequences are disjoint. We adopt theout-of-sample tests for deep models and in-sample tests for others.Baseline Methods. We compare our model with SOTA deep-learning and low-rank imputation methods. For statistical and opti-mization models, we consider: (1) Observation average (Average);(2) Temporal regularized matrix factorization (TRMF) ; (3) Low-rank autoregressive tensor completion (LRTC-AR) ; (4) MICE . For deep imputation models, we select several competitive baselines:(1) SPIN : sparse spatiotemporal attention model with state-of-the-art imputation performance; (2) GRIN : message-passing-based bidirectional RNN model with competitive performance; (3)SAITS : Temporal Transformer model with diagonally maskedattention; (4) BRITS : bidirectional RNN model for imputation; (5)rGAIN : GAIN model with bidirectional recurrent encoder anddecoder; (6) Transformer/ST-Transformer : canonical Trans-former with self-attention in temporal or spatial-temporal dimen-sions; (7) TiDER : matrix factorization with disentangled neuralrepresentations; (8) TimesNet : 2D convolution-based generaltime series analysis model; (9) BiMPGRU: a bidirectional RNN-basedGCN model, which is similar to DCRNN .Ablation Studies. Particularly, we examine the following vari-ations: (a) Temporal blocks: we replace the temporal interactionmodule with MLP or directly remove it; (b) Spatial blocks: we replacethe spatial interaction module with MLP or directly remove it; (c)Loss function: We remove the FIL or replace it with a hierarchicalloss used in ; (d) Architecture: we evaluate the impacts ofthe order of spatial-temporal blocks and the joint attention strategy.We adopt two data to evaluate and other settings remain the same.Evaluation Metrics. To evaluate the model, we simulate differentobservation conditions by removing parts of the raw data to con-struct incomplete samples based on different missing rates (missing).Evaluation metrics are then calculated for these simulated missingpoints. We use a masking indicator Mmissing to denote these loca-tions in which the unobserved (missing) values are marked as ones,observed as zeros. Note that the masked points for evaluation arenot available for the models during all stages. The mean absoluteerror (MAE) is adopted to report the results.",
  "A.3Imputation Visualization": "We provide several visualization examples in . As evidenced,ImputeFormer can generate reasonable imputations by learningthe inherent structures of spatiotemporal data. Previous studieshave discovered that low-rank models can cause oversmoothingestimation . Due to the representation power of deep architec-tures, our model can provide a detailed reconstruction. In particular,although only limited temporal information is available in the blockmissing case, it can resort to the node embedding as the query tospatial relations, thereby generating an effective imputation."
}