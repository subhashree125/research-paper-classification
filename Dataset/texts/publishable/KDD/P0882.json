{
  "Abstract": "This paper presents the solution of our team APEX in the Meta KDDCUP 2024: CRAG Comprehensive RAG Benchmark Challenge. TheCRAG benchmark addresses the limitations of existing QA bench-marks in evaluating the diverse and dynamic challenges faced byRetrieval-Augmented Generation (RAG) systems. It provides a morecomprehensive assessment of RAG performance and contributesto advancing research in this field. We propose a routing-based do-main and dynamic adaptive RAG pipeline, which performs specificprocessing for the diverse and dynamic nature of the question in allthree stages: retrieval, augmentation, and generation. Our methodachieved superior performance on CRAG and ranked 2nd for Task2&3 on the final competition leaderboard. Our implementation isavailable at this link:",
  "Mingyue Cheng is the corresponding author": "Given a question, a RAG system queries external sources to re-trieve relevant information and subsequently provides groundedanswers. Despite its potential, RAG continues to face numerouschallenges, including the selection of the most relevant information,the reduction of question answering latency, and the synthesis ofinformation to address complex questions.To bridge this gap, Meta introduced the Comprehensive RAGBenchmark (CRAG) , a factual question answering benchmarkof 4,409 question-answer pairs and Mock APIs to simulate weband Knowledge Graph (KG) search, and hosted the KDD CUP 2024Challenge.",
  "Dataset Description": "The CRAG contains two parts of data: the QA pairs and the contentfor retrieval.QA pairs. The CRAG dataset contains a rich set of 4,409 QApairs covering five domains: finance, sports, music, movie, andopen domain, and eight types of questions. For the KDD CUP 2024Challenge, the benchmark data were splited into three sets withsimilar distributions: validation, public test, and private test at 30%,30%, and 40%, respectively. In total, 2,706 examples from validationand public test sets were shared.The dataset also reflects varied entity popularity from popularto long-tail entities, and temporal spans ranging from seconds toyears. Given the temporal nature of many questions, each question-answer pair is accompanied by an additional field denoted as \"querytime.\" This temporal marker ensures the reliability and uniquenessof the answers within their specific temporal context.Content for retrieval. The CRAG dataset incorporates twotypes of content for retrieval to simulate a practical scenario forRAG: web search and knowledge graph (KG) search. This encom-passes up to 50 full HTML pages for each question, retrieved from",
  "Task Desription": "This challenge comprises three tasks designed to improve question-answering (QA) systems.TASK 1: The organizers provide 5 web pages per question, poten-tially containing relevant information. The objective is to measurethe systems capability to identify and condense this informationinto accurate answers.TASK 2: This task introduces mock APIs to access informationfrom underlying mock Knowledge Graphs (KGs), with structureddata possibly related to the questions. Participants use mock APIs,inputting parameters derived from the questions, to retrieve rele-vant data for answer formulation. The evaluation focuses on thesystems ability to query structured data and integrate informationfrom various sources into comprehensive answers.TASK 3: The third task increases complexity by providing 50web pages and mock API access for each question, encounteringboth relevant information and noise. It assesses the systems skillin selecting the most important data from a larger set, reflectingthe challenges of real-world information retrieval and integration.Each task builds upon the previous, steering participants towarddeveloping sophisticated end-to-end RAG systems. This challengeshowcases the potential of RAG technology in navigating and mak-ing sense of extensive information repositories, setting the stagefor future AI research and development breakthroughs.",
  "Router": "Routing is a crucial component of RAG systems, especially in real-world QA scenarios. In practical applications, RAG systems fre-quently incorporate multiple data sources. In the CRAG Challenge,we have three distinct data sources: Web Pages, Mock KGs, andMock APIs. The diversity of questions requires routing queries todifferent data sources, individually or in combination. Even withina single data source, such as Mock APIs, the question-specific se-lection of appropriate APIs is crucial. Furthermore, we can tailor prompt templates based on the nature of the question or routequestions to different post-processing components.In response to the specific characteristics of the questions in theCRAG Challenge, we designed two specialized routers: the DomainRouter and the Dynamism Router. These routers are designed to effi-ciently navigate the complex landscape of multisource informationretrieval and question-specific processing in our RAG system.Domain Router. Domain router is fundamentally a classifier,more specifically, a sentence classifier. Given a query, the domainrouter assigns a specific domain from a predefined set: finance,sports, music, movie, and open. Based on the assigned domain, theworkflow is then routed to the corresponding path.We utilize Llama3-8B-Instruct as our base model and enhanceit with a classification head (Multilayer Perceptron, MLP) for do-main classification. The 8B model inherently demonstrates a robustcapability to comprehend the domain of queries. We randomly splitthe CRAG dataset into training, validation, and test sets with aratio of 8:1:1. Based on this split, we performed a simple LORA(Low-Rank Adaptation) fine-tuning to adapt to the distributionof the CRAG dataset. This approach facilitates the development ofa high-quality classifier with minimal additional training.The trained Domain router is employed at multiple stages withinthe system. During the retrieval phase, the Domain router is pri-marily used to select appropriate APIs. Following the retrieval ofWeb Pages and APIs, it is further applied for selective fusion of theretrieved knowledge. In the generation phase, we first customizethe prompt templates based on the domain. Subsequently, afterthe model completes its generation, the Domain is also utilized forcorresponding post-processing.Dynamic Router. Analogous to the Domain router, the Dy-namism router is also a sentence classifier. Given a question, theDynamism router assigns a specific Dynamism, specifically oneof: static, slow-changing, fast-changing, or real-time. The specifictraining methodology for the Dynamism router is congruent withthat of the Domain Router, and thus will not be recapitulated here.Due to the inherent limitation of Large Language Models inupdating their internal knowledge, they are prone to providingoutdated answers to dynamic questions. Even when employingexternal knowledge through RAG, LLMs can readily generate hal-lucinations as most data sources are static, unless real-time APIsare utilized. In the absence of real-time APIs, the more rapidlya question changes over time, the more susceptible LLMs are tohallucinations.To attenuate hallucinations arising from dynamic questions, weimplemented the Dynamism router for post-processing. In scenarioswhere real-time APIs are inaccessible, we excluded certain real-timequestions and those that change rapidly over time.",
  ": The pipeline of Web Retriever": "complexity by offering 50 Web Pages for each question, presentingboth pertinent information and noise.To enhance our QA system, we need to extract useful and rele-vant information from web search results. The primary process forretrieving web content is illustrated in . (1) HTML Parsing: Structured HTML is often unnecessarilyverbose and contains substantial extraneous information thatcan impede subsequent segmentation operations. Therefore,it is crucial to first convert this structured format into naturallanguage text that is more amenable to processing by LargeLanguage Models. We conducted experiments with variousHTML parsing methods, including BeautifulSoup, Newspa-per, Markdownify, and several others. After evaluating bothparsing efficiency and quality, we ultimately selected News-paper. See A.3 for more details about experiment results.",
  "(2) Pre-ranking (Task 3 only): For Task 3, fine-grained pro-cessing 50 Web Pages would be excessively time-consuming": "Therefore, we initially filter out an appropriate amount ofrelevant text before ranking. Specifically, we segment all thetext from the Web Pages into chunks of 1024 tokens (cal-culated based on tokens rather than characters). For thesesegmented text chunks, we use BM25 to select the top50 most relevant text blocks. (3) Ranking: In the ranking phase, we further refine the pre-ranked text blocks. For task 1&2, the text blocks are the 5raw plain text extracted from HTML. The text blocks are seg-mented into smaller chunks, each comprising 256 tokens. Wethen transform these 256-token chunks into embeddings uti-lizing the bge-m3 model. Finally, we calculate the cosinesimilarity to select top 10 relevant chunks.",
  "(4) Re-ranking: We utilized bge-m3-v2-reranker to re-rankthe aforementioned 10 relevant chunks, ultimately selectingthe top 5 segments": "2.2.2Mock APIs. A total of 38 Mock APIs were provided for tasks2&3. As mentioned above, these Mock APIs can be categorizedinto five distinct domains, with no overlap between the APIs ofdifferent domains. Naturally, we designed separate workflows foreach domain using a Domain Router. However, the overarchingprocess flow of the workflows across all domains remains consistent,as shown in : (1) Named Entity Recognition (NER): We directly utilizeLlama3-70B-Instruct to identify and classify named enti-ties in the question into predefined categories, such as movienames and artist names. Specific prompts are presented inthe Appendix A.4.",
  ": The pipeline of API Extractor": "(2) Entity Match: Matching extracted entities with API inputparameters. Taking finance as an example, the input parame-ter for finance API is typically the ticker symbol, while userquestions often contain full company names. We need toconvert the company names to their corresponding tickersymbols. We first perform exact matching, requiring the ex-tracted entity to be exactly the same as the input parameter.If no match is found, we then use BM25 to select the mostsimilar one. (3) Time Information Extraction: In addition to entities, nu-merous API inputs incorporate temporal information, re-quiring the extraction of relevant time points or intervalsfrom user inputs. Notably, temporal information is oftendependent with query time, as illustrated by terms such as\"yesterday.\" In such cases, we must determine the specifictime point based on the query time through relative timecomputation. We first use regular expressions to match cer-tain time and date-related terms. Once matched, we use twoPython packages (pytz and datetime) to calculate the Abso-lute Time. If no matches are found, the current time is usedby default. (4) API Select: Each domain comprises numerous APIs, notall of which are inherently relevant to a users query. Wehave manually designed a set of rules to select APIs thatcorrespond to the given question. To minimize the risk ofoverfitting the rules to the training set, we implementedconstraints on the rule design process, prioritizing simplicityand robustness. (5) Json to Markdown: The JSON output from APIs, whilestructured and machine-readable, may not be optimal forlarge language models (LLMs) to process efficiently. Con-verting this JSON data into a more LLM-friendly Markdownformat can enhance the models ability to understand andutilize the information.",
  "Augmentation": "We employ input-layer integration for generation augmentation,which combines retrieved information/documents with the originalinput/query and jointly passes them to the generator. In contrastto common input-layer integration, we do not utilize all retrieveddocuments. For different domains, we select specific data sourcesand integrate them to construct the final reference.For the open domain, since we did not employ a Mock API, weexclusively utilized web search results. For the movie and musicdomains, where most queries are relatively static or evolve slowly,results retrieved from both web pages and mock APIs remain rele-vant. Therefore, we chose to integrate these two sources. For thesports and finance domains, which involve numerous real-time andfast-changing queries, we exclusively used Mock APIs to ensurethe timeliness and relevance of the retrieved information.",
  "Generation": "In the generation phase, we employed two widely-used methodolo-gies: Chain-of-Thought (CoT) reasoning and In-context Learning.After generation, we performed a simple post-processing proce-dure on the generated results based on the Domain and DynamismRouters. 2.4.1Chain of Thought. Chain-of-Thought (CoT) enhancesthe reasoning process of language models by prompting them toarticulate intermediate steps in problem-solving. This approach notonly enhances the models ability to handle complex tasks but alsosignificantly reduces hallucinations. 2.4.2In-context Learning. We improve the models ability torecognize invalid questions, particularly those based on false premises,through In-context Learning. We develop adaptive few-shot exam-ples , selecting two of the most representative invalid questionsamples for each domain and elucidating the reasons for their inva-lidity. Using the sports domain as an example, our few-shot samplesare as follows:",
  "Steph Curry has never participated in the NBA dunk contest": "2.4.3Post-processing. Before finalizing the results, we imple-ment basic post-processing strategies. Based on the question do-main and volatility, we assign \"I dont know\" responses to queriessusceptible to hallucination. For domains lacking real-time API ac-cess, specifically open, movie, and music categories, we designated\"I dont know\" answers to fast-changing and real-time questions.Furthermore, due to the models limited mathematical computation",
  "Experiments": "In this section, we present our main results and ablation studies forsome crucial components.We did not employ a strategy of fine-tuning the LLM; instead,we used the LLM in a zero-shot setting. According to the rules setby the organizers, we used Llama3-70B-Instruct for all our LLMs.For the embedding model, we used BAAI/bge-m3, and for the rerankmodel, we used BAAI/bge-m3-v2-reranker. In the 1371 public testcases officially released for this round, we compared the followingbaselines: LLM only (using the LLM without retrieving references)and straightforward RAG (the baseline provided by the organizers,using straightforward RAG solutions).",
  "Metrics and Evaluation": "In line with CRAG Benchmark, we conduct a model-based auto-matic evaluation for our experiment. Automatic evaluation employsrule-based matching and GPT-4 assessment to check answercorrectness. It will assign three scores: correct (1 point), missing (0points), and incorrect (-1 point). The final score for a given RAGsystem is computed as the average score across all examples in theevaluation set.",
  "Ablation Study": "presents the ablation study for major strategies employedin our solution.During the retrieval phase, we implemented several strategies,including pre-ranking, re-ranking, Entity Match, and Time Infor-mation Extraction. Ablation studies revealed that pre-ranking andre-ranking marginally reduce performance, while Entity Match andTime Information Extraction significantly decrease performance.Both pre-ranking and re-ranking significantly contribute to theimprovement of retrieval quality. Pre-ranking enhances retrievalperformance by proactively filtering out a significant amount ofnoise, while re-ranking ensures the accuracy of retrieval resultsthrough more refined and granular sorting. The enhancement inretrieval quality ultimately translates into an increase in answeraccuracy. The absence of pre-ranking and re-ranking demonstrablyleads to a substantial decrease in the accuracy of the final answers.Furthermore, pre-ranking significantly enhances retrieval effiencyand reduces the retrieval time. Entity Matching and Time Informa-tion Extraction form the basis of using MOCK APIs. They ensurethe accuracy of API call parameters, which is crucially linked to theoverall performance. The absence of either component can resultin a significant performance decline.",
  "KDD 24, August 25-29, 2024, BarcelonaJie Ouyang, Yucong Luo, Mingyue Cheng, Daoyu Wang, Shuo Yu, Qi Liu, and Enhong Chen": "During the generation phase, we employed 2 main components:domain specific fewshot examples and Chain of Thought prompt.Both aforementioned components led to a substantial reduction inhallucinations. The combined implementation of these componentsyielded a reduction in hallucinations of up to 71%, consequently re-sulting in a 22% increase in the final score. The experimental resultsdemonstrate the effectiveness and necessity of the two components.",
  "Perspectives": "Our method presents a robust and versatile framework for address-ing a wide range of dynamic and complex real-world problems.This approach, however, also opens up several avenues for furtherinvestigation. Model Cognitive Ability Assessment Most conventionalQA evaluation methods primarily focus on accuracy, neglect-ing the impact of hallucinations. Models should be aware oftheir knowledge boundaries, discerning what they shouldand should not answer. CRAG incorporates hallucinationsinto evaluation metrics, but its settings lack sufficient jus-tification. Responding \"I dont know\" to all questions canyield a satisfactory score. Exploring the assessment of mod-els cognitive abilities using methodologies for evaluatinghuman cognition is a promising research direction. API Integration and Scalability. In real-world scenarios,where extensive API usage is common, our manually de-signed matching rules are likely to prove inadequate. Thedevelopment of a more universal method for selecting andcalling APIs, as well as processing the returned results, rep-resents a promising avenue for future research. Handling Dynamic Information. For questions that in-volve information that changes dynamically over time, sim-ply refusing to answer is merely a basic solution. Futureresearch should focus on exploring methods to acquire themost up-to-date knowledge and determine the timeliness ofinformation. This is crucial to avoid hallucinations causedby outdated knowledge and to ensure the system providesaccurate, current information. Developing techniques forreal-time information retrieval and verification, as well asimplementing mechanisms to assess the reliability and cur-rency of data sources, are key areas for investigation.",
  "Conclusion": "In this paper, we introduce our solution for the Meta KDD CUP2024: CRAG Comprehensive RAG Benchmark. We adopt a classicRAG framework with two specific routers. In the retrieval phase,we demonstrated the process of obtaining high-quality informationfrom various data sources and utilizing the Domain Router for in-formation filtering. In the augmentation phase, we employed theDomain Router in a similar manner for information aggregationbased on domain characteristics. Finally, in the generation phase,we implemented two methods to significantly improve the modelsaccuracy and reduce hallucinations, while further mitigating hallu-cinations through post-processing based on the questions domainand dynamic nature. Our approach offers a viable pathway for addressing the diverseand dynamic challenges encountered in real-world scenarios. Nev-ertheless, our method has certain limitations. We have identifiedseveral inherent issues in the current methodology and providedour insights and reflections on the specific problems related to ourapproach. Ultimately, we anticipate that this study will make amodest contribution to the broader RAG and LLM communities. This research was supported by grants from the National Key Re-search and Development Program of China (Grant No. 2021YFF0901000),the National Natural Science Foundation of China (No. 62337001)and the Fundamental Research Funds for the Central Universities. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-cia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, ShyamalAnadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774(2023).",
  "Meta AI. 2024. Meta LLaMA 3. Meta AI Blog (2024)": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, BenjaminChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, IlyaSutskever, and Dario Amodei. 2020. Language models are few-shot learners. InProceedings of the 34th International Conference on Neural Information ProcessingSystems (Vancouver, BC, Canada) (NIPS 20). Curran Associates Inc., Red Hook,NY, USA, Article 159, 25 pages. Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024.Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity textembeddings through self-knowledge distillation. arXiv preprint arXiv:2402.03216(2024). Mingyue Cheng, Hao Zhang, Jiqian Yang, Qi Liu, Li Li, Xin Huang, LiweiSong, Zhi Li, Zhenya Huang, and Enhong Chen. 2024. Towards PersonalizedEvaluation of Large Language Models with An Anonymous Crowd-SourcingPlatform. Companion Proceedings of the ACM on Web Conference 2024 (2024).",
  "Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevanceframework: BM25 and beyond. Foundations and Trends in Information Retrieval3, 4 (2009), 333389": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoningin large language models. Advances in neural information processing systems 35(2022), 2482424837. Xiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla, Xiangsen Chen, SajalChoudhary, Rongze Daniel Gui, Ziran Will Jiang, Ziyu Jiang, et al. 2024. CRAGComprehensive RAG Benchmark. arXiv preprint arXiv:2406.04744 (2024).",
  "Music NER Prompt": "Please identify and list all the named entities present inthe following question about music instead answering it,categorizing them appropriately (e.g., persons, song, band)Your answer should be short and concise in 50 words. Format your response as follows: For each entity, providethe name followed by its category in parentheses. Categoriesinclude persons, songs and bands. Ensure that your responseis clearly structured and easy to read.",
  "Sports NER Prompt": "Please identify and list all the named entities present inthe following question about sports instead answering it,categorizing them appropriately (e.g., nba team, soccer team,nba player, soccer player) Your answer should be short andconcise in 50 words. Format your response as follows: For each entity, providethe name followed by its category in parentheses. Categoriesinclude nba teams, soccer teams, nba players, soccer players.Ensure that your response is clearly structured and easy toread.",
  "Movie NER Prompt": "Please identify and list all the named entities present inthe following question about movie instead answering it,categorizing them appropriately (e.g., person, movie) Youranswer should be short and concise in 50 words. Format your response as follows: For each entity, providethe name followed by its category in parentheses. Categoriesinclude persons, and movies. Ensure that your response isclearly structured and easy to read.",
  "Finance NER Prompt": "Please identify and list all the named entities present in thefollowing question about finance instead answering it, cate-gorizing them appropriately (e.g., company, ticker symbol)Your answer should be short and concise in 50 words. Format your response as follows: For each entity, providethe name followed by its category in parentheses. Categoriesinclude company, and symbol(which means the ticker symbolof a company). Ensure that your response is clearly structuredand easy to read."
}