{
  "Abstract": "The e-commerce platform has evolved rapidly due to its widespreadpopularity and convenience. Developing an e-commerce shoppingassistant for customers is crucial to aiding them in quickly find-ing desired products and recommending precisely what they need.However, most previous shopping assistants face two main prob-lems: (1) task-specificity, which necessitates the development ofdifferent models for various tasks, thereby increasing developmentcosts and limiting effectiveness; and (2) poor generalization, wherethe trained model performs inadequately on up-to-date products.To resolve these issues, we employ Large Language Models (LLMs)to construct an omnipotent assistant, leveraging their adeptness athandling multiple tasks and their superior generalization capability.Nonetheless, LLMs lack inherent knowledge of e-commerce con-cepts. To address this, we create an instruction dataset comprising65,000 samples and diverse tasks, termed as EshopInstruct1.Through instruction tuning on our dataset, the assistant, namedLLaSA, demonstrates the potential to function as an omnipotentassistant. Additionally, we propose various inference optimizationstrategies to enhance performance with limited inference resources.In the Amazon KDD Cup 2024 Challenge2, our proposed method,LLaSA, achieved an overall ranking of 3rd place on ShopBench,",
  "Xuming Hu is the corresponding author.1Our instruction dataset can be found at": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, Aug 28, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-1-4503-XXXX-X/18/06 including 57 tasks and approximately 20,000 questions, and wesecured top-5 rankings in each track, especially in track4, wherewe achieved the best performance result among all student teams.Our extensive practices fully demonstrate that LLMs possess thegreat potential to be competent e-commerce shopping assistants3.",
  "INTRODUCTION1.1Background": "The rapid growth of e-commerce has transformed how we shop,offering unprecedented convenience and access to a vast array ofproducts. However, this convenience comes with the challenge ofnavigating an overwhelming volume of information. When shop-ping online, users often face the daunting task of sifting throughcountless products, reading numerous reviews, comparing prices,and ultimately making a purchase decision. This process can betime-consuming and stressful, highlighting the complexities inher-ent in online shopping .Large language models (LLMs) offer a promising solution toaddress these challenges . Current techniques often struggle",
  "KDDCup 24, Aug 28, 2024, Barcelona, SpainZhang et al": ": Construction pipeline of EshopInstruct. We de-sign three strategies for building the EshopInstruct dataset:generating data from seed data, extracting data from publiclyavailable ECInstruct, and designing new tasks to generatedata. Based on these strategies, we obtained 65k data points. to fully grasp the nuances of specific shopping terms, customerbehaviors, and the diverse nature of products and languages. Incontrast, LLMs, with their multi-task and few-shot learning capabil-ities, have the potential to enhance the online shopping experiencesignificantly.To encourage LLMs to meet the unique needs of online shopping,enhance user experience, and streamline decision-making, Ama-zon has introduced ShopBench and organized the Amazon KDDCup 2024 challenge. This competition features five tracks, focus-ing on four key shopping skills: Shopping Concept Understanding,Shopping Knowledge Reasoning, User Behavior Alignment, andMultilingual Abilities.",
  "Datasets Description": "ShopBench is a multi-task dataset derived from real-world shoppingdata in the Amazon platform, designed for the Amazon KDD Cup2024 challenge. The dataset is divided into a few-shot developmentset and a test set, designed to more accurately simulate the few-shotlearning settings. It contains 57 tasks and approximately 20,000questions, which are all reformulated into a unified text-to-textgeneration format to facilitate LLM-based solutions. The detailedstatistics of the datasets are summarized in .",
  "In the ShopBench benchmark, five abilities, including Generation,Ranking, Retrieval, Multiple-Choice, and NER (Named Entity Recog-nition), are introduced to evaluate four important shopping skills:": "Track1 (Shopping Concept Understanding): Given the preva-lence of domain-specific concepts in online shopping, thegoal is to enhance LLMs ability to effectively understandand respond to queries about these concepts. Track2 (Shopping Knowledge Reasoning): Considering thecomplex reasoning required for shopping decisions, the goalis to assess the models capability in reasoning about prod-ucts and their attributes using domain-specific implicit knowl-edge. Track3 (User Behavior Alignment): Given the diversity andimplicit nature of user behaviors in online shopping, the goalis to align language models with these behaviors to improvetheir effectiveness in this domain. : The data distribution of development set, includ-ing four important shopping skills: (1) Shopping ConceptUnderstanding; (2) Shopping Knowledge Reasoning; (3) UserBehavior Alignment; (4) Multi-Lingual Abilities, and five abil-ities: (i) Generation; (ii) Ranking; (iii) Retrieval; (iv) Multiple-Choice; (v) NER. Track4 (Multi-Lingual Abilities): Recognizing the need formulti-lingual models in online shopping, the goal is to eval-uate a single models performance across different shoppinglocales without re-training, focusing on multi-lingual con-cept understanding and user behavior alignment.",
  "TRAINING DATASET CONSTRUCTION": "While LLMs exhibit strong generalization across multiple tasks,they often perform poorly in specific domains due to a lack ofrelevant knowledge. This competition involves many tasks relatedto online shopping, and general-purpose models lack knowledgein this area. Therefore, directly adapting a general-purpose modelto the online shopping scenario is quite challenging. To improvethe models performance in this domain, we need to inject relevantknowledge into it.In this challenge, the organizers did not provide a large-scaletraining dataset. As a result, we constructed our training dataset us-ing publicly available data, our data construction pipeline is shownin .",
  "Development Set Analysis": "We analyzed the provided development data to gain insights forconstructing the training dataset. The development set comprises96 data points across 18 different tasks, the distribution of tasktypes is shown in . 2.1.1Shopping Concept Understanding. This track focuses on eval-uating the models ability to understand entities and concepts spe-cific to the online shopping domain, which can be divided into thefollowing sub-tasks:",
  "To increase the proportion of the real-world data in theSFT dataset and provide more knowledge to the LLMs, wealso created a substantial amount of data based on externaldatasets": "To create data that aligns with the task types in the developmentset, we adopted two strategies. Firstly, for task types that can be di-rectly constructed or transformed using existing datasets like ECIn-struct, we generated the corresponding data directly from thesedatasets. For example, tasks such as Elaboration, Extraction andSummarization, Relation Inference, Sentiment Analysis in Track1,and Recommendation based on query in Track3, we identified simi-lar data in ECInstruct . For these tasks, we directly extracteddata from ECInstruct and transformed them into the standard for-mat. Secondly, for task types where it was challenging to extractdata from existing datasets, we utilized LLMs, such as GPT-4, fordata generation. For numeric reasoning, implicit and multi-hopreasoning in Track2, as well as user behavior prediction in Track3,we used GPT-4 for data construction. When using GPT-4 to gener-ate this portion of the data, we provided the model with few-shotexamples and employed the chain-of-thought method, enabling itto generate the reasoning process to ensure data quality.Considering that the task types in the development set do notcomprehensively cover all scenarios, we constructed additionaltasks and corresponding data based on descriptions from varioustracks. For instance, we observed that there is no relevant data aboutthe Concept Normalization task in Track1 and the Daily ProductRecommendation in Track2 in the development set. Therefore, weconstructed corresponding data for them. These data constructionsmay involve transformations from external datasets or generationby LLMs. Additionally, we referred to the methods in Self-Instruct to generate a portion of the data. Specifically, we used de-velopment data as seed data and then utilized GPT-3.5-turbo togenerate instructions and corresponding responses based on thisdata. Subsequently, we employed GPT-4 as a judge to filter the data. Furthermore, given that much of the data constructed throughthe first two methods is generated by LLMs, there may be a con-siderable amount of noise, and scalability could be limited due tocost constraints. Therefore, to introduce a substantial amount ofreal-world data to our models, we have also constructed additionaltasks and corresponding data from external datasets. For example,leveraging the Amazon-ESCI dataset, we constructed tasks suchas Query Generation, Related Product Retrieval, etc. It should benoted that in order to enhance our models multilingual processingcapabilities, we incorporated a significant amount of data relatedto products in various languages other than English during thedataset construction phase.Following the above strategy, we ultimately obtained approxi-mately 65,000 data entries in EshopInstruct. Moreover, to furtheraugment our training dataset, we strategically sampled a subsetof data from the ECInstruct dataset. We then used these data forinstruction tuning.",
  "INSTRUCTION TUNING": "We use instruction tuning to incorporate online shopping-relatedknowledge into the LLMs and enhance their instruction-followingcapabilities. Given the size of our constructed dataset (65,000 en-tries) and our limited training resources, we adopted the LoRA (Low-Rank Adaptation) fine-tuning method, following the standardapproach of auto-regressive language modeling. During phases 1and 2 of the challenge, we experimented with four models6 of differ-ent sizes: Mistral-7B7 , LLama3-8B8 , and Qwen2-7B/72B9 .Some key training hyper-parameters are listed in . We usedthe standard AdamW optimizer for supervised fine-tuning (SFT)optimization, with a cosine learning rate schedule, a peak learn-ing rate of 4 e5, and a 10% warmup ratio. All the models weretrained with multiple NVIDIA A800 80G GPUs. For models withfewer than 10 billion parameters, such as Mistral-7B, LLama3-8B,and Qwen2-7B, we trained on a single GPU without quantization.",
  "Quantification": "During the challenge, the submission will run on a T4 GPU with16GB of memory. In Phase 2, four T4 GPUs will be provided, whichmeans only 64GB of GPU memory will be available. To experi-ment with larger models (such as models with 72B parameters)and minimize the reduction in model capability while reducing therequired GPU memory as much as possible, we leveraged quan-tization techniques. Specifically, we adopted GPTQ quantization, a post-training quantization method where each row of theweight matrix is independently quantized to int4 to reduce errorbut restored to fp16 during inference for better performance. In thischallenge, we only applied quantization to the Qwen2-72B model.After training, we utilized 1,000 data samples from Alpaca generated by GPT-4 for quantization calibration of the model.",
  "Prompting Strategies": "Through the analysis of the development set, we found that manytasks in the challenge involve reasoning. To improve the modelsperformance on these tasks, we introduced Chain of Thought (CoT), a technique that can significantly enhance the complex rea-soning abilities of large language models. we implemented a sim-ple zero-shot Chain-of-Thought in our solution. Additionally, weretrieved the three most relevant samples from the constructedtraining dataset as few-shot examples, which yielded better results.The test data can be roughly divided into multiple-choice andnon-multiple-choice types. We adopt different processing measures and prompts for these two types of data. For questions that may in-volve reasoning, we encourage the model to think more deeply anduse regular expressions to extract the final answer. For generation-related questions, we let the model directly output the final result.Considering the importance of user input in online shopping sce-narios, we have also implemented a simple and effective promptingmethod called Re-Reading which entails re-reading the ques-tion to enhance reasoning capabilities in Large Language Models.",
  "RESULTS": "In this section, we will compare and analyze the performance ofdifferent models across the five tracks, as shown in . Over-all, the size of the model parameters has a considerable impact onperformance, with larger parameter models generally performingbetter. (A) shows relatively weaker performance across Track1 toTrack4, especially on Track2, where it scored only 0.529. In compari-son, (B) shows improved performance across all tracks, particularlyon Track1 and Track5. Despite having the same 7B parametersas (A), (C) performs well across all available tracks, especially onTrack2 and Track4. Comparing (B) and (C), we find that Qwen2-7Bowns a greater potential than LLama3-8B in providing e-commerceshopping assistance. Therefore, we chose the Qwen2 series as ourbackbone model since it performs relatively well.With 72B parameters, (D) demonstrates excellent performanceacross all tracks, particularly on Track1 to Track3, achieving highscores of 0.786, 0.716, and 0.706, respectively. However, its perfor-mance slightly drops on Track4 to 0.654, but it remains at a highlevel. To incorporate the domain knowledge of e-commerce, wefine-tuned Qwen2-72B on ECInstruct and our constructed EshopIn-struct, respectively. Comparing (E) and (F), we can see that (F)consistently and considerably outperforms (E) in all tracks, indicat-ing the superiority of supplementing e-commerce shopping taskswith our EshopInstruct. It is worth mentioning that we find LLMsfine-tuned on ECInstruct perform badly on generation tasks. Theperformance of (F) on specific tasks is detailed in .By utilizing our carefully constructed training dataset EshopIn-struct for instruction fine-tuning and employing effective infer-ence strategies, we ultimately secured 3rd place overall in the Ama-zon KDD Cup 2024 Challenge, 3rd place in Track1, 2nd place inTrack4, and ranked within the top 5 for the remaining tracks.",
  "CONCLUSION": "In this paper, we present our solution for the Amazon KDD Cup2024 Challenge. We constructed a multi-task instruction datasetcalled EshopInstruct, which contained 65,000 samples tailoredto online shopping scenarios. In addition, we utilized EshopIn-struct for instruction tuning on large language models, resultingin knowledgeable shopping assistants named LLaSA. To optimizeinference performance with limited resources, we employed GPTQquantization and prompting strategies such as Chain-of-Thoughtand Re-Reading. Evaluation results demonstrated the effectivenessof our approach, securing 3rd place on the overall leaderboardand ranking within the top 5 in each track. Especially in track4(Multi-lingual Abilities), we obtained the best student team award.",
  "Yupeng Hou, Jiacheng Li, Zhankui He, An Yan, Xiusi Chen, and Julian McAuley.2024.Bridging Language and Items for Retrieval and Recommendation.arXiv:2403.03952 [cs.IR]": "Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, SheanWang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of largelanguage models. arXiv preprint arXiv:2106.09685 (2021). Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, De-vendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,Guillaume Lample, Lucile Saulnier, et al. 2023.Mistral 7B.arXiv preprintarXiv:2310.06825 (2023). Wei Jin, Haitao Mao, Zheng Li, Haoming Jiang, Chen Luo, Hongzhi Wen, HaoyuHan, Hanqing Lu, Zhengyang Wang, Ruirui Li, Zhen Li, Monica Xiao Cheng,Rahul Goutam, Haiyang Zhang, Karthik Subbian, Suhang Wang, Yizhou Sun,Jiliang Tang, Bing Yin, and Xianfeng Tang. 2023. Amazon-M2: A MultilingualMulti-locale Shopping Session Dataset for Recommendation and Text Generation.arXiv:2307.09688 [cs.IR] Yangning Li, Shirong Ma, Xiaobin Wang, Shen Huang, Chengyue Jiang, Hai-TaoZheng, Pengjun Xie, Fei Huang, and Yong Jiang. 2023. EcomGPT: Instruction-tuning Large Language Models with Chain-of-Task Tasks for E-commerce.arXiv:2308.06966 [cs.CL] Ilya Loshchilov and Frank Hutter. 2019. Decoupled Weight Decay Regularization.In 7th International Conference on Learning Representations, ICLR 2019, New Or-leans, LA, USA, May 6-9, 2019. OpenReview.net. Shirong Ma, Shen Huang, Shulin Huang, Xiaobin Wang, Yangning Li, Hai-TaoZheng, Pengjun Xie, Fei Huang, and Yong Jiang. 2023. EcomGPT-CT: ContinualPre-training of E-commerce Large Language Models with Semi-structured Data.arXiv:2312.15696 [cs.CL]",
  "OpenAI. 2023. GPT-4 Technical Report. CoRR abs/2303.08774 (2023). arXiv:2303.08774": "Bo Peng, Xinyi Ling, Ziru Chen, Huan Sun, and Xia Ning. 2024. eCeLLM: Gener-alizing Large Language Models for E-commerce from Large-scale, High-qualityInstruction Data. In Forty-first International Conference on Machine Learning. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. ZeRO:memory optimizations toward training trillion parameter models. In Proceedingsof the International Conference for High Performance Computing, Networking,Storage and Analysis, SC 2020, Virtual Event / Atlanta, Georgia, USA, November 9-19,2020, Christine Cuicchi, Irene Qualters, and William T. Kramer (Eds.). IEEE/ACM,20. Chandan K. Reddy, Llus Mrquez, Fran Valero, Nikhil Rao, Hugo Zaragoza,Sambaran Bandyopadhyay, Arnab Biswas, Anlu Xing, and Karthik Subbian. 2022.Shopping Queries Dataset: A Large-Scale ESCI Benchmark for Improving ProductSearch. arXiv:2206.06588 [cs.IR] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, CarlosGuestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Alpaca: A strong,replicable instruction-following model. Stanford Center for Research on FoundationModels. stanford. edu/2023/03/13/alpaca. html 3, 6 (2023), 7. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith,Daniel Khashabi, and Hannaneh Hajishirzi. 2023. Self-Instruct: Aligning Lan-guage Models with Self-Generated Instructions. In Proceedings of the 61st An-nual Meeting of the Association for Computational Linguistics (Volume 1: LongPapers), Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Asso-ciation for Computational Linguistics, Toronto, Canada, 1348413508. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoningin large language models. Advances in neural information processing systems 35(2022), 2482424837.",
  "An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Cheng-peng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. 2024. Qwen2 technicalreport. arXiv preprint arXiv:2407.10671 (2024)": "Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang,Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang,Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A Survey of LargeLanguage Models. arXiv:2303.18223 [cs.CL]"
}