{
  "SCGNet Intrusion Detection and Intrusion Type Classification 3": "1.3.2Data Preprocessing ..............................................71.3.2.1One hot Encoding for categorical variables .........81.3.2.2Data Normalization for Numerical Variables ........81.3.3Class Imbalance Issue (For Multiclass) ..........................91.3.4Data Split and Model Training ..................................91.3.5Machine Learning Models ........................................101.3.6Deep Learning Architecture ......................................111.4Result Analysis ..........................................................121.5Conclusion and Future Work ............................................12 Intrusion detection system (IDS) is a piece of hardware or software that looks formalicious activity or policy violations in a network. It looks for malicious activityor security flaws on a network or system. IDS protects hosts or networks by lookingfor indications of known attacks or deviations from normal behavior (Network-basedintrusion detection system, or NIDS for short). Due to the rapidly increasing amountof network data, traditional intrusion detection systems (IDSs) are far from beingable to quickly and efficiently identify complex and varied network attacks, espe-cially those linked to low-frequency attacks. The SCGNet (Stacked Convolution withGated Recurrent Unit Network) is a novel deep learning architecture that we proposein this study. It exhibits promising results on the NSL-KDD dataset in both task,network attack detection, and attack type classification with 99.76% and 98.92% ac-curacy, respectively. We have also introduced a general data preprocessing pipelinethat is easily applicable to other similar datasets. We have also experimented withconventional machine-learning techniques to evaluate the performance of the dataprocessing pipeline.",
  "INTRODUCTION": "Network security is a dynamic field where new attack types constantly emerge andneed to be countered. The data set contains four main categories of attacks: DoS,Probe, User to Root (U2R), and Remote to Local (R2L). To block traffic travellingto and from the target system is the aim of a DoS attack. The IDS must shut downin order to defend itself due to the unusually large volume of traffic. This preventsa network from being accessed by normal traffic. The network may get overloadedand shut down when an online store gets a lot of orders on a day when there is asignificant discount, prohibiting paying customers from making any purchases. Thisassault happens the most frequently in the data set.On the other hand, an attempt to obtain data from a network is referred to as aprobe or surveillance attack. The goal is to appear as a robber and steal importantinformation, whether it be financial information or customer personal information.On a similar note, In a U2R attack, a regular user account is used to try root accessto the system or network. The attacker tries to take advantage of a systems flaws inorder to gain access or root privileges. The goal of an R2L attack is to get physical",
  "Taylor and Francis Book Chapter": "discover two different categories of variables: category and numerical variables. Theremaining columns in this table represent the numerical variables from this dataset,with just three columns representing the category variables. In this case, we preparedthe data using two distinct processes before fitting it to a specific ML model. Theseprocedures include On-hot-encoding for the preparation of categorical variables andNormalization for the preparation of numerical variables.",
  "SCGNet Intrusion Detection and Intrusion Type Classification 5": "performance and also detect the type of attack. As earlier classification algorithmsused to exact features and deep learning methodologies demonstrate their effective-ness. Anomaly detection is accomplished using a variety of machine-learning meth-ods. Many researchers have Concentrated on deep learning methods to build effectiveIDSs. identifying doubtful network activity is the aim of attack detection. Several ma-chine learning methods are used as the most popular intrusion prevention strategyto lower the mistake rate .Ma T; Cheng J; Classified the type of attack using a Deep Learning NeuralNetwork and an extractor of features using spectral clustering. The DNN Networkoutperformed the SVM, BPNN, and RF Network in terms of accuracy . HoweverSydney; proposes a wireless IDS system based on a Feed-Forward Deep Neural Net-work (FFDNN) and compares it to standard machine learning algorithms such asRandom Forest(RF), Support Vector Machine (SVM), Naive Bayas(NB), DecisionTree (DT) and K-nearest Neighbor (KNN). Binary multiclass attacks are includedin the experimental studies. The solution performed well on the UNSW-NB15 andAWID datasets with 87.10% and 77.16% accuracy for binary and multiclass classifi-cation respectively .Another approach was based on the dataset KDD99, Fatima Ezzahra; proposedthree models: LSTM, LSTM-PCA, and LSTM-MI. These methods were put to thetest for the categorization of objects into binary and multiclass categories, and theresults showed that PCA-based models generated the best results. PCA- based ar-chitectures, particularly those with two components, produced the best results andmulticlass classification with 99.44% and 99.39% respectively .Additionally, Abhishek examined the UNSW-NB15 dataset, which has 10 contem-porary attack classes and a less skewed distribution of targets. To make UNSW-NB15adoption in further studies easier, F1 performance has been compared to models typ-ically trained on KDD-99 and NSL-KDD. The results of this research provide enoughroom for performance to be optimized by employing alternative strategies acrossthe machine learning pipeline, which overcomes the underwhelming performance ofclassifiers trained on NSL-KDD and UNSW-NB15 .However, Yasis proposed IDS model, which uses DL and a DAE for the pre-training process and fine-tuning utilizing DNN through the process of HPO, im-proves the results of attack classification in intrusion detection. Additionally, theideal feature extraction technique for developing a successful DL IDS is also takeninto account . The results of all methods are in Table. 1.1.",
  "METHODOLOGY": "In our paper, we proposed different types of properties to train our model in order tofind Network attack types detection performance. We have deployed machine learningmodels and deep learning models such as SCGNet, AdaBoost, CNN, and XGBoostto detect Network attack types and address imbalance dataset issues. In .1 weillustrated our proposed workflow diagram.",
  "Dataset Description": "There have been several datasets of this kind before the NSL-KDD one. The KDDCup was an international contest for data mining and knowledge discovery tools. Inorder to gather traffic data, this competition was started in 1999. The competitionsobjective was to build a network intrusion detectora prediction model that candistinguish between good connections and bad connections, such as invasions orattacks. This competition resulted in the collection of a substantial amount of internettraffic records, which were then aggregated to form the KDD99 data set. This ledto the development of the NSL-KDD data set at the University of New Brunswick,which is a revised and improved version of the KDD 99.Despite the fact that KDDTest-21 and KDDTrain+ 20 Percent are subsets ofKDDTrain+ and KDDTest+, this data collection is made up of four sub-datasets:KDDTest+, KDDTest-21, KDDTrain+, and KDDTrain+ 20 Percent. KDDTrain+will now be referred to as train, and KDDTest+ will now be known as test. Both theKDDTrain+ 20 Percent and the KDDTrain-21 are subsets of the train dataset thatexclude the records with the highest traffic difficulty (Score of 21) respectively. Inspite of this, the traffic records seen in the KDDTrain+ 20 Percent and KDDTest-21datasets are not brand-new records held outside of either dataset. Rather, they arerecords that already exist in the test and train, respectively. Here we use the KDD-99data set for two types of classification one is Binary Classification and another one",
  "Data Preprocessing": "First, we determine how many attack labels are present using the subclass columns.All assault labels are then classified according to the appropriate attack class. TheDOS, R2l, Probe, and U2R attack classes are the four available. The attack class isthen distributed, and we count every value from each category. Following that, we",
  "One hot Encoding for categorical variables": "The majority of machine learning tutorials and tools demand that you prepare yourdata before fitting it to a specific ML model. Changing categorical data variables sothat machine learning algorithms may use them to create better predictions is onepopular encoding strategy. One hot encoding is a crucial part of feature engineeringfor machine learning. To improve predictions and get the data ready for an algorithm,data can be changed via one-hot encoding. For each categorical value, we constructa new category column using one-hot and assign it a binary value of 1 or 0. Eachinteger value is represented by a binary vector. All values for the index, which isrepresented by a 1, are zero.One-hot encoding has advantages for data that are unrelated to one another.Machine learning algorithms consider the organization of the numbers to be an im-portant attribute. Or, to put it another way, individuals will consider a larger numberto be more important or superior than one that is lower. However, some input datalacks ordering for category values, which can lead to inaccurate predictions and poorperformance even though this is advantageous in some ordinal situations. Then onehot encoder comes to the rescue. Thanks to one-hot encoding, our training data iseasier to scale and more valuable and expressive. Using numerical values allows us tomore rapidly determine a probability for our values.",
  "Data Normalization for Numerical Variables": "Here, we normalize our dataset using the Standardization scaling approach. Stan-dardization The practice of centering data around the mean while adding a unitstandard deviation is known as scaling, which is frequently referred to as Z-scorenormalization. The property is therefore set to zero, and the resulting distributionhas a unit standard deviation. By quantitatively subtracting the feature value fromthe mean and dividing the result by the standard deviation, standardization can becalculated. Standardization (Eqn. 1) can therefore be stated as follows:",
  "(1.1)": "In this case, it stands for the feature values standard deviation and for theirmean. In this case, it stands for the feature values standard deviation and for theirmean. The standardization technique does not, however, limit feature values to a par-ticular range as the Min-Max scaling technique does. This method is beneficial fordifferent distance-based machine learning methods, including KNN, K-means clus-",
  "Class Imbalance Issue (For Multiclass)": "A significant machine learning difficulty is classification issues. There are many diffi-culties we must overcome in the data when we attempt to categorize a dataset basedon an input dataset. One such difficulty is a dataset that is unbalanced. It has twoclasses, one of which is far higher than the other. There are numerous methods fordealing with it, and we employ SMOTE the Synthetic Minority OversamplingTechnique for our dataset.Considering that SMOTE is a technique for oversampling that provides artificialsamples just for the minority class. This strategy aids in addressing the overfittingissue brought on by random oversampling. By interpolating between positively cor-related examples that reside close together, it concentrates on the feature space tocreate new instances (.3).",
  "Data Split and Model Training": "As previously mentioned, various train datasets and test datasets from our datasetare used for model training and testing. In order to prevent data overfitting, weemployed the k-fold cross-validation method here.Training and testing would be carried out precisely once for each set (fold)throughout the entire process. It helps to avoid over fitting. We are aware that train-ing a model with all of the data in a single, quick run yields the highest performanceaccuracy. By avoiding this k-fold cross-validation, we can create a generalized model(.4 and .5). Here, the Test and Train data set will assist in evaluations ofthe building model and its hyperparameters.",
  ": Step by Step Flow of K-Fold Cross-Validation": "The data collection does not need to be divided because we already have variousdatasets for testing and training. Here, training data have been divided into separatesets for training and validation. For our cross-validation value for K=5, we are parti-tioning the given dataset into 5 folds and doing the training and validation. One foldfrom each run is taken into account for validation, with the remaining folds used fortraining and continuing with iterations. Following that, test datasets are subjectedto the same data pre-processing procedures as training datasets in order to acquirecurated forms before testing can begin.",
  "Machine Learning Models": "We used Logistic Regression, Decision Tree Classifier, Random Forest Classifier,Multinomial NB Classifier, SVM with Linear kernel and SVM with RBF kernel,Extreme Gradient Boosting and Adaptive Boosting for classification. Besides that,we have also developed our own deep learning architecture SCGNet and defined thearchitecture with Random Search and estimated the hyperparameters with Hyper-band. Multinomial NB Classifier: Multinomial Naive Bayes is a popular super-vised learning classifier for categorical text analysis, based on Bayes theorem.It guesses text tags and outputs the highest probability tag for a given sample. SVM with Kernel: The kernels are a set of mathematical operations usedby SVM algorithms. A kernels job is to take data as input and change it intothe required form. The scalar product between two points in an incredibly",
  "appropriate feature space is returned by the kernel functions. Thus, even in thesituation of very high-dimensional spaces, by defining a notion of similarity,with little computational expense": "Extreme Gradient Boosting: XGBoost improves model performance andexecution speed, allowing for larger datasets and outperforming current mod-els. In terms of model performance and execution speed, it outperforms othermethods like RF, GBM, and GBDT. AdaBoost: AdaBoost, also known as Adaptive Boosting, is a predictive mod-elling algorithm used as an ensemble. It uses decision trees with one level or splitas estimators. AdaBoost assigns equal weights to data pieces, assigning largerweights to incorrectly classified points, and training models until a smaller erroris observed.",
  "RESULT ANALYSIS": "We have used four metrics to assess our classification models: accuracy, 11-score, pre-cision, and recall . The experiment was implemented using Python library packageslike keras, numpy, sci-kit learn, pandas, and matplotlib. The experiment was run andtrained in NVIDIA 3060 GPU. The models were trained for 500 epochs with batchsize 256 and the learning rate was set to 0.01 with exponential decay. Adam optimizerwas used. Kears Autotuner was used for hyperparameter search and had a callbackfunction that monitors validation loss and accuracy. Models were early stopped usingearly stopping having a patience value for 15 epochs.All the results are generated on the test dataset provided in the NSL KDDdataset. We can see that for both tasks, our model outperforms the current SOTA interms of accuracy and other metrics. Results are shown in Table. 1.3 and Table. 1.4.",
  "CONCLUSION AND FUTURE WORK": "In this paper based on our examination of neural networks and intrusion detectionsystems, we developed a new architecture and our results were compared against vari-ous SOTA models. We compare the performance considering some features, accuracy,precision, recall and f1-score.We want to experiment with our data pipeline and model on SoTA datasetsthat are available like UNSW-NB1, CSE-CIC-IDS2018, and others. We also want toreduce our model size by applying pruning and quantization. Another important partwe would like to address is feature explainability. Thus it will be a complete tool forindustrial use cases.",
  "Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer.Smote: synthetic minority over-sampling technique. Journal of artificial intelli-gence research, 16:321357, 2002": "Sunook Chung and Kwangjo Kim. A heuristic approach to enhance the per-formance of intrusion detection system using machine learning algorithms. InProceedings of the Korea Institutes of Information Security and Cryptology Con-ference (CISC-W15), 2015. Abhishek Divekar, Meet Parekh, Vaibhav Savla, Rudra Mishra, and MaheshShirole. Benchmarking datasets for anomaly-based network intrusion detection:Kdd cup 99 alternatives. In 2018 IEEE 3rd International Conference on Com-puting, Communication and Security (ICCCS), pages 18. IEEE, 2018. Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger.Densely connected convolutional networks. In Proceedings of the IEEE confer-ence on computer vision and pattern recognition, pages 47004708, 2017. Yesi Novaria Kunang, Siti Nurmaini, Deris Stiawan, and Bhakti Yudho Suprapto.Attack classification of an intrusion detection system using deep learning andhyperparameter optimization. Journal of Information Security and Applications,58:102804, 2021.",
  "XuKui Li, Wei Chen, Qianru Zhang, and Lifa Wu. Building auto-encoder intru-sion detection system based on random forest feature selection. Computers &Security, 95:101851, 2020": "Tao Ma, Fen Wang, Jianjun Cheng, Yang Yu, and Xiaoyun Chen. A hybridspectral clustering and deep neural network ensemble algorithm for intrusiondetection in sensor networks. Sensors, 16(10):1701, 2016. Xiaolei Ma, Zhuang Dai, Zhengbing He, Jihui Ma, Yong Wang, and YunpengWang. Learning traffic as images: A deep convolutional neural network for large-scale transportation network speed prediction. Sensors, 17(4):818, 2017."
}