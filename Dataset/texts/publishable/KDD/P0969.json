{
  "ABSTRACT": "The prevailing issue of factual inconsistency errors in conventionalRetrieval Augmented Generation (RAG) motivates the study ofFactual Consistency Evaluation (FCE). Despite the various FCEmethods proposed earlier, these methods are evaluated on datasetsgenerated by specific Large Language Models (LLMs). Without acomprehensive benchmark, it remains unexplored how these FCEmethods perform on other LLMs with different error distributionsor even unseen error types, as these methods may fail to detect theerror types generated by other LLMs. To fill this gap, in this paper,we propose the first comprehensive FCE benchmark Face4RAG forRAG independent of the underlying LLM. Our benchmark consistsof a synthetic dataset built upon a carefully designed typology forfactuality inconsistency error and a real-world dataset constructedfrom six commonly used LLMs, enabling evaluation of FCE meth-ods on specific error types or real-world error distributions. Onthe proposed benchmark, we discover the failure of existing FCEmethods to detect the logical fallacy, which refers to a mismatch oflogic structures between the answer and the retrieved reference. Tofix this issue, we further propose a new method called L-Face4RAGwith two novel designs of logic-preserving answer decompositionand fact-logic FCE. Extensive experiments show L-Face4RAG sub-stantially outperforms previous methods for factual inconsistencydetection on a wide range of tasks, notably beyond the RAG taskfrom which it is originally motivated. Both the benchmark and ourproposed method are publicly available.1",
  "Large Language Model; Factual Consistency Evaluation;": "ACM Reference Format:Yunqi Xu, Tianchi Cai, Jiyan Jiang, and Xierui Song. 2024. Face4RAG: FactualConsistency Evaluation for Retrieval Augmented Generation in Chinese. InProceedings of the 30th ACM SIGKDD Conference on Knowledge Discoveryand Data Mining (KDD 24), August 2529, 2024, Barcelona, Spain. ACM, NewYork, NY, USA, 12 pages. Permission to make digital or hard copies of part or all of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for third-party components of this work must be honored.For all other uses, contact the owner/author(s).KDD 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s).ACM ISBN 979-8-4007-0490-1/24/08",
  "INTRODUCTION": "Retrieval Augmented Generation (RAG), a technique of augment-ing the context of Large Language Models (LLMs) with relevantpassages retrieved from external retrievers or search engines ,has demonstrated strong performance on various knowledge inten-sive tasks such as open domain conversation and questionanswering . Despite its bright prospect, factual consistency re-mains a critical issue for RAG systems. Recent assessment revealsthat even for the leading-edge commercial RAG systems like BingChat and Perplexity, barely over half of their outputs are factualconsistent with the references . This issue urges the need ofstudying factual consistency evaluation (FCE) in the RAG task.Various FCE methods have been proposed to evaluate the fac-tual consistency of specific RAG systems, among which a two-stepapproach shows promising results, especially for evaluating longanswers . As shown in the bottom left of , this ap-proach first segments the answer into shorter pieces, then evaluatesthe factual consistency of each segment with respect to the givenreference. In this way, the evaluation of a long answer is decom-posed into evaluations on several simpler pieces of information,which improves the detection of factual inconsistency.In previous works, these FCE methods are evaluated by answersgenerated by the underlying LLM in the specific RAG system beingstudied . Despite their effectiveness on the specific system,it is unclear how these methods generalize to new RAG systems.As discovered in a recent study , the optimal FCE method mayvary when evaluating different LLMs, hence achieving a superiorperformance regarding some certain LLM does not guarantee astrong performance on other LLMs. In this sense, previous bench-marks generated by a single LLM are not fair enough to evaluatethe overall performance of FCE methods.To fill this gap, in this paper, we first construct a comprehensivebenchmark to enable the evaluation of FCE methods independentof the underlying LLM. Specifically, we first propose a novel errortypology to cover various factual consistency errors in RAG, whichincludes three main categories, i.e., hallucination error, knowledgeerror, and logical fallacy, and is further divided into nine error types.Based on our predefined error typology, we construct a syntheticdataset in Chinese to assess the effectiveness of FCE methods acrossthe different types of errors. Furthermore, we construct a real-worlddataset in Chinese by generating answers using six distinct LLMswithin RAG tasks. Empirical analysis on the real-world datasetshows that 6.96% of all factual inconsistent samples involve logicalfallacies. In addition, we observe that different LLMs exhibit diverseerror distributions, which echoes previous research and justi-fies our motivation of constructing a comprehensive benchmarkindependent of LLMs.",
  "Error Type": ": An overview of our proposed FCE benchmark and method, in comparison with prior works. The upper left plot givesan example from RAG task. The lower left plot demonstrates previous FCE method, and the lower middle plot depicts ourproposed FCE method L-Face4RAG. The upper right plot shows the procedure of constructing the real-world dataset in ourproposed Face4RAG benchmark, which follows the procedure of previous benchmark. The lower right plots illustrates theconstruction of the synthetic dataset in the Face4RAG benchmark. While logical fallacy accounts for a considerable proportionof factual inconsistency errors in the real-world dataset, existingFCE methods may be incapable of detecting these sophisticatederrors involving logical connections among multiple text segments,since the decomposition step may neglect the logical connectionsbetween segments in the original answer. provides a show-case where a careless decomposition may mistakenly remove thecause-effect relation, leading to a wrong evaluation result.To resolve this issue, we develop the Logic-Enhanced FActualConsistency Evaluation for RAG (L-Face4RAG) method to betterhandle the logical consistency in the RAG task. L-Face4RAG hastwo core designs, i.e., logic-preserving answer decomposition andfact-logic FCE. Specifically, in the answer decomposition step, wepropose three principles for decomposition based on semantic link-ages and logical connections. We design an elaborated promptaccordingly and construct few-shot examples to help LLM betterfollow the above principles. In the subsequent FCE step, we assessthe factual consistency of each segment from two perspectives, i.e.,the fact consistency and logical consistency. The former perspectiveaims to detect hallucination or knowledge errors, while the later isresponsible for the logical fallacy errors. We further design a chain-of-thought (COT) prompt at each stage to instruct the LLMto better handle the inconsistency error in a step-by-step manner. gives a detailed demonstration of L-Face4RAG.Finally, we conduct extensive experiments to verify the effective-ness of L-Face4RAG. Compared to previous FCE methods for RAG, L-Face4RAG attains substantially higher accuracy on both syntheticand real-world datasets, regardless of the error type or underlyingLLM. Notably, although it is motivated by FCE in Chinese RAG,its superiority is consistent on other FCE tasks. Specifically, addi-tional experiments on English FCE benchmarks for RAG,summarization, dialogue and fact verificationshow that L-Face4RAG achieves SOTA on most of the tasks (6 out of7), as well as a substantially higher averaged score. We further con-duct ablation studies to verify the core designs of L-Face4RAG, i.e.,the logic-preserving answer decomposition approach and two-stageconsistency evaluation with carefully designed COT prompts.The contributions of this work are summarized as follows: We construct the first comprehensive FCE benchmark inRAG, the Face4RAG, which includes a carefully designederror typology, a synthetic dataset, and a real-world dataset.Face4RAG allows to evaluate FCE method on specific errortypes or various real-world error distributions.",
  "RELATED WORK": "Traditional FCE Methods. Evaluating the factuality of modelgenerated results is widely studied across various language modelgeneration domains like text summarization , dialogue sum-mary and question-answering . When the golden labelsare given, prior methods using exact match metrics orsimilarity-based metrics are proposed . However, high qual-ity answers can vary a lot, hence these approaches using goldenlabels may significantly underestimate the models performances,especially for long answers .FCE for Long-form Answers. To effectively evaluate factual-ity of long answers, recent FCE research mostly take a two stepapproaches , where in the first step, the long-form answeris decomposed into shorter segments, such as sentences ,sub-claims , individual facts and structured triplets .Then the second step evaluates the verifiability of each segmentwith respect to the given reference text , which can beefficiently done by modern general purpose LLM , e.g., GPT-4.Although we follow the two-step approach, our method differs fromthem in the ability of leveraging logical connections via specialdesigns of logic-preserving decomposition and fact-logic FCE.FCE Benchmarks. Prior benchmarks for FCE mostly focuson specialized tasks like summarization . For FCE inRAG, existing benchmarks are derived from specific LLMs, such asRefchecker and FELM , which are constrained by the errortype distribution of the underlying LLMs. Unlike these benchmarks,we construct a synthetic dataset based on our error typology, whichenables evaluation independent to any underlying LLM.",
  "FACE4RAG BENCHMARK": "Recall that existing FCE benchmarks only use answers generatedby some certain LLMs, which may fail to evaluate FCE methods onother LLMs with different error distributions or unseen error types.To remedy this issue, in this section, we propose a novel approachto construct a FCE benchmark for RAG, which is independent ofthe underlying LLMs and called FActual Consistency Evaluation forRAG (Face4RAG). Face4RAG contains an error-type-oriented syn-thetic dataset and a real-world dataset. To construct the syntheticdataset, inspired by the error typology used in an exam designed forhumans, i.e. the National College Entrance Examination of China,we first propose a novel error typology to classify any factual con-sistency error in RAG task, which includes nine types of errorsbelonging to three main categories. Based on the proposed errortypology, we then construct a synthetic dataset to evaluate FCEmethods on each type of the error. Besides the synthetic dataset,we also collect samples from six commonly used LLMs to constructa real-world benchmark, which aims to evaluate the overall factualconsistency of FCE methods in real-world scenarios. The detailsabout Face4RAG can be seen in and . : Statistics of the synthetic and real-world datasets inthe Face4RAG benchmark. For each dataset, the answer-leveland segment-level statistics on the number of samples, theaverage sample length in terms of characters and the rate ofpositive samples are reported.",
  "Error Typology in FCE": "Our error typology for FCE is inspired by the questions in theNational College Entrance Examination of China , which arecarefully designed to test the ability of human to evaluate factualconsistency. In this examination, reading comprehension is a majorsection to evaluate the participants skill of understanding a Chi-nese text. Factual consistency evaluation is a typical task in thissection. Given the text, the participants are required to evaluate thecorrectness of several answers to a specific question , which isessentially a RAG task (see examples in in the appendix).As these questions are designed for a competitive entrance exami-nation of higher education institutions at the undergraduate level,they are generally hard to answer and cover a wide range of factualinconsistency error types. Accordingly, we develop a novel errortypology for RAG, which comprises three main categories and isfurther classified into nine error types. In the following, we give adetailed description of our proposed error typology.Hallucination Error This class of error refers to the situationwhen the answer contains information that cannot be traced backto the reference . Note that there are two main usages of theterm hallucination in previous literature: one refers to \"unfaithfulor nonsensical\" generated answers , the other further includes\"unverifiable\" answers using the given context . Here weadopt the second usage that has a larger scope.",
  "Hallucination Error (Hallu.) refers to the situation when theanswer is either unfaithful or unverifiable using the givencontext (even when it is factually correct)": "Knowledge Error This class of error refers to the situationwhen the information contained by the answer is inaccurate orincorrect regarding the reference . This may occur in variouscomponents of a sentence, such as the subjects, predicates, objects,adverbials of time and place, etc. We classify the knowledge errorinto four error types:",
  "Conceptual Substitution Error (KConc.) refers to the situa-tion when a term or concept in the reference is erroneouslyreplaced by a different (though possibly related) concept": "Logical Fallacy This kind of error occurs when the answercontains statements that are either without logical support or havea logical relation that conflicts with the information from the ref-erence. This incongruity undermines the logical validity of theanswer with respect to the reference, which results in an unsoundargument or misleading information . We further divide thelogical fallacy into four error types:",
  "Confusing Sufficient and Necessary Conditions Error (LConf.)is the case when the necessary conditions in reference aremisinterpreted as sufficient and necessary conditions": "Inclusion Relation Error (LIncl.) is the case where statementsthat are unrelated or have certain relationship except inclu-sion in the reference are misrepresented in the answer tohave an inclusion relationship (e.g., hierarchical or subset).For each of the error types defined above, we provide severalexamples to help better understand and distinguish it from othertypes. See detailed examples in .",
  "Synthetic Dataset": "Based on the above proposed error typology, we construct a syn-thetic dataset. In the dataset, the positive samples are factual con-sistent, whereas each negative sample has at least one factual in-consistency error. The dataset is constructed based on WebCPM, a web-enhanced question answering dataset in Chinese. Dueto the space limit, in the following we briefly describe the processof dataset generation. Please refer to Appendix A for more detailsof the construction of our synthetic dataset.Negative Samples For each specific error type in the typology,we design a prompt to generate samples with this error. For the hal-lucination error, we setup three levels of difficulty for the evaluatorto detect inconsistency and construct samples accordingly. For theremaining two categories, i.e., knowledge error and logical fallacy,we design a specific prompt for each error type except the Contra-diction Error (KCont.). Since KCont. may occur at different levelsof granularity , i.e., word or sentence, we design one promptfor each. Apart from the above error types, we construct a newerror type called Other Logical Fallacy (LOthe.), which accounts forpotential errors in some complex logical connections uncovered byour previously defined four types of logical fallacy.Positive Samples To enrich the sample diversity, we apply theaugmentation technique in . Specifically, the original positivesamples in WebCPM are augmented by synonym replacing andparaphrasing via certain prompt at either word or sentence level.Human Annotation Refinement To enhance the quality of thecoarse labels derived above, we further engage 12 human expertsto annotate the factual consistency of each answer via a two-stepapproach .",
  "Real-World Dataset": "The synthetic dataset is generated based on various predefined errortypes without considering the distribution of these error types inthe real world. Consequently, there is a need for another evaluationdataset that better aligns with the actual distribution of answers inreal-world RAG scenarios, thus serving as a real-world dataset. Incontrast to previous studies that relied solely on GPT-based LLMsfor generating responses to create their evaluation sets , weadopt a more comprehensive approach by utilizing six differentLLMs to construct our real-world dataset.Specifically, we first collect 200 questions along with correspond-ing references. We then prompt six commonly used LLMs to gen-erate answers for the questions based on the references, includinggpt-4-turbo (GPT-4) , gpt-3.5-turbo (GPT-3.5) , Baichuan2-13B-Chat (Baichuan2) , ChatGLM3 , Qwen-14B-Chat (Qwen), and Chinese-Alpaca-2-13B-16k (Alpaca2 (CH)) . In thisway, we derive a total of 1200 data points, which constitute thereal-world dataset.For each data point, we follow the same human annotation proce-dure as in our synthetic dataset to inspect if it is factually consistent.Moreover, if an answer is deemed factually inconsistent, the an-notator will assign a specific error type from our proposed errortypology to the answer. When the annotators notice that error ofthe answer does not fall into the aforementioned error types, theywill mark the answer as \"Other Errors\".We now conduct empirical analyses on the error typology andthe behaviors of various LLMs on the above real-world dataset.Overall Error Type Distribution We first justify our study onlogical fallacy consistency detection by empirically showing thatthe logical fallacy errors are prevailing in the answers generated byvarious LLMs. To this end, we analyze the distribution of the errortypes annotated across the entire real-world dataset. As shown in, the hallucination error, knowledge error and logical fallacyaccount for 73.78% , 28.31%, 6.96% of all the inconsistent samples,respectively. It worth note that this 6.96% logical fallacy errors arenot studied in the previous FCE methods. Besides, only 0.23% of theinconsistent samples are marked as \"Other Errors\" by annotators,which suggests the comprehensiveness and completeness of ourproposed error typology.Error Distribution of Various Models We then look deeperinto the error types and their distributions among various LLMsin RAG. As presented in , various LLMs exhibit distinctdistributions on error types. For instance, LIncl. emerges in threeof the LLMs, and LCaus. and LConf. occurs in four models. In ad-dition, while Hallu. exists in all models, GPT-4 has a notably high",
  "The vitamins and minerals in energy drinks play a certainrole in quickly replenishing nutrients and inducing fatigueafter exercise": "KInve.25-3215-181-3A typical silkworm can live for just over a month, duringwhich the period from hatching to cocooning varies roughlyfrom 25 to 32 days depending on the season, followed by15 to 18 days as a pupa, and finally 1 to 3 days as a moth. 15-1825-321-3A typical silkworm can live for just over a month, duringwhich the period from hatching to cocooning varies roughlyfrom 15 to 18 days depending on the season, followed by25 to 32 days as a pupa, and finally 1 to 3 days as a moth.",
  "Regular exercise can enhance cardiorespiratory fitness, suchas strengthening muscle endurance and improving thebodys resistance to fatigue": "percentage, with 77.91% of its errors being of this specific type;in comparison, Qwen only has 57.81% Hallucination Error and ahigher proportion of logical fallacy at 9.38%. The distinct error typesdistributions of different LLMs suggest that FCE methods evaluatedon a specific LLM may not generalize well to other LLMs, indicatingthe necessity for constructing a benchmark that is independent ofthe underlying LLM.",
  "LOGIC-ENHANCED FACTUALCONSISTENCY EVALUATION": "In the above statistic analysis, logical fallacy accounts for a con-siderable proportion of factual errors in real-world RAG scenarios.However, as we have analyzed before, existing FCE pipelines ne-glect the logical connections between segments in the originalanswer, which may result in wrong factual consistency evaluationresult for samples with logical fallacy. Hence, to improve the eval-uation ability of factuality consistency, a natural direction is to",
  ": A few examples for our proposed logic-preserving answer decomposition": "design an advanced FCE method that is capable of handling logicalconnections in long answers.In this section, we propose a novel pipeline called Logic-EnhancedFActual Consistency Evaluation for RAG (L-Face4RAG), which explic-itly takes logical connections into consideration when evaluatingthe factual consistency. L-Face4RAG has two core modules, i.e.,logic-preserving answer decomposition and fact-logic FCE, whichwill be described as follows.",
  "Logic-Preserving Answer Decomposition": "Most existing studies directly decompose answers into segments,each containing only a single piece of information . In con-trast, we propose to decompose the answers based on semanticlinkages2 and logical connections, which preserves the logical rela-tionships and facilitates logical consistency evaluation. The coredesign in this module is an elaborated prompt based on the follow-ing three principles for answer decomposition.",
  "We prompt GPT-4 to execute the decomposition only whenthe two or multiple sentences do not exhibit strong semanticor logical connection": "To ensure that each segment can be understood by GPT-4independently without leveraging other segments, any pro-noun in a segment that refers to other contextual informationshould be substituted with appropriate reference. During the decomposition process, GPT-4 is required tomaintain the sentence structure of the original answer to thebest extent. This principle alleviates the risk of introducingadditional hallucination to the original answer. In order to help GPT-4 better understand our principles for an-swer decomposition and deal with texts with various formats, weconstruct three kinds of instances to serve as the few-shot examples.The specific type of instances are as follows:",
  "Fact-Logic FCE": "Previous methods directly invoke an LLM to evaluate the decom-posed segments and overlook the logical fallacy. To evaluate thelogical fallacy, we develop a two-stage procedure for factual con-sistency evaluation, which consists of a conventional stage of factconsistency evaluation and an extra stage that evaluates from bothperspectives of fact and logic; we introduce the COT mechanism into both stages to improve LLMs ability of evaluation. Theprompts for each stage are provided at our benchmark webpage.1 Fact Consistency Evaluation In this stage, GPT-4 is instructedto assess the consistency of each piece of information in the segmentagainst the reference, which mainly concerns with the hallucina-tion error and the knowledge error. Unlike previous methods thatdirectly instruct the model to assess the consistency with the ref-erence , we use the COT technique to guide the model toevaluate the segment step-by-step, with the following steps:",
  ": The process of logic consistency evaluation": "(3) Fact Consistency Check: GPT-4 conducts a thorough fact con-sistency check for each informational point against its cor-responding context. A segment is deemed consistent if andonly if every single informational point aligns fact consistentwith the reference. The above instruction imposes GPT-4 to evaluate consistencywith each relevant content rather than the full context of the refer-ence, reducing the probability of misjudging positive samples. Wewill empirically justify this point in our experiments.Logic Consistency Evaluation In this stage, GPT-4 is instructedwith a COT prompt to evaluate the logical fallacy. Since no FCEmethod has explicitly handled logical fallacy before, this is a novelstage for FCE, in which we elaborate a COT prompt as follows. Thespecific process is shown in .",
  "EXPERIMENTS": "In this section, we conduct extensive experiments to evaluate the ef-fectiveness of our proposed L-Face4RAG pipeline. Our experimentsshow that on both synthetic data and real-world data in Face4RAGbenchmark, our L-Face4RAG method substantially outperformsthe existing FCE methods. Notably, its superiority goes beyond theChinese RAG task from which L-Face4RAG is originally motivated,as L-Face4RAG achieves SOTA results on 6 out of 7 of the existingEnglish datasets and also a substantially higher average score onall tasks.",
  "FACTSCORE first breaks the answer into a series ofatomic facts and then assigns a binary label to each atomicfact individually": "FELM first segments the answer into fine-grained textualspans and then evaluates the factual consistency of all textualspans collectively. It outputs the corresponding numbers offactual inconsistent textual spans if existed. Ragas first extracts a set of statements from the answerand then evaluates the factual consistency of all statementscollectively, outputting a binary label for each statementalong with the corresponding reason for the assessment.",
  "Refchecker extracts knowledge triplets from the answerand evaluates each knowledge triplet separately": "Implementation Details As the above FCE baselines are origi-nally designed for tasks in English, we adapt them to our ChineseRAG task by translating their prompts into Chinese.When experimenting with FELM, we utilize the Reference-docaugmented evaluator , in alignment with our task which is fo-cused on evaluating the factual consistency of answers against theirreferences. Specifically, we input our references as the retrievedreference doc in FELMs evaluation framework. We select the best-performing estimator in , i.e., decomposing the answer withsegment-based method and utilizing GPT-4 as the factual errordetector.Since the original settings of FACTSCORE and RAGAS are basedon GPT-3.5, we conduct experiments with both GPT-3.5 and GPT-4 to eschew the effect of the possible performance gap betweenGPT-3.5 and GPT-4 on the empirical results.Finally, to apply our proposed evaluation pipeline, we decomposethe answer into segments and assess the factual consistency of eachsegment respectively. The outputs include both the label and thecorresponding explanations. To derive deterministic output fromGPT-4, we set its temperature to 0.",
  "MethodTotalBaichuan2ChatGLM3GPT-3.5GPT-4Alpaca2 (CH)Qwen": "FACTSCORE(GPT-3.5)53.3354.055.547.551.559.052.5FACTSCORE(GPT-4)54.6755.059.546.552.563.051.5FELM55.0049.656.056.852.055.660.0RAGAS(GPT-3.5)65.9264.568.564.560.065.073.0RAGAS(GPT-4)72.9272.574.071.568.576.574.5RefChecker68.2562.072.066.563.074.571.5L-Face4RAG (Ours)87.7590.088.081.586.093.587.5 Synthetic Dataset In , we report the predictive accuracyof different error types for examined FCE methods on the syntheticdataset. From the results, we have the two main observations. (i) Ourmethod achieves the highest accuracy on most of the error types(except on LIncl. where it is slightly worse than RAGAS with GPT-4),which amounts to a significant improvement on overall accuracycompared to all the baselines. (ii) In particular, the performancegap between our method and baselines on error types of logicalfallacy are much larger than the gap on other error types, whichindicates that our method is especially capable of handling logicalfallacy owing to our specific algorithmic designs.Real-world Dataset In , we compare the predictive per-formance of our proposed pipeline with previous FCE methods onthe real-world dataset. From the results we observe that: (i) Theoverall accuracy of our method is substantially higher than thoseof the baseline FCE methods, showing superiority in real-worldscenarios. (ii) Moreover, on most of the subsets generated by differ-ent LLMs, our method consistently outperforms baseline methods,which indicates the superiority of our method is universal and inde-pendent of the error distribution, which is in line with the empiricalresults on the synthetic dataset.",
  "Performance Comparison on Existing FCEBenchmark": "We then evaluate the robustness and applicability of the proposedL-Face4RAG method on other factuality detection tasks, and inEnglish. Specifically, we consider several commonly used FCEbenchmarks in English on various tasks, including RAG,summarization, dialogue and fact verification.In , we report the predictive accuracy of examined FCEmethods on the above tasks. The results show that our proposed L-Face4RAG achieves SOTA results on 6 out of 7 of the existingdatasets and also a substantially higher average score on all tasks,indicating the effectiveness of L-Face4RAG beyond the original fac-tuality evaluation task in RAG, and its robustness to other languages.This validates the wide-applicability of our proposed method.Besides the above comparison among different methods, we alsoobserve that the ranking of the average score of various methods onthe above commonly used benchmarks is similar to the ranking ofthe average score on all public tasks is 0.9, and the same 0.9 betweenthe rankings on our real-world dataset and the public datasets. Thisvalidates the strong correlation between the evaluation results ofour new benchmark and the results on existing benchmarks.",
  "Ablation Study": "We now verify the specific design choices of our proposed evalua-tion pipeline by ablation study on Face4RAG benchmark. Specif-ically, we examine the effectiveness of each designed module bycomparing L-Face4RAG with the counterpart method without sucha module. Due to space limit, here we only present the results on thesynthetic dataset. Results on the real-world dataset are qualitativelysimilar and deferred to Appendix B.Evaluating the Answer Decomposition Module. (A.D.) Re-call that our decomposition module preserves the logic connectionwithin one segment, which may help better identify logical fal-lacy while reducing extra hallucination induced by decomposition.To verify this point, we conduct an ablation study by replacingour approach by a conventional decomposition method . Aspresented in and , we observe a severe decline ofoverall accuracy in the counterpart method, especially for negativesamples related to logical fallacy. This phenomenon accords withour intuition that conventional answer decomposition method may",
  "LOver.90.1842.8683.93LCaus.92.8632.1435.71LConf.80.0034.0064.00LIncl.51.2231.7129.27LOth.90.7044.1965.12": "fail to detect logical fallacy since some logical connections may bediscarded during the decomposition. In addition, positive samplesalso have a slight decrease in accuracy, which justifies the thirdprinciple in our logic-preserving answer decomposition module,i.e., preserving the structure of the original answer may alleviatethe introduction of extra hallucination.Evaluating the Introduction of COT.(w/o COT) Recall thatCOT is adopted in both stages of factual consistency evaluation,which instructs the model to conduct finer-grained fact consis-tency evaluation and sophisticated logic consistency evaluation,respectively. To validate the introduction of COT, we consider a counterpart method that removes the detailed steps in the instruc-tions and requires GPT-4 to directly generate evaluation withoutoutputting the underlying reasoning process. As presented in Ta-ble 6, the overall accuracy drops severely after removing COT (from93.38% to 79.60%), especially for the positive samples (from 96.19%to 51.27%). This justifies the benefit of introducing COT into FCE.Evaluating the Stage of Logical Consistency Evaluation.(w/o logi. eval) To evaluate the effect of our proposed logicalconsistency evaluation stage on error detection, we construct acounterpart method by removing the second stage from our pipeline.The results presented in show that the counterpart methodincurs a decline in overall accuracy. Among all the error types,logical fallacy contributes the major part of accuracy decline, whichaligns with our main motivation of the second stage design forlogical fallacy evaluation. In addition, there is a slight decreasein the accuracy of knowledge error. A possible reason is that thesecond stage may supplement the detection of some knowledgeerrors that are missed in the first stage. Hence, the second stagealso benefits the detection of knowledge error. Note that for thehallucination error, we have not observed any obvious change inthe detection accuracy; this matches our intuition that hallucinationerror has no relation with logical fallacy.",
  "CONCLUSION": "In this work, we give a systematic study of factual consistencyevaluation in RAG. Specifically, we first propose a comprehensivebenchmark termed Face4RAG, which includes the synthetic datasetand the real-world dataset. In light of the possible failure of existingFCE methods in detecting logical fallacy in RAG, we then proposea novel FCE method termed L-Face4RAG. Compared to previousmethod, our method has two novel designs, i.e., logic-preservingdecomposition and fact-logic FCE, which can better characterize thelogical relations in different pieces of information in the sentence,leading to higher ability of logical fallacy evaluation. Extensiveexperiments on both the synthetic and real-world datasets verify theeffectiveness of the L-Face4RAG method. Notably, the superiorityof L-Face4RAG is consistent on a wide range of factuality detectionbenchmarks beyond the Chinese RAG task. Elaborated ablationstudies also justify our core algorithm designs.",
  "KDD 24, August 2529, 2024, Barcelona, SpainYunqi Xu, Tianchi Cai, Jiyan Jiang, and Xierui Song": "2019. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-cia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, ShyamalAnadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774(2023). Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan,Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, RunjiLin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men,Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, PengWang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang,Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, ZhengYuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, ChangZhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen TechnicalReport. arXiv preprint arXiv:2309.16609 (2023). Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu,Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, et al. 2023. Benchmarking FoundationModels with Language-Model-as-an-Examiner. arXiv preprint arXiv:2306.04181(2023).",
  "Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Readingwikipedia to answer open-domain questions. arXiv preprint arXiv:1704.00051(2017)": "Shiqi Chen, Yiran Zhao, Jinghan Zhang, I Chern, Siyang Gao, Pengfei Liu, JunxianHe, et al. 2023. Felm: Benchmarking factuality evaluation of large languagemodels. arXiv preprint arXiv:2310.00741 (2023). I Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou,Junxian He, Graham Neubig, Pengfei Liu, et al. 2023. FacTool: Factuality De-tection in Generative AIA Tool Augmented Framework for Multi-Task andMulti-Domain Scenarios. arXiv preprint arXiv:2307.13528 (2023).",
  "Gautier Izacard and Edouard Grave. 2020. Leveraging passage retrieval withgenerative models for open domain question answering.arXiv preprintarXiv:2007.01282 (2020)": "Ray S Jackendoff. 1992. Semantic structures. Vol. 18. MIT press. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii,Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination innatural language generation. Comput. Surveys 55, 12 (2023), 138. Zhijing Jin, Abhinav Lalwani, Tejas Vaidhya, Xiaoyu Shen, Yiwen Ding, ZhihengLyu, Mrinmaya Sachan, Rada Mihalcea, and Bernhard Schlkopf. 2022. Logicalfallacy detection. arXiv preprint arXiv:2202.13758 (2022).",
  "Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020.On faithfulness and factuality in abstractive summarization. arXiv preprintarXiv:2005.00661 (2020)": "Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh,Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation.arXiv preprint arXiv:2305.14251 (2023). Dor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine, Nir Ratner, Yonatan Belinkov,Omri Abend, Kevin Leyton-Brown, Amnon Shashua, and Yoav Shoham. 2023.Generating benchmarks for factuality evaluation of language models. arXivpreprint arXiv:2307.06908 (2023). OpenAI. 2022. Chatgpt blog post. Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Under-standing factuality in abstractive summarization with FRANK: A benchmark forfactuality metrics. arXiv preprint arXiv:2104.13346 (2021). Domina Petric. 2020. Logical Fallacies. On-line Article (preprint), doi 10 (2020). Yujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao Liang, Kunlun Zhu, Yankai Lin, XuHan, Ning Ding, Huadong Wang, et al. 2023. WebCPM: Interactive Web Searchfor Chinese Long-form Question Answering. arXiv preprint arXiv:2305.06849(2023).",
  "Tal Schuster, Adam Fisch, and Regina Barzilay. 2021. Get your vitamin C! robustfact verification with contrastive evidence. arXiv preprint arXiv:2103.08541 (2021)": "Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller,Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, et al. 2022. Blenderbot 3:a deployed conversational agent that continually learns to responsibly engage.arXiv preprint arXiv:2208.03188 (2022). Liyan Tang, Tanya Goyal, Alexander R Fabbri, Philippe Laban, Jiacheng Xu,Semih Yavuz, Wojciech Kryciski, Justin F Rousseau, and Greg Durrett. 2022.Understanding factual errors in summarization: Errors, summarizers, datasets,error detectors. arXiv preprint arXiv:2205.12854 (2022).",
  "Cunxiang Wang, Sirui Cheng, Zhikun Xu, Bowen Ding, Yidong Wang, and YueZhang. 2023. Evaluating open question answering evaluation. arXiv preprintarXiv:2305.12421 (2023)": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoningin large language models. Advances in Neural Information Processing Systems 35(2022), 2482424837. Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding,Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b: An openbilingual pre-trained model. arXiv preprint arXiv:2210.02414 (2022).",
  "Baichuan2320.640.5ChatGLM3158.036.5GPT-3.5160.827.5GPT-4359.240.0Alpaca2 (CH)188.847.0Qwen200.629.0": "in Chinese, following the aforementioned error typology. We useGPT-4 to generate the negative samples and design specificprompts corresponding to each type of error. For every samplein WebCPM, we rewrite them for every error type to collect oursynthetic negative sample dataset. Detailed prompts are providedat our benchmark webpage.1 For the hallucination error, we construct corresponding dataaccording to three levels of difficulty for the evaluator to detectinconsistency. To be more specific, the easiest samples, in the firstgroup, are completely off-topic from the reference. The secondgroup includes content that is on-topic but contains ungroundedinformation. The third group of sample mixes factually consistentinformation with hallucinated content, resulting in sentences wheresome parts are supported by the reference, while others are un-grounded. This mixture poses a challenge for evaluators, as it couldmistakenly be labeled as \"consistent\" due to the presence of someconsistent information.For the remaining two categories, i.e., knowledge error and logi-cal fallacy, we design a specific prompt for each error type exceptthe Contradiction Error (KCont.). For KCont., since it may occurs atdifferent levels of granularity , i.e., word or sentence, we designone prompt for each level. Specifically, the prompt of the word-level KCont. aims to select specific words in the answer and replacethem with antonyms, and the prompt of the sentence-level KCont.is designed to construct a new answer semantically contradictingthe reference. Since the types of logical connections are diverse andcomprehensive, for the completeness of the dataset, we consider anew error type called Other Logical Fallacy (LOthe.), which accountsfor potential errors in some complex logical connections uncoveredby our previously defined four types of logical fallacy. The promptof LOthe. is designed to drive GPT-4 to insert an arbitrary logicalconnection error into anywhere of the original answer.Positive Samples To enrich the diversity of positive samples,we employ the commonly used data augmentation techniques to generate more positive samples based on the answers from We-bCPM. Our data augmentation process supplements the positivesamples in WebCPM by synonym replacing and paraphrasing tech-niques via the prompts at the word or sentence level. Specifically, at the word level, we prompt GPT-4 to randomly replace somewords in the answer with their synonyms; at the sentence level, weprompt to summarize the reference or rephrase the answer withoutchanging the meaning of the original sentence.Construction Details Following the methodology in previousresearch , we utilize the few-shot technique , in conjunctionwith the Chain of Thought (COT) approach, to guide GPT-4 to construct high-quality samples. We provide clear directionsand relevant examples in the prompt and ask the model not onlyto produce the newly constructed samples, but also to show thethinking process behind the modifications it makes to the samplesin the output. This ensures that the model is indeed generating newsamples in the direction we desire. The construction prompts forboth positive and negative examples are provided at our benchmarkwebpage.1 Human Annotation Refinement The above construction pro-cess produces a coarse label of factual consistency for each sample.To enhance the quality of the labels, we further engage 12 humanexperts to annotate the factual consistency of each answer via atwo-step approach . Specifically, the human annotator first de-compose the answer into multiple segments; for each segment, theannotator is required to judge whether it is factual consistent withthe reference and give the evidence of the judgement. Then thehuman annotations on all segments are aggregated to yield a factualconsistent label for the answer.",
  "BABLATION STUDY RESULTS ON THEREAL-WORLD DATASET": "further validates the effectiveness of our approach, par-ticularly highlighting its importance in practical scenarios whereenhancing the recall of negative samples is crucial while preservingthe discriminative ability of positive samples. : Ablation Study Results on the Real-world Dataset.Here \"ours\" refers to our original pipeline, \"A.D.\" refers to theablation result of answer decomposition, \"w/o COT\" refers tothe ablation of COT, and \"w/o logi. eval\" refers to the ablationof the logical consistency evaluation.",
  ": Example of the Reading Comprehension Section in the National College Entrance Examination of China": "Translated Reference:The golden age of blue-and-white porcelain development was during the Yongle and Xuande periods of the Ming Dynasty, coinciding withZheng Hes voyages to the Western Seas, prompting us to ponder: Is it mere historical coincidence that both seafaring and porcelaincraftsmanship reached their zenith at the same time? ... It was the blending of Chinese and foreign civilizations that successfully drove thetransformation of Chinese porcelain from monochrome to polychrome, with blue-and-white porcelain uniquely illustrating the culturalevolution of the Ming era, serving as an example of traditional societys progression from uniformity to diversity. (Excerpted and compiledfrom \"The Trajectory of the Rise of Ming Dynasty Blue-and-White Porcelain\" by Wan Ming)",
  "Task: evaluate the correctness of the following sentences:": "Translated Sentence 1: Zheng Hes voyages to the Western Seas stimulated the production, sales, and technological innovation ofporcelain, heralding the golden age of blue-and-white porcelain development.Label: correct Translated Sentence 2: Factors such as the localization of raw materials ushered the development of blue-and-white porcelain into a newphase, at which point its evolution became unrelated to foreign cultures.Label: Incorrect.Error Type: Contradiction Error Translated Sentence 3: Ming Dynasty society is often considered conservative, yet the styles of blue-and-white porcelain indicate that thesociety was relatively open and progressive.Label: Incorrect.Error Type: Conceptual Substitution Error Translated Sentence 4: The blending of Chinese and foreign civilizations promoted the transformation of porcelain from monochrome topolychrome, thereby driving the society of the time towards a more diverse transition.Label: Incorrect.Error Type: Causal Confusion Error"
}