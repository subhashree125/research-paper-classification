{
  "ABSTRACT": "Personalized recommendation serves as a ubiquitous channel forusers to discover information tailored to their interests. However,traditional recommendation models primarily rely on unique IDsand categorical features for user-item matching, potentially over-looking the nuanced essence of raw item contents across multiplemodalities such as text, image, audio, and video. This underutiliza-tion of multimodal data poses a limitation to recommender systems,especially in multimedia services like news, music, and short-videoplatforms. The recent advancements in large multimodal models of-fer new opportunities and challenges in developing content-awarerecommender systems. This survey seeks to provide a comprehen-sive exploration of the latest advancements and future trajectoriesin multimodal pretraining, adaptation, and generation techniques,as well as their applications in enhancing recommender systems.Furthermore, we discuss current open challenges and opportuni-ties for future research in this dynamic domain. We believe thatthis survey, alongside the curated resources1, will provide valuableinsights to inspire further advancements in this evolving landscape.",
  "Equal contribution. Correspondence to: Jieming Zhu. The work was done when the authors were visiting at Huawei Noahs Ark Lab.1 Github repository:": "KDD 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.This is the authors version of the work. It is posted here for your personal use. Notfor redistribution. The definitive Version of Record was published in Proceedings ofthe 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 24),August 2529, 2024, Barcelona, Spain,",
  "INTRODUCTION": "Recommender systems have been widely employed in various on-line applications, including e-commerce websites, advertising sys-tems, streaming services, and social media platforms, to deliverpersonalized recommendations to users. Their primary goal is toenhance user experience, boost user engagement, and facilitatethe discovery of items tailored to individual interests. However,traditional recommendation models primarily rely on unique IDs(e.g., user/item IDs) and categorical features (e.g., tags) for user-itemmatching , potentially overlooking the nuanced essence of rawitem contents across multiple modalities such as text, image, audio,and video . This underutilization of multimodal data posesa limitation to recommender systems, especially in multimediaservices like news, music, and short-video platforms .To tackle this limitation, researchers have extensively investi-gated multimodal recommendation techniques for over a decade,resulting in a large body of research work that explores the inte-gration of multimodal item features into recommendation models.For a comprehensive review, interested readers can refer to re-cent surveys . These surveys primarily delve intotechniques such as multimodal feature extraction , feature rep-resentation , feature interaction , feature alignment ,feature enhancement , and multimodal fusion for rec-ommendation models. However, the majority of these approachesrely on extracted multimodal feature embeddings, leaving otheraspects of multimodal pretraining and generation relatively unex-plored. Nowadays, pretrained large models have gained significantpopularity in the domains of natural language processing (NLP),computer vision (CV), and multimodal systems (MM). The emer-gence of language models like the GPT and Llama serieshas ushered in a new era of capabilities for understanding and gen-erating language, while the CV field has witnessed breakthroughs",
  "KDD 24, August 2529, 2024, Barcelona, SpainQijiong Liu et al": "Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and SameerSingh. 2020. AutoPrompt: Eliciting Knowledge from Language Models withAutomatically Generated Prompts. In Proceedings of the 2020 Conference onEmpirical Methods in Natural Language Processing (EMNLP). 42224235. Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Woj-ciech Galuba, Marcus Rohrbach, and Douwe Kiela. 2022. FLAVA: A FoundationalLanguage And Vision Alignment Model. In IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR). 1561715629. Anima Singh, Trung Vu, Raghunandan H. Keshavan, Nikhil Mehta, Xinyang Yi,Lichan Hong, Lukasz Heldt, Li Wei, Ed H. Chi, and Maheswaran Sathiamoorthy.2023. Better Generalization with Semantic IDs: A case study in Ranking forRecommendations. CoRR abs/2306.08121 (2023).",
  "MULTIMODAL PRETRAINING FORRECOMMENDATION": "In contrast to supervised learning directly on domain-specific data,self-supervised pretraining learns from a large-scale unlabeled cor-pus and then adapts the pretrained model to downstream tasks.This approach allows for the acquisition of rich external knowledgein pretraining data, thus leading to the widespread recognition of itseffectiveness. In this section, we will first provide a review of majorpretraining paradigms and then introduce how they are utilized inthe recommendation domain. a presents an overview ofmultimodal pretraining techniques.",
  "Self-supervised Pretraining Paradigms": "We broadly categorize self-supervised pretraining paradigms intothree types according to their pretraining tasks.Reconstructive Paradigm. This pretraining paradigm aimsto teach models to reconstruct raw inputs within the information bottleneck framework. Examples include mask prediction methodsfor partial reconstruction and autoencoder methods for completereconstruction. Mask prediction methods were initially introducedin BERT , where input tokens are randomly masked, prompt-ing the model to learn to predict them based on the surroundingcontext. In contrast, autoencoder methods (e.g., AE , VAE )encode input data into a concise latent space and subsequently learnto fully recover the input from this latent representation. Thesemethods have found extensive use in self-supervised pretrainingacross various domains such as text , vision , au-dio , and multimodal data . Following their success,researchers have applied the reconstructive pretraining paradigmto recommendation tasks. For instance, methods like mask item pre-diction in Bert4Rec , mask token prediction in Recformer ,autoencoder-based item tokenization , and masked nodefeature reconstruction in PMGT have emerged. Despite signif-icant progress, relying solely on this reconstructive paradigm maynot capture proximity information from user-item interactions ef-fectively. Consequently, these methods are typically complementedwith a contrastive learning paradigm in practice.Contrastive Paradigm. This pretraining focuses on pairwisesimilarity, distinguishing between similar and dissimilar data sam-ples by maximizing distances between negative pairs and mini-mizing them for positive pairs within a representation space. Ithas proven effective in enhancing the quality of representationsacross different domains. Examples such as SimCSE for text,SimCLR for images, CLMR for music, and CLIP formultimodal representations highlight its versatility and applicabil-ity. Given its ability to capture pairwise similarities, this paradigmfinds extensive use in aligning user-item preferences. Applicationslike MGCL , MMSSL , MMCPR , MSM4SR , andMISSRec exemplify the utilization of contrastive learning forenhancing multimodal pretraining in recommender systems.Autoregressive Paradigm. This paradigm has recently achievedremarkable success, particularly with the rise of large languagemodels (LLMs) such as the GPT family . It gener-ates sequence data token by token in an autoregressive manner,where each token is predicted based on previous observations. Inother words, this approach operates in a unidirectional, left-to-rightgeneration framework, which is different from the reconstructiveparadigm that employs bidirectional context to predict masked to-kens. It has also gained rapid adoption in the CV domain andmultimodal domain . In the realm of recommender systems,user behavior sequences naturally lend themselves to sequentialprocessing, fostering the development of numerous autoregressivesequential recommendation models such as SASRec . Recentstudies, such as P5 and VIP5 , have explored the integrationof LLMs or pretrained multimodal models into recommendationtasks. Concurrently, generative recommendation, which frames rec-ommendation as autoregressive sequence generation, has emergedas a burgeoning area of research .",
  ": An overview of multimodal pretraining, adaptation, and generation tasks for recommendation": "Consequently, numerous studies have explored content-enhancedpretraining methods for recommendation systems. In this section,we categorize existing research based on the modalities employedfor pretraining and discuss their application both generally andwithin recommendation systems.Text-based Pretraining. Texts are among the most prevalentforms of content in recommender systems, applied in contextssuch as news recommendation and review-based recommenda-tion. Within the domain of natural language processing (NLP),pretrained language models like BERT and T5 have beendeveloped to capture context-aware representations of text. Thesemodels typically follow a pretraining-finetuning paradigm tailoredto specific tasks. Recently, large language models (LLMs) such asChatGPT and LLaMa have demonstrated significant ca-pabilities in language-related tasks, leveraging techniques such asprompting and in-context learning. Building on their success, text-enhanced pretraining has gained traction in recommender systems.Notable examples include MINER for news recommendation,Recformer for sequential recommendation, UniSRec forcross-domain recommendation, and P5 for LLM-based interac-tive recommendation.Audio-based Pretraining. Music recommendation representsa prominent scenario heavily reliant on audio modalities to capturecontent semantics. Analogous to the NLP domain, various pretrain-ing techniques have been employed to enhance audio representa-tions, including Wav2Vec , MusicBert , MART , andMERT . In the context of music recommendation, researchersexplore leveraging these audio pretraining methods by utilizinguser-item interactions as supervision signals to finetune musicrepresentations. For example, Chen et al. propose learninguser-audio embeddings from track data and user interests throughcontrastive learning techniques. Furthermore, Huang et al. in-tegrate pairwise textual and audio features into a convolutionalmodel to jointly learn content embeddings in a similarity metric.Interested readers can find additional examples in a comprehensivereview paper .Vision-based Pretraining. Images and videos constitute theprimary visual data in multimedia recommendation scenarios as ads, movies, and videos. In the CV domain, the evolution of vision-based pretraining has transitioned from CNN-based architectureslike ResNet to transformer architectures such as ViT andDINOv2 , enabling the extraction of versatile visual features.These pretrained models have significantly advanced vision-awarerecommendation systems. Researchers like Liu et al. and Chenet al. leverage pretrained CNN encoders with category priors toextract image features for industrial recommendation tasks, finetun-ing image encoders alongside recommendation models. Similarly,Wang et al. and Wei et al. utilize pretrained transformerencoders, such as ViT , to encode images and sequences of userbehaviors. Vision foundation models continue to evolve rapidly,promising future applications in recommendation tasks. Lookingahead, there is a growing interest in exploring the use of newly pre-trained models for recommendation tasks. For instance, leveragingpretrained video transformers like Video-LLaVA remains anunexplored area in building video recommendation models.Multimodal Pretraining. In current literature, most studiestend to focus on modeling the primary modality of content, such astext for news recommendation , audio for music recommenda-tion , and images for e-commerce recommendation . How-ever, multimedia content inherently involves multiple modalities.For instance, news articles often include titles, descriptions, andaccompanying images. Similarly, video recommendation involveshandling visual frames, audio signals, and subtitles.Unlike single-modal techniques, multimodal models must cap-ture both commonalities and complementary information acrossmultimodal data sources through techniques like cross-modal align-ment and fusion. In recent years, multimodal pretraining has seenrapid development, resulting in a plethora of pretrained models,including single-stream models (e.g., VL-BERT ), dual-streammodels (e.g., CLIP ), and hybrid models (e.g., FLAVA andCoCa ). Recent research has also focused on achieving uni-fied representations of multimodal data, exemplified by modelslike ImageBind , MetaTransformer , and UnifiedIO-2 .Another emerging trend involves integrating multimodal encoderswith large language models, resulting in multimodal large languagemodels such as BLIP-2 , Flamingo , and Llava . These",
  "MULTIMODAL ADAPTION FORRECOMMENDATION": "While most existing pretrained models are trained on general datacorpora, adapting them for recommender systems requires strategicmethods to fully utilize their learned knowledge. This section sum-marizes four major adaptation techniques: representation transfer,model finetuning, adapter tuning, and prompt tuning. Each tech-nique provides a distinct approach to harnessing the benefits of apretrained model. b presents an overview of multimodaladaptation techniques.",
  "Representation Transfer": "Representation transfer is one of the most commonly used adap-tation techniques for transferring pretrained knowledge to recom-mendation models. Specifically, item representations are extractedfrom frozen pretrained models and used as additional featuresalongside ID embeddings. These representations provide supple-mentary general information to recommender systems, addressingthe cold-start problem where new or infrequently interacted itemsmay have inadequate ID embeddings derived from limited interac-tions . Approaches based on representation transfer have beenextensively studied and proven effective across various domains,e.g., text-based recommendation , vision-based recommenda-tion , multimodal recommendation .For multimodal scenarios specifically, significant efforts have beendirected towards fusing representations from multiple modalities.This includes techniques such as early fusion , in-termediate fusion , and late fusion . An-other research focus involves aligning multimodal representationswithin user behavior spaces using methods like content-ID align-ment , item-to-item matching , user sequence model-ing , and graph neural networks .However, straightforward and efficient representation transfermay encounter a significant domain generalization gap due to mis-alignment between the semantic space and the behavior space,which may not consistently lead to performance improvementsin practice. Model finetuning offers a direct solution to addressthis issue. Furthermore, as noted by KDSR , there is a risk offorgetting modality features in representations, leading them toresemble those trained without incorporating such features. Oneviable approach involves integrating explicit constraints , while",
  "Model Finetuning": "Model finetuning refers to the process of further training a pre-trained model on task-specific data. Its goal is to adapt the modelparameters to effectively capture domain-specific nuances, therebyimproving its performance on the specific downstream task. Thispretraining-finetuning paradigm has proven successful in variouspractical applications. Specifically, finetuning can involve aligningthe semantic space of pretrained models with the behavior spaceof recommendation models. Depending on the application of pre-trained models, they can extract item representations ,user representations , or both . Moreover, basedon the types of downstream tasks, current research can be classi-fied into representation-based matching tasks andscoring-based ranking tasks .However, end-to-end finetuning of pretrained models with rec-ommendation data faces challenges related to training efficiency.Recommendation tasks often require processing millions or evenbillions of samples daily. Fully finetuning a pretrained model sub-stantially amplifies training overhead, which poses practical limita-tions in large-scale recommender systems, particularly given thescale of large language and multimodal models . Moreover,finetuning with large volumes of data easily leads to the issue of cat-astrophic forgetting, where previously learned knowledge rapidlydeteriorates during continual training.",
  "Adapter Tuning": "To reduce training overhead with pretrained large models, parameter-efficient finetuning (PEFT) methods have been developed. Oneprominent approach is through parameter-efficient adapters likeLoRAs , which integrate compact, task-specific modules di-rectly into pretrained models. This strategy effectively reduces thenumber of parameters needed for finetuning and facilitates rapidmodel adaptation. Widely recognized for its efficacy across var-ious domains, PEFT techniques have gained significant tractionin recommendation systems . For example, theONCE framework leverages the pretrained Llama model with LoRAs as item encoders to enhance content-aware recom-mendation. Similarly, UniSRec employs an MOE-based adapterwith the BERT model to improve semantic representations of itemsacross diverse domains. In the realm of multimodal recommen-dation, TransRec and VIP5 have introduced layerwiseadapters. M3SRec utilizes modality-specific MOE adapters , whileEM3 employs multimodal fusion adapters during finetuning.Nonetheless, the ongoing challenge lies in designing adapters thateffectively balance both effectiveness and efficiency, which remainsan active area of research.",
  "Multimodal Pretraining, Adaptation, and Generation for Recommendation: A SurveyKDD 24, August 2529, 2024, Barcelona, Spain": "prompts from task-specific data while keeping the model param-eters frozen. As a result, prompt tuning can avoid catastrophicforgetting and enable fast adaptation with only prompt tokens astunable parameters. Depending on whether prompts are optimizedin a discrete token space, they can be further categorized into hardprompt tuning, such as AutoPrompt , and soft prompt tuning,like Prefix-Tuning . Prompt tuning has been successful in visuallearning and multimodal learning , enhancing modelperformance.For multimodal recommendation tasks, prompt tuning has emergedas a novel technique to adapt pretrained models. Recent studiessuch as RecPrompt , ProLLM4Rec , Prompt4NR , andPBNR employ prompting methods to customize large lan-guage models (LLMs) for news recommendation tasks. Additionally,DeepMP , VIP5 , and PromptMM employ prompttuning to integrate and adapt multimodal content knowledge toenhance recommendation. Despite recent advancements, this areaof research remains underexplored. An interesting direction is thedevelopment of personalized multimodal prompting techniques toadvance multimodal recommendation systems.",
  "With the support of powerful large language models (LLMs), textgeneration has become a mature capability and is now being appliedin various tasks within the recommendation domain": "Keyword Generation: Keyword tagging plays a pivotal role incontent understanding for ads targeting and recommendation.Previous techniques mostly rely on explicit keyword extractionfrom textual content, potentially missing important keywordsabsent from the text. Consequently, keyword generation tech-niques have been widely applied to enhance the keyword taggingprocess . News Headline Generation: The demand for personalized andengaging news content has fueled the exploration of news head-line generation. Conventionally, headline generation is framed asa text summarization task, condensing input text or multimodalcontent into a title . However, typical news headlinesmay lack appeal or relevance to specific users, prompting theneed for personalized approaches. Consequently, personalizedheadline generation has emerged as a compelling research topic,focusing on generating titles tailored to individual users readingpreferences and available news content . Marketing Copy Generation: Marketing copy refers to the textused to promote a product and motivate consumers to purchase.It plays a vital role in capturing users interest and enhancingengagement. Recent efforts have focused on automatic marketingcopywriting based on LLMs Explanation Generation: In interactive scenarios, the demandfor explainable recommendations is growing significantly. Thisinvolves generating natural language explanations to justify therecommendation of items to individual users, thereby enhancinguser understanding and trust in the system . Dialogue Generation: Dialogue generation is essential inconversational recommender systems, encompassing the genera-tion of responses that describe recommended items . More-over, it entails generating questions to guide users towards fur-ther rounds of conversation and interaction . While these tasks benefit from powerful LLMs, two critical chal-lenges persist in text generation for recommendation: 1) Control-lable Generation: Industrial applications necessitate precise con-trol over generated texts to ensure correctness of product descrip-tions, use unique selling propositions, or adhere to specific writingstyles . 2) Knowledge-Enhanced Generation: Existing LLMsoften lack explicit awareness of domain-specific knowledge, suchas product entities, categories, and selling points. Recent researchhas concentrated on integrating domain-specific knowledge basesto achieve more satisfactory results .",
  "Image and Video Generation": "Text-to-image generation has achieved remarkable success withthe prevalence of diffusion models (e.g., SD ). In this section,we delve into their potential applications in e-commerce and ad-vertising. Unlike natural image generation, generating productimages and ad banners involves dealing with complex layouts, en-compassing various elements such as products, logos, and textualdescriptions. Consequently, unique challenges arise in designing acoherent layout and effectively integrating text with appropriatefonts and colors to create visually appealing posters.Specifically, Inoue et al. propose LayoutDM, a model de-signed to effectively handle structured layout data and facilitatethe discrete diffusion process. Hsu et al. enable content-awarelayout generation (namely PosterLayout) by arranging predefinedspatial elements on a given canvas. Lin et al. develop Auto-Poster, a highly automated and content-aware system for gener-ating advertising posters. Concurrently, some studies explore textdesign for poster generation. For example, Gao et al. introduceTextPainter, a novel multimodal approach that leverages contextualvisual information and corresponding text semantics to generatetext images. Tuo et al. propose a diffusion-based multilingualvisual text generation and editing model, AnyText, which addresseshow to render accurate and coherent text in the image.More recently, video generation has made significant strides.Sora emerges as a groundbreaking technology showcasing im-mense potential for generating advertising videos for products. Inthis context, Gong et al. introduce AtomoVideo, a high-fidelityimage-to-video generation solution that effectively transforms prod-uct images into engaging promotional videos for advertising pur-poses. Additionally, Liu et al. have devised a system capableof automatically generating visual storylines from a given set ofvisual materials, producing compelling promotional videos tailoredfor e-commerce. Furthermore, Wang et al. have developed anintegrated approach, merging text-to-image models, video motion",
  "Personalized Generation": "With the rise of AIGC, there is a notable shift towards personalizedgeneration, aiming to enhance the customization and personaliza-tion of generated content. This trend holds particular significancein recommendation scenarios, where personalized content can bet-ter cater to users interests. Pioneering work has been undertakenin various domains, including personalized news headline gener-ation , personalized product description generationin e-commerce , personalized answer generation , person-alized image generation with identity preservation , and per-sonalized multimodal generation . Integrating recommendersystems with personalized generation techniques shows promisefor developing next-generation recommender systems.",
  "In this section, we summarize some common application domainsthat require multimodal recommendation techniques": "E-commerce Recommendation. E-commerce represents oneof the most extensively studied application domains in recom-mender systems research, aimed at assisting users in discover-ing items they are likely to purchase. The abundance of multi-modal data in e-commerce, including product titles, descriptions,images, and reviews, poses a challenge in integrating differentmodalities with user interaction data to enhance recommenda-tion quality. To address this challenge, numerous research effortshave been undertaken. Notable examples include works by Al-ibaba , JD.com , and Pinterest . Advertisement Recommendation. Online advertising servesas a primary revenue source for many web applications. Adver-tising creatives play a pivotal role in this ecosystem, spanningvarious formats such as images, titles, and videos. Aesthetic cre-atives have the potential to engage potential users and enhancethe click-through rate (CTR) of products . There is also a press-ing need to understand ad creatives better to effectively alignadvertisements with users interests . News Recommendation. Personalized news recommendationis a crucial technique for assisting users in discovering news ofinterest. To enhance recommendation accuracy and diversity, rec-ommender systems must comprehend news content and extractsemantic information from a users reading history. This often in-volves learning semantic representations of news titles, abstracts,body text, and cover images. Recent research has focused onmodeling features from multiple modalities, as exemplified byMM-Rec and IMRec . Video Recommendation. With the surge in popularity of micro-video platforms, video recommendation has garnered significantattention within the community. Videos encapsulate a multi-tude of modalities, including titles, thumbnail images, frames,audio tracks, transcripts, and more. Current research efforts have been concentrated on integrating and adapting multimodal infor-mation within micro-video recommendation models. .Notably, Ni et al. have recently introduced a comprehensivemicro-video recommendation dataset, enriched with abundantmultimodal side information, to foster further research in thisdomain. Music Recommendation. The realm of music streaming ser-vices represents another prominent domain that necessitatesmultimodal recommendation techniques. Within this sphere, adiverse array of multimodal data is involved, including musicaudio, scores, lyrics, tags, and reviews. Leveraging these varioustypes of music data has proven effective in crafting more per-sonalized recommendations aimed at engaging users; notableexamples can be found in . Additionally, Shen et al. propose that incorporating multimodal information from userssocial media can offer insights into their personalities, emotions,and mental well-being, thereby enhancing the accuracy of musicrecommendation. Fashion Recommendation. With the visual and aesthetic na-ture of fashion products, fashion recommendation has emergedas a distinct vertical domain. Unlike traditional recommendersystems, fashion recommendation not only suggests individualitems but also outfits that complement multiple items. Multi-modal understanding capabilities play a pivotal role in this area,including tasks such as localizing fashion items from images,identifying their attributes, and computing compatibility scoresfor multiple items . Moreover, pioneering work hasdeveloped text-to-image diffusion models that allow users to vir-tually try on clothes. These techniques are expected to enhancethe personalization of fashion recommendation and elevate userexperience to the next level. LBS Recommendation. Location-based services (LBS) have be-come ubiquitous, offering a wide range of services includingtaxi travel, food delivery, and restaurant recommendation. Inthese contexts, users can share their Points of Interest (POI)check-ins, photos, opinions, and comments, which encompass arich array of multimodal spatio-temporal data. Integrating thismultimodal information and understanding spatio-temporal cor-relations among locations enables more accurate modeling ofuser preferences. Notable examples can be found in .",
  "In this section, we discuss the persistent challenges and emergingopportunities for future research": "Multimodal Information Fusion. Multimodal fusion has beenextensively explored in research. Within recommender systems,current studies primarily concentrate on fusing and adaptingmultimodal feature embeddings of items to recommendationmodels . However, multimodal information for recommen-dation inherently adopts a hierarchical structure, ranging fromuser behavior sequences to individual items, each comprisingmultiple modalities and further subdivided into semantic tokensand objects. Additionally, the impact of information from diversemodalities and regions can vary significantly among different",
  "users. As a result, the challenge lies in effectively fusing multi-modal information in a hierarchical and personalized manner tooptimize recommendations": "Multimodal Multi-domain Recommendation. Multimodalinformation provides rich semantic insights into item content.Despite considerable research into multimodal recommendationand cross-domain recommendation, effectively leveraging multi-modal information to bridge the information gap across domainsremains an open challenge . For instance, recommendingmusic based on a users reading habits entails semantic alignmentacross modalities (audio vs. text) and domains (music vs. books). Multimodal Foundation Models for Recommendation. Whilelarge language models and large multimodal models have emergedas foundation models in the NLP and CV domains, there existsa compelling opportunity to extend this exploration into therecommendation domain. An ideal recommendation foundationmodel should demonstrate robust in-context learning capabili-ties while maintaining generalizability across diverse tasks anddomains . Potential avenues for exploration include adapt-ing existing multimodal LLMs for recommendation tasks (e.g.,), or conducting the pretraining of a multimodal generativemodel from scratch using large-scale multimodal multi-domainrecommendation data. AIGC for Recommendation. The integration of AIGC repre-sents a notable advancement in recommender systems, offeringan opportunity to significantly enhance user personalization,engagement, and overall experience. This encompasses personal-ized news headlines, tailored advertising creatives, and explana-tory content across diverse recommendation contexts. This fieldis rapidly expanding, with the primary challenge lying in achiev-ing a comprehensive understanding of both content and users,facilitating controllable generation, and ensuring accurate format-ting to optimize the user experience. Additionally, it is imperativeto address potential ethical and privacy concerns arising fromthe use of AIGC. Multimodal Recommendation Agent. LLM-based agents have demonstrated exceptional proficiency in automating tasksthrough extensive knowledge and strong reasoning capabili-ties. The integration of these agents has introduced innovativeprospects in the field of recommendation, particularly in con-versational recommendation . This entails directly engagingusers in the task completion process, thereby enhancing the userexperience and the effectiveness of recommender systems. As aconcrete example, integrating conversation and virtual try-ongeneration capabilities may present new opportunities forfashion recommendation. Efficiency of Training and Inference. Recommendation taskstypically have stringent latency requirements to meet real-timeservice demands. Therefore, ensuring training and inference ef-ficiency becomes imperative when applying multimodal pre-training and generation techniques in practice. There is a highdemand for the development of efficient strategies to leverage thecapabilities of multimodal models. Pioneer efforts in this direc-tion include speeding up training by merging item sets to avoidredundant encoding operations and enhancing inferencespeed through caching item and user representations.",
  "CONCLUSION": "Multimodal recommendation, an immensely promising field, hasgarnered significant attention in recent years, fueled by advance-ments in both multimodal machine learning and the recommenda-tion system community. The advent of large multimodal models hastransformed the multimodal recommendation landscape, endow-ing it with enhanced capabilities for comprehension and contentgeneration. This paper provides a systematical overview of thecurrent multimodal recommendation framework, focusing on keyaspects such as multimodal pretraining, adaptation, and generation.Additionally, we delve into its applications, challenges, and futureprospects. Our aim is to offer this survey as a resourceful guide toaid subsequent research in the field.",
  "Dor Bank, Noam Koenigstein, and Raja Giryes. 2020. Autoencoders. CoRRabs/2003.05991 (2020)": "Shuqing Bian, Xingyu Pan, Wayne Xin Zhao, Jinpeng Wang, Chuyuan Wang,and Ji-Rong Wen. 2023. Multi-modal Mixture of Experts Represetation Learningfor Sequential Recommendation. In Proceedings of the 32nd ACM InternationalConference on Information and Knowledge Management (CIKM). 110119. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shot learners. Advances in NeuralInformation Processing Systems (NeurIPS) (2020), 18771901.",
  "Jin Chen, Ju Xu, Gangwei Jiang, Tiezheng Ge, Zhiqiang Zhang, Defu Lian, andKai Zheng. 2021. Automated Creative Optimization for E-Commerce Advertising.In The ACM Web Conference (WWW). 23042313": "Ke Chen, Beici Liang, Xiaoshuan Ma, and Minwei Gu. 2021. Learning audioembeddings with user listening data for content-based music recommenda-tion. In IEEE International Conference on Acoustics, Speech and Signal Processing(ICASSP). IEEE, 30153019. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. 2020.A Simple Framework for Contrastive Learning of Visual Representations. InProceedings of the 37th International Conference on Machine Learning (ICML).15971607. Xin Chen, Qingtao Tang, Ke Hu, Yue Xu, Shihang Qiu, Jia Cheng, and Jun Lei.2022. Hybrid CNN Based Attention with Category Prior for User Image BehaviorModeling. In Proceedings of the 45th International ACM SIGIR Conference onResearch and Development in Information Retrieval (SIGIR). 23362340. Yashar Deldjoo, Fatemeh Nazary, Arnau Ramisa, Julian J. McAuley, GiovanniPellegrini, Alejandro Bellogn, and Tommaso Di Noia. 2024. A Review of ModernFashion Recommender Systems. ACM Comput. Surv. 56, 4 (2024), 87:187:37.",
  "abs/2107.11803 (2021)": "Xiuqi Deng, Lu Xu, Xiyao Li, Jinkai Yu, Erpeng Xue, Zhongyuan Wang, DiZhang, Zhaojie Liu, Guorui Zhou, Yang Song, Na Mou, Shen Jiang, and HanLi. 2024. End-to-end training of Multimodal Model and ranking Model. CoRRabs/2404.06078 (2024). Yang Deng, Yaliang Li, Wenxuan Zhang, Bolin Ding, and Wai Lam. 2022. To-ward Personalized Answer Generation in E-Commerce via Multi-perspectivePreference Modeling. ACM Trans. Inf. Syst. 40, 4 (2022), 87:187:28. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.BERT: Pre-training of Deep Bidirectional Transformers for Language Under-standing. In Proceedings of the 2019 Conference of the North American Chapterof the Association for Computational Linguistics: Human Language Technologies(NAACL-HLT). 41714186. Zijian Ding, Alison Smith-Renner, Wenjuan Zhang, Joel R. Tetreault, and Ale-jandro Jaimes. 2023. Harnessing the power of LLMs: Evaluating human-AI textco-creation through the lens of news headline generation. In Findings of EMNLP.33213339. Xiao Dong, Xunlin Zhan, Yangxin Wu, Yunchao Wei, Michael C. Kampffmeyer,Xiaoyong Wei, Minlong Lu, Yaowei Wang, and Xiaodan Liang. 2022. M5Product:Self-harmonized Contrastive Learning for E-commercial Multi-modal Pretrain-ing. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).2122021230. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. AnImage is Worth 16x16 Words: Transformers for Image Recognition at Scale. In9th International Conference on Learning Representations (ICLR). Xiaoyu Du, Xiang Wang, Xiangnan He, Zechao Li, Jinhui Tang, and Tat-SengChua. 2020. How to learn item representation for cold-start multimedia rec-ommendation?. In Proceedings of the 28th ACM International Conference onMultimedia. 34693477. Haoyi Duan, Yan Xia, Mingze Zhou, Li Tang, Jieming Zhu, and Zhou Zhao.2023. Cross-modal Prompts: Adapting Large Pre-trained Models for Audio-Visual Downstream Tasks. In Advances in Neural Information Processing Systems(NeurIPS). Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, and Huaming Wang.2023. Clap learning audio concepts from natural language supervision. In IEEEInternational Conference on Acoustics, Speech and Signal Processing (ICASSP).IEEE, 15.",
  "Yue Feng, Shuchang Liu, Zhenghai Xue, Qingpeng Cai, Lantao Hu, Peng Jiang,Kun Gai, and Fei Sun. 2023. A Large Language Model Enhanced ConversationalRecommender System. CoRR abs/2308.06212 (2023)": "Junchen Fu, Fajie Yuan, Yu Song, Zheng Yuan, Mingyue Cheng, Shenghui Cheng,Jiaqi Zhang, Jie Wang, and Yunzhu Pan. 2024. Exploring adapter-based transferlearning for recommender systems: Empirical studies and practical insights. InProceedings of the 17th ACM International Conference on Web Search and DataMining (WSDM). 208217. Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple ContrastiveLearning of Sentence Embeddings. In Proceedings of the 2021 Conference onEmpirical Methods in Natural Language Processing (EMNLP). 68946910. Yifan Gao, Jinpeng Lin, Min Zhou, Chuanbin Liu, Hongtao Xie, Tiezheng Ge,and Yuning Jiang. 2023. TextPainter: Multimodal Text Image Generation withVisual-harmony and Text-comprehension for Poster Design. In ACM MM. 72367246. Tiezheng Ge, Liqin Zhao, Guorui Zhou, Keyu Chen, Shuying Liu, Huiming Yi,Zelin Hu, Bochao Liu, Peng Sun, Haoyu Liu, Pengtao Yi, Sui Huang, ZhiqiangZhang, Xiaoqiang Zhu, Yu Zhang, and Kun Gai. 2018. Image Matters: VisuallyModeling User Behaviors Using Advanced Model Server. In CIKM. 20872095. Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2022.Recommendation as language processing (rlp): A unified pretrain, personalizedprompt & predict paradigm (p5). In Proceedings of the 16th ACM Conference onRecommender Systems (RecSys). 299315. Shijie Geng, Juntao Tan, Shuchang Liu, Zuohui Fu, and Yongfeng Zhang. 2023.VIP5: Towards Multimodal Foundation Models for Recommendation. In Findingsof the Association for Computational Linguistics: EMNLP 2023. 96069620. Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan VasudevAlwala, Armand Joulin, and Ishan Misra. 2023. Imagebind: One embeddingspace to bind them all. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR). 1518015190.",
  "Headlines for News Stories. In The Web Conference 2020 (WWW). 17731784": "Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollr, and Ross Gir-shick. 2022. Masked autoencoders are scalable vision learners. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).1600016009. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residuallearning for image recognition. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR). 770778. Yupeng Hou, Shanlei Mu, Wayne Xin Zhao, Yaliang Li, Bolin Ding, and Ji-Rong Wen. 2022.Towards universal sequence representation learning forrecommender systems. In Proceedings of the 28th ACM SIGKDD Conference onKnowledge Discovery and Data Mining (KDD). 585593.",
  "Chengkai Huang, Tong Yu, Kaige Xie, Shuai Zhang, Lina Yao, and Julian J.McAuley. 2024. Foundation Models for Recommender Systems: A Survey andNew Perspectives. CoRR abs/2402.11143 (2024)": "Qingqing Huang, Aren Jansen, Li Zhang, Daniel PW Ellis, Rif A Saurous, andJohn Anderson. 2020. Large-scale weakly-supervised content embeddings formusic recommendation and tagging. In IEEE International Conference on Acous-tics, Speech and Signal Processing (ICASSP). 83648368. Yanhua Huang, Weikun Wang, Lei Zhang, and Ruiwen Xu. 2021. Sliding spec-trum decomposition for diversified recommendation. In Proceedings of the 27thACM SIGKDD Conference on Knowledge Discovery & Data Mining (KDD). 30413049.",
  "Chen Li, Yixiao Ge, Jiayong Mao, Dian Li, and Ying Shan. 2023. TagGPT: LargeLanguage Models are Zero-shot Multimodal Taggers. CoRR abs/2304.03022(2023)": "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Boot-strapping language-image pre-training with frozen image encoders and largelanguage models. In International Conference on Machine Learning (ICML). 1973019742. Jiacheng Li, Ming Wang, Jin Li, Jinmiao Fu, Xin Shen, Jingbo Shang, and JulianMcAuley. 2023. Text is all you need: Learning language representations forsequential recommendation. In Proceedings of the 29th ACM SIGKDD Conferenceon Knowledge Discovery and Data Mining (KDD). 12581267. Jian Li, Jieming Zhu, Qiwei Bi, Guohao Cai, Lifeng Shang, Zhenhua Dong, XinJiang, and Qun Liu. 2022. MINER: Multi-interest matching network for newsrecommendation. In Findings of the Association for Computational Linguistics(ACL). 343352.",
  "Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. 2023.Video-LLaVA: Learning United Visual Representation by Alignment BeforeProjection. CoRR abs/2311.10122 (2023)": "Jinpeng Lin, Min Zhou, Ye Ma, Yifan Gao, Chenxi Fei, Yangjian Chen, Zhang Yu,and Tiezheng Ge. 2023. AutoPoster: A Highly Automatic and Content-awareDesign System for Advertising Poster Generation. In ACM MM. 12501260. Chang Liu, Xiaoguang Li, Guohao Cai, Zhenhua Dong, Hong Zhu, and LifengShang. 2021. Noninvasive self-attention for side information fusion in sequentialrecommendation. In Proceedings of the AAAI Conference on Artificial Intelligence(AAAI). 42494256. Chang Liu, Han Yu, Yi Dong, Zhiqi Shen, Yingxue Yu, Ian Dixon, Zhanning Gao,Pan Wang, Peiran Ren, Xuansong Xie, Lizhen Cui, and Chunyan Miao. 2020.Generating Engaging Promotional Videos for E-commerce Platforms (StudentAbstract). In AAAI. 1386513866. Dairui Liu, Boming Yang, Honghui Du, Derek Greene, Aonghus Lawlor, RuihaiDong, and Irene Li. 2023. RecPrompt: A Prompt Tuning Framework for NewsRecommendation Using Large Language Models. CoRR abs/2312.10463 (2023).",
  "Kang Liu, Feng Xue, Dan Guo, Peijie Sun, Shengsheng Qian, and Richang Hong.2023. Multimodal graph contrastive learning for multimedia-based recommen-dation. IEEE Transactions on Multimedia (2023)": "Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, andGraham Neubig. 2023. Pre-train, Prompt, and Predict: A Systematic Survey ofPrompting Methods in Natural Language Processing. ACM Comput. Surv. 55, 9(2023), 195:1195:35. Qijiong Liu, Nuo Chen, Tetsuya Sakai, and Xiao-Ming Wu. 2024. Once: Boost-ing content-based recommendation with both open-and closed-source largelanguage models. In Proceedings of the 17th ACM International Conference onWeb Search and Data Mining (WSDM). 452461.",
  "Yuqing Liu, Yu Wang, Lichao Sun, and Philip S. Yu. 2024.Rec-GPT4V:Multimodal Recommendation with Large Vision-Language Models.CoRRabs/2402.08670 (2024)": "Yuting Liu, Enneng Yang, Yizhou Dang, Guibing Guo, Qiang Liu, Yuliang Liang,Linying Jiang, and Xingwei Wang. 2023. ID Embedding as Subtle Features ofContent and Structure for Multimodal Recommendation. CoRR abs/2311.05956(2023). Yong Liu, Susen Yang, Chenyi Lei, Guoxin Wang, Haihong Tang, Juyong Zhang,Aixin Sun, and Chunyan Miao. 2021. Pre-training graph transformer withmultimodal side information for recommendation. In Proceedings of the 29thACM International Conference on Multimedia (MM). 28532861. Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, ZhengqingYuan, Yue Huang, Hanchi Sun, Jianfeng Gao, Lifang He, and Lichao Sun. 2024.Sora: A Review on Background, Technology, Limitations, and Opportunities of",
  "Large Vision Models. arXiv:2402.17177": "Yifan Liu, Kangning Zhang, Xiangyuan Ren, Yanhua Huang, Jiarui Jin, YingjieQin, Ruilong Su, Ruiwen Xu, and Weinan Zhang. 2024. An Aligning and TrainingFramework for Multimodal Recommendations. CoRR abs/2403.12384 (2024). Zhuang Liu, Yunpu Ma, Matthias Schubert, Yuanxin Ouyang, and Zhang Xiong.2022. Multi-modal contrastive pre-training for recommendation. In Proceedingsof the 2022 International Conference on Multimedia Retrieval. 99108. Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, RyanMarten, Derek Hoiem, and Aniruddha Kembhavi. 2023. Unified-IO 2: ScalingAutoregressive Multimodal Models with Vision, Language, Audio, and Action.CoRR abs/2312.17172 (2023). Daniele Malitesta, Giandomenico Cornacchia, Claudio Pomo, Felice AntonioMerra, Tommaso Di Noia, and Eugenio Di Sciascio. 2023. Formalizing Multime-dia Recommendation through Multimodal Deep Learning. CoRR abs/2309.05273(2023).",
  "Maxime Oquab, Timothe Darcet, Tho Moutakanni, Huy Vo, Marc Szafraniec,et al. 2023. DINOv2: Learning Robust Visual Features without Supervision.CoRR abs/2304.07193 (2023)": "Yanjun Qin, Yuchen Fang, Haiyong Luo, Fang Zhao, and Chenxing Wang. 2022.Next Point-of-Interest Recommendation with Auto-Correlation Enhanced Multi-Modal Transformer Network. Proceedings of the 45th International ACM SIGIRConference on Research and Development in Information Retrieval (SIGIR). Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sand-hini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.2021. Learning transferable visual models from natural language supervision.In International Conference on Machine Learning (ICML). PMLR, 87488763.",
  "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, IlyaSutskever, et al. 2019. Language models are unsupervised multitask learners.OpenAI blog 1, 8 (2019), 9": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limitsof transfer learning with a unified text-to-text transformer. Journal of MachineLearning Research 21, 140 (2020), 167. Shashank Rajput, Nikhil Mehta, Anima Singh, Raghunandan Hulikal Keshavan,Trung Vu, Lukasz Heldt, Lichan Hong, Yi Tay, Vinh Tran, Jonah Samost, et al.2024. Recommender systems with generative retrieval. Advances in NeuralInformation Processing Systems (NeurIPS) 36 (2024).",
  "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.2022. Hierarchical text-conditional image generation with clip latents. arXivpreprint arXiv:2204.06125 1, 2 (2022), 3": "Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, AlecRadford, Mark Chen, and Ilya Sutskever. 2021. Zero-Shot Text-to-Image Gener-ation. In Proceedings of the 38th International Conference on Machine Learning(ICML), Vol. 139. 88218831. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjrnOmmer. 2022. High-Resolution Image Synthesis with Latent Diffusion Models.In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).1067410685.",
  "Alireza Salemi, Sheshera Mysore, Michael Bendersky, and Hamed Zamani. 2023.LaMP: When Large Language Models Meet Personalization. CoRR (2023)": "Steffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli. 2019.wav2vec: Unsupervised Pre-Training for Speech Recognition. In 20th AnnualConference of the International Speech Communication Association (Interspeech).34653469. Yu Shang, Chen Gao, Jiansheng Chen, Depeng Jin, Meng Wang, and Yong Li.2023. Learning fine-grained user interests for micro-video recommendation.In Proceedings of the 46th International ACM SIGIR Conference on Research andDevelopment in Information Retrieval (SIGIR). 433442. Tiancheng Shen, Jia Jia, Yan Li, Hanjie Wang, and Bo Chen. 2020. Enhancingmusic recommendation with social media content: an attentive multimodalautoencoder approach. In 2020 International Joint Conference on Neural Networks(IJCNN). IEEE, 18.",
  "Mingyang Song, Haiyun Jiang, Shuming Shi, Songfang Yao, Shilong Lu, Yi Feng,Huafeng Liu, and Liping Jing. 2023. Is ChatGPT A Good Keyphrase Generator?A Preliminary Study. CoRR abs/2303.13001 (2023)": "Xuemeng Song, Chun Wang, Changchang Sun, Shanshan Feng, Min Zhou, andLiqiang Nie. 2023. MM-FRec: Multi-Modal Enhanced Fashion Item Recom-mendation. IEEE Transactions on Knowledge and Data Engineering 35 (2023),1007210084. Janne Spijkervet and John Ashley Burgoyne. 2021. Contrastive Learning ofMusical Representations. In Proceedings of the 22nd International Society forMusic Information Retrieval Conference (ISMIR). 673681.",
  "Wenqi Sun, Ruobing Xie, Shuqing Bian, Wayne Xin Zhao, and Jie Zhou. 2023.Universal Multi-modal Multi-domain Pre-trained Recommendation.CoRRabs/2311.01831 (2023)": "Zhulin Tao, Yinwei Wei, Xiang Wang, Xiangnan He, Xianglin Huang, andTat-Seng Chua. 2020. Mgat: Multimodal graph attention network for recom-mendation. Information Processing & Management 57, 5 (2020), 102277. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-AnneLachaux, Timothe Lacroix, Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, et al. 2023. Llama: Open and efficient foundation language models. arXivpreprint arXiv:2302.13971 (2023). Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, ShrutiBhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models.arXiv preprint arXiv:2307.09288 (2023). Bayu Distiawan Trisedya, Jianzhong Qi, Wei Wang, and Rui Zhang. 2022. GCP:Graph Encoder With Content-Planning for Sentence Generation From Knowl-edge Bases. IEEE Trans. Pattern Anal. Mach. Intell. 44, 11 (2022), 75217533.",
  "Aaron Van Den Oord, Oriol Vinyals, et al. 2017. Neural discrete representationlearning. Advances in Neural Information Processing Systems (NeurIPS) 30 (2017)": "Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, YukeZhu, Linxi Fan, and Anima Anandkumar. 2023. Voyager: An Open-EndedEmbodied Agent with Large Language Models. CoRR abs/2305.16291 (2023). Jinpeng Wang, Ziyun Zeng, Yunxiao Wang, Yuting Wang, Xingyu Lu, TianxiangLi, Jun Yuan, Rui Zhang, Hai-Tao Zheng, and Shu-Tao Xia. 2023. Missrec: Pre-training and transferring multi-modal interest-aware sequence representationfor recommendation. In Proceedings of the 31st ACM International Conference onMultimedia (MM). 65486557. Weimin Wang, Jiawei Liu, Zhijie Lin, Jiangqiao Yan, Shuo Chen, Chetwin Low,Tuyen Hoang, Jie Wu, Jun Hao Liew, Hanshu Yan, Daquan Zhou, and Jiashi Feng.2024. MagicVideo-V2: Multi-Stage High-Aesthetic Video Generation. CoRRabs/2401.04468 (2024). Ye Wang, Jiahao Xun, Mingjie Hong, Jieming Zhu, Tao Jin, Wang Lin, HaoyuanLi, Linjun Li, Yan Xia, Zhou Zhao, and Zhenhua Dong. 2024. EAGER: Two-Stream Generative Recommender with Behavior-Semantic Collaboration. InProceedings of the ACM SIGKDD Conference on Knowledge Discovery and DataMining (KDD). Zhenduo Wang, Yuancheng Tu, Corby Rosset, Nick Craswell, Ming Wu, andQingyao Ai. 2023. Zero-shot Clarifying Question Generation for ConversationalSearch. In Proceedings of the ACM Web Conference (WWW). 32883298. Tianxin Wei, Bowen Jin, Ruirui Li, Hansi Zeng, Zhengyang Wang, Jianhui Sun,Qingyu Yin, Hanqing Lu, Suhang Wang, Jingrui He, and Xianfeng Tang. 2024.Towards Unified Multi-Modal Personalization: Large Vision-Language Modelsfor Generative Recommendation and Beyond. CoRR (2024).",
  "Conference 2023. 790800": "Wei Wei, Jiabin Tang, Lianghao Xia, Yangqin Jiang, and Chao Huang. 2024.PromptMM: Multi-Modal Knowledge Distillation for Recommendation withPrompt-Tuning. In Proceedings of the ACM on Web Conference (WWW). 32173228. Yinwei Wei, Wenqi Liu, Fan Liu, Xiang Wang, Liqiang Nie, and Tat-Seng Chua.2023. Lightgt: A light graph transformer for multimedia recommendation. InProceedings of the 46th International ACM SIGIR Conference on Research andDevelopment in Information Retrieval (SIGIR). 15081517.",
  "Yinwei Wei, Xiang Wang, Qi Li, Liqiang Nie, Yan Li, Xuanping Li, and Tat-Seng Chua. 2021. Contrastive Learning for Cold-Start Recommendation. CoRRabs/2107.05315 (2021)": "Yinwei Wei, Xiang Wang, Liqiang Nie, Xiangnan He, and Tat-Seng Chua. 2020.Graph-Refined Convolutional Network for Multimedia Recommendation withImplicit Feedback. In The 28th ACM International Conference on Multimedia(MM). 35413549. Yinwei Wei, Xiang Wang, Liqiang Nie, Xiangnan He, Richang Hong, and Tat-Seng Chua. 2019. MMGCN: Multi-modal graph convolution network for per-sonalized recommendation of micro-video. In Proceedings of the 27th ACMInternational Conference on Multimedia (MM). 14371445. Chuhan Wu, Fangzhao Wu, Suyu Ge, Tao Qi, Yongfeng Huang, and Xing Xie.2019. Neural news recommendation with multi-head self-attention. In Proceed-ings of the Conference on Empirical Methods in Natural Language Processingand the International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 63896394. Chuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng Huang. 2021. Empoweringnews recommendation with pre-trained language models. In Proceedings ofthe 44th international ACM SIGIR Conference on Research and Development inInformation Retrieval (SIGIR). 16521656. Chuhan Wu, Fangzhao Wu, Tao Qi, Chao Zhang, Yongfeng Huang, and TongXu. 2022. MM-Rec: Visiolinguistic Model Empowered Multimodal News Rec-ommendation. In The 45th International ACM SIGIR Conference on Research andDevelopment in Information Retrieval (SIGIR). 25602564. Yunjia Xi, Weiwen Liu, Jianghao Lin, Jieming Zhu, Bo Chen, Ruiming Tang,Weinan Zhang, Rui Zhang, and Yong Yu. 2023. Towards open-world recom-mendation with knowledge augmentation from large language models. arXivpreprint arXiv:2306.10933 (2023).",
  "Fangxiong Xiao, Lixi Deng, Jingjing Chen, Houye Ji, Xiaorui Yang, Zhuoye Ding,and Bo Long. 2022. From Abstract to Details: A Generative Multimodal FusionFramework for Recommendation. In MM. 258267": "Shitao Xiao, Zheng Liu, Yingxia Shao, Tao Di, Bhuvan Middha, Fangzhao Wu,and Xing Xie. 2022. Training Large-Scale News Recommenders with PretrainedLanguage Models in the Loop. In The 28th ACM SIGKDD Conference on Knowl-edge Discovery and Data Mining (KDD). 42154225. Lanling Xu, Junjie Zhang, Bingqian Li, Jinpeng Wang, Mingchen Cai, Wayne XinZhao, and Ji-Rong Wen. 2024. Prompting Large Language Models for Recom-mender Systems: A Comprehensive Framework and Empirical Analysis. CoRRabs/2401.04997 (2024). Song Xu, Haoran Li, Peng Yuan, Yujia Wang, Youzheng Wu, Xiaodong He, YingLiu, and Bowen Zhou. 2021. K-PLUG: Knowledge-injected Pre-trained LanguageModel for Natural Language Understanding and Generation in E-Commerce. InFindings of EMNLP. 117. Jiahao Xun, Shengyu Zhang, Zhou Zhao, Jieming Zhu, Qi Zhang, Jingjie Li,Xiuqiang He, Xiaofei He, Tat-Seng Chua, and Fei Wu. 2021. Why do we click:visual impression-aware news recommendation. In Proceedings of the 29th ACMInternational Conference on Multimedia (MM). 38813890. Guipeng Xv, Si Chen, Chen Lin, Wanxian Guan, Xingyuan Bu, Xubin Li, HongboDeng, Jian Xu, and Bo Zheng. 2022. Visual Encoding and Debiasing for CTRPrediction. In Proceedings of the 31st ACM International Conference on Information& Knowledge Management (CIKM). 46154619. Shiquan Yang, Rui Zhang, Sarah M. Erfani, and Jey Han Lau. 2022. An Inter-pretable Neuro-Symbolic Reasoning Framework for Task-Oriented DialogueGeneration. In Proceedings of the 60th Annual Meeting of the Association forComputational Linguistics (ACL). 49184935. Xiao Yang, Tao Deng, Weihan Tan, Xutian Tao, Junwei Zhang, Shouke Qin, andZongyao Ding. 2019. Learning Compositional, Visual and Relational Represen-tations for CTR Prediction in Sponsored Search. In CIKM. 28512859.",
  "Zhiguang Yang, Lu Wang, Chun Gan, Liufang Sang, and et al. 2023. ParallelRanking of Ads and Creatives in Real-Time Advertising Systems. CoRR (2023)": "Dong Yao, Jieming Zhu, Jiahao Xun, Shengyu Zhang, Zhou Zhao, Liqun Deng,Wenqiao Zhang, Zhenhua Dong, and Xin Jiang. 2024. MART: Learning Hierarchi-cal Music Audio Representations with Part-Whole Transformer. In CompanionProceedings of the ACM on Web Conference (WWW). 967970. Zixuan Yi, Xi Wang, Iadh Ounis, and Craig Macdonald. 2022. Multi-modalGraph Contrastive Learning for Micro-video Recommendation. Proceedings ofthe 45th International ACM SIGIR Conference on Research and Development inInformation Retrieval (SIGIR) (2022).",
  "Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, andYonghui Wu. 2022. CoCa: Contrastive Captioners are Image-Text FoundationModels. Trans. Mach. Learn. Res. 2022 (2022)": "Licheng Yu, Jun Chen, Animesh Sinha, Mengjiao Wang, Yu Chen, Tamara L.Berg, and Ning Zhang. 2022. CommerceMM: Large-Scale Commerce Multi-Modal Representation Learning with Omni Retrieval. In The 28th ACM SIGKDDConference on Knowledge Discovery and Data Mining (KDD). 44334442. Zheng Yuan, Fajie Yuan, Yu Song, Youhua Li, Junchen Fu, Fei Yang, YunzhuPan, and Yongxin Ni. 2023. Where to go next for recommender systems? id-vs. modality-based recommender models revisited. In Proceedings of the 46thInternational ACM SIGIR Conference on Research and Development in InformationRetrieval (SIGIR). 26392649. Mingliang Zeng, Xu Tan, Rui Wang, Zeqian Ju, Tao Qin, and Tie-Yan Liu. 2021.MusicBERT: Symbolic Music Understanding with Large-Scale Pre-Training. InFindings of the Association for Computational Linguistics (ACL). 791800.",
  "Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. 2022. Learningto Prompt for Vision-Language Models. Int. J. Comput. Vis. 130, 9 (2022), 23372348": "Wangchunshu Zhou, Yuchen Eleanor Jiang, Ethan Wilcox, Ryan Cotterell, andMrinmaya Sachan. 2023. Controlled Text Generation with Natural LanguageInstructions. In Proceedings of International Conference on Machine Learning(ICML). 4260242613. Xin Zhou and Zhiqi Shen. 2023. A tale of two graphs: Freezing and denoisinggraph structures for multimodal recommendation. In Proceedings of the 31stACM International Conference on Multimedia (MM). 935943. Jieming Zhu, Quanyu Dai, Liangcai Su, Rong Ma, Jinyang Liu, Guohao Cai, XiXiao, and Rui Zhang. 2022. BARS: Towards Open Benchmarking for Recom-mender Systems. In The 45th International ACM SIGIR Conference on Researchand Development in Information Retrieval (SIGIR). 29122923. Jieming Zhu, Xin Zhou, Chuhan Wu, Rui Zhang, and Zhenhua Dong. 2024.Multimodal Pretraining and Generation for Recommendation: A Tutorial. InCompanion Proceedings of the ACM on Web Conference 2024 (WWW). 12721275. Luyang Zhu, Dawei Yang, Tyler Zhu, Fitsum Reda, William Chan, ChitwanSaharia, Mohammad Norouzi, and Ira Kemelmacher-Shlizerman. 2023. TryOn-Diffusion: A Tale of Two UNets. In IEEE/CVF Conference on Computer Visionand Pattern Recognition (CVPR). 46064615."
}