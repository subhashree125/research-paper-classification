{
  "CRAG Comprehensive RAG Benchmark": "Xiao Yang1, Kai Sun1, Hao Xin3, Yushi Sun3, Nikita Bhalla1, Xiangsen Chen4, SajalChoudhary1, Rongze Daniel Gui1, Ziran Will Jiang1, Ziyu Jiang4, Lingkun Kong1, Brian Moran1,Jiaqi Wang1, Yifan Ethan Xu1, An Yan1, Chenyu Yang4, Eting Yuan1, Hanwen Zha1, Nan Tang3,4,Lei Chen3,4, Nicolas Scheffer1, Yue Liu1, Nirav Shah1, Rakesh Wanga1, Anuj Kumar1, Wen-tau Yih2,and Xin Luna Dong1",
  "Abstract": "Retrieval-Augmented Generation (RAG) has recently emerged as a promising solu-tion to alleviate Large Language Model (LLM)s deficiency in lack of knowledge.Existing RAG datasets, however, do not adequately represent the diverse and dy-namic nature of real-world Question Answering (QA) tasks. To bridge this gap, weintroduce the Comprehensive RAG Benchmark (CRAG), a factual question an-swering benchmark of 4,409 question-answer pairs and mock APIs to simulate weband Knowledge Graph (KG) search. CRAG is designed to encapsulate a diversearray of questions across five domains and eight question categories, reflectingvaried entity popularity from popular to long-tail, and temporal dynamisms rangingfrom years to seconds. Our evaluation of this benchmark highlights the gap tofully trustworthy QA. Whereas most advanced LLMs achieve 34% accuracyon CRAG, adding RAG in a straightforward manner improves the accuracy onlyto 44%. State-of-the-art industry RAG solutions only answer 63% of questionswithout any hallucination. CRAG also reveals much lower accuracy in answer-ing questions regarding facts with higher dynamism, lower popularity, or highercomplexity, suggesting future research directions. The CRAG benchmark laid thegroundwork for a KDD Cup 2024 challenge and attracted thousands of participantsand submissions. We commit to maintaining CRAG to serve research communitiesin advancing RAG solutions and general QA solutions. CRAG is available at",
  "Introduction": "Large Language Models (LLMs) have transformed the landscape of Natural Language Processing(NLP) tasks, especially in Question Answering (QA) . Despite the advancements,the issue of hallucination persists as a significant challenge; LLMs may generate answers that lackfactual accuracy or grounding . Studies have shown that GPT-4s accuracy in answeringquestions referring to slow-changing or fast-changing facts is below 15% ; even for stable (never-changing) facts, GPT-4s accuracy in answering questions referring to torso-to-tail (less popular)entities is below 35% . Overcoming hallucinations thus becomes a priority in building reliableQA systems . Retrieval-Augmented Generation (RAG) has recently emerged as a promising solution toalleviate LLMs deficiency in lack of knowledge and attracted a lot of attention from both academiaresearch and industry. Given a question, a RAG system searches external sources to retrieve relevantinformation and then provides grounded answers (see for an illustration). Despite",
  "its potential, RAG still faces many challenges, such as selecting the most relevant information,reducing question answering latency, and synthesizing information to answer complex questions": "A comprehensive benchmark is currently missing to advance continued research efforts in this field.Our goal is to build a benchmark that can provide a holistic view of the important capabilities andfast but reliable evaluation for RAG to propel the area forward. What is a good benchmark for QAover LLMs? We consider five critical features. 1. Realism: First and foremost, a good benchmark shall best reflect real use cases. In otherwords, a solution that achieves high metrics in the benchmark shall also perform very well inreal scenarios. For example, the questions in a RAG benchmark shall be similar to questionspeople ask in real-world QA scenarios. 2. Richness: The benchmark shall contain a diverse set of instance types, covering bothcommon use cases and some complex and advanced use cases, to represent real-worldchallenges and reveal possible limitations of existing solutions. 3. Insightfulness: The benchmark shall allow for an easy understanding of performance ondifferent slices of the data, reflecting the capability of the solution in addressing differenttypes of challenges. 4. Reliability: The benchmark shall allow reliable assessment of metrics: the ground truthsshall be accurate; the metrics shall well capture the performance of the model; the evaluationshall be easy and reliable, and the computed metrics shall hold statistical significance. 5. Longevity: Finally, to enable research and experimental comparison in a long term, thescenarios and the data in the benchmark shall not quickly expire and ideally shall be refreshedand improved over time.",
  "We strive to create a benchmark that have all of the aforementioned features, and we call it CRAG Comprehensive benchmark for RAG. Our work makes three contributions": "Our first contribution is the dataset itself (). CRAG contains a rich set of 4,409 QA pairsfrom five domains: Finance, Sports, Music, Movie, and Open domain. In addition to simple-factquestions (asking for an attribute of an entity), CRAG contains seven types of complex questions tocover real user queries: questions with Conditions, Comparison questions, Aggregation questions,Multi-hop questions, Set queries, Post-processing-heavy questions, and False-premise questions.CRAG reflects varied entity popularity from popular to long-tail and temporal spans ranging fromseconds to years, allowing easy deep dives for insights. As we generated the questions, we referredto smart assistant use cases to make sure the questions are realistic, paraphrased the questions toincrease the diversity of expressions, and manually verified ground truths to ensure reliability. In addition to QA pairs, CRAG provides mock APIs to simulate retrieval from a diverse range ofavailable information. This includes up to 50 full HTML pages for each question returned from areal-world search enginethe Brave Search API , and mock KGs with 2.6 million entities. For themock KGs, we deliberately make sure that the retrieval candidates reflect noises in a realistic setting. Our second contribution is the evaluation mechanism to allow for reliable comparisons. We designed3 tasks to test different components in RAG solutions: retrieval summarization, knowledge graphand web retrieval, and end-to-end retrieval-augmented generation (). Instead of computingthe percentage of correctly answered questions, our score system distinguishes hallucinated answersand missing answers, and gives the former a higher penalty as it can be more harmful to ruin user",
  "trust. We also design an effective automatic evaluation mechanism to allow for fast evaluations anditerations ()": "Our third contribution is a comprehensive evaluation of straightforward RAG solutions and industrystate-of-the-art solutions on RAG (). Whereas most advanced LLMs achieve 34%accuracy on CRAG, adding RAG in a straightforward manner improves the accuracy only to 44%.State-of-the-art industry RAG solutions answer only 63% questions without any hallucination, stillhaving much lower accuracy in answering questions regarding facts with higher dynamism, lowerpopularity, or higher complexity. These evaluations serve two roles: first, they demonstrate thatCRAG has appropriate level of difficulty and allows insights drawn from different dimensions ofdiversities the benchmark has incorporated; second, they highlight the gaps and research directions toa fully trustworthy QA system. The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge2, has attracted thousandsof participants and submissions within the first 50 days of the competition. We commit to maintainingCRAG to serve research communities in advancing RAG solutions and general QA solutions. Comparison with existing benchmarks. compares CRAG with existing benchmarks forfactual question answering. Traditional QA benchmarks such as Natural Questions (NQ) ,TriviaQA , MS MARCO , and QALD-10 have advanced QA in the past decade butconsider only web retrieved or KG retrieved contents, and do not adequately represent the diverseand dynamic challenges that RAG is facing. New benchmarks for LLM or RAG usually target certaincapabilities of the QA system. Researchers created benchmarks to evaluate how well the systemscan answer simple knowledge questions and handle more advanced scenarios. Theseinclude answering questions with changing answers , integrating information from multipledocuments , addressing multi-hop questions , and answering questions with long texts .Moreover, traditional QA benchmarks usually adopt matching-based metrics such as ROUGE or F1 to evaluate the quality of the responses . These metrics, although working well forextractive methods, are known to not perform very effectively for LLMs that generate free-formresponses . Despite CRAG being smaller than MS MARCO and NQ, it offers several distinctive advantages:comprehensive coverage, realistic testing with mock APIs, dynamic question handling, diverse factpopularity, extensive content beyond Wikipedia, and fast yet reliable evaluation. These features makeCRAG a robust and versatile benchmark for testing RAG systems and broadly QA systems, providinga shared testbed to evaluate how these systems handle real-world, dynamic, and diverse informationretrieval and synthesis challenges for reliable LLM-based question answering.",
  "False PremiseQuestions that have a false preposition or assumption (e.g., Whats the name ofTaylor Swifts rap album before she transitioned to pop? (Taylor Swift has not yetreleased any rap album))": "We designed three tasks. They share the same set of (question, answer) pairs but differ in the externaldata accessible for retrieval to augment QA. Here, we provide the content that can be leveraged inQA to ensure fair comparisons. We describe how we generated the data in . Task 1: Retrieval Summarization. In Task 1, we provide up to five web pages for each question.These web pages are likely, but not guaranteed, to be relevant to the question. This task aims to testthe answer generation capability of a RAG system. Task 2: KG and Web Retrieval Augmentation. In Task 2, we in addition provide mock APIs toaccess information from underlying mock KGs. The mock KGs store structured data relevant to thequestions; answers to the questions may or may not exist in the mock KGs. The mock APIs takeinput parameters, oftentimes parsed from the question, and provide structured data from the mockedKGs to support answer generation. This task tests how well a RAG system 1) queries structured datasources and 2) synthesizes information from different sources. Task 3: End-to-end RAG. Similar to Task 2, Task 3 also provides both web search results and mockAPIs as candidates for retrieval but provides 50 web pages, instead of 5, as candidates. The larger setof web pages are more likely to provide necessary information to answer the question, but meanwhileare more likely to contain noises. As such, Task 3 in addition tests how a RAG system ranks a largernumber of retrieval results. The three tasks, each adding upon the previous one, allow testing different capabilities of the RAGsystems. The only component of a RAG system not covered by these tasks is search retrieval. Onemay easily extend the tasks to use all 220K webpages in our benchmark as the search corpus for fullyend-to-end testing.",
  "Question typeFinanceSportsMusicMovieOpenTotal": "Simple466 (45)23 ( 3)112 (18)519 (46)85 (11)1,205 (27)Simple w. condition113 (11)250 (30)92 (15)112 (10)122 (15)689 (16)Set48 ( 5)93 (11)72 (12)104 ( 9)86 (11)403 ( 9)Comparison146 (14)85 (10)102 (16)105 ( 9)98 (12)536 (12)Aggregation69 ( 7)137 (16)96 (15)71 ( 6)116 (15)489 (11)Multi-hop86 ( 8)64 ( 8)55 ( 9)90 ( 8)87 (11)382 ( 9)Post-processing heavy26 ( 3)24 ( 3)26 ( 4)28 ( 2)76 (10)180 ( 4)False Premise85 ( 8)157 (19)69 (11)96 ( 9)118 (15)525 (12)",
  "Question answering pairs": "CRAG covers five domains: Finance, Sports, Music, Movie, and Open domain, and eight types ofquestions, all in English. The question types are listed in . We constructed the question-answerpairs both from underlying KGs and web contents. QA pairs constructed from KGs. We constructed QA pairs from KGs by collecting a set of entitiesbased on publicly available data and then creating 600+ question templates based on selected entitytypes and relations. Next, we sampled entities with different popularities (head, torso and tail)following from the KGs to fill in the templates and generate the full question and answer. QA pairs constructed from web contents. We asked annotators to write down possible questionsthat users may ask (e.g., most popular action movies in 2023) and created QA pairs from thecorresponding web search results. Using the above methods, we collected 2,425 Web Questions and 1,984 KG Questions, with 661, 658,and 665 KG Questions containing head, torso, and tail entities respectively. Tables 3 and 4 summarizethe distribution of the questions across different dimensions. The size of each dimension slice (e.g.,fast-changing facts) allows us to get metrics with 5% margin-of-error (with 95% confidence level)for most of the cases. The dynamism distribution roughly reflects the nature of the domain (e.g.,much more real-time questions for Finance than for other domains). See Appendix A.1.2 for thedefinition of the dynamism categories.",
  "Web search results. For each question, we used the question text as the search query and stored upto 50 HTML pages from the Brave search API . See in Appendix A.1.5 for an example": "We estimated the web search recall with a heuristic-based method: first check whether the groundtruth answer URL was found among the pages; if not, decide whether the fact in the ground truths iscontained in the page snippet or content with an LLM. In particular, we pass the question, groundtruth, and the pages to Llama 3 70B Instruct and ask it to judge if the context is sufficient to answerthe question. Number of retrieved web pages Estimated recall (%) 45.1 52.856.157.659.560.861.862.863.564.4 69.4 76.279.080.181.182.282.983.683.984.6 Page snippets+ Full pages",
  ": For 85% of CRAG questions, the web search results are estimated to contain the groundtruth facts. The curve shows that the retrieval recall grows sharply at the beginning and flattens outlater on": "shows the estimated web search recall curve for all CRAG questions, with an overall recallof 85% when using all 50 pages. It reflects multiple advantages of the benchmark by design. First,the recall curves are sharp at the beginning and flatten out later, and the recall for the top 5 pages isabout 69%. This is comparable to what we observe in practice when developing a RAG system. Thenon-perfect coverage, especially for Task 1, allows us to test whether the RAG solutions admit Idont know when the retrieval results do not contain the necessary information. Second, comparedto the recall from web snippets, full web pages increase the recall by about 20%, emphasizing theimportance of extracting and understanding HTML contents. Moreover, the estimated web searchrecall (50 web pages) is 93% for Web Questions and 74% for KG Questions, indicating significantlylower recall for KG questions than web questions. This aligns with our observations that web searchrecall for torso and tail entities is typically lower, underlining the crucial role of leveraging KGs inTask 2 and 3. Mock KGs. We created mock KGs that contain publicly available KG data used to generate thequestions, randomly selected entities of the same type, and also hard negative entities with similarnames (e.g., phantom for phantom of the opera). Mock APIs. We created mock APIs with pre-defined parameters to support structured search in themock KGs. For example, for queries asking for stock prices, an example mock API is in the form ofget_price_history(ticker). We collected snapshots of the KG and web search data concurrently while posing real-time andfast-changing questions. This approach ensures that we capture the snapshot of the informationworld at the time of question answering. A RAG solution that performs well on the benchmark shouldalso be capable of reasoning over time and generalizing to evolving questions.",
  "Incorrect. The response provides wrong or irrelevant information to answer the users question": "We use a scoring method with score 1, 0.5, 0, and 1 for each perfect, acceptable, missing, andincorrect answer, respectively, where we penalize hallucinated answers and prefer missing answers toincorrect ones. We then define truthfulness as the average score from all examples in the evaluationset for a given RAG system.",
  "Evaluation": "Similar to previous work , we employ both human evaluation (human-eval) and model-basedautomatic evaluation (auto-eval). In the former, we use manual grading to judge perfect, acceptable,missing, and incorrect for each answer. In the latter, we merge perfect and acceptable, call it accurate,and use a three-way scoring system with 1, 1, 0 for accurate, incorrect, and missing answers. We design a two-step method for automatic evaluation: if the answer matches the ground truth exactly,it is considered accurate; otherwise, we use LLMs to determine whether the response is accurate,incorrect, or missing. To avoid the self-preference problem , we use two LLM evaluators:ChatGPT (gpt-3.5-turbo-0125) and Llama 3 (llama-3-70B-instruct) and report theaverage accurate, hallucination, missing rates, and truthfulness scores from the two models for eachRAG system. Our offline experiment shows that this two-step method yields an average F1 score of94.7% for ChatGPT and 98.9% for Llama 3 compared to human-eval. See Appendix A.2.2 for moredetails. Test data split. We split the data randomly into validation (30%), public test (30%), and private(40%), and released the validation and public test sets (Appendix A.2.3). Participants of the KDDCup challenge can use the validation and public test sets to develop and test their models, and thesubmitted solutions were evaluated on the private test set. Future offline users of CRAG can usethe validation set for development, fine-tuning, and validation, and the public test set for testing andresult reporting.",
  "Straightforward RAG solutions": "Experiment setup: We started with running LLM-only solutions on the CRAG public testset with 1, 335 questions, using simple prompts that encourage brief answers and I dontknow answers when the confidence is low (Appendix A.3.1).We employed Llama 2 Chat(llama-2-7b-chat and llama-2-70b-chat) , Llama 3 Instruct (llama-3-8B-instruct andllama-3-70B-instruct) , Mixtral (Mixtral-8x7B-Instruct-v0.1) , Falcon (40B) ,FLAN-T5 (FLAN-T5-XXL) , and GPT-4 Turbo (gpt-4-turbo-2024-04-09) . The web-onlyRAG solutions we evaluated (Task 1) used a fixed-length web context window (1K tokens for Falconand FLAN-T5, 2K for Llama 2 Chat, and 4K for Llama 3 Instruct and GPT-4 Turbo); we concatenatedwebpage snippets using the original order from the data as the reference text, until filling up thewindow (similar to ). Our KG-based solutions (Tasks 2, 3) additionally used a fixed-lengthKG context window (0.5K tokens for Falcon and FLAN-T5, 1K for Llama 2 Chat, and 2K for Llama3 Instruct and GPT-4 Turbo) to include the results from the mock APIs; we extracted the relevantquery entities using llama-3-8B-instruct with in-context learning (similar to ) detailed inAppendix A.3.1 and concatenated the results returned from all applicable mock APIs (based on theextracted entities), until filling up the window. We provide an extensive comparison of all LLMsin Appendix A.3.2 and focus on the best-performing LLMs (i.e., Llama 3 70B Instruct and GPT-4Turbo) in this section. : Performance of straightforward RAG solutions. All numbers are in percentage. LLM-onlysolutions has up to 34% accuracy and straightforward RAG solutions has up to 44% accuracy. Thesubscript in truthfulnessa denotes the result is reported by auto-eval.",
  ": LLM-only and Task 3 solution auto-eval truthfulness (in percentage) across domain,dynamism, popularity, and question type": "shows the average evaluation results from the two auto-evaluators (ChatGPT and Llama 3) andillustrates that the CRAG benchmark is non-trivial. First, the best LLM-only solutions (GPT-4 Turbo)obtained an accuracy of only 34%, with truthfulness of 20%, showing a big room for improvement.Second, straightforward RAG solutions obtained up to 44% accuracy, showing that extra informationdoes help answer more questions reliably. Interestingly, none of the RAG solutions obtain truthfulnesshigher than 20%; this is because all RAG solutions introduce more hallucinations generated fromirrelevant retrieval results, showing a big challenge in RAGHow to judiciously use retrieval resultswithout being distracted by retrieval noises? Third, we found that Task 2 truthfulness scores arehigher than Task 1, showing that the KG knowledge helps improve accuracy, with a similar or evenlower hallucination rate, because the KG knowledge is typically brief but precise. Unfortunately, theimprovement is mediocre, showing a second challenge in RAGHow to best leverage the power ofKG data? Finally, the truthfulness for Task 3 are also higher than Task 2, because of better searchranking (recall that Task 1 and 2 provide five pages randomly selected from the top-10 search results)and better search recall. In particular, we found that the ground truths of over 30% of questions areavailable in the web retrieval results but are not included in the prompt due to the context windowlimitation. This shows the importance of search ranking in RAG. shows the auto-eval results across the domain, dynamism, popularity, and question typedimension. The results reveal a lot of interesting observations and show that the CRAG benchmarkallows more insightful conclusions. First, it shows which slices of the benchmark are harder. Forexample, we found much lower RAG truthfulness on the Finance and Sports domains, for real- : Benchmarking CRAG questions with industry SOTA RAG systems. Perfect, acceptable(Acc.), hallucination (Hall.), missing rates (Miss.), and truthfulnessh reported by human-eval (Truthh)are in percentages. The best system achieves truthfulness of 51% and provides perfect answers for upto 63% of questions.",
  "TrafficCopilot Pro70.09.514.36.160.5-weightedGemini Advanced67.110.012.710.259.3-ChatGPT Plus61.811.425.71.341.8-Meta SG61.07.114.117.850.5-Perplexity.ai63.76.320.99.145.9-": "time and fast-changing facts, for tail entities, and for complex questions requiring set answers,post-processing, and with false premises. Second, it shows where it is harder to leverage retrievalresults. Take the popularity slices as an example, we observed that GPT-4 Turbos truthfulnessdropped from head (21%) to torso (11%) to tail (8%), consistent with past observations ; however,the straightforward RAG solution based on GPT-4 Turbo improved QA quality regarding torso(+7%) and tail entities (+6%) but lowered the quality regarding head (-4%). Finally, althoughour goal is not to compare different LLMs, the different dimensions allow us to understand thestrengths and weaknesses of each method. For example, although the RAG system based on Llama3 70B Instruct has a lower overall truthfulness score than the one based on GPT-4 Turbo, it has asimilar or slightly higher truthfulness in answering simple and comparison questions, whereas muchlower truthfulness in answering set and post-processing questions, suggesting investigations on thereasoning capabilities.",
  "State-of-the-art industry solutions": "Next, we evaluated industry state-of-the-art (SOTA) RAG solutions on CRAG public test set. Weselected five RAG systems built upon SOTA LLMs and search engines, queried them with CRAGquestions, collected the responses, and applied manual grading (details in Appendix A.4). In addition, we applied traffic weights to the questions to understand the solutions in real-world usecases. The traffic weights reflect the frequency of each question type, as defined in , in realQA traffic. We gave the same weights to all domains and reported the macro average across domains.This is because we have observed quite different domain-level distributions in different use cases, buthave been observing similar distributions at the query-type level. and show the overall performance of the SOTA systems and their performanceacross different dimensions. The evaluation results confirm our belief that the CRAG benchmarkreveals interesting insights and shows room for improvement for existing RAG solutions. First,the results from SOTA solutions achieve much better truthfulness (highest 51%) compared to thestraightforward solutions. However, the hallucination rate ranges from 16% to 25%, so the answersare still not trustworthy. Note that the truthfulness scores between the SOTA solutions and thestraightforward solutions are not completely comparable, as they have different accesses to retrievalcontents (Appendix A.3 and A.4.1), and the former used auto-eval, while the latter used human-eval;however, the trend is valid. Second, we observed very different latency, ranging from 3.4s to 11.6s,reflecting the different design options in trading off latency and quality; for example, Copilot Prohas the highest truthfulness, but meanwhile highest latency, whereas Meta SG has mid-tiertruthfulness but lowest latency. (See Appendix A.4.2 for additional results and how we measuredlatency.) Third, most difficult slices we see in the straightforward solutions remain to be difficult forSOTA solutions: real-time and fast-changing queries, and questions regarding torso and tail entities,showing the improvement needed for handling retrieval noises when the system relies on retrievalresults to answer the question; as another example, we see lower truthfulness for queries requiringaggregation, multi-hop reasoning or post-processing, showing the improvement space for reasoning FinanceSportsMusicMovieOpen (a) Domain Real-timeFast-changingSlow-changingStatic -3 -7-5 -6 (b) Dynamism WebHeadTorsoTail (c) Popularity",
  "Conclusion": "This paper proposes CRAG, a rich and comprehensive benchmark designed to advance research inretrieval-augmented generation (RAG). With detailed empirical studies, CRAG reviewed gaps inexisting RAG solutions and provided valuable insights for future improvement. We plan to continueimproving and expanding the benchmark for multi-lingual questions, multi-modal questions, multi-turn conversations, etc., to ensure CRAG stays at the forefront to push RAG research, adapts toemerging challenges, and evolves for new research needs.",
  "Acknowledgements": "We would like to thank Jinsong Yu for his advice on the data strategy and help in connecting with thepartner teams. We thank Alex Boesenberg for enabling us to create web search results. We thankSam Wexler for coordinating this project with various partners. We thank Hejia Zhang, Rares Bostan,Eryk Helenowski, Nanshu Wang, Sinong Wang, and Denis Savenkov for the discussion regardingLLM and RAG evaluation. We thank Katie Manlove for coordinating with the budget and annotationneeds. We thank our annotation team for creating many great examples: David Vu, Florian Gawlitta,Rani Salomi Pitta, Lauren Gregory Mikus, Tara Welch, Nidhi Modi, Ray De Leon, Jader Ricarte,Joshua Aranzaso. We thank Apoorva Srinivas and Courtnee Parker for coordinating the annotationtasks. We would like to acknowledge the support from Jackie Leader, Mindy Abern, Heather Nolan,Ty Toledano, George Lan, Ben Edgar, Sy Choudhury, Rodrick Shepard, Katie Manlove, Mavis Hu,Jen Vinz, Taha Masood, Parkin Kent, Rebekkah Hogan, Tony Nelli, Jennifer Pak, Jonathan Torres,and Amy Lee. Lei Chens work is partially supported by National Key Research and Development Program ofChina Grant No. 2023YFF0725100, National Science Foundation of China (NSFC) under GrantNo. U22B2060, the Hong Kong RGC GRF Project 16213620, RIF Project R6020-19, AOE ProjectAoE/E-603/18, Theme-based project TRS T41-603/20R, CRF Project C2004-21G, GuangdongProvince Science and Technology Plan Project 2023A0505030011, Hong Kong ITC ITF grantsMHX/078/21 and PRP/004/22FX, Zhujiang scholar program 2021JC02X170, Microsoft ResearchAsia Collaborative Research Grant, HKUST-Webank joint research lab and HKUST(GZ)-ChuanglinGraph Data Joint Lab.",
  "AI@Meta. Llama 3 model card. 2024": "E. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cappelli, R. Cojocaru, M. Debbah, . Goffinet,D. Hesslow, J. Launay, Q. Malartic, et al. The falcon series of open language models. arXivpreprint arXiv:2311.16867, 2023. P. Bajaj, D. Campos, N. Craswell, L. Deng, J. Gao, X. Liu, R. Majumder, A. McNamara,B. Mitra, T. Nguyen, M. Rosenberg, X. Song, A. Stoica, S. Tiwary, and T. Wang. MS MARCO:A human generated machine reading comprehension dataset, 2018.",
  "Z. Chen, Z. Gu, L. Cao, J. Fan, S. Madden, and N. Tang. Symphony: Towards natural languagequery answering over multi-modal data lakes. In CIDR, 2023": "H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li, X. Wang, M. Dehghani,S. Brahma, et al. Scaling instruction-finetuned language models. Journal of Machine LearningResearch, 25(70):153, 2024. X. L. Dong. The journey to a knowledgeable assistant with retrieval-augmented generation (rag).In Companion of the 2024 International Conference on Management of Data, SIGMOD/PODS24, page 3, New York, NY, USA, 2024. Association for Computing Machinery.",
  "N. Kandpal, H. Deng, A. Roberts, E. Wallace, and C. Raffel. Large language models struggle tolearn long-tail knowledge. In International Conference on Machine Learning, pages 1569615707. PMLR, 2023": "T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein,I. Polosukhin, M. Kelcey, J. Devlin, K. Lee, K. N. Toutanova, L. Jones, M.-W. Chang, A. Dai,J. Uszkoreit, Q. Le, and S. Petrov. Natural questions: a benchmark for question answeringresearch. Transactions of the Association of Computational Linguistics, 2019. P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Kttler, M. Lewis, W. tauYih, T. Rocktschel, S. Riedel, and D. Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks, 2021.",
  "OpenAI. ChatGPT. 2023. Accessed: 2024-06-04. A. Panickssery, S. R. Bowman, and S. Feng. Llm evaluators recognize and favor their owngenerations. arXiv preprint arXiv:2404.13076, 2024": "R. Pradeep, N. Thakur, S. Sharifymoghaddam, E. Zhang, R. Nguyen, D. Campos, N. Craswell,and J. Lin. Ragnarz\" ok: A reusable rag framework and baselines for trec 2024 retrieval-augmented generation track. arXiv preprint arXiv:2406.16828, 2024. V. Rawte, S. Chakraborty, A. Pathak, A. Sarkar, S. Tonmoy, A. Chadha, A. P. Sheth, and A. Das.The troubling emergence of hallucination in large language modelsan extensive definition,quantification, and prescriptive remediations. arXiv preprint arXiv:2310.04988, 2023.",
  "Y. Tang and Y. Yang. Multihop-rag: Benchmarking retrieval-augmented generation for multi-hop queries. arXiv preprint arXiv:2401.15391, 2024": "H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu,J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini,R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A.Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra,I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M.Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan,I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, andT. Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. R. Usbeck, X. Yan, A. Perevalov, L. Jiang, J. Schulz, A. Kraft, C. Mller, J. Huang, J. Reineke,A.-C. Ngonga Ngomo, et al. QALD-10the 10th challenge on question answering over linkeddata. Semantic Web, (Preprint):115, 2023.",
  "We first collected a set of entities based on publicly available data. Then we created question-answerpairs in three steps for Simple static and dynamic questions": "Step 1. For each domain, we first selected an entity type and a meaningful relation pe, rq and createda question template. For example, for (music artist, first album), we create a template what is thefirst album of [music artist]?. Step 2. We then sampled entities from the KGs to fill in the templates and generate the full question.We adopted the method described in and sampled entities of top, middle, and bottom popularity.We defined popularity based on heuristics for each entity type and created an equal number ofquestions for each bucket.",
  "Step 3. Last, we took the associated attribute values as the answer to the question to create question-answer pairs": "We created the Comparison, Aggregation, Set, Post-processing, and False-premise questions in asimilar way but 1) made sure the template allows for crisp and deterministic answers and 2) sampledthe subject entities that fit the question. We used heuristics to select entity types for these questioncategories. Finally, we created multi-hop questions in three steps, similar to those described in . We firstsampled an entity e1 from the KG and selected two relation triplets following a two-hop path:pe1, r1, e2q and pe1, r2, e3q. We then created a question template describing the path. For example,for path (company1, is_parent, company2) followed by (company1, ceo, person), we created thetemplate \"who is the CEO of the parent company of [company2]?\". The answer to the new questionwill be e3 in the second triplet.",
  "Step 2. Generate the web search results to answer the question": "Step 3. Finally, annotators reviewed the web search results to determine the ground truth answers tothe questions: 1) If the search results successfully provided the necessary information, annotatorsrecorded the ground truth answer text and the URL associated with it based on the retrieved content.Note that the answer is determined by the query_time at which the web search happened, especiallyfor the Fast-changing and Real-time questions. 2) Otherwise, annotators conducted further websearches to document the correct answers.",
  "We conducted two phases of dataset validation with our in-house Linguist team": "Phase 1. Question and meta-label validation. After the initial round of QA pair generation, anaudit session was conducted, where expert annotators reviewed the question template, questions,and meta-labels (domain, question type, etc), applying edits as necessary with 2x human review(agreement rate 90%`). All problematic questions (e.g., a wrong false-premise question) wererevised, and all conflicting labels were resolved by a third more experienced auditor. Phase 2. Answer validation. To ensure the answers in the benchmark are correct, we furtherconducted an auditing process for all the answers. For the web questions, an annotation teamreviewed each question and conducted an extensive search to make sure the answer is factuallycorrect and includes comprehensive information (such as for set questions) with 2x human review(agreement rate 90%`). A third more experienced auditor then reviewed all conflicting answers andprovided a resolution. For the KG questions, a team of five engineers carefully checked the questionsand queried the mock APIs manually to validate the answers. This step resulted in a 5% answercorrection. In both phases, we paid special attention to examples where the straightforward solutions outputdifferent answers from the ground truth answers and asked the auditing team to double-check thoseexamples. This step yielded an additional 2% answer updates.",
  "CRAG provides mock APIs to simulate retrieval from web, KG, and real-time APIs in the realretrieval environment, allowing accessible facilitating data and fair comparison": "CRAG provides both structured (through mock KGs) and unstructured (web search results) informa-tion to test the effectiveness of RAG systems in leveraging a diverse range of available information.First, for each question in the benchmark, CRAG provides up to 50 web search results from a real-world search enginethe Brave Search API . Different from existing benchmarks that use snippetsor selected text chunks , CRAG provides full HTML pages, containing more information andpotentially more noises as in a realistic setting. Second, CRAG provides mock KG search APIs to teststructured search for RAG. The mock KGs, though much smaller in size, contain both informationnecessary to answer a subset of questions in the benchmark and noises that have similar entity orattribute names, again simulating real settings. Our mock KGs contain about 2.6M entities and havea signal-to-noise ratio of less than 1/30.",
  "open_search_entity_by_name(query: str) -> dictSearch for entities by name in the Open domain.open_get_entity(entity: str) -> dictRetrieve detailed information about an entity in theOpen domain": "movie_get_person_info(person_name: str) -> dictGet information about a person related to movies.movie_get_movie_info(movie_name: str) -> dictGet information about a movie.movie_get_year_info(year: str) -> dictGet information about movies released in a specificyear.movie_get_movie_info_by_id(movie_id: int) -> dictGet movie information by its unique ID.movie_get_person_info_by_id(person_id: int) -> dictGet person information by their unique ID. finance_get_company_name(query: str) -> dictSearch for company names in the finance domain.finance_get_ticker_by_name(query: str) -> dictRetrieve the ticker symbol for a given company name.finance_get_price_history(ticker_name: str) -> dictGet the price history for a given ticker symbol.finance_get_detailed_price_history(ticker_name: str) -> dictGet detailed price history for a ticker symbol.finance_get_dividends_history(ticker_name: str) -> dictGet dividend history for a ticker symbol.finance_get_market_capitalization(ticker_name: str) -> dictRetrieve market capitalization for a ticker symbol.finance_get_eps(ticker_name: str) -> dictGet earnings per share (EPS) for a ticker symbol.finance_get_pe_ratio(ticker_name: str) -> dictGet the price-to-earnings (PE) ratio for a ticker symbol.finance_get_info(ticker_name: str) -> dictGet financial information for a ticker symbol. music_search_artist_entity_by_name(artist_name: str) -> dictSearch for music artists by name.music_search_song_entity_by_name(song_name: str) -> dictSearch for songs by name.music_get_billboard_rank_date(rank: int, date: str = None) -> dictGet Billboard ranking for a specific rank and date.music_get_billboard_attributes(date: str, attribute: str, song_name: str) -> dictGet attributes of a song from Billboard rankings.music_grammy_get_best_artist_by_year(year: int) -> dictGet the Grammy Best New Artist for a specific year.music_grammy_get_award_count_by_artist(artist_name: str) -> dictGet the total Grammy awards won by an artist.music_grammy_get_award_count_by_song(song_name: str) -> dictGet the total Grammy awards won by a song.music_grammy_get_best_song_by_year(year: int) -> dictGet the Grammy Song of the Year for a specific year.music_grammy_get_award_date_by_artist(artist_name: str) -> dictGet the years an artist won a Grammy award.music_grammy_get_best_album_by_year(year: int) -> dictGet the Grammy Album of the Year for a specific year.music_grammy_get_all_awarded_artists() -> dictGet all artists awarded the Grammy Best New Artist.music_get_artist_birth_place(artist_name: str) -> dictGet the birthplace of an artist.music_get_artist_birth_date(artist_name: str) -> dictGet the birth date of an artist.music_get_members(band_name: str) -> dictGet the member list of a band.music_get_lifespan(artist_name: str) -> dictGet the lifespan of an artist.music_get_song_author(song_name: str) -> dictGet the author of a song.music_get_song_release_country(song_name: str) -> dictGet the release country of a song.music_get_song_release_date(song_name: str) -> dictGet the release date of a song.music_get_artist_all_works(artist_name: str) -> dictGet all works by an artist. sports_soccer_get_games_on_date(team_name: str, date: str) -> dictGet soccer games on a specific date.sports_nba_get_games_on_date(team_name: str, date: str) -> dictGet NBA games on a specific date.sports_nba_get_play_by_play_data_by_game_ids(game_ids: List[str]) -> dictGet NBA play by play data for a set of game ids.",
  "A.2.1Human evaluation": "We run human evaluation to score each answer with respect to the metrics defined in .1.We score each perfect, acceptable, missing, or incorrect answer with a score sp, sa, sm, and sin,respectively and define truthfulnessh as the score of the answer by setting sp 1, sa 0.5, sm 0,and sin 1. We then compute the average truthfulness for all examples in the evaluation set as thetruthfulness score for the RAG solution. Human-eval instructions. The human-eval instructions are as follows.Given a query, a query day and time at which the query was made, the chatbots response, grade theaccuracy for the response according to the criteria below:Using an external search engine, please evaluate the factual accuracy of the response based on thegrading rubric below. An accurate answer should be factually correct and provide useful informationto answer the users question. Accuracy = 0 (Missing). It covers the following situations. There is no response. There is afailure to provide a response to the request (e.g. Im sorry ......, I cant do ...) due toinability. E.g., Query: Latest news about the Nobel prize today. Response: I cant findspecific information regarding the Nobel prize ... The response fails to answer and asksfollow-up questions.",
  "Average96.199.195.299.196.798.894.798.9": "wrong numbers, or other significant factual errors). The answer is irrelevant to the usersrequest. Answers in a category, location, or time window that is significantly different fromthe users request, if any. There is a significant structural/formatting error. The response isotherwise a total structural/functional failure and does not contain sufficient well-formedcontent that can be used to determine accuracy Accuracy = 2 (Acceptable). It covers the following situations. The answer is acceptablycorrect and relevant to the users request, but may miss some information, i.e. accurate butnot complete, or mostly accurate with minor issues. The answer may contain some minorhallucination that doesnt significantly alter the overall meaning. Minor hallucinationmeans the answer addressed the users question but might be off on some additional details.The rule of thumb is to see if the answer serves the purpose of the users question, andwhether the hallucination could mislead the users on what they were asking.",
  "accuracy hallucination,": "where accuracy, hallucination, and missing are the percentage of accurate, incorrect, and missinganswers in the test set. These score choices penalize incorrect answers, award correct, and assign avalue of 0 to missing answers. Auto-evaluators. We computed the accuracy and F1 for the two auto-evaluators for the accurate,incorrect, and missing examples in the public test set, respectively. Here, we considered human-evallabels as the ground truth. shows both models attain reasonable accuracy and F1 scores asan evaluator compared to human evaluation. Auto-eval prompt. The prompt we used in the auto-eval is similar to the following. We did notrelease the exact prompt used in the challenge to avoid prompt attack.PROMPT = \"\"\"# Task: You are given a Question, a model Prediction, and a list of Ground Truthanswers, judge whether the model Prediction matches any answer from the list of Ground Truthanswers. Follow the instructions step by step to make a judgement.1. If the model prediction matches any provided answers from the Ground Truth Answer list,\"Accuracy\" should be \"True\"; otherwise, \"Accuracy\" should be \"False\".2. If the model prediction says that it couldnt answer the question or it doesnt have enoughinformation, \"Accuracy\" should always be \"False\".3. If the Ground Truth is \"invalid question\", \"Accuracy\" is \"True\" only if the model prediction isexactly \"invalid question\".# Output:Respond with only a single JSON string with an \"Accuracy\" field which is \"True\" or \"False\".# Examples:Question: how many seconds is 3 minutes 15 seconds?Ground truth: [\"195 seconds\"] Prediction: 3 minutes 15 seconds is 195 seconds.Accuracy: TrueQuestion: Who authored The Taming of the Shrew (published in 2002)?Ground truth: [\"William Shakespeare\", \"Roma Gill\"]Prediction: The author to The Taming of the Shrew is Roma Shakespeare.Accuracy: FalseQuestion: Who played Sheldon in Big Bang Theory?Ground truth: [\"Jim Parsons\", \"Iain Armitage\"]Prediction: I am sorry I dont know.Accuracy: False\"\"\"",
  "A.2.3KDD Cup 2024 Meta CRAG challenge": "The KDD Cup 2024 Meta CRAG challenge has two stages. Stage 1 is designed for the participants todevelop their RAG solutions by submitting their systems against the leaderboard, whereas Stage 2determines the final winners. We split our benchmark data into three sets with similar distributions:validation, public test, and private test at 30%, 30%, and 40%, respectively. We shared the validationand public test sets, in total, 2,706 examples in Stage 1, and held out the private test set to select thefinal winners in Stage 2. We used auto-eval for Stage 1, and selected top teams with auto-eval inStage 2 to conduct manual evaluation.",
  "A.3Evaluating straightforward RAG solutions": "We send each CRAG question in a prompt shown below. This prompt is designed to include theoriginal Query Time for the question and ask the LLM to answer the question based on the querytime and the retrieved result. Note that the retrieved result was also from the same query time andwas provided in CRAG. Moreover, this prompt encourages brief answers and I dont know answerswhen confidence is low.",
  "A.3.1Prompts used in straightforward RAG solutions": "Vanilla LLM. PROMPT = \"\"\" You are given a Question and the time when it was asked in thePacific Time Zone (PT), referred to as \"Query Time\". The query time is formatted as \"mm/dd/yyyy,hh:mm:ss PT\". Your task is to answer the question in as few words as possible.Please follow these guidelines when formulating your answer:1. If the question contains a false premise or assumption, answer invalid question.2. If you are uncertain or dont know the answer, respond with I dont know.### Question{query}### Query Time{query_time}### Answer\"\"\" RAG with web search results (Task 1). PROMPT = \"\"\" You are given a Question, References andthe time when it was asked in the Pacific Time Zone (PT), referred to as \"Query Time\". The querytime is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". The references may or may not help answer thequestion. Your task is to answer the question in as few words as possible.Please follow these guidelines when formulating your answer:1. If the question contains a false premise or assumption, answer invalid question.2. If you are uncertain or dont know the answer, respond with I dont know.### Question{query}### Query Time{query_time}### References{references}",
  "### Answer\"\"\"": "RAG with KG and web search results (Tasks 2 and 3). PROMPT = \"\"\" You are given a Question,References and the time when it was asked in the Pacific Time Zone (PT), referred to as \"QueryTime\". The query time is formatted as \"mm/dd/yyyy, hh:mm:ss PT\". The references may or may nothelp answer the question. Your task is to answer the question in as few words as possible.Please follow these guidelines when formulating your answer:1. If the question contains a false premise or assumption, answer invalid question.2. If you are uncertain or dont know the answer, respond with I dont know.### Question{query}### Query Time{query_time}### References# web{web_results}# knowledge graph{kg_response}### Answer\"\"\"",
  "For encyclopedia and other queries, these are possible keys:- main_entity: extract the main entity of the query": "For finance queries, these are possible keys:- market_identifier: stock identifiers including individual company names, stock symbols.- metric: financial metrics that the query is asking about. This must be one of the following: price,dividend, P/E ratio, EPS, marketCap, and other.- datetime: time frame that the query asks about. When datetime is not explicitly mentioned, useQuery Time as default. For movie queries, these are possible keys:- movie_name: name of the movie- movie_aspect: if the query is about a movie, which movie aspect the query asks. This must be oneof the following: budget, genres, original_language, original_title, release_date, revenue,title, cast, crew, rating, length.- person: person name related to moves- person_aspect: if the query is about a person, which person aspect the query asks. This must beone of the following: acted_movies, directed_movies, oscar_awards, birthday.- year: if the query is about movies released in a specific year, extract the year For music queries, these are possible keys:- artist_name: name of the artist- artist_aspect: if the query is about an artist, extract the aspect of the artist. This must be one of thefollowing: member, birth place, birth date, lifespan, artist work, grammy award count,grammy award date.- song_name: name of the song- song_aspect: if the query is about a song, extract the aspect of the song. This must be one of the",
  "ModelAccuracy (%)Hallucination (%)Missing (%)Truthfulness (%)": "LLM onlyLlama 2 7B Chat14.878.46.7-63.6Llama 2 70B Chat22.328.749.0-6.4Llama 3 8B Instruct23.733.842.6-10.1Llama 3 70B Instruct32.328.938.83.4Falcon 40B10.841.947.3-31.1FLAN-T5-XXL 11B9.48.781.90.7Mixtral-8x7B-Instruct-v0.120.827.052.1-6.2GPT-4 Turbo33.513.553.020.0 Task 1Llama 2 7B Chat16.483.10.5-66.7Llama 2 70B Chat29.361.09.7-31.7Llama 3 8B Instruct28.545.625.9-17.1Llama 3 70B Instruct35.631.133.34.5Falcon 40B21.955.522.5-33.6FLAN-T5-XXL 11B27.536.536.0-9.0Mixtral-8x7B-Instruct-v0.133.644.422.0-10.8GPT-4 Turbo35.928.235.97.7 Task 2Llama 2 7B Chat16.483.10.5-66.7Llama 2 70B Chat29.161.19.7-32.0Llama 3 8B Instruct28.645.525.9-16.9Llama 3 70B Instruct37.529.233.38.3Falcon 40B21.955.422.7-33.5FLAN-T5-XXL 11B27.436.636.0-9.2Mixtral-8x7B-Instruct-v0.133.444.622.0-11.2GPT-4 Turbo41.325.133.616.2 Task 3Llama 2 7B Chat16.083.60.4-67.6Llama 2 70B Chat31.965.72.4-33.7Llama 3 8B Instruct32.156.311.6-24.1Llama 3 70B Instruct40.631.627.89.1Falcon 40B22.056.621.3-34.6FLAN-T5-XXL 11B27.837.135.1-9.3Mixtral-8x7B-Instruct-v0.133.544.122.4-10.6GPT-4 Turbo43.630.126.313.4",
  "A.4.1Quality": "We send the CRAG public test set question as input to each of the SOTA RAG systems and collectthe responses for human grading. Note that the original Query Time and the provided retrieval resultsin CRAG are not used in this setting. We simply test the questions and ask human graders to gradethe responses based on when the query was made to the SOTA system. We called Copilot Pro,Gemini Advanced, and ChatGPT Plus through their web interfaces and Perplexity.ai through its API.Meta SG, designed as a smart glasses (SG) assistant, includes default on-device components such asAutomatic Speech Recognition (ASR) and Text-to-Speech (TTS), which are not typically enabledby default in other systems. To ensure a fair comparison, we excluded these on-device components, ensuring that answer quality was not affected. We called each system on the following dates inPacific Time: 05/12/202405/16/2024 (Copilot Pro), 05/20/202405/28/2024 (Gemini Advanced),05/27/202406/02/2024 (ChatGPT Plus), 05/15/202405/16/2024 (Perplexity.ai), and 07/02/2024(Meta SG). We set the conversation style to Precise when calling Copilot Pro and the temperatureto 0 when calling Perplexity.ai. We select GPT-4o and llama-3-sonar-large-32k-online as thebase LLM when calling ChatGPT Plus and Perplexity.ai, respectively.",
  "We quantified the latency by calculating the time difference between the timestamp of the querysubmission to the system and the timestamp when the complete response was received": "The latency of Perplexity.ai measured via API call is 2,455ms. Since latency measured by API calland web interface interactions are not directly comparable, we further called Perplexity.ai throughits web interface and reported the latency under this setting in . Note that this latency maynot correspond to the accuracy numbers from the API calls. For Meta SG, we estimated a latencycomparable to other web interface interactions by excluding on-device components such as ASR andTTS from the overall end-to-end latency measurement.",
  "A.5Limitations": "The three tasks in our benchmark do not directly evaluate the construction of a first-stage retrievalcandidate pool, a demanding retrieval task in its own right. This design decision ensures that thecompetition remains both challenging and achievable within the KDD Cups required three-monthtimeframe. Despite the limitation, users of our dataset have the option to use the union of all 220Kweb pages as a corpus to build a retriever. While this corpus does not match the entire web, it allowsfor fair comparisons and manageable costs."
}