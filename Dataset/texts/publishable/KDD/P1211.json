{
  "ABSTRACT": "Textual noise, such as typos or abbreviations, is a well-known issuethat penalizes vanilla Transformers for most downstream tasks. Weshow that this is also the case for sentence similarity, a fundamen-tal task in multiple domains, e.g. matching, retrieval or paraphras-ing. Sentence similarity can be approached using cross-encoders,where the two sentences are concatenated in the input allowingthe model to exploit the inter-relations between them. Previousworks addressing the noise issue mainly rely on data augmenta-tion strategies, showing improved robustness when dealing withcorrupted samples that are similar to the ones used for training.However, all these methods still suffer from the token distributionshift induced by typos. In this work, we propose to tackle textualnoise by equipping cross-encoders with a novel LExical-aware At-tention module (LEA) that incorporates lexical similarities betweenwords in both sentences. By using raw text similarities, our ap-proach avoids the tokenization shift problem obtaining improvedrobustness. We demonstrate that the attention bias introduced byLEA helps cross-encoders to tackle complex scenarios with tex-tual noise, specially in domains with short-text descriptions andlimited context. Experiments using three popular Transformer en-coders in five e-commerce datasets for product matching show thatLEA consistently boosts performance under the presence of noise,while remaining competitive on the original (clean) splits. We alsoevaluate our approach in two datasets for textual entailment andparaphrasing showing that LEA is robust to typos in domains withlonger sentences and more natural context. Additionally, we thor-oughly analyze several design choices in our approach, providinginsights about the impact of the decisions made and fostering futureresearch in cross-encoders dealing with typos.",
  "These authors contributed equally to this research": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 23, August 610, 2023, Long Beach, CA, USA 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0103-0/23/08...$15.00",
  "Sentence similarity, Transformers, typos, lexical, e-commerce": "ACM Reference Format:Mario Almagro, Emilio Almazn, Diego Ortego, and David Jimnez. 2023.LEA: Improving Sentence Similarity Robustness to Typos Using LexicalAttention Bias. In Proceedings of the 29th ACM SIGKDD Conference on Knowl-edge Discovery and Data Mining (KDD 23), August 610, 2023, Long Beach,CA, USA. ACM, New York, NY, USA, 11 pages.",
  "INTRODUCTION": "The fast pace of information systems in society makes noise to bepresent in almost every text generated by either humans or auto-mated processes. Medical reports, queries to databases, messages insocial media, receipt transcriptions or product titles in e-commerceare a few examples where real production systems need to copewith a high presence of textual noise like typos, misspellings orcustom abbreviations.",
  "KDD 23, August 610, 2023, Long Beach, CA, USAMario Almagro, Emilio Almazn, Diego Ortego, and David Jimnez": "M. Almagro, D. Jimnez, D. Ortego, E. Almazn, and E. Martnez. 2020. Block-SCL:Blocking Matters for Supervised Contrastive Learning in Product Matching. InInternational ACM SIGIR conference on research and development in InformationRetrieval (SIGIR), Workshop on e-commerce. P. Bajaj, D. Campos, N. Craswell, L. Deng, J. Gao, X. Liu, R. Majumder, A. Mc-Namara, B. Mitra, T. Nguyen, M. Rosenberg, X. Song, S. Tiwary A. Stoica, and T.Wang. 2016. MS MARCO: A Human Generated MAchine Reading COmprehen-sion Dataset. arXiv:1611.09268 (2016).",
  "RELATED WORK2.1Sentence similarity": "Determining the degree of similarity between two sentences is afundamental problem for matching, entailment or paraphrasingtasks and is normally tackled using two type of approaches: bi-encoders and cross-encoders. Bi-encoders are designed to processeach sentence independently, obtaining an embedding for eachof them. These models are typically trained with metric learningobjectives that pull together the representations of positive pairswhile pushing apart those of negative pairs. The authors in propose SimCSE, which exploits dropout to generate embeddingsthat build positive pairs in the unsupervised setup. They also pro-pose a supervised setting where they use textual entailment labelsto construct the positive pair. Tracz et al. adopt a triplet loss ina supervised setup for product matching. The approach describedin , extends SimCSE and propose to learn via equivariant con-trastive learning, where representations have to be insensitive todropout and sensitive to MLM-based word replacement perturba-tions. Supervised contrastive learning is also adopted in for sentence similarity in a general domain, while apply itfor product matching in e-commerce. Another popular method isSBERT , which addresses both the problem of predicting severalsimilarity degrees as a regression task and directly matching pairsof sentences via classification.Cross-encoders, on the other hand, jointly process a concate-nated pair of sentences. These models are often considered to out-perform bi-encoders , obtaining robust results in generaldomains , product matching or retrieval tasks . How-ever, their main drawback is the need to recompute the encodingfor each different pair of sentences. Therefore, many recent worksadopt hybrid solutions to improve bi-encoders. Humeau et al. pro-posed Poly-encoders that utilizes an attention mechanism toperform extra interaction after Siamese encoders. The TransEn-coder method alternates bi- and cross-encoder independenttrainings, while distilling their knowledge via pseudo-labels. Theresulting bi-encoder shows improved performance. Distillation isfurther explored in , where knowledge transfer from the cross-attention of a light interaction module is adopted during trainingand removed at inference time.",
  "Dealing with typos and abbreviations": "Recent literature demonstrates that Transformer-based architec-tures are not robust to textual noise, i.e. to misspellings orabbreviations of the input words . Despite usingsub-word tokenizers (e.g. WordPiece) designed to deal with out-of-vocabulary words, Transformers exhibit in practice performancedrops when exposed to typos. Words with noise are not likely to bepresent in the vocabulary and therefore, they are split into severalsub-words yielding to token distribution shifts with respect to thenoise-free counterpart .Training the model with synthetically generated perturbations is a standard practice to deal with typos. Some techniques usesimple addition, deletion or character swaps, while others use moresophisticated methods such as common misspellings or keyboardand OCR errors . Moreover, depending on the type of perturba-tion, the same type of noise can have a different impact, i.e. typosin various words of a sentence do not influence equally. This issuewas reported in showing that noise in relevant words yieldslarger performance drops. We can find approaches that complementthis practice with different architectural designs or specific losses.For example, in the authors realize that character-based Trans-formers provide improved robustness to typos and exploit this factto propose a self-teaching strategy to boost performance of denseretrievers. Authors in add a module to recognize the presenceof typos in words just before the downstream classifier, which helpsto align representations between noisy and clean versions.",
  "Relative attention bias": "Self-attention modules in Transformers receive as input the tokenrepresentations coming from the previous layers and output con-textual representations for each token estimated from a weightedcombination of the tokens representations. Modifications of theself-attention have been proposed to add a bias that accountsfor the relative distance between words/tokens in the input se-quence . This strategy known as relative positionalembeddings replaces the absolute positional embeddings, wherethe position was injected as part of the input of the Transformer.In the authors follow this idea and extend it with long and shortterm relations. Wennberg et al. propose a more interpretablerepresentation for translation invariant relative positional embed-dings using the Toeplitz matrix. The authors in simplify theembeddings by just adding fixed scalars to the attention values thatvary with the distance. This way of adding relative information be-tween tokens has also been applied to information extraction fromdocuments, where they use 2D relative distances or tabularstructural biases for table understanding .",
  "Self-attention": "A key cornerstone in the success of Transformers is the multi-head self-attention mechanism, which learns token dependenciesand encodes contextual information from the input . In par-ticular, a single head in this attention module receives an inputsequence of token representations coming from the previouslayer = (x1, . . . , x), where x R and computes a new se-quence = (z1, . . . , z) of the same length and hidden dimension. The resulting token representations are computed as follows:",
  "Lexical attention bias for cross-encoders": "As we demonstrate in , in presence of textual noise, struggles to relate similar terms corrupted by noise. To address thisissue, we propose to add a lexical attention bias to the self-attentionmodule of cross-encoders. This bias term guides the attention to-wards tokens with high lexical similarity. We illustrate our proposedarchitecture in .Cross-encoders for textual similarity receive as input the con-catenation of the two sentence representations to be compared:",
  "Pairwise LexicalSimilarity": ": Overview of the attention mechanism in Transform-ers where we add the proposed lexical attention bias (LEA).We use the traditional nomenclature for the key, query andvalue representations (Q, K, V). at the beginning of the training based on the magnitudes of bothterms.To compute the pairwise lexical attention embedding, we firstmeasure the similarity between words considering only inter-sentencerelations, i.e. lexical similarities between words of the same sentenceare set to 0:",
  "(6)": "where X and X represent the pair of input sentences to compare,(x) and(x) denote the input textual word associated to the i-thand j-th tokens, respectively, and Sim(, ) is a metric that measuresthe string similarity between two words. We elaborate on our choicefor the similarity metric in .3.Inspired by , we apply a sinusoidal function over to getan embedding that represents the lexical similarity:",
  ",(8)": "where = 104 and {0, . . . , 1}. The final lexical embedding is the concatenation of the two sinusoidal embeddings in Eq. 7and Eq. 8, respectively. Different from the original proposal wescale the similarity by 2 to cover the full range of the sinusoidalfunctions. This results in embeddings more uniformly distributedacross the output space.",
  "Experimental setting": "The impact of textual noise in the prediction of the models dependson whether it appears on relevant words or not . We argue thatwhen the sentences are short the probability of the random noiseappearing in relevant words is higher and, therefore, we expect ahigher contribution of the lexical attention bias. Hence, the coreof our experiments is conducted in five product matching datasets,where the sentences are short and normally with lack of syntax:Abt-Buy , Amazon-Google and WDC-Computers (small,medium and large) . Moreover, we validate the contributionof LEA in two related tasks of natural language domain: textualentailment (RTE ) and paraphrasing (MRPC ). Details aboutthe datasets are provided in Tables 1 and 2, respectively. We artifi-cially introduce typos on the aforementioned datasets as describedin .1.1.",
  "Keyboard substitution. A random character from the wordis replaced by a close character in the QWERTY keyboard,e.g. screen scteen": "We modify all sentences in the test splits, where each word has a20% chance to be augmented. Only one type of operation is appliedto each word, which is chosen randomly among the five options.We limit the augmentation to words with more than 3 charactersto mitigate the effect of words becoming non-recognizable fromthe original form e.g. ace ate. 4.1.2Baseline models. Due to the lack of prior works dealing withtextual noise in cross-encoders for sentence similarity tasks, weadopt a benchmarking based on the comparison between threeversions of cross-encoders: 1) vanilla, 2) trained with data augmen-tation (DA) and 3) trained with data augmentation and LEA. Weadopt 2) as the reference baseline in the literature following the ap-proach of related works in other domains, where they successfullyapplied data augmentation to deal with typos .For data augmentation during training we apply the same con-figuration as in the synthetic generation of the test splits (see.1.1) and use a 50% chance for each sentence to be aug-mented. We use three popular pre-trained language models (PLMs)of varying sizes, i.e. Electra-small , BERT-Medium and BERT-Base . 4.1.3Implementation details. In all the experiments we fine-tunethe PLMs described in .1.2 for 30 epochs, using AdamWwith a batch size of 32, an initial learning rate of 55, a weightdecay of 55 and we apply a cosine annealing scheduler with awarm-up of 1.5 epochs. For LEA, is automatically fixed in Eq. 5at the beginning of the training for each layer of the Transformerand leave W being trained independently per head. As similarity metric we use Jaccard (see .3 for more details) and applythe proposed lexical attention bias to half of the last layers in allarchitectures (see .4 for a detailed analysis). For moredetails we refer the reader to Appendix B.We use the same training data for all methods and evaluate themon two different test splits, the original (clean) and the corruptedversion with typos. We run three different seeds for the trainingand create three test splits randomly introducing typos, as their in-fluence may differ depending on the words containing typos. Thus,we report the resulting mean and standard deviation over three andnine results for the clean and typo experiments, respectively.The test splits with typos, the binaries of the models and therequired material to reproduce results are available in our reposi-tory2.",
  "Robustness across datasets": "We compare in the F1-score of LEA with that of vanilla cross-encoders trained without and with data augmentation (+ DA) infive product matching datasets. We observe that applying data aug-mentation to mimic typos during training improves the robustnessto them as reported by previous works in the retrieval domain .When we apply LEA, we outperform the baseline by 5.4, 6.1 and 7.0points on average across the five datasets for Electra-small, BERT-Medium and BERT-Base, respectively. Strategies solely based ondata augmentation completely depend on the tokenized data, whichmay lose part of the lexical information when splitting into sub-words. In contrast, LEA exploits character-level similarity betweenwords, an information that is not dependent on the tokenization.Moreover, in we analyze the impact of adding LEA tocross-encoders in the absence of typos. Here, the vanilla cross-encoders trained without data augmentation perform best on av-erage. LEA, however clearly outperforms training with data aug-mentation and provides a competitive performance, achieving thebest performance in some datasets. We refer the reader to Sec-tions 4.6.1 and 4.6.2 for additional experiments with a larger archi-tecture (BERT-Large), autoregressive models (GPT-2 and GPT-Neo)and larger datasets (WDC-XLarge and WDC-All).The results presented in Tables 3 and 4, therefore provide apositive response to RQ1: LEA improves cross-encoders performanceto typos by a large margin, while achieving competitive performancein their absence. 4.2.1Performance on additional domains. Previous experimentsshowing the improvements of LEA were conducted in the e-commercedomain, i.e. short product descriptions with little context. In Ta-ble 5, we further demonstrate the benefits of LEA using BERT-Medium in RTE (textual entailment) and MRPC (paraphrasing)datasets that represent a completely different domain with longersentences. Again, typos dramatically reduce the performance of across-encoder trained without data augmentation. However, LEApalliates this drop and achieves best results in RTE with typos ( 6absolute points gain), while having comparable performance to avanilla cross-encoder trained with data augmentation in MRPC. Incontrast, in a clean setup LEA suffers small performance drops with",
  "Impact of the lexical similarity choice": "The lexical embeddings of LEA are computed with a similarity func-tion between two strings. In (Lexical similarity metric),we analyze the impact of the choice of this similarity metric inthe Abt-Buy dataset using BERT-Medium. We try LEA with thefollowing string similarity metrics: Jaccard (Jac.), Smith-Waterman(Smith), Longest Common Subsequence (LCS), Levenshtein (Lev.)and JaroWinkler (Jaro) . All the metrics improve the per-formance when evaluating with typos, thus supporting the posi-tive contribution of LEA regardless of the lexical similarity metricadopted. In clean scenarios, the Smith-Waterman similarity doesnot outperform the regular cross-encoder (top row), while the re-maining metrics does surpass it. Smith-Waterman is the metric thatis penalized the most by typos appearing in the middle of words,and by lexical variations, as it relies on aligning common substrings.",
  "We decided to adopt the Jaccard similarity for LEA given thatit consistently outperforms both the clean and the noisy scenariosfor short sentences. The Jaccard coefficient applied to characters is": "order agnostic and therefore more robust to character swaps. Ourintuition is that Jaccard provides higher separability between wordpairs with and without typos, which is beneficial in short-texts.However, as the word context increases in long sentence domains,the probability of comparing words with different meaning thatshare characters increases, thus reducing the swap invariance ad-vantage. We refer the reader to Appendix A for further details onthe design choices for the relative attention bias used in LEA.The evidences presented provide a positive answer to RQ2: it isimportant to choose the right metric for better performance, althoughall of them help in preventing performance drops against typos withrespect to the vanilla cross-encoder.",
  "Do we use the same lexical projection matrix for the entiremodel, one per layer, or one independent matrix per head?": "Do we apply LEA in all layers across the architecture, or itis more beneficial to apply it only in certain layers?In we present results to answer these questions. For thefirst decision (W parameter sharing) we show that using an in-dependent projection matrix per head behaves best and observe anincreasing performance tendency towards sharing less parameters,i.e. shared across all layers is the worst choice. We argue that thisbehaviour is reasonable given that using independent W matricesprovides higher flexibility to learn the projection, as the additionto the standard self-attention term in Eq. 5 might need differentbehaviour in different heads for better performance. We, therefore,use for the default LEA configuration this non-shared alternative.Regarding the second decision (Layers with LEA), we evaluateadding LEA to different layer subsets in BERT-Medium (8 layers intotal): all layers (), excluding the first two layers (), secondhalf of the layers () and the last two layers (). We observethat all the choices help dealing with typos, achieving the bestperformance by adding LEA to the second half of layers. Similarbehaviour is observed in clean scenarios, although only adding LEAto the last half and last two layers outperform the vanilla cross-encoder performance. Therefore, we use LEA in half of the deeperlayers in all architectures and experiments.We argue that the character-level similarity provided by LEA canbe considered as a high-level interaction information. Therefore,this complements the high-level features of deep Transformer layers.We left for future work to validate this hypothesis.The results obtained in this set of experiments address RQ3: it isbetter to use dedicated lexical projection matrices for each attentionhead and to add LEA on late layers for better performance.",
  "Impact of the noise strength": "We analyze in (top) the robustness of LEA to differentnoise strengths at test time. These results demonstrate a higherrobustness to typos than vanilla cross-encoder baselines trainedwith and without data augmentation. For this experiment, modelstrained simulating typos use a 20% probability of introducing themin a word, while at test this probability is modified to change thenoise strength. Intuitively, since the character-level similarities",
  "Additional experiments": "4.6.1Comparison with larger models. In order to assess the effec-tiveness of LEA in a larger model, we perform experiments usingBERT-Large. Additionally, we adopt auto-regressive architectures(GPT-2 and GPT-Neo) to compare them with the auto-encoder mod-els used across this work. In , we show that despite followingthe same training procedure, the gap between the vanilla cross-encoder and LEA using BERT-Large increases to 28 absolute points.In we show the effectiveness of LEA for the clean versionsof the datasets.For the GPT-like models, we followed the same approach asin and fine-tuned the backbones as cross-encoders using thelast token embedding for the sentence representation pooling (alsosuggested in ). We used the same hyper-parameters as theones used in the experiments of our paper, i.e. number of epochs,size of the classification head, etc, and the publicly available pre-trained weights in HuggingFace . As we observe in ,embeddings obtained by fine tuning GPT-like architectures in ourdownstream tasks still suffer from the textual noise issue, experi-encing drops in performance of 23 and 7 absolute points on averagefor GPT2-330M without and with DA, respectively. GPTNeo-125Malso shows an average drop in performance of 21 and 4 absolutepoints when trained without and with DA, respectively. Despitethese models being pre-trained in massive data and having moreparameters, we show that by just using BERT-Base equipped with",
  ": Results for BERT-Large in the clean test sets": "LEA we manage to outperform GPT-like architecture in the pres-ence of textual noise. Note that we leave the addition of LEA toGPT-like architectures for future work.These results suggest that larger models might reduce the gap tosome extent (as depicted in ), but they strongly suffer withtextual noise (as shown when comparing results between and ). Overall our approach mitigates the impact of noise,while keeping comparable performance using clean text. 4.6.2Comparison with larger datasets. We have conducted exper-iments considering WDC-Computers XLarge (68,461 data pointsin total for training) and WDC-All (214,661 samples for training)obtaining the results in .In all the experiments, we show that LEA consistently improvesthe baselines performance by a significant margin, confirming theeffectiveness of our proposal in larger datasets. It is worth men-tioning that the average results of BERT-M + DA for the 3 testsplits slightly improves LEA, although with a high standard devi-ation. Nevertheless, LEA clearly outperforms the baselines in theremaining scenarios.",
  "CONCLUSIONS": "This work proposes LEA, a LExical-aware relative Attention moduledesigned to improve the performance of cross-encoder architec-tures in sentence similarity tasks. LEA is particularly intended forscenarios with textual noise (e.g. typos) and short texts, where weshow that vanilla Transformers drop performance due to tokeniza-tion shifts between noisy and clean data. In particular, we proposeto modify the self-attention module by introducing a lexical bias.",
  ": Results in WDC-Computer XLarge and WDC-AllXLarge for three backbones: Electra-Small (Electra-S), BERT-Medium (BERT-M) and BERT-Base (BERT-B)": "This lexical information tackles the tokenization shift by providinga raw character-level similarity that tends to be high for lexicallyclose words, with and without typos. This similarity is independentof the tokenization and does not assume any prior knowledge onthe type of noise present in the data.The results of LEA on five e-commerce datasets using severalbackbones of varying size demonstrate consistent improvements when dealing with typos over cross-encoder baselines. We furtherverify the robustness of LEA against typos in textual entailment andparaphrasing tasks and observe competitive performance despitenot being strictly designed for these scenarios. Moreover, we pro-vide insights to better understand the behaviour of LEA and explorethe impact of: 1) different string similarity metrics, 2) introducingthe lexical bias at varying subsets of layers and 3) sharing parame-ters when encoding the lexical similarities. Finally, we investigatethe generalization to different noise strengths, demonstrating thatLEA performs and generalizes better than the vanilla cross-encoderbaselines.",
  "Limitations and future work": "Despite making no assumption about the type of noise, LEA as-sumes that lexical similarities between two sentences is a relevantbias for similarity matching. It is worth mentioning that in scenarioswithout typos there is a slight drop in performance (lower than 2absolute points on average) as reported in when adding thisbias. However, in the presence of typos LEA largely outperforms aVanilla cross-encoder (more than 30 absolute points on average),thus demonstrating that the proposed lexical bias helps in these sce-narios. LEA is designed for Transformer configurations where twoor more sentences are used as part of the input to the model. Whilelimited to this specific context, it encompasses a wide-ranging topicwithin the sentence similarity literature and LEA can be repurposedacross different but related domains.Future work will focus on improving the use of lexical informa-tion on longer texts and better using this bias in clean scenarios.Another interesting research direction is the extension of LEA tobi-encoders with late-interaction techniques.",
  "Y. Hao, L. Dong, F. Wei, and K. Xu. 2021. Self-Attention Attribution: InterpretingInformation Interactions Inside Transformer. In AAAI Conference on ArtificialIntelligence": "T. Hong, D. Kim, M. Ji, W. Hwang, D. Nam, and S. Park. 2022. Bros: A pre-trainedlanguage model focusing on text and layout for better key information extractionfrom documents. In AAAI Conference on Artificial Intelligence. S. Humeau, K. Shuster, M.-A. Lachaux, and J. Weston. 2020. Poly-encoders:Architectures and Pre-training Strategies for Fast and Accurate Multi-sentenceScoring. In International Conference on Learning Representations (ICLR). V. Karpukhin, O. Levy, J. Eisenstein, and Marjan Ghazvininejad. 2019. Training onSynthetic Noise Improves Robustness to Natural Noise in Machine Translation.Workshop on Noisy User-generated Text (W-NUT) (2019).",
  "O. Khattab, C. Potts, and M. Zaharia. 2021. Baleen: Robust multi-hop reasoningat scale via condensed retrieval. In Advances in Neural Information ProcessingSystems (NeurIPS)": "O. Khattab and M. Zaharia. 2020. Colbert: Efficient and effective passage search viacontextualized late interaction over bert. In International ACM SIGIR conferenceon research and development in Information Retrieval (SIGIR). P. Khosla, P. Teterwak, C. Wang, A. Sarna, Y. Tian, P. Isola, A. Maschinot, C. Liu,and D. Krishnan. 2021. Supervised Contrastive Learning. In Advances in NeuralInformation Processing Systems (NeurIPS).",
  "Y Li, J. Li, Y. Suhara, A. Doan, and W.-C. Tan. 2020. Deep Entity Matching withPre-Trained Language Models. In International Conference on Very Large DataBases (VLDB)": "T. Likhomanenko, Q. Xu, G. Synnaeve, Ronan Collobert, and Alex Rogozhnikov.2021. CAPE: Encoding relative positions with continuous augmented positionalembeddings. Advances in Neural Information Processing Systems (NeurIPS) (2021). F. Liu, Y. Jiao, J. Massiah, E. Yilmaz, and S. Havrylov. 2021. Trans-Encoder:Unsupervised sentence-pair modelling through self-and mutual-distillations. InInternational Conference on Learning Representations (ICLR). Y. Liu, W. Lu, S. Cheng, D. Shi, S. Wang, Z. Cheng, and D. Yin. 2021. Pre-TrainedLanguage Model for Web-Scale Retrieval in Baidu Search. In ACM SIGKDDConference on Knowledge Discovery & Data Mining (KDD).",
  "D. Pruthi, B. Dhingra, and Z.C. Lipton. 2019. Combating Adversarial Misspellingswith Robust Word Recognition. In Annual Meeting of the Association for Compu-tational Linguistics (ACL)": "Majid R.-M., S. Liu, Y. Wang, N. Afzal, L. Wang, F. Shen, S. Fu, and H. Liu.2018. BioCreative/OHNLP Challenge 2018. In ACM International Conference onBioinformatics, Computational Biology, and Health Informatics (BCB). C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, andP.J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-texttransformer. Journal of Machine Learning Research (JMLR) (2020). N. Reimers and I. Gurevych. 2019. Sentence-BERT: Sentence Embeddings usingSiamese BERT-Networks. In Conference on Empirical Methods in Natural LanguageProcessing and International Joint Conference on Natural Language Processing(EMNLP-IJCNLP).",
  "G. Rosa, L. Bonifacio, V. Jeronymo, H. Abonizio, M. Fadaee, R. Lotufo, andR. Nogueira. 2022.In Defense of Cross-Encoders for Zero-Shot Retrieval.arXiv:2212.06121 (2022)": "K. Santhanam, O. Khattab, J. Saad-Falcon, C. Potts, and M. Zaharia. 2022. Col-bertv2: Effective and efficient retrieval via lightweight late interaction. In Confer-ence of the North American Chapter of the Association for Computational Linguistics(NAACL). P. Shaw, J. Uszkoreit, and A. Vaswani. 2018. Self-Attention with Relative PositionRepresentations. In Conference of the North American Chapter of the Associationfor Computational Linguistics (NAACL): Human Language Technologies. G. Sidiropoulos and E. Kanoulas. 2022. Analysing the Robustness of Dual Encodersfor Dense Retrieval Against Misspellings. In International ACM SIGIR Conferenceon Research and Development in Information Retrieval (SIGIR).",
  "A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.N. Gomez, L. Kaiser,and I. Polosukhin. 2017. Attention is all you need. Advances in Neural InformationProcessing Systems (NeurIPS) (2017)": "A. Wang, Amanpreet Singh, Julian Michael, F. Hill, O. Levy, and S. Bowman. 2018.GLUE: A Multi-Task Benchmark and Analysis Platform for Natural LanguageUnderstanding. In International Conference on Learning Representations (ICLR). U. Wennberg and G.E. Henter. 2021. The Case for Translation-Invariant Self-Attention in Transformer-Based Language Models. In Annual Meeting of theAssociation for Computational Linguistics and the International Joint Conferenceon Natural Language Processing (ACL-IJCNLP). C. Wu, F. Wu, and Y. Huang. 2021. DA-Transformer: Distance-aware Transformer.In Conference of the North American Chapter of the Association for ComputationalLinguistics (NAACL): Human Language Technologies. J. Yang, A. Gupta, S. Upadhyay, L. He, R. Goel, and S. Paul. 2022. Tableformer:Robust transformer modeling for table-text encoding. In Annual Meeting of theAssociation for Computational Linguistics (ACL).",
  ": Differences in the configuration of the models usedin all experiments regardless of the model architecture. refers to the size of the pairwise lexical attention embedding": "is to increase the granularity of the lexical similarities by expand-ing the domain of the cosine and sine functions. This modificationyields representations that are better distributed in the space. Notethat we also considered an additional alternative to the sinusoidalembeddings by using learnable embeddings (Learned in ).Apart from showing lower performance, the learnable embeddingsadd extra parameters and require the discretization of the similar-ities to map them into embeddings, a step that could potentiallylead to information loss. As presented in , the use of sinusoidal functions to repre-sent lexical similarities provides improved robustness and flexibilitycompared to the use of raw lexical similarity values (e.g. jaccarddistance) as the bias in the self-attention module.",
  "BEXPERIMENTAL SETUP DETAILS": "In this section, we show additional details about the hyper-parametersused across all experiments and backbone architectures. Unless oth-erwise stated we used the following configuration in all experimentsand models: Batch size: 32. Learning rate scheduler: cosine annealing with warm-up. Initial learning rate: 55. Warm-up: 1.5 epochs. Number of epochs: 30. Weight decay: 55. Sentence representation: mean pooling of all token embed-dings excluding padding tokens. Classification head: Multi-Layer Perceptron with a Linearlayer of size 256 follow by LayerNorm, Dropout (0.1), GeLUactivation and an output Linear layer of size 2.In , we show the differences between the parametersused for the experimental setup of the models under comparison.Note that the Vanilla cross-encoder, regardless the backbone archi-tecture, does not use any data augmentation nor lexical attentionembedding. For both the cross-encoder with DA and with LEA weuse the same data augmentation described in ."
}