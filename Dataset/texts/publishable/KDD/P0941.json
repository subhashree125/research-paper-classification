{
  "ABSTRACT": "Cognitive diagnosis models (CDMs) are designed to learn studentsmastery levels using their response logs. CDMs play a fundamentalrole in online education systems since they significantly influencedownstream applications such as teachers guidance and comput-erized adaptive testing. Despite the success achieved by existingCDMs, we find that they suffer from a thorny issue that the learnedstudents mastery levels are too similar. This issue, which we refer toas oversmoothing, could diminish the CDMs effectiveness in down-stream tasks. CDMs comprise two core parts: learning studentsmastery levels and assessing mastery levels by fitting the responselogs. This paper contends that the oversmoothing issue arises fromthat existing CDMs seldom utilize response signals on exercisesin the learning part but only use them as labels in the assessingpart. To this end, this paper proposes an oversmoothing-resistantcognitive diagnosis framework (ORCDF) to enhance existing CDMsby utilizing response signals in the learning part. Specifically, OR-CDF introduces a novel response graph to inherently incorporateresponse signals as types of edges. Then, ORCDF designs a tailoredresponse-aware graph convolution network (RGC) that effectivelycaptures the crucial response signals within the response graph.Via ORCDF, existing CDMs are enhanced by replacing the inputembeddings with the outcome of RGC, allowing for the considera-tion of response signals on exercises in the learning part. Extensiveexperiments on real-world datasets show that ORCDF not only",
  "Aimin Zhou is the corresponding author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08...$15.00 helps existing CDMs alleviate the oversmoothing issue but alsosignificantly enhances the models prediction and interpretabilityperformance. Moreover, the effectiveness of ORCDF is validated inthe downstream task of computerized adaptive testing.",
  "Cognitive diagnosis, Oversmoothing, Representation, Student per-formance prediction, Online education systems": "ACM Reference Format:Hong Qian, Shuo Liu, Mingjia Li, Bingdong Li, Zhi Liu, and Aimin Zhou.2024. ORCDF: An Oversmoothing-Resistant Cognitive Diagnosis Frame-work for Student Learning in Online Education Systems. In Proceedings ofthe 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining(KDD 24), August 2529, 2024, Barcelona, Spain. ACM, New York, NY, USA,12 pages.",
  "INTRODUCTION": "Cognitive diagnosis (CD) serves as the foundational elementin online intelligent education systems. It exerts an upstream andfundamental influence on subsequent modules such as computeradaptive testing , course recommendation and learningpath suggestions , among others. Specifically, as illustratedin , CD aims to learn students underlying mastery levels(Mas) by analyzing their historical response logs, thereby providinginsights into various attributes of exercises, such as difficulty level(Diff) and discrimination (Disc). In recent years, an array of cogni-tive diagnosis models (CDMs) have emerged, prominently featuringframeworks such as item response theory (IRT) and the neuralcognitive diagnosis model (NCDM) . The two core parts of CDMinclude learning students Mas and assessing the learned Mas byfitting the response logs. The function used in the latter part is often",
  ": An example of CD, as well as relationships betweenCD and downstream tasks": "referred to as the interaction function (IF). IRT utilizes a latent factorto represent Mas and adopts the logistic function as IF. In contrast,NCDM replaces the traditional IFs with multi-layer perceptrons(MLP) and uses concept-specific vectors (i.e., set the embeddingdimension being equal to the number of concepts) to characterizeMas. As embedding-based methods rapidly evolve and gain promi-nence, there is an increasing trend of representing both studentsand exercises in a vectorized form, and they are gradually refinedby using a variety of advanced techniques .Despite the success, this paper, for the first time, identifiesthat existing CDMs share a potential and thorny issue thatthe learned Mas of students are too similar. We refer to thisissue as oversmoothing. Oversmoothing could diminish the CDMseffectiveness in down-stream tasks. To support the motivation ofthis paper and reveal the oversmoothing issue, we conduct a pilotstudy on four real-world datasets collected from the online edu-cation systems, ensuring a diverse range of circumstances in thestudents response logs. To characterize the degree of oversmooth-ing, inspired by , the mean normalized difference (MND) isproposed to measure the Mas learned by CDMs. Intuitively, thelarger the MND value, the bigger the difference among studentsMas that learned by CDMs. Details of MND are elaborated in Sec-tion 5.1. As shown in , although CDMs such as NCDM ,CDMFKC , KSCD and KaNCD achieve commendableprediction performance, the MND values of Mas they have learnedare quite small and hard to distinguish. Since CD is an upstreamtask, addressing this issue is urgent. For instance, if teachers relyon the outcomes of CD to assist student development, exceedinglysubtle distinctions could lead to confusion. Intuitively, if MND is0.005, it implies that the average difference in Mas for two studentsin a class on certain concepts is merely 0.005 (e.g., 0.51 and 0.515).Such a small margin could potentially bring difficulty to teachersto accurately assess the cognitive state of entire class. This not onlyfails to aid students but could also result in misguided instruction.Moreover, for downstream algorithms, a diagnosis result plagued byoversmoothing may lead to erroneous recommendations of learningmaterials, causing irreversible impacts on students.One straightforward approach is to design a regularization termaimed at amplifying the differences between students. However,achieving a balance between the weight of this regularization term NCDM CDMFKC KSCDKaNCD Assist17 EdNet-1",
  "Mean Normalized Difference": ": Results of motivation and pilot experiments: theoversmoothing issue in most existing CDMs is highlighted.The degree of oversmoothing is measured by the mean nor-malized difference (the lower the worse). OOM means theout-of-memory on an NVIDIA 3090 GPU. The vertical axisrepresents the names of four real-world datasets, and thehorizontal axis lists the representative existing CDMs. and the binary cross-entropy (BCE) loss during training is challeng-ing. Besides, although this direct approach may help in mitigatingthe oversmoothing issue, it could compromise the models predic-tion performance, since it forcefully amplifies the differences amongall students and adversely affects the learning of students Mas whoshould, in principle, be closely aligned. In this paper, we contendthat the oversmoothing issue arises because existing CDMs seldomutilize response signals in the learning part but only use them aslabels in the assessing part. For instance, students with right re-sponse on exercises with high difficulty levels should attain higherMas on corresponding concepts in the learning part. Cooperatingresponse signals in both learning and assessing parts of CDMs canwiden the gap among students Mas as they reserve the uniquefeature in students response logs.To this end, this paper proposes an oversmoothing-resistant cog-nitive diagnosis framework (ORCDF) to enhance existing CDMs byutilizing response signals in the learning part. Specifically, ORCDFintroduces a novel response graph, which utilizes response logsand a Q-matrix, inherently incorporating response signals as typesof edges. Then, ORCDF designs a tailored response-aware graphconvolution network (RGC) that effectively captures the crucialresponse signals within the response graph. We reveal that by uti-lizing the multiple layers of RGC, we achieve a multi-perspectiveanalysis of student mastery. This is accomplished by combining theoutcomes from multiple layers of RGC, leading to a more compre-hensive understanding of student learning. Via ORCDF, existingCDMs are enhanced by replacing the input embeddings with theoutcome of RGC through the transformation layer, allowing forthe consideration of response signals on exercises in the learningpart. Nevertheless, ORCDF encounters a new challenge: overem-phasizing the role of response signals can exacerbate the guess andslip problem. This problem occurs when students guess in orderto answer correctly or make mistakes on exercises they actuallymaster, and could potentially lead models to make unreasonableinference of students Mas. Different from previous methods thatintroduce extra parameters for guess and slip probabilities ,this paper addresses the guess and slip problem in student-exerciseinteractions by considering them as noise edges in the responsegraph. Specifically, we flip the student-exercise edge in the response",
  "ORCDF: An Oversmoothing-Resistant Cognitive Diagnosis Framework for Student Learning in Online Education SystemsKDD 24, August 2529, 2024, Barcelona, Spain": "The appendix is organized as follows: Appendix A analyzes the ORCDFs time complexity and com-pares it with other frameworks. Appendix B presents the detailed settings of compared base-lines and other details about student performance perdition. Appendix C presents the detailed settings of the downstreamtasks, namely, computerized adaptive testing. Appendix D further supplements the analysis with additionaldetails regarding the hyperparameter analysis.Notably, our code is available at",
  "RELATED WORK": "Cognitive Diagnosis Models. CDMs involve various approaches,such as latent factor models like IRT and MIRT (multidimensionalIRT), or concept mastery pattern models like the deterministicinput, noisy and gate (DINA) model, to infer students masterylevels. DINA, a classic CDM, employs binary variables to representmastery levels where 0 means unmastered and 1 means mastered.However, recent advances in deep learning have led to significantimprovements in handling large-scale interactions. Notably, NCDMuses MLP as its IF, treating mastery patterns as continuous vari-ables ranging from 0 to 1. This evolution in approach has beenparalleled by diverse methods in analyzing response logs, includingMLP based , graph attention networks and Bayesiannetworks , each contributing to a more nuanced understand-ing of student learning patterns. However, as depicted in ,these advanced CDMs encounter the oversmoothing issue whichcould potentially hinder the application of CD in downstream tasksof intelligent education, affecting their performance and conse-quently impacting student learning. To the best of our knowledge,the oversmoothing issue in the field of CD remains unexplored.Oversmoothing Issue. The oversmoothing issue is a signif-icant problem in graph representation learning (GRL). Many studieshave shown that the layers of graph neural network (GNN) deepen,the representations of graph nodes become increasingly smooth,leading to a substantial decrease in accuracy. This has prompted nu-merous researchers to employ a variety of methods to addressthis issue, enabling deeper GNN architectures. The same phenom-enon is also observed in various fields where graphs are used fordata mining. For example, in recommendation systems, graph col-laborative filtering (GCF) faces the oversmoothing problem,which arises for the same reasons as in GRL due to the stacking ofGNN layers. However, in the context of CD, oversmoothing is not aresult of stacking GNN layers, since most CDMs like NCDM, CDM-FKC, KSCD and KaNCD do not utilize GNN. Yet, this issue doesexist and is urgent, as shown in . Thus, existing solutionsto addressing oversmoothing in GRL and GCF are not suitable toresolve the oversmoothing issue in CD.",
  "PRELIMINARIES": "This section first introduces the fundamental elements of CD andthen introduces the formal problem definition of CD and over-smoothing issue in CDMs. We also give abbreviations for terms in at the beginning of the Appendix. Cognitive Diagnosis Basis. Consider an education scenariowhich contains three sets: = {1, . . . , }, = {1, . . . ,}, and = {1, . . . , }. They symbolize students, exercises and knowl-edge concepts, with respective sizes of , and . represents therelationship between exercises and knowledge concepts, which canbe regarded as a binary matrix Q = (Q) , where Q {0, 1}means whether relates to or not. Students from set , drivenby unique interests and requirements, select exercises from . Theresults are documented as response logs. Specifically, these logscan be illustrated as triplets = {(,,) | , , {0, 1}}. = 1 represents correct and = 0 represents wrong. In thispaper, we treat response logs as interaction matrix I R . Itcontains three categorical values (1 means right, 0 means no inter-action and 1 means wrong). Finally, we give the formal definitionof the CD task and oversmoothing issue in CDMs. Definition 3.1 (Problem Definition). Given interaction matrix I R , a binary matrix Q R , the goal of cognitive diagnosisis to infer Mas R , which denotes the latent mastery level ofstudents on each concept.",
  "METHODOLOGY: THE PROPOSED ORCDF": "This section introduces the proposed ORCDF. It starts by introduc-ing the proposed novel response graph, then explores the response-aware graph convolution (RGC), a technique designed to capturethe rich information embedded in the response graph. Followingthis, we introduce a consistency regularization loss function. Wealso discuss the model training and analyze model complexity. Anoverview of ORCDF is shown in .Response Graph. As illustrated in (a), focusing onresponses, the response graph (ResG), denoted as G = (V, E),comprises three types of nodes and edges. V = involvesstudents, exercises, and concepts, E involves interactions between and (i.e., Right), and (i.e., Wrong), and (i.e., Related).Notably, we incorporate the response signal on exercises as the edgetypes between students nodes and exercises nodes. Next, we willintroduce how to capture the fruitful response signal information.",
  "Response-aware Graph Convolution": "Construct Embeddings. In CD, the primary data elements areresponse logs and the Q. It is crucial to deconstruct these com-plex logs into their fundamental components: students, exercises,and concepts. We encode them with trainable embeddings H R , H R, H R . For instance, h R1 denotesthe row vector of the -th student. To facilitate subsequent convo-lution processes, we stack the aforementioned embeddings to formH(0) R( ++ ).Right-Wrong Decomposition. In the ResG, there are two typesof response signals existing between student nodes and exercisenodes, as shown in (a). To better explore the impact of dif-ferent response signals on learning Mas, we intuitively decompose",
  ": (a) The proposed response graph. (b) Right-wrongdecomposition": "the response graph into a right subgraph and a wrong subgraph.From the perspective of adjacency matrix, this involves splittingthe interaction matrix I into Iright (1 represents right, 0 representsothers) and Iwrong (1 represents wrong, 0 represents others). Forbrevity, in the following sections, we will denote R for right and Wfor wrong. Then we construct the right and wrong subgraphs (i.e,AR, AW as expressed by Eq. (1):",
  ".(1)": "In the ResG, the neighbors of a specific exercise node may includestudents who either answer the exercise right or wrong. However,after disentangling such response signals in the ResG, in each sub-graph, the neighbors of a particular exercise node will only consistof students who displayed the same response signals. For example,if both 1 and 2 are connected to 1, indicating that they both an-swer 1 correctly, there may be some shared information explainingwhy they both got it right. Such crucial response signals will bepropagated during the message-passing mechanism by GCN. Thisprocess enables a deeper understanding of the nuances in studentresponses. In the following part, we will introduce a novel graphconvolution approach tailored to capture the information from thetwo disentangled subgraphs in CD. Embedding Propagation. Considering that in CD, the featuresof students, exercises, and knowledge concepts are quite simple,consisting only of IDs, we draw inspiration from . As a result, weeliminate linear transformations and nonlinear activation functions,opting to use only the fundamental components of GCN. Hence, thegraph embedding propagation layer is designed with the followingmatrix form",
  ",(2)": "where A can be AR or AW. The degree matrix D is a diagonal matrixwith size (++)(++), where each entry D representingthe number of non-zero entries in the -th row vector of the matrixA. Using Eq. (2), we can obtain the convolution outcomes fromthe -th layer of the disentangled subgraphs, specifically H()Rand H()W . However, right and wrong represent completely oppositeresponse signals, it may be inappropriate to directly plus the resultsobtained from convolutions performed on the two subgraphs. Asophisticated function capable of aggregating these two types ofinformation is necessary, as the interaction mechanisms betweenstudents and exercises are quite intricate. It can be expressed as",
  "H()= (H()R Wrc + H()W Wwc) ,(3)": "where H()denotes the final representation of the -th RGC layerand denotes arbitrary nonlinear activate function. Wrc, Wwc R are trainable parameters. Intuitively, H()R Wrc denotes theright channel which obtain the semantic information from rightsignal. Conversely, H()W Wwc represents the opposite. The ultimateembedding H is calculated using a mean pooling operation on theoutcomes from each layer of the RGC which can be expressed as",
  "MND1,2 = Mas1 Mas2 22 = H(0)1 H(0)2 22 .(5)": "Clearly, the difference between the learned Mas of 1 and 2in NCDM reflects the disparity in their individual information.Here, we will use a one-layer RGC incorporated with NCDM as anexample. For brevity, we will omit all normalization coefficients,biases and retain only the key components. The MND of students1 and 2 after utilizing RGC is calculated as",
  "MND1,2 = Mas1 Mas2 22 = H1 H2 22 .(6)": "Via Eq. (2), we can derive that H(1)1 (R) = NR(1) H Wrcwhere NR(1) represents the -th exercise 1 practiced cor-rectly and is also the neighbor of 1 in the right subgraph. Con-sequently, the term H(1)1 (R) represents the normalized summa-tion exercises where 1 practiced correctly. Similarly, we can de-rive H(1)2 (R), H(1)1 (W) and H(1)2 (W) following the same logic. Fi-nally, Via Eq. (2) and Eq. (4), the H1 H2 22 can be calculated",
  "as 1": "2 (H(0)1 H(0)2 + H(1)1 (R) H(1)2 (R) + H(1)1 (W) H(1)2 (W)22).Consequently, we can have the following observations: The first term H(0)1 H(0)2is the same as Eq. (5) which reflectsthe disparity of individual information of 1 and 2. The second term H(1)1 (R) H(1)2 (R) + H(1)1 (W) H(1)2 (W)captures the difference in the exercises that students 1 and 2practiced correctly and incorrectly. The final MND1,2 of RA-NCDM is the mean of the first andsecond terms which can be interpreted as a comparison of thedifferences between students from the aforementioned perspectives.For instance, if both 1 and 2 have similar accuracy in theirexercises, the first term will be quite small due to the monotonicityassumption in CD. However, if the exercises attempted by 1 aremore challenging compared to those of 2, the second term willcapture this disparity and consequently increase the final differencebetween 1 and 2. This suggests that the RGC is capable of captur-ing the differences in the exercises practiced by students, resultingin more distinctive Mas for each student.Notably, as the number of RGC layers increases, the perspectivesfor considering student differences also multiply. For instance, atwo-layer RGC would further compare the differences with otherstudents who have similar exercise performance as the currentstudent. Therefore, by incorporating RGC, CDMs can assess stu-dent differences from multiple angles, thereby mitigating the over-smoothing issue. We will validate this conclusion in our ablationstudy in .3 and give visualizations of the learned Mas byT-SNE in .4.",
  "Consistency Regularization Loss": "After the graph convolution by multiple RGC layers, we can getthe final representation H via Eq. (4). However, as we disentanglethe response signal and capture student differences from variousperspectives, it may exacerbate the notorious impact of the guess and slip problem on CDMs . Previous methods, as referencedin , model the guess and slip probabilities for each exercise asfixed parameters. Evidently, this approach is somewhat brute-forceand might overlook the individual impact of students. This is be-cause the probability of guessing or slipping is likely to vary foreach person across different exercises. Contrary to the aforemen-tioned methods, in this paper, we treat guess and slip as noise edgeswithin the ResG. Specifically, we flip the student-exercise edge type(i.e., from R to W or W to R) with a probability in the ResG.This noised version of the ResG, where some edges are flipped,is referred to as the flipped ResG, as illustrated in the left part of. We aim for the representations derived from the originalResG and the flipped ResG to be similar, in order to ensure thatthe CDMs remain effective even when subject to the disturbancescaused by guess and slip problem. It can be formulated as",
  "= M (H, H, H) ,(8)": "where M () denotes the CDMs, and H represents the inputembedding that contains the representation of the student, exercisesand concepts.Transformation Layer. To facilitate the integration of ORCDFwith the majority of existing CDMs, we need to transform dimen-sions to suit the specific type of CDM in use. If the embeddingsize of CDMs is a latent dimension (e.g., KaNCD), we directly uti-lize H as the input embedding for incorporated CDMs. Otherwise(e.g., NCDM), we introduce a transformation layer which can beformulated asHt = HWt + bt ,(9) where Ht will be employed as input embedding for incorporatedCDMs and Wt R , bt R( ++ )1 are trainable parameters.As a result, unlike the previous RCD which sets = , we canchoose as a latent dimension (e.g., 64). This significantly reducesthe time complexity of graph convolution, a point that will befurther analyzed in the subsequent subsection.Joint Training. The primary loss employed in CD task is tocalculate the BCE loss between the models predictions and the trueresponse scores in a mini-batch. The aforementioned consistencyregularization loss is incorporated jointly optimized with the CDtask. The overall loss can be expressed as",
  "Model Complexity Analysis": "Theoretically, we reveal that the graph convolution in ORCDF takes(4|E|) time complexity. denotes the number of RGC layers,and denotes the dimension of embeddings. By leveraging thelightweight backbone and the transformation, our method is signif-icantly lower in time complexity compared with the recent GNN-based approach RCD . Specifically, RCD has the complexity of(2|E|2), where represents the number of concepts ( ).It suggests that ORCDF is more suitable for current online educa-tion scenario on ground of the increasing granularity of knowledgeconcepts. Indeed, ORCDF showcases a notable speed advantage,being up to 18 times faster than RCD on the Assist17 dataset (i.e., = 102). This dataset is collected from ASSISTment online tutor-ing systems and extensively utilized CDMs . This improvementcomes along with enhanced performance and lower GPU memoryusage. For detailed information, please refer to Appendix A.",
  "EXPERIMENTS": "In this section, we first describe four real-world datasets and eval-uation metrics. Then, through extensive experiments, we aim toverify the superiority of ORCDF, which not only assists existingCDMs in mitigating the oversmoothing issue but also enhances themodels prediction performance and interpretability performance.To ensure the reliability and reproducibility of our experiments,they are independently repeated ten times with different seeds andour code is available at",
  "Experimental Settings": "Datasets Description. The experiments are conducted using fourreal-world datasets: Assist17, EdNet-1, Junyi, and XES3G5M. TheAssist17 dataset is provided by the ASSISTment web-based onlinetutoring systems and are widely used for cognitive diagnosistasks . EdNet-1 is the dataset of all student-system interac-tion collected over 2 years by Santa, a multi-platform AI web-basedtutoring service with more than 780K users in Korea. Junyi is an online math practice log dataset offered by Junyi Academy.XES3G5M is a knowledge tracing benchmark dataset withauxiliary information. For more detailed statistics on these fourdatasets, please cf. . Notably, Sparsity refers to the sparsityof the dataset, which is calculated as | |/(||||). Average CorrectRate represents the average score of students on exercises, and QDensity indicates the average number of concepts per exercise.Evaluation Metrics. To assess the efficacy of ORCDF, we utilizeboth score prediction, interpretability and oversmoothing metrics.",
  "DatasetsAssist17EdNet-1JunyiXES3G5M": "#Students17091776100004000#Exercises3162119257347191#Knowledge Concepts102189734832#Response Logs390,311616,193408,0571,174,514Sparsity0.0720.0290.0550.04Average Correct Rate0.8150.6620.6870.799Q Density1.222.251.01.16 Score Prediction Metrics: Assessing the effectiveness ofCDMs poses difficulties owing to the absence of the true Mas. Acommon approach to address this challenge is to learn the Maswithin the train data and then evaluate the models based on theirlearned Mas to predict students performance on exercises in thetest data. In line with prior CDM studies, we partition the responselogs of students into train, valid and test data with 7:1:2 followingthe previous researches and assess CDMs performance on thetest data using classification metrics such as Area Under the ROCCurve (AUC), Accuracy (ACC). Crucially, we build the ResG solelybased on the train data. Interpretability Metric: Diagnostic results are highly in-terpretable hold significant importance in CD. In this regard, weemploy the degree of agreement (DOA), which is consistent withthe approach used in . The underlying intuition hereis that, if has a greater accuracy in answering exercises relatedto than student , then the probability of getting shouldbe greater than that of . Namely, Mas, > Mas, . Detailsabout DOA can be found in Appendix B. Consistent with , wecompute the average DOA for the top 10 concepts with the highestnumber of response logs in Assist17, EdNet-1, Junyi and XES3G5M. Oversmoothing Metric: We employ the proposed MND tomeasure the Mas learned by CDMs. In CD, since the Mas of studentslearned by CDMs with concept mastery pattern lies within therange of 0 to 1, we utilize the 2 norm of the difference between twostudents mastery level vectors to describe the disparity betweenthem. It can be formulated as follows:",
  "||,(12)": "where , represent the set of students and knowledge concepts,respectively, and Mas stands for the learned Mas of student by CDMs. A larger MND value indicates greater difference in theMas that learned by CDMs, implying that the oversmoothing issueis more adequately addressed.Implementation Details. For parameter initialization, we em-ploy the Xavier , and for optimization purposes, Adam isadopted. For fair comparison, the embedding size is uniformly set to32 for MIRT, KaNCD, and KSCD, and to for NCDM and CDMFKC.The batch size is set as 4096 for all datasets. To regulate the impact ofthe regularization term, we adjust the flip ratio within the range{0.05, 0.15, 0.1, 0.2}, reg within the range {104, 103, . . . , 101}, within the range {0.1, 0.5, 1.0, 3.0, 5.0}. Analysis regarding theaforementioned hyperparameters can be found in .6 andAppendix C.",
  "Student Performance Prediction": "To showcase the effectiveness of ORCDF, we integrate it with vari-ous CDMs, as described in the subsequent part. IRT is a classic model of latent factor CDMs, which usesone dimension to model Mas and utilize logistic function as IF topredict the student score performance. MIRT is a representative model of latent factor CDM,which uses multidimensional to model Mas. NCDM is the first recent deep-learning based CDM whichutilizes MLP to replace the traditional manually designed IFs.",
  "Ablation Study": "In this subsection, we scrutinize and evaluate each key individualcomponent of ORCDF to comprehend their respective impacts andsignificance on the overall performance of the model. The ablationanalysis is conducted using the following three versions. OR-w/o-rgc: This ablation of ORCDF does not integrate theresponse-aware graph convolution. Instead, it directly performconvolution on the entire response graph without decomposition. OR-w/o-reg: This ablation of ORCDF does not utilize the pro-posed consistency regularization loss Lreg. OL: It represents the base CDMs, which can be considered asthe one without the inclusion of response-aware graph convolutionand consistency regularization loss.Due to space constraints, we only present the ablation studyusing OR-NCDM as an example. This choice is motivated by thefact that NCDM is often employed as a classic CDM in downstreamtasks. It is worth noting that the results from incorporating otherCDMs are generally similar.Experimental Results. As indicated in , the proposedmethod outperforms the other two versions, suggesting that eachcomponent plays a significant role in enhancing the models overalleffectiveness. OR-w/o-rgc performs significantly worse, furthervalidating the superiority of the proposed RGC in capturing theinformation within the response graph. We empirically find thatalthough the consistency regularization loss is designed to alleviatethe guess and slip problem, it not only improves the prediction andinterpretability performance but also achieves a higher MND thanthe original version. This indicates that the guess and slip problemindeed exists in real-world scenarios, and addressing this problemis crucial for the effectiveness of CDMs.",
  "In-Depth Analysis of ORCDFs Advantages": "In this subsection, we analyze the proposed ORCDF from two per-spectives: generalization performance and robustness performance.Generalization Performance. To assess the efficacy of ORCDFin addressing the generalization issue, we conduct experiments onthree datasets with varying test ratios = {10%, 20%, 30%, 40%, 50%}.As increases which is consistent with , the generalization abil-ity of CDMs is tested more stringently. As depicted in ofAppendix B, with an increasing test ratio , the number of responselogs used for training decreases. However, OR-NCDM consistentlyoutperforms NCDM, illustrating that ORCDF can provide more ac-curate diagnosis results with fewer student response records. This",
  "KDD 24, August 2529, 2024, Barcelona, SpainHong Qian, Shuo Liu, Mingjia Li, Bingdong Li, Zhi Liu, and Aimin Zhou": "Number of RGC Layers 89.8 90.0 AUC (%) 6.4 7.2 8.0 MND (%) Number of RGC Layers 74.8 74.8 74.9 AUC (%) 4.2 4.5 4.8 MND (%) Number of RGC Layers 80.5 81.0 81.5 AUC (%) 6.6 7.2 7.8 MND (%) Number of RGC Layers 80.2 80.2 80.3 AUC (%) 15.0 20.0 25.0 MND (%)",
  ": Performance under different on four datasets": "KSCD also explores the implicit association among knowl-edge concepts and leverages a knowledge-enhanced interactionfunction. Due to the absence of open-source code online, we haveindependently replicated KSCD. LightGCN is a recent classic model that employs GCN inCF. In our context, we straightforwardly consider users as studentsand items as exercises. We set dimension as 32, the number of GCNlayers as 3 which is the same as OR-NCDM for a fair comparison. HierCDF is also a cognitive diagnosis framework thatemploys a Bayesian network, requiring a directed acyclic graph(DAG) to delineate the dependencies between knowledge concepts.It enables cognitive diagnosis models to learn mastery levels thatadhere to the DAG structure, better aligning with the assumptionof relationships between knowledge concepts in educational theory.We use the hyperparameters recommended in the original paper.The implementation of DINA, MIRT, NCDM and KANCD comesfrom the public repository RCD, we adopt the implementation from the authors in For LightGCN, we also use thecode from the authors For HierCDF, we also use the code from the authors",
  "Validation on the Downstream Task": "As an upstream task in the field of intelligent education, CD is ap-plied in various downstream tasks. To validate the effectiveness ofORCDF, we chose to test it in the context of computerized adaptivetesting (CAT) . Specifically, we integrate the commonly em-ployed IRT and NCDM with our ORCDF, denoting these as OR-IRTand OR-NCDM, respectively. Our experimental settings align withrecent research , which adopts a 7:2:1 split for students in theresponse logs of each dataset. Details can be found in Appendix C.As illustrated in , OR-NCDM performs better than OR-IRT,which validates the superiority of deep learning-based methodsin CAT which is consistent with . OR-IRT and OR-NCDMsignificantly outperform their original versions. This validates theeffectiveness of ORCDF in downstream tasks.",
  "Hyperparameter Analysis": "Effect of . As shown in in Appendix D, a larger de-creases the models training speed, while a smaller results inpoor performance. The recommended values of are 3 or 4, whichcan yield relatively good performance. Notably, as increases, theMND does not continually decrease, a phenomenon that seems dif-ferent from what is observed in graph representation learning. Wecontend this could be related to the heterogeneity of the responsegraph and the complexity of student interactions, which we leavefor future work.The Effect of . As depicted in in Appendix D, OR-NCDM is influenced by the flip ratio parameter. A too high flipratio introduces more noise, deteriorating the models performance.Typically, a =0.15 yields better prediction performance, aligningwith the established fact that everyone has a probability of guessingcorrectly or slipping, neither too high nor too low.The Effect of reg. As illustrated in in Appendix D,this parameter controls the impact of guess and slip on modeltraining, which varies across different datasets and requires tuning.It is observable that as the number of response logs in the datasetgradually increases, the optimal parameter value decreases. Werecommend setting it to 13.The Effect of . As illustrated in in Appendix D, thetemperature parameter affects the similarity between representa-tions learned from the response graph and those from the flippedresponse graph. As the size of the dataset gradually increases, thebetter temperature value also gradually increases. Here, we rec-ommend choosing 0.5 when the number of students is small andopting for 3.0 when there is a larger student population.",
  "CONCLUSION": "This paper proposes an oversmoothing-resistant cognitive diag-nosis framework (ORCDF), where most existing CDMs can be in-tegrated and thus enhanced. We, for the first time, identify theoversmoothing in CD and then address it by learning students Masfrom multiple perspectives, utilizing the proposed response graphand response-aware graph convolution network. Besides, we refor-mulate the guess and slip problem as noise edges in the responsegraph and deign a loss function to alleviate the problem. As longas the oversmoothing is addressed in CD, it greatly helps providedistinctive and personalized diagnostic results for students andteachers. However, ORCDF, while effective, is still not sufficientlyinterpretable enough in the field of intelligent education. Moreinterpretable methods are expected to be developed to mitigate theoversmoothing issue explicably in cognitive diagnosis. We would like to thank the anonymous reviewers for their construc-tive comments. We also would like to thank Xinyue Ma for the reli-able help. The algorithms and datasets in the paper do not involveany ethical issue. This work is supported by the National NaturalScience Foundation of China (No. 62106076), National Social Sci-ence Fund of China (No. BEA230071), and Science and TechnologyCommission of Shanghai Municipality Grant (No. 22511105901).",
  "Jimmy De La Torre. 2009. DINA model and parameter estimation: A didactic.Journal of Educational and Behavioral Statistics 34, 1 (2009), 115130": "Mingyu Feng, Neil T. Heffernan, and Kenneth R. Koedinger. 2009. Addressingthe assessment challenge with an online system that tutors as it assesses. UserModeling and User-adapted Interaction 19, 3 (2009), 243266. Weibo Gao, Qi Liu, Zhenya Huang, Yu Yin, Haoyang Bi, Mu-Chun Wang, JianhuiMa, Shijin Wang, and Yu Su. 2021. RCD: Relation map driven cognitive diagnosisfor intelligent education systems. In Proceedings of the 44th International ACMSIGIR Conference on Research and Development in Information Retrieval. VirtualEvent, 501510. Xavier Glorot and Yoshua Bengio. 2010. Understanding the Difficulty of TrainingDeep Feedforward Neural Networks. In Proceedings of the 13th InternationalConference on Artificial Intelligence and Statistics. Sardinia, Italy, 249256.",
  "Shelby J Haberman. 2005. Identifiability of parameters in item response modelswith unconstrained ability distributions. ETS Research Report Series 2005, 2 (2005),i22": "Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yong-Dong Zhang, and MengWang. 2020. LightGCN: Simplifying and Powering Graph Convolution Net-work for Recommendation. In Proceedings of the 43rd International ACM SIGIRconference on research and development in Information Retrieval. Virtual Event,639648. Lu Jiang, Kunpeng Liu, Yibin Wang, Dongjie Wang, Pengyang Wang, Yanjie Fu,and Minghao Yin. 2023. Reinforced Explainable Knowledge Concept Recommen-dation in MOOCs. ACM Transactions on Intelligent Systems and Technology 14, 3(2023), 43:143:20.",
  "Mingjia Li, Hong Qian, Jinglan Lv, Mengliang He, Wei Zhang, and Aimin Zhou.2024. Foundation Model Enhanced Derivative-Free Cognitive Diagnosis. Frontiersof Computer Science (2024)": "Qimai Li, Zhichao Han, and Xiao-Ming Wu. 2018. Deeper Insights Into GraphConvolutional Networks for Semi-Supervised Learning. In Proceedings of the 32rdAAAI Conference on Artificial Intelligence. New Orleans, LA, 35383545. Sheng Li, Quanlong Guan, Liangda Fang, Fang Xiao, Zhenyu He, Yizhou He,and Weiqi Luo. 2022. Cognitive Diagnosis Focusing on Knowledge Concepts. InProceedings of the 31st ACM International Conference on Information & KnowledgeManagement. Atlanta, GA, 32723281. Qi Liu, Runze Wu, Enhong Chen, Guandong Xu, Yu Su, Zhigang Chen, and Guop-ing Hu. 2018. Fuzzy Cognitive Diagnosis for Modelling Examinee Performance.ACM Transactions on Intelligent Systems and Technology 9, 4 (2018), 126. Shuo Liu, Hong Qian, Mingjia Li, and Aimin Zhou. 2023.QCCDM: A Q-Augmented Causal Cognitive Diagnosis Model for Student Learning. In Pro-ceedings of the 26th European Conference on Artificial Intelligence. Krakw, Poland,15361543. Shuo Liu, Junhao Shen, Hong Qian, and Aimin Zhou. 2024. Inductive CognitiveDiagnosis for Fast Student Learning in Web-Based Intelligent Education Systems.In Proceedings of the ACM on Web Conference 2024. Singapore, 42604271.",
  "Yingjie Liu, Tiancheng Zhang, Xuecen Wang, Ge Yu, and Tao Li. 2023. Newdevelopment of cognitive diagnosis models. Frontiers of Computer Science 17, 1(2023), 171604": "Zitao Liu, Qiongqiong Liu, Teng Guo, Jiahao Chen, Shuyan Huang, XiangyuZhao, Jiliang Tang, Weiqi Luo, and Jian Weng. 2023. XES3G5M: A KnowledgeTracing Benchmark Dataset with Auxiliary Information. In Advances in NeuralInformation Processing Systems 37. New Orleans, LA. Haiping Ma, Manwei Li, Le Wu, Haifeng Zhang, Yunbo Cao, Xingyi Zhang,and Xuemin Zhao. 2022. Knowledge-Sensed Cognitive Diagnosis for IntelligentEducation Platforms. In Proceedings of the 31st ACM International Conference on",
  "Yimeng Min, Frederik Wenkel, and Guy Wolf. 2020. Scattering GCN: Overcom-ing Oversmoothness in Graph Convolutional Networks. In Advances in NeuralInformation Processing Systems 33. Virtual Event": "Pengyang Shao, Chen Gao, Lei Chen, Yonghui Yang, Kun Zhang, and Meng Wang.2024. Improving Cognitive Diagnosis Models with Adaptive Relational GraphNeural Networks. arXiv preprint arXiv:2403.05559 (2024). Junhao Shen, Hong Qian, Wei Zhang, and Aimin Zhou. 2024. Symbolic CognitiveDiagnosis via Hybrid Optimization for Intelligent Education Systems. In Proceed-ings of the 38th AAAI Conference on Artificial Intelligence. Vancouver, Canada,1492814936. Shuanghong Shen, Qi Liu, Zhenya Huang, Yonghe Zheng, Minghao Yin, MinjuanWang, and Enhong Chen. 2024. A Survey of Knowledge Tracing: Models, Variants,and Applications. IEEE Transactions on Learning Technologies (2024). Jianwen Sun, Fenghua Yu, Sannyuya Liu, Yawei Luo, Ruxia Liang, and XiaoxuanShen. 2023. Adversarial Bootstrapped Question Representation Learning forKnowledge Tracing. In Proceedings of the 31st ACM International Conference onMultimedia. Ottawa, Canada, 80168025.",
  "Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.Journal of Machine Learning Research 9, 11 (2008)": "Fei Wang, Qi Liu, Enhong Chen, Zhenya Huang, Yuying Chen, Yu Yin, Zai Huang,and Shijin Wang. 2020. Neural Cognitive Diagnosis for Intelligent EducationSystems. In Proceedings of the 34th AAAI Conference on Artificial Intelligence. NewYork, NY. Fei Wang, Qi Liu, Enhong Chen, Zhenya Huang, Yu Yin, Shijin Wang, and YuSu. 2023. NeuralCD: A General Framework for Cognitive Diagnosis. IEEETransactions on Knowledge and Data Engineering 35, 8 (2023). Xiao Wang, Houye Ji, Chuan Shi, Bai Wang, Yanfang Ye, Peng Cui, and Philip S.Yu. 2019. Heterogeneous Graph Attention Network. In Proceedings of the 28thWorld Wide Web Conference. San Francisco, CA, 20222032. Ting Wu, Hong Qian, Ziqi Liu, Jun Zhou, and Aimin Zhou. 2023. Bi-objective evo-lutionary Bayesian network structure learning via skeleton constraint. Frontiersof Computer Science 17, 6 (2023), 176350. Lianghao Xia, Chao Huang, Yong Xu, Jiashu Zhao, Dawei Yin, and Jimmy X.Huang. 2022. Hypergraph Contrastive Collaborative Filtering. In Proceedingsof the 45th International ACM SIGIR Conference on Research and Development inInformation Retrieval. Madrid, Spain, 7079. Wei Xu and Yuhan Zhou. 2020. Course video recommendation with multimodalinformation in online learning platforms: A deep learning framework. BritishJournal of Educational Technology 51, 5 (2020), 17341747. Shangshang Yang, Haoyu Wei, Haiping Ma, Ye Tian, Xingyi Zhang, Yunbo Cao,and Yaochu Jin. 2023. Cognitive Diagnosis-Based Personalized Exercise GroupAssembly via a Multi-Objective Evolutionary Algorithm. IEEE Transactions onEmerging Topics in Computational Intelligence 7, 3 (2023), 829844. Shangshang Yang, Xiaoshan Yu, Ye Tian, Xueming Yan, Haiping Ma, and XingyiZhang. 2023. Evolutionary Neural Architecture Search for Transformer in Knowl-edge Tracing. In Advances in Neural Information Processing Systems 36. Louisiana,NO. An Zhang, Wenchang Ma, Xiang Wang, and Tat-Seng Chua. 2022. IncorporatingBias-aware Margins into Contrastive Loss for Collaborative Filtering. In Advancesin Neural Information Processing Systems 35. New Orleans, LA. An Zhang, Leheng Sheng, Zhibo Cai, Xiang Wang, and Tat-Seng Chua. 2023.Empowering Collaborative Filtering with Principled Adversarial ContrastiveLoss. In Advances in Neural Information Processing Systems 36. Louisiana, NO. An Zhang, Jingnan Zheng, Xiang Wang, Yancheng Yuan, and Tat-Seng Chua. 2023.Invariant Collaborative Filtering to Popularity Distribution Shift. In Proceedingsof the ACM Web Conference 2023. ACM, Austin,TX, 12401251. Chuxu Zhang, Dongjin Song, Chao Huang, Ananthram Swami, and Nitesh V.Chawla. 2019. Heterogeneous Graph Neural Network. In Proceedings of the 25thACM SIGKDD International Conference on Knowledge Discovery & Data Mining.Anchorage, AK, 793803. Yan Zhuang, Qi Liu, Zhenya Huang, Zhi Li, Shuanghong Shen, and HaipingMa. 2022. Fully Adaptive Framework: Neural Computerized Adaptive Testingfor Online Education. In Proceeddings of the 36th AAAI Conference on ArtificialIntelligence. Virtual Event, 47344742. Yan Zhuang, Qi Liu, GuanHao Zhao, Zhenya Huang, Weizhe Huang, ZacharyPardos, Enhong Chen, Jinze Wu, and Xin Li. 2023. A Bounded Ability Estimationfor Computerized Adaptive Testing. In Advances in Neural Information ProcessingSystems 37. New Orleans, LA.",
  "ATIME COMPLEXITY ANALYSIS": "In this section, we present a detailed time complexity analysis ofour proposed model OR-NCDM. We compare our time complexitywith that of RCD, as RCD is the only CDM based on GNN.Time Complexity Analysis of ORCDF. We take OR-NCDMas an exmaple. In OR-NCDM, we construct a response graph (ResG)G with three node and edge types based on I and Q. Given that wedo not employ the non-linear activation and feature transforma-tion usually found in GNNs, the time complexity can be straight-forwardly computed as (2|E|) for RGC, where denotes thenumber of RGC layers. stands for the size of the embeddings.Due to the need for computing representations through the flippedResG, the total time complexity amounts to (4|E|).Time Complexity Analysis of RCD. In RCD, it construct threerelation maps. Namely, an exercise-concept graph is constructedusing Q and a student-exercise graph is formed using I. Given thatRCD employs the graph attention network, which necessitates thecomputation of attention coefficients between every pair of con-nected nodes, its time complexity belongs to (2|E|2). Herein, represents the number of concepts ( ).OR-NCDM evidently takes less time compared to RCD due totwo main reasons. Firstly, due to the transformation layer reducesthe embedding dimension to , where is much smaller than .Secondly, by removing complex operations like linear transforma-tions in GNN, the graph convolution of RGCs computation becomemuch faster than the GAT used in RCD.In the experiment, we incorporate NCDM into all frameworksand use the speed of NCDM as the baseline, set at 1.0x. As shownin the figure, our proposed ORCDF is 18 times faster than RCDand offers better prediction performance. When the number ofknowledge concepts continuously increases, RCD tends to train tooslowly and runs into out-of-memory issues, especially with largesets of knowledge concepts. In contrast, ORCDF maintains goodperformance, as demonstrated in scenarios like XES3G5M with 832",
  ",": "(13)where = , (Mas,, Mas, ), Q, indicates exercises relevance to concept , (,,) checks if both students and answered , , represents the response of to , and (, ,) verifies if their responses are different, (,,,)is 1 for a right response by and a wrong response by , and 0otherwise.Implementation Details. This section delineates the detailedsettings when comparing our method with the baselines and state-of-the-art methods in both transductive scenario and inductive sce-nario. All experiments are run on a Linux server with two 3.00GHzIntel Xeon Gold 6354 CPUs and one RTX3090 GPU. All the modelsare implemented by PyTorch. For all methods that involve usingMLP as the interaction function, we adopt the commonly usedtwo-layer tower structure with hidden dimensions of 512 and 256.Additionally, we employ the approach used in NCDM to ensurethat it satisfies the monotonicity assumption.In the following, we elaborate on some details regarding theutilization of compared methods. DINA is a representative CDM which models the masterypattern with discrete variables (0 or 1). MIRT is a representative model of latent factor CDMs,which uses multidimensional to model the latent abilities. We setthe latent dimension as 16 which is the same as NCDM is a deep learning based CDM which uses MLPs toreplace the traditional interaction function (i.e., logistic function).We adopt the default parameters which are reported in that paper. RCD leverages GNN to explore the relations among stu-dents, exercises and knowledge concepts. Here, to ensure a faircomparison, we solely utilize the student-exercise-concept compo-nent of RCD, excluding the dependency on knowledge concepts. KANCD improves NCDM by exploring the implicit as-sociation among knowledge concepts to address the problem ofknowledge coverage. Here, we adopt the default parameters whichare reported in that paper.",
  ": Effect of on four datasets": "0.06 0.08 0.10 0.12 0.14 0.16 0.18 0.20 pf 89.98 90.04 90.10 90.16 AUC (%) 0.06 0.08 0.10 0.12 0.14 0.16 0.18 0.20 pf 74.82 74.84 74.86 74.88 AUC (%) 0.06 0.08 0.10 0.12 0.14 0.16 0.18 0.20 pf 81.34 81.37 81.40 81.43 AUC (%) 0.06 0.08 0.10 0.12 0.14 0.16 0.18 0.20 pf 80.24 80.28 80.32 80.36 AUC (%)",
  "CDETAILS ABOUT COMPUTERIZEDADAPTIVE TESTING": "Computerized adaptive testing (CAT) primarily comprises CDMand item selection strategies. Its aim is to accurately determinestudents mastery levels (Mas) with as few exercises as possible. Thecore of CAT often lies in designing a more effective item selectionstrategy . They often opt for simple and classic CDMs likeIRT or NCDM. However, in reality, these diagnostic models sufferfrom the oversmoothing issue and tend to underperform.In this study, we employ a classic dataset, known as Math2,which consists of 3911 students, 16 exercises, and 16 concepts. Thisdataset has been widely used in various researches as referencedin studies such as . The objective of CAT is to accuratelyestimate a students Mas using the fewest possible steps (i.e., thesmallest number of exercises). However, as the true Mas cannotbe obtained as ground truth, we, like previous methods, use thestudent performance prediction task to validate the learned Mas.For more detailed information, we recommend the readers referto . Here, we utilize three commonly selected strategies whichcan be applied on both IRT and NCDM. These strategies can beformulated as follows. Random is a simply strategy which select exercises randomlyfor each student in CAT. MAAT utilizes the proposed expected model change toselect exercises that are likely to have a significant impact on thestudents Mas. BECAT employs the concept of Coreset and utilizes ex-pected gradient difference approximation to select exercises."
}