{
  "ABSTRACT": "Auto-bidding plays a crucial role in facilitating online advertis-ing by automatically providing bids for advertisers. Reinforcementlearning (RL) has gained popularity for auto-bidding. However,most current RL auto-bidding methods are modeled through theMarkovian Decision Process (MDP), which assumes the Markovianstate transition. This assumption restricts the ability to perform inlong horizon scenarios and makes the model unstable when dealingwith highly random online advertising environments. To tackle thisissue, this paper introduces AI-Generated Bidding (AIGB), a novelparadigm for auto-bidding through generative modeling. In thisparadigm, we propose DiffBid, a conditional diffusion modelingapproach for bid generation. DiffBid directly models the correlationbetween the return and the entire trajectory, effectively avoidingerror propagation across time steps in long horizons. Addition-ally, DiffBid offers a versatile approach for generating trajectoriesthat maximize given targets while adhering to specific constraints.Extensive experiments conducted on the real-world dataset andonline A/B test on Alibaba advertising platform demonstrate theeffectiveness of DiffBid, achieving 2.81% increase in GMV and 3.36%increase in ROI.",
  "Work is done during the internship at Alibaba Group.Bo Zheng is the corresponding author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08...$15.00",
  "Online Advertising, Auto-bidding, Generative Learning, DiffusionModeling": "ACM Reference Format:Jiayan Guo, Yusen Huo, Zhilin Zhang, Tianyu Wang, Chuan Yu, Jian Xu, YanZhang, and Bo Zheng. 2024. AIGB: Generative Auto-bidding via DiffusionModeling. In Proceedings of the 30th ACM SIGKDD Conference on KnowledgeDiscovery and Data Mining (KDD 24), August 2529, 2024, Barcelona, Spain.ACM, New York, NY, USA, 12 pages.",
  "INTRODUCTION": "The ever-increasing digitalization of commerce has exponentiallyexpanded the scope and importance of online advertising plat-forms . These ad platforms have become indispensable forbusinesses to effectively target their audience and drive sales. Tra-ditionally, advertisers need to manually adjust bid prices to opti-mize overall ad performance. However, this coarse bidding processbecomes impractical when dealing with trillions of impression op-portunities, requiring extensive domain knowledge and com-prehensive information about the advertising environments.To alleviate the burden of bid optimization for advertisers, thesead platforms provide auto-bidding services . Theseservices automate the determination of bids for each impressionopportunity by employing well-designed bidding strategies. Suchstrategies consider a variety of factors about advertising environ-ments and advertisers, such as the distribution of impression oppor-tunities, budgets, and average cost constraints . Consideringthe dynamic nature of advertising environments, it is essential toregularly optimize the bidding strategy, typically at intervals of afew minutes, in response to changing conditions. With advertisingepisodes typically extending beyond 24 hours, auto-bidding can beseen as a sequential decision-making process with a long planninghorizon where the bidding strategy seeks to optimize performancethroughout the entire episode.Recently, reinforcement learning (RL) techniques have been em-ployed to optimize auto-bidding strategies through the training of",
  ": Correlation Coefficients between History and theNext State": "agents with bidding logs collected from online advertising envi-ronments . By leveraging historical realisticbidding information, these agents can learn patterns and trendsto make informed bidding decisions. However, most existing RLauto-bidding methods are based on the Markovian decision pro-cess (MDP), where the next state only depends on the current stateand action. In the online auto-bidding environment, this assumptionmay been challenged by our statistical analysis presented in Fig-ure 1, which shows a significant increase in the correlation betweenthe sequence lengths of history states and the next bidding state.This result indicates that solving auto-bidding considering only thelast state will encounter several problems, including instability inthe highly random online advertising environment. Additionally,the RL methods that rely on the Bellman equation often result incompound errors . This issue is especially pronounced in theauto-bidding problem characterized by sparse return and limiteddata coverage. A detailed statistical analysis is provided in A.5. Inthis paper, instead of employing RL-based methods, we present anovel paradigm, AI Generated Bidding (AIGB), that regards auto-bidding as a generative sequential decision-making problem. AIGBdirectly capture the correlation between the return and the entirebidding trajectory that consists of a sequence of states or actions,thereby transforming the problem into learning to generate anoptimal bidding trajectory. This approach enables us to overcomethe limitations of RL when dealing with the highly random onlineadvertising environment, sparse returns, and limited data coverage.In the new paradigm, we propose Diff usion auto-bidding modelDiffBid. It gradually corrupts the bidding trajectory by injectingscheduled Gaussian noises into the forward process. Then, it recon-structs trajectory from corrupted ones given returns and temporalconditions via a parameterized neural network. We further proposea non-Markovian inverse dynamics to more accuratelygenerate optimal bidding parameters. Taking one step further, Diff-Bid provides flexibility to closely align with the specific needs ofadvertisers by accommodating diverse constraints like cost-per-click (CPC) and incorporating human feedback. Notably, DiffBidserves as a unified model capable of mastering multiple tasks si-multaneously, dynamically composing various bidding trajectorycomponents to generate sequences that efficiently maximize di-verse targets while adhering to a range of predefined constraints.To assess the effectiveness of DiffBid, we conducted extensive eval-uations offline and online against baselines. Our results indicatethat DiffBid surpasses RL methods for auto-bidding. In summary: We uncover that the Markov assumptions upon which com-mon decision-making methods rely are not applicable tothe auto-bidding problem. Therefore, we propose a novelbidding paradigm with non-Markovian properties based ongenerative learning. This paradigm represents a significantinnovation in modeling methodology compared with exist-ing RL methods commonly used in auto-bidding. Unlike common bidding methods, our approach capturesthe correlation between the return and the entire biddingtrajectory. This design enables the method to address impor-tant challenges, such as sparse returns, and ensures stabilityin the highly random advertising environment. Finally, weprove that the proposed diffusion modeling is equivalentin terms of optimality to solving a non-Markovian decisionproblem. We demonstrate that the method can integrate capabilitiesto handle a variety of tasks within a unified solution, tran-scending the limitations of traditional task-specific methods.It shows that DiffBid outperforms conventional RL methodsin auto-bidding, and achieves significant performance gainon a leading E-commerce ad platform through both offlineand online evaluation. In specific, it achieves 2.81% increasein GMV and 3.36% in ROI.",
  "PRELIMINARY2.1Problem Formulation": "For simplicity, we consider auto-bidding with cost-related con-straints. During a time period, suppose there are impressionopportunities arriving sequentially and indexed by . In this setting,advertisers submit bids to compete for each impression opportunity.An advertiser will win the impression if its bid is greater thanothers. Then it will incur a cost for winning and getting the value.During the period, the mission of an advertiser is to maximizethe total received value , where is the value of impression and is whether the advertiser wins impression . Besides, we havethe budget and several constraints to control the performance of addeliveries. Budget constraints are simply , where isthe cost of impression and is the budget. The other constraintsare complex and according to we have the unified formulation:",
  "=1,(3)": "where is the predicted optimal bid for the impression . , {0, ..., } are the optimal bidding parameters. Specifically, whenconsidering only the budget constraint, it is the Max Return bidding.However, when considering both the budget constraint and theCPC constraint, it is called Target-CPC bidding. From an alternativeperspective, the optimal strategy involves arranging all impressionsin order of their cost-effectiveness (CE) and then selecting everyimpression opportunity that surpasses the optimal CE ratio .This threshold enforces the constraint, and the optimal biddingparameters 0 = 1/.",
  "Auto-Bidding as Decision-Making": "Eq.(3) gives the formation of the optimal bid with bidding pa-rameters , {0, ..., }. However, in practice, the highly randomand complex nature of the advertising environment prevents di-rect calculation of the bidding parameters. They must be carefullycalibrated to adapt to the environment and dynamically adjustedas it evolves, This subsequently makes auto-bidding a sequentialdecision-making problem. To model it with decision-making, weintroduce states S to describe the real-time advertising statusand actions A to adjust the corresponding bidding param-eters. The auto-bidding agent will take action at the state based on its policy , and then the state will transit to the nextstate +1 and gain reward R according to the advertisingenvironment dynamics T. When T : +1 satisfies,it is called the Markovian decision process (MDP). Otherwise, it isa non-Markovian decision process. We next describe the key itemsof the automated bidding agent in the industrial online advertisingsystem: State describes the real-time advertising status at timeperiod , which includes 1) remaining time of the advertiser;2) remaining budget; 3) budget spend speed; 4) real-timecost-efficiency (CPC), 5) and average cost-efficiency (CPC).",
  "A trajectory is the index of a sequence of states, actions,and rewards within an episode": "In the online advertising system, learning policy through directinteraction with the online environment is unfeasible due to safetyconcerns. Nonetheless, access to historical bidding logs, incorpo-rating trajectories from a variety of bidding strategies, is attainableand provides a viable alternative. Prevalent auto-bidding methodspredominantly leverage this offline data to craft effective policies.Our approach is aligned with this practice and will be elaboratedin detail in the subsequent chapters.",
  "AIGB PARADIGM FOR AUTO-BIDDING": "To thoroughly investigate the auto-bidding problem, we conducteda series of statistical analyses of bidding trajectories, with detailedinformation available in appendix ??. These analyses provide uswith the insight that devising an effective bidding strategy is es-sentially equivalent to optimizing a state trajectory. Armed withthis insight, we propose a hierarchical paradigm for auto-biddingthat prioritizes the state trajectory optimization and subsequentlygenerates actions aligned with the optimized trajectory.For state trajectory optimization, we can employ a generativemodel to capture the joint distribution of the entire bidding trajec-tory and its associated returns, subsequently generating the trajec-tory distribution conditioned on the desired return. This approachenables us to address key auto-bidding challenges by employingSOTA generative algorithms. This paper presents an implementa-tion that utilizes Denoising Diffusion Probabilistic Models (DDPM).For action generation, several off-the-shelf methods can be utilizedto predict the proper action given the target state trajectory. Inthis paper, we apply a widely used inverse dynamics model. Thehierarchical paradigm divides auto-bidding into two supervised",
  "DIFFUSION AUTO-BIDDING MODEL": "In this section, we give a detailed introduction of the proposeddiffusion Auto-bidding Model (DiffBid). We will first give the mod-eling of Auto-bidding through diffusion models in .1.1.Then we give a detailed description of the forward process in Sec-tion 4.1.2, the reverse process in .1.3, and the trainingprocess in .2. Finally, we will give the complexity analysisin .4.",
  "maxE [log (0()|())](4)": "where is the trajectory index, 0() is the original trajectoryof states and () is the corresponding property. The goal is toestimate the conditional data distribution with so that the futurestates of a trajectory 0() from information () can be generated.For example, in the context of online advertising, () can be theconstraints or the total value of the entire trajectory. Under sucha setting, we can formalize the conditional diffusion modeling forauto-bidding:",
  "(+1()| ()), (1()| (),()),(5)": "where represents the forward process in which noises are gradu-ally added to the trajectory while is the reverse process where amodel is used for denoising. The detailed introduction of diffusionmodeling can be found in Appendix A.2. The overall framework ispresented in . We will make a detailed discussion about thetwo modeling processes in the following sections.",
  "Forward Process via Diffusion over States. We modelthe forward process (+1()| ()) via diffusion over states,where: () := (1, ..., , ..., ) ,(6)": "where is modeled as a one-dimensional vector. () is a noisesequence of states and can be represented by a two-dimensionalarray where the first dimension is the time periods and the seconddimension is the state values. Merely sampling states is not enoughfor an agent. Given (), we model the diffusion process as aMarkov chain, where () is only dependent on 1():",
  "1(), ,(7)": "when , () approaches a sequence of standard Gauss-ian distribution where we can make sampling through the re-parameterization trick and then gradually denoise the trajectory toproduce the final state sequence. For the design of , = 1, ..., ,we apply cosine schedule to assign the corresponding valueswhich smoothly increases diffusion noises using a cosine functionto prevent sudden changes in the noise level. The details for noiseschedule can be found in the appendix. 4.1.3Reverse Process for Bid Generation. Following weuse a classifier-free guidance strategy with low-temperature sam-pling to guide the generation of bidding, to extract high-likelihoodtrajectories in the dataset. During the training phase, we jointlytrain the unconditional model ( (),) and conditional model ( (),(),) by randomly dropping out conditions. Duringgeneration, a linear combination of conditional and unconditionalscore estimates is used:",
  "= (:, +1),(11)": "where R contains predicted bidding parameters (i.e. , =1, ...,) at time . is the length of history states. The inverse dy-namic function can be trained with the same offline logs as thereverse process. This design disentangles the learning of states andactions, making it easier to learn the connection between statesthus achieving better empirical performance. The overall procedureis summarized in Algorithm 1.",
  "DiffBid Training": "Following , we train DiffBid to approximate the given noiseand the returns in a supervised manner. Given a bidding trajectory0(), we have its corresponding returns e.g., values the advertiserreceived, the constraint the model should obey and the historystates , = 1, ..., before time + 1. Then we just train the reverseprocess model which is parameterized through the noise model and the inverse dynamics through:",
  "max min,(13)": "where min and max are the smallest and the largest return in thedataset. Through Eq. 13 we normalize the return into andmerge it into (). Subsequently, we train the model to generatetrajectories conditioned on the normalized returns. It should benoted that trajectories with more values received have higher nor-malized returns. Thus = 1 indicates the best trajectory with thehighest values which will better fit the advertisers needs. Whengeneration, we just set = 1 and generate the trajectory under themax return condition to the advertiser. 4.3.2Generation with Constraints or Human Feedback. InMCB, the cumulative performance related to the constraints withina given episode should be controlled so as not to exceed the adver-tisers expectations. In such a setting, we can design () to controlthe generation process. For example, in the Target-CPC setting, wecan maintain a binary variable to indicate whether the final CPCexceeds the given constraint :",
  "= I ()(14)": "where = is defined in Eq (2). We can then normalize into through min-max normalization for simplification. can beused to indicate whether trajectory break the CPC constraint. Wecan also design () to include = 1 to make the model generatebids that do not break the CPC constraint. Sometimes it is alsoimportant to adjust the bidding parameters given real-time feedbackprovided by the advertiser to enable flexibility. Here we use twoexample indicators that reflect the experience of advertisers: (1) Smoothness: an advertiser may expect the cost curve assmooth as possible to avoid sudden change. By defining =1 | 1|, we can model it as a binary variable indicating whether the max cost change between adjacenttime period exceeds a threshold as in Eq (14).",
  "The complexity analysis for training DiffBid consists of the trainingprocess and the inference process. For training, given the time com-plexity of the noise prediction model is O(1), the complexity": "for the inverse dynamic model is O(2), the complexity for atraining epoch is O(|B|(1 +2)). It can be seen that the trainingcomplexity is linear with the input given 1 and 2 are relativelyfixed. Thus the training of DiffBid is efficient. For generation, giventhe total diffusion step , the trajectory length , then the timecomplexity for inference is O((1+2)). We can observe that thetime complexity for inference is linearly scaled with the diffusionstep . In image generation, is usually very large to ensure goodgeneration quality, which brings the problem of non-efficiency.However, for bidding generation, we find needs not to be verylarge. Relatively small has already generated promising results.Moreover, in auto-bidding, a higher tolerance for latency is accept-able, enabling the use of relatively larger .",
  "THEORETICAL ANALYSIS": "In this section, we theoretically analyze the property of DiffBid. Inspecific, we show that DiffBid that utilize MLE as the objective hasa corresponding non-Markovian decision problem .The detailed proofs can be found in the Appendix A.7. Lemma 5.1 (MLE as non-Markovian decision-making). As-suming the Markovian transition (+1|,) is known, the ground-truth conditional state distribution (+1|0:) for demonstrationsequences is accessible, we can construct a non-Markovian sequentialdecision-making problem, based on a reward function (+1,0:) :=log ( |0:) (+1|,) for an arbitrary energy-basedpolicy ( |0:). Its objective is",
  "EXPERIMENTS6.1Experimental Setup": "6.1.1Experimental Environment. The simulated experimentalenvironment is conducted in a manually built offline real advertisingsystem (RAS) as in . Specifically, the RAS is composed of twoconsecutive stages, where the auction mechanisms resemble thosein the RAS. We consider the bidding process in a day, where theepisode is divided into 96 time steps. Thus, the duration betweenany two adjacent time steps and + 1 is 15 minutes. The numberof impression opportunities between time step and +1 fluctuatesfrom 100 to 500. Detailed parameters in the RAS are shown in. We keep the parameters the same for all experiments. 6.1.2Data Collection. We use the widely applied auto-biddingRL method USCB in the online environment to generate the biddinglogs for offline RL training. This results in a total 5, 000 trajectoriesfor the based dataset and 50, 000 for a larger one. To increase the",
  "DT a prevalent generative method based on the trans-former architecture for sequential decision-making": "6.1.4Implementation Details. For the implementation of base-lines, we use the default hyper-parameters suggested from theirpapers and also tune through our best effort. For DiffBid, the diffu-sion steps is searched within {5, 10, 20, 30, 50}. is set to 0.008. issearched in {1, 2, 3}. for noise schedule is set to 0.2 empirically.The batch size is set to 2% of all training trajectories. Total trainingepochs is set to 500. For the implementation of , we adopt themost widely used model U-Net for diffusion modeling with hiddensizes of 128 and 256. We use Adam optimizer with a learning rate14 to optimize the model. The condition dropout ratio is set to0.2 during training. We update the model with momentum updatesover a period of 4 steps. 6.1.5Evaluation. For evaluation, we randomly initialize a multi-agent advertising environment with USCB as the base auto-biddingagents and use other methods to compete with these agents. We testthe performance under 4 different budgets, 1500, 2000, 2500, and3000, to test the generalization under different budget scales. Weuse the cumulative reward as the evaluation metric, which reflectsthe total gain received by the target agent. For each method, werandomly initialize 50 times and report the average of top-5 scores.",
  "Performance Evaluation": "The performance against baselines is shown in . In thistable, we show the cumulative reward from different budgets ofall the models. We have the following discoveries. One of the keytakeaways from the performance comparison presented in is that offline RL methods consistently outperform the state-of-the-art auto-bidding method, USCB. This finding underscores theadvantages of leveraging historical bidding data to train RL agents.Offline RL methods, such as BCQ, IQL, and DT, exhibit superiorperformance in terms of cumulative rewards across various budgetscenarios. The superiority of offline RL methods can be attributed totheir ability to learn from past bidding experiences without interac-tion with a simulation environment. This mitigates the challengesassociated with inconsistencies between the online bidding envi-ronment and the offline bidding environment, leading to policiesthat are better aligned with real-world scenarios. Notably, DiffBidstands out as the top-performing approach among all the methodsevaluated. In all budget scenarios and training datasets, DiffBidconsistently achieves the highest cumulative rewards. This remark-able performance highlights the efficacy of the DiffBid approach inoptimizing bidding strategies by directly modeling the correlationwith the returns and entire trajectories. By decoupling the compu-tational complexity from horizon length, DiffBid achieves superiordecision-making capabilities, outperforming traditional RL meth-ods in both foresight and strategy. Another important observation",
  "DiffBid w/o cond1812.641852.21DiffBid w/o non-mkv2254.782287.41": "from the results is the impact of training dataset size on model per-formance. When comparing the \"USCB-5K\" and \"USCBEx-50K\" set-tings, it becomes evident that a larger training dataset consistentlyleads to improved cumulative rewards. This finding underscoresthe significance of data size in training RL models for automatedbidding. A richer dataset allows the models to capture more diversebidding scenarios and make more informed decisions, ultimatelyresulting in better performance. One intriguing aspect of DiffBidsperformance is its resilience to noise. In real-world advertisingenvironments, there can be inherent uncertainty and variabilityin the bidding process due to factors like market dynamics andcompetitor behavior. DiffBid appears to handle such noise moreeffectively than the RL baselines. This means that even in situationswhere bidding outcomes are less predictable, DiffBid manages tomaintain competitive performance.",
  "Ablation Study": "To study different parts of the proposed DiffBid, we run the modelwithout a certain module to see if the removed corresponding mod-ule will result in a performance drop. The result of the ablationstudy is shown in . Due to the space limitation, we onlyprovide the results on USCBEx-5K and USCBEx-50K. w/o condrefers to the DiffBid with the condition set to 0.0 (rather than 1.0).w/o non-mkv refers to the situation where we only use the currentstate and the predicted next state to generate the bidding coeffi-cient. From the table, we find both of the two parts contribute tothe final result, and removing either of them will result in a perfor-mance drop. It verifies the effectiveness of the proposed methodsin boosting DiffBids performance for auto-bidding.",
  "In-depth Analysis": "6.4.1Study of State Transition. Here we compare the statetransition of the baseline method USCB and our proposed methodDiffBid. The result for grouped and non-grouped state transitionduring a day is shown in . In this figure, we plot the budgetleft ratio with time steps in one day. From the figure, we can observethat under USCB, most of the advertisers consumption does notexhaust their budget. This is attributed to the inconsistency betweenthe offline virtual environment and the real online environmentfaced by USCB. On the contrary, the budget completion situationimproves under DiffBid, where most of the advertisers spend morethan 80% of their budgets. One possible reason is that DiffBid findstrajectories with a high budget completion ratio will also have ahigh cumulative reward, and thus tend to generate trajectories witha high budget completion ratio. Moreover, advertisers with smallbudgets undertend to spend money in the afternoon. This is becausethe impressions in the afternoon offer a higher cost-effectiveness,albeit with a limited quantity.",
  ": Performance of Human Feedback": "6.4.2Performance under Constraints and Feedbacks. Weadditionally investigate DiffBids multi-objective optimization ca-pability under specific constraints, comparing its performance withOffline RL. Specifically, we choose CPC ratio and overall return asmetrics and examine the ability of DiffBid and IQL to control theoverall CPC exceeding ratio while maximizing the overall return.During training, we set different thresholds of CPC as in Eq (14).Then when testing, we make DiffBid generating trajectories un-der the expected CPC. In , we show the exceeding ratioand overall return under different CPC constraints and trainingsettings. From the figure, we find that DiffBid has the ability tocontrol diverse levels of exceeding ratio while maintaining an in-tact return, surpassing IQL by a significant margin. Consequently,DiffBid holds a distinct advantage in effectively addressing MCBproblems. We also study the performance under different advertiser",
  "Baseline2068886744834426.10423584.683618538232.221DiffBid2068886744829992.38424078.688319059542.296compare---0.53%+2.09%+2.81%+3.36%": "feedbacks. During training we split the trajectories through thresh-olds of Eq. (14) into high and low levels, and learn the conditionaldistribution under different levels. During generation, we adjust thecondition and generate corresponding samples and summarize themetrics. The results for the statistic distribution of metrics for lowlevel, high level and the original trajectories are shown in .We find that the trajectory obtained from deploying DiffBid is wellcontrolled by the condition. 6.4.3Impact of Diffusion Steps. We also study the overall per-formance under different diffusion steps, which is an importantfactor in influencing the efficiency and performance. The overall im-pact of diffusion steps with respect to different budgets is illustratedin . From the figure, we have the following discoveries. Firstof all, we observe that diffusion steps have a larger impact on ad-vertisers with small budgets (1500 yuan). Secondly, larger budgetsare not sensitive to the diffusion steps, where we can get the bestresult in most situations within 30 diffusion steps. 6.4.4Stability. In this study, we randomly initialized the parame-ters of three models - CQL, IQL, and DiffBid - and conducted thirtytraining trials for each to examine stability in performance. As de-picted in (b), the RL-based models, CQL and IQL, showed atendency towards instability under varying random seeds. Notably,IQL demonstrated slightly better performance than CQL, whichmay be attributed to its design optimized for conservative regu-larization. Contrasting with these, the generative model DiffBidexhibited remarkable stability, with significantly fewer instancesof failure compared to its RL counterparts.",
  "Online A/B Test": "To further substantiate the effectiveness of DiffBid, we have de-ployed it on Alibaba advertising platform for comparison againstthe baseline IQL method, which performs best among variousauto-bidding methods. The online A/B test is conducted from Febru-ary 01, 2024, to February 08, 2024. The results are shown in .It shows that DiffBid can significantly improve the Buycnt by 2.09%, the GMV by 2.81%, the ROI by 3.36%, showing its effectiveness inoptimizing the overall performance. For efficiency, DiffBid takes0.2s per request with GPU acceleration while the baseline is 0.07s,which means latency can be well guaranteed.",
  "RELATED WORKS": "Offline-Reinforcement Learning. Offline reinforcement learn-ing is a research direction that has gained significant attention inrecent years. The primary goal of offline RL is to learn effectivepolicies from a fixed dataset without additional online interactionwith the environment. This approach is particularly beneficial whenonline interaction is costly, risky, or otherwise not feasible.Notableworks include Conservative Q-learning (CQL) by Kumar et al. ,and Batch-Constrained deep Q-learning (BCQ) by Fujimoto et al.. Both algorithms aim to tackle overestimation bias which tendsto occur in offline RL settings. Kostrikov et al. propose animplicit q-learning approach to address the training instability forCQL. Chen et al. propose to use transformers for offline RL toincrease the model capability. Hansen-Estruch et al. proposesa diffusion-based approach with implicit Q-learning for offline RL.Diffusion Models. They recently have shown the capability ofhigh-quality generation , unconditional generation and con-ditional generation . It has shown promising performance indecision-making. Hansen-Estruch et al. proposes a diffusion-based approach with implicit q-learning for offline RL. Wang etal. propose a expressive policy though diffusion modeling.Chen et al. propose to use diffusion models for behavior mod-eling. Hu et al. introduce temporal conditions for trajectorygeneration. Despite these preliminary explorations, no work hasbeen payed for diffusion based auto-bidding which requires themodel to adapt to the random advertising environment. Li et al. utilize diffusion model in anti-money laundering.Auto-bidding. Auto-bidding systems are widely used in program-matic advertising, where they are employed to automatically placebids on ad spaces. The main focus of such systems is to optimise agiven key performance indicator (KPI), such as the number of clicksor conversions, while maintaining a certain budget . Cai et al. proposed an RL-based approach to the problem of auto-biddingfor display advertising. They designed a bidding environment andapplied a deep RL algorithm to learn the optimal bidding strategy.He et al. propose a unified solution with RL to enable multi-ple constraints for auto-bidding. Jin et al. extend the RL to enablemulti-agent competition . Zhang et al. also adopted theRL framework for auto-bidding and showed that their approachcan outperform traditional bidding strategies. Wen et al. pro-pose a multi-agent-based approach for auto-bidding, which enablesthe modeling of multiple auto-bidding agents at the same time toinclude more information and also has been deployed online.",
  "CONCLUSION": "In this paper, we design a new paradigm for auto-bidding throughthe lens of generative modeling. To achieve this goal, we proposea decision-denoising diffusion approach to generate conditionalbidding trajectories and at the same time control the generatedsamples under certain constraints. This new generative modelingapproach enables integrating different kinds of industrial metrics,",
  "AIGB: Generative Auto-bidding via Diffusion ModelingKDD 24, August 2529, 2024, Barcelona, Spain": "which is the first unified model for bidding. Extensive experimentson real-world simulation environments demonstrate the effective-ness of the newly proposed approach. In the future, we will considerdeveloping new methods to accelerate the generation process andnew methods to ensure the robustness of DiffBid. Pulkit Agrawal, Ashvin V Nair, Pieter Abbeel, Jitendra Malik, and Sergey Levine.2016. Learning to poke by poking: Experiential learning of intuitive physics.Advances in neural information processing systems 29 (2016). Anurag Ajay, Yilun Du, Abhi Gupta, Joshua B Tenenbaum, Tommi S Jaakkola,and Pulkit Agrawal. 2022. Is Conditional Generative Modeling all you need forDecision Making?. In The Eleventh International Conference on Learning Represen-tations. Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne VanDen Berg. 2021. Structured denoising diffusion models in discrete state-spaces.Advances in Neural Information Processing Systems 34 (2021), 1798117993.",
  "Santiago Balseiro, Yuan Deng, Jieming Mao, Vahab Mirrokni, and Song Zuo. 2021.Robust auction design in the auto-bidding world. Advances in Neural InformationProcessing Systems 34 (2021), 1777717788": "Santiago R Balseiro, Yuan Deng, Jieming Mao, Vahab S Mirrokni, and Song Zuo.2021. The landscape of auto-bidding auctions: Value versus utility maximization.In Proceedings of the 22nd ACM Conference on Economics and Computation. 132133. Han Cai, Kan Ren, Weinan Zhang, Kleanthis Malialis, Jun Wang, Yong Yu, andDefeng Guo. 2017. Real-time bidding by reinforcement learning in display adver-tising. In Proceedings of the tenth ACM international conference on web search anddata mining. 661670. Chen-Hao Chao, Wei-Fang Sun, Bo-Wun Cheng, Yi-Chen Lo, Chia-Che Chang,Yu-Lun Liu, Yu-Lin Chang, Chia-Ping Chen, and Chun-Yi Lee. 2022. Denoisinglikelihood score matching for conditional score-based data generation. arXivpreprint arXiv:2203.14206 (2022).",
  "Huayu Chen, Cheng Lu, Chengyang Ying, Hang Su, and Jun Zhu. 2022. Offlinereinforcement learning via high-fidelity generative behavior modeling. arXivpreprint arXiv:2209.14548 (2022)": "Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin,Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. 2021. Decision transformer:Reinforcement learning via sequence modeling. Advances in neural informationprocessing systems 34 (2021), 1508415097. Harry L Chiesi, George J Spilich, and James F Voss. 1979. Acquisition of domain-related information in relation to high and low domain knowledge. Journal ofverbal learning and verbal behavior 18, 3 (1979), 257273.",
  "Scott Fujimoto, David Meger, and Doina Precup. 2019. Off-policy deep rein-forcement learning without exploration. In International conference on machinelearning. PMLR, 20522062": "Scott Fujimoto, David Meger, Doina Precup, Ofir Nachum, and Shixiang ShaneGu. 2022. Why Should I Trust You, Bellman? The Bellman Error is a PoorReplacement for Value Error. In Proceedings of the 39th International Conference onMachine Learning (Proceedings of Machine Learning Research, Vol. 162), KamalikaChaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and SivanSabato (Eds.). PMLR, 69186943.",
  "Maor Gaon and Ronen Brafman. 2020.Reinforcement learning with non-markovian rewards. In Proceedings of the AAAI conference on artificial intelligence,Vol. 34. 39803987": "Jiayan Guo, Yaming Yang, Xiangchen Song, Yuan Zhang, Yujing Wang, JingBai, and Yan Zhang. 2022. Learning Multi-granularity Consecutive User IntentUnit for Session-based Recommendation. In Proceedings of the Fifteenth ACMInternational Conference on Web Search and Data Mining (Virtual Event, AZ, USA)(WSDM 22). Association for Computing Machinery, New York, NY, USA, 343352.",
  "Xiaotian Hao, Zhaoqing Peng, Yi Ma, Guan Wang, Junqi Jin, Jianye Hao, ShanChen, Rongquan Bai, Mingzhou Xie, Miao Xu, Zhenzhe Zheng, Chuan Yu, Han": "Li, Jian Xu, and Kun Gai. 2020. Dynamic Knapsack Optimization Towards Ef-ficient Multi-Channel Sequential Advertising. In Proceedings of the 37th Inter-national Conference on Machine Learning, ICML 2020, 13-18 July 2020, VirtualEvent (Proceedings of Machine Learning Research, Vol. 119). PMLR, 40604070. Yue He, Xiujun Chen, Di Wu, Junwei Pan, Qing Tan, Chuan Yu, Jian Xu, andXiaoqiang Zhu. 2021. A unified solution to constrained bidding in online displayadvertising. In Proceedings of the 27th ACM SIGKDD Conference on KnowledgeDiscovery & Data Mining. 29933001.",
  "Jonathan Ho and Tim Salimans. 2021. Classifier-Free Diffusion Guidance. InNeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications": "Jifeng Hu, Yanchao Sun, Sili Huang, SiYuan Guo, Hechang Chen, Li Shen, LichaoSun, Yi Chang, and Dacheng Tao. 2023. Instructed Diffuser with Temporal Condi-tion Guidance for Offline Reinforcement Learning. arXiv preprint arXiv:2306.04875(2023). R Huang, MWY Lam, J Wang, D Su, D Yu, Y Ren, and Z Zhao. 2022. FastDiff: AFast Conditional Diffusion Model for High-Quality Speech Synthesis. In IJCAIInternational Joint Conference on Artificial Intelligence. IJCAI: International JointConferences on Artificial Intelligence Organization, 41574163.",
  "Alexander Quinn Nichol and Prafulla Dhariwal. 2021. Improved denoising diffu-sion probabilistic models. In International Conference on Machine Learning. PMLR,81628171": "Weitong Ou, Bo Chen, Yingxuan Yang, Xinyi Dai, Weiwen Liu, Weinan Zhang,Ruiming Tang, and Yong Yu. 2023. Deep landscape forecasting in multi-slot real-time bidding. In Proceedings of the 29th ACM SIGKDD Conference on KnowledgeDiscovery and Data Mining. 46854695. Deepak Pathak, Parsa Mahmoudieh, Guanghao Luo, Pulkit Agrawal, Dian Chen,Yide Shentu, Evan Shelhamer, Jitendra Malik, Alexei A Efros, and Trevor Darrell.2018. Zero-shot visual imitation. In Proceedings of the IEEE conference on computervision and pattern recognition workshops. 20502053. Aoyang Qin, Feng Gao, Qing Li, Song-Chun Zhu, and Sirui Xie. 2023. Learningnon-Markovian Decision-Making from State-only Sequences. In Thirty-seventhConference on Neural Information Processing Systems. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolu-tional networks for biomedical image segmentation. In Medical Image Computingand Computer-Assisted InterventionMICCAI 2015: 18th International Conference,Munich, Germany, October 5-9, 2015, Proceedings, Part III 18. Springer, 234241.",
  "Information Retrieval 11, 4-5 (2017), 297435": "Zhendong Wang, Jonathan J Hunt, and Mingyuan Zhou. 2022. Diffusion Policiesas an Expressive Policy Class for Offline Reinforcement Learning. In The EleventhInternational Conference on Learning Representations. Chao Wen, Miao Xu, Zhilin Zhang, Zhenzhe Zheng, Yuhui Wang, Xiangyu Liu, YuRong, Dong Xie, Xiaoyang Tan, Chuan Yu, et al. 2022. A cooperative-competitivemulti-agent framework for auto-bidding in online advertising. In Proceedingsof the Fifteenth ACM International Conference on Web Search and Data Mining.11291139.",
  "Yuxin Wu and Kaiming He. 2018. Group normalization. In Proceedings of theEuropean conference on computer vision (ECCV). 319": "Haoqi Zhang, Lvyin Niu, Zhenzhe Zheng, Zhilin Zhang, Shan Gu, Fan Wu, ChuanYu, Jian Xu, Guihai Chen, and Bo Zheng. 2023. A Personalized Automated BiddingFramework for Fairness-aware Online Advertising. In Proceedings of the 29thACM SIGKDD Conference on Knowledge Discovery and Data Mining. 55445553. Peiyan Zhang, Jiayan Guo, Chaozhuo Li, Yueqi Xie, Jae Boum Kim, Yan Zhang,Xing Xie, Haohan Wang, and Sunghun Kim. 2023. Efficiently Leveraging Multi-level User Intent for Session-based Recommendation via Atten-Mixer Network. InProceedings of the Sixteenth ACM International Conference on Web Search and DataMining (, Singapore, Singapore,) (WSDM 23). Association for Computing Ma-chinery, New York, NY, USA, 168176.",
  "SymbolDefinition": "The trajectory index of a serving policy.()Sequence of states of trajectory in diffusion step .()Properties or conditions for .Return of a trajectory.Binary indicator variable.The budget of the advertiser.The s constraint.Whether the advertiser wins impression .The true value of the impression .The optimal bidding price for the impression .The state at time period .Predicted bidding parameters at time period .The bidding parameters.The denoising model that predict the noise.The model that generate bids.",
  "(18)": "where the first term has no learned variable given variance isfixed to constants, thus can be ignored during training. The secondterm is the reconstruction term where () is trained to recover theoriginal sample 0 from the noise sample 1. The last term is the de-noising term where () is trained to denoise to get 1, thuswe can recurrently denoise the latent samples. In the original pa-per the author shows that the last term can be simplified to the",
  "A.3Model Configuration": "We parameterize the noise model with a temporal U-Net ,consisting of 3 repeated residual blocks. Each block is consisted oftwo temporal convolutions, followed by group normalization ,and a final Mish activation function . Timestamp and conditionembeddings, both 128-dimensional vectors, are produced by sep-arate 2-layered MLP (with 256 hidden units and Mish activationfunction) and are concatenated together before getting added to",
  "A.5Analytical Results for Action Control": "We analyze the ability of different models in controlling actions.To achieve this goal, we re-define the return function to be thesummation of actions in odd time steps minus the summation ofactions in even time steps. The results are shown in . We findDiffBid can better control the action than IQL. The main reasonis that controlling of actions is difficult for RL in long horizons.Instead, DiffBid directly models the correlation of trajectories andreturns, thus can well handle the long trajectory situation.",
  "A.6Statistical Analyses for Bidding Trajectory": "The study by indicates that CE follows a power-law declineas the number of winning impressions increases. Our statisticalanalysis confirms that this finding holds true at every discrete timestep, with decay rates varying temporally due to the heterogeneousnature of the impressions. (a) shows three steps sampledfrom the online advertising system, From which another key insightis that the optimal bidding strategys is equivalent to selectinga specific number of winning impressions per time step. We denotethe number at time step as .Another finding illustrated in (b) is that the costs ofimpressions remain relatively stable throughout the total episode,fluctuating by less than 5%. This stability allows us to approximatethe cost of each impression with the average cost = 1 =0 ,where represents the number of winning impressions of thetotal episode. Therefore, the total cost at each time step = . In auto-bidding modeling, can be calculated from the statetrajectory by using the difference in the remaining budget betweentwo consecutive steps. Consequently, we can conclude that theoptimal strategy correlates to a specific state trajectory.",
  "Definition A.2 (History-based Decision Process (HDP)). HDPis a stochastic mapping from a history-action pair to observation-reward pairs. Formally, P : H A O R, where P denotes astochastic mapping": "We show that a sequential decision-making problem can be con-structed to maximize the same objective. The main results are givenby and we put the proofs here for completeness. To start, letthe ground-truth distribution of demonstrations be (0()) andthe learned marginal distributions of state sequences be (0()).Then Eq. (4) is an empirical estimation of",
  "ParametersValues": "Number of advertisers30Time steps in an episode, 96Minimum number of impression opportunities min50Maximum number of impression opportunities max300Minimum budget1000 YuanMaximum budget4000 YuanValue of impression opportunities in stage 1, 1,0 1Value of impression opportunities in stage 2, 2,0 1Minimum bid price, min0 YuanMaximum bid price, max1000 YuanMaximum value of impression opportunity, 1Maximum market price, 1000 Yuan",
  ": Statistical Results From Online Advertising System": "with (+1,0:) := log(+1|0:) = log( |0:)(+1|,), (0: ) := 0. It is worth noting that the defined above involvesthe optimal policy, which may not be known a priori. We can re-solve this by replacing it with for an arbitrary policy ( |0:).All Bellman identities and updates should still hold. The entailedBellman update, value iteration, for arbitrary and is",
  "(;0:) = E(+1|0: ) [ (0:,,+1) + (0:+1)].(24)": "Also note that the and in identities Eq. (23) and Eq. (25) re-spectively are not necessarily associated with the policy ( |0:).Slightly overloading the notations, we use , to denote theexpected returns from policy ( |0:). By now, we finish the con-struction of atomic algebraic components and move on to check ifthe relations between them align with the algebraic structure of a se-quential decision-making problem. We first prove the constructionabove is valid at optimality.",
  "=+1E (+1: |0: ) [ ( |0: )]": "where H () is the entropy term. The last line is derived by recur-sively applying the Bellman equation in the line above until 0: .As an energy-based policy, ( |0:)s entropy is inherently max-imized . Therefore, within the hypothesis space, ( |0:)that optimizes (0:) also leads to the optimal expected returnE ( |0: ) [ (;0:)].Given the convergence proof by Ziebart , we have:",
  "Lemma A.3. If (+1|0:) is accessible and (+1|,) isknown, soft policy iteration and soft learning both converge to ( |0:) = ( |0:) exp((;0:)) under conditions": "Lemma 3 means given (+1|0:) and (+1|,), we canrecover through reinforcement learning methods, instead ofthe proposed MLE. So ( |0:) is a viable policy space for theconstructed sequential decision-making problem. Together, LemmaA.1, Lemma A.2 and Lemma A.3 provide proof for a valid sequentialdecision-making problem that maximizes the same objective ofMLE, by Lemma A.4. Lemma A.4 (MLE as non-Markovian decision-making pro-cess). Assuming the Markovian transition (+1|,) is known,the ground-truth conditional state distribution(+1|0:) for demon-stration sequences is accessible, we can construct a non-Markoviansequential decision-making problem, based on a reward function (+1,0:) := log ( |0:) (+1|,) for an arbitraryenergy-based policy ( |0:). Its objective is"
}