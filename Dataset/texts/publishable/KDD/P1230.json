{
  "ABSTRACT": "In the field of quantitative trading, it is common practice to trans-form raw historical stock data into indicative signals for the markettrend. Such signals are called alpha factors. Alphas in formula formsare more interpretable and thus favored by practitioners concernedwith risk. In practice, a set of formulaic alphas is often used togetherfor better modeling precision, so we need to find synergistic formu-laic alpha sets that work well together. However, most traditionalalpha generators mine alphas one by one separately, overlookingthe fact that the alphas would be combined later. In this paper, wepropose a new alpha-mining framework that prioritizes mining asynergistic set of alphas, i.e., it directly uses the performance of thedownstream combination model to optimize the alpha generator.Our framework also leverages the strong exploratory capabilities ofreinforcement learning (RL) to better explore the vast search spaceof formulaic alphas. The contribution to the combination modelsperformance is assigned to be the return used in the RL process,driving the alpha generator to find better alphas that improve uponthe current set. Experimental evaluations on real-world stock mar-ket data demonstrate both the effectiveness and the efficiency of ourframework for stock trend forecasting. The investment simulationresults show that our framework is able to achieve higher returnscompared to previous approaches.",
  "Computing methodologies Reinforcement learning; Searchmethodologies; Applied computing Economics": "These authors contributed equally.Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS).Xiang Ao is also at Institute of Intelligent Computing Technology, Suzhou, China.Corresponding authors. Permission to make digital or hard copies of part or all of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for third-party components of this work must be honored.For all other uses, contact the owner/author(s).KDD 23, August 610, 2023, Long Beach, CA, USA 2023 Copyright held by the owner/author(s).ACM ISBN 979-8-4007-0103-0/23/08.",
  "Computational Finance, Stock Trend Forecasting, ReinforcementLearning": "ACM Reference Format:Shuo Yu, Hongyan Xue, Xiang Ao, Feiyang Pan, Jia He, Dandan Tu, and QingHe. 2023. Generating Synergistic Formulaic Alpha Collections via Rein-forcement Learning. In Proceedings of the 29th ACM SIGKDD Conference onKnowledge Discovery and Data Mining (KDD 23), August 610, 2023, LongBeach, CA, USA. ACM, New York, NY, USA, 11 pages.",
  "INTRODUCTION": "Currently, it is almost a standard paradigm to transform raw histori-cal stock data into indicative signals for the market trend in the fieldof quantitative trading . These signal patterns are called alphafactors, or alphas in short . Discovering alphas with high returnshas been a trendy topic among investors and researchers due to theclose relatedness between alphas and investment revenues.The prevailing methods of discovering alphas can be in gen-eral divided into two groups, namely machine learning-based andformulaic alphas. Most recent research has focused on the formerones. These more sophisticated alphas are often obtained via deeplearning models, e.g., using sequential models like LSTM , ormore complex ones integrating non-standard data like HIST and REST , etc. On the other end of the spectrum, we havethe alphas that can be represented in simple formula forms. Suchformulaic alphas are traditionally constructed by human expertsusing their domain knowledge and experience, often expressingclear economic principles. To name some, demonstrates 101alpha factors tested on the US stock market. Recently, research hasalso been conducted on frameworks that generate such formulaicalphas automatically . These approaches are able tofind loads of new alphas rapidly without human supervision, whilestill maintaining relatively high interpretability compared to themore sophisticated machine learning-alphas.Despite the existing approaches achieving remarkable success,however, they still have disadvantages in different aspects. Machinelearning-based alpha factors are inherently complex and sometimesrequire more complex data other than the price/volume features.",
  "KDD 23, August 610, 2023, Long Beach, CA, USAShuo Yu, Hongyan Xue, Xiang Ao, Feiyang Pan, Jia He, Dandan Tu, and Qing He": "Aditya Ashok, Manimaran Govindarasu, and Venkataramana Ajjarapu. 2018.Online Detection of Stealthy False Data Injection Attacks in Power System StateEstimation. IEEE Trans. Smart Grid 9, 3 (2018), 16361646. Tianqi Chen and Carlos Guestrin. 2016. XGBoost: A Scalable Tree BoostingSystem. In Proceedings of the 22nd ACM SIGKDD International Conference onKnowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17,2016, Balaji Krishnapuram, Mohak Shah, Alexander J. Smola, Charu C. Aggarwal,Dou Shen, and Rajeev Rastogi (Eds.). ACM, 785794. Can Cui, Wei Wang, Meihui Zhang, Gang Chen, Zhaojing Luo, and Beng ChinOoi. 2021. AlphaEvolve: A Learning Framework to Discover Novel Alphas inQuantitative Investment. In SIGMOD 21: International Conference on Managementof Data, Virtual Event, China, June 20-25, 2021, Guoliang Li, Zhanhuai Li, StratosIdreos, and Divesh Srivastava (Eds.). ACM, 22082216. Chirag Deb, Fan Zhang, Junjing Yang, Siew Eang Lee, and Kwok Wei Shah. 2017.A review on time series forecasting techniques for building energy consumption.Renewable and Sustainable Energy Reviews 74 (2017), 902924.",
  "PROBLEM FORMULATION2.1Alpha Factor": "We consider a stock market with stocks in a period of tradingdays in total. On each trading day {1, 2, , }, each stock corresponds to a feature vector R, comprised of rawfeatures such as opening/closing price in the recent days1. Finally,we define an alpha factor as a function mapping feature vectorsof all stocks on a trading day R into alpha values = () R. We will use the word alpha for both an alpha factorand its corresponding values in the following sections.",
  "=1( )2 =1( )2.(1)": "Such value can be calculated on every trading day between analpha and the prediction target. For convenience, we denote the ICvalues between two sets of vectors averaged over all trading daysas (, ) = E [(, )].We use the average IC between an alpha and the return to mea-sure the effectiveness of an alpha factor on a stock trend series = {1,2, , }:",
  "( ) = ( (),).(2)": "As mentioned above, the output of a combination model canbe seen as a mega-alpha, mapping raw inputs into alpha values.Therefore, we denote the combination model as (; F,), whereF = {1, 2, , } is a set of alphas to combine, and denotes theparameters of the combination model. We would like the combina-tion model to be optimal w.r.t. a given alpha set F on the trainingdataset , that is:",
  "Formulaic Alpha": "Formulaic alphas are expressed as mathematical expressions, con-sisting of various operators and the raw input features mentionedbefore. Some examples of the operators are the elementary func-tions (like + and log) operated on one-day data, called cross-section operators, and operators that require data from a seriesof days, called time-series operators (e.g. Min(close, 5) gives the lowest closing price of a stock in the recent 5 days). A list of all theoperators used in our framework is given in Appendix A.Such formulas can be naturally represented by an expression tree,with each non-leaf node representing an operator, and children ofa node representing the operands. To generate such an expression,our model represents the expression tree by its postorder traverse,with the childrens order also defined by the traversing order. Inother words, the model represents a formula as its reverse Polishnotation (RPN). It is easy to see that such notation is unambiguoussince the arities of the operators are all known constants. See for an example of a formulaic alpha expression together with itscorresponding tree and RPN representations.",
  "METHODOLOGY": "As illustrated in , our alpha-mining framework consistsof two main components: 1) the Alpha Combination Model, whichcombines multiple formulaic alphas to achieve optimal performancein prediction, and 2) the RL-based Alpha Generator, which generatesformulaic alphas in the form of a token sequence. The performanceof the Alpha Combination Model is used as the reward signal totrain the RL policy in the Alpha Generator using policy gradient-based algorithms, such as PPO . Repeating this process, thegenerator is continuously trained to generate alphas that boostthe combination model, thereby enhancing the overall predictivepower.",
  "Alpha Combination Model": "Considering the interpretability of the combined mega-alpha, thecombination model itself should also be interpretable. In this paper,we use a linear model to combine the alphas.The values evaluated from different alphas have drastically dif-ferent scales, which might cause problems in the following opti-mization steps. To counter this effect, we centralize and normalizethe alpha values with their average and standard deviation. SincePearsons correlation coefficient is invariant up to linear transfor-mation, this transformation does not affect the performance of thealphas when they are considered separately. Formally, we introducea normalization operator N, that transforms a vector such that itselements have a mean of 0, and the vector has a length of 1:",
  "[N ()] = =1 2 .(4)": "We will omit explicitly writing the N operator for simplicity. Forthe rest of this paper, we will assume that all the () evaluationsand the targets are normalized to have a mean of 0 and a length of1 before subsequent computations. In other words, treat as N and as N ().Given a set of alpha factors F = {1, 2, , } and theirweights = (1,2, ,) R, the combination model isdefined as follows:",
  "(7)": "The proof of this theorem is provided in Appendix B. Noticethat there is no term on the RHS of Equation 7. Once we haveobtained ( ) for each alpha and their pairwise mutual correla-tions ( (), ()), we can then calculate the loss L() solelyusing these terms, saving time on calculating the relatively large in each gradient descent step.Considering time and space complexity, it is impractical to com-bine all generated alphas together, because to calculate mutualcorrelation for each pair of factors we need O(2) evaluations ofmutual IC. The quadratic growth of this makes it expensive to applythe current procedure to a large number of alphas. However, a fewdozen of alphas will suffice for practical uses. To a certain point,more alphas would not bring much more increment in performance,following the law of diminishing returns. We will demonstrate thiseffect in .2.2.After the alpha generator outputs a new alpha, the alpha is firstadded to the candidate alpha set and assigned a random initialweight. Gradient descent is then performed to optimize the weightswith respect to the extended alpha set. We also set a threshold to",
  "return F,;": "limit the size of the alpha set, leaving only the principal alphaswith the largest absolute weight. If the amount of alphas in theextended set exceeds a certain threshold, the least principal alpha isremoved from the set together with its corresponding weight. Thepseudocode of the training procedure is shown in Algorithm 1.",
  "CategoryExamples": "OperatorsCS-Log, CS-Add, TS-Mean, , . . .Features$open, $volume, . . .Constants30, 10, 5, 2, 1, 0.5, 0.01, 0.01, 0.5, 1, 2, 5, 10, 30Time Deltas10, 20, 30, 40, 50Sequence IndicatorBEG(begin), SEP(end of expression) only deal with sequences. To control and evaluate the generationprocess of valid expressions, we model the generation process as anon-stationary Markov Decision Process (MDP). We will describethe various components of the MDP below in the following para-graphs. An overview of the MDP-based Alpha generator is shownin . 3.2.1Tokens. The token is an important abstraction in our frame-work. A token can be any of the operators, the features, or constantvalues. shows some examples of such tokens. For the fulllist of operators, please refer to Section A; for the full list of featureswe have chosen, please refer to .1.1.",
  ": An illustration of our alpha generation framework": "3.2.2State Space. Each state in the MDP corresponds to a sequenceof tokens denoting the currently generated part of the expression.The initial state is always BEG, so a valid state always starts withBEG and is followed by previously chosen tokens. Since we aim forinterpretability of the alphas, and too long of a formula will insteadbe less interpretable, we cap the length threshold of the formulasat 20 tokens.",
  "Action Space. An action is a token that follows the currentstate (generated partial sequence). It is obvious that an arbitrarily": "generated sequence is not guaranteed to be the RPN of an expres-sion, so we only allow a subset of actions to be taken at a specificstate to guarantee the well-formedness of the RPN sequence. Pleaserefer to Appendix C for more details. 3.2.4Dynamics. Given a state and an action, we can obtain thenext state deterministically. The next state is generated by tak-ing the current states corresponding sequence and appending theaction token at the end. 3.2.5Rewards and Returns. The MDP does not give immediaterewards for partially formed sequences. At the end of each episode,if the final state is valid, the state will be parsed to a formulaic func-tion and evaluated in the combination model shown in Algorithm 1.To encourage our generator to generate novel alphas, we will thenevaluate the new combination model with the new alpha added,and use the models performance as the return of this episode. Sincethe reward varies together with the components of the alpha pool,the MDP is non-stationary.Contrary to common RL task settings, for alpha expression gen-eration we do not necessarily want to penalize longer episodes(longer expressions). In fact, longer alphas that perform well areharder to find than shorter ones, due to exponential explosion ofthe search space. Consequently, we set the discount factor as = 1(no discount).",
  "L () = Emin () , clip ( (), 1 , 1 + ) , (8)": "where () = ( | ) ( | ) and is an estimator of the advantagefunction at timestep . Using the importance sampling mechanism,PPO can effectively take the biggest possible improvement whilekeeping the policy in a trust region that avoids accidental perfor-mance collapse.Since our MDP has complicated rules for the legality of actions,an action sampled from the full discrete action distribution pre-dicted by the learned policy is likely to be invalid as mentioned in.2.3. We adopt the Invalid Action Masking mechanism to mask out invalid actions and just sample from the set of validactions.",
  "Network Architecture": "The PPO algorithm requires the agent to have a value network anda policy network. Under our experiment settings, the two networksshare a base LSTM feature extractor that converts token sequencesinto dense vector representations. Separate value and policy headsare attached after the LSTM. The values of hyperparameters aregiven in Appendix D.",
  "Training with policy gradient-basedmethods": "For the task of alpha mining, we do not require the agent to achieverelatively high average returns in each episode, but place moreimportance on the trajectories the agent takes in the whole train-ing process. For this reason, we maintain a pool of alphas withoutresetting between episodes. We run the alpha generation proce-dure mentioned in .2 and optimize the alpha combinationmodel according to .1 repeatedly. In this way, we trainthe policy to continuously generate novel alpha factors that bringimprovement to the overall prediction performance.The proposed alpha mining process is shown in Algorithm 2.Our implementation is publicly available2.",
  "Experiment Settings": "4.1.1Data. Our experiments are conducted on raw data from theChinese A-shares market3. We select 6 raw features as the inputs toour alphas: {open, close, high, low, volume, vwap (volume-weightedaverage price)}. The target is set to be the 20-day return of thestocks, selling/buying at the closing price (Ref(close, 20)/close 1). The dataset is split by date into a training set (2009/01/01 to2018/12/31), a validation set (2019/01/01 to 2019/12/31), and a testset (2020/01/01 to 2021/12/31). In the following experiments, wewill use the constituent stocks of the CSI300 and the CSI500 indicesof China A-shares as the stock set. 4.1.2Compared Methods. To evaluate how well our frameworkperforms against traditional formulaic alpha generation approaches,we implemented two methods that are designed to generate onealpha at a time. GP is a genetic programming model using thealphas IC as the fitness measure to generate expression trees. Thismodel is implemented upon the gplearn4 framework. PPO is a rein-forcement learning method, based on the same PPO algorithmand expression generator, and uses the single alphas IC as theepisode return instead of the combined performance used in ourfull framework.Since only using the top-most alpha to evaluate the frameworksare extremely prone to overfitting on the training data, we alsoconstructed alpha sets with the ones generated by the two singlealpha generators. The same combination model is then appliedto these alpha sets. Note that the generators still emit alphas in aone-by-one manner, and are agnostic to the combination modelsperformance. The first method to construct the set (top) is to simplyselect the top- alphas emitted by the generator with the highestIC on the training set. The second method (filter) is to select thetop- performing alphas with a constraint that any pair of alphafrom the set must not have a mutual IC higher than 0.7.To better evaluate the model performance, we also comparedour approach to several end-to-end machine learning models imple-mented in the open-source library Qlib . The models receive 60days worth of raw features as the input, and are trained to predictthe 20-day returns directly. Note that these models do not generateformulaic alphas. The hyperparameters of these models are setaccording to the benchmarks given by Qlib.",
  "*Optimal combination size in {10, 20, 50, 100}": "IC, the Pearsons correlation coefficient shown in Eq. 1. Rank IC, the rank information coefficient. The rank IC tellshow much the ranks of our alpha values are correlated withthe ranks of future returns. Rank IC is defined by replacingPearsons correlation coefficient with Spearmans correlationcoefficient. The rank IC is just the IC of ranked data, definedas follows:",
  "Main Results": "4.2.1Comparison across all alpha generators. To answer Q1, wefirst compare our framework against several other alpha-miningmethods and direct stock trend forecasting baselines, including PPO,GP, MLP, LightGBM, and XGBoost. Experiments are conducted onCSI300 and CSI500 stocks respectively.The results are shown in . Our framework is able toachieve the highest IC and rank IC across all the methods we com-pare to. Note that the framework is only explicitly optimized againstthe IC metric. The non-formulaic alpha models come in the secondtier. The baseline formulaic alpha generators perform poorly on thetest set, especially the RL-based ones. The reinforcement learningagent, when optimized only against single-alpha IC, is prone tofalling into local optima and thus overfitting on the training set,and basically stops searching for new alphas after a certain amountof steps. On the other hand, the GP-based methods maintaininga large population can avoid the same problem, but still cannotproduce alphas that are synergistic when used together. The results",
  "also show that the filtering techniques cannot solve the synergyproblem consistently either": "4.2.2Comparison of formulaic generators with varying pool ca-pacity. To answer Q2, we study the four baseline formulaic al-pha generators more extensively, and compare them to our pro-posed framework. The models are evaluated under pool sizes of {1, 10, 20, 50, 100}. The results are shown in .Compared to the baseline method PPO_filter, our method directlyuses the combination models performance as the reward to newlygenerated alphas. This leads to a substantial improvement whenthe pool size increases, meaning that our method can producealpha sets with great synergy. Our method shows scalability forpool size: even when the pool size is large enough, it can stillcontinuously find synergistic alphas that boost the performanceover the existing pool. Conversely, the combined performance ofthe alphas generated by other approaches barely improves uponthe case with just the top alpha, meaning that these alpha factorshave poor synergy. Furthermore, the ability to control the rewardof individual expressions under a certain alpha pool configurationis granted by the flexibility of the RL scheme. The GP scheme ofmaintaining a large population at the same time does not work wellwith fine-grained fitness value control.Also, we can see that for the CSI500 dataset, GP_filter performsworse than GP_top on the IC metric when the pool size increases.This phenomenon demonstrates that the traditionally used mutual-IC filtering is not always effective, answering the question Q3.",
  "Case Study": "shows an example combination of 10 alphas generated byour framework, evaluated on the CSI300 constituent stock set. Mostof the alpha pairs in this specific set have mutual IC values over0.7. Previous work considered this to be too high for theindividual alphas to be regarded as diverse, yet these alphas are",
  "Weighted Combination0.0511": "able to work well in a synergistic manner. For example, the alphas#2 and #6 have a mutual IC of 0.9746, thus traditionally consideredtoo similar to be useful cooperatively. However, the combination0.093172 0.071636 achieves an IC of 0.0458 on the test set, evenhigher than the sum of the respective ICs, showing the synergyeffect.Also, although alpha #1 only has an IC of 0.0011, it still playsa vital role in the final combination. Once we remove alpha #1from the combination and re-train the combination weights on theremaining set, the combinations IC drops to merely 0.0447. Thetwo observations above show that neither the single alpha IC northe mutual IC between alpha pairs is a good indicator of how wellthe combined alpha would perform, answering Q3.One possible explanation for these phenomena is that: Althoughtraditionally these alphas are similar due to the high mutual IC,some linear combinations of the alphas could point to a completelydifferent direction from the original ones. Consider two unit vectorsin a linear space. The more similar these two vectors are, the lesssimilar either of these vectors is to the difference between the twovectors, since the difference vector approaches to be perpendicularto either of the original vectors as the vectors get closer.",
  "Investment Simulation": "To demonstrate the effectiveness of our factors in more realistic in-vesting settings, we use a simple investment strategy and conductedbacktests in the testing period (2020/01/01 to 2021/12/31) on theCSI300 dataset. We use a simple top-/drop- strategy to simulatethe investment: On each trading day, we first sort the alpha valuesof the stocks, and then select the top stocks in that sorted list. Weevenly invest across the stocks if possible, but restrict the strategyto only buy/sell at most stocks on each day to reduce excessivetrading costs. In our experiment, is set to 50 and to 5.We recorded the net worth of the respective strategies in thetesting period, of which a line chart is shown in . Althoughour framework does not explicitly optimize towards the absolutereturns, the framework still performs well in the backtest. Ourframework is able to gain the most profit compared to the othermethods.",
  "RELATED WORK": "Formulaic alphas. The search space of formulaic alphas is enor-mous, due to the large amount of possible operators and featuresto choose from. To our best knowledge, all notable former workuses genetic programming to explore this huge search space. augmented the gplearn library with formulaic-alpha-specific time-series operators, upon which an alpha-mining framework is built. further improved the framework to also mine alphas with non-linear relations with the returns by using mutual information as thefitness measure. used mutual IC to filter out alphas that are toosimilar to existing ones, improving the diversity of resulting alphasets. PCA is carried out on the alpha values for reducing the algo-rithmic complexity of computing the mutual ICs, and various othertricks are also applied to aid the evolution process. AlphaEvolve evolves new alphas upon existing ones. It allows combinations ofmuch more complex operations (for example matrix-wise computa-tions), and uses computation graphs instead of trees to represent thealphas. This leads to more sophisticated alphas and better predictionaccuracy, although at the risk of lowering the alphas interpretabil-ity. Mutual IC is also used as a measure of alpha synergy in thiswork.Machine learning-based alphas. The development of deeplearning in recent years has brought about various new ideas onhow to accurately model stock trends. Early work on stock trendforecasting treats the movement of each stock as a separate timeseries, and applies time series models like LSTM or Transformer to the data. Specific network structures catered to stock fore-casting like the SFM which uses a DFT-like mechanism havealso been developed. Recently, research has also been conducted onmethods to integrate non-standard data with the time series. REST fuses multi-granular time series data together with historicalevent data to model the market as a whole. HIST utilizes con-cept graphs on top of the regular time series data to model sharedcommonness between future trends of various stock groups. Onespecific type of machine learning-based model is also worth men-tioning. Decision tree models, notably XGBoost , LightGBM ,etc., are often considered interpretable, and they could also achieverelatively good performance on stock trend forecasting tasks. How-ever, whether a decision tree with extremely complex structure is",
  ": Backtest results on CSI 300. The lines track the net worth of simulated trading agents utilizing the various alpha-mining approaches": "considered interpretable is at least questionable. When these treemodels are applied to raw stock data, the high dimensionality ofinput only exacerbates the aforementioned problem. Our formulaicalphas use operators that apply to the input data in a more struc-tured manner, making them more easily interpretable by curiousinvestors.Symbolic regression. Symbolic regression (SR) concerns theproblem of discovering relations between variables representedin closed-form mathematical formulas. SR problems are differentfrom our problem settings that there always exists a groundtruthformula that precisely describes the data points in an SR problem,while stock market trends are far too complex to be expressed in thespace of formulaic alphas. Nevertheless, there remain similaritiesbetween the two fields since similar techniques can be used forthe expression generator and the optimization procedure. sug-gested using a custom neural network whose activation functionsare symbolic operators to solve the SR problem. proposed anovel symbolic regression framework based on an autoregressiveexpression generator. The generator is optimized using an aug-mented version of the policy gradient algorithm that values the topperformance of the agent more than the average. developed amethod similar to , but also introduced GP into the optimizationloop, seeding the GP population with RL outputs. applied thelanguage model pretraining scheme to symbolic regression, train-ing a generative autoregressive language model of expressionson a large dataset of synthetic expressions.Discussions. Although the term formulaic alpha is often tieddown to investing, the concept of simple and interpretable formu-laic predictors that could be combined into more expressive modelsis not limited to quantitative trading scenarios. Our frameworkcan be adapted to solve other time-series forecasting problems, forexample, energy consumption prediction , anomaly detection, biomedical settings , etc. In addition, we chose the linearcombination model in this paper for its simplicity. Meanwhile, in theory, other types of interpretable combination models, for exam-ple, decision trees can also be integrated into our framework. Inthat sense, providing these combination models with these featuresexpressed in relatively straightforward formulas might help provideinvestigators with more insights into how the models come to thefinal results.",
  "CONCLUSION": "In this paper, we proposed a new framework for generating inter-pretable formulaic alphas to aid investors in quantitative trading.We proposed to directly use the performance boost brought aboutby the newly added alpha to the existing alpha combination as themetric for alpha synergy. As a result, our framework can producesets of alphas that could cooperate satisfactorily with a combinationmodel, notwithstanding the actual form of the combination model.For the model to explore the vast search space of formulaic alphasmore effectively, we also formulated the alpha-searching procedureas an MDP and applied reinforcement learning techniques to op-timize the alpha generator. Extensive experiments are conductedto demonstrate that the performance of our framework surpassesthose of all previous formulaic alpha-mining approaches, and thatour method can also perform well under more realistic tradingsettings. The research work was supported by the National Key Research andDevelopment Program of China under Grant No. 2022YFC3303302,the National Natural Science Foundation of China under GrantNo.61976204. Xiang Ao is also supported by the Project of YouthInnovation Promotion Association CAS, Beijing Nova ProgramZ201100006820062.",
  "Sepp Hochreiter and Jrgen Schmidhuber. 1997. Long Short-Term Memory.Neural Comput. 9, 8 (1997), 17351780": "Shengyi Huang and Santiago Ontan. 2022. A Closer Look at Invalid ActionMasking in Policy Gradient Algorithms. In Proceedings of the Thirty-Fifth Inter-national Florida Artificial Intelligence Research Society Conference, FLAIRS 2022,Hutchinson Island, Jensen Beach, Florida, USA, May 15-18, 2022, Roman Bartk,Fazel Keshtkar, and Michael Franklin (Eds.). Zura Kakushadze. 2016. 101 Formulaic Alphas. arXiv:1601.00991 Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma,Qiwei Ye, and Tie-Yan Liu. 2017. LightGBM: A Highly Efficient Gradient BoostingDecision Tree. In Advances in Neural Information Processing Systems 30: AnnualConference on Neural Information Processing Systems 2017, December 4-9, 2017,Long Beach, CA, USA, Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M.Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (Eds.). 31463154.",
  "Xiaoming Lin, Ye Chen, Ziyu Li, and Kang He. 2019.Stock Alpha MiningBased On Genetic Algorithm.Technical Report. Huatai Securities ResearchCenter": "Qian Liu, Yihong Chen, Bei Chen, Jian-Guang Lou, Zixuan Chen, Bin Zhou,and Dongmei Zhang. 2020. You Impress Me: Dialogue Generation via MutualPersona Perception. In Proceedings of the 58th Annual Meeting of the Association forComputational Linguistics, ACL 2020, Online, July 5-10, 2020, Dan Jurafsky, JoyceChai, Natalie Schluter, and Joel R. Tetreault (Eds.). Association for ComputationalLinguistics, 14171427. T. Nathan Mundhenk, Mikel Landajuela, Ruben Glatt, Daniel M. Faissol, andBrenden K. Petersen. 2021. Symbolic Regression via Neural-Guided GeneticProgramming Population Seeding. (2021). arXiv:2111.00053 Brenden K. Petersen, Mikel Landajuela, T. Nathan Mundhenk, Cludio PrataSantiago, Sookyung Kim, and Joanne Taery Kim. 2021. Deep symbolic regression:Recovering mathematical expressions from data via risk-seeking policy gradients.In 9th International Conference on Learning Representations, ICLR 2021, VirtualEvent, Austria, May 3-7, 2021.",
  "Edward E Qian. 2007. Quantitative equity portfolio management: modern tech-niques and applications. Chapman and Hall/CRC": "Subham S. Sahoo, Christoph H. Lampert, and Georg Martius. 2018. LearningEquations for Extrapolation and Control. In Proceedings of the 35th InternationalConference on Machine Learning, ICML 2018, Stockholmsmssan, Stockholm, Swe-den, July 10-15, 2018 (Proceedings of Machine Learning Research, Vol. 80), Jennifer G.Dy and Andreas Krause (Eds.). PMLR, 44394447. Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan,Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis,Thore Graepel, et al. 2020. Mastering atari, go, chess and shogi by planning witha learned model. Nature 588, 7839 (2020), 604609.",
  "arXiv:2106.14131": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All youNeed. In Advances in Neural Information Processing Systems 30: Annual Conferenceon Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach,CA, USA, Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach,Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (Eds.). 59986008. Zhicheng Wang, Biwei Huang, Shikui Tu, Kun Zhang, and Lei Xu. 2021. Deep-Trader: A Deep Reinforcement Learning Approach for Risk-Return BalancedPortfolio Management with Market Conditions Embedding. In Thirty-Fifth AAAIConference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Inno-vative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium onEducational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February2-9, 2021. AAAI Press, 643650. Wentao Xu, Weiqing Liu, Lewen Wang, Yingce Xia, Jiang Bian, Jian Yin, andTie-Yan Liu. 2021. HIST: A Graph-based Framework for Stock Trend Forecastingvia Mining Concept-Oriented Shared Information. (2021). arXiv:2110.13716 Wentao Xu, Weiqing Liu, Chang Xu, Jiang Bian, Jian Yin, and Tie-Yan Liu.2021. REST: Relational Event-driven Stock Trend Forecasting. In WWW 21:The Web Conference 2021, Virtual Event / Ljubljana, Slovenia, April 19-23, 2021,Jure Leskovec, Marko Grobelnik, Marc Najork, Jie Tang, and Leila Zia (Eds.).ACM / IW3C2, 110.",
  "Xiao Yang, Weiqing Liu, Dong Zhou, Jiang Bian, and Tie-Yan Liu. 2020. Qlib: AnAI-oriented Quantitative Investment Platform. (2020). arXiv:2009.11189": "Liheng Zhang, Charu C. Aggarwal, and Guo-Jun Qi. 2017. Stock Price Predictionvia Discovering Multi-Frequency Trading Patterns. In Proceedings of the 23rdACM SIGKDD International Conference on Knowledge Discovery and Data Mining,Halifax, NS, Canada, August 13 - 17, 2017. ACM, 21412149. Tianping Zhang, Yuanqi Li, Yifei Jin, and Jian Li. 2020. AutoAlpha: an EfficientHierarchical Evolutionary Algorithm for Mining Alpha Factors in QuantitativeInvestment. arXiv preprint arXiv:2002.08245 (2020).",
  "ALIST OF OPERATORS": "There are four types of operators used in our framework. The fourtypes break down into two groups: cross-section operators, andtime-series operators. Cross-section operators (denoted with CSin the table) only deal with data on the current trading day, whiletime-series operators (denoted with TS) take into considerationdata from a consecutive period of time. Each of the two groupsfurther separates into unary (denoted with U) and binary (denotedwith B) operators that apply to one or two series respectively.",
  "+ , , , /CSBArithmetic operators.Greater(,), Less(,)CSBThe larger/smaller one of the two values": "Ref(,)TSUThe expression evaluated at days before the current day.Mean(,), Med(,), Sum(,)TSUThe mean/median/sum value of the expression evaluated on the recent days.Std(,), Var(,)TSUThe standard deviation/variance of the expression evaluated on recent days.Max(,), Min(,)TSUThe maximum/minimum value of the expression evaluated on the recent days.Mad(,)TSUThe mean absolute deviation E [| E [] |] of the expression evaluated on therecent days.Delta(,)TSUThe relative difference of compared to days ago, Ref(,).WMA(,), EMA(,)TSUWeighted moving average and exponential moving average of the expression evaluated on the recent days.",
  "A multi-token expression should not be equivalent to a con-stant;": "The special SEP token (end of expression) is only allowedwhen the generated sequence is already a valid RPN.For example, when the stack (state) is currently [$open, 0.5],we can choose the Add token (a binary operator), building anexpression Add($open, 0.5). Meanwhile, the operator Log isnot allowed here because Log will take 0.5 and Log(0.5) is aconstant; similarly, the operator TS-Mean is also invalid becauseMean($open, 0.5) is illegal.",
  "C.2Semantic Legality": "Some expressions with correct forms might still fail to evaluate dueto more constraints imposed by the operators. For example, thelogarithm operator cannot be applied to a non-positive value. Thiskind of semantic invalidity is not directly detected by the procedurementioned in the last section. In our experiments, these expressionsare given the reward of -1 (the minimum value of Pearsons corre-lation coefficient) to discourage the agent from generating theseexpressions."
}