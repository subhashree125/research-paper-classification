{
  "Abstract": "In this paper we present a multi-adapter retrieval augmented gen-eration system (MARAGS) for Metas Comprehensive RAG (CRAG)competition for KDD CUP 2024. CRAG is a question answeringdataset contains 3 different subtasks aimed at realistic question andanswering RAG related tasks, with a diverse set of question topics,question types, time dynamic answers, and questions featuringentities of varying popularity.Our system follows a standard setup for web based RAG, whichuses processed web pages to provide context for an LLM to pro-duce generations, while also querying API endpoints for additionalinformation. MARAGS also utilizes multiple different adapters tosolve the various requirements for these tasks with a standardcross-encoder model for ranking candidate passages relevant foranswering the question. Our system achieved 2nd place for Task 1as well as 3rd place on Task 2.",
  "Introduction": "Retrieval augmented generation (RAG) has been a popular approachfor question answering systems for some time , although recentlyhas become a very popular approach for a wide range of tasks dueto the zero-shot capabilities of large language models (LLMs) withan appropriate prompt and access to the relevant context for thetask. Despite the existence of numerous question answering bench-marks, many do accurately reflect the diverse usage of current RAGsystems. Thus, tracking both the efficacy of certain RAG architec-tures as well as tracking process remains difficult. The CRAG benchmark aims to resolve this with 3 different subtasks represent-ing realistic RAG usage scenarios. The final key element of theCRAG benchmark is its scoring metric which explicitly punisheshallucinations. With the rising capabilities of LLMs, increasingly Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from Cup 24, August 2529, Barcelona, Spain their outputs are taken at face value, despite the known issue of hal-lucinations. This has led to high profile incidents causing concernwith their use . The CRAG score aims to punish hallucinatedanswers and encourages returning missing answers, equivalent toreturning \"i dont know\" from the model, by giving scores of 1, 0,and -1 to correct, missing, and hallucinated answers respectively.To address these various tasks, we train individual adapters foreach of the various tasks, as well as API call generation required foraccessing information in the mock API. This approach allows us touse a single LLama 3 in memory while swapping out adaptersbased on the current needs.",
  "Related Works": "The initial approach of Lewis et al. showed the benefits of pro-viding additional text context for seq2seq models for NLP tasks thatare knowledge intensive. Using BART, they were able to improvequestion answering tasks using dual biencoders for retrieval andtraining the model jointly, without the need for knowing whichdocuments were relevant.Adapters have become increasingly used since introduced byHoulsby et al. . LoRa has become a popular adapter approach,particularly for LLMs as they have grown substantially larger inrecent years. The use of adapters allows modifying a models out-put without training the entire network, which substantially saveson VRAM memory when training. Hu et al. discovered thatwhen replacing a dot product between large vectors with an in-termediate dot product of a much lower rank vector, the impactson performance were minimal while further reducing the trainingparameters required.Finally, Stickland and Murray produced a multi-adaptermodel based on BERT, an approach that our system follows. Inparticular for the GLUE benchmark, which is comprised ofmultiple datasets, they showed that simply training a task specificadapter per dataset, they could improve the average performance by0.5 points for BERT, while only introducing 10% more parameters.",
  "CRAG Dataset": "CRAG is a question answering dataset aimed at providing a realistictask to benchmark RAG systems as they are used in practice witha diverse set of questions, including 8 distinct question types and5 distinct domains. Additionally, two sources of diversity whichpose difficulty for LLMs are how dynamic a questions answeris and the popularity of the topic of the question. As shown inthe baseline systems, real-time answers pose a challenge for RAGsystems and they similarly struggle when the topic of the questionis less common (referred to as \"torso\" and \"tail\" questions).",
  "KDD Cup 24, August 2529, Barcelona, SpainMitchell DeHaven": "Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Her-moso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam,Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo,Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, NormanCheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent,Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, PierreRoux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, QianLiang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, RaghuNayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang,Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta,Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, SaurabhVerma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay,Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang,Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chin-tala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, SudarshanGovindaprasad, Sumit Gupta, Sungmin Cho, Sunny Virk, Suraj Subramanian,Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, ThiloKohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, TimothyChou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vi-jai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru,Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, WenwenJiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaofang Wang, XiaojianWu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia,Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang,Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick,Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. 2024. The Llama 3 Herd ofModels. arXiv:2407.21783 [cs.AI]",
  "Task 1": "For the first task the system must process 5 candidate HTML doc-uments for generating answers, reflecting a standard web-basedRAG application. A caveat is that the 5 candidates are sampled fromthe top-10 relevant documents retrieved from web search. Thus,there is no guarantee that the relevant information for answeringthe question is actually found within the top 5 documents. Thiscreates an interesting challenge for hallucinations, as in some casesthe answer should be easily generated by the model without thecontext from retrieved documents.",
  "Task 2": "Task 2 reflects a more sophisticated RAG scenario, where the systemis provided with the same 5 HTML documents from before, howeverit now has access to a knowledge graph, accessible via a REST API.The system must determine which API endpoint to call, with thecorrect arguments, to retrieve additional relevant context from theknowledge graph.",
  "Task 3": "Finally, Task 3 represents an extension of Task 2, where the systemhas access to both HTML and the knowledge graph API. However,in this task, the number of HTML documents to be processed is 50.This task is meant to measure both the computational efficiencyof the approach as well as its ability to filter large amounts ofpotentially irrelevant information.",
  "MARAGS Pipeline4.1Webpage Processing": "Our webpage processing pipeline utilizes BeautifulSoup4 forgenerating candidate segments from the HTML documents pro-vided to the system. A common difficulty with RAG systems is de-termining a process for segmenting documents into smaller chunksto narrow the candidates to relevant sections and also reducing the length of the text sent to the model, given that a single documentcould exceed the context window of a model.In our case, we utilize the structure of the HTML to provide thesegmentation. We traverse the tree structure of the parse HTMLwith breadth-first search. Any time the length of the text containedwithin a node (which includes all of the text of its descendants)is less than 2000 characters, we treat that as a segment. If a leafnode is reached with greater than 2000 characters, the text is spliton the white space and into as many segments are needed suchthat each segment is under the threshold. The segment length wasdetermined via inspection of HTML documents and their associatedsegmentation, thus future work could treat this as a hyperparameterand tune for performance.",
  "API Call Generation": "For Task 2 and 3, the knowledge graph mock API is available to beused for gathering additional information. The difficulty, however,is not only determining which API endpoint is the most appropriate,but also the proper arguments and their formatting for getting validresults from the API.Each API endpoint was transformed to a Python function withrelevant documentation describing the purpose of the endpoint,the arguments, and what the endpoint returned. Each function alsohas an associated formatting function, which takes the returnedJSON and converts it into segmented strings. The doc strings foreach Python function are used to provide additional information tohelp guide the model on which one is the most appropriate to use.For training a model to generate API calls with associated argu-ments, we use LoRa to train one of the several adapters we usewith Llama 3 8B. For generating the target string for training, wefirst use Llama 3 to generate an initial prediction for the API call.Any initial prediction that successfully calls a function is retained asa target, regardless of whether or not the relevant information forthe question is contained in the returned JSON. Initial predictions",
  "Ensemble (mean rank)0.308-0.128": "that fail to make a successful call are inspected and manually cor-rected if the correct function call is clear from the initial predictionand the question. Again, the manually modified targets are evalu-ated only on successfully calling an endpoint, though not validatingthat the relevant information is returned by the API. Any questionwhere the target cannot be quickly modified is changed to a targetof \"None\".We acknowledge that this approach to annotation is not opti-mal, as it likely results in successful, but incorrectly selected APIendpoint calls. However, manually annotating each question todetermine the correct API call and validating the returned infor-mation were indeed relevant would have been too time consuminggiven the size of the dataset.",
  "Candidate Ranking": "For candidate ranking, we attempted 4 different candidate rank-ing approaches. We utilized TF-IDF, a biencoder, cross-encoder,and an ensemble of the mentioned approaches (using mean rankas the ranking metric). Our TF-IDF implementation is based onScikit-Learn . The biencoder and cross-encoder are from theSentenceTransformer library, specifically the \"multi-qa-MiniLM-L6-cos-v1\" 1 and \"ms-marco-MiniLM-L-6-v2\" 2 respectively.Evaluating candidate ranking in isolation is difficult, as relevantinformation is not labeled, so using the system accuracy and CRAGscore is the most straight forward way to compare differences ineach system. However, to test the various systems, we use the baseLlama-3 8B model with no adapters for each retrieval approach anduse the accuracy metric to determine the best performing approach.We use accuracy instead of CRAG at this stage for ranking, aswe think this is a better representation of how often the relevantinformation retrieved. For a test set, we randomly select 500 samplesfrom the Task 1 dataset.The results of this experiment are shown in . From theresults, the cross-encoder is the best performing system, thus weused it for our retriever. We suspect that with proper tuning TF-IDF and ensembling would be much more performant overall, butas mentioned running extensive experimentation is difficult as itrequires LLM generation to get an overall accuracy score. Using anLLM to label passages as relevant or not is a possible approach toallow for tuning of just the retriever, however we did not explorethis.Despite the cross-encoder being the most computationally ex-pensive approach, we found it to be fast enough for processing thecandidates in the required 30 seconds per sample. In the case of",
  "Retrieval Augmented Generation": "Finally, with ranked candidates, we use Llama 3 8B to augmentgeneration with the relevant context for the question. We ran ex-periments with 2 different prompt structures, the primary differencebetween them being the ordering of the question and context.Our initial prompt structure started with the question first, thenall of the retrieved candidates prior to the Llama model response,however we noticed that often due to how much context wouldbe provided, the model would occasionally forget the questionbeing asked. For example a question like \"What is the elevationof Honolulu?\" would result in an answer of \"Honolulu populationis 343,421\", indicating the model remembered the subject of thequestion, but incorrectly answered with the population, rather thanelevation. Our subsequent prompt structure placed the questionafter the context, which resolved the issue.For training Llama 3, we trained LoRa models for each taskindividually. Given the penalization of hallucinations in the scoringmetric, we try to take steps to mitigate further hallucinations dueto fine-tuning, as it has been observed that fine-tuning LLMs canbe a source of further hallucinations . This likely applies to RAGsystems in cases where the expected answer is not answerable giventhe context provided, i.e. no relevant information is given and thequestion is not answerable without further context, yet the model istrained to output the target answer regardless. Thus for our trainingsetup, we first relabel the target for training samples in cases whereour candidate retrieval system likely does not provide the correctinformation and Llama does not know the answer without thatinformation. We use the provided dev set for training, with the 500set sample used for retrieval comparison treated as our holdout set.Our initial approach for determining which training samplesneed relabeling has been explored previously . A common andsimple approach to filter/relabel incorrectly3 labeled samples is touse a particular samples training loss after training to the pointof over-fitting. High loss examples after over-fitting likely indicateexamples that are incorrectly labeled and thus can be filtered out.Not all samples with high loss will be incorrect labels, insteadsimply being hard examples, yet typically the benefit of disposingof incorrectly labeled samples outweighs ignoring difficult ones.Initial experiments, however, indicated that this method did notwork well on finance based questions. Further analysis would berequired for a more definitive answer, though we suspect that this",
  ": CRAG Score results on the test dataset calculated via manual assessment": "is due to the fact that the loss in hallucinated answers when deal-ing with numeric outputs is likely less than typical string outputs.For example, with a question \"What was Apples closing price to-day?\", with a hypothetical correct answer \"$203.51\", a prediction of\"$204.52\" would likely not result in filtering via this method. Com-pare that with a question such as \"Which movie won Best Picture in1990?\" with an answer of \"Driving Miss Daisy\" and a prediction of\"Dancing with Wolves\", the loss will be comparatively much higher.We instead determine these samples by first running the systemwith the base Llama 3 model, with a prompt indicating to alwaysproduce a prediction for each of the 4 candidate retrieval approachesmentioned previously. We use GPT-4o to evaluate whether any ofthe generated answers are correct. If any are correct, the originallabel is retained for the training, otherwise \"i dont know\" is usedas the target label. In the case of false premise questions, we alwaysretain the original label as the target label. We repeat this processfor each task, given that each has access to different data sources,to generate a training dataset for each LoRa adapter.We use the llama-recipes repository for training the LoRaadapters, utilizing the default LoRa configuration. The only modifi-cations were changing the LoRa rank from 8 to 256 and increasingthe weight decay from 0.01 to 1.0.We demonstrate the effectiveness of relabeling in . We ran3 different answer generation setups for the 500 sample Task 1 holdout set we created. The first is an unmodified Llama 3 8B model,the second is a LoRa model using the original targets, and the finala LoRa model with relabeled targets. As shown, using the originaltargets provides the best accuracy, but also worsens hallucinations",
  "Results": "As part of the competition, a manual evaluation was conductedon user submissions. The automatic evaluation throughout wasdependent on scoring via GPT-3.5 Turbo, given that correct usersubmissions may not have been exact matches to the expected an-swer. However, issues such as prompt injection still pose problemsfor automatic evaluation via LLMs. The results of our system acrossthe various aspects of the dataset are shown in . As can beseen by the results, our system suffers many of the problems thedataset is meant to expose with most RAG systems.Similar to the baseline systems for CRAG, finance was the mostchallenging domain. The exact reason warrants further analysis,though contributing factors likely include LLMs known issues withnumber processing and simple mathematics and the fact that muchonline finance data is often not stored in plain text, but rathervisualizations such as graphs.Dynamism proves to be the most challenging question catego-rization, with model performance steadily decreasing as a questionbecomes more dynamic. Real-time questions prove to be the mostchallenging question category of any of the breakouts. Our promptstructure did not include any meta-data provided by the HTMLdocuments, such as publish data or access date, which likely wouldhave improved performance on dynamic questions, although likelynot significantly.The performance difference between head, torso, and tail ques-tions appeared less substantial than our original expectations, though",
  "MARAGS: A Multi-Adapter System for Multi-Task Retrieval Augmented Generation Question AnsweringKDD Cup 24, August 2529, Barcelona, Spain": "clearly performance drops off as popularity falls. Interestingly, Task3 underperforms the other tasks in head, torso, and tail. We sus-pect that including substantially more search results includes over-lapping entities / subjects, at which point conflicting informationwould be difficult to resolve.Finally, the most interesting results in the question type resultsare the false premise category. Our system was able to achievescores similar to the SOTA systems featured in the CRAG paper,despite obviously being a much smaller system overall. Interestingly,the false premise questions were the only type where our trainingsetup always kept the original target label, rather than mappingthe target to \"i dont know\".",
  "Future Work": "Observations we had during the competition were instances ofcatastrophic forgetting due to our attempts to reduce hallucinations.For instance, the question \"Who has had a longer musical career,Sharika or Machine Gun Kelly?\" is answerable by Llama withoutcontext, simply based on the knowledge it has of the two artists.However, after LoRa training, questions like this and others wereoften answered with \"i dont know\" in cases where the answer wasnot discoverable in the retrieved information. Methods to preventthis is something we are interested in pursuing in future work.Additionally, we hope to explore larger Llama models, 70B+,in the future for this task. We were unable to get the 70B modelrunning in the competition compute environments, so did not spendmuch time looking at larger models. However, it is very likelymoving to larger models would provide a substantial improvementover the 8B model.",
  "Conclusion": "In this work we presented MARAGS, a multi-adapter solution tothe CRAG dataset. We demonstrated the effectiveness of trainingindividual LoRa adapters for the 4 tasks in the pipeline, specificallyAPI call generation and Task 1,2, and 3 answer generation. CRAGpresents a variety of different tasks and questions to allow the track-ing the progress of various methods used to build RAG systems. Thepenalization of hallucinations is a unique and important feature asfuture AI systems become increasingly common throughout society,as hallucinations hurt user trust in these systems. We discussedour methods for reducing these hallucinations, but they are notwithout cost, as in some cases the model fails to output previouslyknown knowledge. Clearly the importance of balancing these twofactors is a key to leveraging LLMs to their full potential, while alsoimproving user trust.",
  "Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ah-mad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela": "Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sra-vankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, AurelienRodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, BinhTang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra,Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, CorinneWong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song,Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Maha-jan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, EhabAlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic,Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, GraemeNail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, HannahKorevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, IsabelKloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, JanaVranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, JenniferBillock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, JiawenLiu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca,Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, KartikeyaUpasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini,Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary,Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin,Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira,Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, MarcinKardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, MikeLewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi,Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Olivier Duchenne,Onur elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, PeterWeng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura,Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ra-mon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, RohitGirdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, RossTaylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabas-appa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, ShaoliangNie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhos-ale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, StenSootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Her-man, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, TobiasSpeckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, VibhorGupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, VishVogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers,Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia,Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, YiwenSong, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, ZhengYan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aaron Grattafiori, AbhaJain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, AhuvaGoldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, AlexeiBaevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu,Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton,Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, ArkabandhuChowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yaz-dan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd,Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock,Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo,Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim,Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, ChristophFeichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, DannyWyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh,Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, EdwardDowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, EmilyWood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun,Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzmn,Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, GadaBadeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory Sizov,Guangyi, Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Han-nah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, HenryAspegren, Hunter Goldman, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, ItaiGat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-BaptisteGaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein,Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill,Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, KaiWu, Kam Hou U, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, KatayounZand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, KunHuang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg,Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu,Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, ManishBhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie,Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally,Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov,",
  "Language Models. In International Conference on Learning Representations": "Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,Naman Goyal, Heinrich Kttler, Mike Lewis, Wen-tau Yih, Tim Rocktschel,Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation forknowledge-intensive NLP tasks. In Proceedings of the 34th International Conferenceon Neural Information Processing Systems (Vancouver, BC, Canada) (NIPS 20).Curran Associates Inc., Red Hook, NY, USA, Article 793, 16 pages. Meta. 2024. llama-recipes. F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M.Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cour-napeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: MachineLearning in Python. Journal of Machine Learning Research 12 (2011), 28252830. Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddingsusing Siamese BERT-Networks. In Proceedings of the 2019 Conference on Em-pirical Methods in Natural Language Processing. Association for ComputationalLinguistics.",
  "Yisen Wang, Weiyang Liu, Xingjun Ma, James Bailey, Hongyuan Zha, Le Song,and Shu-Tao Xia. 2018. Iterative Learning with Open-set Noisy Labels. In CVPR": "Xiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla, Xiangsen Chen, SajalChoudhary, Rongze Daniel Gui, Ziran Will Jiang, Ziyu Jiang, Lingkun Kong, BrianMoran, Jiaqi Wang, Yifan Ethan Xu, An Yan, Chenyu Yang, Eting Yuan, HanwenZha, Nan Tang, Lei Chen, Nicolas Scheffer, Yue Liu, Nirav Shah, Rakesh Wanga,Anuj Kumar, Wen tau Yih, and Xin Luna Dong. 2024. CRAG ComprehensiveRAG Benchmark. arXiv:2406.04744 [cs.CL]"
}