{
  "ABSTRACT": "Memory replay based techniques have shown great success forcontinual learning with incrementally accumulated Euclidean data.Directly applying them to continually expanding networks, how-ever, leads to the potential memory explosion problem due to theneed to buffer representative nodes and their associated topologicalneighborhood structures. To this end, we systematically analyzethe key challenges in the memory explosion problem, and presenta general framework, i.e., Parameter Decoupled Graph Neural Net-works (PDGNNs) with Topology-aware Embedding Memory (TEM),to tackle this issue. The proposed framework not only reduces thememory space complexity from O() to O() 1, but also fullyutilizes the topological information for memory replay. Specifically,PDGNNs decouple trainable parameters from the computation ego-subnetwork via Topology-aware Embeddings (TEs), which compressego-subnetworks into compact vectors (i.e., TEs) to reduce the mem-ory consumption. Based on this framework, we discover a uniquepseudo-training effect in continual learning on expanding networksand this effect motivates us to develop a novel coverage maximiza-tion sampling strategy that can enhance the performance with atight memory budget. Thorough empirical studies demonstratethat, by tackling the memory explosion problem and incorporatingtopological information into memory replay, PDGNNs with TEMsignificantly outperform state-of-the-art techniques, especially inthe challenging class-incremental setting.",
  "INTRODUCTION": "Traditional machine learning techniques for networks typicallyassume the types of nodes and their associated edges to be static 2.However, real-world networks often expand constantly with emerg-ing new types of nodes and their associated edges. Consequently,models trained incrementally on the new node types may experi-ence catastrophic forgetting (severe performance degradation) onthe old ones as shown in . Targeting this challenge, con-tinual learning on expanding networks has attractedincreasingly more attention recently. It exhibits enormous valuein various practical applications, especially in the case where net-works are relatively large, and retraining a new model over theentire network is computationally infeasible. For instance, in a so-cial network, a community detection model has to keep adapting itsparameters based on nodes from newly emerged communities; in acitation network, a document classifier needs to continuously up-date its parameters to distinguish the documents of newly emergedresearch fields.Memory replay , which stores representative ex-amples in a buffer to retrain the model and maintain its perfor-mance over existing tasks, exhibits great success in preventingcatastrophic forgetting for various continual learning tasks, e.g.,computer vision and reinforcement learning . Directlyapplying memory replay to network data with the popular mes-sage passing neural networks (MPNNs, the general framework formost GNNs) , however, could give rise to the memoryexplosion problem because the necessity to consider the explicittopological information of target nodes. Specifically, due to themessage passing over the topological connections in networks,retraining an -layer GNN (, left) with buffered nodeswould require storing O() nodes (the number of edgesis not counted yet) in the buffer, where is the average node de-gree. Take the Reddit dataset as an example, its average nodedegree is 492, and the buffer size will easily be intractable evenwith a 2-layer GNN. To resolve this issue, Experience Replay basedGNN (ER-GNN) stores representative input nodes (i.e., nodeattributes) in the buffer but ignores the topological information( a). Feature graph network (FGN) implicitly encodesnode proximity with the inner products between the features of",
  "KDD 24, August 2529, 2024, Barcelona, SpainXikun Zhang, Dongjin Song, Yixin Chen, and Dacheng Tao": "Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Nazanin Alipourfard, KristinaLerman, Hrayr Harutyunyan, Greg Ver Steeg, and Aram Galstyan. 2019. Mixhop:Higher-order graph convolutional architectures via sparsified neighborhoodmixing. In ICML. PMLR, 2129. Kian Ahrabian, Yishi Xu, Yingxue Zhang, Jiapeng Wu, Yuening Wang, andMark Coates. 2021. Structure aware experience replay for incremental learningin graph-based recommender systems. In Proceedings of the 30th ACM CIKM.28322836.",
  "RELATED WORKS2.1Continual Learning & Continual Learningon Expanding Networks": "Existing continual learning (CL) approaches can be categorizedinto regularization, memory replay, and parameter isolation basedmethods. Regularization based methods aim to prevent drasticmodification to parameters that are important for previous tasks. Parameter isolationmethods adaptively allocate new parameters for the new tasks toprotect the ones for the previous tasks . Mem-ory replay based methods store and replay representative data fromprevious tasks when learning new tasks .Recently, CL on expanding networks attracts increasingly moreattention due to its practical importance . Existing methods includeregularization ones like topology-aware weight preserving (TWP) that preserves crucial topologies, parameter isolation methodslike HPNs that select different parameters for different tasks,and memory replay methods like ER-GNN , SSM , andSEM-curvature that store representative nodes or sparsifiedcomputation ego-subnetworks. Our work is also memory based andits key advantage is the capability to preserve complete topologicalinformation with reduced space complexity, which shows signifi-cant superiority in class-IL setting (.4). Finally, it is worthhighlighting the difference between CL on expanding networksand some relevant research areas. First, dynamic graph learning focuses on the temporal dynamicswith all previous data being accessible. In contrast, CL on expandingnetworks aims to alleviate forgetting, therefore the previous datais inaccessible. Second, few-shot graph learning targets fast adaptation to new tasks. In training, few-shot learningmodels can access all previous tasks (unavailable in CL). In test-ing, few-shot learning models need to be fine-tuned on the testclasses, while the CL models are tested on existing tasks withoutfine-tuning.",
  "GNNs & Reservoir Computing": "Graph Neural Networks (GNNs) are deep learning models designedto generate representations for graph data, which typically inter-leave the neighborhood aggregation and node feature transforma-tion to extract the topological features . GNNs without interleaving the neigh-borhood aggregation and node feature transformation have beendeveloped to reduce the computation complexity and increase thescalability . For example, SimpleGraph Convolution (SGC) removes the non-linear activationfrom GCN and only keeps one neighborhood aggregation and",
  "Topology-aware Embedding Memory for Continual Learning on Expanding NetworksKDD 24, August 2529, 2024, Barcelona, Spain": ": Visualization of the node embeddings of different classes of Reddit, after learning 1, 10, and 20 tasks. From the top tothe bottom, we show the results of Fine-tune, ER-GNN, and PDGNNs-TEM. Each color corresponds to a class. intractable memory usage on dense networks like Reddit, and thestrategy to buffer gradients also incurs high memory cost (GEM).SSM could significantly reduce memory consumption with thesparsification strategy. Both PDGNNs-TEM and ER-GNN are highlyefficient in terms of memory space usage. While PDGNNs-TEMexhibits superior performance compared to ER-GNN.",
  "PARAMETER DECOUPLED GNNS WITHTOPOLOGY-AWARE EMBEDDING MEMORY": "In this section, we first introduce the notations, and then explainthe technical challenge of applying memory replay techniquesto GNNs. Targeting the challenge, we introduce PDGNNs withTopology-aware Embedding Memory (TEM). Finally, inspired bytheoretical findings of the pseduo-training effect, we develop thecoverage maximization sampling to enhance the performance whenthe memory budget is tight, which has shown its effectiveness inour empirical study. All detailed proofs are provided in Appendix A.",
  "Preliminaries": "Continual learning on expanding networks is formulated as learn-ing node representations on a sequence of subnetworks (tasks):S = {G1, G2, ..., G }. Each G (i.e., -th task) contains several newcategories of nodes in the overall network, and is associated witha node set V and an edge set E, which is represented as theadjacency matrix A R|V ||V |. V will be used to denote anarbitrary node set in the following. The degree of a node refersto the number of edges connected to it. In practice, A is often",
  "Memory Replay Meets GNNs": "In traditional continual learning, a model f(;) parameterized by is sequentially trained on tasks. Each task ( {1, ..., }) corre-sponds to a dataset D = {(x, y)=1}. To avoid forgetting, memoryreplay based methods store representative data from the old tasksin a buffer B. When learning new tasks. A common approach toutilize B is through an auxiliary loss:",
  ",(1)": "where (, ) denotes the loss function, and 0 balances thecontribution of the old data. Instead of directly minimizing L,the buffer B may also be used in other ways to prevent forget-ting . In these applications, the space complexity of a buffercontaining examples is O().However, to capture the topological information, GNNs obtainthe representation of a node based on a computation ego-subnetworksurrounding . We exemplify it with the popular MPNN framework, which updates the hidden node representations at the + 1-thlayer as:",
  "h = MPNN(x, G; ),(3)": "where Gcontains the -hop neighbors (N()), MPNN(, ; )is the composition of all M (, , ;M ) and U (, ;U ) at differentlayers. Since N() typically contains O() nodes, replaying nodes requires storing O() nodes (the edges are not countedyet), where is the average degree. Therefore, the buffer size will beeasily intractable in practice (e.g. the example of Reddit dataset in In-troduction), and directly storing the computation ego-subnetworksfor memory replay is infeasible for GNNs.",
  "Parameter Decoupled GNNs with TEM": "As we discussed earlier, the key challenge of applying memory re-play to network data is to preserve the rich topological informationof the computation ego-subnetworks with potentially unboundedsizes. Therefore, a natural resolution is to preserve the crucial topo-logical information with a compact vector such that the memoryconsumption is tractable. Formally, the desired subnetwork repre-sentation can be defined as Topology-aware Embedding (TE). Definition 1 (Topology-aware embedding). Given a specificGNN parameterized with and an input G, an embedding vectore is a topology-aware embedding for Gwith respect to this GNN,if optimizing with Gor e for this specific GNN are equivalent,i.e. e contains all necessary topological information of Gfortraining this GNN. However, TEs cannot be directly derived from the MPNNs due totheir interleaved neighborhood aggregation and feature transforma-tions. According to .2, whenever the trainable parametersget updated, recalculating the representation of a node requiresall nodes and edges in G. To resolve this issue, we formulate theParameter Decoupled Graph Neural Networks (PDGNNs) frame-work, which decouples the trainable parameters from the individualnodes/edges. PDGNNs may not be the only feasible framework toderive TEs, but is the first attempt and is empirically effective. GivenG, the prediction of node with PDGNNs consists of two steps.First, the topological information of Gis encoded into an em-bedding e via the function f () without trainable parameters(instantiations of f () are detailed in .4).",
  "y = f (e;).(5)": "With the formulations above, e derived in Eq. (4) clearly satisfiesthe requirements of TE (Definition 1). Specifically, since the train-able parameters acts on e instead of any individual node/edge,optimizing the model parameters with either e or Gareequivalent. Therefore, to retrain the model, the memory bufferonly needs to store TEs instead of the original computation ego-subnetworks, which reduces the space complexity from O()to O(). We name the buffer to store the TEs as Topology-aware",
  "T EM = T EMsampler({e | V },),(6)": "where sampler(, ) is the adopted sampling strategy to populate thebuffer, denotes the set union, and is the budget. According to theexperimental results (.3), as long as T EM is maintained,PDGNNs-TEM can perform reasonably well with different choicesof sampler(, ), including the random sampling. Nevertheless, in.7, based on the theoretical insights in .5, wepropose a novel sampling strategy to better populate T EM whenthe memory budget is tight, which is empirically verified to beeffective in .3. Besides, Equation (6) assumes that all dataof the current task are presented concurrently. In practice, thedata of a task may come in multiple batches (e.g., nodes come inbatches on large networks), and the buffer update have to be slightlymodified by either storing sizes of the computational ego-networksand recalculating the multinomial distribution or adopting reservoirsampling. For task with network G, the loss with T EM thenbecomes:",
  "(7)": "balances the contribution of the data from the current task and thememory, and is typically manually chosen in traditional continuallearning works. However, on network data, we adopt a differentstrategy to re-scale the losses according to the class sizes to counterthe bias from the severe class imbalance, which cannot be handledon networks by directly balancing the datasets.",
  "Vx (,; A),(8)": "where (, ; A) denotes the strategy for computation ego-subnetworkconstruction and determines how would the model capture the topo-logical information. Equation (8) describes the operation on eachnode. In practice, Equation (8) could be implemented as matrixmultiplication to generate TEs of a set of nodes V in parallel, i.e.EV = XV, where each entry , = (,; A). EV R|V| isthe concatenation of all TEs (e R), and XV R|V| is the con-catenation of all node feature vectors x R. In our experiments,we adopt three representative strategies. The first strategy (S1) is a basic version of message passing and can be formulated as = A. The second strategy (S2) considers balancing thecontribution of neighborhood information from different hops via ahyperparameter , i.e. = 1",
  ",and ({, ,}) = 42": "adopt a strategy (S3) that adjusts the contribution of the neigh-bors based on PageRank , i.e. = (1 ) A + I, in which also balances the contribution of the neighborhood information.The linear formulation of f () (Equation (8)) yields bothpromising experimental results () and instructive theoreti-cal results (.5, and 3.7). Equation (8) is also highly efficientespecially for large networks due to the absence of iterative neigh-borhood aggregations. But f () can also take non-linear formswith more complex mappings. For example, we can also adopt areservoir computing module to instantiate f (), which isformulated as,",
  "Pseudo-training Effects of TEs": "In traditional continual learning on independent data without ex-plicit topology, replaying an example x (e.g., an image) only re-inforces the prediction of itself. In this subsection, we introducethe pseudo-training effect, which implies that training PDGNNswith e of node also influences the predictions of the other nodesin G, based on which we develop a novel sampling strategy tofurther boost the performance with a tight memory budget. Theorem 1 (Pseudo-training). Given a node , its computationego-subnetwork G, the TE e, and label y (suppose belongs toclass , i.e. y, = 1), then training PDGNNs with e has the followingtwo properties:1. It is equivalent to training PDGNNs with each node in Gwith Gbeing a pseudo computation ego-subnetwork and y being apseudo label, where the contribution of x (via Equation 8) is re-scaled",
  "(,)E1(y = y),(10)": "where E is the edge set, y is the label of node , and 1() is theindicator function . For any network, the homophily ratio isbetween 0 and 1. For each computation ego-subnetwork, whenthe homophily ratio is high, the neighboring nodes tend to sharelabels with the center node, and the pseudo training would bebeneficial for the performance. Many real-world networks, i.e. thesocial network and citation networks, tend to have high homophilyratios, and pseudo training will bring much benefit,.In our work, the homophily ratio of the 4 network datasetsare: CoraFull-CL (0.567), Arxiv-CL (0.655), OGB-Products (0.807),Reddit-CL (0.755). These datasets cover the ones with high ho-mophily (OGB-Products and Reddit), as well as the ones with lowerhomophily.",
  "# tasks30202023": "When learning on more heterophilous networks (homophilyratio close to 0) () is required to be constructed speciallyconstructed. Heterophilous network learning is largely differentfrom homophilous network learning, and requires different GNNdesigns . Therefore, () should also be instantiatedto be suitable for heterophilous networks. The key difference of het-erophilous network learning is that the nodes belonging to the sameclasses are not likely to be connected, and GNNs should be designedto separately process the proximal neighbors with similar infor-mation and distal neighbors with irrelevant information, or onlyaggregate information from the proximal neighbors .For example, MixHop encodes neighbors from different hopsseparately. A given computation ego-subnetwork will be dividedinto different hops. For each hop, the model generates a separateembedding. Finally, the embeddings of different hops are concate-nated as the final TE. H2GCN only aggregates higher-orderneighbors that are proximal to the center node.In other words, via constructing () to be suitable for het-erophilous networks, the neighborhood aggregation is still con-ducted on the proximal nodes, and so is the pseudo-training. In thisway, the pseudo-training will still benefit the performance.",
  "Coverage Maximization Sampling": "Following the above subsection, TEs with larger computation ego-subnetworks are preferred to be stored. To quantify the size of thecomputation ego-subnetworks, we formally define the coverageratio of the selected TEs as the nodes covered by their computationego-subnetworks versus the total nodes in the network ().Since a TE uniquely corresponds to a node, we may use node andTE interchangeably.",
  "i.e., the ratio of nodes of the entire (training) network covered by thecomputation ego-subnetworks of the selected nodes (TEs)": "To maximize (T EM), a naive approach is to first select the TEwith the largest coverage ratio, and then iteratively incorporate TEthat increases (T EM) the most. However, this requires comput-ing (T EM) for all candidate TEs at each iteration, which is timeconsuming especially on large networks. Besides, certain random-ness is also desired for the diversity of T EM. Therefore, we pro-pose to sample TEs based on their coverage ratio. Specifically, in task, the probability of sampling node V is = ({})V ({}) . Then the nodes in V are sampled according to { | V }without replacement, as shown in Algorithm 1. In experiments, wedemonstrate the correlation between the coverage ratio and theperformance, which verifies the benefits revealed in .5",
  "EXPERIMENTS": "In this section, we aim to answer the following research questions:RQ1: Whether PDGNNs-TEM works well with a reasonable buffersize? RQ2: Does coverage maximization sampling ensure a highercoverage ratio and better performance when the memory budgetis tight? RQ3: Whether our theoretical results can be reflected inexperiments? RQ4: Whether PDGNNs-TEM can outperform thestate-of-the-art methods in both class-IL and task-IL scenarios?RQ5: How to interpret the learned node embedding under contin-ual learning setting. Due to the space limitations, only the mostprominent results are presented in the main content. For simplicity,PDGNNs-TEM will be denoted as PDGNNs in this section. All codesare available at github.com/imZHANGxikun/PDGNNs.",
  "Experimental Setup and Model Evaluation": "Continual learning setting and model evaluation. During train-ing, a model is trained on a task sequence. During testing, the modelis tested on all learned tasks. Class-IL requires a model to classify agiven node by picking a class from all learned classes (more chal-lenging), while task-IL only requires the model to distinguish theclasses within each task. For model evaluation, the most thoroughmetric is the accuracy matrix M R , where M, denotesthe accuracy on task after learning task . The learning dynamicscan be reflected with average accuracy (AA) over all learnt tasks",
  "spective of forgetting, 1=1 M, M,": "1| = 2, ...,. To use a singlenumeric value for evaluation, the AA and AF after learning all tasks will be used. These metrics are widely adopted in continuallearning works , although the names are differentin different works. We repeat all experiments 5 times on one NvidiaTitan Xp GPU. All results are reported with average performanceand standard deviations.Baselines and model settings. Our baselines for continual learn-ing on expanding networks include Experience Replay based GNN(ER-GNN) , Topology-aware Weight Preserving (TWP) ,Sparsified Subgraph Memory (SSM) , and Subgraph EpisodicMemory (SEM) . Milestone works for Euclidean data but alsoapplicable to GNNs include Elastic Weight Consolidation (EWC), Learning without Forgetting (LwF) , Gradient EpisodicMemory (GEM) , and Memory Aware Synapses (MAS) ), arealso adopted. HPNs is designed to work under a stricter task-IL setting, and cannot be properly incorporated for comparison.",
  "PDGNNs81.90.1-3.90.153.20.2-14.70.294.70.4-3.00.473.90.1-10.90.2": "The results of the baselines are adopted from the original works. Besides, joint training (without forgetting problem)and fine-tune (without continual learning technique) are adoptedas the upper and lower bound on the performance. We instantiatef (;) as a multi-layer perceptron (MLP). All methods includingf (;) of PDGNNs are set as 2-layer with 256 hidden dimen-sions, and in .3 is set as 2 for consistency. As detailed in.3, f () is chosen as strategy S1 (.4).",
  "Studies on the Buffer Size & Performance vs.Coverage Ratio (RQ1, 2, and 3)": "In , based on PDGNNs, we compare the proposed coveragemaximization sampling with uniform sampling and mean of feature(MoF) sampling in terms of coverage ratios and performance whenthe buffer size (ratio of the dataset) varies from 0.0002 to 0.4 onthe OGB-Arxiv dataset. Our proposed coverage maximization sam-pling achieves a superior coverage ratio, which indeed enhancesthe performance when the memory budget is tight. In real-worldapplications, a tight memory budget is a very common situation,making the coverage maximization sampling a favorable choice.We also notice that the average accuracy for coverage maximizationsampling is positively related to the coverage ratio in general, whichis consistent with the Theorem 1. also demonstrates the high memory efficiency of TEM.No matter which sampling strategy is used, the performance canreach 50% average accuracy (AA) with only 5% data buffered. In.5, we provide the comparison of the space consumption",
  "Class-IL and Task-IL Scenarios (RQ4)": "Class-IL Scenario. As shown in , under the class-IL sce-nario, PDGNNs significantly outperform the baselines and are evencomparable to joint training on all 4 public datasets. The learningdynamics are shown in . Since the curve of PDGNNs isvery close to that of joint training, we conclude that the forgettingproblem is nearly eliminated by PDGNNs. In and ,PDGNNs sometimes outperform joint training. The reasons aretwo-fold. First, PDGNNs learn the tasks sequentially while jointtraining optimizes the model for all tasks simultaneously, resultingin different optimization difficulties . Second, when learning newtasks, joint training accesses all previous data that may be noisy,",
  "PDGNNs94.60.10.61.089.80.4-0.00.598.90.0-0.50.093.50.5-2.10.1": "while replaying the representative TEs may help filter out noise.To thoroughly understand different methods, we visualize the ac-curacy matrices of 4 representative methods, including PDGNNs(memory replay with topological information), ER-GNN (memoryreplay without topological information), LwF (relatively satisfyingperformance without memory buffer), and Fine-tune (without con-tinual learning technique), in . Compared to the baselines,PDGNNs maintain stable performance on each task even thoughnew tasks are continuously learned.Task-IL Scenario. The comparison results under the task-IL sce-nario are shown in . We can observe that PDGNNs still out-perform most baselines on all different datasets and is comparable",
  "Memory Consumption Comparison (RQ1)": "Memory-replay based methods outperform other methods, but alsoconsume additional memory space. In this subsection, we comparethe space consumption of different memory designs to demonstratethe memory efficiency of PDGNNs-TEM. The final memory con-sumption (measured by the number of float32 values) after learningeach entire dataset is shown in . As a reference, the memoryconsumption of storing full computation ego-subnetwork is alsocalculated. According to , storing full subnetworks costs",
  "Interpretation of Node Embeddings (RQ5)": "To interpret the learning process of PDGNNs-TEM, we visualizethe node embeddings of different classes with t-SNE whilelearning on a task sequence of 20 tasks over the Reddit dataset.In , besides PDGNNs-TEM that replay data with topologi-cal information, we also include two representative baselines forcomparison, i.e., ER-GNN to show how the lack of topological in-formation may affect the node embeddings, and Fine-tune to showthe results without any continual learning technique. As shown in, PDGNNs-TEM can well separate the nodes from differentclasses even when node types of nodes are continuously been in-volved (in new tasks). In contrast, for ER-GNN and Fine-tune, theboundaries of different classes are less clear, especially when moretasks are continuously learned.",
  "CONCLUSION": "In this work, we propose a general framework of Parameter De-coupled Graph Neural Networks (PDGNNs) with Topology-awareEmbedding Memory (TEM) for continual learning on expandingnetworks. Based on the Topology-aware Embeddings (TEs), wereduce the space complexity of the memory buffer from O() toO(), which enables PDGNNs to fully utilize the explicit topologi-cal information sampled from the previous tasks for retraining. Wealso discover and theoretically analyze the pseudo-training effect ofTEs. The theoretical findings inspire us to develop the coverage max-imization sampling strategy, which has been demonstrated to behighly efficient when the memory budget is tight. Finally, thoroughempirical studies, including comparison with the state-of-the-artmethods in both class-IL and task-IL continual learning scenarios,demonstrate the effectiveness of PDGNNs with TEM.",
  "Lucas Caccia, Eugene Belilovsky, Massimo Caccia, and Joelle Pineau. 2020.Online learned continual compression with adaptive quantization modules. InICML. PMLR, 12401250": "Jie Cai, Xin Wang, Chaoyu Guan, Yateng Tang, Jin Xu, Bin Zhong, and WenwuZhu. 2022. Multimodal continual graph learning with neural architecture search.In Proceedings of the ACM Web Conference 2022. 12921300. Antonio Carta, Andrea Cossu, Federico Errica, and Davide Bacciu. 2021. Cata-strophic Forgetting in Deep Graph Networks: an Introductory Benchmark forGraph Classification. arXiv preprint arXiv:2103.11750 (2021). Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajanthan, and Philip HSTorr. 2018. Riemannian walk for incremental learning: Understanding forgettingand intransigence. In Proceedings of the ECCV. 532547.",
  "Ting Chen, Song Bian, and Yizhou Sun. 2019. Are powerful graph neural netsnecessary? a dissection on graph classification. arXiv preprint arXiv:1905.04579(2019)": "Wei Chen, Yajun Wang, and Siyu Yang. 2009. Efficient influence maximization insocial networks. In Proceedings of the 15th ACM SIGKDD international conferenceon Knowledge discovery and data mining. 199208. Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh.2019. Cluster-gcn: An efficient algorithm for training deep and large graphconvolutional networks. In Proceedings of the 25th ACM SIGKDD internationalconference on knowledge discovery & data mining. 257266.",
  "Aristotelis Chrysakis and Marie-Francine Moens. 2020. Online continual learn-ing from imbalanced data. In ICML. PMLR, 19521961": "Weilin Cong, Rana Forsati, Mahmut Kandemir, and Mehrdad Mahdavi. 2020.Minimal variance sampling with provable guarantees for fast training of graphneural networks. In Proceedings of the 26th ACM SIGKDD International Confer-ence on Knowledge Discovery & Data Mining. 13931403. Yuanning Cui, Yuxin Wang, Zequn Sun, Wenqiang Liu, Yiqiao Jiang, KexinHan, and Wei Hu. 2023. Lifelong embedding learning and transfer for growingknowledge graphs. In Proceedings of AAAI, Vol. 37. 42174224.",
  "Marcel Hoffmann, Lukas Galke, and Ansgar Scherp. 2023. Open-World LifelongGraph Learning. In 2023 IJCNN. 19": "Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, BowenLiu, Michele Catasta, and Jure Leskovec. 2020. Open graph benchmark: Datasetsfor machine learning on graphs. NeurIPS 33 (2020), 2211822133. Yushan Jiang, Zijie Pan, Xikun Zhang, Sahil Garg, Anderson Schneider, YuriyNevmyvaka, and Dongjin Song. 2024. Empowering Time Series Analysis withLarge Language Models: A Survey. arXiv preprint arXiv:2402.03182 (2024).",
  "Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How powerfulare graph neural networks? arXiv preprint arXiv:1810.00826 (2018)": "Yishi Xu, Yingxue Zhang, Wei Guo, Huifeng Guo, Ruiming Tang, and MarkCoates. 2020. Graphsail: Graph structure aware incremental learning for recom-mender systems. In Proceedings of the 29th ACM CIKM. 28612868. Yujun Yan, Milad Hashemi, Kevin Swersky, Yaoqing Yang, and Danai Koutra.2022. Two sides of the same coin: Heterophily and oversmoothing in graphconvolutional neural networks. In 2022 IEEE ICDM. IEEE, 12871292.",
  "Xiaocheng Yang, Mingyu Yan, Shirui Pan, Xiaochun Ye, and Dongrui Fan. 2023.Simple and efficient heterogeneous graph neural network. In Proceedings ofAAAI, Vol. 37. 1081610824": "Huaxiu Yao, Chuxu Zhang, Ying Wei, Meng Jiang, Suhang Wang, JunzhouHuang, Nitesh Chawla, and Zhenhui Li. 2020. Graph few-shot learning viaknowledge transfer. In Proceedings of AAAI, Vol. 34. 66566663. Jaehong Yoon, Saehoon Kim, Eunho Yang, and Sung Ju Hwang. 2020. Scalableand order-robust continual learning with additive parameter decomposition. InInternational Conference on Learning Representation."
}