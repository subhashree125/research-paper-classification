{
  "Yuan FangSingapore Management": ": (Left) A projection of a black dragon onto a 2D grid graph. The aim is to find a geodesic (shortest-path) from the source node(boundary) to the target node on the dragons projection on the grid graph. (Center) The geodesic distance map from the source node. (Right)The geodesic distance map is used to find the actual geodesic.",
  "ABSTRACT": "Geodesic distances on manifolds have numerous applications inimage processing, computer graphics and computer vision. In thiswork, we introduce an approach called LGGD (Learned GeneralizedGeodesic Distances). This method involves generating node featuresby learning a generalized geodesic distance function through atraining pipeline that incorporates training data, graph topologyand the node content features. The strength of this method lies inthe proven robustness of the generalized geodesic distances to noiseand outliers. Our contributions encompass improved performancein node classification tasks, competitive results with state-of-the-artmethods on real-world graph datasets, the demonstration of thelearnability of parameters within the generalized geodesic equationon graph, and dynamic inclusion of new labels.",
  "ACM Reference Format:Amitoz Azad and Yuan Fang. 2024. A Learned Generalized Geodesic DistanceFunction-Based Approach for Node Feature Augmentation on Graphs. In": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08",
  "INTRODUCTION": "In recent times, there has been a growing interest in data augmen-tation techniques for graphs . The primary motivation behindaugmenting graphs is to improve model performance by enhancingthe quality of the graph data through some form of denoising. Real-world graphs, which depict the underlying relationships betweennodes, often suffer from noise due to various factors such as fakeconnections , arbitrary edge thresholds , limited or partialobservations , adversarial attacks , and more. These factorscollectively render the graphs suboptimal for graph learning tasks.To address these issues, researchers have been exploring graphstructural and node feature augmentation techniques that aim togenerate improved graphs .Geodesic distances () has found numerous applications incomputer vision, ranging from calculating shortest-path distanceson discrete surfaces , to shape-from-shading , median axisor skeleton extraction , graph classification , statistical datadepth , noise removal, and segmentation .Recently the authors in studied a generalized geodesic dis-tance function equation on graphs Eq. (7), which they referred toas the graph -eikonal equation. The authors provided both theo-retical and experimental evidence to demonstrate that, unlike thegeodesic (shortest-path) distance function on graphs (as can becomputed with classic Dijkstra algorithm, Sec. 2.6), the generalizedgeodesic distance function is provably more robust (less affected bychange) when the graph is subjected to the addition of corruptededges, especially for = 1 in Eq. (7). Contributions. Motivated by the proven robustness of the gener-alized geodesic distance function to edge corruptions (see )and outliers , in this work, we focus on generating node featurevectors using learned generalized geodesic distances on a graph",
  "(b) Robustness of geodesic (shortest-path) distance map Eq. (9)": ": The represents the number of random corrupted edges added to a given graph. The graph construction: 20,000 points (nodes) wererandomly sampled from a unit ball in 2. An -neighborhood unweighted graph was constructed using these sampled points with = 0.05. Allpoints within distance of the boundary of the unit ball are considered boundary nodes. Colors represent the distance from the boundary,with red indicating the boundary where the distance function is zero, and yellow indicating the maximum distance. for node classification task. This learning of generalized geodesicdistances is achieved by formulating the generalized geodesic dis-tance function Eq. (7) as a time-dependent problem Eq. (8). Thistime-dependent version allows us not only to solve Eq. (7), but italso enables gradient-based learning of the generalized geodesic dis-tance function using node content features (such as bag-of-wordsfor citation networks).Since the generalized geodesic distance function Eq. (7) only con-siders the graph topology, the generated node features are robust,but they are purely topological, as it does not consider the originalnode content features. We propose a hybrid model that learns thegeneralized geodesic distance function using gradient descent, andgenerates robust node features which not only consider the graphtopology but also take into account the original node content fea-tures (). Using these learned generalized geodesic distancesat different time values ( in Eq. (8)) as node features improves theperformance of the backbone model, and makes it competitive withstate-of-the-art augmentation methods ().We refer to the node feature generated through learning gener-alized geodesic distances as LGGD\" (Learned Generalized GeodesicDistances) (). To summarize our key contributions:",
  "Notation": "A weighted graph, denoted as = (, ,), is defined by a finiteset of nodes in and a finite set of edges in , where each edge (, )connects nodes and . The weights of the graph are determinedby a weight function : , and the set of edges isdetermined by the non-zero weights: = {(, )|(, ) 0}. Ifthere is no edge between and , then (, ) = 0. We represent theset of nodes neighboring node as (), where () signifiesthat node is in the neighborhood of node , i.e., () = { |(, ) }. In this paper, we consider symmetric graphs, meaningthat(, ) = (,), and the presence of an edge (, ) is equivalentto the presence of its reverse (,). The degree of a node , denotedas (), is computed as the sum of weights for all nodes in itsneighborhood: () = () (, ).Let ( ) be a Hilbert space comprised of real-valued functionsdefined on the graphs nodes. A function : of ( )characterizes a signal associated with each node, assigning a realvalue () to every node in . Similarly, let () denote a Hilbertspace encompassing real-valued functions defined on the edges ofthe graph.",
  "()( )() = 1, \\ 0 () = 0, 0(7)": "Here, () represents the generalized geodesic distance function.() is the potential function (Sec. 2.7). 0 represents the boundarynodes (training set) from which the distances have to be calculated(hence () = 0 at boundary).One way to solve the above equation is by employing the fastmarching or fast iterative methods. Alternatively, one cansolve it numerically by considering a time-dependent version of it:",
  "(,) = ()( )(,) + 1, \\ 0 (,) = 0, 0 (, 0) = 0() (8)": "At steady-state ( ), this equation provides the solution to thegeneralized geodesic distance function Eq. (7). Note the introduc-tion of an extra variable time and the initial condition (, 0). Adefault choice of initializing is to let the distance be zero on theboundary nodes and infinity (a large positive number) on the rest(just like the initialization in Dijkstras algorithm to find the distancemap from a source node). In this work, we utilize this time-dependent version to generate generalized geodesic distance features for everynode for different time () values. As we will see, this formulationprovides us with the capability to incorporate the original nodecontent features, and generate learned generalized geodesic dis-tances for different time () values. This is achieved by employingbackpropagation through an ODE solver , letting (, 0) be anMLP (multi-layer perceptron) function of node content features,and then learning the MLP function through gradient descent.",
  "Solving Eq. (8) with an ODE Solver": "The Eq. (8) can be solved using an ODE solver like Torchdiffeq .Various numerical schemes, consisting of fixed step or adaptivestep sizes, can be employed from Torchdiffeq. Although Eq. (8) is aPDE on a graph, it can be viewed as a system of coupled ODEs onthe graph. This is because the spatial domain is already discretized,and the spatial derivatives at each node can be viewed as finitedifferences (similar to the Finite Difference Method).It must be pointed out that Eq. (8) is a vector-valued equation.Since the training set (boundary nodes) consists of nodes from different classes, Eq. (8) is solved for each class for every node ,thus providing (,) as the solution. Here, (,) represents thegeneralized geodesic distance of node at time from the boundarynodes of class.",
  "Learning Eq. (8) with an ODE Solver": "Torchdiffeq not only allows us to solve a differential equation nu-merically using various numerical schemes, but it also enables usto learn the parameters of the differential equation through back-propagation using the adjoint sensitivity method .To learn the parameters of the differential equation, first, a seg-ment of the differential equation needs to be converted into a lossfunction. This loss function is then minimized using a gradientdescent based technique via the adjoint sensitivity method. In thecase of Eq. (8), to learn () and 0(), the boundary condition (,) = 0 can be employed to construct a loss ( (,), 0) where 0. The approach of converting the boundary condition to a lossfunction is very similar to the inspiring work done in PINNS .",
  "Connection with Dijkstra": "We now explain the connection between Eq. (7), and the celebratedDijkstra algorithm. The Dijkstra algorithm can be used to findthe geodesic (shortest-path) distance map on a graph, which thencan be further used to find the actual shortest-path between theboundary node and a target node. We call Eq. (7) as generalizedgeodesic distance function, because the geodesic (shortest-path) dis-tance function (as can be obtained using Dijkstra) is a special caseof Eq. (7), as we will see shortly.From dynamic programming perspective, for a unweighted graph,the functional equation for Dijkstra algorithm, satisfies the follow-ing shortest-path distance function from the boundary set 0:",
  "Choosing Potential Function ()": "The potential function (), often plays a crucial role in the gen-eralized geodesic distance function of a graph. For instance, intasks related to image processing, such as segmentation, it is oftencontingent on the image gradient at a pixel. This dependency al-lows distances to be shorter in the smooth regions of an image andlonger in the non-smooth regions. In this work, drawing inspira-tion from , we opt to associate the potential function with thelocal density at a node. By making the potential function dependenton local density, generalized geodesic distances are shortened indenser regions and lengthened in sparser areas. This brings gener-alized geodesic distances of nodes within a cluster closer togetherwhile pushing generalized geodesic distances of nodes in differentclusters further apart. We take the node degree, (), as a measureof density at a node . And set () = (), where is a hyperpa-rameter searched within the range of -1 to 0. Later in Sec. 4.2, wewill see that how gradient based learning of this function results inslight boost in the performance.",
  "Generalized Geodesic Distances as Features": "This subsection describes the approach to generate generalizedgeodesic distance as node features with no gradient based learn-ing. To use generalized geodesic distances as node features, weinitially start by generating features using Eq. (8), where we con-sider the training set as the boundary nodes and use the defaultinitial condition in Eq. (8) (). So for the initial condition( (, 0) = 0(), ) we have:",
  ", \\ 00, 0(10)": "For every node , Eq. (8) is solved for different classes in theboundary nodes (training set) for different time () values. Thefinal feature at each node is a set { (,)}, where {1, 2, ..}and {1, 2, .. }. Here (,) represents the generalized geodesicdistance of node at time from the boundary nodes of class.These generalized geodesic distance features are assigned to thenodes (replacing the original node content features) and providedas input to a backbone GNN, along with the graph structure, for thenode classification task (). We refer to the distance featuresgenerated using this approach as GGD.",
  ": Learned Generalized Geodesic Distances as Features": "refer to them as learned generalized geodesic distances (LGGD)because they are generated after gradient-based learning of theparameters of Eq. (8), as we will see shortly. depicts theproposed architecture for generating learned generalized geodesicdistance features, which also takes the original node content fea-tures into account. This architecture can be viewed as a mechanismfor converting the original node content features into learned gen-eralized geodesic distance features.It is essentially a two-step approach involving Pipeline1 andPipeline2 (). Pipeline1 is different from Pipeline0 of theprevious case (), as now we have converted the boundarycondition into a loss function and added an MLP function as theinitial condition () in Solver1, which takes into account theoriginal node content features. Unlike Pipeline0, Pipeline1 is notused to generate the distance features; rather, Pipeline1 is taskedwith learning the weights of the MLP function and optionally learn-ing the parameters (()) in Solver1. Node features are input intothe MLP, and the output serves as the initial distances (, 0), pro-vided to Solver1.The loss function ( (,), 0) within Pipeline1 plays a crucialrole in facilitating the learning process. Specifically, it enforces thatthe self-distances of all nodes on the boundary from the boundary(the training set) should remain zero, as required by the boundary condition in Eq. (8). Its worth noting that the boundary condi-tion should not be directly incorporated into Solver1, as doing sowould result in the loss remaining perpetually at zero, hinderingthe learning process (loss minimization using gradient descent).Pipeline2 serves as the feature-generating pipeline. It is similarto Pipeline0 (no learning case, ), as both of them function asfeature-generating pipelines. The difference between Pipeline2 andPipeline0 is that the former uses the learned parameters MLP(nodefeat) and () from Pipeline1 to generate the features, whereas thelatter uses the default initialization. Observe how the learned MLPfunction from Pipeline1 is used to construct the initial condition (, 0) = 0() of Solver2:",
  "MLP( .), \\ 00, 0(11)": "This construction ensures that self-distances of the boundary nodesare zero from the very beginning = 0.Pipeline2 can be deployed separately once the MLP functionand the parameters () are learned and saved from Pipeline1.Pipeline2s purpose is to generate learned generalized geodesic dis-tance features for different time steps, which are then concatenatedand provided as input to the backbone model for evaluation on the",
  "Dynamic Inclusion of New Labels": "In Pipeline2, after training the backbone model, one can dynam-ically include the new labels by simply updating the boundarycondition ( (,) = 0, 0) and initial condition ( (, 0) =0(), ) in Eq. (8). Then, one can use the same learned pa-rameters (MLP function and () from Pipeline1) to run Solver2with updated conditions and generate new features for differenttime steps. These features can subsequently be used as input forthe backbone model that has already been trained.So let 1 be the set of new incoming labels, the new boundarycondition would become: (,) = 0, (0 1). And the newinitial condition (, 0) would be:",
  "EXPERIMENTS & RESULTS": "In this section, we walk through the research questions pertainingto our proposed models, detail the experiments conducted to answerthem, and analyze the results. For all the experiments, we kept = 1in Eq. (8) as it has been shown to yield the most robust generalizedgeodesic distances . Software. We employed the PyTorch framework for ourwork. To execute the time-dependent generalized geodesic dis-tance Eq. (8), we harnessed the combined power of TorchGeomet-ric along with TorchDiffeq . TorchDiffeq, a well-regardedGPU accelerated ODE solver implemented in PyTorch, offers thecapability to perform backpropagation through ODEs using theadjoint sensitivity method , and it offers a variety of numeri-cal schemes. Throughout all of our experiments, we consistentlyutilized the Runge-Kutta (RK4) method, adjusting the step size andtolerance values as hyperparameters, which were set using theperformance of the backbone model on the validation set. Datasets. We used the well-known citation graphs, which havebeen widely employed for evaluating Graph Neural Networks.These graphs include Cora, Citeseer, and Pubmed , where eachnode signifies a document, edges represent citation links, and nodesare associated with sparse bag-of-words feature vectors and classlabels. In addition to the citation graphs, we incorporated two ad-ditional real-world datasets, namely Amazon Photo and AmazonComputer . In these datasets, nodes represent items, edgessignify frequent co-purchases, node features are represented asbag-of-words from product reviews, and the objective is to assignnodes to their respective product categories.",
  "RQ1. How do the generalized geodesic distance features withno learning (Sec. 3.1, ) obtained from Eq. (8) perform on aGCN backbone for node classification?": "Evaluation. For the experiments, we follow a low-resource set-ting with a train/val set split of 2.5%/2.5%, with the rest constitutingthe test set. We report the average accuracy over 10 random splits.We utilize the performance of the backbone model on the valida-tion set to search the hyperparameters of the ODE solver. The in() = () (see Sec. 2.7) was varied in the range 0 to -1 with aninterval size of -0.1. In the Runge-Kutta (RK4) numerical schemethe relative tolerance (rtol) was varied from 0.001 to 0.05 with aninterval size of 0.001. The absolute tolerance (atol) in RK4 wasalways kept as one tenth of the relative tolerance. The step_sizeparameter in Runge-Kutta scheme was kept either 0.1 or 1. Theinitial distances (, 0) in Eq. (8) are set to be zero for the boundarynodes (training set) and a larger positive value, 1e+6, for the remain-ing nodes. The features are generated for five different time steps,with varying from 1 to 5 with an interval of 1. For the backboneGCN , we employ a hidden layer of size 32, a dropout rate of 0.5,ReLU activation, a fixed learning rate of 0.01, the Adam optimizer,and a weight decay of 1e-6. We train the GCN for 5000 epochs witha patience counter of 100. Observation. In , Row 01 displays the performance of theGCN with original node content features, while Row 08 demon-strates the performance of Generalized Geodesic Distances (GGD)as the node features input to the same GCN. It is evident that theGCN using original node features outperforms significantly theuse of generalized geodesic distances as input. This observationstrongly suggests that the original node content features containvaluable information about the nodes, which the GCN in Row 01effectively leverages. In contrast, the GGD features represent apurely topological approach and does not take into account anynode content features. Even though the generalized geodesic dis-tances are known to be robust to noise, the original node contentfeatures simply outperform GGD features. These findings promptour next research question.",
  "RQ2. How do the learned generalized geodesic distance fea-tures (Sec. 3.2, ) perform on a backbone GCN for nodeclassification task? And how do they compare with other graphaugmentation methods?": "Evaluation. We use the same splits as in RQ1. For the backboneGCN, we retain the settings as before. In the training configurationof Pipeline1, we use 150 epochs and employ the Adam optimizerwith a fixed learning rate 0.01 in all of our experiments, along withL2 weight regularization chosen from {0.0005, 0.001, 0.005, 0.01}.The MLP functions used in Pipeline1 had either one or two hiddenlayers with ReLU activations. The hidden layer size was kept in {32,64, 128, 256, 512, 768}. The dropouts were searched in 0.1 to 0.9 withan interval size 0.1. The chosen loss function was cross-entropy. ForPipeline2, we generate features for five different time steps ( =1to 5, with an interval of 1). The hyperparameters for Solver1 andSolver2 were kept same. They were searched in the same range asdescribed in the in RQ1. To search for the hyperparameters, werelied on the performance of the backbone model on the validationsplit. All experiments were conducted using the NVIDIA RTX 3090.",
  "ModelCoraCiteseerPubmedComputersPhoto": "01 GCN74.13 2.0866.08 2.1679.73 0.7181.72 1.7887.57 1.1802 MixUp72.72 1.7864.14 1.7580.02 0.5280.76 1.4088.67 0.8003 DropEdge72.28 1.3965.73 1.8381.89 0.8481.45 1.0288.29 1.2704 GAug-M72.14 1.3766.38 1.2982.18 1.3684.82 0.7891.05 1.2105 GAug-O71.30 1.5467.22 1.06OOM83.03 0.5090.62 0.3006 GDC (heat)77.52 1.7465.38 1.3682.16 0.9380.18 1.3188.12 2.2107 GDC (ppr)78.13 2.1366.33 1.8480.86 0.7882.88 1.1489.07 2.1908 GGD69.95 2.5143.21 2.4476.49 0.8778.89 1.6185.69 0.9209 LGGD80.18 1.5367.23 1.7983.24 1.7985.23 2.1892.02 2.3310 LGGD w. ()81.56 2.2968.63 1.7083.36 1.8885.49 1.0992.39 2.1111 GPR-GNN79.45 1.6667.18 1.8484.11 0.3882.80 2.0191.48 1.5912 GOAL76.07 1.5666.57 1.2681.83 1.2883.43 1.0491.65 0.69 : The top row shows the performance of LGGD (Learned Generalized Geodesic Distances) features across different datasets for variousbackbone models. The bottom row demonstrates the ability to incorporate new incoming labels without retraining the backbone model (seeSec. 3.3), as illustrated for three datasets. A green dot represents the results obtained after dynamically adding 10% new labels. about these methods in Sec. 5. To learn about the range of their hy-perparameter tuning, refer to Appendix A.3. For all these methods,the backbone model remained a simple GCN with the same settingas mentioned before. In addition to these models, we also utilizedtwo state-of-the-art models for comparison, namely GPRGNN and GOAL (Row 11 & 12, ). Observation. We observe that the proposed model () sig-nificantly enhances the performance of the generalized geodesicdistance features, making it competitive with several other methods(). Row 08 corresponds to Generalized Geodesic Distances(GDD), for which no learning took place. Row 09 corresponds toLearned Generalized Geodesic Distances (LGGD), where a sig-nificant improvement in the performance of the backbone modelis achieved due to the learning factor by incorporating the nodecontent features. RQ3. After training the backbone GCN, how does the dynamicinclusion of new labels (Sec. 3.3, Eq. (12)) affect performance overtest set?In the bottom row of , one can observe the results ofthis approach on the three citation networks. We create a splitconsisting of train/val/nl1/nl2/nl3/test, with percentages of 2.5%,2.5%, 10%, 10%, 10%, and 65%, respectively. The proposed hybridmodel is trained and validated using the 2.5% splits. After trainingthe backbone model, in the Pipeline2, we dynamically add the newlabels (nl1, nl2, nl3) to expand the boundary size (0 = =0 ),and update the initial condition according to Eq. (12), and thengenerate new features using Solver2 and monitor the performanceover the test split of the backbone GCN without retraining the",
  "KDD 24, August 2529, 2024, Barcelona, SpainAmitoz Azad and Yuan Fang": "Since the R.H.S. is zero, for any valid solution of the above equation,there must be at least one () for which ( () () 1) iszero, and this corresponds to the maximum element in the set onL.H.S. Therefore, the above equation can be rewritten as:max (){( () () 1)} = 0",
  "Additional Results": "RQ4. How does the optional gradient-based learning of the po-tential function () affect the performance of the generated learnedgeneralized geodesic distances features?In Row 10 of , we can observe that this change results ina slight performance increase across all datasets. Making it the thetop-performing row across several datasets. It is worth noting thatRow 09 and Row 10 share the same hyperparameters, and the slightgains are achieved simply by allowing gradient-based learning ofthe potential function (). RQ5. How do the proposed learned generalized geodesic distancefeatures perform for backbone models other than a GCN? (top row) showcases the performance of the LGGD fea-tures on three different backbone models: GAT , CHEBNET ,and JKNET . Mean accuracies for the 10 random splits are pre-sented for the same low-resource split setting. We can observe thatthe learned generalized geodesic distance-based features lead toperformance improvements, sometimes quite significant, on mostof the datasets for these models. Refer to Appendix A.4 to knowthe hyperparams of the backbone models.While aims to demonstrate that our method competeswith various state-of-the-art structural and feature augmentationmethods (using a common backbone GCN), (top row) illus-trates how our method enhances the performances across differentbackbone GNNs. It is essential to note that, for both and, the hyperparameter configuration of the backbone GNNis consistently maintained (with and without augmentation(s)),allowing us to focus solely on studying the effect of augmentation.",
  "RELATED WORK": "The field of graph augmentation is vast and rapidly gaining interestwithin the graph learning community. Here, we will focus on somepopular methods for node-level tasks, which essentially make thegraph robust to noise by either changing its topology (structuralaugmentation) or altering its node features.GraphMix and MixUp are two popular methods for nodefeature augmentation in semi-supervised learning. Both GraphMixand MixUp employ training a Graph Neural Network (GNN) byinterpolating node features and node targets using a convex combi-nation. Both methods draw the parameter for the convex combi-nation from a beta distribution. While MixUp involves the mixing of node features and their hidden representations through the mes-sage passing within a GNN, GraphMix utilizes a Fully ConnectedNetwork (FCN) alongside a GNN, exclusively for feature mixing.The FCN layers and GNN layers share their weights and are jointlytrained on a common loss function, combining predictions fromthe training set FCN layer and GNN layer. Additionally, an unsu-pervised loss term is incorporated to ensure that the predictions ofthe GNN on unlabeled nodes match those of the FCN.DropEdge , GAug and GDC are three popular struc-tural augmentation methods for node classification. DropEdge justrandomly removes the edges, and redo the normalization on theadjacency matrix before every training epoch. GAug comes in twoversions: GAug-M and GAug-O. Both use Graph Autoencoder asthe edge prediction module. In GAug-M, an edge prediction moduleis trained before passing the modified graph to the backbone model.Then, edges with high and low probabilities are added and removed,respectively. In GAug-O, the edge prediction module is trained incombination with the backbone model, using a common loss func-tion that combines node classification loss and edge prediction loss.Training is performed by sparsifying the convex combination ofthe edge prediction module and the original graph, using differen-tial Bernoulli sampling on this combination. We find it to be slowand memory intensive (). GDC essentially smooths out theneighborhood by acting as a denoising filter, similar to a Gaussianfilter for images. It achieves this by first calculating an influencematrix using methods such as page rank or a heat kernel to makethe graph fully connected. Then, it sparsifies the influence matrixusing either a top-k cutoff or an epsilon cutoff to retain only theedges with maximum influence.",
  "CONCLUSION": "We proposed a hybrid model in which learned generalized geodesicdistances were used as node features to improve the performanceof various backbone models for the node classification task. Theproposed model allows the dynamic inclusion of new incominglabels without retraining the backbone model.One limitation of our work is that we did not find much successfor heterophilous graph datasets. In fact, most of the structural andnode feature augmentation methods work only on homophilousgraph datasets. One potential way to overcome this issue is totry negative weights to represent dissimilarity . Alternatively,allowing the potential function to take on negative values could beconsidered. These approaches will be investigated in the future. This research project is supported by the Ministry of Education,Singapore, under its Academic Research Fund Tier 2 (Proposal ID:T2EP20122-0041). Any opinions, findings and conclusions or recom-mendations expressed in this material are those of the author(s) anddo not reflect the views of the Ministry of Education, Singapore.",
  "Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. 2021. Adaptive universalgeneralized pagerank graph neural network. In ICLR": "Flavio Chierichetti, Alessandro Epasto, Ravi Kumar, Silvio Lattanzi, and VahabMirrokni. 2015. Efficient algorithms for public-private social networks. In Proceed-ings of the 21th ACM SIGKDD International Conference on Knowledge Discoveryand Data Mining. 139148. Michal Defferrard, Xavier Bresson, and Pierre Vandergheynst. 2016. Convolu-tional neural networks on graphs with fast localized spectral filtering. Advancesin neural information processing systems 29 (2016).",
  "Yiwei Wang, Wei Wang, Yuxuan Liang, Yujun Cai, and Bryan Hooi. 2021. Mixupfor node and graph classification. In Proceedings of the Web Conference 2021.36633674": "Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichiKawarabayashi, and Stefanie Jegelka. 2018. Representation learning on graphswith jumping knowledge networks. In International conference on machine learn-ing. PMLR, 54535462. Tong Zhao, Wei Jin, Yozen Liu, Yingheng Wang, Gang Liu, Stephan Gnnemann,Neil Shah, and Meng Jiang. 2022. Graph data augmentation for graph machinelearning: A survey. arXiv preprint arXiv:2202.08871 (2022). Tong Zhao, Yozen Liu, Leonardo Neves, Oliver Woodford, Meng Jiang, and NeilShah. 2021. Data augmentation for graph neural networks. In Proceedings of theaaai conference on artificial intelligence, Vol. 35. 1101511023.",
  "A.3Hyperparam Tuning of Baselines": "For MixUp , we used random search to uniformly draw pa-rameter from 1 to 5. The parameter is sampled from (, ),which determines the extent of mixing node features. RegardingDropEdge , we adjusted the edge dropout probability from 0 to0.99 with an interval size of 0.01. In the case of GAug-M , wetuned the percentage of the most probable edges to be retained andthe percentage of the least probable edges to be dropped, rangingfrom 0 to 0.9 with an interval size of 0.1. For GAug-O , weused random search to uniformly draw , , and parameterswithin the range of , , and respectively. As for GDC(heat) , we varied the parameter from 1 to 10 with and intervalof 0.5. In the case of GDC (ppr) , we fine-tuned the alpha param-eter within the range of 0 to 0.95 with an interval of 0.05. For bothGDC models, we utilized the top-k method to sparsify the influencematrix, selecting from 32, 64, and 128, either along the dimension 0(row) or dimension 1 (column). GPRGNN and GOAL do notrequire the use of a backbone model for prediction. We used theiravailable hyperparameters from their respective GitHub sources.",
  "A.4Hyperparams of Different Backbones": "For GAT, we employed a hidden layer with a size of 32, utilizinginput attention heads of size 8 and output attention heads of size 1.In the case of CHEBNET, we applied a two-step propagation with ahidden layer of size 32. For JKNET, we implemented a GCN modelwith a hidden layer size of 32. Regarding the layer aggregationcomponent of JKNET, we incorporated a LSTM with 3 layers, each with a size of 16. For all of these models, the learning rate wasset to 0.01, using the Adam optimizer, a weight decay of 1e-6, adropout rate of 0.5, and a training duration of 5000 epochs with apatience counter set to 100. This configuration was used for originalnode content features and for learned generalized geodesic distancedistance features.",
  "A.5Efficiency": "Training Time. The overall training time depends on that of thebackbone in Pipeline2. Regarding backpropagation through theODE solver (Pipeline1), the training time efficiency (training lossvs time) is known to be a few times lower than an MLP (as shownin the of work done by Dupont et al. ). However, thiscan be effectively mitigated by concatenating every feature vectorwith zeros . Complexity. The runtime complexity of the ODE solver is dom-inated by (||)( + ). Here, || represents the number ofedges, represents the number of classes, and and repre-sent the numbers of backward and forward function evaluations,respectively. The complexity of the backbone GNN depends on thespecific backbone. Inference. Once the MLP in Pipeline1 is trained, it can quicklyproduce learned generalized geodesic distance features, taking onlya fraction of a second. The primary benefit for the dynamic inclusionpart is the ability to make fast predictions without needing to retrainthe backbone GNN model. For instance, the prediction times forthe dynamically added new labels case (green dots in ,bottom row) is around 0.1 sec for all three citation graphs, whereasretraining a simple backbone model like a GCN for 1k epochs takesaround 7 sec for Pubmed dataset on RTX 3090. Scalability. The overall scalability depends on the scalability ofthe backbone GNN. Concerning the ODE solvers ability to managelarge-scale graphs, it is worth noting that it has been successfullyutilized in the literature for handling OGB graphs in nodeclassification task."
}