{
  "ABSTRACT": "Domain generalization on graphs aims to develop models with robust generalization capabilities,ensuring effective performance on the testing set despite disparities between testing and training distri-butions. However, existing methods often rely on static encoders directly applied to the target domain,constraining its flexible adaptability. In contrast to conventional methodologies, which concentrateon developing specific generalized models, our framework, MLDGG, endeavors to achieve adaptablegeneralization across diverse domains by integrating cross-multi-domain meta-learning with structurelearning and semantic identification. Initially, it introduces a generalized structure learner to mitigatethe adverse effects of task-unrelated edges, enhancing the comprehensiveness of representationslearned by Graph Neural Networks (GNNs) while capturing shared structural information acrossdomains. Subsequently, a representation learner is designed to disentangle domain-invariant semanticand domain-specific variation information in node embedding by leveraging causal reasoning forsemantic identification, further enhancing generalization. In the context of meta-learning, meta-parameters for both learners are optimized to facilitate knowledge transfer and enable effectiveadaptation to graphs through fine-tuning within the target domains, where target graphs are inacces-sible during training. Our empirical results demonstrate that MLDGG surpasses baseline methods,showcasing the effectiveness in three different distribution shift settings.",
  "Introduction": "Domain generalization is a fundamental research area in machine learning that aims to enhance the ability of modelslearned from source domains to generalize well to different target domains . Whilehandling distribution shifts across domains on Euclidean data has achieved significant success , there has beenlimited focus on graph-structured data due to specific challenges where domains are characterized by variations of nodefeatures and graph topological structures simultaneously. illustrates the presence of distribution disparities ongraphs, with each graph sampled from a distinct domain. Consequently, a model trained on one graph domain (e.g.,gamer networks, TWITCH) may exhibit poor generalization performance when deployed in a different domain (e.g.,social networks, FB-100). To address the problem of domain generalization on graphs, several efforts have been made. Existing approaches oninvariant learning with Graph Neural Networks (GNNs) focus on encoding invariant information of graphs byminimizing the risk across various environments under the assumption that the information determining labels remainconstant. They usually assume access to abundant and diverse training domains, prompting researchers to proposedata augmentation to alleviate the problem, which strives to diversify the training domains as much aspossible to improve the generalization ability of the model. However, an overly flexible domain augmentation strategycan create implausible augmented domains . Additionally, the complexity of real-world graph structures and the",
  "Density": "Tw-DEFb-AmherstWeb-Texas : Visualizations of distribution shifts on graphs demonstrated using energy scores of nodes acrossdifferent graphs, where each graph is sampled from a distinct domain characterized by variations of node features andtopological structures simultaneously. The legend of all sub-figures follows the same naming format, i.e., \"DataName- DomainName\". (Left) Graphs are sampled from the same dataset TWITCH . (Middle and Right) Graphs aresampled from different datasets, TWITCH, FB-100 , and WEBKB .",
  "often unknown underlying data generation mechanisms make it challenging to acquire the knowledge necessary forgenerating new graphs": "Moreover, to alleviate the above obstacles and enhance the interpretability of generalized models, causal reasoning isoften combined with invariant learning . The invariance principle from causality elucidates and models theunderlying generative processes of graphs, targeting the identification of stable causal relationships across differentdomains. Nevertheless, studies show that trained GNNs are heavily biased towards specific graph structures and cannoteffectively address the domain variations on graph topology structures . Additionally, some studies integrate structure learning to improve the robustness of generalized GNNs, such as capturingthe domain-independent and domain-invariant clusters to learn invariant representations by training static encodersshared by all source graphs . However, capturing invariant topology structure learners across domains reduces theadaptability of the structural encoder and limits its ability to accommodate various distributions. Hence, it is urgent totrain models who possess transferable knowledge across domains with various distribution shifts. In this paper, we propose a novel cross-multi-domain meta-learning framework, MLDGG, designed to acquire trans-ferable knowledge from graphs sampled in the source domain and generalize to those in the target domain, wheretarget graphs are inaccessible during training. Specifically, to address the problem of node-level domain generalizationon graphs, where domain variations are characterized by graph topological structures and node features, MLDGGcomprises two key components: a structure learner and a representation learner. The structure learner aims to mitigatethe adverse effects of task-unrelated edges and capture structure knowledge shared across different domains, enhancingthe comprehensiveness of representations learned by GNNs. The representation learner disentangles semantic andvariation factors to capture the invariant patterns of the truly predicting properties in different domains. In the contextof meta-learning, the goal of MLDGG aims to learn optimal meta-parameters (initialization) for both learners so thatthey can facilitate knowledge transfer and enable effective adaptation to graphs through fine-tuning within the targetdomains. Our contributions are summarized as follows: We propose a novel cross-multi-domain meta-learning framework on graphs. It is designed to acquire transferableknowledge from graphs sampled in the source domain and generalize to those in the target domain, where targetgraphs are inaccessible during training. The framework consists of two key learners: a structure learner, which captures shared topology patterns acrossdifferent graph domains to enhance the robustness of GNNs, and a representation learner, which disentangles domain-invariant semantics from domain-specific variations. In the context of meta-learning, the parameter initializationsfor both learners are optimized to facilitate knowledge transfer and enable effective adaptation to graphs throughfine-tuning within the target domains. Empirically, we conduct three distinct cross-domain settings to assess the generalization ability of MLDGG fornode-level prediction tasks under different degrees of distribution shifts using real-world graph datasets. Our methodconsistently outperforms state-of-the-art baseline approaches.",
  "Related Work": "Domain Generalization on Graphs. Domain generalization aims to generalize a model trained on multiple seendomains with diverse distributions to perform well on an unseen target domain . The study of domain generalizationon graphs presents significant challenges due to the irregular nature of graph data and the complex dependencies betweennodes . Methodologically, various strategies such as robust optimization , invariant learning ,causal approaches and meta-learning domain generalization have been employed to tackle thisproblem. Robust optimization improves the generalization ability by improving the models performance in the worst-case data distribution. Invariant learning minimizes prediction variance across domains to capture invariant featuresacross domains. Causal approaches are dedicated to separating inclusive information factors (semantic information)and irrelevant factors, utilizing the principles of causal graphs in an unsupervised or semi-supervised manner. Inaddition, GraphGlow improves GNN generalization by learning generic graph structures. It trains a static structureencoder to capture invariant structure information across domains and apply it in the target domain, which reduces theadaptability of the structural encoder and limits its ability to accommodate various distributions. Despite GNNs abilityto extract abstract representations, they mix the domain-invariant semantic factor with the domain-specific variationfactor. Meta-learning learns prior experiences during meta-training and transforms learned knowledge to the targetdomain by simple fine-tuning. Following similar spirits, we use a combination of learning domain-invariant semanticinformation and learning-to-learn strategies to achieve domain generalization across domains. Meta-Learning on Graphs. Meta-learning, also known as \"learning to learn\", focuses on the ability of a model tolearn and adapt to new tasks or domains quickly and efficiently . It has been widely used in generalizationproblems . Consequently, meta-learning for graphs generally combines the advantages of GNNsand meta-learning to implement generalization on irregular graph data . From the learning tasks of view,these methods generally fall into three categories, node-level , edge-level and graph-level .Methodologically, these approaches incorporate metric-based , which are aimed at learning metrics to quantifythe similarity between task-specific support and query sets, and optimization-based methods that concentrateon effectively training a well-initialized learner capable of rapidly adapting to new few-shot tasks via simple fine-tuning.However, they only consider the scene where all tasks originate from the same domain, the challenge of generalizingprior experiences from cross-multi-domain graphs during meta-training and transferring knowledge to unseen domainsis unexplored in the existing literature.",
  "We list all notations used in this paper in in Appendix A": "Node-level Domain Generalization on Graphs. Given a set of graphs G = {Gei}|E|i=1, where each graph Gei =(Aei, Xei) is sampled from a unique domain ei E and a domain ei is defined as a joint distribution P(Aei, Xei). Ineach graph Gei, we denote Aei {0, 1}|Vei||Vei| the adjacency matrix, where Vei is a collection of nodes. Xei ={xeij }|Vei|j=1 R|Vei|Dei represents the node feature matrix of D-dimensional vectors. yei = {yeij }|Vei|j=1 R|Vei|",
  "denotes node labels in Gei": "For node-level domain generalization, each graph Gei G is associated with a specific variation in graphic topologyAei and node features Xei. The graph set G is partitioned into multiple source graphs Gs = {Gei}Ki=1 where K = |Es|and Es E, and target graphs Gt with Et = E\\Es. The objective is to maintain satisfactory generalization performancein node-level prediction accuracy transitioning from given source graphs Gs to target graphs Gt, with the condition thatGt remains inaccessible during training. Meta-Learning is an approach aimed at training a model over a range of tasks to be able to adapt to new tasks rapidly, where each task T i P(T ) associated with a data batch is partitioned into a support set T isup for thelearning phase and a query set T iqry for evaluation purposes. Meta-learning can be described as \"learning to learn\"because it involves finding a meta-parameter from which one can quickly derive multiple optimized parameters{i}Mi=1 specific to individual tasks {T i}Mi=1. Model-agnostic meta-learning (MAML) is a notable gradient-based meta-learning approach that has demonstratedremarkable success in generalization. The core assumption of MAML is that some internal representations are bettersuited to transfer learning. During training, the model first learns from T isup for each task T i and accordingly optimizes the task-specific parameterto i with one or few gradient steps. The meta-parameter is updated through query losses evaluated from {T iqry}Mi=1",
  "i(i, T iqry), where i = (, T isup),(1)": "where : Rd R is the cross-entropy loss for classification and > 0 is the learning rate. The goal of MAML isto learn an effective model initialization using M training tasks, enabling rapid fine-tuning on the support set T tsup ofthe target task T t to achieve optimal performance on T tqry, where T t = {T tsup, T tqry}. Graph generalization using MAML leverages a similar GNN-based task distribution to accumulate transferableknowledge from prior learning experiences. However, the original MAML assumes that all tasks originate fromthe same distribution, which hinders its ability to generalize across multiple domains . Additionally, GNNs canintroduce noise information from task-unrelated edges, negatively impacting performance. As discussed in ,GNN methods combined with structural optimization typically learn a static structure, which constrains the modelsability to generalize to varying topology distribution shifts. To overcome these limitations, a cross-multi-domain robustalgorithm is required, as we will discuss next. Problem Setup. To address the problem of node-level domain generalization on graphs, where each domain ischaracterized by variations on both topology structures and node attributes, learning cross-multi-domain shared graphtopology and node representation information is essential for capturing transferable knowledge across different domains.As shown in , a novel framework MLDGG is proposed in the context of meta-learning with two key components:a structure learner ft : (A X) A parameterized by t and a representation learner fr : Rd Rdparameterized by r. The goal of MLDGG aims to learn a good parameter initialization = {t, r} across all givensource graphs {Gei}Ki=1, ei Es, such that the learned can be effectively adapted to the target graph GeT , eT Et,which is inaccessible during training.",
  "(2)": "where M K represents the number of tasks and is denoted as element-wise addition operation. GNN : (A X) Rd is a graphic representation function, parameterized by eig specific to the domain ei. The representationlearner fr consists of a semantic encoder Es : Rd Rs, a variation encoder Ev : Rd Rv, and adecoder D : Rs+v Rd. We thus denote r as consisting of the parameters s, v, and d, respectively, i.e.,r = {s, v, d}. Detailed setting and training of fr is introduced in .2. T eisup and T eiqry are support and querysets of the domain ei, which are randomly sampled from the output of the semantic encoder Es. Inspired by MAML,meta-parameters = {t, r} and task-specific parameters ei = {eit , eir } are updated interchangeably throughthe bi-level optimization, using query sets {T eiqry}Mi=1 and support sets {T eisup}Mi=1, respectively. The learned is further",
  "Methodology": "The primary challenges of MLDGG involve modeling and capturing generalizable structure patterns across multipledomains, as well as disentangling domain-invariant semantic factors and domain-specific variation factors. To thisend, the structure learner ft is devoted to capturing shared structural information across domains while enhancing thecomprehensiveness of representations learned by GNN through mitigating the adverse effects of task-unrelated edges(Sec. 4.1). Additionally, the representation learner fr captures the invariant patterns of the truly predictive propertiesthrough the semantic encoder Es by disentangling semantic and variation factors in node representations based on thecausal invariance principle (Sec. 4.2). Finally, in Sec. 4.3, we integrate two learners within the meta-learning frameworkto capture transferable knowledge across various domains. For simplicity, in this section, domain ei is simplified to i.",
  "Structure Learner": "For graph data with both attributes and topologies, how to learn as comprehensive and rich node representation aspossible is a problem that has been explored. One prevalent method is GNNs, which learns node representationsthrough recursive aggregation of information from neighboring nodes. However, based on the model of the message-passing mechanism, small noise propagation to neighboring areas may cause deterioration of the representation quality.",
  "Update)": ": An overview of MLDGG. Each source graph is viewed as a task. For each task, the parameters {t, g, r}of the structure learner (ft), GNN, and representation learner (fr) are updated via Lsup during the inner update phase.Subsequently, the query losses Lqry across all tasks are aggregated to update the meta-parameters = {t, r} in theouter update phase. To generalize to graphs in the target domain, the learned meta-parameters of the structure learnerand the representation learner are further fine-tuned for adaptation. Therefore, we optimize GNN by learning high-quality graph structures. Further, we also explore the common structuralpattern between cross-domain graphs to improve generalization ability. Here, we define a refined graph structure matrixas A learned by a graph structure learner ft. The ft is expected to produce optimal graph structures that can give riseto satisfactory downstream classification performance. First, we learn an intermediate similarity graph matrix F, where Fjk denotes the edge weights of node j and k. Tofuse attributes and topological information, we use the representation of nodes r Rd to calculate the weight of edgesbetween nodes:",
  "j,kAj,k||rj rj||22 ||A||0,(4)": "where the and are hyperparameters controlling different modules importance. We adopt the policy gradientoptimization method for the non-differentiable problem of sampling A. We define the probability for sampling asfollows:(A) = j,kAjkFjk + (1 Ajk)(1 Fjk).(5) Then we independently sample H times to obtain {A}Hh=1 and {(A)}Hh=1. We define the regularization B in Eq. (4)as the reward function, then we optimize the t using REINFORCE algorithm with the gradient:",
  "Representation Learner": "Despite GNNs having the capability to extract abstract representations for predictions, the representation may uncon-sciously mix up semantic factors s and variation factors v due to a correlation between them. So the model still relieson the domain-specific variation factors v for prediction via this correlation. However, this correlation may changedrastically in a new domain, making the effect from v misleading. So we assume the representation of each node isdisentangled into two factors: a domain-invariant semantic factor s determining the label and a domain-specific variationfactor v independent of labels. p(r|s, v) and p(y|s) are invariant across domains, and the change of prior p(s, v) isthe only source of domain change. Based on the above causal generative principle, we develop the representationlearner based on variational Bayes . Here, the representation of the node learned by GNN r and the label y areaccessible variables and we have supervised data from the underlying representation p(r, y) in the source domain.The log marginal likelihood of the r and y is as follows:",
  "log p(r, y) = log p(s, v, r, y)dsdv,(8)": "where p(s, v, r, y) := p(s, v)p(r|s, v)p(y|s). By maximizing the likelihood in Eq. (8), p(r, y) will match the p(r, y).However, the direct optimization of Eq. (8) is intractable, so we utilize the variational inference to approximate themarginal likelihood. We introduce a tractable distribution q(s, v|r, y) and construct the variational objective as follows:",
  "=: Lqs,v|r,y(r, y),(9)": "where Lp,qs,v|r,y(r, y) is called Evidence Lower BOund (ELBO). Unfortunately, the introduced model q(s, v|r, y)fails to facilitate the estimation of p(y|r). To alleviate this problem, we introduce an auxiliary model q(s, v, y|r) totarget p(s, v, y|r), which enables the straightforward sampling of y given r for prediction. Meanwhile, q(s, v, y|r) =q(s, v|r, y)q(y|r) means it can help learning inference model q(s, v|r, y), where q(y|r) :=q(s, v, y|r)dsdv. Dueto p(s, v, y|r) can be factorized as p(s, v|r)p(y|s). Thus, we can instead introduce a lighter inference q(s, v|r) for theminimally intractable component p(s, v|r) and use q(s, v|r)p(y|s) as an approximation to q(s, v, y|r). This turns theobjective Eq. (9) to:",
  "(10)": "where q(y|r) = Eq(s,v|r) [p(y|s)]. The LELBO in Eq. (10) consists of three components. The first term is the negativeof the standard cross entropy (CE) loss and p(y|s) gives the ability to model to predict the target label. The secondterm encourages the latent representation s and v to preserve the salient information of r by reconstruction. The thirdand fourth term drives the variational posteriors q(s, v|r) towards its priors. By maximizing the LELBO in Eq. (10),it becomes feasible to deduce the parameters of distribution over the joint latent variables s and v. The Monte Carlomethod can be used to estimate expectations . The derivation of Eq. (10) is provided in Appendix B. MLDGG-ind. To improve the generalization of the model, we consider another case where s and v are independent,i.e., p(s, v) = p(s)p(v). Formally, the distribution p(s, v) exhibits a higher entropy compared to p(s, v), whichdiminishes specific information of the source domain and promotes dependence on causal mechanisms for enhancedgeneralization. Under the conditional independence assumption, the p(s, v)) can be turned to p(s)p(v) and q(s, v|r)can be turned to q(s|r)q(v|r) in Eq. (10), which is denoted as LELBOind.",
  "is parameterized": "by its Cholesky decomposition . For semantic encoder Es and variation encoder Ev, we formulate their variationalposterior distribution as Gaussian distribution with diagonal covariance structure, parameterized by neural network.For the decoder D, we adopt the Gaussian distribution q(s, v|r) = N(s, v|r, 2rI), where r and r are given by themapping during reconstruct process. The semantic factor s = Es(r) is used to predict node label y, i.e., y = f(s),where f is a classifier. The variation factor v = Ev(r) is independent of label. MLDGG uses both s and v to reconstructrepresentation r, i.e., r = D(s, v). For the prior p(s) and p(v) in MLDGG-ind, we adopt standard Gaussian N(s; 0, I)and N(v; 0, I), respectively.",
  "This section presents a theoretical analysis of the boundary guarantee for domain generalization errors in the meta-learning framework that integrates the structural and representation learner": "Theorem 1 (Upper bound: accuracy). Define the expected error of f in representation space as Acc( f) = E[L( f g(A, X), Y )]. For any f : Rs R, any representation mapping g : AX Rs, and any loss function L : RR Rthat is upper bounded by u, the expected error of f g : A X R at any target domain eT Et is upper bounded:",
  "(14)": "For simplicity, we omit parameters and its domain . In this paper, the g function is g = ft GNN Es. We adoptJensen-Shannon (JS) distance denoted as dJS to quantify the dissimilarity between two distributions. Theorem 2 (Lower bound: accuracy). Suppose L( f g(A, X), Y ) is lower bounded by c when f g(A, X) = Y ,and is 0 when f g(A, X) = Y . Let denote the number of labels, if dJS(PeiY , PeTY ) dJS(PeiS , PeTS ), the expectederror of f at source and target domains is lower bounded:",
  ":Compute Ri using Eq. (7)10:it0 = t, is0 = s, iv0 = v, id0 = d,ig0 = ig11:Sample T iqry and T isup12:For n in 1, . . . , do:13:Compute Lisup on T isup via Eq. (11)": "14:itn = itn1 linLisup15:isn = isn1 linLisup16:ivn = ivn1 linLisup17:idn = idn1 linLisup18:ign = ign1 linLisup19:Compute Li,nqry on T iqry via Eq. (11)20:End21:Liqry = Li,qry22:End23:Update t t loutt1MMi=1 Liqry24:Update s s louts1MMi=1 Liqry25:Update v v loutv1MMi=1 Liqry26:Update d d loutd1MMi=1 Liqry27: End while28: Output: trained initialization parameters t, s, v and d Theorem 2 indicates the infeasibility of optimizing the lower bound of error, which is determined by the datasetdistribution, represented by A, X, and Y. The proof of Theorem 1 and Theorem 2 are provided in Appendix E. To optimize the model on the target graph, we encounter the following issues: (C1) Independent training of the GNNs.A usual assumption is that the testing graph is the same as the training graph. This premise requires independentlytraining the structure learning model from scratch for each graph dataset which leads to prohibitive computationcosts and potential risks for serious over-fitting. (C2) Distribution alignment. Obtaining relatively domain-invariantsemantic information in a disentangled manner is challenging when dealing with source domains with significantdistribution differences. In other words, aligning PeiS|Y and PejS|Y in the second part of Eq. (14) becomes difficult. Addressing (C1) by training the structure learner. If we can learn from the A and X of source graphs how to capturestructural information on the graph, we can then apply this ability to the target domain. Specifically, we decomposethe representation mapping g : A, X Rs in Theorem 1 into three parts: g = ft GNN Es, where ft is a structurelearner to refine the given structure (i.e., A = ft(A, X)), GNN embeds (A, X) and (A, X) to R, and Es learns thesemantic factor, i.e., S = Es(R). Therefore, we can use this pre-trained ft and ES on the target graph instead ofretraining the entire model. Addressing (C2) by feature disentanglement. To ensure the final classification, we aim todecouple domain-invariant information, which has a consistent distribution across each domain. In a perfect decouplingscenario, the second part of Eq. (14) would approach zero.",
  "Datasets": "We utilize three real network datasets that come with ground truth to verify the effectiveness of MLDGG. Experimentsare conducted on the multiple graphs contained within each dataset. The statistical characteristics of these networks areshown in and due to space constraints, we present the details of the dataset in the Appendix C.1.",
  "We establish 3 different scenarios determined by whether the source and target graphs are derived from the same dataset": "S1T1. Both source and target graphs originate from the same dataset. We sequentially test each graph for each datasetwhile training on the remaining ones. S1T2. The source graphs and target graphs are from different datasets. In particular, all graphs in the source arefrom the same dataset. For instance, we use 5 graphs from FACEBOOK-100 for training and testing on the graphs ofTWITCH-EXPLICIT and WEBKB, separately. This approach can be similarly applied to other datasets. S12T3. The source graphs and target graphs are from different datasets. In particular, the source graphs for trainingare selected from different datasets. Here, we choose eight graphs from two distinct datasets for training (e.g.,FACEBOOK-100 and TWITCH-EXPLICIT), and testing on the other dataset (WEBKB). We use GCN architectures as the GNN model in MLDGG. For all baseline models, we implemented them using theauthors provided source code and also set GCN as the backbone. To reduce the time and space cost of the structurelearner, following the simplification of , we convert the sampling of A in the structure learner to the product of the(NP)-dimensional matrix and its transpose. Due to the baseline methods inability to adapt to varying feature and labeldimensions, we employ zero-padding for feature dimensions and label expansion to standardize them after comparingdifferent padding methods. We report the experimental results for all scenarios, with the final result for each datasetderived from the mean of all graphs within that dataset. The results represent the average values obtained from 10 runsfor all the methods compared.",
  "Hyper-parameter Settings": "The parameters of the structure learner include (the weight on original graphs), (the weight on smoothnessconstraints), and (the weight on sparsity constraints). We adjust all values to fall within the range of . Inmeta-training, we set the update step as 5. In meta-testing, we set the update step in {1, 5, 10, 20, 30, 40}. The learningrates for the inner and outer loops are set to lin = 1e3 and lout = 1e1, respectively.",
  "PTBRTWRUESFRENGBDEAvgAmherstJohnsReedCornellYaleAvg": "GraphGlow 65.4 1.1 60.7 1.3 75.4 0.970.70.363.1 1.2 54.5 0.2 60.4 0.5 64.352.7 1.8 51.3 1.0 61.7 1.2 51.3 1.5 52.4 1.3 53.8MD-Gram 64.3 0.4 57.8 0.3 72.0 0.4 70.7 0.8 61.4 0.5 52.2 0.6 58.9 0.5 62.551.1 0.4 48.0 0.3 62.9 0.6 48.7 0.3 44.2 0.6 51.0GMeta 12.7 2.1 23.5 1.3 28.9 1.4 30.8 1.7 13.0 2.1 15.2 1.8 19.3 1.2 22.329.7 1.5 26.6 2.0 19.8 0.9 21.5 1.8 20.4 1.2 23.6FLOOD 10.0 0.3 28.5 0.4 28.1 0.3 31.4 0.2 20.9 0.1 11.8 0.2 16.7 0.5 21.040.7 1.0 39.9 0.4 39.0 0.3 45.5 0.4 41.2 0.8 41.3EERM 10.1 3.3 31.1 2.5 30.8 2.7 34.1 2.9 22.8 3.5 13.9 2.1 15.4 0.8 22.642.8 2.7 41.4 2.6 40.6 2.8 49.1 3.5 43.8 2.6 43.5SRGNN 11.2 0.9 29.6 1.4 28.5 1.7 31.9 2.0 24.1 2.2 13.4 1.3 16.1 1.0 22.121.2 1.8 19.6 1.7 18.3 0.9 20.9 1.7 21.6 1.5 20.3Mixup 12.0 1.3 27.5 1.7 27.9 1.6 31.4 1.5 25.0 1.9 14.2 1.6 16.3 1.0 22.032.6 2.0 31.7 1.2 29.9 0.8 32.0 1.9 30.1 1.6 31.3ERM 34.5 2.9 10.5 0.4 41.1 4.8 20.7 3.3 16.8 0.2 12.6 0.2 15.0 0.1 21.740.9 2.6 46.0 2.1 40.5 2.4 42.1 2.6 44.4 3.8 42.7",
  "TexasCornellWisAvg": "GraphGlow 55.2 0.9 44.8 1.2 45.4 1.0 48.5MD-Gram 55.4 0.5 45.2 0.3 45.1 0.6 48.6GMeta 23.2 1.4 21.9 1.9 19.6 2.0 21.6FLOOD 19.7 0.3 18.1 0.4 15.5 0.4 17.8EERM 20.7 0.0 18.3 0.4 13.9 0.0 17.6SRGNN 19.2 1.8 15.7 1.6 14.0 1.3 16.3Mixup 18.1 1.6 13.9 2.1 13.2 1.2 15.1ERM 21.3 0.9 16.4 2.8 15.2 2.0 17.3",
  "MLDGG-ind59.7 0.349.6 0.254.0 0.254.4": "First, MLDGG exhibits superior performance compared with other competitive baselines on all datasets within threeexperimental settings, highlighting its robust generalization capabilities. The structure learner guides the model to learnmore meaningful node representations and capture shared structure information across domains, and the representationlearner guides the model to disentangle semantic and domain-specific information in node representation. Furthermore,we have observed superior classification performance when s and v are independent. This observation suggests thatdomain-specific and label-independent variation factors disturb the models classification, consequently influencing itsgeneralization capability. Disentangling s and v offers greater advantages for generalization. Second, the results of S1T1, S1T2, and S12T3 demonstrate that the best generalization performance is achieved whenthe training domain originates from diverse datasets. Among all comparative methods, the performance of GraphGlowis second only to our framework, which suggests that integrating structure learning with GNN facilitates the capture ofricher knowledge within graphs. All comparative methods exhibit varying performances under different distributionshift settings. Traditional invariant learning methods exhibit superior performance in scenarios where the source domainoriginates from the same dataset, as opposed to situations involving diverse datasets. It suggests that invariant learningmethods demonstrate limited generalization ability in situations with substantial distribution shifts. Other cross-domainmeta-learning methods do show better generalization ability for cross-domain scenarios. In contrast, cross-domainmeta-learning methods demonstrate superior performance, attributable to meta-learnings advanced ability to captureshared knowledge across domains. Ablation Studies. To answer RQ2, we conduct five ablation studies to evaluate the robustness of key modules, namelystructure learner, representation learner, and MAML. The results of MLDGG and MLDGG-ind are shown in and , where we follow the setting of . In-depth descriptions and the algorithms for these studies andmore results can be found in Appendix D. (1) In MLDGG W/O SL, we input the graph directly into the GNN to learnnode representations and compare its accuracy with MLDGG. We observe declines of 2% to 3% in accuracy across allsettings compared to the full model. Given that GNNs often aggregate task-irrelevant information, which can resultin overfitting and diminish generalization performance, the introduction of the structure learner becomes crucial. Bymitigating the adverse effects of task-unrelated edges, the structure learner facilitates the acquisition of comprehensivenode representations, thereby improving the overall performance. (2) In MLDGG W/O RL, we only keep the structurelearner and just finetune the GNN encoder during the test phase. We observe a more substantial loss in performancedegradation to 3% to 6% across all settings. This indicates that the disentanglement of semantic and variation factorscan enhance the models generalization capability. Class labels are dependent on semantic factors, while variationfactors representing domain-specific elements are not associated with these labels. When the representation learner isabsent, performance degradation occurs, particularly in the presence of OOD samples stemming from distributionalshifts in target domains. Therefore, mitigating the influence of variation factors becomes crucial for improving themodels robustness across diverse domains. (3) In MLDGG W/O MAML, it does not share the semantic and variationencoders across different domains, which significantly decreases model performance by 8% to 10%. This observationindicates the critical role played by the meta-learner modules in facilitating knowledge transfer from source and targetgraphs. The MAML framework serves as an integration for both the structure learner and representation learner,",
  "AmherstJohnsReedCornellYaleAvgTexasCornellWisAvg": "GraphGlow 52.9 1.5 52.3 1.2 60.7 1.7 51.2 1.0 43.5 1.252.153.0 1.1 44.8 1.3 47.0 0.948.3MD-Gram 50.2 0.3 47.9 0.4 62.8 0.4 49.0 0.2 42.7 0.750.553.8 0.2 41.6 0.3 44.2 0.346.5GMeta 20.1 0.8 14.5 1.7 16.0 1.6 16.9 1.4 16.3 2.116.819.8 1.7 16.0 1.4 16.2 1.317.3FLOOD 11.1 0.3 14.0 0.4 11.9 0.3 14.2 0.2 14.3 0.513.119.3 0.2 17.2 0.4 11.0 0.315.8EERM 10.9 2.4 14.3 3.1 12.8 2.0 13.0 2.6 14.4 3.013.120.8 2.3 18.0 2.5 11.2 1.916.7SRGNN 11.7 1.3 15.0 1.8 11.6 1.5 12.7 1.4 13.9 2.613.016.7 1.9 11.4 0.6 10.8 1.513.0Mixup 10.8 1.7 12.9 1.0 12.1 1.3 11.3 1.2 13.2 1.512.117.5 1.1 10.9 1.4 11.1 1.313.2ERM 18.9 2.0 10.2 1.6 11.6 2.2 19.0 2.1 11.9 1.812.320.2 1.4 15.5 2.5 10.6 4.415.4",
  "PTBRTWRUESFRENGBDEAvgTexasCornellWisAvg": "GraphGlow 65.4 0.4 60.7 0.6 75.4 0.970.70.963.1 0.7 54.5 0.6 60.4 0.864.3 59.0 0.5 44.8 0.4 46.6 0.950.1MD-Gram 64.8 0.3 58.7 0.4 73.0 0.1 71.1 0.9 62.0 0.4 52.9 0.5 60.1 0.563.2 54.1 0.4 44.8 0.7 45.0 0.248.0GMeta 10.5 1.9 26.6 1.4 14.8 2.27.1 1.423.7 2.4 13.3 1.0 15.8 2.515.9 20.0 1.9 15.8 2.0 18.6 1.718.1FLOOD 11.4 0.3 24.3 0.2 18.1 0.2 10.2 0.6 24.8 0.1 11.9 0.4 16.0 0.419.1 18.3 0.1 16.9 0.3 11.0 0.415.4EERM 11.4 2.4 29.8 2.5 17.0 1.86.3 2.725.0 2.9 12.3 2.4 16.9 2.917.0 17.5 2.8 12.6 2.9 10.8 1.313.6SRGNN 18.3 1.6 15.9 2.0 15.0 1.8 18.1 1.2 14.7 2.0 10.5 1.8 11.4 1.415.0 18.0 1.1 10.7 1.4 11.5 1.213.4Mixup 19.7 1.0 27.5 1.9 16.4 1.2 17.0 1.8 19.6 1.6 12.0 2.1 13.8 1.718.1 19.7 1.5 12.6 1.0 12.8 0.915.0ERM 16.7 1.3 27.0 2.9 15.0 3.4 13.5 2.6 14.3 2.2 15.7 3.1 14.3 2.015.9 33.5 3.7 38.2 3.0 30.1 1.533.9",
  "MLDGG-ind67.1 0.261.8 0.176.5 0.171.9 0.364.0 0.255.6 0.161.1 0.265.460.5 0.448.3 0.252.3 0.253.7": "thereby enabling efficient knowledge transfer and facilitating effective adaptation to unseen target domains. (4) InMLDGG W/O INNER-SL, we remove each task-specific structure learner so that all tasks share a structure learner. Weobserve declines of 1% to 2% in accuracy across all settings compared to the full model. This indicates that learninginitialization parameters for the structure learner based on the meta-learning framework are conducive to capturingthe structure information shared by different domains and improving the generalization ability. (5) In MLDGG W/OINNER-RL, we remove each task-specific representation learner so that all tasks share a representation learner, whichdecreases model performance by 2% to 3%. This indicates that learning initialization parameters for the representationlearner based on the meta-learning framework can guide the model to learn semantic factors and variation factors, tobetter disentangle to improve generalization ability. The demonstration of the effectiveness of the representation learner. To answer RQ3, we visualize the output rof each node of the GNN, domain-invariant semantic factors s and domain-specific variation factors v respectivelyin (different colors represent different labels). The domain-invariant semantic factors s and domain-specificvariation factors v are disentangled from the node representations r learned from GNNs. We can see that the samplesrepresented by s are more distinguished than those represented by r. The samples represented by v are independent ofclasses. These phenomena indicate that by disentangling the node representation learned from GNN to capture thedomain-invariant semantic information that determines the label, the influence of the variation factors on the labelprediction can be reduced and better generalization performance can be achieved.",
  "Conclusion": "In this paper, we introduce a novel cross-multi-domain meta-learning framework, MLDGG, for node-level graph domaingeneralization. The framework integrates a structure and a representation learner within the meta-learning paradigm tofacilitate knowledge transfer and enable rapid adaptation to target domains previously. The structure learner mitigatesthe adverse effects of task-unrelated edges to facilitate the acquisition of comprehensive node representations of GNNwhile capturing the shared structure information. The representation learner by disentangling the semantic and variationfactors enhances the models generalization. We conduct extensive experiments and the results demonstrate that ourmodel outperforms baseline methods.",
  "PTBRTWRUESFRENGBDEAvgAmherst41JohnsReedCornellYaleAvg": "GraphGlow 65.4 0.5 60.7 0.4 75.4 0.4 70.7 0.4 63.1 0.3 54.5 1.0 60.4 0.7 64.3 53.1 0.8 47.4 1.2 63.2 1.1 50.9 1.1 43.4 1.1 51.6MD-Gram 65.1 0.3 60.9 0.1 73.9 0.2 71.0 0.2 62.6 0.2 53.8 0.1 60.2 0.4 63.9 50.8 0.7 48.3 0.4 63.3 0.5 49.9 0.4 43.1 0.7 51.1GMeta 31.9 1.8 24.0 1.2 27.6 1.8 31.0 2.1 22.8 1.6 26.0 1.1 20.2 1.9 26.2 21.3 2.1 23.2 1.7 19.9 1.6 22.2 1.7 21.5 1.4 21.6FLOOD 24.9 0.6 11.7 0.4 24.1 0.7 15.0 0.3 13.9 0.4 13.0 0.3 14.1 0.4 16.7 19.1 0.1 16.5 0.3 12.0 0.3 25.6 0.4 15.8 0.2 17.8EERM 26.5 2.1 12.3 3.1 25.8 2.8 15.4 2.9 14.8 2.8 13.5 3.0 14.6 2.7 17.6 19.9 4.1 17.1 2.9 12.1 4.3 26.6 3.1 16.9 2.2 18.5SRGNN 11.3 1.2 28.4 1.0 27.7 2.1 29.6 1.2 25.3 1.6 12.7 1.9 17.3 1.5 21.7 18.2 1.8 18.0 1.6 10.6 2.3 24.9 1.8 15.5 1.3 17.4Mixup 12.3 1.7 25.5 1.4 26.9 2.0 30.4 1.2 24.0 0.7 13.1 0.5 17.5 1.3 21.4 16.3 1.6 13.6 1.9 13.2 2.2 22.5 1.4 14.2 1.8 16.0ERM 26.1 2.9 15.0 2.66.4 2.113.8 0.4 17.4 2.8 15.2 3.6 14.0 2.6 15.6 19.3 0.1 24.5 3.9 10.5 2.7 25.0 2.1 12.7 2.8 18.4",
  "Minglai Shao, Dong Li, Chen Zhao, Xintao Wu, Yujie Lin, and Qin Tian. Supervised algorithmic fairness indistribution shifts: A survey. arXiv preprint arXiv:2402.01327, 2024": "Chen Zhao, Kai Jiang, Xintao Wu, Haoliang Wang, Latifur Khan, Christan Grant, and Feng Chen. Algorithmicfairness generalization under covariate and dependence shifts simultaneously. In Proceedings of the 30th ACMSIGKDD Conference on Knowledge Discovery and Data Mining, pages 44194430, 2024. Chen Zhao, Feng Mi, Xintao Wu, Kai Jiang, Latifur Khan, and Feng Chen. Dynamic environment responsiveonline meta-learning with fairness awareness. ACM Transactions on Knowledge Discovery from Data, 18(6):123,2024. Dong Li, Chen Zhao, Minglai Shao, and Wenjun Wang. Learning fair invariant representations under covariateand correlation shifts simultaneously. In Proceedings of the 33rd ACM International Conference on Informationand Knowledge Management, pages 11741183, 2024.",
  "Yujie Lin, Chen Zhao, Minglai Shao, Baoluo Meng, Xujiang Zhao, and Haifeng Chen. Towards counterfactualfairness-aware domain generalization in changing environments. IJCAI, 2024": "Yujie Lin, Chen Zhao, Minglai Shao, Xujiang Zhao, and Haifeng Chen. Adaptation speed analysis for fairness-aware causal models. In Proceedings of the 32nd ACM International Conference on Information and KnowledgeManagement, pages 14211430, 2023. Chen Zhao, Feng Mi, Xintao Wu, Kai Jiang, Latifur Khan, Christan Grant, and Feng Chen. Towards fairdisentangled online learning for changing environments. In Proceedings of the ACM SIGKDD Conference onKnowledge Discovery and Data Mining, 2023. Chen Zhao, Feng Mi, Xintao Wu, Kai Jiang, Latifur Khan, and Feng Chen. Adaptive fairness-aware onlinemeta-learning for changing environments. In Proceedings of the 28th ACM SIGKDD Conference on KnowledgeDiscovery and Data Mining, pages 25652575, 2022.",
  "Divyat Mahajan, Shruti Tople, and Amit Sharma. Domain generalization using causal matching. In Internationalconference on machine learning, pages 73137324. PMLR, 2021": "Fangrui Lv, Jian Liang, Shuang Li, Bin Zang, Chi Harold Liu, Ziteng Wang, and Di Liu. Causality inspiredrepresentation learning for domain generalization. In Proceedings of the IEEE/CVF conference on computervision and pattern recognition, pages 80468056, 2022. Yang Liu, Xiang Ao, Fuli Feng, Yunshan Ma, Kuan Li, Tat-Seng Chua, and Qing He. Flood: A flexible invariantlearning framework for out-of-distribution generalization on graphs. In Proceedings of the 29th ACM SIGKDDConference on Knowledge Discovery and Data Mining, pages 15481558, 2023.",
  "Qitian Wu, Hengrui Zhang, Junchi Yan, and David Wipf. Handling distribution shifts on graphs: An invarianceperspective. arXiv preprint arXiv:2202.02466, 2022": "Hyeonjin Park, Seunghun Lee, Sihyeon Kim, Jinyoung Park, Jisu Jeong, Kyung-Min Kim, Jung-Woo Ha, andHyunwoo J Kim. Metropolis-hastings data augmentation for graph neural networks. Advances in NeuralInformation Processing Systems, 34:1901019020, 2021. Kezhi Kong, Guohao Li, Mucong Ding, Zuxuan Wu, Chen Zhu, Bernard Ghanem, Gavin Taylor, and TomGoldstein. Robust optimization as data augmentation for large-scale graphs. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pages 6069, 2022.",
  "MLDGG: Meta-Learning for Domain Generalization on Graphs (Accepted in KDD 2025)": "Qi Zhu, Natalia Ponomareva, Jiawei Han, and Bryan Perozzi. Shift-robust gnns: Overcoming the limitations oflocalized graph training data. Advances in Neural Information Processing Systems, 34:2796527977, 2021. Yiwei Wang, Wei Wang, Yuxuan Liang, Yujun Cai, and Bryan Hooi. Mixup for node and graph classification. InProceedings of the Web Conference 2021, pages 36633674, 2021. Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical riskminimization. arXiv preprint arXiv:1710.09412, 2017. Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXivpreprint arXiv:1609.02907, 2016.",
  "Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain generalization: A survey. IEEETransactions on Pattern Analysis and Machine Intelligence, 2022": "Qi Qian, Shenghuo Zhu, Jiasheng Tang, Rong Jin, Baigui Sun, and Hao Li. Robust optimization over multipledomains. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 47394746, 2019. Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks forgroup shifts: On the importance of regularization for worst-case generalization. arXiv preprint arXiv:1911.08731,2019. David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, RemiLe Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). In InternationalConference on Machine Learning, pages 58155826. PMLR, 2021. Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy Hospedales. Learning to generalize: Meta-learning for domaingeneralization. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.",
  "Kexin Huang and Marinka Zitnik. Graph meta learning via local subgraphs. Advances in neural informationprocessing systems, 33:58625874, 2020": "Fan Zhou, Chengtai Cao, Kunpeng Zhang, Goce Trajcevski, Ting Zhong, and Ji Geng. Meta-gnn: On few-shot node classification in graph meta-learning. In Proceedings of the 28th ACM International Conference onInformation and Knowledge Management, pages 23572360, 2019. Huaxiu Yao, Chuxu Zhang, Ying Wei, Meng Jiang, Suhang Wang, Junzhou Huang, Nitesh Chawla, and Zhenhui Li.Graph few-shot learning via knowledge transfer. In Proceedings of the AAAI conference on artificial intelligence,volume 34, pages 66566663, 2020. Ning Wang, Minnan Luo, Kaize Ding, Lingling Zhang, Jundong Li, and Qinghua Zheng. Graph few-shot learningwith attribute matching. In Proceedings of the 29th ACM International Conference on Information & KnowledgeManagement, pages 15451554, 2020. Ning Ma, Jiajun Bu, Jieyu Yang, Zhen Zhang, Chengwei Yao, Zhi Yu, Sheng Zhou, and Xifeng Yan. Adaptive-stepgraph meta-learner for few-shot graph classification. In Proceedings of the 29th ACM International Conferenceon Information & Knowledge Management, pages 10551064, 2020. Zhichun Guo, Chuxu Zhang, Wenhao Yu, John Herr, Olaf Wiest, Meng Jiang, and Nitesh V Chawla. Few-shotgraph learning for molecular property prediction. In Proceedings of the web conference 2021, pages 25592567,2021. Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales. Learning tocompare: Relation network for few-shot learning. In Proceedings of the IEEE conference on computer vision andpattern recognition, pages 11991208, 2018. Zhen Tan, Kaize Ding, Ruocheng Guo, and Huan Liu. Graph few-shot class-incremental learning. In Proceedingsof the Fifteenth ACM International Conference on Web Search and Data Mining, pages 987996, 2022.",
  "Antreas Antoniou, Harri Edwards, and Amos Storkey. How to train your maml. In Seventh InternationalConference on Learning Representations, 2019": "Mingkai Lin, Wenzhong Li, Ding Li, Yizhou Chen, Guohao Li, and Sanglu Lu. Multi-domain generalized graphmeta learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 44794487,2023. Junzi Zhang, Jongho Kim, Brendan ODonoghue, and Stephen Boyd. Sample efficient reinforcement learningwith reinforce. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 1088710895,2021.",
  "VeiA collection of nodes in Gei": "FThe similarity matrix of nodesRThe representation matrix of nodes in a graphGAThe learned adjacency matrix of structurelearnerEThe set of domainsEsThe set of source domainsEtThe set of target domainsDThe dimension of node featureTThe set of tasks in meta-learningsThe semantic factorsvThe variation factorsrThe output of GNNEsThe semantic encoderEvThe variation encoderf gA classifierftThe structure learnerfrThe Representation learnertThe initialization parameters of a structurelearnerrThe initialization parameters of a representa-tion learnersThe parameters of a semantic encodervThe parameters of a variation encoderdThe parameters of a decodergThe parameters of GNNsThe weight of the original graphrThe weight coefficient of regularization losslin, loutThe inner loop and outer loop learning rate,respectivelyMThe number of tasksKThe number of source graphs",
  "BModel Details": "The Evidence Lower BOund (ELBO). In this paper, we assume the representation of each node is disentangledinto two factors: a domain-invariant semantic factor s determining the label and a domain-specific variation factor vindependent of labels. A common and effective approach for aligning the model p with the data distribution p(r, y) isthrough maximizing likelihood p(r, y) = log p(r, y). With these two latent variables, the log marginal likelihood can belog p(s, v, r, y)dsdv. However, this is an intractable problem that is difficult to evaluate and optimize. To addressthis problem, one plausible way is to introduce a variational distribution q(s, v|r, y), conditioned on the observedvariables based on variational expectation-maximization (variational EM). Then, a lower bound of the likelihoodfunction can be derived:",
  "(16)": "where Lqs,v|r,y(r, y) is called Evidence Lower BOund (ELBO). The variational distribution q(s, v|r, y) is com-monly instantiated by a standalone model and is regarded as an inference model. Unfortunately, the introducedmodel q(s, v|r, y) fails to facilitate the estimation of p(y|r). To alleviate this problem, we introduce an auxil-iary model q(s, v, y|r) to target p(s, v, y|r), which enables the straightforward sampling of y given r for predic-tion. Meanwhile, q(s, v|r, y) = q(s,v,y|r) q(y|r)means q(s, v, y|r) can help learning inference model q(s, v|r, y), whereq(y|r) :=q(s, v, y|r)dsdv. When the ELBO approaches its maximum, all posterior items gradually tend to convergetowards the prior, thus q(s, v, y|r) = p(s, v, y|r) = p(s, v|r)p(y|s). For p(s, v|r), we instead use inference modelq(s, v|r). Then, q(s, v|y, r) = q(s,v|r)p(y|s)",
  "B.1Complexity Analysis": "In our experiments followed by , the complexity of the structure learner is O(NP), where P is is the number ofpivot nodes. The complexity of GCNs is O(|E|Dd), where |E| and d are the number of edges and classed, D is isthe dimension of the node feature, respectively. The complexity of the representation learner is O(N). Therefore, thecomplexity of our model is O(K(NP +(|E|Dd+N))), where K is the number of source domains and K, P, N,and D, d |E|.",
  "C.1Datasets": "TWITCH-EXPLICIT . It is a gamer network that includes seven networks: DE, ENGB, ES, FR, PTBR, RU, andTW. Each network represents a particular game region. The aforementioned networks have comparable sizes but varyin terms of densities and maximum node degrees. FACEBOOK-100 . This dataset comprises 100 snapshots of the Facebook friendship network, dating back to 2005.Each node represents a user from a particular American university, and the edges indicate the friendships betweenthese users. We use five networks in our experiments: Amherst, John Hopkins, Reed98, Cornell5, and Yale4. WEBKB . It is a web page network dataset. The nodes in the network represent web pages, and the edgessymbolize hyperlinks connecting these web pages. Additionally, the node features are represented using the bag-of-words representation of the web pages. The task is to classify the nodes into one of five categories: student, project,course, staff, and faculty. According to the university. it is split into three networks: Cornell, Texas, and Wisconsin.",
  "GraphGlow employs a meta-learning approach to cultivate a generalized structure learner aimed at discerninguniversally applicable patterns in optimal messaging topologies across diverse datasets": "GMeta exhibits the capacity to generalize to Graph Neural Networks (GNNs) applied to entirely new graphs andlabels that have not been encountered previously. Simultaneously, it showcases the ability to find evidence supportingpredictions based on small datasets within local subgraphs surrounding target nodes or edges. FLOOD employs an adaptive encoder, refined through invariant learning and bootstrapped learning strategies,to enhance performance on a test set. First, it constructs a shared encoder by minimizing the empirical risk acrossvarious domains. Then, it utilizes bootstrapped learning with a self-supervised method to tailor the shared encoder foroptimal adaptation to the test set. MD-Gram is a multi-domain graph meta-learning approach, transforming learning tasks from multiple source-domain graphs into a unified domain. This process facilitates the acquisition of transferable knowledge acrossdomains.",
  "F.1Sensitivity Analysis": "We evaluate the sensitivity of MLDGG to varying numbers of gradient steps during meta-testing and the weight ofthe original graph . The results are shown in Fig 6 and Fig 7. Owing to space constraints, we report the results forthree graphs in WEBKB across the three scenarios. Similar sensitivity trends are observed across other graphs as well.We notice that in-dataset scenarios, fewer update steps are required to attain optimal results compared to cross-datasetscenarios. This is attributed to the greater similarity in semantic information between the test and training domainswhen sourced from the same dataset. Notably, for cross-dataset scenarios, it can quickly adapt to target graphs withvery little fine-tuning, and for in-dataset scenarios, it can achieve good performance even without any fine-tuning. Thisphenomenon indicates that the meta-learned structure learner and representation learner can be applied to the targetgraph to achieve high accuracy. Overall, the model is robust to . When we use only the input graph as GNN input( = 1), the performance degrades, indicating the importance of the structure learner."
}