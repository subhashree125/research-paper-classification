{
  "ABSTRACT": "We consider a ubiquitous scenario in the study of Influence Max-imization (IM), in which there is limited knowledge about thetopology of the diffusion network. We set the IM problem in amulti-round diffusion campaign, aiming to maximize the number ofdistinct users that are influenced. Leveraging the capability of ban-dit algorithms to effectively balance the objectives of explorationand exploitation, as well as the expressivity of neural networks,our study explores the application of neural bandit algorithms tothe IM problem. We propose the framework IM-GNB (InfluenceMaximization with Graph Neural Bandits), where we provide an es-timate of the users probabilities of being influenced by influencers(also known as diffusion seeds). This initial estimate forms the basisfor constructing both an exploitation graph and an explorationone. Subsequently, IM-GNB handles the exploration-exploitationtradeoff, by selecting seed nodes in real-time using Graph Con-volutional Networks (GCN), in which the pre-estimated graphsare employed to refine the influencers estimated rewards in eachcontextual setting. Through extensive experiments on two largereal-world datasets, we demonstrate the effectiveness of IM-GNBcompared with other baseline methods, significantly improving thespread outcome of such diffusion campaigns, when the underlyingnetwork is unknown.CCS CONCEPTS",
  "INTRODUCTION": "Motivated by the rise of influencer marketing in social mediaadvertising, a class of algorithmic problems termed Influence Max-imization (IM) has emerged, starting with the pioneering workof . These algorithms aim to identify the most influentialnodes within a diffusion network for initiating the spread of spe-cific information, thereby maximizing its reach. In many ways, this Permission to make digital or hard copies of part or all of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for third-party components of this work must be honored.For all other uses, contact the owner/author(s).KDD 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s).ACM ISBN 979-8-4007-0490-1/24/08. research directly mirrors the increasingly prevalent and successfulmarketing strategy of targeting key individuals (influencers).The objective of IM is typically formulated by maximizing theexpected spread under a stochastic diffusion model, which charac-terizes the information dissemination process. The work of laidthe foundations for the IM literature, by introducing two prominentmodels: Linear Threshold (LT) and Independent Cascade (IC). Thesemodels, widely adopted in subsequent research, represent diffusionnetworks as probabilistic graphs, where the edges are weighted byprobabilities of information transmission.Selecting the seed nodes maximizing the expected spread is NP-hard under common diffusion models . Despite the developmentof approximate algorithms, exploiting the monotonicity and sub-modularity of the spread, scaling IM algorithms to large networksremains challenging. Acquiring meaningful influence probabilitiesis equally challenging, as learning them from past information cas-cades (e.g., as in ) can be data-intensive and thus impractical.Moreover, the applicability of such models is limited in scenarioswhere historical cascades are not available.In the face of these challenges, since even the most efficient IMalgorithms such as rely on assumptions and parametersthat often fail to capture the complex reality of how informationspreads online, a change in research direction has been followedrecently. It consists of approaches that neither rely on pre-defineddiffusion models nor require upfront knowledge of the diffusionnetwork. Instead, these online methods, such as , learn tospread on the fly. More precisely, they involve a sequential learningagent that actively gathers information through a multi-round influ-ence campaign. In each round, the agent selects so-called seed nodes,observes the resulting information spread, and uses this feedbackto make better choices in subsequent rounds, with the campaignstotal reward being the objective that is to be optimized. Such alearning framework leads naturally to a policy that balances explor-ing unknown aspects (i.e., the diffusion dynamics) with exploitingknown and successful choices (i.e., the high-performing spread seednodes), using multi-armed bandits .We consider in this paper such an an online IM scenario withlimited network information. Specifically, the diffusion graph islargely unknown, except for a set of predefined influencers, repre-senting the potential seeds for information dissemination at eachround of a multi-round diffusion campaign. Additionally, we incor-porate contextual features of both influencers and the informationbeing diffused. Regarding the latter, the rationale is that within acampaign aiming to maximize the reach of a specific message, itsframing and presentation can significantly impact its spread. Forinstance, a political campaign may use various formats like newsarticles, opinion pieces, data visualizations, or multimedia content,each leading to distinct diffusion patterns.",
  "KDD 24, August 2529, 2024, Barcelona, SpainFeng, Tan, and Cautis": "about the diffusion models are made. LogNorm-LinUCB andGLM-GT-UCB are the recent, state-of-the-art approaches tomaximize information diffusion during such IM campaigns. LogNorm-LinUCB directly adapts the LinUCB algorithm by using logarith-mic normalization and contextual information to make sequentialselections of spread seeds, while GLM-GT-UCB employs a gener-alized linear model and the GoodTuring estimator to determinethe remaining potential of influencers. We also compare with FAT-GT-UCB , a context-free model which has the particularity toconsider the fatigue, i.e., an influencers diminishing tendency toactivate basic users as they are re-seeded throughout the campaign.We also generalize several state-of-the-art neural bandit methodsNeuralUCB and NeuralTS , as well as the well-knownLinUCB to our multi-round IM campaign. Finally, we also im-plement a reference model that randomly chooses the influencer(s)at each round, as in .Experimental Setting In our experiments, to reduce the compu-tation cost, we first cluster all the users into 50 groups. For thepre-estimation of the graph weights, we use a 3-layer FC neuralnetwork as the hypothesis class for both (1)and (2) , to estimatethe diffusion probability and potential gain. The functions (1) and(2) that map the users correlations to the weights in the graphsare radial basis functions (RBFs), with their bandwidths set to 5. Forthe GCN model, we explore the use of 3 hops (i.e., = 3) to capturemulti-level relationships within the user graphs, with a 3-layer FCneural network connected at the end. The pooling step sizes toreduce dimensions for the input gradients in (2) are set to1, 000 and 10, 000 respectively for the Twitter and Weibo datasets.Empirical Results For a diffusion campaign, at each round, theenvironment first provides the context, an algorithm then selectsthe rounds influencer(s), and finally, a tweet is sampled for thespecific pair of influencer(s) and context from the dataset. The newactivations are determined by discounting the users previouslyencountered from the set of users associated with the sampledtweet. All our empirical results are averaged over 100 independentruns (the means and standard deviations are reported), and thediffusion budget is set to 500 rounds.Comparison with baselines: We conducted comparisons with variousbaselines on the Twitter and Weibo datasets, varying the numberof chosen seeds () per round within {1, 2, . . . , 5}. The results areshown in and respectively. From the two figures, we canobserve that, across both datasets, our model IM-GNB generallyoutperforms the baselines. Notably, on the Twitter dataset, IM-GNBexhibits a significantly increased advantage over the baselines, asthe number of seeds increases up to = 3. However, this advantagediminishes as continues to grow, as the probability of selectingthe correct arms increases for all models. In fact, for the extremescenario where = || = 10, this results in the same performancesacross all models due to the selection of the entire base set of seeds.Similarly, for the Weibo dataset, IM-GNB demonstrates its largestadvantage at = 4. These results validate our motivation to leveragethe expressivity of both neural networks and bandit algorithms inIM campaigns, enabling us to effectively capture dynamic user-userand user-influencer interactions using GNBs.It is worth noting that, in the Weibo dataset, particularly when is small (e.g., = 1, 2), the performance in the initial rounds is surpassed by certain baseline methods, notably GLM-GT-UCB,which exhibit more rapid learning capabilities. We attribute thisphenomenon to the nature of IM-GNB as a data-driven model, typ-ical of modern deep learning-based approaches. The efficiency ofIM-GNB improves rapidly with the accumulation of data (i.e., asmore rounds pass by), indicating potential slower convergence ini-tially, but yielding better results as the number of rounds increases.Additionally, the Weibo dataset is a publicly available dataset thatconsists of artificially extracted data from diffusion cascades, whilethe Twitter dataset, albeit sparser than Weibo, offers insights closerto real-world IM scenarios.In both datasets, Lognorm-LinUCB generally outperforms theother baselines. While there are instances where GLM-GT-UCBbriefly outperforms Lognorm-LinUCB in the initial stages, the lat-ter demonstrates stable performance with smaller error bars. Thisunderscores the robustness of the log-normal assumption on thereward distribution. NeuralUCB and NeuralTS, as scalarizations ofthe general neural bandit model, exhibit comparable performancesacross both datasets. Notably, their effectiveness lags behind mod-els tailored for multi-round diffusion campaigns when is small.However, when increases, the models are empowered with moredata, showing marked performance improvements.Hyperparameter AnalysisNumber of clusters: We conduct experiments on the number ofclusters in the Twitter dataset with = 2, and the results on thelast round (final accumulated spread) are shown in of theappendix. We observe from that the campaign performanceimproves as the number of clusters increases from 2 to 150 at thebeginning. However, beyond 200 clusters, performance begins todecline. This decline can be likely attributed to insufficient datawithin each cluster for effective learning with the constraints ofa limited budget on the number of rounds. Additionally, compu-tational costs escalate exponentially as the cluster size goes up.Through the analysis, a cluster size of 20 to 50 seems to strike theright balance between performance and computational efficiency.This observation not only validates the initial rationale for cluster-ing users, but also underscores the significance of computationalefficiency in optimizing social campaigns under budget constraints.Boosted Exploration Scores: Bandit algorithms aim to strike a delicatebalance between exploiting known information to maximize short-term gains and exploring unknown options to improve long-termperformance. In this spirit, we also consider in the experimentsa variant of exploration, in which we boost the exploration scoreof unchosen arms having zero reward outcomes, to increase thelikelihood of exploring alternative arms. We run experiments onthe Twitter dataset with = 2 comparing the use of such artificiallyboosted exploration against its absence. The results are presentedin in the appendix, and confirm the effectiveness of thisapproach; this further supports the importance of exploration touncover valuable insights and optimize online decision making. 6CONCLUSIONIn summary, our IM-GNB framework seamlessly leverages the ex-pressivity of GNBs to tackle the challenges of multi-round IM inuncertain environments. Our novel approach tackles key issues inlearning from graph-structured data and makes sequential decisionsin uncertain environments. By incorporating contextual bandits,",
  "extensive experiments, we show that our algorithm outperformsbaseline methods, highlighting the utility of GNBs as a prin-cipled approach to optimize influence campaigns in uncertainenvironments": "2RELATED WORKInfluence Maximization (IM) addresses the challenge of identify-ing a set of seeds (influencers) within a social network to maximizeinformation spread. Researchers first explored this problem in .Later, provided a clear formulation of the problem, includinghow influence spreads through stochastic models like IndependentCascade (IC) and Linear Threshold (LT). They also described theimportant properties of the spread objective, its approximation guar-antees and hardness results. Since then, such stochastic models havebecome widely adopted in the literature, and most works focusedon finding approximate solutions that can be computed efficiently.A key breakthrough was the concept of reverse influence sampling,introduced in and made practical in . Diffusion model-based IM approaches rely on diffusion graphs where the edges arelabeled by weights (spread probabilities). In empirical evaluations,these weights may be data-based (computed from diffusioncascades), degree based, or simply assumed random. Some recentstudies employ representation learning to infer influenceprobabilities from ground-truth diffusion cascades, a resource thatmay not be readily available in many application scenarios. (Seethe recent survey for a review of the IM literature.)Bandits for Influence Maximization By virtue of their versa-tility and sequential nature, bandit algorithms are apt to be usedin IM problems, especially in uncertain diffusion environmentswith which a learning agent may interact repeatedly .A multi-round, sequential setting allows to spread informationand gather feedback, striking a balance between influencing / ac-tivating nodes in each round and learning influence parametersfor uncertain or unexplored network facets. This strategy closelymirrors real-world influencer marketing scenarios, in which cam-paigns often unfold over time. is one of the earliest works thatmap an IM problem formulation to a combinatorial multi-armedbandit (CMAB) paradigm, where diffusions are assumed to fol-low the IC model. IMLinUCB learns the optimal influencersdynamically, while repeatedly interacting with a network underthe IC assumption as well. Vaswani et al. introduces a dif-fusion model-agnostic framework, based on a pairwise-influencesemi-bandit feedback model and the LinUCB-based algorithm, ad-dressing scenarios involving new marketers that exploit existingnetworks. Since the aforementioned approaches leverage a givendiffusion graph topology, the inherent difficulty of obtaining suchdata limits their practical interest.Operating in highly uncertain diffusion scenarios that (i) makeno assumption on the diffusion model and (ii) lack knowledge ofthe diffusion topology and historical activations (cascades), proposes FAT-GT-UCB, where a GoodTuring estimator is usedto capture the utility (called remaining potential) of an influencer,throughout the multiple rounds of a diffusion campaign. They alsoconsider a fatigue effect for influencers, since these may be arerepeatedly chosen in the sequential rounds. GLM-GT-UCB considers the same setting as , while exploiting contextual in-formation (e.g., features pertaining to influencers or the information",
  "Influence Maximization via Graph Neural BanditsKDD 24, August 2529, 2024, Barcelona, Spain": "functions for user , predicting the probability of being influencedby any arm within various contexts. Similarly, for the reward esti-mation for each arm with (1) and (2), (1) takes as input the armfeature vector and context vector, as well as the pre-estimated graph (1), to refine the initial estimation on the diffusion probability, al-lowing to estimate the reward across all users, and the gradient of (1) serves as the input of exploration function (2). Both (1) and (2) undergo continuous training to refine the reward estimationfunction (exploitation) and potential gain (exploration). This itera-tive process ensures that (1) and (2) adapt effectively to diversecontexts and user correlation graphs. 4.2.4Complexity Analysis. Recall from the previous notation con-ventions that we have || = arms, users, and the dimensionsof the feature vectors and context information are 1 and 2 respec-tively. For simplicity, we use to denote the dimension of all theinput gradients, and we assume that the same structure is used forall the FC neural networks in our model. In particular, each neuralnetwork has layers and each layer has neurons.For the pre-estimation of user exploitation and exploration graphs,at each round, the complexity of the pre-defined hypothesis func-tions (1)and (2)(FC neural networks) is || (1 +2) forexploitation and (||) for exploration.For the refined estimation procedure where we use GCNs, as wepredict correlations among all users, the graphs can be seen as com-plete. Assuming that the exploration / exploitation GCNs share thesame NN structure, the time and space complexities for exploitationare || (2(1+2)+(1+2)2) and || (2+(1+2)2+(1 +2)) respectively, and for exploration || (2 +2) and || (2 + 2 + ).From the above discussion, we can observe that the number ofusers and the dimension of the input gradient are the most criticalparameters in determining the time complexity. We consider thefollowing methods to reduce the computational complexity. User clustering. In applications with billions of users on socialmedia, it is impractical and excessively costly to predict diffusionprobabilities and correlations at the granularity of individual users.In response to this challenge, we can leverage the posting activity(e.g., retweeting history) of users to construct a topic distributionvector for each user. We can then cluster users into a specific num-ber of groups, with each user group representing a macro-nodein a smaller social graph. Notably, the theoretical underpinningsoutlined earlier remain applicable to these clustered user groups,and user becomes , = 1, 2, . . . , and =1 = U, with denoting the number of clustered groups. Despite this adjustment,we continue to refer to the user group as user for simplicity.The introduction of clustering can significantly reduce compu-tational cost, transforming the space complexity of the adjacencymatrix in the GCN from to a more computationally efficientscale. Experiments are carried out on the number of clusteringgroups in Sec. 5 to show the impact of the number of clusters onthe performance of the model.",
  "PROBLEM FORMULATION": "We formulate the Influence Maximization (IM) problem with adiscrete-time diffusion model , adopting a combinatorial multi-armed bandit paradigm to estimate the influence spread.IM Problem Within the context of information scenarios charac-terized by stochastic or epidemic information diffusion phenomena,particularly on social media, the information spread is initiated byseed users (influencers) and amplified through sharing and retweet-ing via user interactions. For a campaign of information spreadconsisting of rounds (trials), we select the influencers at eachround to maximize the overall information spread.We are given a known base set of influencers = {}=1 asseeds, a budget of rounds (trials). At each round {1, 2 . . . , },the environment provides us with the message to diffuse, andthere are {1, 2, . . . ,} seeds to be activated initially. With (which has cardinality | | = ) the set of activated seeds, (,)is the rounds spread (all activated users) starting from the chosenseed set . Our objective is to maximize the cumulative and distinctspread of the rounds, i.e., find",
  "(,).(1)": "Adaptation to the bandit setting To adapt the IM problem to acontextual bandit setting, the set of influencers can be consideredthe set of arms to be pulled in rounds. At each round , with theprovided message as the context, the set of arms = {}=1is chosen. For each chosen arm , is the set of basic usersactivated or influenced by seed (arm) . For each basic user ,let denote the total number of times it has been influenced oractivated until round . With the set of activated users (influencespread) as the node semi-bandit feedback, the reward is the numberof new activations as",
  "{ > 0} 1;0 = 0,(2)": "Note that distinct activations are used for the cumulative reward,i.e., a given user will be counted only once in the total reward, evenif it has been influenced several times.Modeling with graph bandits We are mainly motivated by appli-cation scenarios in social media (e.g., information campaigns forelections, online advertising, public awareness campaigns, crisisinformation diffusion, etc.), where users may exhibit similar prefer-ences and influence susceptibility for certain diffusion topics (e.g.,sharing the same political views) initiated by certain influencers(arms), while they may react differently and be more susceptible toother influencers for other topics (e.g., entertainment or sports).Thus, instead of representing the social graph uniformly, inthe bandit setting, we allow each arm at each round to in-duce a distinct graph , (U, ,,) to represent user connectivity.With the 1-dimensional feature vector of arm and the 2-dimensional context vector, the expected reward1 at each round [] brought by arm is defined as",
  ", = (, ,,).(3)": "In ,, each user U = {1, 2, . . . ,} corresponds to a node, is the set of edges connecting users, and , = {, (,) :, U} is the set of weights corresponding to each edge .Modeling real applications, we assume that the weights of the edgesconnecting nodes in , represent users similarity w.r.t. the sameinfluencer (arm ), i.e., the probability to be similarly influencedby arm in round , which is defined as",
  ", (,) = (1) E,|, , E, |, ,(4)": "where , = (, ) the expected diffusion probabil-ity between influencer (arm ) and user under the context ,and (1) : R R R maps the expected diffusion probability ofusers w.r.t. influencer to the weights among users in ,.However, the similarity graph , and the function are un-known in our problem setting. Thus we propose an estimate graph",
  "PROPOSED FRAMEWORK": "Many recent works on the IM problem that exploit banditsfor the exploration-exploitation trade-off assume that the reward isa linear or generalized linear function of arm vectors. Consideringthe high complexity and dynamicity of social network-related data,we use the representation power of neural networks to firstly, learnusers connectivity to build exploitation and exploration graphs andsecondly, learn the underlying reward function and the potentialgains on the estimated reward. The overall framework of our modelis illustrated in .",
  "In this section, we first provide a strategy to estimate the userscorrelations to be influenced by the same arm, forming the basisfor the exploration-exploitation strategy in Sec. 4.2": "4.1.1User exploitation graph. We bridge the users in the socialgraph with diffusion probabilities between influencers and users.The intuition is that given the same message to be diffused (context), users who exhibit high correlations in this graph are more likelyto be influenced by the same influencer. As the context changes, aninfluencer may not exert the same influence on users. Thus, at eachround and for each influencer (arm) , we induce an exploitationgraph (1), to represent the users correlations. In the exploitation graph (1), , the weights among users arereferred to as users correlations w.r.t. the diffusion probability fromarm (hence likelihood to influenced by the same influencer ).For each user U, we use a neural network as the pre-definedhypothesis function (1)= (1)(, ; [P (1)]1) to learn these",
  "L(1)=(1), ; P (1) 2,(8)": "where = 1 if > 0 and 1= 0, else = 0. Recall that we use to denote the total number of times user has been activated(influenced) up to and including round , and we only count thenewly activated nodes at each round. 4.1.2User exploration graph. Recent works on neural bandits take advantage of the representation power of neuralnetworks to learn the uncertainty of estimation (potential gain).These works use the past gradient to incorporate the feature of armsand the learned discriminative information of estimation function((1)(, ) in our work).Qi et al. applied this paradigm in collaborative filteringfor user-item pair prediction in online recommendation scenariosand demonstrated its effectiveness. Since the IM problem sharessimilarity with predicting user preferences towards items (in ourcase susceptibility to influencers), especially when the connections(correlations) among users are reinforced by social ties, we applythe past gradient to quantify the exploration bonus .For a user U, we use a neural network (2)to learn theuncertainty of the estimated diffusion probability between arm and user , i.e., E[, |, , ] (1)(, ), similar to Eq. (6).As in , we apply (2)directly on the previous gradient of (1) .Analogously, the exploration graph (2),= (U, , (2), ) is con-",
  "(2), (,) = (2) (2)(1),(2)(1).(9)": "The previous gradient (1)(, ) = P(1)(, ; [P (1)]1)is the network gradient at round 1, with [P (1)]1 the last up-dated parameters of (1) . In addition, (2) is the function definedin Eq. (6) and (2)will be trained with GD, where the previousgradient of (1)is computed based on the input samples, and theresidual diffusion probability (potential gain on the estimated diffu-sion probability) is the label, with the loss given as",
  "L(2)=(2)(1)(, ) (1)(, )2.(10)": "Regarding the network structure of (1) and (2), since thereare no data characteristics requiring specific models such as RNNsfor sequential dependencies or CNNs for visual content, we simplyemploy an -layer fully connected (FC) neural network at this stagefor initial graph estimation.To summarise, we use (1) , denoting user , to obtain the es-timated diffusion probability from influencer (arm) to (theestimation function is built for each user individually, i.e., there are estimation functions (1)in total), and the exploitation graph",
  "Armselection": ": The framework of IM-GNB. For each arm, we initially take the arm feature vector and the current context vector(, ) as inputs to estimate the diffusion probability for each user-arm pair with (1) . Subsequently, we assess the potentialgain on the diffusion probability with the past gradient of (1) , yielding both exploitation and exploration graphs. With thepre-estimated graphs, we refine the estimate of the diffusion probability for each user-arm pair with (1) and (2). The aggregatereward of the arm across all users is derived from the sum of all the refined individual diffusion probabilities. The potentialgain is measured similarly. Finally, we select the arm with the highest sum of estimated reward and its potential gain.",
  ",(11)": "where is the activation function, P (1)G R(1+2) is thetrainable weight matrix in the GCN model, and is the number ofhops the information propagating over the user graph, indicatingthat after layers a node obtains the feature information from allnodes found hops away in the graph. In the GCN model, (1), is applied to the corresponding weight matrix P (1)Gso that P (1)Gispartitioned for each user U to get the -dimensional arm-userdiffusion representation, corresponding to each row of G R.To further refine the -dimensional arm-user pair representationin G, we add an -layer FC neural network to the GCN model,and for {1, 2, . . . , 1} the representation for each layer is",
  ", 2.(16)": "4.2.2GNN for exploration. Similar to the user graph pre-estimationdescribed in Sec. 4.1, we follow the exploration-exploitation strat-egy by applying a gradient-based exploration function w.r.t. theexploitation function; also see for similar strategies.In round , for each arm , with the induced exploration graph (2), where the pre-estimated weights in Sec. 4.1.2 represent the ex-ploration correlations among users, we apply another GNN model (2) ( (1), (2), ; P (2)) to evaluate the potential gain (the gap be-tween expected reward and estimated reward) for arm , where (1) = P (1) (, ; [P (1)]1), and P (1) and P (2) are theparameters of (1) and (2) respectively.We adopt the same network architecture as in the exploitationnetwork to learn the representation matrix for the exploration graphwith a -hop simplified GCN, and to predict the potential gain withan -layer FC neural network. The architecture of (2) can bealso implemented via Eqs. (11)(13), with the input feature vector (2), R and trainable matrix P (2)G R in the GCNmodel, where is the dimension of input gradient. In the GCN of (2), the input gradient matrix (2), is similarly applied to partition the weight matrix P (2)G , so that each user-arm pair is representedby a -dimensional vector for the purpose of exploration.In the output layer we obtain an -dimensional vector ,,where each element represents the estimated potential gain , R, U (with |U| = ) for each user-arm pair. With the esti-mated potential gains from the output layer, the overall estimatedpotential gain for arm is obtained as the norm of output ,, i.e.,",
  "17return (1), and (2),": "4.2.3IM-GNB arm selection. We summarize the IM-GNB frame-work in Algorithm 1. For an information diffusion campaign with rounds, we select influencers (arms) from a known influencersbase at each round to diffuse the given message . At eachround for each arm , we firstly construct the two user graphsi.e., the exploitation graph and the exploration graph via a procedure(Lines 1317) of pre-estimation on graph weights, which captureusers correlations in terms of exploitation and exploration respec-tively. With the derived graphs, we compute the overall expectedreward , and potential gain , for each arm in Eqs. (14) and (17),as the norms of the output vectors from (1) and (2). Next, weselect the arm set based on the maximum of the sum of rewardestimation and potential gain , + , (Line 8). Finally, for each user U, we train the users neural networks from pre-estimation,and we train for each arm the GNN models (Lines 912).We observe from the above that at each round , (1)will take asinput the feature vector of a certain arm , along with the contextvector to provide an initial estimate on the diffusion probabilityfor user being influenced by arm . Subsequently, the gradientof (1)is employed as input to estimate the potential gain in diffu-sion probability. Parameters in (1)and (2)undergo continuoustraining and updating at each round to refine the approximation",
  "Input gradients. In Sec. 4.1.2 and Sec. 4.2.2, we saw that the in-put dimensions for the previous gradients can pose computational": "challenges due to their potentially large values. This is particu-larly relevant in Sec. 4.2.2, where the input gradient dimension is(1 + 2) + ( 1)2 + . To address this issue, and inspired byapproaches commonly employed in CNN-related works, we use theaverage pooling technique to effectively reduce the input dimensionand improve efficiency. 5EXPERIMENTSIn this section, we evaluate our model IM-GNB on datasets fromTwitter and Sina Weibo. We compare it with baselines also designedfor multi-round diffusion campaigns, and we analyze the compar-ison results in the end. For reproducibility, the IM-GNB code isavailable at Twitter and Weibo are two of the largest social mediaplatforms. We collected the Twitter dataset through its API. In ourcontext analysis for Twitter, we apply -means clustering on thepublic vocabulary glove-twitter-200 , available from the Gen-sim word embedding open-source library2. The resulting clustersprovide centroids that serve as representative themes within thedataset. Subsequently, we represent them as a distribution acrossthese centroids (10 in our experiments) to encode tweets. Each wordin a tweet is assigned to its nearest centroid, resulting in the overalldistribution. The feature vector of the influencer is the normalizedaggregation of all its historical tweets. The Weibo dataset isa publicly available one built for information diffusion studies. Inthis dataset, each post is encoded with a distribution over the 100topics using latent Dirichlet allocation . Similar to the Twit-ter dataset, the feature vector of the influencer is the normalizedaggregate of the topic distribution of all historical tweets.To simulate campaigns on social media, we assume that themarketer has access to only a few most important influencers todiffuse the message in the campaigns. Hence, we fix the size of theinfluencer set by selecting the users with the highest numberof reposts in our Twitter and Weibo logs, and we keep all thetweets related to them. The statistics of the datasets before and afterfiltering are given in Tables 1 and 2. In each campaign, we randomlychose the contexts (tweets), referred to as topic distributions, foreach round from the pool of available contexts within the dataset.",
  ": Comparison of IM-GNB with baselines on the Weibo dataset": "we obtain initial estimates of diffusion probabilities to constructexploitation and exploration graphs. Subsequently, these estimatesare refined with GCNs, to enhance the influence spread. The frame-works scalability, even without prior knowledge of the networktopology, makes it a valuable and versatile tool for optimizing dif-fusion campaigns. Acknowledgements. This work is funded by the Singapore Ministryof Education AcRF Tier 2 (A-8000423-00-00). This research is partof the programme DesCartes and is supported by the NationalResearch Foundation, Prime Ministers Office, Singapore underits Campus for Research Excellence and Technological Enterprise(CREATE) programme. The authors thank Fengzhuo Zhang andJunwen Yang (both NUS) for valuable discussions.",
  "Keke Huang, Sibo Wang, Glenn S. Bevilacqua, Xiaokui Xiao, and Laks V. S.Lakshmanan. 2017. Revisiting the Stop-and-Stare Algorithms for Influence Maxi-mization. Proc. VLDB Endow. 10, 9 (2017), 913924": "Alexandra Iacob, Bogdan Cautis, and Silviu Maniu. 2022. Contextual bandits foradvertising campaigns: A diffusion-model independent approach. In Proceedingsof the 2022 SIAM International Conference on Data Mining (SDM). SIAM, 513521. David Kempe, Jon Kleinberg, and va Tardos. 2003. Maximizing the spread ofinfluence through a social network. In Proceedings of the Ninth ACM SIGKDDInternational Conference on Knowledge Discovery and Data Mining. 137146.",
  "Tor Lattimore and Csaba Szepesvri. 2020. Bandit Algorithms. Cambridge Uni-versity Press": "Yandi Li, Haobo Gao, Yunxuan Gao, Jianxiong Guo, and Weili Wu. 2023. A Surveyon Influence Maximization: From an ML-Based Combinatorial Optimization. ACMTrans. Knowl. Discov. Data 17, 9, Article 133 (Jul 2023), 50 pages. Hung T Nguyen, My T Thai, and Thang N Dinh. 2016. Stop-and-stare: Optimalsampling algorithms for viral marketing in billion-scale networks. In Proceedingsof the 2016 international conference on management of data. 695710. George Panagopoulos, Fragkiskos D Malliaros, and Michalis Vazirgiannis. 2020.Multi-task learning for influence estimation and maximization. IEEE Transactionson Knowledge and Data Engineering 34, 9 (2020), 43984409. Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove:Global vectors for word representation. In Proc. of the 2014 Conference on EmpiricalMethods in Natural Language Processing (EMNLP). 15321543.",
  "Paat Rusmevichientong and John N Tsitsiklis. 2010. Linearly parameterizedbandits. Mathematics of Operations Research 35, 2 (2010), 395411": "Lichao Sun, Weiran Huang, Philip S Yu, and Wei Chen. 2018. Multi-roundinfluence maximization. In Proceedings of the 24th ACM SIGKDD InternationalConference on Knowledge Discovery & Data Mining. 22492258. Youze Tang, Yanchen Shi, and Xiaokui Xiao. 2015. Influence Maximizationin Near-Linear Time: A Martingale Approach. In Proceedings of the 2015 ACMSIGMOD International Conference on Management of Data. 15391554. Youze Tang, Xiaokui Xiao, and Yanchen Shi. 2014. Influence maximization: Near-optimal time complexity meets practical efficiency. In Proceedings of the 2014ACM SIGMOD International Conference on Management of Data. 7586. Michal Valko, Nathaniel Korda, Rmi Munos, Ilias Flaounas, and Nelo Cristian-ini. 2013. Finite-time analysis of kernelised contextual bandits. In Proc. of theUncertainty in Artificial Intelligence (UAI). Sharan Vaswani, Branislav Kveton, Zheng Wen, Mohammad Ghavamzadeh, LaksV. S. Lakshmanan, and Mark Schmidt. 2017. Model-Independent Online Learn-ing for Influence Maximization. In Proc. of the 34th International Conference onMachine Learning. 35303539.",
  "Sharan Vaswani, Laks Lakshmanan, and Mark Schmidt. 2015. Influence maxi-mization with bandits. arXiv preprint arXiv:1503.00024 (2015)": "Zheng Wen, Branislav Kveton, Michal Valko, and Sharan Vaswani. 2017. Onlineinfluence maximization under independent cascade model with semi-banditfeedback. Advances in Neural Information Processing Systems 30 (2017). Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and KilianWeinberger. 2019. Simplifying graph convolutional networks. In Proc. of theInternational Conference on Machine Learning. PMLR, 68616871. Qingyun Wu, Zhige Li, Huazheng Wang, Wei Chen, and Hongning Wang. 2019.Factorization bandits for online influence maximization. In Proceedings of the 25thACM SIGKDD International Conference on Knowledge Discovery & Data Mining.636646.",
  "ASUPPLEMENTARY COMPLEXITYEXPERIMENTS": "We provide in a comparison on running time (in hours) w.r.t.the number of clustering groups when = 2. We can observethat a finer granularity (more groups) may not result in better per-formance (as shown in ), while cost goes up exponentially. Wechose 50 groups that represents a good tradeoff between accuracyand complexity in our experiments.",
  "Here, we present the remaining figures (Figs. 4 and 5) that werementioned in the main text but omitted due to space considerations": "They pertain to the analysis of the number of clustering groupsand for artificially boosted exploration. shows that the campaign performance improves as thenumber of clusters increases from 2 to 150 at the beginning. How-ever, beyond 200 clusters, performance begins to decline. This de-cline can be likely attributed to insufficient data within each clusterfor effective learning under the constraints of a limited roundsbudget. confirms the effectiveness of artificially augmenting / boost-ing the exploration score of the unchosen arms with zero rewardoutcomes, in order to increase the likelihood of exploring alterna-tive arms."
}