{
  "ABSTRACT": "Subgraph representation learning is a technique for analyzing localstructures (or shapes) within complex networks. Enabled by recentdevelopments in scalable Graph Neural Networks (GNNs), this ap-proach encodes relational information at a subgroup level (multipleconnected nodes) rather than at a node level of abstraction. Weposit that certain domain applications, such as anti-money laun-dering (AML), are inherently subgraph problems and mainstreamgraph techniques have been operating at a suboptimal level of ab-straction. This is due in part to the scarcity of annotated datasets ofreal-world size and complexity, as well as the lack of software toolsfor managing subgraph GNN workflows at scale. To enable work infundamental algorithms as well as domain applications in AML andbeyond, we introduce Elliptic2, a large graph dataset containing122K labeled subgraphs of Bitcoin clusters within a backgroundgraph consisting of 49M node clusters and 196M edge transactions.The dataset provides subgraphs known to be linked to illicit activityfor learning the set of shapes that money laundering exhibits incryptocurrency and accurately classifying new criminal activity.Along with the dataset we share our graph techniques, softwaretooling, promising early experimental results, and new domaininsights already gleaned from this approach. Taken together, wefind immediate practical value in this approach and the potentialfor a new standard in anti-money laundering and forensic analyticsin cryptocurrencies and other financial networks.",
  "These authors contributed equally to this research": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from MLF 24, August 26, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",
  "Artificial Intelligence, Machine Learning, Public Dataset, GraphNeural Networks, Subgraph Representation Learning, FinancialForensics, Cryptocurrency, Anti-Money Laundering": "ACM Reference Format:Claudio Bellei, Muhua Xu, Ross Phillips, Tom Robinson, Mark Weber, TimKaler, Charles E. Leiserson, Arvind, and Jie Chen. 2024. The Shape of MoneyLaundering: Subgraph Representation Learning on the Blockchain withthe Elliptic2 Dataset. In KDD MLF 24: KDD Workshop on Machine Learningin Finance, August 26, 2024, Barcelona, Spain. ACM, New York, NY, USA,7 pages.",
  "INTRODUCTION": "Over the last few years, the development of graph neural networks(GNNs) has allowed the extension of deep learning methods todata that have an underlying non-Euclidian structure. GNNs offer arich variety of models and architectures that enable the learning ofmeaningful representations for nodes, edges, and the entire graphs.These variants have found applications in diverse fields such asrecommender systems, chemistry, traffic control, physics, and more. However, when dealing with complex graph structures, it is of-ten possible to identify subgraphs of particular interest. The recentemergence of subgraph representation learning has allowed theprediction of subgraph properties within the larger graph structure. By leveraging this approach, it is possible to gain valuableinsights into the characteristics and behavior of these subgraphswithin the broader background graph.Interesting subgraphs can be found in financial graphs suchas blockchain-based cryptocurrencies. Since their inception withBitcoin , one of the primary features of cryptocurrency is theimmutable and transparent record of all transactions ever logged",
  "KDD MLF 24, August 26, 2024, Barcelona, SpainBellei et al": "move funds through accounts at larger cryptocurrency exchanges,sometimes without the awareness or approval of the exchange. Anested service might receive a deposit from one of their customersinto a cryptocurrency address, and then forward the funds to theirdeposit address at an exchange. Nested services are known to fre-quently have less stringent customer due diligence checks than thecryptocurrency exchanges they utilise, or sometimes have no suchanti-money laundering checks at all, resulting in their misuse forcryptocurrency laundering - potentially causing them to featurein subgraphs deemed by the model as suspicious.",
  "A MASSIVE DATASET FOR SUBGRAPHS": "Available real-world datasets for subgraph representation learninginclude PPI-BP, HPO-METAB, HPO-NEURO, and EM-USER. Detailedproperties for each can be found in [2, Appendix B]. The largestbackground graphs among these datasets constitute 100K nodesand 5M edges, with the number of subgraphs varying between324 and 4,000 and with nodes per subgraph ranging between 10and 155. To our knowledge, these are the largest subgraph datasetsavailable, but they are not so large indeed. This limits scientificadvancement in this field because resolving scalability constraintsand other challenges common to massive network structures areindeed central to the promise of GNNs in many practical settings.To enable research on subgraph learning on a much larger scale,we present Elliptic2, a fully connected network of Bitcoin addressesand transactions between them comprising a background graphalmost three orders of magnitude larger than any availablereal-world dataset that we know of. Within this backgroundgraph are many small subgraphs labeled as suspicious or licit, thetask at hand being the binary classification of suspicious subgraphs.The release of this dataset follows our publication of a standardgraph dataset in 2019, which contained Bitcoin transactions andfocused on node classification. Both datasets enable the advance-ment of research in scalable Graph Neural Networks and theirapplications for anti-money laundering in cryptocurrency.",
  "Retrospect on Elliptic1": "In 2019, we published a labeled graph dataset of Bitcoin transac-tions named The Elliptic Data Set (hereafter referred to as Elliptic1)on Kaggle with an accompanying paper demonstratingthrough experimental results on how GNNs can be used to ex-tract hidden relational information that can be fed to classificationmodels for significant performance boosts. The dataset comprised204K node transactions with 166 features and 234K directed edgepayment flows. Approximately 2% of the node transactions werelabeled as illicit and 21% labeled as licit. The task presented was abinary node classification task, predicting whether a node trans-action was broadcasted to the Bitcoin network by a licit or illicit entity. As of this publication, the dataset has been viewed morethan 100K times and downloaded almost 10K times, with the paperreceiving approximately 400 citations. Traction in both the machinelearning and AML communities motivated us to publish an evenlarger dataset we call Elliptic2 with a modified structure and theaddition of subgraph labels to enable subgraph classification as apotentially powerful new tool for AML professionals. While ourown novel methods on this new dataset are forthcoming, we of-fer this dataset to the community now in the interest of scientificadvancement and the public good.",
  "Introducing Elliptic2": "Our task with Elliptic2 is to combat financial crime by identifyingwhether a specific flow of bitcoins may be linked to money launder-ing activity, specifically the attempt to convert the profits gainedfrom illegal actions into fiat currency or other cryptocurrenciesthrough a legitimate service. For the discussion that follows, someuseful definitions are needed. Definition 2.1. A cluster is a set of Bitcoin addresses thought tobe controlled by a single person or organization.A licit cluster is owned by a licit entity (exchange, walletprovider, miner, licit service, etc.). An illicit cluster is owned byan illicit entity (dark market, scam, hack, etc.). Clusters that areneither licit nor illicit are deemed unknown (e.g., unlabeled clusters).A suspicious path is a path that connects an illicit cluster to alicit cluster. An illicit path is a path that connects an illicit clusterto an illicit cluster. A path that is neither licit nor suspicious norillicit is considered neutral. 2.2.1Assumptions. The definitions above rest on the followingassumption: a path on the blockchain connecting an illicit clusterto a licit cluster without a change of ownership of the funds likelyrepresents the activity of money laundering by a criminal personor organization. The idea is that criminals who intend to deposittheir funds at a legitimate service will try to evade detection on theblockchain, creating a distinct subgraph with associated shape andfeatures that a machine learning model should be able to identify. : Illustrative example of the dataset, with its back-ground graph and annotated subgraphs. Each node repre-sents a Bitcoin cluster, a collection of Bitcoin addresses con-trolled by the same entity, and each edge representing a trans-action between them.",
  ": Dataset properties for the background graph (top) and for the licit and suspicious subgraphs (bottom)": "addresses is done applying well known as wellas proprietary heuristics, human analysis and domain expertise.While annotating the subgraphs, for intellectual property reasonsonly a subset of Elliptics cluster labels were used.The dataset is built following the steps provided below. (1) Define the time window, the maximum number of hops tobe traversed, and an early stopping condition when a changeof ownership is likely to happen (e.g., when an unknowncluster with high activity is found during traversal).",
  "(7) Annotate subgraphs for the final output, keeping only un-known nodes that are part of either licit (licit subgraphs) orsuspicious (suspicious subgraphs) paths": "The time window was chosen to be 1 year of blockchain data, andthe maximum number of hops was 6 for computational reasons.The transactions at step (3) were capped to a maximum for eachcluster, to help balance the dataset for different actors with varyingblockchain activity. 2.2.3Overview of Elliptic2. An overview of some of the propertiesof the dataset can be found in . The dataset is made of abackground graph of 49M clusters and 196M edges. A subset of thebackground graph consists of labeled (licit/suspicious) subgraphs,as shown in . Note that the only labels present in the datasetare those associated with these subgraphs, and that the vast major-ity of the nodes in the graph do not belong to labeled subgraphs.Only about 2% of the subgraphs are labeled suspicious with theremainder being labeled licit, thus introducing the challenge of asignificant class imbalance. On average, the size of the subgraphs : Construction of the dataset requires labeling pathsfirst and then labeling subgraphs. In the example above, thereare 3 licit paths (I. 13 14 15 19; II. 16 17 15 19; III. 23 22 24), 1 illicit path (1 7 4 5 6), 3suspicious paths (I. 1 7 8 9 12; II. 1 7 8 10 11 12; III. 20 21 22 24), and 1 neutral path (1 2 3). The result is one licit subgraph and one suspicioussubgraph (note that the subgraph 21,22 is unlabeled as it ismade of both a suspicious and a licit path).",
  ": Example of a size-3 suspicious subgraph, connect-ing the profits of a scam to two exchanges (Note: the labelcategories are not available in the dataset)": "is small but the tail of the distribution is significant. An example ofa size-3 suspicious subgraph is presented in .Each node in the background graph has 43 features, and eachedge 95 features. The node features include, among others, thenode size (number of addresses) and number of transactions. Bothare examples of features that can vary by several orders of magni-tude (from 1 to millions). The edge features include the transactionvolume, fee, timestamp. To protect the intellectual property of El-liptic, most features were categorized into bins. The number of binsused varied between features. This process converted continuousnumerical features into ordinal ones.",
  "USAGE OF ELLIPTIC23.1The anti-money laundering perspective": "The setup of Elliptic2 is tailored to the regulatory compliance con-straints that companies interacting with cryptocurrency must ad-here to. As in the traditional AML setting, companies receivingfunds are usually required to make a determination regarding thelegitimacy of said funds with regard to potential connections tosuspicious activities or organizations. For this purpose, the taskproposed with Elliptic2 is a binary classification with the objec-tive of determining whether a subgraph is suspicious or not. Thetransparency of Bitcoin is helpful in that it allows for open, privacy-preserving forensic analysis of observable patterns (such as sub-graph shapes) on the public blockchain. Utilizing advanced machinelearning techniques to reduce label dependence, the promise ofaccurate anti-money laundering in cryptocurrency at scale seemsdirectionally attainable. Although some typologies of money laun-dering in cryptocurrency have been identified that can help tacklethis problem , a general solution to it is still difficult to achieve,for example in low profile cases where timeliness is important(such as when scammers try to quickly cash out the proceeds oftheir crime).",
  "The AI perspective": "The Elliptic2 dataset is a timely contribution to the AI community,as graph representation learning research gained explosive tractionover the years and demands real-life, large-scale, and challengingdatasets to benchmark methods and models, typically GNNs andemergingly graph transformers . Example benchmark contribu-tions include Benchmarking GNNs , which offers an extensibleframework for reproducible benchmarking; and the Long RangeGraph Benchmark , which designs graph datasets that can testa models capability in reasoning over long-range interactions ofnodes. These contributions generally do not involve massive graphs,for which the Open Graph Benchmark is an alternative exam-ple, composed of several largest graphs widely used today. Thesegraphs include ogbn-papers100M and MAG240M , which sup-port not only the development of machine learning models but alsothe research on system design.Elliptic2 is a similarly large-scale dataset that can be used tobenchmark the scalability of graph models and the efficiency oftraining systems. It represents a real-life use case in the financialdomaincryptocurrencyfor which there are rarely public datasetsavailable for benchmarking.Besides the large scale and the unique domain, more importantly,Elliptic2 supports the research of an emerging topic: subgraph rep-resentation learning. Traditionally, graph benchmark datasets areused to perform node-level tasks (classification and regression),graph-level tasks (similar), and edge-level tasks (link prediction).Instead, Elliptic2 can be used to perform subgraph-level tasks. Sub-graph classification is mathematically defined in the following. Definition 3.1. Given a graph , let denote a subgraph of itand use to denote the subgraph relation; i.e., . Let Labelbe the label space, Train be the set of training indices, and Test bethe set of test indices. Then, the problem of subgraph classificationis that given a collection of labeled training subgraphs {( ,) |",
  ", Label, Train}, predict the labels of the subgraphsfrom the test set { | , Test}. Note that each subgraphmay be disconnected and different subgraphs may overlap": "Subgraph classification is relatively less studied, compared withnode/graph classification. A straightforward approach is to applya graph model (e.g., GNN), obtain node representations, performpooling (e.g., averaging) over the subgraph nodes, and apply aprediction head to predict the subgraph label. While being a com-mon baseline, this approach is outperformed by several recentlyproposed methods . Elliptic2 is a useful testbed for newmethods regarding performance and scalability benchmarking.",
  "RESULTS": "We experimented with three subgraph classification methods onElliptic2: a conventional GNN trained with the subgraphs as inde-pendent graphs (named GNN-Seg), Sub2Vec , and GLASS .Sub2Vec uses random walk samples within the subgraphs to buildsubgraph embeddings. Both GNN-Seg and Sub2Vec neglect thebackground graph, which may provide important information com-plementing the subgraphs themselves (e.g., the edges crossing asubgraph and the background graph), while GLASS makes use of it.The GNN architecture of GLASS employs basic message-passinglayers, supplemented with additional linear layers for node labeling,and is configured to have two layers. Hyperparameters mostlyfollow the default configuration provided by the authors of GLASS,with a slight tuning of the batch size (4000) and the learning rate(0.001) to improve training speed and quality.We performed a random 80:10:10 split for training, validation,and testing, respectively. The experiments were conducted on aLinux server with 160 CPU cores and 1.2TB of RAM. We did notuse GPUs, because the dataset was too large for the GPU memoryto host all graph and intermediate data. For the same reason, wedid not use the node/edge features. See for a discussionon how one can perform training and inference for datasets of sucha large size, by incorporating neighborhood sampling and using ascalable training sytem.",
  ": Performance of different subgraph classificationmethods on Elliptic2": "compares the performance of the three methods underthree evaluation metrics: micro-averaged F1, PR-AUC, and ROC-AUC. We see that GLASS significantly outperforms the other twoapproaches under the PR-AUC and ROC-AUC metrics, while beingcompetitive on F1. Because of the high imbalance of the labels, theAUC metrics reflect a more fluid choice of the labeling threshold;thus, a higher performance on these metrics translates to a stronger",
  ": Confusion matrix obtained by the GLASS method.Positive is the suspicious class": "To provide a more comprehensive picture of the predictabilityof GLASS in the context of class imbalance, we show the confu-sion matrix in . At least one in four predicted suspicioussubgraphs is truly suspicious, and 85% of the suspicious subgraphsare correctly detected, indicating that GLASS maintains reasonablefalse positive and false negative rates.In terms of computation, Sub2Vec has a lengthy pre-processingstage that takes around seven hours to finish but requires very littletime for model training, whereas GNN-Seg and GLASS requireheavy computation in the training of the GNN model. Overall, allmethods were able to converge in several days of training with aninference time that was less than 8 hours.",
  "Insights from a cryptocurrency exchangeservice": "A cryptocurrency exchange was asked to validate the model predic-tions using their own off-chain insights. Specifically, 52 subgraphsdeemed suspicious and which ended with deposit transactions tothe exchange were chosen, since the exchange would hold infor-mation on the account holders that received these deposits. Thesedeposit transactions were shared with the exchange. On review, theexchange found that 14 of the 52 accounts receiving these depositswere potentially involved in illicit activity, based on their own off-chain insights, obtained through customer due diligence and otheroff-chain information. Notably, of these 14 accounts, 8 had beendefinitively associated with money laundering or fraud. Despitethe exchange having no positive indications that the remaining 38accounts were involved in illicit activity, this does not exclude thepossibility that they were.According to the exchange, less than 0.1% of customer accountshave been associated with money laundering or fraud, whereas atleast 26.9% of the accounts highlighted by the model predictionswere found to have such an association. This suggests that theempirical precision of our model significantly surpasses that of anave random model.",
  "Identifying the source of funds in suspicioussubgraphs": "Another approach to verifying the model predictions is to provethat the unlabeled nodes that fund subgraphs predicted as suspi-cious are in fact illicit entities. To that end, investigations wereundertaken to identify the origin of funds flowing into subgraphsdeemed suspicious, i.e. to identify the entity controlling the nodepreceding the subgraph. These investigations utilised open sourceresearch and other standard identification techniques and resultedin the identification of a number of nodes. For example: (1) At least sixty subgraphs deemed suspicious received theirfunds from a node identified to be a cryptocurrency mixer.Mixers provide obfuscation services and are heavily-used bythose laundering proceeds of illicit activity.",
  "(2) Two suspicious subgraphs received their funds from a Panama-based ponzi scheme": "(3) At least one hundred suspicious subgraphs received theirfunds from a node identified to be a bot that enables anony-mous cryptocurrency trading on a messaging platform. Itis functionally similar to a service offered by the cryptocur-rency exchange Bitzlato, which was shut down after almosthalf of its transactions were linked to criminal activities .",
  "Identification of known cryptocurrencylaundering patterns": "Further validation of the model results can be obtained by examin-ing the suspicious subgraphs for known cryptocurrency launderingtransaction patterns.Many of the suspicious subgraphs were found to contain what areknown as peeling chains. This refers to a transaction pattern cre-ated when a cryptocurrency user sends or peels cryptocurrencyto a destination address, while the remainder is sent to anotheraddress under the users control. This happens repeatedly to forma peeling chain. The pattern can have legitimate financial privacypurposes, for example to avoid address reuse, which can enable ausers transactions to be easily linked together. However the patterncan also be indicative of money laundering, as described in variouscriminal cases , especially where the peeled cryptocurrencyis repeatedly sent to an exchange service. In traditional finance thisis known as smurfing, where large amounts of cash are structuredinto multiple small transactions, to keep them under regulatoryreporting limits and avoid detection. Given the model was usingnode degree, it is understandable that peeling chains are a patternvisible to it, as a peeling chain is likely to result in a chain of nodesall with matching node degree.Aside from peeling chains, many of the suspicious subgraphs con-tain apparent nested services in the vicinity of the final depositsinto cryptocurrency exchanges. Nested services are businesses that",
  "SUBGRAPH LEARNING AT SCALE": "The large size of Elliptic2 poses substantial challenges for train-ing GNNs. It is imperative to build scalable training and inferencesystems that address the bottlenecks and workloads of subgraph rep-resentation learning. We use GLASS as a motivating example,because it is a simple adaptation of node-classification GNNs whilebeing provably expressive and empirically effective. Specifically,GLASS appends an extra, binary attribute to the feature vector,indicating whether a node belongs to the subgraphs of interest.The authors show that such a 0-1 labeling trick enables learningany function over subgraphs, provided that the backbone GNN issufficiently expressive.Subgraph methods like GLASS require some changes to thosefor node classification. Here, we introduce a scalable GNN train-ing system, SALIENT , together with its improved successorSALIENT++ , which was designed for node classification work-loads. Then, we discuss how systems like SALIENT/SALIENT++can be adapted to address subgraph classification workloads.",
  "SALIENT and SALIENT++": "Large-scale, distributed training of GNNs is faced with two majorbottlenecks: the cost of neighborhood sampling dominates thatof model evaluation and the distributed storage of node featuresincurs heavy inter-machine communications.Neighborhood sampling is a means to reduce the explosive sizeof the -hop neighborhood of a minibatch of training nodes. Thisstrategy reduces the computation and memory required in mini-batch training. A commonly employed neighborhood samplingalgorithm is called nodewise sampling wherein the sampled -hopneighborhood is computed by sampling up to neighbors for eachnode in the ( 1)-hop neighborhood. Neighborhood sampling isperformed in CPU memory, as a main computational componentof the data loader; however, the CPU throughput is way below thatof GPUs, which perform model evaluations based on the sampledneighbors.To address the bottleneck of neighborhood sampling, SALIENT implements the sampler in C++ by using the most efficient datastructures, performs shared-memory parallel batch preparation byusing C++ threads as opposed to PyTorchs multi-processing, andpipelines the data transfers between GPUs and CPUs to maximizeGPU utilization. These performance engineerings enable SALIENTto achieve a speedup of 3 over the standard GNN system imple-mented by using the popular PyTorch-Geometric library with asingle GPU and a further 8 parallel speedup with 16 GPUs, on the ogbn-papers100M dataset, one of the largest benchmarks for graphdeep learning.When node features are partitioned (such that each partitioncontains a subset of nodes) and stored in different machines, thecommunication of features across machines becomes another train-ing bottleneck. Inter-machine communication occurs because thesampled -hop neighborhood inevitably includes nodes outside apartition (i.e., those stored in another machine).To reduce the communication volume, SALIENT++ employsa static caching policy that caches the features of the frequentlyaccessed out-of-partition nodes. This policy is based on a techniquecalled vertex-inclusion probability (VIP) analysis, which works inthe following. Given a graph = (, ) and T of vertices inthe training set, VIP analysis begins by estimating the probabilitythat a given vertex will be present in a randomly selected mini-batch of size . The probability that any given vertex appears insuch a minibatch is |T |/| |. An iterative process is then performedto calculate the probability that a vertex will be sampled, usingneighborhood sampling, in each layer of the GNN. This cachingpolicy enables SALIENT++ to achieve a 12.7 speedup over an-other popular training system, DistDGL, on eight machines for theogbn-papers100M benchmark.",
  "Adapting node-classification GNNs tosubgraph classification workloads": "For subgraph methods such as GLASS, existing efficient trainingsystems for node classification can be modified to handle subgraphclassification workloads. We take SALIENT and SALIENT++ forexample.First, the neighborhood sampling code in SALIENT can be adaptedto operate on subgraphs with only minor modifications to the com-position of minibatches. A minibatch of subgraphs can be repre-sented as a list of nodes combined with metadata indicating theranges in the list that correspond to distinct subgraphs. A minibatchcan be constructed during training by shuffling the subgraphs in thetraining set and then forming a minibatch of the selected subgraphscontained nodes. With these adaptations, the existing fast samplingcode in SALIENT can be used to address the computational bottle-necks of neighborhood sampling in subgraph classification.Second, the VIP analysis technique used by SALIENT++ can beadapted to handle subgraph workloads in a fairly straightforwardmanner as well. Conceptually, one can reduce the problem to that ofcomputing an optimal static caching policy for a node classificationworkload with an augmented graph and sampling scheme. Theaugmented graph contains all of the nodes and edges of ,but additionally contains one extra node per subgraph which isconnected to all nodes in contained in that subgraph by directededges. The augmented sampling scheme includes one extra layerthat samples all neighbors of the subgraph nodes. VIP analysis maynow be performed on by treating the set of added subgraphnodes as the training set of a node classification workload. Inpractice, there is no need to actually materialize as the originalgraph combined with a list of subgraphs in is sufficient toperform the required update.",
  "SUMMARY": "We have contributed a second large, labeled cryptocurrency trans-action data set, Elliptic2, five years after Elliptic1 was published.Elliptic2 is more than two orders of magnitude larger than Elliptic1and it supports a different machine-learning task subgraph clas-sification for AML. It offers research opportunities in not onlyfinancial forensics but also machine learning, particularly, subgraphrepresentation learning. We demonstrated its utility through train-ing subgraph-based predictive models and discovered an effectiveone: GLASS.The model produced very promising results, identifying illicitactivity based on on-chain patterns, which were previously onlyidentifiable through off-chain information, once funds entered aregulated exchange service. This opens the way to using the outputof these models as an effective compliance tool.The model also successfully directed us towards previously un-known illicit cryptocurrency wallets, based on the subgraphs -i.e. based on the way funds from these wallets were being laun-dered. This could be utilised by blockchain analytics companies,law enforcement investigators and financial regulators to identifycryptocurrency wallets involved in illicit activity.While the model applied here has demonstrated promising re-sults, future studies can improve the prediction performance byusing a scalable training system that incorporates node/edge fea-tures into the model under a computational budget. The inclusionof more features could enable discovery of more sophisticated andpreviously undocumented money laundering strategies, leading tothe more accurate identification of financial crime.",
  "Weihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong, and JureLeskovec. 2021. OGB-LSC: A Large-Scale Challenge for Machine Learning onGraphs. In NeurIPS Datasets and Benchmarks Track": "Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, BowenLiu, Michele Catasta, and Jure Leskovec. 2020. Open Graph Benchmark: Datasetsfor Machine Learning on Graphs. Preprint arXiv:2005.00687. Tim Kaler, Alexandros Iliopoulos, Philip Murzynowski, Tao Schardl, Charles ELeiserson, and Jie Chen. 2023. Communication-Efficient Graph Neural Networkswith Probabilistic Neighborhood Expansion Analysis and Caching. Proceedingsof Machine Learning and Systems 5 (2023). Tim Kaler, Nickolas Stathas, Anne Ouyang, Alexandros-Stavros Iliopoulos, TaoSchardl, Charles E Leiserson, and Jie Chen. 2022. Accelerating training andinference of graph neural networks with fast sampling and pipelining. Proceedingsof Machine Learning and Systems 4 (2022), 172189. Sarah Meiklejohn, Marjori Pomarole, Grant Jordan, Kirill Levchenko, DamonMcCoy, Geoffrey M Voelker, and Stefan Savage. 2013. A fistful of bitcoins:characterizing payments among men with no names. In Proceedings of the 2013conference on Internet measurement conference. 127140. Malte Mser and Arvind Narayanan. 2022. Resurrecting Address Clustering inBitcoin. In Financial Cryptography and Data Security - 26th International Con-ference, FC 2022, Revised Selected Papers. Springer Science and Business MediaDeutschland GmbH, 386403. Satoshi Nakamoto. 2008. Bitcoin: A peer-to-peer electronic cash system. (2008). United States of America v. Virtual Currency Accounts. 2020.AF-FIDAVIT IN SUPPORT OF ISSUANCE OF WARRANT OF ARREST INREM.",
  "Elliptic Data Set. 2019. Xiyuan Wang and Muhan Zhang. 2022. GLASS: GNN with Labeling Tricksfor Subgraph Representation Learning. In International Conference on LearningRepresentations": "Mark Weber, Giacomo Domeniconi, Jie Chen, Daniel Karl I. Weidele, ClaudioBellei, Tom Robinson, and Charles E. Leiserson. 2019. Anti-Money Launderingin Bitcoin: Experimenting with Graph Convolutional Networks for FinancialForensics. arXiv:1908.02591 [cs.SI] Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu,Lifeng Wang, Changcheng Li, and Maosong Sun. 2021. Graph Neural Networks:A Review of Methods and Applications. arXiv:1812.08434 [cs.LG]"
}