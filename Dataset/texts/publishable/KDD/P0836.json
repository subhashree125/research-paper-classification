{
  "Department of System Engineering and Engineering Management, Chinese University of Hong Kong": "AbstractWe propose an ontology-grounded approach to Knowledge Graph (KG) construction using Large Lan-guage Models (LLMs) on a knowledge base. An ontology is authored by generating Competency Questions(CQ) on knowledge base to discover knowledge scope, extracting relations from CQs, and attempt toreplace equivalent relations by their counterpart in Wikidata. To ensure consistency and interpretabilityin the resulting KG, we ground generation of KG with the authored ontology based on extracted relations.Evaluation on benchmark datasets demonstrates competitive performance in knowledge graph construc-tion task. Our work presents a promising direction for scalable KG construction pipeline with minimalhuman intervention, that yields high quality and human-interpretable KGs, which are interoperable withWikidata semantics for potential knowledge base expansion.",
  ". Introduction": "Knowledge Graphs (KGs) are structured representations of information that capture entitiesand their relationships in a graph format. By organizing knowledge in a machine-readableway, KGs enable a wide range of intelligent applications, such as semantic search, questionanswering, recommendation systems, and decision support . The ability to construct high-quality, comprehensive KGs is thus critical for harnessing the power of these technologiesacross various domains.Traditionally, the process of constructing KGs has relied heavily on manual effort by domainexperts to define the relevant entities and relationships, populate the graph with valid facts,and ensure logical consistency . However, this manual curation approach is time-consuming,expensive, and difficult to scale to large, evolving domains. There is a strong need for (semi-)automatic methods that can aid the KG construction process by extracting structured knowledgefrom unstructured data sources such as text.Recent years have seen growing interest in leveraging Large Language Models (LLMs) forvarious knowledge capture and reasoning tasks . Pre-trained on vast amounts of text data,LLMs can generate fluent natural language and have been shown to memorize and recallfactual knowledge , . However, directly applying LLMs to KG construction still facesseveral challenges. First, LLMs may generate inconsistent or redundant facts due to the lack",
  "arXiv:2412.20942v1 [cs.AI] 30 Dec 2024": "of an explicit, unified schema . Second, the generated KGs may be incomplete or biasedtowards the knowledge present in the LLMs training data, which may not fully cover thetarget domain, especially for proprietary documents not included in pre-training set. Finally,it can be challenging to integrate LLM-generated KGs with existing knowledge bases due tomisalignment with standard ontologies.",
  ": Flowchart of proposed approach": "In this work, we propose a novel approachthat harnesses the reasoning power of LLMsand the structured schema of Wikidata toconstruct high-quality KGs for proprietaryknowledge domains. Our approach begins bydiscovering the scope of knowledge throughthe generation of Competency Questions (CQ)and answers from unstructured documents.We then summarize the relations and prop-erties from these QA pairs into an ontology,matching candidate properties against thosedefined in Wikidata and extending the schemaas needed. Finally, we use the resulting on-tology to ground the transformation of CQ-answer pairs into a structured KG. By incorporating the Wikidata schema into our pipelineand grounding generation of KG on the same ontology, we aim to reduce redundancy, leveragethe implicit knowledge captured during LLM pretraining while improving interpretability, andensure interoperability with public knowledge bases. The generated KGs could be parsed withRDF parsers and used in downstream applications, or audited for correctness.The main contributions of this work are as follows: 1. We propose a novel ontology-grounded approach to LLM-based KG construction thatleverages ontology based on Wikidata schema to guide the extraction and integration ofknowledge from unstructured text. 2. We introduce a pipeline that combines competency question generation, ontology align-ment, and KG grounding to systematically construct high-quality KGs that are consistent,complete, and interoperable with existing knowledge bases. 3. We demonstrate the effectiveness of our approach through experiments on benchmarkdatasets, showing improvements in KG quality compared to traditional methods alongsidewith interpretability and utility of generated KGs.",
  ". Literature Review": "Knowledge graph construction has been an active area of research in recent years, with a widerange of approaches proposed for extracting structured knowledge from unstructured datasources . Early methods relied heavily on rule-based systems and hand-crafted features toidentify entities and relations in text . With the advent of deep learning, neural network-based approaches have become increasingly popular, enabling more flexible and scalable KGconstruction . One prominent line of work focuses on using distant supervision to automatically generatetraining data for relation extraction . These methods assume that if two entities are mentionedtogether in a sentence and also appear in a knowledge base as subject and object of a relation,then that sentence is likely to express the relation. While distant supervision has been shownto be effective at scale, it often suffers from noise and incomplete coverage.Another important direction is the development of unsupervised and semi-supervised meth-ods for KG construction . These approaches aim to reduce the reliance on large amountsof labeled data by leveraging techniques such as bootstrapping, graph-based inference, andrepresentation learning. However, they often struggle with consistency and quality controlissues. More recently, there has been growing interest in using large language models forKG construction , , . These methods take advantage of the vast knowledge cap-tured in pretrained Language Models (LM) to generate KG triples through prompt engineeringand fine-tuning. While promising, these LM-based approaches only produces triplets withoutcanocalization, which makes portability and interoperability difficult. Additionally, some meth-ods rely on vector-based similarity measures to deduce relationships between entities in KG,which yields good performance but falls short in interpretability .As mentiond in Introduction, despite the significant progress in KG construction and LLMapplications, performance, interpretability, coverage of proprietary documents, and interactionwith other knowledge base remain issues. Our pipeline address these by grouding KG generationon ontology based on Wikidata schema, which ensures that output KG is human-readable andmakes integrating with Wikidata or other KG easier; In the experiments below we show thatthese benefits can be also achieved on private documents with decent performance.",
  ". Competency Question (CQ)-Answer Generation": "The first step in our pipeline is to generate a set of competency questions (CQs) and answersthat capture the key information needs of the target domain. We employ an LLM to generateCQs based on the input documents. The LLM is provided with a set of instructions and examplesto guide the generation process, encouraging the creation of well-formed, relevant questionsthat can be answered using the given documents. This step helps to scope the KG constructiontask within the knowledge domain, and ensure that the resulting KG aligns with the intendeduse cases. This also allows further ontology expansion by incorporating user-submitted domain-defining questions when interacting with the knowledge base, which serves as a user friendlyinterface of refining ontology by submitting new CQs and use our proposed pipeline to attachthe incremental knowledge scope to existing ontology.",
  ". Relation Extraction and Ontology Matching": "During our preliminary experiments of prompting LLMs to directly generate ontology ondocuments, we noted that the LLM spontaneously recalled Wikidata knowledge in response,consistent with previous works . In our preliminary experiments, this behaviour alsotransfers to small 7B/14B models.Following this direction, in the second step we extract relations from CQs and match themagainst Wikidata properties to better elicit model memories on Wikidata when constructing andusing ontology. We first prompt LLMs to extract properties from CQ and write brief descriptionon usage of extracted properties, including their domain and range, following editing guidelinesof Wikidata. To match these properties against existing entries in Wikidata ontology, we pre-populate a candidate property list with all Wikidata properties after filtering out propertiesrelated to external database/knowledge base IDs. These extracted properties are then matchedagainst the candidate list by a vector similarity search between description of properties. Therepresentation for property is sentence embedding constructed from description of properties,and the top 1 closest candidate is retrieved for each extracted property. This matching resultbetween each pair of extracted property and matched top 1 candidate is then vetted by LLM tosee if they are really semantically similar as a final deduplication step. If a match is validated,the candidate property is added to the final property list; otherwise, the newly minted propertyis kept in the final list if we allow expansion from the candidate property list derived fromWikidata, and discarded when the final property list is required to be a subset of candidateproperty list. The first scenario is suitable for cases when no prior schema is known for thedomain and some new properties outside of common ontology are expected, whereas the latteris for a known target list of possible properties.",
  ". Ontology Formatting": "In the third stage, we use LLM to generate an OWL ontology based on the matched andnewly created properties. We copy the description, domain and range field from all propertiesunder Wikidata semantics. For new properties, LLM is prompted to infer and summarize classesfor the domain and range of the relations to output a complete OWL ontology, following theformat of copied Wikidata properties. This step ensures that the resulting KG is grounded in aformal, machine-readable ontology that captures relationships between entities, and close tothe semantics of Wikidata for interoperability.",
  ". KG Construction": "In the final stage, we use the LLM to construct a KG based on the CQs and related answersgrounded by the generated ontology in the previous stage. For each (CQ, answer) pair, LLMextracts relevant entities and maps them to the ontology using the defined properties. Theoutput is a set of RDF triples that constitutes the final KG.",
  ". Experiment settings": "We evaluate our ontology-grounded approach to KG construction (KGC) on three datasets forKGC datasets: Wiki-NRE , SciERC , and WebNLG . As Wiki-NRE and WebNLGare partially based on Wikidata and DBpedia (derived from Wikipedia contents), and in ourproposed pipeline, Wikidata schema is utilized, we include SciERC for a more robust evaluation,since SciERC contains relation types that are not equivalent by nature to properties in Wikidata.We used a subset Wiki-NREs test dataset containing 1,000 samples with 45 relation typesfollowing the split in , due to cost constraints. SciERCs test set contain 974 samples undera schema with 7 relation types. For WebNLG, we used test set in Semantic Parsing (SP) task,with 1,165 samples and 159 relation types. For evaluation, we adopt partial F1 on KG tripletsbased on standards in . All experiments are conducted for one-passWe note in the previous reports that annotation in KGC reports may be incomplete in termsof both possible relation types and KG triplets , .As our pipeline is designed to autonomously uncover knowledge structure with no priorassumption on knowledge schema, we report our result in two ways, corresponding to the twoconfigurations of final de-duplication step in .2:",
  ". Target schema constrained: In this setting, we match all relation types in test sets to itsclosest equivalent in Wikidata and constrict ontology to the relation universe in test set": "2. No schema constraint: In this setting, we do not filter matched ontology, even if they arenot in schema of test dataset. This setting is close to real-life applications when processingdocuments with unknown schema. For property conjunction, evaluate for, compare, feature of in SciERC, we select the closestproperties proposed by LLM based on our subjective opinion.To highlight our systems competency, rather than directly prompting triplets, we parseoutput KG with RDF parser and extract all valid RDF triples for KG related to each document intest set, and present triplets to evaluation script for assessment. This ensures that our evaluationis on the generated KG ready to be consumed in downstream application.We test our pipeline on both Mistral-7B-instruct and GPT-4o1. Due to cost constraints,we have only tested GPT-4o on target schema constrained setting. For embedding property usagecomment, we select bge-small-en . We use GenIE , PL-Marker , and ReGen as fine-tuned baseline for Wiki-NRE, SciERC, and WebNLG dataset, respectively (collectivelynamed Non-LLM Baseline). For LLM-based systems, we use results reported in for Wiki-NRE and WebNLG on the same Mistral model, and GPT-4 results in for SciERC. (collectivelynamed LLM Baseline). We note that it is highly unlikely that Mistral-7B poses an advantageover an earlier version of GPT-4, when interpreting result of SciERC.",
  ": Partial F1 scores on test datasets. Best resultis bolded. Results of proposed pipeline undertwo settings are presented as Target schemaconstrained/no schema constraint": "shows the performance of ourmethod compared to state-of-the-artbaselines on this subset. Our proposedapproach exceeds all baseline under tar-get schema constrained setting on Wiki-NRE and SciERC datasets, while display-ing a small performance regression whenwithout schema constraint. On WebNLGdataset, our pipeline maintained compet-itiveness against fine-tuned SOTA whenconstrained on target schema. These re-sults validate the quality of KG generated by our pipeline, especially SciERC, whose semanticscontains properties that are not native to Wikidata. We also note performance improvementwhen using GPT-4o.",
  ". Performance discrepancy on different grounding ontology": "It is worth noting that the relatively lower performance on no schema constraint setting acrossall datasets is due to the fact that the LLM discovers a richer ontology than the predefinedtarget schema. While this expanded schema may capture additional relevant information, it canhinder extraction performance when evaluated solely against the limited target schema. Thisshowcases the trade-off between schema completeness and strict adherence to a predefinedontology, and our pipeline performs best on a large set of documents with a limited scope ofknowledge, requiring a concise schema.Furthermore, the flipside of performance deficit in an absence of schema constraints, i.e.additional ontology entries outside of dataset-defined properties, cannot be evaluated againstthe dataset directly, as the ontology is not entirely covered by test set annotations. Hence, thevirtue of no schema constraint setting is to demonstrate that our pipeline can indeed provide acoverage of the properties in test set, though somewhat limited compared to baselines, when alsocapturing ontology outside test set schema, which is potentially more useful when discoveringontology on a novel document set with no expert knowledge in its schema conposition. Thisability may be validated by manual evaluation on the full set of captured ontology in a futurework down the line.Nevertheless, the marginal performance deficit leaves room for improvement. Recent reportsexplored that long input context may pose challenge to LLMs even if such long context length istechnically supported . We conjecture that aside from trimming grounding ontology, whichhinders the knowledge coverage of our pipeline, few-shot fine-tuning on the new ontology orgeneral pretraining in KG construction task may be helpful. We leave these as possible futuredirections.",
  ". Utility of generated KG": "It should be emphasized that, while the selected evaluation tasks evaluate the correctnessof extracted triplets, the extracted knowledge graph can do more than that. With ongoingdiscussion related to grounding LLM knowledge on trusted knowledge sources to reducehallucination , explicitly generating KG provides a path to audit knowledge elicited wheninteracting with LLM, and with evidence demonstrating that LLM has the potential to reasonon graph and generate an explicit path to retrieve required knowledge , our pipeline mayserve as a foundation for an interpretable QA system, where an LLM autonomously extractsontology and deduces correct retrieval query based on the ontology when handling a set ofunstructured document. The interpretability arise from the fact that KG and query could beunderstood and verified by users. Moreover, our usage of Wikidata schema offers potentialinteroperability with the whole Wikidata knowledge base, which safely expands the knowledgescope of QA system. We propose to continue research on this significant direction.",
  ". Computational resources": "We note the growing concern of sustainability in LLM applications due to intensive requirementon computational resources. This pipeline consumes three separate LLM calls per document,plus one call per extracted relation. It is not straight forward to compare the carbon footprintof our approach compared to Non-LLM baselines, as our work at this stage does not requiremodel fine-tuning, whereas all of the Non-LLM baselines employed various tuning techniqueswhen producing the result. On the other hand, our smallest model adopted, Mistral-7B, is morethan 10x larger in terms of parameter size compared to T5 models used in Non-LLM baselines.Larger models naturally require more powerful GPU clusters in terms of both GPU quantityand capability, but our zero-shot approach may provide an advantage in terms of resourcecost compared to Non-LLM baselines when processing a small number of documents with notraining requirement.When comparing with LLM baselines, we note that the approach by , consumes 1 and2 LLM calls per document, respectively. However, we note that these baselines treat knowledgetriplet as evaluation target, while we generate a formatted ontology at the end, which is moreuseful. Nevertheless, we recognize the performance burden and propose to explore techniquesin fine-tuning and guided decoding to achieve better performance with smaller model and betterreproducibility.",
  ". Conclusion": "We have demonstrated the effectiveness of our ontology-grounded approach to KG constructionusing LLMs. By leveraging the structured knowledge in Wikidata, pretrained on LLM, andgrounding KG construction with generated ontology, our pipeline is able to construct high-quality KGs across various domains while maintaining competitive performance with state-of-the-art baselines. Generated KGs that are conformant with Wikidata schema leaves possiblywide open, of building an interpretable QA system that has robust access to both commonknowledge and proprietary knowledge base.",
  "This work is supported by Centre for Perceptual and Interactive Intelligence (CPII) Ltd, aCUHK-led InnoCentre under InnoHK scheme of Innovation and Technology Commission": "A. Hogan, E. Blomqvist, M. Cochez, C. Damato, G. D. Melo, C. Gutierrez, S. Kirrane,J. E. L. Gayo, R. Navigli, S. Neumaier, A.-C. N. Ngomo, A. Polleres, S. M. Rashid, A. Rula,L. Schmelzeisen, J. Sequeda, S. Staab, A. Zimmermann, Knowledge graphs, ACM Comput.Surv. 54 (2021). URL: doi:10.1145/3447772. S. Ji, S. Pan, E. Cambria, P. Marttinen, P. S. Yu, A survey on knowledge graphs: Represen-tation, acquisition, and applications, IEEE Transactions on Neural Networks and LearningSystems 33 (2020) 494514. URL: Y. Zhu, X. Wang, J. Chen, S. Qiao, Y. Ou, Y. Yao, S. Deng, H. Chen, N. Zhang, LLMs forKnowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportu-nities, 2024. doi:10.48550/arXiv.2305.13168. arXiv:2305.13168.",
  "F. Petroni, T. Rocktschel, P. Lewis, A. Bakhtin, Y. Wu, A. H. Miller, S. Riedel, Languagemodels as knowledge bases?, in: Conference on Empirical Methods in Natural LanguageProcessing, 2019. URL:": "OpenAI, J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida,J. Altenschmidt, S. Altman, S. Anadkat, R. Avila, I. Babuschkin, S. Balaji, V. Balcom,P. Baltescu, H. Bao, M. Bavarian, J. Belgum, I. Bello, J. Berdine, G. Bernadett-Shapiro,C. Berner, L. Bogdonoff, O. Boiko, M. Boyd, A.-L. Brakman, G. Brockman, T. Brooks,M. Brundage, K. Button, T. Cai, R. Campbell, A. Cann, B. Carey, C. Carlson, R. Carmichael,B. Chan, C. Chang, F. Chantzis, D. Chen, S. Chen, R. Chen, J. Chen, M. Chen, B. Chess,C. Cho, C. Chu, H. W. Chung, D. Cummings, J. Currier, Y. Dai, C. Decareaux, T. Degry,N. Deutsch, D. Deville, A. Dhar, D. Dohan, S. Dowling, S. Dunning, A. Ecoffet, A. Eleti,T. Eloundou, D. Farhi, L. Fedus, N. Felix, S. P. Fishman, J. Forte, I. Fulford, L. Gao, E. Georges,C. Gibson, V. Goel, T. Gogineni, G. Goh, R. Gontijo-Lopes, J. Gordon, M. Grafstein, S. Gray,R. Greene, J. Gross, S. S. Gu, Y. Guo, C. Hallacy, J. Han, J. Harris, Y. He, M. Heaton,J. Heidecke, C. Hesse, A. Hickey, W. Hickey, P. Hoeschele, B. Houghton, K. Hsu, S. Hu,X. Hu, J. Huizinga, S. Jain, S. Jain, J. Jang, A. Jiang, R. Jiang, H. Jin, D. Jin, S. Jomoto,B. Jonn, H. Jun, T. Kaftan, ukasz Kaiser, A. Kamali, I. Kanitscheider, N. S. Keskar, T. Khan,L. Kilpatrick, J. W. Kim, C. Kim, Y. Kim, J. H. Kirchner, J. Kiros, M. Knight, D. Kokotajlo,ukasz Kondraciuk, A. Kondrich, A. Konstantinidis, K. Kosic, G. Krueger, V. Kuo, M. Lampe,I. Lan, T. Lee, J. Leike, J. Leung, D. Levy, C. M. Li, R. Lim, M. Lin, S. Lin, M. Litwin,T. Lopez, R. Lowe, P. Lue, A. Makanju, K. Malfacini, S. Manning, T. Markov, Y. Markovski,B. Martin, K. Mayer, A. Mayne, B. McGrew, S. M. McKinney, C. McLeavey, P. McMillan,J. McNeil, D. Medina, A. Mehta, J. Menick, L. Metz, A. Mishchenko, P. Mishkin, V. Monaco,E. Morikawa, D. Mossing, T. Mu, M. Murati, O. Murk, D. Mly, A. Nair, R. Nakano, R. Nayak,A. Neelakantan, R. Ngo, H. Noh, L. Ouyang, C. OKeefe, J. Pachocki, A. Paino, J. Palermo,A. Pantuliano, G. Parascandolo, J. Parish, E. Parparita, A. Passos, M. Pavlov, A. Peng, A. Perelman, F. de Avila Belbute Peres, M. Petrov, H. P. de Oliveira Pinto, Michael, Pokorny,M. Pokrass, V. H. Pong, T. Powell, A. Power, B. Power, E. Proehl, R. Puri, A. Radford, J. Rae,A. Ramesh, C. Raymond, F. Real, K. Rimbach, C. Ross, B. Rotsted, H. Roussez, N. Ryder,M. Saltarelli, T. Sanders, S. Santurkar, G. Sastry, H. Schmidt, D. Schnurr, J. Schulman,D. Selsam, K. Sheppard, T. Sherbakov, J. Shieh, S. Shoker, P. Shyam, S. Sidor, E. Sigler,M. Simens, J. Sitkin, K. Slama, I. Sohl, B. Sokolowsky, Y. Song, N. Staudacher, F. P. Such,N. Summers, I. Sutskever, J. Tang, N. Tezak, M. B. Thompson, P. Tillet, A. Tootoonchian,E. Tseng, P. Tuggle, N. Turley, J. Tworek, J. F. C. Uribe, A. Vallone, A. Vijayvergiya, C. Voss,C. Wainwright, J. J. Wang, A. Wang, B. Wang, J. Ward, J. Wei, C. Weinmann, A. Welihinda,P. Welinder, J. Weng, L. Weng, M. Wiethoff, D. Willner, C. Winter, S. Wolrich, H. Wong,L. Workman, S. Wu, J. Wu, M. Wu, K. Xiao, T. Xu, S. Yoo, K. Yu, Q. Yuan, W. Zaremba,R. Zellers, C. Zhang, M. Zhang, S. Zhao, T. Zheng, J. Zhuang, W. Zhuk, B. Zoph, Gpt-4technical report, 2024. arXiv:2303.08774.",
  "arXiv:2311.07914": "E. Agichtein, L. Gravano, Snowball: extracting relations from large plain-text collections,in: Proceedings of the Fifth ACM Conference on Digital Libraries, DL 00, Association forComputing Machinery, New York, NY, USA, 2000, p. 8594. URL: doi:10.1145/336597.336644. Y. Zhang, V. Zhong, D. Chen, G. Angeli, C. D. Manning, Position-aware attention andsupervised data improve slot filling, in: M. Palmer, R. Hwa, S. Riedel (Eds.), Proceedings ofthe 2017 Conference on Empirical Methods in Natural Language Processing, Associationfor Computational Linguistics, Copenhagen, Denmark, 2017, pp. 3545. URL: doi:10.18653/v1/D17-1004. M. Mintz, S. Bills, R. Snow, D. Jurafsky, Distant supervision for relation extraction withoutlabeled data, in: K.-Y. Su, J. Su, J. Wiebe, H. Li (Eds.), Proceedings of the Joint Conferenceof the 47th Annual Meeting of the ACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP, Association for Computational Linguistics,Suntec, Singapore, 2009, pp. 10031011. URL: X. L. Dong, E. Gabrilovich, G. Heitz, W. Horn, N. Lao, K. Murphy, T. Strohmann, S. Sun,W. Zhang, Knowledge vault: A web-scale approach to probabilistic knowledge fusion,in: The 20th ACM SIGKDD International Conference on Knowledge Discovery and DataMining, KDD 14, New York, NY, USA - August 24 - 27, 2014, 2014, pp. 601610. URL: ~nlao/publication/2014.kdd.pdf, evgeniy GabrilovichWilko HornNiLaoKevin MurphyThomas StrohmannShaohua SunWei ZhangGeremy Heitz. A. Bosselut, H. Rashkin, M. Sap, C. Malaviya, A. Celikyilmaz, Y. Choi, COMET: Com-monsense transformers for automatic knowledge graph construction, in: A. Korhonen,D. Traum, L. Mrquez (Eds.), Proceedings of the 57th Annual Meeting of the Association forComputational Linguistics, Association for Computational Linguistics, Florence, Italy, 2019,pp. 47624779. URL: doi:10.18653/v1/P19-1470. Y. Chen, Y. Liu, L. Dong, S. Wang, C. Zhu, M. Zeng, Y. Zhang,AdaPrompt: Adap-tive model training for prompt-based NLP,in: Y. Goldberg, Z. Kozareva, Y. Zhang(Eds.), Findings of the Association for Computational Linguistics: EMNLP 2022, Associ-",
  "S. J. Semnani, V. Z. Yao, H. C. Zhang, M. S. Lam, WikiChat: Stopping the Hallucina-tion of Large Language Model Chatbots by Few-Shot Grounding on Wikipedia, 2023.arXiv:2305.14292": "B. D. Trisedya, G. Weikum, J. Qi, R. Zhang, Neural relation extraction for knowledge baseenrichment, in: A. Korhonen, D. Traum, L. Mrquez (Eds.), Proceedings of the 57th AnnualMeeting of the Association for Computational Linguistics, Association for ComputationalLinguistics, Florence, Italy, 2019, pp. 229240. URL: Y. Luan, L. He, M. Ostendorf, H. Hajishirzi, Multi-task identification of entities, relations,and coreferencefor scientific knowledge graph construction, in: Proc. Conf. EmpiricalMethods Natural Language Process. (EMNLP), 2018. T. Castro Ferreira, C. Gardent, N. Ilinykh, C. van der Lee, S. Mille, D. Moussallem, A. Shi-morina, The 2020 bilingual, bi-directional WebNLG+ shared task: Overview and evalua-tion results (WebNLG+ 2020), in: T. Castro Ferreira, C. Gardent, N. Ilinykh, C. van derLee, S. Mille, D. Moussallem, A. Shimorina (Eds.), Proceedings of the 3rd InternationalWorkshop on Natural Language Generation from the Semantic Web (WebNLG+), Asso-ciation for Computational Linguistics, Dublin, Ireland (Virtual), 2020, pp. 5576. URL:",
  "J. Chen, S. Xiao, P. Zhang, K. Luo, D. Lian, Z. Liu, Bge m3-embedding: Multi-lingual,multi-functionality, multi-granularity text embeddings through self-knowledge distillation,2023. arXiv:2309.07597": "M. Josifoski, N. De Cao, M. Peyrard, F. Petroni, R. West, GenIE: Generative informationextraction, in: M. Carpuat, M.-C. de Marneffe, I. V. Meza Ruiz (Eds.), Proceedings of the2022 Conference of the North American Chapter of the Association for ComputationalLinguistics: Human Language Technologies, Association for Computational Linguistics,Seattle, United States, 2022, pp. 46264643. URL: doi:10.18653/v1/2022.naacl-main.342.",
  "D. Ye, Y. Lin, P. Li, M. Sun, Packed levitated marker for entity and relation extraction, in:": "S. Muresan, P. Nakov, A. Villavicencio (Eds.), Proceedings of the 60th Annual Meeting ofthe Association for Computational Linguistics (Volume 1: Long Papers), Association forComputational Linguistics, Dublin, Ireland, 2022, pp. 49044917. URL: doi:10.18653/v1/2022.acl-long.337. P. Dognin, I. Padhi, I. Melnyk, P. Das,ReGen: Reinforcement learning for text andknowledge base generation using pretrained language models, in: M.-F. Moens, X. Huang,L. Specia, S. W.-t. Yih (Eds.), Proceedings of the 2021 Conference on Empirical Methodsin Natural Language Processing, Association for Computational Linguistics, Online andPunta Cana, Dominican Republic, 2021, pp. 10841099. URL: doi:10.18653/v1/2021.emnlp-main.83.",
  "This KG was generated under no schema constraint setting for this document: MohammadFirouzi ( Born 1958 Tehran ) is a prolific Iranian musician , whose primary instrument is thebarbat": "< P r e f i x e sandd e f i n i t i o nofdependenciesomitted >wd : Mohammad_Firouzi a wd : human;r d f s : l a b e l\"Mohammad F i r o u z i \"@en ;wdt : occupation wd : Musician;wdt : CountryOfCitizenship wd : Iran;wdt : P l a c e O f B i r t h wd : Tehran;wdt : DateOfBirth\"1958\"^^ xsd : date.",
  "B. Preprocessing of Wikidata schema": "To save space in LLM input context and mitigate performance drop on selected target schemawhen ontology is large, we only include commonly used properties by restricting data typeon item, quantity, string, monolingual text, point in time.2 To align with common pretrainingobjectives of LLM, we substitute entity identifiers (e.g. P19) with its literal label (rdfs:label inPascalCase (e.g. PlaceOfBirth).",
  "We prompt LLM to generate up to 3 CQs per document for efficiency considering nature of testdatasets, but note that this may be adjusted": "Writecompetencyq u e s t i o n sbased on thea b s t r a c tl e v e lconceptsinthedocument .Writequ e st io n st h a tcan be answeredusingthedocument only .Write up to 3q u e s t i o n sper document .Belowaretheexamples andfollowthe same format whengeneratingcompetencyqu e st io n s : ####Document :DouglasNoel Adams (11March 1952 11 May 2001) wasanEnglishauthor ,humourist ,andscreenwriter ,b e st knownf o rThe Hitchhiker s GuidetotheGalaxy(HHGTTG) .O r i g i n a l l ya 1978 BBC radio comedy ,The Hitchhiker s GuidetotheGalaxydevelopedi n t oa\" t r i l o g y \"off i v ebookst h a tsoldmore than 15m i l l i o ncopiesinh isl i f e t i m e .I twasf u r t h e rdevelopedi n t oat e l e v i s i o ns e r i e s ,s e v e r a lstageplays ,comics ,avideo game ,and a 2005f e a t u r ef i l m .Adams sc o n t r i b u t i o nto UK radioi scommemorated in The RadioAcademy sHallof Fame . ####Questions :CQ1 .Whati sthedateofb i r t hofDouglasNoel Adams?CQ2 .Whati sthedateofdeathofDouglasNoel Adams?CQ3 .Whati stheoccupationofDouglasNoel Adams?CQ4 .Whati sthecountryofc i t i z e n s h i pofDouglasNoel Adams?CQ5 .Whati sthemostnotable work ofDouglasNoel Adams?CQ6 .Whati stheo r i g i n a lmedium of The Hitchhiker s GuidetotheGalaxy ?CQ7 .In what year was The Hitchhiker s GuidetotheGalaxyo r i g i n a l l ybroadcast ?CQ8 . How many booksarein The Hitchhiker s GuidetotheGalaxy\" t r i l o g y \"?CQ9 .What othermediaa d a p t a t i o n swerec r e a t e dbased on TheHitchhiker s GuidetotheGalaxy ?",
  "C.3. Relation extraction": "You are ana s s i s t a n tinb u i l d i n ga knowledge graph .Analyzethefo ll owi ngcompetencyq u es ti o nsandi d e n t i f ya l lr e l a t i o n s h i p sandconceptsconceptsmentionedinthequestion .E x t r a c tr e l a t i o nf i r s t ,thend e s c r i b etheusageofeachr e l a t i o nbased on yourunderstandinggiventhecontextofcompetencyq u e s t i o n s .Afterwards ,e x t r a c ta l lr e l a t i o n r e l a t e dconcepts .You shouldonlye x t r a c tp r o p e r t i e sbetweene n t i t i e sandl i t e r a l s ,note n t i t i e sthemselves ,orc l a s s e sofe n t i t i e s .Therefore ,nota l lCQs containv a l i dp r o p e r t i e s .I fyou don t know theanswer ,j u s tsayt h a tyou don t know ,don ttr yto make up an answer .Mergea l lr e l a t i o n si n t oonel i s tanda l lconceptsi n t oonel i s t .Do notreplyusing a completesentence ,and onlygivetheanswerinthef ol low in gformat .",
  "Belowaretheexamples andfollowthe same formattoe x t r a c tther e l a t i o n s :": "####Document :DouglasNoel Adams (11March 1952 11 May 2001) wasanEnglishauthor ,humourist ,andscreenwriter ,b e st knownf o rThe Hitchhiker s GuidetotheGalaxy(HHGTTG) .O r i g i n a l l ya 1978 BBC radio comedy ,The Hitchhiker s GuidetotheGalaxydevelopedi n t oa\" t r i l o g y \"off i v ebookst h a t soldmore than 15m i l l i o ncopiesinh isl i f e t i m e .I twasf u r t h e rdevelopedi n t oat e l e v i s i o ns e r i e s ,s e v e r a lstageplays ,comics ,avideo game ,and a 2005f e a t u r ef i l m .Adams sc o n t r i b u t i o nto UK radioi scommemorated in The RadioAcademy sHallof Fame . ####Questions :CQ1 .Whati sthedateofb i r t hofDouglasNoel Adams?CQ2 .Whati sthedateofdeathofDouglasNoel Adams?CQ3 .Whati stheoccupationofDouglasNoel Adams?CQ4 .Whati sthecountryofc i t i z e n s h i pofDouglasNoel Adams?CQ5 .Whati sthemostnotablework ofDouglasNoel Adams?CQ6 .Whati stheo r i g i n a lmedium of The Hitchhiker s GuidetotheGalaxy ?CQ7 .In what year was The Hitchhiker s GuidetotheGalaxyo r i g i n a l l ybroadcast ?CQ8 . How many booksarein The Hitchhiker s GuidetotheGalaxy\" t r i l o g y \"?CQ9 .What othermediaa d a p t a t i o n swerec r e a t e dbased on TheHitchhiker s GuidetotheGalaxy ? ####R e l a t i o n s :( dateofbirth ,The date on whichthes u b j e c twas born . )( dateofdeath ,The date on whichthes u b j e c tdied . )( occupation ,The occupationofa person . )( countryofc i t i z e n s h i p ,The countryofwhichthes u b j e c ti sac i t i z e n . )( notable work ,The mostnotablework ofa person . )( genre ,The genreortypeof work . )( p u b l i c a t i o ndate ,The dateorperiod when a work wasf i r s tpublishedorr e l e a s e d . )( haspart ,I n d i c a t e st h a tthes u b j e c thas ac e r t a i npart ,component ,orelement . )( s e r i e s ,I n d i c a t e st h a tthes u b j e c ti spartofas e r i e s ,suchas a books e r i e s ,f i l ms e r i e s ,ort e l e v i s i o ns e r i e s . )",
  "C.4. Ontology matching": "Decidei fthe twop r o p e r t i e sares e m a n t i c a l l ys i m i l a rin anontology .You shouldsayyesi fyoudecidet h a tthesep r o p t i e sares i m i l a r ,ori ftheyarei n v e r s ep r o p e r t i e s .Answer in\" yes \"or\" no \"only .Property1 :{ p1 }Property2 :{ p2 }",
  "For properties under Wikidata schema, we retrieve schema:description, rdfs:domain, rdfs:rangefor each property and include it in resulting ontology. Otherwise LLM is prompted to authorontology as so:": "Use ther e l a t i o n s( p r o p e r t i e s )andt h e i rusage comments tob u i l dan ontologyin RDF format .I fyou don t know theanswer ,j u s tsayt h a tyou don t know ,don ttr yto make up an answer .Don tprovideanythingotherthan an ontologyin RDF format .I n f e rand summarizec l a s s e sf o rdomain and rangeofther e l a t i o n sa c r o s stheconceptsprovided ,and addthesec l a s s e stor e l a t i o n sonlyi frequ iredf o rc l o u s r eofr e l a t i o n s .Foreachr e l a t i o n ,addr e l e v a n tontologyentryf o ri t .Add r d f s : comment based on theusage comments .Use wdt :namespacef o ra l lr e l a t i o n sdiscovered .Usee n t i t i e sunderthesep r e f i x e si fnecessary :@prefixr d f :< http : / /www. w3 . org /1999/02/22 rdf syntax ns#>.@prefixxsd :< http : / /www. w3 . org / 2 0 0 1 / XMLSchema#>.@prefixr d f s :< http : / /www. w3 . org / 2 0 0 0 / 0 1 / rdf schema#>.@prefix owl :< http : / /www. w3 . org / 2 0 0 2 / 0 7 / owl#>.@prefixwikibase :< http : / / wikiba . se / ontology #>.@prefix schema :< http : / / schema . org / >.@prefix wd :< http : / /www. wikidata . org / e n t i t y / >.@prefix wdt :< http : / /www. wikidata . org / prop / d i r e c t / >.Uset u r t l esyntax .",
  "####R e l a t i o n s :( r e s u l t s ,r e s u l t s :r e s u l t sofacompetitionsuchass p o r t sore l e c t i o n s )": "####Ontology :wdt : R e s u l t sawikibase : Property;schema : d e s c r i p t i o n\" r e s u l t sof acompetitionsuchass p o r t sore l e c t i o n s \";r d f s : l a b e l\" r e s u l t s \";r d f s : domain wd : referendum , wd : competition , wd : partyconference , wd : s p o r t i n gevent;r d f s : range wd : e l e c t o r a lr e s u l t , wd : votingr e s u l t , wd : sportr e s u l t , wd : racer e s u l t.",
  "C.6. KG generation": "Yourtaski stoc o n s t r u c ta knowledge graphbased on theprovidedontology .Focus on understandingr e l a t i o n s h i p sfromthequestionanswerp a i rand document ,ande x t r a c tr e l a t e de n t i t i e s ,then mapping them totheontologyusingthep r o p e r t i e sdefinedintheontology .Do noti n c l u d e new p r o p e r t i e sotherthanthoseinontology .Only usethosep r o p e r t i e sintheontology .Outputint u r t l eformatfollowingtheontologyprovided .You shouldonlyi n c l u d eknowledgeinquestionanswerp a i r sandthedocument .Do not make up answers .",
  "Belowi san example :": "####Document :DouglasNoel Adams (11March 1952 11 May 2001) was anEnglishauthor ,humourist ,andscreenwriter ,b es t known f o rTheHitchhiker s GuidetotheGalaxy(HHGTTG) .O r i g i n a l l ya 1978BBC radio comedy ,The Hitchhiker s GuidetotheGalaxydevelopedi n t oa\" t r i l o g y \"off i v ebookst h a tsoldmore than15m i l l i o ncopiesinh isl i f e t i m e .I twasf u r t h e rdevelopedi n t oat e l e v i s i o ns e r i e s ,s e v e r a lstageplays ,comics ,avideo game ,and a 2005f e a t u r ef i l m .Adams sc o n t r i b u t i o ntoUK radioi scommemorated in The Radio Academy sHallofFame .",
  "Q:Whati sthe Commons Categoryf o rDouglas Adams?A:The Commons Categoryf o rDouglas Adamsi s\" Douglas Adams \"": "####Ontology :@prefixr d f :http : / /www. w3 . org /1999/02/22 rdf syntax ns #.@prefixr d f s :http : / /www. w3 . org / 2 0 0 0 / 0 1 / rdf schema#.@prefix wdt :http : / /www. wikidata . org / prop / d i r e c t /.@prefix wd :http : / /www. wikidata . org / e n t i t y /.@prefixxsd :http : / /www. w3 . org / 2 0 0 1 / XMLSchema#.wd : Douglas_Adamsr d f s : l a b e l\" Douglas Adams \"@en ;wdt : InstanceOf wd : human;wdt : SexOrGender wd : male;wdt : P l a c e O f B i r t h wd : Cambridge;wdt : PlaceOfDeath wd : S a n t a _ B a r b a r a _ C a l i f o r n i a;wdt : DateOfBirth\"1952 03 11\"^^ xsd : date;wdt : DateOfDeath\"2001 05 11\"^^ xsd : date;wdt : Occupation wd : w r i t e r;wdt : Occupation wd : comedian;wdt : Occupation wd : d r a m a t i s t;wdt : LanguagesSpokenWrittenOrSigned wd : English;wdt : EducatedAt wd : St_Johns_College_Cambridge;wdt : EducatedAt wd : Brentwood_School_Essex;wdt : AlumniOf wd : St_Johns_College;wdt : NotableWork wd : The_Hitchhikers_Guide_to_the_Galaxy;wdt : NotableWork wd : Dirk_Gentlys_Holistic_Detective_Agency;"
}