{
  "ABSTRACT": "Time series anomaly detection is critical for a wide range of appli-cations. It aims to identify deviant samples from the normal sampledistribution in time series. The most fundamental challenge forthis task is to learn a representation map that enables effectivediscrimination of anomalies. Reconstruction-based methods stilldominate, but the representation learning with anomalies mighthurt the performance with its large abnormal loss. On the otherhand, contrastive learning aims to find a representation that canclearly distinguish any instance from the others, which can bring amore natural and promising representation for time series anom-aly detection. In this paper, we propose DCdetector, a multi-scaledual attention contrastive representation learning model. DCde-tector utilizes a novel dual attention asymmetric design to createthe permutated environment and pure contrastive loss to guide thelearning process, thus learning a permutation invariant representa-tion with superior discrimination abilities. Extensive experimentsshow that DCdetector achieves state-of-the-art results on multipletime series anomaly detection benchmark datasets. Code is publiclyavailable at this URL1.",
  "time series anomaly detection, contrastive learning, representationlearning, self-supervised learning": "The first three authors contributed equally to this research.Work done as an intern in DAMO Academy, Alibaba Group. He is now with theDepartment of Computer Science, University of Oxford, OX1 3SA, Oxford, UK.Corresponding author1 Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 23, August 610, 2023, Long Beach, CA, USA 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0103-0/23/08...$15.00 ACM Reference Format:Yiyuan Yang, Chaoli Zhang, Tian Zhou, Qingsong Wen, and Liang Sun.2023. DCdetector: Dual Attention Contrastive Representation Learning forTime Series Anomaly Detection. In Proceedings of the 29th ACM SIGKDDConference on Knowledge Discovery and Data Mining (KDD 23), August610, 2023, Long Beach, CA, USA. ACM, New York, NY, USA, 17 pages.",
  "INTRODUCTION": "Time series anomaly detection is widely used in real-world appli-cations, including but not limited to industrial equipment statusmonitoring, financial fraud detection, fault diagnosis, and dailymonitoring and maintenance of automobiles . Withthe rapid development of different sensors, large-scale time seriesdata has been collected during the systems running time in manydifferent applications . Effectively discovering abnormalpatterns in systems is crucial to ensure security and avoid economiclosses . For example, in the energy industry, detecting anom-alies in wind turbine sensors in time helps to avoid catastrophicfailure. In the financial industry, detecting fraud is essential forreducing pecuniary loss.However, it is challenging to discover abnormal patterns froma mass of complex time series. Firstly, it is still being determinedwhat the anomalies will be like. Anomaly is also called outlier ornovelty, which means observation unusual, irregular, inconsistent,unexpected, rare, faulty, or simply strange depending on the situa-tion . Moreover, the typical situation is usually complex, whichmakes it harder to define what is unusual or unexpected. For in-stance, the wind turbine works in different patterns with differentweather situations. Secondly, anomalies are usually rare, so it takeswork to get labels . Most supervised or semi-supervised meth-ods fail to work given limited labeled training data. Third, anomalydetection models should consider temporal, multidimensional, andnon-stationary features for time series data . Multidimensional-ity describes that there is usually a dependence among dimensionsin multivariate time series, and non-stationarity means the sta-tistical features of time series are unstable. Specifically, temporaldependency means the adjacent points have latent dependence oneach other. Although every point should be labeled as normal orabnormal, it is not reasonable to consider a single point as a sample.Researchers have designed various time-series anomaly detec-tion methods to deal with these challenges. They can be roughly",
  "KDD 23, August 610, 2023, Long Beach, CA, USAYiyuan Yang, Chaoli Zhang, Tian Zhou, Qingsong Wen, & Liang Sun": "classified as statistical, classic machine learning, and deep learning-based methods . Machine learning methods, especially deeplearning-based methods, have succeeded greatly due to their pow-erful representation advantages. Most of the supervised and semi-supervised methods can not handle thechallenge of limited labeled data, especially the anomalies are dy-namic and new anomalies never observed before may occur. Un-supervised methods are popular without strict requirements onlabeled data, including one class classification-based, probabilistic-based, distance-based, forecasting-based, reconstruction-based ap-proaches .Reconstruction-based methods learn a model to reconstruct nor-mal samples, and thereby the instances failing be reconstructed bythe learned model are anomalies. Such an approach is developingrapidly due to its power in handling complex data by combining itwith different machine learning models and its interpretability thatthe instances behave unusually abnormally. However, it is usuallychallenging to learn a well-reconstructed model for normal datawithout being obstructed by anomalies. The situation is even worsein time series anomaly detection as the number of anomalies isunknown, and normal and abnormal points may appear in one in-stance, making it harder to learn a clean, well-reconstructed modelfor normal points.Recently, contrastive representative learning has attracted at-tention due to its diverse design and outstanding performance indownstream tasks in the computer vision field . How-ever, the effectiveness of contrastive representative learning stillneeds to be explored in the time-series anomaly detection area. Inthis paper, we propose a Dual attention Contrastive representa-tion learning anomaly detector called DCdetector to handle thechallenges in time series anomaly detection. The key idea of ourDCdetector is that normal time series points share the latent pattern,which means normal points have strong correlations with otherpoints. In contrast, the anomalies do not (i.e., weak correlationswith others). Learning consistent representations for anomaliesfrom different views will be hard but easy for normal points. Theprimary motivation is that if normal and abnormal points repre-sentations are distinguishable, we can detect anomalies without ahighly qualified reconstruction model.Specifically, we propose a contrastive structure with two branchesand a dual attention module, and two branches share networkweights. This model is trained based on the similarity of two branches,as normal points are the majority. The representation inconsistencyof anomaly will be conspicuous. Thus, the representation differencebetween normal and abnormal data is enlarged without a highlyqualified reconstruction model. To capture the temporal depen-dency in time series, DCdetector utilizes patching-based attentionnetworks as the basic module. A multi-scale design is proposed toreduce information loss during patching. DCdetector takes all chan-nels into representation efficiently with a channel independencedesign for multivariate time series. In particular, DCdetector doesnot require prior knowledge about anomalies and thus can handlenew outliers never observed before. The main contributions of ourDCdetector are summarized as follows:",
  "Architecture: A contrastive learning-based dual-branch at-tention structure is designed to learn a permutation invariantrepresentation that enlarges the representation differences": "between normal points and anomalies. Also, channel inde-pendence patching is proposed to enhance local semanticinformation in time series. Multi-scale is proposed in the at-tention module to reduce information loss during patching. Optimization: An effective and robust loss function is de-signed based on the similarity of two branches. Note that themodel is trained purely contrastively without reconstructionloss, which reduces distractions from anomalies. Performance & Justification: DCdetector achieves perfor-mance comparable or superior to state-of-the-art methodson seven multivariate and one univariate time series anomalydetection benchmark datasets. We also provide justificationdiscussion to explain how our model avoids collapse withoutnegative samples.",
  "In this section, we show the related literature for this work. Therelevant works include anomaly detection and contrastive repre-sentation learning": "Time Series Anomaly Detection. There are various approaches todetect anomalies in time series, including statistical methods, clas-sical machine learning methods, and deep learning methods .Statistical methods include using moving averages, exponentialsmoothing , and the autoregressive integrated moving average(ARIMA) model . Machine learning methods include clusteringalgorithms such as k-means and density-based methods, aswell as classification algorithms such as decision trees andsupport vector machines (SVMs). Deep learning methods includeusing autoencoders, variational autoencoders (VAEs) , andrecurrent neural networks (RNNs) such as long short-termmemory (LSTM) networks . Recent works in time series anom-aly detection also include generative adversarial networks (GANs)based methods and deep reinforcement learning(DRL) based methods . In general, deep learning methodsare more effective in identifying anomalies in time series data, es-pecially when the data is high-dimensional or non-linear.In another view, time series anomaly detection models can beroughly divided into two categories: supervised and unsupervisedanomaly detection algorithms. Supervised methods can perform bet-ter when the anomaly label is available or affordable. Such methodscan be dated back to AutoEncoder , LSTM-VAE , SpectralResidual (SR) , RobustTAD and so on. On the other hand, anunsupervised anomaly detection algorithm can be applied in caseswhere the anomaly labels are difficult to obtain. Such versatilityresults in the communitys long-lasting interest in developing newunsupervised time-series anomaly detection methods, includingDAGMM , OmniAnomaly , GDN , RDSSM and soon. Unsupervised deep learning methods have been widely studiedin time series anomaly detection. The main reasons are as follows.First, it is usually hard or unaffordable to get labels for all timeseries sequences in real-world applications. Second, deep modelsare powerful in representation learning and have the potential toget a decent detection accuracy under the unsupervised setting.Most of them are based on a reconstruction approach where awell-reconstructed model is learned for normal points; Then, theinstances failing to be reconstructed are anomalies. Recently, some",
  "GradGrad": ": Architecture comparison of three approaches. The reconstruction-based approach uses a representation neuralnetwork to learn the pattern of normal points and do reconstruction. In Anomaly Transformer, the prior discrepancy is learnedwith Gaussian Kernel and the association discrepancy is learned with a transformer module; MinMax association learningis also critical and reconstruction loss is contained. DCdetector is concise without a specially designed Gaussian Kernel or aMinMax learning strategy, nor a reconstruction loss.",
  "self-supervised learning-based methods have been proposed toenhance the generalization ability in unsupervised anomaly detec-tion": "Contrastive Representation Learning. The goal of contrastive rep-resentation learning is to learn an embedding space in which similardata samples stay close to each other while dissimilar ones are farapart. The idea of contrastive learning can be traced back to Inst-Dic . Classical contrastive models create <positive, negative>sample pairs to learn a representation where positive samples arenear each other (pulled together) and far from negative samples(pushed apart) . Their key designs are about how todefine negative samples and deal with the high computation pow-er/large batches requirements . On the other hand, BYOL and SimSiam get rid of negative samples involved, and such asimple siamese model (SimSiam) achieves comparable performancewith other state-of-the-art complex architecture.It is illuminating to make the distance of two-type samples largerusing contrastive design. We try to distinguish time series anomaliesand normal points with a well-designed multi-scale patching-basedattention module. Moreover, our DCdetector is also free from nega-tive samples and does not fall into a trivial solution even withoutthe \"stop gradient\".",
  "X = (1,2, . . . , ),": "where each data point IR is acquired at a certain timestamp from industrial sensors or machines, and is the data dimensional-ity, e.g., the number of sensors or machines. Our problem can beregarded as given input time-series sequence X, for another un-known test sequence X of length with the same modality asthe training sequence, we want to predict Y = (1,2, . . . , ).Here {0, 1} where 1 denotes an anomalous data point and 0denotes a normal data point.As mentioned previously, representation learning is a powerfultool to handle the complex pattern of time series. Due to the highcost of gaining labels in practice, unsupervised and self-supervisedmethods are more popular. The critical issue in time series anomaly detection is to distinguish anomalies from normal points. Learningrepresentations that demonstrate wide disparities without anom-alies is promising. We amplify the advantages of contrastive repre-sentation learning with a dual attention structure.In some way, the underlining inductive bias we used here is simi-lar to what Anomaly Transformer explored . That is, anomalieshave less connection or interaction with the whole series than theiradjacent points. The Anomaly Transformer detects anomalies byassociation discrepancy between a learned Gaussian kernel andattention weight distribution. In contrast, we proposed DCdetec-tor, which achieves a similar goal in a much more general andconcise way with a dual-attention self-supervised contrastive-typestructure.To better position our work in the landscape of time series anom-aly detection, we give a brief comparison of three approaches. Tobe noticed, Anomaly Transformer is a representation of a series ofexplicit association modeling works , not implying itis the only one. We merely want to make a more direct comparisonwith the closest work here. shows the architecture com-parison of three approaches. The reconstruction-based approach((a)) uses a representation neural network to learn the pat-tern of normal points and do reconstruction. Anomaly Transformer((b)) takes advantage of the observation that it is difficultto build nontrivial associations from abnormal points to the wholeseries. Thereby, the prior discrepancy is learned with Gaussian Ker-nel and the association discrepancy is learned with a transformermodule. MinMax association learning is also critical for AnomalyTransformer and reconstruction loss is contained. In contrast, theproposed DCdetector ((c)) is concise, in the sense that itdoes not need a specially designed Gaussian Kernel, a MinMaxlearning strategy, or a reconstruction loss. The DCdetector mainlyleverages the designed contrastive learning-based dual-branch at-tention for discrepancy learning of anomalies in different views toenlarge the differences between anomalies and normal points. Thesimplicity and effectiveness contribute to DCdetectors versatility.",
  "Output ' R&#": ": Basic patching attention with channel independence. Each channel in the multivariate time series input is consideredas a single time series and divided into patches. Each channel shares the same self-attention network, and the representationresults are concatenated as the final output. Attention Contrastive Structure module, Representation Discrep-ancy module, and Anomaly Criterion module.The input multivariate time series in the Forward Process mod-ule is normalized by an instance normalization module.The inputs to the instance normalization all come from the inde-pendent channels themselves. It can be seen as a consolidation andadjustment of global information, and a more stable approach totraining processing. Channel independence assumption has beenproven helpful in multivariate time series forecasting tasks to reduce parameter numbers and overfitting issues. Our DCde-tector follows such channel independence setting to simplify theattention network with patching.More specifically, the basic patching attention with channel in-dependence is shown in . Each channel in the multivariatetime series input (X IR ) is considered as a single time series(X IR 1, = 1, 2, . . . ,) and divided into patches. Each chan-nel shares the same self-attention network, and the representationresults (X IR 1, = 1, 2, . . . ,) is concatenated as the final out-put (X IR ). In the implementation phase, running a slidingwindow in time series data is widely used in time series anomalydetection tasks and has little influence on the main design.More implementation details are left in the experiment section.The Dual Attention Contrastive Structure module is critical inour design. It learns the representation of inputs in different views.The insight is that, for normal points, most of them will share the same latent pattern even in different views (a strong correlation isnot easy to be destroyed). However, as anomalies are rare and donot have explicit patterns, it is hard for them to share latent modeswith normal points or among themselves (i.e., anomalies have aweak correlation with other points). Thus, the difference will beslight for normal point representations in different views and largefor anomalies. We can distinguish anomalies from normal pointswith a well-designed Representation Discrepancy criterion. Thedetails of Dual Attention Contrastive Structure and RepresentationDiscrepancy are left in the following .2 and .3.As for the Anomaly Criterion, we calculate anomaly scores basedon the discrepancy between the two representations and use a priorthreshold for anomaly detection. The details are left in .4.",
  "Dual Attention Contrastive Structure": "In DCdetector, we propose a contrastive representation learningstructure with dual attention to get the representations of inputtime series from different views. Concretely, with the patching oper-ation, DCdetector takes patch-wise and in-patch representations astwo views. Note that it differs from traditional contrastive learning,where original and augmented data are considered as two views ofthe original data. Moreover, DCdetector does not construct <posi-tive, negative> pairs like the typical contrastive methods .",
  "Instead, its basic setting is similar to the contrastive methods onlyusing positive samples": "3.2.1Dual Attention. As shown in , input time seriesX IR are patched as X IR where is the size ofpatches and is the number of patches. Then, we fuse the channelinformation with the batch dimension and the input size becomesX IR . With such patched time series, DCdetector learns rep-resentation in patch-wise and in-patch views with self-attentionnetworks. Our dual attention can be encoded in layers, so forsimplicity, we only use one of these layers as an example.For the patch-wise representation, a single patch is consideredas a unit, and the dependencies among patches are modeled by amulti-head self-attention network (named patch-wise attention). Indetail, an embedded operation will be applied in the patch_size ()dimension, and the shape of embedding is XN IR . Then,we adopt multi-head attention weights to calculate the patch-wiserepresentation. Firstly, initialize the query and key:",
  "N = Concat(N1, ,N ) N,(3)": "where N IR is a learnable parameter matrix.Similarly, for the in-patch representation, the dependencies ofpoints in the same patch are gained by a multi-head self-attentionnetwork (called in-patch attention). Note that the patch-wise at-tention network shares weights with the in-patch attention net-work. Specifically, another embedded operation will be applied inthe patch_number () dimension, and the shape of embedding isXP IR . Then, we adopt multi-head attention weights tocalculate the in-patch representation. First, initialize the query andkey:QP, KP = WQXP, WKXP1 ,(4)",
  "where P IR is a learnable parameter matrix.Note that the WQ, WK are the shared weights within the in-patch attention representation network and patch-wise attentionrepresentation network": "3.2.2Up-sampling and Multi-scale Design. Although the patchingdesign benefits from gaining local semantic information, patch-wise attention ignores the relevance among points in a patch, andin-patch attention ignores the relevance among patches. To com-pare the results of two representation networks, we need to doup-sampling first. For the patch-wise branch, as we only have thedependencies among patches, repeating is done inside patches (i.e.,from patch to points) for up-sampling, and we will get the finalpatch-wise representation N. For the in-patch branch, as only de-pendencies among patch points are gained, repeating is done from\"one\" patch to a full number of patches, and we will get the finalin-patch representation P.A simple example is shown in where a patch is notedas , and a point is noted as . Such patching and repeating up-sampling operations inevitably lead to information loss. To keep theinformation from the original data better, DCdetector introducesa multi-scale design for patching representation and up-sampling.The final representation concatenates results in different scales (i.e.,patch sizes). Specifically, we can preset a list of various patches toperform parallel patching and the computation of dual attentionrepresentations, simultaneously. After upsampling each patch part,they are summed to obtain the final patch-wise representation Nand in-patch representation P, which are",
  "N = Upsampling(N),P = Upsampling(P).(7)": "3.2.3Contrastive Structure. Patch-wise and in-patch branches out-put representations of the same input time series in two differentviews. As shown in (c), patch-wise sample representa-tion learns a weighted combination between sample points in thesame position from each patch. In-patch sample representation,on the other hand, learns a weighted combination between pointswithin the same patch. We can treat these two representations aspermutated multi-view representations. The key inductive bias weexploit here is that normal points can maintain their representationunder permutations while the anomalies can not. From such dualattention non-negative contrastive learning, we want to learn apermutation invariant representation. Learning details are leftin .3.",
  "Representation Discrepancy": "With dual attention contrastive structure, representations fromtwo views (patch-wise branch and in-patch branch) are gained. Weformalize a loss function based on KullbackLeibler divergence (KLdivergence) to measure the similarity of such two representations.The intuition is that, as anomalies are rare and normal points sharelatent patterns, the same inputs representations should be similar.",
  "LP{P, N; X} =(P, Stopgrad(N)) + (Stopgrad(N), P),(8)": "LN{P, N; X} =(N, Stopgrad(P)) + (Stopgrad(P), N),(9)where X is the input time series, (||) is the KL divergencedistance, P and N are the representation result matrices of the in-patch branch and the patch-wise branch, respectively. Stop-gradient(labeled as Stopgrad) operation is also used in our loss function totrain two branches asynchronously. Then, the total loss function Lis defined as",
  "(N).(10)": "Unlike most anomaly detection works based on reconstructionframework , DCdetector is a self-supervised framework basedon representation learning, and no reconstruction part is utilizedin our model. There is no doubt that reconstruction helps to detectthe anomalies which behave not as expected. However, it is noteasy to build a suitable encoder and decoder to reconstruct thetime series as they are expected to be with anomalies interference.Moreover, the ability of representation is restricted as the latentpattern information is not fully considered. 3.3.2Discussion about Model Collapse. Interestingly, with onlysingle-type inputs (or saying, no negative samples included), ourDCdetector model does not fall into a trivial solution (model col-lapse). SimSiam gives the main credit for avoiding model col-lapse to stop gradient operation in their setting. However, we findthat DCdetector still works without stop gradient operation, al-though with the same parameters, the no stop gradient versiondoes not gain the best performance. Details are shown in the abla-tion study (.5.1).A possible explanation is that our two branches are totally asym-metric. Following the unified perspective proposed in , considerthe output vector of a branch as and can be decomposed intotwo parts and as = + , where = IE[] is the center vectordefined as an average of in the whole representation space, and is the residual vector. When the collapse happens, all vectors fallinto the center vector and dominates over . With two branchesnoted as = + , = + , if the branches are symmetric,i.e., = , then the distance between them is = . As and come from the same input example, it will lead to collapse.Fortunately, the two branches in DCdetector are asymmetric, soit is not easy for to be the same as even when and aresimilar. Thus, due to our asymmetric design, DCdetector is hard tofall into a trivial solution.",
  "Anomaly Criterion": "With the insight that normal points usually share latent patterns(with strong correlation among them), thus the distances of rep-resentation results from different views for normal points are lessthan that for anomalies. The final anomaly score of X IR isdefined as AnomalyScore(X) =(P, Stopgrad(N))+(N, Stopgrad(P)).(11)It is a point-wise anomaly score, and anomalies result in higherscores than normal points.Based on the point-wise anomaly score, a hyperparameter thresh-old is used to decide if a point is an anomaly (1) or not (0). If thescore exceeds the threshold, the output Y is an anomaly. That is",
  "EXPERIMENTS4.1Benchmark Datasets": "We adopt eight representative benchmarks from five real-worldapplications to evaluate DCdetector: (1) MSL (Mars Science Labo-ratory dataset) is collected by NASA and shows the condition ofthe sensors and actuator data from the Mars rover . (2) SMAP(Soil Moisture Active Passive dataset) is also collected by NASA andpresents the soil samples and telemetry information used by theMars rover . Compared with MSL, SMAP has more point anom-alies. (3) PSM (Pooled Server Metrics dataset) is a public datasetfrom eBay Server Machines with 25 dimensions . (4) SMD (ServerMachine Dataset) is a five-week-long dataset collected from an in-ternet company compute cluster, which stacks accessed traces ofresource utilization of 28 machines . (5) SWaT (Secure WaterTreatment) is a 51-dimension sensor-based dataset collected fromcritical infrastructure systems under continuous operations .(6) NIPS-TS-SWAN is an openly accessible comprehensive, multi-variate time series benchmark extracted from solar photosphericvector magnetograms in Spaceweather HMI Active Region Patchseries . (7) NIPS-TS-GECCO is a drinking water qualitydataset for the internet of things, which is published in the 2018genetic and evolutionary computation conference . Besidesthe above multivariate time series datasets, we also test univariatetime series datasets. (8) UCR is provided by the Multi-dataset TimeSeries Anomaly Detection Competition of KDD2021, and contains250 sub-datasets from various natural sources . It is a uni-variate time series of dataset subsequence anomalies. In each timeseries, there is one and only one anomaly. More details of the eightbenchmark datasets are summarized in in Appendix B.",
  "DCdetector83.5991.1087.1893.6999.6996.6095.6398.9297.0293.1199.7796.3397.1498.7497.94": ": Multi-metrics results on real-world multivariate datasets. Aff-P and Aff-R are the precision and recall of affiliation metric ,respectively. R_A_R and R_A_P are Range-AUC-ROC and Range-AUC-PR , which denote two scores based on label transformation underROC curve and PR curve, respectively. V_ROC and V_RR are volumes under the surfaces created based on ROC curve and PR curve ,respectively. All results are in %, and the best ones are in Bold.",
  "DCdetector98.9597.9454.7182.9391.5592.9388.4190.58": ", CL-MPPCA ; the density-estimation models: LOF , MP-PCACD , DAGMM ; the clustering-based methods: Deep-SVDD , THOC , ITAD ; the classic methods: OCSVM, OCSVM-based subsequence clustering (OCSVM*), IForest ,IForest-based subsequence clustering (IForest*), Gradient boostingregression (GBRT) ; the change point detection and time seriessegmentation methods: BOCPD , U-Time , TS-CP2 . Wealso compare our model with a time-series subsequence anomalydetection algorithm Matrix Profile .Besides, we adopt various evaluation criteria for comprehensivecomparison, including the commonly-used evaluation measures: ac-curacy, precision, recall, F1-score; the recently proposed evaluationmeasures: affiliation precision/recall pair and Volume underthe surface (VUS) . F1-score is the most widely used metric butdoes not consider anomaly events. Affiliation precision and recall are calculated based on the distance between ground truth andprediction events. VUS metric takes anomaly events into consid-eration based on the receiver operator characteristic (ROC) curve.Different metrics provide different evaluation views. We employthe commonly-used adjustment technique for a fair comparison, according to which all abnormalities in an abnormalsegment are considered to have been detected if a single time pointin an abnormal segment is identified.",
  "AnomalyTrans99.4960.4110073.0842DCdetector99.5161.6210074.0546": "various patch size and window size options for different datasets, asshown in in Appendix B. Our model defines an anomaly as atime point whose anomaly score exceeds a hyperparameter thresh-old , and its default value to 1. For all experiments on the abovehyperparameter selection and trade-off, please refer to Appendix C.Besides, all the experiments are implemented in PyTorch withone NVIDIA Tesla-V100 32GB GPU. Adam with default param-eter is applied for optimization. We set the initial learning rate to104 and the batch size to 128 with 3 epochs for all datasets.",
  "Main Results": "4.4.1Multivariate Anomaly Detection. We first evaluate our DCde-tector with nineteen competitive baselines on five real-world multi-variate datasets as shown in . It can be seen that our proposedDCdetector achieves SOTA results under the widely used F1 met-ric in most benchmark datasets. It is worth mentioning thatit has been an intense discussion among recent studies about howto evaluate the performance of anomaly detection algorithms fairly.Precision, Recall, and F1 score are still the most widely used metricsfor comparison. Some additional metrics (affiliation precision/re-call pair, VUS, etc.) are proposed to complement their deficiencies. To judge which metric is the best beyond thescope of our work, we include all the metrics here. As the recentAnomaly Transformer achieves better results than other baselinemodels, we mainly evaluate DCdetector with the Anomaly Trans-former in this multi-metrics comparison as shown in . Itcan be seen that DCdetector performs better or at least comparablewith the Anomaly Transformer in most metrics. We also evaluate the performance of another two datasets NIPS-TS-SWAN and NIPS-TS-GECCO in , which are more challeng-ing with more types of anomalies than the above five datasets. Al-though the two datasets have the highest (32.6% in NIPS-TS-SWAN)and lowest (1.1% in NIPS-TS-GECCO) anomaly ratio, DCdetector isstill able to achieve SOTA results and completely outperform othermethods. Similarly, multi-metrics comparisons between DCdetec-tor and Anomaly Transformer are conducted and summarized in, and DCdetector still achieves better performance in mostmetrics. 4.4.2Univariate Anomaly Detection. In this part, we compare theperformance of the DCdetector and Anomaly Transformer in uni-variate time series anomaly detection. We trained and tested sepa-rately for each of the sub-datasets in UCR datasets, and the averageresults are shown in . The count indicates how many sub-datasets have reached SOTA. The sub-datasets of the UCR all haveonly one segment of subsequence anomalies, and DCdetecter canidentify and locate them correctly and achieve optimal results.",
  "Model Analysis": "4.5.1Ablation Studies. shows the ablation study of stopgradient. According to the loss function definition in .3,we use two stop gradient modules in L{P, N; X}, noted as stopgradient in patch-wise branch and in-patch branch, respectively.With two-stop gradient modules, we can see that DCdetector gainsthe best performances. If no stop gradient is contained, DCdetec-tor still works and does not fall into a trivial solution. Moreover,in such a setting, it outperforms all the baselines except AnomalyTransformer. Besides, we also conduct an ablation study on how thetwo main preprocessing methods (bilateral filter for denoising andinstance normalization for normalization) affect the performanceof our method in . It can be seen that either of them slightlyimproves the performance of our model when used individually.However, if they are utilized simultaneously, the performance de-grades. Therefore, our final DCdetector only contains the instancenormalization module for preprocessing. More ablation studies onmulti-scale patching, window size, attention head, embedding di-mension, encoder layer, anomaly threshold, and metrics in lossfunction are left in Appendix C. 4.5.2Visual Analysis. We show how DCdetector works by visu-alizing different anomalies in . We use the synthetic datageneration methods reported in to generate univariate timeseries with different types of anomalies, including point-wise anom-alies (global point and contextual point anomalies) and pattern-wiseanomalies (seasonal, group, and trend anomalies) . It can beseen that DCdetector can robustly detect various anomalies betterfrom normal points with relatively higher anomaly scores. 4.5.3Parameter Sensitivity. We also study the parameter sensitiv-ity of the DCdetector. (a) shows the performance underdifferent window sizes. As discussed, a single point can not be takenas an instance in a time series. Window segmentation is widelyused in the analysis, and window size is a significant parameter. Forour primary evaluation, the window size is usually set as 60 or 100.Nevertheless, results in (a) demonstrate that DCdetector is",
  ": The averaged GPU memory cost and the averagedrunning time of 100 iterations during training with different sizes": "robust with a wide range of window sizes (from 30 to 210). Actu-ally, in the window size range , the performances fluctuateless than 2.3%. (b) shows the performance under differentmulti-scale sizes. Horizontal coordinate is the patch-size combina-tion used in multi-scale attention which means we combine severaldual-attention modules with a given patch-size combination. Un-like window size, the multi-scale design contributes to the finalperformance of the DCdetector, and different patch-size combina-tions lead to different performances. Note that when studying theparameter sensitivity of window size, the scale size is fixed as .When studying the parameter sensitivity of scale size, the windowsize is fixed at 60. (c) shows the performance under differ-ent numbers of encoder layers, since many deep neural networksperformances are affected by the layer number. (d) and Fig-ure 6(e) show model performances with different head numbers or sizes in attention. It can be seen that DCdetector achieves thebest performance with a small attention head number and size. The memory and time usages with different sizes areshown in . Based on and (e), we set thedimension of the hidden state = 256 for the performance-complexity trade-off, and it can be seen that DCdetector can workquite well under = 256 with efficient running time and smallmemory consumption.",
  "This paper proposes a novel algorithm named DCdetector for time-series anomaly detection. We design a contrastive learning-based": "dual-branch attention structure in DCdetector to learn a permu-tation invariant representation. Such representation enlarges thedifferences between normal points and anomalies, improving de-tection accuracy. Besides, two additional designs: multiscale andchannel independence patching, are implemented to enhance theperformance. Moreover, we propose a pure contrastive loss functionwithout reconstruction error, which empirically proves the effec-tiveness of contrastive representation compared to the widely usedreconstructive one. Lastly, extensive experiments show that DCde-tector achieves the best or comparable performance on eight bench-mark datasets compared to various state-of-the-art algorithms.",
  "Ryan Prescott Adams and David JC MacKay. 2007. Bayesian online changepointdetection. arXiv preprint arXiv:0710.3742 (2007)": "Archana Anandakrishnan, Senthil Kumar, Alexander Statnikov, Tanveer Faruquie,and Di Xu. 2018. Anomaly detection in finance: editors introduction. In KDD2017 Workshop on Anomaly Detection in Finance. PMLR, 17. OD Anderson. 1976. Time-Series. 2nd edn. Rafal Angryk, Petrus Martens, Berkay Aydin, Dustin Kempton, Sushant Mahajan,Sunitha Basodi, Azim Ahmadzadeh, Xumin Cai, Soukaina Filali Boubrahimi,Shah Muhammad Hamdi, Micheal Schuh, and Manolis Georgoulis. 2020. SWAN-SF.",
  "Paul Boniol and Themis Palpanas. 2020. Series2Graph: Graph-based SubsequenceAnomaly Detection for Time Series. ArXiv abs/2207.12208 (2020)": "Loc Bontemps, Van Loi Cao, James McDermott, and Nhien-An Le-Khac. 2016.Collective anomaly detection based on long short-term memory recurrent neuralnetworks. In International conference on future data and security engineering.Springer, 141152. George EP Box and David A Pierce. 1970. Distribution of residual autocorrelationsin autoregressive-integrated moving average time series models. Journal of theAmerican statistical Association 65, 332 (1970), 15091526. Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and Jrg Sander. 2000.LOF: identifying density-based local outliers. In Proceedings of the 2000 ACMSIGMOD international conference on Management of data. 93104. David Campos, Tung Kieu, Chenjuan Guo, Feiteng Huang, Kai Zheng, Bin Yang,and Christian S Jensen. 2021. Unsupervised Time Series Outlier Detection withDiversity-Driven Convolutional EnsemblesExtended Version. arXiv preprintarXiv:2111.11108 (2021).",
  "Mikel Canizo, Isaac Triguero, Angel Conde, and Enrique Onieva. 2019. Multi-headCNNRNN for multi-time series anomaly detection: An industrial case study.Neurocomputing 363 (2019), 246260": "Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, andArmand Joulin. 2020. Unsupervised learning of visual features by contrastingcluster assignments. Advances in neural information processing systems 33 (2020),99129924. Sucheta Chauhan and Lovekesh Vig. 2015. Anomaly detection in ECG timesignals via deep long short-term memory networks. In 2015 IEEE internationalconference on data science and advanced analytics (DSAA). IEEE, 17. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. Asimple framework for contrastive learning of visual representations. In Interna-tional conference on machine learning. PMLR, 15971607. Xuanhao Chen, Liwei Deng, Feiteng Huang, Chengwei Zhang, Zongquan Zhang,Yan Zhao, and Kai Zheng. 2021. Daemon: Unsupervised anomaly detectionand interpretation for multivariate time series. In 2021 IEEE 37th InternationalConference on Data Engineering (ICDE). IEEE, 22252230.",
  "Xinlei Chen and Kaiming He. 2021. Exploring simple siamese representationlearning. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition. 1575015758": "Zekai Chen, Dingshuo Chen, Xiao Zhang, Zixuan Yuan, and Xiuzhen Cheng.2021. Learning graph structures with transformer for multivariate time-seriesanomaly detection in IoT. IEEE Internet of Things Journal 9, 12 (2021), 91799189. Haibin Cheng, Pang-Ning Tan, Christopher Potter, and Steven Klooster. 2009.Detection and characterization of anomalies in multivariate time series. In Pro-ceedings of the 2009 SIAM international conference on data mining. SIAM, 413424.",
  "Andrew A Cook, Gksel Msrl, and Zhong Fan. 2019. Anomaly detection for IoTtime-series data: A survey. IEEE Internet of Things Journal 7, 7 (2019), 64816494": "Hoang Anh Dau, Anthony Bagnall, Kaveh Kamgar, Chin-Chia Michael Yeh, YanZhu, Shaghayegh Gharghabi, Chotirat Ann Ratanamahatana, and Eamonn Keogh.2019. The UCR time series archive. IEEE/CAA Journal of Automatica Sinica 6, 6(2019), 12931305. Shohreh Deldari, Daniel V Smith, Hao Xue, and Flora D Salim. 2021. Time serieschange point detection with self-supervised contrastive predictive coding. InProceedings of the Web Conference 2021. 31243135.",
  "Ailin Deng and Bryan Hooi. 2021. Graph neural network-based anomaly detectionin multivariate time series. In Proceedings of the AAAI conference on artificialintelligence, Vol. 35. 40274035": "Shereen Elsayed, Daniela Thyssens, Ahmed Rashed, Hadi Samer Jomaa, and LarsSchmidt-Thieme. 2021. Do we really need deep learning models for time seriesforecasting? arXiv preprint arXiv:2101.02118 (2021). Jingkun Gao, Xiaomin Song, Qingsong Wen, Pichao Wang, Liang Sun, and HuanXu. 2020. RobustTAD: Robust time series anomaly detection via decompositionand convolutional neural networks. KDD Workshop MileTS (2020). Koosha Golmohammadi and Osmar R Zaiane. 2015. Time series contextualanomaly detection for detecting market manipulation in stock market. In 2015IEEE international conference on data science and advanced analytics (DSAA). IEEE,110. Jean-Bastien Grill, Florian Strub, Florent Altch, Corentin Tallec, PierreRichemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, ZhaohanGuo, Mohammad Gheshlaghi Azar, et al. 2020. Bootstrap your own latent-a newapproach to self-supervised learning. Advances in neural information processingsystems 33 (2020), 2127121284. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2020. Mo-mentum contrast for unsupervised visual representation learning. In Proceedingsof the IEEE/CVF conference on computer vision and pattern recognition. 97299738. Chengqiang Huang, Yulei Wu, Yuan Zuo, Ke Pei, and Geyong Min. 2018. Towardsexperienced anomaly detector through reinforcement learning. In Proceedings ofthe AAAI Conference on Artificial Intelligence, Vol. 32. Alexis Huet, Jose Manuel Navarro, and Dario Rossi. 2022. Local Evaluation ofTime Series Anomaly Detection Algorithms. In Proceedings of the 28th ACMSIGKDD Conference on Knowledge Discovery and Data Mining. 635645. Kyle Hundman, Valentino Constantinou, Christopher Laporte, Ian Colwell, andTom Soderstrom. 2018. Detecting spacecraft anomalies using lstms and nonpara-metric dynamic thresholding. In Proceedings of the 24th ACM SIGKDD interna-tional conference on knowledge discovery & data mining. 387395. Yang Jiao, Kai Yang, Dongjing Song, and Dacheng Tao. 2022. Timeautoad: Au-tonomous anomaly detection with self-supervised contrastive loss for multivari-ate time series. IEEE Transactions on Network Science and Engineering 9, 3 (2022),16041619. Neha Kant and Manish Mahajan. 2019. Time-series outlier detection usingenhanced k-means in combination with pso algorithm. In Engineering Vibration,Communication and Information Processing: ICoEVCI 2018, India. Springer, 363373.",
  "Pawe Karczmarek, Adam Kiersztyn, Witold Pedrycz, and Ebru Al. 2020. K-Means-based isolation forest. Knowledge-based systems 195 (2020), 105659": "Eamonn Keogh, Dutta Roy Taposh, U Naik, and A Agrawal. 2021. Multi-datasetTime-Series Anomaly Detection Competition. In ACM SIGKDD InternationalConference on Knowledge Discovery and Data Mining. hexagonml.com/practice/competition/39. Adnan Khan, Sarah AlBarri, and Muhammad Arslan Manzoor. 2022. Contrastiveself-supervised learning: a survey on different architectures. In 2022 2nd Interna-tional Conference on Artificial Intelligence (ICAI). IEEE, 16. Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, andJaegul Choo. 2021. Reversible instance normalization for accurate time-seriesforecasting against distribution shift. In International Conference on LearningRepresentations.",
  "Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-mization. arXiv preprint arXiv:1412.6980 (2014)": "Kwei-Herng Lai, Daochen Zha, Junjie Xu, Yue Zhao, Guanchu Wang, and XiaHu. 2021. Revisiting time series outlier detection: Definitions and benchmarks.In Thirty-fifth Conference on Neural Information Processing Systems Datasets andBenchmarks Track (Round 1). Dan Li, Dacheng Chen, Baihong Jin, Lei Shi, Jonathan Goh, and See-Kiong Ng.2019. MAD-GAN: Multivariate anomaly detection for time series data with gen-erative adversarial networks. In Artificial Neural Networks and Machine LearningICANN 2019: Text and Time Series: 28th International Conference on ArtificialNeural Networks, Munich, Germany, September 1719, 2019, Proceedings, Part IV.Springer, 703716. Longyuan Li, Junchi Yan, Qingsong Wen, Yaohui Jin, and Xiaokang Yang. 2022.Learning robust deep state space for unsupervised anomaly detection in con-taminated time-series. IEEE Transactions on Knowledge and Data Engineering(2022). Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang,and Xifeng Yan. 2019. Enhancing the locality and breaking the memory bottle-neck of transformer on time series forecasting. Advances in neural informationprocessing systems 32 (2019). Xing Li, Qiquan Shi, Gang Hu, Lei Chen, Hui Mao, Yiyuan Yang, Mingxuan Yuan,Jia Zeng, and Zhuo Cheng. 2021. Block Access Pattern Discovery via CompressedFull Tensor Transformer. In Proceedings of the 30th ACM International Conferenceon Information & Knowledge Management. 957966. Zhihan Li, Youjian Zhao, Jiaqi Han, Ya Su, Rui Jiao, Xidao Wen, and Dan Pei. 2021.Multivariate time series anomaly detection and interpretation using hierarchicalinter-metric and temporal embedding. In Proceedings of the 27th ACM SIGKDDConference on Knowledge Discovery & Data Mining. 32203230.",
  "Zijian Niu, Ke Yu, and Xiaofei Wu. 2020. LSTM-based VAE-GAN for time-seriesanomaly detection. Sensors 20, 13 (2020), 3738": "John Paparrizos, Paul Boniol, Themis Palpanas, Ruey S Tsay, Aaron Elmore, andMichael J Franklin. 2022. Volume under the surface: a new accuracy evaluationmeasure for time-series anomaly detection. Proceedings of the VLDB Endowment15, 11 (2022), 27742787. Daehyung Park, Yuuna Hoshi, and Charles C Kemp. 2018. A multimodal anomalydetector for robot-assisted feeding using an lstm-based variational autoencoder.IEEE Robotics and Automation Letters 3, 3 (2018), 15441551. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, GregoryChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019.Pytorch: An imperative style, high-performance deep learning library. Advancesin neural information processing systems 32 (2019). Mathias Perslev, Michael Jensen, Sune Darkner, Poul Jrgen Jennum, and Chris-tian Igel. 2019. U-time: A fully convolutional network for time series segmentationapplied to sleep staging. Advances in Neural Information Processing Systems 32(2019).",
  "Peter J Rousseeuw and Annick M Leroy. 2005. Robust regression and outlierdetection. John wiley & sons": "Lukas Ruff, Jacob R Kauffmann, Robert A Vandermeulen, Grgoire Montavon,Wojciech Samek, Marius Kloft, Thomas G Dietterich, and Klaus-Robert Mller.2021. A unifying review of deep and shallow anomaly detection. Proc. IEEE 109,5 (2021), 756795. Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib AhmedSiddiqui, Alexander Binder, Emmanuel Mller, and Marius Kloft. 2018. Deepone-class classification. In International conference on machine learning. PMLR,43934402. Mayu Sakurada and Takehisa Yairi. 2014. Anomaly detection using autoencoderswith nonlinear dimensionality reduction. In Proceedings of the MLSDA 2014 2ndworkshop on machine learning for sensory data analysis. 411. David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. 2020.DeepAR: Probabilistic forecasting with autoregressive recurrent networks. Inter-national Journal of Forecasting 36, 3 (2020), 11811191.",
  "Lifeng Shen, Zhuocong Li, and James Kwok. 2020. Timeseries anomaly detectionusing temporal hierarchical one-class network. Advances in Neural InformationProcessing Systems 33 (2020), 1301613026": "Youjin Shin, Sangyup Lee, Shahroz Tariq, Myeong Shin Lee, Okchul Jung, DaewonChung, and Simon S Woo. 2020. Itad: integrative tensor-based anomaly detectionsystem for reducing false positives of satellite systems. In Proceedings of the29th ACM international conference on information & knowledge management.27332740. Ya Su, Youjian Zhao, Chenhao Niu, Rong Liu, Wei Sun, and Dan Pei. 2019. Robustanomaly detection for multivariate time series through stochastic recurrentneural network. In Proceedings of the 25th ACM SIGKDD international conferenceon knowledge discovery & data mining. 28282837. Shahroz Tariq, Sangyup Lee, Youjin Shin, Myeong Shin Lee, Okchul Jung, DaewonChung, and Simon S Woo. 2019. Detecting anomalies in space using multivariateconvolutional LSTM with mixtures of probabilistic PCA. In Proceedings of the25th ACM SIGKDD international conference on knowledge discovery & data mining.21232133.",
  "David MJ Tax and Robert PW Duin. 2004. Support vector data description.Machine learning 54, 1 (2004), 4566": "Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. 2017. Improved texturenetworks: Maximizing quality and diversity in feed-forward stylization andtexture synthesis. In Proceedings of the IEEE conference on computer vision andpattern recognition. 69246932. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin. 2017. Attention is allyou need. Advances in neural information processing systems 30 (2017). Qingsong Wen, Linxiao Yang, Tian Zhou, and Liang Sun. 2022. Robust time seriesanalysis and applications: An industrial perspective. In Proceedings of the 28thACM SIGKDD Conference on Knowledge Discovery and Data Mining. 48364837. Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. 2021. Autoformer: De-composition transformers with auto-correlation for long-term series forecasting.Advances in Neural Information Processing Systems 34 (2021), 2241922430. Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. 2018. Unsupervisedfeature learning via non-parametric instance discrimination. In Proceedings ofthe IEEE conference on computer vision and pattern recognition. 37333742. Haowen Xu, Wenxiao Chen, Nengwen Zhao, Zeyan Li, Jiahao Bu, Zhihan Li,Ying Liu, Youjian Zhao, Dan Pei, Yang Feng, et al. 2018. Unsupervised anomalydetection via variational auto-encoder for seasonal kpis in web applications. InProceedings of the 2018 world wide web conference. 187196.",
  "Yiyuan Yang, Rongshang Li, Qiquan Shi, Xijun Li, Gang Hu, Xing Li, and Mingx-uan Yuan. 2023. SGDP: A Stream-Graph Neural Network Based Data Prefetcher.arXiv preprint arXiv:2304.03864 (2023)": "Yiyuan Yang, Yi Li, and Haifeng Zhang. 2021. Pipeline safety early warningmethod for distributed signal using bilinear CNN and LightGBM. In ICASSP2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing(ICASSP). IEEE, 41104114. Yiyuan Yang, Yi Li, Taojia Zhang, Yan Zhou, and Haifeng Zhang. 2021. Earlysafety warnings for long-distance pipelines: A distributed optical fiber sensormachine learning approach. In Proceedings of the AAAI Conference on ArtificialIntelligence, Vol. 35. 1499114999. Yiyuan Yang, Haifeng Zhang, and Yi Li. 2021. Long-distance pipeline safety earlywarning: a distributed optical fiber sensing semi-supervised learning method.IEEE Sensors Journal 21, 17 (2021), 1945319461. Yiyuan Yang, Haifeng Zhang, and Yi Li. 2021. Pipeline safety early warningby multifeature-fusion CNN and LightGBM analysis of signals from distributedoptical fiber sensors. IEEE Transactions on Instrumentation and Measurement 70(2021), 113. Mang Ye, Xu Zhang, Pong C Yuen, and Shih-Fu Chang. 2019. Unsupervisedembedding learning via invariant and spreading instance feature. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 62106219. Chin-Chia Michael Yeh, Yan Zhu, Liudmila Ulanova, Nurjahan Begum, Yifei Ding,Hoang Anh Dau, Diego Furtado Silva, Abdullah Mueen, and Eamonn Keogh. 2016.Matrix profile I: all pairs similarity joins for time series: a unifying view thatincludes motifs, discords and shapelets. In 2016 IEEE 16th international conferenceon data mining (ICDM). Ieee, 13171322.",
  "Zahra Zamanzadeh Darban, Geoffrey I Webb, Shirui Pan, Charu C Aggarwal,and Mahsa Salehi. 2022. Deep Learning for Time Series Anomaly Detection: ASurvey. arXiv e-prints (2022), arXiv2211": "Chaoning Zhang, Kang Zhang, Chenshuang Zhang, Trung X Pham, Chang DYoo, and In So Kweon. 2022. How does SimSiam avoid collapse without nega-tive samples? a unified understanding with self-supervised contrastive learning.Proceedings of International Conference on Learning Representations (ICLR) (2022). Chaoli Zhang, Tian Zhou, Qingsong Wen, and Liang Sun. 2022. TFAD: A De-composition Time Series Anomaly Detection Architecture with Time-FrequencyAnalysis. In Proceedings of the 31st ACM International Conference on Information& Knowledge Management. 24972507. Yuxin Zhang, Jindong Wang, Yiqiang Chen, Han Yu, and Tao Qin. 2022. Adap-tive memory networks with self-supervised learning for unsupervised anomalydetection. IEEE Transactions on Knowledge and Data Engineering (2022). Hang Zhao, Yujing Wang, Juanyong Duan, Congrui Huang, Defu Cao, YunhaiTong, Bixiong Xu, Jing Bai, Jie Tong, and Qi Zhang. 2020. Multivariate Time-series Anomaly Detection via Graph Attention Network. 2020 IEEE InternationalConference on Data Mining (ICDM) (2020), 841850. Hang Zhao, Yujing Wang, Juanyong Duan, Congrui Huang, Defu Cao, YunhaiTong, Bixiong Xu, Jing Bai, Jie Tong, and Qi Zhang. 2020. Multivariate time-series anomaly detection via graph attention network. In 2020 IEEE InternationalConference on Data Mining (ICDM). IEEE, 841850.",
  "return patch_wise_mean, in_patch_mean": "Two main algorithms and their codes are presented as follows,the DCdetector Outliner (Algorithm 1) and the Dual AttentionContrastive Structure (Algorithm 2).For the DCdetector Outliner (Algorithm 1), we use nn.ModuleList()in PyTorch to define multiple patching scales and perform em-bedding and dual attention operations for each set of patch sizes.After instance normalization, patch-wise encoders and in-patchencoders are calculated in the patch number dimension and the",
  "return series_patch_size, series_patch_num": "patch size dimension, respectively. Finally, we average the differ-ent patching scales to obtain the final patch-wise representation Nand in-patch representation P.For the Dual Attention Contrastive Structure (Algorithm 2), wecalculate patch-wise representation based on Eq.1 - Eq.3 and in-patch representation using Eq.4 - Eq.6, respectively. Finally, weapply different upsampling methods to make the shape of the tworepresentations the same for subsequent comparison of representa-tion discrepancy. Note that, we do not need to calculate the specificattention values, as only two representations are made and theattention weights can also be used as representations as well asimproving the efficiency of the code. Besides, only a single patchscale of dual attention contrastive structure is shown here, whichmay suffer from information loss when upsampling is performed.However, the multi-patch scale will compensate for this issue, asshown in Algorithm 1.",
  "C.1Study on Metrics in Loss Function": "We use different statistical distances to calculate the discrepancybetween patch-wise representation and in-patch representation,and the results are shown in . The loss function proposed in.3 can get the SOTA performance in all benchmarks. Notethat only using simple KL divergence, which is an asymmetricalloss function, we can still get a comparable result. However, forthe Jensen-Shannon (JS) divergence, there is visible performancedegradation, especially for the MSL benchmark.",
  "C.2Study on Multi-scale Patching": "The multi-patching scale {, , , , , , }are tested. Patch size preference is for odd numbers to prevent in-formation loss during upsampling. Generally, multi-scale designresults in larger memory and different datasets have different bestmulti-patching scales. This is perhaps due to different informationdensities and anomaly types in different situations. The details ofevaluation results are shown in .",
  "C.3Study on Window Size": "Window size is a significant hyper-parameter in time series analysis.It is used to split time series into instances, as usually, a single pointcan not be considered as a sample. The results in showthat DCdetector is rather robust in different window sizes. Actually,in a large range , the performances are slightly lower thanthe best ones for all benchmarks. Besides, we also test the impact ofwindow size on memory cost and running time. The window sizewill affect the memory cost in a quadratic computational complex-ity way. So, the trade-off between slide window size and memorycost/running time is pretty important, especially for real-life sce-narios. Fortunately, DCdetector can work optimally with a windowsize of less than 105 in all benchmarks, which greatly decreases thecomplexity of the model and its memory cost.",
  "C.4Study on Attention Head": "Generally, multi-head attention is widely used in attention net-works. We study the influence of attention head number in DCde-tector. In general, the number of attention heads is even, so we set {1, 2, 4, 8}. Fortunately, with a small attention head number, asshown in , our model still achieves good performances (the",
  "C.5Study on Embedding Dimension": "The embedding dimension is another important parameterin the attention network. As a hyperparameter of the hidden chan-nels, it may have impacts on model performance, memory cost, andrunning efficiency. We set {128, 256, 512, 1024} as sug-gested hyperparameters by Transformer . For SMAP and PSM,it has little effect on the final results. As for MSL, it achieves the bestperformance with a small size and small memory. Overall,the proposed DCdetector can achieve quite good performance evenwith a small memory cost and good real-time performance. Detailsare in .",
  "C.6Study on Encoder Layer": "Many deep models performances are dependent on the numberof network layers . We also show the influence of the number ofencoder layers in . We set {1, 2, 3, 4, 5} as suggestedhyperparameters by Transformer . Different benchmarks havedifferent optimal parameters. Luckily, our model can gain the bestperformance in no more than 3 layers, and will not fail with toofew encoder layers or over-fit with too many encoder layers.",
  "C.7Study on Anomaly Threshold": "Anomaly threshold is a hyperparameter, which may affect thedetermination of anomaly or not, based on Eq. 12. We have a defaultvalue of 1 for all benchmarks. As shown in , when it is in therange of 0.5 to 1, it has little effect on the final model performance.PSM and SMAP are also more robust to anomaly threshold thanMSL. For the three benchmarks, its best results appear when equals 0.7 or 0.8."
}