{
  "ABSTRACT": "Time series data has been demonstrated to be crucial in variousresearch fields. The management of large quantities of time seriesdata presents challenges in terms of deep learning tasks, particularlyfor training a deep neural network. Recently, a technique namedDataset Condensation has emerged as a solution to this problem.This technique generates a smaller synthetic dataset that has com-parable performance to the full real dataset in downstream taskssuch as classification. However, previous methods are primarily de-signed for image and graph datasets, and directly adapting them tothe time series dataset leads to suboptimal performance due to theirinability to effectively leverage the rich information inherent intime series data, particularly in the frequency domain. In this paper,we propose a novel framework named Dataset Condensation forTime Series Classification via Dual Domain Matching (CondTSC)which focuses on the time series classification dataset condensationtask. Different from previous methods, our proposed frameworkaims to generate a condensed dataset that matches the surrogateobjectives in both the time and frequency domains. Specifically,CondTSC incorporates multi-view data augmentation, dual domaintraining, and dual surrogate objectives to enhance the dataset con-densation process in the time and frequency domains. Throughextensive experiments, we demonstrate the effectiveness of our pro-posed framework, which outperforms other baselines and learns acondensed synthetic dataset that exhibits desirable characteristicssuch as conforming to the distribution of the original data.",
  "Corresponding Author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08...$15.00",
  "INTRODUCTION": "The exponential growth of time series data across various domains,such as traffic forecasting , clinical diagnosis , and financial analysis , has presented opportunities forresearchers and practitioners. However, the sheer volume of datahas imposed burdens in terms of storage, processing, and analysisin terms of deep learning context. In the context of some deeplearning downstream tasks such as neural architecture search and continual learning , the utilization of the full dataset fortraining has the potential to yield bad efficiency. Consequently,there is an urgent need to develop effective strategies for condensingtime series datasets while preserving their essential information toalleviate the efficiency challenge of training a deep model.To address this issue, Dataset Condensation has emergedas a powerful and efficient approach. The core idea is learning asmall synthetic dataset that achieves comparable performance to theoriginal full dataset when training the identical model, as depictedin . Unlike traditional data compression techniques, the pri-mary objective of dataset condensation is to distill a reduced-sizedsynthetic dataset that could efficiently train a model and effectivelygeneralize to unseen test data rather than just reduce the size with-out information loss. Notably, the fundamental principles of datacondensation encompass several strategies aimed at addressingspecific surrogate objectives. These strategies include maximizingthe testing performance , matching thetraining gradient , and matchingthe hidden state distribution when training .Currently, most of the data condensation research focuses onimage , and graph dataset. How-ever, the dataset condensation research for time series data is notwell explored. Different from the other modalities, time series dataexhibits distinct characteristics, such as periodicity and seasonality,",
  "KDD 24, August 2529, 2024, Barcelona, SpainZhanyu Liu, Ke Hao, Guanjie Zheng, and Yanwei Yu": "which are closely related to its frequency domain. The frequencydomain plays a crucial role, offering valuable insights and aiding intime series analysis . In the frequency domain, thekey patterns and characteristics of time series data that might bedifficult to discern in the time domain alone could be identified. Con-sequently, there is a pressing need to develop dataset condensationmethods specifically tailored for time series data, capitalizing onthe rich information available in the frequency domain to enhancethe efficiency and effectiveness of condensation techniques.In this paper, we propose a framework named Dataset Cond-ensation for Time Series Classification via Dual Domain Matching(CondTSC). The proposed framework aims to generate a condensedsynthetic dataset by matching the surrogate objectives betweensynthetic dataset and full real dataset in both the time domainand frequency domain. Specifically, we first introduce Multi-viewData Augmentation module, which utilizes the data augmentationtechniques to project the synthetic data into different frequency-enhanced spaces. This approach enriches data samples and strength-ens the matching process of surrogate objectives, resulting in syn-thetic data that is more robust and representative. Then, we proposeDual Domain Training module, which leverages both the timedomain and frequency domain of the synthetic data. By encod-ing information from both domains, the downstream surrogatematching module benefits from the rich dual-domain information.Furthermore, we introduce Dual Objectives Matching module.By matching the surrogate objectives, training these networks withthe condensed synthetic dataset yields similar gradient and hiddenstate distributions as training with the full real dataset. This enablesthe synthetic data to learn the dynamics of real data when trainingthe networks such as CNN or Transformer. Overall, the CondTSCframework generates a condensed synthetic dataset that keeps thedynamics of training a network for time series analysis.Extensive experiments show that CondTSC achieves outstandingperformance in the data condensation task in many scenarios suchas human activity recognition (HAR) and insect audio classification.For example, we achieve 61.38% accuracy with 0.1% of the originalsize and 86.64% accuracy with 1% of the original size, comparedwith 93.14% accuracy with full original size in HAR dataset.Our contributions can be summarized as follows: To the best of our knowledge, we are the first to systemati-cally complete the data condensation problem for time seriesclassification. We highlight the importance of the frequencydomain in time series analysis and propose to integrate thefrequency domain view into the dataset condensation pro-cess. This allows for a comprehensive understanding andanalysis of time series patterns and behaviors. We propose a novel framework named CondTSC, whichincorporates multi-view data augmentation, dual domaintraining, and dual surrogate objectives to enhance the datasetcondensation process in the time and frequency domains. We validate the effectiveness of CondTSC through extensiveexperiments. Results show our proposed framework not onlyoutperforms other baselines in the condensed training set-ting but also shows good characteristics such as conformingto the distribution of the original data.",
  "RELATED WORK2.1Time Series Compression": "Time series compression aims to compress the time series data with-out losing the important information, which benefits the process ofstorage, analysis, and transmission . There are many works thatfocus on this problem by using dictionary compression , Huff-man coding , and so on. However, the compressed dataset doesnot work efficiently in training a deep network. In order to utilizethe compressed data, it must first be decoded to its original size,and subsequently, the network iterates through the original-sizeddataset to generate loss for back-propagation. Both of these pro-cesses are time-consuming. Consequently, time series compressionis unsuitable for tasks demanding high training efficiency, such asneural architecture search . To handle this problem, a newlyemerging area Dataset Condensation gives its solution.",
  "Dataset Condensation": "Dataset Condensation aims to learn a small synthetic dataset thatcould achieve comparable performance to the original full datasetwhen training a deep neural network. Currently, there are threemain approaches to Dataset Condensation as follows. (1) The testingperformance maximization approaches focus on maximizing theperformance on the real-world test-split dataset of the networktrained by the condensed synthetic dataset .These approaches would face the challenge of the high cost ofcomputing high-order derivatives and thus some of the works utilizethe kernel ridge regression or neural feature regression to reduce the computational complexity. (2) The training gradientmatching approach aims to match the training gradient of the modeltrained on the condensed dataset with that of the model trained onthe full dataset . By matching thegradients, the condensed dataset captures the essential full-datatraining dynamics and enables efficient model training. Moreover,there are two main surrogate matching objectives including single-step gradient and multi-step gradients . (3) The hidden state distribution matching approach focuseson matching the distribution of hidden states during the trainingprocess . These approaches have high efficiency dueto their low computational complexity. However, all approachesof these three types focus on images or graphs, lacking detailedanalysis for techniques for the time series datasets.",
  "Frequency-enhanced Time series analysis": "Recently, several studies have focused on enhancing time seriesanalysis by leveraging the frequency domain. MCNN incor-porates multi-frequency augmentation of time series data and uti-lizes a simple convolutional network. Frequency-CNN utilizes asimple convolutional network on the frequency domain and getsgood results on automatic early detection. BTSF proposes iter-ative bilinear spectral-time mutual fusion and trains the encoderbased on triplet loss. Recently, many studies have focused on de-veloping frequency-based self-supervised architectures specificallydesigned for time series analysis. CoST and TF-C con-struct contrastive losses based on the frequency domain patternto learn more robust representations. FEDFormer proposes",
  "PRELIMINARY3.1Problem Overview": "The goal of Data Condensation is to learn a synthetic dataset S ={(,)}|S|=1 from the real training dataset T = {(,)}| T|=1, whereeach data , R, and the label Y = {0, 1, . . . , 1}.We want the size of the synthetic dataset to be much smaller thanthe size of the real training dataset, i.e., |S| |T | while keepingthe network trained by synthetic data comparable to the networktrained by the real training dataset.Formally, by denoting as the network parameter trained by Sand as the network parameter trained by T, we could write asfollows:",
  "METHOD": "In this section, we present our proposed framework CondTSC,which comprises three key modules. The frameworks architectureis depicted in , and the detailed pseudo code is shown in Alg. 1.The first module, termed Multi-view Data Augmentation, projectsthe synthetic dataset S into spaces of multi-view to enrich thedata samples. Next, the Dual Domain Encoding module appliesthe Fourier transform and trains separate networks on each do-main to obtain network parameters specific to S and T. Lastly, theDual Objectives Matching module leverages the trained networkparameters to construct two types of surrogate objectives: parame-ter matching and embedding space matching. By matching thesesurrogate objectives, the losses computed across all views can beback-propagated to update the synthetic dataset S accordingly.",
  "Initializing S": "In previous research, pixel-level random initialization has beencommonly utilized to initialize S, as evidenced by numerous stud-ies . However, these studies have focused primar-ily on condensing image or graph datasets. In contrast, empiricalevidence suggests that random initialization results in poor perfor-mance when condensing time-series datasets. Therefore, to addressthis issue, we propose using K-means to cluster the data samples ofeach class from the full dataset T, and then selecting the clusteringcentroids of each class as the initial data samples of S. Formally, itcould be formulated as follows.",
  "The frequency domain is more separable": ": The TSNE visualization of the Insect dataset in both the time domain and the frequency domain. This demonstratesthe intuition to do augmentation and utilize the dual-domain information for the time series data. The data in the frequencydomain shows better decision boundaries and the data augmentations squeeze the boundaries between different classes.",
  "Multi-view Data Augmentation": "Goal: This module essentially projects the synthetic data S intoembedding spaces of multiple views by conducting several data aug-mentations sequentially. The visualization in demon-strates the increased separability of the frequency domain, sug-gesting that augmenting data in the frequency domain yields moreeffective results. Consequently, different from that uses aug-mentations for images such as jittering and cropping, we employfrequency-domain augmentations such as low pass filtering. Theaugmentation process, as depicted in , generates more dataand compresses the boundaries between classes in the frequencydomain. As a result, the synthetic data S can more effectively ad-here to the training dynamics such as gradient and embeddingsexhibited by the real data T by virtue of matching the surrogateobjectives across different views of the synthetic data S.Augmentations: As shown in (a), we will conduct threefrequency-enhanced data augmentations sequentially to S, includ-ing Low Pass Filter (LPF), Fourier Transform Phase Perturbation(FTPP), Fourier Transform Magnitude Perturbation (FTMP).First, by denoting the Fourier Transform and Inverse FourierTransform as () and () respectively, the first data augmen-tation LPF could be formally written as follows:",
  "S = (( (S))).(8)": "Here () indicates the magnitude of the frequency do-main. This perturbation introduces variations in the magnitudeof the frequency domain, allowing the synthetic data to capture awider range of amplitude and intensity characteristics.In summary, the Multi-view Data Augmentation module in theCondTSC framework plays a crucial role in enriching the datasamples and enhancing the matching of surrogate objectives byprojecting the synthetic data S into multiple high-dimensionalspaces through sequential data augmentations.",
  "Dual Domain Training": "Goal: As shown in , the frequency domain is more separa-ble, which means fusing the frequency domain into the datasetcondensation could enhance the performance and effectiveness.Consequently, this module aims to incorporate both the time andfrequency domains and utilize them in the construction of surrogateobjectives in the next module.Training Module: The Training Module receives three types ofinput: the synthetic data in both the time domain S and the fre-quency domain S , the full real data in both the time domain T and the frequency domain T , and the sampled network parame-ters in both time domain 0 and the frequency domain 0 randomlysampled from (). For simplicity, we here introduce the trainingprocess in the time domain, while the process in the frequencydomain is identical.The training process follows standard training procedures, wherewe utilize the synthetic data S and the real data T to train thenetwork parameters 0. We employ standard training settings, suchas stochastic gradient descent or Adam optimization, to update theparameters based on the loss between the network predictions andthe ground truth labels. Furthermore, we train the network withS and T for and iterations respectively. We set toavoid overfitting to the small synthetic data S.The output of the Training Module is the trained parameters (trained by S) and (trained by T). These trained parameters playa crucial role in the subsequent modules as they are utilized in theconstruction of surrogate objectives, which facilitate the matchingof training dynamics between the synthetic and real datasets.",
  "return S": "Model Parameters (): As aforementioned, the aim of datasetcondensation is to train a network using the synthetic dataset Sthat achieves performance comparable to a network trained usingthe real dataset T. To achieve this, we will match the surrogate ob-jectives with the given parameter distribution. However, in practice,computing the exact parameter distribution is infeasible. Therefore,we collect the parameters from the training trajectories when train-ing a network with T and utilize them as an approximation forthe parameter distribution (). Moreover, in line with prior re-search , we employ a single network structure in thetraining process, i.e., the used () is from a parameter space ofsingle network structure . Furthermore, we conduct experimentsthat train S based on one network structure distribution 1()and evaluate the trained S on another distribution 2() to furtherevaluate the effectiveness of our proposed framework.",
  "Goal: This module aims to generate surrogate objectives from differ-ent views using both the synthetic dataset S and the real dataset T,": "and subsequently match these objectives. By computing the match-ing loss across the different views, we can update the condenseddataset S. After the surrogate objectives are well matched, the syn-thetic dataset S could be considered as the condensed dataset ofthe original dataset T for learning the downstream task.Gradient Matching: The aim of gradient matching is to make thegradient of S and T similar. By doing so, training a network withS could generate a similar trained model, and thus the performancecould be comparable. Here we take the multi-step gradient of train-ing a network parameterized by0 as the surrogate objective similarto . Intuitively, we directly minimize the mean squareerror between these gradients. Taking the parameter in the timedomain as an example, the error could be formulated as follows:",
  "|| 0 ||22.(11)": "Parameter Space Matching Previous methods such as those de-scribed in aimed to match the distribution of embeddingbetween S and T using maximum mean discrepancy (MMD). However, these methods use random model parameters and thus did not utilize the rich information of training dy-namics contained in the space of trained parameters and .By matching these spaces, these parameters could generate similarembeddings, and thus the training direction of could be betterguided. To match these spaces, we first compute the embedding ofS in these spaces",
  "EXPERIMENT5.1Datasets": "In line with previous research , which utilizes smallbenchmark datasets for image classification dataset condensationtask, we include five public real-world benchmark datasets as shownin . (1) HAR : The Human Activity Recognition (HAR)dataset comprises recordings of 30 individuals who volunteeredfor a health study and engaged in six different daily activities. (2)Electric : The ElectricDevice dataset comprises measurementsobtained from 251 households, with a sampling frequency of twominutes. (3) Insect : The InsectSound dataset comprises 50,000time series instances, with each instance generated by a singlespecies of fly. (4) Sleep : The Sleep Stage Classification datasetcomprises recordings of 20 people throughout a whole night witha sampling rate of 100 Hz. (5) FD : The Fault Diagnosis (FD)dataset contains sensor data from a bearing machine under fourdifferent conditions, and we chose the first one.",
  "Experiment Setting": "Evaluation protocol: We follow the evaluation protocol of pre-vious studies . Concretely, it contains three stages sequen-tially: (1) train a synthetic data S. (2) train a randomly initializednetwork of the same structure on the trained synthetic data S us-ing the same optimization setting ( with 1e-3 learning rate and300 training epochs). (3) Evaluate the trained network on the sametest data with the same evaluation setting. In all experiments, thisprocess would be repeated five times to reduce the randomnessof the results. We report the mean and the standard deviation ofthe results. In line with prior research that utilize simple deep models to verify the effectiveness ofthe proposed method, we train the synthetic data based on a widerange of basic models. The details of the network architecture andthe parameter searching are in the Appendix B. In summary, webelieve the comparison between our proposed method and baselinemethods is fair.Hyper-parameters: There are only a small number of hyper-parameters associated with CondTSC. For simplicity, we use thesame set of hyper-parameters for all datasets. For the training pro-cess in Train Module, we use a learnable learning rate of 1e-3 following previous settings . Moreover, we set = 10and = 1000 for the number of the training iterations for S and Trespectively. For the loss in Dual Matching Module, we set = 1.0.For the update of the synthetic data S, we use an SGD optimizerwith a learning rate ofS = 1.0 and update epochs = 2000. Thewhole experiment is implemented by Pytorch 1.10.0 on RTX3090.The code and data are available in",
  "Coreset selection methods: We randomly select data samples(Random), choose the K-means clustering centroids (K-means),add samples of each class to the coreset greedily (Herding)": "Data condensation methods: Since there is no method designedfor time series data condensation, we select several data conden-sation methods for image data. DD maximizes the perfor-mance of the network trained by the synthetic data. DC matches the one-step training gradient of the synthetic data andthe real data. DSA implements siamese augmentation basedon DC. DM matches the embedding distribution of the syn-thetic data and the real data. MTT matches the multi-steptraining gradient. IDC uses efficient parameterization to im-prove the space utilization. HaBa decomposes the syntheticdata into bases and hallucinators. Overall Performance shows the accuracy of baselinesand CondTSC. Ratio (%) indicates the condensed ratio which is(S)/(T) and indicates the number of samples per classof the synthetic data. Full indicates the performance of the networktrained by full data. We report the mean and the standard deviationof the accuracy of five experiments with different random seeds.From , the following observations could be drawn: (1)CondTSC achieves superior performance compared to the baselines.The accuracy gap between the condensed data and full data iscomparable to the data condensation task in computer vision , and the condensed data is enough for downstream tasks such asNeural Architecture Search, which we will introduce later. (2) Theresult of CondTSC is significant in all settings, and CondTSC couldapproach the performance of full data by using only 1% of the sizeof the real data. (3) Coreset selection methods achieve stable butmediocre performance. This indicates that directly selecting somesamples of the real data gives a base but not global informationabout the real data. (4) The performance of the data condensationmethods is not as satisfactory as the performance of the imagedataset condensation task. This highlights the significant disparitybetween the task of condensing time series data and image data.Furthermore, we evaluate the performance of CondTSC and core-set selection methods with larger condensed data sizes as shownin . We can observe that increasing the data size makes themethods approach the upper bound and CondTSC has the highestperformance. The users could make a trade-off between the accu-racy and the training cost according to their downstream tasks.",
  "Downstream Task: NAS": "The condensed dataset could be used as a proxy dataset for NeuralArchitecture Search (NAS) . Following the setting of previousresearch , we implement NAS to search the best architectureof 324 Convolutional neural networks on the HAR dataset. Thespace is Depth {2, 3, 4}, Width { 32, 64, 128 }, Normalization{ None, BatchNorm, InstanceNorm, LayerNorm }, Activation {Sigmoid, ReLU, LeakyReLU }, Pooling { None, Max, Mean }. We use10 samples/class condensed datasets generated by each method.The result is shown in . We report 1) the test performanceof the model of the best-selected architecture. 2) Spearmans rankcorrelation coefficient between the validation accuracy obtainedfrom the condensed dataset and the full dataset. We select the top100 accuracies to filter the outliers following previous research . 3) The training time on an RTX3090 GPU and the numberof training samples. We can observe that CondTSC finds the bestarchitecture (92.71%) and performance correlation to the full dataset(0.665) while decreasing the time cost (from 667 to 24 minutes) andtraining samples (7352 samples to 60 samples). The condensing costis one time off and negligible when training thousands of possiblearchitectures in NAS. This result indicates that the condensed data",
  "Ablation Study": "In this part, we evaluate the performance of each module of CondTSCto verify the effectiveness of the proposed module. We denote theBase framework as the simple multi-step gradient matching frame-work that directly matches the multi-step training gradientof the synthetic data S and the real data T. Then we add theMulti-view Data Augmentation module (A), Dual Domain Train-ing module (T), and Dual Objectives Matching module(M) to the",
  "Raw data distribution matchedRaw data distribution not matched": ": the frequency domain and time domain visualization on Insect dataset of the learning process of synthetic data trainedby CondTSC and MTT separately. We could observe that the synthetic data trained by CondTSC conforms to the distribution ofthe real data and consequently achieves remarkable performance. : Accuracy(%) of CondTSC on the HAR dataset with = 5 initialized by different methods. Initial means theperformance of the initial dataset and Final means the per-formance of dataset optimized by CondTSC.",
  "Initial68.731.5473.562.4672.191.8373.621.8172.433.0964.471.8360.611.66Final79.892.9482.431.8682.473.3981.901.9581.742.7780.222.7477.132.84Improvement16.23%12.05%14.24%11.24%12.85%24.42%25.60%": "Base framework sequentially and report their mean and standarddeviation of accuracy on five experiments with random seeds.The results are shown in . We can observe that by sequen-tially adding the modules, the performance is getting better. Byadding the Multi-view Data Augmentation module (Base+A), thesynthetic data is extended to several extra spaces to match with thereal data, which leads to a considerable performance improvement.By adding the Dual Domain Training module (Base+A+T), the in-formation contained in the frequency domain is introduced into theframework, which leads to a big performance improvement. Theimprovement is significant in the dataset that is easy to classify inthe frequency domain such as Insect and FD. By adding the DualObjectives Matching module (Base+A+T+M), the matching betweensynthetic data and real data is strengthened by the parameter spacematching loss, which leads to a decent performance improvement.In summary, each module proposed in this paper is effective andcontributes to the final performance of CondTSC.",
  "In this section, we will investigate why CondTSC outperformsother baselines in the time series dataset condensation task. Werandomly select some data samples of the Insect dataset as the": "initial condensed synthetic dataset S and use CondTSC and MTT(strongest baseline) to train S separately. Then, we visualize thetraining process of a random sample in S in both the frequencydomain and the time domain, which is shown in .From , we could observe that: (1) In the frequency domain,the synthetic data trained by CondTSC exhibits a tendency to con-form to the distribution of the original data (as shown in the dashedrectangular boxes in the figure). The synthetic data (red solid line)are to fit the mean of the real data (black dashed line) and conformto the standard deviation (green vertical segment) simultaneously.This conformity is subsequently manifested in the time domain.We can see significant changes in the data of the time domainsince signals of various frequencies are modified. (2) However, thesynthetic data trained by MTT does not vary much, especially inthe frequency domain. Moreover, we could observe that the initialhigh-frequency signals, which deviate from the underlying dis-tribution of the original data, are not modified and incorporatedinto the final synthetic data. (3) This significant difference in thelearned dataset leads to different results, while CondTSC achievesremarkable performance.",
  "Different Initialization": "In this section, we would like to investigate the influence of differentinitializations of CondTSC. We try different initializations of differ-ent metrics including random selection, K-means, Kernel K-means,and Agglomerative Clustering. We show the performance of theinitial condensed dataset and the final condensed dataset of 5 runsin , from which the following observations could be made. (1)The performance of different initialized condensed datasets is im-proved. This indicates that CondTSC is effective and could improvethe performance of various initializations. (2) CondTSC utilizes the",
  "CONCLUSION": "In this work, we propose a novel framework that incorporates thetime domain and the frequency domain for the time series datasetcondensation task. The proposed framework CondTSC has threenovel modules, and each module is designed with different purposesand contributes to the final performance. Extensive experimentsverify the effectiveness of our proposed framework. In our futurework, we will aim to further analyze condensed data and improvethe performance of CondTSC. We wish to express our sincere appreciation to the researchersof for their exceptional efforts in creating, curating, and main-taining this invaluable collection of time series datasets. This workwas sponsored by National Key Research and Development Pro-gram of China under Grant No.2022YFB3904204, National NaturalScience Foundation of China under Grant No. 62102246, 62272301,62176243, and Provincial Key Research and Development Programof Zhejiang under Grant No. 2021C01034. Part of the work wasdone when the students were doing internships at Yunqi Academyof Engineering. KwangHoon An, Myung Jong Kim, Kristin Teplansky, Jordan R Green, Thomas FCampbell, Yana Yunusova, Daragh Heitzman, and Jun Wang. 2018. AutomaticEarly Detection of Amyotrophic Lateral Sclerosis from Intelligible Speech UsingConvolutional Neural Networks.. In Interspeech. 19131917.",
  "Zhicheng Cui, Wenlin Chen, and Yixin Chen. 2016. Multi-scale convolutionalneural networks for time series classification. arXiv preprint arXiv:1603.06995(2016)": "Hoang Anh Dau, Anthony Bagnall, Kaveh Kamgar, Chin-Chia Michael Yeh, YanZhu, Shaghayegh Gharghabi, Chotirat Ann Ratanamahatana, and Eamonn Keogh.2019. The UCR time series archive. IEEE/CAA Journal of Automatica Sinica 6, 6(2019), 12931305. Hoang Anh Dau, Eamonn Keogh, Kaveh Kamgar, Chin-Chia Michael Yeh, YanZhu, Shaghayegh Gharghabi, Chotirat Ann Ratanamahatana, Yanping Chen,Bing Hu, Nurjahan Begum, Anthony Bagnall, Abdullah Mueen, Gustavo Batista,and Hexagon-ML. 2018. The UCR Time Series Classification Archive.~eamonn/time_series_data_2018/.",
  "Wei Jin, Lingxiao Zhao, Shichang Zhang, Yozen Liu, Jiliang Tang, and NeilShah. 2021. Graph condensation for graph neural networks. arXiv preprintarXiv:2110.07580 (2021)": "Jang-Hyun Kim, Jinuk Kim, Seong Joon Oh, Sangdoo Yun, Hwanjun Song, Joon-hyun Jeong, Jung-Woo Ha, and Hyun Oh Song. 2022. Dataset condensation viaefficient synthetic-data parameterization. In International Conference on MachineLearning. PMLR, 1110211118. Mahinda Mailagaha Kumbure, Christoph Lohrmann, Pasi Luukka, and Jari Porras.2022. Machine learning techniques and data for stock market forecasting: Aliterature review. Expert Systems with Applications 197 (2022), 116659.",
  "Noel Loo, Ramin Hasani, Mathias Lechner, and Daniela Rus. 2023. DatasetDistillation with Convexified Implicit Gradients. arXiv preprint arXiv:2302.06755(2023)": "Alice Marascu, Pascal Pompey, Eric Bouillet, Michael Wurst, Olivier Verscheure,Martin Grund, and Philippe Cudre-Mauroux. 2014. TRISTAN: Real-time ana-lytics on massive time series using sparse dictionary compression. In 2014 IEEEInternational Conference on Big Data (Big Data). IEEE, 291300. Hussein Sh Mogahed and Alexey G Yakunin. 2018. Development of a losslessdata compression algorithm for multichannel environmental monitoring systems.In 2018 XIV International Scientific-Technical Conference on Actual Problems ofElectronics Instrument Engineering (APEIE). IEEE, 483486.",
  "Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. 2017. Instance Normal-ization: The Missing Ingredient for Fast Stylization. arXiv:1607.08022 [cs.CV]": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin. 2017. Attention is allyou need. Advances in neural information processing systems 30 (2017). Kai Wang, Bo Zhao, Xiangyu Peng, Zheng Zhu, Shuo Yang, Shuo Wang, GuanHuang, Hakan Bilen, Xinchao Wang, and Yang You. 2022. Cafe: Learning tocondense dataset by aligning features. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition. 1219612205.",
  "ASURROGATE OBJECTIVE MATCHING": "In this section, we introduce the details of objective matching. Tolearn the condensed synthetic dataset S, we would match somesurrogate objectives of S and T. The aim is to ensure that thesynthetic dataset S accurately captures the dynamics present in thereal dataset T. Consequently, a network trained using the syntheticdataset can exhibit comparable performance to a network trainedusing the complete real dataset. Here, we introduce two types ofsurrogate objectives used in this paper.",
  "A.1Gradient Matching": "The training gradient for a given initial model trained by S and Tis a good surrogate objective. By matching the training gradientsof S and T, we can effectively align the training dynamics of thesynthetic dataset with that of the real dataset . Thesingle-step gradient matching objective is shown as follows.",
  "= arg minSE0 ( ) [D(0(0 (S)), 0(0 (T)))](18)": "Here D(, ) is the distance function such as cosine distance and ()is the distribution of the network parameter. 0(0 (S)) is thegradient of the training loss (0 (S)) of network parameterizedby 0 on dataset S. This equation aims to find the synthetic data Sthat has the most similar gradient to real data T with a given initialnetwork 0. In practice, the synthetic data S would be updated bymany networks 0 to ensure the ability of generalization.",
  "A.2Distribution Matching": "Another surrogate objective is the data distribution generated by agiven model based on S and T. Its essential aim is that the networktrained by S could generate similar embeddings to those from thenetwork trained by T. Consequently, the data distributions of Sand T are similar, which makes S a condensed dataset from T.The core idea of distribution matching is reducing the empiricalestimate of the Maximum Mean Discrepancy , which is shownas follows.",
  "=10 ()||2] (19)": "Here () is the distribution of the network parameter and 0 () isthe network parameterized by 0 which outputs the embeddingof the input. In practice, the calculation of Eq. 19 is usually betweena batch of S and a batch of T. This equation is essentially findingthe synthetic data S that has the most similar distribution to thefull data T in the network embedding space .By matching the surrogate objectives, the models trained by thecondensed synthetic data could be similar to the models trained",
  "Dataset Condensation for Time Series Classification via Dual Domain MatchingKDD 24, August 2529, 2024, Barcelona, Spain": ": Cross-Architecture accuracy(%) performance. We train the synthetic data in one network structure and evaluate thesynthetic data in another network structure. The experiment is conducted in the HAR dataset with = 5. DD is not selectedas one of the baselines due to its bad performance. The best result is highlighted in bold and grey in each column.",
  "BEXPERIMENT DETAILS AND EFFICIENCY": "Network Structure: In line with prior research that utilize simple deep neural networks to verifythe effectiveness of the proposed method, we train the syntheticdata based on a wide range of basic network architectures, includingmulti-layer perception (MLP), convolution neural network (CNN),ResNet , temporal convolutional network (TCN) , LSTM ,Transformer , gate recurrent unit (GRU) . For MLP, we use a3-layer MLP with a hidden size of 128 and a ReLU activationfunction. For CNN, we use a 3-layer CNN with hidden channels of128, kernel size of 3x1, maxpooling function, and ReLU activationfunction. Specifically, following the previous studies , we denoteCNN with batch normalization and instance normalization as CNNBN and CNNIN respectively. For ResNet, we use the default network structure of Resnet18. For TCN, we use a 1-layer TCNwith a kernel size of 64x1. For LSTM, we use a 1-layer LSTM with ahidden size of 100. For Transformer, we use a 1-layer Transformerwith a hidden size of 64. For GRU, we use a 1-layer GRU with ahidden size of 100. These networks except ResNet have a similarsize of model parameters, which is approximately 30,000.Hyper-parameter Searching of Baselines: We use the follow-ing hyper-parameter searching strategy to fairly evaluate baselinemethods. It is hard to explore the same grid search space due tothe time limit (one baseline might take dozens of days to search).Consequently, for each method, we first search for a good range foreach parameter in the same log space while fixing other parameters.Then we use the searched range to do a grid search as follows. wechoose the epoch with the highest validation accuracy for testing.Consequently, we believe the comparisons are fair. DD: Training epochs is 100 and evaluate every 2 epochs. Thelearning rates of the synthetic data are searched in [103, 102,",
  "IDC: Training epochs is 100 and evaluate every 2 epochs. Thematching surrogate objective is set to feature matching. Themulti-formation factor is searched in , and the learningrate is searched in": "HaBa: Training epochs is 2000 and evaluate every 100 epochs.The learning rates of features and styles are searched in . The learning rate of is searchedin .All of the hyper-parameters that are not mentioned above arekept the same as in the original paper. Moreover, we show thetraining curve of different methods with respect to training epochsand time in . The experiment is conducted on the HAR datasetwith = 5 and the mean and standard deviation of validationaccuracy of 5 runs are shown. We can observe that it only costs 10minutes to train CondTSC to converge. Moreover, We can observethat CondTSC achieves the best training accuracy no matter withthe same training epoch or the same training time. Furthermore, thetraining curve indicates all of the baseline methods are convergedand we choose the epoch with the highest validation accuracy fortesting. Consequently, we believe the comparisons are fair.",
  "CCROSS ARCHITECTURE PERFORMANCE": "In the dataset condensation task, the ability of cross-architecturegeneralization is one of the important indicators for evaluatingmethods . This is because when training the synthetic data S,we usually utilize only one network architecture ,i.e., () is from a parameter space of single network structure . Consequently, the learned synthetic data performs well when training the network but might perform satisfactorily when train-ing the network with another structure . To verify the ability tocross-architecture generalization of the learned synthetic data ofCondTSC, we conduct extensive experiments on the HAR datasetwith = 5. We train the synthetic data S with one network struc-ture and evaluate S with another network structure. The result isshown in .From , we could observe that: (1) Overall, CondTSC per-forms best in the cross-architecture experiment. It achieves stableand outstanding performance, especially in the CNN-based andTransformer-based models. This indicates that CondTSC learns arobust and easy-to-generalize synthetic dataset and is resistant tooverfitting one network architecture. (2) In some simple networkarchitectures such as MLP and RNN, the single-step gradient match-ing methods and distribution matching methods, i.e., DC, DSA, andDM perform relatively well due to their simple design. However,their performance is unstable, which indicates that they could notlearn a robust synthetic dataset and lack the ability to generalize toother network architectures.",
  "DPARAMETER SENSITIVITY": "In this section, we would like to show the performance of CondTSCon different hyper-parameters, which throws light on the analysis ofCondTSC. We evaluate the performance of different values of hyper-parameters on all five datasets, and the results are shown in .(1) We evaluate the performance of different loss scaler , whichbalances the gradient matching loss L and the embeddingmatching loss L. We test a range of log-scale values and observesetting = 1 is best for all datasets. (2) We evaluate the performanceof different learning rates for the synthetic data S. We also testa range of log-scale values for S. In previous studies on imagedataset condensation , the value of S are usually setto a large value, i.e., S 13. However, we could observe thatsetting S = 1 is best for our time series dataset condensation task.This demonstrates that the time series dataset condensation taskhas essential differences from other dataset condensation tasks, anddesigning special techniques for time series dataset condensationis necessary. (3) We also evaluate the performance of differentvalues of and , which is the number of training iterations inthe Dual Domain Training module for the synthetic data S and thereal data T respectively. From our analysis in the Method section,we should set to avoid overfitting to the real data. Here,the experiment results show that the performance is relativelygood when 10 and 1000 and the performance could beunsatisfactory when . This further validates our analysis.Overall, we conducted extensive experiments on evaluating theperformance of different values of hyper-parameters. We find thatsetting the loss scaler = 1, the learning rate of synthetic dataS = 1, and training iterations is a good hyper-parametersetting.",
  "# of Train Iteration # of Train Iteration": ": The parameter sensitivity analysis. Different values of hyper-parameters are evaluated on all five datasets. We evaluatefour hyper-parameters, including loss scaler , the learning rate of the synthetic data S, the number of train iterations for thesynthetic data , the number of train iterations for the real data . The yellow star indicates the hyper-parameter value withthe highest test accuracy."
}