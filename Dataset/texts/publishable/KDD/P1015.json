{
  "ABSTRACT": "It has been shown that the effectiveness of graph convolutionalnetwork (GCN) for recommendation is attributed to the spectralgraph filtering. Most GCN-based methods consist of a graph filteror followed by a low-rank mapping optimized based on supervisedtraining. However, we show two limitations suppressing the powerof graph filtering: (1) Lack of generality. Due to the varied noisedistribution, graph filters fail to denoise sparse data where noise isscattered across all frequencies, while supervised training resultsin worse performance on dense data where noise is concentratedin middle frequencies that can be removed by graph filters withouttraining. (2) Lack of expressive power. We theoretically show thatlinear GCN (LGCN) that is effective on collaborative filtering (CF)cannot generate arbitrary embeddings, implying the possibility thatoptimal data representation might be unreachable.To tackle the first limitation, we show close relation betweennoise distribution and the sharpness of spectrum where a sharperspectral distribution is more desirable causing data noise to beseparable from important features without training. Based on thisobservation, we propose a generalized graph normalization (G2N)with hyperparameters adjusting the sharpness of spectral distri-bution in order to redistribute data noise to assure that it can beremoved by graph filtering without training. As for the secondlimitation, we propose an individualized graph filter (IGF) adapt-ing to the different confidence levels of the user preference thatinteractions can reflect, which is proved to be able to generate ar-bitrary embeddings. By simplifying LGCN, we further propose asimplified graph filtering for CF (SGFCF)1 which only requires thetop- singular values for recommendation. Finally, experimentalresults on four datasets with different density settings demonstratethe effectiveness and efficiency of our proposed methods. Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior specific permission and/or afee. Request permissions from acronym XX, June 0305, 2018, Woodstock, NY 2018 Association for Computing Machinery.ACM ISBN 978-1-4503-XXXX-X/18/06...$15.00",
  "INTRODUCTION": "Personalized recommendations have been widely applied to e-commerce, social media platforms, online video sites, etc., and hasbeen indispensable to enrich peoples daily life by offering the itemsuser might be interested in based on the data such as user-iteminteractions, reviews, social relations, temporal information, etc.Among various recommendation scenarios, we focus on collabora-tive filtering (CF), a fundamental task for recommender systems.Conventional CF methods such as matrix factorization (MF) characterizes users and items as low dimensional vectors and pre-dict the rating via the inner product between the correspondingembedding vectors. Subsequent works replace the linear design ofMF with other advanced algorithms such as neural networks ,attention mechanisms , transformer , diffusion mod-els , etc. to model complex user-item relations.While the aforementioned advanced recommendation algorithmsshow superior non-linear ability to model user-item relations, theirperformance are unstable due to the data sparsity issue in rec-ommendation datasets. Graph Convolutional Networks (GCNs)recently have shown great potential in recommender systems dueto the ability of capturing higher-order neighbor signals that canaugment the training data to alleviate the sparsity issue. Early GCN-based methods adapt classic GCNs such as vanilla GCN andGraphSage to recommendation . Subsequent worksempower GCN by incorporating other advanced algorithms such ascontrastive learning , learning in hyperbolic space , dis-entangled representation learning , etc., slimming GCN modelarchitectures to improve efficiency and scalability , andstudy the effectiveness of GCN .",
  "Conference acronym XX, June 0305, 2018, Woodstock, NYShaowen Peng, Xin Liu, Kazunari Sugiyama, and Tsunenori Mine": "Recent studies have shown that the effectiveness of GCN forrecommendation is mainly attributed to the spectral graph filter-ing which emphasizes important features (i.e., low frequencies)and filters out useless information. Most existing graph filteringdesigns can be classified to two categories: (1) Repeatedly propagat-ing the node embeddings across the graph where the embeddingsare optimized based on supervisory signals. This type of methodsis actually equivalent to a low pass filter followed by a low-rankmapping (see Equation (3)). (2) A simple graph filterwithout model training . This type of methods only relies onthe graph filters to denoise. We can see these two kinds of methodsare actually contradictory: the type (2) methods implies that datanoise is only distributed in certain frequencies that can be simplyremoved by graph filters without training, which is contrary tothe type (1), that we need model training to further remove datanoise. However, we show that neither of them are perfect by point-ing out two limitation suppressing the power of graph filtering.Firstly, both of the two designs lack generality. We find that noisedistribution varies on datasets with different densities. Particularly,graph filters show poor performance on sparse datasets where datanoise is scattered across all frequencies as they denoise by maskingcertain frequencies while fail to remove intrinsic noise in the fea-tures. On the other hand, despite the superior ability of supervisedtraining learning from data polluted by noise, it results in worseperformance on dense datasets on which the noise is concentratedin middle frequencies that can be simply removed by graph fil-ters. Moreover, most effective GCN-based methods are basicallylinear GCNs (LGCNs) without non-linearity. We theoretically showthat they are incapable of generating arbitrary embeddings withmulti-dimensions, implying the possibility that they cannot gener-ate desirable user/item representations and demonstrating the lackof expressive power.To tackle the first limitation, we further show the close relationbetween noise distribution and the sharpness of spectral distribu-tion, that a sharper distribution is more desirable on which noiseand important features are separable by graph filtering withoutsupervised training. Based on this observation, we propose a gen-eralized graph normalization (G2N) to adjust the sharpness of spec-trum via hyperparameters. As a result, data noise is redistributedthrough G2N, making the graph filtering generalizable on datasetswith different densities. To tackle the second limitation, we proposean individualized graph filter (IGF) which is proved to generatearbitrary embeddings. Specifically, considering interactions do notequally reflect user preference (i.e., more (less) similar of the inter-acted items implies a higher (lower) consistency between the usersfuture and past behaviour), the proposed IGF emphasizes differentfrequencies based on the distinct confidence levels of user prefer-ence that interactions can reflect. Finally, by simplifying LGCN,we propose a simplified graph filtering for CF (SGFCF) only re-quiring the top- singular values. Our main contributions can besummarized as follows: : We point out two limitations suppressing the power of graphfiltering for recommendation: (1) The lack of generality due to thethe performance inconsistency of graph filters and supervisedtraining on data with different densities and (2) The lack ofexpressive power of LGCN that is effective for CF. We propose a generalized graph normalization to tackle thefirst limitation, which redistributes data noise by adjusting thesharpness of spectrum, enabling the graph filtering to denoisewithout training on datasets with different densities. We propose an individualized graph filtering to adapt to distinctconfidence levels of user preference that interactions can reflect.It is proved to generate arbitrary data representations thus solvesthe second limitation.",
  "PRELIMINARIES": "We first introduce a commonly used GCN-learning paradigm forCF. Given an interaction matrix with implicit feedbacks containing|U| users and |I| items: R {0, 1}|U||I|, we can define a bipar-tite graph G = (V, E), where the node set contains all users anditems: V = U + I, the edge set contains the user-item pairs withinteractions: E = R+ where R+ = { = 1| U, I}. For sim-plicity, we let |U| + |I| = . Then, we can define the correspondingadjacency matrix A of G and formulate the updating rules as:",
  "RD- 1": "2is a normalized interaction matrix, D andD are matrices with diagonal elements representing user and itemdegrees, respectively. The initial state is the stacked user/item em-beddings H(0) = E R, where each user and item arerepresented as learnable low-dimensional vectors. W(+1) R is a linear transformation. It has been shown that the followinglinear GCN (LGCN) removing the activation function and lineartransformation is effective for CF , with the final representa-tions O generated as:",
  "SGF0.08160.10080.11060.0629LGCN, = 640.07070.09120.12040.1317LGCN, = 1280.07060.09240.12180.1338LGCN, = 2560.06990.09280.12400.1341": "According to Definition 1, low (high) frequencies emphasizethe similarity (dissimilarity) between nodes and their neighbor-hood. We can adjust the weight of different frequencies via ()as ( A) = ()vv . It has been shown that low frequen-cies are significantly contributive to recommendation , andmost GCN-based methods can be classified to two categories: (1)a low pass filter followed by a linear mapping and optimization(i.e., LGCN) and (2) a simple graph filter without training . Thelearning process can be formulated as follows:",
  "GSpectrum V R ( ) V() RE O R,(6)": "where V() is the top- low frequency components. Here, we raisetwo questions on existing works: (1) The underlying assumption ofthe second type of methods is that data noise is only concentratedin certain frequencies that does not pollute important features(i.e., V()), which is contrary to the first type of methods thatfurther training is required to denoise V(). Since the two typesof methods seem to be contradictory, are they general on differentdatasets? (2) Despite the effectiveness and simplicity of LGCN, italso has a weakened expressive power compared with vanilla GCN,is it capable of generating desirable data representations? We willanswer the two questions in .",
  "ANALYSIS ON GRAPH FILTERING": "In this section, we evaluate existing graph filtering designs in termsof generality and expressive power. In the light of the effectivenessof LGCN for CF, our analysis is mainly based on LGCN and a simplegraph filter (SGF) ( A). The difference between the two models liesin the necessity of model training.",
  "Generality": "3.1.1Performance Inconsistency under Different Densities. We firstevaluate graph filtering on datasets with different densities. Wechange the data density by adjusting the training ratio % (theremaining is used as the test set), where = {80, 60, 40, 20}%. Wechoose an extensively used setting for CF with 0 =, , = where ( A) is a low pass filter. We compare LGCN and SGF ontwo datasets and report the results in . We observe thatthe performance of SGF and LGCN consistently decrease and in-crease as the training data is sparser, respectively. Particularly, SGFtends to be effective on the dense data (e.g., = 80% and 60%)and significantly outperforms LGCN, indicating the uselessness of 0.04 0.08 0.12 0.16 0.2 0.24 Recall@20 Top K% SGF LGCN",
  ": The accuracy (Recall@20) of SGF and LGCN whenonly considering top-% low frequencies": "model training. On the other hand, the positive effect of trainingcan be identified on the sparse data (e.g., = 20% and 40%) as LGCNshows better performance. In addition, despite the improvementbrought by increasing the embedding size, the performance tendsto converge and still underperforms SGF on the dense data. 3.1.2Noise Distribution Varies on Densities. The above observa-tions show the inconsistency and lack of generality of LGCN andSGF. Since SGF is a low pass filter, the poor performance on sparsedata leads to a reasonable assumption that low frequencies might bepolluted by noise. To verify this assumption, we conduct frequencyanalysis with the following filter:",
  "A= V()V() ,(7)": "We investigate the noisiness of certain frequencies by increasing from 0 and observing how accuracy changes after introducingcertain frequencies. As the original polynomial filter empha-sizes more on the lower frequencies leading to biased results, wechoose a uniform filter here. We focus on = 80% and = 20% asSGF (LGCN) is most (least) and least (most) effective, respectively.We observe the followings from the results shown in : On = 80% ((a) and (c)), the accuracy of SGF increases thensignificantly drops and rises again as increases, and the bestperformance outperforms LGCN, showing that the noise is con-centrated in middle frequencies that can be removed by SGF.",
  "The superior performance when only incorporating low frequen-cies shows that important graph features are distributed in lowfrequencies on both settings": "The above observations verify our assumption that the poor perfor-mance of SGF on sparse data is attributed to the noise distributionthat is across all frequencies. More importantly, both graph filtersand supervised training lack generality due to their inconsistentperformance on data with different densities, thus a more genericdesign is required. In addition, we notice that the performance ofSGF is highly symmetric with respect to the middle frequency (i.e.,/2 = 0) which is due to the following theorem and corollary:",
  "where B R is a full-rank matrix with B = , R is aparameter matrix. Equation (10) degenerates to LGCN when = 1": "Theorem 2 shows that a shared global filter cannotgenerate arbitrary multidimensional embeddings, and it is apparentone-dimensional representation is not enough to characterize usersand items, which also demonstrates the incapability of LGCN togenerate the optimal representations and shows the poor expressivepower despite its effectiveness. Theorem 2 can be directly applied toSGF by setting = . Since the output has dimensions, it requires different filters to fit arbitrary embeddings.",
  "> ,(11)": "where 1 > > /2. Most of the energy is concentrated in thetop- low frequencies while the information distributed in themiddle frequencies is trivial and noisy to data representations. Inthis case, the important features and noise can be separated bygraph filtering without training. Inspired by this observation, wereport the eigenvalue distribution with different density settings in (a). We observe that the denser data on which the noiseand important features tend to be separable has a sharper spectrum.Particularly, in (b), we can see that the eigenvalue closer tothe middle frequency tends to drop faster than the low frequency.This means that the energy of middle frequencies is close to 0 whilelow frequencies stay important to data representation, causing theimportant features and noise to be distributed in different frequen-cies instead of mixed up together. Therefore, a sharper spectraldistribution is more desirable as it is closer to an ideal low passfilter. Furthermore, consider a rank- approximation consisting ofthe top- low frequencies: A = =1 vv , then we have:",
  "Generalized Graph Normalization (G2N)": "The analysis in .1 provides a solution to tackle the dilemmamentioned in .1, that graph filters and supervised train-ing perform inconsistently on data with different density settingswhich is attributed to the varied noise distribution. If we are ableto generate a desirable spectrum in a way that clearly differentiatesimportant features from noise, it becomes possible to depend exclu-sively on graph filters for recommendation. Since the spectrum isclosely related to how we normalize the graph, we study what graphnormalization leads to a desirable spectrum in this subsection.According to Definition 1, we know that putting more emphasison low frequencies leads to a higher similarity between nodes andneighborhood. Thus, it is reasonable to assume that more energy isconcentrated in the low frequencies if we increase the similarity bymodifying the graph normalization, leading to a sharper spectrum.Since () shares the same eigenspace with , we can simplydefine the similarity on ( A) = A:",
  "Definition 2. (Variation on the second-order Graph). The varia-tion of the eigenvectors on the second-order graph is defined as:v A2v = 1 2 .(14)": "Interpretation of Equation (13). Definition 2 measures thedifference between the signal samples of eigenvectors at each node(V) and at its second-order neighbors ( V). Intuitively, vwith | | 1 implies that the nodes are similar to their second-order neighborhood: |V V | 0, while the middle fre-quency with | | 0 emphasizes the difference between them. Consider as a band-pass filter, if the node similarity increases, thecomponents with | | 1 and | | 0 should be correspondinglyemphasized and suppressed to make the equation hold, respectively,leading to a sharper spectrum. In other words, the sharpness of thespectral distribution is closely related to the average node similaritydefined on the normalized adjacency matrix.Then, our question is transformed to: how do we increase thenode similarity through a new graph normalization? The originalsetting is defined as A =1 . Here, we define a renormal-",
  "|I|2,": "(15)where N/N is the set of first-hop neighborhood of /. We cansee that the user/item with a higher degree has a larger impacton the average similarity (proportional to 2/2 ), leading to theconclusion that the higher weights over the high-degree nodesresults in higher node similarity. Based on the original design, wecan propose two new designs with higher weights over high-degreenodes: (1) () =1+ , and (2) () = , and propose ageneralized graph normalization as follows:",
  "where and are the min and max node degree, respectively": "Particularly, increasing and shrinks and scales the eigenvalue,respectively, making the spectrum not normalized any more. As wefocus on the sharpness of the spectrum, we normalize and visu-alize / to observe how our proposed G2N adjusts the spectrumshown in . We observe that the eigenvalue closer to themiddle frequency drops more quickly, while the one closer to thelow frequency tends to remain unchanged, and such a trend is moreobvious as or increases. The results in indicate thatG2N can generate a desirable spectrum which is more equivalentto an ideal low pass filter assuring that data noise and importantfeatures are linearly separable without further training.",
  "where DG(, ) is the graph distance2, DG/ (, ) measures the dis-tance between and which does not pass through , and 1 is anindicator function producing 1 if two nodes are close enough (i.e., < )": "Intuitively, if a users interactions are similar (i.e., high homophilicratio), then it is possible his/her future behaviour is consistent withthe past interactions. While it is hard to rely on a users past in-teractions if they are quite different (low homophilic ratio). Giventhat different frequencies emphasize the similarity between nodesand neighborhood with different degrees, it is reasonable to im-plement the individualized filter based on the homophilic ratio.After evaluating multiple graph filters, we choose a monomial filter:() = , map the homophilic ratio to via a linear func-tion, where the min and max of the homophilic ratio is mapped to 1and 2, respectively. Then, the individualized filter is implementedas () = , where is determined by homo().",
  "Discussion": "Compared with existing GCN-based methods, our proposed SGFCFmainly differs from them in three aspects: (1) We provide a closed-form solution with complexity only as: O(R++2 |U| +2 |I|).(2) Our method is generalizable on datasets with different densi-ties. (3) Compared with the methods that can be summarized asLGCN, our method is proven to have stronger expressive powerwhich is capable of generating arbitrary embeddings. Particularly,compared with non-parametric methods such as GFCF , oursuperiority comes from two aspects: (1) GFCF is equivalent to a lowpass filter which does not consider the varied noise distribution ondata with different densities, thus is not generalizable. (2) It can besummarized as a LGCN showing poor expressive power. We willempirically demonstrate our superiority in .",
  "EXPERIMENT5.1Experimental Setup": "5.1.1Datasets and Evaluation Metrics. We evaluate our methodon four datasets in this work, the statistics are summarized in. CiteULike3 is collected from a social bookmarking serviceCiteULike which allows users to bookmark and share researcharticles; Pinterest is constructed for evaluating content-basedimage recommendation; Yelp is from the Yelp Challenge data;Gowalla is a check-in dataset which records the locations usershave visited. We focus on = 80% and = 20%, randomly select 5%as validation set, and leave the remaining for test. We adopt Recalland nDCG , two widely used evaluation metrics for personalizedrecommendation. The recommendation list is generated by rankingunobserved items and truncating at position = 10. 5.1.2Implementation. We use stochastic gradient descent (SGD)as the optimizer for training-based models. The embedding size is set to 64, the regularization rate is set to 0.01 on all datasets,the learning rate is tuned with step size 0.1, the model parametersare initialized with Xavier initialization and the batch size isset to 256. 0 and [0.5, 0] are tuned with step size 1 and0.02, respectively. Other hyperparameters in this work are all tunedwith step size 0.1. We set = 2 which is the smallest graph distancebetween homogeneous nodes; we use a monomial filter:() = ,1 and 2 are tuned after determining the best . 5.1.3Baselines. We compare our proposed methods with com-petitive baselines which can be categorized to training-based andnon-parametric (i.e., training-free) methods. For training-basedmethods, we choose BPR and DirectAU implemented onMF, and seven GCN-based methods: LightGCN , SGL-ED ,LightGCL , XSimGCL , DCCF , GDE , and JGCF .Additionally, we adopt four non-parametric methods: EASE ,GF-CF , PGSP , and BPSM . Note that the hyperparam-eters are properly set after conducting tuning for them on thedatasets used in this work.",
  "Comparison": "5.2.1Accuracy. We report the accuracy of baselines and our methodin and 4. We have the following observations: Overall, GCN-based methods show better performance espe-cially on sparse data, indicating the superior ability to tackledata sparsity by incorporating higher-order neighborhood. Forinstance, LightGCN underperforms BPR on two datasets with = 80% while significantly outperforms BPR on four datasetswith sparse setting = 20%. Comparing non-parametric methods (i.e., Ease, GFCF, PGSPand BPSM) with training-based methods, we can see that non-parametric methods tend to be effective on dense data (e.g., = 80%) and show relatively poor performance on sparse data(e.g., = 20%). For instance, GFCF achieves the best baselineon = 80% while shows the second worst performance on = 20% on CiteULike. The relatively poor performance of GFCF",
  "on sparse data also verifies our previous analysis in .1that a simple low pass filter cannot work well on datasets withdifferent densities due to the varied noise distribution": "Contrary to non-parametric methods, GCN-based methods re-quiring training such as LightGCN, GDE, and JGCF show supe-rior performance on sparse data while are less effective on densedata, which further verifies the analysis in .1 showingthe performance inconsistency of graph filters and supervisedtraining on data with different densities. Our proposed SGFCF, significantly outperforms competitivebaselines almost across all datasets, demonstrating the effec-tiveness of our proposed method. Particularly, SGFCF outper-forms GFCF which shares similarities with our designs by 14.0%on = 80% and by 41.0% on = 20% on average, in termsof nDCG@10. The larger improvement on sparser data furtherproves the superiority of our proposed designs over GFCF.",
  "DirectAU3.33 103s7.92 102s4.53 103s3.30 104sLightGCN1.13 104s2.73 103s5.09 103s1.22 103sGFCF71.0s1.47 102s48.1s2.40 102sSGFCF6.3s2.4s12.3s5.5s": "5.2.2Efficiency. We report the training time of SGFCF and severalbaselines that have been shown efficient in , where the re-sults are obtained on a server equipped with AMD Ryzen 9 5950Xand GeForce RTX 3090. Despite the light architecture comparedwith other GCN-based methods, LightGCN still requires much moretraining time than DirectAU whose complexity is comparable to MF.However, it requires hundreds of training epochs before conver-gence for training-based methods due to the non-convex loss func-tions, causing them to be inefficient compared with GFCF whichdoes not require model training. Our proposed SGFCF achievesover 10x and 1000x speedup over GFCF and LightGCN, respectively.Particularly, the higher efficiency of SGFCF over GFCF is attrib-uted to G2N. By generating a desirable spectrum via G2N, graphinformation is concentrated in fewer low frequency components,reducing the number of required spectral features . For instance, = 100, 50, and 90 on CiteULike, Yelp, and Gowalla with = 20%when the accuracy is maximized, as opposed to = 512 on GFCF.",
  ": How performance changes with 1 and 2": "can adjust the sharpness of spectrum in .2, we separatelytune them and choose the better one, where we fix = 0 and =0.5 when studying the other one. We can observe that the accuracyis more sensitive to than . Note that + and = 0 whennodes are equally weighted, thus1+ is a smoother function than since [0, +] while [0.5, 0], explaining why overallperforms better than . Moreover, the hyperparameter value islarger on the dense setting = 80% than the sparse setting = 20%.A reasonable explanation is that nodes have smaller degrees onthe sparse setting on average, on which the model performance ismore sensitive to changes in hyperparameters. We can also see thedifference between the values on = 20% and = 80% when thebest performance is achieved is roughly consistent with their nodedegree difference (i.e., around 4 times). 5.3.2Effect of . We study the effect of and report the results in and . We can see that introducing leads to a betterperformance, demonstrating that middle frequency componentsstill contain some information contributing to the data representa-tions. Particularly, the improvement on the dense setting = 80%tends to be more significant than that on the sparse setting = 20%.Intuitively, the sparser data are composed of fewer spectral features.For instance, only = 20 spectral features are required on Yelpwith = 20% when the best performance is achieved, as opposedto = 300 with = 80%, and we observe a similar trend on otherdatasets. This observation implies that the middle frequencies aremore noisy and useless on sparse datasets. The results in",
  "further verifies our observation, that the accuracy is more sensitiveto the change in on sparse setting": "5.3.3Effect of IGF. We show how accuracy changes with 1 and2 in , where we fix 2 = 2.0 in (a) and 1 = 2.5 in (b).We can observe that an individualized filter adapting to differentusers/items shows better performance than a shared graph filter. 5.3.4Impact of . Intuitively, the homophilic ratio of distinctiveusers/items tends to shift towards 1 as increasing , making theirdifference more insignificant. Meanwhile, a larger also results inhigher computational complexity. As shown in , the bestperformance is achieved at = 2 on most settings except the slightimprovement on CiteULike with = 20%. Thus, we set = 2considering the trade-off between effectiveness and efficiency.",
  "RELATED WORK6.1Collaborative Filtering": "Collaborative filtering is a fundamental task for recommender sys-tems as it provides recommendations by learning from the user-item historical interactions without rely on specific knowledgeor user and item profiles. The underling assumption of CF is thatsimilar users tend to have similar preference . Matrix fac-torization , one of the simplest yet effective methods for CF,characterizes users and items as learnable low-dimensional vectors,where the rating between a user and an item is estimated as theinner product between user and item vectors. Most CF methods canbe considered as enhanced MF variants addressing drawbacks ofMF which can be mainly classified into three categories: (1) Due tothe limited available data, some works incorporate side informationto help infer user preference. Rendle et al. introduces temporalinformation and combine MF with Markov chain (MC) to predictusers next behaviour. Ma et al. integrates social relations anduser-item interactions with MF. is an enhanced MF to incor-porate geological information. (2) To address the drawback thatMF uses a simple linear function to model complex user-item rela-tions, much effort has been devoted to exploit advanced algorithmsto learn form user-item interactions, such as multilayer percep-tron , autoencoder , attention mechanism , trans-former , etc. (3) Due to the data sparsity, negative sampling iscritical to generate desirable data representations. Therefore, a lotof effective sampling strategies have been proposed .",
  "CiteULike0.15420.15470.1548Yelp0.09630.09600.0953Pinterest0.14360.14380.1439Gowalla0.19710.19510.1932": "PinSage is closely related to GraphSAGE . Subsequentworks show the redundancy of GCN-based methods such as non-linearity and linear transformation , demystify how GCNscontribute to recommendation and analyze the expressivepower of GCN for recommendation . Furthermore, researcheffort has also be devoted to empower GCN with other advancedalgorithms, such as transformer , sampling strategy , con-trastive learning , etc. and achieve further improvement.Spectral-bsed GCNs, focusing the spectral domain of graphs,have also received much attention . By analyzing GCN froma perspective of graph signal processing, recent works show thatGCN is essentially a low pass filter, and low/high frequencies aresignificantly contributive to recommendation accuracy .Based on this finding, several spectral GCN-based methods havebeen proposed and show superiority, that can be classified into twocategories: (1) non-parametric graph filters and (2) graphfilters combined with supervised training . However, weempirically demonstrated that they fail to perform well on datasetswith different densities due to the varied noise distribution.",
  "CONCLUSION": "In this work, we addressed two limitations of existing GCN-basedmethods: the lack of generality and expressive power. We proposeda generalized graph normalization (G2N) to adjust the sharpness ofspectrum, making graph filtering generalizable on datasets with dif-ferent densities, and an individualized graph filtering (IGF), wherewe emphasize different frequencies based on the homophilic ratiomeasuring the distinct confidence levels of user preference thatinteractions can reflect, which is proved to generate arbitrary em-beddings. Finally, we proposed a simplified graph filtering for CF(SGFCF) requiring only the top- singular values. Extensive ex-perimental results on four datasets demonstrated the effectivenessand efficiency of our proposed designs. In future work, we plan toanalyze the potential of GCNs from other perspectives and applyour proposed method to other recommendation tasks. AcknowledgementThis paper is based on results obtained from the project, Researchand Development Project of the Enhanced infrastructures for Post-5G Information and Communication Systems (JPNP20017), com-missioned by the New Energy and Industrial Technology Develop-ment Organization (NEDO).",
  "ASUPPLEMENTARY EXPERIMENTSA.1Ablation Study": "To demonstrate the effectiveness of our proposed designs, we com-pare three variants: (1) SGFCF, (2) SGFCF without IGF, and (3)SGFCF without both G2N and IGF, and report the results in Ta-ble 8. We observe that removing either of G2N and IGF resultsin performance degradation, showing that both G2N and IGF arecontributive to model performance. Particularly, IGF tends to bemore effective on the sparse setting, indicating that it might requirestronger expressive power to generate the optimal representationson the sparse data. Moreover, G2N contributes more to the accuracythan IGF, as the poor performance of graph filters is mainly due tothe varied noise distribution polluting the low frequencies beingimportant to the data representations.",
  "A.2Experiments on Other Density Settings": "To further verify the effectiveness of our methods, we compare ourSGFCF with several competitive baselines on x=40% and 60%, andreport the results in . We can observe that GFCF withouttraining shows better performance on the denser setting x=60%,while training-based methods JGCF and LightGCN achieve betterresults on the sparser setting x=40% as supervised training showssuperior ability learning from noisy data. Our proposed SGFCFoutperforms all baselines across the board, demonstrating the gen-erality of our proposed designs.",
  "A.3Graph Filter Designs": "We compare four commonly used graph filters for recommendation:monomial filter, exponential diffusion kernel, Markov diffusion ker-nel, and Jacobi polynomials (with detailed introduction as follows)and report the results in . Overall, the monomial filter: with the simplest design shows better performance than other fil-ters. Due to the important features that are concentrated in only afew low frequencies via G2N, a simple increasing function is alreadyable to appropriately emphasize different frequencies according totheir importance instead of complicated band-pass filters such asthe Markov diffusion kernel and Jacobi polynomials.",
  ",(28)": "where B R, B = . By extending the order of () to (i.e., = ), B becomes a full rank matrix as long as A has norepeated eigenvalue, thus we can always find a solution to satisfyEquation (28). To generalize the above result to the situation of > 1, multiple filters are required:"
}