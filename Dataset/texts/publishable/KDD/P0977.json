{
  "Abstract": "Large language models (LLMs) show amazing performance onmany domain-specific tasks after fine-tuning with some appropriatedata. However, many domain-specific data are privately distributedacross multiple owners. Thus, this dilemma raises the interest inhow to perform LLM fine-tuning in federated learning (FL). How-ever, confronted with limited computation and communication ca-pacities, FL clients struggle to fine-tune an LLM effectively. To thisend, we introduce FedBiOT, a resource-efficient LLM fine-tuningapproach to FL. Specifically, our method involves the server gen-erating a compressed LLM and aligning its performance with thefull model. Subsequently, the clients fine-tune a lightweight yetimportant part of the compressed model, referred to as an adapter.Notice that as the server has no access to the private data ownedby the clients, the data used for alignment by the server has a dif-ferent distribution from the one used for fine-tuning by clients.We formulate the problem into a bi-level optimization problem tominimize the negative effect of data discrepancy and derive theupdating rules for the server and clients. We conduct extensiveexperiments on LLaMA-2, empirically showing that the adapter hasexceptional performance when reintegrated into the global LLM.The results also indicate that the proposed FedBiOT significantlyreduces resource consumption compared to existing benchmarks,all while achieving comparable performance levels.",
  "Introduction": "The recent advancements in large language models (LLMs) havedemonstrated incredible performance in various tasks, such asquestion-answering and problem-solving. This success owes tothe pretraining on large datasets, covering a wide range of linguis-tic patterns and general knowledge. However, in specific domainssuch as legal advice and medical diagnosis , LLMsmay not provide professional responses because the terminology",
  "Work was done while Feijie Wu was an intern at Alibaba Group": "and context significantly differ from general language use. To ad-dress this limitation and enable the generation of domain-specificcontent, it becomes imperative to fine-tune LLMs with relevantdata. This fine-tuning process allows the models to learn from thespecific instances and nuances of the target application, ensuringtheir capability within specialized fields. The quality and quantityof the task-specific data are directly related to the performance ofthe fine-tuned model on downstream tasks: large and well-labeleddata can significantly improve the model, while small and irrele-vant data can only benefit the model marginally. However, there aremany cases where task-specific data are possessed by multiple dataparties, while each of them may have a limited number of samplesthat can be used to fine-tune LLMs. For example, a hospital in arural area may only have a limited number of lung cancer casesrecorded in its own system; if an LLM is only fine-tuned on oneset of those cases, it may not obtain comprehensive knowledge andeasily be overfitted.To incorporate all the distributed data in the fine-tuning of LLMs,one may consider the batch fine-tuning as follows. If we demandall the data owners to share their data with the LLM server, thenLLM fine-tuning could be conducted at the server side. For example,some LLM owners offer fine-tuning APIs as services, but the usersmust pack their data as files and upload them to use a black-boxfine-tuning . Apparently, this setup is not applicable to userswho have privacy concerns. Especially, some businesses are subjectto data privacy regulations , which makes it challenging toshare local data with LLM server.Therefore, a more practical setting is to let individual data own-ers keep their data locally, run fine-tuning locally and aggregatethe fine-tuning results at the LLM server. This fits well the feder-ated learning (FL) framework, which is a distributed paradigm thatplaces a paramount emphasis on preserving privacy. Its conven-tional algorithms, such as FedAvg , are considered practicalsolutions to overcome data barriers across different data owners.In this paradigm, data owners are treated as clients, and an LLMserver coordinates the computation. The standard FL workflowinvolves three steps repeatedly: (i) The server distributes the globalmodel to all clients; (ii) Each client trains the model locally formultiple iterations and sends the updated model to the server; (iii)The server aggregates the models from the clients and updates theglobal model accordingly. Despite the potential of this method tofacilitate collaborative fine-tuning of an LLM without sharing localdata, its feasibility is hindered by two main limitations:",
  "Feijie Wu, Zitao Li, Yaliang Li, Bolin Ding, and Jing Gao": "extracts some layers out of the total layers of transformers to forma submodel. We denoted this by a function LayerExtract(M, ),which means extracting the layers with indices in [] from themodel M. The function consists of the following three steps, andits pseudocode implementation of the first two steps is presentedin Algorithm 1. Step 1: Identify the adapters in the original model. We choosethe bottom few layers1 of the original LLM as the adapter. To bemore specific, suppose the size of the adapter is , and denote theadapter as A. Therefore, A LayerExtract(M, A) with thelayer indices A = { + 1, . . . ,}. We denote A as the param-eters of A. The A are also the trainable parameters that can befine-tuned by clients. The remaining part of the model is demotedas E = M \\ A.The choice of adapters brings two advantages. First, regardingthe computation constraints of the clients, this proposed adapter iscomputation-efficient because it only needs to store the activationsof transformers in the last few layers, leading to a lower memorycost. Second, as the adapter focuses more on domain-specific fea-tures, it is eco-friendly to spend the effort fine-tuning the last fewlayers. The conclusion is drawn from a well-known finding in neural networks that the first few layers tend to learn generalfeatures while the last layers encode specific ones. Step 2: Layer dropout to form emulator. Inspired by the ex-perimental results presented by Xiao et al. , we form an emu-lator by means of a uniform layer dropout from the remain-ing part E. Therefore, the emulator is a sub-model obtained asE LayerExtract(E, E). Denote there are E layers trans-former in E. The dropout rate of the emulator is denoted as = |E | E . For convenience, we call E as emulator and E as non-compressed emulator. Let E and E be the parameters of E andE, respectively.After training, we can attain two combined models, namely,Adaptor + Emulator (AdapEmu, i.e., E A), and Adaptor + Full(AdapFu, i.e., EA). As Xiao et al. describes, AdapFu performsbetter than AdapEmu. These two models have different function-alities in real-world scenarios: AdapEmu is adopted if the inputcontains sensitive information that cannot be shared with the LLMowner, e.g., drafting a petition letter, while AdapFu is adopted whenthe users aim to have better generation results, e.g., solving a mathproblem. Step 3: Pre-alignment. Before the FL training stage, we pre-align the emulator with the non-compressed one such that it canmimic the performance of the raw model. Assume there is a publicdataset D available on the server, which consists of a bunchof data (,), representing the input and the ground truth, respec-tively. Therefore, in the rest of the section, we assume the input contains an attention mask that can identify the ground truth.Instead of training the compressed model with the ground truth,we utilize knowledge distillation to transfer the general linguis-tic patterns from the original LLM to the compressed one by tuningthe emulator E. In specific, we ensure the emulator generates rep-resentations that have subtle differences from the non-compressed",
  "Contributions. Throughout the paper, our contributions arehighlighted as follows:": "We propose an algorithm FedBiOT that avoids full model fine-tuning and significantly reduces the communication and compu-tation overhead. To the best of our knowledge, this is the firstwork that addresses both the aforementioned two challenges inthe federated LLM fine-tuning framework. With our proposedframework, clients data are ensured to be kept locally and com-putation and communication burden is significantly reduced. We formulate a bi-level optimization problem that enables theLLM fine-tuning without access to the full model. By partitioningthe compressed model into the adapter and the emulator, theemulator acts as a simulator of the original raw model, while theadapter adeptly learns domain-specific linguistic patterns withclients local datasets. To this end, we realize that fine-tuningthe compressed model is equivalent to the refinement of thecounterpart of the complete LLM. We conduct extensive experiments on LLaMA-2 for fine-tuningwith three tasks, i.e., code generating, math problem solving,and question answering. The empirical studies also demonstratethat the proposed approach has significant improvement over allthese tasks compared with the baseline approaches in terms ofcomputation and communication overheads and final accuracy.",
  "(,)D (M(;);) ,(1)": "where M(;) is the output on a given model parameterized by and an input . The loss function is defined on the model outputand the ground truth . In this dataset, we assume that the groundtruth is part of the input , where a sequence of tokens in theinput is used to predict the next token, and the ground truth isused to identify the part needing to be predicted by the model.Such a dataset is commonly adopted in previous works to fine-tunean LLM . Then, based on the definition, a conventional FLsystem aims to find an optimal model across all clients, which is",
  "Step 3: The server collects the locally updated model parameters ( ) from clients and aggregates them into a single global model (+1) for next round": "Applying PEFT to federated LLM fine-tuning. The existingFL algorithms are confronted with computationand communication bottlenecks when fine-tuning an LLM. To miti-gate the limitations, researchers have extended existing parameter-efficient fine-tuning (PEFT) approaches to FL, named FedPEFT. These methods minimize the number of trainable pa-rameters by introducing a PEFT module and keeping the originalLLM parameters unchanged. By focusing local updates exclusivelyon the PEFT module rather than the entire model, these methodseffectively reduce computational load and support larger batch sizeson a single GPU. Additionally, the FL server merely aggregates theupdated parameters of a given model, thus obviating the need totransmit unchanged parameters and minimizing communicationoverheads.Nevertheless, FedPEFT is still confronted with the intrinsic chal-lenge wherein clients face obstacles in loading an LLM due to itssubstantial computation prerequisites. For instance, the loading ofa full-precision LLaMA-2-7B necessitates a memory capacity of noless than 28 GB.",
  "Related Work": "The era of LLM poses the necessity of model privacy protection,where the details of LLM cannot be visible to the clients. To thisend, Xiao et al. proposes a method named Offsite-tuning un-der the scenario where there is a server (a.k.a. LLM owner) and aclient, while Kuang et al. extends this work to an FL versionand names it as FedOT. They achieve model privacy protection bycompressing the model, where only some layers are visible to theclients. However, these works require the preservation of a largenumber of layers to guarantee the performance, hindering the ef-fectiveness of model privacy protection. In contrast, our work onlydiscloses a few model parameters of the original LLM to the clients,i.e., the clients only know the adapter parameters that come fromthe original LLM, while the emulator parameters have been updatedand different from the original LLM. Besides, neither offsite-tuning nor FedOT consider the difference between alignmentdata on the server and the fine-tuning data on clients. In contrast,the bi-level optimization problem proposed in our work naturallyconsiders this factor and we design updating rules based on it.Black-box is also a practical way to protect model privacy, wherethe clients access the LLM via an API, and they cannot fine-tune the LLM. Therefore, the optimization solely relies on prompt-basedlearning . In the context of FL, there are two typ-ical works, namely, Fed-BBPT and FedBPT . These twoworks guarantee the model privacy in FL, but they should trans-mit the prompt together with the input to the LLM owner, leadingto concerns about data privacy when the input contains sensitiveinformation, violating the requirement of FL. In contrast, the pro-posed FedBiOT will not lead to this concern because its training isfully on the clients such that the data are never shared with others.",
  "FedBiOT": "Given that some clients may be unable to load a complete LLM, thissection introduces an algorithm designed to enable these clients tofine-tune the LLM without requiring access to its full version. Inother words, our goal is to refine the part of a compressed modelthat should yield performance comparable to fine-tuning its coun-terpart within a full model. To accomplish this, the server initiallycompresses the LLM and divides it into two distinct components,each serving specific functions. The first component, termed an em-ulator, is tasked with replicating the behavior of the uncompressedLLM. The second component, referred to as an adapter, focuses onadeptly acquiring domain-specific linguistic patterns from clients.Upon reintegrating the adapter into the uncompressed emulator,its performance should demonstrate significant improvement com-pared to the original LLM.However, direct fine-tuning of the adapter on its models presentstwo significant limitations. Firstly, given that a single layer of alarge language model (LLM) comprises millions of parameters, suchas the decoder layer of LLaMA-2 with 202 million parameters, theadapters parameter count is immense. This necessitates clientsto possess powerful computational equipment to handle the fine-tuning of the layer. Additionally, transmitting the layer updates tothe server poses another bottleneck, particularly in scenarios withunreliable network connections or limited bandwidth, hinderingthe smooth transmission of updates to the server.To address these constraints, we integrate LoRA , a PEFTmodule, into our proposed method. LoRA significantly reduces thenumber of tunable parameters, with a LoRA module for LLaMA-2 comprising 0.13 million trainable parameters, which is merely0.06% of the original layers size. Consequently, the communicationcost experiences a remarkable reduction of 99.94% compared totransmitting a full layer. Organization. In the subsequent sections, we will delve intothe concrete details of the algorithm design. Specifically, .1illustrates how the compressed model is prepared. Following that,.2 discusses the problem formulation for the aforemen-tioned objectives. On top of this, .3 and .4 outlinethe detailed steps of the proposed algorithm, namely local updatesand server aggregation, showcasing the seamless integration ofLoRA modules. Full implementation of the pseudocode is given inAlgorithm 2.",
  "DL + L(5)": "Let the optimal emulator E for Equation (5) be E with theparameter of E . Denote the selected adapter A with the param-eter of A . To this end, we distribute a compressed model to theclients with the initial parameters of { A, E }. To reducethe computation and communication costs, we incorporate LoRA for the adapter A and the emulator E, denoted as A andE, respectively.Before diving into the details of the proposed FedBiOT, we brieflygo through the workflow as described in . The figure visu-ally presents the workflow of the federated learning process of ourproposed FedBiOT, including the local updates on clients (.3) and the aggregation on the server (.4). At the beginningof clients fine-tuning, the server broadcasts the adapter A andthe emulator E to the clients. Subsequently, the clients performmultiple local updates to fine-tune the adapter A with theirlocal datasets. After the local updates, the client uploads the adapterA to the server, and the server thereby aggregates the adapters.To ensure that the emulator is still able to reproduce the behaviorof the uncompressed LLM, the server fine-tunes the emulator Ewith the public dataset. Finally, the server distributes the updatedparameters to the clients and launches a new round of training.",
  "L() =E (; E) E (; E)22+ (M(; { A, E})M(; { A, E}))(7)": "where ( )A is the adapter LoRA received at the beginning of eachcommunication round, A reconstructs for the same size of theadapter A. D represents the public dataset on the server,which can be unlabeled. () is the KL divergence betweentwo logits. and are hyperparameters. The upper-level objective (Equation (6)). The upper-level ob-jective function consists of two terms. The first term represents theloss of the model on local clients data, with the current emulatorand adapter. It follows a classic weighted average loss in FL to bal-ance the loss of different clients heterogeneous local data. The goal of introducing this term is straightforward: by minimizing the lossof the first term, we expect the emulator-adapter combination tobe improved on the local training set. The second term is a regu-larization of the adapter component to ensure it will be within areasonable distance from the synchronized and broadcast adapterat the beginning of each communication round. Enforcing a restric-tion on the adapters change can reduce the difference of losses forthe emulator distillation after locally adapter are tuned locally onclients, so it can help the convergence of emulator distillation. The lower-level objective (Equation (7)). The first term in theconstraint is the 2-norm difference between the activation outputby the emulator and the full model. The second term is the KLdivergence between the output of output distribution of the fullmodel-adapter combination and the emulator-adapter. Althoughonly the emulator is trainable to minimize the loss of these twoterms, these two terms provide different optimization meaning forthe emulator. The first term encourages the emulator to provideactivations as close as possible to the full model, excluding theeffect of the adapter. The second term ensures the emulator canprovide output distributions close to the one when the full modelwith adapters is added on. Discussion. The introduced algorithm can optimize the bi-levelproblems (i.e., Equation (6) and (7)) to an equilibrium point forboth adapter and emulator. This is because when we optimize theadapter, the fixed emulator constrains its updates, and vice versa,and thereby, the emulator and adapter are distilled or trained inter-changeably. At this equilibrium, the emulator can more faithfullyextract and encode the information for the clients dataset andbenefit from the training of the adapter in reverse.Additionally, FedBiOT does not require the design of an emulatorto follow linear dropout. Instead, this is a general framework thatcompresses an LLM and divides it into two components: an emulatorand an adapter. There are numerous designs for the emulator, butthey share the same objective where the emulator simulates thenon-compressed part of an LLM. For simplicity, we follow offsite-tuning and prepare the emulator by means of uniform layerdropout to demonstrate the effectiveness of FedBiOT.",
  "Client Updates": "During the local updates, the clients barely fine-tune the parametersof the adapter A while fixing the parameters of the emulator E.By enabling LoRA, the LoRA of the adapter will get updated, andtherefore, the clients should upload the updated A to the serverafter the local fine-tuning ends.Consider client [] performs the local updates at -th round.Before optimizing the adapter locally, the client receives the updatedemulator E and adapter A from the client, and we denotethem by",
  "Model Aggregation": "During the server aggregation, the server performs the weightedaverage to update the adapters A and fine-tune the emulator E. Byenabling the LoRA, only the parameters A in the adapter and E in the emulator are updated, while the rest (i.e., A and E ) remain unchanged.First, the server collects a set of updated LoRAs of the adapter,i.e., A,[] from the clients. Based on the definition ofEquation (6), the server performs weighted aggregation via",
  "This section discusses the implementation of our experiments, cov-ering details such as the model utilized and evaluation metrics. Thecode is now available at": "Model and computation environment. The experiments uti-lize LLaMA-2-7B, an open-source pre-trained LLM maintained byMeta and released in July 2023 . Preceding this, the modelsfirst generation was introduced in February 2023 . This modelsupports a maximum of 4096 input tokens and consists of 32 hiddenlayers with a total of 6.7 billion parameters. The experimental setupinvolves machines equipped with Nvidia A100 GPU cards, IntelXeon Platinum 8369B CPUs, and a 512GB RAM configuration. Datasets and Tasks. In the experiments, we use the benchmarkdatasets and tasks in to train and evaluate the LLM on three dif-ferent NLP tasks, covering math problem-solving, code generation,and question-answering:",
  "For math problem-solving, we split the GSM-8K training dataset ensuring i.i.d. across three clients, and we assess the updatedmodel using the GSM-8K test dataset": "For code generation, we fine-tune the model with the Rosettadataset , which is partitioned across the programming lan-guages, and a total of nine clients separately hold the data fromnine different programming languages. Regarding its evaluation,we utilize HumanEvalX , an extension of a coding evaluationdataset that requires the model to fill in the code for a givenproblem in the required programming language (i.e., C++, GO,Java, Python).",
  "TaskTrainingDataset# trainingsamples# clientsPartition RulesMax.Min.Std.TestDataset# testsamples": "Math Problem SolvingGSM-8K74733i.i.d.249124910GSM-8K1319Code GenerationRosetta79549Prog. Lang.1172439236.94HumanEvalX656Question AnsweringDolly150158Category3611711795.06HelmNAPublic DatasetAlpaca52002 Implementation. This work is built upon an open-source fed-erated learning platform named FederatedScope . The trainingdata are reformatted following the predesigned instructions .Different from , we regard the last two and the last fourdecoders as the adapter. The experiments consider two dropoutrates, i.e., {0.2, 0.5}, and we obtain the emulators with layerdropout following Xiao et al. . Without special annotation, weuse the following local training setting: in each communicationround, each client performs 30 local updates, and the batch sizeof every local update is 10. Before launching the FL training, wefine-tune the emulator for 500 iterations to generate a distilledemulator E towards minimizing the loss of Equation (7). Duringthe FL training, the server takes 10 iterations to align the emulatorE with E between two successive communication rounds afteraggregating local adapters with FedAvg . These experimentsrun for 500 communication rounds, and we report the results basedon the fine-tuned LLM obtained at the 500th round. During thetraining, we only fine-tune the adapter in the clients local updateprocedures, and we update the emulator on the server side. Inother words, other parts of the pre-trained model, such as wordembeddings, are frozen during the training. LoRA, Optimizers and Hyperparameters. We add the LoRAto all decoder layers in the adapter and the emulator by settingthe rank to 8 and the alpha to 16. We use AdamW as an optimizerto solve Equation (6) and (7) on the clients (for the adapters) andthe server (for the emulators), respectively. We search for the bestlearning rate in {1 105, 3 105, 5 105, 8 105, 1 104}.We set the momentum for (0.9, 0.95). As for other hyperparametersrelated to the optimizer, we use the default setting. Furthermore, wealso conduct grid search for FedBiOT-specific hyperparameters, i.e., and . Throughout the experiments, we demonstrate the result ofthe best hyperparameter combination. To avoid randomness, weutilize three different random seeds and report the averaged results. Baselines. Offsite-tuning is the only method that satisfies theconstraints that fine-tuning without access to full model. Xiao et al. introduces a single-client offsite-tuning, while Kuang et al. extends it to an FL version (i.e., FedOT). We apply offsite-tuningwith one single client, where all data are loaded to the client. AsFedOT supports FL, we reproduce the algorithm to work on theFL tasks. In terms of the setting of the adapters and the emulators,both Offsite-tuning and FedOT treat the first two and the last twodecoders as the adapter. To enable the parameter-efficient fine-tuning for both baselines, we add LoRA to both baselines, the sameas the setting adopted by FedBiOT.",
  "Quantitative Evaluation on i.i.d. Data": "We demonstrate the experimental results of GSM-8K provided in and highlight the worth-noted phenomenon when the dataare i.i.d. across the clients.A notable phenomenon observed in the table is that AdapEmusignificantly falls behind AdapFu, particularly at a low dropoutrate (i.e., = 0.2). To explain this, we examine the accuracy of theLLaMA-2 model with a dropout rate of 0.2, which is 2.12% withoutfine-tuning and increases to 2.43% after fine-tuning the emulatorwith a public dataset. The performance gap between AdapEmu andAdapFu can be attributed to layer dropout, which reduces the sizeof the LLM and subsequently impacts its performance. Additionally,this result highlights the difficulty of accurately reproducing thenon-compressed parts with the emulator. Fortunately, all methodsimprove AdapEmus performance compared to the version withoutfine-tuning.When we take a look at the proposed FedBiOT at differentadapters sizes, we notice that FedBiOT with adapter 4 achieves bet-ter performance than that with adapter 2 under the AdapFu setting.As we know, a larger adapter has more trainable parameters, andtherefore, it can easily absorb the knowledge from the downstreamtasks. Note that the performances of these two adapter settingshave subtle differences under AdapEmu, meaning that their emula-tor achieves very similar effects to the non-compressed emulator.When we plug the adaptor back into the non-compressed emulator,the adapter with more trainable parameters obviously can achievea better performance.",
  "FedBiOT (Adapter 4)AdapFu5.0311.096.258.477.4113.3213.5416.74": "When comparing our proposed model with the baselines, we cannotice a significant dominance in performance, especially in theAdapFu setting. More specifically, when the dropout rate becomeslarger, the performance of AdapFu with FedBiOT decreases moremildly in contrast to other baselines. This is thanks to two factors:1) the regularization term ensures the adapters will not changedramatically; 2) the on-the-fly distillation of the emulator withmixed losses can work better with clients data. Although the othertwo baselines use a public dataset to achieve similar functionality,the deterioration may still occur due to the data domain shift andthe significant information loss.",
  "Quantitative Evaluation on non-i.i.d. Data": "According to , code generation and question answeringare two tasks split in non-i.i.d. styles. In this section, we evaluateour proposed FedBiOT when it trains an LLM with a non-i.i.d.dataset. It is worth noting that the evaluation task could be eitherin-distribution or out-of-distribution to the training dataset. Code generation. and 4 illustrate the best results in dif-ferent programming languages based on different hyperparametersettings. Let us take a look at the results of the FedBiOT at differ-ent adapter sizes. Apparently, FedBiOT with two layers of adapterconstantly outperforms FedBiOT with four under both AdapEmuand AdapFu. This conclusion is different from the one when anLLM is trained with an i.i.d. dataset. The discrepancy can be attrib-uted to the clients objectives: under i.i.d. datasets, a larger adaptersize benefits training by absorbing downstream linguistic patternsuniformly. Conversely, with non-i.i.d. datasets, clients are biasedtowards their local optima, where the emulators effect becomescrucial. When comparing our proposed algorithm with the baselines,we notice a distinct dominance in AdapFu across all programminglanguages. In particular, when the dropout rate is 0.5, we can achieveup to 6% improvement over other baselines in terms of Pass@1,and up to 10% improvement of Pass@10. Notably, the most distinctdominance can be witnessed under the column of Java in . Question Answering. shows the evaluation results us-ing the HELM benchmark while we train the LLM with Dolly-15K.Generally speaking, FedBiOT (Adapter 2) performs significantlybetter than Adapter 4 in some tasks in terms of AdapEmu. As bothAdapEmu have the same number of layers, this result exhibits theimportance of the emulator, i.e., the model with a larger emula-tor can achieve leading performance. To some extent, this resultsupports our previous conclusion that an emulator plays a moreimportant role than an adapter in a non-i.i.d. task. As for AdapFu,the performance difference is trivial between the two adapter sizes.The proposed algorithm outperforms offsite-tuning and FedOTin most datasets, which is consistent with the findings in other train-ing tasks. The dominance of AdapFu becomes more pronounced asthe dropout rate increases from 0.2 to 0.5. For instance, FedBiOT isapproximately 10% better than the baselines at a 0.5 dropout rate inNatural Questions (closed-book), compared to a 2% improvement ata 0.2 dropout rate. Notably, comparing b and 2c, we noticethat FedBiOT is mildly affected by changes in the dropout rate,while the baselines suffer significant degradation as the dropoutrate increases. This stability can be attributed to round-by-roundemulator alignment, where the non-compressed part of the fullmodel is set as an anchor, regardless of the dropout rate. Conse-quently, this approach stabilizes the adapter training process, en-suring that adapters of the same size achieve similar performanceacross varying dropout rates.",
  "Discussion on Computation andCommunication Overhead": "presents the computation and communication overheadof different methods under different dropout rates. As mentionedin the experimental setting, all algorithms have been applied withLoRA, and therefore, the number of trainable parameters dramati-cally reduces. From the clients perspectives, the number of train-able parameters is determined by the number of decoder layers inthe adapter. Apparently, FedBiOT (Adapter 2) should be with theminimum number of trainable parameters among other methods.The computation costs in are measured by per-tokenfloating point operation (FLOP/token). As we can see, the proposedFedBiOT costs less overhead than offsite-tuning and FedOT. Thedifference arises on account of the position of the trainable parame-ters. The adapter of the proposed FedBiOT is near the output layer.As for offsite-tuning and FedOT, the adapters are located separatelyat the top and the bottom two layers, thereby consuming morecomputation costs in the backward propagation for transmittingthe derivative from the bottom to the top.However, our proposed method may require more communica-tion overhead than the baselines. This is because the server shouldtransmit the LoRA parameters of both the adapter and the emulatorto the clients in our proposed method, while in offsite-tuning andFedOT, the server merely transmits the aggregated LoRA of theadapter to the clients. However, the overall cost is trivial, comparedto the full LLM transmission at a cost of 28GB.",
  "Conclusion": "In this paper, we introduce FedBiOT, a federated learning algorithmthat avoids full model fine-tuning while substantially reducing com-putation overhead. Specifically, we compress the LLM and divide itinto two components, namely, an emulator and an adapter. By for-mulating a bi-level optimization problem, our proposed FedBiOTensures that the emulator partially simulates the original LLM,while the adapter focuses on learning domain-specific linguisticpatterns. Extensive experiments show the superiority of the pro-posed FedBiOT working with LLaMA-2, where it can achieve sig-nificant accuracy improvement than the existing baselines (i.e.,Offsite-tuning and FedOT) in all tasks (i.e., math problem-solving,code generation, and question answering). The authors would like to thank the anonymous reviewers for theirconstructive comments. This work is supported in part by the USNational Science Foundation under grants NSF-IIS 1747614 andNSF-IIS 2141037. Any opinions, findings, and conclusions or rec-ommendations expressed in this material are those of the author(s)and do not necessarily reflect the views of the National ScienceFoundation. Durmus Alp Emre Acar, Yue Zhao, Ramon Matas, Matthew Mattina, Paul What-mough, and Venkatesh Saligrama. 2020. Federated Learning Based on DynamicRegularization. In Proc. of International Conference on Learning Representations(ICLR20).",
  "Sahil Chaudhary. 2023. Code Alpaca: An Instruction-following LLaMA modelfor code generation": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondede Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,Greg Brockman, et al. 2021. Evaluating large language models trained on code.arXiv preprint arXiv:2107.03374 (2021). Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun,Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,et al. 2021. Training verifiers to solve math word problems. arXiv preprintarXiv:2110.14168 (2021). Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, SamShah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin.2023.Free Dolly: Introducing the Worlds First Truly Open Instruction-Tuned LLM.",
  "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge ina neural network. arXiv preprint arXiv:1503.02531 (2015)": "Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, LuWang, Weizhu Chen, et al. 2021. LoRA: Low-Rank Adaptation of Large LanguageModels. In Proc. of International Conference on Learning Representations (ICLR21). Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebas-tian Stich, and Ananda Theertha Suresh. 2020. Scaffold: Stochastic controlledaveraging for federated learning. In Proc. of International conference on machinelearning (ICML20). 51325143. Weirui Kuang, Bingchen Qian, Zitao Li, Daoyuan Chen, Dawei Gao, Xuchen Pan,Yuexiang Xie, Yaliang Li, Bolin Ding, and Jingren Zhou. 2024. FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models inFederated Learning. In Proc. of the ACM SIGKDD Conference on Knowledge Dis-covery and Data Mining (KDD24). Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The Power of Scalefor Parameter-Efficient Prompt Tuning. In Proc. of the Conference on EmpiricalMethods in Natural Language Processing (EMNLP21). 30453059.",
  "Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang.2019. On the Convergence of FedAvg on Non-IID Data. In Proc. of InternationalConference on Learning Representations (ICLR19)": "Xiang Lisa Li and Percy Liang. 2021. Prefix-Tuning: Optimizing ContinuousPrompts for Generation. In Proc. of the Annual Meeting of the Association for Com-putational Linguistics and the International Joint Conference on Natural LanguageProcessing (ACL/IJNLP21). 45824597. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michi-hiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al.2022. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110(2022). Tao Lin, Lingjing Kong, Sebastian U Stich, and Martin Jaggi. 2020. Ensembledistillation for robust model fusion in federated learning. In Proc. of Advances inNeural Information Processing Systems (NeurIPS20). 23512363. Zihao Lin, Yan Sun, Yifan Shi, Xueqian Wang, Lifu Huang, Li Shen, and DachengTao. 2023. Efficient federated prompt tuning for black-box large pre-trainedmodels. arXiv preprint arXiv:2310.03123 (2023).",
  "Ilya Loshchilov and Frank Hutter. 2018. Decoupled Weight Decay Regularization.In Proc. of International Conference on Learning Representations (ICLR18)": "Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, andBlaise Aguera y Arcas. 2017. Communication-efficient learning of deep net-works from decentralized data. In Proc. of Artificial intelligence and statistics(AISTAT17). 12731282. John J Nay, David Karamardian, Sarah B Lawsky, Wenting Tao, Meghana Bhat,Raghav Jain, Aaron Travis Lee, Jonathan H Choi, and Jungo Kasai. 2024. Largelanguage models as tax attorneys: a case study in legal capabilities emergence.Philosophical Transactions of the Royal Society A 382, 2270 (2024), 20230159.",
  "Hassan Sajjad, Fahim Dalvi, Nadir Durrani, and Preslav Nakov. 2023. On theeffect of dropping layers of pre-trained transformer models. Computer Speech &Language 77 (2023), 101429": "Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung WonChung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al.2023. Large language models encode clinical knowledge. Nature 620, 7972 (2023),172180. Alessandro Sordoni, Xingdi Yuan, Marc-Alexandre Ct, Matheus Pereira, AdamTrischler, Ziang Xiao, Arian Hosseini, Friederike Niedtner, and Nicolas Le Roux.2023. Joint prompt optimization of stacked llms using variational inference. InProc. of Advances in Neural Information Processing Systems (NeurIPS23). Jingwei Sun, Ziyue Xu, Hongxu Yin, Dong Yang, Daguang Xu, Yiran Chen, andHolger R Roth. 2023. FedBPT: Efficient Federated Black-box Prompt Tuning forLarge Language Models. arXiv preprint arXiv:2310.01467 (2023).",
  "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, CarlosGuestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford Alpaca: AnInstruction-following LLaMA model": "Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, LauraGutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. 2023. Large language modelsin medicine. Nature medicine 29, 8 (2023), 19301940. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-AnneLachaux, Timothe Lacroix, Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, et al. 2023. Llama: Open and efficient foundation language models. arXivpreprint arXiv:2302.13971 (2023). Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-mine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-ale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXivpreprint arXiv:2307.09288 (2023). Haozhao Wang, Yichen Li, Wenchao Xu, Ruixuan Li, Yufeng Zhan, and ZhigangZeng. 2023. Dafkd: Domain-aware federated knowledge distillation. In Proc. ofthe IEEE/CVF conference on Computer Vision and Pattern Recognition (CVPR23).2041220421. Haozhao Wang, Haoran Xu, Yichen Li, Yuan Xu, Ruixuan Li, and Tianwei Zhang.2023. FedCDA: Federated Learning with Cross-rounds Divergence-aware Ag-gregation. In Proc. of The International Conference on Learning Representations(ICLR23). Haoyu Wang, Handong Zhao, Yaqing Wang, Tong Yu, Jiuxiang Gu, and JingGao. 2022. FedKC: Federated knowledge composition for multilingual naturallanguage understanding. In Proc. of the ACM Web Conference 2022 (WWW22).18391850. Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor.2020. Tackling the objective inconsistency problem in heterogeneous feder-ated optimization. In Proc. of Advances in neural information processing systems(NeurIPS20). 76117623. Sheng Wang, Zihao Zhao, Xi Ouyang, Qian Wang, and Dinggang Shen. 2023.Chatcad: Interactive computer-aided diagnosis on medical image using largelanguage models. arXiv preprint arXiv:2302.07257 (2023). Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, BrianLester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned LanguageModels are Zero-Shot Learners. In Proc. of International Conference on LearningRepresentations (ICLR21). Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits rea-soning in large language models. In Proc. of Advances in Neural InformationProcessing Systems (NeurIPS22). 2482424837. Feijie Wu, Song Guo, Zhihao Qu, Shiqi He, Ziming Liu, and Jing Gao. 2023.Anchor sampling for federated learning with partial client participation. In Proc.of International Conference on Machine Learning (ICML23). 3737937416.",
  "Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014. How trans-ferable are features in deep neural networks?. In Proc. of Advances in neuralinformation processing systems (NeurIPS14)": "Jie Zhang, Song Guo, Xiaosong Ma, Haozhao Wang, Wenchao Xu, and Feijie Wu.2021. Parameterized knowledge transfer for personalized federated learning. InProc. of Advances in Neural Information Processing Systems (NeurIPS21). 1009210104. Jianyi Zhang, Saeed Vahidian, Martin Kuo, Chunyuan Li, Ruiyi Zhang, Tong Yu,Guoyin Wang, and Yiran Chen. 2024. Towards building the federatedGPT: Fed-erated instruction tuning. In Proc. of IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP24). 69156919. Zhuo Zhang, Yuanhang Yang, Yong Dai, Qifan Wang, Yue Yu, Lizhen Qu, andZenglin Xu. 2023. FedPETuning: When federated learning meets the parameter-efficient tuning methods of pre-trained language models. In Proc. of AnnualMeeting of the Association of Computational Linguistics (ACL23). 99639977. Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Lei Shen,Zihan Wang, Andi Wang, Yang Li, et al. 2023. Codegeex: A pre-trained modelfor code generation with multilingual benchmarking on humaneval-x. In Proc. ofthe ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD23).56735684.",
  "As described in , we utilize three datasets to assess the fine-tuning performance. In this section, we briefly introduce all thesedatasets and provide the details about how they evaluate a givenLLM": "GSM-8K.. We use the GSM-8K test set to evaluate the abil-ity of a large language model (LLM) to solve math problems. Thisdataset includes \"questions\" and \"ground truth\" answers. We as-sess correctness by determining how often the LLM answers agiven question correctly. Following chain of thought (CoT) , weprepare a set of sample questions (a.k.a. few-shot prompting) andprompt the LLM to generate step-by-step solutions, ensuring theanswers are formatted correctly. Finally, we extract the answersfrom these solutions and compare them with the ground truth tocalculate the correctness rate. HumanevalX.. This is a task for code autofill, which consists of164 test samples for five programming languages . It is worthnoting that we use four of them (i.e., C++, GO, Java, and Python)because there are no JavaScript codes in the training dataset. Each test sample is constituted with task id, prompt (i.e., Task de-scription with partial codes), entry point (i.e., the function to beachieved), canonical solution (i.e., a sampled solution), and test(i.e., evaluate if the generated code can obtain the correct answerbased on the given input). In this task, we use prompt as theinput and generate five versions of codes using a given model. Wecompile the code and check if it can pass the given test. Let bethe number of correct codes generated by LLM and passed unittests, and therefore, Pass@k can be computed via"
}