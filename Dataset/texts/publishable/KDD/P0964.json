{
  "ABSTRACT": "Hierarchical time-series forecasting (HTSF) is an important prob-lem for many real-world business applications where the goal isto simultaneously forecast multiple time-series that are related toeach other via a hierarchical relation. Recent works, however, donot address two important challenges that are typically observedin many demand forecasting applications at large companies. First,many time-series at lower levels of the hierarchy have high sparsityi.e., they have a significant number of zeros. Most HTSF methodsdo not address this varying sparsity across the hierarchy. Further,they do not scale well to the large size of the real-world hierarchytypically unseen in benchmarks used in literature. We resolve boththese challenges by proposing HAILS, a novel probabilistic hier-archical model that enables accurate and calibrated probabilisticforecasts across the hierarchy by adaptively modeling sparse anddense time-series with different distributional assumptions andreconciling them to adhere to hierarchical constraints. We showthe scalability and effectiveness of our methods by evaluating themagainst real-world demand forecasting datasets. We deploy HAILSat a large chemical manufacturing company for a product demandforecasting application with over ten thousand products and ob-serve a significant 8.5% improvement in forecast accuracy and 23%better improvement for sparse time-series. The enhanced accuracyand scalability make HAILS a valuable tool for improved businessplanning and customer experience.",
  "KDD 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s).ACM ISBN 979-8-4007-0490-1/24/08": "ACM Reference Format:Harshavardhan Kamarthi, Aditya B. Sasanur, Xinjie Tong, Xingyu Zhou,James Peters, Joe Czyzyk, and B. Aditya Prakash. 2024. Large Scale Hierar-chical Industrial Demand Time-Series Forecasting incorporating Sparsity.In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discoveryand Data Mining (KDD 24), August 2529, 2024, Barcelona, Spain. ACM, NewYork, NY, USA, 10 pages.",
  "INTRODUCTION": "Hierarchical time-series forecasting is a problem that profoundly in-fluences decision-making across various domains. These time-seriesdata possess inherent hierarchical relationships and structures .Instances of such situations include predicting employment trends across diverse geographical scales, forecasting the spread ofepidemics , etc. When dealing with time-series datasets thatexhibit underlying hierarchical dependencies, the objective of hier-archical time-series forecasting is to generate precise forecasts forall individual time-series while capitalizing on the hierarchical inter-connections among them . For instance, at a large manufacturingcompany, forecasting demand at various levels of aggregation is im-portant . Forecasts at a middle level of the business hierarchy areimportant for procuring raw materials and determining the amountof intermediate materials (or product families) to produce in themedium-term. Near-term forecasts at lower levels of the hierarchyrelate more to specific products and even package sizes that areneeded. Additionally, companies do not forecast based solely usinghistorical data but include external variables (such as macroeco-nomic forecasts which incorporate reasonable assumptions aboutthe future) to improve demand forecasts.Previous forecasting methods have not typically placed an em-phasis on providing well-calibrated probabilistic forecasts thatmodel uncertainty. Instead, traditional methods have primarilyconcentrated on providing single-point predictions. In contrast,recent post-processing techniques refine forecast distri-butions generated by independent base models as a preprocessingstep. These post-processing methods offer the advantage of beingreadily applicable to forecasts generated by various models andare usually simple to implement and tractable even for large-scaledatasets with thousands of time-series in the hierarchy. However,they fall short in enabling the base forecasting models to grasp the",
  "KDD 24, August 2529, 2024, Barcelona, SpainHarshavardhan Kamarthi et al": "test on samples from training data of the time-series. Intuitively, thedispersion test tests if the mean and variance of the data samples aresimilar. We observe that using a value threshold of 0.1 is a goodmeasure to classify nodes as sparse or dense. We also make sure thatthe parents of a node classified as dense are automatically dense.We observe this to be always true for our benchmark datasets. But,in case it does not hold, we explicitly classify the parents as dense.Notationally, we denote all nodes in {1, . . . , } that are classifiedas sparse as S. 4.2.2Base forecasting model. The requirements for choosing abase forecasting model are based on the applications specific needs.ProfHiT uses CaMuL due to its superior performance interms of accuracy and uncertainty quantification. However, it makesdeploying to large industrial hierarchies infeasible. First, CaMuL is astochastic model that leverages multiple sampling componentsthat makes stable training a hard technical challenge when scaling itto train tens of thousands of time-series independently. Secondly, itrequires significant amount of historical data that it uses as referencepoints to map similar patterns from historical data to current inputtime-series for uncertainty quantification. Along with the challengeof the high compute requirement of storing and embedding thesehistorical time-series, in many real-world applications we do nothave sufficient historical data to learn reliably. Finally, CaMuL is notdesigned to model sparse predictions and instead parameterizedthe output as a Gaussian. Instead, we chose a simpler model: aGated Recurrent Unit (GRU) neural network, a widely adoptedrecurrent deep learning model. Depending on the nature of thenode, the base forecasts output the forecast parameter. For a node S classified as dense, it outputs two parameters of the normaldistribution: (, exp()). For any node S classified as sparse,it simply outputs only the Poisson mean parameter: = . 4.2.3Hierarchy-aware Refinement Module. This module uses thebase forecasts from RNNs and refined them to 1) leverage infor-mation of the time-series across the hierarchy 2) enables themto be distributionally consistent by training on the SDCR. Let = [1 . . . , ] be a vector of means of base distributions for allnodes. is the weighted sum of and base mean of all time-series:",
  "RELATED WORKS": "Classical methods in hierarchical time-series forecasting tradition-ally employed a two-phase method, concentrating on point pre-dictions . These methods predicted time-series at a singularhierarchy level, then extrapolated forecasts to other levels usinghierarchical relationships. Recent techniques, such as MinT andERM, act as post-processing procedures, refining forecasts across allhierarchy levels. MinT operates under the assumption thatbaseline forecasts are independent and unbiased, aiming to mini-mize forecast error variance from historical data. ERM modifiesthis by not assuming unbiased forecasts.Recent neural network approaches offer more end-to-end learn-ing of patterns of individual time-series as well as hierarchicalrelations across time-series. Rangapuram et al. adopt a strategyof projecting the forecasts into a subspace of reconciled forecastsvia a differentiable operation and optimize the loss on the projectedforecasts. SHARQ represents another novel deep-learning prob-abilistic method that employs quantile regression and regularizesconsistency across various forecast distribution quantiles. ProfHiT imposes hierarchical constraints on the forecast distributionsby minimizing distributional distance between the parent forecastand the sum of the child forecasts. However, none of the methodsare designed to adapt to sparse time-series and therefore performsub-optimally on real-world industrial demand time-series.",
  "PROBLEM STATEMENT": "We denote the dataset D of time-series over the time horizon1, 2, . . . ,. Let y R be time-series and ( )its value at time. The hierarchical relations across time-series is denote as T =(T, T) where T is a tree of nodes rooted at time-series 1(time-series 1 is the aggregate of all leaf time-series). Consider a non-leaf node (time-series) with children C. The hierarchical relationsare of the form T = {y = C y : {1, 2, . . . , }, |C | >0} where values of are constant and known.Our problem can be formulated as follows: Given a dataset Dwith underlying hierarchical relations T, we learn a model thatprovides accurate probabilistic forecast distributions{ ((+1)1|D), . . . ((+)|D)} across all levels of the hierar-chy where is the forecast horizon.",
  "METHODOLOGY": "Most hierarchical forecasting models struggle to adapt to largehierarchies found in real-world industrial applications both in termsof effectiveness and efficiency in learning from tens of thousandsof time-series as well as adapting to sparse time-series at the lowerlevels of the hierarchy. HAILS overcomes these challenges in twoways. First, we make architectural design choices to enable moreefficient in learning from larger hierarchies as well as support sparsetime-series. Second, we develop optimization methods to allow for",
  "Large Scale Hierarchical Industrial Demand Time-Series Forecasting incorporating SparsityKDD 24, August 2529, 2024, Barcelona, Spain": "average value of ground truth in training dataset to get the weightedRMSSE. This metric was used in the M5 competition to evaluatethe accuracy of point predictions .Cumulative Ranked Probability Score (CRPS) is a widely usedstandard metric for the evaluation of probabilistic forecasts thatmeasures both accuracy and calibration. Given ground truth andthe predicted probability distribution , let be the CDF. Then,CRPS is defined as:",
  "Probabilistic Hierarchical Forecasting": "ProfHiT is a state of the art probabilistic forecasting modelthat. It optimizes the full distribution of forecasts of the hierarchy tobe both accurate and consistent with the hierarchical constraints. Itfirst produces base forecasts for each node of the hierarchy indepen-dently via a differentiable neural model. The authors of ProfHiTchose to use CaMuL , a state of the art neural probabilisticforecasting model to produce the base forecasts parameterized bynormal distribution {(, )}=1. The base forecast parameters areused as prior distribution parameters to generate refined distribu-tion that leverage inter-series relations and hierarchical constraintsto produce the refined parameters {(, )}=1. This is achievedby the Hierarchy-aware Refinement Module and the whole model istrained on both the Log-likelihood loss for accuracy and Soft Dis-tributional Consistency Regularization (SDCR) for DistributionalConsistency by minimizing the Distributional Consistency Errordefined as follows:",
  "HAILS: Forecasting for large hierarchieswith sparse time-series": "HAILS models sparse and dense time-series using appropriate dis-tributions when forecasting. A key challenge which we overcome inthe process is to enable learning consistent forecasts in cases whereparts of the hierarchy have both sparse and dense time-series. To en-able this feature, HAILS proposes important architectural changesto ProfHiT and a novel loss: Distributional consistency regulariza-tion with Sparse adaptation. We describe the various modules ofHAILS as follows. 4.2.1Testing for Poisson Distribution. We first determine whetherwe should model a given node of the hierarchy as a sparse time-series. We use the Poisson distribution to model the time-seriesif it is deemed sparse since we can model the high probability ofobserving zeros. Therefore, to systematically classify the data froma given node of the hierarchy as sparse, we use the Poisson dispersion",
  "= sigmoid( ), = + (1 )w (2)": "where { }=1 and {w}=1: are parameters of the model andsigmoid() denotes the sigmoid function. intuitively denotes thetradeoff between relying on the base mean and information fromrest of the distribution. Let = { | S} be a vector of variancesfor dense nodes base forecasts. The variance parameter of therefined distribution is derived from the base distribution parameters",
  "where {v1}=1, {v2}=1 and {}=1 are parameters and is apositive constant hyperparameter": "4.2.4Soft Distributional Consistency Regularization. ProfHiT learnsto generate forecasts that are distributionally consistent by intro-ducing SDCR. It forces the model to minimize the DistributionalConsistency Error across the forecasts of the hierarchy leading tothe aggregated forecasts of the children being similar to the parentforecast. However, SDCR only deals with dense time-series since it models them as Gaussians. Moreover. it cannot deal with differenttypes of distributions across the hierarchy. Therefore, SDCR can-not be directly applied to hierarchies that have sparse time-series(S ). HAILS introduces Distributional Consistency Regulariza-tion with Sparse adaptation (DCRS) that allows for hierarchies withvarying time-series sparsities to provide distributional consistency.DCRS applies different consistency losses across the subtrees ofthe hierarchies based on the the sparsity of the parents and chil-dren. We specifically look at three cases that are observed in thehierarchies:Dense Parent-Dense Children: If the parent node as well as childrennodes are dense, we use the same distributional consistency loss asProfHiT: we model the parent and children forecast distributionsas gaussians and compute the Jenson-Shannon divergence:",
  "(5)": "Mixed Subtrees: Now we examine the case where the parent is adense node but some or all of her children are sparse. We note thatas we go further up the hierarchy, sparsity of time-series decreases.Therefore, the sparsity assumption on these nodes gets weaker.We therefore propose to approximate the sparse forecasts of thesetime-series as Gaussian distributions and apply Eq. 4 to optimizefor distributional consistency. We perform the approximation lever-aging the central limit theorem as follows: Theorem 1. Let 1,2, . . . , be independent Poisson ran-dom variables with parameters 1, 2, . . . , . Then denote as ==1 . Then is a Poisson variable with parameter = =1 .Then for sufficiently large , can be approximated by a Gaussiandistribution = N (,",
  "EXPERIMENTS5.1Setup": "We evaluate HAILS against top hierarchical forecasting baselineson two large hierarchical demand forecasting benchmarks. Weevaluated all models on a system with Intel 64-core Xeon Pro-cessor with 128 GB memory and Nvidia Tesla V100 GPU with 32GB VRAM. We provide our implementation of HAILS at We used PyTorch for training neuralnetworks and Numpy for other data processing steps. 5.1.1Datasets. While most hierarchical forecasting benchmarksconsist of small hierarchies with all time-series being dense, wechoose to evaluate on two benchmarks for our specific application:large hierarchies for demand forecasting. We choose one publicdataset and also evaluate on a proprietary real-world use-case ofproduct demand forecasting at a large chemical company.M5 dataset: M5 forecasting competition featured a monthly retailsales forecasting dataset with hierarchically structured sales datawith intermittent and erratic characteristics . The datasethad 12 levels of hierarchy and consisted of 3914 time-series in total.The forecast horizon was up to 28 months ahead.Dow Demand forecasting: The dataset contains monthly histori-cal sales (in volume) from January 2018 to June 2023 made by Dowin 10+ major industries across 160+ countries. The dataset has a hi-erarchical structure where the top levels represents the aggregatedsales at the country and industry levels, and the lower levels con-tain the sales data in more granular product classes. The historicalsales from January 2018 to June 2022 along with external businessindicators were used to train the model. Product demand forecastswere generated for July 2022 to June 2023, and the actual salesduring this period are used as ground truth. The forecast horizonwas 12 months ahead with the following hierarchical structure: 5.1.2Baselines. We compare HAILSs performance against state-of-the-art hierarchical forecasting methods as well as generic time-series forecasting methods. We first compare against a standardheuristic of averaging past 6 months values (6-Average). ARIMA is a commonly used statistical time-series models. We also useGRU without any reconciliation as a common neural RNN-basedforecasting baseline. For GRU, we used Monte-Carlo dropout to generate multiple forecast samples for probabilistic forecasts.Finally we also considered DeepAR , popular deep probabilisticforecasting models which do not exploit hierarchy relations. Notethat 6-Average cannot produce probabilistic forecasts due to itsdeterministic mechanics.",
  ": Number of time-series and sparsity (% of zeros) byLevel for Dow time-series": "In the case of hierarchical forecasting, we considered PEMBU ,the state-of-art post-processing method applied on DeepAR fore-casts reconciled by MinT. With respect to the state-of-art neuralhierarchical forecasting methods, we compare against SHARQ a deep learning-based approach that reconciles forecast distribu-tions by using quantile regressions and making the quantile values consistent. We also compare against HierE2E , a deep learning-based approach that projects the base predictions onto a space ofconsistent forecasts and trains the model in an end-to-end manner.For the M5 benchmark, we also include the scores from the topsubmission of the M5 competition, denoted as M5-Leader. 5.1.3Evaluation Metrics. We evaluate our models and baselinesusing carefully chosen metrics to measure both point accuracy andprobabilistic distribution calibration of the forecasts. For a groundtruth ( ), let the predicted probability distribution be () with mean ( ). Also let () be the CDF.Weighted Root Mean Squared Scaled Error (WRMSSE) isa scale-invariant metric for point-predictions that can be used tocompare across different time-series of varying scales. RMSSE for atime-series is defined as:",
  "( ( ) 1{ > })2": "We approximate as a Gaussian distribution formed from samplesof the model to derive CRPS. We normalize the value of CRPS foreach time-series by the average value of ground-truth in trainingdata to get normalized CRPS.Root Mean Squared Error (RMSE) is used to calculate the fore-cast performance at specific time-step since the opther metrics areusually used to calculate over the full forecast horizon.",
  "We evaluate the performance of HAILS on the M5 dataset and thenperform a detailed case study showcasing the impact of HAILS ondemand forecasting at Dow": "5.2.1Forecasting performance on M5. We evaluate the forecastingperformance at each of the individual levels and across the entirehierarchy in . We observe that the average performance ofHAILS is significantly better than all the other baselines as well asProfHiT and M5-Leader across the hierarchy as well as in mostof the hierarchy levels. Specifically, we observe significant increaseof about 12% in performance at lower levels (L10- L12) with sparsetime-series over ProfHiT, showcasing the importance of leveragingpoisson distributions at the lower level and using the novel DCRSloss. Overall HAILS achieves best or close to best performance atall levels of the hierarchy.In terms of the performance of probabilistic forecasts (), we also observe over 40% better CRPS scores of HAILS overProfHiT and 32% over best baselines with consistently better per-formance across all levels of the hierarchy. Similarly, we observea significant 20% better performance at the lower levels of thehierarchy. 5.2.2Case Study: Demand Forecasting at Dow. Background: AtDow, hierarchical time series models are developed to forecastproduct demand and raw material price to facilitate business plan-ning. These models offer substantial value to the businesses byminimizing the cost to serve through improved planning and fore-casting, thereby enhancing customer experience and relationships.Currently, forecasts are performed by applying Microsoft AzureAuto Machine Learning (AutoML), a cloud-based service that auto-mates the selection and tuning of machine learning models. Oneof the main drawbacks of this approach is the restriction on thenumber of predictor variables that can be included in the model, resulting from poor model scalability. In addition, the relationshipsamong different layers in the hierarchy are expected to provideuseful insights on the product demand but are not accounted for inthe model training (i.e., aggregated data at a higher level of gran-ularity were provided for model training and inference and aredisaggregated to lower levels based on proportions derived fromhistorical data). Last but not the least, the lack of transparency, anduncertainty-driven risk assessment associated with the model per-formance and forecasts impose significant challenges to businessdecision-making processes.",
  ": HAILS has significantly lower WRMSSE than Dowbaseline across all levels of the hierarchy": "Impact: We developed HAILS to overcome these challenges that arecommonly existed in large-scale business planning. HAILS allevi-ates these crucial challenges: First, it efficiently scales to predict thetime-series across all levels of the hierarchy. It additionally lever-ages DCRS to optimize for distributional consistency according tounderlying hierarchical relationships. Finally, being a state-of-artprobabilistic model it provides reliable forecast distributions thatare both accurate and have dependable uncertainty measures. Thesebenefits allow HAILS to have a vastly lower RMSE across the entireforecast horizon.Historical demand is used as the criterion to identify the top coun-tries and industries. This is based on the assumption that higherdemand corresponds to higher value, and thus more potential forprofit. By improving forecast accuracy for these segments, we canoptimize our business planning and reduce costs. We summarizethe forecasting performance of HAILS, Dows AutoML baselineand other baselines in Tables 4, 5. HAILS outperforms the previousbaseline used by Dow by over 8.5% overall in RMSSE with an aver-age improvement of 26% for the last three layers which have over10% of the values zeroes (). Similarly, HAILSs CRPS score is30% better than the best baseline models with over 23% better in thelast 3 sparser levels of the hierarchy. The improvement in forecastperformance is seen consistently during testing across the year(). We also observe that HAILS is 70% faster to train than thenext best model. We also observe 44.14% average improvement inperformance for forecasts in the top seven countries and industriesidentified by magnitude of past demand (). We also visualizefew examples forecasts. We also observe that the confidence inter-vals of the forecasts closely follow the ground truth compared thethe Dow baseline ().",
  ": HAILS provides an average of 44.14% improvementover Dow Model over top 7 industries and countries": "We measure the total training time in hours for the model andbaselines in . We run the code on workstation with IntelXeon CPU with 64 cores, 128 GB RAM and a Nvidia V100 GPUwith 32GB VRAM. HAILS is more efficient than end-to-end neuralmodels like SHARQ, HierE2E and ProfHiT, finishing training inless than 42% of the total time of the second best baseline for most",
  "CONCLUSION": "HAILS is designed to solve challenges motivated by our experiencedealing with real-world large scale demand forecasting problem:scalability and modeling sparse time-series across the hierarchy.HAILS improves on ProfHiT to support sparse time-series at lowerlevels of the hierarchy, an important property of real-world demandforecasting scenario that enables it to perform 8-30% better thanprevious best baselines with consistent performance across all lev-els of the hierarchy. HAILS also outperformed the baselines by over20% in the sparse layers of the hierarchy. Our model design andtraining enables HAILS to train up to three times more efficientlythan similarly sized state-of-art models enabling effective and accu-rate real-time forecasting. Our model was successfully applied to areal-world application of demand forecasting in one of the worldslargest chemical companies and yielded significantly superior per-formance across the hierarchy. This enables significant reductionsin cost due to manufacturing planning, inventory management andfulfillment scheduling.There are other deployment challenges for HAILS that includedata collection, data cleaning, choosing the right hierarchy, ex-plainability and deployment. Collecting reliable data across thehierarchy in a large corporation is complicated by the number of systems, businesses and geographical areas and various productunits of measure that need to be standardized. Therefore, buildingsystems that can understand and leverage data quality informationto improve the robustness of the forecasts is an important prob-lem . Another important challenge is providing interpretabilityas block-box neural models are not readily accepted in the businessprocess. Developing reliable interpretability methods for hierarchi-cal forecasting is essential for successful deployment. Additionally,the hierarchy structures may change due to reasons such as re-classifications from one business grouping to another, addition ordeletion of products, etc. While we can recalculate the time-seriesvalues of the past for new hierarchy, deriving information from adynamic hierarchy structure is a novel research direction.",
  "ACKNOWLEDGEMENTS": "This paper was supported in part by The Dow Chemical Company,the NSF (Expeditions CCF-1918770, CAREER IIS-2028586, MediumIIS-1955883, Medium IIS-2106961, PIPP CCF-2200269), CDC MInDprogram, Meta faculty gifts, and funds/computing resources fromGeorgia Tech. George Athanasopoulos, Puwasala Gamakumara, Anastasios Panagiotelis, Rob JHyndman, and Mohamed Affan. 2020. Hierarchical forecasting. Macroeconomicforecasting in the era of big data: Theory and practice (2020), 689719. Souhaib Ben Taieb and Bonsoo Koo. 2019. Regularized regression for hierarchicalforecasting without unbiasedness conditions. In Proceedings of the 25th ACMSIGKDD International Conference on Knowledge Discovery & Data Mining. 13371347. Joos-Hendrik Bse, Valentin Flunkert, Jan Gasthaus, Tim Januschowski, DustinLange, David Salinas, Sebastian Schelter, Matthias Seeger, and Yuyang Wang. 2017.Probabilistic demand forecasting at scale. Proceedings of the VLDB Endowment10, 12 (2017), 16941705.",
  "Rob J Hyndman and George Athanasopoulos. 2018. Forecasting: principles andpractice. OTexts": "Arindam Jati, Vijay Ekambaram, Shaonli Pal, Brian Quanz, Wesley M Gif-ford, Pavithra Harsha, Stuart Siegel, Sumanta Mukherjee, and ChandraNarayanaswami. 2023. Hierarchical Proxy Modeling for Improved HPO in TimeSeries Forecasting. In Proceedings of the 29th ACM SIGKDD Conference on Knowl-edge Discovery and Data Mining. 891900. Harshavardhan Kamarthi, Lingkai Kong, Alexander Rodriguez, Chao Zhang,and B Aditya Prakash. 2021. When in Doubt: Neural Non-Parametric Uncer-tainty Quantification for Epidemic Forecasting. Thirty-fifth Conference on NeuralInformation Processing Systems (2021). Harshavardhan Kamarthi, Lingkai Kong, Alexander Rodriguez, Chao Zhang, andB Aditya Prakash. 2022. CAMul: Calibrated and Accurate Multi-view Time-SeriesForecasting. ACM The Web Conference (WWW) (2022). Harshavardhan Kamarthi, Lingkai Kong, Alexander Rodrguez, Chao Zhang, andB Aditya Prakash. 2023. When Rigidity Hurts: Soft Consistency Regularizationfor Probabilistic Hierarchical Time Series Forecasting. In Proceedings of the 29thACM SIGKDD Conference on Knowledge Discovery and Data Mining. 10571072.",
  "Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. 2022. M5accuracy competition: Results, findings, and conclusions. International Journalof Forecasting 38, 4 (2022), 13461364": "Syama Sundar Rangapuram, Lucien D Werner, Konstantinos Benidis, PedroMercado, Jan Gasthaus, and Tim Januschowski. 2021. End-to-End Learning ofCoherent Probabilistic Forecasts for Hierarchical Time Series. In InternationalConference on Machine Learning. PMLR, 88328843. Nicholas G Reich, Logan C Brooks, Spencer J Fox, Sasikiran Kandula, Craig JMcGowan, Evan Moore, Dave Osthus, Evan L Ray, Abhinav Tushar, Teresa KYamana, et al. 2019. A collaborative multiyear, multimodel assessment of seasonalinfluenza forecasting in the United States. Proceedings of the National Academy"
}