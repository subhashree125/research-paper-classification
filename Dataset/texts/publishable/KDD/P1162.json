{
  "Abstract": "In many randomized experiments, the treatment effect of the long-term metric (i.e. the primary outcome of interest)is often difficult or infeasible to measure. Such long-term metrics are often slow to react to changes and sufficientlynoisy they are challenging to faithfully estimate in short-horizon experiments. A common alternative is to measureseveral short-term proxy metrics in the hope they closely track the long-term metric so they can be used to effectivelyguide decision-making in the near-term. We introduce a new statistical framework to both define and construct anoptimal proxy metric for use in a homogeneous population of randomized experiments. Our procedure first reducesthe construction of an optimal proxy metric in a given experiment to a portfolio optimization problem which dependson the true latent treatment effects and noise level of experiment under consideration. We then denoise the observedtreatment effects of the long-term metric and a set of proxies in a historical corpus of randomized experiments toextract estimates of the latent treatment effects for use in the optimization problem. One key insight derived fromour approach is that the optimal proxy metric for a given experiment is not apriori fixed; rather it should depend onthe sample size (or effective noise level) of the randomized experiment for which it is deployed. To instantiate andevaluate our framework, we employ our methodology in a large corpus of randomized experiments from an industrialrecommendation system and construct proxy metrics that perform favorably relative to several baselines.",
  "Introduction": "Randomized controlled trials (RCTs) are the gold standard approach for measuring the causal effect of an intervention[Hernan and Robins, 2010]; however, designing and analyzing high-quality RCTs requires various considerations toensure scientifically robust results. For example, an experimenter must clearly define the intervention, control, andchoose a primary outcome for the study. In this work, we will assume that the intervention and control are clearlydefined, and consider the problem of choosing a good primary outcome. A common approach is to choose the primaryoutcome to be a key metric which drives downstream decision-making. Such metrics are critical components in thedecision-making pipelines of many large-scale technology companies [Chen and Fu, 2017, Rachitsky] as well as usedto guide policy decisions in economics and medicine [Athey et al., 2019, Elliott et al., 2015]. Unfortunately, directmeasurement of such a metric can be impractical or infeasible. In many cases, they are long-term outcomes observedwith a significant temporal delay, making them slow to move (i.e. insensitive) in the short term, and inherently noisy.Moreover, they may be prohibitively expensive to query.On the other hand, proxy metrics (or surrogates) that are easier to measure or faster to react are often available touse in lieu of the long-term outcome. For example in clinical settings, CD4 white-blood cell counts in blood serveas a surrogate for mortality due to AIDS [Elliott et al., 2015], while in online experimentation platforms diversity ofconsumed content serves a proxy for long-term visitation frequencies [Wang et al., 2022]. A significant literature existson designing and analyzing proxy metrics and experiments that use them as a primary outcome. One important questionaddressed by this literature is choosing (or combining) proxy metrics to be a good surrogate for measuring the effect ofthe intervention on the long-term outcome [Prentice, 1989, Hohnhold et al., 2015, Parast et al., 2017, Athey et al., 2019,Wang et al., 2022, 2023, Zhang et al., 2023]. To do so, one needs a principled reason for why the measured treatmenteffect on the proxy outcome is related to the treatment effect on the long-term outcome. Frequently, this is done bymaking causal assumptions about the relationship between the treatment, proxy outcome, and long-term outcome [see,",
  "arXiv:2309.07893v2 [stat.ME] 15 Jun 2024": "e.g., VanderWeele, 2013, Athey et al., 2019, Kallus and Mao, 2022]. However, motivated by the unique way that trialsare run in technology product applications, we take a different approach based on statistical regularity assumptions in apopulation of experiments, similar to meta-analytic approaches such as that taken by Elliott et al. .RCTs performed in technology products are typically referred to as A/B tests. They are used for a wide variety ofapplications in the technology industry, however one of the most common applications is for assessing the effect of acandidate launch of a new product feature or change on the users experience. If the results of the A/B test suggest thatthe candidate launch has a positive effect on the user experience, then it will be deployed to all users. Depending on thescale of the product and engineering team, many candidate launches requiring many A/B tests may be required on aregular basis. The results of these A/B tests on long-term outcomes and proxy metrics are logged, serving as a historyof past candidate launches that we may use to guide the choice of future proxy metrics to use for decision making.This perspective has been studied in the technology research literature previously, for example in Richardson et al. , Wang et al. , Deng et al. , to develop useful heuristics for choosing a proxy metric for use in futureA/B tests. In this work, we define a precise statistical framework for choosing a proxy metric based on this historicalA/B test data, and develop a method for optimizing a composite proxy, an affine combination of base proxy metrics,that can be used as a primary outcome for future A/B tests.The central contributions of our paper are the following: We define a new notion of optimality for a proxy metric we term proxy quality, for use in a homogeneous populationof randomized experiments. Our definition differs from the existing literature in that it phrases optimality as ensuringthe observed proxy metric closely tracks the unobserved population treatment effect on the long-term outcome (see). Conveniently, our definition also packages two important considerations for a proxy metric short-termsensitivity of the proxy metric and directional alignment with the long-term outcome into a single objective (seeEquation (2)). We show this new notion of proxy quality can be used to construct optimal proxy metrics for new A/B tests via anefficient two-step procedure. The first step reduces the construction of an optimal weighted combination of baseproxies, that maximize our definition of proxy quality, to a classic portfolio optimization problem. This optimizationproblem is a function of the latent variability of the unobservable population treatment effects and the noise levelof the new experiment under consideration (see .1.2). We then use a hierarchical model to denoise theobserved treatment effects on the proxy and long-term outcome in a historical corpus of A/B tests to extract thevariation in the unobserved population treatment effects (see .2). The variance estimates of the populationTEs are then used as plug-ins to the aforementioned optimization. We highlight the adaptivity of our proxy metric procedure to the inherent noise level of each experiment for which itwill be used. In our framework the optimal proxy metric for a given experiment is not apriori fixed. Rather it shoulddepend on the sample size (or effective noise level) of the randomized experiment for which it is deployed in orderto profitably trade-off bias from disagreement with the long-term outcome and intrinsic variance (see .1.1and ). Finally, we instantiate and evaluate our framework on a set of 307 real A/B tests from an industrial recommendationengine showing how the proxy metrics we construct can improve decision-making in a large-scale system (see).",
  "Statistical Framework": "Consider a corpus of K randomized experiments (or A/B tests) where the i {1, . . . , K}-th experiment is of samplesize ni. In each experiment, there is a specific intervention that has some population treatment effect (TE) that we denotei.1 In the experiment, we measure an estimated TE i on the subset of the population included in our experiment.Note that the entire population may be included in the experiment, but i remains a random quantity, given i, becauseof the random assignment of treatments in the experiment.To differentiate between the TE on the long-term outcome and the proxy metrics, we will attach a superscript Nas in Ni or Ni for the long-term outcome (we use N since these are sometimes referred to as north star metrics),and a superscript P as in Pi and Pi for the proxy metrics. Note that there may be multiple base proxy metrics, so",
  "This may be an average TE (ATE) or relative ATE, but we will not emphasize the differences between these two as they can be handle similarlyin our work. For our dataset we use relative ATEs": "Pi Rd. Throughout, we assume that ni is large enough, and that the experimental design is sufficiently regularsuch that conditional on (Ni , Pi ), ( Ni , Pi ) is well-approximated by a Normal distribution centered around thepopulation TE2 (due to the central limit theorem); and that the joint (within-experiment) covariance of ( Ni , Pi ),denoted as , has a good estimator, denoted as . Our discussion is agnostic to the precise estimator of the TEsand their covariances. We only require black-box access to their values. For the historical corpus of K randomizedexperiments, we assume that these triplicates of measurements {( Ni , Pi , i)}Ki=1 are available.Our goal in this paper is to revisit the proxy metric problem: the selection of short-term proxy metrics (or a weightedcombination thereof) that track the long-term outcome in a new K + 1st experiment where measurements of thelong-term outcome are unavailable, but measurements of short-term proxy metrics are. In order to develop a statisticalframework to construct proxies in a new experiment, we leverage a meta-analytic approach to model the relationshipbetween different experiments. To this end, we assume the population TEs for each experiment are drawn i.i.d. from acommon joint distribution D,",
  "D() i.i.d.(1)": "supported over Rd+1. We acknowledge this assumption is strong and not suitable for all applications. However, inour motivating application of interest studying a corpus of A/B tests from a large technology company historicalintuition and various tests do not provide significant evidence this assumption is violated. The approach of placing adistributional prior on the population ATE in similar settings of A/B testing at large-scale technology companies [Dengand Shi, 2016, Deng] as well as other meta-analytic studies of RCTs [Elliott et al., 2015, Elliott, 2023] has also beenadvocated for as a useful assumption in prior work.",
  "Methods": "With the above setting in place we first define the measure of quality of a proxy metric which relates the estimatedTE on the short-term proxies to the population TE on the long-term outcome for a new experiment. Subsequently, weshow how the relevant latent parameters contained in the definition of proxy quality can be efficiently estimated via ahierarchical model.",
  "Optimal Proxy Metrics": "In order to judge the quality of a proxy metric we first define a new notion of utility for a proxy metric. Our key insightis that in a new experiment3 whereNP D(), the observed TEs of the short-term proxies P shouldclosely track the latent TE of the long-term outcome (see ).4 This is because decisions that are intended tomove N will be made on the basis of P . Thus, we would like these quantities to be well-correlated.",
  "Proxy Quality of a Single Short-Term Metric in a New Experiment": "For simplicity, we first consider the case when the vector-valued sequence of proxies reduces to a single scalar proxy.In order to capture the intuition that the short-term estimated proxy TE, P should track the population long-termoutcome TE, N we define the proxy quality as the correlation between the aforementioned quantities. The correlationis a simple and natural measure which captures the predictive relationship between the proxy metric and long-termoutcome. Under stronger conditions in .2, we also argue that optimizing for this measure of proxy qualityminimizes the probability of a (signed) decision error or surrogate paradox.In our setting we consider the case where the estimated TEs are unbiased estimators of their underlying latentpopulation quantities so we can parameterize P = P +",
  "P P , where is an independent random zero-mean,": "2Note that in our framework, the notion of proxy quality in .1.1 and .1.2 only relies on low-order moments and doesnt explicitlyrequire Gaussianity (although does use unbiasedness) but the estimation procedure in .2 makes explicit use of this structure.3In the following since we assume all the experiments are i.i.d. we suppress index notation on this arbitary experiment drawn from D().4Note in a new experiment the estimated treatment effect on the long-term outcome N may be unavailable.",
  "+": ": In a new experiment, we view the observed TEs as being generated from their corresponding (unobserved)latent values by a noisy channel which adds independent, mean-zero experimental noise with covariance . In this newexperiment the noisy, observed long-term outcome N is inaccessible. We seek to find noisy proxy metrics whose TEsclosely track the population TEs on the long-term outcome.",
  "Var(P ).(2)": "In our setting, the definition of proxy quality decomposes the predictive relationship between the estimated proxy TEand population long-term outcome TE into latent predictive correlation corr(P , N) a property of the distributionD() and an effective inverse signal-to-noise ratio P P /Var(P ) which is also a function of the noise level of theexperiment. We now make several comments on the aforementioned quantities. The latent predictive correlation corr(P , N) tracks the alignment between the population proxy metric TE andthe population long-term outcome TE. In particular, this correlation is reflective of the intrinsic predictive quality ofa fixed proxy metric. This quantity is not easily accessible since we do not directly observe data sampled from D().We return to the issue of estimating such quantities in .2. The quantity P P /Var(P ) computes the ratio of the within-experiment noise in the estimated proxy metric TEdue to fluctuations across experimental units and treatment assignmentsto the latent variation of the populationproxy metric TE across experiments. For the former quantity we expect P P to depend on the size of the randomizedexperiment in consideration (i.e. P P 1 n, where n is the sample size of the experiment), since it is a varianceover independent treatment units. Meanwhile Var(P ) captures how easily the population proxy metric TE movesin the experiment population D(). In a (large enough) given experiment, P P is easily estimated by P P using thesample covariance estimator. Meanwhile, Var(P ) is difficult to measure directly, just like corr(N, P ). Welater show how to use a hierarchical model to estimate these parameters (see .2). Finally, it is worth notingthis ratio term in the denominator is also closely related to a formal definition of metric sensitivity which appears inthe A/B testing literature [Deng, Richardson et al., 2023]. Together the numerator and denominator in Equation (2) trade off two (often competing) desiderata into a singleobjective: the numerator favors alignment with the population TE on the long-term outcome while the denominatordownweights this by the signal-to-noise ratio of the proxy metric.5 One unique property of this proxy quality measure isthat, given a set of base proxies, the optimal single proxy is not an intrinsic property of the proxy metric or distributionof treatment effects captured by D(). Rather, it also depends on the experimental design. Specifically, it is a functionof the experiment sample size n, which will control the size of P P . This behavior represents a form of bias-variance",
  "This tradeoff between directional alignment of the proxy metric/long-term outcome and sensitivity of the proxy metric is further discussed in[Richardson et al., 2023]": "trade-off. For large sample sizes, as P P 0, Equation (2) will favor less biased metrics whose population-level TEsare aligned with the long-term outcome (i.e. the numerator is large). Meanwhile, for small sample sizes where P P islarge, Equation (2) will favor less noisy metrics with a high signal-to-noise ratio so the denominator is small.",
  "Composite Proxy Quality in a New Experiment": "The previous discussion on assessing the quality of a single proxy metric captures many of the important featuresbehind our approach. However, in practice, we are often not restricted to picking a single proxy metric to approximatethe long-term outcome. Rather, we are free to construct a composite proxy metric which is a convex combination ofthe TEs of a set of base proxy metrics to best predict the effect on the long-term outcome.In our framework, the natural extension to the vector-valued setting takes a convex combination of the proxiescorr(N, w P ) for a normalized weight vector w, instead of restricting ourselves a single proxy. However, beyondjust defining the quality of a weighted proxy metric, we can also optimize for the quality of this weighted sum of baseproxy metrics:",
  "w(Cov(P , P ) + P P )w: 1w = 1, w 0.(4)": "Essentially all considerations noted in the previous section translate to the vector-valued setting mutatis mutandis.In particular, the numerator in Equation (4) captures the alignment between the true latent weighted proxy and thepopulation long-term outcome, while the denominator downweights the numerator by the effective noise in eachparticular experiment. As before we expect P P 1 n with the sample size, n, of the experiment. Hence, the optimalweights for a given experiment will adapt to the noise level (or equivalently sample size) of the experiment run.The formulation in Equation (4) raises the question of how to efficiently compute w. Indeed, at first glance theoptimization problem as phrased in Equation (4) is non-convex. Fortunately, the objective in Equation (4) (up to aconstant pre-factor) maps exactly onto the Sharpe ratio (or reward-to-volatility ratio) maximization problem oftenencountered in portfolio optimization [Sharpe, 1966, 1998]. As is well-known in the optimization literature, the programin Equation (4) can be converted to an equivalent (convex) quadratic programming problem which can be efficientlysolved [Cornuejols and Tutuncu, 2006, .2]. We briefly detail this equivalence explicitly in Appendix B.The portfolio perspective also lends an additional interpretation to the objective in Equation (4). If we analogizeeach specific proxy metric as an asset to invest in, then Cov(N, P ) is the returns vectors of our assets, andCov(P , P )+P P is their effective covarianceso w(Cov(P , P )+P P )w captures the risk of our portfolioof proxies. Just as in portfolio optimization, where two highly-correlated assets should not be over-invested in, if twoproxy metrics are both strongly aligned with the long-term outcome, but are themselves very correlated, the objective inEquation (4) will not assign high weights to both of them.",
  "Estimation of Latent Parameters via a Hierarchical Model": "As the last piece of our framework, we finally turn to the question of obtaining estimates of the unobservable latentquantities arising in Equation (4). While P P is easily estimated from the within-experiment sample covariance P P ,the quantities Cov(N, P ), Cov(P , P ) are tied to the latent, unobservable population TEs of the proxy metricsand long-term outcome.In order to gain a handle on these quantities, we take a meta-analytic approach which combines two key pieces.First, as in our previous discussion, we require the setting described in Equation (1) that is we assume the truepopulation TEs are drawn i.i.d. from a common joint distribution. While only this assumption was needed for ourprevious discussion, we now introduce additional parametric structure in the form of an explicit generative model toallow for tractable estimation of the parameters Cov(N, P ) and Cov(P , P ). Second, we assume access to apool of homogeneous RCTs for which unbiased estimates of the TE on the short-term proxy metrics and long-term",
  "(c) Estimated TEs with raw / estimated covari-ance from hierarchical model": ": The panel visualizes the denoising effect of fitting a hierarchical model to raw TEs to uncover their latentvariation on synthetic data. We generate 1500 synthetic datapoints sampled from the model in Equation (5) with oneproxy metric. Each datapoint represents a synthetic TE measurement from a single A/B test. We use parameters withN",
  ",i [K].(7)": "We use the notation to capture the latent covariance of the joint distribution D() which we parameterize by a multivari-ate normal (i.e. MVN). So in our case, Cov(N, P ) = NP and Cov(P , P ) = P P forNP D().Moreover, for the purposes of inference we simply use the plug-in estimate which is routinely done in similarhierarchical modeling approaches [Gelman et al., 1995]. While using multivariate normality in Equation (5) is anassumption (albeit we believe reasonable in our case), it is not essential to the content of our results. Our proxyquality definition relies only on inferring low-order moments of D() for which this parametric structure is convenient.The second approximation that the noisy TEs are multivariate normal around their true latent values (Equation (6))is well-justified by the central limit theorem in our case, since the experiments we consider all have at least O(105)treatment units. Since inference in this model is not closed-form, we implement the aforementioned generative modelin the open-source probabilistic programming language NumPyro [Phan et al., 2019] to extract the latent parameters6.Additional details on the inference procedure are deferred to Appendix C. provides an example where the inference procedure is used to extract the latent population variation in asynthetically generated dataset. Although this example is synthetic (and exaggerated), empirically in our corpus weobserve many base proxy metrics with correlations to the long-term outcome of 0.6 in their experimental noise 6As an alternative to method presented, we could eschew the parametric Gaussian structure by using an estimator which uses sample-splittingwithin each RCT to estimate . We prefer to use our current approach in order to implicitly reweight by the heteroscedasticity in our observations(i.e. i) and avoid sample-splitting/cross-fitting. Additional details are provide in Appendix C.",
  "Output:w, w PK+1 (Proxy Weights and Composite Proxy for New Experiment)": "matrix . Thus, the example shows a case where the raw correlation may provide an over-optimistic estimate of theunderlying alignment between a proxy metric and long-term outcome. The denoising model we fit helps mitigate theimpact of correlated within-experiment noise in our setting. We schematically detail the end-to-end algorithm whichcomposes the denoising model fit and portfolio optimization to construct a proxy for a new A/B test in Algorithm 1.Lastly, with the generative model in Equations (5) and (6) in place we can provide an alternative interpretation of",
  "Results": "In this section, we turn to evaluating the performance of our composite proxy procedure against several baselines. Asraw proxy metrics to consider in our evaluations we use a small set of 3 hand-selected proxy metrics which capturedifferent properties domain experts believe are relevant to long-term user satisfaction in our setting. We first highlight aunique feature of our proxy procedure its adaptivity to the sample size of the experiment for which it will be applied.We then perform a comparison of our new proxy procedure against the raw proxy metrics and a baseline procedure[Richardson et al., 2023] appearing in the literature.",
  "Proxy Quality and Sample Size Dependence": "One unique feature of our procedure is its adaptivity to the noise level (or effectively sample size of the experiment);recall in Equation (4) the optimal weights will depend on the latent parameters which are inferred from the pool ofhomogeneous RCTs on which they are fit, but also the experiment noise estimate which depends on the new A/B testit is to be used for. While in practice one could recompute a proxy metric depending on the aposteriori results of eachA/B test (so is known), it is also desirable to be able to fit a proxy for each A/B test apriori, without knowledge ofits results. To do so, we found that in our application, P Picould be estimated with reasonable accuracy purely onthe basis of historical data of other A/B tests in our population of experiments by postulating a scaling of the formP Pi= P Pref /ni. Here the reference matrix P Pref can be thought of as the within experiment variance of an A/B test inthe population with one sample. The ansatz P Pi= P Pref /ni, combines two observations. The first is that the varianceof a TE estimate decays as 1ni in the number of treatment units ni, which is immediate from the independence oftreatment units. However, the second is that the constant prefactor in the variance P Pref is approximately the sameacross different A/B tests in our corpus. The reference matrix P Pref can then be estimated as a weighted average of ifrom the corpus. Additional details and verification of these hypotheses are provided in Appendix A. The upshot of thisapproach is that the computation of the optimal weights in Equation (4) for a new A/B test can then be done using onlythe sample size (ni) of this new experiment (i.e. before the new experiment is run). 7This condition may not be true in all applications but is approximately satisfied in our dataset with all metrics having the ratio of their globalmean to global standard deviation being bounded by 0.1 but often being even less. : The optimal weighting dependence on sample size for our new composite proxy, represents a bias-variancetrade-off. For large sample sizes the weighting favors potentially noisier metrics that are more aligned with the longterm outcome. However, for smaller sample sizes the optimal weighting backs off to metrics which are less noisy butalso less aligned to the long term outcome. To understand the dependence of our new composite proxies weighting on the experiment sample size, we fit thelatent parameters (from the hierarchical model in Equations (5) and (6)) and P Pref on the entire corpus for the resultsin . We then use the scaling = P Pref /n to estimate the optimal weights of our new composite proxy fromEquation (4) for different sample sizes n for a hypothetic new A/B test. shows how as the sample size increasesthe new composite proxy smoothly increases its weighting on raw metrics which are noisier but more strongly correlatedwith the long term outcome. Moreover, while the Auxiliary Metric 3 is an intuitively reasonable metric, its value asdetermined by the measure of proxy quality is dominated by a mixture of the other two components.",
  "New Composite Proxy0.1810.6660.302Baseline Composite Proxy0.1820.6110.279Auxiliary Metric 10.0620.6110.174Auxiliary Metric 20.3680.2220.258Auxiliary Metric 30.1660.1040.030": ": Comparison of our new composite proxy against a baseline method, and their constituent base proxies alongstseveral criterion which are computed on a held-out evaluation. Our new composite proxy performs favorably across allmeasures notably achieving the highest proxy score and proxy quality amongst all considered. All evaluation criteriaare bounded in and for each higher is better. The primary difficulty of this evaluation is that in TE estimation there is the lack of ground-truth labels of thetreatment effect (i.e. in our framework the population latent TEs such as N are never observed). However, in oursetting we do have access to a large corpus of 307 A/B tests as noted earlier. Hence, we use held-out/cross-validated evaluations of certain criterion which depend on the noisy metrics aggregated over an evaluation set, to gauge theperformance of proxy metrics fit on a training set.We consider several relevant criteria for performance which have been used in the literature. Two importantmeasures which appear in [Richardson et al., 2023] are the proxy score and sensitivity. To define the criteria, recallthat a TE metric is often used to make a downstream decision by thresholding its t-statistic, tstat =",
  "Var( ) as": "tstat > 2 + (positive), 2 < tstat < 2 0 (neutral), and tstat < 2 (negative). Given a corpus of A/Btests we can then compute the decisions induced by a short-term proxy metric, the decisions induced by the long-termoutcome and check the number of A/B tests for which they align. After normalization, the number of detections (bothmetrics decisions are positive or negative) minus the number of mistakes (one metric decision is positive while the otheris negative) defines the proxy score. Similarly, for a short-term proxy metric we can compute its sensitivity which isthe number of times it triggers a statistically significant decision by being positive or negative. Loosely speaking, thesetwo criterion function like the notions of precision and recall in information retrieval. Ideally, a short-term metric wouldmaximize both quantities by being sensitive and triggering often (so as to not miss any A/B tests where the TE for thelong-term outcome is significant) but not over triggering and leading to many false positives (or negatives). Additionaldetails on these metrics are provided in the Appendix D8. As another measure of performance, we also compute andreport our definition of our composite proxy quality for a given composite proxys learned weights.We use the same set of raw proxy metrics as before in our evaluation. As a procedure to compare our methodologyagainst, we use the baseline of optimizing the convex combination of these 3 metrics to optimize the aforementionedproxy score which is detailed in [Richardson et al., 2023]. In each case we use stratified 4-fold cross-validation (CV) tocompute the weights for each procedure on a training subset of the corpus and evaluate the metrics on the held-out setby computing the aforementioned evaluation scores. The baseline proxy method and base proxy metrics each learn afixed set of weights9 depending on the training fold, which is applied to each A/B test in the evaluation fold. For ourprocedure we use the ansatz P Pi= P Pref /ni mentioned in the previous setting to calculate P Pref as a simple weightedaverage from data only in the training fold of our CV split. The optimal proxy metric for each A/B test in the test foldcan then be refit using only the sample size ni of that A/B test. This strategy has the additional benefit of enforcingstrict separation of the data in train/test set folds in our CV split.Results for our cross-validated evaluation are displayed in across our corpus of 307 historical A/B testswhich come from a real industrial recommendation engine. Note that both our new composite proxy and the baselinecomposite proxy improve significantly in the proxy score and proxy quality over the raw metrics without sacrificingunduly on sensitivity relative to Auxiliary Metric 2. Moreover, the new composite proxy achieves not only the highestproxy quality but also proxy score despite not explicitly optimizing for proxy score on the training set. We believe thismay be a feature of our new composite proxy whose weights are adaptive to the size of each experiment in the A/B test(which vary in our corpus from approximately O(106) to O(108) in size).",
  "Conclusion": "We have presented a framework for both defining and constructing optimal composite proxy metrics, which are used toapproximate the decisions induced by a difficult-to-measure long-term metric. One key insight from our frameworkis that the optimal proxy for a given experiment should depend on the noise level (or equivalently the size) of thatexperiment. In our work, the first component of our procedure reduces the composite proxy selection problem to aportfolio optimization relating the unobserved long-term/north star TE and observed TE. This does not explicitly usethe population distribution assumption in Equation (1), although it relies on the unobservable treatment effects, andwe believe this is a valuable and natural framing of the proxy selection problem. Equation (1) is implicitly needed toidentify the latent covariance parameters between a new future RCT and past RCTs, and accordingly for estimation ofthe unobserved latents from the corpus of RCTs. Due to the lack of ground truth (true treatment effects are unobservedin RCTs) Equation (1) is important for our meta-analysis, although we acknowledge that it is not suitable for allapplications. One of the limitations of our work is that highly non-stationary settings (where Equation (1) is not a goodapproximation) may not be well addressed by our second-stage denoising procedure. Accordingly, this assumptionshould be probed by model-fit diagnostics and domain-specific intuition. 8It is worth noting these measures are still imperfect in the sense that comparisons are made against the decisions induced by the noisy estimate ofthe long-term outcome TE not the latent population long-term outcome TE.9the base proxy metrics simply place all their weight on themselves. An interesting direction for future work is to further relax this assumption through different structural or causalassumptions which are application-dependent in the second part of the procedure (while still maintaining the firstportfolio optimization component). Here, more flexible modeling of the joint latent effect distribution to accommodatemore structured latent effects could be useful. Extending our framework to handle the construction of nonlinearcomposite proxy metrics in situations where higher-order interactions between the north star and short-term proxies areimportant, is also an interesting avenue for further research. As an additional point of exploration, understanding howour approach might generalize to the problem of heterogeneous treatment effect estimation to provide contextualdecision-making power would also be valuable.",
  "AWithin-Experiment Covariance Scaling": "As we note before, one interesting feature of our composite proxy quality procedure is its dependence on the noiselevel of of the randomized experiment it will be applied too. Recall in Equation (4) the optimal weights will dependnot only on the latent parameters which are instrinsic properties of D(), but the experiment noise estimate whichdepends on the particular A/B test too which it is applied.While in practice, one could recompute a composite proxy metric after an A/B test is run (so is known), in manyapplications it is also desirable to be able to fit weights for a composite proxy metric before each A/B test is run. Inorder to do this we found that we could build a simple predictive model for the experimental noise level P Piin a givenexperiment on the basis of historical data of other A/B tests in our population of experiments. We did so by making anansatz of the form P Pi= P Pref /ni, where P Pref can be thought of as the within experiment variance of an A/B testin the population with one sample. This ansatz follows from two facts. The first is that the variance of a TE estimatedecays as 1ni in the number of treatment units ni, which is immediate from the independence of treatment units.However, the second that the constant prefactor in the variance P Pref is approximately the same across different A/Btests is an empirical observation due to underlying homogeneity in the population of A/B tests. This approximatehomogeneity is evidenced in .",
  "(b) Variance of A/B tests for Auxiliary Metric 2": ": Both displays show the within-experiment marginal sample variance (blue dots) for two different metricscomputed across 307 different A/B tests and their corresponding power-law fit (red line). Despite the underlying A/Btests being different, we found that the variance were reasonably well modeled by a single inverse-power law with thesame constant prefactor over the entire population.",
  "i=1ini i(9)": "for some convex combination of weights i. While we can use an equal weighting scheme where i =1K , since thesample variance estimates i themselves are noisy, we instead use a precision weighted combination of them to reducevariance by taking i ni.With this estimate ref in hand, for a new K + 1st A/B test with sample size nK+1 we can approximates itswithin-experiment covariance as,",
  "CInference in Hierarchical Model": "In order to extract estimates of the latent parameter we perform full Bayesian inference over the generative model inEquation (7) using Numpyro Phan et al. which uses the NUTS sampler to perform MCMC on the posterior. Wefound Bayesian inference to be more stable then estimating the MLE of the model. We augmented the generative modelin Equation (7) with the weak priors:",
  "= s C s": "where we use the operator to denote coordinatewise broadcasted multiplication. Here the vector-valued parametersmeanscale, and devscale are set to match the overall scales of the raw mean and raw covariance of the corpous A/Btests. We found the overall inferences to be robust to the choice of scales in the Half-Cauchy prior on the pooledvariance parameter and normal prior mean, which are both weakly-informative. [Gelman, 2006] and [Polson and Scott,2012] both argue for the use of the Half-Cauchy prior for the top-level scale parameter in hierarchical linear modelsas opposed to the more traditional use of the Inverse-Wishart prior on both empirical and theoretical grounds. Thechoice of the LKJ prior with concentration parameter set to 1 is essentially a uniform prior over the space of correlationmatrices [Gelman et al., 1995, Lewandowski et al., 2009].Inference in this model was performed using the default configuration of the NUTS sampler in Numpyro [Phanet al., 2019]. We also found it useful to initialize the parameters (N, P ) and to the scales of the raw mean andraw covariance of the corpus A/B tests. We diagnosed convergence and mixing of the NUTS sampler using standarddiagnostics such as the r-hat statistic [Gelman et al., 1995]. In all our experiments we found the sampler mixedefficiently and we achieved a perfect r-hat statistic for all parameters of 1.0. For each MCMC run we generated 10000burn-in samples and 50000 MCMC samples for 4 parallel chains. We used the posterior means of the samples to extractestimates of for use in our proxy quality score.As noted in the main text an alternative to using the generative model presented here which eschews the Gaussianparametric structure (but of course relies on the Equation (1)) is to use a sample-splitting estimator within each RCT.Our given procedure is agnostic to the details of TE estimation so long as each estimate is unbiased. However for thepresent discussion, assume as before, we have a corpus of RCTs with true TEs satisfying Equation (1),",
  "D() i.i.d., i {1, . . . , n},": "with mean and covariance vectors and . Further assume in each RCT we have two unbiased estimates for the TEsatisfying i,1 = i + 1/2 i,1 and i,2 = i + 1/2 i,2 with mutually independent mean-zero observationnoise i,1, i,2. Such estimators can easily be obtained by randomly splitting the units in treatment and control groups in the RCT into two disjoint subsets and computing an unbiased TE estimate (such as the difference-of-means estimator)on each subset. Since they are from same RCT they will provide unbiased estimates of the same unobserved TE i.So it follows that E[ i,1 i,2] = + for each i {1, . . . , n}, due to Equation (1) and the independence of theobservation noise. Note the expectation is taken over the observation noise and latent randomness. Finally, averagingover the corpus1",
  "nni=1 i,1)( i,2 1": "nni=1 i,2) then provides an unbiased estimate of .The reduction in efficiency due to sample-splitting for this estimate within each RCT can also be partially mitigatedthrough cross-fitting techniques [Chernozhukov et al., 2018] or a jackknife approach as explored in Bibaut et al. [2024,.2]. In our work we find the hierarchical modeling approach to be natural as it incorporates the observedheteroscedasticity in observation noise across RCTs which vary significantly in size in our corpus.",
  "Var( ) as tstat > 2 + (positive), 2 < tstat < 2 0 (neutral),": "and tstat < 2 (negative). The formal definitions of proxy score and sensitivity are most easily defined in thecontext of a contingency table visualized in which takes these decisions as inputs. The contingency tabletabulates the decisions induced by a particular observed short term proxy metric and the observed long-term north starmetric jointly over 554 A/B tests.",
  ": A synthetic contingency table which captures the alignment of the decisions induced by the t-statistics of theTEs of the north star metric and a proxy metric": "The green cells in represent cases where the proxy and long-term north star are both statistically significantand move in the same direction (i.e. Detections). The red cells in again represent cases where the proxy andlong-term north star are both statistically significant, but where proxy and long-term north star are misaligned (i.e.Mistakes). The remaining cells correspond to cases where at least one of the metrics is not statistically significant. Therelative importance of these cells is more ambiguous.In this setting, the sensitivity can be defined as:",
  "Num. expts. long-term north star is significant": "The denominator here can be obtained by summing over the first column and last column. The sensitivity metriccaptures the ability of a metric to detect a statistically significant effect which inherently takes into account its inherentmoveability and susceptibility to experimental noise. Given that north star metrics are often noisy and slow to react inthe short-term the goal of a proxy is to be sensitive.The proxy score rewards metrics that are both sensitive and directionally aligned with the north star. Sensitivemetrics need only populate the first and third rows of the contingency table. However, metrics in the first and third rowscan only increase the proxy score if they are in the same direction as the long-term north star. A similar score, calledLabel Agreement, has been used by Dmitriev and Wu . It is worth noting, these measures are still imperfect inthe sense that comparisons are made against the decisions induced by the noisy estimate of the long-term outcomeTE not the latent population long-term outcome TE. This is further complicated by the fact that we empirically findthat experimental noise in the A/B tests is correlated between short-term proxies and the long-term outcome (i.e. thephenomena detailed in ). In fact, this phenomena provides partial motivation for our definition of denoisedproxy quality. Susan Athey, Raj Chetty, Guido W Imbens, and Hyunseung Kang. The surrogate index: Combining short-term proxiesto estimate long-term treatment effects more rapidly and precisely. Working Paper 26463, National Bureau ofEconomic Research, November 2019. URL",
  "A. Charnes and W. W. Cooper. Programming with linear fractional functionals. Naval Research Logistics Quarterly, 9(3-4):181186, 1962. doi: URL": "Albert C. Chen and Xin Fu. Data + intuition: A hybrid approach to developing product north star metrics. InProceedings of the 26th International Conference on World Wide Web Companion, WWW 17 Companion, page617625, Republic and Canton of Geneva, CHE, 2017. International World Wide Web Conferences SteeringCommittee. ISBN 9781450349147. doi: 10.1145/3041021.3054199. URL Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and JamesRobins. Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal, 21(1):C1C68, 01 2018. ISSN 1368-4221. doi: 10.1111/ectj.12097. URL",
  "Alex Deng. Metric Sensitivity Decomposition. Causal Inference and Its Applications in Online Industry. #metric-sensitivity-decomposition. [Online; accessed 21-December-2022]": "Alex Deng and Xiaolin Shi. Data-driven metric development for online controlled experiments: Seven lessons learned.In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,pages 7786, 2016. Alex Deng, Michelle Du, Anna Matlin, and Qing Zhang. Variance reduction using in-experiment data: Efficientand targeted online measurement for sparse and delayed outcomes. In Proceedings of the 29th ACM SIGKDDConference on Knowledge Discovery and Data Mining, KDD 23, page 39373946, New York, NY, USA, 2023.Association for Computing Machinery. ISBN 9798400701030. doi: 10.1145/3580305.3599928. URL"
}