{
  "ABSTRACT": "Multivariate time series forecasting (MTSF) is crucial for decision-making to precisely forecast the future values/trends, based on thecomplex relationships identified from historical observations ofmultiple sequences. Recently, Spatial-Temporal Graph Neural Net-works (STGNNs) have gradually become the theme of MTSF modelas their powerful capability in mining spatial-temporal dependen-cies, but almost of them heavily rely on the assumption of historicaldata integrity. In reality, due to factors such as data collector failuresand time-consuming repairment, it is extremely challenging to col-lect the whole historical observations without missing any variable.In this case, STGNNs can only utilize a subset of normal variablesand easily suffer from the incorrect spatial-temporal dependencymodeling issue, resulting in the degradation of their forecastingperformance. To address the problem, in this paper, we propose anovel Graph Interpolation Attention Recursive Network (namedGinAR) to precisely model the spatial-temporal dependencies overthe limited collected data for forecasting. In GinAR, it consists oftwo key components, that is, interpolation attention and adaptivegraph convolution to take place of the fully connected layer of sim-ple recursive units, and thus are capable of recovering all missingvariables and reconstructing the correct spatial-temporal depen-dencies for recursively modeling of multivariate time series data,respectively. Extensive experiments conducted on five real-worlddatasets demonstrate that GinAR outperforms 11 SOTA baselines,",
  "Fei Wang and Yongjun Xu are the corresponding authors": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from acronym XX, June 0305, 2018, Woodstock, NY 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-1-4503-XXXX-X/18/06",
  "Multivariate time series forecasting, Variable missing, Adaptivegraph convolution, Interpolation attention, Graph InterpolationAttention Recursive Network": "ACM Reference Format:Chengqing Yu, Fei Wang, Zezhi Shao, Tangwen Qian, Zhao Zhang, WeiWei, and Yongjun Xu. 2018. GinAR: An End-To-End Multivariate TimeSeries Forecasting Model Suitable for Variable Missing. In Proceedings ofMake sure to enter the correct conference title from your rights confirmationemai (Conference acronym XX). ACM, New York, NY, USA, 12 pages.",
  "INTRODUCTION": "Multivariate time series forecasting (MTSF) is widely used in prac-tice, such as transportation , environment and others .It predicts future values of multiple interlinked time series byusing their historical observations, and contributes to decision-making . Indeed, multivariate time series (MTS) can beformalized as a kind of classical spatial-temporal graph data ,such as traffic flow , each variable of which is collected inchronological order, using a sensor deployed at an independentposition. Naturally, they usually have two key factors, temporaldependency and spatial correlation . The former charac-terizes complex patterns (e.g., causal relationships) of instances inchronological order, and the later depicts the differences of timeseries corrected each other in spatial dimension. Therefore, effec-tively mining spatial-temporal dependencies is crucial for MTSFto precisely predict future values of the time series, or to betterunderstand of how they interact .",
  "(b) MTSF with missing values": ": The principle and examples of multivariate timeseries forecasting with variable missing. V1 to V5 representdifferent variables. Compared to the other two tasks, ourtask can only use historical observations of certain variablesto predict the future values of all variables. The forecastingperformance of TGCN declines as the missing rate increases. Recently, Spatial-Temporal Graph Neural Networks (STGNNs)combine the sequence model and graph convolution (GCN) to cap-ture spatial-temporal dependencies of MTS and achieve significantprogress in MTSF , but their superior performances heavily relyon the data quantity . Since the time series data in practice isalways incomplete, it is very challenging to obtain whole historicalobservations of all variables for accurate forecasting . To makethings worse, the data from some variables may be even unavailablefor a long time under certain conditions . We can take a classicalMTS application (i.e., air quality forecasting) as an example, the datacollectors may easily work anomaly owing to some unforeseen fac-tors (e.g., horrible weather) . Because equipment maintenanceusually takes days or even months, corresponding data collectorsonly output outliers for a long time . Thus, STGNNs need toaddress a problem, namely, whole history observations missingof some variables. This means that STGNNs only achieve MTSFusing the remaining normal variables (shown in (c)), whichseverely limits their performance. To alleviate this problem, someworks only predict values of remaining normal variables bydiscarding all missing variables. However, if missing variables arekey samples (e.g., important locations like hub nodes), the inabilityto predict their values will profoundly affect decision-making .The above phenomenon shows that MTSF faces a significantpractical challenge: how to forecast MTS when missing partof variables? By rethinking the characteristics of STGNNs andthis task, the main problem is that STGNNs easily capture incor-rect spatial-temporal dependencies during the modeling process,resulting in error accumulation and degraded forecasting perfor-mance. On the one hand, since each missing variable is usually astraight-line sequence composed of outliers, the sequence modelin STGNNs cannot mine any valuable pattern and informa-tion, resulting in incorrect temporal dependencies. On the other hand, existing STGNNs need to use historical observa-tions from all variables to construct spatial correlations. Becausewhole history observations of some variables are missing, existingSTGNNs cannot establish spatial correlations between missing andnormal variables, leading to incorrect spatial correlations. In thiscase, as the missing rate increases, the above phenomena becomemore serious, leading to a significant decline in the performanceof STGNNs. For example, a classic STGNN model, temporal graphconvolutional network (TGCN) , is used for further analysiswhen given different missing rates on PEMS04. (d) showsthat its performance deteriorates while increasing the missing rate.At present, an intuitive strategy for addressing this challenge isto combine imputation and forecasting methods and propose two-stage models . However, classic imputation methods primarily rely on the context information of time series to recovermissing values. When history observations from some variables areunavailable for a long time, these methods cannot achieve reliablerecovery effects since the missing variables do not have any normalvalue in the temporal dimension. In addition to the above classicalmethods, existing mainstream imputation methods combinethe context information and spatial correlations of MTS for generat-ing plausible missing values. However, they also have two problems:(1) Components that use context information in these imputationmethods also introduce incorrect temporal dependencies, limitingthe effectiveness of data recovery and leading to error accumula-tion . (2) these imputation methods mainly rely on fixed spatialcorrelations (such as road network structure) to establish correspon-dences between missing variables and normal variables . Whenthe missing rate is significant, they cannot fully use all normalvariables to recover missing variables, resulting in the ineffectiverecovery of missing variables that do not correspond with nor-mal variables . In general, due to the introduction of incorrecttemporal dependencies and the lack of sufficient correspondencesbetween missing variables and normal variables, two-stage modelscannot work well in MTSF with variable missing.To solve the above problems and realize MTSF with variable miss-ing, forecasting models need to fully utilize historical observationsof all normal variables to correct spatial-temporal dependenciesduring the modeling process. To this end, we propose an end-to-end framework called Graph Interpolation Attention RecursiveNetwork (GinAR). Specifically, we use simple recursive units (SRU)based on the RNN framework as the backbone and propose two keycomponents (interpolation attention (IA) and adaptive graph con-volution (AGCN)) to replace all fully connected layers in SRU. Thisis done to realize end-to-end forecasting while correcting spatial-temporal dependencies. On the one hand, during the process ofrecursive modeling, for data at each time steps, IA first generatescorrespondences between normal variables and missing variables,then uses attention to restore all missing variables to plausible rep-resentations. In this way, the sequence model avoids directly miningmissing variables that do not have any valuable patterns, therebycorrecting temporal dependencies. On the other hand, for repre-sentations processed by IA, we use AGCN to reconstruct spatialcorrelations between all variables. Since all missing variables arerecovered, AGCN can more accurately utilize their representationsto generate a more reliable graph structure and obtain more accu-rate spatial correlations. In this way, GinAR mines more accurate",
  "GinAR: An End-To-End Multivariate Time Series Forecasting Model Suitable for Variable MissingConference acronym XX, June 0305, 2018, Woodstock, NY": "spatial-temporal dependencies in the process of recursive modelingand effectively avoids the error accumulation problem. Thus, Gi-nAR can implement MTSF with variable missing more accurately.The main contributions of this paper are as follows: To the best of our knowledge, this is the first work thatchallenges to achieve MTSF with variable missing. The pro-posed end-to-end framework can address the problem oferror accumulation in the modeling process. To achieve this challenging task, we carefully design GraphInterpolation Attention Recursive Network, which containstwo key components (interpolation attention and adaptivegraph convolution). We use above components to replaceall FC layers in SRU and propose the GinAR cell, aiming tocorrect spatial-temporal dependencies during the process ofrecursive modeling. We design experiments on five real-world datasets. Resultsshow that GinAR can outperform 11 baselines on all datasets.Even when 90% of variables are missing, it can still accuratelypredict the future values of all variables.",
  "RELATED WORKS2.1Spatial-Temporal Forecasting Method": "STGNNs combine the advantages of GCN and sequence mod-els to fully mine spatial-temporal dependencies of MTS, andfurther improve the ability of spatial-temporal forecasting . Li et al. combine gated recursive unit (GRU) and GCNto propose the diffused convolutional recurrent neural network(DCRNN) and realize MTSF. Wu et al. propose the graphwavenet (GWNET) by combining temporal convolutional network(TCN) and GCN. Compared with traditional methods, above twomodels achieves excellent results. However, these methods ignorehidden spatial correlations between variables, which limits theireffectiveness . To further improve the ability of STGNN tomine spatial correlations, graph learning has been widely stud-ied . Zheng et al. design a spatial attention mechanism tolearn attention scores by considering traffic features and variableembeddings in the graph structure. Shang et al. use histori-cal observations of all variables to learn the discrete probabilitygraph structure. Shao et al. propose decoupled spatial-temporalframework and dynamic graph learning to explore spatial-temporaldependencies between variables. Although STGNNs have made sig-nificant progress in MTSF, they need to use the variable features orprior knowledge to mine spatial-temporal dependencies .However, in MTSF with variable missing, the graph structure basedon prior knowledge and the graph learning based on variable fea-tures are affected by missing variables, which leads to inaccuratemodeling of spatial correlations .",
  "Imputation Method": "Existing imputation methods include classical models and deeplearning-based models . Compared with traditional mod-els, deep learning can analyze hidden correlations between missingand normal data and improve performance . Wu et al. combine the matrix transformation with CNN to realize the miss-ing data imputation, but it ignores correlations between different variables, limiting its performance. Marisca et al. combine cross-attention and temporal attention to achieve the recovery of missingdata, but they do not take full advantage of the spatial correlationsbetween variables, which leads to inadequate data recovery. In ad-dition to the above methods, GNN-based methods combineGCN and sequence models to analyze spatial-temporal dependen-cies between missing data and normal data, and further recover allmissing data . Wu et al. propose inductive graph neuralnetwork to recover missing data. Compared with classical methods,the proposed model has better performance. Chen et al. usethe adaptive graph recursive network to realize the imputation ofmissing data. Experiments show that the framework combininggraph convolution with recurrent neural networks can better usetemporal information and spatial correlation to recover missingdata. Although imputation methods can recover missing data andimprove the performance of forecasting models, they often sufferfrom several problems: (1) Classical imputation methods need toreconstruct both missing data and normal data, resulting in the lossof effective information. (2) Existing imputation methods require full use of temporal information to recover missing data.When the data from some variables are unavailable for a long time,existing methods introduce incorrect temporal dependencies, re-sulting in limited recovery performance .",
  "METHODOLOGY3.1Preliminaries": "Dependency graph. In multivariate time series, the change ofeach time series depends not only on itself but also on other timeseries. Such a dependency can be captured by the dependency graph = (, ). is the set of variables, and | | = . Each variablecorresponds to a time series. is the set of edges. The dependencygraph can be represented by an adjacency matrix: .Multivariate time series forecasting. Given a historical ob-servation tensor from time slices in history, themodel can predict the value of the nearest time stepsin the future. is the number of features. The goal of MTSF is toconstruct a mapping function between and .Multivariate time series forecasting with variable missing.Compared with MTSF, the main difference of this task is that thereare some variables with whole history data missing in historicalobservations . Thus, we mask variables randomlyfrom variables of the historical observation . Thevalues of these variables are treated as 0, i.e. missing values anda new input feature is obtained. The core goal ofthis task is to construct a mapping function between input and output .",
  "Overall Framework of GinAR": "The framework of GinAR is shown in , which uses multipleGinAR layers as the encoder and MLP as the decoder. The GinARlayer adopts the idea of recursive modeling, and its core structure isthe GinAR cell. By transmitting input features with variable missingto GinAR, it can predict future values of all variables. Next, webriefly introduce the design motivation of GinAR and the functionof its components.",
  "Input": "c12 c1H-1 h11 h12 h1H GinARlayer + (b) Interpolation attention (c) AGCN (a) GinAR : (a) The overall framework of GinAR. The GinAR layer adopts the RNN-based sequence framework and encodeshistorical observations of MTS with variable missing. The MLP-based decoder is used to predict future values of all variables.(b) The specific structure of the interpolation attention. (c) The specific structure of the adaptive graph convolution. Firstly, we intuitively discuss the design idea for the GinAR cell.Specifically, we use IA and AGCN to replace all fully connectedlayers in SRU. IA can use normal variables to restore the missingvariables to plausible representations, which can help the sequencemodel to better mine temporal dependencies of missing variables.Furthermore, for all variables processed by IA, their spatial cor-relations cannot be determined by a predefined graph based onprior knowledge. Therefore, we use AGCN, which introduces graphlearning, to reconstruct spatial correlations between all variables.Then, we briefly discuss the encoder, which uses the recursivemodeling framework. Specifically, at each time step , the inputfeatures of the current moment and the cell state 1of the previous moment are transmitted to the GinAR cell. Then, theGinAR cell outputs the cell state for the next cell and obtains thehidden feature . In this way, the GinAR layer utilizes the GinARcell to restore missing variables and reconstruct spatial correlations,while simultaneously capturing temporal dependencies throughthe recursive modeling framework. Besides, due to the introductionof skip connections in the GinAR cell, stacking multiple GinARlayers can capture deeper hidden information.Finally, we discuss the decoder and the forecasting process. Animportant step in the forecasting process is to properly filter thehidden features obtained by the encoder. On the one hand, since theencoder takes the form of recursive modeling, the last hidden stateof each GinAR layer contains all the information from the historicalobservation . On the other hand, due to the introduction of skipconnections, the hidden features obtained by each GinAR layercontains different information . Therefore, we concatenate thelast hidden state of all GinAR layers and use the concatenated tensoras the input to the decoder. Besides, we use the MLP, which is basedon the direct multi-step (DMS) forecasting strategy , to predictfuture changes for all nodes. Compared with decoders based onauto-regressive or iterated multi-step (IMS) forecasting, theproposed method can solve the problem of error accumulation andimprove the forecasting accuracy.",
  "Interpolation Attention": "For each missing variable, interpolation attention needs to select thenormal variables for induction and give the corresponding weightfor the selected normal variables. Thus, it contains two main steps:(1) It first generates correspondences between missing variables andnormal variables. (2) Based on above correspondences, attentionis used to realize the induction of missing variables. The mainschematic diagram of interpolation attention is shown in .The specific modeling steps of IA are shown below:Step 1: First, we need to generate correspondences betweenmissing variables and normal variables. Specifically, we initialize adiagonal matrix and randomly initialize two variable-embedding matrices 1 and 2 . The value ofvariable embedding matrix can be iterated continuously duringnetwork training. Based on following formulas, correspondencesbetween missing variables and normal variables can be obtained:",
  "= ( + softmax(ReLU(12)),(1)": "where, softmax() is the activation function. ReLU() is the activa-tion function. is a two-dimensional matrix. When thevalue of row and column in the is greater than 0, it meansthat there is a correlation between the variable and the variable. In other words, the interpolation attention can use the normalvariable to recover the missing variable . Based on the abovevariable correlation matrix , we can obtain the set ofnormal variables () associated with the missing variable .Step 2: Next, the missing variables are recovered by usingattention mechanism and other associated normal variables (). The attention coefficient between the missing variable and normal variable () can be calculated as follows:",
  "Adaptive Graph Convolution": "By introducing prior knowledge to define an adjacency matrix ,the predefined graph can help models establish a basic spatial cor-relation. However, for MTSF with missing variables, the predefinedgraph cannot adequately model the spatial correlation of all vari-ables due to a large number of missing variables. To this end, wepropose the data-based adaptive graph convolution, which consistsof the predefined graph and the adaptive graph.Predefined graph: In this paper, distance is used to constructadjacency matrix for traffic data with road network information.For the data without road network information, the Pearson corre-lation coefficient is used to form the adjacency matrix . Thepredefined graph for the graph convolution networkis obtained by the following formula:",
  "= ( + 1/21/2),(4)": "where, represents the diagonal matrix with value 1. is the degree matrix of .Adaptive graph: It needs to initialize a diagonal matrix of value 1 and randomly initialize a variable-embedded ma-trix . The value of variable embedding matrix can beiterated continuously during neural network training. Then, basedon the variable representation obtained by the in-terpolation attention and the variable-embedded matrix ,the new variable embedding are obtained.",
  "= ( + softmax(GeLU( )),(6)": "where, represents the transpose of .Adaptive graph convolution: Based on the above formulas, thepredefined graph and the adaptive graph can be obtained, whichcan reflect the spatial correlation of all variables from differentperspectives. Then, we combine the adaptive graph convolution andlayer normalization to fuse these graph information. The formulaof the adaptive graph convolution is given as follows:",
  "GinAR": "The main idea of GinAR is to integrate the proposed interpolationattention and adaptive graph convolution into the simple recursiveunits. Next, we introduce the composition of the GinAR cell andthe overall modeling process of GinAR in detail.GinAR cell: The GinAR cell is the most basic component ofGinAR. Specifically, we introduce IA into the simple recursive unitcell to recover missing variables. Besides, we use the AGCN toreplace all full connected layers in the SRU cell, enhancing theability to correct spatial-temporal dependencies. The formula foreach GinAR cell is given as follows:",
  ") + 1,(11)": "= ELU( ) + (1 ) ,(12)where, stands for reset gate. stands for forget gate. rep-resents the cell state of the current GinAR cell. is the hiddenstate of the current GinAR cell. stands for the Hadamard product.GeLU() and ELU() are activation functions. () stands for theinterpolation attention.GinAR: The main components of GinAR include GinAR layersand an MLP-based decoder. Each GinAR layer contains multipleGinAR cells. The modeling process of GinAR is given as follows:Step 1: The original input feature is preprocessedand the input feature for modeling is obtained. Thevalues of the variables in the variables of the input feature is 0.",
  "Conference acronym XX, June 0305, 2018, Woodstock, NYYu et al": "where, is the length of historical observation. is the numberof variables. is the length of future forecasting results. standsfor embedding size.Step 2: is passed to the first GinAR layer. Each GinAR layercontains GinAR cells, which are used to model 1 to .Step 3: Initialize a cell state 0. 1 and 0 are passed to the firstGinAR cell in the GinAR layer. Based on the calculation formula ofGinAR cell, the hidden state 11 of the current cell and the cell state1 are obtained. 1 and 2 are passed to the next GinAR cell.Step 4: Repeat Step 3 to obtain hidden states of all GinAR cellsin the first GinAR layer. These hidden states 1 are used as theinput features to the next GinAR layer.",
  "Datasets. Five real-world datasets are selected to conduct compar-ative experiments, including two traffic speed datasets(METR-LAand PEMS-BAY)1, two traffic flow datasets (PEMS04 and PEMS08)2": "and an air quality dataset (China AQI)3.Baselines. In order to fully compare and analyze the perfor-mance of the proposed GinAR, eleven existing SOTA methods areselected as the main baselines, which include forecasting models(MegaCRN , DSformer and STID ), and forecasting mod-els with data recovery components (LGnet , TriD-MAE , GC-VRNN , and BiTGraph ). Besides, we design two-phase mod-els (DCRNN + GPT4TS , DFDGCN + TimesNet and MTGNN + GRIN , FourierGNN +GATGPT) asadditional baselines to further demonstrate GinARs effect.Setting. shows the main hyperparameters of the pro-posed model. We design experiments from the following aspects:(1) Our code is available at this link4 . (2) All datasets are uniformlydivided into training sets, validation sets and test sets according tothe ratio in the reference . (3) We set the history/future lengthbased on the existing work . The history length and futurelength of GinAR are both 12. Metrics are the average of 12-stepforecasting results. (4) We randomly set mask variables accordingto the ratio of 25%, 50%, 75% and 90%. Values of the masked variableare uniformly treated as 0. Besides, the experiment was repeatedwith 5 different random seeds for each missing rate. The final met-rics are the mean values of repeated experiments. (5) To ensure",
  "ConfigValues(25%, 50%, 75%, 90%)": "loss functionL1 LossoptimizerAdamlearning rate0.006embedding size32/32/16/16variable embedding size16/16/8/8number of layers2/2/3/3dropout0.15learning rate scheduleMultiStepLRclip gradient normalization5milestonegamme0.5batch size16epoch100 the fairness of experiments, we train the two-stage model in twoways: first, train the two models separately. Second, based on thereference , the two models are spliced together for training. Thefinal metrics are the optimal results.Metrics. To comprehensively evaluate the forecasting perfor-mance of different models, this paper utilizes three classical metrics:Mean Absolute Error (MAE), Root Mean Square Error (RMSE), andMean Absolute Percentage Error (MAPE) .",
  "Main Results": "gives the performance comparison results of all baselinesand GinAR on five datasets (The best results are shown in bold).Based on results, the following conclusions can be obtained: (1)Compared with SOTA forecasting models, all two-stage forecastingmodels can achieve better forecasting results. The main reasonis that imputation methods use normal variables to recover miss-ing variables, which reduces the impact of missing variables onthe forecasting model. However, the error accumulation problemexists in two-stage models, which limits the performance of thedownstream predictors. (2) The forecasting models with data re-covery components can work better than other baselines. On theone hand, they address the problem that one-stage models cannothandle missing data. On the other hand, they avoid the error ac-cumulation problem of two-stage models. (3) GinAR can achieveoptimal experimental results on all datasets and all settings. Basedon interpolation attention, adaptive graph convolution and RNN-based framework, the GinAR can realize missing variable recovery,spatial-temporal correlation reconstruction and end-to-end fore-casting. Compared with one-stage models and two-stage models,GinAR can avoid the problem of error accumulation and producemore accurate spatial-temporal dependencies. Therefore, GinARcan achieve better results than all baselines in MTSF with variablemissing. To further evaluate the effects of each component in Gi-nAR, we conduct ablation experiments. Besides, to demonstrate theeffect of the end-to-end framework, we analyze the performancerecovery effect of interpolation attention on MLP-based models.",
  "Ablation Experiment": "GinAR has three important components: interpolation attention,predefined graph, and adaptive graph learning. To demonstratethe importance of these components, ablation experiments areconducted from the following three perspectives: (1) w/o ia: We remove the interpolation attention. (2) w/o pg: The predefinedgraph is deleted. It means that GinAR only uses the adaptive graphto construct spatial correlations. (3) w/o ag: The adaptive graph isremoved. It means that spatial correlations are determined mainlythrough prior knowledge. shows the results of the ablation",
  ": The results of the ablation experiment": "experiment. Based on the experimental results, the following con-clusions can be drawn: (1) When the missing rate is low, deletingthe predefined graph has a great impact on the forecasting result.However, when the missing rate is large, deleting the predefinedgraph has little impact on the result. (2) When the missing rateis large, deleting the adaptive graph can significantly reduce theforecasting accuracy. The main reason is that when there are moremissing variables, the adaptive graph can better analyze the spatialcorrelation according to the characteristics of the data. Therefore,the adaptive graph plays an important role in this task. (3) WhenIA is removed, the performance of GinAR decreases significantly,proving that IA is the most important component. The main reasonis that IA realizes the recovery of missing variables, which providesan important support for correcting spatial-temporal dependenciesand avoiding error accumulation. To further analysis the effect ofthe IA, we compare IA with imputation methods in next section.",
  "Performance Evaluation of IA": "As one of the most important components proposed in this paper, itis important to further evaluate the effect of interpolation attention.In addition, it is important to further evaluate the effectiveness ofthe end-to-end framework. Therefore, this section compares theperformance improvement effects of IA, GRIN, GATGPT, GPT4TSand TimesNet on STID. Specifically, TimesNet, GATGPT, GPT4TSand GRIN adopt the two-stage modeling framework (imputationand forecasting) to optimize the performance of STID. IA uses theend-to-end modeling framework to optimize the effects of STID. shows the performance comparison results (MAE values) ofthese models (The best results are shown in boldface and the secondbest results are underlined). Based on the experimental results, thefollowing conclusions can be drawn: (1) Compared with other meth-ods, TimesNet has minimal performance improvements to STID.The main reason is that TimesNet uses temporal information torecover missing variables, without fully analyzing correspondencesbetween missing variables and normal variables. (2) The proposedIA method and other imputation methods can effectively improvethe forecasting effect of STID. (3) IA and GRIN can recover the per-formance of STID and obtain better forecasting results than otherimputation methods. The main reason is that IA and GRIN adoptthe graph-based framework to effectively reconstruct the spatialcorrelation between missing variables and normal variables, andthen recover the missing variable data based on the normal variable.",
  "STID+GPT4TS15.3716.4517.7919.25STID+TimesNet15.8117.1418.3319.74STID+GATGPT14.7715.6416.9818.29STID+GRIN14.2515.1316.4817.92STID+IA13.7514.8716.2517.83": "(4) Compared with other two-stage models, the end-to-end frame-work based on IA and STID can achieve good forecasting results.On the one hand, the two-stage models need to realize the featurereconstruction, and the problem of error accumulation results inthe decline of forecasting accuracy. On the other hand, IA realizesadaptive induction by generating correspondences between normalvariables and missing variables.",
  "Hyperparameter Experiment": "The setting of the superparameter can affect the forecasting effect ofthe GinAR. In this section, we evaluate the influence of three mainhyperparameters on the experimental results, including embeddingsize, variable embedding size, and number of layers. showsthe influence of different hyperparameters on the experimentalresults (PEMS04 dataset). Based on the experimental results, thefollowing conclusions can be obtained: (1) The variable embeddingsize has the least influence on the forecasting results. It provesthat the adaptive graph with good performance can be generatedwithout a large number of parameters. (2) The embedding size canbe increased appropriately when the missing rate is small. And theembedding size cannot be too large when the missing rate is large.The main reason is that when the missing rate is large, the largeembedding size can lead to overfitting, thus affecting the forecastingaccuracy. (3) The number of layers has the greatest influence onthe forecasting result. Too few layers can not adequately mine andanalyze the data. Too many layers can lead to problems such asoverfitting. Therefore, the best forecasting results is achieved whenthe number of layers is set to 2 or 3.",
  ": Hyperparameter experiment results (PEMS04)": "(China AQI dataset). Based on the visualization results, we can getthe following conclusions: (1) As shown in (a), (b) and (c), GinAR can accurately predict the AQI valueof all variables when the missing rate is not particularly large. (2)As shown in (a), (d) and (e), even thoughthe number of normal variables is very sparse, GinAR can stillaccurately predict the spatial distribution of the AQI data. (3) GinARcan make full use of normal variables to realize accurate spatial-temporal forecasting for all variables. The visualization results canfurther demonstrate the practical value of GinAR.",
  "CONCLUSION AND FUTURE WORK": "In this paper, we try to address a new challenging task: MTSF withvariable missing. In this task, to solve the problems of producingincorrect spatial-temporal dependencies and error accumulation inexisting models, we carefully design two key components (Interpo-lation Attention and Adaptive Graph Convolution) and use themto replace all fully connected layers in the simple recursive unit. Inthis way, we propose the Graph Interpolation Attention RecursiveNetwork based on the end-to-end framework, which can simul-taneously recover all missing variables, correct spatial-temporaldependencies, and predict the future values of all variables. Experi-mental results on five real-world datasets demonstrate the practicalvalue of our model, and even when only 10% of variables are normal,it can predict the future values of all the variables. In the future, we Input featureForecasting results",
  "Dimitris Bertsimas, Agni Orfanoudaki, and Colin Pawlowski. 2021. Imputationof clinical covariates in time series. Machine Learning 110 (2021), 185248": "Ane Blzquez-Garca, Kristoffer Wickstrm, Shujian Yu, Karl yvind Mikalsen,Ahcene Boubekki, Angel Conde, Usue Mori, Robert Jenssen, and Jose A Lozano.2023. Selective imputation for multivariate time series datasets with missingvalues. IEEE Transactions on Knowledge and Data Engineering (2023). Defu Cao, Yujing Wang, Juanyong Duan, Ce Zhang, Xia Zhu, Congrui Huang,Yunhai Tong, Bixiong Xu, Jing Bai, Jie Tong, et al. 2020. Spectral temporal graphneural network for multivariate time-series forecasting. Advances in neuralinformation processing systems 33 (2020), 1776617778.",
  "Vitor Cerqueira, Nuno Moniz, and Carlos Soares. 2021. Vest: Automatic featureengineering for forecasting. Machine Learning (2021), 123": "Jatin Chauhan, Aravindan Raghuveer, Rishi Saket, Jay Nandy, and BalaramanRavindran. 2022. Multi-Variate Time Series Forecasting on Variable Subsets. InProceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery andData Mining. 7686. Ling Chen, Donghui Chen, Zongjiang Shang, Binqing Wu, Cen Zheng, Bo Wen,and Wei Zhang. 2023. Multi-scale adaptive graph neural network for multivariatetime series forecasting. IEEE Transactions on Knowledge and Data Engineering(2023). Xinyu Chen, Mengying Lei, Nicolas Saunier, and Lijun Sun. 2021. Low-rankautoregressive tensor completion for spatiotemporal traffic data imputation. IEEETransactions on Intelligent Transportation Systems 23, 8 (2021), 1230112310. Xiaodan Chen, Xiucheng Li, Bo Liu, and Zhijun Li. 2023. Biased TemporalConvolution Graph Network for Time Series Forecasting with Missing Values..In The Twelfth International Conference on Learning Representations. Yuzhou Chen, Sotiris Batsakis, and H Vincent Poor. 2023. Higher-Order Spatio-Temporal Neural Networks for Covid-19 Forecasting. In ICASSP 2023-2023 IEEEInternational Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE,15. Yong Chen and Xiqun Michael Chen. 2022. A novel reinforced dynamic graphconvolutional network model with data imputation for network-wide traffic flowprediction. Transportation Research Part C: Emerging Technologies 143 (2022),103820. Yakun Chen, Zihao Li, Chao Yang, Xianzhi Wang, Guodong Long, and GuandongXu. 2023. Adaptive graph recurrent network for multivariate time series impu-tation. In Neural Information Processing: 29th International Conference, ICONIP2022, Virtual Event, November 2226, 2022, Proceedings, Part V. Springer, 6473. Yuanyuan Chen, Yisheng Lv, and Fei-Yue Wang. 2019. Traffic flow imputationusing parallel data and generative adversarial networks. IEEE Transactions onIntelligent Transportation Systems 21, 4 (2019), 16241630.",
  "Yakun Chen, Xianzhi Wang, and Guandong Xu. 2023. Gatgpt: A pre-trained largelanguage model with graph attention network for spatiotemporal imputation.arXiv preprint arXiv:2311.14332 (2023)": "Yu Chengqing, Yan Guangxi, Yu Chengming, Zhang Yu, and Mi Xiwei. 2023.A multi-factor driven spatiotemporal wind power prediction model based onensemble deep graph attention reinforcement learning networks. Energy 263(2023), 126034. Ranak Roy Chowdhury, Xiyuan Zhang, Jingbo Shang, Rajesh K Gupta, and DezhiHong. 2022. Tarnet: Task-aware reconstruction for time-series transformer. InProceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery andData Mining. 212220.",
  "Andrea Cini, Ivan Marisca, and Cesare Alippi. 2022. Filling the G_ap_s: Mul-tivariate Time Series Imputation by Graph Neural Networks. In InternationalConference on Learning Representations": "Razvan-Gabriel Cirstea, Bin Yang, Chenjuan Guo, Tung Kieu, and Shirui Pan.2022. Towards spatio-temporal aware traffic time series forecasting. In 2022 IEEE38th International Conference on Data Engineering (ICDE). IEEE, 29002913. Jinliang Deng, Xiusi Chen, Zipei Fan, Renhe Jiang, Xuan Song, and Ivor W Tsang.2021. The pulse of urban transport: Exploring the co-evolving pattern for spatio-temporal forecasting. ACM Transactions on Knowledge Discovery from Data(TKDD) 15, 6 (2021), 125. Jinliang Deng, Xiusi Chen, Renhe Jiang, Xuan Song, and Ivor W Tsang. 2021. St-norm: Spatial and temporal normalization for multi-variate time series forecasting.In Proceedings of the 27th ACM SIGKDD conference on knowledge discovery & datamining. 269278. Jinliang Deng, Xiusi Chen, Renhe Jiang, Xuan Song, and Ivor W Tsang. 2022. Amulti-view multi-task learning framework for multi-variate time series forecast-ing. IEEE Transactions on Knowledge and Data Engineering (2022). Jinliang Deng, Xiusi Chen, Renhe Jiang, Du Yin, Yi Yang, Xuan Song, and Ivor WTsang. 2024. Disentangling Structured Components: Towards Adaptive, Inter-pretable and Scalable Time Series Forecasting. IEEE Transactions on Knowledgeand Data Engineering (2024).",
  "Wenjie Du, David Ct, and Yan Liu. 2023. Saits: Self-attention-based imputationfor time series. Expert Systems with Applications 219 (2023), 119619": "Vincent Fortuin, Dmitry Baranchuk, Gunnar Rtsch, and Stephan Mandt. 2020.Gp-vae: Deep probabilistic time series imputation. In International conference onartificial intelligence and statistics. PMLR, 16511661. Jingxuan Geng, Chunhua Yang, Yonggang Li, Lijuan Lan, and Qiwu Luo. 2022.MPA-RNN: a novel attention-based recurrent neural networks for total nitrogenprediction. IEEE Transactions on Industrial Informatics 18, 10 (2022), 65166525.",
  "Jia Hu, Xianghong Lin, and Chu Wang. 2022. MGCN: Dynamic Spatio-TemporalMulti-Graph Convolutional Neural Network. In 2022 International Joint Confer-ence on Neural Networks (IJCNN). IEEE, 19": "Marisca Ivan, Cini Andrea, and Cesare Alippi. 2022. Learning to ReconstructMissing Data from Spatiotemporal Graphs with Sparse Observations. In 36thConference on Neural Information Processing Systems (NeurIPS 2022). 117. Renhe Jiang, Zhaonan Wang, Jiawei Yong, Puneet Jeph, Quanjun Chen, Ya-sumasa Kobayashi, Xuan Song, Shintaro Fukushima, and Toyotaro Suzumura.2023. Spatio-temporal meta-graph learning for traffic forecasting. In Proceedingsof the AAAI Conference on Artificial Intelligence, Vol. 37. 80788086. Renhe Jiang, Du Yin, Zhaonan Wang, Yizhuo Wang, Jiewen Deng, Hangchen Liu,Zekun Cai, Jinliang Deng, Xuan Song, and Ryosuke Shibasaki. 2021. Dl-traff:Survey and benchmark of deep learning models for urban traffic prediction. InProceedings of the 30th ACM international conference on information & knowledgemanagement. 45154525. Tung Kieu, Bin Yang, Chenjuan Guo, Razvan-Gabriel Cirstea, Yan Zhao, YaleSong, and Christian S Jensen. 2022. Anomaly detection in time series withrobust variational quasi-recurrent autoencoders. In 2022 IEEE 38th InternationalConference on Data Engineering (ICDE). IEEE, 13421354.",
  "Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. 2018. Diffusion ConvolutionalRecurrent Neural Network: Data-Driven Traffic Forecasting. In InternationalConference on Learning Representations": "Ke Liang, Yue Liu, Sihang Zhou, Wenxuan Tu, Yi Wen, Xihong Yang, XiangjunDong, and Xinwang Liu. 2023. Knowledge Graph Contrastive Learning Basedon Relation-Symmetrical Structure. IEEE Transactions on Knowledge and DataEngineering (2023). Ke Liang, Lingyuan Meng, Meng Liu, Yue Liu, Wenxuan Tu, Siwei Wang, Si-hang Zhou, Xinwang Liu, and Fuchun Sun. 2022. Reasoning over differenttypes of knowledge graphs: Static, temporal and multi-modal. arXiv preprintarXiv:2212.05767 (2022), 75767584. Ke Liang, Jim Tan, Dongrui Zeng, Yongzhe Huang, Xiaolei Huang, and Gang Tan.2023. Abslearn: a gnn-based framework for aliasing and buffer-size informationretrieval. Pattern Analysis and Applications (2023), 119.",
  "Linfeng Liu, Michael C Hughes, Soha Hassoun, and Liping Liu. 2021. Stochasticiterative graph matching. In International Conference on Machine Learning. PMLR,68156825": "Yutian Liu, Soora Rasouli, Melvin Wong, Tao Feng, and Tianjin Huang. 2024.RT-GCN: Gaussian-based spatiotemporal graph convolutional network for robusttraffic prediction. Information Fusion 102 (2024), 102078. Yonghong Luo, Ying Zhang, Xiangrui Cai, and Xiaojie Yuan. 2019. E2gan: End-to-end generative adversarial network for multivariate time series imputation.In Proceedings of the 28th international joint conference on artificial intelligence.AAAI Press Palo Alto, CA, USA, 30943100. Soumen Pachal and Avinash Achar. 2022. Sequence Prediction under MissingData: An RNN Approach without Imputation. In Proceedings of the 31st ACMInternational Conference on Information & Knowledge Management. 16051614. Tangwen Qian, Yile Chen, Gao Cong, Yongjun Xu, and Fei Wang. 2023. AdapTraj:A Multi-Source Domain Generalization Framework for Multi-Agent TrajectoryPrediction. arXiv preprint arXiv:2312.14394 (2023). Xiaobin Ren, Kaiqi Zhao, Patricia J Riddle, Katerina Taskova, Qingyi Pan, andLianyan Li. 2023. DAMR: Dynamic Adjacency Matrix Representation Learningfor Multivariate Time Series Imputation. Proceedings of the ACM on Managementof Data 1, 2 (2023), 125.",
  "Chao Shang, Jie Chen, and Jinbo Bi. 2021. Discrete Graph Structure Learningfor Forecasting Multiple Time Series. In International Conference on LearningRepresentations": "Pan Shang, Xinwei Liu, Chengqing Yu, Guangxi Yan, Qingqing Xiang, and XiweiMi. 2022. A new ensemble deep graph reinforcement learning network forspatio-temporal traffic volume forecasting in a freeway network. Digital SignalProcessing 123 (2022), 103419. Zezhi Shao, Fei Wang, Yongjun Xu, Wei Wei, Chengqing Yu, Zhao Zhang, Di Yao,Guangyin Jin, Xin Cao, Gao Cong, et al. 2023. Exploring Progress in Multivari-ate Time Series Forecasting: Comprehensive Benchmarking and HeterogeneityAnalysis. arXiv preprint arXiv:2310.06119 (2023). Zezhi Shao, Zhao Zhang, Fei Wang, Wei Wei, and Yongjun Xu. 2022. Spatial-Temporal Identity: A Simple yet Effective Baseline for Multivariate Time SeriesForecasting. In Proceedings of the 31st ACM International Conference on Informa-tion and Knowledge Management. 44544458. Zezhi Shao, Zhao Zhang, Fei Wang, and Yongjun Xu. 2022. Pre-training enhancedspatial-temporal graph neural network for multivariate time series forecasting.In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery andData Mining. 15671577. Zezhi Shao, Zhao Zhang, Wei Wei, Fei Wang, Yongjun Xu, Xin Cao, and Chris-tian S Jensen. 2022. Decoupled dynamic spatial-temporal graph neural networkfor traffic forecasting. Proceedings of the VLDB Endowment 15, 11 (2022), 27332746. Mengshuai Su, Hui Liu, Chengqing Yu, and Zhu Duan. 2023. A novel AQIforecasting method based on fusing temporal correlation forecasting with spatialcorrelation forecasting. Atmospheric Pollution Research 14, 4 (2023), 101717. Tao Sun, Fei Wang, Zhao Zhang, Lin Wu, and Yongjun Xu. 2022. Human mobilityidentification by deep behavior relevant location representation. In InternationalConference on Database Systems for Advanced Applications. Springer, 439454. Jing Tan, Hui Liu, Yanfei Li, Shi Yin, and Chengqing Yu. 2022. A new ensemblespatio-temporal PM2. 5 prediction method based on graph attention recursivenetworks and reinforcement learning. Chaos, Solitons & Fractals 162 (2022),112405. Peiwang Tang, Qinghua Zhang, and Xianchao Zhang. 2023. A Recurrent NeuralNetwork based Generative Adversarial Network for Long Multivariate TimeSeries Forecasting. In Proceedings of the 2023 ACM International Conference onMultimedia Retrieval. 181189. Xianfeng Tang, Huaxiu Yao, Yiwei Sun, Charu Aggarwal, Prasenjit Mitra, andSuhang Wang. 2020. Joint modeling of local and global temporal dynamics formultivariate time series forecasting with missing values. In Proceedings of theAAAI Conference on Artificial Intelligence, Vol. 34. 59565963.",
  "Fei Wang, Di Yao, Yong Li, Tao Sun, and Zhao Zhang. 2023. AI-enhanced spatial-temporal data-mining technology: New chance for next-generation urban com-puting. The Innovation 4, 2 (2023)": "Pu Wang, Zhihong Feng, Yan Tang, and Yuzhi Zhang. 2019. A fingerprint data-base reconstruction method based on ordinary kriging algorithm for indoorlocalization. In 2019 International Conference on Intelligent Transportation, BigData & Smart City (ICITBS). IEEE, 224227. Peixiao Wang, Tong Zhang, Yueming Zheng, and Tao Hu. 2022. A multi-viewbidirectional spatiotemporal graph network for urban traffic flow imputation.International Journal of Geographical Information Science 36, 6 (2022), 12311257. Zhiyuan Wang, Fan Zhou, Goce Trajcevski, Kunpeng Zhang, and Ting Zhong.2023. Learning Dynamic Temporal Relations with Continuous Graph for Multi-variate Time Series Forecasting (Student Abstract). In Proceedings of the AAAIConference on Artificial Intelligence, Vol. 37. 1635816359. Yuanyuan Wei, Julian Jang-Jaccard, Wen Xu, Fariza Sabrina, Seyit Camtepe, andMikael Boulic. 2023. LSTM-autoencoder-based anomaly detection for indoor airquality time-series data. IEEE Sensors Journal 23, 4 (2023), 37873800. Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and MingshengLong. 2023. TimesNet: Temporal 2D-Variation Modeling for General Time SeriesAnalysis. In The Eleventh International Conference on Learning Representations. Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. 2021. Autoformer: De-composition transformers with auto-correlation for long-term series forecasting.Advances in Neural Information Processing Systems 34 (2021), 2241922430. Yuankai Wu, Dingyi Zhuang, Aurelie Labbe, and Lijun Sun. 2021. Inductivegraph neural networks for spatiotemporal kriging. In Proceedings of the AAAIConference on Artificial Intelligence, Vol. 35. 44784485. Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, Xiaojun Chang, and ChengqiZhang. 2020. Connecting the dots: Multivariate time series forecasting with graphneural networks. In Proceedings of the 26th ACM SIGKDD international conferenceon knowledge discovery & data mining. 753763. Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, and Chengqi Zhang. 2019.Graph wavenet for deep spatial-temporal graph modeling. In Proceedings of the28th International Joint Conference on Artificial Intelligence. 19071913. Yi Xu, Armin Bazarjani, Hyung-gun Chi, Chiho Choi, and Yun Fu. 2023. Uncov-ering the Missing Pattern: Unified Framework Towards Trajectory Imputationand Prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision",
  "Yongjun Xu, Fei Wang, Zhulin An, Qi Wang, and Zhao Zhang. 2023. Artificialintelligence for sciencebridging data to wisdom. The Innovation 4, 6 (2023)": "Yongchao Ye, Shiyao Zhang, and James JQ Yu. 2021. Spatial-temporal trafficdata imputation via graph attention convolutional network. In InternationalConference on Artificial Neural Networks. Springer, 241252. Kun Yi, Qi Zhang, Wei Fan, Hui He, Liang Hu, Pengyang Wang, Ning An, Long-bing Cao, and Zhendong Niu. 2023. FourierGNN: Rethinking Multivariate TimeSeries Forecasting from a Pure Graph Perspective. In Thirty-seventh Conferenceon Neural Information Processing Systems.",
  "Jinsung Yoon, James Jordon, and Mihaela Schaar. 2018. Gain: Missing data impu-tation using generative adversarial nets. In International conference on machinelearning. PMLR, 56895698": "Chengqing Yu, Fei Wang, Zezhi Shao, Tao Sun, Lin Wu, and Yongjun Xu. 2023.Dsformer: A double sampling transformer for multivariate time series long-termprediction. In Proceedings of the 32nd ACM International Conference on Informationand Knowledge Management. 30623072. Chengqing Yu, Guangxi Yan, Chengming Yu, Xinwei Liu, and Xiwei Mi. 2024.MRIformer: A multi-resolution interactive transformer for wind speed multi-stepprediction. Information Sciences 661 (2024), 120150. Kai Zhang, Chao Li, and Qinmin Yang. 2023. TriD-MAE: A Generic Pre-trainedModel for Multivariate Time Series with Missing Values. In Proceedings of the32nd ACM International Conference on Information and Knowledge Management.31643173. Ling Zhao, Yujiao Song, Chao Zhang, Yu Liu, Pu Wang, Tao Lin, Min Deng, andHaifeng Li. 2019. T-gcn: A temporal graph convolutional network for trafficprediction. IEEE transactions on intelligent transportation systems 21, 9 (2019),38483858. Chuanpan Zheng, Xiaoliang Fan, Cheng Wang, and Jianzhong Qi. 2020. Gman: Agraph multi-attention network for traffic prediction. In Proceedings of the AAAIconference on artificial intelligence, Vol. 34. 12341241. Fan Zhou, Chen Pan, Lintao Ma, Yu Liu, Shiyu Wang, James Zhang, Xinxin Zhu,Xuanwei Hu, Yunhua Hu, Yangfei Zheng, et al. 2023. SLOTH: Structured Learningand Task-Based Optimization for Time Series Forecasting on Hierarchies. InProceedings of the AAAI Conference on Artificial Intelligence, Vol. 37. 1141711425.",
  "shows the statistics of these datasets. A brief overview ofall datasets is shown as follows:": "METR-LA: It is a traffic speed dataset collected by loop-detectors located on the LA County road network, whichcontains data collected by 207 sensors from Mar 1st, 2012to Jun 30th, 2012. Each time series is sampled at a 5-minuteinterval, totaling 34272 time slices. PEMS-BAY: It is a traffic speed dataset collected by Califor-nia Transportation Agencies (CalTrans) Performance Mea-surement System (PeMS), which contains data collected by325 sensors from Jan 1st, 2017 to May 31th, 2017. Each timeseries is sampled at a 5-minute interval, totaling 52116 timeslices. PEMS04: It is a traffic flow dataset collected by CalTransPeMS, which contains data collected by 307 sensors fromJanuary 1st, 2018 to February 28th, 2018. Each time series issampled at a 5-minute interval, totaling 16992 time slices.",
  "MET-LA207342725minutesPEMS-BAY325521165minutesPEMS04307169925minutesPEMS08170178565minutesChina AQI350597101hour": "PEMS08: It is a traffic flow dataset collected by CalTransPeMS, which contains data collected by 170 sensors fromJuly 1st, 2018 to Aug 31th, 2018. Each time series is sampledat a 5-minute interval, totaling 17833 time slices. China AQI: It is an air quality dataset collected by ChinaEnvironmental Monitoring Station, which contains data col-lected by 350 cities in China from January 2015 to December2022. Each time series is sampled at a 1 hour interval, totaling59710 time slices.",
  "BEFFICIENCY": "In this section, we compare the efficiency of GinAR with that of sev-eral baselines (GC-VRNN, TriD-MAE, MTGNN + GRIN, DFDGCN+ TimesNet and DCRNN + GPT4TS) on the PEMS08 dataset. To en-sure the fairness of the experiment, we compare the mean trainingtime of each epoch of each model. The experimental equipment isthe Intel(R) Xeon(R) Gold 5217 CPU @ 3.00GHz, 128G RAM com-puting server with RTX 3090 graphics card. The batch size is setto 16. Based on , it can be found that the training time ofGinAR is not large. Compared with several two-stage models, theGinAR does not require the imputation stage, which reduces theoverall training time. Besides, although the training time of GinARis greater than that of the one-stage models, it solves the problem ofvariable missing, which can improve its forecasting performance.",
  "NotationsizeDefinitions": "ConstantThe length of historical observationConstantThe length of future forecasting resultsConstantBatch sizeConstantNumber of variablesConstantNumber of missing variablesConstantEmbedding sizeConstantVariable embedding sizeConstantNumber of GinAR layers Input features Input features with missing variables Forecasting results Predefined graph Adaptive graph Variable embedding of adaptive graphFunctionsLayer normalizationFunctionsFully connected layerReLUFunctionsActivation function ReLUELUFunctionsActivation function ELUGeLUFunctionsActivation function GeLULeakyReLUFunctionsActivation function LeakyReLUsoftmaxFunctionsActivation function softmax"
}