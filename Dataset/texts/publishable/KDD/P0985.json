{
  "ABSTRACT": "Given an input query, a recommendation model is trained usinguser feedback data (e.g., click data) to output a ranked list of items.In real-world systems, besides accuracy, an important considerationfor a new model is novelty of its top-k recommendations w.r.t. an ex-isting deployed model. However, novelty of top-k items is a difficultgoal to optimize a model for, since it involves a non-differentiablesorting operation on the models predictions. Moreover, novel items,by definition, do not have any user feedback data. Given the seman-tic capabilities of large language models, we address these problemsusing a reinforcement learning (RL) formulation where large lan-guage models provide feedback for the novel items. However, givenmillions of candidate items, the sample complexity of a standard RLalgorithm can be prohibitively high. To reduce sample complexity,we reduce the top-k list reward to a set of item-wise rewards andreformulate the state space to consist of query, item tuples suchthat the action space is reduced to a binary decision; and show thatthis reformulation results in a significantly lower complexity whenthe number of items is large. We evaluate the proposed algorithmon improving novelty for a query-ad recommendation task on alarge-scale search engine. Compared to supervised finetuning onrecent <query, ad>pairs, the proposed RL-based algorithm leadsto significant novelty gains with minimal loss in recall. We obtainsimilar results on the ORCAS query-webpage matching dataset anda product recommendation dataset based on Amazon reviews.",
  "ACM Reference Format:Amit Sharma, Hua Li, Xue Li, and Jian Jiao. 2024. Optimizing Novelty ofTop-k Recommendations using Large Language Models and Reinforcement": "KDD 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s).This is the authors version of the work. It is posted here for your personal use. Notfor redistribution. The definitive Version of Record was published in Proceedings ofthe 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 24),August 2529, 2024, Barcelona, Spain,",
  "INTRODUCTION": "Given a users profile or an input query, the recommendation prob-lem is to fetch a ranked list of top-k items based on a task-specificgoal. We consider the retrieval layer of a recommendation system,where the input is typically millions of candidate items and outputis hundreds of ranked items (e.g., k=200). For the retrieval layer,semantic relevance is a common goal and models are trained torank relevant items higher . A common way to train such recom-mendation models is to use supervised learning on user feedbackdata such as clicks, using losses such as contrastive learning thatencourage representations of clicked query-item pairs to be closerto each other than negative (or random) query-item pairs .However, in addition to relevance, real-world recommendationsystems typically have additional goals to optimize for the pre-dicted top-k items. For instance, an important goal is that a can-didate model predicts top-k items that are novel w.r.t. the existingdeployed models in the system . Novelty is a desirable propertysince it can avoid showing repetitive or redundant items to a user,enhance global coverage over all available items, and help avoidany systematic bias towards previously clicked items in the sys-tem . However, it is difficult to train a model to optimize noveltyof top-k items since evaluating novelty requires a non-differentiablesorting operation to obtain the top-k list. Moreover, novel items,by definition, do not have any user feedback data correspondingto the particular query. As a result, getting relevance feedback onnovel items requires conducting an online experiment showingexploratory, novel items to the user, which can be a costly andinfeasible procedure for many systems.To obtain scalable feedback for novel items, we utilize recentresults that show that large language models (LLMs) like GPT-3.5or GPT-4 can match or surpass quality of crowd-sourced relevancefeedback for recommendation tasks, such as matching user inputto ad keywords or matching user history to suggested moviesand games . Specifically, we propose that LLMs can be used asreward models to provide relevance feedback for novel items, usingan appropriate task-specific prompt. As a result, (noisy) relevancefeedback for novel items can be obtained at scale and be used for",
  "KDD 24, August 2529, 2024, Barcelona, SpainSharma et. al": "Nick Craswell, Daniel Campos, Bhaskar Mitra, Emine Yilmaz, and Bodo Billerbeck.2020. ORCAS: 20 million clicked query-document pairs for analyzing search. InProceedings of the 29th ACM International Conference on Information & KnowledgeManagement. 29832989. Kunal Dahiya, Ananye Agarwal, Deepak Saini, K Gururaj, Jian Jiao, Amit Singh,Sumeet Agarwal, Purushottam Kar, and Manik Varma. 2021. Siamesexml: Siamesenetworks meet extreme classifiers with 100m labels. In International Conferenceon Machine Learning. PMLR, 23302340. Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov,Kelvin Guu, Keith B Hall, and Ming-Wei Chang. 2022. Promptagator: Few-shotdense retrieval from 8 examples. arXiv preprint arXiv:2209.11755 (2022). Jorge Dez, David Martnez-Rego, Amparo Alonso-Betanzos, Oscar Luaces, andAntonio Bahamonde. 2019. Optimizing novelty and diversity in recommendations.Progress in Artificial Intelligence 8 (2019), 101109. Gabriel Dulac-Arnold, Richard Evans, Hado van Hasselt, Peter Sunehag, TimothyLillicrap, Jonathan Hunt, Timothy Mann, Theophane Weber, Thomas Degris, andBen Coppin. 2015. Deep reinforcement learning in large discrete action spaces.arXiv preprint arXiv:1512.07679 (2015). Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple ContrastiveLearning of Sentence Embeddings. In 2021 Conference on Empirical Methodsin Natural Language Processing, EMNLP 2021. Association for ComputationalLinguistics (ACL), 68946910. Jiafeng Guo, Yinqiong Cai, Yixing Fan, Fei Sun, Ruqing Zhang, and Xueqi Cheng.2022. Semantic models for the first-stage retrieval: A comprehensive review.ACM Transactions on Information Systems (TOIS) 40, 4 (2022), 142. Xingwei He, Zhenghao Lin, Yeyun Gong, Alex Jin, Hang Zhang, Chen Lin, JianJiao, Siu Ming Yiu, Nan Duan, Weizhu Chen, et al. 2023. Annollm: Makinglarge language models to be better crowdsourced annotators. arXiv preprintarXiv:2303.16854 (2023). Jonathan L Herlocker, Joseph A Konstan, Loren G Terveen, and John T Riedl.2004. Evaluating collaborative filtering recommender systems. ACM Transactionson Information Systems (TOIS) 22, 1 (2004), 553. Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley,and Wayne Xin Zhao. 2024. Large language models are zero-shot rankers forrecommender systems. In European Conference on Information Retrieval. Springer,364381.",
  "RELATED WORK2.1Enhancing novelty of top-k results": "Outputting novel items compared to existing algorithms is an impor-tant goal for recommendation systems . A common approach isto consider a multi-objective optimization problem with rele-vance and novelty as the two objectives . Recently, reinforce-ment learning has been used for novelty optimization since RL canbe applied to non-differentiable objectives like novelty .However, past work uses approximations of novelty, such as anitem being less popular or belonging to less popular cate-gories , to enhance novelty, since truly novel itemsthose thathave not been shown by the current systemwould not have anyuser feedback data for training. To ascertain relevance of truly novelitems, heuristics like estimating the curiosity of a user (more curios-ity admits more novel items as relevant) have been proposed ,but no systematic way to estimate relevance exists. Consequently,without relevance feedback, there exists a direct tradeoff betweenoptimizing novelty and relevance of a top-k list. In this work, weuse the semantic capabilities of LLMs to propose a general, scal-able method to estimate relevance of novel items. As a result, wecan directly optimize for the novelty objective without sacrificingrelevance.Note that encouraging novelty in top-k items is different fromaddressing the cold-start problem . Novel items are definedwrt. a query whereas cold-start items are defined globally for asystem. In many cases, a novel item wrt. a query may be a popularitem globally. As a result, methods for the cold-start problem maynot directly apply to novelty optimization.",
  "RL with large action spaces": "Given a query, information retrieval can be divided into two stages ;1) retrieval of relevant items from a large candidate pool of items;2) ranking the retrieved items to select a smaller set of items thatare shown to the user. RL algorithms find many applications in theranking phase, including contextual bandits , markov decisionprocesses , and policy gradient algorithms . Many ofthese algorithms introduce a task-specific loss to the ranking prob-lem. However, applying these techniques to the retrieval phase ischallenging because of the large number of candidate items (ac-tions).Large action spaces is a general challenge for RL beyond retrievalmodels , even when using policy gradient, a method that doesnot explicitly parameterize the action space and thus is better suitedfor higher number of actions. For sequential recommendation prob-lems, appropriately parameterizing the policy and using off-policycorrection helps in scaling to large actions . For one-step RLproblems, recent theoretical work in contextual bandits tries to address the large actions problem. However, the focus ofthese approaches is on obtaining an optimal solution from scratch,which may be difficult in practice and misses the complementary",
  "OPTIMIZING NOVELTY W/ LLM FEEDBACK3.1Problem statement: Novelty optimization": "Commonly used retrieval models use a bi-encoder architecture ,where the same neural network model embeds a query and iteminto a common representation space. The top-k items are selectedbased on the nearest neighbors to a query, as measured by a suitabledistance function over the embeddings (e.g., cosine similarity). Theencoder is typically optimized using a variant of contrastive learn-ing, encouraging that positive <query,item> pairs in the trainingset should be closer in embedding space than non-positive pairs.Non-positive pairs may be random pairs or mined from traindata. Thus, given a set of queries and items , a train dataset D with positive query-item pairs, and a similarity functionsim, the trained retriever can be written as (using the InfoNCEcontrastive loss function ),",
  "y() = Topk sim( (), ())(1)": "where y = refers to the sequence of top-k itemsreturned by a trained encoder and Topk is a non-differentiableranking operation. At inference time, given a query, the trainedencoder is used to compute the similarity with each item and thetop-k closest items are returned as the prediction.Given the trained encoder , our goal is to finetune the encodersuch that the novelty of its top-k items compared to a base modelis optimized, while ensuring that the accuracy of the encoder doesnot decrease substantially. Specifically, we assume that there is anexisting base retrieval model deployed in the production recom-mendation system and we want to deploy another retrieval modelthat introduces novel items. Both the base model and the new, fine-tuned model would run in parallel and their results can be mergedfor the downstream ranking layer. Formally, novelty@k is definedas,",
  "Definition 1. Novelty: Given a query , Novelty@k (,,, )is defined as the number of items in top-k predictions of a candidatemodel that do not exist in top-L predictions of the base model": "Typically, is set as the number of items that the retrieval layersends downstream to the ranking layer (e.g., = 200 in our exper-iments). If a candidate model outputs an item that is beyond thetop-L of the base model, then it will result in a new item added tothe recommendation systems ranking layer. Note that need notbe the same as . For efficiency reasons, we may be interested in",
  "RL formulation using policy gradient": "Novelty@k, as defined above, cannot be optimized directly since itincludes a non-differentiable component w.r.t. the encoders param-eters, the top-k sorting operation over items. As in past work onrecommendation systems , a natural solution for non-differentiable rewards is to use reinforcement learning. Formally,our problem can be formulated as a one-step RL setting. The queryis considered as the state and the set of top-k items as the action. Apolicy : { : } is a function that outputs a set oftop-k items given a query. For each action selected by the policy,the environment provides reward feedback on the < state,action> pair. For instance, in the query-ad recommendation task, usersinput query is the state, the top-k predicted ads are the action by thepolicy, and the environment provides reward based on the queryand its top-k predicted ads, e.g., based on the novelty and relevanceof the predicted ads. Given a reward function R, the task to learn apolicy (parameterized by ), that optimizes,",
  "In our work, we consider a pre-trained encoder : R": "to correspond to the initial policy 0. For simplicity, given an en-coder/policy at any stage of training, the top-k items y are assumedto be independently sampled from the discrete action probabilitydistribution, : ; (|) {1, 2, 3..}; where (|) = softmax sim( (), ()). We use (y|) to denote thetop-k items generated using the policy.As in past work on using RL in recommendation systems [2, 24, 27], we use a policy gradient algorithm to optimize the novelty re-ward. Policy gradient algorithms are well-suited for recommendersystems since they do not explicitly parameterize the action space(thus allowing a large number of actions) and can directly updatethe parameters of the encoder . Specifically, we use the REIN-FORCE algorithm that depends on a Monte Carlo approxima-tion of the reward expectation from Eqn. 2.",
  "LLMs make novelty optimization practical": "While the formulation is reasonable, there is a key limitation: novelitems, by definition, do not have any user feedback data since theywere never shown to users by the base production model for thatquery. Hence, if we use only the log-based training data for Eqn. 4,relevance feedback for the novel items would be missing and thusno novel items reward can be computed.In this paper, we show that LLMs help avoid this limitation byproviding relevance feedback for novel query-item pairs. LLMssuch as GPT-3.5 and GPT-4 have been shown to provide a substan-tial improvement in semantic understanding compared to existingencoder-based models. For instance, recent work shows that GPT-3.5 can match or surpass crowd-sourced human labellers in theiraccuracy on labelling or ranking retrieval outputs forrelevance. In addition, even though the relevance criterion for acorrect recommendation may differ for different tasks, we need nottrain separate relevance models. For example, while the criteria formatching books to other books may be different from matchingqueries to ads, which in turn may be slightly different for matchingqueries to webpages; a single LLM like GPT-3.5 can be used for pro-viding relevance feedback for all these domains by simply changingthe prompt. Hence, the accuracy and universality of LLMs makeit possible to obtain relevance feedback for arbitrary query-itempairs and optimize novelty directly for retrieval tasks.",
  "The challenge with large action spaces": "While LLMs provide a solution to the problem of relevance feed-back for novel query-item pairs, another key challenge is the largenumber of potential query-item pairs to label since available itemsare typically in the millions. Below we show that the number ofrelevance reward samples needed for a policy gradient algorithmto obtain a desired accuracy (sample complexity) increases propor-tional to the square of the number of actions. The result is based onapplying finite sample convergence bounds for policy gradientalgorithms to the retrieval setup.Let denote the optimal policy that maximizes Eqn 2 and referto the steps of the optimization. We consider a one-step MarkovDecision Process where each query corresponds to a state and theactions are the top-k predicted items. Hence, for each state, weonly need to optimize the current reward. We assume a uniformprobability for sampling each state (query).",
  "where = | | is the number of states and is the number of actions": "Proof is in Appendix. The proof uses Thm. 4 from for aMarkov Decision Process and adapts it to the single-step problemand additionally uses Assumption 2 to express the bound in termsof and . Since the policy outputs a list of top-k items, the totalnumber of possible actions is = (| |,), indicating a high samplecomplexity. In the next section, we describe how we reduce theeffective sample complexity and derive a practical algorithm.",
  "LARGE ACTION POLICY GRADIENT": "Proposition 1 shows that a naive application of the policy gra-dient algorithm will be slow to converge to the optimal reward-maximizing solution due to a combinatorially large action space.First, we show that the combinatorial action space = (| |,) canbe decomposed into item-wise action space, = | | whenever thereward over top-k items can be decomposed as an additive sum overitem-wise rewards. We show that most common novelty rewardfunctions satisfy this property. Second, we provide a reformulationof the RL problem that reduces the action space to a binary decisionand further increases the rate of convergence.",
  "Reduction from Top-k to item-wise rewards": "As stated in , the reward function is a combination ofnovelty and relevance rewards. We assume that both novelty andrelevance rewards compose additively. That is, given a query ,@(,,, ) = =1 (,,,, ), and we have @(,) = =1 (,,); where are theindividual item predictions and @ is typically recall@kor precision@k. As a result, we can reformulate the action spaceto consist of individual items, = | |. In expectation, maximizingthe novelty and relevance reward function for separatelywould imply maximizing the top-k reward.While the action space is reduced in size, a key benefit of ourformulation is that the reward can still be a function of the top-kretrieved items from some model. This is because the environmentcan decide the reward based on whether the item is a part of thetop-k items for the query. Recall that, given a state (query) and anaction (item) , the novelty reward is dependent on whetherthe is part of the top-k items returned by the base model . As anexample, we provide a simple reward function combining item-wiserelevance and novelty w.r.t. . Given any query and item , and arelevance oracle, Rel (i.e., an LLM), the reward is given by,",
  "(7)": "4.2Reduction to binary action spaceTo further improve the convergence rate, we consider a differentRL formulation where the state is query, item pair and the pol-icy outputs the probability of selecting the item. The item-wisereward function changes slightly to accommodate the two actionsof selecting the item (1) or not (0). Assuming the same rewardlogic from Eqn. 6, the reward function becomes, R((,),) =R (,) + (1 )(R (,)). Intuitively, if the policy selects theitem for a query, then its reward for the action is proportional toR. Otherwise, if it does not select the item, then its reward isproportional to negative of R. The corresponding gradient is,",
  "=1[() R ( (),() ) (1 () ) R ( (),() )] log (() | (),() )": "where ( = 1|,) = (,) = softmax sim( (), ()) and ( = 0|,) = 1 (,) = 1 softmax sim( (), ()).With this formulation, the number of states increases to but the number of actions reduces to 2. As we show below, theconvergence rate is significantly faster since the error now growslinearly with A rather than quadratic.",
  "(9)": "Note that in practice, may be higher for the binary action for-mulation since there are only 2 actions. Assuming a good enough\"supervised policy, conservative value for may be 50, implyingthat probability of the optimal action under supervised policy isalways 1/(2 50) = 0.01. Even under this conservative estimate,as long as the number of actions is of the order of millions, 2 << and hence the convergence rate in Proposition 2 would be signifi-cantly faster. In other words, as long as 2 << , the binary-actionpolicy gradient algorithm will converge at a faster rate.For maximizing a reward based on novelty and relevance suchas Eqn. 6, the parameter in Proposition 2 also implies that usingthe base policy as the initial policy may not be the best choice. Thebase policy is expected to be significantly better than a randompolicy at relevance, so it will assign higher than random probabilityto relevant itemsboth novel and not novel items. However, withinrelevant items, by definition, its probability for novel items will belower than that of the not novel items. To increase the probabilityof the optimal action under the initial policy even further (anddecrease ), we can use additional training data to finetune the basepolicy using supervised learning (e.g., InfoNCE loss from Eq. 1).We call this model the Supervised Model. As long as the relevance",
  "Proposed algorithm: PG-Ret": "The above discussion indicates the following conditions for a fastconvergence rate with policy gradient, 1) the action space shouldbe small; and 2) the initial policy should be as close to the optimalas possible. We addressed the first condition through a binary-action formulation of the problem. For the second condition, weproposed initializing the policy optimization with a policy trainedusing supervised learning (i.e., using Eq. 1).Algorithm 1 shows the resultant training algorithm. In eachbatch, query-item pairs (states) are sampled and the reward iscomputed for each state. Using Eqn. 8, we compute the gradientand update the policy parameters at the end of the batch. For eachapplication, one needs to specify the sampling procedure for query-item pairs, the reward function, and the estimation procedure for(|,).Sampling query-item states. Since items are also a part of thestate in our proposed formulation, a key question is how to samplethe states ,. To map to the original policy gradient formulation,we can sample queries randomly from the train dataset for eachbatch. For each query, we compute the similarity scores with allitems using the encoder corresponding to the current policy andthen sample items proportional to their score. Note that we are notrestricted to only sampling items proportional to their similarityscore (since items are not actions now). Therefore, we also addexploration by sampling items from another retrieval model (trainedindependently). For example, for novelty, a natural choice is tosample from predictions of the base model, restricting the samplingonly to items that are ranked beyond top-L. Such items will benovel by definition and thus we only need to check for relevance.More generally, we may use any pretrained encoder suitable for thetask. Finally, there is a risk that the policy overfits to the rewardfrom the relevance oracle and forgets the true user feedback dataon which the supervised policy was trained . Thus, we shouldalso add the user feedback data (,) during training. To thisend, in practice, all three sources are combined stochastically: wesample a query randomly from the dataset and then sample either () with probability ; () with probability; or D () with probability 1 .Estimating (|,). While (|,) is defined as a softmax op-eration, computing the softmax over all items is a compute-intensiveprocedure. Moreover, training procedures for recommendation sys-tems have benefitted from using contrastive losses. Therefore, wewe implement an approximation of the softmax using contrastivelosses. A straightforward approximation is to only use the items inthe current batch as negatives (random in-batch negatives). How-ever, since we initialize with a well-trained supervised policy, we may find that most of the contrastive losses are zero since the cho-sen item is already closer to the query than random negatives. Tospeed up training, we use negatives that are aimed at optimizingnovelty. Specifically, for each query, we use the top-M items re-turned by the base model as negatives. is a hyperparameter; toohigh may destroy all relevance. Therefore, we define two kinds oflosses: 1) Aggressive-Novelty: InfoNCE loss from Eqn. 1 with top-Mnegatives (typically M is small, e.g., = 5); 2) Conservative-Novelty:Triplet loss , bounded and margin=0 (effectively = 1).Efficiency considerations. Note that to compute top-k items orsample items for a given query, the entire set of actions have to beencoded by the current encoder . For computationally efficiency,we fix the item encoder to be the initial encoder and only update thequery encoder. This avoids having to recompute the item embed-dings for each query in the batch. That is, only the query encoderis updated during policy optimization.",
  "Setup: Datasets, Metrics, and Baselines": "Setup. We consider the production setup for a recommendationsystem, wherein there is an existing retrieval algorithm in produc-tion. We call this model the base model. Typically, this model istrained on millions of query-item clicked pairs collected from logdata. We assume access to a small amount of new training data(e.g., query-item pairs collected from log data after the base modelhas been deployed) and evaluate different ways to produce a newretrieval model with high novelty compared to the base model. We",
  "consider an independent test set for evaluation. As in prior work ,the candidate pool of items remains the same across train and testdatasets, typically of the order of millions of items": "Datasets. On the query-document matching task, we use a datasetfrom a commercial search engine and a public dataset (see ).We also use a Amazon dataset for product recommendation. Query-keyword recommendation. This dataset contains queriesand ad keywords from a commercial search engines logs. Givena query, the goal is to predict ad keywords that share the same in-tent or a more general intent than the query (i.e., phrase match).The base model is a 4-layer bi-encoder model that has been fine-tuned with contrastive loss on more than 20M query-keywordpairs. The training data consists of 1M new query-item pairs,along with a candidate pool of over 33 million items. ORCAS . This public dataset contains clicked webpages fromBing for queries from the TREC deep learning challenge. For ourevaluation, we consider the search query as the user input andthe webpage title as the item. We filter the dataset to remove clickdata where either the query or the webpage title are empty. Thedataset contains 17.5 million query-webpage pairs. To simulate aproduction setup, we utilize the majority (16.5M) of the datasetfor training a supervised model, that acts as the base model. Thebase model is initialized with SimCSE and trained for 5 epochs.The remaining 1M are used as new training data for optimizingnovelty. We also reserve a separate, randomly sampled test setconsisting of 8.5K query-keyword pairs. AmazonReviews . This dataset contains users product re-view histories on Amazon. Based on a users previous reviews,the goal is to predict the next product that they would review. Liet al. convert it to text-based problem by represent items asa text sequence using their metadata (e.g., Title: Philips motorCategory: Home Appliances Color: Black). Users are representedas a text concatenation of each item in their profile. For our exper-iments, we consider the Industrial & Scientific domain consistingof over 11K user histories and 5K products. Li et al. sort eachusers history by time and break it down into a train set, valida-tion set (second-last product), and a test set (most recent productin the history). We use the train set for training the base model,which is initialized with the pre-trained RecFormer model from. To simulate the production scenario where users reviewadditional items over time, we use augmented review historyfor finetuning novelty models that includes both the train andvalidation set products (Finetuning dataset). In both cases, thetest set remains identical and we ensure that the test set is neverused during training (since the finetuning stage does not havea validation set for early stopping; we train models for a fixednumber of epochs). Relevance Feedback. For all datasets, we use GPT-3.5 as the re-ward model for providing relevance feedback during training. Forthe first dataset where the goal is produce keywords with moregeneral intent than the query, we use the prompt,",
  "Provide a brief reasoning and then the final answerwithin Yes/No tag": "Metrics. We evaluate PG-Ret on the following offline metrics. Novelty@k: Novelty of top-k items compared to the base model,as defined in Definition 1. For the query-keyword dataset, we useL=200 for the base model since each retrieval model sends roughly200 keywords to the downstream ranking layer. For ORCAS andAmazonReviews dataset, we use L=50 since we observe that therelevance of predicted items decreases significantly beyond 50items.",
  "Recall@k is the number of clicked <query, item>pairs from testdata that are in the top-k predictions. As novelty is optimized,recall over the top-k items should not decrease substantially": "Precision@k: While recall is an important metric, it is depen-dent on the available clicks in the test data for each query. It ispossible that a model makes relevant predictions but they arenot counted since those <query, item>pairs do not exist in thetest data. Hence, we use an advanced LLM, GPT-4 as the rele-vance evaluator for top-k items. Note that we use a differentLLM for evaluation than the one used in training because 1) Amore capable model like GPT-4 can provide more reliable rele-vance feedback; 2) using a different model ensures fairness ofevaluation when compared to other baselines. Moreover, we usestandardized prompts that are used in the production system forevaluating retrieval models. For each task, these prompts havebeen validated against human-labelled data and they achievemore than 85% accuracy, thus serving as a reliable approxima-tion of human feedback. The first task uses a prompt tuned forestimating phrase-match relevance of keyword and the second",
  ": Example predictions from base model and PG-Ret. Bolded items are novel compared to top-200 from the base model": "task uses a prompt tuned for estimating the general relevance ofa query and a webpage title.In addition, we evaluate PG-Ret using an A/B experiment for thefirst task of matching query to relevant ad keywords.Baselines. For each dataset, we compare PG-Ret to a supervisedmodel initialized with the base model and trained on the sametraining data using the InfoNCE loss with random negatives (Eq. 1).All models are trained using Adam optimizer with a learning rateof 105 and batch size of 128. PG-Ret (Aggressive) uses = 5. Forthe AmazonReviews dataset, we use the same hyperparameters asin .",
  "Query-keyword recommendation task": "Novelty. For the query-keyword dataset, the goal is to increasethe novelty of policys recommendations. For training PG-Ret, weuse the reward from Equation 7. The results are shown in .Compared to supervised finetuning on the same training set, PG-Ret leads to 2X-5X gains in novel keywords in top-50. In top-200,PG-Ret (Conservative) and PG-Ret (Aggressive) obtain 108 and 152novel keywords respectively compared to 79 from the supervised finetuned model. At the same time, recall of PG-Ret (Conserva-tive) is almost the same as the base model. In fact, recall@200 isslighly higher than the base model. To check the quality of the PG-Ret models, we also evaluate precision@3 as evaluated by GPT-4.While we would expect the precision to decrease due to the noveltyloss, we find that PG-Ret (Conservative) has a substantially higherprecision than the base model. This may be possible because thenovelty loss can encourage the model to move away from localminima and sometimes find a more optimal solution. In comparison,PG-Ret (Aggressive) suffers a significant drop in both recall andprecision, indicating that novelty optimization has led to a decreasein the models accuracy. The offline results indicate that PG-Ret(Conservative) is a good balance between novelty and accuracy. shows qualitative results for a sample of the querieswhere PG-Ret led to novel keywords in the top-5 predictions. Thebase model tends to match keywords based on lexical overlap whear-eas PG-Ret is able to find rephrases with the same meaning. A/B test. Finally, we evaluate PG-Ret (Conservative) on real usertraffic using an A/B experiment over a 10 day period. As mentionedbefore, the retrieval system is engineered such that keywords froma new algorithm (e.g., PG-Ret) are selected for downstream ranking",
  ": Novelty and Recall on the AmazonReviews dataset. Compared to supervised finetuning, PG-Ret obtains substantialgains in novelty with almost the same recall": "layer (which in turn, may lead to user impressions) only if theyare novel compared to the existing algorithm. By including PG-Ret in the retrieval pipeline, we observe a 1% increase in query-admatching density, the average number of relevant ads selected perquery (as determined by the downstream ranker). We also observea 0.14% increase in coverage, the fraction of queries for whichrelevant ads are shown to users. This indicates that the noveltyoptimization can help to match ads to the queries that other algosare not able to match relevant ads with. Finally, we also observea 0.26% increase in click yield, the number of ad clicks per searchquery, indicating the new keywords recommended by the noveltyoptimization are well received by the real-world users. While theabsolute number may look small, an increase of 0.26% can lead to asubstantial impact when scaled to millions of users.",
  "ORCAS: Query-webpage matching": "To simulate the production setting, we train a supervised model onover 16M <query, webpage title>pairs as the base model. Our goalis to produce top-k webpages that are novel wrt. top-50 webpagespredicted by the base model, using a training set of 1M pairs. We firstuse the InfoNCE loss to finetune the base model over the 1M trainingdataset (Supervised Finetuning). PG-Ret model is finetuned usingour proposed method using the base model as the initialization. shows the results. We report results for two supervisedfinetuning models (epochs 3 and 10). In both models, the recall andprecision decreases compared to base model, indicating that the1M train set may lead to overfitting. PG-Ret (Conservative) alsoleads to a drop in recall but the corresponding novelty is signifi-cantly higher than that of a supervised model with similar recall.At comparable recall, PG-Ret (Conservative) obtains Novelty@50of 2.64 compared to 0.65 for the supervised model (3 epochs). Over-all, PG-Ret (Aggressive) obtains the highest noveltyon average,there are 4.03 webpages in top-50 predictions of PG-Ret (Conserva-tive) that did not exist in top-50 predictions of the base model. Thenovelty is significantly higher than a supervised model (2.82, 10epochs) with similar recall. Recall and precision of PG-Ret models",
  "Amazon: User-Product recommendation": "shows the novelty and recall metrics for the AmazonRe-views dataset. Both Supervised Finetuning and PG-Ret models areinitialized with the base pretrained RecFormer model and trained onthe finetuning set. Since these models are trained on an additionalrecent product from the users history, both models obtain slighlyhigher recall than the base model. However, at the same recall,novelty of the PG-Ret (Conservative) model is significantly higherthan the supervised model. On average, PG-Ret recommends 3.6products in its top-10 list that are novel wrt. the top-50 recommen-dations from the base model, compared to 1.9 novel products forthe supervised model. As in the previous datasets, PG-Ret leads toimproved novelty while incurring a minimal loss in recall comparedto the supervised model.",
  "CONCLUSION": "We presented a technique to optimize a non-differentiable, task-specific loss in information retrieval applications. We justified thebinary-action formulation of the problem through theoretical andempirical results. On empirical recommendation datasets, the pro-posed technique leads to substantial gains in novelty of top-k items.While we used the simple REINFORCE-based RL algorithm, fu-ture work can consider Actor-critic or proximal policy optimizationalgorithms for optimizing novelty and how they can be extended tolarge action spaces. Exploring robust reward functions in the pres-ence of noisy LLM feedback is also an important future direction. Daman Arora, Anush Kini, Sayak Ray Chowdhury, Nagarajan Natarajan, Gau-rav Sinha, and Amit Sharma. 2023. GAR-meets-RAG Paradigm for Zero-ShotInformation Retrieval. arXiv preprint arXiv:2310.20158 (2023). Minmin Chen, Alex Beutel, Paul Covington, Sagar Jain, Francois Belletti, andEd H Chi. 2019. Top-k off-policy correction for a REINFORCE recommendersystem. In Proceedings of the Twelfth ACM International Conference on Web Searchand Data Mining. 456464.",
  "Neil Hurley and Mi Zhang. 2011. Novelty and diversity in top-n recommendationanalysis and evaluation. ACM Transactions on Internet Technology (TOIT) 10, 4(2011), 130": "Marius Kaminskas and Derek Bridge. 2016. Diversity, serendipity, novelty, andcoverage: a survey and empirical analysis of beyond-accuracy objectives inrecommender systems. ACM Transactions on Interactive Intelligent Systems (TiiS)7, 1 (2016), 142. Ee Yeo Keat, Nurfadhlina Mohd Sharef, Razali Yaakob, Khairul Azhar Kasmiran,Erzam Marlisah, Norwati Mustapha, and Maslina Zolkepli. 2022. MultiobjectiveDeep Reinforcement Learning for Recommendation Systems. IEEE Access 10(2022), 6501165027.",
  "Phuc H Le-Khac, Graham Healy, and Alan F Smeaton. 2020. Contrastive represen-tation learning: A framework and review. Ieee Access 8 (2020), 193907193934": "Jiacheng Li, Ming Wang, Jin Li, Jinmiao Fu, Xin Shen, Jingbo Shang, and JulianMcAuley. 2023. Text is all you need: Learning language representations forsequential recommendation. In Proceedings of the 29th ACM SIGKDD Conferenceon Knowledge Discovery and Data Mining. 12581267. Lihong Li, Wei Chu, John Langford, and Robert E Schapire. 2010. A contextual-bandit approach to personalized news article recommendation. In Proceedings ofthe 19th international conference on World wide web. 661670. Yong Liu, Zhiqi Shen, Yinan Zhang, and Lizhen Cui. 2021. Diversity-promotingdeep reinforcement learning for interactive recommendation. In 5th InternationalConference on Crowd Science and Engineering. 132139.",
  "via Policy Gradient for Semi-structured Mathematical Reasoning. In The EleventhInternational Conference on Learning Representations": "Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. 2020. Onthe global convergence rates of softmax policy gradient methods. In InternationalConference on Machine Learning. PMLR, 68206829. Ali Montazeralghaem, Hamed Zamani, and James Allan. 2020. A reinforcementlearning framework for relevance feedback. In Proceedings of the 43rd internationalacm sigir conference on research and development in information retrieval. 5968.",
  "Guy Shani and Asela Gunawardana. 2011. Evaluating recommendation systems.Recommender systems handbook (2011), 257297": "Xiaoyu Shi, Quanliang Liu, Hong Xie, Di Wu, Bo Peng, MingSheng Shang, andDefu Lian. 2023. Relieving popularity bias in interactive recommendation: Adiversity-novelty-aware reinforcement learning approach. ACM Transactions onInformation Systems 42, 2 (2023), 130. Dusan Stamenkovic, Alexandros Karatzoglou, Ioannis Arapakis, Xin Xin, andKleomenis Katevas. 2022. Choosing the best of both worlds: Diverse and novelrecommendations through multi-objective reinforcement learning. In Proceedingsof the Fifteenth ACM International Conference on Web Search and Data Mining.957965.",
  "Ronald J Williams. 1992. Simple statistical gradient-following algorithms forconnectionist reinforcement learning. Machine learning 8 (1992), 229256": "Xuansheng Wu, Huachi Zhou, Yucheng Shi, Wenlin Yao, Xiao Huang, and Ning-hao Liu. 2024. Could Small Language Models Serve as Recommenders? TowardsData-centric Cold-start Recommendation. In Proceedings of the ACM on WebConference 2024. 35663575. Ruobing Xie, Shaoliang Zhang, Rui Wang, Feng Xia, and Leyu Lin. 2021. Hierar-chical reinforcement learning for integrated recommendation. In Proceedings ofthe AAAI Conference on Artificial Intelligence, Vol. 35. 45214528. Jun Xu, Zeng Wei, Long Xia, Yanyan Lan, Dawei Yin, Xueqi Cheng, and Ji-Rong Wen. 2020. Reinforcement Learning to Rank with Pairwise Policy Gra-dient. In Proceedings of the 43rd International ACM SIGIR Conference on Re-search and Development in Information Retrieval (Virtual Event, China) (SI-GIR 20). Association for Computing Machinery, New York, NY, USA, 509518. Pengfei Zhao and Dik Lun Lee. 2016. How much novelty is relevant? it dependson your curiosity. In Proceedings of the 39th International ACM SIGIR conferenceon Research and Development in Information Retrieval. 315324.",
  "(15)": "where the third equality is because the initial state distributionis the distribution of queries in the training data.Now, using Assumption 2, the minimum initial probability forthe optimal action is1 for all states. Assuming that the gradientupdates do not decrease the probability of the optimal action, =inf, 1 (()|) =1. Substituting c in the above equation,we obtain the result."
}