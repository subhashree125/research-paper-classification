{
  "ABSTRACT": "Deploying pre-trained transformer models like BERT on down-stream tasks in resource-constrained scenarios is challenging dueto their high inference cost, which grows rapidly with input se-quence length. In this work, we propose a constraint-aware andranking-distilled token pruning method ToP, which selectively re-moves unnecessary tokens as input sequence passes through layers,allowing the model to improve online inference speed while pre-serving accuracy. ToP overcomes the limitation of inaccurate tokenimportance ranking in the conventional self-attention mechanismthrough a ranking-distilled token distillation technique, which dis-tills effective token rankings from the final layer of unpruned mod-els to early layers of pruned models. Then, ToP introduces a coarse-to-fine pruning approach that automatically selects the optimalsubset of transformer layers and optimizes token pruning decisionswithin these layers through improved 0 regularization. Extensiveexperiments on GLUE benchmark and SQuAD tasks demonstratethat ToP outperforms state-of-the-art token pruning and modelcompression methods with improved accuracy and speedups. ToPreduces the average FLOPs of BERT by 8.1 while achieving com-petitive accuracy on GLUE, and provides a real latency speedup ofup to 7.4 on an Intel CPU. Code is available at here 1.",
  "Work was done during the internship at Microsoft ResearchCorresponding author1": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 23, August 610, 2023, Long Beach, CA, USA 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0103-0/23/08...$15.00",
  "Transformer; Token Pruning; Inference Acceleration": "ACM Reference Format:Junyan Li, Li Lyna Zhang, Jiahang Xu, Yujing Wang, Shaoguang Yan, Yun-qing Xia, Yuqing Yang, Ting Cao, Hao Sun, Weiwei Deng, Qi Zhang, and MaoYang. 2023. Constraint-aware and Ranking-distilled Token Pruning for Ef-ficient Transformer Inference. In Proceedings of the 29th ACM SIGKDDConference on Knowledge Discovery and Data Mining (KDD 23), August610, 2023, Long Beach, CA, USA. ACM, New York, NY, USA, 11 pages.",
  "INTRODUCTION": "Pre-trained transformer models have achieved greatsuccess for a wide variety of NLP tasks. However, the superiorperformance comes at the cost of increasingly larger model sizes andcomputation overhead, making it difficult to efficiently deploy themon different downstream tasks in various latency-critical scenariossuch as online servers and edge devices.Accelerating transformer inference is often achieved throughmodel compression methods such as pruning , quantiza-tion , and knowledge distillation . These techniquesaim to reduce the size of the model, with quantization and distilla-tion resulting in a smaller, fixed model. Structured pruning, whicheliminates redundant heads or dimensions, can effectively meetdeployment requirements . However, structured pruningmay not guarantee optimal accuracy, particularly for small trans-formers or long input sequences, as the attention mechanism hasa (2) computation complexity with input token length . Thismeans a significant portion of the model must be pruned to meettight deployment constraints, potentially compromising accuracy.Recently, a promising subfield in NLP has emerged that focuseson reducing latency during model inference by pruning input to-kens. Its based on the intuition that not all tokens in the inputsequence are critical for making a final prediction. As tokens passthrough the encoder layers, some tokens have been captured byother tokens via attention in the early layer and do not require",
  "KDD 23, August 610, 2023, Long Beach, CA, USAJunyan Li et al": ": Comparison of token importance score distribu-tions on the GLUE MRPC dataset using attention values fromthe BERT model. Note that the y-axis is log-scaled to bet-ter visualization of tokens with low importance scores. However, the attention values scoring in Equation 3 can inac-curately measure token importance in early transformer layers.Previous works have shown that some important tokens,such as [SEP], receive little attention in early layers but gain in-creased attention in deeper layers. As such, these critical tokensobtain low importance scores in Equation 3 initially and higherscores in deeper layers. However, this creates a problem wherecrucial tokens may be misclassified as unimportant in early layersand removed before they have the chance to reach deeper layers,where they would have received the correct importance scores.To validate this, we analyze importance score distribution in atrained BERT model finetuned with the GLUE MRPC dataset. compares the distribution of importance scores at layers 1,2, 9, and 12. Tokens are selected based on their ranking positionsaccording to the importance scores. The scores of top3 tokens indeep layers, such as layers 9 and 12, are significantly higher thanthose of less important tokens ranked in the top10-15 and top20-25.However, at early layers such as layers 1 and 2, the scores for top3tokens are not well distinguishable from those of other low-rankedtokens. This indicates that early layers tend to assign relativelysimilar scores to both top-ranked and low-ranked tokens, poten-tially resulting in important tokens being deemed unimportant.As a result, while being computationally efficient, using attentionvalue-based scoring can be problematic at early layers.",
  "RELATED WORKS2.1Model Compression": "To reduce the inference cost of pre-trained transformer models, avariety of compression techniques have been proposed, includingweight pruning , quantization and distilla-tion . Token-level pruning has been shown to complementknowledge distillation and quantization . Here, we focus onpruning and distillation and briefly discuss the related work.Weight pruning is categorized into 1) unstructured and 2) struc-tured pruning. Unstructured methods achieve high sparsitywithout accuracy drop but offer minimal latency benefits due toirregular sparse patterns. In contrast, structured pruning removescoherent weight groups, reducing latency without special hardwaresupport. CoFi achieves 10 speedup with a small accuracy dropby jointly pruning layers, attention heads, FFN, and hidden units.SwiftPruner is a latency-aware pruning method that finds opti-mal layer-wise pruning policies under a given latency requirementthrough AutoML. However, structure pruning may result in a lossof accuracy when the deployment requirements are highly con-strained and the downstream task has a long input sequence. This",
  "Constraint-aware and Ranking-distilled Token Pruning for Efficient Transformer InferenceKDD 23, August 610, 2023, Long Beach, CA, USA": ": Our approach learns layer gate masks and tokenranking masks to prune tokens under a desired constraint.When a layer gate turns off (i.e., mask=0), we skip the currentlayer. When a layer gate turns on (i.e., mask=1), unimportanttokens are removed after the self-attention mechanism. gate masks progressively activate earlier layers, starting from thedeepest one. Eventually, the gate masks activate all transformerlayers under high pruning sparsity. Inspired by the observation,we gradually increase the sparsity level from 0 to the target duringpruning. By doing this, gate masks provide improved decision-making for pruning fewer tokens within early layers. This is becausegate masks prioritize learning masks in deeper layers, where tokenimportance is more easily predictable by self-attention values. Token ranking position mask . For fine-grained tokenpruning, assigning a mask to each token and removing those with amask value of 0 may seem an intuitive solution. However, it can leadto a problem known as \"static token pruning.\" This occurs becausethe final mask values are fixed after training, causing tokens to bepruned at the same positions for all input sequences in the datasetduring inference. This can be problematic as informative tokenscan appear in different positions across different input sequences.Removing tokens at the same positions for all input sequencescan also inadvertently remove important tokens and result in asignificant loss of accuracy.To address this challenge, we follow PoWER-BERT to useranking masks (see ). Instead of applying a mask to eachtoken directly, we mask tokens ranking positions based on theirimportance score, which is computed by utilizing attention values,as outlined in Equation 3. The insight is that by scoring tokensbased on their importance to the final model prediction, crucialtokens will always rank in the topmost positions after sorting. Thismeans that although informative tokens may appear in differentpositions based on input content, their ranking positions are static(i.e., topmost) and can be indicated by a static mask. For example,given an input sequence of 1 = (1,2, ...) for layer , wesort the tokens by their importance scores, resulting in a rankingof (, 3, ...1). The corresponding ranking masks are then definedas (,, ,3, ...,1), where the value of , indicateswhether prune or keep the ranked tokens for layer .",
  "BACKGROUND AND MOTIVATIONS3.1Background": "Transformer models, such as BERT , are stacked up with multi-ple encoder layers. A basic transformer layer wraps a multi-headself-attention (MHA) and feed-forward (FFN) layer with residualconnection and layer normalization (LN). Given an input sequenceof tokens and hidden size of , the hidden state of the layer, = (1,2, ...) R, is computed from the previous layer:",
  "Attention = ( )(2)": "Complexity analysis. The above self-attention measures the pair-wise importance of each token on every other token in the input,and the total complexity of MHA layer is (2 + 2), whichis quadratic with . For FFN layer, the computation complexityis (2), which is linear with . When applied to long input se-quences (i.e., a large ), the computation and memory of MHA layergrow quadratically and become very expensive.To address this limitation, our work introduces token pruning,where unimportant tokens are gradually dropped as the inferenceproceeds. For each transformer layer, which initially has tokens,we aim to remove a specific number of unimportant tokens fromthem. These removed tokens will not be considered in subsequentlayers. This leads to a linear (for FFN) or quadratic (for MHA) reduc-tion in operations, resulting in significantly faster model inference.Identifying unimportant tokens to be discarded is a major chal-lenge in token pruning. Current methods address this by eitherusing self-attention values to assign a score to each token, or byadding a prediction module to predict scores. However, both meth-ods have their limitations, which will be discussed in detail.",
  "=1(3)": "where denotes the number of heads. indicates theattention received by token from on head . Thus, token isconsidered important if it receives more attention from all tokensacross all heads. The term can reuse the results fromthe self-attention mechanism in Equation 2. Therefore, attentionvalue-based scoring in Equation 3 is computationally lightweight.",
  "Limitations of prediction-based methods": "In contrast to the attention value-based scoring method, prediction-based methods incorporate an additional neural networkto predict the importance score for each token. The recently pro-posed Transkimmer adds an extra prediction module beforeeach transformer layer, which composes of 2 linear layers witha layernorm , GeLU activation and GumbelSoftmax .These prediction modules gradually update their parameters andlearn to predict which tokens should be pruned, which has beenshown to outperform attention value-based approaches.However, prediction-based token pruning faces limitations inachieving real latency reduction. First, the prediction module it-self introduces additional inference latency and FLOPs. As shownin , the additional FLOPs and latency introduced by Tran-skimmer prediction modules account for 8.43% and 30% of BERT,respectively. This suggests that token pruning needs to prune much",
  "BERT (12L-768H)+8.43%+11.82%+29.35%BERT6 (6L-512H)+8.48%+12.95%+28.79%BERT4 (4L-256H)+8.63%+23.53%+30.50%": ": Compared to the original model ( input tokenlength=256), Transkimmer introduces significant infer-ence latency on both CPU and GPU due to prediction modulescoring. FLOPs are calculated using thop , while latencymeasurements are obtained using onnxruntime . more tokens to counteract the 30% latency slowdown. Second, Tran-skimmers dynamic token pruning relies on the computationally-intensive GumbelSoftmax operation, necessitating specialized run-time and hardware support for efficient implementation.In our work, our goal is to implement token pruning in practicalapplications that accelerate real inference latency. While prediction-based approaches can be challenging in achieving actual latencyreduction, we instead leverage attention values for token pruning.",
  "METHODOLOGY": "In this section, we present ToP, a novel token pruning approachthat incorporates two key techniques for learning the optimal tokenpruning decisions. First, we introduce the end-to-end token pruningalgorithm that leverages 0 regularization. Then, we describe theranking-aware token distillation approach that enhances the abilityof self-attention values to rank token importance. 4.1Constraint-aware token pruningFor a given deployment constraint, we propose constraint-awaretoken pruning to remove a set of unimportant tokens so that themodel with the retained tokens can achieve the best accuracy underthe constraint. The basic idea is (1) we introduce a set of binarydecision masks {0, 1} to represent the sparsity ratio and indicatewhether to drop ( = 0) or keep each token ( = 1); (2) use thesemasks to construct a constraint-aware loss function; (3) optimizethe constraint-aware loss using an improved 0 regularization method. Next, we will introduce the details.Unlike prior works that treat all layers inthe same manner, leading to a vast design space, we introducea novel coarse-to-fine token pruning scheme, as shown in .Specifically, gate masks are used to select a subset of layers fortoken pruning, while token ranking masks dynamically determinewhich specific tokens within the selected layers should be pruned. Layer gate mask . We introduce a gate mask for eachtransformer layer to control whether token pruning is performedin that layer. Concretely, if is set to 1, token pruning will beapplied to layer . If is 0, layer will be skipped, and retainthe tokens from its previous layer 1.The design choice is built on two insights. First, it has beenobserved that pre-trained transformer models have varying levelsof token redundancy across different layers . Second, pruningtokens across all layers significantly expands the design space andposes unnecessary difficulty in optimization, particularly whentrying to maintain a low or medium level of compression sparsity.In our experiments, we observe an interesting trend where asthe target sparsity increases (i.e., constraint becomes tighter), the",
  "+=1(2 d )(4)": "where the two items calculate the FLOPs of MHA and FFN layers,respectively. denotes the hidden size, is the number of headsin MHA layer, denotes FFN intermediate size. represents thenumber of tokens that are retained for the layer, which can beeasily computed by multiplying E( > 0) with the originaltoken length . Note that when the gate mask turns off fine-grained token pruning, keeps the same number of tokens withits previous layer (i.e., =1). Learning masks under a desired constraint. Now we introducehow to determine the values of gate masks and ranking masksfor minimal accuracy loss under a given FLOPs constraint. Let denote the original model and denote all the pruning masks.We formalize the task of token pruning as an end-to-end learningproblem by adding a regularization term :",
  "= (, ) + ()(5)": "Optimizing the above loss function requires the masks and to be differentiable. However, the original masks are discretebinary values. To overcome this, we use the 0 reparameterizationmethod proposed by , which is specifically designed for modelpruning. Following the standard 0 reparameterization, masks are regulated using the hard concrete distribution as follows",
  "(6)": "where (0, 1) is a uniform distribution in the interval ; < 0and > 0 are two constants that stretch the sigmoid output into theinterval (,). is a hyperparameter that controls the steepness ofthe sigmoid function. We adopt the common practice of setting to-0.1, to 1.1 and to 2 3. = { }| |=1 are the main learnable param-eters. The hard concrete distribution assigns a significant portion ofprobability mass on the integer values {0,1}, which serves as a goodcontinuous approximation of the binary (Bernoulli) distribution.During training, the hard concrete parameters and deter-mine the values of masks . We learn masks by updating theselearnable parameters of the distributions from which the masks aresampled in the forward pass. Moreover, these learnable parame-ters and masks can be jointly optimized with the original modelparameters, resulting in better performance.In prior token pruning works , 1 is widely used asthe regularization loss in Equation 5, and controls the trade-offbetween final model accuracy and the achieved sparsity. However, itrequires careful hyper-parameter tuning to make sure it convergesto a desired sparsity , which lacks effective control on thecomplexity of final model inference.",
  ": Our ranking-aware token distillation uses impor-tance rankings generated from the unpruned models finallayer and distill it to early layers during the training": "In our work, we aim to control the achieved model FLOPs aftertoken pruning. We follow to replace the vanilla 0 objectivewith a Lagrangian multiplier. Let be the target FLOPs, () bethe expected FLOPs determined by the masks in Equation 4. Weimpose an equality constraint () = by introducing a penalty:",
  "Distillation of token importance rankings": "In our proposed constraint-aware token pruning algorithm, theranking masks are applied to the sorted tokens based on their im-portance scores. Ideal importance scoring mechanism that providesreliable token importance rankings is essential for maintaining theperformance of the final model prediction. The most crucial tokenswill be at the top and retained by the ranking masks.In ToP, we utilize self-attention values to calculate token impor-tance (Equation 3) for ease of hardware deployment. However, asdiscussed in Sec. 3.2, accurately ranking the importance of tokensusing the self-attention mechanism is challenging in the early lay-ers of the transformer. Despite the potential for improved tokenrankings in deeper layers, crucial tokens may be mistakenly rankedas unimportant and removed before they have the opportunity toreach these layers, leading to a substantial decrease in accuracy.To overcome this limitation, we propose a method to distill moreaccurate ranking decisions to the early layers during the trainingprocess, enhancing the ability of self-attention values to rank tokenimportance. To this end, we enable the preservation of the mostimportant tokens during inference, thus improving overall accuracy. Ranking-aware token distillation. Before distillation, we needto obtain accurate token importance rankings as the knowledge.However, obtaining such ground truth or labeled rankings is chal-lenging. Our experiments, as shown in , suggest that attentionvalues in final layers can effectively identify important tokens. Thus,we use the token importance rankings generated from the finaltransformer layer of the unpruned model as our knowledge source.The next challenge is to develop an effective distillation objective.Previous studies have shown that combining distillationwith model weight pruning can improve performance. These studieshave used distillation objectives based on cross-entropy loss or Mean Squared Error (MSE) loss between the student and teacher layer-wise representations . However, when it comes totoken distillation, the conventional MSE loss may not be suitabledue to the different levels of information captured by early and deeplayers of transformer models like BERT . The representationsof early and deep layers are meant to be distinct, making it difficultto use conventional distillation objectives.Instead, we use token importance rankings from the final layerof the teacher model as the information to be distilled and developa ranking-aware distillation loss. Our objective is to align the tokenimportance rankings of the early transformer layers with those ofthe final layer in the teacher model. This way, self-attention layersare encouraged to give more importance to the crucial tokens,increasing the chance of retaining them in early layers. illustrates the overview process of our proposed ranking-aware token distillation. To effectively distill the knowledge, weadopt a ranking loss as the distillation objective. Specifically, we usethe LambdaLoss as proposed in to optimize Normalized Dis-counted Cumulative Gain (NDCG), which is a widely used rankingmetric in information retrieval systems that places more emphasison the importance of top-ranked tokens.Specifically, let denote the rankings obtained by sorting theimportance scores of each token in the final layer in the teachermodel. refers to the token importance scores in the layer ofthe student model, and represents a mini-batch of training data.We define the distillation loss as follows:",
  "Training and inference": "We now describe the full training process of our ToP. Given a deployconstraint, ToP simultaneously trains the masks and model param-eters, allowing for effective token pruning and improved modeladaptation to token sparsification. Additionally, we incorporateranking-aware distillation to enhance the ability of the attentionvalues in determining the importance of tokens in early layers. Thefull training objective is a combination of the above objectives:",
  "= (, ) + () + (9)": "where () is defined as in Equation 7, and is a hyperparam-eter controlling the contribution of distillation loss. In particular,the two hyperparameters 1 and 2 introduced by () are au-tomatically adjusted using the AdamW optimizer. Therefore, theonly additional hyperparameter that needs to be tuned is .Once the training finishes, the token pruning decisions are deter-mined by combining the gate masks and ranking masks. Specifically,only the layers chosen by the gate masks with = 1 are as-signed with ranking masks for token selection. The other layersare not considered for token pruning.During the inference, we use self-attention values to calculatetoken importance scores in the selected layers. We then sort thetokens based on these scores. By discarding the tokens through useof ranking masks ( = 0), we can effectively eliminate unnecessarytokens and improve the efficiency of the model.",
  "EXPERIMENTS5.1Experiment Setup": "Datasets and Models. We evaluate ToP on a diverse set of datasets,including 8 classification and regression tasks from the GLUE bench-mark , the SQuAD-v2.0 extractive question answeringdataset, and the 20News long sequence classification dataset.The diversity of these tasks and the range of token lengths (i.e.,64, 128, 256, 384, 512) showcase the general applicability of ourmethod. A detailed summary of these datasets can be found inAppendix. To demonstrate the generality of our approach on bothlarge and small pre-trained language models, we implement ToPon RoBERTa , BERT and BERT6.Training setup. Following existing works , we performfine-tuning on the downstream datasets before the token pruning.Then we conduct token pruning with the optimization objectivein Equation 9 under multiple FLOPs constraints. We use FLOPssparsity to denote the constraint, which is defined as the ratioof FLOPs that have been pruned or removed from the model. Wealso define FLOPs reduction as the ratio of the full model FLOPsunder the original input sequence length to the remaining FLOPs.The token pruning process begins with a warmup stage wherewe gradually increase sparsity to the target value using a linearscheduler. The initial value of is determined by hyper-parametergrid search from [1e-2, 1e-3, 1e-4] on development set with 20%data randomly picked from training set. At the start, layer gatemasks are initialized to 0, and ranking masks are initialized to 1,meaning all tokens are retained. After reaching the final sparsitylevel, we fix it and continue training the model and pruning masksuntil convergence (i.e., fine-tuning stage). See Appendix for detailedsettings of other hyper-parameters. We conduct all experimentswith random seed 57.Baselines. We compare ToP with the state-of-the-art token prun-ing and model compression methods. Token pruning baselinesinclude attention value-based methods such as PoWER-BERT ,LAT , LTP , and strong prediction-based method Transkim-mer . Model compression baselines include DistilBERT , andCoFi , which is a state-of-the-art structured pruning method.",
  "Main results": "Overall performance. We evaluate ToP on both BERT andRoBERTa, and compare with the state-of-the-art token pruningbaselines. LTP lacks an implementation for BERT, we followtheir official code and implement for BERT. For a fair compari-son, we follow the provided instructions and conduct a grid searchfor the optimal hyper-parameters. To prove ToPs effectiveness onvarying input sequence lengths, we conduct experiments on boththe GLUE benchmark and two long sequence tasks.We start by comparing ToP to the original BERT and RoBERTa.As shown in and , ToP achieves comparable accu-racy on GLUE benchmark, SQuAD and 20News while significantlyreducing FLOPs. For instance, ToP even outperforms the originalBERT with +2.7%, +4.3%, +0.3%, and +0.5% improvement on CoLA,RTE, MRPC, and SST-2 respectively, and achieves an average FLOPreduction of 6.7. On other datasets, the accuracy drop is minimalat < 0.8%. These results demonstrate the effectiveness of ToP inreducing computational cost while maintaining accuracy.Compared to other token pruning methods based on attentionvalues, ToP significantly surpasses strong baselines such as PoWER-BERT, LAT, and LTP across all datasets, despite being based on thesame approach. This is mainly due to our ranking-aware tokendistillation mechanism, which greatly enhances the ability of self-attention values to rank token importance.Also, as can be seen from and , current attention value-based methods cannotsurpass the prediction method-based Transkimmer. However, ToPoutperforms Transkimmer on many tasks. Specifically, under the",
  ": Comparison of token pruning methods under various FLOPs sparsity ratios": "same-level FLOPs reduction, ToP on BERT achieves +2.9%, +0.1%,+0.3%, +0.9%, +0.2%, +0.9% higher accuracy on CoLA, QQP, MRPC,SST-2, MNLI, SQuAD and 20News, respectively.Its worth noting that ToP also outperforms Transkimmer interms of real inference latency improvement. As discussed in Sec. 3.3,its challenging to deploy Transkimmer for real latency reduction.In contrast, ToP delivers real latency improvement of up to 7.4,which will be further discussed in later sections.Under various deployment constraints. We now evaluate theeffectiveness of ToP under various FLOPs sparsity and compareit to two state-of-the-art methods: LTP (representing attentionvalue-based methods) and Transkimmer (representing prediction-based methods). For a fair comparison, we use their official codeimplementations and perform a grid search to find the optimalhyper-parameters that achieve the desired token sparsity. summarizes the results. ToP surpasses both LTP and Tran-skimmer at all FLOPs sparsity levels. At equivalent levels of accu-racy on the 20News long sequence, ToP reduces 17% and 20% moreFLOPs compared to Transkimmer and LTP respectively. Withoutany loss in BERT accuracy, ToP reduces FLOPs by 86%, 78%,76%, and 88% on MRPC, SST-2, MNLI, and 20News, respectively.Comparison with model compression. In addition to tokenpruning baselines, we compare ToP with state-of-the-art modelcompression techniques, including structured pruning and knowl-edge distillation (i.e., the DistilBERT6). We specifically comparewith CoFi , a top-performing structured pruning method. Weevaluate CoFi and ToP on two model sizes: BERT, a large trans-former model, and BERT6, a small model. shows the results by different methods. ToP and CoFioutperform DistilBERT6 in terms of higher compression ratios.ToP consistently surpasses the original BERT consistently withhigher accuracy and a 6.7 average FLOPs reduction, whereas CoFisees a significant drop in accuracy for CoLA and 20News at highcompression ratios. On the smaller BERT6 model, ToP exhibitsa much smaller accuracy loss compared to CoFi, showcasing itseffectiveness and robustness across models of various sizes.",
  "ToP60.067.9 90.988.893.283.289.186.4ToP Distill59.460.6 90.786.492.983.088.082.7ToP (MSE Distill loss)57.662.8 90.687.593.082.987.973.5ToP (CE Distill loss)59.060.3 90.588.392.382.788.184.8": ": Alabtion studies of different methods under the sametoken pruning sparisity. Above: effectiveness of our coarse-to-fine grained pruning strategy. Below: ablation study ofdifferent token importance distillation objectives. impact of this design on performance, we compare the results withand without gate masks by removing the masks and pruning alllayers at the fine-grained level. We run all experiments under thesame FLOPs sparsity (tables 2 and 3) and hyper-parameter settings. shows the accuracy results on GLUE benchmark. Remov-ing gate masks causes a noticeable accuracy drop on all datasetsexcept SST-2. Specifically, the accuracy drops by a substantial 6.2%,3.5%, 2.9%, and 2.9% on RTE, MRPC, QNLI, and STS-B, respec-tively. These results highlight the effectiveness of our coarse-to-finegrained approach in making much better token selection decisions. Different token distillation approaches. We also ablate on theranking-aware token distillation component to evaluate its con-tribution to the performance of ToP. We first remove the entiredistillation process from the token pruning process. As shown in, the removal of our proposed ranking-aware token distil-lation results in accuracy drops across all datasets. Moreover, weobserve that the effect of ranking-aware token distillation varies",
  ": The retained tokens as input for each layer in BERT": "based on the length of the input sequences. On datasets with rel-atively short sample lengths (see in Appendix), such asCoLA, QQP, SST-2, and MNLI, token distillation slightly improvesaccuracy by 0.6%, 0.2%, 0.3%, and 0.2%, respectively. However, ondatasets with longer sample lengths, such as RTE, MRPC, QNLI, andSTS-B, token distillation has a crucial role in improving accuracyby 7.2%, 4.4%, 1.1%, and 3.7%, respectively. The underlying reason isthat its much easier for conventional attention-based scoring meth-ods to determine the significance of tokens in shorter sequencescompared to longer sequences. However, when the task involveslonger sequences, accurately identifying the most critical tokens atearly transformer layers becomes challenging. In such cases, ourproposed ranking-aware token distillation effectively tackles thisproblem and leads to a significant improvement in accuracy.Moreover, we compare the use of conventional distillation ob-jectives to the in Equation 8. Our alternatives includethe MSE loss which seeks to reduce the discrepancy betweenteachers and students token importance scores, and the generalcross-entropy (CE) loss that aims to minimize the KL diver-gence between teachers and students token importance scoredistributions. indicates that relying on conventional distil-lation objectives fails to improve the accuracy effectively.",
  "Retained Tokens Visualization and Analysis": "To further understand the behavior of ToP, we analyze the numberof retained tokens in each layer. We conduct experiments on fourdatasets using two groups of FLOPs sparsity ratios: (i) low sparsity45%, 20%, 20%, 50% and (ii) high sparsity 65%, 65%, 60%, 80% for QQP,SST-2, MNLI, and 20News respectively. The low sparsity numbersare used for experiments in and the high sparsity numbersare evaluated in and . The model accuracy loss isnegligible as demonstrated in previous sections. Notably, for bettervisualization, we exclude PAD tokens when analyzing ToPs pruningdecision on the original effective input tokens. illustrates the number of remaining tokens used as inputfor each layer in BERT under different levels of sparsity ratios.",
  ": Real inference latency on an 8-core Intel CPU": "Interestingly, we discover common token pruning patterns in themodels: (1) deep transformer layers have high token redundancy.Under high sparsity, over 95% of tokens are pruned when they arriveat layer 8, indicating that earlier layers have effectively extractedtheir information, making them non-informative for deep layers.This observation is consistent with the conclusions of studies onBERT behaviours , indicating that ToP is capable of automat-ically identifying the optimal patterns for token pruning. (2) ToPprioritizes token pruning in deeper layers over an even distributionacross all layers. We observe that ToP selects different layers fortoken pruning under varying levels of sparsity ratios. In ToP, tokenpruning is initially performed in deeper layers when the sparsityis low, and as the sparsity increases, it gradually extends to earlierlayers. For example, on the SST-2 task, when the sparsity is set to20%, token pruning is not applied to layers 1 to 7. However, whenthe sparsity increases to 65%, token pruning is activated in layers 3to 7, while only layers 1 and 2 are excluded from pruning.",
  "Inference latency on Hardware": "Finally, we assess the practical efficiency of ToP in resource-limitedenvironments by evaluating BERT on MRPC, RTE, SQuAD, and20News datasets using an 8-core Intel(R) Xeon(R) CPU@2GHz. Thelearned ranking masks are applied layer-by-layer to discard tokens,and latency is measured using the high-performance Onnxruntimeinference engine . The batch size is set to 1 for simplicity.As shown in , BERT inference without token pruningincurs a high latency on the CPU, particularly for long input se-quences. However, ToP significantly reduces this latency by dis-carding unnecessary tokens. Specifically, ToP delivers inference ac-celeration of 2.9, 5.8, 4.1, 7.4 on MRPC, RTE, SQuAD, 20Newsrespectively, with minimal impact on accuracy. The latency reduc-tion increases with the input length, indicating the big potential ofToP in handling long sequence tasks.",
  "CONCLUSION": "In this paper, we propose ToP, a novel token pruning approachthat leverages 0 regularization to determine the optimal token re-moval under a target inference constraint. ToP adopts hierarchicalpruning, where it selects the ideal subset of transformer layers forfine-grained token pruning. Coupled with ranking-aware tokenimportance distillation, ToP significantly enhances the ability ofself-attention values to rank token importance, leading to superioraccuracy even at high compression ratios. Extensive evaluationson the GLUE benchmark and SQuAD have shown that ToP out-performs existing token pruning and model compression baselines.With the removal of unnecessary tokens, ToP achieves up to 12.6FLOP reduction for BERT with less than 1% drop in accuracy.",
  "Mitchell A. Gordon, Kevin Duh, and Nicholas Andrews. 2020.Compress-ing BERT: Studying the Effects of Weight Pruning on Transfer Learning.arXiv:2002.08307 [cs.CL]": "Saurabh Goyal, Anamitra Roy Choudhury, Saurabh Raje, Venkatesan Chakar-avarthy, Yogish Sabharwal, and Ashish Verma. 2020. PoWER-BERT: AcceleratingBERT Inference via Progressive Word-vector Elimination. In Proceedings of the37th International Conference on Machine Learning (Proceedings of Machine Learn-ing Research, Vol. 119), Hal Daum III and Aarti Singh (Eds.). PMLR, 36903699. Yue Guan, Zhengyi Li, Jingwen Leng, Zhouhan Lin, and Minyi Guo. 2022. Tran-skimmer: Transformer Learns to Layer-wise Skim. In Proceedings of the 60thAnnual Meeting of the Association for Computational Linguistics (Volume 1: LongPapers). Association for Computational Linguistics, 72757286.",
  "Francois Lagunas, Ella Charlaix, Victor Sanh, and Alexander M. Rush. 2021. BlockPruning For Faster Transformers. In EMNLP": "Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, PiyushSharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised learningof language representations. arXiv preprint arXiv:1909.11942 (2019). Ken Lang. 1995. NewsWeeder: Learning to Filter Netnews. (1995), 331339. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, OmerLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: Arobustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692(2019).",
  "Microsoft. 2022. onnxruntime": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the Limitsof Transfer Learning with a Unified Text-to-Text Transformer. Journal of MachineLearning Research 21, 140 (2020), 167. Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know What You Dont Know:Unanswerable Questions for SQuAD. In Proceedings of the 56th Annual Meeting ofthe Association for Computational Linguistics (Volume 2: Short Papers). Associationfor Computational Linguistics, 784789. Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh.2021. Dynamicvit: Efficient vision transformers with dynamic token sparsification.Advances in neural information processing systems 34 (2021), 1393713949.",
  "Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Well-Read Students Learn Better: The Impact of Student Initialization on KnowledgeDistillation. ArXiv abs/1908.08962 (2019)": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R.Bowman. 2019. GLUE: A Multi-Task Benchmark and Analysis Platform for Natu-ral Language Understanding. In International Conference on Learning Representa-tions. Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng Zhu, Chuang Gan,and Song Han. 2020. HAT: Hardware-Aware Transformers for Efficient NaturalLanguage Processing. In Annual Conference of the Association for ComputationalLinguistics. Hanrui Wang, Zhekai Zhang, and Song Han. 2021. Spatten: Efficient sparse atten-tion architecture with cascade token and head pruning. In 2021 IEEE InternationalSymposium on High-Performance Computer Architecture (HPCA). IEEE, 97110. Xuanhui Wang, Cheng Li, Nadav Golbandi, Michael Bendersky, and Marc Najork.2018. The LambdaLoss Framework for Ranking Metric Optimization. In Pro-ceedings of the 27th ACM International Conference on Information and KnowledgeManagement (Torino, Italy) (CIKM 18). Association for Computing Machinery,13131322. Xuanhui Wang, Cheng Li, Nadav Golbandi, Mike Bendersky, and Marc Najork.2018. The LambdaLoss Framework for Ranking Metric Optimization. In Proceed-ings of The 27th ACM International Conference on Information and KnowledgeManagement (CIKM 18). 13131322.",
  "Mengzhou Xia, Zexuan Zhong, and Danqi Chen. 2022. Structured Pruning LearnsCompact and Accurate Models. In Association for Computational Linguistics(ACL)": "Jin Xu, Xu Tan, Renqian Luo, Kaitao Song, Jian Li, Tao Qin, and Tie-Yan Liu. 2021.Nas-BERT: task-agnostic and adaptive-size BERT compression with neural archi-tecture search. In Proceedings of the 27th ACM SIGKDD Conference on KnowledgeDiscovery & Data Mining. 19331943. Deming Ye, Yankai Lin, Yufei Huang, and Maosong Sun. 2021. TR-BERT: Dy-namic Token Reduction for Accelerating BERT Inference. In Proceedings of the2021 Conference of the North American Chapter of the Association for Computa-tional Linguistics: Human Language Technologies. Association for ComputationalLinguistics, 57985809. Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, ChrisAlberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang,et al. 2020. Big bird: Transformers for longer sequences. Advances in NeuralInformation Processing Systems 33 (2020). Li Lyna Zhang, Youkow Homma, Yujing Wang, Min Wu, Mao Yang, Ruofei Zhang,Ting Cao, and Wei Shen. 2022. SwiftPruner: Reinforced Evolutionary Pruning forEfficient Ad Relevance. In Proceedings of the 31st ACM International Conferenceon Information & Knowledge Management. 36543663.",
  "A.2Hyperparameters": "We follow CoFi for setting the total pruning epochs. We use100 pruning epochs for small GLUE datasets (such as CoLA, RTE,MRPC, and STS-B), and 60 epochs for large datasets (such as QQP,SST-2, MNLI, QNLI, and 20News). For the warmup stage, we usehalf of the total epochs for small datasets, and 25% for large datasets.The hyper-parameter in Equation 9 is used to balance the sig-nificance of token importance distillation. We use a linear schedulerthat reduces the value of from its initial value throughout thewarmup stage. This setting allows the model parameters to adaptmore effectively to the teachers token importance rankings in theearly pruning stages, while shifting focus to fine-tuning the modelparameters and pruning masks for improved accuracy in the laterstages.We report the hyperparameters used in our experiments in Ta-ble 8. for rank-distillation loss is chosen from {1e-2, 1e-3, 1e-4}.LTP, Transkimmer, and CoFiPruning are trained based on theirimplementation details in the paper and open-source code.",
  "A.3Case Study": "One example from SQuADv2.0 dataset is presented in toshow the reason why we use the attention score from the last layerto distill the attention score of the first few layers. For finetuned model, the attention score for the answer token ranks114th in layer 2 among all the attention scores of the total 170 tokens,while in layer 12 it ranks 24th. The answer token is a very importanttoken, and should not be pruned during inference. However, it rankslow in layer 2, so it will be probably pruned for a high sparsity,resulting in a wrong answer. After applying rank distillation, itsrank goes higher, making it less likely to be pruned. Question: What century did the Normans first gain their separateidentity?Context: The Normans (Norman: Nourmands; French: Normands; Latin:Normanni) were the people who in the 10th and 11th centuries gave theirname to Normandy, a region in France. They were descended from Norse(\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark,Iceland and Norway who, under their leader Rollo, agreed to swear fealtyto King Charles III of West Francia. Through generations of assimilationand mixing with the native Frankish and Roman-Gaulish populations,their descendants would gradually merge with the Carolingian-basedcultures of West Francia. The distinct cultural and ethnic identity of theNormans emerged initially in the first half of the 10th century, and itcontinued to evolve over the succeeding centuries.Answer: 10th.",
  "A.4Inference on GPU": "We implement a simple inference version using PyTorch to measurelatency on V100 GPU. Our ToP approach demonstrates latencyspeedup on GPUs, achieving 1.2x speedup on RTE (20.04 ms) and1.24x speedup on MRPC (11.00 ms), although the acceleration ratiois not as high as on CPUs.We believe that there is still potential for improving GPU infer-ence acceleration with high-performance inference engines andsystem optimizations. We found that token pruning requires somememory operations, such as removing tokens with mask value 0from hidden states. Although this operation requires no computa-tion, it is time-consuming on GPU. In our future work, we plan toutilize high-performance inference engines and leverage systemoptimizations to achieve greater GPU inference acceleration."
}