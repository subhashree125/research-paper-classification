{
  "ABSTRACT": "Contrastive learning (CL) has become the de-facto learning para-digm in self-supervised learning on graphs, which generally fol-lows the augmenting-contrasting learning scheme. However, weobserve that unlike CL in computer vision domain, CL in graph do-main performs decently even without augmentation. We conduct asystematic analysis of this phenomenon and argue that homophily,i.e., the principle that like attracts like, plays a key role in thesuccess of graph CL. Inspired to leverage this property explicitly,we propose HomoGCL, a model-agnostic framework to expand thepositive set using neighbor nodes with neighbor-specific signifi-cances. Theoretically, HomoGCL introduces a stricter lower boundof the mutual information between raw node features and nodeembeddings in augmented views. Furthermore, HomoGCL can becombined with existing graph CL models in a plug-and-play waywith light extra computational overhead. Extensive experimentsdemonstrate that HomoGCL yields multiple state-of-the-art resultsacross six public datasets and consistently brings notable perfor-mance improvements when applied to various graph CL methods.Code is avilable at",
  "Corresponding authors": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 23, August 610, 2023, Long Beach, CA, USA. 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0103-0/23/08...$15.00 CIFAR10STL-10 Acc.(%) w/ aug.w/o aug.MLP",
  "(b) Graph datasets with GRACE": ": Performance of CL in vision and graph domainswith/without augmentation. SimCLR and GRACE ,two prevalent and similar CL architectures in vision andgraph domains, are adopted on the respective datasets. MLPis the baseline by simply training a Multi-Layer Perceptronfrom image RGB features/raw node features. When with-out augmentation, the performance of vision datasets dropsdrastically, while the performance of graph datasets is ratherstable and still outperforms the MLP counterpart.",
  "INTRODUCTION": "Graph Neural Networks (GNNs) have achieved overwhelming ac-complishments on a variety of graph-based tasks like node classifi-cation and node clustering, to name a few . Theygenerally refer to the message passing mechanism where node fea-tures first propagate to neighbor nodes and then get aggregated tofuse the features in each layer.Generally, GNNs are designed for supervised tasks which re-quire adequate labeled data. However, it is hard to satisfy as an-notated labels are always scarce in real-world scenarios .To tackle this common problem in deep learning, many pioneerendeavors have been made to self-supervised learning (SSL) inthe computer vision domain, of which vision contrastive learning(VCL) has dominated the field. Generally, VCL follows theaugmenting-contrasting learning pattern, in which the similaritybetween two augmentations of a sample (positive pair) is maxi-mized, while the similarities between other samples (negative pairs)are minimized. The model can thus learn high-quality representa-tions free of label notation. There are also many work adapting CLto graph representation learning, referred to as graph contrastivelearning (GCL) . Research hotspot in GCL mainly focuses",
  "KDD 23, August 610, 2023, Long Beach, CA, USA.Wen-Zhi Li, Chang-Dong Wang, Hui Xiong, and Jian-Huang Lai": "on graph augmentation , since unlike naturallyrotating or cropping on images, graph augmentation would discardunderlying semantic information which might result in undesirableperformance. Though these elaborate graph augmentation-basedapproaches can achieve state-of-the-art performances on manygraph-based tasks, we argue that the role of graph augmentationis still overemphasized. Empirically, we observe that GCL withoutaugmentation can also achieve decent performance ((b)),which is quite different from VCL ((a)). A natural questionarises thereby:What causes the huge gap between the performance declines ofGCL and VCL when data augmentation is not leveraged?To answer this question, we conduct a systematic analysis andargue that homophily is the most important part of GCL. Specifically,homophily is the phenomenon that like attracts like , or con-nected nodes tend to share the same label, which is a ubiquitousproperty in real-world graphs like citation networks or co-purchasenetworks . According to recent studies , GNN back-bones in GCL (such as GCN , GAT , and GraphSAGE )heavily rely on the homophily assumption, as message passingis applied for these models to aggregate information from directneighbors for each node.As a distinctive inductive bias of real-world graphs , ho-mophily is regarded as an appropriate guide in the case that nodelabels are not available . Many recent GCLs have leveragedgraph homophily implicitly by strengthening the relationship be-tween connected nodes from different angles. For example, Xia etal. tackle the false negative issue by avoiding similar neighbornodes being negative samples, while Li et al. , Wang et al. ,Park et al. , and Lee et al. leverage community structure toenhance local connection. However, to the best of our knowledge,there is no such effort to directly leverage graph homophily, i.e., totreat neighbor nodes as positive.In view of the argument, we are naturally inspired to leveragegraph homophily explicitly. One intuitive approach is to simplytreat neighbor nodes as positive samples indiscriminately. How-ever, although connected nodes tend to share the same label in thehomophily scenario, there also exist inter-class edges, especiallynear the decision boundary between two classes. Treating theseinter-class connected nodes as positive (i.e., false positive) wouldinevitably degenerate the overall performance. Therefore, our con-cern is distinguishing intra-class neighbors from inter-class onesand assigning more weights to them being positive. However, itis non-trivial since ground-truth node labels are agnostic in SSL.Thus, the main challenge is estimating the probability of neighbornodes being positive in an unsupervised manner.To tackle this problem, we devise HomoGCL, a model-agnosticmethod based on pair-wise similarity for the estimation. Specif-ically, HomoGCL leverages Gaussian Mixture Model (GMM) toobtain soft clustering assignments for each node, where node simi-larities are calculated as the indicator of the probability for neighbornodes being true positive. As a patch to augment positive pairs, Ho-moGCL is flexible to be combined with existing GCL approaches,including negative-sample-free ones like BGRL to yield bet-ter performance. Furthermore, theoretical analysis guarantees theperformance boost over the base model, as the objective functionin HomoGCL is a stricter lower bound of the mutual information",
  "between raw node features and augmented representations in aug-mented views.We highlight the main contributions of this work as follows:": "We conduct a systematic analysis to study the mechanismof GCL. Our empirical study shows that graph homophilyplays a key role in GCL, and many recent GCL models canbe regarded as leveraging graph homophily implicitly. We propose a novel GCL method, HomoGCL, to estimatethe probability of neighbor nodes being positive, thus di-rectly leveraging graph homophily. Moreover, we theoreti-cally show that HomoGCL introduces a stricter lower boundof mutual information between raw node features and aug-mented representations in augmented views. Extensive experiments and in-depth analysis demonstratethat HomoGCL outperforms state-of-the-art GCL modelsacross six public benchmark datasets. Furthermore, HomoGCLcan consistently yield performance improvements when ap-plied to various GCL methods in a plug-and-play manner.The rest of this paper is organized as follows. In , webriefly review related work. In , we first provide the basicpreliminaries of graph contrastive learning. Then, we conduct anempirical study to delve into the graph homophily in GCL, afterwhich we propose the HomoGCL model to directly leverage graphhomophily. Theoretical analysis and complexity analysis are alsoprovided. In , we present the experimental results andin-depth analysis of the proposed model. Finally, we conclude thispaper in .",
  "Graph Contrastive Learning": "GCL has gained popularity in the graph SSL community for its ex-pressivity and simplicity . It generally refers to the para-digm of making pair-view representations to agree with each otherunder proper data augmentations. Among them, DGI , HDI ,GMI , and InfoGCL directly measure mutual information be-tween different views. MVGRL maximizes information betweenthe cross-view representations of nodes and graphs. GRACE and its variants GCA , ProGCL , ARIEL , gCooL adopt SimCLR framework for node-level representations, whilefor graph-level representations, GraphCL , JOAO , Sim-GRACE also adopt the SimCLR framework. Additionally, G-BT , BGRL , AFGRL , and CCA-SSG adopt new CLframeworks that free them from negative samples or even data aug-mentations. In this paper, we propose a method to expand positivesamples in GCL on node-level representation learning, which canbe combined with existing node-level GCL in a plug-and-play way.",
  "(c) Ablation study on two graph datasets": ": Empirical studies on graph homophily. (a), (b) are similarities between positive and negative pairs w.r.t. the trainingprocesses with/without augmentation on vision dataset CIFAR10 and graph dataset Cora. The similarity between negativepairs drops to 0 swiftly on CIFAR10 without augmentation, while the similarity between negative pairs drops gradually onCora without augmentation, which is analogous to its counterpart with augmentation. Please note that the similarity betweenpositive pairs remains as 1 when without augmentation since the two views are identical. To analyze the role that homophilyplayed in this phenomenon, we conduct an ablation study for GRACE without augmentation in (c) by (1) only enabling messagepassing (w/ MP), and (2) disabling message passing (w/o MP), together with the MLP baseline on two graph datasets Cora andPhoto. The performance shows the functionality of message passing, which relies on the homophily assumption. study AutoSSL shows graph homophily is an effective guidein searching the weights on various self-supervised pretext tasks,which reveals the effectiveness of homophily in graph SSL. How-ever, there is no such effort to leverage homophily directly in GCL tothe best of our knowledge. It is worth noting that albeit heterophilygraphs also exist where dissimilar nodes tend to be connected,the related study is still at an early stage even in the supervisedsetting . Therefore, to be consistent with previous work on GCL,we only consider the most common homophily graphs in this work.",
  "Preliminaries and Notations": "Let G = {V, E} be a graph, where V = {1, 2, } is the nodeset with nodes and E V V is the edge set. The adjacencymatrix and the feature matrix are denoted as A {0, 1} andX R , respectively, where A = 1 iff (, ) E, and R is the -dim raw feature of node . The main notions usedthroughout the paper are summarized in Appendix A.Given a graph G with no labels, the goal of GCL is to traina graph encoder (X, A) and get node embeddings that can bedirectly applied to downstream tasks like node classification andnode clustering. Take one of the most popular GCL frameworkGRACE as an example, two augmentation functions 1, 2(typically randomly dropping edges and masking features) are firstlyapplied to the graph G to generate two graph views G1 = (X1, A1) =1(G) and G2 = (X2, A2) = 2(G). Then, the two augmented graphsare encoded by the same GNN encoder, after which we get nodeembeddings U = (X1, A1) and V = (X2, A2). Finally, the loss",
  "Homophily in GCL: an EmpiricalInvestigation": "In , we can observe that the performance of VCL collapseswithout data augmentation, while the performance of GCL is stillbetter than the MLP counterpart, which implies that there is agreat discrepancy in the mechanism between VCL and GCL, al-though they adopt seemingly similar frameworks SimCLR andGRACE . To probe into this phenomenon, we plot the simi-larities between positive and negative samples w.r.t. the trainingprocesses, as shown in (a), (b).",
  "(b) Soft clustering": ": The pipeline of HomoGCL (a). Two graph views G1 and G2 are first generated via graph augmentation from graph G,after which the three graphs are fed into the shared GNN encoder to learn representations. The representation of G is utilizedto generate soft clustering assignments via Gaussian Mixture Model, based on which the edge saliency is calculated (b). Edgesaliency is leveraged as the weight of neighbor nodes being positive. From the figures, we can see that for vision dataset CIFAR10, thesimilarity between negative pairs drops swiftly without augmenta-tion. We attribute this to that the learning objective is too trivialby enlarging the similarity between the sample and itself whilereducing the similarity between the sample and any other samplein the batch. Finally, the model can only learn one-hot-like embed-dings, which eventually results in the poor performance. However,for graph dataset Cora, the similarity between negative pairs with-out augmentation still drops gradually and consistently, which isanalogous to its counterpart with augmentation. We attribute thisphenomenon to the message passing in GNN. Albeit InfoNCE lossalso enlarges the similarity between the identical samples in twoviews while reducing the similarity between two different samples,the message passing in GNN enables each node to aggregate in-formation from its neighbors, which leverages graph homophilyimplicitly to avoid too trivial discrimination. From another perspec-tive, the message passing in GNN enables each node to no longerbe independent of its neighbors. As a result, only the similaritiesbetween two relatively far nodes (e.g., neighbors 3-hop for atwo-layer GCN), are reduced.",
  "where denotes the label of and 1() is the indication function": "As analyzed above, we hypothesize that message passing whichrelies on the homophily assumption prevents GCL without aug-mentation from corruption. To validate this, we conduct anotherablation experiment on two graph datasets Cora and Photo, asshown in (c). Experimental setup. We devise two variants of GRACE with-out augmentation, i.e., GRACE w/ MP and GRACE w/o MP, wherethe former is the version that message passing is leveraged (i.e., thew/o aug. version in (b)), while the latter is the version thatmessage passing is blocked via forcing each node to only connectwith itself (i.e., substituting the adjacency matrix with the eye ma-trix). For MLP, we directly train an MLP with raw node features asthe input in a supervised learning manner without graph structure,which is the same as in . The two variants are equipped withthe same 2-layer GCN backbone of 256 dim hidden embeddingsand are trained until convergence. Observations. From (c), we can see that the performanceof GRACE (w/o MP) is only on par with or even worse than the MLPcounterpart, while GRACE (w/ MP) outperforms both by a largemargin on both datasets. It meets our expectation that by disablingmessage passing, nodes in GRACE (w/o MP) cannot propagatefeatures to their neighbors, which degenerates them to a similarsituation of VCL without augmentation. GRACE (w/ MP), on theother hand, can still maintain the performance even without rawfeatures. In a nutshell, this experiment verifies our hypothesis thatmessage passing, which relies on the homophily assumption, is thekey factor of GCL.",
  "HomoGCL": "As analyzed above, graph homophily plays a crucial role in theoverall performance of GCL. Therefore, we are naturally inspired toleverage this property explicitly. Simply assigning neighbor nodesas positive is non-ideal, as nodes near the decision boundaries tendto link with nodes from another class, thus being false positive. Tomitigate such effect, it is expected to estimate the probability ofneighbor nodes being true positive. However, the task is intractableas node labels are not available to identify the boundaries in the",
  "HomoGCL: Rethinking Homophily in Graph Contrastive LearningKDD 23, August 610, 2023, Long Beach, CA, USA": "SSL setting. To tackle this challenge, we leverage GMM on -meanshard clusters to get soft clustering assignments of the initial graphG, where pair-wise similarity (saliency) is calculated as the afore-mentioned probability. The overall framework of HomoGCL andsoft clustering are illustrated in . Please note that althoughwe introduce HomoGCL based on GRACE framework as anexample, HomoGCL is framework-agnostic and can be combinedwith other GCL frameworks. Soft clustering for pair-wise node similarity. An unsupervisedmethod like clustering is needed to estimate the probability ofneighbor nodes being true positive. However, traditional clusteringmethods like -means can only assign a hard label for each node,which cannot satisfy our needs. To tackle this problem, we view-means as a special case of GMM, where soft clustering ismade possibile based on the posterior probabilities. Specifically,for GMM with centroids {1, 2, , } defined by the meanembeddings of nodes in different -means hard labels, posteriorprobability can be calculated as",
  "In this way, we can get a cluster assignment matrix R R": "where R = ( | ) indicates the soft clustering value betweennode and cluster .Based on the assignment matrix R, we are able to calculate anode saliency S between any connected node pair (, ) via S =norm(R) norm(R ) with norm() being the 2 normalizationon the cluster dimension. S can thus indicate the connectionintensity between node and , which is an estimated probabilityof neighbors being true positive.",
  "Theoretical Analysis": "Though simple and intuitive by design, the proposed HomoGCLframework is theoretically guaranteed to boost the performanceof base models from the Mutual Information (MI) maximizationperspective, as induced in Theorem 1 with GRACE as an example. Theorem 1. The newly proposed contrastive loss L in Eq. (10)is a stricter lower bound of MI between raw node features X and nodeembeddings U and V in two augmented views, comparing with theraw contrastive loss L in Eq. (1) proposed by GRACE. Formally,",
  "Proof. See Appendix B": "From Theorem 1, we can see that maximizing L is equivalentto maximizing a lower bound of the mutual information betweenraw node features and learned node representations, which guaran-tees model convergence . Furthermore, the lowerbound derived by HomoGCL is stricter than that of the GRACE,which provides a theoretical basis for the performance boost ofHomoGCL over the base model.",
  "Complexity Analysis": "The overview of the training algorithm of HomoGCL (based onGRACE) is elaborated in Algorithm 1, based on which we analyzeits time and space complexity. It is worth mentioning that the extracalculation of HomoGCL introduces light computational overheadover the base model. Time Complexity. We analyze the time complexity according tothe pseudocode. For line 6, the time complexity of -means with iterations, clusters, node samples with -dim hidden em-beddings is O(). Obtaining cluster centroids needs O()based on the hard pseudo-labels obtained by -means. For line 7, weneed to calculate the distance between each node and each clustercentroid, which is another O() overhead to get the assignmentmatrix R. For line 8, the saliency S can be obtained via 2 norm",
  ": return , H": "and vector multiplication for connected nodes in O(( + |E|)).For line 10, the homophily loss can be calculated in O(|E|) bydefinition. Overall, the extra computational overhead over the basemodel is O(+( +|E|)) for HomoGCL, which is lightweightcompared with the base model, as is usually set to a small number. Space Complexity. Extra space complexity over the base modelfor HomoGCL is introduced by the -means algorithm, the assign-ment matrix R, and the saliency S. For the -means algorithm men-tioned above, its space complexity is O(). For R R , itsspace complexity is O(). As only the saliency between each con-nected node pair will be leveraged, the saliency costs O(|E|). Over-all, the extra space complexity of HomoGCL is O(( + ) + |E|),which is lightweight and on par with the GNN encoder.",
  "Cora2,70810,5561,4337CiteSeer3,3279,2283,7036PubMed19,71788,6515003Photo7,650238,1637458Computer13,752491,72276710arXiv169,3431,166,24312840": "(Computer) , and one large-scale network ogbn-arXiv (arXiv) to conduct the experiments throughout the paper. The statistics ofthe datasets are provided in . We give their detailed descrip-tions are as follows: Cora, CiteSeer, and PubMed1 are three academic net-works where nodes represent papers and edges represent ci-tation relations. Each node in Cora and CiteSeer is describedby a 0/1-valued word vector indicating the absence/presenceof the corresponding word from the dictionary, while eachnode in PubMed is described by a TF/IDF weighted wordvector from the dictionary. The nodes are categorized bytheir related research area for the three datasets. Amazon-Photo and Amazon-Computers2 are two co-purchase networks constructed from Amazon where nodesrepresent products and edges represent co-purchase rela-tions. Each node is described by a raw bag-of-words featureencoding product reviews, and is labeled with its category. ogbn-arXiv3 is a citation network between all Com-puter Science arXiv papers indexed by Microsoft academicgraph , where nodes represent papers and edges rep-resent citation relations. Each node is described by a 128-dimensional feature vector obtained by averaging the skip-gram word embeddings in its title and abstract. The nodesare categorized by their related research area. Baselines. We compare HomoGCL with a variety of baselines, in-cluding unsupervised methods Node2Vec and DeepWalk ,supervised methods GCN , GAT , and GraphSAGE ,graph autoencoders GAE and VGAE , graph contrastive learn-ing methods including DGI , HDI , GMI , InfoGCL ,MVGRL , G-BT , BGRL , AFGRL , CCA-SSG ,COSTA , GRACE , GCA , ProGCL , ARIEL , andgCooL . We also report the performance obtained using anMLP classifier on raw node features. The detailed description ofthe baselines could be found in Appendix C. Configurations and Evaluation protocol. Following previouswork , each model is firstly trained in an unsupervised mannerusing the entire graph, after which the learned embeddings areutilized for downstream tasks. We use the Adam optimizer for boththe self-supervised GCL training and the evaluation stages. Thegraph encoder is specified as a standard two-layer GCN modelby default for all the datasets. For the node classification task, wetrain a simple 2-regularized one-layer linear classifier. For Cora,",
  "The results not reported are due to unavailable code": "CiteSeer, and PubMed, we apply public splits to split theminto training/validation/test sets, where only 20 nodes per class areavailable during training. For Photo and Computer, we randomlysplit them into training/validation/testing sets with proportions10%/10%/80% respectively following , since there are no publiclyaccessible splits. We train the model for five runs and report theperformance in terms of accuracy. For the node clustering task, wetrain a -means model on the learned embeddings for 10 times,where the number of clusters is set to the number of classes foreach dataset. We measure the clustering performance in terms oftwo prevalent metrics Normalized Mutual Information (NMI) score:NMI = 2 ( Y; Y)/[ ( Y) + (Y)], where Y and Y being the predictedcluster indexes and class labels respectively, () being the mutualinformation, and () being the entropy; and Adjusted Rand Index(ARI): ARI = RI E[RI]/(max{RI} E[RI]), where RI being theRand Index .",
  "Node Classification (RQ1)": "We implement HomoGCL based on GRACE. The experimentalresults of node classification on five datasets are shown in Ta-ble 2, from which we can see that HomoGCL outperforms all self-supervised baselines or even the supervised ones, over the fivedatasets except on CiteSeer, which we attribute to its relativelylow homophily. We can also observe that GRACE-based methodsGCA, ProGCL, and ARIEL cannot bring consistent improvementsover GRACE. In contrast, HomoGCL can always yield significantimprovements over GRACE, especially on Cora with a 3% gain. We also find CCA-SSG a solid baseline, which can achieve runner-up performance on these five datasets. It is noteworthy that CCA-SSG adopts simple edge dropping and feature masking as aug-mentation (like HomoGCL), while the performances of baselineswith elaborated augmentation (MVGRL, COSTA, GCA, ARIEL) varyfrom datasets. It indicates that data augmentation in GCL might beoveremphasized.Finally, other methods which leverage graph homophily implic-itly (AFGRL, ProGCL) do not perform as we expected. We attributethe non-ideal performance of AFGRL to the fact that it does notapply data augmentation, which might limit its representation abil-ity. For ProGCL, since the model focuses on negative samples byalleviating false negative cases, it is not as effective as HomoGCLto directly expand positive ones.",
  "Node Clustering (RQ1)": "We also evaluate the node clustering performance on Photo andComputer datasets in this section. HomoGCL is also implementedbased on GRACE. As shown in , HomoGCL generally out-performs other methods by a large margin on both metrics forthe two datasets. We attribute the performance to that by enlarg-ing the connection density between node pairs far away from theestimated decision boundaries, HomoGCL can naturally acquirecompact intra-cluster bonds, which directly benefits clustering. Itis also validated by the visualization experiment, which will bediscussed in .8.",
  "Improving Various GCL Methods (RQ1)": "As a patch to expand positive pairs, it is feasible to combine Ho-moGCL with other GCL methods, even negative-free ones likeBGRL , which adapts BYOL in computer vision to GCL tofree contrastive learning from numerous negative samples. Sketch of BGRL. An online encoder and a target encoder are leveraged to encoder two graph views G1 and G2 respectively,after which we can get H1 and H2. An additional predictor isapplied to H1 and we can get Z1. The loss function is defined as",
  "=1 Z(1,), H(2,),(14)": "where(, ) is a similarity function. A symmetric loss 1 is obtainedby feeding G1 into the target encoder and G2 into the online encoder,and the final objective is L1 = 1 + 1. For online encoder , itsparameters are updated via stochastic gradient descent, while theparameters of the target encoder are updated via exponentialmoving average (EMA) of as + (1 ).To combine BGRL with HomoGCL, we feed the initial graph Gto the online encoder and get H. Then we get the assignment matrixR via Eq. (6) and the saliency S via multiplication. The expandedloss can thus be defined as",
  "J = L1 + L + L2,(16)": "where , are two hyperparameters.We evaluate the performance of BGRL+HomoGCL on PubMed,Photo, and Computer. The results are shown in , from whichwe can see that HomoGCL brings consistent improvements overthe BGRL base. It verifies that HomoGCL is model-agnostic and canbe applied to GCL models in a plug-and-play way to boost their per-formance. Moreover, it is interesting to see that the performanceson Photo and Computer even surpass GRACE+HomoGCL as wereported in , which shows the potential of HomoGCL tofurther boost the performance of existing GCL methods.",
  "Results on Large-Scale Dataset (RQ1)": "We also conduct an experiment on a large-scale dataset arXiv. Asthe dataset is split based on the publication dates of the papers, i.e.,the training set is papers published until 2017, the validation set ispapers published in 2018, and the test set is papers published since2019, we report the classification accuracy on both the validationand the test sets, which is a convention for this task. We extend thebackbone GNN encoder to 3 GCN layers, as suggested in .The results are shown in .Since GRACE treats all other nodes as negative samples, scalingGRACE to the large-scale dataset suffers from the OOM issue. Sub-sampling nodes randomly across the graph as negative samplesfor each node is a feasible countermeasure , but it is sensitiveto the negative size . BGRL, on the other hand, is free from neg-ative samples, which makes it scalable by design, and it showsa great tradeoff between performance and complexity. Since thespace complexity of HomoGCL mainly depends on the performanceof the base model as discussed in .5, we implement Ho-moGCL based on BGRL as described in .4. We can see that",
  "Case Study (RQ2)": "To obtain an in-depth understanding for the mechanism of thesaliency S in distinguishing the importance of neighbor nodes be-ing positive, we conduct this case study. Specifically, we first sortall edges according to the learned saliency S, then divide them intointervals of size 500 to calculate the homophily in each interval.From we can see that the saliency can estimate the prob-ability of neighbor nodes being positive properly as more similarnode pairs in S tend to have larger homophily, which validates theeffectiveness of leveraging saliency in HomoGCL.",
  "Hyperparameter Analysis (RQ3)": "In , we conduct a hyperparameter analysis on the numberof clusters and the weight coefficient in Eq. (12) on the Coradataset. From the figures we can see that the performance is stablew.r.t. the cluster number. We attribute this to the soft class assign-ments used in identifying the decision boundary, as the pairwisesaliency is mainly affected by the relative distance between nodepairs, which is less sensitive to the number of clusters. The per-formance is also stable when is below 10 (i.e., contrastive lossin Eq. (7) and homophily loss in Eq. (11) are on the same order ofmagnitude). This shows that HomoGCL is parameter-insensitive,thus facilitating it to be combined with other GCL methods in aplug-and-play way flexibly. In practice, we tune the number ofclusters in {5, 10, 15, 20, 25, 30} and simply assign to 1 to balancethe two losses without tuning.",
  "Visualization (RQ4)": "In addition to quantitative analysis, we also visualize the embed-dings learned by BGRL in (a), GRACE in (b), BGRL+HomoGCL in (c), and GRACE+HomoGCL in (d) onthe Cora dataset using t-SNE . Here, each point represents anode and is colored by its label. We observe that the embeddingslearned by +HomoGCL counterparts generally possess clearerclass boundaries and compact intra-class structures, which showsthe effectiveness of HomoGCL intuitively. This observation alignswith the remarkable node clustering performance reported in Sec-tion 4.3, which shows the superiority of HomoGCL.",
  "CONCLUSIONS": "In this paper, we investigate why graph contrastive learning canperform decently when data augmentation is not leveraged andargue that graph homophily plays a key role in GCL. We thus de-vise HomoGCL to directly leverage homophily by estimating theprobability of neighbor nodes being positive via Gaussian MixtureModel. Furthermore, HomoGCL is model-agnostic and thus canbe easily combined with existing GCL methods in a plug-and-playway to further boost their performances with a theoretical founda-tion. Extensive experiments show that HomoGCL can consistentlyoutperform state-of-the-art GCL methods in node classification andnode clustering tasks on six benchmark datasets. The authors would like to thank Ying Sun from The Hong Kong Uni-versity of Science and Technology (Guangzhou) for her insightfuldiscussion. This work was supported by NSFC (62276277), Guang-dong Basic Applied Basic Research Foundation (2022B1515120059),and the Foshan HKUST Projects (FSUST21-FYTRI01A, FSUST21-FYTRI02A). Chang-Dong Wang and Hui Xiong are the correspond-ing authors.",
  "SymbolDescription": "M, matrix, -th row of the matrixGinput graphVnode set of the graphEedge set of the graph = |V|number of nodes in the graphX R node features of the graphA R adjacency matrix of the graphY Rnode labels of the graph1, 2graph augmentation functionsG1, G2augmented graphsN1() , N2()node sets of s neighbors in G1, G2(X, A)GNN encoder with parameter H, U, V R encoded embeddings of G, G1, G2number of clustersC Rcluster centroidsR R cluster assignment matrixS R saliency (, )similarity functiontemperature parametercoefficient of loss functions",
  "CBASELINES": "In this section, we give brief introductions of the baselines usedin the paper which are not described in the main paper due to thespace constraint. DeepWalk , Node2vec : DeepWalk and Node2vec aretwo unsupervised random walk-based models. DeepWalk adoptsskip-gram on node sequences generated by random walk, whileNode2vec extends DeepWalk by considering DFS and BFS whensampling neighbor nodes. They only leverage graph topologystructure while ignoring raw node features.",
  "DGRAPH AUGMENTATIONS": "As we mentioned in the main paper, we adopt two simple andwidely used augmentation schemes, i.e., edge dropping and fea-ture masking, as the graph augmentation. For edge dropping, wedrop each edge with probability . For feature masking, we seteach raw feature as 0 with probability . and are set as thesame for two augmented graph views, which is widely used in theliterature .",
  "EABLATION STUDY": "In this section, we investigate how each component of HomoGCL,including the homophily loss (Eq. (11)) and the soft pair-wise nodesimilarity (the saliency S) contributes to the overall performance.The result is shown in . Here, in w/o homophily loss, wedisable the homophily loss in (Eq. (11)), and in HomoGCL, weadd neighbor nodes as positive samples indiscriminately (i.e., hardneighbors) to leverage graph homophily directly. From the table,we have two observations: Homophily loss is effective, as graph homophily is a distinctiveinductive bias for graph data in-the-wild. HomoGCL which estimates the probability of neighbors beingpositive performs better than HomoGCL, as the soft clusteringcan successfully distinguish true positive samples.The above observations show the effectiveness of the homophilyloss and the saliency S in HomoGCL."
}