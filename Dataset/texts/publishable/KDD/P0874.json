{
  "ABSTRACT": "In an era marked by robust technological growth and swift in-formation renewal, furnishing researchers and the populace withtop-tier, avant-garde academic insights spanning various domainshas become an urgent necessity. The KDD Cup 2024 AQA Chal-lenge is geared towards advancing retrieval models to identify per-tinent academic terminologies from suitable papers for scienticinquiries. This paper introduces the LLM-KnowSimFuser proposedby Robo Space, which wins the 2nd place in the competition. Withinspirations drawed from the superior performance of LLMs onmultiple tasks, after careful analysis of the provided datasets, werstly perform ne-tuning and inference using LLM-enhanced pre-trained retrieval models to introduce the tremendous language un-derstanding and open-domain knowledge of LLMs into this task,followed by a weighted fusion based on the similarity matrix de-rived from the inference results. Finally, experiments conductedon the competition datasets show the superiority of our proposal,which achieved a score of 0.20726 on the nal leaderboard.",
  "INTRODUCTION": "The overarching aim of scholarly data mining is to enhance ourcomprehension of the progression, essence, and direction of sci-ence. It possesses the capability to unveil substantial scientic, tech-nological, and educational worth. In an age of vigorous techno-logical expansion and rapid informational refreshment, equippingscholars and the general public with premier, cutting-edge aca-demic knowledge across diverse disciplines is now an imperativedemand. The 2024 KDD Cup AQA competition is oriented toward Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor prot or commercial advantage and that copies bear this notice and the full cita-tion on the rst page. Copyrights for components of this work owned by others thanACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-publish, to post on servers or to redistribute to lists, requires prior specic permissionand/or a fee. Request permissions from , August, 2024, Barcelona, Spain 2024 Association for Computing Machinery.ACM ISBN 978-1-4503-XXXX-X/18/06...$15.00 enhancing retrieval algorithms with the aim to pinpoint relevantacademic publications for scientic queries. Inspired by the re-markable achievements of large language models (LLMs) such asChatGPT , GPT4 in a variety of tasks due to their marvelouscapability in language comprehension and generation, in this pa-per, we introduces the LLM-KnowSimFuser solution proposed byRobo Space, which incorporates the tremendous language under-standing and open-domain knowledge of LLMs in this solution andwins the 2nd place in the competition (achieved a score of 0.20726on the nal leaderboard) and organizes this technical report as fol-lows: First, we outline the task objectives and present the statistics ofthe given datasets in detail. Subsequently, we introduce our data processing ow and pro-cess for ne-tuning and inference on LLM-enhanced pre-trainedretrieval modelswith a carefully designed similarity fusion mech-anism. Finally, we conduct comprehensive ablation study and param-eter analysis experiments on the competition datasets, whichdemonstrate the eectiveness and superiority of our proposal.",
  "DATASETS": "The KDD Cup 2024 Academic Question Answering (AQA) Chal-lenge is centered on tackling an academic retrieval problem. Thisendeavor employs a dataset that is systematically organized intotwo primary components: queries and documents. Queries embodyacademic questions, each structured with a concise title and anelaborative body that delineates the questions specics. Documents,on the other hand, represent academic papers, each comprising adescriptive title and an informative abstract that encapsulates thepapers core contributions.All participants are required to navigate through two phasesof the competition, with the latter building upon the former withenhanced complexity. The initial phase challenge contenders witha dened set of queries and documents, requiring the identicationof the most pertinent documents for each query. Progressing to thesecond phase, the test set expands and the document collectionsignicantly grows, escalating the tasks intricacy while the coreobjective of discerning relevancy persists. We list the details of thestatistics and objectives in .",
  ": Competition phases statistics and objectives summary": "Tuning a Pre-trained Model: Among the pre-trained models,one is ne-tuned. We then use this tuned model to extract em-beddings for both queries and documents. Similarity Matrix Computation and Fusion: Embeddings fromthe above models (ve in total, including the tuned one) are usedto compute similarity matrices between queries and documents.We then fuse and rank these ve similarity matrices to improverelevance assessment.",
  "Utilization of Pre-trained Retrieval Models": "We utilize fourpre-trained models: NV-Embed-v1, SFR-Embedding-Mistral1, GritLM-7B, and Linq-Embed-Mistral. All these mod-els are based on the Mistral framework , which excel in cap-turing rich semantic relationships and contextual nuances with ad-ditional open-domain knowledge, leading to more accurate and rel-evant search results compared to traditional retrieval models2. Be-sides, they share similar methods for prompt construction and em-bedding extraction. Notably, the GritLM-7B model employs meanpooling by default, while the other three models utilize last tokenpooling. Although it is feasible to use dierent pooling techniqueswith GritLM, we adhere to mean pooling to remain consistent withthe convention established during pre-training.For document embedding extraction, embeddings can be directlyobtained without additional prompt words. However, for queries,which are typically short and sparse in content, it is crucial to dif-ferentiate them from documents in a retrieval setting. Therefore,we experimented with various instructions and tags as prompts toenhance query embeddings, where we present the results of dier-ent congurations of tags and instructions in .",
  "Supervised Fine-Tuning of RetrievalModels": "Among the evaluated models, the SFR-Embedding-Mistral modelproved to be the most suitable candidate for ne-tuning due to itssimplicity and inherent exibility. We opt to use the Tevatron framework in conjunction with the Low-Rank Adaptation (LoRA) methodto optimize the models performance. For the ne-tuningprocess, we employed a comprehensive dataset comprising queriesand academic papers in the training set, ensuring that the modelcan be well-adapted to handle the specic academic retrieval taskswith high accuracy.To achieve this, we meticulously congure several key LoRA pa-rameters. Specically, we set the scaling factor to 64 and applieda dropout rate of 0.1 to prevent overtting by randomly deactivat-ing a fraction of neurons during training. Additionally, we dene 1 experiments, we incorporate traditional retrieval models such as BGE whentesting, however the outcome is not promising which demonstrates the superiority ofLLM-enhanced retrieval models in such specic domains. the rank of the low-rank matrices used for adaptation as 8. Thesecongurations are chosen to strike a balance between model com-plexity and performance. Notably, the model is ne-tuned for onlyone epoch. This decision is based on empirical evidence indicatingthat additional epochs of training led to a decline in performance,likely due to overtting. By limiting the ne-tuning process to asingle epoch, we are able to maintain the models optimal perfor-mance and generalization capabilities without compromising itseectiveness.",
  "Similarity Fusion of Pretrained Models": "In this section, we describe the process of integrating similaritymatrices derived from multiple models to achieve a unied and ro-bust retrieval outcome. We utilize four pretrained models and onene-tuned model, as detailed in Sections 3.1 and 3.2. The followingsteps will outline the detailed procedure for computing, normaliz-ing, and fusing the similarity matrices.Firstly, we compute the similarity matrices for the embeddingsof queries and documents from each model independently. Thisinvolves measuring the similarity between the query embeddingsand document embeddings generated by each model. We employFaiss , an ecient library that leverages GPU acceleration, toexpedite these similarity calculations. The use of GPU accelerationsignicantly enhances the computation speed, making it feasibleto handle large-scale data eciently. Next, to ensure comparabilityacross dierent models, we normalize the similarity matrices foreach model on a per-query basis. Normalizing the similarity matri-ces ensures that the scores from dierent models are on a uniformscale, which is crucial for fair and eective fusion. After normaliza-tion, we perform a weighted fusion of the similarity matrices. Eachmodels normalized similarity matrix is assigned a weight that re-ects its relative importance or performance, which combines thestrengths of the individual models, leveraging their diverse per-spectives to improve the robustness and accuracy of the similaritymeasurements. Finally, based on the aggregated similarity scores,we rank the documents for each query. We identify and select thetop 20 documents with the highest similarity scores as the nal re-sults for submission. This selection process ensures that the mostrelevant documents, as determined by the combined insights ofmultiple models, are presented as the output.",
  "Advancing Academic Knowledge Retrieval via LLM-enhanced Representation Similarity FusionKDD24, August, 2024, Barcelona, Spain": "with a learning rate of 1e-4, per-device batch size of 8, 1 epoch oftraining, query and passage lengths limited to 32 and 156 tokens,respectively. To promote reproducibility, our source code is pub-licly available on GitHub3, providing comprehensive guidance onthe operational processes. Detailed instructions on how to run thecode can be found in the README.md le within the repository.Additionally, specic execution parameters and hyperparametersfor each component are clearly outlined in their corresponding di-rectories.",
  "Performance Analysis of Individual andFused Retrieval Models": "In this section, we analyze the impact of various retrieval mod-els and the fused variant on performance across two evaluationphases. presents the best scores achieved in Phase 1 andPhase 2 by each model and ourproposedLLM-KnowSimFuser whichfuses the former models via similarity metrics, allowing for a com-parative assessment of their eectiveness and robustness.",
  ": Performance comparison in two evaluation phases": "The comparative analysis highlights that while dierent modelsexhibit varying degrees of eectiveness and stability, LLM-KnowSimFuserstands out with the highest scores in both evaluation phases. Itsability to maintain strong performance across diverse conditionsmakes it a highly eective and reliable retrieval model. This perfor-mance is indicative of the successful integration of LLM-enhancedrepresentation similarity fusion, enabling more accurate and con-sistent academic knowledge retrieval. The inclusion of similarityfusion results further underscores the potential of combining mul-tiple models to achieve superior performance, showcasing the ben-ets of an ensemble approach in enhancing retrieval tasks.",
  ": Dierent instruction congurations": "1. Given a question including title and body,retrieve relevant papers that answer thequestion.2. Given a question including title and body,retrieve the paper's title and abstractthat answer the question.3. Given a web search query, retrieve relevantpassages that answer the query.4. Given a question, retrieve passages thatanswer the question. Tag 1 (\\): which demonstrates robust performanceacross multipleinstructions, particularly with the SFR-Embedding-Mistral model, achieving the highest score of 0.18659 with In-struction 2. This suggests that this tag format is well-suited formodels that process structured text eectively. Tag 2 (< _ > < /_ > \\ <_ > < /_ >): which exhibitsstrong results with the GritLM-7B model. The structured XMLformat appears to enhance the models ability to parse and re-trieve relevant information, as evidenced by the score of 0.18622with Instruction 1. Tag 4 ( : \\ : ): which is the most ver-satile one, especially with the Linq-Embed-Mistral model. Thehighest overall performance score of 0.18925 was recorded withTag 4 and Instruction 2, indicating that this tag formats clearseparation of title and content is highly eective for this model.",
  "Instruction Impact": "Instruction 2 (Given a question including title and body,retrieve the papers title and abstract that answer the ques-tion.): which generally provides the best results across dier-ent tags and models. This instruction seems to align well withthe models retrieval mechanisms, suggesting that a focused re-trieval objective (title and abstract) enhances performance. Instruction 1 (Given a question including title and body,retrieve relevant papers that answer the question.): which",
  "also performs well, particularlywith Tag 1 and the SFR-Embedding-Mistral model. This indicates that a broader retrieval scope (en-tire papers) can be eective when paired with suitable tag for-mats": "4.3.3Model Specific Observations. SFR-Embedding-Mistral:which consistently performswell withTag 1 and dierent instructions, indicating its robustness andadaptability to this tag format. GritLM-7B: which shows strong performance with Tag 2, high-lighting its preference for well-structured tags. The model alsoperforms well with Tag 4 and Instruction 4, suggesting a degreeof exibility in handling structured queries. Linq-Embed-Mistral: which achieves the highest score overall,particularly with Tag 4 and Instruction 2. This combinationseectiveness underscores the importance of choosing the righttag-instruction pairing for maximizing model performance. NV-Embed-v1: which shows consistent performance, howeverit does not achieve the highest scores compared to the other mod-els. The highest score for NV-Embed-v1 was 0.18315 with Tag 3and Instruction 1, indicating potential areas for optimization. 4.3.4Possible Directions for Future Work. Further Exploration of Untested Congurations: there re-mains potential in exploring the full range of untested tag andinstruction combinations. By systematically testing these con-gurations, it may be possible to discover even more eectivepairings that are not covered in this study. Automated Prompt Generation: developing automated sys-tems to generate and test prompts dynamically could signi-cantly enhance the eciency of identifying optimal congura-tions. This approach would allow for a broader exploration ofthe parameter space and potentially uncover novel congura-tions that yield superior performance. Focused Optimization for Lower-Performing Models: spe-cic attention should be directed towards optimizing tags and in-structions for models such as NV-Embed-v1. By understandingand addressing the limitations that led to lower performance, itmay be possible to enhance the retrieval eectiveness of thesemodels. Model-Specic Tailoring: customizing tags and instructionsbased on the characteristics and strengths of individual modelscould further improve performance. For instance, models thatexcel with structured tags (like GritLM-7B) could benet fromeven more rened tagging strategies. In conclusion, the ndings of this study underscore the signi-cant impact of tag and instruction congurations on retrieval modelperformance, and strategic selection and optimization of these el-ements can lead to substantial improvements, and future researchshould continue to explore and rene these congurations to fullyrealize their potential.",
  "CONCLUSION": "In this paper, we presents our solution for the KDD Cup 2024 OAG-Challenge AQA. To tackle this task, we employ a multi-step ap-proach. Firstly, we ne-tune and perform inference using LLM-enhanced pre-trained retrieval models, capitalizing on the power-ful language understanding and retrieval capabilities of large lan-guage models. Next, we conduct a weighted fusion of the inferenceresults, leveraging a similarity matrix derived from these results tooptimize the retrieval performance. Through this meticulous pro-cess, our team, Robo Space, achieves a commendable nal score of0.20726, and ranks the 2nd place on the nal leaderboard. Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2022. Tevatron: AnEcient and Flexible Toolkit for Dense Retrieval. arXiv preprint arXiv:2203.05765(2022). Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, SheanWang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of LargeLanguage Models. In ICLR. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, De-vendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel,Guillaume Lample, Lucile Saulnier, Llio Renard Lavaud, Marie-Anne Lachaux,Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothe Lacroix,and William El Sayed. 2023. Mistral 7B. arXiv preprint arXiv:2310.06825 (2023). Je Johnson, Matthijs Douze, and Herv Jgou. 2021. Billion-Scale SimilaritySearch with GPUs. IEEE Trans. Big Data 7, 3 (2021), 535547. Junseong Kim, Seolhwa Lee, Jihoon Kwon, Sangmo Gu, Yejin Kim, MinkyungCho, Jy yong Sohn, and Chanyeol Choi. 2024. Linq-Embed-Mistral:ElevatingText Retrieval with Improved GPT Data Through Task-Specic Control andQuality Renement. Linq AI Research Blog (2024). Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, MohammadShoeybi, Bryan Catanzaro, and Wei Ping. 2024.NV-Embed: Improved Tech-niques for Training LLMs as Generalist Embedding Models.arXiv preprintarXiv:2405.17428 (2024). Niklas Muennigho, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu,Amanpreet Singh, and Douwe Kiela. 2024. Generative Representational Instruc-tion Tuning. arXiv preprint arXiv:2402.09906 (2024). OpenAI. 2023. Chatgpt: Optimizing language models for dialogue. OpenAI. 2023. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774 (2023)."
}