{
  "ABSTRACT": "Most applications of machine learning for finance are related toforecasting tasks for investment decisions. Instead, we aim to pro-mote a better understanding of financial markets with machinelearning techniques. Leveraging the tremendous progress in deeplearning models for natural language processing, we construct ahierarchical Reformer () model capable of processing a largedocument level dataset, SEDAR, from canadian financial regulatoryfilings. Using this model, we show that it is possible to predict tradevolume changes using regulatory filings. We adapt the pretrainingtask of HiBERT () to obtain good sentence level representationsusing a large unlabelled document dataset. Finetuning the modelto successfully predict trade volume changes indicates that themodel captures a view from financial markets and processing regu-latory filings is beneficial. Analyzing the attention patterns of ourmodel reveals that it is able to detect some indications of materialinformation without explicit training, which is highly relevant forinvestors and also for the market surveillance mandate of financialregulators.",
  "INTRODUCTION": "The entire financial services industry is based on the trust investorshave in the system. Preserving this trust is a core mandate fromall regulators across the world. Machine learning techniques havethe potential to improve several aspects in this industry, includingtrust. However, most interest so far has been on forecasting tasksfor investment decisions.A better understanding of the markets is useful for everyone inthe system. For investors, this translates to a more accurate analysisof the risk-return trade-off. For regulators, it helps to focus theirefforts on detecting potential market manipulations and ensuringtrust in the overall market.In this work, we focus on providing answers to the followingresearch questions:",
  "PROBLEM DEFINITION2.1Background": "\"Material change\", according to securities legislation, is defined asa change in the business, operations or capital of the issuer thatwould reasonably be expected to have a significant effect on themarket price or value of any of the securities of the issuer andincludes a decision to implement such a change made by the boardof directors of the issuer by senior management of the issuer whobelieve that confirmation of the decision by the board of directors isprobable. Regulators require issuers to disclose immediately these\"material changes\" in order to ensure a level playing field for allinvestors, and therefore, to ensure trust in financial markets.",
  "Formal definition": "In order to better understand potential material changes, we de-fine the following surrogate downstream task: using documents,publicly available for investors, we try to predict the trade volumemovement within a 1 business day time horizon from the releasedate for the stock associated with this document. Let the dailyvolume for a specific stock for the day .",
  "RELATED WORK3.1Market signals and market efficiency": "Finance is the management of money by companies, organisations,or governments, as defined by Wikipedia1. It relates to spendingand investment of capital, which are decisions made from opinions.Decision processes may be the result of human opinions directly,with manual trade executions, or indirectly, thanks to algorithmicexecutions using human prior knowledge. With this insight, from amarket perspective, indicators, resulting from market activities, arethe equilibrium from different investor opinions. These indicatorsare commonly referred to as market signals. Market signals can betrade prices, prices volatility, limit order book information, or tradevolumes.The main challenge with using market signals, which can beused as direct inputs for investment decisions, is related to theEfficient Market Hypothesis (EMH), Malkiel and Fama . TheEMH suggests that no one can beat the market without additionalrisk, which is described with the \"No Free Lunch\" principle. Whilethere is still a debate regarding the validity of this hypothesis, Fama, Malkiel , research suggests that the semi strong form ofEMH, where all past and present public information is reflected inmarket prices, seems to hold for developed markets, at least to someextent, see Rossi for literature review. Thus, the consensus isthat EMH appears to be a good approximation of market behavioursand one of the main reasons for the difficulty in discovering andusing market signals.In order to tackle this issue, several strategies are often used likeincorporating domain knowledge, such as feature engineering withcreation of domain specific lexicon from 10-K filings, Loughran andMcDonald , capturing specific events with open informationextraction, Ding et al. , and also exploiting new dataset, referredas alternative data, such as Twitter to capture investors sentiment,Smailovi et al. .Due to the strong interest of exploiting market signals for in-vestment decisions, most of research focused on price forecasting or price movement direction, (Chong et al. , , Bartram et al.). Some exceptions can be found for alternative market signals.For price volatility, Theil et al. suggests SEC 10-k filings havegot predictive power. Bordino et al. suggests web traffic haspredictive power for trade volumes. However, to the best of ourknowledge, none are leveraging these market signals as interpreta-tion from markets point of views.",
  "NLP": "Contextual Word Embeddings. Recently, there has been a shiftfrom using distributional word representations (Mikolov et al. ,Pennington et al. ), which result in a single global represen-tation for each word ignoring their context, to contextual embed-dings, where each token is associated with a representation that is afunction of the entire input sequence. These context-dependent rep-resentations can capture many syntactic and semantic properties ofwords under diverse linguistic contexts. Previous work (Clark et al., Devlin et al. , Joshi et al. , Lan et al. , Liu et al. , Pe-ters et al. ) has shown that contextual embeddings pretrainedon large-scale unlabelled corpora achieve state-of-the-art perfor-mance on a wide range of natural language processing tasks, suchas text classification, question answering and text summarization.Document level embeddings. HIerachical Bidirectional EncoderRepresentations from Transformers (Zhang et al. ) builds uponBERT (Devlin et al. ) and proposes a pretraining scheme fordocument level embeddings. To obtain the representation of a docu-ment, they use two encoders: a sentence encoder to transform eachsentence in the document to a vector and a document encoder tolearn sentence representations given their surrounding sentencesas context. Both the sentence encoder and document encoder arebased on the Transformer encoder (Vaswani et al. ) nested in ahierarchical fashion. They use a variant of the Masked LanguageModeling paradigm using sentences as the basic unit instead ofwords. i.e. they predict masked out sentences given the context.They show that such a pretraining scheme is highly effective andallows them to achieve SOTA results on summarization tasks.Self-Attention Variants. Recently, there has been a lot of interestin breaking the quadratic self attention used in transformer (Beltagy",
  "Discovering material information on financial regulatory filingsKDD Workshop on Machine Learning in Finance 2021, August, 2021, Virtual": "et al. , Kitaev et al. , Wang et al. , Zaheer et al. ) withlower time and memory complexities, enabling the processing oflarger sequences and giving rise to better models. Simplest methodsin this category just employ a sliding window, but in general, mostwork fits into the following general paradigm: using some othermechanism select a smaller subset of relevant contexts to feed inthe transformer and optionally iterate. In this work, we use theReformer model which introduces the following improvements:(1) using reversible layers to remove the need to store intermediaryactivations for the backpropagation algorithm; (2) splitting activa-tions inside the feed-forward layers and processing them in chunks;(3) approximating attention computation based on locality-sensitivehashing.",
  "We use the following notation: = (1,2, ...,| |) for the sequence": "of sentences in a document, = (1 ,2 , ..., | |) for the sequenceof subword tokens for the th sentence and for the th subwordtoken for the th sentence.The base model is the module shared by all the different tasksand is composed of two main submodules, both Reformer models(Kitaev et al. ):",
  "Hierarchical model for the pretraining task": "For the pretraining task, we used Hibert (Zhang et al. ) withthe adaptation to long documents by using Reformer (Kitaev et al.) instead of Transformer (Vaswani et al. ).Let represents the set of indices of the masked sentences.For the masked sentences with indices , and for eachsubword tokens with indices [1; | |], we replace all originaltokens by the mask token <MASK>. We then use a base modelto get the contextual embeddings { | } at sentence level forthese masked sentences.As the context, represented by , is a fixed vector for eachsubword prediction, this context is injected by adding this sentence",
  "Hierarchical model for the surrogatedownstream task": "The classification task aims to incorporate knowledge from marketsignals into the hierarchical model. For this task, for a document ,the label is is constructed as per equation 1.On top of the base model, we add a single Reformer layer withglobal attention mechanism to retrieve a latent representation ofthe sequence of sentences (1,2, ...,| |) for the document . Theattention weights used by this Reformer layer will be used laterto retrieve the focus of the model.",
  "EXPERIMENTS AND RESULTS5.1Dataset": "5.1.1SEDAR. The System for Electronic Document Analysis andRetrieval (SEDAR)2 is a filing system developed for the CanadianSecurities Administrators3 (CSA), umbrella organization of provin-cial and territorial regulators in Canada, to: facilitate the electronicfiling of securities information as required by CSA; allow for thepublic dissemination of Canadian securities information collectedin the securities filing process; and provide electronic communica-tion between electronic filers, agents and the CSA. It can be viewedas the Canadian equivalent of EDGAR, the filing system managedby the SEC4.For the purpose of this work, we collected 3.8M documents inEnglish from 1997 to October 2018. These documents were origi-nally in PDF and have been converted into raw text format. Thisconversion has resulted in a certain amount of noise into the rawtext. For the pretraining task, we used 2M documents randomly se-lected from the overall dataset. All selected documents were before2018 to prevent information leakage.For the downstream task, we only used News releases and Man-agement, Discussion and Analysis (MD&A) documents for firmspart of the TSX S&P 60 index as of 30th July 2020. The selecteddocument types are among the most known ones to broadcast fi-nancial information that are not part of standardised accountingmetrics (e.g.: net income, cash flow from operations, ...). They aretherefore highly susceptible to contain new material information.The training set contained 14,241 documents before 2018. The vali-dation and test set are constituted by all documents in 2018, fromwhich a random allocation has been performed to have 50% for val-idation set and 50% for test set. This results into 612 documents forvalidation set and 613 documents for test set. 89.5% of documentscontain less than 512 sentences and 94% sentences contain less than128 subwords. 5.1.2Market data. For the downstream task, we collected dailytrade volumes from 2004 from the Bloomberg Terminal for all com-ponents from the TSX S&P 60 Index. We then computed the dailychange as per equation 1 and joined this dataset with SEDAR usingfiling dates and ticker codes. Labels are fairly imbalanced with 59%,63.6% and 57.8% up movements for train, validation and test setsrespectively.",
  "Implementation details": "5.2.1Tokenization. We used BBPE (Sennrich et al. ) implemen-tation from HuggingFace5 with a vocabulary size 8,000. Similar tocommon practices in natural language processing, for each sen-tence , <BOS> and <EOS> are added to represent the beginning ofsentence and the end of sentence. 5.2.2Base model. The base model used for following hyperparam-eters: 8 attention heads, 8 layers (4 for the Reformer model at wordlevel and 4 for the Reformer model at sentence level), intermediarysize 2048, maximum sentence length 128 and maximum number ofsentences 512. The number of parameters for the base model was67M. We used the Reformer implementation from HuggingFace6. 5.2.3Pretraining. We trained our model with 2M documents for1 epoch with a learning rate of 2e4 with a linear learning rateschedule and with a batch size 32 using gradient accumulation. Theduration of the training was 20 days using a single GPU (11GB). 5.2.4Downstream task. After hyperparameter search, we trainedour best model for 2 epochs with a learning rate of 3e6 for modelswith frozen base model encoder and 2e5 otherwise, with a1 factor0.1, with a cosine annealing learning rate schedule and with a batchsize 32 using gradient accumulation. We used 2 layers MLP forclassification head. Each training run lasted less than 1 day using asingle GPU (11GB).",
  "Results and discussion": "5.4.1Predicting market signals from regulatory filings. Our results,summarized in the table 1, confirm the predictive power from onlyusing regulatory filings for trade volume change prediction.Our model, without pretraining, outperforms significantly \"sim-ple\" baselines with the exception of F1 score. F1 score doesnt takeinto account true negatives, which penalizes our model comparedto the majority class prediction model. Despite the former, ourmodel still achieves similar performance statistically. We only keepF1 score for comparison as this metric is commonly reported inrelated studies.As a conclusion, our results support the importance of infor-mation from regulatory reporting, at least for News releases andMD&A, for investors in their investment decisions. 5.4.2Pretraining on document dataset. In this experiment, we com-pare pretrained models with frozen base model with randomly ini-tialized model with frozen base model and a fully trainable model,also randomly initialized.As indicated by the table 2, the model pretrained with 2M docu-ments outperforms the fully trainable model with statistical signifi-cance on ROC-AUC metric. For MCC metric, the model pretrainedwith 600K documents is statistically similar to the fully trainablemodel. For the F1 score, all models are statistically similar, makingthis metric less useful for ranking. Therefore, models ranking de-pends on the selected metric, but pretrained models outperform thefully trainable model on all metrics, except MCC for which resultsare statistically similar.Moreover, we observed that the best performing model, in termof validation loss, on the pretraining task was the one pretrainedon 600K documents. This model is also the best on the MCC metric.For other metrics, it still remains statistically similar to the onepretrained on 2M documents. Whereas the common knowledgeassumes that further pretraining helps, see Liu et al. , we didntfind it in our case. As model capacity limited by the single GPUmemory constraint, our models were smaller in number of param-eters than Zhang et al. . Thus, we believe bigger models mayprovide better results for this task.Furthermore, an interesting observation is that, we noticed thatpretrained models have less variance on training set and validationset losses during training than the one with frozen weight and ran-dom initialization. This suggests that initialization from pretrainingdoes help to have a more robust training.We conclude that the Hibert (Zhang et al. ) pretraining taskis slightly beneficial for learning good representation from financialtext corpus. 5.4.3Qualitative analysis. For this experiment, we analyzed thebest predictions on validation set from our model, meaning themost confident predictions for increase and decrease trade volumes.We reviewed the documents qualitatively and we also looked at theattention weights at the Reformer layer, on top of the base model, which is specialized for the downstream task, see section 4.3. Weapplied two attention patterns analysis, one using raw attentionweights values similar to the study of BERT attention weights fromClark et al. and one using attention vector norms (Kobayashiet al. ). For both analyses, results were fairly consistent, even ifsome orders slightly changed.Despite our use of 1 regularizer to encourage sparsity, attentionpatterns are quite broad among attention heads. This indicates thatthere is no strong focus on a single sentence, but rather a broad fo-cus on all sentences. While the focus appears to contain some noise,we noticed the model tends to focus more on: the second sentence ofthe document; sentences containing information about the firm (ex:ticker, firm name, web site url); statements related to disclosures ofrisks (ex: \"Forward-looking statements in this document include, butare not limited to, statements relating to our financial performanceobjectives, vision and strategic goals, and include our President andChief Executive Officers statements.\"); and also sometimes on men-tions to non \"Generally Accepted Accounting Principles\" (GAAP)measures. While focus on firm name information is quite intuitive,the focus on information about non GAAP measures is quite sur-prising. Indeed, non GAAP measures have received some attentionsfrom literature and regulators for decades as they may mislead in-vestors in some cases, Bradshaw and Sloan , Entwistle et al. ,Marques . The level of noise in the attention pattern remains anopen question. Some reasons may be due to the model capacity, ourassumption of using only one document to predict market eventwithout further context, or also to the intrinsic evolving nature ofmarkets.At the document level, the most confident predictions of de-crease of trade volumes appear to be on documents containingno clear new information: such as date confirmation for a resultannouncement; termination of already announced third party sharerepurchase program; or third party recognition related to \"Envi-ronmental, Social and Corporate Governance\" (ESG). All of thesedocuments contained no accounting or new business developmentinformation. For the most confident predictions of increase of tradevolumes, documents contain new accounting information, such asincrease of net income or debt refinancing. See figures in appendixfor examples. The previously mentioned patterns are consistentwith the common knowledge for fundamental analysis.To conclude, attention weights patterns appear to be too noisyto be used as a clear extractive summary for documents with ourapproach. At the document level, our model predictions are inlinewith accepted knowledge about the focus of the market, despite noprior knowledge about markets and no strong labels. Thus, withoutthe need of labelling documents, we believe that our model canbe relevant for finding documents containing potential materialinformation, which could help investors for their risk managementand also regulators for their market surveillance mandate.",
  "ModelROC-AUCMCCF1": "Ours - random init.[56.9%, 57.8%][12.8%, 14.5%][72.5%, 73.2%]Ours - frozen+random init.[55.1%, 56.2%][ 3.5%, 5.6%][72.7%, 73.4%]Ours - frozen+pretrained 600K docs[57.5%, 58.4%][13.6%, 15.2%][72.2%, 73.0%]Ours - frozen+pretrained 2M docs[58.0%, 59.0%][11.6%, 13.1%][72.2%, 73.0%] hierarchical model for capturing sentence level and document levelcontextualized embeddings; and a surrogate downstream task toalign market signals, volume prediction in our work, with financialfilings text dataset. We also show the benefits of the HiBERT (Zhanget al. ) pretraining task to improve the quality of sentence levelembeddings by using a large unlabelled financial corpus. Finally,while attention patterns learnt by our model are still noisy, we wereable to demonstrate the ability to discover material informationwithout prior knowledge, which is relevant to regulators for theirmarket surveillance mandate. With this work, we hope to encour-age research between deep learning and finance communities asbenefits could deserve all actors in the financial industry, includingregulators, and ultimately users.",
  "M. T. Bradshaw and Richard E G Sloan. Gaap versus the street: An empiricalassessment of two alternative definitions of earnings. Journal of AccountingResearch, 40:4166, 2000": "Eunsuk Chong, Chulwoo Han, and Frank C Park. Deep learning networks forstock market analysis and prediction: Methodology, data representations, andcase studies. Expert Systems with Applications, 83:187205, 2017. Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning.What does BERT look at? an analysis of BERTs attention. In Proceedings of the2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks forNLP, pages 276286, Florence, Italy, August 2019. Association for ComputationalLinguistics. doi: 10.18653/v1/W19-4828. URL",
  "Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning.ELECTRA: Pre-training text encoders as discriminators rather than generators.In ICLR, 2020. URL": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT:Pre-training of deep bidirectional transformers for language understanding. InProceedings of the 2019 Conference of the North American Chapter of the Associationfor Computational Linguistics: Human Language Technologies, Volume 1 (Long andShort Papers), pages 41714186, Minneapolis, Minnesota, June 2019. Associationfor Computational Linguistics. doi: 10.18653/v1/N19-1423. URL Xiao Ding, Yue Zhang, Ting Liu, and Junwen Duan. Using structured events topredict stock price movement: An empirical investigation. In Proceedings of the2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),pages 14151425, 2014.",
  "Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translationof rare words with subword units. CoRR, abs/1508.07909, 2015. URL": "Jasmina Smailovi, Miha Grar, Nada Lavra, and Martin nidari. Predictivesentiment analysis of tweets: A stock market application. In Andreas Holzingerand Gabriella Pasi, editors, Human-Computer Interaction and Knowledge Discov-ery in Complex, Unstructured, Big Data, pages 7788, Berlin, Heidelberg, 2013.Springer Berlin Heidelberg. ISBN 978-3-642-39146-0. Christoph Kilian Theil, Sanja tajner, and Heiner Stuckenschmidt.Wordembeddings-based uncertainty detection in financial disclosures. In Proceedingsof the First Workshop on Economics and Natural Language Processing, pages 3237,Melbourne, Australia, July 2018. Association for Computational Linguistics. doi:10.18653/v1/W18-3104. URL"
}