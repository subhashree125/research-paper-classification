{
  "ABSTRACT": "Interpretability of Deep Neural Networks using concept-based mod-els offers a promising way to explain model behavior throughhuman-understandable concepts. A parallel line of research fo-cuses on disentangling the data distribution into its underlyinggenerative factors, in turn explaining the data generation process.While both directions have received extensive attention, little workhas been done on explaining concepts in terms of generative factorsto unify mathematically disentangled representations and human-understandable concepts as an explanation for downstream tasks.In this paper, we propose a novel method CoLiDR - which utilizesa disentangled representation learning setup for learning mutuallyindependent generative factors and subsequently learns to aggre-gate the said representations into human-understandable conceptsusing a novel aggregation/decomposition module. Experimentsare conducted on datasets with both known and unknown latentgenerative factors. Our method successfully aggregates disentan-gled generative factors into concepts while maintaining parity withstate-of-the-art concept-based approaches. Quantitative and visualanalysis of the learned aggregation procedure demonstrates theadvantages of our work compared to commonly used concept-basedmodels over four challenging datasets. Lastly, our work is general-izable to an arbitrary number of concepts and generative factors -making it flexible enough to be suitable for various types of data.",
  "disentanglement, xai, concept learning, generalization": "ACM Reference Format:Sanchit Sinha, Guangzhi Xiong, and Aidong Zhang. 2024. CoLiDR: ConceptLearning using Aggregated Disentangled Representations. In Proceedings ofthe 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining(KDD 24), August 2529, 2024, Barcelona, Spain. ACM, New York, NY, USA,12 pages. Permission to make digital or hard copies of part or all of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for third-party components of this work must be honored.For all other uses, contact the owner/author(s).KDD 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s).ACM ISBN 979-8-4007-0490-1/24/08",
  "INTRODUCTION": "The increasing proliferation of Deep Neural Networks (DNNs) hasrevolutionized multiple diverse fields of research such as vision,speech, and language . Given the black-box nature of DNNs,explaining DNN predictions has been an active field of researchthat attempts to impart transparency and trustworthiness in theirdecision-making processes. Recent research has categorized ex-plainability into progressively increasing levels of granularity. Themost fine-grained approaches attempt to assign importance scoresto the raw features (e.g. pixels) extracted from the data, while lessgranular approaches assign importance scores to data points (setsof features). Explaining DNNs using concepts provides the high-est level of abstraction, as concepts are high-level entities sharedamong multiple similar data points that are aligned with humanunderstanding of the task at hand. This makes concept explanationmuch more global in nature. Many recent approaches for concept-based explanations have attempted to either 1) infer concepts post-hoc from trained models or 2) design inherently explainableconcept-based models , such as the concept bottleneck model(CBM) .A parallel field of research on disentanglement representationlearning attempts to learn a low-dimensional datarepresentation where each dimension independently represents adistinct property of the data distribution. These approaches learnmutually independent generative factors of data by estimating theirprobability distribution from observed data. Once the probabilitydistribution of the generative factors is estimated, a given samplecan be theoretically decomposed and re-generated from its genera-tive factors. Due to their ability to uncover the underlying gener-ative factors, disentanglement approaches are considered highlyinterpretable.Present state-of-the-art approaches do not effectively unify dis-entangled representation learning with concept-based approaches.Approaches like GlanceNets attempt to align concepts with dis-entanglement with strong assumptions, which are not valid for real-world datasets. To address the above issues, in this paper, we pro-pose Concept Learning using Aggregated Disentangled Representa-tions (CoLiDR), a self-interpretable approach that combines disen-tangled representation learning with concept-based explainability.Specifically, CoLiDR learns disentangled generative factors usinga disentangled representation learning module, followed by theaggregation of learned disentangled representations into human-understandable concepts using a novel aggregation/decompositionmodule and subsequently a task prediction module that maps con-cepts to task labels. Our experiments show that interventions onlearned concept representations can fix wrongly classified sam-ples, which makes CoLiDR useful for model debugging.",
  "Disentangled Generative Factors": "InputImage TaskLabel ....... ... AggregationDecomposition Human-Understandable Concepts : Schematic overview of the proposed CoLiDR ap-proach. The input data distribution X is first disentangledinto mutually independent generative factors (GFs). Sub-sequently, the GFs are aggregated into concepts. Note thatthe concepts are modeled as a set of annotated concepts cor-responding to concepts with annotation by humans and aseparate set of concepts that are useful for prediction, but areunannotated. Finally, the concepts are utilized for predictingthe task label Y. demonstrates the schematic overview of CoLiDR, where the learneddisentangled generative factors from data are aggregated into con-cepts, which are further utilized for task prediction. Specifically,our main contributions are as follows: 1) Propose CoLiDR frame-work, a novel method to aggregate disentangled representationsinto human understandable concepts, while achieving at-par per-formance to other commonly utilized concept-based models such asCBMs and GlanceNets, 2) Improve model debugability by test-timeinterventions, and 3) Provide a flexible framework which can begeneralized to arbitrary number of generative factors and concepts.",
  "RELATED WORK2.1Related Work on DisentangledRepresentation Learning": "Disentangled representation learning has long been a fascinatingstudy aimed at separating distinct informational factors of varia-tions in real-world data . Due to its probabilistic frameworkand flexibility to customize training objectives, the variational au-toencoder (VAE) is a commonly used architecture in the studyof disentangled representation learning, which can capture differ-ent factors of variation with its encoder and decoder. Based onthe traditional VAE, Higgins et al. proposed a variant, -VAE,which introduces an additional hyperparameter to scale the impor-tance of the regularization term. By changing the weight of theregularization term, -VAE can control the trade-off between thereconstruction of inputs and the disentanglement of latent vari-ables. Instead of optimizing the KullbackLeibler (KL) divergencebetween the latent distribution and standard Gaussian prior fordisentanglement as done by VAE and -VAE, FactorVAE and -TCVAE further decompose the regularization term and proposeto directly penalize the total correlation between latent variables,which are shown to better disentangle the variables.While early studies in disentanglement representation learningattempted to learn independent latent variables by modifying thetraining objective of VAE , Locatello et al. showed that it is impossible to learn identifiable disentangled latent vari-ables without any supervision. A series of subsequent studies werethen proposed to learn disentangled representations with betteridentifiability using different kinds of supervision .",
  "Related Work on Concept BasedExplanations": "Due to the black-box nature of deep learning models, variousapproaches have been explored to provide explanations for theoutcome of deep neural networks . One impor-tant direction is to interpret the models with intuitive and human-understandable concepts, which are usually high-level abstractionsof the input features. Various attempts have been made to automat-ically learn the concepts for different tasks . Amongthem, the concept bottleneck model (CBM) is a commonlyused approach to incorporate the learning of concepts into deepneural networks. By constructing a low-dimensional intermediatelayer in the models, CBMs are able to capture the high-level con-cepts that are related to the downstream tasks. Numerous studieshave been carried out to adapt CBMs for tasks in various domains. The most relevant works to our proposed ap-proach are Concept Bottleneck Models , Concept EmbeddingModels , CLAP and GlanceNets , details of which arediscussed in the next Section.",
  "UNIFYING CONCEPT-BASED MODELINGWITH VARIATIONAL INFERENCE": "Problem setup. The concept learning problem is characterizedas a two-step process for a given training sample {x, y, C}.Given an observation , a concept-based model learns a functionto map the input to its associated human-understandable conceptsC = {1, , }. Subsequently, a predictor will project the conceptembeddings to , which is the prediction for the label of the samplein a downstream task.Assumptions. Following previous work in concept-based models, concept models follow two fundamental assumptions asfollows: 1) the data distribution in the input space can be accuratelymapped to the distributions of concepts in the latent space, and 2)the concept scores are necessary and sufficient to predict labels ofsamples in downstream tasks.",
  "Variational Inference-basedConcept-learning for Greater Interpretablity": "Utilizing disentanglement approaches to explain the data genera-tion process has been well studied recently . A typical disen-tanglement system attempts to learn a fundamental representationof mutually independent generative factors (GFs) usually modeled asrandom variables from independent probability distributions. TheVariational Inference (VI) process utilizes the learned generativefactors to emulate the data generation procedure. Even though vari-ational inference has been successfully utilized for controlled datageneration with success , utilizing them to improve the inter-pretability of concept-based models is still relatively underexplored.Our work attempts to fill this gap. Below, we list the advantages ofusing variational inference-based approaches for concept learning:",
  "CoLiDR: Concept Learning using Aggregated Disentangled RepresentationsKDD 24, August 2529, 2024, Barcelona, Spain": ": Architecture of the proposed CoLiDR approach. CoLiDR consists of three modules - the Disentangled RepresentationsLearning (DRL) Module which learns disentangled generative factors (top), the Aggregation/Decomposition Module whichlearns to aggregate the generative factors into concepts and subsequently decompose them back into generative factors (bottom-left) and the Task Learning module (bottom-right) which utilizes the concepts to perform task label prediction.",
  "Comparison with Existing Approaches": "We begin the discussion by presenting an anti-causal modelto visualize the data generation process of our proposed approachand compare it to multiple comparable recent state-of-the-art ap-proaches in . Note that we represent the task labels as Y,the set of concepts as C, the disentangled generative factors as Z,and the data distribution as X. The single-edge arrows model thedependency between distributions while the double-ended arrowsrepresent variational inference - modeling the generative process.Comparisons to CBM/CEM. Concept Bottleneck Models andConcept Embedding Models do not incorporate variationalinference. Nevertheless, they visualize the data-generative process as being conditioned on the annotated concepts and also assumethat the task labels are entirely conditioned on the concepts.Comparisons to CLAP. CLAP is one of the first works uti-lizing variational inference to model the data generation process.CLAP considers a part of the disentangled space representing rel-evant GFs to be conditioned on the task labels and a part of itrepresenting confounds to be conditioned on independent normaldistributions. However, CLAP does not utilize human-annotatedconcepts - making the learned disentangled space Z unidentifiable(Refer ). This makes CLAP not comparable to our approach.Comparisons to GlanceNets. GlanceNets , to our knowledge,is the only existing approach that attempts to bridge the gap be-tween VI and concept learning. The GlanceNets approach incorpo-rates a Variational Autoencoder based disentangling mechanism tolearn the generative factors (GFs) of the distribution and formulatethe concept learning problem as a one-to-one mapping between asubset of generative factors (or disentangled representations) andhuman-understandable concepts. As shown in , the anno-tated concepts directly supervise a part of the learned disentangledspace Z. However, in doing so it makes an implicit assumption thatthe human-understandable concepts act as generative factorsof the data distribution. This is a strong assumption for two distinctreasons. Firstly, multiple generative factors can contribute as a con-stituent of a particular concept and multiple concepts can sharethe same generative factors. Secondly, human-annotated conceptsare by definition, abstract and high-level and do not necessarilymodel the fine-grained data generation process. Utilizing abstractconcepts to supervise the disentangling process may underminethe disentanglement procedure, as disentangled representations aresupposed to capture the low-level GFs of the data distribution in-stead of high-level abstract human concepts. The model is shown tobe effective on two synthetic datasets, dSprites and MPI-3D, whichare procedurally generated using carefully curated GFs that cor-respond to human-understandable concepts. In addition, CoLiDRcan effectively generalize to datasets where GFs are completelyunknown. Unannotated concepts. Note that we model the con-cept set as a union of known, annotated concepts, and unannotatedconcepts. This modeling approach is common in unsupervised con-cept learning approaches like , but has not yet been studied forVariational Inference based concept learning. The assumption isthat there exists some concepts relevant to the task prediction butare not manually annotated. lists the differences between the discussed approachesbased on - (i) the Presence of VI, (ii) the Presence of unsupervisedGFs, (iii) the Presence of supervised concepts, and (iv) the Presenceof unannotated concepts (Discussed in Methodology).",
  "Drawbacks of Supervising GFs withConcepts": "Although it might be tempting to utilize concepts as GFs like whatGlanceNets did, it is dangerous for a variety of reasons. Inthis section, we provide the drawbacks associated with this lineof thought with a concrete and practical example. Let us consideras a sample from the CelebA dataset which consists of facialphotographs of celebrities. Each image in the dataset is annotatedwith binary concepts such as the presence of black hair, blondehair, wavy hair, straight hair which are easily understandableby humans - implying they are abstract. Consider the assumption(fundamental in ) that each of these concepts also align withthe generative factors of the data distribution as well.Limited concept abstraction. As it can be seen, the assumption isflawed - two identical images with straight hair and different colors(Black and Blonde) would be sampled from completely differentdistributions despite sharing the same type of hair. Images of twopeople having the same hair color but with wavy and straighthair respectively would also be sampled from completely differentdistributions despite sharing the same color. Hence, there is no fine-grained control over the actual generative factors. Furthermore, asconcepts such as attractiveness are very subjective and containa large number of constituent underlying distributions, it is muchmore effective to understand constituent distributions than directlyaligning attractiveness with a single disentangled representation.Limited intra-concept hierarchy modeling. By definition ofdisentanglement, generative factors and concepts would be inde-pendent of each other. Any hierarchical relationship cannot beencoded in such a space. For example, the concept black hair isideally sampled from mutually independent distributions repre-senting blackness and multiple other distributions constitutingin construction of hair themselves. As such, blackness can alsobe combined with distributions modeling facial hairs for beardconcepts.Hence, instead of directly aligning the concepts with disentan-gled representations, it is more explainable and reasonable to under-stand the actual distributions constituting a concept. Relaxing theassumption of aligning disentangled latent space representationswith concepts helps us to better capture and explain constituentdistributions in each concept - in turn improving explainability.",
  "METHODOLOGY": "We first provide a broad overview of the proposed approach CoL-iDR in .1. .2 details the architectures utilized forcomputing disentangled representations. Subsequently, .3introduces the Aggregation/Decomposition module and the taskprediction network. Finally, .4 describes the training proce-dure and additional Disentangled Representation (DR) Consistencyloss which regularizes the Aggregation/Decomposition module tocorrectly learn representations to concept mapping.",
  "Disentangled Representations Learning(DRL) Module": "The first step of the proposed approach involves learning disen-tangled representations corresponding to the various generativefactors in the data. As depicted in , the generative factorsform the basis of the data generation process itself. Our DRL modulelearns the disentangled generative factors that can be used for datageneration. Suppose is the given embedding of the generative fac-tors and is the corresponding generated data. The underlying datageneration process p (|) can be estimated using a -VAE, whichestimates the maximum likelihood using variational inference. Fol-lowing , we maximize the Evidence Lower Bound (ELBO) tomodel the posterior distribution (|) and the distribution ofdata generation (|) as detailed below.",
  "= D ( (z|x) || (z)) + E (|) [ (x|z)](1)": "The E term in Formula 1 comprises the reconstruction loss betweenthe input and predicted reconstruction , usually taken as MeanSquare Error (MSE). The prior distribution p is usually taken as astandard Gaussian distribution which encourages the covariancematrix of the learned distribution to be diagonal, enforcing inde-pendence constrain . The tunable hyperparameter acts asa quantitative measure of the extent of disentanglement.-VAE estimates using an encoder that maps from the inputobservation space in R to the disentangled representation space inR, where and are the dimensions of the input and latent spacerespectively. The distribution is estimated using a decoder thatmaps the disentangled representation space back to the observationspace (R R). Specifically, the encoder encodes the input as anestimated mean R and a standard deviation vector R,from which the latent representation is sampled from the Gaussianmultivariate distribution N (, diag(2)).",
  "Aggregation/Decomposition Module": "While the DRL module learns disentangled generative factors inthe latent space, the factors may be too fine-grained to be alignedwith human understanding. However, these disentangled genera-tive factors can be considered as an independent basis of human-understandable concepts as shown in . 4.3.1Aggregation of Generative Factors into Concepts. We proposethe Aggregation module which aggregates the disentangled underly-ing generative factors of data into human-understandable concepts.Specifically, given a latent representation = , eachconcept {}=1 can be considered as a combination of alldisentangled factors 1, ,. To increase the expressiveness ofour model in learning concepts from the generative factors whilekeeping each concept as a linear aggregation of different factorsfor interpretability, we propose to encode each generative factor ( {1, ,}) with an independent neural network , whichlearns the non-linear mapping from to for concept learning.Subsequently, our model learns the linear combinations of com-ponents in = to represent high-level concepts",
  "=1BCE(,),(4)": "where BCE is the Binary Cross Entropy Loss. In addition to thesupervised learning of human-annotated concepts, our model isdesigned to automatically capture the information of the remainingconcepts +1,+2 , from the input in an unsupervised man-ner as it is not possible to annotate a truly closed set of concepts.Specifically, the unsupervised learning of the unannotated conceptsis performed through the training of concept decomposition forthe reconstruction of generative factors, which are described inSections 4.3.3 and 4.4.",
  "= D(()) = [1(()), ,(())](5)": "where is the estimated generative factors from the given con-cepts . Here the decomposition module acts as the inverse of theaggregation module and maps concepts back to the disentangledrepresentations. Instead of using one decoder directly for the map-ping from to , we use independent decoders to project eachdimension in to the corresponding dimension in , which miti-gates the problem of concept leakage from concepts to unrelatedgenerative factors . 4.3.4Task Prediction using Learned Concepts. Task prediction en-tails the mapping from learned concepts in R to the prediction oftask labels in R, where is the number of categories in classifi-cation tasks. The model prediction can be formulated as",
  "L = 22(9)": "For a given set of disentangled generative factors z, there exists afamily of surjective functions - which map from z to concepts c.Consequently, there exists a family of functions inverse coupledwith , which maps from concepts to . As the function issubjective, computing a direct inverse is intractable and hencerequires enforced consistency through L. The function is NOTa standalone family of functions as they are inverse of a surjectivefunction .In addition, we also encourage sparsity on the transformed rep-resentations to identify the most important generative factorsthat are aggregated to compose the concepts and reduce the impactof non-relevant factors. The overall training objective can be givenby:",
  "EXPERIMENTAL SETUP5.1Dataset Descriptions": "D-Sprites : D-Sprites consists of procedurally generatedsamples from six independent generative factors. Each object inthe dataset is generated based on two categorical factors (shape,color) and four numerical factors (X position, Y position, orienta-tion, scale). The six factors are independent of each other. Thedataset consists of 737,280 images. We randomly split data intotrain-test sets in a 70/30 split. Shapes3D : Shapes3D consists of synthetically generatedsamples from six independent generative factors consisting ofcolor (hue) of floor, wall, object (float values) and scale, shape,and orientation in space (integer values). The dataset consists of480,000 images. We randomly split the data into train and testsets in a 70/30 split. CelebA : CelebA consists of about 200,000 178 218 sizedRGB images of center-aligned facial photographs of celebrities.The faces are annotated with 40 binary concepts like hair color,smile, attractiveness, etc. Some of the features in the set are simpleand observable like color (black, blonde) and style of hair (wavy,bangs). However, many concepts are abstract and subjective likeattractiveness, heavy makeup, etc. We only consider the objectiveconcepts for experiments. We center-crop the images to 148x148and subsequently resize them to 64x64. AWA2 : Animals with Attributes-2 consists of 37,322 imagesof a combined 50 animal classes with 85 binary concepts likenumber of legs, presence of tail, etc. We remove certain subjec-tive concepts such as eats fish. AWA2 is neither centered norcropped and consists of significant background noise, making itsignificantly harder to disentangle. We resize all images to 64x64and combine the train and test splits. We use 70% of the data fortraining and 30% for testing.",
  "Dataset Task Descriptions": "Synthetic datasets: As dSprites and Shapes3D datasets are pro-cedurally generated, they do not contain an inherent downstreamtask, hence we construct downstream tasks using combinations ofGFs Similar to and . For each task, we consider two GFsat random and a sample has the label 1 when all factors satisfy apre-defined criterion and. For categorical factors, we consider thepresence of exact values as Truth, while for continuous factors weuse a threshold. More details on task construction can be found inthe Appendix.Real-world datasets: For the CelebA dataset, the downstreamtask is cluster assignment . For AWA2, the downstream task isclassification.",
  "Model Implementation Details": "Disentangled representations learning (DRL) module. Weutilize two different Variational Autoencoder architectures for esti-mating disentangle representations in the DRL module. We utilizea standard VAE and a -VAE . Even though similar in for-mulation, a -VAE is parameterized by a tunable hyperparameter which controls the strength of disentanglement. For both VAEs, theencoder is a 5-layer CNN with BatchNorm and LeakyReLU as theactivation function. The decoder is modeled symmetrically to theencoder with five Transpose Convolutional Layers. The size of thelatent space is set as 64 for d-Sprites and Shapes3D and 512 forCelebA and AWA2 datasets.Aggregation/Decomposition module. The Aggregation mod-ule is composed of the set of transformation operations A and map-ping function between transformed representations and concepts . We model each of the neural networks for {0, 1, ..,} asa 3-layer network. For dSprites and Shapes3D the network consistsof layers sized and for CelebA and AWA2 the networkconsists of layers sized with ReLU as the activationfunction. The function is modeled as a single fully connectedlayer. Similarly, the Decomposition module is composed of the setof inverse transformation operations D and mapping function be-tween concepts and transformed representations . We model eachof the neural networks for {0, 1, ..,} as a 3-layer network.For dSprites and Shapes3D the network consists of layers sized and for CelebA and AWA2 the network consists of layerssized with ReLU as the activation function. The function is modeled as a single fully connected layer.Task prediction module. As proposed in and , weutilize a single fully connected layer mapping from the concepts topredictions (). The final output is passed through a softmax layerto compute probability scores for the label.",
  "CoLiDR - -VAE0.9290.9160.8280.5210.0690.09100.03750.36": ": Average task accuracy and concept errors across four datasets for concept-based models with disentanglement learning(GlanceNet, CoLiDR - VAE, CoLiDR - -VAE) and models without disentanglement learning (CBM). CoLiDR - VAE is aversion of our model with a vanilla VAE (parameterized by = 1) while CoLiDR - -VAE is another version with a -VAE(parameterized by various s) discussed in Appendix. Concept Errors are reported for dSprites and Shapes3D as RMSE while forCelebA and AWA2 as 0-1 error. Best results for models with disentanglement learning are marked in bold.",
  "Evaluation Setup": "Task and concept accuracy. For a Concept-Based Model to bedeemed effective, it is required to be at par on performance with non-inherently explainable models (or black box models). We measurethe task accuracy and compare it against a standard Deep NeuralNetwork formed using the same encoder as the VAE and a fullyconnected layer as the classification head.Next, we compare the concept-accuracy of CoLiDR against CBMsas proposed in . For datasets with binary concepts, accuracyis reported as 0-1 error (misclassification). For datasets with non-binary concepts, Root Mean Square Error (RMSE) is reported. Disentangled representation aggregation performance. Ef-fective aggregation of disentangled representations serves as themost important desiderata for CoLiDR. However, it is not straight-forward to understand the aggregation effect. Unlike superviseddisentanglement where each dimension of the disentangled repre-sentations is forced to correspond to a concept, CoLiDR aggregatesmultiple dimensions into a concept. Hence, instead of identifyingindividually significant dimensions as concepts, it is important toconsider a set of representative dimensions from the aggregationmodule.Due to VAEs inherent disentanglement procedure, we can safelyassume the mutual independence of dimensions among the learneddisentangled representations. Hence, the latent dimensions them-selves can be thought of as features of the aggregation module. Ineffect, the problem of selecting a representative set of dimensionsis identical to assigning importance scores to the most importantfeatures in a deep neural network. Multiple post-hoc interpretabil-ity methods are proposed . We utilize Integrated Gradients(IG) to assign importance scores to each dimension. For an input and a baseline (zero-vector), IG computes attribution scoresfor each feature using the following path integral:",
  "GradCAM visualizations: For each dimension in the top-kmost important dimensions, the GradCAM attribution visualiza-tion plots are plotted": "Latent space traversal: For concepts with one dominant di-mension in the representative set, we linearly interpolate thenormalized latent space between two terminal values to identifyvisual cues in each generated image Oracle Classification: Even though the aforementioned visu-alizations provide qualitative evaluation, a robust quantitativeevaluation is required to assuage the effects of confirmation bias.To achieve this, we train a Oracle Network to automatically clas-sify images to the associated concepts with high accuracy. Sub-sequently, we compare heatmaps generated using CBMs andCoLiDR with the actual ground-truth annotated bounding boxesand report the average Intersection over Union (IoU) scores whichmeasure the ratio of overlap to the combined are between twobounding boxes (Refer Appendix for mathematical formulation).We utilize a fine-tuned VGG-16 model as the backbone of theoracle for each dataset. For more details, refer to Appendix. Concept decomposition performance (intervention). An-other desirable desideratum of a Concept-based model is its ease ofdebugging. For a misclassified sample, fixing the concept annotationby a domain expert should be able to correct the model predictions.Recall that the decomposition module decomposes concepts backinto disentangled representations. We define an intervention to besuccessful for a wrongly classified sample, replacing the predictedconcept score with the ground truth concept annotations changesthe wrong prediction label to the correct ground truth label .",
  "Evaluation Metric Descriptions": "5.5.1Oracle Training. We utilize a fine-tuned VGG-16 which con-tains 5 blocks of convolutional layers followed by a max poolinglayer at the end of each block. The final output is passed througha 3-layer fully connected network to perform concept prediction.The model is trained similarly to the concept network where eachconcept is weighted by its relative occurrence in the dataset.",
  "RESULTS AND DISCUSSION6.1Task and Concept Accuracy": "In , we compare our model against Concept BottleneckModels (CBMs) , which are trained without disentanglementlearning, and GlanceNets , which involves the learning of dis-entangled factors, on the average task performance and concepterrors. We implement CBMs by replacing the VAE with a stan-dard Autoencoder and the task learning module and the Aggre-gation/Decomposition module are replaced by identity functions.For GlanceNets, we provide supervision on not only the learnedconcepts but also a part of disentangled latent space (represented asGV in ). Differently, we do not utilize the Open-Set RecognitionMechanism as we do not specifically study concept leakage.As can be seen in , CoLIDR variants perform the bestamong all concept-based models with disentanglement learning andeven outperforms CBM on 3 out of 4 datasets. The results show thatCBM performs better than GlanceNet and CoLiDR on the AWA2dataset (0.531). A possible reason for this observation stems fromthe fact that disentanglement performance on the AWA2 dataset isnot effectively captured by VAEs (Refer to Appendix) due to lesstraining data and significant noise in the dataset. Furthermore, theperformance of GlanceNet and CoLIDR is at par on dSprites andShapes3D implying that datasets that are easier to disentangleyield similar performances across methods.For the concept identification task, CoLiDR still achieves thebest performance with the lowest concept errors among all modelswith disentanglement learning. Compared with the model withoutdisentanglement learning, shows that our model performscomparable and sometimes better than CBM on dSprtes, Shapes3D,and CelebA datasets. For the AWA2 datasets, CBM outperformsboth CoLiDR and GlanceNet. We attribute this to the fact thatCBM can learn concepts very flexibly without the regularization ofdisentanglement. However, this also leads to the problem of learningspurious correlation instead of real semantics of concepts, as shownin Appendix. Note that CoLiDR with no consistency regularization(L) performs the worst on concept learning - implying strongdisentanglement performance is vital to concept learning.",
  "Disentangled Representation Visualizations": "Figures 4 and 5 respectively demonstrate the most important di-mensions constituting a concept. In the , we demonstratethe concept brown_hair and its associated dimensions with scorescalculated using IG. For example, in GradCAM heatmapvisualizations on the two highest computed constituent dimen-sions for a correctly identified concept straight_hair (top) andwavy_hair are shown. We see that both dimensions correctly cor-respond to the distributions constituting the hair of the people in the image. Similarly, in , the GradCAM visualizationscorresponding to the concepts heart (left) and ellipse (right) areshown. In addition, we also provide a linear interpolation of theidentified dimensions. As can be seen, the dimensions are indeedresponsible for the shapes of hearts and ellipses.",
  "Comparision with Oracle Network": "We partition a subset of concepts that can be easily understood inthe CelebA dataset and dub them simple (Refer to Appendix for theexact splits). We report average IoU values for correctly classifiedconcepts on both simple and all concepts in . Column-2, 3and 4 list the average IoUs for simple concepts across CBMs, CoLiDRwith only the top-2 and top-5 dimensions respectively. Heatmapsproduced by CBMs show very low IoU values implying theyare not able to capture the spatiality of concepts well. On the otherhand, CoLIDR with -VAEs give much higher IoUs than CBMs -demonstrating that CoLiDR captures the spatial nature of conceptsextremely well. The more dimensions visualized (two vs five), thelower the IoU goes - implying that most of the spatial conceptinformation is encoded in only a few dimensions.",
  "Test-time Intervention": "shows the effect of intervening on a wrongly predictedconcept by its ground truth value. As the number of concepts varyfor each dataset, we plot the percentage of samples getting correctedafter intervention (y-axis) v/s percentage of concepts replaced bytheir ground truth values (x-axis). We see that intervention is moresuccessful on datasets with fewer concepts.",
  "FUTURE WORK": "CoLiDR is a highly generalizable framework that can be pluggedinto any disentanglement architecture. In this paper we demon-strate our results on Variational Autoencoders, however many morecomplex architectures can be utilized for learning disentangled rep-resentations. Generative Adversarial Networks are another classof stochastic non-linear latent representation learning frameworkwhich can be directly utilized as a DRL module. Another avenue ofexploration can go along the lines where concepts instead of beingindependent of each other, are causally related - similar to .",
  "CONCLUSION": "In this paper, we propose CoLiDR, a novel interpretable concept-based model that learns concepts using aggregated disentangledrepresentations. Empirical results demonstrate that CoLiDR bridgesthe gap between concept learning and disentangled representationlearning by formatting human-understandable concepts as aggre-gations of fundamental generative factors. The performance ofCoLiDR for task and concept prediction is on par with the concept-focused models (CBM/CEM) that lack the explainability of gener-ative factors. CoLiDR can flexibly learn complex concepts as anaggregation of disentangled generative factors which improves itsperformance on both task and concept learning. Using an aggre-gation step, concepts learned using CoLiDR can be constituentsof multiple generative factors, giving a much finer overview ofinterpretability.",
  "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residuallearning for image recognition. In Proceedings of the IEEE conference on computervision and pattern recognition. 770778": "Tom Heskes, Evi Sijben, Ioan Gabriel Bucur, and Tom Claassen. 2020. Causalshapley values: Exploiting causal knowledge to explain individual predictionsof complex models. Advances in neural information processing systems 33 (2020),47784789. Irina Higgins, Loc Matthey, Arka Pal, Christopher P. Burgess, Xavier Glorot,Matthew M. Botvinick, Shakir Mohamed, and Alexander Lerchner. 2017. beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework.In 5th International Conference on Learning Representations, ICLR 2017, Toulon,France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net. Jeya Vikranth Jeyakumar, Luke Dickens, Luis Garcia, Yu-Hsi Cheng,Diego Ramirez Echavarria, Joseph Noor, Alessandra Russo, Lance Kaplan, ErikBlasch, and Mani Srivastava. 2022. Automatic Concept Extraction for ConceptBottleneck-based Video Classification. arXiv preprint arXiv:2206.10129 (2022). Jeya Vikranth Jeyakumar, Joseph Noor, Yu-Hsi Cheng, Luis Garcia, and ManiSrivastava. 2020. How can i explain this to you? an empirical study of deepneural network explanation methods. Advances in Neural Information ProcessingSystems (2020).",
  "Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. 2015. Deep learningface attributes in the wild. In Proceedings of the IEEE international conference oncomputer vision. 37303738": "Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly,Bernhard Schlkopf, and Olivier Bachem. 2019. Challenging common assump-tions in the unsupervised learning of disentangled representations. In interna-tional conference on machine learning. PMLR, 41144124. Francesco Locatello, Michael Tschannen, Stefan Bauer, Gunnar Rtsch, BernhardSchlkopf, and Olivier Bachem. 2020. Disentangling Factors of Variations UsingFew Labels. In 8th International Conference on Learning Representations, ICLR 2020,Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.",
  "LoicMatthey,IrinaHiggins,DemisHassabis,andAlexanderLer-chner.2017.dSprites:DisentanglementtestingSpritesdataset": "Matthew OShaughnessy, Gregory Canal, Marissa Connor, Mark Davenport, andChristopher Rozell. 2020. Generative causal explanations of black-box classifiers.Advances in Neural Information Processing Systems (2020). Tejaswini Pedapati, Avinash Balakrishnan, Karthikeyan Shanmugam, and AmitDhurandhar. 2020. Learning Global Transparent Models Consistent with LocalContrastive Explanations. Advances in Neural Information Processing Systems 33(2020). Federico Pittino, Vesna Dimitrievska, and Rudolf Heer. 2021. Hierarchical ConceptBottleneck Models for Explainable Images Segmentation, Objects Fine Classifica-tion and Tracking. Objects Fine Classification and Tracking (2021).",
  "Yoshihide Sawada and Keigo Nakamura. 2022. Concept bottleneck model withadditional unsupervised concepts. IEEE Access 10 (2022), 4175841765": "Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedan-tam, Devi Parikh, and Dhruv Batra. 2017. Grad-cam: Visual explanations fromdeep networks via gradient-based localization. In Proceedings of the IEEE interna-tional conference on computer vision. 618626. Xinwei Shen, Furui Liu, Hanze Dong, Qing Lian, Zhitang Chen, and Tong Zhang.2022. Weakly supervised disentangled generative causal representation learning.The Journal of Machine Learning Research 23, 1 (2022), 1099411048. Rui Shu, Yining Chen, Abhishek Kumar, Stefano Ermon, and Ben Poole. 2020.Weakly Supervised Disentanglement with Guarantees. In 8th International Con-ference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30,2020. OpenReview.net. Sanchit Sinha, Mengdi Huai, Jianhui Sun, and Aidong Zhang. 2023. Understandingand enhancing robustness of concept-based models. In Proceedings of the AAAIConference on Artificial Intelligence, Vol. 37. 1512715135.",
  "Armeen Taeb, Nicol Ruggeri, Carina Schnuck, and Fanny Yang. 2022. Provableconcept learning for interpretable predictions using variational inference. arXivpreprint arXiv:2204.00492 160 (2022)": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin. 2017. Attention is allyou need. Advances in neural information processing systems 30 (2017). Weibin Wu, Yuxin Su, Xixian Chen, Shenglin Zhao, Irwin King, Michael R Lyu,and Yu-Wing Tai. 2020. Towards Global Explanations of Convolutional NeuralNetworks With Concept Attribution. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition. 86528661. Chih-Kuan Yeh, Been Kim, Sercan Arik, Chun-Liang Li, Tomas Pfister, andPradeep Ravikumar. 2020. On completeness-aware concept-based explanationsin deep neural networks. Advances in Neural Information Processing Systems 33(2020), 2055420565.",
  "AAPPENDIXA.1Downstream Task Descriptions": "A.1.1D-Sprites/Shapes3D. We create 6 distinct binary tasks forthe D-Sprites dataset. Each task is created by splitting the trainingset based on two factors. As the training set is randomly sampled,the number of training points in each task is different. However,the training set for each task is ensured to have balanced labels. A.1.2CelebA. For CelebA dataset, we utilize the labels formedby clustering all the images into 10 clusters using the clusteringalgorithm. The choice of the number of clusters are made usingTSNE visualization of the actual clusters. It is worth noting that weutilize ten clusters instead of four in . This important changein task is made due to the fact that we utilize a different subset ofconcepts (simple concepts) to model CoLiDR. lists the setof concepts for which annotations are provided. We compute allthe experimental results on the simple set of concepts (left). Thesimple concepts are chosen as they are objective in nature and canbe easily discerned by visual inspection of heatmaps. A.1.3AWA2. For AWA2 dataset, we keep the task the same as theidentity classification of 50 different identities. The original datasetis annotated with 85 different binary concepts. The list of conceptscan be found 1. For our experiments, we utilize a smaller subsetof 30 concepts. The subset is specifically chosen to only captureconcepts which are objective and human-understandable.",
  "A.2Implementation Details": "A.2.1Model Architecture. Disentangled Representations Learn-ing (DRL) Module: details the major components of botharchitectures respectively. The standard VAE suffers from the well-documented problem of over-regularization of the disentangledrepresentations z, which hinders the quality of visualized recon-structions x due to the information constraint imposed by the in-dependent Gaussian priors. Hence, we evaluate CoLiDR on botha standard VAE with = 1 and a -VAE, where the values of are tuned separately on each dataset. Specifically, we employ the following values: dSprites: = 0.025, Shapes3D: = 0.025, CelebA: = 2.5 5 and AWA2: = 1 5.Aggregation/Decomposition Module: The Aggregation moduleA maps between the latent representations z and concept set Cwhile the Decomposition module D maps the concept set C tothe latent representations z using a set of non linear transforms{} and {} where i={0,..k}. We model each transformation as afully connected network mapping from a single dimension in thelatent space (z) to the transformed latent space (z) and vice-versa.The number of the intermediate layers is 2 for dSprites/Shapes3Deach of size 512, and the number of the intermediate layers is 4 forCelebA/AWA2 each of size 512. The map between the transformedlatent space and and vice versa - is modeled as a single feed-forward layer.Task Prediction Module: The task prediction module utilizes alinear fully connected layer mapping from the annotated concepts(Row-5 in ) to the number of task labels (Row-7 in ).",
  "final predictions are performed after converting the modeloutputs into probabilities using the softmax operator": "A.2.2Training Hyperparameter Settings. Sequential Training:To avoid posterior collapse, we first train the VAE without theinfluence of any other loss term, i.e., 1, 2, 3, 4 = 0, only utilizingthe ELBO term (Equation 10). Subsequently, we only learn theconcept mappings, i.e., 2 = 0 as proposed in Sequential trainingof . Finally, we train the entire model architecture end-to-endusing carefully tuned hyperparameters 1, 2, 3, 4 for each dataset.The detailed values are given below: dSprites: 1 = 0.5, 2 = 0.2, 3 = 1, 4 = 0.1 Shapes3D: 1 = 0.5, 2 = 0.2, 3 = 1, 4 = 0.1 CelebA: 1 = 10, 2 = 0.08, 3 = 0.1, 4 = 0.01 AWA2: 1 = 5, 2 = 0.05, 3 = 0.1, 4 = 0.01Learning Rate and Scheduling: Training VAEs are notoriouslyhighly sensitive to the LR and optimizer. We utilize the Adam Opti-mizer with an initial learning rate of 5e-3 for dSprites/Shapes3D,1e-3 for CelebA and 5e-4 for AWA2. The batch sizes are 64 eachfor dSprites and Shapes3D, while it is set at 32 for CelebA and 16for AWA2. We train the entire model architecture for 100 epochswith the first 50 epochs training the VAE, the subsequent 25 epochstraining the Aggregation/Decomposition module while the last 25epochs train the entire model, end-to-end.",
  "A.3Additional Visual Results": "demonstrates the highest attributed latent dimensions asso-ciated with correctly classified concepts. In particular. we comparethe heatmaps generated using CoLiDR with the heatmaps generatedusing CBMs. As can be seen in , the heatmaps correspond-ing to the most important dimensions of blond hair, narrow eyes,and rosy cheeks correspond to their respective correct positions inthe image using CoLiDR as compared to CBMs. Similarly, ashows the heatmaps corresponding to correctly classified conceptsfrom the Shapes3D dataset. In addition, in b, we providelinear interpolation of the most contributing latent dimensionscorresponding to a concept."
}