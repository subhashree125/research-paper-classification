{
  "Abstract": "Higher-order interactions (HOIs) are ubiquitous in real-world com-plex systems and applications. Investigation of deep learning forHOIs, thus, has become a valuable agenda for the data miningand machine learning communities. As networks of HOIs are ex-pressed mathematically as hypergraphs, hypergraph neural net-works (HNNs) have emerged as a powerful tool for representationlearning on hypergraphs. Given the emerging trend, we present thefirst survey dedicated to HNNs, with an in-depth and step-by-stepguide. Broadly, the present survey overviews HNN architectures,training strategies, and applications. First, we break existing HNNsdown into four design components: (i) input features, (ii) input struc-tures, (iii) message-passing schemes, and (iv) training strategies.Second, we examine how HNNs address and learn HOIs with eachof their components. Third, we overview the recent applicationsof HNNs in recommendation, bioinformatics and medical science,time series analysis, and computer vision. Lastly, we conclude witha discussion on limitations and future directions.",
  "Hypergraph Neural Network, Self-supervised Learning": "ACM Reference Format:Sunwoo Kim, Soo Yong Lee, Yue Gao, Alessia Antelmi, Mirko Polato, and Ki-jung Shin. 2024. A Survey on Hypergraph Neural Networks: An In-Depthand Step-by-Step Guide. In Proceedings of the 30th ACM SIGKDD Con-ference on Knowledge Discovery and Data Mining (KDD 24), August 2529, 2024, Barcelona, Spain. ACM, New York, NY, USA, 11 pages.",
  "Equal contributionCorresponding author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08",
  "Introduction": "Higher-order interactions (HOIs) are pervasive in real-world com-plex systems and applications. These relations describe multi-wayor group-wise interactions, occurring from physical systems ,microbial communities , brain functions , and social net-works , to name a few. HOIs reveal structural patterns unob-served in their pairwise counterparts and inform network dynamics.For example, they have been shown to affect or correlate with syn-chronization in physical systems , bacteria invasion inhibitionin microbial communities , cortical dynamics in brains ,and contagion in social networks .Hypergraphs mathematically express higher-order networksor networks of HOIs , where nodes and hyperedges respec-tively represent entities and their HOIs. In contrast to an edgeconnecting only two nodes in pairwise graphs, a hyperedge canconnect any number of nodes, offering hypergraphs advantagesin their descriptive power. For instance, as shown in , theco-authorship relations among researchers can be represented asa hypergraph. With their expressiveness and flexibility, hyper-graphs have been routinely used to model higher-order networksin various domains to uncover their structural pat-terns .As hypergraphs are extensively utilized, the demand grew tomake predictions on them, estimating node properties or identi-fying missing hyperedges. Hypergraph neural networks (HNNs)have shown strong promise in solving such problems. For example,they have shown state-of-the-art performances in industrial and",
  ": Taxonomy on modeling higher-order interactions. The term neg. sam. denotes negative sampling": "scientific applications, including missing metabolic reaction pre-diction , brain classification , traffic forecast , productrecommendation , and more .The research on HNNs has been exponentially growing. Simul-taneously, further research on deep learning for higher-order net-works is an imminent agenda for the data mining and machinelearning communities . Therefore, we provide a timely surveyon HNNs that addresses the following questions:",
  "Preliminaries": "In this section, we present definitions of basic concepts related tohypergraphs and HNNs. See for frequently-used symbols.A hypergraph G = (V, E) is defined as a set of nodes V ={1, 2, , |V|} and a set of hyperedges E = {1,2, ,| E|}.Each hyperedge is a non-empty subset of nodes (i.e., V). Alternatively, E can be represented with an incidence matrixH {0, 1}|V||E|, where H, = 1 if and 0 otherwise. Theincident hyperedges of a node , denoted as NE (), is the set ofhyperedges that contain (i.e., NE () = { E : }).We assume that each node and hyperedge are equipped with(input) node features R and hyperedge features R,respectively.1 Similarly, we denote node and hyperedge featurematrices as X R|V| and Y R| E|, respectively, where the-th row X corresponds to and -th row Y corresponds to .In Sec. 3.1, we detail approaches to obtain the features.Hypergraph neural networks (HNNs) are neural functions thattransform given nodes, hyperedges, and their features into vectorrepresentations (i.e., embeddings). Typically, their input is repre-sented as either (X, E) or (X, Y, E). HNNs first prepare the inputhypergraph structure E (Sec. 3.2). Then, HNNs perform messagepassing between nodes (and/or hyperedges) to update their embed-dings (Sec. 3.3). A node (or hyperedge) message roughly refers to its",
  "NotationDefinition": "G = (V, E)Hypergraph with nodes set V and hyperedges set EH {0, 1}|V||E|Incidence matrixX R|V|, Y R|E|Node features (X) and hyperedge features (Y)P() R|V|, Q() R|E|-th layer embeddings of nodes (P()) and hyperedges (Q())NE ( )Incident hyperedges of node I-by- identity matrixI[cond]Indicator function that returns 1 if cond is True, 0 otherwise ()Non-linear activation functionM,: -th row of matrix MM, (, )-entry of matrix M vector representation for other nodes (or hyperedges) to aggregate.The message passing operation is repeated times, where each iter-ation corresponds to one HNN layer. Here, we denote the -th layerembedding matrix of nodes and hyperedges as P() R|V| andQ() R| E|, respectively. Unless otherwise stated, we assumeP(0) = X and Q(0) = Y. We use I, , , and () to denote the-by- identity matrix, vector concatenation, elementwise product,and a non-linear activation function, respectively.",
  "Step 1: Design features to reflect HOIs": "First, HNNs require a careful choice of input node features X R|V| and/or hyperedge features Y R| E|. Their quality canbe vital for a successful application of HNNs . Thus, studieshave crafted input features to enhance HNNs in encoding HOIs.Three primary approaches include the use of (i) external featuresor labels, (ii) structural features, and (iii) identity features. 3.1.1External features or labels. External features or labelsbroadly refer to information that is not directly obtained fromthe hypergraph structure. Using external features allows HNNs tocapture information that may not be transparent in hypergraphstructure alone. When available, using external node features Xand hyperedge features Y as HNN input is the standard practice.Some examples of node features from widely-used benchmarkdatasets are bag-of-words vectors , TF-IDFs , visual ob-ject embeddings , or noised label vectors . Interestingly, asin label propagation, HyperND constructs input node fea-tures X by concatenating external node features with label vectors.Specifically, one-hot-encoded label vectors and zero vectors areconcatenated for nodes with known and unknown labels, respec-tively. Since external hyperedge features are typically missing in the",
  "benchmark datasets, in practice, input features of can be obtainedby averaging its constituent nodes (i.e., = /| |)": "3.1.2Structural features. On top of external features, studieshave also utilized structural features as HNN input features. Struc-tural features are typically derived from the input hypergraph struc-ture E to capture structural proximity or similarity between nodes.While leveraging them in addition to the structure E may seemredundant, several studies have highlighted their theoretical andempirical advantages, particularly for hyperedge prediction and for transformer-based HNNs .Broadly speaking, studies have leveraged either local or globalstructural features. To capture local structures around each node,some HNNs use the incidence matrix H as part of the input fea-tures . Notably, HyperGT parameterizes its struc-tural node features X R|V| and hyperedge features Y R|E| as follows: X = H and Y = H , where R| E| and R|V| are learnable weight matrices. Some HNNs leveragestructural patterns within each hyperedge. Intuitively, the impor-tance or role of each node may vary depending on hyperedges.For instance, WHATsNet uses within-order positional encod-ing, where node centrality order within each hyperedge servesas edge-dependent node features (detailed in Sec. 3.3.2). Also, astudy utilizes the occurrence of each hypergraphlet (i.e., apredefined pattern of local structures describing the overlaps ofhyperedges within a few hops) around each node or hyperedge asinput features. Global features based on roles and proximity in theentire hypergraph context have also been adopted. For example,Hyper-SAGNN uses a Hyper2Vec variant to incorporatestructural features preserving node proximity. VilLain lever-ages potential node label distributions inferred from the hypergraphstructure. HyperFeat aims to capture the structural identityof nodes through random walks. THTN integrates learnablenode centrality, uniqueness, and positional encodings. 3.1.3Identity features. Some HNNs use identity features, espe-cially for recommendation applications. Generally, identity featuresrefer to features uniquely assigned to each node (and hyperedge),enabling HNNs to learn distinct embeddings for each node (andhyperedge) . Prior studies have typically used randomlygenerated features or separately learnable ones . 3.1.4Comparison with GNNs. Graph neural networks (GNNs)also require node and/or edge features for representation learningon pairwise graphs , while typical structural featuresfor GNNs do not focus on HOIs.",
  ": An example hypergraph (a), its clique-expandedgraph (b), and its star-expanded graph (c)": "application of methods developed for graphs, such as spectral fil-ters , to hypergraphs. However, it may result in informationloss, and the original hypergraph structure may not be preciselyrecovered after transformation. Reductive transformation includestwo approaches: clique and adaptive expansion. Each expansion isrepresented as : (E, X, Y) A, where A R|V||V|. We elabo-rate on the definition of each entry of A for both expansions.Clique expansion.Clique expansion.Clique expansion.Clique expansion.Clique expansion.Clique expansion.Clique expansion.Clique expansion.Clique expansion.Clique expansion.Clique expansion.Clique expansion.Clique expansion.Clique expansion.Clique expansion.Clique expansion.Clique expansion. Clique expansion converts each hyperedge E into a clique (i.e., complete subgraph) formed by the set of nodes (see (b)). Consider two distinct hypergraphs: (1 ={1, 2, 3}) and (1 = {1, 2, 3}, 2 = {1, 3}, and 3 = {2, 3}).Despite their changes, both result in identical clique-expandedgraph (1 = {1, 2}, 2 = {1, 3}, and 3 = {2, 3}) if edgesare unweighted. This example illustrates that, in clique expan-sion, assigning proper edge weights is crucial for capturing HOIs.To weigh the edges, studies have utilized (i) the nodepair co-occurrence, such that pairs appearing together more fre-quently in hyperedges are assigned larger weights, or (ii) hyperedgesizes, such that that node pairs in larger hyperedges are assigned",
  "smaller weights. An example is = E (,, )": "| |, where(, ,) = I[({, } ) ( )], and I[cond] is an indica-tor function that returns 1 if cond is True and 0 otherwise.Adaptive expansion.Adaptive expansion.Adaptive expansion.Adaptive expansion.Adaptive expansion.Adaptive expansion.Adaptive expansion.Adaptive expansion.Adaptive expansion.Adaptive expansion.Adaptive expansion.Adaptive expansion.Adaptive expansion.Adaptive expansion.Adaptive expansion.Adaptive expansion.Adaptive expansion. Within each transformed clique, some edgesmay be redundant or even unhelpful. Adaptive expansion selec-tively adds and/or weighs edges within each clique, often tailoredto a given downstream task . For example, AdE uses a feature-distance-based edge weighting strategy. It obtainsprojected node features X R|V| by X = X W, where allrow vectors of W are sigmoid(MLP( V /|V|)). Then, AdEselects two distant nodes , and , within each hyperedge , i.e.,{,, , } = arg max{, }(2 ) | =1(X, X,)|. After that, itconnects the all nodes in with , and ,, essentially addingE = {{,, } : \\ {, }} {{,, } : \\ {, }}.AdE assigns weights to each edge in E as follows:",
  "KDD 24, August 2529, 2024, Barcelona, SpainSunwoo Kim, Soo Yong Lee, Yue Gao, Alessia Antelmi, Mirko Polato, and Kijung Shin": "Star expansion.Star expansion.Star expansion.Star expansion.Star expansion.Star expansion.Star expansion.Star expansion.Star expansion.Star expansion.Star expansion.Star expansion.Star expansion.Star expansion.Star expansion.Star expansion.Star expansion. A star-expanded graph of a hypergraph G = (V, E)has two new groups of nodes: the node group, which is the same asthe node set V of G, and the hyperedge group, consisting of nodescorresponding to the hyperedges E (refer to (c)). Star expan-sion captures HOIs by connecting each node (i.e., a node from thenode group) with the hyperedges (i.e., nodes from the hyperedgegroup) it belongs to, resulting a bipartite graph between the twogroups. Star expansion is expressed as : (E, X, Y) A, whereeach entry of A R(|V|+| E|)(|V|+| E|) is defined as",
  "(2)": "Here, we assume WLOG that the corresponding index of V inA is , and the corresponding index of E in A is |V| + .Line expansion.Line expansion.Line expansion.Line expansion.Line expansion.Line expansion.Line expansion.Line expansion.Line expansion.Line expansion.Line expansion.Line expansion.Line expansion.Line expansion.Line expansion.Line expansion.Line expansion. In a line-expanded graph of a hypergraphof G = (V, E), each pair of a node and a hyperedge containing it isrepresented as a distinct node. That is, its node set is {(,) : , E}. Edges are established between these nodes to connecteach pair of distinct nodes (,) and (,), where = or = .Tensor expression.Tensor expression.Tensor expression.Tensor expression.Tensor expression.Tensor expression.Tensor expression.Tensor expression.Tensor expression.Tensor expression.Tensor expression.Tensor expression.Tensor expression.Tensor expression.Tensor expression.Tensor expression.Tensor expression. Several recent HNNs represent hypergraphsas tensors . For example, T-HyperGNNs expressesa uniform (i.e., | | = , E) hypergraph G = (V, E)with a order tensor A R|V| . That is, if = 3, A,, = 1 if{, , } E, and A,, = 0 otherwise.",
  "Step 3: Pass messages to reflect HOIs": "With input features (Sec. 3.1) and structure (Sec. 3.2), HNNs learnnode (and hyperedge) embeddings. They use neural message passingfunctions for each node (and hyperedge) to aggregate messages, i.e.,information, from other nodes (and hyperedges). Three questionsarise: (i) whose messages should be aggregated? (ii) what messagesshould be aggregated? (iii) how should they be aggregated? 3.3.1Whose messages to aggregate (target selection). Formessage passing, we should decide whose message to aggregate,typically based on the structural expression of the input hypergraph(Sec. 3.2). We provide three representative examples: one clique-expansion-based approach and two star-expansion-based ones.2On clique-expanded graphs (V V)On clique-expanded graphs (V V)On clique-expanded graphs (V V)On clique-expanded graphs (V V)On clique-expanded graphs (V V)On clique-expanded graphs (V V)On clique-expanded graphs (V V)On clique-expanded graphs (V V)On clique-expanded graphs (V V)On clique-expanded graphs (V V)On clique-expanded graphs (V V)On clique-expanded graphs (V V)On clique-expanded graphs (V V)On clique-expanded graphs (V V)On clique-expanded graphs (V V)On clique-expanded graphs (V V)On clique-expanded graphs (V V). Similar to typical GNNs,clique-expansion-based HNNs perform message passing betweenneighboring nodes . They also oftenincorporate techniques that are effective in applying GNNs. Thisis expected since clique expansion transforms a hypergraph into ahomogeneous, pairwise graph. A notable instance is SHNN ,which constructs a propagation matrix W from A (Sec. 3.2.1) using are-normalization trick as W = D 1",
  "Regarding target selection, adaptive-expansion- , line-expansion- andtensor-representation-based are similar to clique-expanded ones (V V)": "where , are hyperparameters, () R is a learn-able weight matrix, and P(0) = MLP(X).On star-expanded graphs (V E and E V).On star-expanded graphs (V E and E V).On star-expanded graphs (V E and E V).On star-expanded graphs (V E and E V).On star-expanded graphs (V E and E V).On star-expanded graphs (V E and E V).On star-expanded graphs (V E and E V).On star-expanded graphs (V E and E V).On star-expanded graphs (V E and E V).On star-expanded graphs (V E and E V).On star-expanded graphs (V E and E V).On star-expanded graphs (V E and E V).On star-expanded graphs (V E and E V).On star-expanded graphs (V E and E V).On star-expanded graphs (V E and E V).On star-expanded graphs (V E and E V).On star-expanded graphs (V E and E V). In HNNs basedon star expansion, message passing occurs from the node group tothe hyperedge group (V E) and vice versa (E V) , either sequentially or simultaneously.First, we illustrate sequential message passing using ED-HNN. Its message passing at each -th layer for each node Vis formalized as follows:()=",
  "()= MLP3(1)() |NE ()|,(6)": "where denotes the concatenation of vector and scalar .MLP1, MLP2, and MLP3 are MLPs shared across all layers. Note that,in Eq. (4), hyperedge embeddings are updated by aggregating theembeddings of their constituent nodes. Subsequently, in Eq. (5) andEq. (6), node embeddings are updated by aggregating transformedembeddings of incident hyperedges. Here, the message passing ineach direction (Eq. (4) and Eq. (5)) occurs sequentially.Second, we present an example of simultaneous message passingwith HDS . Its message passing at each -th layer for node V and hyperedge E is formalized as follows:",
  "where (), () are hyperparameters, (), () R": "are learnable weight matrices, and (), () R are learnablebiases. After projecting node and hyperedge embeddings (Eq. (7)and Eq. (8)), each node embedding is updated by aggregating theprojected embeddings of its incident hyperedge (Eq. (9)), and eachhyperedge embedding is updated by aggregating the projectedembeddings of its constituent nodes (Eq. (10)). The message passingin each direction (Eq. (9) and Eq. (10)) occurs simultaneously. 3.3.2What messages to aggregate (message representation).After choosing message targets, the next step is determining mes-sage representations. HNNs typically use embeddings from the pre-vious layer as messages, which we term hyperedge-consistent mes-sages . In contrast, several recent studies propose adaptivemessage transformation based on its target, which we refer to ashyperedge-dependent messages .Hyperedge-consistent messages.Hyperedge-consistent messages.Hyperedge-consistent messages.Hyperedge-consistent messages.Hyperedge-consistent messages.Hyperedge-consistent messages.Hyperedge-consistent messages.Hyperedge-consistent messages.Hyperedge-consistent messages.Hyperedge-consistent messages.Hyperedge-consistent messages.Hyperedge-consistent messages.Hyperedge-consistent messages.Hyperedge-consistent messages.Hyperedge-consistent messages.Hyperedge-consistent messages.Hyperedge-consistent messages. In this widely-used approach, embeddings from the previous layer are directly treatedas vector messages. A notable example is UniGNN , a family ofHNNs that obtain node (and hyperedge) embeddings by aggregat-ing the embeddings from its incident hyperedges (or constituentnodes). UniGIN, a special case of UniGNN, is formalized as follows:",
  "NE ( )()(),": "where R and () R respectively can either be a learnableor fixed scalar and is a learnable weight matrix.Hyperedge-dependent messages.Hyperedge-dependent messages.Hyperedge-dependent messages.Hyperedge-dependent messages.Hyperedge-dependent messages.Hyperedge-dependent messages.Hyperedge-dependent messages.Hyperedge-dependent messages.Hyperedge-dependent messages.Hyperedge-dependent messages.Hyperedge-dependent messages.Hyperedge-dependent messages.Hyperedge-dependent messages.Hyperedge-dependent messages.Hyperedge-dependent messages.Hyperedge-dependent messages.Hyperedge-dependent messages. The role or importance of anode may vary across the hyperedges it belongs to . Severalstudies have devised hyperedge-dependent node mes-sages, enabling a node to send tailored messages to each hyperedgeit belongs to. For example, MultiSetMixer learns differentnode messages for each incident hyperedge to aggregate with thefollowing message passing function:",
  "where (), is the -th layer message of that is dependent on ,": "MLP()1and MLP()2are MLPs, and LN is layer normalization .Alternatively, some HNNs update messages based on hyperedge-dependent node features. WHATsNet introduces within-orderpositional encoding (wope) to adapt node messages for each target.Within each hyperedge, WHATsNet ranks constituent nodes ac-cording to their centralities for positional encoding. Formally, letF R|V| be a node centrality matrix, where and F, respec-tively denote the number of centrality measures (e.g., node degree)and the -th centrality measure score of node . The order of anelement in a set C is defined as Order(, C) = C I.Then, wope of a node at a hyperedge is defined as follows:",
  "where () R is a learnable projection matrix.3": "3.3.3How to aggregate messages (aggregation function).The last step is to decide how to aggregate the received messagesfor each node (and hyperedge).Fixed pooling.Fixed pooling.Fixed pooling.Fixed pooling.Fixed pooling.Fixed pooling.Fixed pooling.Fixed pooling.Fixed pooling.Fixed pooling.Fixed pooling.Fixed pooling.Fixed pooling.Fixed pooling.Fixed pooling.Fixed pooling.Fixed pooling. Many HNNs use fixed pooling functions, includ-ing summation and average . For example, ED-HNN uses summation to aggregate the embeddings of con-stituent nodes (or incident hyperedges), as described in Eq. (4)and Eq. (5). Clique-expansion-based HNNs without adaptive edgeweights also fall into this category . For example, SHNN uses a fixed propagation matrix W (see Eq. (3)) to aggregate nodeembeddings. Specifically, ()= V W,(1). Learnable pooling.Learnable pooling.Learnable pooling.Learnable pooling.Learnable pooling.Learnable pooling.Learnable pooling.Learnable pooling.Learnable pooling.Learnable pooling.Learnable pooling.Learnable pooling.Learnable pooling.Learnable pooling.Learnable pooling.Learnable pooling.Learnable pooling. Several recent HNNs enhance their poolingfunctions through attention mechanisms, allowing for weighting",
  "()= LN()+ MLP()3(); ()= LN + MH, S(,),": "where LN is layer normalization , () is row-wise softmax, = =1 is a learnable vector, and MLP,1, MLP,2, and MLP3 areMLPs. Note that Eq. (15) is a widely-used multi-head attention op-eration , where serves as queries, and S serves as keys andvalues. This process is target-agnostic since it considers only theglobal variables and the embeddings S of incident hyperedges,without considering the embedding of the target itself.In target-aware attention approaches, target information is incor-porated to compute attention weights. HyGNN is an example,with the following message passing function:",
  "( ()3 ()4 ) R are attention weight functions, where": "{ ()1 , ()2 , ()3 , ()4 } and {(,1), (,2)} are sets of learnablevectors and matrices, respectively. Note that the attention weightfunctions consider messages from both sources and targets. Target-aware attention has also been incorporated into clique-expansion-based HNNs, with HCHA as a notable example. 3.3.4Comparison with GNNs. GNNs also use neural messagepassing to aggregate information from other nodes .However, since GNNs typically perform message passing directlybetween nodes, they are not ideal for learning hyperedge (i.e., HOI)representations or hyperedge-dependent node representations.",
  "Similarly, many HNNs use rule-based NS for hyperedge classifica-tion . Others leverage domain knowledge todesign NS strategies": "4.1.2Learnable negative sampling. Notably, Hwang et al. show that training HNNs with the aforementioned NS strategiesmay cause overfitting to negative hyperedges of specific types. Thismay be attributable to the vast population of potential negativehyperedges, where the tiny samples may not adequately representthis population. To mitigate the problem, they employ adversarialtraining of a generator that samples negative hyperedges. 4.1.3Comparison with GNNs. Link prediction on pairwisegraphs is a counterpart of the HOI classification task .However, the space of possible negative edges significantly differsbetween them. In pairwise graphs, the size of the space is (|V|2). However, in hypergraphs, since a hyperedge can contain an ar-bitrary number of nodes, the size of the space is (2|V|), whichmakes finding representative unlikely HOIs, or negative hyper-edges, more challenging . Consequently, learning the distin-guishing patterns of HOIs by classifying the positive and negativehyperedges may be more challenging.",
  "Learning to contrast": "Contrastive learning (CL) aims to maximize agreement betweendata obtained from different views. Intuitively, views refer to dif-ferent versions of the same data, original or augmented. Trainingneural networks with CL has shown strong capacity in capturingthe input data characteristics . For HNNs, several CL techniqueshave been devised to learn HOIs . Here, we describethree steps of CL for HNNs: (i) obtaining views, (ii) encoding, and(iii) computing contrastive loss. 4.2.1View creation and encoding. First, we obtain views forcontrast. This can be achieved by augmenting the input hypergraph,using rule-based or learnable methods.Rule-based augmentation.Rule-based augmentation.Rule-based augmentation.Rule-based augmentation.Rule-based augmentation.Rule-based augmentation.Rule-based augmentation.Rule-based augmentation.Rule-based augmentation.Rule-based augmentation.Rule-based augmentation.Rule-based augmentation.Rule-based augmentation.Rule-based augmentation.Rule-based augmentation.Rule-based augmentation.Rule-based augmentation. This approach stochastically corruptsnode features and hyperedges. For nodes, an augmented featurematrix is obtained by either zeroing out certain entries (i.e., featurevalues) of X or adding Gaussian noise to them . Forhyperedges, augmented hyperedges are obtained by excluding somenodes from hyperedges or perturbing hyperedge membership(e.g., changing = {1, 2, 3} to = {1, 2, 4}) . Learnable augmentation.Learnable augmentation.Learnable augmentation.Learnable augmentation.Learnable augmentation.Learnable augmentation.Learnable augmentation.Learnable augmentation.Learnable augmentation.Learnable augmentation.Learnable augmentation.Learnable augmentation.Learnable augmentation.Learnable augmentation.Learnable augmentation.Learnable augmentation.Learnable augmentation. This approach utilizes a neural net-work to generate views . Specifically, HyperGCL gener-ates synthetic hyperedges E using HNN-based VAE .Once an augmentation strategy : (X, E) (X, E) is decided,a hypergraph-view pair (G(1), G(2)) can be obtained in two ways:",
  "Both G(1) and G(2) are augmented by applying to (X, E) .They likely differ due to the stochastic nature of": "Encoding.Encoding.Encoding.Encoding.Encoding.Encoding.Encoding.Encoding.Encoding.Encoding.Encoding.Encoding.Encoding.Encoding.Encoding.Encoding.Encoding. Then, the message passing on the two views (sharingthe same parameters) results in two pairs of node and hyperedgeembeddings denoted by (P, Q) and (P, Q) .4.2.2Contrastive loss. Then, we choose a contrastive loss. Be-low, we present node-, hyperedge-, and membership-level contrastivelosses. Here, ,, R are hyperparameters.Node level.Node level.Node level.Node level.Node level.Node level.Node level.Node level.Node level.Node level.Node level.Node level.Node level.Node level.Node level.Node level.Node level. A node-level contrastive loss is used to (i) maximizethe similarity between the same node from two different views and(ii) minimize the similarity for different nodes :",
  "Learning to generate": "HNNs can also be trained by learning to generate hyperedges. Ex-isting HNNs aim to generate either (i) ground-truth hyperedges tocapture their characteristics or (ii) latent hyperedges potentiallybeneficial for designated downstream tasks. 4.3.1Generating ground-truth HOIs. Training neural networksto generate input data has shown strong efficacy in various domainsand downstream tasks . In two recent studies, HNNs aretrained to generate ground-truth hyperedges to learn HOIs .HypeBoy by Kim et al. formulates hyperedge generation as ahyperedge filling task, where the objective is to identify the missingnode for a given subset of a hyperedge. Overall, HypeBoy involvesthree steps: (i) hypergraph augmentation, (ii) node and hyperedge-subset encoding, and (iii) loss-function computation.HypeBoy obtains the augmented node feature matrix X andaugmented input topology E, respectively by randomly maskingsome entries of X and by randomly dropping some hyperedgesfrom E. Hypeboy, then, feeds X and E into an HNN to obtainnode embedding matrix P. Subsequently, for each node andsubset = \\ {}, HypeBoy obtains (final) node embedding = MLP1() and subset embedding = MLP2( ).Lastly, the HNN is trained to make embeddings of the true node-subset pairs similar and of the false node-subset pairs dissimilar.Specifically, it minimizes the following loss:",
  "For example, HSL adopts a learnable augmenter to replaceunhelpful hyperedges with the generated ones. HSL prunes hyper-edges using a masking matrix M R|V|| E|. Each th column is = sigmoid((log (": "1 ) + (0 1))/), where 0 and 1, R,and , E respectively are random samples fromGumbel(0, 1), a hyperparameter, and a learnable scalar. An un-helpful is expected to have small to be pruned.After performing pruning by H = H M, HSL modifies H byadding generated latent hyperedges H. Specifically, H, = 1 if(H, = 0) (S, top(S, )), and 0 otherwise. top(S, ) denotesthe set of top-N entries in a learnable score matrix S R|V||E|.Each score in S is S, = 1 =1 sim( , ), where { }=1and sim respectively are learnable vectors and cosine similarity. Tosummarize, node and hyperedge similarities learned by an HNNserve to generate latent hyperedges H. Lastly, H + H is fed intoanother HNN for a target downstream task (e.g., node classification).All learnable components, including the HNN for augmentation,are trained end-to-end.Note that the HNNs learning to generate latent hyperedges gen-erally implement additional loss functions to encourage the latenthyperedges to be similar to the original ones . Further-more, some studies have explored generating latent HOIs wheninput hypergraph structures were not available . 4.3.3Comparison with GNNs. Various GNNs also target to gen-erate ground-truth pairwise interactions or latent pairwiseinteractions . In a pairwise graph, the inner product of two nodeembeddings is widely used to model the likelihood that an edgejoins these nodes . However, modeling the likelihood of ahyperedge, which can join any number of nodes, using an innerproduct is not straightforward.",
  "Recommendation": "5.1.1Hypergraph construction. For recommender system ap-plications, many studies utilized hypergraphs consisting of itemnodes (being recommended) and user hyperedges (receiving recom-mendations). For instance, all items that a user interacted with wereconnected by a hyperedge . When sessions were available,hyperedges connected item nodes by their context window . Some studies leveraged multiple hypergraphs. For in-stance, Zhang et al. incorporated user- and group-level hy-pergraphs. Ji et al. constructed a hypergraph with item nodesand a hypergraph with user nodes, where their hyperedges wereinferred from heuristic-based algorithms. In contrast, other studiesincorporated learnable hypergraph structure . 5.1.2Application tasks. Hypergraph-based modeling allows nat-ural applications of HNNs for recommendation, typically formu-lated as a hyperedge prediction problem. HNNs have been used forsequential , session-based , group ,conversational , and point-of-interest recommendation.",
  "Bioinformatics and medical science": "5.2.1Hypergraph construction. For bioinformatics applications,molecular-level structures have often been regarded as nodes. Stud-ies used hyperedges to connect the structures based on their jointreaction , presence within each drug , and associationwith each disease . Some studies used multiple node types. Astudy considered cell line nodes and drug nodes, with hyperedgeconnecting those with a synergy relationship . Drugs andtheir side effects were also considered as nodes, where a hyperedgeconnected those with drug-drug interaction . Drug nodes ortarget protein nodes were also connected by hyperedges based ontheir similarity in interactions or associations . Studies alsoused kNN or learnable hyperedges to build hypergraphs .Some other studies used hypergraphs to model MRI data. Many ofthem had a region-of-interest serving as a node, while a hyperedgeconnected the nodes using interaction strength estimation ,k-means , or random-walk-based sampling . On the otherhand, in some studies, study subjects were nodes, and hyperedgesconnected the neighbors found by kNN .Lastly, electronic health records (EHR) data were often modeledwith hypergraphs. Most often, nodes were either medical codes or clinical events . A hyperedge connectedthe codes or clinical events that were shared by each patient. 5.2.2Application tasks. For bioinformatics applications, HNNshave been applied to predict interactions or associations amongmolecular-level structures. Thus, many of the tasks could be natu-rally formulated as a hyperedge prediction task. Specifically, theapplication tasks include predictions of missing metabolic reac-tions , drug-drug interactions , drug-target interactions , drug-gene interactions , herbdiseaseassociations , and miRNA-disease associations .For MRI analysis, when a region-of-interest served as a node,HNNs have been applied to solve a hypergraph classification prob-lem. Alzheimers disease classification , brain connectome anal-ysis , autism prediction , and brain network dysfunc-tion prediction problems have been solved with HNNs.In analyzing EHR data, since a hyperedge consisted of medicalcodes or clinical events of a patient, HNNs have been applied forhyperedge prediction. Studies used HNNs to predict mortality ,readmission , diagnosis , medication , phenotype , clinical outcomes , and clinical pathways .",
  "Time series analysis": "5.3.1Hypergraph construction. A variety of nodes have beenused for time series forecast applications. Depending on the data,nodes were cities , gas regulators , rail segments ,train stations , stocks , or regions . Studies oftenleveraged similarity- or proximity-based hyperedges or learnable hyperedges . 5.3.2Application tasks. When applying HNNs, many time se-ries forecast problems can be formulated as node regression prob-lems. Specifically, the prior works used HNNs to forecast taxi de-mands , gas pressures , vehicle speeds , traffic , electricity consumptions , meteorolog-ical measures , stocks , and crimes .",
  "Computer vision": "5.4.1Hypergraph construction. Hypergraph-based modelinghas also been adopted for computer vision applications. Studiesused nodes to represent image patches , features , 3Dshapes , joints , and humans . To connect thenodes by a hyperedge, kNN , Fuzzy C-Means , and otherlearnable functions were adopted. 5.4.2Application tasks. For computer vision tasks, studies usedHNNs to solve problems including image classification , objectdetection , video-based person re-identification , imageimpainting , action recognition , pose estimation , 3D shape retrieval and recogntion , and multi-human meshrecovery . Due to the heterogeneity of the applied tasks, wefound no consistent hypergraph learning task formulation.",
  "Discussions": "In this work, we provide a survey on hypergraph neural networks(HNNs), with a focus on how they address higher-order interactions(HOIs). We aim for the present survey to be in-depth, coveringHNN encoders (Sec. 3), training objectives (Sec. 4), and applications(Sec. 5). Having reviewed the exponentially growing literature, weclose the survey with some future directions.HNN theory.HNN theory.HNN theory.HNN theory.HNN theory.HNN theory.HNN theory.HNN theory.HNN theory.HNN theory.HNN theory.HNN theory.HNN theory.HNN theory.HNN theory.HNN theory.HNN theory. Studies have theoretically investigated graph neuralnetworks (GNNs) on their graph isomorphism recognition , approximation abilities , and relation to homophily . However, given the complex nature of hypergraphs, directlyapplying these theoretical findings to hypergraphs can be non-trivial . Therefore, many theoretical properties of HNNs remainyet to be unveiled, and some areas have begun to be explored,including their generalization abilities and transferability .Advantages of HNNs.Advantages of HNNs.Advantages of HNNs.Advantages of HNNs.Advantages of HNNs.Advantages of HNNs.Advantages of HNNs.Advantages of HNNs.Advantages of HNNs.Advantages of HNNs.Advantages of HNNs.Advantages of HNNs.Advantages of HNNs.Advantages of HNNs.Advantages of HNNs.Advantages of HNNs.Advantages of HNNs. Instead of leveraging HNNs, one could useGNNs for a hypergraph by reducing its structure to a pairwise one.While studies have empirically shown that HNNs outperform thesealternatives , the factors that confer HNNs theadvantages remain unclear. While the advantages of using HOIs fora heuristic classifier have been investigated , studies dedicatedto HNNs may inspire improved HNNs and their training strategies.Complex hypergraphs.Complex hypergraphs.Complex hypergraphs.Complex hypergraphs.Complex hypergraphs.Complex hypergraphs.Complex hypergraphs.Complex hypergraphs.Complex hypergraphs.Complex hypergraphs.Complex hypergraphs.Complex hypergraphs.Complex hypergraphs.Complex hypergraphs.Complex hypergraphs.Complex hypergraphs.Complex hypergraphs. Networks of HOIs often exhibit temporal,directional, and heterogeneous properties, which are respectivelymodeled by temporal , directed , and heterogeneous hypergraphs. Although their structural patterns have beenstudied , developing HNNs to learn such complexHOIs is in the early stages . Thus, morebenchmark datasets and tasks for complex hypergraphs are neces-sary. The proper datasets and tasks will catalyze studies to developHNNs that better exploit the complex nature of HOIs.",
  "Acknowledgements": "This work was partly supported by Institute of Information & Communica-tions Technology Planning & Evaluation (IITP) grant funded by the Koreagovernment (MSIT) (No. 2022-0-00157, Robust, Fair, Extensible Data-CentricContinual Learning) (No. RS-2019-II190075, Artificial Intelligence GraduateSchool Program (KAIST)). This work has been partially supported by thespoke FutureHPC & BigData of the ICSC Centro Nazionale di Ricerca inHigh-Performance Computing, Big Data and Quantum Computing fundedby European Union NextGenerationEU.",
  "Song Bai, Feihu Zhang, and Philip HS Torr. 2021. Hypergraph convolution andhypergraph attention. Pattern Recognition 110 (2021), 107637": "Federico Battiston, Enrico Amico, Alain Barrat, Ginestra Bianconi, GuilhermeFerraz de Arruda, Benedetta Franceschiello, Iacopo Iacopini, Sonia Kfi, VitoLatora, Yamir Moreno, et al. 2021. The physics of higher-order interactions incomplex systems. Nature Physics 17, 10 (2021), 10931098. Federico Battiston, Giulia Cencetti, Iacopo Iacopini, Vito Latora, Maxime Lucas,Alice Patania, Jean-Gabriel Young, and Giovanni Petri. 2020. Networks beyondpairwise interactions: Structure and dynamics. Physics Reports 874 (2020), 192. Federico Battiston and Giovanni Petri. 2022. Higher-order systems. Springer. Tatyana Benko, Martin Buck, Ilya Amburg, Stephen J Young, and Sinan G Aksoy.2024. Hypermagnet: A magnetic laplacian based hypergraph neural network.arXiv preprint arXiv:2402.09676 (2024).",
  "Langzhang Liang, Sunwoo Kim, Kijung Shin, Zenglin Xu, Shirui Pan, and YuanQi. 2024. Sign is Not a Remedy: Multiset-to-Multiset Message Passing forLearning on Heterophilic Graphs. In ICML": "Sihao Liao, Liang Xie, Yuanchuang Du, Shengshuang Chen, Hongyang Wan,and Haijiao Xu. 2024. Stock trend prediction based on dynamic hypergraphspatio-temporal network. Applied Soft Computing 154 (2024), 111329. Luotao Liu, Feng Huang, Xuan Liu, Zhankun Xiong, Menglu Li, Congzhi Song,and Wen Zhang. 2023. Multi-view contrastive learning hypergraph neuralnetwork for drug-microbe-disease association prediction. In IJCAI.",
  "Heechan Moon, Hyunju Kim, Sunwoo Kim, and Kijung Shin. 2023. Four-sethypergraphlets for characterization of directed hypergraphs. arXiv preprintarXiv:2311.14289 (2023)": "Manon A Morin, Anneliese J Morrison, Michael J Harms, and Rachel J Dut-ton. 2022. Higher-order interactions shape microbial interactions as microbialcommunity complexity increases. Scientific Reports 12, 1 (2022), 22640. Duc Anh Nguyen, Canh Hao Nguyen, Peter Petschner, and Hiroshi Mamitsuka.2022. Sparse: A sparse hypergraph neural network for learning multiple types oflatent combinations to accurately predict drug-drug interactions. Bioinformatics38 (2022), i333i341. OpenAI. 2023. Gpt-4 technical report. (2023). Theodore Papamarkou, Tolga Birdal, Michael M Bronstein, Gunnar E Carlsson,Justin Curry, Yue Gao, Mustafa Hajij, Roland Kwitt, Pietro Lio, Paolo Di Lorenzo,et al. 2024. Position: Topological Deep Learning is the New Frontier for Rela-tional Learning. In ICML.",
  "Shilin Qu, Weiqing Wang, Yuan-Fang Li, Xin Zhou, and Fajie Yuan. 2023. Hy-pergraph node representation learning with one-stage message passing. arXivpreprint arXiv:2312.00336 (2023)": "Ding Ruan, Shuyi Ji, Chenggang Yan, Junjie Zhu, Xibin Zhao, Yuedong Yang,Yue Gao, Changqing Zou, and Qionghai Dai. 2021. Exploring complex andheterogeneous correlations on hypergraph for the prediction of drug-targetinteractions. Patterns 2, 12 (2021). Khaled Mohammed Saifuddin, Mehmet Emin Aktas, and Esra Akbas. 2023.Topology-guided hypergraph transformer network: Unveiling structural in-sights for improved representation. arXiv preprint arXiv:2310.09657 (2023).",
  "Peihao Wang, Shenghao Yang, Yunyu Liu, Zhangyang Wang, and Pan Li. 2023.Equivariant hypergraph diffusion neural operators. In ICLR": "Shun Wang, Yong Zhang, Xuanqi Lin, Yongli Hu, Qingming Huang, and BaocaiYin. 2024. Dynamic Hypergraph Structure Learning for Multivariate Time SeriesForecasting. IEEE Transactions on Big Data 01 (2024), 113. Wei Wang, Gaolin Yuan, Shitong Wan, Ziwei Zheng, Dong Liu, Hongjun Zhang,Juntao Li, Yun Zhou, and Xianfang Wang. 2024. A granularity-level informationfusion strategy on hypergraph transformer for predicting synergistic effects ofanticancer drugs. Briefings in Bioinformatics 25, 1 (2024), bbad522."
}