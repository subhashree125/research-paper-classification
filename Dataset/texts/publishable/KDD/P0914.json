{
  "ABSTRACT": "Persistent homology, a fundamental technique within TopologicalData Analysis (TDA), captures structural and shape characteristicsof graphs, yet encounters computational difficulties when appliedto dynamic directed graphs. This paper introduces the DynamicNeural Dowker Network (DNDN), a novel framework specificallydesigned to approximate the results of dynamic Dowker filtration,aiming to capture the high-order topological features of dynamicdirected graphs. Our approach creatively uses line graph transfor-mations to produce both source and sink line graphs, highlightingthe shared neighbor structures that Dowker complexes focus on.The DNDN incorporates a Source-Sink Line Graph Neural Network(SSLGNN) layer to effectively capture the neighborhood relation-ships among dynamic edges. Additionally, we introduce an innova-tive duality edge fusion mechanism, ensuring that the results forboth the sink and source line graphs adhere to the duality principleintrinsic to Dowker complexes. Our approach is validated throughcomprehensive experiments on real-world datasets, demonstratingDNDNs capability not only to effectively approximate dynamicDowker filtration results but also to perform exceptionally in dy-namic graph classification tasks.",
  "Corresponds Author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain. 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08...$15.00 ACM Reference Format:Hao Li, Hao Jiang, Fan Jiajun, Dongsheng Ye, and Liang Du. 2024. Dy-namic Neural Dowker Network: Approximating Persistent Homology inDynamic Directed Graphs. In Proceedings of the 30th ACM SIGKDD Con-ference on Knowledge Discovery and Data Mining (KDD 24), August 2529, 2024, Barcelona, Spain. ACM, New York, NY, USA, 11 pages.",
  "INTRODUCTION": "In recent years, an increasing number of researchers are focusing onintegrating high-order topological features with graph learning fordownstream tasks such as node classification, link prediction, andgraph classification . As a method under the framework ofTDA, persistent homology captures multi-scale features of graphsto describe their structural and shape characteristics .Nevertheless, when the subject of study is dynamic directed graphscommonly found in the real world, existing methods tailored forundirected static graphs are inadequate in capturing the topologicalinformation of graphs.Dynamic Dowker filtration is an effective persistent homologymethod for capturing the structural and shape characteristics ofdynamic directed graphs . As demonstrated in fig. 1a,Dowker complexes focus on capturing the shared neighbor struc-ture of graphs. This approach is particularly sensitive to the direc-tion and weight of edges, a feature illustrated with a simple examplein fig. 1b. Such sensitivity makes Dowker complexes adept at an-alyzing complex, continuously evolving dynamic directed graphsin the real world. Their ability to discern nuanced relationshipsbased on edge directionality and weights allows for a more detailedunderstanding of these graphs topological characteristics.Similar to other persistent homology methods, Dowker com-plexes face computational challenges, struggling to efficiently han-dle dynamic graphs. The work in explores the capability ofneural networks to learn persistent homology features in digitalimages and filtered cubical complexes. Inspired by graph neuralexecutors , we aim to develop a learning-based method toapproximate the complex results of Dowker computations. Graphneural executors are a type of neural network designed to approx-imate the execution of algorithms on graphs. The literature",
  "(b)": ": (a) A simple example of a Dowker source complex. The existence of a shared neighbor (in this case, 4) between 1 and2 creates a higher-order relationship, which the Dowker source complex captures. (b) Illustration of Dowker filtration sensitiveto edge weights and directions in graphs: represents a common type of subgraph in social media diffusion graphs. Swappingthe timestamps of two edges () or changing the direction of an edge () leads to different diffusion. Dowker source filtrationcan effectively distinguish these three types of graphs, whereas Vietoris-Rips (VP) filtration generates the same persistentbarcode for all three cases. has utilized graph neural networks to approximate extended per-sistence diagrams (EPDs), demonstrating that persistent homologyresults can be effectively predicted through embeddings of edges.However, applying graph neural executors to Dowker dynamicfiltration presents two key challenges. (1) How to approximatewith a neural network the structural features captured byDowker complexes. Traditional graph neural networks, relyingon information transfer between a node and its neighbors, are notadept at directly representing the computational results of Dowkercomplexes. (2) How to preserve the inherent duality charac-teristic of Dowker complexes. Dowker complexes, designed fordirected graphs, can be divided into source Dowker complexes andsink Dowker complexes . According to the principle of duality,under the same filtration, the Persistence Diagrams (PDs) corre-sponding to both types of complexes should be identical . It isimperative for the neural executor to ensure consistency betweenthese two forms of complexes. This involves designing a neuralnetwork architecture that can simultaneously capture and reconcilethe distinct yet complementary information presented in the sourceand sink Dowker complexes, reflecting their dual nature in the PDs.To address the aforementioned challenges, we focus on linegraphs . A line graph () transforms the edges of a graph into nodes of a new graph and connects edges that have a com-mon node in . As depicted in fig. 2, the line graph establishesa critical linkage between Dowker complexes and Graph NeuralNetworks (GNNs), facilitating the direct computation of edge em-beddings essential for predicting persistent homology. Further, thispaper expands on the definition of directed line graphs. Based onthe direction of the edges, a directed graph is transformed into",
  ", 43()))": "Edge-based MPNNSource Line Graph : An example demonstrating the relationship be-tween the source Dowker complex and the source line graphfor a directed graph. In the source line graph, the presence ofan edge between nodes representing 41 and 42 reflects theshared neighbor (4) between 1 and 2 in the original graph.This edge-based perspective is particularly allows the neu-ral network to focus on the interactions and relationshipsbetween edges. source and sink line graphs, corresponding to Dowker complexes.This approach aligns well with the duality requirement of Dowkercomplexes.Specifically, we develop the Dynamic Neural Dowker Network(DNDN), designed to approximate the computational results ofdynamic Dowker filtration and further validated its effectiveness",
  "Dynamic Neural Dowker Network: Approximating Persistent Homology in Dynamic Directed GraphsKDD 24, August 2529, 2024, Barcelona, Spain": "For the persistent diagram approximation experiments, the base-lines used include PDGNN , TOGL , and RePHINE . Forthe dynamic graph classification experiments, we utilized staticgraph embedding methods such as GCN , GAT , GraphSage, and GIN , as well as dynamic graph embedding methodslike DHGAT , DySAT , Roland , EGCNO, and EGCNH.",
  "We proposed an innovative line graph transformation method,which creates sink and source line graphs to directly repre-sent the shared neighbor structures of interest in Dowkercomplexes": "We developed the Dynamic Neural Dowker Network (DNDN),leveraging Source-Sink Line Graph Neural Network to an-alyze complex high-order structures in dynamic directedgraphs. Additionally, we designed a duality edge fusionmechanism to align with the unique properties of Dowkercomplexes. We conducted experiments on real-world datasets and foundthat DNDN performs well in approximating the computa-tional results of dynamic Dowker filtration and in graphclassification tasks. These results demonstrate the effective-ness of our approach in practical applications.",
  "Dynamic graph neural networks": "Recent studies on dynamic graphs mainly rely on temporal granu-larity and are divided into two types: discrete-time dynamic graphsusing time snapshots and continuous-time dynamic graphs withedge timestamps . Existing research on dynamic graph neuralnetworks focuses on extending traditional static GNNs to dynamicgraphs. An intuitive approach involves using time-aware encodersto capture the dynamic evolution of nodes between discrete snap-shots. For example, EvolveGCN dynamically updates the GNNmodel weights using LSTM and GRU. DySAT and DHGAT employ attention mechanisms to capture node embeddings throughstructural and temporal evolution. Recent dynamic GNNs based onmeta-learning aim to model temporal factors, with meta-learningadeptly adapting to new temporal data. The Roland framework generates new node embeddings through meta real-time updates.WinGNN utilizes a meta-learning strategy to model associa-tions in sliding windows of adjacent and consecutive snapshots,without the need for specific time-aware encoders. However, thesedynamic network studies, based on neighborhood aggregation GNNarchitectures, struggle to capture the global topological character-istics of dynamic graphs from a holistic perspective. Our approachemploys graph neural networks to approximate the results of dy-namic Dowker filtration, obtaining global topological features of",
  "Persistent Homology with Graph Learning": "As a significant component in topological machine learning, the in-tegration of persistent homology with graph learning has garneredwidespread attention. Given the superiority of persistent homologyin capturing the global structural information of graphs, a consid-erable amount of research has focused on predicting the labels ofentire graphs, that is, graph classification tasks .Some methods utilize the topological features of node neigh-borhood subgraphs as representations for node classification tasks.Similarly, constructs neighborhood graphs for each pair oftarget nodes and calculates their topological features, applying theoutcomes to link prediction tasks. TOGL developed a universallayer that integrates topological information into the hidden repre-sentations of nodes, capable of computing topological features atall scales, and demonstrated the efficacy of this approach in bothgraph classification and node classification tasks.Due to the high computational complexity of traditional persis-tent homology calculations, some methods have focused on usingneural networks to approximate the results of persistent homology,especially for large-scale graphs. Inspired by neural executors ,the work in reinterprets the computation of extended persis-tent homology as a prediction problem of paired edges, which canbe resolved using a union-find algorithm. Subsequently, a graphneural network is designed to learn this union-find algorithm, andthe effectiveness of the resulting Edge-based Persistence Diagrams(EPDs) in downstream tasks has been validated.These studies successfully integrate the topological features ofgraphs with graph learning methods. However, most of these ap-proaches focus predominantly on static, undirected graphs, therebyoverlooking the rich and complex information present in real-worlddirected dynamic graphs. Addressing this gap, our paper introducesthe Dynamic Neural Dowker Network, which focuses on machinelearning methods for computing persistent homology, an area lessexplored in existing work. We combine edge-based Dowker com-plexes with neural networks to execute persistence diagram (PD)computations and apply this approach to dynamic graph classifica-tion tasks. This innovation extends the application of topologicaldata analysis to more complex and dynamic network structures.",
  "Persistent Homology": "Persistent homology is a key technique in topological data analysis,which studies the shape and structure of data. The central idea is toanalyze datasets by constructing a series of topological spaces andobserving the persistence of features across changes in a parameter.When focusing on persistent homology in the context of graphs, theapproach typically involves examining how graph-based structuresevolve.",
  "KDD 24, August 2529, 2024, Barcelona, Spain.Hao Li et al": "Given a graph G = (V, E), where V is the set of nodes and Eis the set of edges, with E representing an edge betweennodes , V. The key step in computing persistent homologyinvolves creating a nested sequence of subgraphs G1 G2 . . . G = G based on a filter function , let C be the simplicial complexinduced by G. This nested sequence of simplicial complexes C1 C2 . . . C is referred to as a filtration of G.Using the additional information provided by the filtration, wecan obtain the persistent homology groups and their ranks .The , captures the topological features of the graph at variousdimensions , such as connected components for = 0 and loopsfor = 1, and represent the birth and death of these topologicalfeatures. The computed persistent homology can be encoded as amulti-point set in 2, known as the persistence diagram (PD), wherethe and coordinates represent the birth and death of topologicalfeatures, respectively. This approach to analyzing graphs capturestheir underlying topological characteristics, providing valuable in-sights into their structure and dynamics beyond what is observablethrough traditional graph metrics.",
  "Dowker Filtration": "The Dowker Complex is a type of simplicial complex specificallydesigned for directed graphs. Given a weighted directed graph G =(V, E, W), where V is the set of nodes, E is the set of directededges, and E represents a directed edge from source node to target node , with W () being the weight of the edge. The Dowker -sink complex is defined as a simplicial complexas follows:",
  ".(4)": "According to , by converting the temporal information car-ried by edges into weights, we can achieve a stable dynamicDowker filtration. Given the sensitivity of Dowker filtration toedge directionality and weights, dynamic Dowker filtration is capa-ble of deeply analyzing the high-order interaction relationships indynamic graphs as they evolve over time. This approach leverages the inherent temporal dynamics of edges, allowing for a more nu-anced understanding of how graphs change and how these changesimpact the topological features of interest.Dowker Duality. For a given directed graph G, any thresholdvalue R, and dimension 0, the persistent modules inducedby the Dowker sink filtration and the Dowker source filtrationare isomorphic. This statement reflects a fundamental propertyof Dowker complexes in topological data analysis. The duality ofDowker complexes necessitates that, in designing neural networks,we must consider both the distinctions and the eventual consistencybetween source Dowker complexes and sink Dowker complexes.",
  "E(G) ={{, }| and },(6)": "The threshold in Dowker filtration is defined on the edges,which contrasts with the node-centric approach common in tra-ditional graph neural networks. To address this, our paper ex-tends the concept of line graphs, introducing sink line graphs andsource line graphs, corresponding to Dowker complexes. Specifi-cally, given a weighted directed graph G = (V, E), its correspond-ing source line graph (G) = (V(G ), E(G )) and sink line",
  "METHOD": "Given a dynamic directed graph G = (V, E), our goal is to learna function : E R2 to approximate the 0-dimensional and1-dimensional PDs under dynamic Dowker filtration. To predictthe high-order topological features of dynamic graphs, we proposethe overall framework of the Dynamic Neural Dowker Network(DNDN) as shown in fig. 3. It specifically includes the followingmodules: (1) Line Graph Embedding Module. The DNDN obtainsembeddings for each edge in the dynamic graph through a source-sink line graph neural network combined with an edge fusion layer.(2) Joint Prediction Module. The DNDN divides the prediction taskinto two joint tasks: PD prediction and graph label prediction, anddesigns the loss function incorporating the Wasserstein distance.",
  "Line Graph Embedding Module": "To obtain edge embeddings for predicting persistent homology un-der dynamic Dowker filtration, this paper utilizes line graph trans-formations to derive the source and sink line graphs correspondingto the dynamic directed graphs. Two distinct line graph neuralnetworks are employed to calculate embeddings for the source andsink line graphs, respectively. Finally, an edge fusion mechanism isimplemented to ensure the duality of Dowker complexes.Line graph transformation. Initially, for the given input dy-namic directed graph G = (V, E), we transform it into sourceand sink line graphs according to eq. (1) and eq. (2). The temporal at-tributes of E are transferred to the nodes in the line graphs V(G )and V(G ), facilitating subsequent processing by the source-sinkline graph neural network.Source-sink line graph neural network. Next, we introducethe backbone neural network layer used for generating edge embed-dings in line graphs. After the transformation, the original directedgraph is divided into source line graphs and sink line graphs. Wehave designed a Source-Sink Line Graph Neural Network (SSLGNN)to generate their edge embeddings. The SSLGNN consists of twodistinct line graph neural networks and an edge fusion module.Specifically, the computation process of the -th layer of SSLGNNis as follows:",
  "=,(12)": "where and represent the features of edge at the -thlayer in the source line graph and sink line graph, respectively, while denotes the features after edge fusion. N () and N ()respectively denote the neighborhoods of edge in the source linegraph and sink line graph. represents the aggregation function,and denotes the message-passing function. The configurationsof and are consistent with those described in . The initial features of an edge 0 = 0 are computed using adynamic Dowker filtration. We have developed a simple yet efficientdynamic Dowker filtration W() to meet both the requirements ofdynamic Dowker duality and dynamic Dowker structural stability. The initial feature calculation is defined as follows:",
  "Joint Prediction Module": "The topological features at different dimensions of a graph representvarious structural aspects, such as connected components for = 0and loops for = 1. However, the quantity of Dowker PDs in agraph does not directly correlate with the number of nodes or edges.To better characterize topological features across dimensions, wehave designed a prediction module for simultaneously predictingthe 0-dimensional and 1-dimensional Persistence Diagrams (0-PDsand 1-PDs) of dynamic graphs. Simultaneously, inspired by theconcept of joint learning, we have designed a graph label predictionmodule to accommodate the needs of downstream tasks.0-PD Prediction. We extend the concept from and classifythe elements of a graphs 0-PD into three elements:",
  "(2) Unpaired points (, +), representing a connected compo-nent that is born but does not die": "(3) Disappearing points (,), indicating that the correspondingconnected component quickly dies after formation.As shown in fig. 4, in graph , with 0 < 1 < 2 < 3 < 4,(0, +), (1, +), (3, +) correspond to the unpaired points for0,1,3 respectively, (4,5) corresponds to the paired point for4, and the edges 2,5 forming higher-dimensional complexescorrespond to the disappearing points (2,2), (5,5). This approach",
  "= (Hn),(14)": "where 0 represents the predicted results for 0-dimensional Per-sistence Diagrams (0-PD), and denotes the edge embeddingsoutput by the Source-Sink Line Graph Neural Network (SSLGNN).1-PD Prediction. The interpretation of the birth and deathof 1-dimensional Dowker PDs from a geometric perspective ischallenging, as each PD point is associated with multiple edges.Therefore, we propose a neighborhood-based aggregation methodthat utilizes dynamic Dowker filtration values W() to weight theaggregation of each edges neighborhood, resulting in a subgraphstructural representation. Subsequently, a Multilayer Perceptron(MLP) layer is employed to calculate the 1-PD.",
  "= (Hagg),(16)": "in this formulation, belongs to H and the neighborhood of edge, denoted as N () N (), includes its neighborhoods in boththe source and sink line graphs.Graph Label Prediction. We propose a graph classificationmodule based on the idea of joint learning, which predicts thelabels of graphs after pooling the edge embeddings.",
  "= (Pooling(H))(17)": "where represents the predicted label of the graph, and Poolingis the pooling operation, for which we opt for max pooling in ourmethod.For the PD prediction task, we employ the 2-Wasserstein distancebetween the predicted results and the ground truth as the loss func-tion. Simultaneously, for the graph classification task, we choosecross-entropy as the loss function to train the model. This dualapproach ensures that our model is not only capable of accuratelyapproximating PDs but also effectively classifies graphs, leveragingthe topological features captured by the PDs for enhanced perfor-mance in classification tasks.",
  "EXPERIMENTS": "In this section, we provide a thorough evaluation of the proposedDynamic Neural Dowker Network from three perspectives. Sec-tion 5.1 details the datasets and baselines employed in our experi-ments. In .2, we assess the models proficiency in approxi-mating true persistent homology outcomes. .3 evaluatesthe models transferability and efficiency. Building upon the in-sights gained from the initial experiments, .4 is dedicatedto validating the models performance in graph classification tasks.The results demonstrate that our algorithm successfully approxi-mates the computational results of Dowker complexes and can beeffectively applied to downstream graph classification tasks, withnotable efficiency and transferability to larger graphs.1",
  "Datasets and baselines": "The datasets used in the experiments are categorized into twotypes: static and dynamic, with both large and small graph datasetsconstructed for each category. The static datasets include REDDIT-BINARY, REDDIT-MULTI-5K and REDDIT-MULTI-12K, where Red-dit serves as an online forum, with nodes representing users andedges representing discussion threads. The dynamic datasets en-compass four categories: citation graphs, Bitcoin graphs, Q&Agraphs and social graphs. The Bitcoin graphs consist of who-trusts-whom graphs from two Bitcoin platforms: Bitcoin OTC and BitcoinAlpha. The citation graphs include two distinct domains of papercitation graphs: HEP-PH (high energy physics phenomenology) andHEP-TH (high energy physics theory). The Q&A graphs compilerecords from various websites, including StackOverflow, MathOver-flow, SuperUser, and AskUbuntu. The social graph datasets consistof Hashtag diffusion graphs from the Weibo platform, categorizedinto two types: entertainment and current affairs topics. displays the details of these datasets.",
  "Classes GraphsAvg.Avg.GraphsAvg.Avg.Nodes EdgesNodes Edges": "REDDIT-B2160023327440012121392REDDIT-5K54000375433100010431246REDDIT-12K11950725827723909241066Citation240081289810028864288Bitcoin2160412977408802996Q&A4800918139720042955795Social280049245820027132410 All datasets are divided into 80% small graphs and 20% largegraphs. For the two static network datasets, they are sorted inascending order of node count, with the top 80% categorized assmall graphs and the bottom 20% as large graphs, which are thenshuffled. The four dynamic graph datasets are sampled based ona set target number of edges, where small and large networks aresampled differently, ensuring no overlap between the two sizes.",
  "DHGAT : A dynamic hyperbolic graph attention net-work that utilizes a spatiotemporal self-attention mechanismbased on hyperbolic distances": "EvolveGCN : A Euclidean dynamic graph embeddingmodel that uses GCNs to capture structural information ofnodes and RNNs to update the GCN parameters directly.This category includes two architectures: EGCN O andEGCN H. Roland : A Euclidean dynamic graph embedding modelthat adapts static GNNs for dynamic graphs by treating nodeembeddings at different GNN layers as hierarchical statesand updating them over time. It also approaches the trainingprocess as a meta-learning problem for quick adaptation.",
  "Approximating PD": "In this subsection, we evaluate the approximation error betweenthe PDs predicted by different methods and the ground truth.Experimental Setup. Consistent with the setup in , we usethe following two metrics to assess the quality of the approxima-tion: (1) The 2-Wasserstein distance between the predicted PD andthe ground truth. (2) The total squared distance between the persis-tence images (PI) converted from the predicted PD and the groundtruth, denoted as PIE. Our DNDN method uses the 2-Wassersteindistance(WD) as the loss function, and the suffix _ indicates thatthe methods loss function is designed based on PIE.Experiments were conducted on small graph datasets, with thedataset being randomly split into an 80%/20% distribution for thetraining/testing set. For our Dynamic Neural Dowker Network(DNDN) method, edge features are directly input into the graphas initial features. On static datasets, the edge filter function isdesigned based on the degree of nodes connected by edges. Con-currently, the line graph transformation generates identical sourceand sink line graphs. For models that are designed based on graphneural networks, node features result from the aggregation of theiredges features. Results. As seen in table 2 and table 2, our method exhibits asignificant advantage in approximating Dowker persistent homol-ogy results, with the best results highlighted in bold. The methodsGIN_PI and GAT_PI, which use PI as the loss function, do notproduce PD outputs, hence the 2-Wasserstein distance cannot becalculated for them. PDGNN, designed to capture EPD, performssuboptimally and lacks support for dynamic directed graph data,leading to poorer performance on some dynamic datasets. Acrossboth static and dynamic datasets, our method significantly out-performs the node-based GNN baselines, demonstrating that ouredge-based line graph neural network effectively captures the topo-logical features corresponding to Dowker complexes.",
  "Transferability and Efficiency": "In this subsection, we designed experiments to investigate the trans-ferability and efficiency of the DNDN algorithm, focusing primarilyon its adaptability to real-world large graphs that are computation-ally expensive to analyze.Experimental Setup. For transferability, we employed pre-trained models obtained from training on small graph datasetswithin the same category and tested them on large graph datasets.This approach was used to verify DNDNs capability to learn topo-logical features from small graphs of the same category and applythis knowledge to larger graphs. Regarding efficiency, we comparedthe time taken by DNDN and the GUDHI method to computeDowker PDs.Results. In table 4, \"Standard\" refers to the results obtained bytraining directly on large graph datasets, \"Pre_train\" denotes the re-sults of testing pretrained models (trained on small graph datasets)on large graph datasets, and \"Fine_tune\" represents the outcomesafter fine-tuning the pretrained models for 10 epoches. Across alldatasets, the performance of models after fine-tuning surpassesthat of models trained directly from scratch. This improvement islikely attributed to the fine-tuned models having learned more datainformation. On dynamic datasets, although \"Pre_train\" does notsurpass Standard, \"fine_tune\" generally yields better results than di-rect training. These experiments validate our models transferabilityto large graphs, which is crucial for addressing the computationalexpense of persistent homology methods on large dynamic datasets.In table 5, we compare the efficiency of algorithms in GUDHIand DNDN in processing Dowker PDs on large graph datasets. It isevident that our method significantly outperforms GUDHI on largegraph datasets, demonstrating the efficiency and applicability ofDNDN in handling complex, large-scale topological data analysistasks.",
  "Graph Classification": "In this subsection, having validated DNDNs ability to approximateDowker PDs and its transferability, we explore the algorithmsaccuracy in downstream tasks.Experimental Setup. To ascertain DNDNs efficacy in graphclassification tasks, we orchestrated two experiments: (1) Classifica-tion on small graphs, employing 5-fold cross-validation across eachdataset for final outcome derivation. (2) Transferable classificationexperiment, where the model, initially trained on small graphs,was subjected to graph classification tasks on larger graphs. All",
  "WDPIEWDPIEWDPIEWDPIE": "GIN_PI-4.71e-04 1.6e-04-2.73e-03 9.1e-04-5.15e-03 1.8e-03-1.44e-03 1.9e-04GAT_PI-7.82e-04 2.2e-04-1.93e-03 3.6e-04-2.80e-03 7.9e-04-9.04e-04 1.1e-04GAT0.960 0.111.40e-03 6.3e-032.508 0.111.09e-01 1.8e-013.185 1.101.44e-01 2.4e-010.900 0.019.20e-04 3.7e-04PDGNN1.313 0.441.87e-02 2.0e-022.016 0.446.81e-02 9.4e-023.708 1.744.13e-01 4.5e-011.010 0.162.34e-03 3.4e-03TOGL0.935 0.072.45e-03 2.0e-031.622 0.072.12e-02 3.3e-022.064 0.198.73e-03 3.2e-020.943 0.041.54e-03 1.3e-03RePHINE0.775 0.023.38e-04 1.5e-041.867 0.022.50e-02 8.8e-032.270 0.014.88e-02 2.3e-020.703 0.016.81e-04 4.6e-04DNDN-EF0.815 0.013.98e-04 4.7e-051.364 0.018.28e-03 1.0e-021.442 0.231.22e-02 1.1e-020.654 0.123.15e-04 3.1e-04DNDN0.591 0.02 1.29e-04 3.1e-05 0.804 0.02 1.33e-03 7.4e-04 0.908 0.04 2.13e-03 2.1e-04 0.514 0.03 1.01e-04 1.37e-05",
  "GUDHI16.46 0.0014.64 0.0025.10 0.0021.37 0.0011.33 0.0023.73 0.0014.85 0.002DNDN1.69 0.0030.66 0.0030.70 0.0030.13 0.0030.14 0.0030.17 0.0010.31 0.003": "experimental approaches leveraged the mean-pooling method toderive graph embeddings for label prediction.Results. As indicated in table 6, Small denotes the classificationexperiments conducted directly on small graph datasets, whereasLarge refers to graph classification on larger datasets after trainingon small graph datasets. Dynamic network embedding methodslike DySAT, EGCNO, and DHGAT, which focus on capturing in-dividual nodes evolving patterns, tend to lose some evolutionaryinformation after mean-pooling. Conversely, our method achievedoptimal performance in most scenarios, primarily because it learnsDowker PDs that represent the graphs topological features, thuscapturing the graphs global characteristics. Most baseline methodsare designed around a nodes neighborhood, failing to adequately capture the graphs comprehensive information. Particularly inthe transferable classification experiments, DNDN outperformedacross mostly datasets, demonstrating that insights learned fromsmall graph datasets can be effectively transferred to larger graphdatasets.",
  "SmallLargeSmallLargeSmallLargeSmallLargeSmallLargeSmallLarge": "GCN73.8 0.550.1 0.133.0 0.320.7 0.050.0 0.052.5 0.765.0 0.849.5 0.084.4 1.152.3 0.384.0 0.688.2 0.2GAT79.4 1.252.1 0.038.1 0.220.0 0.051.2 0.149.0 0.141.9 0.257.0 0.487.5 1.250.1 0.371.3 1.281.3 0.6GraphSage79.1 0.452.3 0.233.2 0.221.5 0.067.5 0.356.0 0.176.3 0.753.0 0.478.1 0.162.3 0.391.8 1.589.5 1.2GIN73.8 0.251.1 0.020.9 0.025.5 0.183.8 0.351.1 0.670.0 1.268.0 1.868.8 1.371.2 1.281.3 0.678.5 0.6DySAT78.3 0.754.5 0.032.2 0.122.3 0.086.9 1.363.0 1.678.4 2.577.3 2.787.4 1.472.2 0.791.2 1.575.2 2.0DHGAT76.4 0.156.2 0.136.7 0.221.7 0.085.8 1.265.9 0.376.7 0.178.4 0.286.4 0.365.9 0.392.3 0.486.5 0.2EGCN-O82.2 0.461.0 0.034.8 0.130.1 0.184.5 0.863.4 0.379.2 0.478.3 0.689.3 0.673.3 0.289.2 0.481.2 0.9EGCN-H82.5 0.559.5 0.232.5 0.028.3 0.086.7 0.262.4 0.580.4 0.580.2 0.689.6 0.475.5 0.688.6 1.280.5 1.4Roland84.6 0.265.2 0.247.2 0.140.2 0.287.5 0.268.2 0.778.5 0.678.6 0.480.2 0.272.1 0.590.5 0.291.2 0.3DNDN83.4 0.273.6 1.2 56.7 0.4 41.3 1.085.6 0.272.4 0.4 83.4 0.4 81.2 0.584.5 0.577.6 1.0 94.5 0.289.2 0.1 DNDN effectively captures and approximates the Dowker persistenthomology results of dynamic networks. Our experimental evalu-ation, spanning both static and dynamic datasets, demonstratesDNDNs superior performance in approximating true persistenthomology, highlighting its potential to enhance graph classificationtasks. Notably, the method showcases remarkable transferability,proving its efficacy in learning topological features from smallergraphs and applying them to larger counterparts with enhancedefficiency. In the future, we will focus on two critical directionsto address the current limitations of DNDN: (1) extending the dy-namic Dowker filtration method to node-level tasks by constructingdynamic neighborhood subgraphs of nodes to study their higher-order evolutionary patterns, and (2) building on the approximationof 0-dimensional and 1-dimensional persistence diagrams (PDs),exploring methods to approximate higher-dimensional PDs.",
  "Mehmet E Aktas, Esra Akbas, and Ahmed El Fatmaoui. 2019. Persistence homol-ogy of networks: methods and applications. Applied Network Science 4, 1 (2019),128": "Lowell W Beineke and Jay S Bagga. 2021. Line graphs and line digraphs. Springer. Lei Cai, Jundong Li, Jie Wang, and Shuiwang Ji. 2021. Line graph neural networksfor link prediction. IEEE Transactions on Pattern Analysis and Machine Intelligence44, 9 (2021), 51035113. Mathieu Carrire, Frdric Chazal, Yuichi Ike, Tho Lacombe, Martin Royer, andYuhei Umeda. 2020. Perslay: A neural network layer for persistence diagramsand new graph topological signatures. In International Conference on ArtificialIntelligence and Statistics. PMLR, 27862796.",
  "Guido Montufar, Nina Otter, and Yu Guang Wang. 2020. Can neural networkslearn persistent homology features?. In TDA {\\&} Beyond": "Aldo Pareja, Giacomo Domeniconi, Jie Chen, Tengfei Ma, Toyotaro Suzumura,Hiroki Kanezashi, Tim Kaler, Tao Schardl, and Charles Leiserson. 2020. Evolvegcn:Evolving graph convolutional networks for dynamic graphs. In Proceedings ofthe AAAI Conference on Artificial Intelligence, Vol. 34. 53635370. Aravind Sankar, Yanhong Wu, Liang Gou, Wei Zhang, and Hao Yang. 2020.Dysat: Deep neural representation learning on dynamic graphs via self-attentionnetworks. In Proceedings of the 13th International Conference on Web Search andData Mining. 519527.",
  "ADOWKER COMPLEXESA.1Diffences between VP complexes andDowker complexes": "As fig. 5 illustrated, Vietoris-Rips (VP) complexes and Dowker com-plexes focus on different graph structures. VP complexes directlyconcern the neighbor structures on the graph, whereas Dowkercomplexes are interested in the shared neighbor structures on thegraph. In graph , 1 and 2 are connected by edge 12, corre-sponding to a one-dimensional complex. In graph , 1 and 2correspond to the same one-dimensional complex because 31 and32 share a common source node 3, even though 1 and 2 maynot be directly connected by an edge. Traditional graph neuralnetworks aggregate the neighbors of a node, making them suit-able for VP complexes but not necessarily for Dowker complexes.However, we find that the relationship between 31 and 32 can betransformed into the form of a line graph, thus adapting line graphneural networks for computation with Dowker complexes. 1-dim Complex 1-dim Complex 0-dim Complex Vietoris-Rips Complex"
}