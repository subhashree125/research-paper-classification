{
  "ABSTRACT": "Community detection (CD) is a classic graph inference task thatpartitions nodes of a graph into densely connected groups. Whilemany CD methods have been proposed with either impressive qual-ity or efficiency, balancing the two aspects remains a challenge.This study explores the potential of deep graph learning to achievea better trade-off between the quality and efficiency of -agnosticCD, where the number of communities is unknown. We proposePRoCD (Pre-training & Refinement for Community Detection), asimple yet effective method that reformulates -agnostic CD asthe binary node pair classification. PRoCD follows a pre-training &refinement paradigm inspired by recent advances in pre-trainingtechniques. We first conduct the offline pre-training of PRoCD onsmall synthetic graphs covering various topology properties. Basedon the inductive inference across graphs, we then generalize thepre-trained model (with frozen parameters) to large real graphsand use the derived CD results as the initialization of an existingefficient CD method (e.g., InfoMap) to further refine the quality ofCD results. In addition to benefiting from the transfer ability regard-ing quality, the online generalization and refinement can also helpachieve high inference efficiency, since there is no time-consumingmodel optimization. Experiments on public datasets with variousscales demonstrate that PRoCD can ensure higher efficiency in-agnostic CD without significant quality degradation.",
  "Community Detection, Graph Clustering, Inductive Graph Infer-ence, Pre-training & Refinement": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08 ACM Reference Format:Meng Qin, Chaorui Zhang, Yu Gao, Weixi Zhang, and Dit-Yan Yeung. 2024.Pre-train and Refine: Towards Higher Efficiency in K-Agnostic CommunityDetection without Quality Degradation. In Proceedings of the 30th ACMSIGKDD Conference on Knowledge Discovery and Data Mining (KDD 24),August 2529, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.",
  "INTRODUCTION": "Community detection (CD) is a classic graph inference task that par-titions nodes of a graph into several groups (i.e., communities) withdense linkage distinct from other groups . It has been validatedthat the extracted communities may correspond to substructuresof various real-world complex systems (e.g., functional groups inprotein interactions ). Many network applications (e.g., cellularnetwork decomposition and Internet traffic profiling ) arethus formulated as CD. Due to the NP-hardness of some typicalCD objectives (e.g., modularity maximization ), balancing thequality and efficiency of CD on large graphs remains a challenge.Most existing approaches focus on either high quality or efficiency.On the one hand, efficient CD methods usually adopt heuristicstrategies or fast approximation w.r.t. some relaxed CD objectivesto obtain feasible CD results (e.g., the greedy maximization andsemi-definite relaxation of modularity). Following the graphembedding framework, which maps nodes V = {} of a graph intolow-dimensional vector representations {z R} ( |V|) withmajor topology properties preserved, several efficient embeddingapproaches have also been proposed based on approximated di-mension reduction (e.g., random projection for high-order topology). Some of them are claimed to be community-preserving andable to support CD using a downstream clustering module .Despite the high efficiency, these methods may potentially sufferfrom quality degradation due to the information loss of heuristics,approximation, and relaxation.On the other hand, recent studies have demonstrated the abilityof deep graph learning (DGL) techniques , e.g., those basedon graph neural networks (GNNs), to achieve impressive qualityof various graph inference tasks including CD . However, theirpowerful performance usually relies on iterative optimization algo-rithms (e.g., gradient descent) that direct sophisticated models tofit complicated objectives, which usually have high complexities.",
  ": Overview of mainstream (a) task-dependent and (b)-independent DGL methods as well as (c) our PRoCD method": "In this study, we explore the potential of DGL to ensure higherefficiency in CD without significant quality degradation, comparedwith mainstream efficient and DGL methods. It serves as a pos-sible way to achieve a better trade-off between the two aspects.Different from existing CD methods with an assump-tion that the number of communities is given, we consider themore challenging yet realistic -agnostic CD, where is unknown.In this setting, one should simultaneously determine and thecorresponding community partition.Dilemmas. As shown in (a) and (b), existing DGL-basedCD techniques usually follow the unified graph embedding frame-work and can be categorized into the (i) task-dependent and (ii)task-independent approaches. We argue that they may suffer fromthe following limitations w.r.t. our settings.First, existing DGL methods may be inapplicable to -agnosticCD. Given a graph G, some task-dependent approaches (see (a)) generate embeddings {z} via an embedding encoderEnc() (e.g., a multi-layer GNN) and feed {z} into an output mod-ule Out() (e.g., a multi-layer perceptron (MLP)) to derive the CDresult C, i.e., {z} = Enc(G) and C = Out({z}). Since parametersin Out() are usually with the dimensionality related to , thesetask-dependent methods cannot output a feasible CD result when is unknown. Although some task-independent methods (see (b)) do not contain Out() related to , their original de-signs still rely on a downstream clustering algorithm (e.g., Means)with {z} and as required inputs.Second, the standard transductive inference of some DGL tech-niques may result in low efficiency. As illustrated in (a) and (b),during the inference of CD on each single graph, these transductivemethods must first optimize their model parameters from scratchvia unsupervised objectives, which is usually time-consuming. Somerelated studies only consider transductive inference re-gardless of its low efficiency.Focusing on the CD with a given and task-independent archi-tecture in (b), recent research has validated that the in-ductive inference, which (i) trains a DGL model on historical graphs{G } and (ii) generalizes it to new graphs {G} without additionaloptimization, can help achieve high inference efficiency on {G}. One can extend this method to -agnostic CD by replacing thedownstream clustering module (e.g., Means) with an advancedalgorithm unrelated to (e.g., DBSCAN ). Our experimentsindicate that such a naive extension may still have low inferencequality and efficiency compared with conventional baselines.Contributions. We propose PRoCD (Pre-training & Refinementfor Community Detection), a simple yet effective method as illus-trated in (c), to address the aforementioned limitations.A novel model architecture. To derive feasible results for -agnostic CD in an end-to-end way, we reformulate CD as the binarynode pair classification. Unlike existing end-to-end models that di-rectly assign the community label {1, , } of each node (e.g., via an MLP in (a)), we develop a binary classifier,with a new design of pair-wise temperature parameters, to deter-mine whether a pair of nodes (, ) are partitioned into the samecommunity. Given the classification result w.r.t. a set of node pairsP = {(, )} sampled from a graph G, we construct another auxil-iary graph G, from which a feasible CD result of G can be extracted.Each connected component in G corresponds to a community inG, with the number of components as the estimated .A novel learning paradigm. Inspired by recent advances ingraph pre-training techniques, PRoCD follows a novel pre-training& refinement paradigm with three phases as in (c). Based onthe assumption that one has enough time to prepare a well-trainedmodel in an offline way (i.e., offline pre-training), we first pre-trainPRoCD on small synthetic graphs with various topology properties(e.g., degree distributions) and high-quality community ground-truth. We then generalize PRoCD (with frozen parameters) to largereal graphs {G} (i.e., online generalization) and derive correspond-ing CD results { C} via only one feed-forward propagation (FFP)of the model (i.e., inductive inference of DGL). In some pre-trainingtechniques , online generalization only provides an initializa-tion of model parameters, which is further fine-tuned w.r.t. differenttasks. Analogous to this motivation, we treat C as the initializationof an efficient CD method (e.g., InfoMap ) and adopt its outputC as the final CD result, which refines C (i.e., online refinement).In particular, the online generalization and refinement may benefitfrom the powerful transfer ability of inductive inference regardingquality while ensuring high inference efficiency, since there is notime-consuming model optimization.Note that our pre-training & refinement paradigm is differentfrom existing graph pre-training techniques . We arguethat they may suffer from the following issues about CD. Besidespre-training, these methods may require another optimization pro-cedure for the fine-tuning or prompt-tuning of specific tasks on{G}, which is time-consuming and thus cannot help achieve highefficiency on {G}. To the best of our knowledge, related studies merely focus on supervised tasks (e.g., node classification)and do not provide unsupervised tuning objectives for CD.A better trade-off between quality and efficiency. Experi-ments on datasets with various scales demonstrate that PRoCDcan ensure higher inference efficiency in CD without significantquality degradation, compared with running a refinement methodfrom scratch. In some cases, PRoCD can even achieve improvementin both aspects. Therefore, we believe that this study provides apossible way to obtain a better trade-off between the two aspects.",
  "RELATED WORK": "In the past few decades, many CD methods have been proposedbased on different problem statements, hypotheses, and techniques. summarizes some representative approaches accordingto their original designs. Most of them focus on either high effi-ciency or quality. Conventional efficient CD methods use heuristicstrategies or fast approximation w.r.t. some relaxed CD objectivesto derive feasible CD results, including the (i) greedy modularitymaximization in FastCom and Louvain , (ii) semi-definite re-laxation of modularity in Locale , (iii) label propagation heuristicin LPA , as well as (iv) Monte Carlo approximation of stochasticblock model (SBM) in MC-SBM and Par-SBM .Recent studies have also demonstrated the powerful potential ofDGL to ensure high quality of CD, following the architectures in (a) and (b). However, some methods (e.g., DNR , DCRN, and DMoN ) only considered the inefficient transductiveinference, with time-consuming model optimization in the infer-ence of CD on each single graph. Other approaches (e.g., ClusNet, LGNN , GAP , and ICD ) considered inductive in-ference across graphs. Results of ICD validated that the inductiveinference can help achieve a better trade-off between quality andefficiency in online generalization (i.e., directly generalizing a modelpre-trained on historical graphs {G } to new graphs {G}). Nev-ertheless, the quality of online generalization may be affected bythe possible distribution shift between {G } and {G}. In contrast,online generalization is used to construct the initialization of onlinerefinement in our PRoCD method, which can ensure better qualityon {G}. Moreover, most task-dependent DGL methods (e.g., Clus-Net, LGNN, GAP, and DMoN) are inapplicable to -agnostic CD,since they usually contain an output module related to . Althoughtask-independent approaches (e.g., DNR, DCRN, and ICD) do notcontain such a module, their original designs still rely on a pre-set (e.g., Means as the downstream clustering algorithm).Different from the aforementioned methods, our PRoCD methodcan ensure higher efficiency in -agnostic CD without significantquality degradation via a novel model architecture following thepre-training & refinement paradigm, as highlighted in (c).As reviewed in , existing graph pre-training techniquesusually follow the paradigms of (i) pre-training & fine-tuning (e.g.,GCC , L2P-GNN , and W2P-GNN ) as well as (ii) pre-training & prompting (e.g., GPPT , GraphPrompt , and ProG). In addition to offline pre-training, these methods rely on an-other optimization procedure for the fine-tuning or prompt-tuningof specific inference tasks, which is usually time-consuming. More-over, most of them merely focus on supervised tasks (e.g., nodeclassification, link prediction, and graph classification) and do notprovide unsupervised tuning objectives for CD. Therefore, one can-not directly apply these pre-training methods to ensure the highefficiency in CD without quality degradation.",
  "PROBLEM STATEMENTS & PRELIMINARIES": "In this study, we consider the disjoint CD on undirected graphs. Agraph can be represented as G = (V, E), with V = {1, , }and E = {(, )|, V)} as the sets of nodes and edges.The topology of G can be described by an adjacency matrix A {0, 1} , where A = A = 1 if (, ) E and A = A = 0otherwise. Since CD is originally defined only based on graphtopology , we assume that graph attributes are unavailable.Community Detection (CD). Given a graph G, CD aims topartition the node set V into subsets (defined as communities)C = {C1, , C } ( = for ) s.t. (i) the linkagewithin each community is dense but (ii) that between communitiesis relatively loose. We define that a CD task is -agnostic if thenumber of communities is unknown for a given graph G, whereone should simultaneously determine and the corresponding CDresult C. We consider -agnostic CD in this study. Whereas, someexisting methods assume that is given for each inputG and thus cannot tackle such a challenging yet realistic setting.Modularity Maximization. Mathematically, CD can be formu-lated as the modularity maximization objective , which maxi-mizes the difference between the exact graph topology (described byA) and a randomly generated case. Given G and , it aims to derivea partition C that maximizes the following modularity metric:",
  "minH tr(H QH) s.t. H = 1, 0, otherwise,(2)": "where Q R is defined as the modularity matrix with Q :=[A deg()deg()/(2)]; H {0, 1} indicates the com-munity membership of C. Our method does not directly solve theaforementioned problem. Instead, we try to extract informativecommunity-preserving features from Q.Pre-training & Refinement Paradigm. To achieve a bettertrade-off between the quality and efficiency, we propose a pre-training & refinement paradigm based on the inductive inference ofDGL. We represent a DGL model as C = (G; ), which derives theCD result C given a graph G, with as the set of trainable model pa-rameters. As in (c), the proposed paradigm includes (i) offlinepre-training, (ii) online generalization, and (iii) online refinement.In offline pre-training, we first generate a set of synthetic graphsT = {G1, , G } via a generator (e.g., SBM ). The generationof each graph G T includes the topology (V, E) and commu-nity assignment ground-truth C( ). We then pre-train on T (i.e.,updating ) based on an objective regarding {(V, E)} and {C( )}",
  ": Overview of the model architecture of PRoCD": "in an offline way. After that, we generalize to a large real graphG with frozen (i.e., online generalization), which can derive afeasible CD result C via only one FFP of (i.e., inductive inference).Analogous to the fine-tuning in existing pre-training techniques, wetreat C as the initialization of a conventional efficient CD method(e.g., InfoMap ) and adopt its output C as the final CD result,which further refines C (i.e., online refinement).Evaluation Protocol. In real applications, it is usually assumedthat one has enough time to prepare a well-trained model in an of-fline way (e.g., pre-training of LLMs). After deploying , one mayachieve high efficiency in the online inference on graphs {G} (e.g.,via one FFP of without any model optimization). In contrast, con-ventional methods cannot benefit from pre-training but have to runtheir algorithms on {G} from scratch, due to the inapplicability toinductive inference. Our evaluation focuses on the online inferenceof CD on {G} (e.g., online generalization and refinement of PRoCD),which is analogous to the applications of foundation models. Forinstance, users benefit from the powerful online inference of LLMs(e.g., generating high-quality answers in few seconds) but do notneed to train them using a great amount of resources.",
  "Model Architecture": "To enable PRoCD to derive feasible results for -agnostic CD inan end-to-end architecture, we reformulate CD as the binary nodepair classification. For each graph G = (V, E), we introduce anauxiliary variable S {0, 1} ( := |V|), where S = S = 1if nodes and are partitioned into the same community andS = S = 0 otherwise. Given a set of node pairs P = {(, )}, wealso rearrange corresponding elements in S as a vector y {0, 1}| P|,where y = S = S w.r.t. each node pair = (, ) P. provides an overview of the model architecture with a runningexample. Given a graph G, the model samples a set of node pairs P",
  "Feature Extraction Module. As shown in (a), wefirst extract community-preserving features, arranged as X R": "( ), for an input graph G from the modularity maximizationobjective (2). The modularity matrix Q in (2) is a primary componentregarding graph topology. It can be considered as a reweightingof original topology, where nodes (, ) with similar neighbor-induced features (Q,:, Q,:) are more likely to belong to a commoncommunity. To minimize the objective in (2), which is equivalentto maximizing the modularity metric in (1), Q with a large valueindicates that (, ) are more likely to be partitioned into the samecommunity (e.g., Q17 = Q71 = 0.78 in ). Therefore, we believethat Q encodes key characteristics regarding community structures.Note that Q R is usually dense. To utilize the sparsityof topology, we reduce Q to a sparse matrix Q R , whereQ = Q if (, ) E and Q = 0 otherwise. Although thereduction may lose some information, it enables the model to bescaled up to large sparse graphs without constructing an dense matrix. Our experiments demonstrate that PRoCD can stillderive high-quality CD results using Q. Given Q, we derive X via",
  "X = MLP( Q), with R N (0, 1/).(3)": "Concretely, the Gaussian random projection , an efficient dimen-sion reduction technique that can preserve the relative distancesbetween input features with rigorous theoretical guarantees, is firstapplied to Q. We then incorporate non-linearity into the reducedfeatures using an MLP. 4.1.2Embedding Encoder. Given the extracted features X, wethen derive low-dimensional node embeddings {z R} using amulti-layer GNN with skip connections, as shown in (b). Here,we adopt GCN as an example building block. One can easilyextend the model to include other advanced GNNs. Let Z andZ[] be the input and output of the -th GNN layer, with Z = X.The multi-layer GNN can be described as",
  "Z = LN(Linear( Z)), Z = Z[],Z[] = LN(tanh( D0.5 A D0.5ZW[])),(4)": "where A := A + I is the adjacency matrix containing self-edges; Dis the degree diagonal matrix w.r.t. A; W[] R is a trainableweight matrix; LN(U) denotes the row-wise 2 normalization on amatrix U (i.e., U,: U,:/|U,:|2); Z R is the matrix form ofthe derived embeddings, with Z,: = z R as the embedding ofnode . In each GNN layer, Z[],: is an intermediate representation ofnode , which is the non-linear aggregations (i.e., weighted mean)of features w.r.t. {} Nei(), with Nei() as the set of neigh-bors of . Since this aggregation operation forces nodes (, )with similar neighbors (Nei(), Nei()) (i.e., dense local topology)to have similar representations (Z[],: , Z[],: ), the multi-layer GNNcan enhance the ability of PRoCD to derive community-preservingembeddings. To obtain Z, we first sum up the intermediate repre-sentations {Z[]} of all the layers and apply a linear mapping to thesummed representation Z. Furthermore, the row-wise 2 normal-ization is applied. It forces {z} to be distributed in a unit sphere,with |z z |2 = 2 2zz .",
  "treat each extracted component as a community to form C": "4.1.3Binary Node Pair Classifier. As highlighted in (c),given a pair of nodes (, ) sampled from a graph G, we use abinary classifier to estimate S based on embeddings (z, z) anddetermine whether (, ) are partitioned into the same community.A widely adopted design of binary classifier is as follow",
  "S = exp(|z z |2) = exp(2 (zz 1)),with = (z) (z),(6)": "where and are MLPs with the same configuration. In (6), Sis estimated based on the distance between (z, z) and a pair-wisetemperature parameter . Different node pairs {(, )} may havedifferent parameters { } determined by embeddings {(z, z)}. 4.1.4Result Derivation Module. As illustrated in (d), wedevelop a result derivation module, which outputs a feasible CDresult C based on a set of node pairs sampled from the input graphG. Algorithm 1 summarizes the procedure of this module.In lines 1-4, we construct a set of node pairs P = {(, )} (e.g.,dotted lines in (d)), which includes (i) all the edges of inputgraph G (e.g., (1, 7) and (3, 4) in (d)) and (ii) randomlysampled node pairs (e.g., (6, 7) and (3, 7) in (d)).In line 5, we derive the estimated values {S } w.r.t. node pairsin P (via one FFP of the model) and rearrange them as a vector y R| P|. In particular, y = S represents the probability that a pairof nodes = (, ) are partitioned into a common community.In lines 6-9, we further construct an auxiliary graph G = (V, E)based on y and P. G has the same node set V as the input graphG but a different edge set E. For each = (, ) P, we add(, ) to E (i.e., preserving this node pair as an edge in G) ify > 0.5. G may contain multiple connected components (e.g., thetwo components in (d)). All the edges { = (, )} within acomponent are with high values of {y = S }, indicating that theassociated nodes are more likely to belong to the same community.In lines 10-11, we derive a feasible CD result C w.r.t. G by extract-ing all the connected components of G via the depth-first search (DFS) or breadth-first search (BFS). Each connected component ofG corresponds to a unique community in G (e.g., the two communi-ties in (d)), with the number of components as the estimatednumber of communities . Therefore, the aforementioned designsenable our PRoCD method to tackle -agnostic CD.",
  "Offline Pre-Training": "We conducted the offline pre-training of PRoCD on a set of syntheticgraphs T = {G1, , G }. One significant advantage of using syn-thetic pre-training data is that we can simulate various propertiesof graph topology with different levels of noise and high-qualityground-truth by adjusting parameters of the synthetic generator.In particular, we consider a challenging setting of pre-trainingPRoCD on small synthetic graphs (e.g., with 5 103 nodes)but generalize it to large real graphs (e.g., > 106). One reasonof using small pre-training graphs is that it enables PRoCD toconstruct dense matrices in pre-training objectives (e.g.,S R for binary classification) and thus fully explore thetopology and community ground-truth of pre-training data. Incontrast, constructing dense matrices for large graphs maybe intractable. Although there may be distribution shifts betweenthe (i) small pre-training graphs {G } and (ii) large graphs {G} tobe partitioned, our experiments demonstrate that offline pre-trainingon {G } is essential for PRoCD to derive feasible CD results { C}for {G}. By using { C} as the initialization, their quality can befurther refined via an efficient CD method. 4.2.1Generation of Pre-Training Data. As a demonstration,we adopt the degree-corrected SBM (DC-SBM) implementedby graph-tool1 to generate synthetic pre-training graphs. Besidestopology, the generator also produces community assignments w.r.t.a specified level of noise, which can be used as the high-qualityground-truth of CD. Our experiments also validate that the syn-thetic ground-truth can help PRoCD derive a good initialization foronline refinement. Whereas, community ground-truth is usually un-available for existing methods (pre-)trained on real graphs.Instead of using fixed generator parameters, we let them followcertain probability distributions to simulate various topology prop-erties (e.g., distributions of node degrees and community sizes) andnoise levels of community structures. Namely, different graphs maybe generated based on different parameter settings sampled fromcertain distributions. Due to space limits, we leave details aboutthe parameter setting and generation algorithm in Appendix A. 4.2.2Pre-Training Objectives & Algorithms. For each syn-thetic graph G, suppose there are nodes partitioned into communities. We use M( ) to denote a vector/matrix associatedwith G (e.g., y( ) and S(t)). Given a graph G, some previous stud-ies adopt a relaxed version of the modularity maximizationobjective (2) for model optimization and demonstrate its potentialto capture community structures. This relaxed objective allowsH( ) {0, 1} in (2), which describes hard community assign-ments, to be continuous values (e.g., H( ) R +derived viaan MLP). Note that we reformulate CD as the binary node pairclassification with S( ) {0, 1} . S( ) R +derived from",
  "(D( ) S( )D( ))], (7)": "where y( ) is the rearrangement of elements in S( ) w.r.t. the edgeset E; D( ) is the degree diagonal matrix of G; > 0 is theresolution parameter of modularity maximization , with < 1(> 1) favoring the partition of large (small) communities.In addition, we utilize the synthetic community ground-truthC of each graph G to enhance the ability of PRoCD to capturecommunity structures. Note that different graphs {G } may havedifferent numbers of communities { }. However, some DGL-basedCD methods are only designed for graphs with a fixed. Community labels are also permutation-invariant. For instance,(1,2,3,4,5) = (1, 1, 1, 2, 2) and (1,2,3,4,5) = (2, 2, 2, 1, 1)represent the same community assignment, with as the labelassignment of node . Hence, the standard multi-class cross en-tropy objective cannot be directly applied to integrate communityground-truth. PRoCD handles these issues by reformulating CD asthe binary node pair classification with auxiliary variables {S( )},where the dimensionality and values of {S( )} are unrelated to { }and permutations of community labels. Given a graph G, one canconstruct S( ) based on ground-truth C( ) and derive S( ) via oneFFP of the model. We introduce the following binary cross entropyobjective that covers all the node pairs:",
  "LPTN(G) := LMOD(G) + LBCE(G),(9)": "where > 0 is the hyper-parameter to balance LMOD and LBCE.The offline pre-training procedure is concluded in Algorithm 2,where the Adam optimizer with learning rate is applied to up-date model parameters ; the updated after epochs are thensaved for online generalization and refinement. Since only the em-bedding encoder and binary classifier contain model parameters tobe learned, we do not need to apply Algorithm 1 of the result deriva-tion module to derive a feasible CD result in offline pre-training.",
  "Online Generalization & Refinement": "After offline pre-training, we can directly generalize PRoCD to largereal graphs {G} (with frozen model parameters ) and derive fea-sible CD results { C} via only one FFP of the model and Algorithm 1(i.e., online generalization).Recent advances in graph pre-training techniques demon-strate that pre-training may provide good initialized model param-eters for downstream tasks, which can be further improved viaa task-specific fine-tuning procedure. To the best of our knowl-edge, most existing graph pre-training methods merely considersupervised tasks (e.g., node classification) but do not provide unsu-pervised tuning objectives for CD. A straightforward fine-tuningstrategy for CD is applying the modularity maximization objective(7) to large real graphs {G} (e.g., gradient descent w.r.t. a relaxedversion of (2) with large dense matrices {Q}), which is usuallyintractable. Analogous to fine-tuning, we introduce the online re-finement phase with a different design. Instead of fine-tuning, wetreat the CD result C derived in online generalization as the ini-tialization of a conventional efficient CD method (e.g., LPA ,InfoMap , and Locale in our experiments) and run thismethod to derive a refined CD result C.As shown in , we adopt two strategies to construct theinitialization. First, a super-graph G can be extracted based on C,where we merge nodes in each community C C as a super-node(e.g., 3 w.r.t. C3 = {5, 6, 7} in ) and set the number of edgesbetween communities as the weight of each super-edge (e.g., 2 for(1, 3) in ). We then use G as the input of an efficient methodthat can handle weighted graphs (e.g., InfoMap and Locale) to derivea CD result w.r.t. G, which is further recovered to the result C w.r.t.G. Second, we also use C as the intermediate label assignmentof a method that iteratively updates community labels (e.g., LPA)and refines the label assignment until convergence (e.g., refining6 from 3 to 1 in ). Compared with running the refinementmethod on G from scratch, online refinement may be more efficient,because the constructed initialization reduces the number of nodesto be partitioned and iterations to update community labels. It is",
  "DatasetsMinMaxAvg DegDensity": "Protein 81,5741,779,88514,98343.65e-4ArXiv 169,3431,157,799113,16113.78e-5DBLP 317,0801,049,86613436.62e-5Amazon 334,863925,87215495.52e-5Youtube 1,134,8902,987,624128,7545.35e-6RoadCA 1,957,0272,760,3881122.821e-6 also expected that PRoCD can transfer the capability of capturingcommunity structures from the pre-training data to {G}.Algorithm 3 concludes the aforementioned procedure. The com-plexity of online generalization (i.e., lines 1-2) is O(( + )2),where and are the numbers of nodes and edges; is the em-bedding dimensionality. The complexity of online refinement (i.e.,lines 3-4) depends on the refinement method, which is usuallyefficient. We leave detailed complexity analysis in Appendix B.",
  "Experiment Setup": "Datasets. We evaluated the inference efficiency and quality ofour PRoCD method on 6 public datasets with statistics depictedin , where and are the numbers of nodes and edges.Note that these datasets do not provide ground-truth about thecommunity assignment and the number of communities . Somemore details regarding the datasets can be found in Appendix C.1.Baselines. We compared PRoCD over 11 baselines.(i) MC-SBM , (ii) Par-SBM , (iii) GMod , (iv) LPA , (v)InfoMap , (vi) Louvain , and (vii) Locale , and (viii) RaftGP are efficient end-to-end methods that can tackle -agnostic CD.Note that some other approaches whose designs rely on a pre-set (e.g., FastCom , GraClus , ClusterNet , LGNN , andDMoN ) could not be included in our experiments.In addition, we extended (ix) LouvainNE , (x) SketchNE ,and (xi) ICD , which are state-of-the-art efficient graph embed-ding methods, to -agnostic CD by combining the derived embed-dings with DBSCAN , an efficient clustering algorithm thatdoes not require . In particular, LouvainNE and ICD are claimedto be community-preserving methods. RaftGP-C, RaftGP-M, ICD-C,and ICD-M denote different variants of RaftGP and ICD.As a demonstration, we adopted LPA, InfoMap, and Locale (an ad-vanced extension of Louvain) for the online refinement of PRoCD,forming three variants of our method. In , the first strategy ofconstructing the refinement initialization has a motivation similar to graph coarsening (GC) techniques , which merge a large graphG into a super-graph G (i.e., reducing the number of nodes to bepartitioned) via a heuristic strategy. Whereas, PRoCD constructsG based on the output of a pre-trained model. To highlight thesuperiority of offline pre-training and inductive inference beyondGC, we introduced another baseline that provides the initializa-tion (with the same number of super-nodes as PRoCD) for onlinerefinement using the efficient GC strategy proposed in .ICD is an inductive baseline with a paradigm similar to PRoCD,including offline (pre-)training on historical graphs and online gen-eralization to new graphs. However, there is no online refinementin ICD. We conducted the offline pre-training of ICD and PRoCDon the generated synthetic graphs as described in .2.1 andgeneralized them to each dataset for online inference. For the restbaselines, we had to run them from scratch on each dataset.Evaluation Criteria. Following the evaluation protocol de-scribed in , we adopted modularity and inference time(sec) as the quality and efficiency metrics. Moreover, we define thata method encounters the out-of-time (OOT) exception if it fails toderive a feasible CD result within 2 104 seconds.Due to space limits, we elaborate on other details of experimentsetup (e.g., parameter settings) in Appendix C.",
  "Quantitative Evaluation & Discussions": "The average evaluation results are reported in . Besides thetotal inference time, we also report the time w.r.t. each inferencephase of PRoCD in , where Feat, FFP, Init, and Rfndenote the time of (i) feature extraction, (ii) one FFP, (iii) initialresult derivation, and (iv) online refinement. Analysis about theinference time of GC and embedding baselines is given in Tables 5and 6, where GC, Rfn, Emb, and Clus denote the time of (i)graph coarsening, (ii) online refinement, (iii) embedding derivation,and (iv) downstream clustering (i.e., via DBSCAN).In , three variants of PRoCD achieve significant improve-ment of efficiency w.r.t. the refinement methods while the qualitydegradation is less than 2%. In some cases, PRoCD can even obtainimprovement for both aspects. Compared with all the baselines,PRoCD can ensure the best efficiency on all the datasets and is ingroups with the best quality. In summary, we believe that the pro-posed method provides a possible way to obtain a better trade-offbetween the quality and efficiency in CD.In most cases, PRoCD outperforms GC baselines in both aspects,which validates the superiority of offline pre-training and inductiveinference beyond heuristic GC. The extensions of some embeddingbaselines (e.g., SketchNE and ICD combined with DBSCAN) sufferfrom low quality and efficiency compared with other end-to-end ap-proaches (e.g., Louvain and Locale). It implies that the task-specificmodule (e.g., binary node pair classifier in PRoCD) is essential forthe embedding framework to ensure the quality and efficiency in-agnostic CD, which is seldom considered in related studies.According to , online refinement is the major bottleneckin the inference of PRoCD, with the runtime depending on con-crete refinement methods. Compared with running a refinementmethod from scratch, online refinement has much lower runtime. Itvalidates our motivation that constructing the initialization (as de-scribed in ) can reduce the number of nodes to be partitioned",
  "ICD-M+DBSCAN12.90OOT": "and iterations to update label assignments for refinement methods.To construct the initialization of online refinement, GC is not asefficient as the online generalization of PRoCD (i.e., feature extrac-tion, one FFP, and initial result derivation) according to .In , the downstream clustering (i.e., DBSCAN to derive afeasible result for -agnostic CD) is time-consuming, which resultsin low efficiency of the extended embedding baselines, althoughtheir embedding derivation phases are efficient.",
  ": Ablation study w.r.t. the number of pre-traininggraphs on DBLP and Amazon in terms of modularity": "in (6), (vi) cross-entropy objective LBCE, and (vii) modularity maxi-mization objective LMOD by removing corresponding componentsfrom the original model. In case (iii), we let X be the one-hot encod-ings of node degree, which is a standard feature extraction strategyfor GNNs when attributes are unavailable. We directly used X asthe derived embeddings in case (iv), while we adopted the naivebinary classifier (5) to replace our design (6) in case (v). The averageablation study results on DBLP and Amazon are reported in ,where there are quality declines in all the cases. In some extremecases (e.g., the one without pre-training), the model mistakenlypartitions all the nodes into one community, resulting in a modu-larity value of 0.0000. In summary, all the considered proceduresand components are essential to ensure the quality of PRoCD.To further verify the significance of offline pre-training in ourPRoCD method, we conducted additional ablation study by setting",
  ": Parameter analysis w.r.t. and on DBLP and Ama-zon in terms of modularity": "the number of pre-training graphs {0, 10, 50, 12, 52, 13, 53}.The corresponding results on DBLP and Amazon in terms of modu-larity are reported in . Compared with our standard setting(i.e., = 13), there are significant quality declines for all the vari-ants of PRoCD when there are too few pre-training graphs (e.g., < 50). It implies that the offline pre-training (with enough syntheticgraphs) is essential to ensure the high inference quality of PRoCD.",
  "Parameter Analysis": "We also tested the effects of {, } in the pre-training objective (9)by adjusting {0.01, 0.1, 1, 10, 100} and {0.1, 1, 10, 100, 1000}.The example parameter analysis results on DBLP and Amazon interms of modularity are shown in , where different settingsof {, } would not significantly affect the quality of PRoCD. Theinfluence of {, } may also differ from refinement methods. Forinstance, LPA is more sensitive to the parameter setting, comparedwith InfoMap and Locale.Due to space limit, we leave further parameter analysis about thenumber of sampled node pair (see Algorithm 1) in Appendix D.",
  "CONCLUSION": "In this paper, we explored the potential of DGL to achieve a bet-ter trade-off between the quality and efficiency of -agnostic CD.By reformulating this task as the binary node pair classification,a simple yet effective PRoCD method was proposed. It follows anovel pre-training & refinement paradigm, including the (i) offlinepre-training on small synthetic graphs with various topology prop-erties and high-quality ground-truth as well as the (ii) online gener-alization to and (iii) refinement on large real graphs without addi-tional model optimization. Extensive experiments demonstrate thatPRoCD, combined with different refinement methods, can achievehigher inference efficiency without significant quality degradationon public real graphs with various scales. Some possible futureresearch directions are summarized in Appendix E.",
  "Rosa I Arriaga and Santosh Vempala. 2006. An algorithmic theory of learning:Robust concepts and random projection. Machine Learning 63 (2006), 161182": "Ayan Kumar Bhowmick, Koushik Meneni, Maximilien Danisch, Jean-Loup Guil-laume, and Bivas Mitra. 2020. Louvainne: Hierarchical louvain method for highquality and scalable network embedding. In WSDM. 4351. Vincent D Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefeb-vre. 2008. Fast unfolding of communities in large networks. Journal of StatisticalMechanics: Theory & Experiment 2008, 10 (2008), P10008. Yuxuan Cao, Jiarong Xu, Carl J. Yang, Jiaan Wang, Yunchao Zhang, ChunpingWang, Lei Chen, and Yang Yang. 2023. When to Pre-Train Graph Neural Net-works? From Data Generation Perspective!. In SIGKDD. 142153.",
  "22generate an edge (, ) for E via Poisson( )": "degmin and degmax are the minimum and maximum node degreesof a graph; is a parameter to control the power-law distributionsof node degrees with deg() {/degmax=degmin } (degmin degmax); is a ratio between the number of within-communityand between-community edges; is a parameter to control theheterogeneity of community size, with the size of each communityfollowing Dirichlet(10/).For each synthetic graph G, suppose there are nodes par-titioned into communities, with and sampled from thecorresponding distributions in . Let C( ) = {C( )1, , C( ) }denote the community ground-truth of G. We also sample (, , )from corresponding distributions in , which define the topol-ogy properties of G. Algorithm 4 summarizes the procedure of gen-erating a graph G via DC-SBM, based on the parameter settings of",
  "BDETAILED COMPLEXITY ANALYSIS": "For each graph G, let , , and be the number of nodes, numberof edges, and embedding dimensionality, respectively. By usingthe efficient sparse-dense matrix multiplication, the complexity ofthe feature extraction defined in (3) is O( + 2). Similarly, thecomplexity of one FFP of the embedding encoder described by (4)is no more than O( + 2).To derive a feasible CD result via Algorithm 1, the complexitiesof constructing the node pair set P, one FFP of the binary classifierto derive auxiliary graph G, and extracting connected componentsvia DFS/BFS are O( +), O(( +)2), and O( + ), where is the number of randomly sampled node pairs; := | E| is thenumber of edges in the auxiliary graph G.In summary, the complexity of online generalization is O(( +2)+(+2)+(+)+(+)2+(+ )) = O((+)2),where we assume that , and .",
  "CDETAILED EXPERIMENT SETUPC.1Datasets": "In the six real datasets (see ), Protein2 was collectedbased on the protein-protein interactions in the BioGRID repository.ArXiv3 and DBLP4 are public paper citation and collabora-tion graphs extracted from ArXiv and DBLP, respectively. Amazon5 was collected by crawling the product co-purchasing relationsfrom Amazon, while Youtube6 was constructed based on thefriendship relations of Youtube. RoadCA7 describes a roadnetwork in California. To pre-process Protein, we abstracted eachprotein as a unique node and constructed graph topology basedon corresponding protein-protein interactions. Since there are mul-tiple connected components in the extracted graph, we extractedthe largest component for evaluation. Moreover, we used originalformats of the remaining datasets provided by their sources.",
  "C.2Environments & Implementation Details": "All the experiments were conducted on a server with one AMDEPYC 7742 64-Core CPU, 512GB main memory, and one 80GB mem-ory GPU. We used Python 3.7 to implement PRoCD, where the fea-ture extraction module (3), embedding encoder (4), and binary nodepair classifier (6) were implemented via PyTorch 1.10.0. Hence, thefeature extraction and FFP of PRoCD can be speeded up via the GPU.The efficient function scipy.sparse.csgraph.connected_componentswas used to extract connected components in Algorithm 1 to derivea feasible CD result. For all the baselines, we adopted their officialimplementations and tuned parameters to report the best quality.To ensure the fairness of comparison, the embedding dimensional-ity of all the embedding-based methods (i.e., LouvainNE, SketchNE,RaftGP, ICD, and PRoCD) was set to be the same (i.e., 64).",
  "C.3Parameter Settings & Layer Configurations": "The recommended parameter settings and layer configurationsof PRoCD are depicted in , where and are hyper-parameters in the pre-training objective (9); and are the numberof epochs and learning rate of pre-training in Algorithm 2; isthe number of randomly sampled node pairs in Algorithm 1 forinference; is the embedding dimensionality; Feat, GNN, and BCare the numbers of MLP layers in (3), GNN layers in (4), and MLPlayers in (6). Given an MLP, let u and u[] be the input and out-put of the -th perceptron layer. The MLP in the feature extractionmodule (3) is defined as",
  "DFURTHER EXPERIMENT RESULTS": "We also conducted parameter analysis for the number of samplednode pairs (in Algorithm 1) for inference, where we set {0, 12, 52, 13, 53, 14, 54}. Results on DBLP and Amazon interms of modularity are shown in . In most cases except thevariant with InfoMap on Amazon, our PRoCD method is not sen-sitive to the setting of . To ensure the inference quality of theexception case, one needs a large setting of (e.g., 14).",
  "EFUTURE DIRECTIONS": "Some possible future directions are summarized as follows.Theoretical Analysis. This study empirically verified the po-tential of PRoCD to achieve a better trade-off between the qualityand efficiency of -agnostic. However, most existing graph pre-training techniques lack theoretical guarantee for their transferability w.r.t. different pre-training data and downstream tasks. Weplan to theoretically analyze the transfer ability of PRoCD fromsmall synthetic pre-training graphs to large real graphs, followingprevious analysis on random graph models (e.g., SBM ).Extension to Dynamic Graphs. In this study, we considered-agnostic CD on static graphs. CD on dynamic graphs is amore challenging setting, involving the variation of nodes, edges,and community membership over time. Usually, one can formulatea dynamic graph as a sequence of static snapshots with similarunderlying properties . PRoCD can be easily extended tohandle dynamic CD by directly generalizing the pre-trained modelto each snapshot for fast online inference, without any retraining.Further validation of this extension is also our next focus.Integration of Graph Attributes. As stated in , weconsidered CD without available graph attributes. A series of pre-vious studies have demonstrated the complicatedcorrelations between graph topology and attributes, which are in-herently heterogeneous information sources, for CD. On the onehand, the integration of attributes may provide complementarycharacteristics for better CD quality. On the other hand, it may alsoincorporate noise or inconsistent features that lead to unexpected qual-ity decline compared with those only considering one source. In ourfuture work, we will explore the adaptive integration of attributes forPRoCD. Concretely, when attributes match well with topology, weexpect that PRoCD can fully utilize the complementary informationof attributes to improve CD quality. When the two sources mismatchwith one another, we try to adaptively control the contribution ofattributes to avoid quality decline."
}