{
  "ABSTRACT": "Meta-learning is a practical learning paradigm to transfer skillsacross tasks from a few examples. Nevertheless, the existence oftask distribution shifts tends to weaken meta-learners generaliza-tion capability, particularly when the training task distribution isnaively hand-crafted or based on simple priors that fail to covercritical scenarios sufficiently. Here, we consider explicitly gener-ative modeling task distributions placed over task identifiers andpropose robustifying fast adaptation from adversarial training. Ourapproach, which can be interpreted as a model of a Stackelberggame, not only uncovers the task structure during problem-solvingfrom an explicit generative model but also theoretically increasesthe adaptation robustness in worst cases. This work has practicalimplications, particularly in dealing with task distribution shifts inmeta-learning, and contributes to theoretical insights in the field.Our method demonstrates its robustness in the presence of tasksubpopulation shifts and improved performance over SOTA base-lines in extensive experiments. The code is available at the projectsite (",
  "Meta Learning, Generative Models, Game Theory": "ACM Reference Format:Qi (Cheems) Wang, Yiqin Lv, Yixiu Mao, Yun Qu, Yi Xu, and Xiangyang Ji.2025. Robust Fast Adaptation from Adversarially Explicit Task DistributionGeneration. In Proceedings of the 31st ACM SIGKDD Conference on Knowl-edgeDiscovery and Data Mining V.1 (KDD 25), August 37, 2025, Toronto,ON, Canada. ACM, New York, NY, USA, 27 pages.",
  "These authors contributed equally to this research.Correspondence: ;": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for third-party components of this work must be honored.For all other uses, contact the owner/author(s).KDD 25, August 37, 2025, Toronto, ON, Canada 2025 Copyright held by the owner/author(s).ACM ISBN 979-8-4007-1245-6/25/08 Initial Task DistributionGenerated Task Distribution : Diagram of Generating Task Distribution as the Adversaryin Meta-Learning. Here, the initial task distribution 0() is a uni-form distribution governed by two task identifiers . Then, itis transformed into an explicit distribution () with the help ofnormalizing flows NF.",
  "INTRODUCTION": "Deep learning has made remarkable progress in the past decade,ranging from academics to industry . However, training deeplearning models is generally time-consuming, and the previouslytrained model on one task might perform poorly in deploymentwhen faced with unseen scenarios .Fortunately, meta-learning, or learning to learn, offers a schemeto generalize learned knowledge to unseen scenarios .The strategy is to leverage past experience, extract meta knowledgeas the prior, and utilize a few shot examples to transfer skills acrosstasks. This way, we can avoid learning from scratch and quicklyadapt the model to unseen but similar tasks, catering to practicaldemands, such as fast autonomous driving in diverse scenarios. Dueto these desirable properties, such a learning paradigm is playingan increasingly crucial role in building foundation models .Literature Challenges: Despite the promising adaptation per-formance in meta-learning, several concerns remain. Among them,the automatically task distribution design is under-explored andchallenging in the field, which closely relates to the models gener-alization evaluation .Overall, task identifiers configure the task, such as the topictype in the corpus for large language models , the ampli-tude and phase in sinusoid functions, or the degree of freedom inrobotic manipulators . Most existing studies adopt simpleprior, such as uniform distributions over task identifiers ,or hand-crafted distributions, which heavily rely on domain-specificknowledge difficult to acquire.Some scenarios even pose more realistic demands for task distri-butions. In testing an autopilot system, an ideal task distribution",
  "KDD 25, August 37, 2025, Toronto, ON, CanadaWang et al": "observation and action. For both dynamical systems, we use a random policy as the controller to interact with the environment to collecttransition samples. For the few-shot purpose, we sample 200 transitions from each Markov Decision Process as one batch and randomlysplit them into the support dataset (10-shot transitions) and the query dataset. In meta training, the meta-batch in iteration is 16, and themaximum iteration number is 500. In meta-testing phases, we randomly sample 100 tasks respectively from the initial and generated taskdistributions with each task one batch in evaluation, which results in .Meta Learning Continuous Control. The environment of reinforcement learning is mainly treated as a Markov decision process .And the meta RL is about the distribution over environments. We consider the navigation task to examine the performance of methods inreinforcement learning. The mission is to guide the robot, e.g., the point robot and the Ant from Mujoco, to move towards the target goalstep by step. Hence, the task identifier is the goal location (1,2). The agent performs 20 episode explorations to identify the environmentand enable inner policy gradient updates as fast adaptation. The environment information, such as transitions and rewards, is accessible at( Particularly, for the task distribution like U([0.5, 0.5] [0.5, 0.5]) in the pointrobot, we set the dark area in c as the sparse reward region, discounting the step-wise reward by 0.6 to area [0.25, 0.5] [0.25, 0.5]and 0.4 to aera [0.25, 0.5] [0.25, 0.5]. In meta training, the meta-batch in the iteration is 20 for the point robot, and 40 for Ant, and themodel is trained for up to 500 meta-iterations. In meta-testing, we randomly sampled 100 MDPs, specifically from the initial and generatedtask distributions. For each MDP, we run 20 episodes as the support dataset and compute the accumulated returns after fast adaptation.The general implementation details are retained the same as those in MAML ( andCAVIA (",
  "LITERATURE REVIEW": "The past few years have developed a large body of work on skilltransfer across tasks or domain generalization in different ways . This section overviews the field regarding meta-learningand adaptation robustness.Meta Learning. Meta learning is a learning paradigm that con-siders a distribution over tasks. The key is to pursue strategiesfor leveraging past experiences and distilling extracted knowl-edge into unseen tasks with a few shots of examples .Currently, there are various families of meta-learning methods.The optimization-based ones, like model agnostic meta-learning(MAML) and its extensions , aim at finding a goodmeta-initialization of model parameters for adapting to all tasks viagradient descent. The deep metrics methods optimize the task repre-sentation in a metric space and are superior in few-shot image classi-fication tasks . Typical context-based methods, e.g., neural processes (NPs) and variants ,constitute the deep latent variable model as the stochastic processto accomplish tasks. Besides, memory-augmented networks ,hyper-networks , and so forth are designed for meta-learningpurposes.Robustness in Meta Learning. In most previous work, thetask distribution is fixed in the training set-up. In order to robustifythe fast adaptation performance, a couple of learning strategies orprinciples emerge. Increasing the robustness to worst cases is acommonly seen consideration in adaptation, and these scenariosinclude input noise, parameter perturbation, and task distributions. To alleviate the effects of adversarial exam-ples in few-shot image classification, Goldblum et al. meta-trainthe model in an adversarial way. To handle the distribution mis-match between training and testing tasks, Zhang et al. adoptthe adaptive risk minimization principle to enable fast adaptation.Wang et al. propose to optimize the expected tail risk in meta-learning and witness the increase of robustness in proportionalworst cases. Ours is a variant of a distributionally robust frame-work , and we seek equilibrium for fast adaptation.Task Distribution Studies in Meta Learning. Task distribu-tions are directly related to the generalization capability of meta-learning models, attracting increasing attention recently. Aiming toalleviate task overfitting, Murty et al. , Ni et al. , Rajendranet al. , Yao et al. enrich the task space with augmentationtechniques. Task relatedness can improve generalization acrosstasks, Fifty et al. devise an efficient strategy to group tasks inmulti-task training. In , neural task samplers are developedto schedule the probability of task sampling in the context of few-shot classification. To increase the fidelity of generated tasks, Wuet al. adopt the task representation model and constructs theup-sampling network for meta-training task augmentation. To re-duce the required tasks, take the task interpolation strategyand shows that the interpolation strategy outperforms the standardset-up. Distinguished from the above, this work takes more interestin explicitly understanding task identifier structures concerninglearning performance and cares about fast adaptation robustnessunder subpopulation shift constraints. Optimizing the task distribu-tion might reserve the potential to improve generative performancein large models .",
  "PRELIMINARIES": "Notation. Throughout this paper, we use () to denote the taskdistribution with T the task domain. Here, D represents the metadataset with a sampled task . With the model parameter domain and the support/query dataset construction, e.g., D = D D ,the risk function in meta-learning is a real-value function L : T R.As an example, D consists of data points {(,)}+=1 in fewshot regression, and it is mostly split into the support dataset Dfor fast adaptation and query dataset D for evaluation.",
  "min E ()L(D , D ;)(1)": "Here, refers to the meta-learning model parameters for metaknowledge and fast adaptation. The risk function depends on spe-cific meta-learning methods. For example, in MAML, the formcan be L(D , D ;) := L(D ; L(D ;)) in regression,where the gradient update with the learning rate in the bracketreflects fast adaptation.Distributionally Robust Meta Learning Optimization Ob-jective. Recently, tail risk minimization has been adopted for meta-learning, effectively alleviating the effects towards fast adaptationin task distribution shifts . In detail, we can express the op-timization objective as Eq. (2) in the presence of the constraineddistribution (;), which characterizes the (1 ) proportional-dependent worst cases in the task space.",
  "Two-Player Stackelberg Game": "Before detailing our approach, it is necessary to describe elementsin a two-player, non-cooperative Stackelberg game .Let us assume two competitive players are involved in the game := {P1, P2}, { , }, J (, ), where the meta learneras the leader P1 makes a decision first in the domain while thedistribution adversary as the follower P2 tries to deteriorate theleader decisions utility in the domain . We refer to J (, ) as thecontinuous risk function of the leader P1, and that of the followerP2 corresponds to the negative form J (, ). Without loss ofgenerality, all the players are rational and try to minimize riskfunctions in the game.",
  "As part of an indispensable element in meta-learning, the taskdistribution is mostly set to be uniform or manually designed from": "the heuristics. Such a setup hardly identifies a subpopulation oftasks that are tough to resolve in practice and fails to handle taskdistribution shifts.In contrast, this paper considers an explicit task distribution tocapture along with the learning progress and then automaticallycreates task distribution shifts for the meta-learner to adapt robustly.Our framework can be categorized as curriculum learning , butthere places a constraint over the distribution shift in optimization.Adversarially Task Robust Optimization with DistributionShift Constraints. Now, we translate the meta-learning problem,namely generative task distributions for robust adaptation, into amin-max optimization problem:",
  "(6)": "where the constant terms, e.g., R+ and E0()ln0()areeliminated.As previously mentioned, the role of the distribution adversaryattempts to transform the initial task distribution into one thatraises challenging task proposals with higher probability. Such asetup drives the evolution of task distributions via adaptively shiftingtask sampling chance under constraints, which can be more crucialfor generalization across risky scenarios. The term 0() ()inside Eq. (5) works as regularization to avoid the modecollapse in the generative task distribution. In , the goal ofthe meta learner retains that of traditional meta-learning, while thedistribution adversary continually generates the task distributionshifts along optimization processes.",
  "Distribution Shifts": ": Diagram of Adversarially Task Robust Meta Learning. The proposed framework consists of two players, the distribution adversaryand the meta player, in the game of meta-learning. On the left side of the figure: the distribution adversary seeks to transform the distributionfrom an initial task distribution, e.g., N(0, ) or U, via the neural network parameterized by with the purpose of deteriorating metaplayers fast adaptation performance. On the right side of the figure: the meta player parameterized by attempts to learn robust strategies forfast adaptation in sampled worst-case tasks (MAML algorithm as an illustration).",
  "where 1 and 2 are respectively a set encoder and the decoder net-works": "Here, we take two typical methods, e.g., MAML and CNP, to illustrate the meta learner within the adversarially taskrobust framework, see Examples 1/2 for details.Explicit Task Distribution Adversary Construction withNormalizing Flows. Learning to transform the task distributionis treated as a generative process: : T T R in this paper.Admittedly, there already exist a collection of generative models toachieve the goal of generating task distributions, e.g., variationalautoencoders , generative adversarial networks , andnormalizing flows .Among them, we propose to utilize the normalizing flow toachieve due to its tractability of the exact log-likelihood, flexibilityin capturing complicated distributions, and a direct understanding oftask structures. The basic idea of normalizing flows is to transforma simple distribution into a more flexible distribution with a seriesof invertible mappings G = {}=1, where : T T R indi-cates the smooth invertible mapping. We refer to these mappingsimplemented in the neural networks as NN afterward. Specifically,with the base distribution 0() and a task sample 0, the model",
  "Solution Concept & Explanations": "This work separates players regarding the decision-making order,and the optimization procedure is no longer a simultaneous game.The nature of Stackelberg game enables us to technically expressthe studied asymmetric bi-level optimization problem as:min J (, ), s.t. S()(11) with the-dependent conditional subset S() := { |J (, ) max J (, )}. This suggests the variables and are entan-gled in optimization.Moreover, we can define the resulting equilibrium as a localminimax point in adversarially task robust meta-learning, dueto the non-convex optimization practice.",
  "Strategies for Finding Equilibrium": "Given the previously formulated optimization objective, we pro-pose to approach it with the help of estimated stochastic gradients.As noticed, the involvement of adaptive expectation term ()requires extra considerations in optimization.Best Response Approximation. Given two players with com-pletely distinguished purposes, the commonly used strategy tocompute the equilibrium is the Best Response (BR), which means:",
  "+1 + 2J (+1, ).(14b)": "This can be viewed as the gradient approximation for the BRstrategy, which leads to at least a local Stackelberg equilibrium forthe considered minimax problem .Stochastic Gradient Estimates & Variance Reduction. Ad-dressing the game-theoretic problem is non-trivial especially whenit relates to distributions. A commonly-used method is to performthe sample average approximation w.r.t. Eq. (14). It iteratively up-dates the parameters of the meta player and the distribution adver-sary to approximate the saddle point.More specifically, we can have the Monte Carlo estimates of thestochastic gradients for the leader P1:",
  "(16)": "where the particle () denotes the task sampled from thegenerative task distribution, and means the particle sampledfrom the initial task distribution to enable NN () = .As validated in , the score estimator is an unbiased estimateof J (, ). However, such a gradient estimator in Eq. (16) mostlyexhibits higher variances, which weakens the stability of trainingprocesses. To reduce the variances, we utilize the commonly-used",
  "EXPERIMENTS": "Previous sections recast the adversarially task robust meta-learningto a Stackelberg game, specify the equilibrium, and analyze theo-retical properties in distribution generation. This section focuseson the evaluation, and baselines constructed from typical risk mini-mization principles are reported in . These include vanillaMAML , DRO-MAML , TR-MAML , DR-MAML , andAR-MAML (ours).Technically, we mainly answer the following Research Ques-tions (RQs):",
  "(2) How does the type of the initial task distribution influence theperformance of resulting solutions?": "(3) Can generative modeling the task distribution discover mean-ingful task structures and afford interpretability?Implementation & Examination Setup. As our approach is agnos-tic to meta-learning methods, we mainly employ AR-MAML as theimplementation of this work. Concerning the meta testing distribu-tion, tasks are from the initial task distribution and the adversarialtask distribution, respectively. The latter corresponds to the gener-ated task distribution under shift constraints after convergence.Evaluation Metrics. Here, we use both the average risk and con-ditional value at risk (CVaR) in evaluation metrics, where CVaRcan be viewed as the worst group performance in .",
  "Benchmarks": "We consider the few-shot synthetic regression, system identifi-cation, and meta reinforcement learning to test fast adaptationrobustness with typical baselines. Notably, the task is specified bythe generated task identifiers as shown in .Synthetic Regression. The same as that in , we conduct exper-iments in sinusoid functions. The goal is to uncover the function () = sin( ) with -shot randomly sampled function points.And the task identifiers are the amplitude and phase .System Identification. Here, we take the Acrobot System and the Pendulum System to perform system identification. Inthe Acrobot System, we generate different dynamical systems astasks by varying masses of two pendulums. And the task identifiersare the pendulum mass parameters 1 and 2. In the PendulumSystem, the system dynamics are distinguished by varying the massand the length of the pendulum. And the task identifiers are themass parameter and the length parameter . For both bench-marks, we collect the dataset of state transitions with a completerandom policy to interact with sampled environments. The goalis to predict state transitions conditioned on randomly sampledcontext transitions from an unknown dynamical system.Meta Reinforcement Learning. We evaluate the role of task dis-tributions in meta-learning continuous control. In detail, the PointRobot in and the Ant-Pos Robot in Mujoco are includedas navigation environments. We respectively vary goal/positionlocations as task identifiers within a designed range to generatediverse tasks. The goal is to seek a policy that guides the robot to thetarget location with a few episodes derived from an environment.We refer the reader to Appendix I for set-ups, hyper-parameterconfigurations and additional experimental results.",
  "Empirical Result Analysis": "Here, we report the experimental results, perform analysis andanswer the raised RQs (1)/(2).Overall Performance: shows that AR-MAML mostly out-performs others in the adversarial distribution, seldom sacrificingperformance in the initial distribution. Similar to observations in, task distributionally robust optimization methods, like DR-MAML and DRO-MAML, not only retain robustness advantageon shifted distribution but also sometimes boost average perfor-mance on the initial distribution. Cases with two types of initialtask distributions (Uniform/Normal) come to similar conclusions onaverage and CVaR performance. Figures 4/5 show the meta rein-forcement learning results for Point Robot and Ant Pos navigationtasks. AR-MAML exhibits similar superiority on both continuouscontrol benchmarks compared to baselines. MAMLDR-MAMLDRO-MAMLAR-MAML",
  "Point Robot": "InitialAdversarial MAMLDR-MAMLDRO-MAMLAR-MAML CVaR_0.5 InitialAdversarial : Meta Testing Returns in Point Robot Navigation Tasks (4runs). The charts report average and CVaR returns with = 0.5in initial and adversarial distributions, with standard error barsindicated by black vertical lines. The higher, the better. Multiple Tail Risk Robustness: Note that CVaR metrics imply themodels robustness under the subpopulation shift. reportsCVaR values with various confidence values on pendulum systemidentification. The AR-MAMLs merits in handling the proportionalworst cases are consistent across diverse levels. We also illustrateand include these statistics on other benchmarks in Appendix J.Moreover, as suggested in , a robust learner seldom encountersa performance gap between a standard (initial) test set and a test set MAMLDR-MAMLDRO-MAMLAR-MAML",
  "Adversarial": "MAMLTR-MAMLDR-MAMLDRO-MAMLAR-MAML : CVaR MSEs with Various Confidence Level . Sinusoid-U/N denotes Uniform/Normal as the initial distribution type. The plotsreport meta testing CVaR MSEs in initial and adversarial distributions with standard error bars in shadow regions. Impacts of Shift Distribution Constraints. Still, by varying the Lagrange multipliers , we include the learned task structures and themeta-testing results in Figures 16/17/18. For sinusoid and acrobot cases, empirical findings are similar to those in the main paper. In pointrobot navigation, we observe high probability density regions significantly change, and even the meta-testing results are improved in theinitial task distribution with increasing values.",
  "Task Structure Analysis": "In response to RQ (3), we turn to the analysis of the learned dis-tribution adversary. As a result, we visualize the adversarial taskprobability density.Explicit Task Distribution: As displayed in , our approachenables the discovery of explicit task structures regarding problem-solving. The general learned patterns seem to be regardless of theinitial task distributions. In sinusoid regression, more probabilitymass is allocated in the region with [3.0, 5.0] [0.0, 1.0], whichreveals more difficulties in adaptation with larger amplitude de-scriptors. For the Pendulum, the distribution adversary assigns lessprobability mass to two corner regions, implying that the combi-nation of higher masses and longer pendulums or lower massesand shorter pendulums is easier to predict. Similar phenomena areobserved in mass combinations of Acrobat systems. Consistently,the existence of constraint decreases all task distribution entropiesto a certain level, which we report in Appendix I. Though such a de-crease brings more concentration on some task subsets, AR-MAML still probably fails to cover other challenging combinations in modecollapse.Initial Task Distributions Influence on Structures: Comparingthe top and the bottom of , we notice that the uniformand the normal initial distribution results in similar patterns afternormalizing flows transformations on separate benchmarks. Thenormal initial distribution can be transformed into smooth onesand captures high-density regions around centroids.",
  "Length": "= 0.2 0.00.10.2 0.20 0.21 0.22 0.23 0.24 0.25 0.26 Average MSEs InitialAdversarial 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.0 0.1 0.2 0.3 0.4 0.0 0.1 0.2 0.3 0.4 Acrobot-U : The first three plots show adversarial task probability distributions with varying Lagrange multipliers in the acrobot-U benchmark.The last plot depicts meta testing MSEs across different values of .",
  "Other Investigations": "Here, we conduct additional investigations through the followingperspectives.Impacts of Shift Distribution Constraints: Our studied frameworkallows the task distribution to shift at a certain level. In Eq. (6),larger values tend to cause the generated distribution to collapseinto the initial distribution. Consequently, we empirically test thenaive and severe adversarial training, e.g., setting = {0.0, 0.1, 0.2}on sinusoid regression. As displayed in , the generateddistribution with = 0.0 suffers from severe mode collapse, merelycovering diagonal regions in the task space. Such a curse is allevi-ated with increasing values. In , the meta learner, afterheavy distribution shifts, catastrophically fails to generalize wellin the initial distribution, illustrating higher adaptation risks in = 0.0.",
  ": Adversarial Task Probability Distribution on SinusoidRegression with Various Lagrange Multipliers": "Compatibility with Other Meta-learning Methods: Besides the AR-MAML, we also check the effect of adversarially task robust trainingwith other meta-learning methods. Here, AR-CNP in Example 2is employed in the evaluation. Take the sinusoid regression as anexample. observes comparable performance between AR-CNP and DR-CNP on the initial task distribution, while results onthe adversarial task distribution uncover a significant advantage",
  "CONCLUSIONS": "Discussions & Society Impacts. This work develops a game-theoretical approach for generating explicit task distributions inan adversarial way and contributes to theoretical understandings.In extensive scenarios, our approach improves adaptation robust-ness in constrained distribution shifts and enables the discovery ofinterpretable task structures in optimization.Limitations & Future Work. The task distribution in this workrelies on the task identifier, which can be inaccessible in somecases, e.g., few-shot classification. Also, the adopted strategy toderive the game solution is approximate, leading to suboptimalityin optimization. Hence, future efforts can be made to overcomethese limitations and facilitate robust adaptation in applications. This work is funded by National Natural Science Foundation ofChina (NSFC) with the Number # 62306326 and # 62495091. And wethank Dong Liang, Yuhang Jiang, Chen Chen, Daming Shi, otheranonymous reviewers, and KDD2025 Area Chairs Prof. Yan Liu andProf. Auroop R Ganguly for suggestions and helpful discussions.",
  "Kelsey Allen, Evan Shelhamer, Hanul Shin, and Joshua Tenenbaum. 2019. Infinitemixture prototypes for few-shot learning. In International Conference on MachineLearning. PMLR, 232241": "Timothe Anne, Jack Wilkinson, and Zhibin Li. 2021. Meta-learning for fastadaptive locomotion with uncertainties in environments and robot dynamics. In2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS).IEEE, 45684575. Yutong Bai, Xinyang Geng, Karttikeya Mangalam, Amir Bar, Alan Yuille, TrevorDarrell, Jitendra Malik, and Alexei A Efros. 2023. Sequential Modeling EnablesScalable Learning for Large Vision Models. arXiv:2312.00785 [cs.CV]",
  "Yoshua Bengio, Jrme Louradour, Ronan Collobert, and Jason Weston. 2009.Curriculum learning. In Proceedings of the 26th annual international conferenceon machine learning. 4148": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shot learners. Advances in neuralinformation processing systems 33 (2020), 18771901. Haoang Chi, He Li, Wenjing Yang, Feng Liu, Long Lan, Xiaoguang Ren, TongliangLiu, and Bo Han. 2024. Unveiling Causal Reasoning in Large Language Models:Reality or Mirage?. In Neural Information Processing Systems. Haoang Chi, Feng Liu, Wenjing Yang, Long Lan, Tongliang Liu, Bo Han, WilliamCheung, and James Kwok. 2021. TOHAN: A one-step approach towards few-shot hypothesis adaptation. In Neural Information Processing Systems, Vol. 34.2097020982. Haoang Chi, Feng Liu, Wenjing Yang, Long Lan, Tongliang Liu, Bo Han, GangNiu, Mingyuan Zhou, and Masashi Sugiyama. 2022. Meta Discovery: Learning toDiscover Novel Classes given Very Limited Data. In International Conference onLearning Representations.",
  "Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and PieterAbbeel. 2016. Rl2: Fast reinforcement learning via slow reinforcement learning.arXiv preprint arXiv:1611.02779 (2016)": "Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. 2020. On the conver-gence theory of gradient-based model-agnostic meta-learning algorithms. InInternational Conference on Artificial Intelligence and Statistics. PMLR, 10821092. Bernard Faverjon and Pierre Tournassoud. 1987. A local based approach forpath planning of manipulators with a high number of degrees of freedom. InProceedings. 1987 IEEE international conference on robotics and automation, Vol. 4.IEEE, 11521159. Chris Fifty, Ehsan Amid, Zhe Zhao, Tianhe Yu, Rohan Anil, and Chelsea Finn.2021. Efficiently identifying task groupings for multi-task learning. Advances inNeural Information Processing Systems 34 (2021), 2750327516.",
  "Micah Goldblum, Liam Fowl, and Tom Goldstein. 2019. Robust few-shot learningwith adversarially queried meta-learners. (2019)": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020. Generative adversarialnetworks. Commun. ACM 63, 11 (2020), 139144. Jonathan Gordon, Wessel P Bruinsma, Andrew YK Foong, James Requeima, YannDubois, and Richard E Turner. 2019. Convolutional Conditional Neural Processes.In International Conference on Learning Representations.",
  "Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep learning. nature521, 7553 (2015), 436444": "Kimin Lee, Younggyo Seo, Seunghyun Lee, Honglak Lee, and Jinwoo Shin. 2020.Context-aware dynamics model for generalization in model-based reinforcementlearning. In International Conference on Machine Learning. PMLR, 57575766. Seanie Lee, Bruno Andreis, Kenji Kawaguchi, Juho Lee, and Sung Ju Hwang. 2022.Set-based meta-interpolation for few-task meta-learning. Advances in NeuralInformation Processing Systems 35 (2022), 67756788.",
  "Wenbin Li, Lei Wang, Xingxing Zhang, Lei Qi, Jing Huo, Yang Gao, and JieboLuo. 2022. Defensive Few-Shot Learning. IEEE Transactions on Pattern Analysisand Machine Intelligence 45, 5 (2022), 56495667": "Chenghao Liu, Zhihao Wang, Doyen Sahoo, Yuan Fang, Kun Zhang, andSteven CH Hoi. 2020. Adaptive task sampling for meta-learning. In ComputerVisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020,Proceedings, Part XVIII 16. Springer, 752769. Qi Liu, Tao Liu, Zihao Liu, Yanzhi Wang, Yier Jin, and Wujie Wen. 2018. Se-curity analysis and enhancement of model compressed deep learning systemsunder adversarial attacks. In 2018 23rd Asia and South Pacific Design AutomationConference (ASP-DAC). IEEE, 721726. Yixiu Mao, Hongchang Zhang, Chen Chen, Yi Xu, and Xiangyang Ji. 2023. Sup-ported trust region optimization for offline reinforcement learning. In Interna-tional Conference on Machine Learning. PMLR, 2382923851.",
  "Martin J Osborne et al. 2004. An introduction to game theory. Vol. 3. Oxforduniversity press New York": "Jack Parker-Holder, Minqi Jiang, Michael Dennis, Mikayel Samvelyan, JakobFoerster, Edward Grefenstette, and Tim Rocktschel. 2022. Evolving curriculawith regret-based environment design. In International Conference on MachineLearning. PMLR, 1747317498. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, GregoryChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019.Pytorch: An imperative style, high-performance deep learning library. Advances",
  "Aravind Rajeswaran, Chelsea Finn, Sham M Kakade, and Sergey Levine. 2019.Meta-learning with implicit gradients. Advances in neural information processingsystems 32 (2019)": "Davis Rempe, Jonah Philion, Leonidas J Guibas, Sanja Fidler, and Or Litany.2022. Generating useful accident-prone driving scenarios via a learned trafficprior. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. 1730517315. James Requeima, Jonathan Gordon, John Bronskill, Sebastian Nowozin, andRichard E Turner. 2019. Fast and flexible multi-task classification using con-ditional neural adaptive processes. Advances in Neural Information ProcessingSystems 32 (2019).",
  "Danilo Rezende and Shakir Mohamed. 2015. Variational inference with normaliz-ing flows. In International conference on machine learning. PMLR, 15301538": "Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. 2014. Stochas-tic backpropagation and approximate inference in deep generative models. InInternational conference on machine learning. PMLR, 12781286. Andrei A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu,Simon Osindero, and Raia Hadsell. 2018. Meta-Learning with Latent EmbeddingOptimization. In International Conference on Learning Representations. Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. 2019.Distributionally robust neural networks for group shifts: On the importanceof regularization for worst-case generalization. arXiv preprint arXiv:1911.08731(2019). Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and TimothyLillicrap. 2016. Meta-learning with memory-augmented neural networks. InInternational conference on machine learning. PMLR, 18421850.",
  "Richard S Sutton, Andrew G Barto, et al. 1998. Introduction to reinforcementlearning. Vol. 135. MIT press Cambridge": "Shuhan Tan, Kelvin Wong, Shenlong Wang, Sivabalan Manivasagam, MengyeRen, and Raquel Urtasun. 2021. Scenegen: Learning to generate realistic trafficscenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition. 892901. Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht,and Ludwig Schmidt. 2020. Measuring Robustness to Natural Distribution Shiftsin Image Classification. In Advances in Neural Information Processing Systems(NeurIPS). Sebastian Shenghong Tay, Chuan Sheng Foo, Urano Daisuke, Richalynn Leong,and Bryan Kian Hsiang Low. 2022. Efficient distributionally robust Bayesianoptimization with worst-case sensitivity. In International Conference on MachineLearning. PMLR, 2118021204.",
  "Lihe Yang, Wei Zhuo, Lei Qi, Yinghuan Shi, and Yang Gao. 2021. Mining latentclasses for few-shot segmentation. In Proceedings of the IEEE/CVF internationalconference on computer vision. 87218730": "Huaxiu Yao, Long-Kai Huang, Linjun Zhang, Ying Wei, Li Tian, James Zou,Junzhou Huang, et al. 2021. Improving generalization in meta-learning via taskaugmentation. In International Conference on Machine Learning. PMLR, 1188711897. Huaxiu Yao, Yu Wang, Ying Wei, Peilin Zhao, Mehrdad Mahdavi, Defu Lian, andChelsea Finn. 2021. Meta-learning with an adaptive task scheduler. Advances inNeural Information Processing Systems 34 (2021), 74977509. Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, ChelseaFinn, and Sergey Levine. 2020. Meta-world: A benchmark and evaluation formulti-task and meta reinforcement learning. In Conference on robot learning.PMLR, 10941100. Jesse Zhang, Brian Cheung, Chelsea Finn, Sergey Levine, and Dinesh Jayaraman.2020. Cautious adaptation for reinforcement learning in safety-critical settings.In International Conference on Machine Learning. PMLR, 1105511065. Marvin Zhang, Henrik Marklund, Nikita Dhawan, Abhishek Gupta, Sergey Levine,and Chelsea Finn. 2021. Adaptive risk minimization: Learning to adapt to domainshift. Advances in Neural Information Processing Systems 34 (2021), 2366423678.",
  "Contents": "Abstract11Introduction12Literature Review23Preliminaries23.1Problem Statement23.2Two-Player Stackelberg Game34Task Robust Meta Learning under Distribution Shift Constraints34.1Generate Task Distribution within A Game-Theoretic Framework34.2Solution Concept & Explanations44.3Strategies for Finding Equilibrium54.4Theoretical Analysis55Experiments65.1Benchmarks65.2Empirical Result Analysis75.3Task Structure Analysis85.4Other Investigations86Conclusions9Acknowledgments9References9Contents12AQuick Guideline for This Work13A.1Pseudo Code of the AR-MAML & AR-CNP13A.2Players Order in Decision-making14A.3Benefits of the Explicit Generative Modeling14A.4Set-up of Base Distributions14A.5Related Work Summary15BRobust Fast Adaptation Strategies in the Task Space15CStochastic Gradient Estimates16DTheoretical Understanding of Generating Task Distribution16EEquilibrium & Convergence Guarantee17E.1Existence of Stackelberg Equilibrium17E.2Convergence Guarantee17E.3Convergence Properties19FGeneralization Bound19F.1Importance Weighted Generalization Bound19F.2Formal Adversarial Generalization Bounds20GExplicit Generative Task Distributions20HEvolution of Entropies in Task Distributions20IExperimental Set-up & Implementation Details21I.1Meta Learning Benchmarks21I.2Modules in Pytorch22I.3Neural Architectures & Optimization25I.4Distribution Adversary Implementations25JAdditional Experimental Results25KPlatforms & Computational Tools25",
  "AQUICK GUIDELINE FOR THIS WORK": "Here, we include some guidelines for this work. Our focus is to explicitly generate the task distribution with adversarial training. Theuse of normalizing flows enables task structure discovery under the risk minimization principle. The theoretical understanding is fromthe Stackelberg game, together with some analyses. Allowing the task distribution to shift at an acceptable level is a promising topic inmeta-learning and can help improve robustness in the presence of worse cases. The following further complements these points.",
  "min max J (, ) max min J (, )": "The Remark 3 indicates the game is asymmetric w.r.t. the players decision-making order. This work abstracts the decision-makingprocess for the distribution adversary and the meta-learner in a sequential game. Hence, the order inevitably influences the solution concept.Actually, we take MAML as an example to implement the adversarially task robust meta-learning framework. As noted in Algorithm 1,updating the distribution adversarys parameter requires the evaluation of task-specific model parameters, which are not available in theinitialization. The traditional game theory no longer applies to our studied case, and simple counter-examples such as rock/paper/scissorsshow no equilibrium in practice. Hence, we pre-determine the order of decision-makers for implementation, as reported in the main paper.",
  "A.3Benefits of the Explicit Generative Modeling": "The use of flow-based generative models is beneficial, as the density distribution function is accessible in numerical analysis, allowing one tounderstand the behavior of the distribution adversary. Our training style is adversarial, and the equilibrium is approximately obtained in agradient update way. As the stationary point works for both the distribution adversary and the meta player, the resulting meta player is thedirect solution to robust fast adaptation. All of these have been validated from learned probability task densities and entropies. We treat thisas a side product of the method.",
  "A.4Set-up of Base Distributions": "This work does not create new synthetic tasks and only considers the regression and continuous control cases. Our suggestion of acomprehensive base distribution is designed to cover a wide range of scenarios, making it a practical solution in most default setups. Throughthe optimization of the distribution adversary, the meta-learner converges to a distribution that focuses on more challenging scenariosand lowers the importance of trivial cases. Even though enlarging the scope of the scenarios can result in additional computational costs,generative modeling of the task distribution captures more realistic feedback from adaptation performance. In other words, our study isbased on the hypothesis that when the task space is vast enough, the subpopulation shift is allowed under a certain constraint.",
  "A.5Related Work Summary": "As far as we know, robust fast adaptation is an underexplored research issue in the field. In principle, we include the latest SOTA andtypical SOTA methods to compare in this work as illustrated in . Note that in , the idea of increasing the robustness is tointroduce the task selector for risky tasks in implementations. Though there are some curriculum methods to reschedule thetask sampling probability along the learning, our setup focuses on (i) the semantics in the task identifiers to explicitly uncover the taskstructure from a Stackelberg game and (ii) the robust solution search under a distribution shift constraint. That is, ours requires capturingthe differentiation information from the task identifiers of tasks, such as masses and lengths in the pendulum system identification. Oursis agnostic to meta-learning methods and avoids task distribution mode collapse, which might happen in adversarial training withoutconstraints. These configuration and optimization differences drive us to include the risk minimization principle in the comparisons. : A Summary of Used Methods from Diverse Optimization Principles and Corresponding Properties in Meta-Learning. These areimplemented within the empirical/expected risk minimization (ERM), the distributionally robust risk minimization (DRM), and the adversariallytask robust risk minimization (ARM). Here, MAML works as the backbone method to illustrate the difference.",
  "R() := supGE ()L(D , D ;),(19)": "where G denotes a collection of uncertainty sets at a group level. When tasks of interest are finite, induces a probability measure ()over a subset, which we call the group in the background of meta-learning scenarios. The optimization step inside the bracket of Eq. (19)describes the selection of the worst group in fast adaptation performance. By minimizing the worst group risk, the fast adaptation robustnesscan be enhanced when confronted with the task distribution shift.Tail Risk Minimization (CVaR) for Meta Learning. This method is from the risk-averse perspective, and the tail risk measuredCVaR by is incorporated in stochastic programming. The induced meta training objective is written as follows:",
  "DTHEORETICAL UNDERSTANDING OF GENERATING TASK DISTRIBUTION": "Unlike previous curriculum learning or adversarial training in the task space, this work places a distribution shift constraint over the taskspace. Also, our setup tends to create a subpopulation shift under a trust region.Subpopulation Shift Constraint as the Regularization. The generated task distribution corresponds to the best response in deterio-rating the meta-learners performance under a tolerant level of the task distribution shift. Note that the proposed optimization objectiveJ (, ) includes a KL divergence term w.r.t. the generated task distribution.",
  "s.t. 0() () (24)": "Initial Task DistributionFeasible Generated Task Distributions : : Intuition of distribution shifts and Optimization Steps. Left is the initial task distribution, while the generated task distributionswithin the constraint of the distribution shift are illustrated on the right. Tolerant Region in distribution shifts. Next, we interpret the above-induced optimization objective. As exhibited in Eq. (24), thecondition constrains the generative task distribution () within a neighborhood of the initial task distribution 0(). This can be viewed asthe tolerant region of task distribution shifts, which means larger allows more severe distribution shifts concerning the initial task distribution.As for the role of the distribution adversary, it attempts to seek the task distribution that can handle the strongest task distribution shiftunder the allowed region, as displayed in .As mentioned in the main paper, the equivalent unconstrained version can be",
  ",(25)": "where the second penalty is to prevent the generated task distribution from uncontrollably diverging from the initial one. Hence, it encouragesthe exploration of crucial tasks in the broader scope while avoiding mode collapse from adversarial training. The larger the Lagrangemultiplier , the weaker the distribution shift in a generation. In this work, we expect that the tolerant distribution shift can cover a largerscope of shifted task distributions. Hence, is configured to be a small value as default.",
  "R = J (, ) = E ()L(D , D ;)+ E0()ln ().(27)": "Note that for the nonconvex-nonconcave min-max optimization problem, there is no general guarantee for the existence of saddle points.The solution concept in the Definition 3 requires no convexity w.r.t. the optimization objective and provides a weaker equilibrium, alsoreferred to as the global minimax point .With the Assumption 1 and the Definition 3, the global Stackelberg equilibrium always exists for the proposed min-max optimizationmin max J (, ). However, its exact search is NP-hard, and the stochastic optimization practically leads to the local Stackelbergequilibrium in the Definition 2.",
  "+1 + 2J (+1, ).(28b)": "when using the alternating GDA for solving the adaptively robust meta-learning. illustrates the iteration rules and steps.Let be the obtained (local) Stackelberg equilibrium, we denote the difference between the updated model parameter and theequilibrium by = . As the utility function J (, ) is Lipschitz smooth, we can perform linearization of J (, )and J (, ) round the resulting stationary point respectively as follows.",
  "FGENERALIZATION BOUND": "Note that the task distribution is adaptive and learnable, and we take interest in the generalization in the context of generative taskdistributions. It is challenging to perform direct analysis. Hence, we propose to exploit the importance of the weighting trick. To do so, wefirst recap the reweighted generalization bound from as Lemma 1. The sketch of proofs mainly consists of the importance-weightedgeneralization bound and estimates of the importance weights range.",
  "F.1Importance Weighted Generalization Bound": "Lemma 1 (Generalization Bound of Reweighted Risk ). Given a risk function L and arbitary hypothesis inside the hypothesisspace together with the pseudo-dimension C = Pdim({L(;) : }) in and the importance variable (), then the followinginequality holds with a probability 1 over samples {1,2, . . . , }:",
  "maxE0() [2()L2(D , D ;)], E 0() [2()L2(D , D ;)] 2sup T|L(D , D ;)|2(42)": "This formulates the generalization bound of meta learners under the learned adversarial task distribution.Theorem 2 (Generalization Bound with the Distribution Adversary) Given the pretrained normalizing flows {}=1, where ispresumed to be (, )-bi-Lipschitz, the pretrained meta learner from the hypothesis space together with the pseudo-dimensionC = Pdim({L(;) : }) in , we can derive the generalization bound when the initial task distribution is uniform.",
  ".(45)": "The above implies that the generated task distribution entropy is governed by the change of task identifiers in the probability measure of taskspace.Next is to provide the proof w.r.t. the above Remark.Proof: Given the initial distribution 0() and the sampled task 0, we know the transformation within the normalizing flows:",
  "Pos-Anttarget position (1,2)(1,2) [3.0, 3.0] [3.0, 3.0]U([3.0, 3.0] [3.0, 3.0])": "The following details the meta-learning dataset, and we also refer the reader to the List (1) for the preprocessing of data.Sinusoid Regression. In sinusoid regression, each task is equivalent to mapping the input to the output of a sine wave. Here, the taskidentifiers are the amplitude and the phase . Data points in regression are collected in the way: 10 data points are uniformly sampled fromthe interval [5.0, 5.0], coupled with the output = sin( ). These data points are divided into the support dataset (5-shot) and thequery dataset. As for the range of task identifiers and the types of initial distributions, please refer to . In meta-testing phases, werandomly sample 500 tasks from the initial task distribution and the generated task distribution to evaluate the performance, and this resultsin .System Identification. In the Acrobot System, angles and angular velocities characterize the state of an environment as .The goal is to identify the system dynamics, namely the transited state, after selecting Torque action from {1, 0, +1}. In the PendulumSystem, environment information can be found in the OpenAI gym. In detail, the observation is (cos, sin, ) with , and thecontinuous action range is [2.0, 2.0]. The torque is executed on the pendulum body, and the goal is to predict the dynamics given the",
  "As noted in the distribution adversary, the last layer is to normalize the range of the task identifiers into the pre-defined range, where themin-max normalization () =min": "maxmin is utilized. We consider the dimension of the task identifier to be . Let = normalize_tensor[:, :, 0] = [1,2, . . . ,] and = normalize_tensor[:, :, 1] = [1,2, . . . ,], the final layer of normalizing flows can be expressed as = (1) = (1) + , where indicates the transformed task and the resulting sequence after {}=1 is 0 1 . Then, the resulting log-probability follows the computation:",
  "I.3Neural Architectures & Optimization": "Our approach is meta-learning method agnostic, and the implementation is w.r.t. MAML and CNP in this work. As a result, werespectively describe the neural architectures in separate implementations and benchmarks. We do not vary the neural architecture of metalearners, and only risk minimization principles are studied in experiments.In the sinusoid regression and system identification benchmarks, we set the Lagrange multiplier = 0.2 to cover most shifted taskdistributions. All MAML-like methods employ a multilayer perceptron neural architecture. This architecture comprises three hidden layers,with each layer consisting of 128 hidden units. The activation function utilized in these models is the Rectified Linear Unit (ReLU). The innerloop utilizes the stochastic gradient descent (SGD) algorithm to perform fast adaptation on each task, while the outer loop employs themeta-optimizer Adam to update the initial parameters of the model. The learning rate for both the inner and outer loops is set to 1e-3. As forCNP-like methods, the encoder uses a three-layer MLP with 128-dimensional hidden units and outputs a 128-dimensional representation.The output representations are averaged to form a single representation. This aggregated representation is concatenated with the querydataset and passed through a two-layer MLP decoder. The optimizer used is Adam, with a learning rate of 1e-3.In the continuous control benchmark, we set = 0.0, and MAML serves as the underlying neural network architecture in our research.The agents reward is the negative squared distance to the goal. The agents are trained for one gradient update, employing policy gradientwith the generalized advantage estimation in the inner loop and trust-region policy optimization (TRPO) in the outer loop update.The learning rate for the one-step gradient update is set to 0.1.",
  "I.4Distribution Adversary Implementations": "The distribution adversary is implemented with the help of normalizing flows. For AR-MAML, we adopt the distribution adversary with theneural network as follows. We employ a 2-layer Planar flow to transform the initial distribution. In each layer, the dimension of thelatent variable is 2, and the activation function is leaky ReLU. In the last layer, the min-max normalization is used. We use Adam with acosine learning rate scheduler for the distribution adversary optimizer.",
  "JADDITIONAL EXPERIMENTAL RESULTS": "Tail Risk Robustness. Due to the page limit in the main paper, we present experimental results on tail risk robustness across variousconfidence levels for sinusoid regression and acrobot system identification. As illustrated in Figures 14/15, the advantage of AR-MAML issimilar to that in pendulum system identification. AR-MAML exhibits a more significant performance gain in the adversarial distributionthan in the initial distribution. 0.50.70.9",
  "Phase": "= 0.2 0.00.10.2 0.3 0.4 0.5 0.6 0.7 0.8 Average MSEs InitialAdversarial 0.00 0.05 0.10 0.15 0.20 0.00 0.02 0.04 0.06 0.08 0.10 0.00 0.02 0.04 0.06 0.08 0.10 Sinusoid-U : The first three plots show adversarial task probability distributions with varying Lagrange multipliers in the sinusoid-U benchmark.The last plot depicts meta testing MSEs across different values of . 0.40.60.81.01.21.41.6",
  "Average Returns": "InitialAdversarial 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 Point Robot-U : The first three plots show adversarial task probability distributions with varying Lagrange multipliers in the point robot-Ubenchmark. The last plot depicts meta testing MSEs across different values of ."
}