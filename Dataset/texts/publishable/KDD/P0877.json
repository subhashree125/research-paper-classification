{
  "ABSTRACT": "How can we automatically select an out-of-distribution (OOD) detection modelfor various underlying tasks? This is crucial for maintaining the reliability ofopen-world applications by identifying data distribution shifts, particularly in crit-ical domains such as online transactions, autonomous driving, and real-time pa-tient diagnosis. Despite the availability of numerous OOD detection methods, thechallenge of selecting an optimal model for diverse tasks remains largely under-explored, especially in scenarios lacking ground truth labels. In this work, we in-troduce MetaOOD, the first zero-shot, unsupervised framework that utilizes meta-learning to select an OOD detection model automatically. As a meta-learningapproach, MetaOOD leverages historical performance data of existing methodsacross various benchmark OOD datasets, enabling the effective selection of a suit-able model for new datasets without the need for labeled data at the test time. Toquantify task similarities more accurately, we introduce language model-basedembeddings that capture the distinctive OOD characteristics of both datasets anddetection models. Through extensive experimentation with 24 unique test datasetpairs to choose from among 11 OOD detection models, we demonstrate that theMetaOOD significantly outperforms existing methods and only brings marginaltime overhead. Our results, validated by Wilcoxon statistical tests, show thatMetaOOD surpasses a diverse group of 11 baselines, including established OODdetectors and advanced unsupervised selection methods.",
  "INTRODUCTION": "Out-of-distribution (OOD) detection is the process of identifying data points that deviate signifi-cantly from the distribution of the training data. This capability is essential for ensuring the relia-bility of machine learning (ML) models when they encounter new, unseen data (Yang et al., 2021).Common applications of OOD detection include safety-critical systems like autonomous vehicles(Filos et al., 2020) and medical diagnosis (Ulmer et al., 2020) to prevent erroneous predictions. No-tably, many OOD detection algorithms are unsupervised due to the high annotation cost, leveragingstatistical methods or reconstruction errors from models such as autoencoders to identify deviatedsamples without labeled OOD data (Yang et al., 2022; Dong et al., 2024). However, despite the vari-ety of OOD detection algorithms, each adepts at identifying different aspects of in-distribution (ID)and OOD data, there lacks a systematic method for choosing the best OOD detection model(s) un-der the unsupervised setting without labels. It is acknowledged that each OOD detection algorithmmight excel in specific scenarios but may not perform well universally due to the no-free-lunch the-orem (Wolpert & Macready, 1997). Moreover, without labels, it is difficult to objectively evaluateand compare the performance of different OOD detection models. Present Work. In the most related field, unsupervised outlier detection (OD) has benefitted sig-nificantly from meta-learning, with models like MetaOD (Zhao et al., 2021), ELECT (Zhao et al.,2022a), ADGym (Jiang et al., 2024), and HyPer (Ding et al., 2024) demonstrating notable advance-ments. By leveraging historical performance data, these methods estimate a models efficacy on newdatasets. However, they are not directly adaptable to OOD detection due to several key differences:",
  "datasets and models to their performance P; the online model selection (3.4) is shown at the bottomby transferring the meta-predictor f to predict the test data paired with OOD models for selection": "First, OD and OOD detection differ in their problem settings. OD models are trained on both nor-mal and outlier samples (Zhao et al., 2019; Ma et al., 2023), focusing on anomalies within a singledistribution (Zhao et al., 2022b; Zhao, 2024). In contrast, OOD detection involves training solelyon in-distribution (ID) data and aims to identify samples from entirely different distributions, oftenacross multiple datasets. Second, OOD detection tasks primarily deal images, presenting greatercomplexity compared to time series and tabular data in OD. Third, the embeddings used to measuredataset similarity in OD do not translate well to OOD contexts. Current OD embedding generationare largely heuristic and feature-specific (Zhao et al., 2021), inadequate for the diverse and complexnature of OOD datasets, especially when considering the different data modalities. Thus, addressingthese challenges with a tailored method for OOD model selection is crucial. Our Work. In this study, we introduce MetaOOD, the first unsupervised OOD 1 model selectionmethod that employs meta-learning. illustrates the overall workflow of the proposed ap-proach. The key rationale is that an OOD model is likely to perform well on a new dataset if itexcels in a similar historical dataset. As a meta-learning approach, we train a suite of OOD detec-tion models offline using a variety of carefully curated datasets to gauge their performance acrossdifferent scenarios during the meta-train phase (, top). When a new dataset arrives, we applyknowledge derived from historical data to select an appropriate OOD model (, bottom). Thisselection is based on the similarity between the new dataset and those used in the meta-train phase.To enhance the accuracy of this similarity assessment, we designed two versions of data embeddingsvia specialized OOD meta-features and language embeddings from language models. As our exper-iment shows, these language model-based embeddings effectively capture complex, nuanced datasetcharacteristics that traditional meta-features might miss. We summarize our technical contributions:",
  "First OOD Detection Model Selection Framework. We introduce the first meta-learning-basedframework for zero-shot OOD detection model selection without training and evaluation": "Specialized Embeddings for OOD Datasets and Models. We use language model-generatedfeatures to quantify the similarity among OOD tasks, facilitating a better understanding of OODcharacteristics and enhancing OOD detection model selection. Effectiveness. The proposed MetaOOD outperforms eleven well-known model selection methodsand unsupervised meta-learners on a testbed with 24 unique test data pairs. It is superior andstatistically better than all the baselines w.r.t. average rank, and efficient with small runtime.",
  "RELATED WORK": "2.1UNSUPERVISED OOD MODEL SELECTIONOOD detection faces the challenge of both OOD samples and OOD distributions being unknownduring the training phase (Hendrycks et al., 2019; Liang et al., 2018). Consequently, selectingmodels for OOD relies solely on in-distribution or training data (Lee et al., 2018b). Once deployedin an open-world setting, the OOD detector often handles a diverse range of test inputs originatingfrom various OOD distributions (Yang et al., 2021), and consequently, unsupervised methods areexpected to be more suitable for selecting OOD detection models (Liu et al., 2020b). Within theseunsupervised OOD detection models, some recent works focus on actively choosing sample-wisedetectors (e.g., choosing different groups of detectors for different input samples) (Xue et al., 2024),while it is not doing actual model selection at the single model level as this work tries to address. Most existing methods rely on trial-and-error or empirical heuristics. For instance, the simplest ap-proach may be not to actively select a model, but just use popular OOD detection models likeMaximum Softmax (Hendrycks & Gimpel, 2017) and ODIN (Liang et al., 2017). There are othersimple approaches; the confidence level of the ID data may be used as an indicator for OOD detec-tion model selection. However, such a simple method may lead to a low true positive rate (TPR)(Hendrycks & Gimpel, 2017). Another direction is similarity-based methods, where the model se-lection is based on the similarity or cluster of the dataset, which has also been utilized for algorithmrecommendation (Kadioglu et al., 2010; Nikolic et al., 2013; Xu et al., 2012; Misir & Sebag, 2017).We thus include these algorithms as our baselines in 4. 2.2SUPERVISED OOD MODEL SELECTIONThere has been considerable research into the use of supervised model selection techniques for effec-tively training and evaluating predictive models on labeled datasets (James et al., 2013; Hastie et al.,2015; Tibshirani, 1996). These methods are particularly valuable in scenarios where generalizationto unseen data is critical. Model selection in supervised learning encompasses a variety of strategies,including cross-validation, grid search, and performance metrics such as accuracy and F1 score toassess model efficacy. Randomized (Bergstra & Bengio, 2012), bandit-based (Li et al., 2017), andBayesian optimization (BO) techniques (Shahriari et al., 2015) are various SOTA approaches to hy-perparameter optimization and/or algorithm selection. However, it is notable that such methods donot apply to unsupervised OOD model selection, as ground truth values are absent.2.3REPRESENT DATASETS AND MODELS AS EMBEDDINGS FOR META-LEARNINGIn terms of data representation, especially when it comes to meta-learning, embeddings play an im-portant role to measure dataset/task similarity. Traditionally, computational-based meta-features areused as data representation in the meta-learning process (Vanschoren, 2018). More recently, moresophisticated, learning-based meta-features have been designed, as in dataset2vec (Jomaa et al.,2021) and HyPer (Ding et al., 2024). Additionally, there has been a notable integration of languageembeddings to encapsulate data features, aiming to enhance model understanding and data repre-sentation (Drori et al., 2019; Fang et al., 2024). While the first approach primarily relies on heuristicmethods and the latter can be hindered by the slow pace of model training, we adopt language em-beddings to represent data for this OOD detection model selection task. In this study, the traditionalmeta-feature approach is implemented as well for comparison and evaluation.",
  "M(x) =1if x Pin,0if x / Pin": "The OOD detection task involves training datasets Xtrain sampled fromthe in-distribution (ID) Pin, denoted Xtrain Pin, and testing datasetsXtest, which may contain both ID and OOD samples. The goal is to traina model M to classify if a new sample x Xtest belongs to Pin. Generally, a model is trained exclusively on the ID data to learn to perform tasks such as classifica-tion on this dataset. For example, one common approach is using the models confidence scores asthe method M to distinguish between ID and OOD data. When evaluating an OOD detector on Xtest, both ID and OOD test data are present to assess thedetectors capability to accurately distinguish between known and unknown data samples. There-fore, in this study, we structure the data into dataset pairs D = {Xtrain, Xtest}, each consisting of thetraining dataset with ID samples only and the test dataset with both ID and OOD samples.",
  "Preprint": "Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detectingout-of-distribution samples and adversarial attacks. In Advances in Neural Information Process-ing Systems, pp. 71677177, 2018b. Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyperband:A novel bandit-based approach to hyperparameter optimization. The Journal of Machine LearningResearch, 18(1):67656816, 2017.",
  "A collection of n historical (i.e., meta-train) OOD detection dataset pairs, Dtrain = {D1, . . . , Dn}with ground truth labels, i.e., D = {Xtrain, (Xtest, ytest)}": "Historical performance P of the pre-set model set M = {M1, . . . , Mm} (with m models), on themeta-train datasets. We refer to P Rnm as the performance matrix, where Pi,j correspondsto the j-th model Mjs performance on the i-th meta-train dataset Di. Our objective is to choose the best candidate OOD detection model M M, given a new pair ofdatasets Dnew = {Xnewtrain, Xnewtest } as input, where we have no ground truth labels ynewtest for evaluation. Problem 1 (OOD detection model selection) Given a new input dataset Dnew = {Xnewtrain, Xnewtest }(detect OOD samples on Xnewtest with only in-distribution data Xnewtrain and no labels), select a modelM M to employ on the new (test) task. The problem is similar to the OD model selection task such as MetaOD, ELECT, and HPOD (Zhao,2024). However, in contrast to OD model selection, which focuses solely on a single dataset, OODdetection model selection requires the consideration of both training and test datasets. This necessitystems from the need to take into account the similarities and differences among both datasets thatimpact pre-training on the ID data and the actual OOD detection on the test data, all crucial tomeasuring the inherent characteristics of OOD detection task. In a nutshell, our MetaOOD consists of two phases: (i) offline (meta-) training of the meta-learner onDtrain where the goal is to learn the mapping from OOD detection models performance on various(historical/meta) datasets, and (ii) online model selection that uses the meta-train information tochoose the model at test time for the new dataset Dnew. outlines the workflow and keyelements of MetaOOD, with the offline training phase shown in white and the online model selectionstage shown in grey. The details of the two phases are discussed in 3.3 and 3.4, respectively. 3.3OFFLINE META-TRAININGDuring the offline training, we generate embeddings for both dataset pair Dtrain and method M,and train the latent mapping from these embeddings to the performances P. The meta-learner cangeneralize and select the best-performing model for new, unseen datasets by learning the relation-ship between {Dtrain, M} P. Prior works have shown that performance mapping is empiricallylearnable, although imperfect, in related fields like outlier detection (Zhao, 2024). To predict the performance of the candidate model on a new dataset pair, we propose training ameta-predictor as a regression problem. The input to the meta-predictor consists of Emetai, Emodelj,corresponding to the embedding of the i-th dataset pair and the embedding of the j-th OOD detector.Dataset embedding of dataset pair D is denoted as Edata = (D), and method embedding extractedfrom M is denoted as Emodel = (M, M); we provide more details below on the embeddinggeneration in 3.3.1. Our goal is to train the meta-predictor f 1 to map the characteristics of thedatasets and the OOD detectors to their corresponding performance ranking across all historicaldataset pairs. The steps of the (offline) meta-train are shown in , top and Appx. Algo. 1.",
  "DATA AND MODEL EMBEDDINGS": "Data and model embeddings, as inputs of f, offer compact, standardized context about the data andmodels utilized in the meta-learning process. Instead of directly using data with arbitrary sizes, wehope to generate embeddings that can effectively represent the data and help develop models thatcan adapt rapidly to new tasks. In this work, we try two types of data and model embeddings: (1)classical (data) meta-features and (method) one-hot encoding embedding, and (2) language modelbased approach to get data and model embedding via language models. Classical Way: Data Embedding via Meta-Features and Model Embeddings via One-hot En-coding. Meta-features, or features of features, are attributes used in meta-learning, model selec-tion, and feature engineering to provide higher-level information about a dataset or its characteristics(Vanschoren, 2018). These include basic statistics, such as the mean and maximum values of thedata, as well as outputs or performance metrics from preliminary models tested on the dataset.Meta-features facilitate understanding the nuances of various learning tasks and guide the selectionor configuration of models for new tasks by drawing parallels with previously encountered tasks.In this work, we first create meta-features that capture OOD characteristics across different datasetpairs. These features aim to identify similarities between data in the meta-training database and testdata, enhancing the model selection process based on prior successful applications. To achieve this,we categorize the extracted meta-features into statistical and landmarker categories. Statistical meta-features capture the underlying data distributions characteristics, including vari-ance, skewness, covariance, and other relevant statistics of individual features and their combina-tions. For image datasets utilized in OOD, we incorporate meta-features that reflect pertinent imagecharacteristics (Aguiar et al., 2019), such as: (i) Color-based: Simple statistical measures from colorchannels; (ii) Border-based: Statistical measures obtained after applying border-detector filters; (iii)Histogram: Statistics from histograms of color and intensity; and (iv) Texture features: Values de-rived from an images texture, analyzed using the co-occurrence matrix and Fast Fourier Transform. Landmarker features summarize the performance of specific learning algorithms on a dataset, pro-viding quick and effective estimates of algorithm performance. These features, derived from evaluat-ing simple/fast models, offer a snapshot of dataset characteristics and approximate the performanceof more sophisticated models in specific tasks. For OOD detection, our landmarkers focus on fea-tures related to Softmax probability outputs that can be rapidly generated without model fitting.Detailed descriptions and a complete list of our OOD meta-features are in Appx. Table A. New Approach: Data Embedding via Language Models. Meta-features, often handcrafted andheuristic, have limitations in scalability and adaptability. This manual approach to feature engi-neering can be labor-intensive and may not capture all the nuanced relationships within OOD data,potentially affecting the efficacy of model selection. In contrast, leveraging language models offersa transformative approach for generating data embeddings (Peng et al., 2024). These models havebeen recently utilized to generate embeddings from textual descriptions of datasets and methods,capturing essential information that reflects the datasets and methods intrinsic properties due totheir comprehensive training (Drori et al., 2019). Given that language models are designed to pro-cess text inputs, we hypothesize that utilizing text descriptions of data as input to a language modelfor embedding generation could effectively encapsulate the inherent features of the data. In thisstudy, we experiment with various language models to produce the language embeddings.",
  "Contains images of 10 types of objects, including airplanes, cars,birds, cats, deer, dogs, frogs, horses, ships, and trucks.Eachimage is a small 32x32 RGB image": "For OOD model embeddings, we take the textual descriptions of methods from the Pytorch-OODlibrary (Kirchheim et al., 2022) as input. This includes detailed information about the OOD detector,such as its components, whether it needs fitting, and what is used as OOD indicator. An example ofthis is the description for the Openmax method (Bendale & Boult, 2016):",
  "detect outliers by fitting a Weibull function to the tail of thedistance distribution.The activation of the unknown class isused as the outlier score": "The complete lists of the datasets and methods used are listed in 4.1. In brief, this approach lever-ages language models natural language processing capabilities to transform qualitative descriptionsinto quantitative embeddings. Compared to the traditional meta-feature approach, it is more com-putationally efficient and has better generalization ability. Also, it is interesting to see whether wecould use language models to choose an OOD model directly. We provide all these analyses in 4. 3.4ONLINE MODEL SELECTIONIn the online model selection process, we generate the embeddings for the test dataset pair Dnew andreuse the model embeddings of M, apply the trained meta performance predictor f from the offlinestage to predict different OOD detection models performance, and select the model with the highestpredicted performance, as described in Eq. (2).",
  "M := arg maxMjMPnew,j,wherePnew,j = f(Emetanew , Emodelj)(2)": "Specifically, for a new dataset pair, we acquire the predicted relative performance ranking of differ-ent OOD detection methods using the trained f, and select the top-1 method1, as shown in Eq. (2).It is important to note that this procedure is zero-shot, and does not require any network training onthe test sample. The (online) model selection steps are given in , bottom and Appx. Algo. 2.",
  "EXPERIMENTS": "Our experiments answer the following research questions (RQ): RQ1 (4.3): How effective is theproposed MetaOOD in unsupervised OOD model selection in comparison to other leading base-lines? RQ2 (4.4.1): How do different design choices in MetaOOD impact its effectiveness? RQ3(4.4.2): How much time overhead/saving MetaOOD brings to OOD detection in general?",
  ": OOD detection models in this study": "We construct the ID-OOD dataset pair, andset the training and testing set as follows: (i)Training: CIFAR10 from ID and OOD fromthe classic OOD group shown above; and (ii)Testing: CIFAR100, ImageNet, and Fashion-MNIST from ID, and OOD from large-scaleOOD dataset group. The other ID datasets un-dergo a similar train test split, which resultsin 24 unique testing pairs. Our split guaran-tees that no dataset in the testing pair has beenseen/leaked during the meta-train process.",
  "Hardware. For consistency, all models are built using the pytorch-ood library (Kirchheim et al.,2022) on NVIDIA RTX 6000 Ada, 48 GB RAM workstations": "Training the meta-predictor f (see details in 3.3). In this work, we use an XGBoost (Chen &Guestrin, 2016) model as f due to its simplicity and expressiveness. Meanwhile, we use the data andmodel embeddings generated by the pre-trained BERT-based all-mpnet-base-v2 model by HuggingFace (Reimers & Gurevych, 2019). We provide more ablations on f in 4.4.1. Evaluation. To compare the performance of MetaOOD and baselines, we examine the performancerank 1 of the OOD detector chosen by each method among all the candidate detectors through aboxplot and the rank diagram (which is the average across all dataset pairs). Clearly, the best rank is1, and the worst is 12 (i.e., 11 baselines and MetaOOD) To compare our algorithm with a baseline,we employ the pairwise Wilcoxon rank test on performances across dataset pairs (significance levelp < 0.05). The full selection results are in Appendix Table B.4.2MODEL SELECTION BASELINESWe select the baselines following the literature in meta-learning for unsupervised model selection(Zhao et al., 2021; 2022a; Jiang et al., 2024; Park et al., 2023) with four categories: (a) No model selection or random selection: always employs either the ensemble of all the modelsor the same single model, or randomly selects a model: (1) Maximum Softmax Probability (MSP)(Hendrycks & Gimpel, 2017), as a popular OOD model, uses the maximum softmax score of a neuralnetworks output as threshold to identify whether an input belongs to the distribution the networkwas trained on. (2) ODIN (Liang et al., 2017) applies temperature scaling and small perturbationsto the input data, which helps to amplify the difference in softmax scores between ID and OODsamples. (3) Mega Ensemble (ME) averages OOD scores from all the models for a given dataset.As such, ME does not perform model selection but rather uses all the models. (4) Random Selection(Random) randomly selects a model from the pool of candidate models. (b) Simple meta-learners that do not involve optimization: (5) Global Best (GB) is the simplestmeta-learner that selects the model with the largest average performance across all meta-traindatasets. GB does not use any meta-features. (6) ISAC (Kadioglu et al., 2010) clusters the meta-train datasets based on meta-features. Given a new dataset pair, it identifies its closest cluster andselects the best model of the cluster. (7) ARGOSMART (AS) (Nikolic et al., 2013) finds the closestmeta-train dataset (1 nearest neighbor) to a given test dataset, based on meta-feature similarity, andselects the model with the best performance on the 1NN dataset. (c) Optimization-based meta-learners which involves a learning process: (9) ALORS (Misir &Sebag, 2017) factorizes the perf. matrix to extract latent factors and estimate perf. as the dot productof the latent factors. A regressor maps meta-features onto latent factors. (9) NCF (He et al.,2017) replaces the dot product used in ALORS with a more general neural architecture that predictsperformance by combining the linearity of matrix factorization and non-linearity of deep neuralnetworks. (10) MetaOOD 0 uses manually crafted heuristic meta-features, which consist of bothstatistical and landmarker features for dataset meta-features, along with one-hot encoding for modelembeddings, as discussed in 3.3.1. (d) Large language models (LLMs) as a model selector: (11) GPT-4o mini (Reimers & Gurevych,2019) is used as a zero-shot meta-selector. The dataset and method descriptions are directly providedto the LLM, allowing it to select the methods based on these descriptions. Note there is no meta-learning here. The details are presented in Appendix B.1.",
  "OVERALL RESULTS": "First, we present the rank distribution of the actual rank of the top-1 OOD detector chosen by eachmodel selection method across the 24 test data pairs in . To compare two model selectionalgorithms (e.g., ours with a baseline), we perform Wilcoxon rank test on the AUROC value ofthe top-1 models selected by our method and the baseline method, as shown in . Also, weshow the aggregated average rank plot in , where the average performance rank of the OODdetection model selected by each algorithm is shown. Here are the key findings of the results: 1. MetaOOD outperforms all baselines, as shown in , , and . As shown in , MetaOOD consistently performs well with small variance. On the aggregated level, the averagerank of MetaOOD surpasses all of the 11 baselines in . Moreover, MetaOOD has demon-strated statistically superior performance compared to all these baselines. Such enhancement inperformance is indicative of MetaOODs robust approach to tackling complex datasets while gener-ating stable performance. We credit its superiority to the combination of a meta-learning frameworkand the use of language models in embedding datasets and models. 2. Optimization-based meta-learning methods (i.e., MetaOOD, MetaOOD 0 and ALORS) per-form better in unsupervised model selection over other baselines. One reason is that meta-learningleverages prior experiences to adapt to new tasks. It optimizes the learning process by extractingcommon patterns and representations across different tasks, enhancing the models generalizationability. Meanwhile, the optimization process helps models converge to an optimal solution effi-ciently. Compared to simple meta-learners, such optimization-based methods take advantage of themeta-features for a better mapping of the model performance. For instance, AS and ISAC do notinvolve optimization and resemble finding the closest sample or closest sample cluster for choosingthe top-1 detector. They are inferior to MetaOOD as there is no optimization on the OOD modelperformance, but fully depends on the data embeddings for performance knowledge transfer.",
  ". The underperformance of no model selection and random selection baselines justifies theneed for OOD model selection. We analyze their performance below": "ME: Averaging the OOD scores of all models does not yield strong results, as demonstrated in and 2. This may stem from certain models consistently underperform across various datasets,and combining all models indiscriminately reduces overall effectiveness. While using selectiveensembles might offer improvements (Zhao & Hryniewicki, 2019), constructing ensembles fromnumerous models can be impractical due to high costs. In contrast, MetaOOD learns to make op-timal selections without constructing any models, allowing it to operate efficiently during testing. Random: According to and , random selection performs worse than all the meta-learners. This indicates that all the meta-learner baselines we chose do have some improvementscompared to random choice, and it is not advised to select an OOD detection model randomly. ODIN and MSP: As expected, a single method does not perform well across all datasets. Thisresult is not surprising since different OOD detectors emphasize various aspects of the datasets,and real-world datasets have diverse characteristics. Relying on a single approach tends to limitthe scope of solutions, making it difficult to capture the distribution shift among different datasets.",
  ": Ablation study on different choicesof meta-predictor f. Tree-based models havebetter performance": "presents ablation studies results on MetaOOD that uses different embedding variants with ffixed to XGBoost. The results indicate that language models are highly effective at generating em-beddings. MetaOOD, along with the other two language model-generated embeddings, demonstratesolid performance. We observed that the BERT-based embeddings perform slightly better than thosederived from LLMs. shows the dataset embeddings produced by different language models,with the embeddings reduced to 2D using t-SNE. Note that MetaOOD (BERT-based embedding) isslightly better to capture similar datasets (e.g. Texture and Textures) than decoder-based model em-beddings. This could be attributed to the decoder models causal attention, where the representationof a token is influenced only by preceding tokens, making it less effective for text embedding tasksas it restricts capturing information from the entire input sequence (BehnamGhader et al., 2024). CIFAR-10 CIFAR-100",
  ": Runtime of MetaOOD vs.selectedOOD detector. MetaOOD incurs a small overhead(difference shown with black arrows)": "One of the big advantages of MetaOOD overMetaOOD 0 is the fast dataset embedding gen-eration via language models, making the modelselection overhead negligible to the actual OODdetection fitting on the image datasets.Wedemonstrate the time of MetaOOD and the fit-ting of the selected OOD detector on selecteddataset pairs in . Notably, the languageembedding for both datasets and models andonline model selection via f just takes sec-onds to finish. Thus, the MetaOOD only bringsthe marginal cost to the entire OOD detectionpipeline while finding a top-performing model. 5CONCLUSION, LIMITATIONS, AND FUTURE DIRECTIONSIn this work, we introduced MetaOOD, the first unsupervised out-of-distribution (OOD) detectionmodel selection framework. This meta-learner utilizes an extensive pool of historical data on OODdetection models and dataset pairs, employing language model-based embeddings to enhance themodel selection process based on past performances. Despite its innovative approach, MetaOODdepends on the availability and quality of historical dataset pairs. This reliance may limit its effec-tiveness in scenarios where such data are sparse or less similar. Moreover, the current frameworkis designed for single-modality data, which restricts its application in highly diverse or multimodalenvironments. Looking ahead, we plan to broaden our testbed to include a more diverse group ofdatasets and models, thereby enhancing the meta-learning capabilities of MetaOOD. We also aim toextend MetaOOD to support top-k selection, offering a range of viable models rather than a singlerecommendation. Additionally, equipping MetaOOD with an uncertainty quantification mechanismwill enable it to output an I do not know response when applicable, further refining its utility incomplex scenarios where no suitable meta-train knowledge can be transferred.",
  "BROADER IMPACT STATEMENT, ETHICS STATEMENT, AND REPRODUCIBILITY": "Broader Impact Statement: MetaOOD revolutionizes OOD detection model selection by enablingpractitioners to choose appropriate models for unlabeled tasks automatically. This is particularlycrucial in sectors like healthcare, finance, and security, where rapid adaptation to new data typescan significantly enhance system reliability and prevent critical errors. By providing a systematicapproach to select the most effective models, MetaOOD promotes robust applications in dynamicallychanging environments, ensuring ongoing reliability and accuracy in critical systems. Ethics Statement: Our research adheres to the general Code of Ethics, ensuring that MetaOODis developed and applied with ethical considerations at the forefront, particularly in privacy, bias,and fairness across diverse applications. By facilitating more accurate and unbiased model selec-tion, MetaOOD helps mitigate potential ethical risks in its deployments, such as in surveillance andhealthcare, promoting fairness and protecting privacy. Continuous ethical evaluations accompanyMetaOODs development to ensure it meets societal and legal standards. Reproducibility Statement: We advocate reproducibility in MetaOOD. Comprehensive documen-tation of our methodologies and experimental designs is detailed in the main text and appendices.We have made our code, testbed, and meta-learner fully accessible at providing the resources for replication and exploration.",
  "Hadi S Jomaa, Lars Schmidt-Thieme, and Josif Grabocka. Dataset2vec: Learning dataset meta-features. Data Mining and Knowledge Discovery, 35(3):964985, 2021": "Serdar Kadioglu, Yuri Malitsky, Meinolf Sellmann, and Kevin Tierney. Isac - instance-specific algo-rithm configuration. In ECAI, volume 215 of Frontiers in Artificial Intelligence and Applications,pp. 751756. IOS Press, 2010. URL #KadiogluMST10. Konstantin Kirchheim, Marco Filax, and Frank Ortmeier.Pytorch-ood: A library for out-of-distribution detection based on pytorch. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition (CVPR) Workshops, pp. 43514360, June 2022.",
  "Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for bench-marking machine learning algorithms. ArXiv, abs/1708.07747, 2017. URL": "Lin Xu, Frank Hutter, Jonathan Shen, Holger H Hoos, and Kevin Leyton-Brown. Satzilla2012:Improved algorithm selection based on cost-sensitive classification models. Proceedings of SATChallenge, pp. 5758, 2012. Feng Xue, Zi He, Yuan Zhang, Chuanlong Xie, Zhenguo Li, and Falong Tan. Enhancing the powerof ood detection via sample-aware model selection. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pp. 1714817157, 2024.",
  "Jingkang Yang, Kaiyang Zhou, Yixuan Li, and Ziwei Liu. Generalized out-of-distribution detection:A survey. arXiv preprint arXiv:2110.11334, 2021": "Jingkang Yang, Pengyun Wang, Dejian Zou, Zitang Zhou, Kunyuan Ding, Wenxuan Peng, HaoqiWang, Guangyao Chen, Bo Li, Yiyou Sun, et al. Openood: Benchmarking generalized out-of-distribution detection. Advances in Neural Information Processing Systems, 35:3259832611,2022. Y. Zhao, S. Zhang, and L. Akoglu. Toward unsupervised outlier model selection. In 2022 IEEEInternational Conference on Data Mining (ICDM), pp. 773782, Los Alamitos, CA, USA, dec2022a. IEEE Computer Society.",
  "maxX ,X": "maxXnormalized mean, normalized medianmaxX minX, Gini(X)sample range, sample giniedianm(X X)median absolute deviationavg(X X)average absolute deviationq75q25q75+q25Quantile Coefficient DispersionCoefficient of varianceIf a sample differs from a normal dist.normality5th to 10th momentsskewness, 4 4skewness, kurtosisMCo-occurence MatrixImage featurescolor-basedmean, std of the HSV channelcolor-basedstd of the intensity channelcolor-basedentropy of the RGB channelhistogramstd of the (RGB, HSV, intensity) channelborderAverage white pixelsborderAverage Hu Moments of sobel imagetexture(Co-occurence Matrix)contrast mean, std(Co-occurence Matrix)dissimilarity mean, std(Co-occurence Matrix)homogeneity mean, std(Co-occurence Matrix)energy mean, std(Co-occurence Matrix)correlation mean, std(Co-occurence Matrix)entropy mean, std(FFT)entropy mean, std(FFT)inertia mean, std(FFT)energy mean, std(FFT)homogeneity mean, stdDataset featuresn, pnum of samples, num of featuresd, cdim, num of classEMDEarth Movers Distance",
  "[Dataset descriptions provided]": "Your task is to select the best OOD detection method for a setof 24 test ID-OOD dataset pairs.You will be provided withdescriptions of both the ID-OOD dataset pairs and the availableOOD detection methods.You should pick the best model that hasthe highest AUROC metric.For each dataset pair, output therecommended OOD detection method in the format:RecommendedMethod:[Recommended Method]."
}