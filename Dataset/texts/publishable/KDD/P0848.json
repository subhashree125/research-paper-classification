{
  "ABSTRACT": "Multimodal entity linking (MEL) aims to link ambiguous mentionswithin multimodal contexts to corresponding entities in a multi-modal knowledge base. Most existing approaches to MEL are basedon representation learning or vision-and-language pre-trainingmechanisms for exploring the complementary effect among multi-ple modalities. However, these methods suffer from two limitations.On the one hand, they overlook the possibility of considering neg-ative samples from the same modality. On the other hand, theylack mechanisms to capture bidirectional cross-modal interaction.To address these issues, we propose a Multi-level Matching net-work for Multimodal Entity Linking (M3EL). Specifically, M3ELis composed of three different modules: (i ) a Multimodal FeatureExtraction module, which extracts modality-specific representa-tions with a multimodal encoder and introduces an intra-modalcontrastive learning sub-module to obtain better discriminativeembeddings based on uni-modal differences; (ii) an Intra-modalMatching Network module, which contains two levels of match-ing granularity: Coarse-grained Global-to-Global and Fine-grainedGlobal-to-Local, to achieve local and global level intra-modal in-teraction; (iii) a Cross-modal Matching Network module, which ap-plies bidirectional strategies, Textual-to-Visual and Visual-to-Textualmatching, to implement bidirectional cross-modal interaction. Ex-tensive experiments conducted on WikiMEL, RichpediaMEL, andWikiDiverse datasets demonstrate the outstanding performance ofM3EL when compared to the state-of-the-art baselines.",
  "Contact Authors": "The Black Widow spider spins web and captures prey in the dark, showing the endless power and cunning... Black Widow is produced by Marvel Studios and released in 2021, earning $379.8 million at the box office... Black Widow is the song from singer Iggy Azalea debut studio album The New Classic, released on April 22, 2014... Animal_Black_WidowMovie_Black_WidowSong_Black_Widow candidate entities Black Widow is a movie starring actress Scarlett Johansson and depicting a female superhero in the Marvel Universe... movie mention : An example of MEL. Dotted boxes of different col-ors represent different features: color purple for mentiontextual description (mention text), color orange for mentionvisual context (mention image), color green for entity textualdescription (entity text), color blue for entity visual context(entity image). EL supports numerous downstream information-retrieval appli-cations, such as question answering , semanticsearch , dialog systems , and so on . Mostexisting EL frameworks focus on mention disambiguation via con-text resolution in the textual modality. However, for multimodalinformation including images along with text, conventional text-based approaches to EL struggle to effectively encode such complexcontent. Multimodal Entity Linking (MEL) extends traditional EL byconsidering multimodal information, i.e., it aims at linking textualand visual mentions into their corresponding entities in a mul-timodal knowledge base. For example, in , the mentionBlack Widow can be linked to the entities Animal_Black_Widow,Movie_Black_Widow and Song_Black_Widow. Although, it is pos-sible to roughly guess the entity to which the mention has to belinked by relying solely on the textual description, the associatedvisual information can be used to improve the confidence of text-based predictions. Indeed, the image associated to the mentionBlack Widow has a high degree of semantic similarity with that ofthe entity Movie_Black_Widow, both including the actress ScarlettJohansson, which substantially increases the probability that themention Black Widow is linked to the entity Movie_Black_Widow.",
  "Zhiwei Hu, Vctor Gutirrez-Basulto, Ru Li, and Jeff Z. Pan": "that of the textual attribute, providing richer textual modal content.Under the same experimental conditions, we observe that M3ELattroutperforms existing SoTA baselines by a large margin across allmetrics. We have three main findings: (1) Compared with the bestperforming baseline, MIMIC, M3ELattr respectively achieves 0.48%and 0.51% improvements in MRR and Hits@1 on the WikiMELdataset. A similar behavior is observed on the RichpediaMEL andWikiDiverse datasets. (2) On the three datasets, when comparedwith MIMIC, the improvement on the Hits@1 metric is higherthan that on the Hits@3 metric, which shows that our M3ELattrpays more attention to highly accurate matching in the multimodalentity linking task. (3) The use of description knowledge furtherimproves the performance: on the three datasets, the performanceof M3ELdesc is better than that of M3ELattr. The bigger gain is inthe WikiDiverse dataset. The main reason is that in the WikiDi-verse dataset, a large percentage of the entities have short textdescriptions, with the average text word length being only 1.24.After replacing attribute knowledge with description knowledge,the average word length of the entity text corresponding to themention becomes 4.50. Therefore, we believe that the availabil-ity of textual knowledge is one of the important factors affectingthe model performance. If there is no special explanation, M3ELappearing in subsequent parts will uniformly refer to M3ELdesc. Low Resource Setting. To better understand the performanceof M3ELl and existing baselines in low-resource scenarios, we con-ducted experiments on the RichpediaMEL and WikiDiverse datasetswith 10% and 20% of the training data, while keeping the validationand test sets unchanged. The corresponding results are presentedin . We observe that M3EL consistently achieves optimalperformance on almost all subsets. We have the following two mainfindings. 1) Except for the case when only 20% of the RichpediaMELtraining data is used, M3EL achieves optimal performance. Notethat there is no unique best performing (second overall) baseline,switching between GHMFC, CLIP and MIMIC. This phenomenonreveals that M3EL is more stable and can adapt to a variety ofdatasets under different conditions. In fact, in the RichpediaMEL(20%) scenario, M3EL also achieves competitive results. 2) On theWikiDiverse dataset as the training proportion increases, the gapbetween M3EL and MIMIC gradually becomes larger. We attributethis performance improvement to the fact that the intra-modalcontrastive learning module is able to obtain better discrimina-tive representations as the size of the training data increases. Theadditional support of the multi-level matching mechanism of intra-modal and inter-modal has further brought a positive impact offinal results.",
  "RELATED WORK": "Entity Linking. Recent methods for Entity Linking (EL) mainlyfocus on exploiting ambiguous mentions to the referent unambigu-ous entities in a given knowledge base, which can be divided intotwo series: local-level methods and global-level methods. Local-level methods primarily consider mention along with itssurrounding words or sentence to capture contextual information.Global-level methods also take entity or topiccoherence into account to calculate the mention and entity seman-tic consistency. However, these methods do not work well whenprocessing multimodal data, including textual and visual content. Multimodal Entity Linking. Multimodal entity linking is an ex-tension of the traditional entity linking task that utilizes additionalmultimodal information (e.g., visual information) to support thedisambiguation of entities. Mainstream approaches can be classi-fied into two categories: Representation Learning (RL) frameworksand Vision-and-Language Pre-training (VLP) methods. (i): RL-basedmethods: DZMNED utilizes a multimodal attention mechanismto fuse textual, visual and character features of mentions and enti-ties. JMEL introduces fully connected layers to embed the textualand visual information into an implicit joint space. VELML designs a deep modal-attention neural network to aggregate dif-ferent modality features and map visual objects to the entities.GHMFC extracts the hierarchical features of textual and vi-sual co-attention through a multi-modal co-attention mechanism.DRIN explicitly encodes four different types of alignmentsbetween mentions and entities, and builds graph convolutional net-work to dynamically select the corresponding alignment relationsfor different input samples. MMEL proposes a joint featureextraction module to learn textual and visual representations, anda pairwise training schema and multi-mention collaborative rank-ing mechanism to model the potential connections. (ii): VLP-basedmodels: CLIP trains on large-scale image-caption pairs withcontrastive self-supervised objectives to attain textual and visualrepresentations. ViLT discards convolutional visual featuresand adopts a vision transformer to model long-range dependen-cies over a sequence of fixed-size non-overlapping image patches.ALBEF introduces a contrastive loss to align the textual andvisual representations before fusing them through cross-modalattention to enable more grounded vision and language representa-tion learning. METER systematically investigates how to train",
  "Multimodal Feature Extraction": ": The structure of the M3EL model, containing three modules: Multimodal Feature Extraction (MFE) with Intra-modalContrastive Learning (ICL), Intra-modal Matching Network (IMN) and Cross-modal Matching Network (CMN). Att and M-Attdenote the attention and multi-heads attention mechanisms, respectively. a full-transformer vision-language pre-trained model in an end-to-end manner. MIMIC utilizes BERT and CLIP as textualand visual encoder, respectively, and organizes three interactionmechanisms to comprehensively explore the intra-modal and inter-modal interaction among embeddings of entities and mentions.(iii): generative-based method: GEMEL leverages the in-contextlearning capability of LLMs (e.g., LlaMa-2-7B ) by retrievingmultimodal instances as demonstrations, and then applies a con-strained decoding strategy to efficiently search the valid entityspace. Considering that it requires the use of an LLM with intensivecomputation and parameters, this poses a great challenge to theefficiency of GEMEL. Furthermore, there is a large gap between itsperformance and the state-of-the-art. Therefore, generative-basedmethod is not mainstream and is still in an exploration phase. Al-though existing RL-based or VLP-based methods have made sig-nificant progress, they still have two limitations that need to beaddressed: On the one hand, VLP-based MEL methods directly uti-lize VLP as the textual and visual encoder, however VLP does notconsider negative samples within a modality during the pre-trainingprocess. Both entities and mentions in the MEL task contain textualand visual knowledge. Considering negative samples of entities andmentions in the same modality is useful for improving contrastivelearning capabilities. On the other hand, existing methods only con-sider the information flow from textual to visual or from visual totextual when interacting between modalities, and lack bidirectionalcross-modal interaction.",
  "Task Definition": "Multimodal Entity Linking. Let E be a set of entities in amultimodal knowledge base K. Each entity E is of the form{,,}, where denotes the name of the entity, is a textualdescription of the entity and represents the visual context of theentity associated with its textual description. A mention (andits context) is of the form {,,}, where , , and respectively are the name of the mention, the token sequence inwhich the mention is located, and the corresponding visual imageof the mention. The multimodal entity linking (MEL) task aims toretrieve the ground truth entity E that is the most relevant tothe mention . For example, in , the mention Black Widowrequires to be linked to one of the three candidate entities in the set{Animal_Black_Widow, Movie_Black_Widow, Song_Black_Widow}.After combining the textual description and visual information,we can conclude that the entity Movie_Black_Widow is the mostrelevant for the mention. Usually, the MEL task can be formulatedby maximizing the log-likelihood over the training set D as:",
  "MFE: Multimodal Feature Extraction": "3.2.1Multi-modal Embeddings. For an entity , we treat its tex-tual description and visual context as a text-image pair ={;}. We similarly obtain the text-image pair representationof a mention: = {;}. As feature extractor in and ,we utilize a pre-trained CLIP model , which trains two neural-network-based encoders using a contrastive loss to match pairs oftexts and images. More precisely, take as an example, we obtaintextual and visual embedding representations as follows:",
  "Vm": "(b). Intra-modal Contrastive Learning : Illustrative comparison of intra-modal contrastivelearning with CLIP, where the red dashed lines represent thepositive samples, yellow lines denote the negative samples inCLIP, purple and blue lines represent the inner-source andintra-source negative samples. Circles and squares representtextual and visual features, and represent entity andmention, respectively. Textual Modal Embedding. We first concatenate the entityname and the corresponding textual description using the spe-cial tokens [EOT] and [SEP] to obtain the input sequence {[EOT][SEP][SEP]}. Then, we tokenize the input sequence to the tokensequence { [EOT],1 ;2 ; . . . ; }, where +1 is the length of the to-ken sequence. Additionally, we use CLIPs text encoder to extractthe hidden states: {t[EOT]; t1; t2; . . . ; t } R(+1) , where isthe dimension of textual features. Finally, we consider the embed-ding of [EOT] as the global textual feature T R and the entirehidden state embeddings as local textual features T R(+1) .",
  "Similarly, we can also obtain the global textual feature T andlocal textual feature T of the mention textual modality": "Visual Modal Embedding. Given an entity image R ,we reshape into a sequence of image patches = {1;2; . . . ; } R(2), where is the original image resolution, isthe number of channels, represents the patch image resolu-tion, = /2 is the number of patches. We also prepend anadditional [CLS] token to before the image patches to representthe visual global feature, the corresponding patches sequence be-comes = { [CLS];1;2; . . . ; }. We feed into the CLIP visualencoder and further apply a fully connected layer to convert thedimension of the output hidden status into . The correspondingembedding is denoted as {v[CLS]; v1; v2; . . . ; v } R(+1). Wetake the embedding of the special token [CLS] as the global featureV R and the full embedding as local features V R(+1),where is the dimension of the visual features.",
  "Similarly, we can also obtain the global visual feature V andlocal visual feature V of the mention visual modality": "3.2.2Intra-modal Contrastive Learning. CLIP learns a joint vision-language embedding space by bringing matching image-text repre-sentations together, while pushing unpaired instances away fromeach other. However, CLIP lacks an explicit mechanism that en-sures that similar features from the same modality stay close inthe joint embedding . For example in , given theentity textual feature T, entity visual feature V, mention textualfeature T and mention visual feature V, CLIP only considersnegative samples of T from different modalities (i.e., V for ),ignoring the possibility of having negative samples from the samemodality, i.e., T and T. CLIP is able to map image-text pairs close together in the embedding space, while it fails to ensure thatsimilar inputs from the same modality stay close by, which mayresult in degraded representations . However, the main aimof multimodal entity linking is to retrieve the ground truth entitymost relevant to the mention, so simply considering the interactionbetween different modalities will inevitably lead to a one-sidedembedding representation. Thus, it is necessary to consider thesemantic difference between positive and negative samples withinthe same modality. To obtain better discriminative embedding rep-resentations that are faithful to unimodal differences, we introducean Intra-modal Contrastive Learning module (ICL), composed ofthe following two steps: Step 1. Obtaining Positive and Negative Samples. We takethe textual modality as an example. Assume that there are entitytextual descriptions { 1 , 2 , . . . , } and mention textual descrip-tions { 1, 2, . . . ,}, we can obtain the global textual featurecorresponding to each piece of text based on the Textual ModalEmbedding in .2.1. Note that for computational efficiency,we only perform intra-modal contrastive learning on the globalfeature level, we leave the interaction of local features to the Intra-modal Matching Network in .3. Furthermore, we found outthat introducing contrastive learning at the local feature level willactually lead to performance degradation. Therefore, we omit super-scripts and from textual embeddings of entities and mentionsand simply denote them as {T1, T2, . . . , T } and {T1, T2, . . . , T}.For the entity embedding T, the matching mention embedding Tprovides a positive sample, while the embeddings of other entitiesand mentions are regarded as negative samples. More precisely,negative samples come from two sources: inner-source from the en-tity aspect and inter-source from the mention aspect. Inner-sourcemeans that the negative samples for T are entity embeddingsN = {T | } in the same entity view where T is located, whileinter-source means that the negative samples are the embeddingsN = {T | } in the mention view where T is not located. Step 2. Contrastive Learning Loss Computation. Take theentity textual embedding T and the mention textual embeddingT as an example, we define the intra-modal contrastive learningloss of the positive pair (T, T,) as follows:",
  "(2)": "where (,) = (,)/, is a temperature parameter, (,) isthe cosine similarity to measure the distance between two embed-dings . The and in the denominator sumup the inner-source and inter-source intra-modal negative sam-ples, respectively. and are the hyper-parameters to control theinner-source and inter-source alignment importance, respectively.While the nominator is symmetric, the denominator is not, so forthe positive pair (T, T), the corresponding intra-modal loss isL(T, T).",
  "IMN: Intra-modal Matching Network": "Through the CLIP encoder, we can obtain global and local featuresof textual and visual modalities. Most previous works either exploitglobal features while overlooking the local features, or measure thelocal feature similarity whereas ignoring the global coherence .MIMIC considers the interaction between global and local fea-tures but it uses different independent mechanisms for the textualand visual modalities, making the global and local interaction deeplycoupled with the modality. To alleviate the deep coupling betweenthe interaction strategy and the modality type, we introduce theIntra-modal Matching Network (IMN) module to uniformly capturethe interaction between local and global features within a modality.IMN contains two sub-modules, Coarse-grained Global-to-Globalmatching (G2G) and Fine-grained Global-to-Local matching (G2L).Take the textual modality as an example, using the CLIP encoder,we can obtain the entity global textual feature T , entity local tex-tual feature T , mention global textual feature T and mentionlocal textual feature T. We will use these features as the input tothe G2G and G2L sub-modules. Coarse-grained Global-to-Global matching. To measure globalconsistency, we directly perform the dot product between the en-tity global feature T and the mention global feature T to obtainthe coarse-grained global-to-global matching score, formulated as:",
  "M2= T (5)": "where {MLP1, MLP2, MLP3}: R R are three multi-layerperceptron networks, denotes the scaled dimension size, isthe matrix multiplication operation, Mean() represents the meanpooling operation, is the attention score generated from localfeatures to impose constraints on global features. Afterwards, we average the global-to-global and global-to-localmatching scores to obtain the textual intra-modal matching scoreM = (M2+ M2)/2. Similarly, for the visual modality, wecan obtain the global-to-global matching score M2and global-to-local matching score M2, and the final combined matchingscore M = (M2+ M2)/2.",
  "Since the embeddings of different modalities are separately matchedin the IMN module, it is difficult to model the complex interaction": "between modalities solely based on an intra-modal matching mech-anism. Although the CLIP model considers the alignment of infor-mation between multi-modalities, it is not specifically tailored forthe MEL task. Therefore, it is necessary to appropriately adapt theinter-modal interaction to the embedding representation obtainedfrom CLIP, based on the characteristics of the MEL task. To reducethe gap between the distribution over different modalities, we intro-duce the Cross-modal Matching Network (CMN) module, whichcontains two-way matching strategies: Textual-to-Visual matching(T2V) and Visual-to-Textual matching (V2T). Considering that thedifference between T2V and V2T is only in the input embeddingscontent, we mainly give details for T2V. The T2V strategy has asinput an entity-level textual-to-visual matching between the entityglobal textual feature T and entity local visual features V , anda mention-level textual-to-visual matching between the mentionglobal textual feature T and the mention local visual featuresV. The entity-level and mention-level textual-to-visual matchingmechanisms both include the following three steps. We use theentity-level to explain the details. (1) Bidirectional Matching Interaction. First, we employ a one-layerMLP to scale the dimensions of T and V to the same size.Then, we introduce an attention mechanism to obtain the localvisual-aware global attention Hand global textual-awarelocal attention H . Finally, we utilize the attention contentto get an enhanced entity global textual feature T R andlocal visual features V R(+1) . The entire interactionprocess is bidirectional since the data flows from local to globaland global to local.",
  "(6)": "where MLP4 : R R and MLP5 : R R are multi-layer perceptron networks, represents the scaled embeddingdimension, Softmax() and Relu() are two different activationfunctions. To better understand Equation 6, we elaborate on thedimensional transformation occurring in it. It should be notedthat to simplify the presentation, all dimensions do not includethe batch size dimension, because the batch size of all opera-tions is the same. (i): for the first row, we change T R andV R(+1) to T R and V R(+1) ; (ii): for the",
  "(7)": "where Concat[; ] and avg() denote the concatenation andaverage operation, respectively. Tanh() is an activation func-tion, represents the number of heads, denotes the element-wise addition, is the algebraic multiplication operation, and > 0 is the temperature controlling the sharpness of scores.It should be noted that before executing the Concat operation,the dimensions of Tneed to be expanded to convert it fromT R to T R1 . (3) Analogously, taking the mention global textual feature T andmention local visual features V as inputs, after going throughStep 1 and Step 2 above, we obtain the mention-level textual-to-visual matching representation M2. The textual-to-visualmatching score M2 is then calculated as follows:",
  "M2 = M2 M2(8)": "In the same way, we can obtain the entity-level visual-to-textualmatching representation M 2and the mention-level visual-to-textual matching embedding M 2. We can also apply thedot product of M 2and M 2to get the visual-to-textualmatching score M 2 . Finally, we combine the textual-to-visualscore and the visual-to-textual score to get the cross-modalmatching network result M = (M2+ M 2)/2.",
  "Joint Training": "Consider the textual intra-modal matching score M and the visualintra-modal matching score M from the IMN module and thecross-modal matching score M from the CMN module. We canobtain the union matching score as M = (M + M + M)/3.We select the Unit-Consistent Objective Function L as thebasic loss function. Specifically, the entire joint training processloss L contains three aspects of loss contents as shown inEquation 9: i) the intra-modal contrastive learning loss L; ii) theunion matching loss L = L (M ); iii) to avoid the wholemodel to excessively rely on the union score, we also introducethe independent intra-model textual loss L = L (M ), theintra-modal visual loss L = L (M ), and the cross-modal lossL = L (M).",
  "Experimental Setup": "Datasets. We evaluate the M3EL model on three well-knowndatasets: WikiMEL , RichpediaMEL , and WikiDiverse .WikiMEL has more than 22K multimodal samples, where entitiescome from WikiData and their corresponding textual and visualdescriptions from WikiPedia. RichpediaMEL has more than 17Kmultimodal samples, where the entity ids are from the large-scalemultimodal knowledge graph Richpedia and the correspondingmultimodal information from WikiPedia. WikiDiverse has morethan 7k multimodal samples, constructed from WikiNews. For faircomparison, we adopt the dataset division ratio from MIMIC .The statistics of these datasets are shown in .",
  "# Num. of mentions25,84617,80515,093# Img. of mentions22,13615,8536,697# mentions in train18,09212,46311,351# mentions in valid2,5851,7801,664# mentions in test5,1693,5622,078": "Evaluation Metrics. We evaluate the performance using twocommon metrics: MRR and Hits@N. MRR defines the inverse ofthe rank for the first correct answer, Hits@N is the proportion ofcorrect answers ranked in the top N, with N={1,3,5}. The higher thevalues of MRR or Hits@N, the better the performance. Implementation Details. We conduct all experiments on six 32GTesla V100 GPUs, and use the AdamW optimizer. FollowingMIMIC , we employ the pre-trained CLIP-Vit-Base-Patch32model1 as the initialization textual and visual encoder. For thetextual modality, we set the maximal input length of the text to 40and the dimension of the textual output features to 512. For thevisual modality, we rescale all the images into a 224224 resolutionand set the dimension of the visual output features to 96. Thescaled dimension size is set to 96 and the patch size is set to 32.We use grid search to select the optimal hyperparameters, mainlyincluding: the learning rate {5e-6, 1e-5, 2e-5, 3e-5, 4e-5}, thebatch size {48, 64, 80, 96, 112}, the temperature coefficient {0.03, 0.10, 0.25, 0.5, 0.75} in Equation 2, the number of heads {3,4, 5, 6, 7}, the inner-source alignment weight {0.2, 0.4, 0.6, 0.8,1.0}, the inter-source alignment weight {0.2, 0.4, 0.6, 0.8, 1.0}. Baselines. We compare M3EL with three types of baselines.(i) textual-based methods, including BLINK , BERT andRoBERTa ; (ii) representation learning frameworks, includingDZMNED , JMEL , VELML and GHMFC ; (iii) vision-and-language-based methods, including CLIP , ViLT , AL-BEF , METER and MIMIC ; (iv) generative-based meth-ods, including GPT-3.5-Turbo and GEMEL .",
  "Multi-level Matching Network for Multimodal Entity Linking": ": Evaluation of different models on three MEL datasets, all the baselines results are from the MIMIC paper. Bestscores are highlighted in bold, the second best scores are underlined. It should be noted that there are a large number of - labeledresults in GPT-3.5-Turbo and GEMEL. The main reasons are, on the one hand, only provides results for the WikiMEL andWikiDiverse datasets, and the WikiDiverse dataset is completely different from that used by other baselines. only usesHits@1 as the evaluation indicator.",
  "Overall Performance Comparison. presents resultsof the performance of M3EL and the baselines on the WikiMEL,RichpediaMEL and WikiDiverse datasets. We consider two variants": "of M3EL, M3ELattr and M3ELdesc, which differ in the available tex-tual knowledge. For M3ELattr, following MIMIC, we use the entityattribute knowledge as the textual content of the correspondingentity. For M3ELdesc, we use the textual description of the entity inWikiData2 as its textual modal content. The main motivation forreplacing attribute knowledge with the description knowledge fromWikidata is that the word length of the description is larger than",
  "Ablation Studies": "We conduct ablation experiments on the WikiMEL, RichpediaMELand WikiDiverse datasets under different conditions. presentsresults showing the contribution of each component of M3EL. Theseinclude the following three points: a) removing one of L , L ,L , L or L from the loss function L; b) removing thetextual intra-modal matching module (w/o L + M ), the visualintra-modal matching module (w/o L + M ), or the cross-modalmatching module (w/o L + M) from M3EL, it should be noted that there is no ablation scenario w/o M , w/o M , w/o M, be-cause L , L , L are calculated based on M , M , M, andremoving M , L , L would mean that L , L , L are alsoremoved; c) replacing ICL with two contrastive learning losses, i.e.,w/ InfoNCE and w/ MCLET . In addition, we also conductexperiments Effect of Different Pooling Operation in Equation 5and Resource Consumption Using Various Contrastive Loss. Impact of Different Losses. The upper part of showsthe individual contribution of different losses. We can observe thatremove any loss will generally bring certain degree of performancedegradation. On the RichpediaMEL and WikiDiverse datasets, theperformance loss caused by removing the visual loss is higher thanby removing the textual loss. However, on the WikiMEL dataset, thesituation is the opposite, i.e., the performance fluctuation causedby removing the textual loss is larger. This shows that the richnessof modal knowledge is the main factor that determines the impactof the corresponding modality on model performance. In addition,removing the contrastive loss also has certain impact on perfor-mance, especially on the WikiDiverse dataset. This is mainly dueto the fact that we also consider the negative examples within amodality in the contrastive learning process to obtain better dis-criminative modal embedding representations. Although there aresome singularies in some indicators of some datasets when L ,L , L are removed, this is acceptable because the informationrichness of textual and visual modalities in each dataset is differ-ent. Indiscriminately removing modules for specific modalities willcause unknown performance losses. The overall performance is thebest when all losses and modules are simultaneously used. Impact of Different Modules. The middle part of showsthe individual contribution of different modules. It should be men-tioned that removing the corresponding module involves removingtwo aspects: for example, when removing the cross-modal match-ing network (CMN), the matching score M related to the CMNmodule needs to be removed from the union matching score M ,and the loss L introduced by M also needs to be removed. Sincethe loss is calculated from the matching score of the correspondingmodule, there will be no corresponding loss value without a match-ing score. From the experimental results in , we can observethat when compared to only removing the loss value L related tothe corresponding module, removing the matching score of the thatmodule from the union matching score will lead to a greater perfor-mance degradation. Taking the textual modality of the WikiMELdataset as an example, if only the textual modality loss is removed,the corresponding MRR and Hits@1 metrics respectively are 91.64and 87.60. After further removing the matching score M relatedto the textual modality, the corresponding MRR and Hits@1 metricsbecome 90.22 and 86.40, falling by 1.42% and 1.2% respectively. Asimilar situation occurrs when modules related to visual modalitiesand cross-modalities are removed, which confirms the usefulnessof each module. Impact of Various Contrastive Loss. The lower part of shows the experimental results of replacing the ICL module with theInfoNCE and MCLET contrastive losses. We can observethat with our ICLs M3EL can achieve better experimental results inmost cases. Specifically, taking the WikiMEL dataset as an example,compared with InfoNCE and MCLET, the ICL module improves the",
  "M3EL92.3088.8495.2096.7188.2682.8292.7395.3481.2974.0686.5790.04": "MRR metric by 1.84% and 0.45%, respectively. This is due to thefact that we consider both inner-source and inter-source negativesamples in the ICL module. In addition, we introduce a weightcoefficient in Equation 2 to reconcile the possible imbalance ofnegative samples. We conduct parameter analysis experiments onthe effect of inner-source and inter-source alignment weights and in Appendix A.",
  "mean81.2974.0686.5790.04": "Effect of Different Pooling Operation in Equation 5. Themean pooling operation in Equation 5 is mainly used to reducethe spatial dimension of to facilitate its calculation with T toobtain the global-to-local matching score M2. We replace themean pooling operation with max pooling and soft pooling operations on the WikiDiverse dataset to study the impact of thepooling operation. The corresponding results are shown in .We find that the advantage of using mean pooling operation ismore prominent. A possible explanation is that it is more effectiveto comprehensively consider the representation of each token ina sentence or each patch in an image than to consider only someimportant tokens or patches. Resource Consumption Using Various Contrastive Loss. Weanalyze the resource consumption from two perspectives: time com-plexity and space complexity. The corresponding results on threedifferent datasets are shown in the . The time complexityis measured by the number of floating-point operations (#FLOPs)required during training phase, while the space complexity refersto the amount of a models parameters (#Params). The larger the",
  "#Params InfoNCE335.259K335.259K324.753KMCLET321.423K321.423K311.721KICL314.529K314.529K305.217K": "values of #FLOPs and #Params are, the more computing power andhigher memory usage are required during the training process. Weobserve that compared with InfoNCE and MCLET, ICL requiresless time and space overhead in each dataset. Specifically, on theWikiMEL dataset, ICL reduces #FLOPs and #Params by 2.4% and2.1%, respectively, compared to MCLET, which demonstrates thatICL is more efficient.",
  "CONCLUSION": "In this paper, we propose M3EL, a multi-level matching networkfor multimodal entity linking. M3EL simultaneously considers thediversity of negative samples from the same modality and bidi-rectional cross-modality interaction. Specifically, we introduce anintra-modal contrastive loss to obtain better discriminative repre-sentations that are faithful to certain modality. Furthermore, wedesign intra-modal and inter-modal matching mechanisms to ex-plore multi-level multimodal interactions. Extensive experimentson three datasets demonstrate M3ELs robust performance. This work has been supported by the National Natural Science Foun-dation of China (No.61936012, No.62076155), by the Key Researchand Development Project of Shanxi Province (No.202102020101008),by the Science and Technology Cooperation and Exchange SpecialProject of Shanxi Province (No.202204041101016).",
  "Zhiwei Hu, Vctor Gutirrez-Basulto, Zhiliang Xiang, Ru Li, and Jeff Z. Pan. 2023.Multi-view Contrastive Learning for Entity Typing over Knowledge Graphs. InEMNLP. ACL, Singapore, 1295012963": "Zhiwei Hu, Vctor Gutirrez-Basulto, Zhiliang Xiang, Ru Li, and Jeff Z. Pan. 2024.Leveraging Intra-modal and Inter-modal Interaction for Multi-Modal EntityAlignment. CoRR abs/2404.17590 (2024). arXiv:2404.17590 Zhiwei Hu, Vctor Gutirrez-Basulto, Zhiliang Xiang, Xiaoli Li, Ru Li, and Jeff Z.Pan. 2022. Type-aware Embeddings for Multi-Hop Reasoning over KnowledgeGraphs. In IJCAI. ijcai.org, Vienna, Austria, 30783084.",
  "Phong Le and Ivan Titov. 2018. Improving Entity Linking by Modeling LatentRelations between Mentions. In ACL. ACL, Melbourne, Australia, 15951604": "Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. 2023. BLIP-2:Bootstrapping Language-Image Pre-training with Frozen Image Encoders andLarge Language Models. In ICML. PMLR, Honolulu, Hawaii, USA, 1973019742. Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. 2022. BLIP: Boot-strapping Language-Image Pre-training for Unified Vision-Language Understand-ing and Generation. In ICML. PMLR, Baltimore, Maryland, USA, 1288812900. Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Gotmare, Shafiq R. Joty, CaimingXiong, and Steven Chu-Hong Hoi. 2021. Align before Fuse: Vision and Lan-guage Representation Learning with Momentum Distillation. In NeurIPS. CurranAssociates, online, 96949705.",
  "OpenAI. 2023. Chatgpt. Weiran Pan, Wei Wei, and Xian-Ling Mao. 2021. Context-aware Entity Typingin Knowledge Graphs. In EMNLP. ACL, online, 22402250": "Matthew E. Peters, Mark Neumann, Robert L. Logan IV, Roy Schwartz, VidurJoshi, Sameer Singh, and Noah A. Smith. 2019. Knowledge Enhanced ContextualWord Representations. In EMNLP. ACL, Hong Kong, China, 4354. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,Gretchen Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual ModelsFrom Natural Language Supervision. In ICML. PMLR, online, 87488763.",
  "Senbao Shi, Zhenran Xu, Baotian Hu, and Min Zhang. 2024. Generative Multi-modal Entity Linking. In COLING. ELRA and ICCL, Torino, Italy, 76547665": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-mine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-ale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, GuillemCucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, SagharHosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux,Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, XavierMartinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, AndrewPoulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, RuanSilva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang,Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, IliyanZarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, AurlienRodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama2: Open Foundation and Fine-Tuned Chat Models. CoRR abs/2307.09288 (2023).arXiv:2307.09288",
  "Chengmei Yang, Bowei He, Yimeng Wu, Chao Xing, Lianghua He, and Chen Ma.2023. MMEL: A Joint Learning Framework for Multi-Mention Entity Linking. InUAI. PMLR, Pittsburgh, PA, USA, 24112421": "Jinyu Yang, Jiali Duan, Son Tran, Yi Xu, Sampath Chanda, Liqun Chen, BelindaZeng, Trishul Chilimbi, and Junzhou Huang. 2022. Vision-Language Pre-Trainingwith Triple Contrastive Learning. In CVPR. IEEE, New Orleans, LA, USA, 1565015659. Xiyuan Yang, Xiaotao Gu, Sheng Lin, Siliang Tang, Yueting Zhuang, Fei Wu,Zhigang Chen, Guoping Hu, and Xiang Ren. 2019. Learning Dynamic ContextAugmentation for Global Entity Linking. In EMNLP. ACL, Hong Kong, China,271281.",
  "APPENDIXA Parameter Sensitivity": "We carry out parameter sensitivity experiments on the WikiMEL,RichpediaMEL and WikiDiverse datasets. The results are shownin , including: a) effect of numbers of heads ; b) effect oftemperature coefficient ; c) effect of inner-source and inter-sourcealignment weight and . Effect of Numbers of Heads . In Equation (7), we use amulti-head attention mechanism to achieve the fusion betweencross-modal local features and the global feature. In (a),we observe that the number of multi-head attention heads has agreater impact on the WikiMEL and RichpediaMEL datasets than onWikiDiverse. We achieve the best performance on all three datasetswhen the number is set to 5. Effect of Temperature Coefficient . The temperature coeffi-cient can be used to adjust the similarity measure between samples.When the temperature coefficient is higher, the model is more likelyto reduce the difference between positive samples and negative sam-ples. This can cause the learned feature representation to be lesssensitive to the differences between different samples. When thetemperature is lower, the model pays more attention to subtle dif-ferences between samples. This may lead to being overly sensitiveto noise and local changes, thereby reducing the models gener-alization ability. Therefore, selecting an appropriate temperaturecoefficient value requires appropriate adjustments based on thecharacteristics of a dataset. We can see in (b) that selectinga smaller temperature value is beneficial to all three datasets andthe best results are achieved when the temperature value is set to0.03. Effect of Inner-source and Inter-source Alignment Weights and. The weight coefficients and in Equation (2) can be usedto control the importance of negative samples from inner-sourceand inter-source. An appropriate adjustment of the weights values,based on the dataset, can help avoiding the performance impactcaused by improper selection of negative samples. We can observein (c) and (d) that selecting a larger value is more beneficialto the three datasets. However, there is no rule to follow for theselection of the value because as increases, the performance ofthe three datasets fluctuates.",
  "B Additional Experiments": "Effect of Bidirectional Interaction Mechanism. We conductfurther ablation experiments on the T2V and V2T matching mech-anisms of the CMN module in .4 on three datasets. Thecorresponding results are shown in . We can observe thatusing only the T2V or V2T matching mechanism will bring a certain degree of performance degradation on the three datasets. Specifi-cally, the performance loss caused by using only T2V is greater thanthat of using only V2T. Furthermore, on the WikiDiverse dataset,the performance degradation is the largest after removing T2V orV2T, which fully demonstrates the necessity of the two mecha-nisms. We further give the rational behind the existence of the twomechanisms. First, we need to give some explanations of terms.As described in .2.1, global textual feature refer to theembedding representation of the entire textual modality sentence,local textual features refers to the embedding representation ofeach token in the textual modality sentence, global visual featurerefers to the embedding representation of the entire visual modalityimage, and local visual features refer to the embedding represen-tation of each patch in the visual modality image. Secondly, forT2V, its input is global-textual feature and local visual features, i.e.,using the global representation at the sentence level to supervisethe local representation at the image patch level, which is suitablefor scenes with richer textual semantic knowledge, while V2Tsinput is global visual feature and local textual features, which ismore suitable for scenes with more visual information from theimage. The T2V and V2T mechanisms are applicable to differentscenarios. The combination of the two can complement each otherand be more robust to changes in scenarios. Effect of Different Feature Extractor. We use CLIP as thefeature extractor in MFE, mainly considering the following twofactors: On the one hand, CLIP has good performance. On the otherhand, the memory size of our experimental machine is limited,making it difficult to run larger models. In principle, CLIP can bereplaced by any encoder that can obtain textual and visual modalityembeddings, such as BLIP or BLIP-2 . Considering thecurrent status of our experimental machine, we choose BLIP-vqa-base to replace CLIP for the experiment. The corresponding resultsare shown in . We find that using BLIP actually obtains worseresults, the main reason is that in order to run the experiment, weperformed float16 quantization on BLIP-vqa-base (otherwise thememory will overflow), which will lead to a large degree of precisionloss. We leave experiments with full-scale runs of BLIP and BLIP-2as future work. Effect of Independent Mechanism in IMN Module. To verifythe performance of the independent mechanism proposed by theMIMIC model on our M3EL model, we conduct experimentson the WikiMEL, RichpediaMEL, and WikiDiverse datasets. Thecorresponding results are shown in . We observe that afterreplacing our unified mechanism with an independent mechanism,there is a certain degree of performance loss on the three datasets,which illustrates that the independent mechanism is not suitablefor M3EL. A possible explanation is that the interaction mechanismwithin the modality is similar, and there is no need to customizea unique interaction system for each modality. Instead, a simplerapproach is more appropriate."
}