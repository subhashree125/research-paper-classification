{
  "Abstract": "Large Language Model (LLM) based agents have garnered significant attentionand are becoming increasingly popular. Furthermore, planning ability is a crucialcomponent of an LLM-based agent, involving interaction with the environment andexecuting actions to complete a planning task, which generally entails achieving adesired goal from an initial state. This paper investigates enhancing the planningabilities of LLM-based agents through instruction tuning, referred to as agenttraining. Recent studies on agent training have demonstrated that utilizing expert-level trajectory data (sequences of action-observation pairs) for instruction-tuningLLMs effectively enhances their planning capabilities. However, existing workprimarily focuses on synthesizing trajectories from manually designed planningtasks and environments. The labor-intensive nature of creating these environmentsand tasks impedes the generation of sufficiently varied and extensive trajectoriesfor agent training. To address this limitation, this paper explores the automatedsynthesis of diverse environments and a gradual range of planning tasks, from easyto difficult. We introduce a framework, AGENTGEN, that leverages LLMs first togenerate environments and subsequently generate planning tasks conditioned onthese environments. Specifically, to improve environmental diversity, we proposeusing an inspiration corpus composed of various domain-specific text segmentsas the context for synthesizing environments. Moreover, to increase the difficultydiversity of generated planning tasks, we propose a bidirectional evolution method,BI-EVOL, that evolves planning tasks from easier and harder directions to synthe-size a task set with a smoother difficulty curve, thereby enhancing the learningprocess of LLMs more effectively. These methods collectively contribute to thegeneration of diverse trajectory data for instruction-tuning. Based on AGENTGEN,we greatly expanded the number of environments and planning tasks available foragent training. The evaluation results from AgentBoard indicate that AGENTGENgreatly enhances the planning capabilities of LLMs. For instance, the AGENT-GEN instruction-tuned Llama-3.1-8B outperforms GPT-3.5 in overall performance.Moreover, the AGENTGEN-tuned Llama-3.1-70B model achieves state-of-the-artresults in planning tasks.",
  "Introduction": "Recently, owing to advancements in Large Language Models (LLMs) , the LLM-basedAgents have garnered widespread attention from the artificial intelligence community. Generally,an LLM-based agent refers to utilizing LLMs to perceive the environment, make decisions, andexecute actions to substitute or help people accomplish some specific tasks . Furthermore,planning is often regarded as one of the most important applications of LLM-based agents, such as",
  "arXiv:2408.00764v2 [cs.CL] 28 Nov 2024": "robotic planning , travel planning , etc. In this study, planning is conceptualizedas the systematic process of identifying a sequence of executable actions within a given environmentto complete a planning task, defined as the transition from an initial state to achieve specified goalconditions, considering constraints and available resources . Improving planning capabilities through instruction-tuning LLMs is a significant research problem,referred to as agent training. As shown in , similar to imitation learning , a typical agenttraining process can be divided into three stages: (i) Preparing environments and planning tasks. (ii)Synthesizing expert-level trajectories (sequences of action-observation pairs) on these planning tasks.For example, utilizing state-of-the-art LLMs (e.g., GPT-4 ) as the agent and filtering trajectorybased on reward score . (iii) Instruction-tuning LLMs with the synthesized trajectory data.Recently, the effectiveness of enhancing the planning capabilities of LLMs through agent traininghas been demonstrated by many studies . Despite their success, one keylimitation of these works is that they primarily rely on manually designed environments and planningtasks. The labor-intensive nature of creating environments and planning tasks hinders the generationof diverse and extensive trajectory data. More explicitly, designing diverse environments requiresdefining a range of rich and practical scenarios, and implementing these environments typicallyinvolves the participation of human experts with programming skills. Additionally, formulating tasksoften demands creating a task set with a gradual difficulty progression. Due to this constraint, existingagent training studies typically use only a few environments for data synthesis. : A typical agent training process includes three stages: task preparation, trajectory synthesis,and instruction tuning. AGENTGEN primarily distinguishes itself from existing agent training litera-ture in the task preparation stage, where we introduce a fully automated task generation frameworkAGENTGEN for constructing diverse environments and planning tasks with gradual learning curves. To address the aforementioned deficiencies, this paper introduces an automatic framework AGENT-GEN that utilizes LLMs to construct diverse environments and planning tasks for agent training,expanding the available environments from a few to hundreds. More specifically, AGENTGEN isstructured around two stages: (1) Environment Generation: Achieving sufficient environmentaldiversity is essential for creating diverse planning tasks, which involves covering a broad rangeof scenarios and domains. To ensure this, we use an inspiration corpus composed of diverse textsegments as context for generating environment specifications with LLMs, where actions, restrictions,and other details are defined using natural language. For example, in , we randomly selecteda text segment from the inspiration corpus: How to boost your diet with peanut butter powder?This prompted the generation of a related environment specification: You are a nutritionist taskedwith creating a new healthy recipe book that incorporates peanut butter powder as a key ingredient.Subsequently, we prompt the LLM to produce the corresponding code based on this specification,which may be composed of Python, Planning Domain Definition Language (PDDL) , or otherdomain-specific languages. Furthermore, we constructed an environment library to serve as in-contextexamples and iteratively expanded it by incorporating high-quality newly generated environments.(2) Task Generation: Conditioned on the generated environment, we aim to create multiple planningtasks. In this stage, it is crucial to have a gradual set of tasks ranging from easy to difficult, i.e.,difficulty diversity. To achieve greater difficulty diversity, we propose a bidirectional evolution method, BI-EVOL, where the LLM first generates random planning tasks and then evolves thesetasks by applying constraints towards both simplification and increased difficulty. The created task setwith BI-EVOL has a smooth difficulty curve that facilitates LLMs smoother acquisition of planningskills. To verify the effectiveness of our method, we synthesized environments and planning tasks based onPDDL and constructed a dataset comprising 592 environments, each with 20 tasks. We thenused a domain-independent planner to obtain 7,246 high-quality trajectories. Subsequently, we usedthis trajectory data for instruction-tuning a series of LLMs and demonstrated the trained model onAgentBoard . Since our instruction-tuning dataset is composed of trajectory synthesized fromPDDL-based planning tasks, we refer to evaluation tasks implemented in PDDL as in-domain tasksand tasks implemented in other programming languages as out-of-domain tasks. Importantly, thisevaluation was conducted in a zero-shot manner without utilizing any trajectory data from thesetasks. Experimental results demonstrate that AGENTGEN achieved more than a tenfold improvementover the raw LLama-3.1-8B on in-domain tasks (33.3 vs. 3.0), with overall performance surpassingthat of GPT-3.5. Furthermore, the performance of AGENTGEN-tuned Llama-3.1-70B exceededGPT-4, setting a new state-of-the-art in planning tasks. In out-of-domain tasks, AGENTGEN alsodemonstrated similar experimental outcomes. Specifically, it led to a significant improvement inaverage success rates, with the raw LLama-3.1-8B model achieving a 10.0% increase and the 70Bmodel a 3.7% improvement. In summary, the proposed environment and planning task generationmethod AGENTGEN can help improve planning ability. Moreover, not only can in-domain tasksbenefit from this, but out-of-domain tasks also improve, which confirms both the effectiveness andgeneralization. Our contributions can be summarized as follows:",
  "Planning Problems": "We consider goal-directed deterministic planning problems , which are formally defined as atuple P = (T, E), where E denotes the environment in which the agent interacts and T denotesthe task that the agent needs to complete. Specifically, an environment E typically models a world,encompassing the definitions of the action space A and state space S, as well as the transitionfunction T : S A S. Task T is further defined by the tuple T = (G, I), where G refers tothe goal conditions and I refers to initial states of the agent. The initial states I are a subset ofthe state space Si that specifies the starting conditions of the agent. The goal G is a subset of thestate space Sg that specifies the desired outcomes or conditions. Specifically, G can be expressed asG = {s Sg | (s) = true}. Here, (s) is a boolean-valued function representing conditions orpropositions that must be satisfied for the state s to be considered part of the goal set.",
  "Planning Problem Implementation": "A planning problem can be implemented with programming languages such as Python or domain-specific languages such as Planning Domain Definition Language (PDDL) . For example, ina PDDL-based planning problem, the domain PDDL file can be regarded as the environment E,defining states (predicates) and actions and specifying the transition function using preconditions andeffects of each action. The problem PDDL file, on the other hand, can be seen as the task T. Bothinitial states and goal conditions are typically defined as combinations of predicates. Another widely used programming language for constructing planning problems is Python. For example, in OpenAIgym1, a planning problem will be implemented as a Python class, where the transition function isimplemented as a method of the class, usually named the \"step\" or \"update\" function. Meanwhile,the goal G is typically represented as a reward function that indicates the objective of the task, andthe initial states I are defined in a method named \"reset.\"",
  "Large Language Model based Agent": "An LLM-based agent leverages a pre-trained language model to operate within the defined environ-ment E and complete the given task T. Given an environment E, the LLM-based agent perceives itsstate S and takes actions A based on its understanding and processing of the input. The transitionfunction T : S A S remains consistent, where the LLM-based agent determines the nextstate by generating appropriate actions through natural language processing. The goal G guides theLLM-based agent in selecting actions that maximize the reward. The agent utilizes the languagemodel to interpret the task requirements and generate actions that align with achieving the specifiedgoal. In essence, the LLM-based agent forms a policy : S A using the LLM, where (s) is theaction taken in state s based on the LLMs understanding and processing of the task.",
  "Methodology": "Problem DefinitionThe process of generating planning tasks can be formalized as a functionf : I (T, E), where I is the input space (e.g., instructions or prompts) and tuple (T, E) is thespace of all possible planning tasks and environments. Based on the definition in .1, wecan express this as f(i) = (Ti, Ei),i I, where Ti is the generated planning task and Ei isthe generated environment for a given input i. Our two-stage approach can be further decomposedas follows: i) Environment Generation (3.1): In the first stage, we generate the environment Eibased on the input instruction i. This can be represented as Ei = gE(i), where gE is the environmentgeneration function that takes the instruction i as input and produces the environment Ei. ii) TaskGeneration (3.2): In the second stage, we generate the task Ti, conditioned on the environment Eigenerated in the first stage. This can be expressed as: Ti = gT(i, Ei), where gT is the task generationfunction that takes both the original instruction i and the generated environment Ei as inputs toproduce the task Ti. We will detail the implementation of these two stages in the following section.",
  ": Overview of the process of environment generation": "OverviewAs is shown in , we propose a sophisticated framework for environment genera-tion structured around three main components: (1) an environment specification generation modulewhere an LLM first generates a specification of the environment, typically including a generaloverview of the environment, descriptions of the state space and action space, and definitions ofthe transition functions; (2) an environment implementation module that generates correspondingcode based on the environment specification; and (3) an environment library that stores previouslygenerated high-quality environments, serving as a comprehensive environment dataset and providingin-context examples for generating new environments. Each component will be elaborated on in thefollowing paragraph. Environment SpecificationWe initially prompt the LLM to generate an environment specifica-tion, which typically includes an overall depiction of the environment, specific actions and theircorresponding preconditions and effects, and certain restrictions within the environment. The en-vironment specification will serve as the basis for generating specific environment codes. Thistwo-stage approach, similar to the Chain-of-Thought , can better assist the LLM in creatinghigh-quality environments. For generating environment specifications, One direct approach is toprompt LLMs to generate random environments. However, due to the inherent inductive bias ofLLMs, they struggle to generate diverse environments in this way. Therefore, to address this issue,we build an inspiration corpus D = {t0, t1, , tn}, containing sufficiently diverse text segments usedto serve as the \"inspiration\" for generating environment specification with LLMs. More specifically,when generating an environment, we first sample a text segment ti from D, then prompt the LLM togenerate a related environment based on ti. Taking the example in , we first sample a textsegment \"How to boost your diet with peanut butter powder?\" from D. Then we prompt an LLMto generate a related environment where the agent is defined as a nutritionist tasked with creating anew healthy recipe book that prominently features peanut butter powder as a key ingredient. Thisapproach significantly enhances the diversity of generated environments, thereby empowering moregeneralized agent training. The inspiration corpus can be implemented in various ways, such as usinga large-scale pre-trained corpus like Common Crawl. Alternatively, a domain-specific corpus, such asa code generation dataset , can be used to generate environments for a specific domain. Thispaper uses LIMA as the inspiration corpus, an instruction-tuning dataset with sufficient diversity. Environment ImplementationConditioned on the generated environment specification, we gener-ate its corresponding code, i.e., implementing the environment. This can be formulated as a typicalcode-generation problem with LLMs. We also introduce a validation tool capable of capturing syntaxerrors to provide feedback during the code generation process, thereby iteratively refining it. Environment LibraryWe define the library at iteration t as: Lt = L0 tk=1{Ei|Ei =gE(i, Lk1), i Ik, v(Ei) = true}, where L0 is the initial seed library, and the union represents allverified environments generated up to iteration t. This iterative process allows continuous expansionand refinement of the environment library, potentially leading to increasingly complex and diverseenvironments over time.",
  "Task Generation": "OverviewAs depicted in , conditioned on the generated environments, we prompt LLMsto generate corresponding planning tasks. We employ a two-stage generation approach BI-EVOL forcreating a diverse range of planning tasks in terms of difficulty. We begin by prompting the LLMwith a specific environment, enabling it to generate an initial set of planning tasks in a zero-shotway. Subsequently, we adjust these tasks to make them simpler or more challenging, forming acomprehensive set of planning tasks. Bidirectional EvolutionMany studies have proposed evolving instructions, primarily focusingon making instructions more difficult . The effectiveness of this approach relies heavilyon the assumption that LLMs inherently possess the ability to follow simple instructions. However,according to findings from some studies , LLMs often exhibit poor performance even insimple planning tasks. Therefore, we propose BI-EVOL, which introduces evolution in two directions:easy-evol and hard-evol. Easy-evol typically involves simplifying the goal conditions. The motivationis that easier tasks can facilitate learning when the agent performs poorly and cannot directly learnfrom typically difficult goals. Conversely, hard-evol usually involves making the goal conditions : Overview of the process of task generation. The two-stage task generation processincludes first generating unconditioned tasks, then applying BI-EVOL to evolve these planning tasks.Ultimately, both parts are incorporated into the task set. In examples of evolving methods, redindicates evolution towards more difficult tasks, while green indicates the opposite. more complex, increasing the number of steps required for the agent to complete the task. This canfurther enhance the agents capability to perform the planning task. To our knowledge, we are thefirst to introduce bidirectional evolution in the agent data generation scenario. The prompt examplesare shown in .",
  "Experiments": "To evaluate the effectiveness of the proposed framework, we synthesize environments and planningtasks using the Planning Domain Definition Language (PDDL), a widely adopted programminglanguage for planning. Subsequently, we evaluate its performance across various unseen planningtasks in a zero-shot manner. To validate the effectiveness and generalizability of AGENTGEN,we categorized the evaluated tasks into two distinct groups: i) In-Domain Tasks: Planning tasksimplemented using PDDL. ii) Out-of-Domain Tasks: These comprise tasks developed using otherprogramming languages, such as Python.",
  "Experimental Setup": "Evaluation TasksFor In-Domain Tasks, we select four widely used PDDL-based planning tasks:Blocksworld, Gripper, Tyreworld, and Barman . More explicitly, Blocksworld requires an agentto achieve a target configuration by moving blocks, while Gripper involves moving objects betweendifferent rooms. Tyreworld simulates changing a car tire, including removing the flat tire, replacing itwith a spare, and installing the new tire. Barman emulates a bartenders tasks in mixing cocktails,which include combining various ingredients, using shakers, and garnishing drinks. For Out-of-Domain Tasks, we select three challenging partial-observable planning tasks: Alfworld andBabyAI , Jericho . Alfworld is an environment designed to test agents abilities to performeveryday household tasks. While in BabyAI, the agent interprets and executes natural languageinstructions in a grid-world setting. Jericho is a collection of text-based interactive fiction gamesin which players issue textual commands to alter the environment. Evaluation MetricsWe utilized two evaluation metrics to evaluate planning ability: success rateand progress rate . During each interaction round, we assigned a progress rate, denoted asrt, to measure the progression towards the goal state g. As the agent transitions through statesst = [s0, . . . , st], its progress is assessed using a matching score f(, g) , which quantifiesthe similarity between the current state and the goal state. Initially, rt is set to 0, indicating noprogress. Only when the progress rate reaches 1 does the success rate attain 1; all other scenariosyield a 0 outcome. The success rate reflects the agents capacity to complete a comprehensive task. BaselinesWe compare AGENTGEN with a series of widely-used multipurpose foundation modelsthat exhibit state-of-the-art performance, such as GPT-3.5 and GPT-4 , CodeLlama ,Mistral , Llama-2 , and Llama-3.1 . We use their instruct-tuned versions for all multi-purpose foundation models (A.1). Additionally, some models have undergone specialized trainingon agent trajectory data, such as AgentLM , FireAct , Agent-Flan . We also utilize theAgentInstruct dataset to train Llama-3.1, following the training configuration of AGENTGEN asa baseline model. Implementation DetailsWe followed the environment and task implementation of Agent-Board . For the configuration of evaluation tasks, we employ act-only prompting , setting themaximum step length for the LLM agent to 30. We selected LIMA as the text corpus D for gen-erating environments, which leverages various data manipulation techniques to ensure a diverse rangeof instructions. For environment generation and task generation, we employ GPT-4 2, configuringthe inference parameters with a temperature of 0 and a top_p value of 0.95. Based on AGENTGEN,we generated a total of 592 environments. For each environment, we generated ten unconditionedtasks, which were then evolved into ten refined tasks using BI-EVOL. To generate trajectory data fortraining, we utilized the domain-independent planner FastDownward3, ensuring optimal trajectorydata. This process ultimately led to 7246 trajectories. More details of the dataset can be found inAppendix B and C. Since the trajectory data is structured, such as \"pickup(o1)\", we employ GPT-4 togenerate a natural language mapping, for example, \"pick up object {arg1}\", to transform structuredactions into natural language actions. We detailed the generation of natural language mapping in A.2.During the training process, we employed Llama-3.1-8B (base version) as our foundation model,blending general instruction data from the ShareGPT dataset in a 1:4 ratio. For the 70B model,we selected Llama-3.1-70B-Instruct and trained it using LoRA without incorporating generalinstructions, with a rank of 16. The hyperparameters were configured as follows: a batch size of64, 10 epochs, a context length of 4096 tokens, and no warmup steps. Checkpoints from epochs 5through 10 were retained and subsequently evaluated on in-domain tasks. The model demonstratingoptimal performance was then selected for further evaluation on out-of-domain tasks. We conductedall experiments utilizing V100 and A100 GPUs.",
  "Evaluation on In-Domain Tasks": "As shown in , the AGENTGEN-tuned Llama-3.1-8B model outperforms GPT-3.5 in overallprogress rate (33.3 vs. 25.0). Furthermore, the AGENTGEN-tuned Llama-3.1-70B model slightlysurpasses GPT-4 (81.5 vs. 81.2). When compared to other models with similar parameter scales,AGENTGEN consistently demonstrates superior performance across four distinct tasks. In relation tothe base Llama-3.1 model, our model exhibits a substantial improvement for both the 8B and 70Bversions, with overall progress rates increasing by 30.3 and 2.5, respectively. Notably, in tasks wherethe success rate of Llama-3.1-8B is zero, AGENTGEN achieves significant breakthroughs, furthervalidating the efficacy of AGENTGEN. From the above, we can draw the following conclusions: i)AGENTGEN-tuned Llama-3.1-8B outperforms GPT-3.5 in overall performance, while the 70B versionachieves state-of-the-art results; ii) AGENTGEN-tuned Llama-3.1 has significantly improved bothsuccess rate and progress rate; iii) AGENTGEN consistently outperforms other models with similarparameter scales.",
  "Robustness": "To validate the robustness of the constructed dataset with AGENTGEN, we conducted a series ofexperiments to evaluate its performance across different foundation models. We selected severalwidely used 7-8B foundation models, including Llama-3-8B, CodeLLama-7B, and Mistral-7B, to testthe versatility and effectiveness of AGENTGEN. As is shown in , all three models exhibitedsignificant improvements after training, with Llama-3-8B showing the highest success rate increase of10.0 and CodeLlama-7B demonstrating a maximum progress rate increase of 9.9. These experimentalresults prove that the dataset constructed with AGENTGEN for agent training is highly effective acrossdifferent models.",
  "Evaluation on Out-of-Domain Tasks": "We also conducted evaluations on out-of-domain agent tasks. As illustrated in , similarexperimental phenomena were observed. Firstly, AGENTGEN demonstrates a substantial performanceimprovement over Llama-3.1, with an increase of 13.1% in the average progress rate for the 8B modeland 5.0% for the 70B model. Additionally, the AGENTGEN-tuned Llama-3.1-8B model outperformsGPT-3.5. When compared to general models and agent fine-tuning models with similar parameterscales, AGENTGEN consistently outperforms them on both tasks. The superior performance onout-of-domain tasks further emphasizes the effectiveness and generalization capability of our datasynthesis methods.",
  "Related Work": "Large Language Model based Agent.Large Language Models have demonstrated exceptionalreasoning capabilities . Owing to such abilities, over the past two years, LLM-based agents have experienced significant development . Unlike the traditionalmethod of using LLMs for text-based reasoning, such as Chain-of-Thought , LLM-based agentstypically involve interaction with the environment, adjusting the output in a closed-loop mannerbased on environmental information. These LLM-based agents, now fortified with capabilities likeMemorizing , Tool-use , and Planning , exhibit a marked enhancement in their overall efficacy. Although this paper mainly",
  "focuses on the planning capability of LLM-based agents, we believe AGENTGEN has the potential togeneralize to other scenarios of LLM-based agents": "Planning with Large Language Models.Planning is one of the key applications of LLM-basedagents, applicable in various scenarios such as robotic planning , travelplanning , calendar scheduling , code generation and others . It is typically definedas the process of systematically determining a sequence of actions or steps required to achieve adesired goal from an initial state, considering constraints and available resources. This definitionprimarily differentiates from studies that utilize LLMs to generate ungrounded plans as guidancefor problem-solving , rather than directly producing executable actions. Planning can becategorized into two types: open-loop planning, where the LLM outputs an entire action sequencebefore execution , and closed-loop planning, where the LLM-based agent decides the nextaction based on real-time environmental interaction after executing a previous action . This paper mainly focuses on close-loop planning, which is more adaptable forerror correction, human interaction, and environmental grounding. Recent studies on close-loopplanning have integrated chain-of-thought reasoning into the planning process . Additionally,some papers have explored the use of tree-search methods to enhance the performance of LLMplanning . Instead of designing novel frameworks or engaging in promptengineering, this paper explores how training can enhance the planning capabilities of LLM-basedagents. Agent Training.Recently, numerous studies have aimed to enhance LLM-based agent capabilitiesby incorporating agent trajectory data into their training . Advanced works suchas AgentTuning utilize GPT-4 to generate trajectory data across six distinct environments.Subsequently, this data is filtered and employed in training Large Language Models, enhancing theagent capabilities of base models. Another work, FireAct , proposes training with both CoT data 3AgentTuning utilized Alfworlds training set, meaning Alfworld cannot be considered an out-of-domaintask. Consequently, we did not evaluate the performance of AgentLM or the AgentTuning-trained model onAlfworld. and ReAct format data, enabling the model to discern when to use reasoning to solve problems andwhen to call external tools. Agent LUMOS suggests separately training Planning and Groundingmodels, enabling LLM-based agents to learn to decompose complex problems before execution.LLM-Modulo framwork proposes to leverage LLMs generating candidate plans and verify themwith an external verifier. Then, use the verified trajectories for fine-tuning LLMs. Similarly, takesa generate-test loop to synthesize trajectories for LLM training. Unlike previous papers on all agenttraining, AGENTGEN goes beyond merely generating trajectory data using Large Language Models.Instead, we utilize Large Language Models to generate agent environments, which can be considereda more foundational application. As a result, we have constructed over 500 environments for training,whereas previous works typically use fewer than 10 environments to synthesize agent data. Environment and Task Generation with Large Language Models.The utilization of LLMs togenerate environments and tasks is an emerging application. Some studies have explored utilizingLLMs to generate layouts in robotic simulations, typically involving the creation of configurationfiles . While these methods can construct numerous scene-level environments, they oftenstruggle to achieve diversity at the underlying mechanism level. AgentTuning employs a taskgeneration approach similar to the Self-instruct method, using the test set as seed data. Thisapproach not only poses a risk of data leakage but also leads to insufficient diversity in task difficulty.ByteSized32 uses LLMs to generate Python-based games based on predefined task specificationsautomatically. Similarly, other works leverage LLMs to automatically construct PDDL domainsbased on a task specification. In contrast to these studies, this paper proposes using a diverse textcorpus to generate environment code automatically. This approach facilitates the creation of a widerange of rich environments without predefined definitions.",
  "Conclusion": "In this paper, we explore using LLMs to automatically generate environment and planning tasks forLLM-based agent training. Specifically, for generating diverse environments, we propose utilizing aninspiration corpus composed of various domain-specific text segments as the context for environmentsynthesis. To enhance the difficulty diversity of generated planning tasks, we introduce a bidirectionalevolution method, BI-EVOL, which evolves planning tasks from both easier and more challengingdirections to create a task set with a more gradual difficulty curve, thereby improving the effectivenessof LLM learning. Based on AGENTGEN, we developed a dataset consisting of 592 environments and7246 trajectories and trained it on a series of LLMs. The AGENTGEN-tuned Llama-3.1-8B modelsurpassed GPT-3.5 on planning tasks, while the AGENTGEN-tuned Llama-3.1-70B model achieved anew state-of-the-art performance.",
  "Daman Arora and Subbarao Kambhampati. Learning and leveraging verifiers to improveplanning capabilities of pre-trained language models. arXiv preprint arXiv:2305.17077, 2023": "Ramakrishna Bairi, Atharv Sonwane, Aditya Kanade, Arun Iyer, Suresh Parthasarathy, SriramRajamani, B Ashok, and Shashank Shet. Codeplan: Repository-level coding using llms andplanning. Proceedings of the ACM on Software Engineering, 1(FSE):675698, 2024. Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, DanielHo, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, et al. Do as i can, not as i say: Groundinglanguage in robotic affordances. In Conference on robot learning, pages 287318. PMLR, 2023.",
  "Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, and Shunyu Yao.Fireact: Toward language agent fine-tuning. arXiv preprint arXiv:2310.05915, 2023": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, JaredKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating largelanguage models trained on code. arXiv preprint arXiv:2107.03374, 2021. Zehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Jiangning Liu, Dahua Lin, Kai Chen,and Feng Zhao. Agent-flan: Designing data and methods of effective agent tuning for largelanguage models. arXiv preprint arXiv:2403.12881, 2024. Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, CaimingXiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, et al. Binding language models insymbolic languages. arXiv preprint arXiv:2210.02875, 2022. Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, ChitwanSaharia, Thien Huu Nguyen, and Yoshua Bengio. Babyai: A platform to study the sampleefficiency of grounded language learning. arXiv preprint arXiv:1810.08272, 2018.",
  "Yan Ding, Xiaohan Zhang, Chris Paxton, and Shiqi Zhang. Task and motion planning withlarge language models for object rearrangement, 2023": "Zeyu Gao, Yao Mu, Jinye Qu, Mengkang Hu, Lingyue Guo, Ping Luo, and Yanfeng Lu. Dag-plan: Generating directed acyclic dependency graphs for dual-arm cooperative planning. arXivpreprint arXiv:2406.09953, 2024. Lin Guan, Karthik Valmeekam, Sarath Sreedharan, and Subbarao Kambhampati. Leveragingpre-trained large language models to construct and utilize world models for model-based taskplanning. Advances in Neural Information Processing Systems, 36:7908179094, 2023.",
  "Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhit-ing Hu.Reasoning with language model is planning with world model.arXiv preprintarXiv:2305.14992, 2023": "Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Ct, and Xingdi Yuan. Inter-active fiction games: A colossal adventure. In Proceedings of the AAAI Conference on ArtificialIntelligence, volume 34, pages 79037910, 2020. Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, ZiliWang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. MetaGPT: Meta programming formulti-agent collaborative framework. arXiv preprint arXiv:2308.00352, 2023.",
  "Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXivpreprint arXiv:2106.09685, 2021": "Mengkang Hu, Yao Mu, Xinmiao Yu, Mingyu Ding, Shiguang Wu, Wenqi Shao, Qiguang Chen,Bin Wang, Yu Qiao, and Ping Luo. Tree-planner: Efficient close-loop task planning with largelanguage models. arXiv preprint arXiv:2310.08582, 2023. Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models aszero-shot planners: Extracting actionable knowledge for embodied agents. In Internationalconference on machine learning, pages 91189147. PMLR, 2022. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng,Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, TomasJackson, Linda Luu, Sergey Levine, Karol Hausman, and Brian Ichter. Inner monologue:Embodied reasoning through planning with language models, 2022. Wenlong Huang, Fei Xia, Dhruv Shah, Danny Driess, Andy Zeng, Yao Lu, Pete Florence, IgorMordatch, Sergey Levine, Karol Hausman, and Brian Ichter. Grounded decoding: Guiding textgeneration with grounded models for robot control, 2023.",
  "Ahmed Hussein, Mohamed Medhat Gaber, Eyad Elyan, and Chrisina Jayne. Imitation learning:A survey of learning methods. ACM Computing Surveys (CSUR), 50(2):135, 2017": "Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra SinghChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, LucileSaulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Leslie Pack Kaelbling and Toms Lozano-Prez. Hierarchical task and motion planning in thenow. In 2011 IEEE International Conference on Robotics and Automation, pages 14701477,2011. doi: 10.1109/ICRA.2011.5980391. Subbarao Kambhampati, Karthik Valmeekam, Lin Guan, Kaya Stechly, Mudit Verma, SiddhantBhambri, Lucas Saldyt, and Anil Murthy. Llms cant plan, but can help planning in llm-moduloframeworks. arXiv preprint arXiv:2402.01817, 2024. Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wen-tau Yih, Daniel Fried, Sida Wang, and Tao Yu. Ds-1000: A natural and reliable benchmarkfor data science code generation. In International Conference on Machine Learning, pages1831918345. PMLR, 2023. Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li,Fei Huang, and Yongbin Li. Api-bank: A comprehensive benchmark for tool-augmented llms.arXiv preprint arXiv:2304.08244, 2023. Xinnian Liang, Bing Wang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, andZhoujun Li. Unleashing infinite-length input capacity for large-scale language models withself-controlled memory system. arXiv e-prints, pages arXiv2304, 2023. Bill Yuchen Lin, Chengsong Huang, Qian Liu, Wenda Gu, Sam Sommerer, and Xiang Ren.On grounded planning for embodied tasks with language models. In Proceedings of the AAAIConference on Artificial Intelligence, volume 37, pages 1319213200, 2023. Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and PeterStone. Llm+ p: Empowering large language models with optimal planning proficiency. arXivpreprint arXiv:2304.11477, 2023. Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, and Guannan Zhang.Think-in-memory: Recalling and post-thinking enable llms with long-term memory. arXivpreprint arXiv:2311.08719, 2023.",
  "Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding,Kaiwen Men, Kejuan Yang, et al. AgentBench: Evaluating llms as agents. arXiv preprintarXiv:2308.03688, 2023": "Yanming Liu, Xinyue Peng, Yuwei Zhang, Jiannan Cao, Xuhong Zhang, Sheng Cheng, XunWang, Jianwei Yin, and Tianyu Du. Tool-planner: Dynamic solution tree planning for largelanguage model with tool clustering. arXiv preprint arXiv:2406.03807, 2024. Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng,Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical rea-soning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583,2023. Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, JingMa, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language modelswith evol-instruct. arXiv preprint arXiv:2306.08568, 2023. Chang Ma, Junlei Zhang, Zhihao Zhu, Cheng Yang, Yujiu Yang, Yaohui Jin, Zhenzhong Lan,Lingpeng Kong, and Junxian He. Agentboard: An analytical evaluation board of multi-turn llmagents. arXiv preprint arXiv:2401.13178, 2024.",
  "Meta AI. Introducing meta Llama 3: The most capable openly available LLM to date, April2024. URL Accessed: 2024-04-18": "Yao Mu, Junting Chen, Qinglong Zhang, Shoufa Chen, Qiaojun Yu, Chongjian Ge, RunjianChen, Zhixuan Liang, Mengkang Hu, Chaofan Tao, et al. Robocodex: Multimodal codegeneration for robotic behavior synthesis. arXiv preprint arXiv:2402.16117, 2024. Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang,Jifeng Dai, Yu Qiao, and Ping Luo. Embodiedgpt: Vision-language pre-training via embodiedchain of thought. Advances in Neural Information Processing Systems, 36, 2024.",
  "Aaron Parisi, Yao Zhao, and Noah Fiedel. Talm: Tool augmented language models. arXivpreprint arXiv:2205.12255, 2022": "Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and AntonioTorralba. Virtualhome: Simulating household activities via programs. In Proceedings of theIEEE conference on computer vision and pattern recognition, pages 84948502, 2018. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong,Xiangru Tang, Bill Qian, et al. ToolLLM: Facilitating large language models to master 16000+real-world apis. arXiv preprint arXiv:2307.16789, 2023. Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan,Yossi Adi, Jingyu Liu, Tal Remez, Jrmy Rapin, et al. Code llama: Open foundation modelsfor code. arXiv preprint arXiv:2308.12950, 2023. Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing Du, ShiweiShi, Hangyu Mao, Xingyu Zeng, and Rui Zhao. Tptu: Task planning and tool usage of largelanguage model-based ai agents. arXiv preprint arXiv:2308.03427, 2023.",
  "Stuart J Russell and Peter Norvig. Artificial intelligence: a modern approach. Pearson, 2016": "Timo Schick, Jane Dwivedi-Yu, Roberto Dess, Roberta Raileanu, Maria Lomeli, Luke Zettle-moyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teachthemselves to use tools. CoRR, abs/2302.04761, 2023. doi: 10.48550/ARXIV.2302.04761.URL Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hug-ginggpt: Solving AI tasks with chatgpt and its friends in huggingface. CoRR, abs/2303.17580,2023. doi: 10.48550/ARXIV.2303.17580. URL Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan, and Shunyu Yao.Reflexion: Language agents with verbal reinforcement learning. In Thirty-seventh Conferenceon Neural Information Processing Systems, 2023. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Ct, Yonatan Bisk, Adam Trischler, andMatthew Hausknecht. Alfworld: Aligning text and embodied environments for interactivelearning. arXiv preprint arXiv:2010.03768, 2020. Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay,Dieter Fox, Jesse Thomason, and Animesh Garg. Progprompt: Generating situated robot taskplans using large language models. In 2023 IEEE International Conference on Robotics andAutomation (ICRA), pages 1152311530. IEEE, 2023.",
  "Simeng Sun, Yang Liu, Shuohang Wang, Chenguang Zhu, and Mohit Iyyer. Pearl: Promptinglarge language models to plan and execute actions over long documents. arXiv preprintarXiv:2305.14564, 2023": "Jihoon Tack, Jaehyung Kim, Eric Mitchell, Jinwoo Shin, Yee Whye Teh, and Jonathan RichardSchwarz. Online adaptation of language models with a memory of amortized contexts. arXivpreprint arXiv:2403.04317, 2024. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Openfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Karthik Valmeekam, Matthew Marquez, Alberto Olmo, Sarath Sreedharan, and SubbaraoKambhampati. Planbench: An extensible benchmark for evaluating large language models onplanning and reasoning about change. Advances in Neural Information Processing Systems, 36,2024.",
  "Boshi Wang, Hao Fang, Jason Eisner, Benjamin Van Durme, and Yu Su. Llms in the imag-inarium: tool learning through simulated trial and error. arXiv preprint arXiv:2403.04746,2024": "Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen,Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomousagents. arXiv preprint arXiv:2308.11432, 2023. Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-PengLim. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by largelanguage models. arXiv preprint arXiv:2305.04091, 2023. Lirui Wang, Yiyang Ling, Zhecheng Yuan, Mohit Shridhar, Chen Bao, Yuzhe Qin, BailinWang, Huazhe Xu, and Xiaolong Wang. Gensim: Generating robotic simulation tasks via largelanguage models. arXiv preprint arXiv:2310.01361, 2023. Renxi Wang, Haonan Li, Xudong Han, Yixuan Zhang, and Timothy Baldwin. Learning fromfailure: Integrating negative examples when fine-tuning large language models as agents. arXivpreprint arXiv:2402.11651, 2024. Ruoyao Wang, Graham Todd, Eric Yuan, Ziang Xiao, Marc-Alexandre Ct, and Peter Jansen.Bytesized32: A corpus and challenge task for generating task-specific world models expressedas text games. arXiv preprint arXiv:2305.14879, 2023. Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic,Eric P Xing, and Zhiting Hu. Promptagent: Strategic planning with language models enablesexpert-level prompt optimization. arXiv preprint arXiv:2310.16427, 2023. Yufei Wang, Zhou Xian, Feng Chen, Tsun-Hsuan Wang, Yian Wang, Katerina Fragkiadaki,Zackory Erickson, David Held, and Chuang Gan. Robogen: Towards unleashing infinite datafor automated robot learning via generative simulation. arXiv preprint arXiv:2311.01455, 2023.",
  "Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan andselect: Interactive planning with large language models enables open-world multi-task agents,2023": "Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, NanDu, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXivpreprint arXiv:2109.01652, 2021. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.Advances in Neural Information Processing Systems, 35:2482424837, 2022. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li,Li Jiang, Xiaoyun Zhang, and Chi Wang. AutoGen: Enabling next-gen llm applications viamulti-agent conversation framework. arXiv preprint arXiv:2308.08155, 2023.",
  "Zhenyu Wu, Ziwei Wang, Xiuwei Xu, Jiwen Lu, and Haibin Yan. Embodied task planning withlarge language models, 2023": "Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang,Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language modelbased agents: A survey. arXiv preprint arXiv:2309.07864, 2023. Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao,and Yu Su. Travelplanner: A benchmark for real-world planning with language agents. arXivpreprint arXiv:2402.01622, 2024. Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan Weng, Yitao Liu, Toh JingHua, Junning Zhao, Qian Liu, Che Liu, Leo Z. Liu, Yiheng Xu, Hongjin Su, DongchanShin, Caiming Xiong, and Tao Yu. Openagents: An open platform for language agents inthe wild. CoRR, abs/2310.10634, 2023. doi: 10.48550/ARXIV.2310.10634. URL Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, andDaxin Jiang. Wizardlm: Empowering large language models to follow complex instructions.arXiv preprint arXiv:2304.12244, 2023. Yue Yang, Fan-Yun Sun, Luca Weihs, Eli VanderBilt, Alvaro Herrasti, Winson Han, Jiajun Wu,Nick Haber, Ranjay Krishna, Lingjie Liu, et al. Holodeck: Language guided generation of 3dembodied ai environments. In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 1622716237, 2024.",
  "Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629,2022": "Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and KarthikNarasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXivpreprint arXiv:2305.10601, 2023. Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, RitheshMurthy, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, et al. Retroformer: Retrospective largelanguage agents with policy gradient optimization. arXiv preprint arXiv:2308.02151, 2023. Da Yin, Faeze Brahman, Abhilasha Ravichander, Khyathi Chandu, Kai-Wei Chang, YejinChoi, and Bill Yuchen Lin. Lumos: Learning agents with unified data, modular design, andopen-source llms. arXiv preprint arXiv:2311.05657, 2023.",
  "Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang.Agenttuning: Enabling generalized agent abilities for llms. arXiv preprint arXiv:2310.12823,2023": "Jianguo Zhang, Tian Lan, Rithesh Murthy, Zhiwei Liu, Weiran Yao, Juntao Tan, Thai Hoang,Liangwei Yang, Yihao Feng, Zuxin Liu, et al. Agentohana: Design unified data and trainingpipeline for effective agent learning. arXiv preprint arXiv:2402.15506, 2024. Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. Expel:Llm agents are experiential learners. In Proceedings of the AAAI Conference on ArtificialIntelligence, volume 38, pages 1963219642, 2024.",
  "Zirui Zhao, Wee Sun Lee, and David Hsu. Large language models as commonsense knowledgefor large-scale task planning. Advances in Neural Information Processing Systems, 36, 2024": "Huaixiu Steven Zheng, Swaroop Mishra, Hugh Zhang, Xinyun Chen, Minmin Chen, AzadeNova, Le Hou, Heng-Tze Cheng, Quoc V Le, Ed H Chi, et al. Natural plan: Benchmarkingllms on natural language planning. arXiv preprint arXiv:2406.04520, 2024. Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. Memorybank: Enhancinglarge language models with long-term memory. In Proceedings of the AAAI Conference onArtificial Intelligence, volume 38, pages 1972419731, 2024. Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang.Language agent tree search unifies reasoning acting and planning in language models. arXivpreprint arXiv:2310.04406, 2023. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma,Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. Advances in NeuralInformation Processing Systems, 36, 2024. Denny Zhou, Nathanael Schrli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, DaleSchuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enablescomplex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022. Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, BinLi, Lewei Lu, Xiaogang Wang, et al. Ghost in the minecraft: Generally capable agents foropen-world environments via large language models with text-based knowledge and memory.arXiv preprint arXiv:2305.17144, 2023.",
  "Natural Language Mapping Generation": "I would like you to create natural language mapping for PDDL.The form of the natural language mapping is a Python dictionary, wherein1. The key corresponds to the name of a predicate or action within the domain PDDL.2. The value is its equivalent in natural language, with parameters presented in \"{argn}\",where n is the index of its parameters in the PDDL expression.3.You must ensure that the number of \"{}\" corresponds precisely to the number ofparameters in predicates or actions.4. You should very carefully check the order of {argn}.",
  "Example:PDDL Domain:pddl(define (domain hanoi)(:requirements :strips)(:predicates(clear ?x)(on ?x ?y)(smaller ?x ?y))": "(:action move:parameters (?disc ?from ?to):precondition (and (smaller ?to ?disc) (on ?disc ?from)(clear ?disc) (clear ?to)):effect (and (clear ?from) (on ?disc ?to) (not (on ?disc ?from))(not (clear ?to)))))Specification:Your goal is to solve the Tower of Hanoi puzzle, which involves moving a stack of discs fromone peg to another, with the restriction that no disc may be placed on top of a smaller disc.",
  "The puzzle is solved when all the discs are moved to the target peg following these rules": "The actions defined in this domain include:- move <disc> <from> <to>: This action allows moving a disc from one peg to another. Thepreconditions for this action are that the target peg is smaller than the disc being moved, thedisc is on the source peg, and both the disc and the target peg are clear (i.e., there is no discon top of them). The effect of this action is that the source peg becomes clear, the disc is nowon the target peg, the disc is no longer on the source peg, and the target peg is no longer clear. You have the following restrictions on your actions:- A disc can only be moved if it is clear, meaning there is no other disc on top of it.- A disc can only be placed on another disc or peg that is larger than itself.- A disc can only be moved to a peg that is clear.- Once a disc is moved from a peg, that peg becomes clear.- Once a disc is placed on a peg, that peg is no longer clear.Natural Language Mapping:python{\"clear\": \"{arg1} is clear.\",\"on\": \"{arg1} is on {arg2}.\",\"smaller\": \"{arg1} is smaller than {arg2}.\",\"move\": \"Move {arg1} from {arg2} to {arg3}.\"}",
  "B.1Environment Specification": "We analyzed the token distribution within the environmental specifications. Among the 592 environ-mental specifications, the average token count is 473.55, with a median of 467.00. The minimumtoken count is 207, and the maximum is 934. As depicted in , the number of specificationtokens for the environment is predominantly concentrated within the range of 300 to 699.",
  "B.2Environment Implementation": "The scale of action space and state space in an environment typically dictates its complexity, witha greater number of actions and states generally indicating a more complex environment. Anenvironment library with a greater variety of difficulty levels is preferable for a training set. As shownin , there is a significant diversity in the number of actions and predicates.",
  "B.3Diversity Analysis": "We evaluate the diversity of generated environments using cosine similarity. More specifically, werandomly sampled 100 environment specifications for better visualization and converted them intoTF-IDF vectors. After calculating the cosine similarity matrix between all pairs of specifications, wevisualize the matrix using heatmap as is shown in . The computed average cosine similarity 200-299 300-399 400-499 500-599 600-699 700-799 800-899 900+ Number of Tokens",
  "Environment Specification": "You are a nutritionist tasked with creating a new healthy recipe book that incorporates peanutbutter powder as a key ingredient. Your environment consists of your office, a computer forresearch and writing, a kitchen for testing recipes, and a collection of ingredients includingpeanut butter powder. The computer can be used to write and save recipes, but it needs to becharged after every three hours of use. The actions defined in this domain include:- research_ingredient <location> <ingredient> <nutritionist>:This action allows thenutritionist to research peanut butter powder at the office. The effect of this action is that thenutritionist gains knowledge about the nutritional benefits and potential uses of peanut butterpowder in recipes. - develop_recipe <location> <computer> <nutritionist> <ingredient>: This action allowsthe nutritionist to create a new recipe using peanut butter powder. The precondition for thisaction is that the nutritionist has researched peanut butter powder. The effect of this action isa draft of a new recipe that incorporates peanut butter powder.- test_recipe <location> <kitchen> <nutritionist> <recipe>: This action allows the nutritionistto test the drafted recipe in the kitchen. The preconditions for this action are that thenutritionist has developed a recipe and has all the necessary ingredients, including peanutbutter powder. The effect of this action is feedback on the recipes taste, nutritional value,and overall feasibility.- finalize_recipe <location> <computer> <nutritionist> <tested_recipe>: This action allowsthe nutritionist to finalize the recipe after testing and receiving feedback. The preconditionsfor this action are that the nutritionist has tested the recipe and the computer is charged. Theeffect of this action is a completed recipe that is ready to be included in the healthy recipebook. You have the following restrictions on your actions:- You can only research peanut butter powder if you are in the office where your researchmaterials are located.- You can only develop a recipe after researching peanut butter powder and must have acharged computer.- You can only test a recipe in the kitchen if you have developed a recipe and have all thenecessary ingredients.- You can only finalize a recipe after testing it in the kitchen and receiving feedback, and ifyour computer is charged.",
  "Trajectory Data": "(User) You are a nutritionist tasked with creating a new healthy recipe book that incorporatespeanut butter powder as a key ingredient. Your environment consists of your office, acomputer for research and writing, a kitchen for testing recipes, and a collection of ingredientsincluding peanut butter powder. The computer can be used to write and save recipes, but it",
  "needs to be charged after every three hours of use": "The actions defined in this domain include:- research_ingredient <location> <ingredient> <nutritionist>:This action allows thenutritionist to research peanut butter powder at the office. The effect of this action is that thenutritionist gains knowledge about the nutritional benefits and potential uses of peanut butterpowder in recipes.- develop_recipe <location> <computer> <nutritionist> <ingredient>: This action allowsthe nutritionist to create a new recipe using peanut butter powder. The precondition for thisaction is that the nutritionist has researched peanut butter powder. The effect of this action isa draft of a new recipe that incorporates peanut butter powder.- test_recipe <location> <kitchen> <nutritionist> <recipe>: This action allows the nutritionistto test the drafted recipe in the kitchen. The preconditions for this action are that thenutritionist has developed a recipe and has all the necessary ingredients, including peanutbutter powder. The effect of this action is feedback on the recipes taste, nutritional value,and overall feasibility.- finalize_recipe <location> <computer> <nutritionist> <tested_recipe>: This action allowsthe nutritionist to finalize the recipe after testing and receiving feedback. The preconditionsfor this action are that the nutritionist has tested the recipe and the computer is charged. Theeffect of this action is a completed recipe that is ready to be included in the healthy recipebook. You have the following restrictions on your actions:- You can only research peanut butter powder if you are in the office where your researchmaterials are located.- You can only develop a recipe after researching peanut butter powder and must have acharged computer.- You can only test a recipe in the kitchen if you have developed a recipe and have all thenecessary ingredients.- You can only finalize a recipe after testing it in the kitchen and receiving feedback, and ifyour computer is charged.Goal: The goal is to satisfy the following conditions: The computer is charged. jordan hastested the recipe almond_butter_bars.Observation:The computer is charged. The recipe almond_butter_bars has all the necessaryingredients. jordan has a recipe draft for almond_butter_bars. jordan has researched peanutbutter. jordan is in the kitchen.(Assistant) Action: jordan tests the recipe almond_butter_bars.(User) Observation: The computer is charged. The recipe almond_butter_bars has allthe necessary ingredients. jordan has a recipe draft for almond_butter_bars. jordan hasresearched peanut butter. jordan has tested the recipe almond_butter_bars. jordan is in thekitchen.(Assistant) Action: jordan develops a recipe using almond_butter_bars."
}