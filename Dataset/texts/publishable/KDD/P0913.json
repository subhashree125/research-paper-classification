{
  "ABSTRACT": "Federated Graph Learning (FGL) aims to learn graph learning mod-els over graph data distributed in multiple data owners, which hasbeen applied in various applications such as social recommenda-tion and financial fraud detection. Inherited from generic FederatedLearning (FL), FGL similarly has the data heterogeneity issue wherethe label distribution may vary significantly for distributed graphdata across clients. For instance, a client can have the majorityof nodes from a class, while another client may have only a fewnodes from the same class. This issue results in divergent localobjectives and impairs FGL convergence for node-level tasks, es-pecially for node classification. Moreover, FGL also encounters aunique challenge for the node classification task: the nodes from aminority class in a client are more likely to have biased neighboringinformation, which prevents FGL from learning expressive nodeembeddings with Graph Neural Networks (GNNs). To grapple withthe challenge, we propose FedSpray, a novel FGL framework thatlearns local class-wise structure proxies in the latent space andaligns them to obtain global structure proxies in the server. Ourgoal is to obtain the aligned structure proxies that can serve as reli-able, unbiased neighboring information for node classification. Toachieve this, FedSpray trains a global feature-structure encoder andgenerates unbiased soft targets with structure proxies to regularizelocal training of GNN models in a personalized way. We conductextensive experiments over four datasets, and experiment resultsvalidate the superiority of FedSpray compared with other baselines.Our code is available at",
  "INTRODUCTION": "Graph Neural Networks (GNNs) are a prominent approachfor learning expressive representations from graph-structured data.Typically, GNNs follow a message-passing mechanism, where theembedding of each node is computed by aggregating attribute in-formation from its neighbors . Thanks to their powerfulcapacity for jointly embedding attribute and graph structure in-formation, GNNs have been widely adopted in a wide variety ofapplications, such as node classification and link predic-tion . The existing GNNs are mostly trained in a centralizedmanner where graph data is collected on a single machine beforetraining. In the real world, however, a large number of graph datais generated by multiple data owners. These graph data cannot beassembled for training due to privacy concerns and commercialcompetitions , which prevents the traditional centralized man-ner from training powerful GNNs. Taking a financial system withfour banks in as an example, each bank in the system hasits local customer dataset and transactions between customers. Aswe take the customers in a bank as nodes and transactions betweenthem as edges, the banks local data can naturally form a graph.These banks aim to jointly train a GNN model for classificationtasks, such as predicting a customers occupation (i.e., Doctor orTeacher) without sharing their local data with each other.Federated Learning (FL) is a prevalent distributed learningscheme that enables multiple data owners (i.e., clients) to collab-oratively train machine learning models under the coordinationof a central server without sharing their private data. One criticalchallenge in FL is data heterogeneity, where data samples are notindependent and identically distributed (i.e., non-IID) across theclients. For instance, assume that Bank A in locates in acommunity adjacent to a hospital. Then most customers in BankA are therefore likely to be labeled as Doctor while only a few cus-tomers are from other occupations (e.g., Teacher). In contrast, BankC adjoining a school has customers labeled mostly as Teacher andonly a few as Doctor. Typically, the nodes from a class that claimsthe very large proportion of the overall data in a client are themajority nodes (e.g., Doctor in Bank A) while minority nodes (e.g.,Teacher in Bank A) account for much fewer samples. The data het-erogeneity issue results in divergent local objectives on the clientsand consequently impairs the performance of FL . A number",
  "Model": ": An example of a financial system including fourbanks. The four banks aim to jointly train a model for pre-dicting a customers occupation (i.e., Doctor or Teacher) or-chestrated by a third-party company over their local datawhile keeping their private data locally. of approaches have been proposed to address this issue, to name afew .When we train GNNs over distributed graph data in a federatedmanner, however, the data heterogeneity issue can get much moresevere. This results from a unique challenge in Federated GraphLearning (FGL) : the high heterophily of minority nodes,i.e., their neighbors are mostly from other classes . A majoritynode in a client (e.g., Teacher in Bank D) can benefit from themessage-passing mechanism and obtain an expressive embeddingas its neighbors are probably from the same class. On the contrary,a minority node in another client (e.g., Teacher in Bank A) mayobtain biased information from its neighbors when they are fromother classes (e.g., Doctor in Bank A). In FGL, this challenge isusually entangled with the data heterogeneity issue. As a result, theminority nodes will finally get underrepresented embeddings givenadverse neighboring information and be more likely to be predictedas the major class, which results in unsatisfactory performance.Although a few studies have investigated the data heterogeneityissue about graph structures in FGL , they did not fathomthe divergent impact of neighboring information across clients fornode classification.To tackle the aforementioned challenges in FGL, we proposeFedSpray, a novel FGL framework with structure proxy alignmentin this study. The goal of FedSpray is to learn personalized GNNmodels for each client while avoiding underrepresented embed-dings of the minority nodes in each client caused by their adverseneighboring information in FGL. To achieve this goal, we first intro-duce global class-wise structure proxies which aim to providenodes with informative, unbiased neighboring information, espe-cially for those from the minority classes in each client. Moreover,FedSpray learns a global feature-structure encoder to obtain reli-able soft targets that only depend on node features and alignedstructure proxies. Then, FedSpray uses the soft targets to regu-larize local training of personalized GNN models via knowledgedistillation . We conduct extensive experiments over five graphdatasets, and experimental results corroborate the effectiveness ofthe proposed FedSpray compared with other baselines.We summarize the main contributions of this study as follows.",
  "Problem Formulation. We formulate and make an initialinvestigation on a unique issue of unfavorable neighboringinformation for minority nodes in FGL": "Algorithmic Design. We propose a novel framework Fed-Spray to tackle the above problem in FGL. FedSpray aimsto learn unbiased soft targets by a global feature-structureencoder with aligned class-wise structure proxies whichprovide informative, unbiased neighboring information fornodes and guide local training of personalized GNN models. Experimental Evaluation. We conduct extensive experi-ments over four graph datasets to verify the effectivenessof the proposed FedSpray. The experimental results demon-strate that our FedSpray consistently outperforms the state-of-the-art baselines.",
  "PROBLEM FORMULATION2.1Preliminaries": "2.1.1Notations. We use bold uppercase letters (e.g., X) to repre-sent matrices. For any matrix, e.g., X, we denote its -th row vectoras x. We use letters in calligraphy font (e.g., V) to denote sets. |V|denotes the cardinality of set V. 2.1.2Graph Neural Networks. Let G = (V, E, X) denote anundirected attributed graph, where V = {1, 2, , } is the setof |V| nodes, E is the edge set, and X R|V| is the node featurematrix. is the number of node features. Given each node V,N () denotes the set of its neighbors. The ground-truth label ofeach node V can be denoted as a -dimensional one-hotvector y where is the number of classes. The node homophily is defined as",
  "h = (h1, {h1: N ()};),(2)": "where h is the embedding of node after the -th layer , and is the parameters of the message-passing function in . Theraw feature of each node is used as the input layer, i.e., h0 = x.For the node classification task, the node embedding h after thefinal layer is used to compute the predicted label distribution y =Softmax(h ) R by the softmax operator.",
  "L() (D (); ()),(3)": "where L() ( ()) is the local average loss (e.g., the cross-entropyloss) over local data in client , and = =1 (). Standard FLmethods aim to learn a global model = (1) = (2) = = (). As a representative method in FL, FedAvg performs localupdates in each client and uploads local model parameters to a",
  "Given a set of clients, each client owns a local graph G() =(V (), E (), X()). For the labeled node set V () V () in client": ", each node () V ()is associated with its label y(). The goalof these clients is to train personalized GNN models ( ()) in eachclient for the node classification task while keeping their privategraph data locally. Based on the aforementioned challenge and pre-liminary analysis, this study aims to enhance collaborative trainingby mitigating the impact of adverse neighboring information onnode classification, especially for minority nodes.",
  "MOTIVATION": "In this section, we first conduct an empirical study on the PubMeddataset to investigate the impact of divergent neighboring in-formation across clients on minority nodes when jointly trainingGNNs in FGL. The observation from this study is consistent withour example in and motivates us to learn global structureproxies as favorable neighboring information. We then develop the-oretical analysis to explain how aligning neighboring informationacross clients can benefit node classification tasks in FGL.",
  "Empirical Observations": "To better understand the divergent neighboring information acrossclients with its impact on the node classification task in FGL, weconduct preliminary experiments to compare the performance offederated node classification with MLP and GNNs as local modelson the PubMed dataset . Following the data partition strategy inprevious studies , we synthesize the distributed graph databy splitting each dataset into multiple communities via the Louvain Client ID Accuracy of Minority Nodes FedAvg with MLPFedAvg with GNN",
  ": Classification accuracy (%) of minority nodes ineach client by training MLP and GNN via FedAvg over thePubMed dataset. Average accuracy for all nodes: 82.35% forMLP VS 87.06% for GNN": "algorithm . We retain seven communities with the largest numberof nodes; each community is regarded as an entire graph in a client. shows the statistics of each client. According to ,although one client may have the majority class different fromanother, the average node-level homophily of the majority classis consistently higher than that of the other classes for all theclients. For instance, the nodes in client 2 that do not belong toclass 1 have only 24% neighbors from the same class on average. Itmeans that the minority nodes will absorb unfavorable neighboringinformation via GNNs and probably be classified incorrectly.To validate our conjecture, we perform collaborative trainingfor MLPs and GNNs following the standard FedAvg over thePubMed dataset. illustrates the classification accuracy ofminority nodes in each client by MLPs and GNNs. We can observethat MLPs consistently perform better than GNNs on minoritynodes across the clients, although GNNs have higher overall accu-racy for all nodes. Given that MLPs and GNNs are trained over thesame node label distribution, we argue that the performance gapon minority nodes results from aggregating adverse neighboringinformation from other classes via the message-passing mechanismin GNNs, especially from the majority class. On the contrary, MLPsonly need node features and do not require neighboring informa-tion throughout the training; therefore, they can avoid predictingmore nodes as the majority class.",
  "Theoretical Motivation": "According to the above empirical observations, minority nodeswith the original neighboring information are more likely to bemisclassified. One straightforward approach to this issue is enablingnodes to leverage favorable neighboring information from otherclients for generating node embeddings. Specifically, we considerconstructing global neighboring information in the feature space.The server collects neighboring feature vectors from each clientand computes the global class-wise neighboring information viaFedAvg . We aim to theoretically investigate whether the globalneighboring information can benefit node classification tasks whenreplacing the original neighbors of nodes. Following prevalent waysof graph modeling , we first generate random graphs ineach client using a variant of contextual stochastic block model with two classes.",
  "(b) Feature-structure encoder(a) The overview of FedSpray": ": (a) An overview of the proposed FedSpray. The backbone of FedSpray is personalized GNN models ( ()). A globalfeature-structure encoder () with structure proxies S is also employed in FedSpray to tackle underrepresented node embed-dings caused by adverse neighboring information in FGL. (b) An illustration of the feature-structure encoder in FedSpray.",
  "In the generated graph G() in client , the nodes are labeled bytwo classes 1 and 2. For each node (), its initial feature vec-": "tor x() R is sampled from a Gaussian distribution (1, I)if labeled as class 1 or (2, I) if labeled as class 2 (1 R ,2 R , and 1 2). For each client , a neighbor of each nodeis from the majority with probability () and from the minoritywith probability 1 (). The ratio of minority nodes and major-ity nodes is (). In our setting, we assume 1",
  "< () < 1 and0 < () < 1. We denote each graph generated from the abovestrategy in client as G() Gen(1, 2, (),())": "3.2.2Better Separability with Global Neighboring Informa-tion. To figure out the influence of global neighboring information,we focus on the separability of the linear GNN classifiers with thelargest margin when leveraging global neighboring information.Concretely, we aim to find the expected Euclidean distance fromeach class to the decision boundary of the optimal linear GNNclassifier when it uses either the original neighboring informationor the global neighboring information. We use and todenote the expected Euclidean distances in these two scenarios,respectively. We summarize the results in the following proposition.",
  "METHODOLOGY": "In this section, we present the proposed FedSpray in detail. (a) illustrates an overview of FedSpray. The goal of FedSpray isto let the clients learn personalized GNN models over their privategraph data while achieving higher performance by mitigating theimpact of adverse neighboring information in GNN models. Toreach this goal, FedSpray employs a lightweight global feature-structure encoder which learns class-wise structure proxies andaligns them on the central server. The feature-structure encodergenerates reliable unbiased soft targets for nodes given their rawfeatures and the aligned structure proxies to regularize local train-ing of GNN models.",
  "We first introduce personalized GNN models in FedSpray": "4.1.1GNN backbone Model. Considering their exceptional abil-ity to model graph data, we use GNNs as the backbone of the pro-posed framework. In this study, we propose to learn GNN modelsfor each client in a personalized manner to tackle the data hetero-geneity issue in FGL. Specifically, the personalized GNN model ( ()) in client outputs the predicted label distribution y()for",
  "Federated Graph Learning with Structure Proxy AlignmentKDD 24, August 2529, 2024, Barcelona, Spain": "where CE(, ) denotes the cross-entropy loss. However, simply min-imizing L()_ can lead () to overfitting during local training. In addition, the minority nodes are particularly prone toobtaining underrepresented embeddings due to biased neighboringinformation, as discussed above. To tackle this challenge, we pro-pose to design an extra knowledge distillation term and use it toregularize local training of (). More concretely, we first employthe soft target p() R for each node () V () generated bythe global feature-structure encoder to guide local training of ()",
  "Global Feature-Structure Encoder withStructure Proxies": "In this subsection, we will elucidate our design for the global feature-structure encoder and class-wise structure proxies in FedSpray. Thefeature-structure encoder aims to generate a reliable soft target (i.e.,p()) for each node with its raw features and structure proxy. 4.2.1Structure Proxies. As discussed above, a minority nodecan obtain adverse neighboring information from its neighbors viathe message-passing mechanism, given its neighbors are proba-bly from other classes. To mitigate this issue, we propose to learnunbiased class-wise structure proxies in FedSpray, providing favor-able neighboring information for each node. Here, we formulateeach structure proxy in a vectorial form. Let S R denoteclass-wise structure proxies, and each row s S denotes the -dimensional structure proxy of the -th node class. For each node () V (), its structure proxy s()will be s if it is from the -thclass. Then, the structure proxies will be used as the input of thefeature-structure encoder. 4.2.2Feature-Structure Encoder. In FedSpray, we employ alightweight feature-structure encoder to generate a reliable softtarget for a node with its raw feature and structure proxy as theinput. (b) illustrates our design for the feature-structureencoder. Let () denote the feature-structure encoder param-eterized by . Given a node () V (), the feature-structure",
  "where e()= (x();). Here, Combine(, ) is the operation to": "combine e()and s()together (e.g., addition).Structure proxies for unlabeled nodes. The feature-structureencoder can generate soft targets only for labeled nodes by Eq. (9)because the structure proxy s()requires the ground-truth label information of node v(). To better regularize local training of theGNN model, we need to obtain soft targets for unlabeled nodesand use them to compute L()_ by Eq. (6). To achieve this, wedesign a projector () in the feature-structure encoder. It hasthe same structure as the classifier . The difference is that theprojector generates soft targets only based on feature embed-",
  "Server Update": "As stated above, FedSpray will learn the feature-structure encoderand the structure proxies globally. In this subsection, we presentthe global update in the central server for the feature-structureencoder and the structure proxies, respectively. 4.3.1Update global feature-structure encoder. During eachround , the server performs weighted averaging of local feature-structure encoders following the standard FedAvg with eachcoefficient determined by the local node size",
  "FedSpray48.21 1.0329.72 0.7550.07 0.7528.46 2.1251.45 0.7227.52 0.42": "concern about uploading local structure proxies first. In fact, struc-ture proxies naturally protect data privacy. First, they are synthetic1D vectors to provide high-quality neighboring information in thelatent space. In other words, they do not possess any raw featureinformation. Second, they are generated by averaging the structureproxies from the same class, which is an irreversible operation.Moreover, we can employ various privacy-preserving techniquesto further improve confidentiality. 4.5.2Communication Efficiency. The proposed FedSpray re-quires clients to upload local feature-structure encoders and struc-ture proxies. As we introduced above, the feature-structure encoderis a relatively lightweight model. As for structure proxies, theirsize is generally much smaller than that of model parameters given . In addition, we can further reduce the number of uploadedparameters by setting smaller . 4.5.3Computational Cost. The additional computational costin FedSpray is mainly on local updates for the feature-structureencoder and structure proxies. Compared with GNN models, thefeature-structure encoder and structure proxies require fewer oper-ations for updating parameters. Training GNN models is usually time-consuming since GNN models need to aggregate node infor-mation via the message-passing mechanism during the forwardpass . However, the feature-structure encoder only incorpo-rates node features and structure proxies with fully connectedlayers to obtain soft targets. Therefore, the time complexity of localupdates for the feature-structure encoder and structure proxies willbe smaller than GNN models. Let , , and denote the numberof nodes of the local graph in a client, the number of node fea-tures, and the number of edges, respectively. Considering a 2-layerGCN model with hidden size , its computational complexity isapproximately ( + ). Similarly, we can conclude thatthe computational complexity of the feature-structure encoder withthe -dimensional structure proxy is about (), apparentlysmaller than the GCN model when we set = . Therefore, thefeature-structure encoder in FedSpray does not introduce signifi-cant extra computational costs compared with FedAvg using GCN.Furthermore, setting a smaller can also reduce computation costs.",
  "Experiment Setup": "5.1.1Datasets. We synthesize the distributed graph data basedon four common real-world datasets from various domains, i.e.,PubMed , WikiCS , Coauthor Physics , and Flickr .We follow the strategy in .1 to simulate the distributedgraph data and summarize the statistics and basic information aboutthe datasets in Appendix B.1. We randomly select nodes in clientsand let 40% for training, 30% for validation, and the remaining fortesting. We report the average classification accuracy for all nodesand minority nodes over the clients for five random repetitions. 5.1.2Baselines. We compare FedSpray with six baselines includ-ing (1) Local where each client train its GNN model individually; (2)FedAvg , the standard FL algorithm; (3) APFL , an adaptiveapproach in personalized FL; (4) GCFL , (5) FedStar , and(6) FedLit , three state-of-the-art FGL methods. More detailsabout the above baselines can be found in Appendix B.2. 5.1.3Hyperparameter setting. As stated previously, FedSprayis compatible with most existing GNN architectures. In the exper-iments, we adopt three representative ones as backbone models:GCN , SGC , and GraphSAGE . Each GNN model in-cludes two layers with a hidden size of 64. The size of featureembeddings and structure proxies is also set as 64. Therefore, thefeature-structure encoder has similar amounts of parameters withGNN models. Each component in the feature-structure encoderis implemented with one layer. We use an Adam optimizer with learning rates of 0.003 for the global feature-structure encoderand personalized GNN models, 0.02 for structure proxies. The twohyperparameters 1 and 2 are set as 5 and 1, respectively. We runall the methods for 300 rounds, and the local epoch is set as 5.",
  "Effectiveness of FedSpray": "We first show the performance of FedSpray and other baselines onnode classification over the four datasets with three backbone GNNmodels. reports the average classification accuracy on allnodes and minority nodes in the test set across clients.First, we analyze the results of overall accuracy on all test nodes.According to , our FedSpray consistently outperforms allthe baselines on node classification accuracy for overall test nodesacross clients. Local and FedAvg achieve comparable performance ds",
  ": Classification accuracy (%) of FedSpray on all nodesand minority nodes in the test sets with different over (a)WikiCS and (b) Physics with GCN": "over the four datasets. In the meantime, APFL does not surpassLocal and FedAvg. As for FGL methods, GCFL, FedStar, FedLit failto show remarkable performance gain. Although GCFL and FedStartackle the data heterogeneity issue of graph structures across clientsin FGL, they do not take the node-level heterophily into account.While FedLit models latent link types between nodes via multi-channel GNNs, it involves more GNN parameters that are hard tobe well trained within limited communication rounds.Second, we analyze the results of accuracy on minority nodesin the test set. Note that FedSpray aims to learn reliable unbiasedstructure information for guiding local training of personalizedGNN models, particularly for minority nodes. We can observe thatFedSpray outperforms all the baselines by a notable margin. Eventhough Local and FedAvg achieve comparable performance onoverall test nodes, they show different accuracy results on minoritynodes. Among the three FGL methods, FedStar encounters signifi-cant performance degradation on minority nodes since the designof structure embeddings in FedStar does not provide beneficialneighboring information for node-level tasks.",
  "Analysis of FedSpray": "5.3.1Influence of hyperparameter 1. The hyperparameter1 controls the contribution of the regularization term in L ().We conduct the sensitivity analysis on 1 in FedSpray. reports the classification accuracy of FedSpray on all nodes andtest nodes in the test sets with different values of 1 over PubMed(left) and WikiCS (right) with GraphSAGE. The accuracy on allnodes remains high when 1 is relatively small (i.e., 1 = 0.1, 1, 5).However, the accuracy of minority nodes will decrease when 1 istoo small because the feature-structure encoder cannot sufficientlyregularize local training of GNN models with too small 1. When1 gets too large, the accuracy of all nodes decreases in both figures.In this case, the regularization term weighs overwhelmingly inthe loss for training GNN models; then GNN models cannot besufficiently trained with label information. According to the aboveobservations, we will recommend 10 for PubMed with GraphSAGEand 5 for WikiCS with GraphSAGE as the best setting for 1.",
  "PhysicsFedSpray95.59 0.2480.98 1.39FedSpray (S = 0)93.23 0.2772.57 0.38": "the performance of FedSpray with different values of whilefixing the hidden dimension of the GNN model as 64. demonstrates the classification accuracy of FedSpray on all nodesand test nodes in the test sets with different values of overWikiCS (left) and Physics (right) with GCN as the backbone. Wecan observe that FedSpray can obtain comparable accuracy with smaller than 64 (e.g., = 32). In the meantime, FedSpray doesnot obtain significant performance gain when is larger than 64.From the above observation, we can reduce communication andcomputation costs by setting a smaller value such as 32. 5.3.3Effectiveness of structure proxies. In this study, we de-sign structure proxies in FedSpray to serve as global unbiased neigh-boring information for guiding local training of GNN models. Tovalidate the effectiveness of structure proxies, we investigate theperformance of the proposed framework when structure proxiesare removed. Specifically, we set class-wise structure proxies S as0 consistently during training. We report the performance of Fed-Spray with S = 0 over PubMed and WikiCS in . Accordingto , we can observe that FedSpray suffers from significantperformance degradation when removing structure proxies. It sug-gests that structure proxies play a significant role in FedSpray.Without them, the feature-structure encoder generates soft targetsonly based on node features . In this case, the soft labels canbe unreliable when node labels are not merely dependent on nodefeatures and, therefore, provide inappropriate guidance on localtraining of personalized GNN models in FedSpray.",
  "RELATED WORK6.1Federated Learning": "Recent years have witnessed the booming of techniques in FL and itsvarious applications in a wide range of domains, such as computervision , healthcare , and social recommendation . The most important challenge in FL is data heterogeneity acrossclients (i.e., the non-IID problem). A growing number of studieshave been proposed to mitigate the impact of data heterogeneity. Forinstance, FedProx adds a proximal term to the local trainingloss to keep the updated parameters close to the global model.Moon uses a contrastive loss to increase the distance betweenthe current and previous local models. FedDecorr mitigatesdimensional collapse to prevent representations from residing ina lower-dimensional space. In the meantime, a battery of studiesproposed personalized model-based methods. For example, pFedHN trains a central hypernetwork to output a unique personalizedmodel for each client. APFL learns a mixture of local and globalmodels as the personalized model. FedProto and FedProc utilize the prototypes to regularize local model training. FedBABU proposes to keep the global classifier unchanged during thefeature representation learning and perform local adoption by fine-tuning in each client.",
  "Federated Graph Learning": "Due to the great prowess of FL, it is natural to apply FL to graphdata and solve the data isolation issue. Recently, a cornucopia ofstudies has extended FL to graph data for different downstreamtasks, such as node classification , knowledge graph completion, and graph classification , cross-client missing informa-tion completion . Compared with generic FL, node attributesand graph structures get entangled simultaneously in the data het-erogeneity issue of FGL. To handle this issue, a handful of studiesproposed their approaches. For example, GCFL and FedStar are two recent frameworks for graph classification in FGL. Theauthors of GCFL investigate common and diverse properties inintra- and cross-domain graphs. They employ Clustered FL inGCFL to encourage clients with similar properties to share modelparameters. A following work FedStar aims to jointly train aglobal structure encoder in the feature-structure decoupled GNNacross clients. FedLit mitigates the impact of link-type hetero-geneity underlying homogeneous graphs in FGL via an EM-basedclustering algorithm.",
  "CONCLUSION": "In this study, we investigate the problem of divergent neighbor-ing information in FGL. With the high node heterophily, minoritynodes in a client can aggregate adverse neighboring informationin GNN models and obtain biased node embeddings. To grapplewith this issue, we propose FedSpray, a novel FGL framework thataims to learn personalized GNN models for each client. FedSprayextracts and shares class-wise structure proxies learned by a globalfeature-structure encoder. The structure proxies serve as unbiasedneighboring information to obtain soft targets generated by thefeature-structure encoder. Then, FedSpray uses the soft labels to reg-ularize local training of the GNN models and, therefore, eliminatethe impact of adverse neighboring information on node embeddings.We conduct extensive experiments over four real-world datasetsto validate the effectiveness of FedSpray. The experimental resultsdemonstrate the superiority of our proposed FedSpray comparedwith the state-of-the-art baselines. This work is supported in part by the National Science Founda-tion under grants IIS-2006844, IIS-2144209, IIS-2223769, IIS-2331315,CNS-2154962, and BCS-2228534; the Commonwealth Cyber Ini-tiative Awards under grants VV-1Q23-007, HV-2Q23-003, and VV-1Q24-011; the JP Morgan Chase Faculty Research Award; the CiscoFaculty Research Award; and Snap gift funding.",
  "Jaehoon Oh, Sangmook Kim, and Se-Young Yun. 2022. Fedbabu: Towards en-hanced representation for federated image classification. In International Confer-ence on Learning Representations": "Liang Peng, Nan Wang, Nicha Dvornek, Xiaofeng Zhu, and Xiaoxiao Li. 2022.Fedni: Federated graph learning with network inpainting for population-baseddisease prediction. IEEE Transactions on Medical Imaging (2022). Felix Sattler, Klaus-Robert Mller, and Wojciech Samek. 2020. Clustered feder-ated learning: Model-agnostic distributed multitask optimization under privacyconstraints. IEEE Transactions on Neural Networks and Learning Systems (2020).",
  "Alysa Ziying Tan, Han Yu, Lizhen Cui, and Qiang Yang. 2022. Towards person-alized federated learning. IEEE Transactions on Neural Networks and LearningSystems (2022)": "Yue Tan, Yixin Liu, Guodong Long, Jing Jiang, Qinghua Lu, and Chengqi Zhang.2023. Federated learning on non-iid graphs via structural knowledge sharing. InProceedings of the AAAI conference on artificial intelligence. Yue Tan, Guodong Long, Lu Liu, Tianyi Zhou, Qinghua Lu, Jing Jiang, andChengqi Zhang. 2022. Fedproto: Federated prototype learning across heteroge-neous clients. In Proceedings of the AAAI Conference on Artificial Intelligence.",
  "Han Xie, Li Xiong, and Carl Yang. 2023. Federated node classification over graphswith latent link-type heterogeneity. In Proceedings of the ACM Web Conference2023": "Yujun Yan, Milad Hashemi, Kevin Swersky, Yaoqing Yang, and Danai Koutra.2022. Two sides of the same coin: Heterophily and oversmoothing in graphconvolutional neural networks. In IEEE International Conference on Data Mining. Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and ViktorPrasanna. 2020. Graphsaint: Graph sampling based inductive learning method.In International Conference on Learning Representations.",
  "(17)": "for each client = + 1, + 2, , .The expectation of node embeddings after the message-passingmechanism will be E1 [x()+h()] for class1 and E2 [x()+h()]for class 2. We omit the linear transformation because it can beabsorbed in the linear GNN classifiers. The decision boundary ofthe optimal linear classifier is defined by the hyperplane P that isorthogonal to",
  "BEXPERIMENT DETAILSB.1Datasets": "Here we provide a detailed description of the four datasets weadopted to support our argument. These datasets are commonlyused in graph learning from various domains: PubMed in citationnetwork, WikiCS in web knowledge, Physics in co-author graph,and Flickr in social images. summarizes the statistics andbasic information of the distributed graph data.",
  "GCFL : GCFL employs a clustering mechanism basedon gradient sequences to dynamically group local modelsusing GNN gradients, effectively mitigating heterogeneityin both graph structures and features": "FedStar : FedStar is devised to extract and share struc-tural information among graphs. It accomplishes this throughthe utilization of structure embeddings and an independentstructure encoder, which is shared across clients while pre-serving personalized feature-based knowledge. FedLit : FedLit is an FL framework tailored for graphswith latent link-type heterogeneity. It employs a clustering al-gorithm to dynamically identify latent link types and utilizesmultiple convolution channels to adapt message-passing ac-cording to these distinct link types."
}