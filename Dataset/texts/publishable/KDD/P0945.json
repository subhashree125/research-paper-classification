{
  "ABSTRACT": "The text-attributed graph (TAG) is one kind of important real-worldgraph-structured data with each node associated with raw texts.For TAGs, traditional few-shot node classification methods directlyconduct training on the pre-processed node features and do notconsider the raw texts. The performance is highly dependent onthe choice of the feature pre-processing method. In this paper, wepropose P2TAG1, a framework designed for few-shot node classi-fication on TAGs with graph pre-training and prompting. P2TAGfirst pre-trains the language model (LM) and graph neural network(GNN) on TAGs with self-supervised loss. To fully utilize the abil-ity of language models, we adapt the masked language modelingobjective for our framework. The pre-trained model is then usedfor the few-shot node classification with a mixed prompt method,which simultaneously considers both text and graph information.We conduct experiments on six real-world TAGs, including papercitation networks and product co-purchasing networks. Experimen-tal results demonstrate that our proposed framework outperformsexisting graph few-shot learning methods on these datasets with+18.98% +35.98% improvements.",
  "Both authors contributed equally to this research. This work was done when the author was interned at Zhipu AI.. Corresponding author: JT.1 Our code is available at": "Permission to make digital or hard copies of part or all of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for third-party components of this work must be honored.For all other uses, contact the owner/author(s).KDD 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s).ACM ISBN 979-8-4007-0490-1/24/08",
  "graph self-supervised learning, text-attributed graphs, few-shotnode classification, graph neural networks": "ACM Reference Format:Huanjing Zhao, Beining Yang, Yukuo Cen, Junyu Ren, Chenhui Zhang,Yuxiao Dong, Evgeny Kharlamov, Shu Zhao, Jie Tang. 2024. Pre-Trainingand Prompting for Few-Shot Node Classification on Text-Attributed Graphs.In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discoveryand Data Mining (KDD 24), August 2529, 2024, Barcelona, Spain. ACM, NewYork, NY, USA, 12 pages.",
  "INTRODUCTION": "The few-shot node classification task involves identifying the classesof nodes in a given graph structure using only a limited number oflabeled examples. This task has practical applications in areas suchas social network analysis, recommendation systems, and more.Inspired by the successful experiences in the field of Computer Vi-sion (CV), several works, such as G-Meta and TENT ,apply meta-learning to graphs to address the few-shot node classifi-cation problem. These methods learn transferable knowledge frommeta-tasks, enabling rapid adaptation to new, unseen labels. Unlikeimages, graphs represent a form of structured data. Through graphpre-training, models can also accumulate a substantial amount ofdomain-specific knowledge.Recently, a series of graph pre-training methods emerged with self-supervised learning (SSL) to yield generalizednode representations in the absence of labels. These methods mainlyinclude contrastive and generative ones. Contrastive SSL methodsutilize data augmentation to generate multiple views of data forcontrastive loss. Generative SSL methods such as GraphMAE aim to reconstruct the (masked) attributes of the graph. These self-supervised learning methods have greatly contributed to manyaspects, such as graph augmentation and graph-based pretext tasks.They only consider part of the self-supervised learning on TAGs,making the training of GNNs independent of LMs text encoding.",
  "P2TAG (Ours)Mixed Prompt": "The nodes in the graph usually contain rich textual informa-tion such as titles and abstracts in citation networks. Somerecent works attempt to integrate the training of LMsand GNNs. These methods directly deal with the original texts andthe topological structure in the graph, and achieve better perfor-mance on the downstream tasks. GIANT aims to obtain strongertextual representations by the language model with neighborhoodprediction in TAGs. Although GIANT utilizes graph information tofine-tune the language models, the text encoding and graph prop-agation steps are still independent. GraphFormer proposes anested architecture of LMs and GNNs and jointly trains them with alink-prediction-based objective. GLEM alternatively optimizesLMs and GNNs using label information from the downstream tasks.In few-shot node classification, prompting helps guide the pre-trained model to generalize from a few examples by providingspecific input cues. Prog constructs prompts at the graph level,which is not directly applicable to TAGs. With the leapfrog devel-opment of large language models (LLMs), some works attempt toconstruct prompts by processing raw texts in the TAG, the namewe refer to as hard text prompt, exemplified by TAPE andENG . GPrompt leverages the GNN model as a downstreamadapter, using hard text prompts to improve adaptation efficiency.These works rely on the quality of constructed prompts. It is highlychallenging since there are both text (LM side) and structure infor-mation (GNN side) on TAGs. G2P2 tries to solve this problemsolely from the LM side, named soft text prompt. They reweight thepre-trained GNN embedding by the neighboring nodes texts andlabel texts. However, the potential capability of GNN is neglectedin this way and may lead to sub-optimal results. Present work: P2TAG. In this paper, we propose the Pre-trainingand Prompting for few-shot node classification on TAGs. In thepre-training phase, different from previous works, our frameworkintegrates the LM and GNN effectively with joint training in aself-supervised way. Some solutions are proposed to address thechallenges of integration of LMs and GNNs. We incorporate theencoder of GNNs as a supplement to LMs and keep the originalself-supervised loss of the LM model, i.e., the masked language mod-eling objective, which is a very hard pretext task and can alleviatethe over-fitting issue. To train the GNNs and the LMs jointly, ourframework first samples mini-batch subgraphs through a random-walk-based sampler. Each mini-batch will be fed to the LMs fortext encoding, and the output of the classification token (i.e., [CLS]token) will be considered as the representation of the node (or thetext sequence). The output embeddings of [CLS] tokens will be fed to the GNNs to aggregate the information from neighbors throughgraph propagation. For the masked language modeling, we incor-porate the aggregation of neighbor information into the outputs of[MASK] tokens. Finally, the cross-entropy loss of the outputs andthe original tokens is computed.In the prompting phase, to bridge the gap between pre-traintasks and the downstream ones, we process with a mixed approachof LM and GNN, which differs from previous works as shown in. Specifically, we jointly concatenate the label text initializedgraph prompt and a simple LM output initialized text prompt tomimic the training paradigm. As illustrated in , by leverag-ing the label text embedding, we can easily align the node to thedownstream label space without discrete and intuitive hand-craftprompts. The empirical results presented in .3 attest to thesuperior efficacy of our prompt design, which achieves a substantialimprovement of 11.1% 25.2% over the existing soft text promptedG2P2 method .The key contributions of our framework are summarized in thefollowing.",
  "In this section, we introduce the background of our paper includingtext-attributed graph and few-shot node classification": "Notations. Denote a graph = (V, E), where V is a set of nodes and E is a set of edges between nodes. R is theadjacency matrix where its entry (, ) 0, if nonzero, denotesthere is an edge between node and with edge weight (, ). Inpractice, the network could be either directed or undirected. If is directed, we have (, ) (,); if is undirected, we have(, ) (,).",
  "Few-shot Node Classification": "Few-shot node classification on graphs to categorize nodes withina graph with limited labeled nodes. Drawing inspiration from thesuccess of meta-learning in few-shot classification tasks within com-puter vision , several studies apply meta-learning techniquesto graph-based tasks. GFL and GPN utilize prototype net-works to learn the distance from nodes to classes. Meta-GNN optimizes the graph neural network with model-agnostic meta-learning. G-Meta addresses the meta-learning problem on bothsingle and multiple graphs by extracting local subgraphs. TENT enhances the models generalization capabilities by adapting atthree levels: nodes, edges, and tasks.With the increasing attention towards LLMs, several works at-tempt to follow the paradigm of pre-training and prompt tuningto address the problem of few-shot node classification. Prog incorporates both node-level and edge-level tasks by constructingprompts at the graph level, but lacks analysis of the text attribute.ENG employs LLM with designed prompts to refine node textsand reconstructs the adjacency matrix semantically, which is thenused as input for the GNN. G2P2 enhances text informationthrough graph structure, aligning text representations in threeforms during the pre-training phase. In the tuning phase, it usesthe neighborhood text of the target node and label text to generatethe initial parameters of prompts while freezing the parameters ofthe LM and GNN during the tuning process.",
  "Q = {(1,1), (2,2), ..., ( , )},": "where and represent nodes, their corresponding labels aredenoted as and . In the support and query sets, each label C C associated with nodes and nodes, respectively. For thesupport set, there exists (1)+ C, where {1, 2, .., }and {1, 2, .., }. Similarly, there exists (1)+ C, where {1, 2, .., } and {1, 2, ..,} for the query set.",
  "Pre-training Framework": "In this part, we introduce our proposed pre-training framework indetail. For self-supervised learning on TAGs, previous works usuallytake two separate steps. The first step is to encode the raw texts tonode features using bag-of-words, word2vec , or pre-trainedlanguage models . Most graph self-supervisedmethods use the node features pre-processedin the first step to construct a self-supervised objective. The two-step process can bring non-negligible information loss. To addressthis issue, we propose an end-to-end self-supervised framework totrain directly on the TAGs. Inspired by the recent prosperity of pre-trained language models (LM), we choose an effective LM to encodethe raw texts. To model the relations between nodes (texts), we canutilize powerful graph neural networks (GNNs). As illustrated in, our framework consists of two core modules, including apre-trained language model and a GNN encoder. The GNN encoder is to help make better node representations. In our framework,we mainly use the DeBERTa-base with 100M parameters asthe LM of our framework. DeBERTa utilizes two techniques basedon BERT and RoBERTa and significantly improves theperformance on the natural language understanding and generationtasks. The choice of the LMs is flexible, and we also explore otherLMs in our experiments.The pre-training of LMs and GNNs is much more challengingbecause we need to 1) choose an appropriate self-supervised trainingobjective to avoid over-fitting and 2) sample small mini-batches toaddress the high computational and space costs of large LMs. Self-supervised training objective. The training objective playsan important role in self-supervised learning. Different from con-ventional graph self-supervised learning, the design of self-supervisedobjective for TAGs are more challenging. Our model architecturecontains two different modules with different scales of model pa-rameters (large LM v.s. small GNN), making the training muchmore difficult. Simple self-supervised objectives will probably makethe model overfitted. To make a harder self-supervised objective,we adopt the masked language modeling (MLM) introduced in as our objective. Given a node , we first feed the text sequence = [1,2, . . . , ] into a language model to get the hidden rep-resentations associated with where is -th token of and is the length of . In the training stage, we randomly mask a pre-defined portion of tokens by replacing them with a special tokennamed [MASK]. We use = [ 1, 2, . . . , ] to denote the textsequence after masking and each token is a random variablewith the following distribution:",
  "Pr = [] = and Pr = = 1 .(1)": "Here, (0, 1) is a hyper-parameter representing the mask rate.When working with BERT-like language models, we usually adda starting token (e.g., [CLS]) and an ending token (e.g., [SEP]) tothe sequence. Therefore, each node is represented as a sequence ofhidden vectors: [0, 1, . . . , , +1] = [[], 1, 2, . . . , , []],(2)where R is the hidden representations of -th token of .Notice that the first hidden vector 0 corresponds to the specialtoken [CLS] and can be treated as the summary representationof the whole text sequence . To capture the correlations amongnodes, we then use a graph neural network to propagate the hiddenrepresentations of nodes where the input of node , denoted as, is the first hidden vector 0 of . The node representationsafter passing a GNN encoder are denoted as = (, ).The propagated node presentations are exploited to construct theself-supervised training objective.For each node , we concatenate the hidden representation with the output vector of each token and then feed the concate-nated vector to an MLP as:",
  "graph encoding": ": Our proposed framework P2TAG. A toy example illustrating the 3-way classification of childrens books. (1) In the pre-trainingphase, we jointly train the LM and GNN using a masked language modeling objective. For the GNN module, a subgraph-based sampleremploying random walks generates mini-batch subgraphs for training. (2) In the prompting phase, we construct the ego graph for eachtarget node and generate node features with the pre-trained LM. Graph tokens are utilized to learn the graph structure and create a promptgraph. These tokens are initialized either by encoding label text or through a random initialization process.",
  "Due to the heavy language models, we need to adopt a mini-batchtraining strategy even if we train our model on small graph datasets": "There are several genres of mini-batch training techniques forGNNs. To trade off the efficiency and flexibility, we choose a subgraph-based method, GraphSAINT , as our training strategy. At everytraining step, GraphSAINT constructs a mini-batch by sampling asubgraph from the original graph and generates node representa-tions according to the sampled subgraph. In this work, we adoptthe random walk sampler to preserve the connectivity of the wholegraph. The sampling process starts by selecting root nodes fromthe whole node set V uniformly at random. Starting from eachroot node, a random walk of length is sampled from the originalgraph structure. We then have a sampled node set V by addingall the nodes that occurred in the random walks and the subgraph induced by V is used in the GNN encoder to generate noderepresentations in the minibatch.",
  "Graph-Text Mixed Prompt Learning": "To prevent tuning the whole pre-trained model in the few-shotdownstream tasks, we adopt the prompt learning paradigm, i.e.,using a few trainable parameters to be the substitution of full-scalemodel parameters. The goal of prompt design is to mitigatethe gap between the pre-train objective and the downstreamtasks. It is highly challenging since there are both text (LM side)and structure information (GNN side) on TAG graphs. We try toinvestigate the joint prompting method, namely the graph promptand the text prompt, the detailed design as follows. To simplifythe discussion, we start with a target node (e.g., the 0-th node in). We further introduce the concept of the ego graph nodeset V. We select up to 100 first-order neighbors of the givennode, along with the node itself, to form the node set of the egograph. Then we define the induced ego graph from V as .",
  "Homophily hypothesis": ": Initialize tokens via the label text. Crafting the promptgraph via downstream label texts under the homophily hypothe-sis, thereby making the target node embedding more aligned withthe downstream label space. Initialize tokens via the label text. While the prompt graphservers as an efficient substitute for full-scale model parameters,we empirically find that the naive performs sub-optimally. Weattribute this to the inconsistency in the pre-trained task objectivesbetween our masked token prediction task and previous graph-based ones (e.g., the link prediction task).Referencing as an example, during the pre-trainingphase where token prediction tasks are undertaken, the model isequipped to align input with the pre-trained label space. Conse-quently, the prompt should slightly modify the input distributionto approximate the downstream label space more closely. Hence,unlike traditional cumbersome and hand-crafted text prompt tem-plates, we discovered that employing label text embeddings forprompt graph initialization is more direct and efficient. Under thehomophily hypothesis, the target nodes embedding is im-mediately positioned more closer to the desired label space.Specifically, we utilize the pre-trained as our label text en-coder, each label text (e.g., Animals, Science Fiction and Fantasy,and Activities, Crafts and Games) can be represented as . Thenwe can formalize the initialization of as follows:",
  "( )if ,Random Initializationotherwise.(7)": "Here, the (0)denotes the initial embedding of i-th prompt node,and for the additional prompt nodes, we adopt the random initiliza-tion. Then the inner structure of each nodes will be constructedthrough the Eq. 5. The links among the prompt nodes and the targetego graph are built by the Eq. 6. 3.3.2Text prompt design. As shown in Equation 3, we use the jointrepresentation of the LM model output (text information) and theGNN model output (structure information). Therefore, to align thepre-train and downstream tasks, we use a similar strategy to concatboth output features in the downstream scenario.Then, instead of making intricate text-based prompts, we use arather simple way: letting the second concat feature be a trainableparameter, namely R1, i.e., treating the LM model output it-self as a tunable parameter. The initialization process can be definedsimilarly as:0 = (),(8)",
  "Model Inference": "Finally, we incorporate the pretrained models , , and theprompt graph in the model inference stage. We outline threeinference methods. The first utilizes the trained LM, referred to asP2TAG (LM). It computes the output of the [CLS] token of theLM for each node in the graph via a mini-batch manner. The sec-ond method involves further feeding the node [CLS] outputs fromP2TAG (LM) into a GNN encoder, referred to as P2TAG (GNN).In the training stage, the node representation of is estimatedaccording to the sampled neighborhood in the subgraph, and thus,varies with the sampling process. The randomness introduced bythe sampler is unpleasant during the test stage. To get the exactnode representation, we use the full neighbor sampling strategywhen performing inference. That is, The node representation of at the -th layer is obtained by aggregating the representationsof its full neighborhood at the ( 1)-th layer. The computationprocess is performed layer by layer. Taking the node representa-tions at the ( 1)-th layer as the input, we can compute a batch ofnode representations at the -th by first loading their full neighbor-hoods and then aggregating the representations of those neighbornodes at the ( 1)-th layer. The third method involves conductingfew-shot node classification through prompting, as described in.3. The P2TAG inference loop of our framework is listedin Algorithm 2.",
  "the arXiv website, such as transforming \"NA\" into \"NumericalAnalysis\". ogbn-products is an undirected and unweighted graphrepresenting an Amazon product co-purchasing network": "Amazon Review. Amazon Review includes graphs composedof products, which are constructed based on co-purchasedand co-viewed patterns. We use Children, History, Comput-ers and Photo datasets compiled by . For the four datasets,we extract bag-of-words features and utilize principal compo-nent analysis (PCA) to reduce dimensions, generating a 100-dimensional feature for each node. In terms of the class split, ogbn-arxiv adopts the same partitioningstrategy as TENT . Meanwhile, for other datasets, we employa random division approach. Notice that our P2TAG differs frommeta-learning methods, as it does not require the establishment ofspecific training and validation tasks. Instead, we ensure fairnessusing the same test tasks as other baselines. Compared methods. We choose three types of methods for com-parison, all of which can address the problem of few-shot node clas-sification. These include methods based on meta-learning on graphs,self-supervised pre-training methods on text attribute graphs, andmethods following the paradigm of pre-training and prompting.Specifically, meta-learning methods such as GPN , G-Meta and TENT ; GIANT , proposed P2TAG (LM) and P2TAG(GNN) as self-supervised pre-training methods; G2P2 followsthe same pre-training and prompting paradigm as proposed P2TAG.In the pre-training phase, we concurrently train both the LM andGNN, considering two scenarios during inference. P2TAG (LM) de-rives vector representations from the original text of nodes. Mean-while, P2TAG (GNN) leverages these node representations as inputsto the GNN, resulting in the output obtained. Evaluation. GPN, G-Meta, and TENT are methods based on meta-learning, where they are learned on train tasks and are subsequentlyevaluated on test tasks. In contrast, the proposed P2TAG alongwith G2P2 and GIANT, are evaluated exclusively on test tasks.Specifically, P2TAG (LM), P2TAG (GNN), and GAINT infer theclasses of nodes in the query set by fitting a logistic regressionclassifier on the support set. Furthermore, P2TAG and G2P2 extendtheir approach by constructing prompts through the support set.For a detailed comparison, we conduct experiments in four differentsettings for OGB datasets: 5-way 3-shot, 5-way 5-shot, 10-way 3-shot, and 10-way 5-shot. Considering the number of labels, we usethree settings for Amazon Review datasets: 3-way 3-shot, 3-way5-shot, and 3-way 10-shot. Parameter configuration. For our framework, the selection ofLMs and GNNs is flexible. In our experiment, we choose a represen-tative LM DeBERTa-base and a powerful GAT model for the mainexperiments. The DeBERTa-base is a 100M-parameter pre-trainedlanguage model with a hidden size of 768. We keep the same hiddensize of the GAT model with DeBERTa-base. We also explore otherLMs in the ablation studies. We use AdamW optimizer withlearning rate lr = 1e-5 for model optimization. We run 3 epochs forall datasets. We construct 5 groups of test tasks for each -way-shot setting, with each group consisting of 50 tasks specificallyformulated from the test set label. In each task, the support setlength is , and the query set length is set to 10. We calculate",
  "Performance Analysis": "Our main results are summarized in and . Theproposed P2TAG (LM) outperforms the meta-learning methods,increasing the average accuracy on six datasets from +2.27% +35.51%; the P2TAG (GNN) achieves an average improvement with+16.38% +27.55%; the P2TAG performs best on most datasets,with an average improvement of +18.98% +35.98%. Comparedwith the pre-training method that also utilize raw text such as GI-ANT and G2P2, our P2TAG still has better performance, whichdemonstrates its effectiveness. On the ogbn-arxiv dataset, althoughG2P2 constructs prompts on the basis of pre-training, its perfor-mance is lower than that of GIANT, P2TAG (LM) and P2TAG (GNN).This underscores the importance of a well pre-trained model for few-shot node classification. Pre-training on TAGs often comeswith increased time expenditure, such as G2P2 taking over 30 dayson the ogbn-products dataset. The proposed P2TAG employs amore general approach to jointly train LM and GNN, consumingless than one day on this dataset. The P2TAG, employing mixedprompts, further enhances performance on the pre-trained modelP2TAG (LM) and P2TAG (GNN). It secures the best outcomes onmost datasets, with an average enhancement of 2.89% comparedto P2TAG (GNN), demonstrating the effectiveness of prompts. Onthe History dataset, P2TAG (LM) achieves the second-best results;however, after passing through the GNN encoder, P2TAG (GNN)experiences an average of 4.63% decrease. This might be attributedto the quality of the topological structure within the data. Thishypothesis is further supported by the minimal improvement ob-served when comparing meta-learning methods that do not utilizeraw texts (GPN, G-Meta, TENT) to the node features method. The",
  "In the previous sections, we demonstrate the powerful performanceof the P2TAG (LM), P2TAG (GNN), and P2TAG. This part analyzesthe impact of the different types of prompting and LMs": "Effect of LMs. To better analyze the impact of LMs, we exploreother LMs such as e5-v2-base with 110M parameters . We alsotry larger LMs such as DeBERTa-large with 350M parameters ande5-v2-large with 330M parameters. The results are reported in Ta-ble 5. Generally, the results of LMs are quite similar, with differenceswithin 1.5%. The reason that e5-large does not achieve better re-sults may be attributed to insufficient training iterations. This paperselects DeBERTa-base, intending to address the joint learning prob-lem of LMs and GNNs in a more general manner. There remainsroom for further exploration in the specific choice of LMs. ComputersPhoto Performance (%) 68.5670.97 74.7773.49 86.65 83.1984.37 74.57 93.76 86.73 G2P2P2TAG (LM) P2TAG w/o GPP2TAG w/o wt P2TAG",
  ": Effect of different prompt types. We omit label ini-tialization on the prompt graph (w/o ) and node text embedding(w/o w) and report the classification results on two datasets": "Effect of different prompt types. In .3, we discussedtwo significant enhancements. Here, we further investigate theseimprovements by dissecting them and presenting the findings in. We use the P2TAG w/o when the label text embeddingis not utilized to initialize our prompt graph, and the P2TAG w/o wwhen node text embedding is not employed to bolster the promptedgraph embedding (i.e., the graph embedding is directly inputted into : The accuracy of classification by P2TAG with dif-ferent hyperparameters on ogbn-arxiv dataset. We report theresults with P2TAG (GNN) and P2TAG, where the underscoredannotations indicate the parameters used in the main result tables.",
  "Hyperparameter Analysis": "In this part, we conduct a comprehensive analysis of hyperparame-ters to elucidate their impact on the performance of our framework.We analyze the four key hyperparameters in our experiments. The\"sequence length\" means the reserved length of raw texts. The\"mask rate\" represents the proportion of masked tokens in training.The \"walk length\" means the length of random walks used in theGraphSAINT sampler for mini-batch training. The \"token num\"denotes the number of tokens in the graph prompt. The results ofhyperparameter experiments are shown in . Preprocessing: length of truncated sequences. Since each batchwill be fed to both the language model and the GNN encoder intraining, we need to increase the batch size to better leverage theability of GNNs. However, the batch size is limited due to the largememory cost of the LM. Therefore, considering both training effi-ciency and information loss, we explore using a smaller truncatedlength in our experiments to utilize a larger batch size. Pre-training: mask rate. A smaller mask rate represents an easierself-supervised task and might make the model overfitted. To makethe self-supervised task harder, we need to choose a large enoughmask rate to let the model learn better. From the results, we findthat our framework achieves the best performance with a 75% maskrate on the ogbn-arxiv dataset.",
  "Pre-Training and Prompting for Few-Shot Node Classification on Text-Attributed GraphsKDD 24, August 2529, 2024, Barcelona, Spain": "Sampling: lengths of random walks. For each batch, the samplerconducts multiple random walks from different root nodes withthe same length. The GNN encoder of our framework relies onthe topology of the sampled mini-batch graph to propagate andaggregate the information between nodes. Therefore, the length ofrandom walks used to construct the mini-batch will influence themodel performance. Prompting: number of tokens. Incorporating the prompt graphas a trainable parameter highlights the importance of token quan-tity for downstream tasks. Intuitively, more tokens might create aricher internal structure, offering enhanced information. However,managing more parameters often complicates convergence in few-shot scenarios. To accurately incorporate the textual informationof labels into prompting, we typically set the number of tokens tomatch the number of classes in the task. Surprisingly, 2 tokens alsoyield results, implying a minimal internal structure. This suggeststhe effectiveness of token features in improving performance.",
  "Graph Representation Learning": "Graph neural networks (GNNs) provide the foundation for apply-ing deep learning on graphs and yielding good results on severaldownstream tasks. The earlier works perform con-volution on small-scale graphs using all topological relations in asemi-supervised way. The subsequent works focus on samplingstrategies and model architecture to enhance thescalability of GNNs and apply them to large-scale graphs. Graph-SAGE first proposed the idea of neighborhood sampling, andlater it was applied in a real-world recommendation system byPinSAGE . GraphSAINT , first samples subgraphs andruns full-batch GNNs on sampled subgraphs. Additionally, thereare works focusing on the hierarchy within the graph toobtain better representations. The self-supervised learning methodson GNNs are developed via contrastive and generative ways. Thecontrastive methods adopt data augmentationwith or without negative sampling to construct samples to optimizethe contrastive loss under different augmentations. GRACE aims to maintain node uniformity across views. BGRL de-signs two encoders for two views with data augmentation. Somestudies focus on graph generative learning. GraphMAE andGraphMAE2 are proposed to reconstruct masked attributes ofcorrupted nodes for representation learning.The semantics and graph topology of TAGs can express real-world relationships between entities or objects. Most GNNs donot consider the text processing in the TAG but directly use thenumerical features, which are generated through text encoding, asattributes of nodes. Recent studies utilize graph topology toenhance the representation during text pre-training. Despite thepromising results, the language model is still independent of theGNNs . GraphFormer designs a nested architectureof GNNs and Transformers. GLEM implements the fusionof LMs and GNNs on large-scale text-attributed graphs with thevariational expectation-maximization framework. Although these methods progress in integrating LMs and GNNs, there is still alack of discussions on self-supervised learning on large-scale text-attribute graphs. Our proposed pre-train framework enhances LMsutilizing GNNs and achieves joint training with the objective ofself-supervision. Yan et al. release the benchmark on TAGs,and provide multiple clean TAG datasets. These datasets also lay asolid foundation for our experiments.",
  "CONCLUSION": "Our paper focuses on few-shot node classification on the TAG.We address this problem by employing a graph pre-training andprompting approach. The proposed framework P2TAG utilizes amasked language modeling objective for the joint training of thelanguage model and GNN model. We also propose a new promptingmethod that mixes graph and text information, enabling the pre-trained model on TAG to better adapt to downstream few-shotnode classification tasks. We conduct experiments on six real-worldTAG datasets and our P2TAG framework achieves state-of-the-artresults on the six datasets with +18.98% +35.98% improvement.",
  "ACKNOWLEDGEMENT": "This work is supported by National Key R&D Program of China2021ZD0113304, Natural Science Foundation of China (NSFC) 62276148 and 62425601, CCF-Zhipu AI Large Model Fund (Grant 202213),Zhipu AI - Anhui University Joint Research Center on FoundationModel and the University Synergy Innovation Program of AnhuiProvince (GXXT-2023-050), the New Cornerstone Science Founda-tion through the XPLORER PRIZE, and Tsinghua-Bosch Joint MLCenter.",
  "Kexin Huang and Marinka Zitnik. 2020. Graph meta learning via local subgraphs.NeurIPS20 (2020)": "Xuanwen Huang, Kaiqiao Han, Dezheng Bao, Quanjin Tao, Zhisheng Zhang,Yang Yang, and Qi Zhu. 2023. Prompt-based node feature extractor for few-shotlearning on text-attributed graphs. arXiv preprint arXiv:2309.02848 (2023). Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and S Yu Philip. 2021.A survey on knowledge graphs: Representation, acquisition, and applications.IEEE transactions on neural networks and learning systems 33, 2 (2021), 494514.",
  "Jure Leskovec and Christos Faloutsos. 2006. Sampling from large graphs. InKDD06": "Chaozhuo Li, Bochen Pang, Yuming Liu, Hao Sun, Zheng Liu, Xing Xie, TianqiYang, Yanling Cui, Liangjie Zhang, and Qi Zhang. 2021. Adsgnn: Behavior-graphaugmented relevance modeling in sponsored search. In SIGIR21. 223232. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, OmerLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: Arobustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692(2019).",
  "A.2Comparing P2TAG with GPrompt": "Though P2TAG shares a similar task and intuition with GPrompt ,there are significant differences in the prompt design. Since theprompting parts are crucial for the pre-training and promptingparadigm, solely considering the task and intuition may not be fair.Therefore, we further elaborate our design here:Difference in Fundamental Settings of Prompting. GPromptuses the entire GNN model as the prompt, which requires fine-tuning the GNN for each downstream task. In contrast, we freeze theGNN parameters and instead use an auxiliary prompt graph to alterthe input. This approach, we believe, introduces a significant changeto the community. Our method not only preserves the knowledgeencapsulated within the GNN model but also reduces the number oftraining parameters to a minimal set of node embeddings, therebyenhancing computational efficiency.Novel Label Text Prompt Initialization Strategy. Further-more, we introduce a novel strategy for initializing the promptgraph by encoding the label texts to embeddings, which is a uniqueand effective element in the TAG scenario. By incorporating thelabel text embedding, we can easily initialize the prompt graph andalign the label space more closely with the downstream task. For ex-ample, a straightforward text prompt design might be: Given labelsstudent, teacher, worker, the person with attributes [feature] belongsto [cls]. In the graph domain, we initialize the prompt graph withthese label text embeddings. As a result, the target node embeddingnaturally aligns closely with the label text embedding via message passing, replicating the prompt initialization process. Moreover,the label text embeddings within the prompt graph can adaptivelyinfluence the target node, potentially offering more effectivenessthan static text prompts with uniform contributions.",
  "A.3Baselines": "We conduct five baselines, among which GPN, G-Meta, and TENTare based on meta-learning and do not utilize raw texts. GIANTemploys raw texts for self-supervised learning on graphs. G2P2also leverages raw texts and follows the paradigm of pre-trainingand prompting. All of them address the few-shot node classificationproblem. For each -way -shot few-shot setting, we sample 5groups, each containing 50 tasks for testing. To ensure a fair com-parison, we save the constructed tasks, and all baselines, includingour proposed model, are tested on these tasks. G-Meta is an excep-tion due to its more complex sampling process, but we ensure thesame number of testing tasks.",
  "TENT3 tunes the model across multiple tasks. We modify thismethod by Deep Graph Library (DGL) to enable its applicationon larger-scale graphs, such as the ogbn-products": "GIANT4 performs extreme multi-label classification on the rawtexts, resulting in stronger node features. We directly downloadtheir pre-trained node features for evaluation on the ogbn-arxivand ogbn-products datasets. G2P25 enhances text representation with graph structure, align-ing at three levels, and subsequently performs few-shot classifi-cation through prompting. We set the hidden layer dimensionto 128 and the number of epochs to 3.",
  "A.4Analysis of P2TAG Inference Cost": "We analyze the inference cost of P2TAG and baselines with respectto the text sequence length , embedding dimension , number ofnodes , number of nodes in the ego-graph, and the number ofgraph tokens .The cost of P2TAG (LM) stems from the language model (LM),with a complexity of( (2+2)). P2TAG (GNN) additionallyutilizes a GNN encoder with negligible computation overhead. Thecomplexity is ( (2 + 2)). P2TAG includes two stages:pre-training and prompting. The complexity is (( + + ) (2 + 2)). The complexity of GIANT is ( (2 + 2) + log()), where is a hyperparameter. The complexity of G2P2is (( + + 1) (2 + 2)), where represents the number ofneighbors.Overall, the proposed P2TAG exhibits computational costs simi-lar to other baselines, yet achieves improved performance in few-shot classification.",
  "A.6Comparing More Classes Settings inFew-Shot Experiments": "We chose the 3-way, 5-way, and 10-way settings across all datasetsfor two reasons. First, most works on few-shot node classification retain these settings, so we chose to follow them. Second,the limited number of classes in the datasets, such as the Amazondataset with only slightly more than 10 classes, restricts us to thesesettings. Some meta-learning methods require splitting the classes into training, validation, and test sets, which limits us to the 3-way setting. We further conducted new experiments on the ogbn-arxiv dataset under the 15-way setting for nine models (includingvariants). The results are as follows:",
  "Model15-way 3-shot (%)15-way 5-shot (%)": "Raw Features32.24 0.2637.37 0.82GPN38.38 0.3944.46 0.68G-Meta39.36 0.2844.85 0.48TENT37.61 0.3240.20 0.37GIANT52.77 0.5057.74 0.86G2P248.48 0.9752.24 0.85P2TAG (LM)57.31 0.5762.71 0.66P2TAG (GNN)58.49 0.5363.60 0.68P2TAG59.42 0.42 (6.65 )64.05 0.56 (6.31 ) We can find that increasing the number of classes indeed makesthe few-shot node classification task more challenging for all mod-els. For example, the raw feature method achieves 59.31% in the5-way setting, but only 44.59% in the 10-way setting. Nevertheless,P2TAG consistently outperforms the baselines, improving over thenext best method by 6.65% and 6.31%, respectively."
}