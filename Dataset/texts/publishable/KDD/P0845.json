{
  "Abstract": "Multivariate time series forecasting is crucial for various applica-tions, such as financial investment, energy management, weatherforecasting, and traffic optimization. However, accurate forecastingis challenging due to two main factors. First, real-world time seriesoften show heterogeneous temporal patterns caused by distributionshifts over time. Second, correlations among channels are complexand intertwined, making it hard to model the interactions amongchannels precisely and flexibly.In this study, we address these challenges by proposing a generalframework called DUET, which introduces DUal clustering on thetemporal and channel dimensions to Enhance multivariate Timeseries forecasting. First, we design a Temporal Clustering Module(TCM) that clusters time series into fine-grained distributions tohandle heterogeneous temporal patterns. For different distributionclusters, we design various pattern extractors to capture their in-trinsic temporal patterns, thus modeling the heterogeneity. Second,we introduce a novel Channel-Soft-Clustering strategy and designa Channel Clustering Module (CCM), which captures the relation-ships among channels in the frequency domain through metriclearning and applies sparsification to mitigate the adverse effectsof noisy channels. Finally, DUET combines TCM and CCM to in-corporate both the temporal and channel dimensions. Extensiveexperiments on 25 real-world datasets from 10 application domains,demonstrate the state-of-the-art performance of DUET.",
  "ACM Reference Format:Xiangfei Qiu, Xingjian Wu, Yan Lin, Chenjuan Guo, Jilin Hu , and BinYang. 2018": "DUET: Dual Clustering Enhanced Multivariate Time SeriesForecasting. In Proceedings of Make sure to enter the correct conference titlefrom your rights confirmation emai (Conference acronym XX). ACM, NewYork, NY, USA, 14 pages. Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from acronym XX, June 0305, 2018, Woodstock, NY 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-1-4503-XXXX-X/18/06",
  "Introduction": "Multivariate time series is a type of time series that organizestimestamps chronologically and involves multiple channels (a.k.a.,variables) at each timestamp . In recent years,multivariate time series analysis has seen remarkable progress,with key tasks such as anomaly detection ,classification , and imputation , among others , gaining attention. Among these, multivariate time seriesforecasting (MTSF) stands out as a criticaland widely studied task. It has been extensively applied in diversedomains, including economics , traffic ,energy , and AIOps , highlighting itsimportance and impact.Building an MTSF method typically involves modeling correla-tions on the temporal and channel dimensions. However, real-worldtime series often exhibit heterogeneous temporal patterns causedby the shifting of distribution over time, a phenomenon calledTemporal Distribution Shift (TDS) . Additionally, the corre-lation among multiple channels is complex and intertwined .Therefore, developing a method that can effectively extract hetero-geneous temporal patterns and channel dependencies is essentialyet challenging. Specifically, achieving these goals is hindered bytwo major challenges.",
  "Channel-Dependent (CD)": ": Channel strategies. Different colors represent dif-ferent channels, with squares representing features beforeprocessing with various channel strategies, and squares withrounded corners representing features after processing. First, heterogeneous temporal patterns caused by TDS aredifficult to model. In real applications, time series that describeunstable systems are easily influenced by external factors .Such non-stationarity of time series implies the data distributionchanges over time, a phenomenon called TDS . TDS causestime series to have different temporal patterns, formally knownas heterogeneity of temporal patterns . For example,(a) illustrates a time series in economics, which shows thechanges with the international circumstances. We can observe thatthe three intervals A, B, and C follow different temporal distribu-tions, as evidenced by the value histograms shown in Figures 1(b),1(c), and 1(d). This shift in distribution also comes with varyingtemporal patterns. As shown in (a), the blue interval Ashows a descending trend, the green interval B shows an increasingtrend, and the yellow interval C shows a steeper descending trend.It is crucial to incorporate these patterns considering their commonpresence in time series. However, recent studies primarily address heterogeneous temporal patterns in an implicitmanner, which ultimately undermines prediction accuracy.Second, complex channel interrelations are difficult tomodel flexibly. For an MTSF task, it is crucial to model the corre-lations among different channels, as the predictive accuracy for aparticular channel can often be enhanced by leveraging informationfrom other related channels. For example, in weather forecasting,temperature predictions can be improved by incorporating data onhumidity, wind speed, and pressure, as these factors are interrelatedand provide a more comprehensive view of weather conditions.Researchers have explored various strategies to manage multiplechannels, including 1) treating each channel independently (Channel-Independent, CI), 2) assuming each channel correlateswith all other channels (Channel-Dependent, CD), and 3) group-ing channels into clusters (Channel-Hard-Clustering, CHC). illustrates these three strategies. CI imposes the constraintof using the same model across different channels. While it offersrobustness , it overlooks potential interactions among chan-nels and can be limited in generalizability and capacity for unseenchannels . CD, on the other hand, considers all channels si-multaneously and generates joint representations for decoding ,but may be susceptible to noise from irrelevant channels, reducingthe models robustness. CHC partitions multivariate time series intodisjoint clusters through hard clustering, applying CD modeling DUET (ours)PDF (2024)iTransformer (2024) Pathformer (2024)FITS (2024)TimeMixer (2024)",
  ": Performance of DUET. Results (MSE) are averagedfrom all forecasting horizons. DUET outperforms strongbaselines in 10 commonly used datasets": "methods within each cluster and CI methods among clusters .However, this approach only considers relationships within thesame cluster, limiting flexibility and versatility. In conclusion, thereis yet an approach to model the complex interactions among chan-nels precisely and flexibly.In this study, we address the above two challenges by proposinga general framework, DUET, which introduces DUal clustering onthe temporal and channel dimensions to Enhance multivariate Timeseries forecasting. First, to model heterogeneous temporal patternscaused by TDS, we design a Temporal Clustering Module (TCM).This module clusters time series into fine-grained distributions, al-lowing us to use various pattern extractors to capture their intrinsictemporal patterns, thereby modeling the heterogeneity of temporalpatterns. This method effectively handles both stationary and non-stationary data, demonstrating strong generality. Second, to flexiblymodel relationships among channels, we propose a Channel Clus-tering Module (CCM). Using a channel-soft-clustering strategy, thismodule captures relationships among channels in the frequencydomain through metric learning and applies sparsification. Thisapproach enables each channel to focus on those beneficial fordownstream prediction tasks, while mitigating the impact of noisyor irrelevant channels, thereby achieving effective channel softclustering. Finally, the Fusion Module (FM), based on a maskedattention mechanism, efficiently combines the temporal featuresextracted by the TCM with the channel mask matrix generatedby the CCM. Experimental results show that the proposed DUETachieves state-of-the-art performance on real-world forecastingdatasetssee .Our contributions are summarized as follows.",
  "Related Works2.1Temporal Distribution Shift in MTSF": "Time series forecasting suffers from Temporal Distribution Shift(TDS), as the distribution of real-world series changes over time . In recent years, various methods have been proposed toaddress this issue. Some works tackle TDS from a normalizationperspective. DAIN adaptively normalizes the series with non-linear neural networks. RevIN utilizes instance normalizationon input and output sequences by normalizing the input sequencesand then denormalizing the model output sequences. Dish-TS identifies intra- and inter-space distribution shifts in time series andmitigates these issues by learning distribution coefficients. Non-stationary Transformer presents de-stationary attention thatincorporates non-stationary factors in self-attention, significantlyimproving transformer-based models. Some works address TDSfrom a distribution perspective. DDG-DA predicts evolvingdata distribution in a domain adaptation fashion. AdaRNN proposes an adaptive RNN to alleviate the impact of non-stationaryfactors by characterizing and matching distributions. Other worksaddress TDS from a time-varying model parameters perspec-tive. Triformer proposes a light-weight approach to enablevariable-specific model parameters, making it possible to capturedistinct temporal patterns from different variables. ST-WA usedistinct sets of model parameters for different time period.Despite the effectiveness of existing methods, our work explicitlymodels heterogeneous temporal patterns separately under differentdistributions, which can further improve the performance.",
  "Channel Strategies in MTSF": "It is essential to consider the correlations among channels in MTSF.Most existing methods adopt either a Channel-Independent (CI) orChannel-Dependent (CD) strategy to utilize the spectrum of infor-mation in channels. CI strategy approaches sharethe same weights across all channels and make forecasts indepen-dently. Conversely, CD strategy approaches consider all channels simultaneously and generates joint represen-tations for decoding. The CI strategy is characterized by low modelcapacity but high robustness, whereas the CD strategy exhibitsthe opposite characteristics. DGCformer proposes relativelybalanced channel strategies called Channel-Hard-Clustering (CHC),trying to mitigate this polarization effect and improve predictivecapabilities. Specifically, DGCformer designs a graph clusteringmodule to assign channels with significant similarities into thesame cluster, utilizing the CD strategy inside each cluster and theCI strategy across them. This approach adopts the CHC strategy,focusing solely on channel correlations within the same cluster. As a result, this method suffers from the limitations of rigidly adheringto channel-similarity rules defined by human experience.Different from the above methods, we adopt a Channel-Soft-Clustering (CSC) strategy and devise a fully adaptive sparsity mod-ule to dynamically build group for each channel, which is a morecomprehensive design covering the CHC strategy.",
  "Preliminaries3.1Definitions": "Definition 3.1 (Time series). A time series R is a time-oriented sequence of N-dimensional time points, where is thenumber of timestamps, and is the number of channels. If = 1,a time series is called univariate, and multivariate if > 1. For convenience, we separate dimensions with commas. Specifi-cally, we denote , R as the -th channel at the -th timestamp,,: R as the time series of -th channel, where = 1, , .We also introduce some definitions used in our methodology: Definition 3.2 (Temporal Distribution Shift ). Given a timeseries X R , by sliding the window, we get a set of time serieswith the length of , denoted as D = {X,:+ | & }, where each X,:+ equals to such ,:. Then, temporaldistribution shift is referred to the case that D can be clusteredinto sets, i.e., D = =1 D, where each D denotes the setwith data distribution D (), where D () D (), and1 , . Definition 3.3 (Frequency Metric Space). The Frequency MetricSpace is defined by the frequency space and the distance metric. Thefrequency space consists of functions from 2 space representedby infinite fourier basis, of which the coordinates are calculated byFourier Transform. And the metric is used to measure the distancesamong functions in the frequency space. For discrete univariatetime series with length , we use rFFT to represent them under2 fourier basis and design proper metric to measure the distances.",
  "Methodology4.1Structure Overview": "shows the architecture of DUET, which adopts a dual clus-tering on both temporal and channel dimensions, simultaneouslymining intrinsic temporal patterns and dynamic channel correla-tions. Specifically, we first use the Instance Norm to unifythe distribution of training and testing data. Then, the TemporalClustering Module (TCM) utilizes a specially designed DistributionRouter (a) to capture the potential latent distributions ofeach time series ,: R in a channel-independent way, and thenclusters time series with similar latent distributions by assigningthem to the same group of Linear-based Pattern Extractors (Fig-ure 5b). In this way, we can mitigate the issue that single structure",
  "SparsicationMetric Learning": ": The architecture of DUET. Temporal Clustering Module clusters time series into fine-grained distribution. For differentdistribution clusters, various pattern extractors are designed to capture their intrinsic temporal patterns. Channel ClusteringModule flexibly captures the relationships among channels in the frequency domain space through Metric Learning and appliesSparsification. Fusion Module combines the temporal features and the channel mask matrix. cannot fully extract temporal features due to heterogeneity of tem-poral patterns, even with millions of parameters. Meanwhile, theChannel Clustering Module (CCM) captures the correlations amongchannels in the frequency space in a channel soft clustering way.By leveraging an adaptive metric learning technique and applyingsparsification, the CCM outputs a learned channel-mask matrix,so each channel can focus on those beneficial for downstream pre-diction and isolate the adverse effects of irrelevant channels in asparse connection way. Finally, the Fusion Module (FM), based ona masked attention mechanism (d), effectively combinesthe temporal features extracted by the TCM and the channel-maskmatrix generated by the CCM. A linear predictor is then used toforecast the future values at the end of our framework.According to the above-mentioned description, the process ofDUET can be formulated as follows:",
  "Temporal Clustering Module (TCM)": "To model heterogeneous temporal patterns caused by TDS, wedesign a Linear-based Pattern Extractor Cluster, where each Linear-based Pattern Extractor extracts temporal features for time seriesthat has the same latent distribution. We adopt a linear model asthe basic structure because it has been proven to efficiently extracttemporal information , which also helps to keep the cluster lightweight. Moreover, we design a simple yet effectiveDistribution Routersee a, which can extract the potentiallatent distributions of the current time series and determine thecorresponding Linear-based Pattern Extractor. Both the routingand extracting processes are conducted in a channel-independentway, focusing on clustering univariate time series ,: R basedon their distributions and fully extracting their temporal patterns.Distribution Router: Inspired by VAE , we first design twofully connected layer-based encoders to adaptively capture can-didate latent distributions for each time series ,: R (see Fig-ure 5a), where denotes the size of the Linear-based Pattern Ex-tractor Cluster. Empirically, we assume each time series followsa latent normal distribution and the process can be formulated asfollows.",
  "Encoder (,:) = ReLU(,: 0 ) 1 ,(6)": "where 0 , 0 R 0, 1 , 1 R0. Then, we utilize theNoisy Gating technique to choose most possible distributionswhich ,: potentially belongs to and calculate the correspondingweights. We can observe that the reparameterization trick usedto measure normal distributions shares similarities with the noiseaddition technique in Noisy Gating, so we elegantly combine themin a unified form:",
  "(d) Fusion Module": ": (a) The structure of the Distribution Router, which consists of Distribution Characterization and Routing. (b) Thestructure of the Linear-based Pattern Extractor, which decomposes the series into seasonal and trend parts, separately extractstemporal features with linear models and reads them. (c) The Learnable Distance Metric is to capture the relationships amongchannels. (d) The Fusion Module is to combine the temporal features and the channel mask matrix.",
  "(10)": "where (,:) R denotes the probabilities of each candidatedistributions. From the perspective of clustering, univariate timeseries ,: R, = 1, , belonging to the same most possi-ble latent distributions tend to be processed by the same group of Linear-based Pattern Extractors.Linear-based Pattern Extractor: The above-mentioned distri-bution router determines the latent distributions. Then, ,: ispassed to corresponding selected Linear-based Pattern Extractorsfor temporal feature extraction. For the -th Linear-based PatternExtractorsee b, we first use the moving average techniqueto decompose ,: into seasonal and trend parts , then sepa-rately extract features and finally fuse them to better capture thepatterns of time series from the same distribution:",
  "Channel Clustering Module (CCM)": "To mitigate the adverse effects caused by improper consideration ofcross-channel relationships during prediction, we devise an efficientmetric learning method to softly cluster channels in the frequencyspace, and generate a corresponding learned channel-mask matrixto achieve sparse connections.Learnable Distance Metric. To fully utilize the cross-channelenhancement in prediction, we manage to leverage more usefulinformation by modeling the correlations among channels in theperspective of frequency metric space. Theoretically, the frequencyspace describes functions under the fourier basis, and their coordi-nates come from Fourier Transform by computing integrals in 2",
  "-dimension frequencyspace with the same": "2 Fourier basis (in complex form). Then, weneed to find a proper distance metric that precisely evaluates chan-nel relations in the frequency space.We propose such a distance metric that precisely evaluates chan-nel relations in the frequency space and make each channel obtainmaximum neighbour gain in the prediction task. Specifically, we",
  "where R2": "2 is a learnable semi-positive definite matrix. Itcan be practically constructed by = , where is also alearnable matrix. This process introduces a more general and light-weight method from the perspective of metric space and adaptivelyexplores a distance metric to measure the channel correlations forbetter prediction accuracy.Normalization. With the learnable channel distance metric, wefirst compute the relationship matrix of channels and normalize itto the range of :",
  "1if = ,(18)": "where ,, R are distance, relationship, and probabilitymatrices, (0, 1) is a discount factor to avoid the absolute connec-tion. Above processes probabilize the relationships among channels,where represents the probability that channel is useful forchannel in the prediction task.Reparameterization. Since our goal is to filter out the adverseeffects of irrelevant channels and retain the beneficial effects ofrelevant channels, we further perform Bernoulli resampling onthe probability matrix to obtain a binary channel mask matrixM R , where M Bernoulli(). Higher probability results in M closer to 1, indicating a relationship between channel and channel . Since contains learnable parameters, we usethe Gumbel Softmax reparameterization trick during Bernoulliresampling to ensure the propagation of gradients.",
  "mix = Softmax(MaskedScores) ,(21)": "where , , R are projection matrices in the atten-tion block, MaskedScores R is the attention score matrix, mix R is the fused feature. With the application of themasked attention mechanism , the Fusion Module effectivelyfuses the temporal features extracted by TCM based on the sparsecorrelations captured by CCM. We also adopt LayerNorm, Feed-Forward and skip-connection in the Fusion Module (FM) like aclassic transformer block. Specifically, LayerNorm and Skip-Connection ensure FMs stability and robustness, while the Feed-Forward layer empowers the FM to capture complex fea-tures. Their synergistic operation optimizes the representationalcapacity and training efficiency of the Fusion Module.Finally, we adopt a linear projection to predict the future values,which is formulated as follows.",
  "Experiments5.1Experimental Settings": "5.1.1Datasets. We use 25 real-world datasets from 10 differentdomains in time series forecasting benchmark (TFB) to com-prehensively evaluate the performance of DUET, more details ofthe benchmark datasets are included in . Due to space con-straints, we report the results on 10 well-acknowledged forecastingdatasets which include ETT (ETTh1, ETTh2, ETTm1, ETTm2), Ex-change, Weather, Electricity, ILI, Traffic, and Solar in the main text.The results for the remaining 15 datasets are available in our coderepository at 5.1.2Baselines. We choose the latest state-of-the-art models toserve as baselines, including CNN-based models (TimesNet ),MLP-based models (FITS , TimeMixer , and DLinear ),and Transformer-based models (PDF , iTransformer , Path-former , PatchTST , Crossformer , and Non-stationaryTransformer (Stationary) ). 5.1.3Implementation Details. To keep consistent with previousworks, we adopt Mean Squared Error (mse) and Mean AbsoluteError (mae) as evaluation metrics. We consider four forecastinghorizon : 24, 36, 48, and 60 for FredMd, NASDAQ, NYSE, NN5,ILI, Covid-19, and Wike2000, and we use another four forecastinghorizon, 96, 192, 336, and 720, for all other datasets which havelonger lengths. Since the size of the look-back window can affect theperformance of different models, we choose the look-back windowsize in 36 and 104 for FredMd, NASDAQ, NYSE, NN5, ILI, Covid-19,and Wike2000, and 96, 336, and 512 for all other datasets and reporteach methods best results for fair comparisons.We utilize the TFB code repository for unified evaluation, withall baseline results also derived from TFB. Following the settingsin TFB and FoundTS , we do not apply the Drop Lasttrick to ensure a fair comparison. All experiments of DUET areconducted using PyTorch in Python 3.8 and executed on anNVIDIA Tesla-A800 GPU. The training process is guided by the L1loss function and employs the ADAM optimizer. The initial batchsize is set to 64, with the flexibility to halve it (down to a minimumof 8) in case of an Out-Of-Memory (OOM) issue.",
  "DatasetDomainFrequencyLengthsDimSplitDescription": "METR-LATraffic5 mins34,2722077:1:2Traffic speed dataset collected from loopdetectors in the LA County road networkPEMS-BAYTraffic5 mins52,1163257:1:2Traffic speed dataset collected from the CalTrans PeMSPEMS04Traffic5 mins16,9923076:2:2Traffic flow time series collected from the CalTrans PeMSPEMS08Traffic5 mins17,8561706:2:2Traffic flow time series collected from the CalTrans PeMSTrafficTraffic1 hour17,5448627:1:2Road occupancy rates measured by 862 sensors on San Francisco Bay area freewaysETTh1Electricity1 hour14,40076:2:2Power transformer 1, comprising seven indicators such as oil temperature and useful loadETTh2Electricity1 hour14,40076:2:2Power transformer 2, comprising seven indicators such as oil temperature and useful loadETTm1Electricity15 mins57,60076:2:2Power transformer 1, comprising seven indicators such as oil temperature and useful loadETTm2Electricity15 mins57,60076:2:2Power transformer 2, comprising seven indicators such as oil temperature and useful loadElectricityElectricity1 hour26,3043217:1:2Electricity records the electricity consumption in kWh every 1 hour from 2012 to 2014SolarEnergy10 mins52,5601376:2:2Solar production records collected from 137 PV plants in AlabamaWindEnergy15 mins48,67377:1:2Wind power records from 2020-2021 at 15-minute intervalsWeatherEnvironment10 mins52,696217:1:2Recorded every for the whole year 2020, which contains 21 meteorological indicatorsAQShunyiEnvironment1 hour35,064116:2:2Air quality datasets from a measurement station, over a period of 4 yearsAQWanEnvironment1 hour35,064116:2:2Air quality datasets from a measurement station, over a period of 4 yearsZafNooNature30 mins19,225117:1:2From the Sapflux data project includes sap flow measurements and nvironmental variablesCzeLanNature30 mins19,934117:1:2From the Sapflux data project includes sap flow measurements and nvironmental variablesFRED-MDEconomic1 month7281077:1:2Time series showing a set of macroeconomic indicators from the Federal Reserve BankExchangeEconomic1 day7,58887:1:2ExchangeRate collects the daily exchange rates of eight countriesNASDAQStock1 day1,24457:1:2Records opening price, closing price, trading volume, lowest price, and highest priceNYSEStock1 day1,24357:1:2Records opening price, closing price, trading volume, lowest price, and highest priceNN5Banking1 day7911117:1:2NN5 is from banking, records the daily cash withdrawals from ATMs in UKILIHealth1 week96677:1:2Recorded indicators of patients data from Centers for Disease Control and PreventionCovid-19Health1 day1,3929487:1:2Provide opportunities for researchers to investigate the dynamics of COVID-19Wike2000Web1 day7922,0007:1:2Wike2000 is daily page views of 2000 Wikipedia pages",
  "Traffic0.3940.2570.3980.2630.4390.2770.3960.2600.4010.263": "of absolute performance, DUET demonstrates a significant improve-ment against the second-best baseline PDF, with an impressive 6.9%reduction in MSE and a 6.2% reduction in MAE.2) Considering different channel strategies, DUET also showsthe advantage of CSC strategy. Compared with CI models suchas FITS and PatchTST, DUET outperforms them comprehensively,especially on large datasets with abundant channel correlationssuch as Solar and Traffic. Besides, as the previous state-of-the-art,iTransformer and Crossformer with the CD strategy fail in manycases of ETT (4 subsets). Due to the weak correlations among chan-nels of ETT, they may be affected by noise channels (uncorrelatedchannels), leading to a decrease in performance. In contrast, DUETwith CSC strategy implements a soft clustering among channels(where each channel only focuses on the channels related to it), canbetter address this situation.3) DUET also demonstrates strong performance in tackling tem-poral heterogeneity caused by Temporal Distribution Shift. Com-pared to the state-of-the-art Non-stationary Transformer model fornon-stationary time series modeling. DUET achieves a significantreduction of 32.3% in MSE and 21.5% in MAE. This result indicatesthat the proposed Temporal Clustering Module is more effective inmodeling heterogeneous temporal patterns.",
  "Model Analyses": "5.3.1Ablation Studies. To ascertain the impact of different mod-ules within DUET, we perform ablation studies focusing on thefollowing components.(1) w/o TCM: Remove the Temporal Cluster Module.(2) w/o CCM: Remove the Channel Cluster Module.(3) Full Attention: Replace the Mask Attention with Full Attention.(4) Tem-Info: Change the channel distance calculation from thefrequency domain to the temporal domain. illustrates the unique impact of each module. We havethe following observations: 1) When the temporal clustering mod-ule is removed, the performance decreases relatively slightly fordatasets with fewer distribution changes such as Traffic. Whileevaluating datasets with substantial temporal distribution changes,such as ETTh2, the performance drops more significantly. Thisindicates the effectiveness of the temporal clustering module. 2)When the channel clustering module is removed and our modelworks in CI strategy, the performance on Traffic, which has stronginter-channel correlations, drops significantly. This highlights thenecessity of soft clustering among channels. 3) When a full atten-tion mechanism (where each variable calculates attention scoreswith all other variables) is used among channels instead of a maskedmatrix, our model works in CD strategy and the performance on theETTh2 dataset, which has less noticeable inter-channel correlations,also drops significantly. This demonstrates the effectiveness of ourproposed CSC strategy with the masked matrix applying to chan-nels. 4) When the distances among channels are calculated in thetemporal domain instead of the frequency domain, the algorithmsperformance also decreases. This suggests that clustering channelsin the frequency domain introduces more semantic informationand helps learn a robust distance metric to evaluate the channelrelations.",
  "Traffic0.3940.2570.3970.2600.3980.2620.3960.2590.3970.261": "5.3.2Comparsion Among Different Distance Metrics. To obtain themask matrix in the channel view, we adopt a learnable Mahalanobisdistance metric to adaptively evaluate the correlations among chan-nels in the frequency domain. We have demonstrated in 5.3.1 that frequency domain actually introduces more useful informationfor channel clustering. Then, we demonstrate that the learnableMahalanobis distance metric is better than other similarity mea-surement methods. We choose to replace the learnable metric withnon-learnable similarity measurement methods: Euclidean distance,Cosine similarity, and DTW . As shown in , using thesemethods results in a decrease in model performance, which fullydemonstrates the effectiveness of the learnable metric. Furthermore,we also replace the learned mask matrix with a random mask matrixand prove it performs far worse than the learned one. 5.3.3Parameter Sensitivity: Varying the Number of Extractors. DUETclusters time series into classes based on their temporal distri-butions, where is the size of Linear-based Pattern ExtractorCluster. We analyze the influence of different values on predic-tion accuracy in . We have the following observations: 1)",
  "Exchange0.3650.4040.3480.3970.3660.4050.3220.3830.2970.372": "Four samples from the ETTh1 Sample 1Sample 2Sample 3Sample 4 D1D2D3D4 S1 S2 S3 S4 0.7090.1490.0670.075 0.7050.1440.0710.080 0.0390.1320.1230.706 0.0670.6070.1190.207 Distribution weights of the ETTh1 Four samples from the Weather Sample 1Sample 2Sample 3Sample 4 D1D2D3D4 S1 S2 S3 S4 0.5480.0520.3410.059 0.1030.5390.1560.202 0.0570.0770.6960.170 0.0670.0690.7010.163 Distribution weights of the Weather 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.1 0.2 0.3 0.4 0.5 0.6 0.7",
  ": The distribution weights of different time seriessamples from the ETTh1 and Weather. S1-S4 denote distinctsamples, while D1-D4 represent different distributions": "The performance of M=1 is inferior to that of M not equal to 1.2) Datasets from the same domain, such as the electricity domaindatasets ETTh1 and ETTh2, their best values of are the same,which are 4. 3) While for datasets from different domains, such asILI (health) and Exchange (economic), their best values of are 2and 5. 4) With the most proper , the performance leads other vari-ants remarkablely in most cases. This highlights the effectivenessof clustering in the temporal view, and implies datasets from thesame domain often have similar temporal distributions, vice versa. 5.3.4Visualization of Distributions Weights: The distribution routerin TCM is designed to cluster samples by extracting the mean andvariance of the implict distributions. During clustering, we do notconsider the temporal alignment of the samples; instead, we focuson the distributional characteristics of each sample to determine itsdistribution category. We show four time series samples with thesame length of 96 from the ETTh1 and Weather datasets respec-tively, and described their unique distribution weights in .We have the following observations: 1) Samples 1 and 2 in ETTh1exhibit similar seasonal patterns, show similar distribution weights.However, for samples 1, 3, and 4, their distributions are vastly differ-ent. 2) Sample 3 and Sample 4 in Weather have similar trends, show",
  ": The attention scores of different channels from theETTh2. C1C7 denote distinct channels": "similar distribution weights. However, for samples 1, 2, and 3, theirdistributions are vastly different. These observations emphasizethe adaptability of DUET and its ability to distinguish and clustersamples with different distributions. 5.3.5Visualization of Channel Weights: We show the mutual at-tention weights of the seven channels in ETTh2 to exhibit theeffectiveness of our Channel Clustering Module (CCM). For chan-nels in the same time interval, the CCM measures their correlationsfor the downstream prediction task in the frequency space and gen-erates corresponding mask matrices to obtain a clustering effect. shows an example of this flexible clustering paradigm byvisualizing the masked attention weights in the Fusion Module. Wecan observe that channels with similar frequency component com-binations may be clustered into a soft group while the cross-groupcorrelations are also partially kept(see 0.127 between C6 and C3)to maximize the neighbor information gain for the prediction task.",
  "Conclusions": "In this paper, we propose a general framework, DUET, which intro-duces a dual clustering on the temporal and channel dimensions toenhance multivariate time series forecasting. It integrates a Tempo-ral Clustering Module (TCM) which clusters time series into fine-grained distributions. Various pattern extractors are then designedfor different distribution clusters to capture their unique temporalpatterns, modeling the heterogeneity of temporal patterns. Further-more, we intorduce the Channel Clustering Module (CCM) usinga channel-soft-clustering strategy. This captures the relationshipsamong channels in the frequency domain through metric learningand applies sparsification. Finally, the Fusion Module (FM), basedon a masked attention mechanism, efficiently combines the tempo-ral features extracted by the TCM with the channel mask matrixgenerated by the CCM. These innovative mechanisms collectivelyempower DUET to achieve outstanding prediction performance.",
  "Peter J Brockwell and Richard A Davis. 1991. Time series: theory and methods.Springer science & business media": "David Campos, Tung Kieu, Chenjuan Guo, Feiteng Huang, Kai Zheng, Bin Yang,and Christian S. Jensen. 2022. Unsupervised Time Series Outlier Detection withDiversity-Driven Convolutional Ensembles. Proc. VLDB Endow. 15, 3 (2022),611623. David Campos, Miao Zhang, Bin Yang, Tung Kieu, Chenjuan Guo, and Christian S.Jensen. 2023. LightTS: Lightweight Time Series Classification with AdaptiveEnsemble Distillation. Proc. ACM Manag. Data 1, 2 (2023), 171:1171:27. Peng Chen, Yingying Zhang, Yunyao Cheng, Yang Shu, Yihang Wang, QingsongWen, Bin Yang, and Chenjuan Guo. 2024. Pathformer: Multi-scale Transformerswith Adaptive Pathways for Time Series Forecasting. In ICLR.",
  "Zhichao Chen, Leilei Ding, Zhixuan Chu, Yucheng Qi, Jianmin Huang, and HaoWang. 2023. Monotonic Neural Ordinary Differential Equation: Time-seriesForecasting for Cumulative Data. In CIKM. 45234529": "Yunyao Cheng, Peng Chen, Chenjuan Guo, Kai Zhao, Qingsong Wen, Bin Yang,and Christian S. Jensen. 2023. Weakly Guided Adaptation for Robust Time SeriesForecasting. Proc. VLDB Endow. 17, 4 (2023), 766779. Razvan-Gabriel Cirstea, Chenjuan Guo, Bin Yang, Tung Kieu, Xuanyi Dong, andShirui Pan. 2022. Triformer: Triangular, Variable-Specific Attentions for LongSequence Multivariate Time Series Forecasting. In IJCAI. 19942001.",
  "Jacob Goldberger, Sam T. Roweis, Geoffrey E. Hinton, and Ruslan Salakhutdinov.2004. Neighbourhood Components Analysis. In NeurIPS. 513520": "Chenjuan Guo, Bin Yang, Ove Andersen, Christian S Jensen, and Kristian Torp.2015. Ecomark 2.0: empowering eco-routing with vehicular environmentalmodels and actual vehicle fuel consumption data. GeoInformatica 19 (2015),567599. Lu Han, Han-Jia Ye, and De-Chuan Zhan. 2024. The Capacity and RobustnessTrade-Off: Revisiting the Channel Independent Strategy for Multivariate TimeSeries Forecasting. IEEE Trans. Knowl. Data Eng. 36, 11 (2024), 71297142.",
  "Wendi Li, Xiao Yang, Weiqing Liu, Yingce Xia, and Jiang Bian. 2022. Ddg-da:Data distribution generation for predictable concept drift adaptation. In AAAI,Vol. 36. 40924100": "Zhe Li, Xiangfei Qiu, Peng Chen, Yihang Wang, Hanyin Cheng, Yang Shu, JilinHu, Chenjuan Guo, Aoying Zhou, Qingsong Wen, et al. 2024. FoundTS: Com-prehensive and Unified Benchmarking of Foundation Models for Time SeriesForecasting. arXiv preprint arXiv:2410.11802 (2024). Shengsheng Lin, Weiwei Lin, Keyi Wu, Songbo Wang, Minxian Xu, and James ZWang. 2024. Cocv: A compression algorithm for time-series data with continuousconstant values in IoT-based monitoring systems. Internet of Things 25 (2024),101049.",
  "Shengsheng Lin, Weiwei Lin, Feiyu Zhao, and Haojun Chen. 2025. Benchmarkingand revisiting time series forecasting methods in cloud workload prediction.Cluster Computing 28, 1 (2025), 71": "Peiyuan Liu, Beiliang Wu, Yifan Hu, Naiqi Li, Tao Dai, Jigang Bao, and Shu-taoXia. 2024. TimeBridge: Non-Stationarity Matters for Long-term Time SeriesForecasting. arXiv preprint arXiv:2410.04442 (2024). Peiyuan Liu, Beiliang Wu, Naiqi Li, Tao Dai, Fengmao Lei, Jigang Bao, YongJiang, and Shu-Tao Xia. 2024. WFTNet: Exploiting Global and Local Periodicityin Long-Term Time Series Forecasting. In ICASSP. 59605964.",
  "Meinard Mller. 2007. Dynamic time warping. Information retrieval for musicand motion (2007), 6984": "Guy P Nason. 2006. Stationary and non-stationary time series. (2006). Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. 2023.A Time Series is Worth 64 Words: Long-term Forecasting with Transformers. InICLR. Zhicheng Pan, Yihang Wang, Yingying Zhang, Sean Bin Yang, Yunyao Cheng,Peng Chen, Chenjuan Guo, Qingsong Wen, Xiduo Tian, Yunliang Dou, et al. 2023.Magicscaler: Uncertainty-aware, predictive autoscaling. Proc. VLDB Endow. 16,12 (2023), 38083821. Nikolaos Passalis, Anastasios Tefas, Juho Kanniainen, Moncef Gabbouj, andAlexandros Iosifidis. 2020. Deep Adaptive Input Normalization for Time SeriesForecasting. IEEE Trans. Neural Networks Learn. Syst. 31, 9 (2020), 37603765.",
  "DUET: Dual Clustering Enhanced Multivariate TimeSeries ForecastingConference acronym XX, June 0305, 2018, Woodstock, NY": "H 0.30 0.35 0.40 0.45 0.50 0.55 MSE ETTm1 F=96 H 0.2 0.3 0.4 0.5 0.6 MSE Solar F=96 H 0.14 0.16 0.18 0.20 0.22 0.24 MSE Electricity F=96 H 0.40 0.45 0.50 0.55 0.60 0.65 0.70 MSE ETTm1 F=720 H 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 MSE Solar F=720 H 0.20 0.22 0.24 0.26 0.28 0.30 MSE Electricity F=720 DUETiTransformerFITSDLinearPatchTST : Forecasting performance (MSE) with varying look-back windows on 3 datasets: ETTm1, Solar, and Electricity. Thelook-back windows are selected to be H = 48, 96, 192, 336, 512, and 720, and the forecasting horizons are F = 96, and 720.",
  "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is Allyou Need. In NeurIPS. 59986008": "Chengsen Wang, Qi Qi, Jingyu Wang, Haifeng Sun, Zirui Zhuang, Jinming Wu,and Jianxin Liao. 2024. Rethinking the Power of Timestamps for Robust TimeSeries Forecasting: A Global-Local Fusion Perspective. In NeurIPS. Chengsen Wang, Zirui Zhuang, Qi Qi, Jingyu Wang, Xingyu Wang, HaifengSun, and Jianxin Liao. 2023. Drift doesnt Matter: Dynamic Decompositionwith Diffusion Reconstruction for Unstable Multivariate Time Series AnomalyDetection. In NeurIPS.",
  "Hao Wang. 2024. Improving Neural Network Generalization on Data-LimitedRegression with Doubly-Robust Boosting. In AAAI. 2082120829": "Hao Wang, Zhichao Chen, Zhaoran Liu, Haozhe Li, Degui Yang, Xinggao Liu,and Haoxuan Li. 2024. Entire Space Counterfactual Learning for Reliable ContentRecommendations. IEEE Trans. Autom. Sci. Eng. (2024), 112. Hao Wang, Zhichao Chen, Zhaoran Liu, Licheng Pan, Hu Xu, Yilin Liao, HaozheLi, and Xinggao Liu. 2024. SPOT-I: Similarity Preserved Optimal Transport forIndustrial IoT Data Imputation. IEEE Transactions on Industrial Informatics (2024). Hao Wang, Xinggao Liu, Zhaoran Liu, Haozhe Li, Yilin Liao, Yuxin Huang, andZhichao Chen. 2024. LSPT-D: Local Similarity Preserved Transport for DirectIndustrial Data Imputation. IEEE Trans. Inf. Forensics Security (2024). Hao Wang, Zhiyu Wang, Yunlong Niu, Zhaoran Liu, Haozhe Li, Yilin Liao, YuxinHuang, and Xinggao Liu. 2024. An Accurate and Interpretable Framework forTrustworthy Process Monitoring. IEEE Trans. Artif. Intell. 5, 5 (2024), 22412252.",
  "Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. 2021. Autoformer:Decomposition Transformers with Auto-Correlation for Long-Term Series Fore-casting. In NeurIPS. 2241922430": "Xingjian Wu, Xiangfei Qiu, Zhengyu Li, Yihang Wang, Jilin Hu, Chenjuan Guo,Hui Xiong, and Bin Yang. 2024. CATCH: Channel-Aware multivariate Time SeriesAnomaly Detection via Frequency Patching. arXiv preprint arXiv:2410.12261(2024). Xinle Wu, Xingjian Wu, Bin Yang, Lekui Zhou, Chenjuan Guo, Xiangfei Qiu,Jilin Hu, Zhenli Sheng, and Christian S Jensen. 2024. AutoCTS++: zero-shotjoint neural architecture and hyperparameter search for correlated time seriesforecasting. The VLDB Journal 33, 5 (2024), 17431770. Xinle Wu, Xingjian Wu, Dalin Zhang, Miao Zhang, Chenjuan Guo, Bin Yang, andChristian S Jensen. 2024. Fully Automated Correlated Time Series Forecasting inMinutes. arXiv preprint arXiv:2411.05833 (2024).",
  "Yunhao Zhang and Junchi Yan. 2022. Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting. In ICLR": "Wendong Zheng, Putian Zhao, Gang Chen, Huihui Zhou, and Yonghong Tian.2023. A Hybrid Spiking Neurons Embedded LSTM Network for MultivariateTime Series Learning Under Concept-Drift Environment. IEEE Trans. Knowl.Data Eng. 35, 7 (2023), 65616574. Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,and Wancai Zhang. 2021. Informer: Beyond efficient transformer for long se-quence time-series forecasting. In AAAI, Vol. 35. 1110611115.",
  "ATHEORETICAL ANALYSISA.1Computational Complexity Analysis": "We compare the theoretical complexity across different Transformer-based models in because the main Computational complex-ity of DUET comes from the Transformer-based Fusion Module.Subsequent works mainly utilize attention mechanism to capturetemporal dependencies and channel correlations. Benefiting fromour dual clustering design, DUET efficiently extracts temporal pat-terns so that we only need to capture the correlations among chan-nels with attention mechanism, which can be paralleled on thelook-back window size. This ensures low computational costs evenwhen the look-back window size is extremely large.",
  "BHYPERPARAMETER SENSITIVITYB.1Varying Look-back Window": "In time series forecasting tasks, the size of the look-back windowdetermines how much historical information the model receives.We select models with better predictive performance from the mainexperiments as baselines. We configure different look-back windowto evaluate the effectiveness of DUET and visualize the predictionresults for look-back window H of 48, 96, 192, 336, 512, 720, andthe forecasting horizons are F = 96, 720. From , DUETconsistently outperforms the baselines on the ETTm1, Solar, andElectricity. As the look-back window increases, the prediction met-rics of DUET continue to decrease, indicating that it is capable ofmodeling longer sequences.",
  "B.2The Advantages of Dual Clustering": "As aforementioned, recent approaches assume the time series issampled from a single distribution and use a single backbone tomodel the heterogeneous temporal patterns. While DUET clusterstime series into different distributional categories and use parameterindependent backbones to model heterogeneous temporal patterns. and show that other baselines perform insuffi-cient sensibility to distributional changing while DUET fits wellto such cases. For datasets like Electricity with strong seasonality,CI strategy fails to take advantage of correlations among channelswhile CD strategy suffers from the adverse effects of delays amongchannels. Both of them capture a fuzzy seasonality while DUET",
  "Conference acronym XX, June 0305, 2018, Woodstock, NYXiangfei Qiu, Xingjian Wu, Yan Lin, Chenjuan Guo, Jilin Hu , and Bin Yang": "0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 DUET groundtruthprediction 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 iTransformer groundtruthprediction 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 FITS groundtruthprediction 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 TimeMixer groundtruthprediction 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 PatchTST groundtruthprediction 0.04 0.02 0.00 0.02 0.04 0.06 0.08",
  ": Visualization of input-96-predict-96 results on the Weather dataset": "1.0 0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 DUET groundtruthprediction 1.0 0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 iTransformer groundtruthprediction 1.0 0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 FITS groundtruthprediction 1.0 0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 TimeMixer groundtruthprediction 1.0 0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 PatchTST groundtruthprediction 1.0 0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0"
}