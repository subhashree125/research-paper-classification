{
  "ABSTRACT": "Mixup has shown considerable success in mitigating the challengesposed by limited labeled data in image classification. By synthesiz-ing samples through the interpolation of features and labels, Mixupeffectively addresses the issue of data scarcity. However, it has rarelybeen explored in graph learning tasks due to the irregularity andconnectivity of graph data. Specifically, in node classification tasks,Mixup presents a challenge in creating connections for syntheticdata. In this paper, we propose Geometric Mixup (GeoMix), a simpleand interpretable Mixup approach leveraging in-place graph editing.It effectively utilizes geometry information to interpolate featuresand labels with those from the nearby neighborhood, generatingsynthetic nodes and establishing connections for them. We conducttheoretical analysis to elucidate the rationale behind employing ge-ometry information for node Mixup, emphasizing the significanceof locality enhancementa critical aspect of our methods design.Extensive experiments demonstrate that our lightweight GeometricMixup achieves state-of-the-art results on a wide variety of stan-dard datasets with limited labeled data. Furthermore, it significantlyimproves the generalization capability of underlying GNNs acrossvarious challenging out-of-distribution generalization tasks. Ourcode is available at",
  "Corresponding author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08...$15.00 ACM Reference Format:Wentao Zhao, Qitian Wu, Chenxiao Yang, and Junchi Yan. 2024. GeoMix:Towards Geometry-Aware Data Augmentation . In Proceedings of the 30thACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD24), August 2529, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.",
  "INTRODUCTION": "Graph Neural Networks (GNNs) have become thede facto method for modeling the increasingly popular graph-structured data. However, in real world, labeling data is expensiveand many datasets have very few labeled examples. This scarcityof labeled data can lead to severe over-fitting issues and weakenthe generalization performance of GNNs, especially when the testdata comes from a distribution that differs from the training data,which is a common scenario in many real-world datasets.Motivated by the above issues, we set out to design Mixup forgraph learning, a technique that has demonstrated substantial suc-cess in mitigating challenges caused by limited data and enhancingmodel performance . At its core, Mixup trains neural networkson convex combinations of pairs of examples and their correspond-ing labels. This approach broadens the distribution of training dataand regularizes neural networks, serving as the key factors behindits capacity to reduce over-fitting, facilitate the learning of more dis-criminative representations, and enhance model generalization .These attributes are pivotal when handling datasets with limitedlabeled data or where the training data only encompass a subset ofthe diverse data distributions, which might not fully represent thedistributions of the testing data.Though prevailingly used in other fields, Mixup has rarely beenexplored in graph learning tasks, due to the connectivity in graph.In node classification task, questions have arisen about how toeffectively connect synthetic nodes. Current general Mixup strate-gies often attempt to circumvent the explicit connection ofsynthetic nodes by performing Mixup between layers of neural net-works, potentially limiting their adaptability. Other Mixup methodsaimed at addressing class-imbalance problems incorporate complexedge prediction modules. Unfortunately, this sacrifices Mixupsinherent lightweight nature and may diminish the generalizationpower due to increased model complexity.",
  "KDD 24, August 2529, 2024, Barcelona, SpainWentao Zhao, Qitian Wu, Chenxiao Yang, & Junchi Yan": "To address these challenges, this paper proposes a simple andgeometry-aware Mixup approach that in-place modifies raw dataand explicitly establishes connections for synthetic nodes, enhanc-ing interpretability. Theoretical analysis on the mixed features andlabels reveals: (1) the interpolation effects of this geometry-awareMixup; (2) its rationale for utilizing geometry information; (3) sce-narios where this fundamental strategy may succeed or fail.Building upon the theoretical analysis and recognizing potentialfailure cases, we further refine our approach and present GeometricMixup. It not only considers geometry details but also enhanceslocality information, enabling it to adapt to both homophilic graphs(where adjacent nodes are likely to have similar labels and features)and heterophilic graphs (adjacent nodes tend to have dissimilarlabels). Moreover, we elucidate the connection between GeometricMixup and graph structure learning to provide more insight andbetter interpretability for our design.Extensive experiments across twelve datasets demonstrate that:(1) Geometric Mixup achieves state-of-the-art results on both ho-mophilic and heterophilic graphs with limited labeled data; (2)it significantly improves the generalization ability of underlyingGNNs in various out-of-distribution generalization tasks with lim-ited distributions of training data; (3) it assists underlying GNNs inlearning more discriminative representations, improving predictionperformance.The major contributions of our work are:1) We propose a simple and interpretable Mixup strategy lever-aging in-place graph editing, which is a novel perspective.2) Our approach effectively utilizes graph geometry while en-hancing locality information to accommodate to both homophilicand heterophilic graphs.3) Theoretical analysis provides insights into leveraging geom-etry information for Mixup and underlines the significance of en-hancing locality information.4) Extensive experiments substantiate that Geometric Mixupeffectively improves the performance and generalization of under-lying GNNs in challenging tasks with limited training data.To distinguish our approach from existing ones, we compareGeometric Mixup with other node Mixup methods in .",
  "PRELIMINARIES2.1Semi-supervised Node Classification": "Let = (V, E) denotes a graph with node set V and edge set E.Each node V is associated with a feature vector x and a label, represented in one-hot form as y. Denote node feature matrix asX = {x}|V|=1 , adjacency matrix as A. The goal of node classificationtask is to train a classifier () that can accurately predict nodelabels based on X and A. In semi-supervised setting, the classifierhas access to the complete feature matrix X and adjacency matrixA, but is restricted to having labels for only a subset of nodes,constituting the labeled node set V (we denote the unlabeled nodeset as V). Therefore, the standard loss function for semi-supervisednode classification is",
  "METHODS3.1A Basic Geometry-Aware Mixup": "Though it may seem natural to apply Eq. (3) and (4) to node featuresand node labels to create synthetic nodes, how to establish connec-tions for synthetic nodes remains an unsolved problem. Inspiredby message passing which iteratively updates node features bycombining information from neighboring nodes, we propose an in-place-editing-based Mixup, where a nodes feature/label is adjustedusing a convex combination of features/labels from its immediateneighborhood. It explicitly connects synthetic nodes without neces-sitating a complex edge prediction module and effectively leveragesprior knowledge from the given graph.However, one challenge of semi-supervised learning lies in thescarcity of ground truth labels, leading to incomplete or inaccessi-ble label information for neighborhoods of certain nodes. To thisend, we first employ the training model () to predict the pseudolabel for each unlabeled node. For the convenience of subsequentderivation, denote y as:",
  "GraphMix Mixup GraphMixup GeoMix (ours)": "like D1A and D1/2AD1/2 where D is the degree matrix. Weconduct theoretical analysis on Eq. (6) and (7) to demonstrate itsinterpolation effects and offer insights into the circumstances underwhich this fundamental form of Mixup may succeed or fail.Assumptions on Graphs. To ease the analysis, we pose thefollowing assumptions on graphs. Denote the number of classesas . Assume that for any node : (1) Its feature x is sampledfrom feature distribution D associated with its label, with ()denoting its mean; (2) dimensions of x are independent to eachother; (3) the feature values in x are bounded by a positive scalar, i.e., max |x []| ; (4) due to lack of ground truth labels anderrors in pseudo label prediction, the expectations of y is",
  "e,(8)": "where e is the -th standard basis vector (all elements are 0except that the -th element is 1), (0, 1) is related to thelabel rate and the accuracy of model used to predict pseudo-labels.When the label rate is large or the model is highly dependable, should be close to 0; (5) Its neighbors labels { : N ()} areconditionally independent given , and have the same label asnode with probability . They belong to any other class with probability (1 )/( 1).We use G = {V, E, {D, }, ,} to denote a graph followingthe above assumptions. Note that we use subscript to indicatethat distribution D is shared by all nodes with the same label .Then we have the following theorem about mixed features:",
  ".(12)": "The proofs of Theorem 1 and 2 can be found in the appendix. Theabove theorems demonstrate two facts. Firstly, when is large and is small, the mixed feature and label of node will stay compara-tively close to its input feature and label in expectation. Secondly,the distance between the mixed feature/label of node and its ex-pectation is small with a high probability. Together, they show thatthe locality information is preserved in the Mixup and justify therationale to place the updated node in its previous position. Further-more, within Eq. (9) and (11), we observe the desired interpolationeffects achieved through Mixup.However, when is small, as is in some challenging heterophilicgraphs, the mixed feature/label of node will be far from its origi-nal feature/label in expectation. Thus, the locality information isnot well-preserved and it becomes dubious to place the updatednode in its original position. Another problem arising from a smallvalue of is the decreased distinguishability in the expectations ofmixed features/labels from nodes belonging to different classes. Inextreme cases when 1/, the expected features/labels of nodesfrom different classes converge to the same point, which greatlydiminishes the diversity of mixed features and labels. Therefore,this basic geometry-aware Mixup may fail to perform well in someheterophilic graphs, and we will provide solutions next.",
  "N() y( ) .(14)": "The residual connection h( )encompasses information from nodeswithin a distance of hops from node and thus helps to better pre-serve the locality information of node . is a hyper-parameter con-trolling the effect of locality reinforcement. By choosing a propervalue for , the expectation of h(+1)will stay relatively close toits preceding value. Consequently, the expected features/labels ofnodes from different classes will not converge to the same pointeven when is small and the locality information is effectivelypreserved. Therefore, the diversity of synthetic data will not becompromised and it is reasonable for the updated node to remainin its position. Note that we may repeat the Mixup operation for times to add more comprehensive range of geometry informationto the mixed features and labels. In practice, one or two consecutiveMixup achieves good performance.A more radical and effective choice of preserving locality infor-mation from the input graph is to utilize node s original featureh(0)and label y(0)to establish the residual connection.",
  "N() y( ) .(16)": "In challenging heterophilic graphs, this may produce better resultsin virtue of its better enhancement of locality information frominput graph, which will be demonstrated in experimental sections.During the training stage, we feed the mixed features H ={h}|V|=1 (here we drop the superscript without causing confu-sion) and the adjacency matrix A to GNN to predict labels. As isshown in Eq. (17), the loss function consists of two parts. For la-beled nodes, we use the ground truth labels as supervision signals.While for unlabeled nodes, we use the mixed labels for guidance. is a hyper-parameter used to balance the influence of mixed labelsY. At the inference stage, we dont perform Mixup and the GNNaccepts the original features and adjacency matrix as input.",
  "V( (A, H), y).(17)": "Relationship with Structure Learning. The above Mixup is insome extent linked to graph structure learning, which helps GNNlearning by optimizing the given graph structure to meet somedesirable properties such as smooth node features and connectivity. In Mixup, however, we modify the node features instead of thegraph structure. In this sense, the training procedure after incorpo-rating Mixup can be considered as a bilevel optimization problem.The upper-level optimization task treats the GNN as the decisionvariable and aims to minimize the label prediction loss in Eq. (17),while the lower-level optimization task is to minimize a regulariza-tion function that regularizes the learned graph by modifying nodefeatures H and labels Y, which we will explain next.As a theoretical intuition and justification, Mixup operation (13)and (15) are gradient descent steps of two separate regularizationfunctions which assess the quality of the mixed node features. As-sume node features H( ) to be a continuous function over 0with H(0) = X, we have the following theorem:",
  "where is related to": "The first term in Eq. (18) and (19) promote the proximity be-tween the updated node feature and its current state or originalstate, while the second term encourages the similarity of featuresamong neighboring nodes. We can obtain a similar cost functionregarding mixed labels Y, which operations in Eq. (14) and (16)serve to descend.Complexity analysis. In both Mixup operations (13) and (15),the computational complexity of calculating H( ) is(|V| +|E|),where is the number of input features. This is because the ag-gregating part involving can be implemented as a product of asparse matrix with a dense matrix. Applying consecutive Mixupoperations multiplies the storage and time requirements by a factorof . In practice, is usually 2. Similarly, the complexity of Mixupoperation for labels is (|V| + |E|), where is the number ofclasses. As a result, the overall GNN training complexity after in-cluding Geometric Mixup remains consistent with conventionalGNN training procedure.",
  "Extending Geometric Mixup BeyondVicinity: An Adaptive Approach": "Geometric Mixup operations provided in previous sections have twolimitations. Firstly, the aggregating weights are non-parametric,i.e., they are determined solely by adjacency matrix and need notraining. Consequently, inappropriate weights may be assignedwhen the graph structure contains noise. Secondly, restriction im-posed by graph structure greatly reduces Mixup choices, since anode can never have a chance to be mixed with a distant node.To address the aforementioned limitations, we allow a node tobe mixed with any other node and adaptively learn the aggregatingweights, as shown in Eq. (20).",
  "h( ) W( )2,(23)": "where W( ) R and W( ) R ( and is the dimensionof input features and hidden features) are two learnable projectionmatrices. Eq. (22) aligns with the self-attention mechanism of theTransformer . In this paradigm, each node can potentially mixits features/labels with those of any other node whose projectedfeature is similar. This addresses the limitation of having only a fewchoices of nodes for Mixup in previous Geometric Mixup. Moreover,the trainable parameters W( )and W( )can remedy the problemof inappropriate aggregating weights assigned by input graph.Complexity analysis. The all-pair aggregating operation guidedby weight ( ) in Eq. (20) can be written in the following matrixform",
  "M( ) =diagQ( ) (K( ))11Q( ) (K( ))H( ).(25)": "By first calculating (K( ))1 and (K( ))H( ) rather than Q( ) (K( )),we reduce the quadratic complexity to linear w.r.t the number ofnodes. The time complexity of Eq. (25) is (|V| ) where and are dimensions of input features and hidden features. Combiningthis with the complexity analysis in Sec. 3.2, the overall complex-ity of Mixup in Eq. (20) is (|V| + (|V| + |E|)). Through asimilar analysis, the time complexity of label Mixup in Eq. (21) is(|V| + (|V| + |E|)), where is the number of classes. There-fore, this variant of Geometric Mixup still preserves the complexityorder of conventional GNN training.",
  "EXPERIMENTS": "In this section, we conduct comprehensive experiments to evaluateGeometric Mixup on an extensive set of node classification datasets.Specifically, we focus on the following research questions: 1) Can Geometric Mixup consistently and significantly improvethe performance of GNNs on common benchmarks with limitedlabeled data? Additionally, can it cope with both homophily (whereadjacent nodes tend to share similar labels) and heterophily(which means adjacent nodes tend to have different labels)? 2) Can Geometric Mixup consistently and significantly enhancethe ability of GNNs in out-of-distribution (OOD) generalizationtasks? That is, one has access to limited distributions in trainingset and needs to generalize to datasets from distributions differentfrom those of the training data. 3) Are the proposed components in Geometric Mixup effectiveand necessary for the achieved performance? 4) Can Geometric Mixup help GNNs learn more discriminativerepresentations for improved class differentiation? Implementation details. We implement the three proposed Geo-metric Mixup methods defined in Eq. (13), (15), and (20), namingthem GeoMix-I, GeoMix-II, and GeoMix-III, respectively. Unlessotherwise specified, we employ GCN as the foundational GNNfor both Geometric Mixup and other competing methods that utilizea GNN backbone. Following an optimization of architecture-relatedhyperparameters for the standard GCN, which includes the numberof layers and hidden size, we adopt the same architecture for Geo-metric Mixup to ensure a fair comparison. To reduce the number ofhyper-parameters, we set in Eq. (17) to a default value of 1 exceptin a few cases, since this consistently produces exemplary resultsacross a wide range of test cases. For additional implementationand hyper-parameter details, please refer to the appendix. Competitors. We mainly compare with GCN , the GNN back-bone of Geometric Mixup, for testing the efficacy of GeometricMixup. We also compare with several state-of-the-art Mixup meth-ods for node classification: Mixup , GraphMix and Graph-Mixup . Furthermore, we compare with more advanced GNNs:GAT , SGC , APPNP and GloGNN . In OOD general-ization tasks, we add standard empirical risk minimization (ERM),DANN , EERM as competitive methods.",
  "MethodsCoraCiteSeerPubMedCSPhysicsSquirrelChameleon": "GCN81.63 0.4571.64 0.3378.88 0.6591.16 0.5292.85 1.0339.47 1.4741.32 3.22GAT82.98 0.8872.20 0.9978.58 0.5290.57 0.3792.70 0.5835.96 1.7339.29 2.84SGC80.35 0.2471.87 0.1478.75 0.1790.37 1.0192.80 0.1539.04 1.9239.35 2.82APPNP83.33 0.5271.83 0.5279.78 0.6691.97 0.3393.86 0.3337.64 1.6338.25 2.83GloGNN82.31 0.4272.16 0.6478.95 0.4290.82 0.4592.79 0.6735.77 1.3240.13 3.91 Mixup81.84 0.9472.20 0.9579.16 0.4991.36 0.3793.89 0.4937.95 1.5239.56 3.13GraphMixup82.16 0.7472.13 0.8678.82 0.5291.27 0.5593.62 0.4137.84 1.4639.82 2.35GraphMix83.80 0.6274.28 0.4579.38 0.3991.89 0.3694.32 0.2838.41 1.3641.75 3.51 GeoMix-I84.08 0.7475.06 0.3680.06 0.9392.13 0.0694.51 0.0740.95 1.1241.94 3.41GeoMix-II83.94 0.5075.12 0.2679.98 0.3592.14 0.1194.56 0.0640.75 1.3042.67 2.44GeoMix-III84.22 0.8573.60 0.8380.18 0.9992.23 0.1494.34 0.0440.78 1.7542.58 3.38",
  "Common Node Classification Datasets": "We first conduct experiments on several commonly used graphdatasets, including three citation networks Cora, CiteSeer andPubMed ; two co-authorship networks: CS and Physics ;and two heterophilic graphs: Squirrel and Chameleon , whereneighboring nodes tend to have distinct labels. For citation net-works, we use the same data splits as in , which selects 20nodes from each class as training set, 1,000 nodes in total as valida-tion set and 500 nodes as test set. For two co-authorship networks,we follow the splits in , i.e., 20 labeled nodes per class as thetraining set, 30 nodes per class as the validation set, and the restas the test set. For the two heterophilic datasets, we follow the re-cent paper that filters out the overlapped nodes in the originaldatasets and use its provided data splits.As displayed in , all three variantsGeoMix-I, GeoMix-II,and GeoMix-IIIsignificantly enhance the performance of GCN,their foundational GNN architecture, across all datasets. In Compar-ison to other advanced GNNs, they consistently achieve superioraccuracy even using simple GCN as the GNN backbone. Further-more, each of the three proposed Geometric Mixup variants con-sistently outperforms state-of-the-art Mixup competitors. Theseresults suggest that leveraging geometry information for Mixup ishighly effective in improving the performance of GNN and address-ing challenges caused by limited labeled data. It is likely to yieldsuperior results compared to Mixup that randomly pairs nodes.",
  "Handling Distribution Shifts in UnseenDomains": "We proceed to test Geometric Mixups capability of handling distri-bution shifts in OOD generalization tasks. We conduct experimentson Twitch-explicit dataset, which contains multiple networkswhere Twitch users are nodes, and mutual friendships betweenthem are edges . Since each graph is associated to users of aparticular region, distribution shifts occur between different graphs.We train and validate our model on three graphs: DE, EN, ES, andperform a random split into 50% training, 25% validation, and 25%in-distribution-test sets. After training, we directly evaluate themodel on FR, PT and RU datasets.",
  "GeoMix-I57.82 2.6166.96 0.6364.00 1.62GeoMix-II61.67 1.3865.58 1.4669.95 2.67GeoMix-III60.97 2.1465.38 1.8969.55 2.46": "To make our experiments solid, we not only compare our meth-ods with state-of-the-art Mixup methods for node classification,but also include comparisons with EERM and DANN, two of themost advanced methods designed to address distribution shifts. AsGraphMixup contains an edge prediction module, which relies ondomain knowledge and cannot handle distribution shifts effectively,we do not include it in this section. We report the results in . The three Geometric Mixup variants substantially enhance theperformance over ERM, the most basic OOD training approach.Notably, on Twitch-RU, the relative improvement reaches a re-markable 17.09%. Furthermore, Geometric Mixup shows superiorperformance over other advanced methods across all three datasets,thus validating the efficacy of our design in improving the gener-alization capabilities of the underlying GNN. As indicated in ,the graphs within Twitch-explicit exhibit heterophilic charac-teristics. Consequently, experiments in this section also underscoreGeometric Mixups effectiveness in handling heterophilic graphs.Moreover, the results presented in clearly indicate thatGeoMix-II outperforms GeoMix-I on such graphs. This observationsubstantiates the assertions made in .2.",
  "OOD Generalization in High Energy Physics": "Next, we test Geometric Mixup in OOD generalization using PileupMitigation dataset from the realm of High Energy Physics (HEP). It comprises multiple graphs, with each corresponding to abeam of proton-proton collisions. The nodes in each graphs rep-resent particles generated by these collisions in the Large HadronCollider, categorized into primary collisions (LC) and nearby bunchcrossings (OC). Node features encode various physics characteris-tics of these particles. Graphs are constructed from input featuresusing KNN method . The task is to identify whether a neutralparticle is from LC or OC. The distribution shifts can be attributedto two sources: first, variations in pile-up (PU) conditions, such asgeneralization from PU10 to PU30; second, changes in the types ofthe particle decay, for example, generalization from to . Toestablish a semi-supervised learning setting, for each generalizationtask, we choose 10 graphs from the source domain and randomlyallocate 20% of neutral nodes (particles) as the training set, 80%forming the validation set. For the target domain, we use 20 graphsand test the model on all the neutral nodes.This task presents a twofold challenge. Firstly, it necessitatesan in-depth understanding of complex domain knowledge withinthe HEP field. Secondly, it involves conditional structure shifts, anew type of challenging distribution shift identified by . Theresults are presented in . Despite the substantial challenges,GeoMix-I, GeoMix-II and GeoMix-III all significantly enhance thetesting accuracy of the underlying GCN across all tasks. Notably,the most substantial improvements are observed in distributionshifts caused by different physical processes, which are more de-manding than shifts arising from variations in PU conditions .In and , Geometric Mixup results in relativeimprovements as high as 7.22% and 11.14% over ERM. Addition-ally, Geometric Mixup consistently outperforms other advancedcompetitors throughout all scenarios. These findings demonstrate that with the aid of Geometric Mixup, GNNs can effectively acquirecomplex scientific knowledge from limited training data to addressreal-world challenges. They also highlight Geometric Mixups ca-pability of addressing distribution shifts between source and targetgraphs.",
  "Image and Text Classification with LowLabel Rates": "We extend our experiments to include the STL10, CIFAR10, and20News datasets to evaluate Geometric Mixups performance instandard classification tasks with limited labeled data. In 20Newsprovided in , we select 10 topics and use words with a TF-IDFscore exceeding 5 as features. For STL10 and CIFAR10, both imagedatasets, we initially employ the self-supervised approach SimCLR, which does not use any labels for training, to train a ResNet-18model for extracting feature maps used as input features. Sincethese datasets lack inherent graphs, we utilize the KNN method toconstruct input graphs. We leave more details in the appendix.The results are presented in . Notably, all three GeometricMixup methods consistently outperform GCN, their underlyingGNN, as well as other GNNs across all cases. Furthermore, theyachieve superior results compared to three state-of-the-art Mixupcompetitors. These findings underscore the broad applicability ofGeometric Mixup, spanning not only graph-structured datasets butalso image and text classifications where explicit graphs are absent.",
  "Ablation Study": "In this section, we conduct ablation studies to demonstrate theefficacy and necessity of certain design choices in Geometric Mixup.Firstly, we aim to assess the improvements brought about by theutilization of geometry information in Mixup. To achieve this, werandomly pair nodes for Mixup while keeping all other aspects ofthe training pipeline consistent with Geometric Mixup. Secondly,",
  "w/o Locality84.0 0.673.7 0.838.3 1.6(-0.12%)(-1.86%)(-6.59%)": "we seek to understand the effects of locality enhancement, so weremove the locality enhancement part in GeoMix-I and keep theother design elements unchanged.We conduct these experiments on Cora, CiteSeer and Squirrel,with the latter being a heterophilic graph. The results are displayedin . There is a substantial drop in performance when we donot incorporate geometry information. One possible explanationis that randomly mixing nodes can introduce unwanted externalnoise into each nodes receptive field, thus negatively affecting theaccuracy of information exchange during message passing.After disabling the locality enhancement, noticeable performancedeclines are observed in CiteSeer and Squirrel, while no sig-nificant difference is observed in Cora. These outcomes can beattributed to homophily. As analyzed in Sec. 3.1, a reduction inhomophily can cause the basic geometry-aware Mixup withoutlocality enhancement to inadequately preserve locality informa-tion and ensure the diversity of synthetic data, thereby diminish-ing the efficacy of Mixup. According to , even though Coraand CiteSeer are homophilic graphs, CiteSeer exhibits a lowerhomophily ratio. Consequently, there is a more pronounced per-formance drop in CiteSeer compared to Cora. In the case of theheterophilic graph Squirrel, the performance drop is even moresubstantial, reaching 6.59%. These results substantiate the necessityof locality enhancement for Geometric Mixup.",
  "Results of Using Other GNN Backbones": "In this section, we investigate the versatility of Geometric Mixup byaltering the underlying GNN architectures. Specifically, we employGAT and APPNP as backbone GNNs and evaluate theirperformance with all three variants of Geometric Mixup. The resultsare presented in . As shown, Geometric Mixup consistentlyenhances the performance of GAT and APPNP across both standarddatasets and out-of-distribution (OOD) generalization tasks.",
  "RELATED WORKS": "Graph Neural Networks. Graph Neural Networks (GNNs) havebecome the de facto method for modeling graph-structured data.Among the various types of GNNs, message-passing-based ap-proaches have gained prominence by defininggraph convolutions through information propagation. These ap-proaches generate the representation of a node by aggregatingits own features along with those of its neighbors. Our work isorthogonal to them in that our model-agnostic Mixup operationserves as a data preprocessing step to enlarge the training set andbroaden the data distribution, ultimately enhancing performanceand generalization. Mixup. As discussed in previous studies , Mixup is ahighly effective data augmentation technique that generates train-ing samples through the interpolation of existing samples. However,Mixup is mostly used in image classification and has rarely beenexplored in graph learning tasks, particularly the node classifica-tion task. When considering node classification, while interpolatingnode features and labels to generate synthetic nodes seems intu-itive, the challenge lies in effectively establishing connections forthese synthetic nodes. Care must be exercised during this process to avoid introducing excessive external noise into the informationpropagation mechanism, as it can detrimentally impact the per-formance of GNNs. In this domain, a few existing works eitheravoid explicitly connecting synthetic nodes or introducecomplex edge prediction modules . The former performs Mixupbetween layers of neural networks and tightly couples with thetraining model, potentially limiting its versatility. Conversely, thelatter compromises on efficiency and generalization capability. Tothe best of our knowledge, our research marks the pioneering effortin integrating the graph geometry into Mixup operation. This inte-gration allows for the construction of an explicit augmented graph,wherein synthetic nodes are systematically connected to relevantnodes. This approach enhances interpretability while maintainingthe efficiency of the Mixup technique. Generalization on Graph Learning. Owing to the distributionshifts encountered between real-world testing and training data,there has been a growing emphasis on enhancing the capacity ofGNNs to perform effectively on out-of-distribution (OOD) data.One line of work involves the application of adversarial training topromote the smoothness of the output distribution, such as BVAT and GraphAT . A more recent invariance learning approach,EERM , introduces multiple context explorers, which are im-plemented as graph editors and are adversarially trained. Anotherrecent work proposes learning a generalizable graph structurelearner that can enhance the quality of the input graph structurewhen generalizing to unseen graphs, thereby improving the perfor-mance of the downstream GNN. However, it is worth noting thatthese methods introduce significant extra computational costs. Incontrast, Geometric Mixup is more lightweight. It introduces onlya few message-passing-based Mixup operations and operates inlinear time with respect to the number of nodes and edges.",
  "CONCLUSION": "This paper proposes Geometric Mixup, a method leveraging geome-try information for Mixup by interpolating features and labels withthose from nearby neighborhood. We provide theoretic insightsinto our approach for utilizing graph structure and emphasizingthe importance of enhancing locality information, a critical designaspect enabling our method to accommodate to both homophilicand heterophilic graphs. Additionally, we extend our strategy tofacilitate all-pair Mixup and dynamically learn the mixing weights,overcoming the challenges posed by noise in the given graph struc-ture. Our extensive experiments demonstrate that Geometric Mixupsubstantially improves the performance of underlying GNNs onboth standard datasets and OOD generalization tasks.",
  "Thomas N Kipf and Max Welling. 2017. Semi-supervised classification with graphconvolutional networks. In International Conference on Learning Representations": "Johannes Klicpera, Aleksandar Bojchevski, and Stephan Gnnemann. 2019. Pre-dict then Propagate: Graph Neural Networks meet Personalized PageRank. InInternational Conference on Learning Representations. Xiang Li, Renyu Zhu, Yao Cheng, Caihua Shan, Siqiang Luo, Dongsheng Li, andWeining Qian. 2022. Finding Global Homophily in Graph Neural Networks WhenMeeting Heterophily. In International Conference on Machine Learning.",
  "Shikun Liu, Tianchun Li, Yongbin Feng, Nhan Tran, Han Zhao, Qiang Qiu, andPan Li. 2023. Structural Re-weighting Improves Graph Domain Adaptation. InInternational Conference on Machine Learning": "Fabian Pedregosa, Gal Varoquaux, Alexandre Gramfort, Vincent Michel,Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss,Vincent Dubourg, et al. 2011. Scikit-learn: Machine learning in Python. theJournal of machine Learning research 12 (2011), 28252830. Oleg Platonov, Denis Kuznedelev, Michael Diskin, Artem Babenko, and LiudmilaProkhorenkova. 2023. A critical look at the evaluation of GNNs under het-erophily: Are we really making progress?. In International Conference on LearningRepresentations.",
  "Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, PietroLi, and Yoshua Bengio. 2018. Graph Attention Networks. In International Con-ference on Learning Representations": "Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas,David Lopez-Paz, and Yoshua Bengio. 2019. Manifold mixup: Better represen-tations by interpolating hidden states. In International conference on machinelearning. Vikas Verma, Meng Qu, Kenji Kawaguchi, Alex Lamb, Yoshua Bengio, JuhoKannala, and Jian Tang. 2021. Graphmix: Improved training of gnns for semi-supervised learning. In Proceedings of the AAAI conference on artificial intelligence.",
  "Yiwei Wang, Wei Wang, Yuxuan Liang, Yujun Cai, and Bryan Hooi. 2021. Mixupfor node and graph classification. In Proceedings of the Web Conference 2021": "Felix Wu, Amauri H. Souza Jr., Tianyi Zhang, Christopher Fifty, Tao Yu, andKilian Q. Weinberger. 2019. Simplifying Graph Convolutional Networks. InInternational Conference on Machine Learning. 68616871. Lirong Wu, Jun Xia, Zhangyang Gao, Haitao Lin, Cheng Tan, and Stan Z Li. 2022.Graphmixup: Improving class-imbalanced node classification by reinforcementmixup and self-supervised context prediction. In Joint European Conference onMachine Learning and Knowledge Discovery in Databases. Springer.",
  "CADDITIONAL EXPERIMENTAL DETAILS": "For STL10, we utilize all 13,000 images, each categorized into oneof the ten classes. For CIFAR10, we choose 1,500 images from eachof 10 classes and obtain a total of 15,000 images. In these two imagedatasets, we randomly select 10/20 images per class as training set,4,000 images in total as validation set and the remaining instancesas testing set. We also evaluate our model on 20News, which is a textclassification dataset consisting of 9,607 instances. We follow totake 10 classes from 20 and use words (TF-IDF) with a frequencyof more than 5% as features. In this dataset, we randomly select100/200 instances per class as training set, 2,000 instances in totalas validation set and the remaining ones as testing set.We implement our approach using PyTorch. All experiments areconducted on an NVIDIA GeForce RTX 2080 Ti with 11GB memory.Grid search is used on validation set to tune the hyper-parameters.The learning rate is searched in {0.001, 0.005, 0.01, 0.05}; dropoutrate is searched in {0, 0.2, 0.3, 0.5, 0.6}; weight decay is searched in"
}