{
  "ABSTRACT": "In the realm of education, both independent learning and grouplearning are esteemed as the most classic paradigms. The formerallows learners to self-direct their studies, while the latter is typi-cally characterized by teacher-directed scenarios. Recent studiesin the field of intelligent education have leveraged deep tempo-ral models to trace the learning process, capturing the dynamicsof students knowledge states, and have achieved remarkable per-formance. However, existing approaches have primarily focusedon modeling the independent learning process, with the grouplearning paradigm receiving less attention. Moreover, the recip-rocal effect between the two learning processes, especially theircombined potential to foster holistic student development, remainsinadequately explored. To this end, in this paper, we propose RIGL,a unified Reciprocal model to trace knowledge states at both theindividual and group levels, drawing from the Independent andGroup Learning processes. Specifically, we first introduce a time",
  "Work was done at Career Science Lab, BOSS Zhipin supervised by Chuan Qin.Corresponding authors": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior specific permission and/or afee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Association for Computing Machinery.ACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00 frame-aware reciprocal embedding module to concurrently modelboth student and group response interactions across various timeframes. Subsequently, we employ reciprocal enhanced learningmodeling to fully exploit the comprehensive and complementaryinformation between the two behaviors. Furthermore, we design arelation-guided temporal attentive network, comprised of dynamicgraph modeling coupled with a temporal self-attention mechanism.It is used to delve into the dynamic influence of individual andgroup interactions throughout the learning processes, which iscrafted to explore the dynamic intricacies of both individual andgroup interactions during the learning sequences. Conclusively,we introduce a bias-aware contrastive learning module to bolsterthe stability of the models training. Extensive experiments onfour real-world educational datasets clearly demonstrate the effec-tiveness of the proposed RIGL model. Our codes are available at",
  "INTRODUCTION": "In the domain of education, both independent learning andgroup learning are regarded as the most classic learningparadigms. The former allows learners to self-direct their studies,whereas the latter is typically characterized by scenarios that areguided and structured by teachers. It is widely acknowledged thatexclusive reliance on a singular learning modality is insufficient topromote continuous long-term development in students .In recent years, with the advancement of artificial intelligencetechnology, the field of intelligent education has yielded notablemodeling paradigms conducive to understanding student learningbehaviors . Among these, a foundational and potent para-digm is knowledge tracing, which aims at dynamically monitoringthe evolving knowledge states of learners and predicting their fu-ture performance by modeling the exercise-solving sequences .However, most existing methods mainly focus onmodeling independent learning behaviors, with the group learningparadigm receiving less attention. Furthermore, the reciprocal effectbetween independent and group learning, particularly their com-bined potential to significantly drive holistic student development,has yet to be thoroughly explored and investigated .To this end, in this paper, we introduce a new task called holis-tic knowledge tracing (HKT), which refers to tracing knowledgestates at both the individual and group levels simultaneously, draw-ing from independent and group learning processes. As shown in, RS and R O indicate the interaction sequences for a stu-dent and a group respectively, which are organized in time frames.Each response interaction of a student and a group under eachtime frame is represented by triples ( , , ) and ( , , ), re-spectively, where and denote the exercise and the knowledgeconcept, and {0,1} as well as indicates the studentsresponse and the groups answer accuracy rate. The goal of HKT isto model both learning processes holistically.However, HKT is not a trivial task and encompasses the follow-ing technical challenges. Firstly, within the real-world educationalenvironment, there exists a notable absence of interactive behaviors among students spanning various learning scenarios, particularlywithin group learning processes. This absence significantly ampli-fies the challenge of tracing knowledge states at both individual andgroup levels. For instance, missing data resulting from the absenceof a student from a class test can cause bias in the overall assess-ment. Secondly, in contrast to traditional knowledge tracing thatonly focuses on individuals, the HKT task requires simultaneouslyand effectively modeling learning processes while exploring thedynamic interactions between individuals and groups during thelearning journey, which is quite confronting.To address these challenges, in this paper, we propose a unifiedReciprocal approach to trace the Independent and Group Learningprocesses (RIGL), aimed at delivering a productive dynamic as-sessment for both students and groups. Specifically, we first designa time frame-aware reciprocal embedding module to simultane-ously model students and groups response interactions over timeframes and then used the reciprocal enhanced learning modeling tofully exploit the comprehensive and complementary informationbetween the two behaviors. Subsequently, we propose a relation-guided temporal attentive network comprised of dynamic graphmodeling and a temporal self-attentive mechanism for exploringthe dynamic complexity of individual and group interactions duringthe learning processes. In particular, the relation-guided dynamicgraph is constructed by mining potential associations between stu-dents and groups. Finally, a bias-aware contrastive learning moduleis introduced to ensure the stability of training. Extensive experi-ments on four real-world educational datasets clearly demonstratethe effectiveness of the proposed RIGL model in the HKT task.",
  "RELATED WORK2.1Knowledge Tracing": "Knowledge tracing (KT) aims to monitor the changing knowl-edge states of learners by modeling their exercise-solving sequencesas a sequence prediction task, which has been recognized as animmensely crucial research task in the field of intelligent education.Over the past decades, numerous effective KT models have beenproposed. Among these, the traditional KT approaches play an es-sential role, which usually utilizes probabilistic models or logistic functions to model the knowledge states ofstudents. In recent years, the rapid advancements of deep learninghave propelled neural network (NN)-based KT approaches into thedominant paradigm . These approaches leveragethe power of neural networks to dynamically mine the knowledgeacquisition process of students by solving the sequence predictiontask, by which performance improvement and personalized edu-cational experiences are achieved. For instance, DKT utilizesa recurrent neural network (RNN) to model the students exercis-ing sequence and represent student cognitive proficiency with thehidden states. In particular, DKVMN introduces the memory-augmented neural network into KT, which defines two matricescalled key and value to store and update students knowledge mas-tery, respectively. Furthermore, SAKT exploits the transformerarchitecture to explore long-term relations of interaction be-haviors in students learning history for the first time. Despite thesuccess of these approaches, they primarily focus on individualassessment and thus leave a gap in the availability of a holistic",
  "knowledge tracing framework that simultaneously models individ-ual and group learning behaviors.2.2Dynamic Graph Representation Learning": "Dynamic graph representation learning is a rapidly evolvingfield that focuses on effectively capturing temporal dependenciesand changing patterns within dynamic graph-structured data. Inrecent years, many approaches have been proposed to effectivelymodel and learn the structural information and representation ofdynamic graphs in various research problems, such as link predic-tion , knowledge retrieval , and career devel-opment . One classical category of approaches is toconceptualize dynamic graphs by dividing them into multiple graphsnapshots with discrete time characteristics . For example,DySAT employs a dual-dimension self-attention mechanism,combining structural attention for local node features in graphsnapshots with temporal attention to track graph evolution, en-hanced by multiple attention heads for diverse graph structureanalysis. EvolveGCN introduces a dynamic adaptation of thegraph convolutional network (GCN) model across time, using anRNN to evolve its parameters and capture the dynamics of graphsequences, with two different architectures for parameter evolution.Another avenue of exploration regards time as a continuousfeature, treating the dynamic graph as a stream of timestampedevents to derive node representations . For instance,DyRep is a dynamic graph framework conceptualizing rep-resentation learning as a latent mediation process bridging twoobserved processes namelydynamics of the network and dynam-ics on the network, which leverages a two-time scale deep temporalpoint process and a temporal-attentive network to intertwine net-work topology and node activity dynamics. Moreover, dynamicgraph learning are also applied in intelligent tutoring systems (ITS),e.g., TEGNN presents a method that combines a heterogeneousevolution network with a temporal extension graph neural networkto dynamically model entities and relations in the intelligent tu-toring system. Although these strategies behave well in many tasks,how to introduce this idea into holistic knowledge tracing, wherethe knowledge states of individuals and groups vary dynamicallyover time and the associations and influences between them aredifficult to construct and capture directly, has not been explored.3PROBLEM FORMULATION In this section, we formally define the holistic knowledge trac-ing (HKT) problem. Let O = {1, . . . , } be the set of groups,S = {1, . . . , } be the set of students, E = {1, . . . ,} be theset of exercises, and C = {1, . . . , } be the set of knowl-edge concepts. The relationship between exercises and conceptsis denoted by -matrix = { }, where = 1 if exercise requires concept and 0 otherwise. Each group consists of acertain number of students, e.g., the -th group = {1, . . . ,| |}, where S and | | is the size of group .Generally, students perform personalized learning activities un-der time frames, i.e., answering a certain number of exercisesat specific time intervals. We denote the whole interaction se-quence of a student with time frames as RS = {F1, . . . , F },where F = {(1,1,1), . . . , (| F |,| F |,| F |)} stands for the in-",
  "teraction sequence under -th time frame; the triple ( , , )": "refers -th exercising record; E is the exercise; C isthe concept associated with the exercise , which is obtainedfrom the ; and {0, 1} is the response score. Meanwhile, stu-dents would engage in collective learning behaviors under timeframes, where all students in the group completed the same batchof exercises. We denote the whole group interaction sequencefor a group with time frames as R O = {H1, . . . , H }, whereH = {(1,1,1), . . . , (|H |,|H |,|H |)} is the group-exercise interaction sequence under -th time frame; the triple ( , , )denotes -th interaction log; E, C and is thecorrect rate that the group got.Problem Definition. Given students interaction sequence RS ={F1, . . . , F }, where F = {(1,1,1), . . . , (| F |,| F |,| F |)} and groups interaction sequence R O = {H1, . . . , H }, where H ={(1,1,1), . . . , (|H |,|H |,|H |)}, the goal of holistic knowledgetracing is twofold: (1) simultaneously diagnosing the knowledge stateof each student within a group and the group-level proficiency of thecorresponding group from time frame 1 to ; and (2) simultane-ously predicting students performance scores as well as the groupscorrect rate on specific exercises at time frame +1. Notably, unliketraditional KT task, the interaction elements (e.g., ( , , ) F)under each current time frame in the HKT task are not used forprediction to avoid information leakage.4METHODOLOGY In this section, we present the RIGL model in detail. As illustrated in, the architecture of RIGL mainly consists of three compo-nents, which are a time frame-aware reciprocal embedding module,a relation-guided temporal attentive network, and a contrastivelearning module.4.1Time Frame-Aware Reciprocal EmbeddingModule Effectively representing student interaction and modeling knowl-edge acquisition during the learning process has always been verycritical in traditional knowledge tracing task . Similarly, to ef-fectively model the learning process of students and groups undereach time frame in the HKT task, we carefully design three sub-modules: the individual interaction modeling, the group interactionmodeling, and the reciprocal enhanced learning module, which aredetailed in the following. 4.1.1Individual Interaction Modeling. As mentioned earlier, eachstudent typically solves multiple exercises under each time frameF. Each interaction behavior ( , , ) contains the interactiveexercises, the involved knowledge concept, and the correspondingresponse, which are often rich in information . We first encodethe question and response for each student interaction , as follows:",
  "x = [e c ] W1 + b1; z = r ,(1)": "where e R , c R , and r R denote the latent represen-tations of , , and respectively, W1 R and b1 R arethe trainable parameters, and refers to the element-wise additionoperator. Notably, , , and are the dimensions of the embed-dings of exercise, concept, and response respectively, and here equals to .",
  "KDD 24, August 2529, 2024, Barcelona, SpainXiaoshan Yu et al": "Table S2: Performance of RIGL and baselines (only using individual data) on all datasets on predicting individual-levelperformance. () means the higher (lower) score the better performance. denotes the statistically significant improvementof RIGL model compared to the best baseline method (i.e., two-sided t-test with p<0.05). Bold: the best, Underline: the runner-up.",
  "After the student interaction encoding, we obtain a set of exerciseencoding x = {x1, x2, . . . , x| F |} and a set of response encoding": "z = {z1, z2, . . . , z| F |} under the time frame F. Considering that astudents capability is usually stable over a short period of time ,we utilize all of the students exercising behaviors under a timeframe to comprehensively model her knowledge acquisition. Specif-ically, we leverage the average pooling operation as a knowledgeaggregator to fuse all interactions thereby perceiving knowl-edge gain during student learning as below:",
  "where x R and z R denote the knowledge representationand the performance representation of student under -th timeframe, respectively": "4.1.2Group Interaction Modeling. Similar to the exercise-solvingprocess of students, there will be multiple group-exercise interac-tion records under one time window. Given any one interaction( , , ) under the -th time frame H of group , it consists ofthe question, the concept, and the percentage of correct responses.We first encode the exercise traits and the collective response infor-mation:",
  "x = [e c ] W2 + b2; z = h1 + 1,(3)": "where e R and c R refer to the embeddings of and respectively, W2 R , b2 R, h1 R, and 1 R arethe trainable parameters. Note that we set the new parametersW2 and b2 different from Eq. 1 to model the question embeddingsunder the perspective of the student group. After acquiring thegroup interaction encoding sets x = {x1, x2, . . . , x|H |} and z =",
  "=1z .(4)": "4.1.3Reciprocal Enhanced Learning. Individual and group learn-ing are always interrelated and complementary in organizations(e.g., classes, teams ), where the collective activities in whichstudents participate contribute to the complementation of the stu-dents knowledge proficiency, as well as the individualized learninghistory of the students promotes the perception of the group-levelability . Inspired by this, we propose a reciprocal enhancedlearning module for tracing individual and group interaction si-multaneously (as shown in (a)). Specifically, we first utilizegroup interaction information to enrich individual learning fea-tures:",
  "Relation-Guided Temporal AttentiveNetwork": "Considering the dynamic complexity of the learning process ofstudents and groups and the information interaction between them,in this section, we propose a relation-guided temporal attentivenetwork to model this complex learning process with dynamicchanging knowledge, which consists of the relation-guided dynamicgraph modeling and a temporal self-attentive network. 4.2.1Relation-Guided Dynamic Graph Modeling. In this section,we first describe how to construct the relation-guided dynamicgraph and then design a dynamic GCN module to enhance therelation modeling. Referring to previous work , we consider theglobal dynamic graph as a series of static graph snapshots, i.e., G ={G1, G2, . . . , G }, where is the number of time frames. For timeframe , with respect to the personalized learning behaviorsof the students in the group O and the interactive responsebehaviors of the group, we construct the corresponding group-individual graph G = (, ) with a node set = {,1, . . . ,||}and a edge set = (, ), where denotes the set ofedges connecting the group node and all other student nodes, and denotes the set of connecting edges between students undertime stage , which is dynamically changing.Specifically, a relation-guided approach is proposed to constructthe changing edges between students. We first incorporate theabove interaction encoding as node feature:",
  "v0 = [x ;z ]; v = [x ;z ], {1, 2, . . . , ||},(8)": "where v0, v R2 denote the feature vector of group node andstudent nodes respectively. Then, we obtain the node feature matrixV R(||+1)2. For any two student nodes v and v in the timeframe , where , {1, 2, . . . , ||}, we acquire the relation distanceby calculate the exercise interaction similarity between them andthen obtain the relation matrix as follows:",
  "v ,v = (v , v ),(9)": "where D R|||| denotes the relation matrix, and () standsfor similarity function (e.g., cosine similarity). Subsequently, forany student node v , we select top-k other student nodes with thehighest potential associations to construct its first-order neighborsaccording to D,, {1, . . . , ||} \\ {}, and then we can get the",
  "adjacency matrix A {0, 1}(||+1)(||+1), which includes theconnectivity between group node and student node, as shown inupper part of (b)": "After the construction of G, we design a dynamic GCN modulecomposed of static GCN units to effectively model the deeper asso-ciations between group and student as well as student and studentby introducing the GCN layers . Given the node feature matrixV and the adjacency matrix A, the -layer GCN of the unit underthe -th time frame is defined as:",
  "and U = A": "4.2.2Temporal Self-Attentive Network. To more effectively capturethe intricate temporal effects in the overall abilities and knowledgestates of groups and individual students during the learning process,we introduce a temporal self-attentive network in this section. Fol-lowing previous work , we define the temporal self-attentionmodule and obtain the retrieved knowledge states of the group andstudent (i.e., the selection of Q, K, V parameters is essentially aknowledge retriever) as follows:",
  "V() [, :] R, 1 || are the interaction features of group and student extracted from the learned -th layer node rep-resentations, =": "2 is the dimension size of final representationand () denotes the Self-Attention module. Notably, similarto AKT , we can only model current interaction with visiblehistorical learning behaviors to prevent information leakage (asshown in the lower part of (b)).Finally, we construct a readout module consisting of a two-layerfully connected network for the next time frame performance pre-diction of the group and student (as shown in part (1) of (c)).Specifically: ,+1 = ([h+1; e+1]),",
  "EXPERIMENTS": "In this section, we conduct extensive experiments on four real-worldeducation datasets aiming at verifying the effectiveness and superi-ority of our proposed RIGL model. Specifically, we will answer thefollowing research questions (RQs) to unfold the experiments: RQ1: What about the effectiveness and superiority of the pro-posed RIGL model on the holistic knowledge tracing task?",
  "StatisticsASSIST12 NIPS-Edu SLP-Math SLP-Bio": "#Students2,2811,1381,4881,727#Groups10191126150#Exercises8,838747142121#Knowledge concepts1622254122Avg. group size22.7912.5111.8011.51Avg. responses per student67.1687.5378.5494.58Avg. responses per group55.3367.4679.9397.84Avg. responses per time frame33.5332.4732.9335.04 NIPS-Edu , SLP-Math and SLP-Bio . All datasets containthe group labels (i.e., the class to which the students belong), andstudents from the same group share the same label category. shows the statistics of the datasets, and more details of the datasetdescription and data preprocessing are available in Appendix A.2. 5.1.2Baseline Approaches. The performance of RIGL is com-pared with eight strong and commonly used baselines includingDKT , SAKT , AKT , LPKT , GIKT , simpleKT ,AT-DKT and DTransformer . Notably, these baselines areindividual-based knowledge tracing models, so we adapt them onthe HKT task. The introduction and implementation details can befound in Appendix A.3. 5.1.3Evaluation Metrics. To comprehensively evaluate the per-formance of all methods on holistic knowledge tracing, we adoptfour evaluation metrics, including the area under the ROC curve(AUC), accuracy (ACC), root mean square error (RMSE), and meanabsolute error (MAE). Specifically, the AUC and ACC are used toevaluate the individual-level performance prediction, and RMSEand MAE for the group-level performance on future exercises. 5.1.4Parameter Settings. We performed 5-fold cross-validationin the experiments. For each fold, 80% of samples are set as forthe training data, and others are for the test set. We implementedall methods with PyTorch by Python. The dimension size of em-beddings (i.e., , , ) was set as 256. The number of the GCNlayers 1 was set to 2, where each layers hidden size is 256, andthe number of self-attentive network layers 2 is set to 4. We usedthe Adam optimizer, where the learning rate was searched in [1e-4,5e-4, 1e-3, 2e-3, 1e-2]. The coefficient of contrastive loss was set to0.01. We adopted the cosine similarity as the similarity calculationfunction () (Eq.9 and Eq.13).5.2Performance Comparison (RQ1) shows the performance of the proposed RIGL includingindividual-level and group-level compared with the baseline modelson the four datasets. We highlighted the best results of all mod-els in boldface and underlined the suboptimal results. Accordingto the results, there are several observations: (1) Our RIGL modeldemonstrates significant improvements over the baselines acrossall datasets. Specifically, it shows an average increase of 4.01% and20.32% over the best baseline model at both the individual andgroup levels, respectively, which underscores the effectiveness ofthe RIGL. Especially, in comparison to the runner-up method onthe SLP-Bio dataset, our model achieves an average of 5.83% and31.59% performance improvements in terms of individual-level andgroup-level. (2) RIGL generally has a higher percentage of increasedperformance in terms of the group-level than the individual-level,demonstrating that personalized learning information is highly",
  "RIGL: A Unified Reciprocal Approach for Tracing the Independent and Group Learning ProcessesKDD 24, August 2529, 2024, Barcelona, Spain": "Shangshang Yang, Cheng Zhen, Ye Tian, Haiping Ma, Yuanchao Liu, PanpanZhang, and Xingyi Zhang. 2023. Evolutionary Multi-Objective Neural Architec-ture Search for Generalized Cognitive Diagnosis Models. In 2023 5th InternationalConference on Data-driven Optimization of Complex Systems (DOCS). IEEE, 110. Yang Yang, Jian Shen, Yanru Qu, Yunfei Liu, Kerong Wang, Yaoming Zhu, WeinanZhang, and Yong Yu. 2021. GIKT: a graph-based interaction model for knowledgetracing. In Machine Learning and Knowledge Discovery in Databases: EuropeanConference, ECML PKDD 2020, Ghent, Belgium, September 1418, 2020, Proceedings,Part I. Springer, 299315. Kaichun Yao, Jingshuai Zhang, Chuan Qin, Xin Song, Peng Wang, Hengshu Zhu,and Hui Xiong. 2023. Resuformer: Semantic structure understanding for resumesvia multi-modal pre-training. In 2023 IEEE 39th International Conference on DataEngineering (ICDE). IEEE, 31543167. Yu Yin, Le Dai, Zhenya Huang, Shuanghong Shen, Fei Wang, Qi Liu, EnhongChen, and Xin Li. 2023. Tracing Knowledge Instead of Patterns: Stable KnowledgeTracing with Diagnostic Transformer. In Proceedings of the ACM Web Conference2023. 855864. Xiaoshan Yu, Chuan Qin, Dazhong Shen, Haiping Ma, Le Zhang, Xingyi Zhang,Hengshu Zhu, and Hui Xiong. 2024. RDGT: Enhancing Group Cognitive Diag-nosis with Relation-Guided Dual-Side Graph Transformer. IEEE Transactions onKnowledge and Data Engineering (2024). Rui Zha, Chuan Qin, Le Zhang, Dazhong Shen, Tong Xu, Hengshu Zhu, andEnhong Chen. 2023. Career mobility analysis with uncertainty-aware graph au-toencoders: A job title transition perspective. IEEE Transactions on ComputationalSocial Systems (2023). Jiani Zhang, Xingjian Shi, Irwin King, and Dit-Yan Yeung. 2017. Dynamic key-value memory networks for knowledge tracing. In Proceedings of the 26th inter-national conference on World Wide Web. 765774. Yunfei Zhang, Chuan Qin, Dazhong Shen, Haiping Ma, Le Zhang, Xingyi Zhang,and Hengshu Zhu. 2023. ReliCD: A Reliable Cognitive Diagnosis Framework withConfidence Awareness. In 2023 IEEE International Conference on Data Mining(ICDM). IEEE, 858867. Zhi Zheng, Xiao Hu, Zhaopeng Qiu, Yuan Cheng, Shanshan Gao, Yang Song,Hengshu Zhu, and Hui Xiong. 2024. Bilateral Multi-Behavior Modeling for Recip-rocal Recommendation in Online Recruitment. IEEE Transactions on Knowledgeand Data Engineering (2024). Linhong Zhu, Dong Guo, Junming Yin, Greg Ver Steeg, and Aram Galstyan. 2016.Scalable temporal latent space inference for link prediction in dynamic socialnetworks. IEEE Transactions on Knowledge and Data Engineering 28, 10 (2016),27652777.",
  "RIGL0.83040.0049 0.78530.0102 0.13830.0048 0.10780.0113 0.79590.0080 0.74420.0085 0.13570.0042 0.10580.0039": "effective and has a more pronounced impact on the group-level di-agnosis during the reciprocal learning modeling. (3) In the baselinemethods, the DTransformer and LPKT models exhibit relativelybetter performance compared to other baselines. This phenomenonmay be attributed to the gains from modeling forgetting behav-iors in both group and individual learning processes in those twomodels.In particular, we also conducted experiments for baselines indi-vidual version (i.e., traditional knowledge tracing model) utilizingonly individual interaction data and present the results of LPKT-Indand simpleKT-Ind in (the complete results can be found inAppendix A.4). As can be observed from the results, our RIGL re-mains significantly superior in terms of individual-level proficiencyassessment. Specifically, it shows an average increase of 5.91% and 3.48% over the best individual knowledge tracing baseline in termsof AUC and ACC metrics, respectively, which demonstrates thecontribution of group learning behaviors to the modeling of inde-pendent learning, and the effectiveness of the proposed reciprocallearning modeling in our RIGL model. In addition, some of the base-line models are less effective at the individual-level than modelingthe individual alone, suggesting that simple joint training may notalways work, and this is further evidence of the validity of RIGL.An interesting phenomenon is that the advantage of RIGL over theKTM-ind model behaves differently on different datasets, whichmay be caused by the characteristics of the dataset. Since the RIGLmodel requires tracing both individual and group ability changes,its performance suffers when the group-exercise interactions arerelatively sparse in the dataset. Whereas KTM-ind (e.g., LPKT-ind)",
  "Ablation Study (RQ2)": "To answer RQ2, we first conducted a comprehensive ablation studyto investigate the impact of each module in the RIGL model by defin-ing the following variations: 1) w/o Att: removing the absence-perceived attention aggregation module in reciprocal enhancedlearning; 2) w/o RL: removing the reciprocal enhanced learningmodule; 3) w/o DG: removing the dynamic graph modeling; 4) w/oCL: removing the contrastive loss. As illustrated in , theresults reveal insightful observations: (1) In comparison to RIGL, allvariants suffer relative performance degradation on four datasetsacross various evaluation metrics, demonstrating the contributionof the designed submodules to our proposed model. (2) The mostsignificant decrease in the model performance occurs after remov-ing the reciprocal enhanced learning module, which exhibits thatjointly and interactively modeling the reciprocal learning processof students and groups plays a crucial role in the HKT task, andalso confirms the soundness of our designed model. (3) The per-formance degradation of removing the dynamic graph modelingmodule is also quite noticeable, reflecting the fact that dynamicgraph modeling is important for capturing the evolving knowledgestates of groups and individuals.In addition, we further conducted an ablation study to investi-gate the effect brought by reciprocal learning on the performanceof holistic knowledge tracing by disassembling the reciprocal en-hanced learning module in RIGL. Specifically, we removed the 1e-45e-41e-32e-31e-2 0.7850 0.7875 0.7900 0.7925 0.7950 0.7975 0.8000 AUC",
  ": Sensitivity analysis of coefficient on SLP-Bio": "individual-level features, the group-level features, and both featuresin the reciprocal enhanced learning module (i.e., w/o Ind, w/o Grp,and w/o Dual), respectively, and then inspected the performancevariations. Notably, the experiments in this section are completedby removing the features of individual and group interactions inthe reciprocal module, respectively, within the framework of HKT,which requires that the ablated RIGL is still capable of tracking boththe evolution of individual and group abilities. The experimentalresults are shown in . We have the following observations:(1) The absence of individual-level and group-level features bringsabout performance degradation, demonstrating the complementaryfacilitation of individuals and groups learning features in the HKTtask. (2) The performance degradation introduced when neither in-dividual modeling nor group modeling utilizes each others featuresduring the information fusion process is very significant, which isfurther evidence of the effectiveness of reciprocal learning.5.4Parameter Sensitivity Analysis (RQ3) To answer RQ3, we conducted a parameter sensitivity analysis inthis section to investigate the effects of hyper-parameters, whichmainly include the learning rate and the weight coefficient of thecontrastive loss. Specifically, we set the list of learning rates to be{1e-4, 5e-4, 1e-3, 2e-3, 1e-2}, as well as the values {1e-4, 1e-3, 1e-2,5e-2, 1e-1}, and mainly show the experimental results on the SLP-Bio dataset. As shown in , we observe that 1e-3 is sufficientfor the learning rate, and the performance presents a trend of rising",
  "Case Study (RQ4)": "5.5.1Visualization of Proficiency Evolution. To further un-derstand how RIGL traces the evolution of the individual-leveland group-level knowledge states, in this case study, we demon-strated the tracing process. illustrates the evolution ofboth a learners and a groups knowledge proficiency across fivetime frames on four knowledge concepts, as traced by RIGL. Fromthe visualization, we can observe that the individuals mastery ofthe corresponding concept of the exercise increases even if it isanswered incorrectly, which implies that wrongly responding to theexercise also brings knowledge gain. In addition, it can be seen thatthe knowledge gained from individual learning affects the groupsknowledge level, yet the degree of impact varies under differenttime frames, meaning that the effect of the individual on the groupvaries over time, which is in line with the real scenario. 5.5.2Dynamic Relationship Mining. Specifically, a case studyof relationship observations was conducted to explore the dynamicrelationships between individuals and the group, as well as amongindividuals in the holistic knowledge tracing process. shows the dynamic relationship graph across the group under fivetime frames by presenting the correlation distances (as mentionedin .2; for convenience of the display, three students underone group were selected). It can be observed that the relationshipsbetween students and groups and among students are dynamicallychanging (e.g., the increasing similarity of both 1 and as well as1 and 3), justifying the exploitation and modeling of potential as-sociations through our use of the dynamic graph. In addition, basedon the mined latent relationships, potential subgroups within thegroup can be effectively identified, which will help us to perceivethe evolution of the internal structure of the group.",
  "CONCLUSION": "In this paper, we introduced a novel framework termed as RIGL (aunified Reciprocal approach for Independent and Group Learningprocesses), which aims to provide comprehensive and dynamicmodeling for both independent learning and group learning. Ourapproach comprises several key components. Initially, we deviseda time frame-aware reciprocal embedding module to concurrentlycapture the temporal interactions between students and groups dur-ing the learning processes, followed by a reciprocal-enhanced learn-ing mechanism that maximizes the synergistic insights from thesetwo learning behaviors. To further mine the intricate dynamics ofstudent-group associations, we proposed a relation-guided tempo-ral attentive network encompassing dynamic graph modeling andtemporal self-attention mechanisms. Notably, the relation-guideddynamic graph is formed by uncovering potential links between stu-dents and groups. Finally, we incorporated a bias-aware contrastivelearning module to ensure model stability during training. Exten-sive experiments were conducted on four real-world educationaldatasets to substantiate the efficacy of our RIGL model, particularlyin addressing the HKT task. We hope this work could lead to furtherstudies on holistic knowledge tracing. This work was supported in part by the National Natural ScienceFoundation of China under Grant (No. U21A20512, No. 62107001,and No.62302010), in part by the Anhui Provincial Natural ScienceFoundation (No. 2108085QF272), and in part by by China Postdoc-toral Science Foundation (No. 2023M740015).",
  "Albert T Corbett and John R Anderson. 1994. Knowledge tracing: Modeling theacquisition of procedural knowledge. User modeling and user-adapted interaction4 (1994), 253278": "Chuyu Fang, Chuan Qin, Qi Zhang, Kaichun Yao, Jingshuai Zhang, Hengshu Zhu,Fuzhen Zhuang, and Hui Xiong. 2023. Recruitpro: A pretrained language modelwith skill-aware prompt learning for intelligent recruitment. In Proceedings ofthe 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.39914002. Mingyu Feng, Neil Heffernan, and Kenneth Koedinger. 2009. Addressing theassessment challenge with an online system that tutors as it assesses. Usermodeling and user-adapted interaction 19, 3 (2009), 243266.",
  "John Hayes and Christopher W Allinson. 1998. Cognitive style and the theory andpractice of individual and collective learning in organizations. Human relations51, 7 (1998), 847871": "Feihu Jiang, Chuan Qin, Kaichun Yao, Chuyu Fang, Fuzhen Zhuang, Hengshu Zhu,and Hui Xiong. 2024. Enhancing Question Answering for Enterprise KnowledgeBases using Large Language Models. arXiv preprint arXiv:2404.08695 (2024). Feihu Jiang, Chuan Qin, Jingshuai Zhang, Kaichun Yao, Xi Chen, Dazhong Shen,Chen Zhu, Hengshu Zhu, and Hui Xiong. 2024. Towards Efficient ResumeUnderstanding: A Multi-Granularity Multi-Modal Pre-Training Approach. arXivpreprint arXiv:2404.13067 (2024). Seyed Mehran Kazemi, Rishab Goel, Kshitij Jain, Ivan Kobyzev, Akshay Sethi,Peter Forsyth, and Pascal Poupart. 2020. Representation learning for dynamicgraphs: A survey. The Journal of Machine Learning Research 21, 1 (2020), 26482720.",
  "Wonsung Lee, Jaeyoon Chun, Youngmin Lee, Kyoungsoo Park, and Sungrae Park.2022. Contrastive learning for knowledge tracing. In Proceedings of the ACM WebConference 2022. 23302338": "Hao Lin, Hengshu Zhu, Yuan Zuo, Chen Zhu, Junjie Wu, and Hui Xiong. 2017.Collaborative company profiling: Insights from an employees perspective. InProceedings of the AAAI Conference on Artificial Intelligence, Vol. 31. Sannyuya Liu, Shengyingjie Liu, Zongkai Yang, Jianwen Sun, Xiaoxuan Shen,Qing Li, Rui Zou, and Shangheng Du. 2023. Heterogeneous Evolution NetworkEmbedding with Temporal Extension for Intelligent Tutoring Systems. ACMTransactions on Information Systems 42, 2 (2023), 128. Shuhuan Liu, Xiaoshan Yu, Haiping Ma, Ziwen Wang, Chuan Qin, and XingyiZhang. 2023. Homogeneous Cohort-Aware Group Cognitive Diagnosis: A Multi-grained Modeling Perspective. In Proceedings of the 32nd ACM InternationalConference on Information and Knowledge Management. 40944098. Zitao Liu, Qiongqiong Liu, Jiahao Chen, Shuyan Huang, Boyu Gao, Weiqi Luo,and Jian Weng. 2023. Enhancing deep knowledge tracing with auxiliary tasks. InProceedings of the ACM Web Conference 2023. 41784187. Zitao Liu, Qiongqiong Liu, Jiahao Chen, Shuyan Huang, and Weiqi Luo. 2022.simpleKT: A Simple But Tough-to-Beat Baseline for Knowledge Tracing. In TheEleventh International Conference on Learning Representations. Yu Lu, Yang Pian, Ziding SHEN, Penghe CHEN, and Xiaoqing Li. 2021. SLP: AMulti-Dimensional and Consecutive Dataset from K-12 Education. In Proceedingsof the 29th International Conference on Computers in Education. 261266.",
  "Haiping Ma, Siyu Song, Chuan Qin, Xiaoshan Yu, Limiao Zhang, Xingyi Zhang,and Hengshu Zhu. 2024. DGCD: An Adaptive Denoising GNN for Group-levelCognitive Diagnosis. In IJCAI": "Haiping Ma, Jingyuan Wang, Hengshu Zhu, Xin Xia, Haifeng Zhang, XingyiZhang, and Lei Zhang. 2022. Reconciling Cognitive Modeling with KnowledgeForgetting: A Continuous Time-aware Neural Network Approach.. In IJCAI.21742181. Haiping Ma, Yong Yang, Chuan Qin, Xiaoshan Yu, Shangshang Yang, XingyiZhang, and Hengshu Zhu. 2024. HD-KT: Advancing Robust Knowledge Tracingvia Anomalous Learning Interaction Detection. In Proceedings of the ACM on WebConference 2024. 44794488. Haiping Ma, Jinwei Zhu, Shangshang Yang, Qi Liu, Haifeng Zhang, Xingyi Zhang,Yunbo Cao, and Xuemin Zhao. 2022. A prerequisite attention model for knowl-edge proficiency diagnosis of students. In Proceedings of the 31st ACM InternationalConference on Information & Knowledge Management. 43044308. Bill Meyer, Naomi Haywood, Darshan Sachdev, and Sally Faraday. 2008. Whatis independent learning and what are the benefits for students. Department forChildren, Schools and Families Research Report 51 (2008), 16. Shalini Pandey and George Karypis. 2019. A self-attentive model for knowledgetracing. In 12th International Conference on Educational Data Mining, EDM 2019.International Educational Data Mining Society, 384389. Aldo Pareja, Giacomo Domeniconi, Jie Chen, Tengfei Ma, Toyotaro Suzumura,Hiroki Kanezashi, Tim Kaler, Tao Schardl, and Charles Leiserson. 2020. Evolvegcn:Evolving graph convolutional networks for dynamic graphs. In Proceedings ofthe AAAI conference on artificial intelligence, Vol. 34. 53635370.",
  "Radek Pelnek. 2017. Bayesian knowledge tracing, logistic models, and beyond:an overview of learner modeling techniques. User Modeling and User-AdaptedInteraction 27 (2017), 313350": "Chris Piech, Jonathan Bassen, Jonathan Huang, Surya Ganguli, Mehran Sahami,Leonidas J Guibas, and Jascha Sohl-Dickstein. 2015. Deep knowledge tracing.Advances in neural information processing systems 28 (2015). Chuan Qin, Le Zhang, Yihang Cheng, Rui Zha, Dazhong Shen, Qi Zhang, XiChen, Ying Sun, Chen Zhu, Hengshu Zhu, and Hui Xiong. 2023. A comprehensivesurvey of artificial intelligence techniques for talent analytics. arXiv preprintarXiv:2307.03195 (2023). Chuan Qin, Hengshu Zhu, Dazhong Shen, Ying Sun, Kaichun Yao, Peng Wang,and Hui Xiong. 2023. Automatic skill-oriented question generation and rec-ommendation for intelligent job interviews. ACM Transactions on InformationSystems 42, 1 (2023), 132. Chuan Qin, Hengshu Zhu, Chen Zhu, Tong Xu, Fuzhen Zhuang, Chao Ma, Jing-shuai Zhang, and Hui Xiong. 2019. DuerQuiz: A personalized question rec-ommender system for intelligent job interview. In Proceedings of the 25th ACMSIGKDD International Conference on Knowledge Discovery & Data Mining. 21652173. Emanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, FedericoMonti, and Michael Bronstein. 2020. Temporal graph networks for deep learningon dynamic graphs. arXiv preprint arXiv:2006.10637 (2020). Aravind Sankar, Yanhong Wu, Liang Gou, Wei Zhang, and Hao Yang. 2020.Dysat: Deep neural representation learning on dynamic graphs via self-attentionnetworks. In Proceedings of the 13th international conference on web search anddata mining. 519527.",
  "Rakshit Trivedi, Mehrdad Farajtabar, Prasenjeet Biswal, and Hongyuan Zha. 2019.Dyrep: Learning representations over dynamic graphs. In International conferenceon learning representations": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin. 2017. Attention is allyou need. Advances in neural information processing systems 30 (2017). Zichao Wang, Angus Lamb, Evgeny Saveliev, Pashmina Cameron, Yordan Za-ykov, Jos Miguel Hernndez-Lobato, Richard E Turner, Richard G Baraniuk,Craig Barton, Simon Peyton Jones, et al. 2020. Instructions and Guide for Di-agnostic Questions: The NeurIPS 2020 Education Challenge. arXiv preprintarXiv:2007.12061 (2020).",
  "Shangshang Yang, Linrui Qin, and Xiaoshan Yu. 2024. Endowing Interpretabilityfor Neural Cognitive Diagnosis by Efficient Kolmogorov-Arnold Networks. arXivpreprint arXiv:2405.14399 (2024)": "Shangshang Yang, Haoyu Wei, Haiping Ma, Ye Tian, Xingyi Zhang, Yunbo Cao,and Yaochu Jin. 2023. Cognitive diagnosis-based personalized exercise groupassembly via a multi-objective evolutionary algorithm. IEEE Transactions onEmerging Topics in Computational Intelligence (2023). Shangshang Yang, Xiaoshan Yu, Ye Tian, Xueming Yan, Haiping Ma, and XingyiZhang. 2023. Evolutionary Neural Architecture Search for Transformer in Knowl-edge Tracing. In Thirty-seventh Conference on Neural Information ProcessingSystems.",
  "SymbolsDescription": "O, S, E, C, QThe set of groups, students, exercises,knowledge concepts, and the Q-matrix, re-spectively.,,,The group, the student, the exercise, andthe knowledge concept.R, RThe whole interaction sequence of the stu-dent and the group, respectively.F, HThe student-exercise and group-exerciseinteraction sequence under -th timeframe. ,The score that the student got on the -thlog under -th time frame and the correctrate that group got on the -th log under-th time frame. x, zThe set of exercise encoding and responseencoding, respectively.x, zThe set of interaction encodings of groups.x ,z ,x,zThe enhanced interaction encoding ofgroup and student.G = (, )The group-individual graph with a nodeset and a edge set .V = {,1, . . . ,||}The node set. = (, )The edge set including the connectingedges between students and groups.V, D, AThe node feature matrix, the relation ma-trix and the adjacency matrix in the -thtime frame, respectively.Q, K, V Q, K, VThe query matrix, the key matrix, and thevalue matrix of the self-attention module.m, nThe interaction feature of the group andstudent extracted from the learned graphlayer.h , hThe knowledge states of group and stu-dent under -the time frame, respec-tively.W, bThe trainable matrix and parameters. | |The cardinality of a set.,The temperature parameter and the coef-ficient weight parameter., , , The size of the group set O, the studentset S, the exercise set E, and the conceptset C, respectively.,,,,The dimension of exercise, the dimensionof concept, the dimension of response, thedimension of -th graph layer, and the hid-den dimension, respectively.L, L, L, LThe total loss, the group loss, the studentloss, and the contrastive loss, respectively. A.2.2Data Preprocessing. All datasets above contain grouplabels (i.e., the class to which the students belong), and studentsfrom the same group share the same group category. In particular, toensure the feasibility, we conducted preprocessing on the datasets.Specifically, for each dataset, we first constructed two exercising",
  "RIGL0.73940.0141 0.76730.0126 0.73260.0115 0.67790.0144 0.83040.0049 0.78530.0102 0.79590.0080 0.74420.0085": "sequences of the individual and the group based on time perioddivisions, where each time frame contains two types of interactiondata, i.e., student-exercise responses and group-exercise responses.Particularly, for each group-exercise response under each timeframe, we calculated the correct rate of this group of students onthe exercise as the groups response result. We screened out groupswith fewer than three students and students with less than threeresponse logs. Due to the different temporal characteristics of thedifferent datasets, each dataset is not divided over exactly the sametime span, where ASSIST12 and NIPS-Edu are divided in days, andSLP-Math and SLP-Bio are divided in hours. Finally, since the group-exercise interactions are very sparse under each time frame in theraw data, i.e., the number of exercises answered by all studentswithin the same group is extremely limited, we set a threshold of0.6 to prevent the group learning sequence from being empty.",
  "A.3.1Baselines. In this paper, we compare RIGL with eight base-line approaches. The details of all the comparison methods are:": "DKT : DKT is one of the most classical knowledge tracingmethods, which utilizes a recurrent neural network (RNN) tomodel the exercise interaction sequences and mine the cognitivepattern between learners and exercises. SAKT : SAKT is the first knowledge tracing model intro-ducing the self-attention mechanism , which exploits thetransformer structure to model long-range dependencies of in-teraction behaviors in students exercising sequences. AKT : AKT designs a novel monotonic attention module onthe basis of transformer architecture for effectively modeling theforgetting behaviors of learners, which leverages an exponentialdecay function that can perceive contextual distance informationto learn the attention weights. LPKT : LPKT proposes a learning process-consistent modelto explore the consistency of students changing knowledge stateduring the learning process, which consists of a learning module,a forgetting module, and a predicting module. GIKT : GIKT is a graph-based Interaction model for Knowl-edge Tracing that leverages a graph convolutional network (GCN)to effectively integrate question-skill correlations and addressesthe challenge of dispersed relevant questions by considering",
  "DTransformer : DTransformer introduces a DiagnosticTransformer with a novel contrastive learning-based trainingapproach, designed to accurately diagnose and trace learnersknowledge proficiency": "A.3.2Implementation. To adapt these baselines that only focuson individual-level knowledge tracing to the HKT task, we describethe implementation details. Specifically, for the inputs including in-dependent learning sequence RS and the group learning sequenceR O, the baseline first encodes their interaction behavior, respec-tively, i.e., ( , , ) and ( , , ) under each time frame F andH. Subsequently, the baseline performs an average aggregation ofthe interaction encodings under each time frame in each of the twolearning sequences. Furthermore, the obtained encoding sequencesare fed into the knowledge tracing module underlying the baselinefor modeling the changing knowledge states of the individual andgroup. Finally, a joint loss function, comprising a cross-entropy lossfor predicting students performance and a mean square error lossfor predicting the groups performance, is used to train the model.A.4Complete Comparison Results ofIndividual-based Baselines As described in the .2, we also conducted experiments forbaselines individual version (i.e., traditional knowledge tracingmodel) utilizing only individual interaction data. The completecomparison results can are shown in Table S2. It can be observedthat in comparison to baseline models, the proposed RIGL remainssignificantly superior. Specifically, it shows an average increaseof 5.91% and 3.48% over the best individual knowledge tracingbaseline in terms of AUC and ACC metrics, respectively, whichdemonstrates the contribution of group learning behaviors to themodeling of independent learning, and the effectiveness of theproposed reciprocal learning modeling in our RIGL model."
}