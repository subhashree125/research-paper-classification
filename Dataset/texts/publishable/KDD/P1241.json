{
  "ABSTRACT": "Class imbalance is the phenomenon that some classes have muchfewer instances than others, which is ubiquitous in real-worldgraph-structured scenarios. Recent studies find that off-the-shelfGraph Neural Networks (GNNs) would under-represent minor classsamples. We investigate this phenomenon and discover that the sub-spaces of minor classes being squeezed by those of the major onesin the latent space is the main cause of this failure. We are naturallyinspired to enlarge the decision boundaries of minor classes andpropose a general framework GraphSHA by Synthesizing HArderminor samples. Furthermore, to avoid the enlarged minor bound-ary violating the subspaces of neighbor classes, we also propose amodule called SemiMixup to transmit enlarged boundary informa-tion to the interior of the minor classes while blocking informationpropagation from minor classes to neighbor classes. Empirically,GraphSHA shows its effectiveness in enlarging the decision bound-aries of minor classes, as it outperforms various baseline methods inclass-imbalanced node classification with different GNN backboneencoders over seven public benchmark datasets. Code is avilable at",
  "Corresponding authors": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 23, August 610, 2023, Long Beach, CA, USA. 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0103-0/23/08...$15.00 0.00.20.40.60.81.0 Prob. GCN GraphSmote ReNode GraphENS TAM GraphSHA MajorMinor 0.00.20.40.60.81.0 Prob. MajorMinor : Probability distribution of misclassified sampleson Cora-LT (left) and CiteSeer-LT (right) datasets w.r.t. dif-ferent methods. Both datasets are preprocessed to follow along-tailed distribution with an imbalance ratio of 100 asin . We treat the half classes with fewer instances as mi-nor ones and the other half as major ones. We can see thatGCN suffers from the squeezed minority problem, as nearlyall false samples are classified as major classes. Though base-line methods GraphSmote, ReNode, GraphENS, and TAM canremit this problem to some extent, the minor subspaces arestill squeezed as their distributions are still highly biased.Our GraphSHA, on the other hand, can significantly enalrgethe minor subspaces as the probability of misclassified sam-ples being minor classes is close to 0.5 (black dotted line).",
  "INTRODUCTION": "Node classification is regarded as a vital task for graph analy-sis . With the fast development of neural networks in thepast few years, Graph Neural Networks (GNNs) have become thede-facto standard to handle this task and achieved remarkable per-formances in many graph-related scenarios . CurrentGNNs are implicitly based on the class balance assumption wherethe numbers of training instances in all classes are roughly bal-anced . This assumption, however, barely holds for graph datain-the-wild as they tend to be class-imbalanced intrinsically .In class-imbalanced graphs, some classes (minor ones) possess muchfewer instances than others (major ones). For example, in a large-scale citation network , there are more papers on artificial",
  "KDD 23, August 610, 2023, Long Beach, CA, USA.Wen-Zhi Li, Chang-Dong Wang, Hui Xiong, and Jian-Huang Lai": "Aleksandar Bojchevski and Stephan Gnnemann. 2018. Deep Gaussian Embed-ding of Graphs: Unsupervised Inductive Learning via Ranking. In 6th InternationalConference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April30 - May 3, 2018, Conference Track Proceedings. OpenReview.net. Kay Henning Brodersen, Cheng Soon Ong, Klaas Enno Stephan, and Joachim M.Buhmann. 2010. The Balanced Accuracy and Its Posterior Distribution. In 20thInternational Conference on Pattern Recognition, ICPR 2010, Istanbul, Turkey, 23-26August 2010. IEEE Computer Society, 31213124.",
  "Class Imbalance Problem": "The class imbalance problem is widespread in real-world applica-tions for various machine learning tasks . As major classeshave much more samples than minor classes in the training set, ma-chine learning models are believed to easily under-represent minorclasses, which results in poor overall classification results .Existing countermeasures to remit the class imbalance problemcan be roughly divided into two categories: loss-modifying andgenerative approaches. Loss-modifying approaches aredevoted to modifying the loss function to focus more on minorclasses. Generative approaches are devoted to generatingminor samples to balance the training set. Directly applying theseapproaches to graph data cannot achieve satisfactory results asgraphs possess edges between node samples intrinsically .For graph data, methods to handle class-imbalanced node classifi-cation are primarily generative approaches, as edges can be synthe-sized together with node samples. Among them, GraphSmote synthesizes minor nodes by interpolating between two minor nodesin the same class in the SMOTE manner, and an extra edgepredictor is leveraged to generate edges for the synthesized nodes.DR-GCN and ImGAGN leverage neural network GAN to synthesize minor nodes. However, ImGAGN is only capable of bi-nary classification tasks of distinguishing minor nodes from majorones, which is non-trivial to be applied to multi-label classificationtasks in this paper. For these methods, the synthesized nodes aregenerated based on existing minor nodes, which are still confinedto the raw minor subspaces and bring limited gain to the squeezed",
  "/12, ->": ": Comparison of the synthesis for GraphSmote ,GraphENS , and GraphSHA. GraphSmote can only gener-ate minor nodes within the subspace, which is not beneficialto alleviating the squeezed minority problem. GraphENS,on the other hand, generates minor nodes far beyond thedecision boundary, which degenerates major class accuracy.Different from these methods, GraphSHA generates minornodes beyond the minor decision boundary properly, whichcan effectively enlarge the subspace of minor class. minor class. GraphENS , on the other hand, synthesizes ego net-works for minor classes by combining an ego network centered ona minor sample and another one centered on a random sample fromthe entire graph, which certainly enlarges the decision boundaryof minor classes. However, the heuristic generation overdoes theminor node generation, thus unavoidably increasing false positivesfor major classes. To sum up, these generative methods cannotenlarge the minor subspaces effectively and precisely to alleviatethe bias, which is the main focus of this work. The comparison ofGraphSmote, GraphENS, and GraphSHA is illustrated in .It is worth mentioning that there are also loss-modifying ap-proaches for graph data like ReNode and TAM . However,they mainly focus on the topological imbalance issue, i.e., the im-balanced connectivity of nodes in the graph, which is beyond thescope of our work. Nevertheless, we also compare these methodsin .",
  "Hard Sample Mining": "Hard samples, i.e., samples that are difficult for the current modelto discriminate, are believed to play a crucial role in classificationtasks . They are often leveraged in self-supervised learning,where the objectives are roughly defined as maximizing the similar-ity between positive pairs while minimizing the similarity betweennegative pairs . As positive pairs are often limited while nega-tive pairs are exhaustive, hard sample mining is typically referred toas hard negative mining, to name a few . Similarto the SemiMixup module in our work, FaceNet also discoversthat naively selecting hard negative samples in face recognitionwould practically lead to local minima, thus introducing semi-hardsamples. However, it differs from our work as we consider datasynthesis (FaceNet only chooses among existing negative samples)in graph data (FaceNet deals with image data).In the scope of class-imbalanced classification, some pioneeringwork also leverage hard samples to avoid easy negatives overwhelming the training phase. However, they are generally loss-modifying methods. As mentioned above, they cannot exploit thetopology information, which makes them hard to be applied to class-imbalanced graphs. On the other hand, GraphSHA synthesizes hardsamples in the graph domain with edges, which enables it to tacklethe problem naturally.",
  "PRELIMINARIES3.1Notations and Imbalance Settings": "We focus on semi-supervised node classification task on an un-weighted and undirected graph G = {V, E}, where V = {1, , }is the node set with nodes and E V V is the edge set. The ad-jacency matrix and the feature matrix are denoted as {0, 1} and R respectively, where = 1 iff (, ) E, and R is a -dim raw feature of node . N is the direct neighborset of node .Every node corresponds with a class label () {1, ,}with classes in total, and we denote all nodes in class as . Inclass-imbalanced node classification, the labeled nodes in trainingset V V are imbalanced, where the imbalance ratio is definedas = max | |/min | |.",
  "original edge, sp. sampling,": ": GraphSHA overview where 1 is minor class and 2, 3 are major classes. (Left): two source nodes and arefirstly identified via three samplings: sampling from minor nodes in 1 according to their hardness H to get ; sampling frommajor classes 2, 3 according to s confidence on them to get neighbor class ; and sampling from nodes in neighborclass according to their confidences on minor class 1 to get . (Middle): SemiMixup is conducted by mixuping s1-hop subgraph and solely to get synthesized node . Here, the unweighted subgraph is transformed into a weightedone via diffusion-based smoothing based on graph topology. (Right): the augmented graph is fed into a GNN for traditionalnode classification task where the minor class decision boundary is enlarged properly without degenerating neighbor classes.",
  "Identifying Source Samples": "We are motivated to enlarge the minor decision boundary, whichis determined by hard minor anchor node . We also need anauxiliary node from s neighbors in the latent space sothat the boundary is enlarged from towards .Many methods can be leveraged to calculate node hardness inthe latent space, such as confidence and -Nearest Neighbor(NN). Without loss of generalizability, we adopt confidence as thehardness metric for its light-weighted computational overhead. Ex-tra experiments with NN-based node hardness are in Appendix B.",
  "temperature": "In practice, node logits from the previous epoch can be lever-aged to get their hardness H. Thus we can identify minor anchornodes by sampling from a multinomial distribution with thehardness as the probability.In identifying auxiliary node , we first determine the neigh-bor class of each anchor node by sampling from a multinomialdistribution with () as the probability. Then, for nodes in, we sample from another multinomial distribution with theirconfidence in class () as the probability. In this way, we canget two souce nodes and near the boundary of minorclass. An example in identifying and is given on the leftside of .",
  "= + (1 ), .(4)": "Here, smaller will force the generated node feature to be moreanalogous to the auxiliary node, which is expected to be morebeneficial to enlarge the minor decision boundary. We validate thisvia an empirical study in .7 as smaller E() will contributeto better model performance. Synthesizing Edges. As features are propagated via edges in GNN,we expect the synthesized edges to enable propagating informationbeyond the minor class boundary to the interior of the minor classwhile blocking propagation from the minor class to the neighborclass to avoid degenerating the neighbor class. To this end, insteadof connecting with the nodes in the union of s 1-hopsubgraph and s 1-hop subgraph, we make a simple adaptationby only connecting with the nodes in s 1-hop subgraph, asonly s 1-hop subgraph tend to share the same minor label with according to graph homophily . In this way, messagepassing in GNNs would enable the enlarged boundary information represented as the synthesized node feature to be propagatedto the interior of the minor class.However, s 1-hop sugraph may contain less nodes than wewant to sample. Furthermore, as the graph is unweighted, uniform",
  ": return": "sampling from the subgraph may ignore important topology infor-mation. Our countermeasure is to transform the unweighted hardgraph into a weighted soft one based solely on graph topology. Here,we refer to graph diffusion-based smoothing proposed in GDC ,which recovers meaningful neighborhoods in a graph. Specifically,the diffusion matrix is defined as = =0 , which has twopopular versions of Personalized PageRank (PPR) with = 1, = (1 ) , and Heat Kernel (HK) with = 1, = ! ,where is the diagonal matrix of node degrees, i.e., = ,and is the diffusion time. After sparsifying as in , we geta weighted and sparse graph adjacency matrix R , whichcan be regarded as a weighted version of the adjacency matrix .Afterward, we can leverage as the probability of a multi-nomial distribution to sample neighbors of . The number ofneighbors is sampled from another degree distribution based onthe entire graph to keep degree statistics, as suggested in .To sum up, the feature of the synthesized minor sample is generated via the mixup of the features of and , whilethe edges connecting is generated from the 1-hop subgraph of without mixup. We give a formal definition of the synthesizedharder minor sample derived from SemiMixup below.",
  "Complexity Analysis": "It is worth noting that the extra calculation of GraphSHA introduceslight computational overhead over the base GNN model. Let bethe number of training samples. Then, the number of synthesizednodes for each class is O(). As logits can be obtained from theprevious epoch, we can get node hardness without extra computa-tion . We need O( 2) extra time for sampling anchor nodesand auxiliary nodes. For generating node features of synthesizednodes, we need O() extra time, where is the dimension of theraw feature. For generating edges of synthesized nodes, we needO(/ |E|) time. Overall, the extra computational overheadover the base model is O( 2 + + / |E|). As in traditional semi-supervised node classification, GraphSHA onlyintroduces lightweight computational overhead, which enables itto be applied to large-scale graphs, as introduced in .3.",
  "Experimental Setup": "Datasets. We adopt seven benchmark datasets, including Cora,CiteSeer, PubMed , Amazon-Photo (Photo), Amazon-Computers(computer), Coauthor-CS (CS) and ogbn-arXiv (arXiv) toconduct all the experiments. The statistics of these datasets areprovided in . A detailed description of them is provided inAppendix A.1. Compared Baselines. We compare GraphSHA with various im-balance handling methods. For loss-modifying approaches, we com-pare (1) Reweight, which reweights class weights to be proportionalto the numbers of class samples; (2) PC Softmax and (3) Class-Balanced Loss (CB Loss) , which are two general methods tomodify the loss function in handling the imbalance issue; (4) Focal",
  "GraphSHA78.800.2473.560.3574.270.3063.760.3858.250.3758.040.4578.200.1974.070.2374.930.23": "Loss , which modifies the loss function to focus on hard samples;and (5) ReNode , which also considers topological imbalance inthe context of graph data based on the class imbalance. For gen-erative approaches, we compare (1) Upsample, which directly du-plicates minor nodes; (2) GraphSmote and (3) GraphENS ,which have been introduced in . Furthermore, we alsocompare the state-of-the-art method (4) TAM based on thebest-performed GraphENS by default, which aims to decrease thefalse positive cases considering the graph topology.Configurations and Evaluation Protocols. We adopt variousGNNs, namely GCN , GAT , and GraphSAGE as thebackbones of our model and baseline models. Their hidden layeris set to 2, both in 64-dim by default. For GAT, the multi-head number is set to 8. For our GraphSHA, for node feature synthesis issampled from a beta distribution as (1,2), and we presentthe hyper-parameter analysis of the distribution in .7. Fordiffusion matrix , we adopt the PPR version with = 0.05, andwe also adopt top- with = 128 to select highest mass percolumn of to get a sparsified , both of which are suggestedin . The implementation details for baselines are described inAppendix A.2. Samples are synthesized until each class reachesthe mean or maximum number of samples among all classes inthe training set as a hyper-parameter. We apply Accuracy (Acc.),balanced Accuracy (bAcc.), and macro F1 score (F1) as the evaluationmetrics following , where bAcc. is defined as the averagerecall for each class .",
  "Results on Manually Imbalanced Datasets": "We consider both long-tailed class imbalance setting on Cora,CiteSeer, PubMed and step class imbalance setting on Photo,Computer, and CS to conduct the experiments. In the long-tailedsetting, we adopt full data split for the three datasets, and weremove labeled nodes in the training set manually until they followa long-tailed distribution as in . The imbalance ratio is setto an extreme condition of 100. In step setting, the datasets are splitinto training/validation/test sets with proportions 10%/10%/80%respectively as in , where half of the classes are major classesand share the same number of training samples , while theother half are minor classes and share the same number of trainingsamples = / in the training set. The imbalance ratio is set to 20 in this setting. The results are shown in and respectively for the two settings.From both tables, we can find that GraphSHA shows significantimprovements compared with almost all other contenders withdifferent GNN backbones, which shows the effectiveness of theoverall framework of GraphSHA. We provide more observations asfollows. Firstly, the performances of different methods are similaracross different GNN backbones, which shows that the performancegaps result from the models intrinsic properties. Secondly, gen-erative approaches generally perform better than loss-modifyingapproaches, which benefits from the augmented topological struc-ture. The results in step setting with GCN and GAT backbones areprovided in Appendix B due to the space constraint.",
  "Results on Naturally Imbalanced Datasets": "Class-imbalance problem is believed to be a common issue on real-world graphs , especially for those large-scale ones. We adoptarXiv dataset from OGB benchmark , which is highly imbal-anced with a training imbalance ratio of 775, validation imbalanceratio of 2,282, test imbalance ratio of 2,148, and overall imbalanceratio of 942. Though the dataset is highly imbalanced, this problemis barely studied in previous work.The result is shown in . As GraphENS suffers from Out-Of-Memory, we conduct TAM based on ReNode. We report theaccuracy on validation and test sets with hidden layer size 256,",
  "GraphSHA73.040.1172.140.2853.750.1653.130.20": "which is a common setting for this task as the dataset is split basedon chronological order. We also report balanced accuracy and F1score on the test set. From the table we can see that (1) Nearlyall imbalance handling approaches can improve balance accuracy.However, accuracy and F1 score are reduced compared with vanillaGCN for the baselines, which we attribute to the decision boundaryof minor classes not being properly enlarged as the boundariesof major classes are seriously degenerated. On the other hand,our GraphSHA outperforms all baselines in terms of all metrics,which verifies that it can enlarge the minor subspaces properlyvia the SemiMixup module to avoid violating neighbor classes. (2)Generative approaches GraphSmote and GraphENS both sufferfrom the OOM issue, which results from the calculation of thenearest sample in the latent space and adjacent node distribution,respectively. On the other hand, Our GraphSHA introduces lightextra computational overhead by effectively choosing source nodesvia the confidence-based node hardness.",
  ": Changing trend of F1-score with the increase ofimbalance ratio on Cora-LT with GCN": "as shown in on Cora-LT with GCN backbone. We can seethat the F1 scores of all methods are high when is small. Withthe increase of , the performance of GraphSHA remains relativelystable, and its superiority increases when becomes larger, whichindicates the effectiveness of GraphSHA in handling extreme classimbalance problem on graphs.",
  "Case Study": "We also present a case study on per-class accuracy for the baselinemethods and GraphSHA with GCN backbone on Cora-LT in .For generative approaches, GraphSmote only shows a tiny improve-ment for minor classes compared to Upsample, which verifies thatsynthesizing within minor classes could hardly enlarge the decisionboundary. GraphENS, on the other hand, shows decent accuracyfor minor classes. However, it is at the cost of the performancereduction for major classes, as the accuracy for 6 is the lowest,which verifies that GraphENS overdoes the minor node generation.Our GraphSHA can avoid both problems as it shows superior ac-curacy for both minor and major classes, which benefits from theSemiMixup module to synthesize harder minor samples to enlargethe minor decision boundaries effectively.",
  "Ablation Study": "In this subsection, we conduct a series of experiments to demon-strate the effectiveness of each component of GraphSHA, includinghow synthesizing harder minor samples and the SemiMixup moduleaffect the model performance. Specifically, we compare GraphSHAwith several of its ablated variants starting from the vanilla GNN.The results are shown in on the Cora-LT dataset with GCN",
  "GraphSHA51.676.966.065.476.593.892.1": "backbone. Here, easy samples are those that are far from the classboundary. Synthesizing easy samples is somewhat like GraphSmoteas they both generate samples within class subspace, and we cansee that they achieve similar results only slightly better thanthe Upsample baseline. Harder samples w/o SemiMixup are gener-ated via the mixup between auxiliary nodes 1-hop subgraph andthe anchor nodes, i.e., substituting N 1 () in Eq. (5) to N 1 () to take a more adventurous step. We cansee that the performance of minor classes improves. However, aswe analyzed before, it is at the cost of degrading major classes it performs the worst for the most major class. On the otherhand, Applying the SemiMixup module can alleviate the degra-dation while maintaining the accuracy for minor classes. Further-more, GraphSHA can achieve better performance when weightedadjacency matrix is leveraged via Heat Kernel (HK) or PersonalPageRank (PPR), which shows the importance of distinguishing thedifferent importance of topological structure.",
  "Hyper-parameter Analysis": "In GraphSHA, random variable is used to control the hardness ofthe synthesized sample as = +(1), and smaller indicates bias to harder samples. Here, we change the distributionwhere is sampled from, and the classification performance interms of F1 is elaborated in on Cora-LT with GCN back-bone. We can see that model performance drops as E() increases,which shows that synthesizing harder samples via the SemiMixupmodule is more beneficial for the model, as it can enlarge the minorsubspaces to a greater extent.",
  "Synthesizing easy samples cannot tackle the squeezed minorityproblem, as these generated samples are confined in the raw latentspaces of minor classes": "Synthesizing harder samples without SemiMixup can tackle thesqueezed minority problem by enlarging the minor subspaces to alarge margin. However, it overdoes the enlargement as it is proneto classify major classes as minor ones. Synthesizing via SemiMixup can remit the squeezed minorityproblem properly, as the probability of misclassified samples be-ing minor classes is close to 0.5. Furthermore, the closer the syn-thesized node feature to the neighbor class (i.e., larger E()), thebetter the problem is remitted, as the minor subspaces are en-larged to a greater extent. This observation is consistent with thehyper-parameter analysis in .7.",
  ": Visualization of GraphSHA on Cora-LT with GCN,where each node is colored by its label. In (a), the hardnessof each training node is marked via the node size": "are pretty small. And GraphSHA can recognize node hardness effec-tively as the nodes near the minor decision boundary are prone tobe chosen as source nodes. In (b), GraphSHA can factuallyenlarge the minor class boundaries by synthesizing plausible harderminor nodes. As the enlarged minor subspaces could include minorsamples in the test set to a large extent, GraphSHA can effectivelyremit the squeezed minority problem, as shown in (c), (d).",
  "CONCLUSION AND FUTURE WORK": "In this paper, we study class-imbalanced node classification andfind the squeezed minority problem, where the subspaces of minorclasses are squeezed by major ones. Inspired to enlarge the minorsubspaces, we propose GraphSHA to synthesize harder minor sam-ples with semiMixup module to avoid invading the subspaces ofneighbor classes. GraphSHA demonstrates superior performanceon both manually and naturally imbalanced datasets comparedagainst ten baselines with three GNN backbones. Furthermore, in-depth investigations also show the effectiveness of leveraging hardsamples and the semiMixup module to enlarge minor class bound-aries. For future work, we expect GraphSHA to be generalized toother quantity-imbalanced scenarios on graphs like heterogeneousinformation network which has imbalanced node types. The authors would like to thank Zhilin Zhao from University ofTechnology Sydney, Kunyu Lin from Sun Yat-sen University, andDazhong Shen from University of Science and Technology of Chinafor their insightful discussions. This work was supported by NSFC(62276277), Guangdong Basic Applied Basic Research Foundation(2022B1515120059), and the Foshan HKUST Projects (FSUST21-FYTRI01A, FSUST21-FYTRI02A). Chang-Dong Wang and Hui Xiongare the corresponding authors.",
  "Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O. Hall, and W. Philip Kegelmeyer.2002. SMOTE: Synthetic Minority Over-sampling Technique. J. Artif. Intell. Res.16 (2002), 321357": "Deli Chen, Yankai Lin, Guangxiang Zhao, Xuancheng Ren, Peng Li, Jie Zhou,and Xu Sun. 2021. Topology-Imbalance Learning for Semi-Supervised NodeClassification. In Advances in Neural Information Processing Systems. Jie Chen, Tengfei Ma, and Cao Xiao. 2018. FastGCN: Fast Learning with GraphConvolutional Networks via Importance Sampling. In 6th International Conferenceon Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3,2018, Conference Track Proceedings. OpenReview.net. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. 2020.A Simple Framework for Contrastive Learning of Visual Representations. InProceedings of the 37th International Conference on Machine Learning, ICML 2020,13-18 July 2020, Virtual Event (Proceedings of Machine Learning Research, Vol. 119).PMLR, 15971607. Zhixuan Chu, Stephen L. Rathbun, and Sheng Li. 2021. Graph Infomax AdversarialLearning for Treatment Effect Estimation with Networked Observational Data.In KDD 21: The 27th ACM SIGKDD Conference on Knowledge Discovery and DataMining, Virtual Event, Singapore, August 14-18, 2021. ACM, 176184. Ching-Yao Chuang, Joshua Robinson, Yen-Chen Lin, Antonio Torralba, and Ste-fanie Jegelka. 2020. Debiased Contrastive Learning. In Advances in Neural Infor-mation Processing Systems 33: Annual Conference on Neural Information Process-ing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, Hugo Larochelle,MarcAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin(Eds.).",
  "Diane J Cook and Lawrence B Holder. 2006. Mining graph data. John Wiley &Sons": "Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge J. Belongie. 2019. Class-Balanced Loss Based on Effective Number of Samples. In IEEE Conference onComputer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June16-20, 2019. Computer Vision Foundation / IEEE, 92689277. Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge J. Belongie. 2019. Class-Balanced Loss Based on Effective Number of Samples. In IEEE Conference onComputer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June16-20, 2019. Computer Vision Foundation / IEEE, 92689277.",
  "Matthias Fey and Jan E. Lenssen. 2019. Fast Graph Representation Learning withPyTorch Geometric. In ICLR Workshop on Representation Learning on Graphs andManifolds": "Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. 2014. GenerativeAdversarial Networks. arXiv preprint arXiv:1406.2661 (2014). Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. 2017. On Calibrationof Modern Neural Networks. In Proceedings of the 34th International Confer-ence on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017(Proceedings of Machine Learning Research, Vol. 70). PMLR, 13211330. William L. Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive Represen-tation Learning on Large Graphs. In Advances in Neural Information ProcessingSystems 30: Annual Conference on Neural Information Processing Systems 2017,December 4-9, 2017, Long Beach, CA, USA. 10241034.",
  "Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. 2015. Distilling the Knowl-edge in a Neural Network. arXiv preprint arXiv:1503.02531 abs/1503.02531 (2015)": "Youngkyu Hong, Seungju Han, Kwanghee Choi, Seokjun Seo, Beomsu Kim,and Buru Chang. 2021. Disentangling Label Distribution for Long-Tailed VisualRecognition. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR2021, virtual, June 19-25, 2021. Computer Vision Foundation / IEEE, 66266636. Zhenyu Hou, Xiao Liu, Yukuo Cen, Yuxiao Dong, Hongxia Yang, Chunjie Wang,and Jie Tang. 2022. GraphMAE: Self-Supervised Masked Graph Autoencoders. InKDD 22: The 28th ACM SIGKDD Conference on Knowledge Discovery and DataMining, Washington, DC, USA, August 14 - 18, 2022. ACM, 594604. Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, BowenLiu, Michele Catasta, and Jure Leskovec. 2020. Open Graph Benchmark: Datasetsfor Machine Learning on Graphs. In Advances in Neural Information ProcessingSystems 33: Annual Conference on Neural Information Processing Systems 2020,NeurIPS 2020, December 6-12, 2020, virtual.",
  "Justin M. Johnson and Taghi M. Khoshgoftaar. 2019. Survey on deep learningwith class imbalance. J. Big Data 6 (2019), 27": "Yannis Kalantidis, Mert Blent Sariyildiz, No Pion, Philippe Weinzaepfel, andDiane Larlus. 2020. Hard Negative Mixing for Contrastive Learning. In Ad-vances in Neural Information Processing Systems 33: Annual Conference on NeuralInformation Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, JiashiFeng, and Yannis Kalantidis. 2020. Decoupling Representation and Classifier forLong-Tailed Recognition. In 8th International Conference on Learning Representa-tions, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification withGraph Convolutional Networks. In 5th International Conference on LearningRepresentations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference TrackProceedings. OpenReview.net. Johannes Klicpera, Stefan Weienberger, and Stephan Gnnemann. 2019. Dif-fusion Improves Graph Learning. In Advances in Neural Information ProcessingSystems 32: Annual Conference on Neural Information Processing Systems 2019,NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada. 1333313345. Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He, and Piotr Dollr. 2017.Focal Loss for Dense Object Detection. In IEEE International Conference on Com-puter Vision, ICCV 2017, Venice, Italy, October 22-29, 2017. IEEE Computer Society,29993007. Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott E. Reed,Cheng-Yang Fu, and Alexander C. Berg. 2016. SSD: Single Shot MultiBox Detector.In Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, TheNetherlands, October 11-14, 2016, Proceedings, Part I (Lecture Notes in ComputerScience, Vol. 9905). Springer, 2137. Zemin Liu, Trung-Kien Nguyen, and Yuan Fang. 2021. Tail-GNN: Tail-Node GraphNeural Networks. In KDD 21: The 27th ACM SIGKDD Conference on KnowledgeDiscovery and Data Mining, Virtual Event, Singapore, August 14-18, 2021. ACM,11091119.",
  "Joonhyung Park, Jaeyun Song, and Eunho Yang. 2022. GraphENS: Neighbor-Aware Ego Network Synthesis for Class-Imbalanced Node Classification. InInternational Conference on Learning Representations": "Liang Qu, Huaisheng Zhu, Ruiqi Zheng, Yuhui Shi, and Hongzhi Yin. 2021. Im-GAGN: Imbalanced Network Embedding via Generative Adversarial Graph Net-works. In KDD 21: The 27th ACM SIGKDD Conference on Knowledge Discoveryand Data Mining, Virtual Event, Singapore, August 14-18, 2021. ACM, 13901398. Joshua David Robinson, Ching-Yao Chuang, Suvrit Sra, and Stefanie Jegelka.2021. Contrastive Learning with Hard Negative Samples. In 9th InternationalConference on Learning Representations, ICLR 2021, Virtual Event, Austria, May3-7, 2021. OpenReview.net. Florian Schroff, Dmitry Kalenichenko, and James Philbin. 2015. FaceNet: Aunified embedding for face recognition and clustering. In IEEE Conference onComputer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12,2015. IEEE Computer Society, 815823.",
  "Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and StephanGnnemann. 2018. Pitfalls of Graph Neural Network Evaluation. In RelationalRepresentation Learning Workshop@NeurIPS": "Min Shi, Yufei Tang, Xingquan Zhu, David A. Wilson, and Jianxun Liu. 2020.Multi-Class Imbalanced Graph Convolutional Network Learning. In Proceedingsof the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI2020. ijcai.org, 28792885. Abhinav Shrivastava, Abhinav Gupta, and Ross B. Girshick. 2016. TrainingRegion-Based Object Detectors with Online Hard Example Mining. In 2016 IEEEConference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV,USA, June 27-30, 2016. IEEE Computer Society, 761769. Jaeyun Song, Joonhyung Park, and Eunho Yang. 2022. TAM: Topology-AwareMargin Loss for Class-Imbalanced Node Classification. In International Conferenceon Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA(Proceedings of Machine Learning Research, Vol. 162). PMLR, 2036920383. Kaihua Tang, Jianqiang Huang, and Hanwang Zhang. 2020. Long-Tailed Classifi-cation by Keeping the Good and Removing the Bad Momentum Causal Effect.In Advances in Neural Information Processing Systems 33: Annual Conference onNeural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,virtual.",
  "Kuansan Wang, Zhihong Shen, Chiyuan Huang, Chieh-Han Wu, Yuxiao Dong,and Anshul Kanakia. 2020. Microsoft Academic Graph: When experts are notenough. Quant. Sci. Stud. 1, 1 (2020), 396413": "Xiao Wang, Hongrui Liu, Chuan Shi, and Cheng Yang. 2021. Be Confident!Towards Trustworthy Graph Neural Networks via Confidence Calibration. InAdvances in Neural Information Processing Systems 34: Annual Conference onNeural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021,virtual. 2376823779. Felix Wu, Amauri H. Souza Jr., Tianyi Zhang, Christopher Fifty, Tao Yu, andKilian Q. Weinberger. 2019. Simplifying Graph Convolutional Networks. InProceedings of the 36th International Conference on Machine Learning, ICML 2019,9-15 June 2019, Long Beach, California, USA (Proceedings of Machine LearningResearch, Vol. 97). PMLR, 68616871. Mike Wu, Milan Mosse, Chengxu Zhuang, Daniel Yamins, and Noah D. Goodman.2021. Conditional Negative Sampling for Contrastive Learning of Visual Rep-resentations. In 9th International Conference on Learning Representations, ICLR2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. Jun Xia, Lirong Wu, Ge Wang, Jintao Chen, and Stan Z. Li. 2022. ProGCL:Rethinking Hard Negative Mining in Graph Contrastive Learning. In InternationalConference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland,USA (Proceedings of Machine Learning Research, Vol. 162). PMLR, 2433224346.",
  "Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net": "Hongyi Zhang, Moustapha Ciss, Yann N. Dauphin, and David Lopez-Paz. 2018.mixup: Beyond Empirical Risk Minimization. In 6th International Conference onLearning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3,2018, Conference Track Proceedings. OpenReview.net. Shaofeng Zhang, Meng Liu, Junchi Yan, Hengrui Zhang, Lingxiao Huang, Xi-aokang Yang, and Pinyan Lu. 2022. M-Mix: Generating Hard Negatives via Multi-sample Mixing for Contrastive Learning. In KDD 22: The 28th ACM SIGKDDConference on Knowledge Discovery and Data Mining, Washington, DC, USA, Au-gust 14 - 18, 2022. ACM, 24612470. Yongshun Zhang, Xiu-Shen Wei, Boyan Zhou, and Jianxin Wu. 2021. Bag of Tricksfor Long-Tailed Visual Recognition with Deep Convolutional Neural Networks.In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-ThirdConference on Innovative Applications of Artificial Intelligence, IAAI 2021, TheEleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021,Virtual Event, February 2-9, 2021. AAAI Press, 34473455. Tianxiang Zhao, Xiang Zhang, and Suhang Wang. 2021. GraphSMOTE: Imbal-anced Node Classification on Graphs with Graph Neural Networks. In WSDM21, The Fourteenth ACM International Conference on Web Search and Data Mining,Virtual Event, Israel, March 8-12, 2021. ACM, 833841. Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and DanaiKoutra. 2020. Beyond Homophily in Graph Neural Networks: Current Limitationsand Effective Designs. In Advances in Neural Information Processing Systems 33:Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,December 6-12, 2020, virtual.",
  "A.1Datasets": "We adopt six widely-used datasets in the community, includingCora, CiteSeer, PubMed, Amazon-Photo, Amazon-Computers, andCoauthor-CS to conduct all the experiments throughout the pa-per. These datasets are collected from real-world scenarios of cita-tion networks and co-purchase networks. Please note that all thedatasets are accessible via PyTorch Geometric library . Cora, CiteSeer, and PubMed are three citation networkswhere nodes represent papers and edges represent citation rela-tions. Each node in Cora and CiteSeer is described by a 0/1-valuedword vector indicating the absence/presence of the correspond-ing word from the dictionary, while each node in PubMed isdescribed by a TF/IDF weighted word vector from the dictio-nary. The nodes are categorized by their related research areafor the three datasets. These datasets are accessible via Amazon-Photo and Amazon-Computers are two co-purchasenetworks constructed from Amazon where nodes represent prod-ucts and edges represent co-purchase relations. Each node is de-scribed by a raw bag-of-words feature encoding product reviewsand is labeled with its category. These datasets are accessiblevia Coauthor-CS is an academic network where nodes representauthors and edges represent co-author relations. Each node isdescribed by a raw bag-of-words feature encoding keywords ofhis/her publication and is labeled with the most related researchfield. The dataset is accessible via ogbn-arXiv is a citation network between all ComputerScience arXiv papers indexed by Microsoft academic graph ,where nodes represent papers and edges represent citation rela-tions. Each node is described by a 128-dimensional feature vectorobtained by averaging the skip-gram word embeddings in its titleand abstract. The nodes are categorized by their related researcharea. According to the benchmark, the data split is based on thepublication dates of the papers where the training set is paperspublished until 2017, the validation set is papers published in 2018,and the test set is papers published since 2019. The dataset is ac-cessible via #ogbn-arxiv.",
  "A.2Implementation Details for Baselines": "We give detailed configurations for baseline methods. Please notewe adopt same 2-layer GNN (GCN , GAT , GraphSAGE )with hidden dimension 64 as the encoder for all the methods (exceptfor arXiv where we set the hidden dimension as 256 to make a faircomparision on the leaderboard), including proposed GraphSHA,for a fair comparison.For Reweight, the loss weight for each class is proportional to thenumber of samples. For CB Loss, hyper-parameter is set to 0.999. For Focal, hyper-parameter is set to 2.0. For ReNode, we combinethe reweighting method for topological imbalance with Focal loss,and the lower and upper bounds for the topological imbalance areset to 0.5 and 1.5, respectively. For Upsample, we duplicate minornodes along with their edges until the number of each class samplereaches the mean number of class samples. For GraphSmote, wechoose the GraphSmote variant, which predicts discrete valueswithout pretraining, as it shows superior performance among mul-tiple versions. For GraphENS, we set the feature masking rate as0.01 and temperature as 1, as suggested in the released codes. ForTAM, we choose the GraphENS-based version as it performs thebest according to the paper, where the coefficient for ACM , thecoefficient for ADM , and the coefficient for classwise temperature are set to 2.5, 0.5, 1.2 respectively, which are the default settingsin the released codes.",
  "BEXTRA EXPERIMENTS": "We first present an extra experiment on CoraFull which hasa larger class set (70 classes). As the dataset follows a long-taileddistribution intrinsically, we sample the same number of nodesfrom each class for validation/test sets randomly and assign theremaining nodes as the training set. The imbalance ratio is set to100 and GraphSAGE is adopted as the backbone encoder. As the testset is balanced, the Acc. and bAcc. metrics are the same. The resultis shown in , from which We can see that the superiorityof GraphSHA is still significant, which shows the generalizationand effectiveness of GraphSHA in handling class-imbalanced nodeclassification tasks.We also present experiments in the step class imbalance settingwith GCN and GAT as GNN backbones. The result is shown in, from which we can see that GraphSHA still achieves thebest performance overall.In addation to confidence-based node hardness discussed in Sec-tion 4.1, we also consider NN-based hardness, which is defined asH = |{ | RF (), () ()}|/ where RF () is theNN receptive field for node in the latent space. For sampling and near the minor decision boundary, we can get"
}