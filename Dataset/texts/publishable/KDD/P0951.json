{
  "Abstract": "In this paper, we introduce the second-place solution in the KDD-2024 OAG-Challenge paper source tracing track. Our solution ismainly based on two methods, BERT and GCN, and combines thereasoning results of BERT and GCN in the final submission toachieve complementary performance. In the BERT solution, wefocus on processing the fragments that appear in the references ofthe paper, and use a variety of operations to reduce the redundantinterference in the fragments, so that the information received byBERT is more refined. In the GCN solution, we map informationsuch as paper fragments, abstracts, and titles to a high-dimensionalsemantic space through an embedding model, and try to build edgesbetween titles, abstracts, and fragments to integrate contextualrelationships for judgment. In the end, our solution achieved aremarkable score of 0.47691 in the competition.",
  "Introduction": "With the rapid development of science and technology, the num-ber of papers has exploded. For academic papers in various fields,it is very difficult to find the technical development process froma large amount of literature. The source paper tracing task aimsto find important references in the paper, that is, whether theyhave made inspiring contributions to the ideas or main methods ofthe paper. At present, the solutions to this task are mainly dividedinto three categories: large models, graph convolutional neural net-works (GCNs), and machine learning. Among them, thanks to thestronger robustness and generalization of large models , andthe powerful relationship modeling and feature aggregation capabil-ities of graph convolutional neural networks , their excellentperformance has attracted more attention from researchers. Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from , July 2017, Washington, DC, USA 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-x-xxxx-xxxx-x/YY/MM Under this background, the KDD-2024 OAG-Challenge PST pa-per source tracing task requires contestants to design an efficientand accurate algorithm to identify the source paper . Herewe will introduce our second place solution.Our main contributions are as follows: For extremely redundant XML format context data, we de-signed data cleaning methods such as removing XML char-acters with unclear semantic information, embedding textcontent prompts, and setting dynamic context acquisitionlength, so that the context used to train BERT-like modelwill contain richer classification semantic information. In terms of GCN, we constructed a relationship networkbetween reference title information, paper title information,paper abstract information, and paper context information,and performed node classification with the help of GCNspowerful relationship modeling capabilities. Finally, we integrated the reasoning results of BERT and GCN toachieve complementary performance. We believe that the combina-tion of BERTs powerful context understanding ability and GCNsability to integrate full-text structural information has greatly im-proved classification accuracy.",
  "Related Work": "Understanding scientific trends and the flow of ideas is criticalto both policy development by funding agencies and knowledgediscovery by researchers. Citation relationship analysis revealsthe trajectory of scientific evolution, but there is a significant gapbetween large-scale semantically rich citation relationships and thebackbone structure of scientific evolution. To reveal the contextin which science develops, we need to simplify citation graphs totrack the sources of publications and reveal heuristic relationshipsbetween papers.Tang et al . studied citation semantics, defined three cate-gories of citation links, and constructed a computer science datasetcontaining approximately 1000 citation pairs. Valenzuela et al .present a new dataset of 450 citation pairs and distinguish betweenincidental and important citations. Jurgens et al . proposed alarger dataset in the NLP field that contained nearly 2000 citationpairs, but less than 100 citation pairs were annotated as motivations.Pride and Knoth believe that abstract similarity is one of themost indicative features for predicting citation importance. Hassanet al . used random forest to evaluate the importance of citationsand combined context-based and clue word-based features. He et",
  "Conference17, July 2017, Washington, DC, USAShupeng Zhong, Xinger Li, Shushan Jin, and Yang Yang": "al . adapted the LDA model to the citation network and devel-oped a new inherited topic model to describe topic evolution. Withthe rapid development of large models, imageand text features are constantly being learned by large models. Frber et al . proposed an approach basedon convolutional recurrent neural networks to classify potentialcitation contexts. Jiang and Chen proposed a SciBERT-based contex-tual representation model to classify citation intent and achievedover 90% AUC on some datasets. Yin et al . designed MRT, anunsupervised framework to identify important previous work orevolutionary trajectories by building an annotated evolutionaryroadmap. The framework utilizes generative expression embeddingand clustering methods to group publications to capture potentialrelationships between publications.",
  "Method": "The training dataset released for this task is mainly given inthe form of XML papers. According to our observation, XML datacontains a lot of redundant information, so extra attention needs tobe paid to data cleaning. Inspired by the , this article divides thepaper tracing task into two aspects: BERT-based text classificationtask and GCN-based node classification task .The BERT-like text classification task performs text classifica-tion by understanding the context in which the reference appearsbased on text cleaning. The GCN node classification task focuseson understanding the relationship between the reference title andthe paper title based on the content of the reference fragment andthe paper abstract.",
  "Text sequence classification task based onBERT-like model": "The text directly regards the paper tracing task as a text sequencebinary classification task, that is, the probability that a certain paperis the source paper. This article mainly cleans the XML formattraining data and does not modify the model module. The officialJson file for training contains 788 papers including source papersand reference papers. We only use 2/3 of the papers for trainingand the remaining 1/3 for verification.",
  "GCN-based node classification task": "Graph Convolutional Network (GCN) is a neural network modelfor processing graph structured data. GCN extracts features onthe graph through convolution operations, which can effectivelycapture the relationship between nodes and their neighbors. Un-like traditional convolutional neural networks, GCN is speciallydesigned for graph data and can process non-Euclidean structureddata, such as social networks, knowledge graphs, and citation net-works.The core idea of GCN is to aggregate the features of each nodewith the features of its neighboring nodes through convolutionoperations, thereby updating the representation of the node. Theforward propagation formula of GCN is as follows:",
  "where:": "() denotes the node feature matrix at the -th layer. is the adjacency matrix of the graph. is the degree matrix of the adjacency matrix. () is the weight matrix at the -th layer. is an activation function (such as ReLU). Drawing on the research of Yao et al . and Wu et al . onGCN text classification and combining GCNs powerful relationshipmodeling and feature aggregation capabilities with its excellentefficiency, we used GCN to classify text in the competition. shows how we use GCN to build graphs for papers. Weuse the Spacy library to semantically segment each paragraph of thepaper, then build a unidirectional edge between the paper abstractand the paper title, a bidirectional edge between the referencetitle and the paper title, and a unidirectional edge between thereference and the text block where the reference appears. The nodeinformation is a high-dimensional vector of the text processed bythe embedding model.",
  "BERT-like models": "Here we present our experimental results on BERT-like mod-els.We mainly tried two models: SCIBERT and his variantsCS_ROBERTA_BASE Dataset The data is provided by the official. The data includestraining Json files and a large number of XML format papers. TheJson files contain the basic information of 788 papers such as ID,title, references, and source papers; the XML format papers areconverted from the PDF files of the original papers using Grobid.It is worth noting that although the official allows the use of OAGand DBLP Citation data, this article does not use them.Details Considering that the task background is academic papersin various fields, we tried to use models such as SCIBERT andCS_ROBERTA_BASE that have been trained on a large amountof scientific text. Eventually, we found that CS_ROBERTA_BASEachieved the best results on the locally partitioned Train, val, officialVal, and Test. We only used one NVIDIA RTX 4090 to train theCS_ROBERTA_BASE model, setting the initial learning to 5e-6,batch_size=24, epoch=10 and implemented early stopping after 5epochs when performance did not continue to improve. Finally, weselected the model with the highest local AP value as the optimalmodel.Results shows the performance of our method on theofficial final test set. We add the method step by step according tothe order of the labels in the table. The experimental results fullyconfirm the effectiveness of the method. The content cleaning ofthe XML format context removes a lot of redundant information.The embedding of the paper title and the corresponding promptwords makes the model pay more attention to and understand thecorresponding content. At the same time, the CS_ROBERTA_BASEmodel has larger parameters and stronger learning ability.",
  "GCN": "Here we mainly introduce our experimental results on GCN. Wetried GCN models with various architectures and different textembedding models.Details We tried GCN models with different architectures, dif-ferent numbers of layers, and different numbers of hidden units.We also tried GAT and RGCN based on heterogeneous relationalgraphs. In terms of experimental results, they seem to be not muchdifferent. We first divide the paper into semantic-based chunks witha chunk size of 300 characters. Then these text chunks are mappedto a high-dimensional semantic space as the node information ofthe GNN through an embedding model. We train our model on anRTX4090. epoch=100.Results The experimental results are shown in . Wefound that the main factors affecting the effect of GCN are the wayof graph construction and the choice of embedding model. Themapping dimension of the embedding model should not be greaterthan 1024, and the graph construction should not be too complex.The number of GCN layers should not be too many.In addition to the information shown in . We also madeother attempts on the size of the text chunk and the way of graphconstruction. The results show that the size of the text chunk is bestbetween 200 and 300. We also tried to decompose the introductioninto more chunks and build edges with more references, but theeffect showed a downward trend.",
  "Ensemble": "At the end, we integrated the results from different solutions.Compared with the results of the original single solution, the re-sults were significantly improved, which reflects the advantages ofensemble learning. The integration results are shown in .In the final stage, our solution mainly uses the integration ofthree solutions, including ROBERTA text classification based on se-mantic segmentation (Roberta-spacy), ROBERTA text classification",
  "Other attempts": "In addition to the above two solutions based on BERT and GCN,we also tried a solution based on a large language model inthe validation set stage. The effect of this solution was reducedin the test set stage, but due to the lack of hardware and timeresources, we did not continue to try the large model in the test setstage. In the validation set stage, we input reference fragments andpaper abstracts into the large language model, and by modifyingthe prompts, the large model can break through the original textlimitations and output more in-depth text, and then input it intothe BERT-like model for training. As shown in .The results of the large model processing are shown in .",
  "Kunze Wang, Yihao Ding, and Soyeon Caren Han. 2023. Graph sample-sigconf-authordraftneural networks for text classification: a survey. (2023)": "Yang Yang, Zhao-Yang Fu, De-Chuan Zhan, Zhi-Bin Liu, and Yuan Jiang. 2019.Semi-supervised multi-modal multi-instance multi-label deep network withoptimal transport. IEEE Transactions on Knowledge and Data Engineering, 33, 2,696709. Yang Yang, Zhen-Qiang Sun, Hengshu Zhu, Yanjie Fu, Yuanchun Zhou, HuiXiong, and Jian Yang. 2021. Learning adaptive embedding considering incre-mental class. IEEE Transactions on Knowledge and Data Engineering, 35, 3, 27362749. Yang Yang, Hongchen Wei, Zhen-Qiang Sun, Guang-Yu Li, Yuanchun Zhou,Hui Xiong, and Jian Yang. 2021. S2osc: a holistic semi-supervised approach foropen set classification. ACM Transactions on Knowledge Discovery from Data(TKDD), 16, 2, 127. Yang Yang, Yi-Feng Wu, De-Chuan Zhan, Zhi-Bin Liu, and Yuan Jiang. 2018.Complex object classification: a multi-modal multi-instance multi-label deepnetwork with optimal transport. In Proceedings of the 24th ACM SIGKDD Inter-national Conference on Knowledge Discovery & Data Mining, 25942603. Yang Yang, Yi-Feng Wu, De-Chuan Zhan, Zhi-Bin Liu, and Yuan Jiang. 2019.Deep robust unsupervised multi-modal network. In Proceedings of the AAAIConference on Artificial Intelligence number 01. Vol. 33, 56525659. Yang Yang, Jia-Qi Yang, Ran Bao, De-Chuan Zhan, Hengshu Zhu, Xiao-Ru Gao,Hui Xiong, and Jian Yang. 2021. Corporate relative valuation using heteroge-neous multi-modal graph neural network. IEEE Transactions on Knowledge andData Engineering, 35, 1, 211224. Yang Yang, De-Chuan Zhan, Yi-Feng Wu, and Yuan Jiang. 2018. Multi-networkuser identification via graph-aware embedding. In Advances in Knowledge Dis-covery and Data Mining: 22nd Pacific-Asia Conference, PAKDD 2018, Melbourne,VIC, Australia, June 3-6, 2018, Proceedings, Part II 22. Springer, 209221. Yang Yang, De-Chuan Zhan, Yi-Feng Wu, Zhi-Bin Liu, Hui Xiong, and YuanJiang. 2019. Semi-supervised multi-modal clustering and classification withincomplete modalities. IEEE Transactions on Knowledge and Data Engineering,33, 2, 682695. Yang Yang, Chubing Zhang, Xin Song, Zheng Dong, Hengshu Zhu, and WenjieLi. 2023. Contextualized knowledge graph embedding for explainable talenttraining course recommendation. ACM Transactions on Information Systems,42, 2, 127. Yang Yang, Da-Wei Zhou, De-Chuan Zhan, Hui Xiong, Yuan Jiang, and JianYang. 2021. Cost-effective incremental deep model: matching model capacitywith the least sampling. IEEE Transactions on Knowledge and Data Engineering,35, 4, 35753588."
}