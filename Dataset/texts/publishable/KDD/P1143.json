{
  "ABSTRACT": "As location-based services (LBS) have grown in popularity, morehuman mobility data has been collected. The collected data can beused to build machine learning (ML) models for LBS to enhancetheir performance and improve overall experience for users. How-ever, the convenience comes with the risk of privacy leakage sincethis type of data might contain sensitive information related touser identities, such as home/work locations. Prior work focuseson protecting mobility data privacy during transmission or prior torelease, lacking the privacy risk evaluation of mobility data-basedML models. To better understand and quantify the privacy leakagein mobility data-based ML models, we design a privacy attack suitecontaining data extraction and membership inference attacks tai-lored for point-of-interest (POI) recommendation models, one of themost widely used mobility data-based ML models. These attacks inour attack suite assume different adversary knowledge and aim toextract different types of sensitive information from mobility data,providing a holistic privacy risk assessment for POI recommen-dation models. Our experimental evaluation using two real-worldmobility datasets demonstrates that current POI recommendationmodels are vulnerable to our attacks. We also present unique find-ings to understand what types of mobility data are more susceptibleto privacy attacks. Finally, we evaluate defenses against these at-tacks and highlight future directions and challenges. Our attacksuite is released at",
  "Correspondence to: Kunlin Cai, Yuan Tian, and Jianfeng Chi. Work unrelated to Meta": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for third-party components of this work must be honored.For all other uses, contact the owner/author(s).KDD 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s).ACM ISBN 979-8-4007-0490-1/24/08",
  "POI recommendation; privacy-preserving machine learning; dataextraction; membership inference": "ACM Reference Format:Kunlin Cai, Jinghuai Zhang, Zhiqing Hong, William Shand, Guang Wang,Desheng Zhang, Jianfeng Chi, and Yuan Tian. 2024. Where Have You Been?A Study of Privacy Risk for Point-of-Interest Recommendation. In Proceed-ings of the 30th ACM SIGKDD Conference on Knowledge Discovery and DataMining (KDD 24), August 2529, 2024, Barcelona, Spain. ACM, New York,NY, USA, 18 pages.",
  "INTRODUCTION": "With the development and wide usage of mobile and wearable de-vices, large volumes of human mobility data are collected to supportlocation-based services (LBS), such as traffic management ,store location selection , and point-of-interest (POI) recom-mendation . In particular, POI recommendation involvesrelevant POI suggestions to users for future visits based on per-sonal preferences using ML techniques , which has recentlygained much research attention1. POI recommendation modelshave also been integrated into popular services such as Yelp andGoogle Maps to assist users in making informed decisions aboutthe next destination to visit. However, mobility data collected totrain POI recommendation models are highly sensitive as they canleak users sensitive information such as their social relationships,trip purposes, and identities .Although there are a significant number of studies on mobility data privacy, the existing research primarily focuseson analyzing attacks and evaluations within the context of mobil-ity data transmission and release processes. For example, previousstudies have demonstrated the linkages of mobility data from vari-ous side channels, including social networks , open-sourcedatasets , and network packets . The linkages be-tween these side channels can lead to the identification of individ-uals. As a result, efforts to protect mobility data have primarilyconcentrated on data aggregations and releases . Thesestudies neglect the risk of adversaries extracting sensitive attributes",
  "KDD 24, August 2529, 2024, Barcelona, SpainCai, et al": ": Our attack suite highlights the privacy concerns inPOI recommendation models. In particular, we demonstratethat an adversary can extract or infer membership informa-tion of locations or trajectories in the training dataset. or properties from the ML models (e.g., POI recommendation mod-els) that use mobility data for training, which are inherently sus-ceptible to privacy attacks .Evaluating privacy risks in POI recommendation models remainschallenging because existing attack and defense mechanisms areineffective due to the unique features of mobility data. Previousprivacy attacks have mainly focused on ML models trained withimage and text data , where each data point can uniquelyidentify itself. However, mobility data, such as locations, are lesssemantically unique without the context. Moreover, mobility datais special in that it contains multimodal spatial and temporal infor-mation, which describes each individuals movements and behaviorpatterns over time. All existing attacks fail to construct meaning-ful context and leverage spatial-temporal information, resulting intheir failures when applied to POI recommendations. Furthermore,existing defense mechanisms have mainly been testedon classification models trained with image or text data. Given thetask and data are significantly different, the effectiveness of defensemechanisms is unknown when applied to POI recommendation.In this paper, we design a comprehensive privacy attack suite tostudy the privacy leakage in POI recommendation models trainedwith mobility data. Specifically, our privacy attack suite containsthe two most popular kinds of privacy attacks on machine learningmodels, data extraction and membership inference attacks, to assessthe privacy vulnerabilities of POI recommendation models at bothlocation and trajectory levels. In contrast to privacy attacks forimage and text data, the attacks in our attack suite are tailoredfor mobility data and aim to extract different types of sensitiveinformation based on practical adversary knowledge.We perform experiments on three representative POI recommen-dation models trained on two mobility datasets. We demonstratethat POI recommendation models are vulnerable to our designeddata extraction and membership inference attacks. We further pro-vide an in-depth analysis to understand what factors affect theattack performance and contribute to the effectiveness of the at-tacks. Based on our analysis, we discover that the effect of dataoutliers exists in privacy attacks against POI recommendations,making training examples with certain types of users, locations,and trajectories particularly vulnerable to the attacks in the attacksuite. Finally, We test several existing defenses and find that theydo not effectively thwart our attacks with negligible utility loss,which calls for better methods to defend against our attacks.",
  "Contributions:": "We introduce a novel privacy attack suite that incorporates uniquecharacteristics of mobility data (e.g., spatial-temporal informa-tion) into the attack design. In particular, we target a previouslyunder-defended attack surface: neural-network-based POI rec-ommendation. To the best of our knowledge, our work is the firstto comprehensively evaluate the privacy risks in POI recommen-dation models using inference attacks from both location andtrajectory levels. We conduct extensive experiments on state-of-the-art POI rec-ommendation models and datasets to demonstrate that POI rec-ommendation models are vulnerable to data extraction and mem-bership inference attacks in our attack suite. We provide an in-depth analysis to understand what uniquefactors in mobility data make them vulnerable to privacy attacks.We also explore the reason regarding how our attack designworks and test existing defenses against our attacks. Our analysisidentifies the challenges and future directions for developingprivacy-preserving POI recommendation models.",
  "BACKGROUND2.1Point-of-Interest Recommendation": "POI recommendation has recently gained much attention due toits importance in many business applications , such as userexperience personalization and resource optimization. Initially, re-searchers focused on feature engineering and algorithms such asMarkov chain , matrix factorization algorithms , andBayesian personalized ranking for POI recommendation.However, more recent studies have shifted their attention towardsemploying neural networks like RNN , LSTM , andself-attention models . Neural networks can better learnfrom spatial-temporal correlation in mobility data (e.g., check-ins)to predict users future locations and thus outperform other POI rec-ommendation algorithms by a large margin. Meanwhile, this couldintroduce potential privacy leakage. Thus, we aim to design anattack suite to measure the privacy risks of neural-network-basedPOI recommendations systematically.We first provide the basics of POI recommendations and no-tations used throughout this paper. Let U be the user space, Lbe the location space, and T be the timestamp space. A POI rec-ommendation model takes the observed trajectory of a user asinput and predicts the next POI that will be visited, which is for-mulated as : U L T R| L|. Here, the length of theinput trajectory is . We denote a user by its user ID U forsimplicity. For an input trajectory with check-ins, we denote itstrajectory sequence as 0:1= {(0,0), . . . , (1,1)}, where L and T indicate the POI location and correspondingtime interval of -th check-in. Also, the location sequence of thistrajectory is denoted as 0:1= {0, . . . ,1}. The POI recommen-dation model predicts the next location (also denoted as byconvention) by outputting the logits of all the POIs. Then, the usercan select the POI with the highest logit as its prediction , where = arg max (,0:1). Given the training set tr sampled froman underlying distribution D, the model weights are optimizedto minimize the prediction loss on the overall training data, i.e.,",
  "|tr|(,0:1,)tr ( (,0:1),), where is the cross-": "entropy loss, i.e., ( (,0:1),) = log( (,0:1)). Thegoal of the training process is to maximize the performance of themodel on the unseen test dataset D, which is drawn fromthe same distribution as the training data. During inference, thisprediction is then compared to the next real location label tocompute the prediction accuracy. The performance evaluation ofPOI recommendation models typically employs metrics such astop- accuracy (e.g., = 1, 5, 10).",
  "Threat Models": "Adversary Objectives. To understand the potential privacy leak-age of training data in POI recommendation models, we designthe following four attacks from the two most common privacyattack families: membership inference attack and data ex-traction attacks , based on the characteristics of the mobilitydata for POI recommendation, namely common location extrac-tion (LocExtract), training trajectory extraction (TrajExtract),location-level membership inference attack (LocMIA), and trajectory-level membership inference attack (TrajMIA). These four attacksaim to extract or infer different sensitive information about a userin the POI recommendation model training data.LocExtract focuses on extracting a users most frequently vis-ited location; TrajExtract extracts a users location sequence witha certain length given a starting location; LocMIA infers whether auser has been to a location and used for training; TrajMIA inferswhere a trajectory sequence has been used for training.Adversary Knowledge. For all attacks, we assume the attackerhas access to the query interface of the victim model. Specifically,the attacker can query the victim model with the target user andobtain the corresponding output logits. This assumption is realis-tic in two scenarios: (1) A malicious third-party entity is grantedaccess to the POI model query API hosted by the model owner(e.g., location service providers like Foursquare or Yelp) for spe-cific businesses such as personalized advertisement. This scenariois well-recognized by . (2) The retention period of thetraining data expires. Still, the model owner keeps the model andan adversary (e.g., a malicious insider of location service providers)can extract or infer the sensitive information using our attack suite,even if the training data have been deleted. In this scenario, themodel owner may violate privacy regulations such as GDPR .Depending on different attack objectives, the adversary alsopossesses different auxiliary knowledge. In particular, for TrajEx-tract, we assume the attacker can query the victim model witha starting location 0 that the target user visited. This assump-tion is reasonable because an attacker can use real-world obser-vation , LocExtract, and LocMIA as cornerstones. As forLocMIA and TrajMIA, we assume the attacker has access to ashadow dataset following the standard settings of membershipinference attacks .",
  "Data Extraction Attacks": "Our data extraction attacks are rooted in the idea that victim mod-els display varying levels of memorization in different subsets oftraining data. By manipulating the spatial-temporal information inthe queries, the attacker can extract users locations or trajectoriesthat these victim models predominantly memorize.LocExtract Common location extraction attack (LocExtract)aims to extract a users most frequently visited location in the victimmodel training, i.e.,",
  "LocExtract(,) 1, . . . ,": "The attack takes the victim model and the target user as theinputs and generates predictions 1, . . . , to extract themost frequently visited location of user . The attack is motivatedby our key observation: querying POI recommendation modelswith a random location reveals that these models tend to over-learn a users most frequently visited locations, making theselocations more likely to appear in the model output. For example,we randomly choose 10 users and query the victim model using100 randomly selected locations. Of these queries, 32.5% yield themost frequent location for the target user. Yet, these most commonlocations are present in only 18.7% of these users datasets.In LocExtract, we first generate a set of different random inputsfor a specific user and use them to make iterative queries to thevictim model. Each query returns the prediction logits with a lengthof |L| outputted by the victim model. The larger the logit value,the more confident the model is in predicting the correspondinglocation as the next POI. Therefore, by iterating queries to the modelgiven a target user and aggregating the logit values of all queries, themost visited location is more likely to have a large logit value afteraggregation. Here, we use a soft voting mechanism, i.e., averagingthe logit values of all queries, as the aggregation function (see alsoSec. D for the comparison with different aggregation functions).With the resulting mean logits, we output the top- locations with largest logit values as the attack results. Algorithm 1 gives theoutline of LocExtract. Though the attack is straightforward, itis effective and can be a stepping stone for TrajExtract in ourattack suite.TrajExtract Our training trajectory extraction attack(TrajExtract) aims to extract the location sequence 0:1={0, . . . ,1} in a training trajectory of user with a length of from the victim model . Formally,",
  "=0log Pr (,0:1),": "where Pr () is the likelihood of observing 0:1with user un-der the victim model . In order to get the lowest log perplexityof location sequences with a length of , we have to enumerate allpossible location sequences. However, in the context of POI recom-mendation, there are O(|L|1) possible location sequences for agiven user. |L| equals the number of unique POIs within the mobil-ity dataset and can include thousands of options. Thus, the cost ofcalculating the log perplexity of all location sequences can be veryhigh. To this end, we use beam search to extract the location se-quences with both time and space complexity O(|L|), where is the beam size. In particular, to extract a trajectory of length, we iteratively query the victim model using a set of candidatetrajectories with a size of and update the candidate trajectoriesuntil the extraction finishes. As highlighted in the prior work ,when using beam search to determine the final outcome of a se-quential neural network, there is a risk of generating non-diverseoutputs and resembling the training data sequence. However, inour scenario, this property can be leveraged as an advantage inTrajExtract, as our primary objective revolves around extractingthe training location sequence with higher confidence. As a finalremark, both LocExtract and TrajExtract need a query times-tamp to query the victim model, and we will show the effects of thetimestamp in our experiments. Algorithm 2 in Appendix A givesthe detailed steps of TrajExtract.",
  "Membership Inference Attacks": "Membership inference attack (MIA) aims to determine whether atarget data sample is used in the model training. We extend thenotion to infer whether certain sensitive information (e.g., user-location pair (,) and trajectory sequence (, )) of the usersdata is involved in the training of the victim model . Since POIrecommendation models use multi-modal sequential data as inputsand adversaries lack sufficient information to construct a completeinput, we propose attack designs to manipulate the spatial-temporalinformation in queries to enhance effectiveness of attacks. Themembership inference attack can be formulated as follow:",
  "MIA(,, ) {member, nonmember},": "where represents the target sensitive information ( =(,) in LocMIA and = (, ) in TrajMIA), and is theshadow dataset owned by the adversary.To effectively infer the membership of a given , we adaptthe state-of-the-art membership inference attack likelihood ratioattack (LiRA) to the context of POI recommendation. The keyinsight of LiRA is that the model parameters trained with differ from those trained without it, and the effect of the modelparameter on a data sample can be well approximated using a lossvalue. By conducting a hypothesis test on the distributions of theloss values, we can identify if the victim model is trained withthe or not. LiRA consists of four steps: (1) train multipleshadow models, (2) query the shadow models trained with and without to obtain two distributions, (3) query the vic-tim model to obtain the output logits, and (4) conduct a hypothesis test to infer the membership of the based on thetwo distributions and the query results. Due to the space limit, wedefer the details of LiRA to Appendix A.LocMIA In this attack, the adversary aims to determine whether agiven user has visited a location in the training data. However,it is not feasible to directly apply LiRA to LocMIA as the victimmodel takes the trajectory sequences as inputs, but the adversaryonly has a target location without the needed sequential context.In particular, LocMIA needs the auxiliary inputs to calculate themembership confidence score since this process cannot be com-pleted only using = (,). This attack is a stark contrast toMIA for image/text classification tasks where the itself issufficient to compute the membership confidence score.To this end, we design a spatial-temporal model query algorithm(Algorithm 3 in Appendix A) to tailor LiRA to LocMIA and optimizemembership confidence score calculation. The idea behind the algo-rithm is that if a particular user has been to a certain POI location,the model might unintentionally memorize its neighboring POIlocations and the corresponding timestamp in the training data.Motivated by this, each time we query the models (e.g., the vic-tim and shadow models), we generate random locations and fixed-interval timestamps. To obtain stable and precise membershipconfidence scores, we first average the corresponding confidencescores at the target location by querying with locations at thesame timestamp. While the adversary does not possess the groundtruth timestamp linked with the target POI for queries, the adver-sary aims to mimic a query close to the real training data. To achievethis, we repeat the same procedure of querying different locationsfor timestamps and take the maximum confidence scores amongthe averaged confidence scores as the final membership infer-ence score for the target example. Algorithm 4 gives the outlineof LiRA in terms of LocMIA, and the lines marked with red arespecific to LocMIA.TrajMIA The attack aims to determine whether a trajectory isused in the training data of the victim model. Unlike LocMIA, = (, ) suffices to calculate the membership confidencescore in LiRA, and we do not need any auxiliary inputs. To fullyleverage information of the target example querying the victimmodel and improve the attack performance, we also utilize the 2intermediate outputs and the final output from the sequence with a length of to compute the membership confidence score, i.e.,we take the average of all 1 outputs. This change improves theattack performance as the intermediate outputs provide additionalmembership information for each point in the target trajectory. Thepurple lines in Algorithm 4 highlight steps specific to TrajMIA.",
  "Practical Implications of the Attack Suite": "Our attack suite is designed as an integrated framework focusingon the basic units of mobility data locations and trajectories. Itcontains two prevalent types of privacy attacks: data extractionand membership inference attacks. Each attack in our attack suitetargets a specific type of mobility data and could serve as a privacyauditing tool . They can also be used to infer additional sensitiveinformation in mobility data:",
  "EXPERIMENTS": "We empirically evaluate the proposed attack suite to answer thefollowing research questions: (1) Whats the performance of theproposed attacks in extracting or inferring the sensitive informationfrom POI recommendation models (Sec. 4.2.1)? (2) What uniquefactors (e.g., user, location, trajectory) in mobility data correlatewith the attack performance (Sec. 4.2.2)? (3) How do different attackdesigns (e.g., spatial-temporal querying for membership inferenceattacks) improve the attack performance (Sec. 4.2.3)?",
  "Experimental Setup": "We briefly describe the datasets, models, and evaluation metricsused in our experiments. Due to the space limit, we defer the detailsof datasets (e.g., statistics of each dataset), data pre-processingpipeline, default training and attack parameters to Appendix A.Datasets Following the literature , we comprehensivelyevaluate four privacy attacks on two POI recommendation bench-marks: FourSquare (4sq) and Gowalla .Models We experiment with three representative POI recom-mendation models, including GETNext , LSTPM , andRNN . Note that GETNext and LSTPM are the state-of-the-art POI recommendation methods based on the transformer andhierarchical LSTM, respectively. We also include RNN since it is acommonly used baseline for POI recommendation.Evaluation Metrics We use the top- extraction attack suc-cess rate (ASR) to evaluate the effectiveness of data extraction at-tacks. For LocExtract, the top- ASR is defined as |extracted|/|U|,where extracted is the set of users whose most visited locations arein the top- predictions outputted by our attack; For TrajExtractthe top- ASR is |correct extractions|/|all (,0) pairs|, where cor-rect extractions are (,0) pairs with top- extracted results match-ing an exact location sequence in the training data. For LocMIA and TrajMIA, we utilize the commonly employedmetrics for evaluating membership inference attacks, namely thearea under the curve (AUC), average-case accuracy (ACC), andtrue positive rate (TPR) versus false positive rate (FPR) in the low-false positive rate regime. Our primary focus is the TPR versusFPR metric in the low-false positive rate regime because evaluat-ing membership inference attacks should prioritize the worst-caseprivacy setting rather than average-case metrics, as emphasizedin .",
  "Experimental Results and Analysis": "4.2.1Attack performance (RQ1). Figures 2 and 3 visualize the at-tack performance of data extraction and membership inferenceattacks, respectively. In , we observe that LocExtract andTrajExtract can effectively extract users most common locationsand trajectories across various model architectures and datasets asthe attack performance is significantly better than the random guessbaseline, i.e., 1/|L| (0.04% for LocExtract) and 1/|L|1(108%for TrajExtract). Likewise, as shown in , LocMIA andTrajMIA successfully determine the membership of a specific user-location pair or trajectory, significantly outperforming the randomguess baseline (represented by the diagonal line in both figures).The attack performance also demonstrates that trajectory-levelattacks are significantly more challenging than location-level at-tacks, evident from the better performance of LocExtract andLocMIA compared to TrajExtract and TrajMIA for data extrac-tion and membership inference. We suspect this is because POIrecommendation models are primarily designed to predict a singlelocation. In contrast, our trajectory-level attacks aim to extract orinfer a trajectory encompassing multiple consecutive locations. Theexperiment results also align with the findings that longer trajec-tories are less vulnerable to our attacks (see Figures 17 and 19 inAppendix D).The attack performance also differs across different model ar-chitectures and datasets. We see a general trend of privacy-utilitytrade-off in POI recommendation models based on the model per-formance of the victim model in : with better victim modelperformance comes better attack performance. While this is a com-mon trend, it might not hold in some cases. For example, the MIAperformance against RNN is sometimes better than GETNext andLSTPM performances. This might be because GETNext and LSTPMimprove upon RNN by better leveraging spatial-temporal informa-tion in the mobility datasets. However, the adversary cannot usethe exact spatial-temporal information in shadow model trainingsince the adversary cannot access that information. This result canbe inspiring in that even though spatial-temporal information caneffectively improve attack performance, victim models that betterutilize spatial-temporal information are still more resilient to MIAs.Future studies should also consider this characteristic when de-signing attacks or privacy-preserving POI recommendation modelswith better privacy-utility trade-offs. 4.2.2Factors in mobility data that make it vulnerable to the attacks(RQ2). Prior research demonstrates that data outliers are the mostvulnerable examples to privacy attacks in image and textdatasets. However, it is unclear whether the same conclusion holdsin mobility data and what makes mobility data as data outliers.",
  ": Attack performance of (LocMIA and TrajMIA) on three victim models and two POI recommendation datasets. Thediagonal line indicates the random guess baseline": "To this end, we investigate which factors of the mobility datasetsinfluence the attacks efficacy. In particular, we collect aggregatestatistics of mobility data from three perspectives: user, location,and trajectory. We analyze which factors in these three categoriesmake mobility data vulnerable to our attacks. We defer the detailsof selecting the aggregate statistics and the list of selected aggregatestatistics in our study in Appendix C.1. Our findings are as follows: For LocExtract, we do not identify any meaningful patterncorrelated with its attack performance. We speculate that a usersmost common location is not directly related to the aggregatestatistics we study. For TrajExtract, our findings indicate that users who have vis-ited fewer unique POIs are more vulnerable to this attack, asreferenced in in Appendix C. This can be explained bythe fact that when users have fewer POIs, the model is less uncer-tain in predicting the next location due to the reduced numberof possible choices that the model memorizes. For LocMIA, as shown in Figures 4(a) and 4(b), we find thatlocations visited by fewer users or have fewer surrounding check-ins are more susceptible to LocMIA. We believe this is becausethose locations shared with fewer users or surrounding check-insmake them training data outliers. For TrajMIA, users with fewer total check-ins ((a)), uniquePOIs ((b)), and fewer or shorter trajectories (Figures 5(c)and 5(d)) are more susceptible. In Figures 6(a) and 6(b), we also seethat trajectories intercepting less with others or with more check-insare more vulnerable to TrajMIA. We believe these user-level andtrajectory-level aggregate statistics make the target examplesdata outliers.",
  "In summary, we conclude that the effect of data outliers also exists inprivacy attacks against POI recommendations. In the context of POI": "recommendation, the mobility data outliers could be characterizedfrom the perspectives of user, location, and trajectory. Differentattacks in our attack suite might be vulnerable to particular typesof data outliers, which are more unique and are vulnerable againstour attacks compared to other data. 4.2.3The impact of different attack designs (RQ3). We explore dif-ferent attack designs that may affect the performance of each attack.In particular, we defer detailed results in Appendix D and summa-rize our key findings as follows: For LocExtract, as shown in Figures 7 and 16 in the Appendix,we find that employing appropriate query timestamp and softvoting mechanism leads to better attack performance. The reasonis that the POI recommendation relies on temporal informationto make more accurate predictions. For TrajExtract, similar to LocExtract, we find that an appro-priate query timestamp also promotes the attack performance.Moreover, in the Appendix shows that our attack ismore effective when extracting location sequences of shorter tra-jectories as the influence of the starting location becomes weakerwhen the prediction moves forward. For LocMIA, as shown in , utilizing more queries with dif-ferent timestamps in our spatial-temporal model query algorithmimproves the results of inferring the membership of a target user-location pair (,). Since the adversary lacks information aboutreal input sequences that are followed by the target location .Utilizing more queries helps to traverse the search space andpromotes the attack performance. For both LocMIA and TrajMIA, as shown in in theAppendix, we find that a larger number of shadow models yieldsbetter attack performance since they provide more samples tosimulate the loss distributions of in-samples and out-samples.",
  "Where Have You Been? A Study of Privacy Risk for Point-of-Interest RecommendationKDD 24, August 2529, 2024, Barcelona, Spain": "a final remark, our attacks differ from previous MIAs in mobilitydata , which focus on the privacy risks of data aggregation.Mobility Data Privacy Mobility data contain rich information thatcan reveal individual privacy such as user identity. Previous workutilizes side-channel attacks to extract sensitive information aboutmobility data from LBS, including social relationships ,aggregated trajectories , trajectory history ,network packets and location embeddings . Despitethe focus of previous work, deep neural networks (DNN) built onlarge volumes of mobility data have recently become state-of-the-art backbones for LBS, opening a new surface for privacy attacks.To the best of our knowledge, our work is the first of its kind toinvestigate the vulnerabilities of DNN models in leaking sensitiveinformation about mobility data using inference attacks.More Related Works Due to the limited space, we have deferredmore related work of defense to Appendix section F.",
  "Defense Techniques": "In particular, we evaluate two streams of defense mechanisms onproposed attacks, including (1) standard techniques to reduce over-fitting (e.g., early stopping, 2 regularization) and (2) differentialprivacy-based defenses (e.g., DP-SGD ) for provable risk miti-gation. The standard techniques reduce the victim models mem-orization to some degree, but they are insufficient due to the lackof statistical guarantees. To fill this gap, differential privacy is also used to defend against our attacks, which can theoreticallylimit the impact of a single data point on the models performance.Specifically, we first experiment with DP-SGD , the most rep-resentative DP-based defense, to train differentially-private POIrecommendation models. The key idea of DP-SGD is to add Gauss-ian noises N (0, 22) to the clipped gradients of the modelduring its training process. is a clipping threshold that boundsthe sensitivity of by ensuring . To achieve (,)-DP, we",
  "ln 1.25": "/. Despite that DP-SGD provides promisingdefense performance on language tasks , we find that it cansubstantially sacrifice the models utility on the POI recommenda-tion task. Specifically, the top-10 accuracy is only 4.97% when themechanism satisfies (5, 0.001)-DP, while the original top-10 accu-racy without DP is 71%. The reason for this performance decreaseis that POI recommendation aims to make accurate user-level pre-dictions within a large output space (i.e., > 4, 000 possible POIs).For different users, even the same location sequence may lead to",
  "LocExtractMost common location of each userTrajExtractEach location sequence/sub-sequence ()LocMIAEach user-location pair (,)TrajMIAEach trajectory sequence/sub-sequence ( )": "a different result, which means that the model needs to captureuser-specific behavior patterns from a relatively small user dataset.As a result, the training is quite sensitive to the noises introducedby DP-SGD, making it not applicable to POI recommendations.However, we argue that DP-SGD provides undifferentiated pro-tection for all the mobility data, while for POI recommendation,protecting more tailored sensitive information is more important.For example, a defender may only care about whether a list ofcheck-ins about home addresses is protected or not. To this end, weintroduce the notion of selective DP to relax DP and improvethe models utility-privacy trade-offs. Specifically, we apply thestate-of-the-art selective DP method JFT to protect differentlevels of sensitive information for each attack. The key idea of JFTis to adopt a two-phase training process: in the phase-I training, JFTredacts the sensitive information in the training dataset and opti-mizes the model with a standard optimizer; in the phase-II training,JFT applies DP-SGD to finetune the model on the original datasetin a privacy-preserving manner. Due to the phase-I training, weobserve that the models utility is significantly promoted. In addi-tion to JFT, we also apply Geo-Indistinguishability (Geo-Ind) to protect common locations in LocExtract. We note that Geo-Ind is only applicable to LocExtract (but not LocMIA) because itrequires modifying the training data and is incompatible with thenotion of membership inference.",
  "Takeaway Messages from the Defense": "We evaluate different defense mechanisms in terms of their perfor-mance in preventing each attack from stealing the correspondingsensitive information. summarizes the exposure of sensi-tive information in each attack. Besides, Appendix E illustrates ourevaluation metrics, experimental setup, and the results. Recall thatthe check-ins within a mobility dataset are not equally important.Therefore, in our experiments, we comprehensively evaluate the de-fense mechanisms from two perspectives. Specifically, we measuretheir performance in (1) protecting all the sensitive informationand (2) protecting a targeted subset of sensitive information frombeing attacked. Tables 20 and 21 in the Appendix show that exist-ing defenses provide a certain degree of guarantee in mitigatingprivacy risks of ML-based POI recommendations, especially forthe targeted subset of sensitive information. However, there is nosuch unified defense that can successfully defend against all theproposed attacks within a small utility drop. In other words, ourexploration highlights the need for more advanced defenses.",
  "(d) Avg User Traj Length": ": How user-level aggregate statistics are related to TrajMIA. x-axis: Percentile categorizes users/locations/trajectoriesinto different groups according to their feature values. y-axis: indicates the (averaged) likelihood ratio of training trajecto-ries/locations being the member over non-member from the hypothesis test for each group, with a higher value indicatingthe larger vulnerability. The users with fewer total check-ins, fewer unique POIs, and fewer or shorter trajectories are morevulnerable to TrajMIA. (4sq)",
  ": The optimal query timestamp can significantlyimprove the performance of LocExtract and TrajEx-tract.(4sq)": "Spatial-Temporal Query, which better utilizes spatial-temporal in-formation to boost attack performance) and findings (e.g., identi-fying the types of data more likely to be memorized by the model)also benefit the measurement of privacy leakage for a broaderrange of learning-based models that involve spatio-temporaldata, e.g., pre-trained trajectory models and spatio-temporalLLMs . The defense results indicate that selectively protecting sensitiveinformation and using approaches like fine-tuning pre-trainedmodels on a smaller private dataset might improve the utility-privacy trade-off. In other words, our study reveals that selectivedefense serves as a promising direction to reduce the privacyrisks of these models.",
  "Related Work": "Mobility data contain rich information that can reveal individualprivacy such as user identity. Previous work utilizes side-channelattacks to extract sensitive information about mobility data fromLBS, including social relationships , aggregated trajecto-ries , trajectory history , network packets and location embeddings . Despite the focus of previous work,deep neural networks (DNN) built on large volumes of mobility datahave recently become state-of-the-art backbones for LBS, openinga new surface for privacy attacks. To the best of our knowledge,our work is the first of its kind to investigate the vulnerabilities ofDNN models in leaking sensitive information about mobility datausing inference attacks.Privacy Attacks. Various types of privacy attacks, such as mem-bership inference attacks , training data extractionattacks , and model inversion attacks have been proposedto infer sensitive information from model training data. Our attacksuite contains membership inference and data extraction attacks.Existing data extraction and membership inference attacks are insufficient for POI recommendation models due to the spatio-temporal nature of the data. Our work takes the first step to ex-tracting sensitive location and trajectory patterns from POI rec-ommendation models and solving unique challenges to infer themembership of both user-location pairs and user trajectories. As",
  "CONCLUSION": "In this work, we take the first step to evaluate the privacy risks ofthe POI recommendation models. In particular, we introduce anattack suite containing data extraction attacks and membershipinference attacks to extract and infer sensitive information aboutlocation and trajectory in mobility data. We conduct extensiveexperiments to demonstrate the effectiveness of our attacks. Addi-tionally, we analyze what types of mobility data are vulnerable tothe proposed attacks. To mitigate our attacks, we further adapt twomainstream defense mechanisms to the task of POI recommenda-tion. Our results show that there is no single solid defense that cansimultaneously defend against proposed attacks. Our findings un-derscore the urgent need for better privacy-preserving approachesfor POI recommendation models. Interesting future works include:(1) Generalize the attack suite to measure privacy risks of real-worldlocation-based services (e.g., Google Maps) in a more challengingsetting (e.g., label-only setting). (2) Develop more advanced defensemechanisms against our attacks.Ethics Statement. This work introduces a novel attack suite onPOI recommendation models trained on public anonymized mobil-ity datasets with no personally identifiable information, aimed atbringing potential vulnerabilities in POI models to public attention.The success of our attacks offers insights into future privacy leakagemeasurement in learning-based models involving spatio-temporaldata. Moreover, it underscores the need for improved defense so-lutions for POI models with better utility-privacy trade-offs. Wehope our study fosters further research in protecting the privacy ofmobility data.",
  "Kunlin and Jinghuai conducted the analysis experiments andproduced result visualizations": "Jianfeng and Jinghuai proposed defense strategies. Jinghuai implemented the defense mechanisms. Kunlin, Jianfeng, and Jinghuai wrote and edited the paper. Jianfeng, Yuan, Desheng, and Guang provided suggestions forpaper writing. We sincerely thank the reviewers for their valuable feedback on thepaper. This work is supported in part by the National Science Foun-dation (NSF) Awards 1951890, 1952096, 2003874, 2047822, 2317184,2325369, 2411151, 2411152, 2411153, UCLA ITLP and Okawa foun-dation. Any opinions, findings, conclusions, or recommendationsexpressed in this publication are those of the authors and do notnecessarily reflect the views of sponsors. Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov,Kunal Talwar, and Li Zhang. 2016. Deep learning with differential privacy. ACMSIGSAC Conference on Computer and Communications Security. Miguel E Andrs, Nicols E Bordenabe, Konstantinos Chatzikokolakis, and Catus-cia Palamidessi. 2013. Geo-indistinguishability: Differential privacy for location-based systems. ACM SIGSAC Conference on Computer and CommunicationsSecurity.",
  "Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural storygeneration. (2018)": "Natasha Fernandes, Mark Dras, and Annabelle McIver. 2019. Generalised differ-ential privacy for text document processing. Springer International Publishing,Principles of Security and Trust: 8th International Conference, POST 2019, Heldas Part of the European Joint Conferences on Theory and Practice of Software,ETAPS 2019, Prague, Czech Republic, April 611, 2019, Proceedings 8, 123148. Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. 2015. Model inversionattacks that exploit confidence information and basic countermeasures. ACMSIGSAC Conference on Computer and Communications Security. Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. 2015. Model Inversion At-tacks That Exploit Confidence Information and Basic Countermeasures. Proceed-ings of the 22nd ACM SIGSAC Conference on Computer and Communications Se-curity, New York, NY, USA, 13221333.",
  "Bugra Gedik and Ling Liu. 2005. Location privacy in mobile systems: A per-sonalized anonymization model. IEEE International Conference on DistributedComputing Systems": "Philippe Golle and Kurt Partridge. 2009. On the anonymity of home/work locationpairs. Springer, Pervasive Computing: 7th International Conference, Pervasive2009, Nara, Japan, May 11-14, 2009. Proceedings 7, 390397. Maziar Gomrokchi, Susan Amin, Hossein Aboutalebi, Alexander Wong, andDoina Precup. 2023. Membership Inference Attacks Against Temporally Corre-lated Data in Deep Reinforcement Learning. IEEE Access (2023).",
  "Mudhakar Srivatsa and Mike Hicks. 2012. Deanonymizing mobility traces: Usingsocial network as a side-channel. ACM Conference on Computer and Communi-cations Security": "Ke Sun, Tieyun Qian, Tong Chen, Yile Liang, Quoc Viet Hung Nguyen, andHongzhi Yin. 2020. Where to go next: Modeling long-and short-term user pref-erences for point-of-interest recommendation. AAAI Conference on ArtificialIntelligence. Florian Tramr, Reza Shokri, Ayrton San Joaquin, Hoang Le, Matthew Jagielski,Sanghyun Hong, and Nicholas Carlini. 2022. Truth serum: Poisoning machinelearning models to reveal their secrets. ACM SIGSAC Conference on Computerand Communications Security.",
  "AATTACK ALGORITHMS": "TrajExtract To start with, we initialize candidate trajectorieswith the same starting location and the query time given a user(lines 1-3). Next, we iteratively extend the candidate trajectories: toextract the -th ( 1) locations of 0:1in candidate trajectories,we query the model using 0:1and compute the log perplexityfor L possible new trajectories with a length of + 1. We thenchoose the trajectories with the lowest log perplexity as the newcandidate trajectories in the next iteration (line 6). The iterationsend until the length of the candidate trajectories reaches . Lastly,we take the location sequences from the final trajectories. Note thatboth LocExtract and TrajExtract need the timestamp to querythe victim model, and we will show the effects of timestamp inour experiments.LiRA The key idea of LiRA is that models trained with orwithout (i.e., and ) would produce different lossdistributions for . Specifically, LiRA consists of four mainsteps:(1) Train an equal number of shadow models with and without using the shadow dataset owned by the attacker to simulatethe behavior of the black-box victim model.(2) Use to query each shadow model to obtain loss val-ues. The obtained loss values will be used to calculate two lossdistributions, depending on whether the queried shadow modelwas trained on .(3) Query the victim model with to obtain the outputlogits.(4) Conduct a hypothesis test to infer the membership of the. The output score indicates whether the output from thevictim model is closer to one of the two loss distributions.LiRA originally trained 2 shadow models for each target exam-ple. However, this approach suffers from computational inefficiencywhen the number of target examples is large. To address this issue,we employ the parallelized approach described in , which reusesthe same set of 2 shadow models for inferring the membershipof multiple . Our attacks further extend the key concept ofLiRA and make it feasible for POI recommendation models by in-corporating spatial-temporal information through unique designs,such as our spatial-temporal model query algorithm.",
  "BDETAILED EXPERIMENTAL SETUPB.1Datasets": "We conduct experiments on two POI recommendation benchmarkdatasets FourSquare (4sq) and Gowalla datasets. Fol-lowing the literature , we use the check-ins collected inNYC for both sources. The 4sq dataset consists of 76,481 check-insduring ten months (from April 12, 2012, to February 16, 2013). TheGowalla dataset comprises 35,674 check-ins collected over a dura-tion of 20 months (from February 2009 to October 2010). In bothdatasets, a check-in record can be represented as [user ID, check-intime, latitude, longitude, location ID].",
  ": return max() Take the confidence scores withlargest confidence at position as output": "times to reduce noises introduced by uncommon check-ins. (2) Toconstruct trajectories of different users in a daily manner, the entirecheck-in sequence of each user is divided into trajectories with24-hour intervals. Then, we filter out the trajectories with onlya single check-in. (3) We further normalize the timestamp (from0:00 AM to 11:59 PM) in each check-in record into . After theaforementioned steps, the key statistics of the 4sq and Gowalladatasets are shown in . (4) Lastly, we split the datasets intothe training, validation, and test sets using the ratio of 8:1:1.Victim Model Training Settings: We use the official implementa-tion of GETNext2 and LSTPM3 to train victim models. In particular,we train each model with a batch size 32 for 200 epochs by default.We use five random seeds in all experiments and report the averageresults.",
  "(obs|N(out,2out)) Hypothesis test": "default, and we will present how the change of the query timestampaffects the attack performance in the ablation study.TrajExtract In this attack, we experiment with = 4 by default,though the attacker can potentially extract location (sub-)sequenceswith arbitrary length. We set the beam size = 50 in the beamsearch to query the victim model and update candidate trajectories.For each query, we also have the default query timestamp = 0.5.LocMIA In our experiments, since we randomly sample 80% of tra-jectories as the training dataset to build a victim model for MIA,we treat the remaining 20% data as non-members. For each targetuser and the POI location pair, we generate = 64 synthesis tra-jectories using TrajSynthesis with the query timestamp = 0.5.With the synthesis trajectories, we can also have 64 in-models ()and 64 out-models (). We also set = 10 and = 10. Forevaluation, we conduct a hypothesis test on a balanced number ofmembers and non-members.TrajMIA We extract the membership information of some trajec-tory sequences with arbitrary lengths from the victim model. Wealso build = 64 in-models () and = 64 out-models () for atarget trajectory sequence. For evaluation, we conduct a hypothesistest on a balanced number of members and non-members.",
  "CMORE DETAILS OF ANALYZING FACTORSIN MOBILITY DATA THAT MAKE ITVULNERABLE TO THE ATTACKSC.1How to Select Aggregate Statistics": "This section outlines the basic principles and details for select-ing representative aggregate statistics for analysis. For user-levelaggregate statistics, we target the basic statistical information quan-tifying properties of locations and trajectories of a user. For location-level and trajectory-level aggregate statistics, we study their users,neighboring check-ins and trajectories, and the check-in time in-formation. In summary, we select the following aggregate statistics: User-level aggregate statistics:(1) Total number of check-ins;(2) Number of unique visited POIs;(3) Number of trajectories;(4) Average trajectory length; Location-level aggregate statistics:(1) Number of users who have visited this POI;(2) Number of check-ins surrounding ( 1km) this POI;(3) Number of trajectories sharing this POI;(4) Average time in a day for the visits to the POI; Trajectory-level aggregate statistics:(1) Number of users who have the same trajectories;(2) Number of check-ins surrounding ( 1km) all POI in thetrajectory;",
  ": How trajectory-level aggregate statistics are relatedto TrajMIA. The trajectories with fewer intercepting trajec-tories or fewer POIs in the trajectory are more vulnerable toTrajMIA. (Gowalla)": "training stages, we aim to gain insights into the relationship be-tween overfitting and ASR.Based on the results presented in , we observe thatLocExtract achieves the best performance when the model isin the convergence stage. We speculate that continuing trainingbeyond convergence leads to overfitting, causing a loss of general-ization. Specifically, when the model is overfitted, it tends to assignhigher confidence to the training data while disregarding the gen-eral rules present in the dataset. In contrast, our attack employsrandom queries to extract the general rules learned by the model from the dataset, resulting in better performance when applied tothe optimally fitted model.The attack performance of TrajExtract improves as the train-ing process progresses, which can be attributed to the model be-coming increasingly overfitted. The overfit model is more likelyto output the exact training trajectory and generates more precisetraining trajectories than the best-fitted model when given thesame number of queries. Similarly, the results of our membershipinference attacks reveal a trend of attack performance consistentlyimproving with the progression of the training process. This obser-vation aligns with our expectations, as when the model undergoesmore training iterations, the effects of training data are more em-phasized. Consequently, the distribution of query results in ourattack on the seen training data diverging further from the dis-tribution derived from the unseen data. This growing disparitybetween the two distributions facilitates the membership inferencetask, particularly on overfitted models. The analysis of these threeattacks indicates a consistent trend, highlighting the increased riskof privacy leakage due to overfitting with respect to the originaltraining data.",
  "D.2Ablation Study on Our Data ExtractionAttacks": "Data extraction attacks are effective given a limited num-ber of queries In a realistic attack scenario, the adversary mayencounter query limitations imposed by the victim model, allowingthe adversary to query the model for only a certain number ofqueries. illustrates that our data extraction attacks areeffective given a limited number of queries. For example, as shownin (a), a mere = 50 query is sufficient for the adversaryto achieve a high ASR and infer a users frequently visited location.In terms of TrajExtract attack ((b)), the adversary canopt for a small beam width of = 10, requiring only 1000 queriesto extract a trajectory of length = 4. This practicality of our dataextraction attack holds true even when the query limit is very small.Appropriate query timestamp improves the effectiveness ofdata extraction attacks POI recommendation models rely on tem-poral information to make accurate location predictions. However,obtaining the same timestamps as training for attack can be chal-lenging and is an unrealistic assumption. Therefore, in our dataextraction attack setup, we set the query timestamp to = 0.5 (i.e.,the middle of the day).",
  ": The impact of the model generalization on the performance of four privacy attacks": "To analyze the effect of how different query timestamps af-fect data extraction attack performance, we conduct extractionattacks and vary different timestamps that represent various sec-tions within a 24-hour window in the experiments. The results,illustrated in and , indicate that utilizing times-tamps corresponding to common check-in times, such as the middleof the day or late afternoon, yields better attack outcomes. Thisfinding aligns with the rationale that users are more likely to engagein check-ins during the daytime or after work hours.Soft voting improves LocExtract For LocExtract, we havethe option to employ either hard voting or soft voting to determinethe most frequently occurring location. Hard-voting ensemblesmake predictions based on a majority vote for each query whilesoft-voting ensembles consider the average predicted probabilitiesand select the top-k locations with the highest probabilities. Fromthe experimental results depicted in , we observe that thereis not a substantial difference in ASR-1 when using hard voting orsoft voting. However, employing soft voting yields better ASR-3and ASR-5 results.Location sequences of shorter trajectories are more vulnera-ble to TrajExtract For TrajExtract, we conduct an ablationstudy to extract trajectories of varying lengths . The results, illus-trated in , indicate that the attack achieves higher ASR onshorter trajectories than longer ones. This observation can be attrib-uted to our assumption that the attacker possesses prior knowledgeof a starting location. As the prediction moves further away fromthe starting location, its influence on subsequent locations becomesweaker. Consequently, predicting locations farther from the startingpoint becomes more challenging, decreasing the attacks successrate for longer trajectories. Moreover, the extraction of long trajec-tories presents additional difficulties. With each step, the probabilityof obtaining an incorrect location prediction increases, amplifyingthe challenges the attack algorithm faces.",
  "D.3Ablation Study on Our MembershipInference Attacks": "A larger number of shadow models improves the effective-ness of MIAs As mentioned in , it has been observed that theattacks performance of LiRA tends to improve as the number ofshadow models increases. Consistently, our attacks also followthis pattern, as depicted in . Both location-level MIA andtrajectory-level MIA show enhanced performance as we incorporatemore shadow models. This improvement is because an increased number of shadow models allows for a better approximation of thedistributions for and , thereby simulating the victim modelmore accurately.LocMIA is effective given a limited number of queries Sinceour LocMIA involves multiple queries to explore locations preced-ing the target location, as well as the corresponding timestamps,it is essential to consider potential limitations on the number ofqueries in real-world scenarios. Thus, we conduct experiments toinvestigate the impact of query limits on LocMIA. The results, de-picted in (a), indicate that our attack remains effective evenwith a limited number of queries for different location choices. Thefurther increase in query locations would not significantly improveattack results.We also conduct experiments with different settings for the num-ber of query timestamps, denoted as . The rationale behind thisstep is that the adversary does not possess information about thereal timestamp used to train the victim model. To simulate the ef-fect of selecting the correct timestamp, we perform experimentswith varying timestamps to identify the timestamp that yieldedthe highest confidence score for the targeted location. Based onempirical observations from our experiment on the 4sq dataset (see(b)), increasing the number of query timestamps tends toyield better overall results in practice.TrajMIA is less effective as the length of target trajectoryincreases From , we note that the attack performancedrops as target trajectories are longer. This decline happens becauseall trajectory query scores influence the attack. Lengthier trajecto-ries introduce increased randomness for the query, affecting theoutcome of TrajMIA.",
  "EDEFENSE": "We evaluate existing defenses against our privacy attacks, includingstandard techniques to reduce overfitting and differential privacybased defense. Detailed descriptions of defense techniques can befound in . Then, in E.1, we describe the evaluation metricsto measure the defense performance from two perspectives. In E.2,we compare different defenses and analyze the numerical resultswith detailed explanations.",
  ": Comparing soft voting with hard voting for logitsaggregation in LocExtract. Soft voting has larger improve-ments over hard voting as increases": "their performance in preventing each attack from stealing the cor-responding sensitive information. Specifically, we measure theirdefense performance on protecting all the sensitive information anda targeted subset of sensitive information for each attack, respec-tively. Here, we define the targeted subset of sensitive informationas the mobility data a defender wants to protect in practice (e.g.,some selected user-location pairs in LocMIA). We introduce thismetric because not all the mobility data are sensitive or equallyimportant. Take LocMIA as an example: Since the utility of POIrecommendation is highly related to the models memorization ofuser-location pairs, a user may want the model to recognize mostof the POIs in his trajectory history while hiding those that are very",
  ": The location sequences of shorter (sub-)trajectoriesare more vulnerable to TrajExtract": "likely to leak his personal identity (e.g., home). In other words, notall the mobility data need to be protected and its more important toevaluate how defense mechanisms perform on the targeted subsetof sensitive information.To this end, we jointly measure the defense performance inprotecting all the sensitive information and the targeted subsetof sensitive information for each attack. Based on different attackobjectives, we construct a different targeted subset of sensitive in-formation for measurement by randomly sampling a portion of(e.g., 30%) the most common locations in LocExtract, locationsequences in TrajExtract, user-location pairs in LocMIA andtrajectory sequences in TrajMIA. It is noted that we randomlysample 30% of sensitive information in each attack to constructthe targeted subset for the ease of experiments. In practice, the de-fender may have more personalized choices based on user-specificrequirements, which we leave as future work.",
  "E.2Defense Setup and Results": "Setup GETNext models trained on the 4sq dataset are used forexperiments. For 2 regularization, we use weight decay =12 and 32. For early stopping, we stop the training after 5epochs. For JFT, we mask sensitive information that needs to beprotected in phase-I. Then in phase-II, we use DP-SGD withdifferent (1 and 5) to finetune the model. The and are set to 10and 13. For Geo-Ind against LocExtract, we apply different (0.01 and 0.05) to replace each sensitive POI with its nearby locationsuch that the original POI is indistinguishable from any locationwithin = 400 meters. Since both JFT and Geo-Ind can be used",
  ": The longer trajectories are less vulnerable to Tra-jMIA": "to protect different amounts of sensitive information, we eitherprotect nearly all the sensitive information or only the targetedsubset of sensitive information for each attack, denoted by suffixes(A) and (T).LocExtract (a) shows the defense results on protectingall the sensitive information for LocExtract. From the figure, weobserve that DP-based defenses achieve better performance thanstandard techniques. Both JFT (A) and Geo-Ind (A) reduce ASR from30% to 1% with only a 10% drop in utility. The reason is that thesemethods allow the defender to selectively protect common loca-tions only. Besides, when protecting the same amount of sensitiveinformation, JFT achieves slightly better accuracy than Geo-Indbecause it involves phase-II training to further optimize the model.Moreover, although the ASR is still high for JFT (T) and Geo-Ind(T) in (a), we notice that they can substantially reduce theattack performance on the targeted subset of sensitive information,as shown in (a). This allows a defender to protect thetargeted subset with negligible utility drop. further showsthat Geo-Ind can predict nearby locations of a protected POI asprediction results to maintain its usage.TrajExtract Figures 20(b) and 21(b) show that all the defensescan well protect location (sub-)sequences from being extractedby TrajExtract. This is because sequence-level extraction is achallenging task that pretty much relies on memorization.LocMIA Figures 20(c) and 21(c) show that none of the existingdefenses can be used to protect user-location membership informa-tion. While JFT (A) reduces the TPR@10%FPR to less than 20%, itsignificantly sacrifices the models utility. The reason is that thedefender needs to redact a large number of user-location pairs soas to protect them. As a result, the model may learn from wrongsequential information in the phase-I training, leading to a large utility drop. Even for protecting the targeted subset with 30% oftotal user-location pairs only, theres still a 20% drop in utility.TrajMIA Figures 20(d) and 21(d) show the utility-privacy trade-offof different defenses against TrajMIA We notice that JFT (T) caneffectively mitigate the MIA on the targeted subset of trajectorysequences with a small degradation in accuracy. However, the utilitydrop is still large if a defender aims to protect the membershipinformation of all trajectory sequences.Summary Existing defenses provide a certain degree of guaranteein mitigating the privacy risks of ML-based POI recommendations.However, it is still challenging to remove all the vulnerabilitieswithin a reasonable utility drop. This is because existing POI rec-ommendation models heavily rely on memorizing user-specifictrajectory patterns to make predictions, which lack semantic in-formation as guidance. As a result, defense mechanisms such asDP-SGD can easily compromise the utility of the protected modeldue to the noises added to the gradients. Moreover, defenses suchas JFT are not general for all inference attacks since each attacksteals different sensitive information. To this end, our evaluationcalls for more advanced mechanisms to defend against our attacks.",
  "Ground Truth 400 m": ": For LocExtract, when ground truths are the mostcommon locations, Geo-Ind (A) can predict a nearby location( 400 m) of each protected location as the next POI withhigher accuracy than JFT (A). The reason is that Geo-Indapplies the Laplacian mechanism to replace each protectedPOI with its nearby location. information from mobility data. Consequently, researchers havealso explored various approaches to safeguard the privacy of mobil-ity data, including K-Anonymity , which aims to generalizesensitive locations by grouping them with other locations, LocationSpoofing , which involves sending both real and dummylocations to deceive adversaries, Geo-indistinguishability ,and local differential privacy (LDP) . However, these priordefense mechanisms primarily focus on data aggregation and re-lease processes and can not be directly used in the context of POIrecommendation. In contrast, our work is the first to concentrate on protecting against privacy breaches originating from deep learningmodels such as POI recommendation models and thats why weare primarily focusing on testing defense mechanisms related tothe model training process as mentioned in Section E.Defenses against privacy attacks on deep neural networksThere are also multiple works on protecting the privacy of deeplearning models, with some notable examples including regulariza-tion and early stopping, which are commonly employed techniquesto mitigate overfitting . Another approach is differentially pri-vate stochastic gradient descent (DP-SGD) , which achieves dif-ferential privacy by introducing noise during the gradient descentprocess while training the model. Additionally, selective differen-tial privacy (S-DP) has been proposed to safeguard the privacy ofspecific subsets of a dataset with a guarantee of differential privacy. However, these methods have primarily been tested onimage or language-related models and require customization to fitinto the usage of POI recommendation models. In our work, we fo-cus on adapting these defense mechanisms to POI recommendationmodels by developing privacy definitions that are specifically tai-lored to the attacks we propose. In addition, we follow the conceptof selective DP to relax the original DP and selectively protectthe sensitive information (e.g., most common locations) in mobilitydata."
}