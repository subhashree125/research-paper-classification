{
  "ABSTRACT": "Enhancing diverse human decision-making processes in an urbanenvironment is a critical issue across various applications, includ-ing ride-sharing vehicle dispatching, public transportation man-agement, and autonomous driving. Offline reinforcement learning(RL) is a promising approach to learn and optimize human ur-ban strategies (or policies) from pre-collected human-generatedspatial-temporal urban data. However, standard offline RL facestwo significant challenges: (1) data scarcity and data heterogeneity,and (2) distributional shift. In this paper, we introduce MODA aMulti-Task Offline Reinforcement Learning with Contrastive DataShAring approach. MODA addresses the challenges of data scarcityand heterogeneity in a multi-task urban setting through ContrastiveData Sharing among tasks. This technique involves extracting latentrepresentations of human behaviors by contrasting positive andnegative data pairs. It then shares data presenting similar represen-tations with the target task, facilitating data augmentation for eachtask. Moreover, MODA develops a novel model-based multi-taskoffline RL algorithm. This algorithm constructs a robust MarkovDecision Process (MDP) by integrating a dynamics model with aGenerative Adversarial Network (GAN). Once the robust MDP isestablished, any online RL or planning algorithm can be applied.Extensive experiments conducted in a real-world multi-task urbansetting validate the effectiveness of MODA. The results demon-strate that MODA exhibits significant improvements compared tostate-of-the-art baselines, showcasing its capability in advancingurban decision-making processes. We also made our code availableto the research community. Permission to make digital or hard copies of part or all of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for third-party components of this work must be honored.For all other uses, contact the owner/author(s).KDD 24, August 2529, 2024, Barcelona, Spain. 2024 Copyright held by the owner/author(s).ACM ISBN 979-8-4007-0490-1/24/08.",
  "Data Sharing, Offline Reinforcement Learning, Contrastive Learn-ing": "ACM Reference Format:Xinbo Zhao, Yingxue Zhang, Xin Zhang, Yu Yang, Yiqun Xie, YanhuaLi, and Jun Luo. 2024. Urban-Focused Multi-Task Offline ReinforcementLearning with Contrastive Data Sharing. In Proceedings of the 30th ACMSIGKDD Conference on Knowledge Discovery and Data Mining (KDD 24),August 2529, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.",
  "INTRODUCTION": "Problem and Goal. In urban scenarios, individuals, referred toas human agents, engage in strategic planning to optimize theirdaily activities. These strategies devised to achieve specific objec-tives govern the human actions and decisions. For example, taxidrivers strategize to maximize earnings and minimize travel timeby selecting optimal pickup locations, adjusting working hours,and planning efficient routes. Many urban strategies, such as thoserelated to passenger seeking, public transit commuting and travelroute selection, are individually crafted based on personal prefer-ences. Consequently, they may not represent the most effective orefficient approaches and often require enhancement. In addition,these urban strategies are generally implicit to observers or eventhe human agents themselves, making their learning and improve-ment a complex task. Therefore, in this paper, we aim to addressthis essential problem: How can we learn and enhance diverse humandecisions in urban environments?Prior works and limitations. Different human strategies anddecisions employed in various tasks lead to diverse human behav-iors which are collected as human-generated spatial-temporal data",
  ": Example of data scarcity and heterogeneity, anddistributional shift": "(HSTD). Examples of HSTD include GPS trajectories from taxisand personal vehicles, route choices from bus and train passengers,etc. Leveraging HSTD, data-driven techniques including imitationlearning and offline reinforcement learning (RL) stand out as reli-able tools for understanding human decision-making processes. Forinstance, imitation learning and its variants aim atunderstanding human decision-making strategies and mimickinghuman behaviors across diverse urban scenarios using deep neuralnetworks (DNNs). However, these approaches primarily focus onreplicating human behaviors presented in the dataset, which isbounded by the true behavior policies that produce the dataset. Inthis sense, the learned policies can hardly outperform true behaviorpolicies, which renders them less suitable for enhancing human ur-ban strategies with HSTD. In contrast, offline RL leverages theinherent optimization mechanisms of RL to not only learn but alsoimprove and optimize policies given HSTD. Many works have ex-plored offline RL to solve urban problems. For example, some offlineRL approaches are designed for traffic signal control and autonomous driving . However, these methods are gen-erally limited to single-task scenarios within urban settings, as theycan only address an individual task using a specific dataset, lack-ing the capability to learn from a composite dataset encompassingmultiple tasks. This limitation restricts their applicability in morecomplex, multi-task urban environments. Some multi-task offlineRL approaches learn diverse skills for multiple tasks withvarious data-sharing techniques. However, these solutions are nottransferrable to the urban domain as they rely on strong presump-tions about the availability of explicit reward functions, which areinaccessible in most urban scenarios.The present work. To address the above limitations and learn andenhance diverse human strategies, we introduce MODA a novelMulti-task Offline RL with Contrastive Data shAring framework,which addresses the following two challenges: Data scarcity and data heterogeneity. Learning from observationsrequires extensive amount of data from a task. However, in prac-tice, it is difficult to collect a large amount of mobility data (i.e.,HSTD) from each human agent. This leads to data scarcity. Morecommonly, the collected data originates from a variety of hu-man agents, each employing a distinct urban strategy generatingunique trajectories. This leads to data heterogeneity. Both datascarcity and heterogeneity problems complicate the process oflearning and improving strategies for diverse human agents. Distributional shift. Without online data collection during thelearning process, offline RL often encounters failures attributed tosubstantial extrapolation errors when evaluating the -functionon out-of-distribution actions . This issue worsens over the learning process as the learned policies increasingly deviate fromthe behavior policy. Moreover, the distributional shift problem isexacerbated in multi-task urban scenarios when learning urbanstrategies from diverse human agents, which leads to a hugedivergence of the learned policies and poor performance.As shown in , MODA integrates a novel Contrastive DataSharing method and an innovative model-based multi-task offlineRL algorithm. MODA first addresses the data scarcity and hetero-geneity issues through Contrastive Data Sharing across all tasks. Itthen develops a novel model-based multi-task offline RL algorithm,which constructs a robust Markov Decision Process (MDP) by com-bining a dynamics model with a Generative Adversarial Network(GAN) . This design can effectively mitigate the distributionalshift challenge inherent in offline RL. The primary contributions ofthis paper can be summarized as follows: We make the first attempt to optimize diverse human strategiesby a novel multi-task offline RL framework named MODA. A keycomponent of MODA is an innovative Contrastive Data Sharingstrategy, which learns representations of human behaviors bycontrasting positive and negative data pairs. It then selectivelyshares data from other tasks that display patterns akin to thoseof the target task, ensuring a more effective sharing process totackle the data scarcity and data heterogeneity problems. MODA also incorporates a novel model-based multi-task offlineRL algorithm which can successfully address the distributionalshift issue. It creates a robust MDP by learning a dynamics modeland a GAN model from HSTD. While the dynamics model gen-erated transitions are not universally accurate given that theoffline dataset after Contrastive Data Sharing still does not coverthe entire state space the GANs generator has a natural abilityto generalize the offline dataset and learn the real MDPs datadistribution. This, in turn, improves the discriminators abilityto tell correct transitions from erroneous ones, making it morecapable and generalizable. Once the learned dynamics model isintegrated with the discriminator, which acts as a detector todistinguish reliable and out-of-distribution transitions, the ac-curacy of the dynamics model can be improved. Any online RLor planning algorithm can be applied once the robust MDP isconstructed. Extensive experiments are conducted under a real-world multi-task urban setting with HSTD to validate the effectiveness of ourMODA framework. The results demonstrate that MODA exhibitssignificant improvements compared to state-of-the-art baselineswhen learning different human urban strategies. We also madeour code available to the research community1.",
  ": MODA overview": "not just individual ones. In this paper, we treat the urban strategiesof different agents as distinct tasks, and formalize the MDP asa 5-tuple M = (S, A, ,, {}=1), where S and A denote thespaces of states and actions, respectively, (|,) is the dynamicsfunction, [0, 1) is the discount factor, and represents thereward function for each task among a total of tasks, detailedfurther below.Definition 1 (Grid cells). A city is divided into grid cells,with each cell having equal side-length in latitude and longitude.We denote the collection of grid cells in a city as G = { }, wherethe indices and satisfy 1 and 1 , respectively.Definition 2 (A state ) represents a spatial-temporal locationwhich is defined as a tensor R. The state contains different feature maps R at time . Since the human agentsusually consider the status of the surrounding area when makingurban decisions, each feature map characterizes a specific urbanfeature (e.g., traffic speed, travel demand, etc.) of both the targetgrid cell and its neighboring grid cells.Definition 3 (An action ) is a decision made by a human agentat state . Following an action , the human agent moves from astate to the next state . We denote the set of actions as A = {}.Definition 4 (A dynamics ) (or transition probability) is decidedby the environment and is defined as : S A S .(|;) characterizes the probability of transiting to state atstate by following action .Definition 5 (A reward function ) is a mapping as : SA R, which provides a numerical score based on a state and an action, and incentivizes a human agent to accomplish a task. Note thatwe treat distinct agents as separate tasks, using different agentsand different tasks synonymously.Definition 4 (A policy ( | ,)) is a probability distributiondefined as : SA indicating the probability of choosingan actions given the state . In this work, we follow Yu et al. and target on data sharing strategies to learn a conditional policy( | ,) which indicates a policy for a target task .Definition 5 (A trajectory ) is a sequence of states and actionsthat a human agent traverses when completing a task in a geo-graphic region, i.e., = (0,0, , , ) of length . The set of",
  "Urban-focused Multi-Task Offline RLProblem": "This paper aims to develop a new multi-task offline RL frameworkthat can effectively optimize diverse human urban strategies whileminimizing distributional shift and deal with the data scarcity andheterogeneity problems at the same time. The formal definition ofthe urban-focused multi-task offline RL problem is as follows:Problem Definition. Given a set of human mobility trajectoriesT collected from different human agents, assume the data fromeach individual is generated by a specific behavior policy (|,)to implement a certain task , we aim to learn the urban decision-making policy ( | ,) for every agent so that they can producehigher expected cumulative rewards compared to the correspondingbehavior policy ( | ,).",
  "METHODOLOGIES": "In this section, we solve the urban-focused multi-task offline RLproblem by introducing an innovative multi-task offline RL withcontrastive data sharing approach, in short, MODA, as shown in. MODA has two components to address the above challenges:(1) Contrastive Data Sharing. To tackle the challenge of data scarcityand heterogeneity, we introduce a Contrastive Data Sharing mech-anism. It enables data sharing across tasks by learning data repre-sentations and strategically sharing data from other tasks with thetarget task in a contrastive manner. The shared data is selected basedon its similarity to the data of the target task, ensuring that theyreflect comparable behaviors, preferences, and decision-makinglogic (See .1).(2) Model-based multi-task offline RL with Robust MDP. To tackle thechallenge of distributional shift, we propose a novel model-basedoffline RL approach. We employ a GAN model to enhancethe reliability of the learned dynamics model, where the generatoris trained to understand the data distribution of the actual envi-ronment and produce reliable transitions, while the discriminatordistinguishes between reliable and out-of-distribution transitions.A robust MDP is formed through the integration of the dynamicsmodel and the discriminator. Any online RL or planning algorithmcan be utilized once a robust MDP is established (See .2).",
  "Contrastive Data Sharing": "Contrastive Data Sharing is an important component in our MODAframework, which is specifically designed to address the data scarcityand heterogeneity challenge and thus enable effective data sharingamong tasks. Standard multi-task offline RL usually requires largeamounts of data for each task to ensure decent performance. How-ever, HSTD is generally sourced from a variety of human agentsor tasks, with each individual contributing a limited dataset. Thus,when attempting to learn different urban decision-making pro-cesses in a multi-task offline RL setting, we face data scarcity andheterogeneity problems. 3.1.1Limitations of state-of-the-art works. Previous multi-task of-fline RL works have indicated the benefits of sharingdata across multiple tasks to assist a target task . However, thesemulti-task methods often presume direct access to the functionalform of the reward for the target task . In urban scenarios, re-ward functions and human preferences tend to be implicit, not onlyto external observers but also to the agents themselves, resulting ininaccessible reward functions. This lack of accessibility makes theprior multi-task offline RL techniques with data sharing ineffectivefor urban applications. 3.1.2Contrastive Data Sharing Objective. We introduce a noveldata sharing method that employs contrastive learning to effec-tively augment the dataset for a target task by incorporating sim-ilar data from other tasks. Consider a static multi-task datasetD = =1 D, where represents the number of tasks, for a targettask , the data (typically a set of transitions {(,, ,)}) sharedfrom task to task is denoted as D. Consequently, the aug-mented dataset for task , termed as the effective dataset, is definedas: Deff:= D D. The challenge then lies in identify-ing and sharing relevant data from other tasks to the target task effectively. Merely sharing all transitions indiscriminately amongall tasks, a method we term Sharing All, has been shown to yieldsuboptimal results in previous studies . A more reasonableapproach would be first learning the meaningful latent representa-tions of trajectories collected from all tasks and understanding thetheir similarities. Subsequently, only those trajectories from othertasks that exhibit behaviors similar to those of the target task areincorporated into the effective dataset Deff .Thus, to discern the similarities among the latent representa-tions of all trajectories =1{(0,0, , , )}, we design a Con-trastive Data Sharing method. This method chooses a more nuancedapproach for learning representations and data sharing by focusingon sub-trajectories rather than entire trajectories when sharing datafrom other tasks to the target task . This is because the behaviorsand decision-making patterns of human agents can significantlyalign in specific locations or time slots with certain segments ofthe target agents trajectories. However, for a whole trajectory, be-haviors and decision-making logic may vary considerably, leadingto huge differences between complete trajectories. Ignoring thecommonalities within sub-trajectories would result in an inefficientdata sharing process, as only a few trajectories from other tasks oragents would be considered relevant to the target task. By lever-aging sub-trajectories, our method ensures a more effective data sharing process, which captures the nuanced similarities betweenagents behaviors.In Contrastive Data Sharing, a trajectory = (0,0, , , )is partitioned into multiple sub-trajectories = {}, where each represents a distinct sub-trajectory composed of consecutive tran-sitions, e.g., 1 = {(0,0, 1,1), , (1,1, ,)}. Thegoal is to learn latent embeddings and discern similarities across alltrajectories =1{(0,0, , , )} by examining the relation-ships between their sub-trajectories in a contrastive manner. In thisprocess, we construct positive samples as pairs of sub-trajectoriesfrom the same target task(/agent) exhibiting strong similarities,and negative samples as pairs of sub-trajectories from the targetagent and other agents showing clear dissimilarities, as illustratedin (a). As decision-making processes in adjacent locationsand time are often alike , sub-trajectories in close proximity typ-ically exhibit similar behavior patterns. Given a trajectory from thetarget task, once we set a sub-trajectory as an anchor, we constructpositive pairs composed of the anchor and another sub-trajectorywithin a specified positive range . Conversely, sub-trajectoriesfrom different tasks at the same spatial-temporal states are consid-ered negative samples due to behavioral differences.The embedding of a sub-trajectory is represented by () R, where is the contrastive neural network parameterized by. To learn latent representations of sub-trajectories, ContrastiveData Sharing uses multi-view metric learning via a triplet loss ,This loss ensures that a pair of sub-trajectories (anchor) and (positive) are closer to each other in the latent space than anysub-trajectory (negative). Thus, we aim to learn a contrastivenetwork such that:",
  "( , ) D, D \\ D,": "where is a margin that is enforced between positive and negativepairs, and D is the dataset for target task. The core idea is that twosub-trajectories (anchor and positive) coming from the target andshowing a lot in common are pulled together, while a sub-trajectoryfrom different tasks is pushed apart.After learning the contastive network, the similarities betweentwo sub-trajectories can be evaluated by the Euclidean distance oftheir corresponding embeddings. The final data sharing strategy isas below:",
  "(2)": "Here, is the number of all possible anchors, 2 is the varianceof the embeddings of all anchors from the target task , is theaverage embedding of all anchors. Based on the sharing rule shownin Eq. (2), if the similarities are smaller than the variance of theanchor, sub-trajectories { } from other tasks can be added tothe effective dataset Defffor target task . Note that once we get",
  "Urban-Focused Multi-Task Offline Reinforcement Learning with Contrastive Data SharingKDD 24, August 2529, 2024, Barcelona, Spain": ": MODA structure. (a) illustrates the detailed structure of Contrastive Data Sharing, which consists of acontrastive network working on positive and negative pairs of sub-trajectories from different tasks, a final embedding space isproduced by following the contrastive loss. (b) indicates the structure of the model-based multi-task offline RL withrobust MDP, which is constructed by the combination of a dynamics model and the discriminator of a GAN.",
  "Model-based Multi-task offline RL withRobust MDP": "After Contrastive Data Sharing, a novel model-based multi-taskoffline RL approach is designed to learn better policies comparedto agents behavior policies. The proposed model-based offline RLapproach aims to construct a robust MDP by learning a dynamicsmodel and a generative adversarial network (GAN). Once the dy-namics model and the GANs discriminator are integrated, a robustMDP can be established. Any state-of-the-art online RL or planningalgorithm, such as Soft Actor-Critic (SAC) , can be employed",
  "within this robust MDP to finally acquire the optimized policies forthe target tasks": "3.2.1Robust MDP. To successfully construct a robust MDP, we fol-low the steps including (i) learning a dynamics model, (ii) learninga GAN, and (iii) Robust MDP construction.Learning a dynamic model. In our proposed model-based of-fline RL algorithm, the first step involves using the offline dataset(i.e., the final effective dataset Defffor a target task ) to learn anapproximate dynamics model with rewards (,|,,), where is a deep neural network parameterized by . Note we cannotuse the whole dataset D to learn a (,|,) applicable to allagents since different agents have different preferences and rewards,and correspond to different (,|,,)2. This can be achievedthrough maximum likelihood estimation or other techniques fromgenerative and dynamics modeling . However, urban en-vironments are inherently complex and multifaceted. Even withthe dataset obtained after Contrastive Data Sharing, it is impossibleto cover the entire state space. Consequently, it is challenging tomodel every potential transition within the environment using thelearned dynamics model. Thus, simply learning a dynamics modelwith rewards from the data Deffis far from enough to correctlyreflect the real-world dynamics.Learning a GAN. Given that the transitions generated by thelearned dynamics model are not universally reliable, relying solelyon this model could lead to a highly sub-optimal policy. This is pri-marily due to the models potential to produce incorrect transitions, 2In practice, D is much larger than Deff , if we learn a unified dynamics model (|,) using D and individual reward function (,) using Deff , (,) willnot be compatible with (|,), and cannot produce reliable rewards for transitionsnot in Deff , leading to a sub-optimal policy.",
  "KDD 24, August 2529, 2024, Barcelona, Spain.Xinbo Zhao et al": "particularly in unexplored parts of the environment. Consequently,we utilize the discriminator of a GAN model to discern betweenreliable and unreliable transitions. Additionally, GANs generatorcan generate more data based on the offline dataset.By training a GAN using the dataset Deff , the generator triesto learn the data distribution of Deff . The goal of is to generatetransitions that closely mimic the real transitions presented in thereal MDP M. The goal of discriminator is to differentiate be-tween these generated transitions and the actual transitions fromthe dataset. When the generator tries to create transitions indistin-guishable from real ones to effectively fooling the discriminator, thediscriminator aims to enhance its ability to identify the generatedtransitions, thereby not being deceived. This adversarial processensures continuous improvement of both components.One significant advantage of incorporating a GAN in our model-based multi-task offline RL is the generators inherent ability togeneralize beyond the offline dataset. The generator will learn thedata distribution of the real MDP and thus the generated transitionsare more generalized and not only limited to Deff . This broaderscope of generated transitions will benefit the discriminator aswell, enhancing its capability in accurately distinguishing correcttransitions from out-of-distribution ones, even for transitions thatare not in Deff . The objective of training the GAN model is listedas below:",
  "+ E () [log(1 ( ()))],(3)": "where (,, ,) is a transition, is the data distribution of thereal MDP, is a random noise sampled from a Gaussian. In this way,we get a discriminator working as the transition detector whichcan tell whether a transition is from the real data distribution ornot. Both and are deep neural nets parameterized by and .Robust MDP construction. Once we get the learned dynamicsmodel and the discriminator, we combine them to construct a robustMDP. For every transition produced by the dynamics model, weuse the discriminator to tell whether it is a reliable one. The finalrobust MDP is as below: robust,|,, = ( = Terminate)if (,, ,) , (,|,,)if o.w,(4)where robust is the final robust MDP, ( = Terminate) is theDirac delta function, which forces the MDP to transit to the ter-mination state, is a threshold for the discriminator output (i.e.,a score determining the reliability of a given transition), a scorehigher than indicates a transition is reliable, while a score below suggests otherwise. 3.2.2Policy Optimization. After the construction of the robustMDP, we treat each individual agent as a task under its robustMDP. Therefore, for each task , we can apply any online RL orplanning algorithms to optimize the generalized policy ( | ,).In this paper, we directly apply Soft-Actor-Critic (SAC) forpolicy optimization. The detailed algorithm for our model-basedoffline RL is shown in Algorithm 2.The structure of our model-based offline RL is shown in Fig-ure 3(b). The dynamics model contains several fully-connected layers followed by ReLU. uses a state and an action as the in-put and tries to predict the next state and reward . The generator eats a random noise and generates a transition (,, ,), thediscriminator uses a real or generated transition (,, ,) asinput and outputs a score. Both and contains fully-connectedlayers followed by ReLU and Sigmoid, respectively.",
  "EXPERIMENTS": "In our experiments, we aim to evaluate whether MODA can achievesatisfactory performance in the real-world multi-task urban setting.We will answer the following questions with extensive experiments:(1) Can our MODA learn good policies for different tasks comparedto state-of-the-art baseline methods in the multi-task urban setting?(2) Can the Contrastive Data Sharing method in MODA effectivelyshare data across tasks compared to other data sharing methods?(3) How do different features (including the amount of shared dataand other hyperparameters) affect the performance?",
  "Dataset and Experiment Descriptions": "Data description. We evaluate our MODA using the taxi trajec-tory dataset representing diverse taxi drivers passenger-seekingstrategies. Different drivers employ different strategies for seekingpassengers which can be viewed as different tasks. The passenger-seeking trajectories are collected from 17,877 taxis in Shenzhen,China from July 1 to Sep 31, 2016. Each passenger-seeking trajec-tory is formed by multiple consecutive GPS records of a certaintaxi. A GPS record includes five attributes including the taxi plateID, longitude, latitude, time stamp and passenger indicator.In this dataset, the whole Shenzhen City is first divided into 40 50 equal-sized grid cells with a side-length 1 = 0.0084 in latitudeand 2 = 0.0126 in longitude. And the time of a day is dividedinto five-minute time slots. A spatial-temporal state is defined as amulti-dimensional tensor which is composed of different featuremaps of its neighboring 5 5 grid cells in a specific time slot, herethe features includes traffic volume, travel demand, traffic speed,",
  ": Performance of Offline RL algorithms with differ-ent data sharing approaches. All results are averaged over 20rollouts, utilizing three random seeds": "waiting time, and the distance to the selected Points of Interests(PoIs). When a taxi is in a specific state, the taxi driver has 10 actionsto choose from, including going to 8 neighboring grid cells, stayingat the current grid cell, and terminating the trip.Passenger-seeking simulation: We created a simulated passenger-seeking environment based on the above taxi trajectory data forthe comparison of approaches requiring environmental interac-tions. We have opened source this simulation environment. Moredetails about data and the simulated environment can be found inAppendix.",
  "Baselines": "To evaluate our proposed model-based offline RL with robust MDP,we evaluate multiple state-of-the-art offline RL models as baselinesincluding model-free algorithms CQL , BCQ , BEAR ,and a model-based algorithm MOReL . All algorithms are run-ning on the same effective datasets after applying data sharing fortarget tasks. Besides, a baseline called MODA is also comparedwhich removes the GAN from our MODA. Furthermore, to vali-date whether our Contrastive Data Sharing method is effective, wecompare it with other data sharing strategies without requiringreward functions including No Sharing , Sharing All , andUDS . No Sharing doesnt perform any data sharing, SharingAll naively shares all data across tasks, UDS relabels data from othertasks with zero reward and then share with the target.",
  "Contrastive Data Sharing. We train the Contrastive learningmodels by randomly sampling triples containing Anchor, positive,and negative from the dataset. We use 4 convolutional layers with": ": Impact of different number of transitions sharedby Contrastive Data Sharing. MODA exhibits varying per-formance across drivers of different expertise levels when adistinct number of transitions are shared with the target dri-ver. All results are averaged over 20 rollouts, utilizing threerandom seeds. kernel size 3. The batch size is set to 32. Adam optimizer is applied.All models are trained for 2000 epochs.Model-based Multi-task offline RL with Robust MDP. We usetransitions from effective dataset Deffto train GAN and the dynam-ics model. Then we use the discriminator in GAN and dynamicsmodel to build a Robust MDP. More details about these two modelsand Robust MDP can be found in the Appendix.",
  "Empirical Results": "Results of Question (1). To evaluate whether our proposed model-based offline RL with robust MDP in MODA can produce goodpolicies for different tasks, we compare MODA with baselines in-cluding CQL, BCQ, BEAR, MOReL and MODA for 3 differentdrivers. All baselines are built upon the same effective dataset for aspecific driver after contrastive data sharing. As shown in Table. 2,we pick 3 drivers from our dataset including an expert driver, amedium-skilled driver and a random driver, whose behavior poli-cies are optimal, medium and random policies, respectively. Eachvalue in Table. 2 is the average expected cumulative reward for 20rollouts in the simulated environment over three random seeds.We also present the average results of all drivers for different mod-els. The findings, as detailed in , demonstrate that MODAsignificantly surpasses the baselines, substantiating the effective-ness of our robust MDP construction and our approachs abilityto address distributional shifts. Notably, MODA, which excludesthe GAN component from MODA, exhibits inferior performance,underscoring the critical role of GAN in achieving a robust MDP",
  ": Impact of hyper-parameters on MODA": "within MODA. Furthermore, MODA can successfully enhance pol-icy performance across drivers of varying skill levels, with evenmedium and random drivers whose behavior policies are highlysuboptimal achieving outcomes comparable to those of the expertdriver. This underscores MODAs capacity to markedly improvepolicy effectiveness across diverse tasks.Results of Question (2). To assess the efficacy of the ContrastiveData Sharing (DS) method in MODA, we conducted a comparativeanalysis against alternative data sharing strategies, including NoSharing, Sharing All, and UDS. As shown in , for a certaindriver, we run different offline RL models including MODA, CQL,BCQ and BEAR on the effective datasets constructed by ContrastiveDS, No Sharing, Sharing All and UDS. We find No Sharing leads topoor performance mainly due to the lack of data for the target driver.Naively Sharing All will introduce a lot of bias to the target driverand result in low rewards. UDS relabels the rewards before sharingdata which degrades the RL performance. In contrast, all offline RLmodels achieve the highest rewards on the effective dataset builtby Contrastive Data Sharing, which indicates our Contrastive DataSharing can successfully share similar data from other tasks withthe target task by learning effective data representations, ensuringthat the shared data reflect comparable behaviors, preferences, anddecision-making logic.Results of Question (3). We also evaluate how different amountsof data shared with the target task by Contrastive Data Sharingaffect the MODA performance. As shown in , we find MODAexhibits varying performance across drivers of different expertiselevels when a distinct number of transitions are shared with thetarget driver. For example, the expert driver needs the least numberof shared data and achieves the best performance, and the randomdriver needs the most number of shared data to achieve a betterperformance. This results align with the real-world cases where itsmuch easier for an expert driver to get high rewards if the shareddata is effective, and for random drivers, they usually need moreeffective data to improve their policies.Besides, we also evaluate how different hyperparameters includ-ing the number of transitions in a sub-trajectory, positive range, the margin and the training epochs affect our MODA perfor-mance. As shown in (a), we find the MODA performancedecreases if a sub-trajectory contains more transitions, since a longsub-trtajectory will contain different spatial-temporal states andactions and thus introduce more bias to the model. As shown in(b), we find large positive range will degrade the perfor-mance since there is no guarantee that positive pairs will exhibit",
  "RELATED WORK": "Offline RL. Offline RL is a data-driven RL framework thatlearns better policies from previously collected datasets withoutfurther environment interaction. It shows promising performanceacross various domains, including robotic control , NLP ,and healthcare . However, offline RL faces the challenge of distri-butional shift, where the learned policy diverges from the behaviorpolicy, leading to erroneous value backups . To address thisproblem, some offline RL algorithms introduce constraints on thepolicy or on the learned Q-function ,to ensure the learned policy does not stray excessively from the be-havior policy. Additionally, model-based offline RL strategies havebeen explored to mitigate distributional shift, such as constructinga conservative MDP or implementing reward penalties .However, all these works are not applicable in our multi-task urbansettings since they did not consider the practical data issues (e.g.,data scarcity and heterogeneity) and the spatial-temporal nature ofurban decisions.Multi-Task RL. Multi-task RL learns multiple tasks simultaneouslyto improve generalization and learning efficiency in RL settings.The prior work utilize the transfer learning concept, whichshares a \"distilled\" policy across tasks to encourage positive transferwhile maintaining task-specific policies. More recent approacheshave focused on leveraging meta-learning for multi-task RL. Finnet al. introduced Model-Agnostic Meta-Learning (MAML), aversatile framework applicable to multi-task RL, demonstratingsubstantial improvements in learning efficiency. Similarly, Hesselet al. proposed the PopArt normalization technique to stabilizevalue estimates across varying reward scales in multi-task settings.Besides, the works related to multi-task offline RLmainly focus on goal-conditioned RL. In contrast, in this paper, wefocus on the multi-task urban setting.RL with data sharing. Sharing data across tasks has been demon-strated as highly beneficial in both multi-task RL andmeta-RL . Prior works have explored various data-sharingapproaches, such as utilizing learned Q-values , domainknowledge , or distances to goals in goal-conditioned RL set-tings . Another approach UDS treats all unlabeleddata as having zero reward to facilitate data sharing. Moreover,",
  "CONCLUSION": "In this paper, we aim to enhance diverse human decision-makingprocesses in an urban environment using offline RL which canlearn policies from a static dataset pre-collected by a certain be-havior policy. However, standard offline RL faces two significantchallenges: (i) data scarcity and heterogeneity, and (ii) distributionalshift. To address both challenges in a multi-task offline RL settingwhere learning the policy for each human agent can be viewedas a task, we introduce MODA a Multi-Task Offline Reinforce-ment Learning with Contrastive Data Sharing approach. MODAincludes a new contrastive data sharing method which can extractlatent representations of human behaviors by contrasting positiveand negative data pairs. Moreover, MODA develops a novel model-based offline RL algorithm. This algorithm constructs a robust MDPby integrating a dynamics model with a Generative AdversarialNetwork (GAN). Once the robust MDP is established, any onlineRL or planning algorithm can be applied. Extensive experimentsconducted in a real-world multi-task urban setting has validatedthe effectiveness of MODA. This work was supported in part by NSF grants IIS-1942680 (CA-REER), CNS-1952085 and DGE-2021871. Jun Luo is supported byThe Innovation and Technology Fund (Ref. ITP/069/23LP).Disclaimer: Any opinions, findings, conclusions or recommenda-tions expressed in this material/event (or by members of the projectteam) do not reflect the views of the Government of the HongKong Special Administrative Region, the Innovation and Technol-ogy Commission or the Innovation and Technology Fund ResearchProjects Assessment Panel.",
  "Damien Ernst, Pierre Geurts, and Louis Wehenkel. 2005. Tree-Based Batch ModeReinforcement Learning. J. Mach. Learn. Res. 6 (dec 2005), 503556": "Benjamin Eysenbach, Xinyang Geng, Sergey Levine, and Ruslan Salakhutdinov.2020. Rewriting History with Inverse RL: Hindsight Inference for Policy Improve-ment. CoRR abs/2002.11089 (2020). arXiv:2002.11089 Benjamin Eysenbach, Xinyang Geng, Sergey Levine, and Ruslan Salakhutdinov.2020. Rewriting History with Inverse RL: Hindsight Inference for Policy Improve-ment. CoRR abs/2002.11089 (2020). arXiv:2002.11089 Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. In Proceedings of the 34th Inter-national Conference on Machine Learning - Volume 70. 11261135.",
  "Jonathan Ho and Stefano Ermon. 2016. Generative adversarial imitation learning.Advances in neural information processing systems 29 (2016)": "Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, gataLapedriza, Noah Jones, Shixiang Gu, and Rosalind W. Picard. 2019. Way Off-PolicyBatch Deep Reinforcement Learning of Implicit Human Preferences in Dialog.CoRR abs/1907.00456 (2019). arXiv:1907.00456 Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, EricJang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, andSergey Levine. 2018. QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation. CoRR abs/1806.10293 (2018). arXiv:1806.10293 Dmitry Kalashnikov, Jacob Varley, Yevgen Chebotar, Benjamin Swanson, RicoJonschkowski, Chelsea Finn, Sergey Levine, and Karol Hausman. 2021. MT-Opt: Continuous Multi-Task Robotic Reinforcement Learning at Scale. CoRRabs/2104.08212 (2021). arXiv:2104.08212 Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims.2020. MOReL: Model-Based Offline Reinforcement Learning. In Proceedingsof the 34th International Conference on Neural Information Processing Systems(Vancouver, BC, Canada) (NIPS20). Curran Associates Inc., Red Hook, NY, USA,Article 1830, 14 pages.",
  "Yifan Wu, George Tucker, and Ofir Nachum. 2019. Behavior Regularized OfflineReinforcement Learning. CoRR abs/1911.11361 (2019). arXiv:1911.11361": "Tianhe Yu, Aviral Kumar, Yevgen Chebotar, Karol Hausman, Chelsea Finn, andSergey Levine. 2022. How to Leverage Unlabeled Data in Offline ReinforcementLearning. CoRR abs/2202.01741 (2022). arXiv:2202.01741 Tianhe Yu, Aviral Kumar, Yevgen Chebotar, Karol Hausman, Sergey Levine,and Chelsea Finn. 2021. Conservative Data Sharing for Multi-Task Offline Re-inforcement Learning. CoRR abs/2109.08128 (2021). arXiv:2109.08128 Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, SergeyLevine, Chelsea Finn, and Tengyu Ma. 2020. MOPO: Model-based Offline PolicyOptimization. In Advances in Neural Information Processing Systems, Vol. 33.Curran Associates, Inc., 1412914142.",
  "A.1.1Contrastive network. Our dataset contains a large numberof taxi drivers seeking passenger trajectories. We train the con-trastive network by sampling triplets randomly from the dataset": "Specifically, we first select the target driver, set the size of the sub-trajectory, and the Positive range, and then select the Anchor, andthe Positive sub-trajectory in the manner described in the maintext. At this point, we iterate through all the trajectories of all theother drivers, and find the the negative sub-trajectory, thus formingthe triples. In the training process, for a triple, we forward prop-agate each of the three sub-trajectories to get embeddings, andthen compute the contrastive loss. In the training process, for atriple, we forward propagate each of the three sub-trajectories toget embedding, and then compute the contrastive loss.It is worth noting that the number of triples composed in thisway will be very large, so in order to train the model more efficiently,we will randomly filter the composition of triples to decide whetherto use it to train the model or not. Usually, we filter 10% to 20%, ifthe sub-trajectory contains more transitions, it means that we needto filter fewer triples because the GPU memory is limited. A.1.2Sharing data. Despite we have Contrastive models, sharingdata can be done in many ways. We choose Eq. (2) for data-sharingto improve efficiency. According to the Contrastive loss, the mostintuitive way to share data is to collect data as in the training process(without random filtering), and then form triples. then calculatethe Contrastive loss, and then set a threshold to decide whether toshare the current negative sub-trajectory that gets this Contrastiveloss or not. However, in the process of implementation, we foundthat this way of sharing data consumes a lot of arithmetic power,and at the same time, due to the huge number of triples, it is almostimpossible to check all negatives, so it is inefficient. Thats why weuse the Eq. (2) description for data analysis. This approach improvesthe efficiency of the whole data sharing by hundreds of times, andat the same time, it also reduces a hyperparameter (Contrastive losssharing threshold). Also, In the end, the data-sharing approach weadopted works well.",
  "We use transitions from effective dataset Deffto train GAN modelsand dynamics models": "A.2.1GAN. In GAN, both the Generator and Discriminator utilizefully connected neural networks with ReLU activation functions. Itis configured with a batch size of 32 and trained over 2000 epochs.The learning rates for the generator and discriminator are set to0.0005 and 0.0002, respectively. A.2.2Dynamics model. Dynamics model encompasses twomodels tasked with predicting rewards and subsequent states, re-spectively. The learning rate is set to 0.02, with a batch size of 32,and the model is trained for 2000 epochs.For state Sub-Model: This model predicts the next state giventhe current state and action. It has two fully connected layers withReLU activation functions. The input size is the sum of the statesize and action size, and the output size is the state size.For reward Sub-Model: This model predicts the reward giventhe current state and action. It has two fully connected layers with",
  "ReLU activation functions. The input size is the sum of the statesize and action size, and the output size is the reward size": "A.2.3Robust MDP construction. After completing the training ofthe GAN and dynamics models, we need to build a Robust MDP bycombining the GANs discriminators with the dynamics models. Itworks like this: first, we randomize an initial state that is randomlydrawn from the entire dataset in a transition Then the policy ob-tained by the subsequent algorithm generates an action based onthis state, and this state and action are fed to the dynamics models,which predicts the forward and next states and forms a transition.this transition is then this transition will be input to the discrimina-tor, which will output 0 or 1 to determine whether the transition isreal or not. If the discriminator outputs 0, it means that the currenttransition is false, and the environment will be automatically set tothe halt state and return to done, which represents the end of thecurrent episode. This halt state will set the reward to a very smallvalue as a penalty.About why the initial state is randomly selected instead of ran-domly generated. Because the environment we simulate is a con-tinuous environment with very high dimensionality, the randomlygenerated state has a high probability of being unrealistic and alsohas a high probability of being different from the distribution ofthe data that the model has learned, which will cause the RobustMDP to end the current episodes immediately. A.2.4Soft-Actor-Critic (SAC). This algorithm contains 2 models,SACActor and SACCritic. The SACActor model is responsible forpredicting action logits given the state, while the SACCritic modelestimates Q-values given the state-action pair. Additionally, a Re-playBuffer class is implemented to store and sample experiencesfor training. The hyperparameters used in training include the totalnumber of episodes (20000), batch size (64), discount factor (gamma= 0.99), soft update coefficient (tau = 0.005), and entropy regular-ization coefficient (alpha = 0.2). The replay buffer capacity is set to300000.",
  "A.3Passenger-seeking simulation and dataselection details": "Our dataset comprises trajectories collected from 17,877 drivers,however, the data quality of all these drivers varies, many of themsuffer from incomplete trajectories. And in our evaluation, we usemany transitions data from whole dataset for environment simula-tion, and we use 20 drivers (i.e., 20 tasks) for training all offline RLmodels (including MODA and all baselines), these drivers providethe most complete and good quality datasets. Briefly speaking, weare solving a 20-task offline RL problem in our experiment insteadof a 17,877-task problem. To simulate the real environment of thecity aiming to test the policies learned with our MODA and allbaselines, We created a simulated passenger-seeking environmentbased on the above taxi trajectory data. Specifically this model is anensemble model, which contains ensemble size dynamics models,each of which has the same structure as previously mentioned. Inour experiments, we set the ensemble size to 5. Most importantly,the data we use to train the ensemble model contains all the transi-tions in the dataset, which is several times more than the amount",
  "A.4Baselines": "For CQL, BCQ and BEAR, since they are all offline reinforcementlearning algorithms, they are trained directly on the dataset and donot need to be deployed in a certain environment like SAC. CQL. This model consists of three fully connected layers, eachfollowed by a rectified linear unit (ReLU) activation function. Theinput dimension of the network corresponds to the state space,and the output dimension corresponds to the action space. Thenetwork is trained using the Adam optimizer with a learningrate of 0.001. Additionally, the model sets alpha to 0.1, and trains1000 epochs with batch size 64. BCQ This model includes two neural networks: a Q-networkand an action generator. Both networks consist of three fullyconnected layers with ReLU activation functions. The Q-networktakes the state as input and outputs Q-values for each action,while the action generator takes the state as input and outputsa probability distribution over actions using the softmax func-tion. The model is trained using behavior cloning from expertdemonstrations, with data loaded from the provided buffer path.The training process runs for 1000 epochs with a learning rate of0.0001. BEAR. This model consists of two neural networks: a Q-networkand a policy network. The Q-network has three fully connectedlayers with ReLU activation functions, taking both state andaction as input. It outputs Q-values estimating the expected cu-mulative future rewards for each action. The policy network alsohas three fully connected layers with ReLU activation functions,taking only the state as input and outputting a probability distri-bution over actions using the softmax function. Both networksare optimized using the Adam optimizer with a learning rateof 0.001. Additionally, the model employs a discount factor of0.99 for future rewards and sets the MMD loss weight to 0.1. Thetraining loop iterates for 1000 epochs with a batch size of 64.",
  "A.5Detailed Settings of experiments on Impactof different number of transitions shared bycontrastive data sharing": "After getting effective dataset Deffagain, the number of transitionsit contains is fixed, so for comparison experiments, we labeledthe transitions that were shared in, and then randomly chose thespecified number of transitions to be used to construct the datasetused as the comparison test. If the specified number is larger thanthe total number of shared transitions, we select the remainingnumber of unshared transitions."
}