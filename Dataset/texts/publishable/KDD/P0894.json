{
  "Jos Centre for the Future ofIntelligence, University of CambridgeCambridge, UKVRAIN, ValGRAI, UniversitatPolitcnica de ValnciaValncia, Spain": ": Our proposed pipeline for predicting the performance of a new LLM on a new instance by testing on a few instances:starting from instance-level evaluation results of a set of LLMs, a reference set of instances is extracted (1). Then, we train ageneric assessor that predicts the performance of each LLM-instance pair, based on features intrinsic to the instance (e.g.,vector embeddings) and the performance of the considered LLM on the reference set (2). The performance of the new LLM on anew instance can be predicted by evaluating the new LLM on the reference set and applying the trained generic assessor (3).",
  "Abstract": "Predicting the performance of LLMs on individual task instances isessential to ensure their reliability in high-stakes applications. Todo so, a possibility is to evaluate the considered LLM on a set oftask instances and train an assessor to predict its performance basedon features of the instances. However, this approach requires eval-uating each new LLM on a sufficiently large set of task instancesto train an assessor specific to it. In this work, we leverage theevaluation results of previously tested LLMs to reduce the num-ber of evaluations required to predict the performance of a newLLM. In practice, we propose to test the new LLM on a small set ofreference instances and train a generic assessor which predicts theperformance of the LLM on an instance based on the performanceof the former on the reference set and features of the instance ofinterest. We conduct empirical studies on HELM-Lite and Kind-sOfReasoning, a collection of existing reasoning datasets that weintroduce, where we evaluate all instruction-fine-tuned OpenAImodels until gpt4-0125-preview. When predicting performanceon instances with the same distribution as those used to train thegeneric assessor, we find this achieves performance comparable to",
  "GenAI Evaluation KDD2024, August 25, 2024, Barcelona, Spain. 2024 Copyright held by the owner/author(s).This is the authors version of the work": "the LLM-specific assessors trained on the full set of instances. Ad-ditionally, we find that randomly selecting the reference instancesperforms as well as some advanced selection methods we tested. Forout of distribution, however, no clear winner emerges and the over-all performance is worse, suggesting that the inherent predictabilityof LLMs is low.",
  "Introduction": "Large Language Models (LLMs) are being used as components ofmultiple services and products, such as agents performing generalcomputer tasks , performing ML experiments , and evenoperating unmanned aerial vehicles . These systems typicallyquery an LLM on a specific instance of a task and use their outputto determine a sequence of actions. For some of these uses, it is",
  "GenAI Evaluation KDD2024, August 25, 2024, Barcelona, Spain.Lorenzo Pacchiardi, Lucy Cheke, and Jos Hernndez-Orallo": "essential to determine whether the output produced by the LLMon a specific task instance is correct (or, more generally, valid) before the subsequent steps are executed1. A nascent line ofresearch is addressing this problem by developing asses-sors, namely, independent modules that predict the correctness (ora continuous performance score) of an AI system on an instancebased on features intrinsic to the latter (such as linguistic featuresor sentence vector embeddings). Assessors can be specific to anAI system, or generic, in which case they also take as input in-formation on the AI system at hand and are trained to predict theperformance of different LLMs on different instances.Meanwhile, the rate at which new LLMs are released has dras-tically increased. Some providers, such as OpenAI, are iterativelyretiring old versions when new ones are released, forcing devel-opers to update the LLM version used in their applications (see). An even larger explosion is occurring in the open-sourceworld, fuelled by inexpensive fine-tuning techniques . To buildan assessor specific to a new LLM version, users must evaluate iton a sufficiently large set of task instances, causing the costs to risequickly when considering many LLM versions. On the other hand,the system information one might use to build a generic assessor,such as the number of parameters or statistics of the training dataor architecture, is not standardised across LLMs and unavailablefor proprietary models.As such, this paper investigates the following question: canwe combine information across LLMs to predict the perfor-mance of a new LLM on a new instance by relying only onobservational (or behavioural) features of the LLMs? In prac-tice, we propose to characterise each LLM by its performance on asmall set of reference instances and to build a generic assessor usingthose as system features. More precisely, we first select a smallset of reference instances from the labelled dataset on which pastLLMs were evaluated. Then, we train the generic assessor on theconcatenation of instance-specific features and the LLM-specificsuccess vector on the reference instances. Finally, to estimate theprobability of success of a new LLM on a novel instance, it sufficesto evaluate the former on the reference instances, concatenate itsperformance to the features of the instance, and apply the trainedgeneric assessor.In our empirical studies, we rely on HELM-Lite , which pro-vides instance-level results for 30 LLMs from different providers(at the time we conducted our experiments), and a collection ofpreviously existing datasets we introduce, named KindsOfReason-ing, on which we evaluated the full set of instruction-followingmodels from OpenAI until gpt4-0125-preview. We only considertasks with binary correctness score (thus discarding the datasetsin HELM-Lite that do not satisfy this) and therefore build binaryassessors.We train specific assessors using different prompt features andfind that OpenAI embeddings lead to better in-distribution per-formance than simpler methods such as Word2vec , FastText, and n-grams. Although this analysis is not the main focus ofour work, it is a valuable tangential contribution. Subsequently,we build generic assessors using various methods to select the",
  "Notice that this cannot rely on the ground truth of the task instance, as that is notavailable in practical use cases (otherwise, there would be no need to query the LLM)": "reference instances and combine the performance on these withthe instance-specific features. When predicting performance on in-stances with the same distribution as those used to train the genericassessor, we find the latter to perform comparably to the specificassessors, which require the LLM to be evaluated on many moreinstances. Additionally, we find that a random selection of referenceinstances performs as well as the advanced selection methods wetested. However, in out-of-distribution scenarios, the predictivepower of all assessors declines significantly, indicating a lack ofgeneral predictability in LLMs.In essence, the main contributions of our work are the following:",
  "We study the performance of various methods for selectingthe reference instances and combining their performancewith instance-specific features to build the generic assessor": "Finally, we introduce the KindsOfReasoning collection ofexisting datasets testing various kinds of reasoning and, inthe spirit of making instance level results available , wepublicly release the raw outputs and the evaluation resultsof all instruction-tuned models from OpenAI. To the best ofour knowledge, this is the first public release of its kind. The rest of the paper is organised as follows: reviewsrelated works in the area of predicting the performance of largelanguage models (LLMs). In , we describe our methodology. presents our empirical studies, where we compare the per-formance of the generic assessor with that of independent assessors,and study how well the generic assessor can select the most suitableLLM for a task. In , we conclude the paper and discuss thelimitations of our study and directions for future work (5).",
  "Related work2.1Instance-level prediction of success of AIsystems": "The motivation for our work follows , which advocates for theimportance of instance-level success predictions for AI systemsand coins the term predictable AI; in particular, they highlighthow ensuring predictability should be prioritised over increasesin average performance for risky use cases, and how having thiscould help with compliance with upcoming regulatory frameworks,such as the EU AI Act .Following this motivation, introduces the concept of an asses-sor model, which accompanies an ML system and estimates the prob-ability of success of the system on individual instances. In particular,they discuss how an assessor can be trained on the evaluation re-sults of the ML system on test data (i.e., which has not been usedfor training the ML system). Finally, applies this idea to LLMs,by showing how a smaller external model can be used to predict theperformance of a bigger and more expensive LLM on individual in-stances without passing the latter through the model. They also findit possible to reject almost half of the failure cases before runningmuch larger LLMs, resulting in a significant saving of compute.",
  "Predictability of aggregated benchmarkscores from LLM-specific features": "Two works studied the extent to which an LLMs aggregateperformance on BIG-Bench tasks can be predicted using infor-mation on the LLM such as number of parameters or the amount ofused compute. In contrast, our work does not rely on these quan-tities, which are often unavailable, instead characterising LLMsaccording to their performance on reference samples. Moreover,while these works focus on predicting aggregate performance, ourwork and the ones mentioned in the previous subsection provideinstance-level predictions for new unlabelled instances.",
  "Extracting LLM-specific features fromexisting evaluations": "Recently, built observational scaling laws that link perfor-mance on complex downstream tasks to hypothesised latent capabil-ities, whose values can be inferred by decomposing the performanceof various LLMs on different benchmarks into components linkedby a log-linear relation with compute measures for LLM training.Doing so allows us to combine information across different LLMfamilies, which may differ for their efficiency in converting raw com-pute into benchmark scores. Once this relation is established, theperformance of a new model on downstream tasks can be predictedby knowing its performance on simple benchmarks and its computecost. also select a subset of LLMs that maintains high predictionaccuracy while reducing cost by requiring the evaluation of perfor-mance on downstream tasks for fewer models. Their work is similarto ours in determining LLM-specific features by using evaluationresults of multiple LLMs and using them to predict the performanceof a new LLM. However, the aim of our work is to predict the perfor-mance of the new LLM on a specific instance with as few evaluationsas possible, while the aim of is to avoid the cost of evaluatingcomplex downstream tasks and predict the performance on the lat-ter from results on simple benchmarks and compute measures. Assuch, the LLM-specific features they use (the latent capabilities) areobtained from the performance of the new LLM on simple bench-marks (which assumes to be available), while our method onlyneeds to evaluate the LLM on a small number of instances. Moreover,our method can be applied to new instances for which no groundtruth is available, while the simple benchmarks and the downstreamtasks employed in must have a grading mechanism.",
  "Predicting performance by benchmarksubsampling": "Several works share the motivation of reducing the number of eval-uations (and hence the cost) needed to evaluate a LLM. For instance,a Lite version with a reduced number of tasks was introducedalongside the larger BIG-Bench benchmark ; however, the wayin which the task selection was done is unknown, to the best ofour knowledge. Similarly, HELM-Lite is a revised and reducedversion of HELM . However, both of these perform the reduc-tion at the level of tasks of which the benchmark is constituted.Instead, subsample a dataset by clustering models confidenceto predict the overall accuracy on the whole dataset, while MixEval extracts a subset of instances from various benchmarks which is most predictive of the performance on Chatbot Arena2, an onlineplatform performing pairwise comparison of LLM outputs. Closerto our work is TinyBenchmarks , which selects informativeinstances from HELM-Lite and estimates the performance of a newLLM on the whole benchmark by evaluating it only on those in-stances. In particular, TinyBenchmarks uses Item Response Theory(IRT) on the matrix of success of each LLM on each instance presentin the HELM-Lite dataset to extract a vector of item demands andLLM capabilities. Then, it uses either the item demands or the rawLLM success on each instance to build a representative subset ofinstances by clustering the items and taking the cluster centroids.Similarly to our approach, a new LLM is then only evaluated onthe representative subset; however, in contrast to our work, theyaim to predict the aggregate score on the benchmark, while wefocus on predicting instance-level performance. In practice, theirIRT method provides instance-level predictions (which the authorsaggregate), but these predictions are limited to instances on whichprevious LLMs have been evaluated (as this is necessary to obtainthe item demands), which requires access to the ground truth. Incontrast, our approach is applicable to new instances for whichwe do not know the ground truth, as the trained assessor does notrequire any information beyond the intrinsic features of test in-stances. A similar work to is metabench , which considered6 different datasets, and performed a two-step procedure (randomsampling for each dataset, followed by item selection based on theFisher information matrices of IRT item parameters) to extract asmall set of instances, the performance on which accurately predictsaggregate performance on the 6 datasets. As they fit the IRT modelonly the pre-selected instances, their method is unable to predictinstance-level performance. Finally, despite not tackling predictabil-ity directly, finds that the vector of successes of different LLMsis correlated across instances belonging to 4 benchmarks, and, forone of those benchmarks, the similarity between the embeddingsor a pair of instances predicts the similarity between the successvectors; this suggests that patterns in success across LLMs can befound and related to the embeddings.",
  "Evaluations of reasoning in LLMs": "found reasoning to be one of three factors in the capabilitiesof LLMs. Indeed, reasoning in LLMs has been extensively studied:see for a survey on LLM reasoning evaluations and fora broader survey also encompassing ways to improve and elicitreasoning in LLMs.Recently, several collections of reasoning datasets have beenintroduced. GLoRE collects 12 logical reasoning datasets withthree different types of tasks (multiple choice, natural languageinference, and binary answers). Similarly, LogiGLUE collects24 datasets related to inductive, deductive and abductive reasoning,with four different types of tasks (the same ones as GLoRe andfree-form question answering); they only selected datasets that donot require external domain knowledge, but some of these datasetsare poorly formatted. Finally, CALM-Bench is a collection of6 diverse tasks requiring both causal reasoning and knowledge.KindsOfReasoning, the collection we introduce combining previ-ously existing datasets testing various kinds of reasoning, partly",
  "Methodology": "Let us denote by L = {, = 1, . . . ,}, a set of trained LLMs.Moreover, let D = {(,), = 1, . . . , } be a test dataset used toevaluate the performance of the LLMs, with denoting instanceindex, the input to the LLM (i.e., the prompt) and the targetvalue (i.e., the expected completion by the LLM). Further, we willdenote by () the output produces when given as input3 and by , a binary value indicating the correctness of ()with respect to . The correctness , can be defined in multiplemanners (for instance, exact match or whether is a substringof ()); the most suitable manner depends on the consideredtask, but in general the aim of , is to capture what a human judgewould perceive as a correct answer4.In the following, we first frame the problem of predicting thecorrectness , and then discuss our main contribution, namely aframework to predict the performance of a new LLM by evaluatingit on a small subset of instances.",
  "Predicting success of a LLM using featuresintrinsic to the prompt": "To begin with, let us now consider a single LLM, say 1; our aim isto train a classifier (termed assessor) to predict the performance1, from the prompt . To do so, we split the test dataset D intodifferent splits used to train, validate and evaluate the assessor ,denoted as Dtrain, Dval and Dtest, such that D = train Dval Dtest and Dtrain Dval = Dval Dtest = Dtrain Dtest = . Ina real-world scenario, Dtest will represent instances for which wedid not evaluate the considered LLM (and for which we may nothave access to the ground truth); in contrast, available evaluationresults are split into Dtrain and Dval.In practice, we can extract some numerical features () fromthe textual prompt ; we use intrinsic features, i.e. features thatare defined independently of the problem at hand (such as thenumber of negations or the vector embeddings of the sentence).Formally, we consider a loss function and a family of classifiers, where denotes the parameters of the classifier (for instance,the weights in a logistic regression classifier), and aim to minimise",
  "over using some optimisation algorithm; we can then select thebest hyper-parameters for solving the above problem using the per-formance on the validation data Dval, leading to picking a classifier": "3As LLMs are stochastic, ( ) is in general a random variable, and so is ,. In ourempirical study, we sample the LLMs at 0 temperature, but, even so, there is still aresidual amount of stochasticity, even though the reason for this is unclear .4Particularly in the case of free-form question answering, it can be tricky to find aformulation that always matches what a human judge would perceive as a correctanswer.",
  "Predicting success by evaluation onreference instances": "Now, consider the case in which we have previously evaluatedsome LLMs on Dtrain and Dval. We are interested in predictingthe performance of a new LLM, say new on new instances Dtest.Using the approach in .1, we could test the new LLM onall instances in Dtrain and Dval and train an assessor specific tothat LLM. Instead, we want to leverage the information containedin the available evaluation results for previous LLMs to predict theperformance of new on Dtest without evaluating it on the fullDtrain (and assuming that we do not have access to the labels inDtest, which prevents us from evaluating the other LLMs on it).Thus, we build a generic assessor, namely a classifier that predictsthe success , from the pair (, ). In practice, let us split theLLMs for which full evaluation results are available into a train-ing and validation split Ltrain and Lval. For each pair (, ) Ltrain Dtrain, we concatenate the prompt-intrinsic features ()with LLM-specific features () and aim to fit a classifier thatminimises",
  "Dtrain( ((), ()),,)(2)": "over . Similarly to what we did before (.1), we keep theperformance of Lval on Dval to perform model selection, leadingto a trained classifier . Then, the performance of new on aninstance new Dtest can be obtained as ((new), (new)).The LLM-specific features () could include statistics on thetraining data of and architectural information (for example,number of attention layers and parameters). However, the high va-riety of hyperparameters involved in the definition and training ofLLMs and the unavailability of detailed information on proprietarymodels makes defining broadly informative features hard, if notimpossible. To circumvent this problem, we propose to use the per-formance of on a small set of reference instances Dref Dtrain",
  "on Dref to predict their performance on news instances Dtest. See for a graphical description of our method. Next, we discussvarious methods to determine Dref": "3.2.1Selecting the reference instances. In order to select the mostinformative instances (,) Dtrain to form Dref , we can useinformation intrinsic to the instances as well as the evaluationresults of Ltrain on Dtrain (while keeping aside Dval and Lval tochoose the best selection method; see .2.3). In general,let us denote by R a feature vector associated to andX R| Dtrain| the matrix whose columns are . Finally, let usdefine Ztrain = (,): Ltrain,: Dtrain. We attempt using thefollowing features:",
  "The binary successes/failure vector on Ltrain, which identi-fies X = Ztrain and for which = train": "The item demands obtained by applying the IRT approachin (discussed in ), which obtains a set of itemdemands and LLM capabilities starting from the success ma-trix Ztrain. Thus, we set to be the obtained item demands,whose size can be chosen by the user (we fix this to = 10following ).For all possible choices of X above, we apply the following meth-ods to determine the reference instances: Clustering on intrinsic features: we perform KMeansclustering on the columns of X and, for each identified cluster,add the instance closest to the cluster centroid to Dref. Thepre-specified number of clusters determines the number ofselected instances. Factor Analysis (FA): FA decomposes X = WH + E, whereW R is the loading matrix, H R| Dtrain| is a matrixwhose columns are the latent factors for each of the sam-ples, E is a matrix of Gaussian noise and is the number ofhidden factors. The features for each instance is assumed tobe independent from the other instances given the matrixW. In practice, we first fit FA with a high number of factors,set to the number of eigenvalues of the correlation matrixXX which are larger than 1 and re-fit FA with the varimaxrotation method . Then, we select the required numberof reference instances by picking, for each = 1, . . . ,, anapproximately equal number of instances with the highestvalues of |, |6. In total, we have 6 ways of selecting Dref (3 sets of features times2 selection methods), two of which (clustering on success/failuresand IRT item parameters) correspond to the selection method usedin . We compare these methods with a random reference subset;moreover, we also draw 20 random reference subsets, fit an asses-sor using the performance on the reference instances, and pick therandom subset that leads to the highest performance (random bestof 20). 3.2.2Predicting success by concatenating intrinsic features and per-formance on the reference instances. Once we select the referenceinstances Dref, we can extract the success of each LLM on Dref todefine () = (,)Dref. We can then concatenate this to thefeature vector () (which does not need to be the same used forselecting the reference instances in .2.1) and train a genericassessor aiming to minimise Eq. (2). Notice how the features ()can also rely on the reference dataset, as that is fixed for all newLLMs: for instance, we also attempt using a measure of similaritybetween the vector embeddings of and each of the instances inDref as ().",
  "For instance, if we want to select 35 reference instances and = 10, we select thes corresponding to the top 4 |, | for = 1, . . . , 5 and those with the top 3 for = 6, . . . , 10": "the intrinsic features . We can also choose multiple families ofclassifiers and hyperparameters of the optimisation algorithmto minimise Eq. (2). As such, we pick the combination of optionswhich best predicts the performance of the validation LLMs Lval on the validation data Dval. Hence, once we want to predict theperformance of a new LLM new on a new instance new Dtest,we only need to evaluate new on Dref and apply the trainedgeneric assessor. In our empirical studies below, we will test eachmethod on multiple new LLMs, which we group into Ltest.",
  "We consider two collections of datasets in our experiments7:": "HELM-Lite , a revised and reduced version of the popu-lar HELM , which includes 10 different scenarios (i.e.,datasets), some of which are stratified into sub-scenarios. Ofthose, we keep the scenarios and subscenarios for which theperformance metric is binary, and further discard those forwhich different LLMs were tested with a different numberof few-shot examples; the resulting subset spans 6 scenar-ios for a total of 4285 instances. The list of included anddiscarded scenarios and sub-scenarios can be found in Ap-pendix A.1. On this benchmark, the results for 30 LLMs fromdifferent families were available at the time we conductedour experiments (see ). KindsOfReasoning, a collection that we introduce in thispaper, which is aimed at testing various kinds of reason-ing (logical, common sense, inductive, deductive, abductive,counterfactual, causal, analogical, spatial and arithmetic rea-soning). Our collection includes 22 different datasets withvarying number of instances, for a total of 37,529 instances.On this dataset, we tested all instruction-tuned models re-leased from OpenAI, from text-ada-0018 togpt-4-0125-preview, for a total of 14 LLMs (see ).The instance-level outputs of all models will be released9,in the spirit of . To the best of our knowledge, this is thefirst collection of instance-level results covering all versionsof a given model family from such a large time window,and we hope other researchers can find insights in this data.We provide more information about the construction of thiscollection in Appendix A.2. For each of these collections, we repeat all our experiments con-sidering different choices for the train, validation, and test splitsDtrain, Dval and Dtest. In particular, we consider a random split,where the various splits are sampled by shuffling together all in-stances of all datasets. In addition, we consider multiple out-of-distribution (OOD) splits, where we keep one set of datasets asDtest (according to some criteria), and Dtrain and Dval are ob-tained from randomly shuffling the other ones. In this way, thedata used to train and select the best assessor (both in the genericand specific setup) have the same distribution, which is however",
  "for each dataset collection. Analogously to how we selected theOOD splits, we make Ltest as different as possible from Ltrain and": "Lval: concretely, we select LLMs from two producers as Ltest forHELM-Lite and all versions of gpt4 models for KindsOfReasoning.In this way, we test the performance of our proposed methodologyin the hard case where the new LLM we want to predict performancefor is substantially different from the previously seen ones. TheLLM splits are given in .Notice how the diversity of LLMs in HELM-Lite is higher thanthat in KindsOfReasoning, as the latter has been evaluated on asingle family of models. This is interesting as it allows us to under-stand how the performance of our proposed method changes whenconsidering a broad or narrow set of LLMs.",
  "Considered prompt features": "Our methodology, discussed in , relies on choosing a trans-formation that converts a given prompt into a set of numericalfeatures = (), where we refer to these features as intrinsicas they do not depend on the particular LLM whose performancewe are interested in predicting (as mentioned in .2.2, in thegeneric assessor setup, we allow the intrinsic features to dependon the set of reference instances, as the set of reference instancesis fixed for all LLMs in Ltest).Empirically, we attempted using the following features:",
  "the Word2vec and FastText word embeddings, whichcompute a vector for each word of the prompt which weaverage to obtain a feature vector for the whole prompt;": "the 1-gram vectors, which are obtained as a measure of thefrequency of words in a specific prompt normalized overthat of the words in the entire set of training prompts.We studied the performance of these in the specific assessor setup(complete results in Appendix A.3) and found that OpenAI embed-dings perform better more frequently. Moreover, the OpenAI embed-dings obtained from the endpoint text-embedding-3-large weretrained using Matryoshka Representation Learning , whichallows them to be truncated (by removing the final elements ofthe vector) without the embedding losing its concept-representing",
  "instances is all you needGenAI Evaluation KDD2024, August 25, 2024, Barcelona, Spain": "properties. As such, we investigate the performance of the specificassessor by truncating the OpenAI embeddings (Appendix A.4)and we found that the performance saturates using 1024 (out ofa total of 3072) embeddings. As such, our experiments on genericassessors use the first 1024 elements of the OpenAI embeddings.Additionally, in the generic assessor framework, we also attempt touse the cosine similarity between the embeddings of each elementof the selected Dref and the considered instance as ().",
  "Metrics and other details": "We use the Area Under the Curve (AUC) as a metric for the perfor-mance of the generic and specific assessor. The AUC measures howwell a binary probabilistic classifier (i.e., a classifier that provides aprobabilistic prediction for a binary variable) discriminates betweenthe two classes: a classifier whose assigned probabilities for the twoclasses do not overlap achieves the maximum value AUC = 1, whilea classifier assigning random values to the two classes achievesAUC = 0.5. We employ the AUC as its extreme values are insensitiveto the proportion of positive and negative samples in the dataset,and it can therefore be used to compare results across various sce-narios (in our case, the various train/validation/test splits and thetwo dataset collections). However, AUC is insensitive to monotonictransformation of the output probabilities and this implies that aclassifier achieving AUC = 1 can be miscalibrated (for instance, aclassifier assigning probability 0.51 to all positive samples and 0.49to all negative samples achieves AUC = 1, but its predictions aremiscalibrated).We test various values of the size of Dref (results in Appen-dix A.6) and we find that the performance on the test set saturatesaround 100 reference instances; as such, all results reported in themain text are obtained with that value.Next, for any data split and any choice of Dref in the genericassessor setup, we attempt to use various classifiers as assessors(logistic regression with 2 and 1 penalty and xgboost). Further-more, as mentioned in .2.2, in the generic assessor setup,we attempt to use the OpenAI embeddings as well as their cosinesimilarity to those of the elements of Dref as instance features. Toselect the best method, we do the following:",
  "in the specific assessor setup, we compute the AUC of eachclassifier trained on each test LLM on Dval, pick the onewith the highest value, and report the AUC of that classifieron Dtest": "In the generic assessor setup, for each data split, we eval-uate the AUC of each combination of classifier, choice ofDref and instance features on Dval for each LLM in Lval.To select the best combination, we compute the win rateof each combination for each validation LLM and pick thecombination with the highest average win rate over Lval",
  "correspondingly and requires evaluating each test LLM on Dref. Thelatter case is therefore fully representative of the case of a new LLM": "evaluated on a new instance. The winning combination for eachdata split is reported in . Interestingly, for multiple data splits,the randomly sampled Dref performs better than those determinedaccording to the advanced criteria in .2.1. While surprisingat first, other works had found that benchmarks canbe reduced by random sampling for multiple purposes.In terms of classifier, instead, XGBoost generally performs better.Finally, using similarity between the embeddings of the referenceinstances and those of the considered instance more frequentlyperforms better than directly using the latter as ()."
}