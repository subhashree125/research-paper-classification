{
  "ABSTRACT": "Multimodal electronic health record (EHR) data can offer a holisticassessment of a patients health status, supporting various predic-tive healthcare tasks. Recently, several studies have embraced themultitask learning approach in the healthcare domain, exploitingthe inherent correlations among clinical tasks to predict multi-ple outcomes simultaneously. However, existing methods necessi-tate samples to possess complete labels for all tasks, which placesheavy demands on the data and restricts the flexibility of the model.Meanwhile, within a multitask framework with multimodal in-puts, how to comprehensively consider the information disparityamong modalities and among tasks still remains a challenging prob-lem. To tackle these issues, a unified healthcare prediction model,also named by FlexCare, is proposed to flexibly accommodate in-complete multimodal inputs, promoting the adaption to multiplehealthcare tasks. The proposed model breaks the conventional par-adigm of parallel multitask prediction by decomposing it into aseries of asynchronous single-task prediction. Specifically, a task-agnostic multimodal information extraction module is presentedto capture decorrelated representations of diverse intra- and inter-modality patterns. Taking full account of the information disparitiesbetween different modalities and different tasks, we present a task-guided hierarchical multimodal fusion module that integrates the",
  "Corresponding author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08 refined modality-level representations into an individual patient-level representation. Experimental results on multiple tasks fromMIMIC-IV/MIMIC-CXR/MIMIC-NOTE datasets demonstrate theeffectiveness of the proposed method. Additionally, further anal-ysis underscores the feasibility and potential of employing sucha multitask strategy in the healthcare domain. The source code isavailable at",
  "electronic health record, healthcare prediction, multimodal data,multitask learning": "ACM Reference Format:Muhao Xu, Zhenfeng Zhu, Youru Li, Shuai Zheng, Yawei Zhao, Kunlun He,and Yao Zhao. 2024. FlexCare: Leveraging Cross-Task Synergy for FlexibleMultimodal Healthcare Prediction. In Proceedings of the 30th ACM SIGKDDConference on Knowledge Discovery and Data Mining (KDD 24), August2529, 2024, Barcelona, Spain. ACM, New York, NY, USA, 11 pages.",
  "INTRODUCTION": "To comprehensively assess a patients health status, clinical prac-tice often employs a variety of methods to capture diverse patientinformation, resulting in multimodal EHR data that comprise bothstructured and unstructured data. These diverse multimodal EHRdata can underpin multiple predictive tasks in the clinical, facilitat-ing the identification of high-risk patients for early intervention.For instance, time-series data such as vital signs and laboratorytest results are frequently utilized for clinical risk and outcomeprediction . Medical images (e.g., X-rays and computerized",
  "(b)(c)": ": (a) Illustration of the multimodal data and the multi-task predictions during a patients admission; (b) The numberof samples from multiple tasks that depend on time-seriesdata in the MIMIC-IV dataset; (c) The number of sampleswith different modality data in the MIMIC-IV dataset. tomography scans), are instrumental in detecting, localizing, andclassifying diseases relevant to patients . Compared to struc-tured information, textual clinical notes furnish comprehensive in-sights into a patients medical history, symptoms, and the reasoningfor diagnoses, offering a more macroscopic and holistic perspec-tive . Considering the complementary nature of informationwithin multimodal EHR data, some previous methodologies havebeen developed to integrate various modalities for enhancing theaccuracy of clinical event prediction . Further-more, some studies leverage the inherent correlationsamong clinical tasks, employing multitask learning approaches tosimultaneously predict multiple tasks.However, current multitask models for healthcare prediction typ-ically necessitate complete labels for all tasks . Such demandfor data is exceedingly stringent, especially within the medical do-main. Indeed, the requirements for EHR data manifest considerablevariation across different clinical tasks for the same patient, encom-passing distinct time spans and modalities, as illustrated in (a). For example, in-hospital-mortality prediction commonly relieson time-series data from the first 48 hours after a patient is admittedto the hospital, while disease diagnosis mandates the presence ofimage modality, with no restrictions on the other modalities. Hence,requiring identical input data and complete labeling for multitaskconstitutes a significant waste of already scarce healthcare data.The statistical information for the MIMIC-IV dataset pre-sented in (b) and (c) illustrates that samples with all tasklabels and samples with all modalities represent only a small frac-tion of the total dataset. In summary, to overcome the limitations ofprevious models that require completeness in data and labels, mul-timodal multitask healthcare prediction models face the followingchallenges:",
  ": (a) Single-task model; (b) Conventional multi-taskmodel; (c) Our proposed flexible multi-task model": "Challenge 1: How to develop a flexible model capable ofsupporting multimodal inputs and adapting to various het-erogeneous tasks, without requiring comprehensive labelsfor each sample across all tasks? Flexibility here is manifested inthe model not requiring each sample to possess inputs for all modal-ities and labels for all tasks. An intuitive approach is to deconstructparallel multitask simultaneous predictions into asynchronous mul-tiple single-task predictions, where each sample corresponds to asingle task label, thus satisfying the data differences in terms oftime spans and modalities used between multiple tasks for the samepatient. As shown in , the hallmark of a flexible multitaskmodel lies in employing a unified model to process multiple hetero-geneous datasets, encompassing both heterogeneous multimodalinputs and heterogeneous tasks. In the realm of universal naturallanguage and visual understanding, MT-DNN and Unit have made attempts with a multitask approach using heteroge-neous datasets. However, current studies still lack suchconsideration in the field of healthcare prediction.Challenge 2: How to deal with the information disparitiesamong modalities and tasks comprehensively within a multi-task framework? Three forms of information disparity need to beconsidered in the unified framework: modality-modality disparity,task-task disparity, and task-data disparity. Concretely, in the het-erogeneous EHR data, different modalities may encapsulate distinctaspects of a patients health status. It is imperative that this infor-mational disparity present among multimodal data be taken intoconsideration. Furthermore, different clinical tasks focus on EHRdata from various time spans and modalities of the patient, posingchallenges to the construction of the multitask model. On the otherhand, due to the extensive sharing of modules in multitask mod-els, negative interference may arise when inter-task correlationsare weak. In multimodal multitask models, the shared modules arenot only cross-task but also cross-modal, further complicating theabove issue.By jointly considering the above issues, we propose a modelleveraging cross-task synergy for flexible multimodal healthcareprediction (FlexCare). Our main contributions are summarized asfollows: To the best of our knowledge, this work is the first attempt tostudy a unified healthcare prediction model that flexibly supports",
  "multimodal inputs and adapts to healthcare multitask. It is em-powered with multitasking capabilities in the form of multiplesingle-task asynchronous predictions": "To comprehensively capture the information from various modal-ity combination patterns, a task-agnostic multimodal informationextraction module is presented, with the covariance regulariza-tion to decorrelate the different modality combination represen-tations. A task-guided hierarchical multimodal fusion module is designedto learn adaptive representations for different tasks, by incorpo-rating implicit task indication in the aggregation process frommodality-level to patient-level representation. Extensive experiments conducted on MIMIC-IV dataset show thatour model achieves competitive results on multiple tasks. Besides,further analysis demonstrates the feasibility and potential ofadopting such multitask strategy to construct a unified model inthe healthcare domain.",
  "RELATED WORK": "Multimodal learning for healthcare. In the quest for a thoroughcomprehension of patient patterns to enhance the precision of clini-cal event prediction, researchers have delved into the realm of multi-modal learning utilizing healthcare data .HAIM leverages different pre-trained feature extraction modelsto process multimodal inputs and obtains the overall representationof the patient. Zhou et al. proposes a transformer-based modelthat processes multimodal EHR data in a unified manner. However,the aforementioned methods lack consideration for incompletemodalities, which limits the application scenarios of the models.To handle the pervasive issue of missing modalities in clinicalpractice, many researchers have developed models capable of eitherimputing missing modalities or adapting to the absence of certainmodalities. MedFuse is an LSTM-based fusion model that canprocess uni-modal as well as multimodal input. Lee et al. learnsthe EHR data with missing modal by the modality-aware attentionwith skip bottleneck. To overcome the limitations of traditionalparallel fusion approaches, MultiModN sequentially inputs anynumber or combination of modalities into a sequence of modality-specific encoders and can skip over missing modalities. Comparedwith the above method of ignoring missing modalities, M3Care imputes the information of the missing modalities in the latent spacefrom the similar neighbors of each patient. While these methodsare effective, when extending to a multitasking setting, the processof handling multimodal information must take into account thedifferent focal points of various tasks.Multitask learning for healthcare. Multitask learning, an effec-tive method that enhances performance through the joint learningof multiple related tasks, has been explored for use in healthcareprediction. In the healthcare domain, the term \"tasks\" has differentdefinitions. Several studies treat the mortality predictionof different patient cohorts as multiple tasks. Some studies intro-duce auxiliary tasks, such as time series reconstruction andprediction based on unimodal data , to enhance the modelsrepresentational capacity and downstream task performance. Otherstudies predict multiple clinical tasks simultaneously, such as riskprediction and disease diagnosis . GenHPF converts EHRs into hierarchical textual representations and offersa solid framework for multi-task and multi-source learning, but itcan only handle EHRs in text form. Recently, UniMed sequen-tially predict four medical tasks based on multimodal EHR data,utilizing the time-progressive correlation between tasks. However,these methods require samples to have labels for all tasks, imposingstricter demands on the already scarce medical data.",
  "=1, where is the number of samples, X()": "and () Y are the multimodal input and ground truth of the-th sample, respectively. Y is the set of labels for the -th task.Definition 2 (Patient multimodal data). Given M = {,,} aset of modalities (i.e., time-series data, image, note), the input of-th sample can be defined as: X()= {X(),}M. Consideringthe absence of some modalities, the incomplete input is X()={X(),}M () , where M () are the modalities actually presentin the -th sample, and M () M. Note that |M () | 1, becauseat least one modality is present for each sample.Definition 3 (Modality combination). The modality combinationset represents all patterns of unimodal or multimodal combination,defined as: C = 2M\\ (i.e., all nonempty subsets of M). When|M| = 3, the number of modality combination set |C| = 7.Multitask prediction problem. Given the multimodal datasetsfor different tasks, the objective is to learn a unified task-adaptive",
  "NotationDefinition": "; The -th task; total number of tasksD; The dataset of the -th task; the number of samples; MModality; set of multimodal; CModality combination; set of modality combinationX()The -th patients data of the -th task()Ground truth of the target()Prediction result HLearned representations of modality htask; HcombLearnable representation of task token and modalitycombination tokensztask; ZcombLearned representation of task token and modalitycombination tokens through intra/inter-modalityencoderMThe mask that aids modality combination tokens incapturing specific informationThe covariance regularization term of Zcomb",
  "Multimodal Information Fusion": ": The framework of the FlexCare model. It consists of three modules: (a) Task-agnostic multimodal informationextraction; (b) Task-guided hierarchical multimodal fusion; (c) Task-specific prediction heads. predictive function: ()= (X(),), where (, ) is parameter-ized by and ()denotes the corresponding prediction result ofthe -th task.Besides, the necessary notations used in the paper are listed in for ease of understanding. Please note that in the Methodology, we take a single patient as an example. Therefore,we simplify X()to X to enhance the reading experience.",
  "METHODOLOGY4.1Overview": "The overall framework of the proposed model is shown in ,which mainly consists of three components: The Task-agnostic multimodal information extraction mod-ule leverages unimodal feature extractors and a unified multi-modal encoder to learn a spectrum of modality combinationrepresentations. The Task-guided hierarchical multimodal fusion moduleachieves hierarchical fusion from modality-level to patient-levelthrough the task/modality-aware Mixture of Experts (MoE) andan attention-based fusion mechanism. The Task-specific prediction heads are configured with indi-vidual predictors for each task, making predictions for the currenttask based on patient-level representation.Due to the model being trained and tested in an asynchronousmultiple single-task paradigm, the following introduction will takeone task as an example.",
  "To enhance the generalization capability of the model, the task-agnostic multimodal information extraction module is deeply sharedacross multiple tasks. The module maps raw data with different": "dimensions and modalities into latent representations in a unifiedspace, and captures task information and modality-level informa-tion through learnable task token and modality combination tokensrespectively. 4.2.1Unimodal Information Extraction. Given the sample Xin the -th task dataset and the unimodal representation extractionmodel (), the raw input of modality is converted into a seriesof 1D tokens H R with the same dimension via:",
  "H = (X ) + p,(1)": "where p R is a learned positional embedding added tothe tokens to retain positional information, is the dimensionof the latent representation and denotes the number of to-kens for modality . For the three modalities {,,}, the unimodalmodels () are set as follows: a linear projection, patch projec-tion followed by a linear projection, and the pre-trained & frozenBioBERT , respectively. 4.2.2Intra/Inter-modality Encoder. To inject task-specific in-formation into the model, thereby facilitating subsequent implicitguidance for multimodal representation learning under specifictasks, a learnable task token is allocated for each task category. Forthe given -th task, its corresponding task token is represented ashtask R. Furthermore, in order to thoroughly capture the infor-mation contained in individual intra-modality as well as variousinter-modality patterns, a set of learnable modality combinationtokens Hcomb= {h }C is presented.To realize the information extraction from intra- and inter-modality,the task token, modality combination tokens, and tokens extractedfrom each modality, are stacked along the token dimension to ob-tain a multimodal sequence: H0 = [htask, Hcomb, H, H, H ], whereH0 R and = 1 + |C| + + + . Subsequently, themultimodal sequence is fed into the encoder, facilitating the in-teraction of multimodal information and obtaining the output of",
  "+ M)VW , (3)": "where Q, K, V represent the query, key, and value embedding, respec-tively. W, W, W denote the weight matrices, is the dimensionof the latent representation, and M is the additional mask.The additional mask M R enables modality combi-nation tokens to precisely target information relevant to diversemodality combination patterns. Specifically, for modality combina-tion tokens and modality tokens, (i.e., 0 < , < ), the mask isdenoted as:",
  ",otherwise,(4)": "where : index (M|C) defines a function that maps the tokento the modality or modality combination it belongs to. () () indicates that the token H, originates from modality , thetoken H, = h from modality combination , and . () =() indicates that the token H, and token H, is from the samemodality or modality combination .Given our intention for the multimodal information extractionprocess to be task-agnostic, the task token exclusively aggregates in-formation from other tokens unidirectionally, without transmittingits own information to them. Consequently, for the mask associatedwith the task token (i.e., = 0 and 0 < ), we establish thatM (, ) = 0.Through layers of the encoder equipped with a specializedattention mask, we have obtained representation enriched withtask-specific information ztask= htask,and various task-agnosticmodality combination representations Zcomb= Hcomb,. 4.2.3Representation Decorrelation of Modality Combina-tion. The modality combination tokens, due to the sharing ofpartial modality information within the encoder, may result ina phenomenon of feature redundancy. To decorrelate the differ-ent embeddings of the modality combination tokens and preventthem from encoding similar information, a token-level covarianceregularization method is proposed.Unlike the previous method that constrains the correlationsbetween different dimensions of the embeddings, our approachcomputes the covariance matrix along the token dimension ratherthan the feature dimension. The function to calculate the covariancematrix Cov() and the token-level covariance regularization term are defined as follows:",
  "Task-guided Hierarchical MultimodalFusion": "To facilitate the adaptive representation learning for the specifictask, it is necessary to infuse implicit task information into task-agnostic modality-level representations for their refinement. Guidedby the specific task, these representations are subsequently aggre-gated into patient-level representations. 4.3.1Task/Modality-aware Mixture of Experts. Within a mul-titask multimodal framework, it is imperative to consider the infor-mation disparities among modalities and tasks. Employing identicalnetwork layers for different modalities across varied tasks may leadto negative interference, hindering the individualized representa-tion learning for specific downstream tasks.Specifically, after acquiring the task token and various modalitycombination tokens from the original embeddings, a task/modality-aware Mixture-of-Experts module is employed for the refinementof multimodal representations in the context of a specific task.The module obtains the output s R for the input token z byweighted average of the selected experts from a total of , whichcan be formulated as:",
  ",otherwise,": "(10)where W1 R and W2 R are learnable projectorsfor the modality combination token and task token, respectively.To mitigate the adverse impacts of imbalanced loading, we haveincorporated regularization terms aimed at balancing the distri-bution of expert assignments, following the design and defaulthyperparameters in previous work . 4.3.2Patient-level Representation Learning. Currently, therepresentations for various modality combinations under a specifictask have been obtained. The next step involves aggregating theminto a final patient-level representation. We design an attention-based mechanism that leverages implicit task information for guid-ance, achieving task-specific differentiated attention to various",
  "Task-specific Prediction Heads": "For different tasks, we employ task-specific prediction heads toobtain the prediction results: = (s ), where () is the pre-diction head for the -th task. For the binary classification task andmulti-label classification task, () contains a linear transformationwith a Sigmoid activation. For the multi-class classification task, () contains a liner transformation with a Softmax activation.The overall objective function for the -th task is as follows:",
  "Experimental Settings": "5.1.1Datesets. In this study, we use three EHR datasets: MIMIC-IV1, MIMIC-CXR JPG2 and MIMIC-IV-NOTE3 . Since thesedatasets share the same patient cohort, we gathered time-seriesdata from MIMIC-IV, X-ray images from MIMIC-CXR, and clinicalnotes from MIMIC-NOTE, thereby constructing a comprehensivemultimodal patient dataset. We divide the patients into a trainingset, a validation set, and a test set in a 7:1:2 ratio. On the basisof the multimodal dataset, we extract sub-datasets for 6 tasks: in-hospital-mortality (IHM), length-of-stay (LOS), decompensation(DEC), phenotyping (PHE), readmission (REA) and diagnosis (DIA).It is imperative to note that each of these tasks utilizes heteroge-neous datasets, meaning the input data and output labels vary acrosstasks. provides detailed statistics of datasets for multipletasks, revealing variations in sample sizes and modality messinessamong different tasks. 5.1.2Baselines and Implementation Details. To assess theeffectiveness of our proposed model, we compare it against thefollowing multimodal baselines: MedFuse , MT , M3Care ,MMF and MultiModN .For binary classification tasks (IHM, DEC, and REA), we useAUROC and AUPRC as evaluation metrics. For multi-class tasks(PHE and DIA), we use macro-AUROC and micro-AUROC; and formulti-label classification task (LOS), we use macro-F1 and micro-F1.Due to differences in the problem setting, previous multitaskmodels are not applicable to our constructed dataset. Because thesedatasets are independent, each sample has a label for only one task.Moreover, it should be noted that as some of the above modelsdo not encompass all modalities present in our dataset, we haveextended them accordingly while ensuring the consistency of thefoundational embedding layer across all models (i.e., a linear pro-jection, patch projection and the pre-trained & frozen BioBERT).Given the varying training difficulties across different tasks, weemploy distinct weights for each task in FlexCare and mitigate theissue of inconsistent convergence rates among multitask learningthrough weight decay.",
  "TaskMetricMedFuse MT M3Care MMF MultiModN FlexCare-stFlexCare": "IHMAUROC0.8772 (0.003)0.8726 (0.002)0.8732 (0.006)0.8804 (0.001)0.8751 (0.002)0.8749 (0.004)0.8823 (0.002)0.8823 (0.002)0.8823 (0.002)AUPRC0.5158 (0.006)0.5133 (0.008)0.5148 (0.017)0.5136 (0.010)0.5055 (0.006)0.5116 (0.007)0.5372 (0.006)0.5372 (0.006)0.5372 (0.006) LOSma-F10.1487 (0.006)0.1531 (0.006)0.1549 (0.007)0.1554 (0.006)0.1554 (0.006)0.1554 (0.006)0.1503 (0.010)0.1492 (0.005)0.1479 (0.005)mi-F10.6289 (0.005)0.6298 (0.003)0.6267 (0.004)0.6282 (0.004)0.6307 (0.006)0.6317 (0.001)0.6358 (0.003)0.6358 (0.003)0.6358 (0.003) DECAUROC0.9396 (0.002)0.9409 (0.001)0.9406 (0.004)0.9435 (0.001)0.9470 (0.001)0.9420 (0.002)0.9538 (0.001)0.9538 (0.001)0.9538 (0.001)AUPRC0.4782 (0.006)0.4792 (0.010)0.4911 (0.011)0.4981 (0.008)0.4922 (0.005)0.4926 (0.010)0.5123 (0.006)0.5123 (0.006)0.5123 (0.006) PHEma-AUROC0.8340 (0.001)0.8362 (0.001)0.8429 (0.001)0.8446 (0.001)0.8446 (0.001)0.8446 (0.001)0.8424 (0.000)0.8417 (0.000)0.8393 (0.005)mi-AUROC0.8785 (0.001)0.8769 (0.001)0.8830 (0.001)0.8845 (0.000)0.8845 (0.000)0.8845 (0.000)0.8826 (0.000)0.8820 (0.000)0.8803 (0.004) REAAUROC0.7598 (0.002)0.7585 (0.002)0.7618 (0.001)0.7627 (0.002)0.7622 (0.001)0.7604 (0.002)0.7680 (0.002)0.7680 (0.002)0.7680 (0.002)AUPRC0.3618 (0.003)0.3481 (0.008)0.3562 (0.003)0.3482 (0.006)0.3526 (0.004)0.3517 (0.003)0.3702 (0.004)0.3702 (0.004)0.3702 (0.004) DIAma-AUROC0.6651 (0.007)0.6715 (0.005)0.6756 (0.006)0.6692 (0.002)0.6717 (0.005)0.6750 (0.005)0.6845 (0.006)0.6845 (0.006)0.6845 (0.006)mi-AUROC0.8920 (0.002)0.8960 (0.002)0.8955 (0.001)0.8960 (0.001)0.8944 (0.001)0.8948 (0.001)0.8984 (0.001)0.8984 (0.001)0.8984 (0.001)",
  "Model Performance (RQ1)": "In the experiments, baseline models are independently trained foreach task, whereas FlexCare is trained on all tasks within a singlemodel. As shown in , two key conclusions can be drawn: (1)Compared to single-task baseline models, FlexCare achievescompetitive results across various evaluation metrics whilealso being adaptive to a range of tasks. Overall, the perfor-mance differences among the baseline models are not significant.This could be attributed to the fact that after replacing with theunified unimodal encoders, various multimodal fusion strategiesunder a single-task are capable of effectively addressing the currentissue, yet they have reached a performance plateau. Unlike othermodels that are optimized for a single task, our multi-task modelaims at the overall performance improvement and may not achieveoptimal results in some individual metrics across all 6 tasks. Bymeans of the synergistic effect between tasks, our model achievesthe best results on the IHM, DEC, REA, and DIA tasks, with per-formance on the remaining two tasks closely matching that of thebaselines. (2) Through leveraging cross-task synergy, tasksthat are closely related can benefit from each other. FlexCareachieves significant performance improvements on the IHM andDEC tasks, which are related to patient mortality risk. This demon-strates that FlexCare can leverage cross-task synergy to acquireadditional knowledge, aiding in achieving superior performance forrelated tasks. Moreover, comparing to FlexCare-st that is trainedindependently for each task, we can observe that the strategy ofheterogeneous multitask joint training is both rational and effective,with certain tasks benefiting from other related tasks.",
  "FlexCare": "FlexCarea-, FlexCareb-, FlexCarec- and FlexCared-. The specific set-ting is outlined in , where different models comprise variouscombinations of three modules.As shown in , the performance disparity between Flex-Care and both FlexCarea- and FlexCareb- highlights the effective-ness of modality combination tokens and covariance regulariza-tion. Furthermore, FlexCarec- performs well on several tasks, butworse on others (e.g., DEC, REA, DIA), even falling below thatof FlexCarea- and FlexCareb-. Additionally, FlexCarec- exhibits ahigher overall standard deviation, indicating less stability. These re-sults corroborate the phenomenon of negative interface in multitasklearning mentioned above. 5.3.2Analysis of Learnable Task Token. To facilitate multi-modal representation learning for specific tasks, we incorporatetask information as input and learn a corresponding task tokenfor each task. To demonstrate the impact of the task token on thefinal representations, we randomly select 30 mini-batch samples(960 samples in total) for each task, comparing scenarios with andwithout the use of the task token. shows the visualizedresults of final embeddings learned w/o and w/ the task token. It isobserved that without explicitly providing task information as inputto the model, the representations of samples from different tasks are",
  ": Visualization of patient-level representationlearned w/o and w/ the task token": "intermingled and indistinguishable. However, upon inputting taskinformation, the representations of various tasks are effectively seg-regated. Moreover, from (b), it should be noted that samplesfrom the five tasks primarily based on time-series modalities aremore clustered together, whereas samples from the DIA task, whichrelies on the imaging modality, are situated farther from the others.This indicates the ability of FlexCare to learn the modality-specificdifferences inherent to each task.",
  ": Routing specialization at task and modality levels": "combination. From (a), we observe that the experts em-ployed for task DIA exhibit significant differences compared tothose used for other tasks. This discrepancy arises because DIAprimarily relies on the imaging modality, with other modalities serv-ing as auxiliary information, whereas other tasks predominantlydepend on time-series modality. (b) presents the resultsof modality-level routing specialization, reflecting the distinct ca-pabilities of each expert, which aids in the refined processing ofmultimodal information. 5.4.2Interaction Between Tasks. To demonstrate whether Flex-Care effectively utilizes cross-task synergy, we analyze the impactof the current training task on other tasks. Specifically, we sequen-tially train the six tasks in each epoch: IHM, LOS, DEC, PHE, REA,and DIA. Upon the completion of training for the current task, wereport the test performance across all tasks. As illustrated in , it can be observed that after the training of a specific task, theperformance of other tasks does not necessarily decline; rather, it",
  "Analysis of Extensibility (RQ4)": "As a flexible multimodal multitask model, FlexCare can be adaptedto other new tasks. We construct a new task, Diagnosis-relatedGroup (DRG) prediction , which primarily relies on clinicalnotes. This task features a sizable dataset, encompassing 236,770samples. We conduct experiments under two settings, training for40 epochs with 1% of the training data and 100% of the trainingdata, respectively. presents the experimental results, whereFlexCarepretrain employs the model trained on the six tasks men-tioned previously, while FlexCare denotes the model trained fromscratch on the current task. It can be observed that with limitedtraining data, both the baseline models and FlexCare that trainedfrom scratch, do not perform as well as pre-trained FlexCare. How-ever, when utilizing 100% of the training data, the advantage of pre-trained FlexCare is diminished due to the significantly larger samplesize of this task compared to the others. This experiment demon-strates FlexCare can be flexibly extended to new tasks, achievingcommendable performance even with smaller data sizes.",
  "CONCLUSION": "In this paper, we introduce FlexCare, a unified healthcare predic-tion model that flexibly accommodates incomplete multimodal in-puts and adapts to multiple tasks. It is endowed with multitaskingcapabilities realized through asynchronous multiple single-taskpredictions. Specifically, the task-agnostic multimodal informationextraction module is designed to thoroughly capture informationacross a spectrum of modality combination patterns. Meanwhile, atoken-level covariance regularization method is developed to pre-vent different modality combination tokens from encoding similarinformation. Furthermore, we propose the task-guided hierarchicalmultimodal fusion module to learn adaptive representation tailoredto the specific task. In addition, we experimented on multiple tasksfrom MIMIC-IV/MIMIC-CXR/MIMIC-NOTE datasets to show theeffectiveness of the proposed model.In future work, we will investigate effective solutions to addressthe issues of gradient conflicts and inconsistencies in convergencerates during multitask training, contributing to the advancementof a general prediction model in the healthcare domain. This work was supported in part by the National Key Research andDevelopment Program of China under Grant No.2021ZD0140407,the Beijing Natural Science Foundation under Grant No.7222313,and the National High Level Hospital Clinical Research Fundingunder Grant No.2022-PUMCH-C-041. Raquel Aoki, Frederick Tung, and Gabriel L Oliveira. 2022. Heterogeneous multi-task learning with expert diversity. IEEE/ACM Transactions on ComputationalBiology and Bioinformatics 19, 6 (2022), 30933102.",
  "Adrien Bardes, Jean Ponce, and Yann LeCun. 2022. VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning. In ICLR22": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, GeorgHeigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image isWorth 16x16 Words: Transformers for Image Recognition at Scale. In ICLR21. Michal Golovanevsky, Carsten Eickhoff, and Ritambhara Singh. 2022. Multimodalattention-based deep learning for Alzheimers disease diagnosis. Journal of theAmerican Medical Informatics Association 29, 12 (2022), 20142022.",
  "Ronghang Hu and Amanpreet Singh. 2021. Unit: Multimodal multitask learningwith a unified transformer. In ICCV21. 14391449": "Kyunghoon Hur, Jungwoo Oh, Junu Kim, Jiyoun Kim, Min Jae Lee, EunbyeolCho, Seong-Eun Moon, Young-Hak Kim, Louis Atallah, and Edward Choi. 2024.GenHPF: General Healthcare Predictive Framework for Multi-Task Multi-SourceLearning. IEEE Journal of Biomedical and Health Informatics 28, 1 (2024), 502513. Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, ChrisChute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al.2019. Chexpert: A large chest radiograph dataset with uncertainty labels andexpert comparison. In AAAI19. 590597.",
  "Trent Kyono, Fiona J Gilbert, and Mihaela Schaar. 2019. Multi-view multi-tasklearning for improving autonomous mammogram diagnosis. In MLHC19. 571591": "Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim,Chan Ho So, and Jaewoo Kang. 2020. BioBERT: a pre-trained biomedical languagerepresentation model for biomedical text mining. Bioinformatics 36, 4 (2020),12341240. Kwanhyung Lee, Soojeong Lee, Sangchul Hahn, Heejung Hyun, Edward Choi,Byungeun Ahn, and Joohyung Lee. 2023. Learning Missing Modal ElectronicHealth Records with Unified Multi-modal Data Embedding and Modality-AwareAttention. In MLHC23. 423442.",
  "Fei Li and Hong Yu. 2020. ICD coding from clinical text using multi-filter residualconvolutional neural network. In AAAI20, Vol. 34. 81808187": "Jinghui Liu, Daniel Capurro, Anthony Nguyen, and Karin Verspoor. 2021. Earlyprediction of diagnostic-related groups and estimation of hospital cost by pro-cessing clinical notes. NPJ Digital Medicine 4, 1 (2021), 103. Luchen Liu, Zequn Liu, Haoxian Wu, Zichang Wang, Jianhao Shen, YipiingSong, and Ming Zhang. 2020. Multi-task learning via adaptation to similar tasksfor mortality prediction of diverse rare diseases. In AMIA Annual SymposiumProceedings, Vol. 2020. 763. Sicen Liu, Xiaolong Wang, Yongshuai Hou, Ge Li, Hui Wang, Hui Xu, Yang Xiang,and Buzhou Tang. 2022. Multimodal data matters: language model pre-trainingover structured and unstructured electronic health records. IEEE Journal ofBiomedical and Health Informatics 27, 1 (2022), 504514.",
  "Chantal Pellegrini, Nassir Navab, and Anees Kazi. 2023. Unsupervised pre-training of graph transformers on patient population graphs. Medical ImageAnalysis 89 (2023), 102895": "Laila Rasmy, Yang Xiang, Ziqian Xie, Cui Tao, and Degui Zhi. 2021. Med-BERT:pretrained contextualized embeddings on large-scale structured electronic healthrecords for disease prediction. NPJ Digital Medicine 4, 1 (2021), 86. Wei Shao, Tongxin Wang, Liang Sun, Tianhan Dong, Zhi Han, Zhi Huang, JieZhang, Daoqiang Zhang, and Kun Huang. 2020. Multi-task multi-modal learningfor joint diagnosis and prognosis of human cancers. Medical Image Analysis 65(2020), 101795. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le,Geoffrey Hinton, and Jeff Dean. 2017. Outrageously large neural networks: Thesparsely-gated mixture-of-experts layer. In ICLR17.",
  "Harini Suresh, Jen J Gong, and John V Guttag. 2018. Learning tasks for multitasklearning: Heterogenous patient populations in the icu. In KDD18. 802810": "Vinitra Swamy, Malika Satayeva, Jibril Frej, Thierry Bossy, Thijs Vogels, MartinJaggi, Tanja Kser, and Mary-Anne Hartley. 2023. MultiModN-Multimodal, Multi-Task, Interpretable Modular Networks. In NeurIPS23. Siyi Tang, Amara Tariq, Jared A Dunnmon, Umesh Sharma, Praneetha Elugunti,Daniel L Rubin, Bhavik N Patel, and Imon Banerjee. 2023. Predicting 30-dayall-cause hospital readmission using multimodal spatiotemporal graph neuralnetworks. IEEE Journal of Biomedical and Health Informatics 27, 4 (2023), 20712082. Muhao Xu, Zhenfeng Zhu, Youru Li, Shuai Zheng, Linfeng Li, Haiyan Wu, andYao Zhao. 2023. Cooperative dual medical ontology representation learning forclinical assisted decision-making. Computers in Biology and Medicine (2023),107138. Muhao Xu, Zhenfeng Zhu, Yawei Zhao, Kunlun He, Qinghua Huang, and YaoZhao. 2024. RedCDR: Dual Relation Distillation for Cancer Drug ResponsePrediction. IEEE/ACM Transactions on Computational Biology and Bioinformatics(2024), 112. Yongxin Xu, Kai Yang, Chaohe Zhang, Peinie Zou, Zhiyuan Wang, Hongxin Ding,Junfeng Zhao, Yasha Wang, and Bing Xie. 2023. VecoCare: visit sequences-clinicalnotes joint learning for diagnosis prediction in healthcare data. In IJCAI23. 49214929. Yuyang Xu, Haochao Ying, Siyi Qian, Fuzhen Zhuang, Xiao Zhang, Deqing Wang,Jian Wu, and Hui Xiong. 2023. Time-Aware Context-Gated Graph AttentionNetwork for Clinical Risk Prediction. IEEE Transactions on Knowledge and DataEngineering 35, 7 (2023), 75577568. Ruoxi Yu, Yali Zheng, Ruikai Zhang, Yuqi Jiang, and Carmen C. Y. Poon. 2020.Using a Multi-Task Recurrent Neural Network With Attention Mechanisms toPredict Hospital Mortality of Patients. IEEE Journal of Biomedical and HealthInformatics 24, 2 (2020), 486492.",
  "Xiongjun Zhao, Xiang Wang, Fenglei Yu, Jiandong Shang, and Shaoliang Peng.2022.UniMed: Multimodal Multitask Learning for Medical Predictions. InBIBM22. 13991404": "Shuai Zheng, Zhenfeng Zhu, Zhizhe Liu, Zhenyu Guo, Yang Liu, Yuchen Yang,and Yao Zhao. 2022. Multi-modal graph learning for disease prediction. IEEETransactions on Medical Imaging 41, 9 (2022), 22072216. Hong-Yu Zhou, Yizhou Yu, Chengdi Wang, Shu Zhang, Yuanxu Gao, Jia Pan, JunShao, Guangming Lu, Kang Zhang, and Weimin Li. 2023. A transformer-basedrepresentation-learning model with unified processing of multimodal input forclinical diagnostics. Nature Biomedical Engineering 7, 6 (2023), 743755.",
  "ADETAILS OF EXPERIMENTAL SETTINGSA.1Tasks": "Following previous works for dataset creation, we extract sub-datasets for 7 tasks from MIMIC-IV dataset, 6 of which are utilizedfor multitask performance comparison, and the remaining one forextensibility analysis: In-hospital mortality (IHM): predicts in-hospital mortalitybased on the first 48 hours of an ICU stay. This is a binary classi-fication task. Length-of-stay (LOS): predicts remaining time spent in ICU.We define it as a classification problem with 10 classes (one forICU stays shorter than a day, seven day-long buckets for eachday of the first week, one for stays of over one week but less thantwo, and one for stays of over two weeks).",
  "A.2Data Preprocessing": "Time series data: 17 variables are sampled every one hour, andthe missing value is imputed using the previous one. Then, theyare discretized and standardized to obtain the 76-dimension inputat each time-step. Image data: Images classified as AP (Anteroposterior) are re-tained. Following the previous work , during the trainingphase, the images are resized to 256 256 pixels, and randomhorizontal flip and random affine transformation are applied, fol-lowed by cropping to 224 224 pixels. For validation and testing",
  "A.4Model Implementation": "Our model is implemented using PyTorch 1.10.0 and Python 3.7.9.The experiment environment is a machine equipped with Ubuntu20.04 and NVIDIA GeForce RTX 3090 GPU. For models with trans-former encoders, the number of transformer layers is set to 4 andthe number of attention heads is set to 2. For all models, the dimen-sion of the hidden layer is 128. The number of the selected experts and the total experts are 2 and 10. Due to large search spacefor 6 task weights, we set them with {0.2,0.5,0.2,1,0.2,0.2} based onloss magnitude and training difficulty of single-task models. Weuse Adam as the optimizer with batch size 32, learning rate 1e-3 or",
  "BFURTHER ANALYSIS ON TASK TOKEN": "In .3.2, we visualize the patient-level representation learnedw/o and w/ the task token. Here, we present quantitative exper-imental results under two scenarios. As observed in , theperformance across various tasks declines when task informationis not provided to the model, which is precisely because the rep-resentations of different tasks are mixed together and cannot beclearly distinguished."
}