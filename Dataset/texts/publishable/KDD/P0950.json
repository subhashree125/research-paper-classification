{
  "Abstract": "With the ongoing rapid adoption of Articial Intelligence (AI)-basedsystems in high-stakes domains, ensuring the trustworthiness, safety,and observability of these systems has become crucial. It is essen-tial to evaluate and monitor AI systems not only for accuracy andquality-related metrics but also for robustness, bias, security, inter-pretability, and other responsible AI dimensions. We focus on largelanguage models (LLMs) and other generative AI models, whichpresent additional challenges such as hallucinations, harmful andmanipulative content, and copyright infringement. In this surveyarticle accompanying our tutorial",
  "Introduction": "Considering the increasing adoption of Articial Intelligence (AI)technologies in our daily lives, it is crucial to develop and deploythe underlying AI models and systems in a responsible manner andensure their trustworthiness, safety, and observability. Our focus ison large language models (LLMs) and other generative AI modelsand applications. Such models and applications need to be evalu-ated and monitored not only for accuracy and quality-related met-rics but also for robustness against adversarial attacks, robustnessunder distribution shifts, bias and discrimination against under-represented groups, security and privacy protection, interpretabil-ity, hallucinations (and other ungrounded or low-quality outputs), Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor prot or commercial advantage and that copies bear this notice and the full cita-tion on the rst page. Copyrights for components of this work owned by others thanthe author(s) must be honored. Abstracting with credit is permitted. To copy other-wise, or republish, to post on servers or to redistribute to lists, requires prior specicpermission and/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08 harmful content (such as sexual, racist, and hateful responses), jail-breaks of safety and alignment mechanisms, prompt injection at-tacks, misinformation and disinformation, fake, misleading, andmanipulative content, copyright infringement, and other respon-sible AI dimensions.In this tutorial, we rst highlight key harms associated withgenerative AI systems, focusing on ungrounded answers (halluci-nations), jailbreaks and prompt injection attacks, harmful content,and copyright infringement. We then discuss how to eectively ad-dress potential risks and challenges, following the framework ofidentication, measurement, mitigation (with four mitigation lay-ers at the model, safety system, application, and positioning lev-els), and operationalization. We present real-world LLM use cases,practical challenges, best practices, lessons learned from deploy-ing solution approaches in the industry, and key open problems.Our goal is to stimulate further research on grounding and evalu-ating LLMs and enable researchers and practitioners to build morerobust and trustworthy LLM applications.We rst present a brief tutorial outline in 1.1, followed by anelaborate discussion of dierent responsible AI dimensions in 2.We devote 3 to the problem of grounding for LLM applications,and 4 to the emerging area of LLM operations. For each dimen-sion (discussed in 2 to 4), we present key business problems, tech-nical solution approaches, and open challenges.",
  "consists of the following parts:1": "Introduction and Overview of LLM Applications. We give anoverview of the generative AI landscape in industry and motivatethe topic of the tutorial with the following questions. What con-stitutes generative AI? Why is generative AI an important topic?What are key applications of generative AI that are being deployedacross dierent industry verticals? Why is it crucial to develop anddeploy generative AI models and applications in a responsible man-ner? Holistic Evaluation of LLMs. We highlight key challenges thatarise when developing and deploying LLMs and other generativeAI models in enterprise settings, and present an overview of solu-tion approaches and open problems. We discuss evaluation dimen-sions such as truthfulness, safety and alignment, bias and fairness,robustness and security, privacy, model disgorgement and unlearn-ing, copyright infringement, calibration and condence, and trans-parency and causal interventions.",
  "KDD 24, August 2529, 2024, Barcelona, SpainKrishnaram Kenthapadi, Mehrnoosh Sameki, and Ankur Taly": "Ethan Perez, Saron Huang, Francis Song, Trevor Cai, Roman Ring, JohnAslanides, Amelia Glaese, Nat McAleese, and Georey Irving. 2022. Red Team-ing Language Models with Language Models. In Proceedings of the 2022 Confer-ence on Empirical Methods in Natural Language Processing. 34193448. Pinecone. [n. d.]. Pinecone Vector Database. Jingyuan Qi, Zhiyang Xu, Ying Shen, Minqian Liu, Di Jin, Qifan Wang, andLifu Huang. 2023. The Art of SOCRATIC QUESTIONING: Recursive Thinkingwith Large Language Models. In Proceedings of the 2023 Conference on EmpiricalMethods in Natural Language Processing. 41774199. Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, andPeter Henderson. 2024. Fine-tuning Aligned Language Models CompromisesSafety, Even When Users Do Not Intend To!. In The Twelfth International Con-ference on Learning Representations. Colin Rael, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring theLimits of Transfer Learning with a Unied Text-to-Text Transformer. J. Mach.Learn. Res. 21 (2020), 140:1140:67. Hannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Lora Aroyo, MichaelCollins, Dipanjan Das, Slav Petrov, Gaurav Singh Tomar, Iulia Turc, and DavidReitter. 2023. Measuring Attribution in Natural Language Generation Models.Computational Linguistics 49, 4 (2023), 777840. Tilman Ruker, Anson Ho, Stephen Casper, and Dylan Hadeld-Menell. 2023.Toward transparent AI: A survey on interpreting the inner structures of deepneural networks. In 2023 IEEE Conference on Secure and Trustworthy MachineLearning (SaTML). IEEE, 464483. Vipula Rawte, Swagata Chakraborty, Agnibh Pathak, Anubhav Sarkar, SM Ton-moy, Aman Chadha, Amit P Sheth, and Amitava Das. 2023. The TroublingEmergence of Hallucination in Large Language Models-An Extensive Deni-tion, Quantication, and Prescriptive Remediations. In Proceedings of the 2023Conference on Empirical Methods in Natural Language Processing. 25412573. Traian Rebedea, Razvan Dinu, Makesh Narsimhan Sreedhar, ChristopherParisien, and Jonathan Cohen. 2023. NeMo Guardrails: A Toolkit for Control-lable and Safe LLM Applications with Programmable Rails. In Proceedings of the2023 Conference on Empirical Methods in Natural Language Processing: SystemDemonstrations. Philip Resnik. 2024. Large Language Models are Biased Because They Are LargeLanguage Models. arXiv preprint arXiv:2406.13138 (2024). Adam Roberts, Colin Rael, and Noam Shazeer. 2020. How Much KnowledgeCan You Pack Into the Parameters of a Language Model?. In Proceedings of the2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Alexey Romanov, Maria De-Arteaga, Hanna Wallach, Jennifer Chayes, Chris-tian Borgs, Alexandra Chouldechova, Sahin Geyik, Krishnaram Kenthapadi,Anna Rumshisky, and Adam Kalai. 2019. Whats in a Name? Reducing Biasin Bios without Access to Protected Attributes. In Proceedings of the 2019 Con-ference of the North American Chapter of the Association for Computational Lin-guistics: Human Language Technologies, Volume 1 (Long and Short Papers). 41874195. Kevin Roose. 2023. A Conversation With Bings Chatbot Left Me Deeply Un-settled. New York Times (2023). Jon Saad-Falcon, Omar Khattab, Christopher Potts, and Matei Zaharia. 2024.ARES: An Automated Evaluation Framework for Retrieval-Augmented Gener-ation Systems. In Proceedings of the 2024 Conference of the North American Chap-ter of the Association for Computational Linguistics: Human Language Technolo-gies (Volume 1: Long Papers). 338354. Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, andTatsunori Hashimoto. 2023. Whose opinions do language models reect?. InInternational Conference on Machine Learning. PMLR, 2997130004. Toby Shevlane, Sebastian Farquhar, Ben Garnkel, Mary Phuong, Jess Whit-tlestone, Jade Leung, Daniel Kokotajlo, Nahema Marchal, Markus Anderljung,Noam Kolt, et al. 2023. Model evaluation for extreme risks. arXiv preprintarXiv:2305.15324 (2023). Weijia Shi, Xiaochuang Han, Mike Lewis, Yulia Tsvetkov, Luke Zettlemoyer,and Wen-tau Yih. 2024. Trusting Your Evidence: Hallucinate Less with Context-aware Decoding. In Proceedings of the 2024 Conference of the North Ameri-can Chapter of the Association for Computational Linguistics: Human LanguageTechnologies (Volume 2: Short Papers), Kevin Duh, Helena Gomez, and StevenBethard (Eds.). Association for Computational Linguistics, Mexico City, Mex-ico, 783791. Ben Snyder, Marius Moisescu, and Muhammad Bilal Zafar. 2023.On EarlyDetection of Hallucinations in Factual Question Answering.arXiv preprintarXiv:2312.14183 (2023). Nisan Stiennon, Long Ouyang, Jerey Wu, Daniel Ziegler, Ryan Lowe, ChelseaVoss, Alec Radford, Dario Amodei, and Paul F Christiano. 2020. Learning tosummarize with human feedback. In Advances in Neural Information ProcessingSystems. 30083021. Alex Tamkin, Amanda Askell, Liane Lovitt, Esin Durmus, Nicholas Joseph,Shauna Kravec, Karina Nguyen, Jared Kaplan, and Deep Ganguli. 2023. Evaluat-ing and mitigating discrimination in language model decisions. arXiv preprint arXiv:2312.03689 (2023). Yuchao Tao, Ryan McKenna, Michael Hay, Ashwin Machanavajjhala, andGerome Miklau. 2021. Benchmarking dierentially private synthetic data gen-eration algorithms. arXiv preprint arXiv:2112.09238 (2021). Yi Tay, Vinh Q. Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta,Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, Tal Schuster, William W. Cohen, andDonald Metzler. 2022. Transformer Memory as a Dierentiable Search Index.In Advances in Neural Information Processing Systems. Katherine Tian, Eric Mitchell, Huaxiu Yao, Christopher D Manning, andChelsea Finn. 2024. Fine-Tuning Language Models for Factuality. In The TwelfthInternational Conference on Learning Representations. Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov,Huaxiu Yao, Chelsea Finn, and Christopher Manning. 2023. Just Ask for Cal-ibration: Strategies for Eliciting Calibrated Condence Scores from LanguageModels Fine-Tuned with Human Feedback. In Proceedings of the 2023 Confer-ence on Empirical Methods in Natural Language Processing. 54335442. Helen Toner, Jessica Ji, John Bansemer, Lucy Lim, Chris Painter, Courtney Cor-ley, Jess Whittlestone, Matt Botvinick, Mikel Rodriguez, and Ram Shankar SivaKumar. 2023. Skating to Where the Puck is Going: Anticipating and ManagingRisks from Frontier AI Systems. Report from the July 2023 Roundtable hostedby the Center for Security and Emerging Technology (CSET) at GeorgetownUniversity and Google DeepMind. Lifu Tu, Semih Yavuz, Jin Qu, Jiacheng Xu, Rui Meng, Caiming Xiong, andYingbo Zhou. 2023. Unlocking Anticipatory Text Generation: A ConstrainedApproach for Faithful Decoding with Large Language Models. arXiv preprintarXiv:2312.06149 (2023). Miles Turpin, Julian Michael, Ethan Perez, and Samuel Bowman. 2023. Lan-guage models dont always say what they think: Unfaithful explanations inchain-of-thought prompting. Advances in Neural Information Processing Sys-tems 36 (2023). Giorgos Vernikos, Arthur Brazinskas, Jakub Adamek, Jonathan Mallinson, Ali-aksei Severyn, and Eric Malmi. 2024. Small Language Models Improve Giantsby Rewriting Their Outputs. In Proceedings of the 18th Conference of the Euro-pean Chapter of the Association for Computational Linguistics (Volume 1: LongPapers). 27032718. Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, ChrisTar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, and Thang Luong. 2023. Fresh-LLMs: Refreshing Large Language Models with Search Engine Augmentation.arXiv:2310.03214 Eric Wallace, Tony Zhao, Shi Feng, and Sameer Singh. 2021. Concealed DataPoisoning Attacks on NLP Models. In Proceedings of the 2021 Conference of theNorth American Chapter of the Association for Computational Linguistics: Hu-man Language Technologies. 139150. Xuezhi Wang, Haohan Wang, and Diyi Yang. 2022. Measure and Improve Ro-bustness in NLP Models: A Survey. In Proceedings of the 2022 Conference of theNorth American Chapter of the Association for Computational Linguistics: Hu-man Language Technologies. 45694586. Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parvez, and GrahamNeubig. 2023. Learning to Filter Context for Retrieval-Augmented Generation.arXiv:2311.08377 Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. 2023. Jailbroken: HowDoes LLM Safety Training Fail?. In Thirty-seventh Conference on Neural Infor-mation Processing Systems. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,Quoc V Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits rea-soning in large language models. Advances in Neural Information ProcessingSystems 35 (2022), 2482424837. Simon Willison. 2022. Prompt injection attacks against GPT-3. Simon WillisonsWeblog (2022). Robert Wolfe, Yiwei Yang, Bill Howe, and Aylin Caliskan. 2023. Contrastivelanguage-vision AI models pretrained on web-scraped multimodal data exhibitsexual objectication bias. In Proceedings of the 2023 ACM Conference on Fair-ness, Accountability, and Transparency. 11741185. Kevin Wu, Eric Wu, and James Zou. 2024. ClashEval: Quantifying the tug-of-war between an LLMs internal prior and external evidence. arXiv:2404.10198 Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang. 2023. DataSelection for Language Models via Importance Resampling. In Thirty-seventhConference on Neural Information Processing Systems. Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu,Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and BryanCatanzaro. 2024. Retrieval meets Long Context Large Language Models. In TheTwelfth International Conference on Learning Representations. Kevin Yang and Dan Klein. 2021. FUDGE: Controlled Text Generation WithFuture Discriminators. In Proceedings of the 2021 Conference of the North Amer-ican Chapter of the Association for Computational Linguistics: Human LanguageTechnologies. Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Zhibo Sun, and Yue Zhang.2024. A survey on large language model (LLM) security and privacy: The good,",
  "Holistic Evaluation of LLMs": "The overarching goal of evaluation is to determine whether a trainedLLM is t for deploymentin an enterprisesetting. A commonly quotedmaxim is that LLMs must ensure helpful, truthful, and harmlessresponses . While this seems straightforward, each of these di-mensions has several nuances. For instance, lack of truthfulnesscan range from subtle misrepresentations to making blatant falsestatements (colloquially known as hallucinations) . Similarly,harmful responses can vary from racially biased responses, to vio-lent, hateful, and other inappropriate responses, to responses caus-ing social harm (e.g., instruction on how to cheat in an examinationwithout getting caught). Further, in the context of evaluating LLMs,it is important to be aware of shortcomings that have been high-lighted with human and automatic model evaluations and withcommonly used datasets for natural language generation .Besides evaluations of response quality, practitioners also haveto worry about training data privacy, model stealing, copyright vi-olations, and security risks such as jailbreaking and promptinjection . In some settings, one may also seek calibrated con-dence scores for responses, interpretability, and robustness to ad-versarial prompts.In the rest of this section, we outline several evaluation dimen-sions that arise in enterprise deployments. Evaluation of LLMs isan important topic and there have been a number of dedicatedframeworks describing evaluation datasets, metrics,and benchmarks for various dimensions. A growing collection oftools and resources have been proposed across dierent phases ofLLM development . Here, we focus on the key business con-cerns, leading solution approaches, and open challenges for eachevaluation dimension.",
  "Business problems: How do we ensure that LLM responses areinformed, relevant, and trustworthy? How do we detect and re-cover from hallucinations?": "Solution approaches: There is extensive work on hallucinationsin LLMs , including, the causes and sources of hallucina-tions , and measures for evaluating LLMs based on their vul-nerability to producing hallucinations . A variety of methodshave been proposed to detect hallucinations, ranging from sam-pling based approaches to approaches leveraging internal statesof the LLM . There is also early work on detecting and pre-venting hallucinations in large vision language models andother multimodal foundation models .A number of methods have been proposed to fundamentally re-duce hallucinations by tuning models. One line of work involvestraining or ne-tuning LLMs on highly curated textbook-likedatasets. Another approach involves ne-tuning LLMs on pref-erence data for factuality, i.e., response pairs ranked by factual-ity . A fundamental hypothesis here is that LLMs have sys-tematic markers for when they are being untruthful . Thene-tuning process aims to train LLMs to tap into these markersand upweight factual responses. Related to this, it has been conjec-tured that LLMs internalize dierent personas during pretrain-ing, and by training on truthful question-answer pairs, one canupweight the truthful persona (even on unseen domains) .Reducing hallucination on a synthetic task has been explored as away to reduce hallucination on real-world downstream tasks .Finally, a recent work shows that ne-tuning LLMs on new infor-mation that was not acquired during pretraining can encouragethe model to hallucinate . Curating ne-tuning sets to avoidthis issue paves another path to reducing hallucinations.While truthful responses are table stakes for enterprise deploy-ments, we may want to go one step further and ensure that allresponses are aligned with a specic knowledge base (e.g., a set ofenterprise documents). This is known as grounding. This is a vasttopic in itself, and therefore we dedicate 3 entirely to it.Finally we emphasize that not all hallucinations are equally bad.For instance, hallucinations in response to nonsensical prompts orprompts with false premises (see for examples of questionswhose premises are factually incorrect and hence ideally need to berebutted) are relatively less concerning than hallucinations in re-sponse to well-meaning prompts. Furthermore, hallucinations inhigh stakes verticals like healthcare and life sciences may be farmore concerning than hallucinations in other verticals. Open challenges: A key open challenge is detecting hallucina-tions in video, speech, and multimodal settings. Another open chal-lenge is getting LLMs to generate citations when they answer fromparametric knowledge. More specically, can the LLM be madeaware of document identiers during pre-training, similar to thework on dierential search indexes , so that it can generatethe appropriate markers as citations for various claims in its re-sponse? A broader challenge is to leverage ideas and lessons fromsearch and information retrieval literature to improve rel-evance, trustworthiness, and truthfulness of LLM responses. Forexample, how can we incorporate valuable information such asdocument authors, document quality, authoritativeness of the do-main, timestamp, and other relevant metadata during pre-trainingand subsequent stages of LLM development?",
  "Safety and Alignment": "Business problems: How do we prevent an LLM from generatingtoxic, violent, oensive, or otherwise unsafe output? How do wedetect such content in cases where prevention fails to work? Howdo we ensure that the responses from an LLM are aligned with hu-man intent even in settings where it is hard for human experts toverify such alignment? Solution approaches: The problem can be addressed during dif-ferent stages of the LLM lifecycle. During data collection and cura-tion, we can apply mechanisms to detect unsafe content and takeremedial steps, such as excluding or modifying such content. Dur-ing pretraining and ne-tuning, we can incorporate constraints orpenalties to discourage the learning of unsafe sequences. In thereinforcement learning from human feedback (RLHF) stage, wecan include response pairs with preference labels on which oneis more appropriate, and tune the model to align its responseswith the preferences . As part of prompt engineering, we caninclude instructions to discourage the LLM from generating unde-sirable outputs. Finally, when prevention fails, we can apply toxic-ity classiers to detect undesirable outputs (as well as undesirableinputs) and ag such instances for appropriate treatment by theuser-facing AI applications.Another direction in alignment research is leveraging more pow-erful LLMs to detect safety and alignment issues with a weakerLLM in a cost-eective and latency-sensitive fashion. The prob-lem can be framed as a constrained optimization problem: givencost or latency constraints, determine the subset of prompts andresponses to be evaluated using a more powerful LLM (e.g., GPT-4). In certain settings, the task to be evaluated could be too hard foreven human experts (e.g., comparing two dierent summaries ofa very large collection of documents or judging the quality of hy-potheses generated based on a large volume of medical literature),necessitating the use of powerful LLMs in a manner that alignswith human intent. The converse problem of leveraging less pow-erful LLMs to align more powerful LLMs with human intent hasalso been explored in alignment research. A related challenge isto ensure that AI systems with superhuman performance (whichcould possibly be smarter than humans) are designed to follow hu-man intent. While current approaches for AI alignment rely onhuman ability to supervise AI (using approaches such as reinforce-ment learning from human feedback), these approaches would notbe feasible when AI systems become smarter than humans .Overall, alignment is an active area of research, with approachesranging from data-ecient alignment to alternatives to RLHF to aligning cross-modal representations . Open challenges: There has a been a bunch of recent work ongenerating adversarial prompts to bypass existing mechanisms formitigating toxic content generation . A key open chal-lenge is mitigating toxic content generation even under such adver-sarial prompts. Recent research has shown that LLM based guardrailmodels couldthemselves be attacked. For instance, a two-step prex-based attack procedure that operates by (a) constructing a uni-versal adversarial prex for the guardrail model, and (b) propagat-ing this prex to the response has been shown to be eective across multiple threat models, including ones in which the adver-sary has no access to the guardrail model at all . How do wedevelop eective LLM based guardrails that are robust to such at-tacks (and even better, have provable robustness/security guaran-tees)? Another challenge lies in balancing reduction of undesirableoutputs with preservation of the models ability towards creativegeneration. Finally, as LLMs are increasingly deployed as part ofopen-ended applications, an important socio-technical challengeis to investigate the opinions reected by the LLMs, determinewhether such opinions are aligned with the needs of dierent appli-cation settings, and design mechanisms to incorporate preferencesand opinions of relevant stakeholders (including those impacted bythe deployment of LLM based applications) .",
  "Business problems: How do we detect and mitigate bias in foun-dation models? How can we apply bias detection and mitigationthroughout the foundation model lifecycle?": "Solution approaches: There is extensive work on detecting andmitigating bias in NLP models . In additionto known categories of bias observed in predictive ML models,new types of bias arise in LLMs and other generative AI models,e.g., gender stereotypes, exclusionary norms, undesirable biases to-wards mentions of disability, religious stereotypes, and sexual ob-jectication . Additionally, due to the sheer sizeof datasets used, it is dicult to audit and update the training dataor even anticipate dierent kinds of biases that may be present.Mitigation approaches include counterfactual data augmentation(or other types of data improvements), netuning, incorporatingfairness regularizers, in-context learning, and natural language in-structions. For a longer discussion, we direct the readers to thesurvey by Gallegos et al. . More broadly, we can view bias mea-surement and mitigation as an important component of buildinga reliable and robust application that works well across dierentsubgroups of interest (including but not necessarily limited to pro-tected groups). By performing ne-grained evaluation and robust-ness testing across such groups, we can identify underperforminggroups, improve the performance for such groups, and thereby po-tentially boost even the overall performance. Openchallenges: Bias and fairness mitigation is a relatively nascentspace, and a key open question is identifying and designing prac-tical, scalable processes from the large class of bias measurementand mitigation techniques proposed for LLMs. A related challengeis ensuring that the bias mitigation approach does not cause themodel to inadvertently demonstrate disparate treatment, whichcould be considered unlawful in a wide range of scenarios underUS law . Further, how do we audit LLMs and other genera-tive AI models for dierent types of implicit or subtle biases anddesign mechanisms to mitigate or recover from such biases, al-though the models may not show explicit bias on standard bench-marks ? It has recently been argued that harmful biasesare an inevitable consequence arising from the design of LLMs asthey are currently formulated, and that the connection betweenbias and fundamental properties of language models needs to be",
  "Robustness and Security": "Business problems: How do we measure and improve the ro-bustness of LLMs and other generative AI models and applicationsagainst minor prompt perturbations, natural distribution shifts, andother unseen or challenging scenarios? Howdo we safeguard LLMsagainst manipulative eorts by bad actors to (jail-)break alignment,reveal system prompts, and inject malicious instructions into prompts(also called prompt injection attacks )? Solution approaches: Many techniques proposed for measuringand improving robustness in NLP models can be adopted or ex-tended for LLMs. In particular, the following ideas and notionscould be relevant for LLMs: denitions, metrics, and assumptionsregarding robustness (such as label-preserving vs. semantic-preserving);connections between robustness against adversarial attacks and ro-bustness under distribution shifts; similarities and dierences inrobustness approaches between vision and text domains; model-based vs. human-in-the-loop identication of robustness failures.Mitigation approaches involve learning invariant representations,and ensuring models do not rely on spurious patterns using tech-niques like data augmentation, reweighting, ensembling, inductive-prior design, and causal intervention . Open-source evalua-tion frameworks and benchmarks such as Stanford HELM ,Eleuther Harness , LangTest , and Fiddler Auditor canbe utilized for benchmarking dierent LLMs and evaluating robust-ness in application-specic settings.LLMs have been shown to be vulnerable to adversarial pertur-bations in prompts , prompt injection attacks , data poi-soning attacks , and universal and transferable adversarial at-tacks on alignment . Several benchmarks have been proposedfor red-teaming / testing LLMs against adversarial attacks and re-lated issues . Metrics for quantifying LLM cybersecu-rity risks, tools to evaluate the frequency of insecure code sugges-tions, and tools to evaluate LLMs to make it harder to generatemalicious code or aid in carrying out cyberattacks have also beenproposed . Additional discussion and approaches can be foundin survey articles by Barrett et al. and Yao et al. . Open challenges: A key challenge is to ensure that robustnessand security mechanisms are not intentionally or unintentionallyremoved in the process of netuning an LLM . Another chal-lenge lies in ensuring that the mechanisms work not just duringevaluation but also during deployment (e.g., not subject to decep-tive attacks ). A broader challenge is to investigate robustness,security, and safety of systems that could be composed of multipleLLMs. For example, it has been shown that adversaries can mis-use combinations of models by decomposing a malicious task intosubtasks, leveraging aligned frontier models to solve hard but be-nign subtasks, and leveraging weaker non-aligned models to solveeasy but malicious subtasks . As such attacks do not require thealigned frontier models to generate malicious outputs and hence",
  "Privacy, Unlearning, and CopyrightImplications": "Business problems: How do we ensure that LLMs, diusion mod-els, and other generative AI models do not memorize training datainstances (including personally identiable information (PII)) andreproduce such data in their responses? How do we detect PII inLLM prompts / responses? How do prevent copyright infringe-ment by LLMs? How can we make an LLM / generative AI modelforget specic parts, facts, or other aspects associated with thetraining data? Solution approaches: Recent studies have shown that trainingdata can be extracted from LLMs and from diusion models (which could have copyright implications in case the modelis perceived as a database from which the original images or othercopyrighted data can be approximately retrieved). Several approachesfor watermarking (or otherwise identifying / detecting) AI generated content have been proposed. Detecting PII inLLM prompts / responses can be done using o-the-shelf packages,but may require domain-specic modications since what is con-sidered as PII could vary based on the application. Unlearning inLLMs , and more broadly, model disgorgement (the elim-ination of not just the improperly used data, but also the eectsof improperly used data on any component of an ML model) arelikely to become important for copyright and privacy safeguards,ensuring responsible usage of intellectual property, compliance,and related requirements as well for reducing bias or toxicity andincreasing delity.",
  "Open challenges: A key challenge would be designing practicaland scalable techniques. For example, how can we develop dieren-tially private model training approaches (e.g., DPSGD , PATE )2": "that are applicable for billions or trillions of parameters in gener-ative AI models? How can we ensure privacy of end users whenleveraging inputs from end users as part of retraining of LLMs (us-ing, say, PATE-like approaches)? Considering the importance ofhigh quality datasets for evaluating LLMs for truthfulness, bias, ro-bustness, safety, and related dimensions, and the challenges withobtaining such datasets in highly sensitive domains such as health-care, how do we develop practical and feasible approaches for dif-ferentially private synthetic data generation , poten-tially leveraging a combination of sensitive datasets (e.g., patienthealth records and clinical notes) and publicly available datasetsalong with the ability to generate data by querying powerful LLMs?",
  "Business problems: How can we deploy LLMs in a human-AI hy-brid setting to quantify the uncertainty (condence score) associ-ated with an LLM response and defer to humans when condence": "2Examples of dierentially private model training include DPSGD and PATE.While DPSGD operates by controlling the inuence of training data during gradi-ent descent, PATE transfers to a student model the knowledge of an ensemble ofteacher models, with intuitive privacy provided by training teachers on disjoint dataand strong privacy guaranteed by noisy aggregation of teachers answers.",
  "is low? Specically, how can we achieve this in high-stakes andlatency-sensitive domains such as AI models used in healthcaresettings?": "Solution approaches: Learning to defer in human-AI settings isan active area of research , necessitating uncertainty quanti-cation and condence estimation for the underlying AI models. Italso involves understanding the conditions under which humanscan eectively complement AI models . In the context of LLMs,recent approaches such as selective prediction, self-evaluation andcalibration, semantic uncertainty, and self-evaluation-based selec-tive prediction have been proposed (see references there-in). Open challenges: A key challenge is to ensure that self-evaluation,calibration, selective prediction, and other condence modeling ap-proaches for LLMs are eective in out-of-distributionsettings. Thisis particularly important for adoption in high-stakes settings likehealthcare. Another challenge is ensuring robustness of condencemodeling approaches against adversarial prompts.",
  "Transparency and Causal Interventions": "Business problems: How do we explain the inner workings andresponses of LLMs and other generative AI models, especially inscenarios requiring the development of end-user trust and meetingregulatory requirements? How can we modify factual associationslinked to an LLM without retraining it? Solution approaches: Explainability methods for LLMs have beenwell studied , including techniques such as Chain-of-ThoughtPrompting and variants. However, there is work on unfaith-ful explanations in chain-of-thought prompting , with con-nections to language model alignment through externalized rea-soning (getting models to do as much processing/reasoning throughnatural language as possible). Mechanistic interpretability isanother active area of research, which has the potential to be fur-ther accelerated by the availability of small language models likephi-2. Causal tracing approaches have been proposed to locate andedit factual associations in LLMs. This involves rst identifyingneuron activations that are decisive in the models factual predic-tions, and then modifying these neuron activations to update spe-cic factual associations . Open challenges: Analogous to the use of simpler approximatemodels for explaining complex predictive ML models (e.g., LIME),can we employ simpler approximate models to explain LLMs andother generative AI models (e.g., using approaches such as modeldistillation) in a faithful manner? Additionally, can we developmore ecient and practical causal intervention approaches?",
  "Grounding for LLMs": "Business problem: How do we ensure that responses generatedby an LLM are grounded in a user-specied knowledge base? Here,grounding means that every claim in the response can be attrib-uted to a document in the knowledge base. We distinguish between the terms grounding and factuality. While ground-ing seeks attribution to a user-specic knowledge base, factual-ity seeks attribution to commonly agreed world knowledge.In the context of grounding, the knowledge base may be a setof public and/or private documents, one or more Web domains, orthe entire Web. For instance, a healthcare company may want itschatbot to always produce responses that are grounded in a set ofhealthcare articles it consider authoritative. In addition to ground-ing to the knowledge base, one may also want responses to containcitations into the relevant documents in the knowledge base. Thisenables transparency and allows the end-user to corroborate allclaims in the response.",
  "Solution Approaches": "The emerging area of LLM operations deals with processes andtools for designing, developing, and deploying LLMs, as well asmonitoring LLM applications once they are deployed in produc-tion. Frameworks such as the following have been proposed toaddress potential harms and challenges pertaining to grounding,robustness, and evaluation in real-world LLM applications . Identication : Recognizing and prioritizing po-tential harms through iterative red-teaming, stress-testing,and thorough analysis of the AI system. Measurement : Establishing clear metrics,creating measurement test sets, and conducting iterative,systematic testingboth manual and automatedto quan-tify the frequency and severity of identied harms. Mitigation : Implementing tools and strate-gies, such as prompt engineering and content lters, to re-duce or eliminate potential harms. Repeated measurementsneed to be conducted to assess the eectiveness of the imple-mented mitigations. We could consider four layers of miti-gation at model, safety system, application, and positioninglevels. Operationalization : Dening and executing a de-ployment and operational readiness plan to ensure the re-sponsible and ethical use of AI systems. Depending on the domain requirements, an AI safety layer fordetecting toxicity and other undesirable outputs in realtime can beincluded between the model and the application. Measuring shiftsin the distribution of LLM prompts or responses could be helpful toidentify potential degradation of the model quality over time, andfurther this information can be combined with any user feedback signals to determine regions where the model may be underper-forming .Further, we need to dierentiate undesirable outcomes or fail-ures in LLM applications caused by adversarial attacks from fail-ures due to the LLMs behavior in an unexpected manner in certaincontexts. To address the latter class of unknown unknown fail-ures, we should not only perform extensive testing and red team-ing to preemptively identify and mitigate as many potential harmsas possible but also incorporate processes and mechanisms to reactquickly to any unanticipated harms during deployment. As an ex-ample, Microsoft introduced a new category of harms called Dis-paraging, Existential, and Argumentative harms as part of the re-sponsible AI evaluation for conversational AI applications in re-sponse to the unexpected behavior of the Bing AI chatbot as re-ported by a New York Times journalist .More broadly, the risk prole associated with frontier AI sys-tems is expected to expand in light of extensions of existing LLMs,e.g., multimodality, tool use, deeper reasoning and planning, largerand more capable memory, and increased interaction between AIsystems . Of these, tool use is considered to create severalnew risks and vulnerabilities.",
  "Open Challenges": "A key challenge is to classify potential risks associated with tooluse, AI agents, interaction between AI systems, etc., in terms of thelevel of attention and action needed now and at dierent points inthe future. This involves prioritizing investments to address suchrisks, especially in the following two areas: (1) Identifying failuremodes and tendencies of LLM-based applications: We need to pin-point how these applications can be led astray, and (2) Develop-ing new safety and monitoring practices: This involves leveragingmetrics like weight updates, activations, and robustness statistics,which are not currently available as part of LLM APIs.",
  "Grounding and Evaluation for Large Language Models: Practical Challenges and Lessons Learned (Survey)KDD 24, August 2529, 2024, Barcelona, Spain": "the bad, and the ugly. High-Condence Computing (2024), 100211. Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yun-hang Shen, Ke Li, Xing Sun, and Enhong Chen. 2023.Woodpecker: Hallu-cination correction for multimodal large language models.arXiv preprintarXiv:2310.16045 (2023). Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, SoumyaSanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. 2023. Generate ratherthan Retrieve: Large Language Models are Strong Context Generators. In TheEleventh International Conference on Learning Representations. Hanning Zhang, Shizhe Diao, Yong Lin, Yi Fung, Qing Lian, Xingyao Wang,Yangyi Chen, Heng Ji, and Tong Zhang. 2024.R-Tuning: Instructing LargeLanguage Models to Say I Dont Know. In Proceedings of the 2024 Conferenceof the North American Chapter of the Association for Computational Linguistics:Human Language Technologies (Volume 1: Long Papers). 71067132. Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai,Shuaiqiang Wang, Dawei Yin, and Mengnan Du. 2024. Explainability for largelanguage models: A survey. ACM Transactions on Intelligent Systems and Tech-nology 15, 2 (2024), 138. Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng,Fangcheng Fu, Ling Yang, Wentao Zhang, Jie Jiang, and Bin Cui. 2024. Retrieval-Augmented Generation for AI-Generated Content: A Survey. arXiv:2402.19473 Ruochen Zhao, Hailin Chen, Weishi Wang, Fangkai Jiao, Xuan Long Do, Cheng-wei Qin, Bosheng Ding, Xiaobao Guo, Minzhi Li, Xingxuan Li, and Shaq Joty.2023. Retrieving Multimodal Information for Augmented Generation: A Sur-vey. In Findings of the Association for Computational Linguistics: EMNLP 2023.47364756. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer,Jiao Sun, Yuning Mao, XuezheMa, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, LukeZettlemoyer, and Omer Levy. 2023.LIMA: Less Is More for Alignment. InThirty-seventh Conference on Neural Information Processing Systems. Kaijie Zhu, Qinlin Zhao, Hao Chen, Jindong Wang, and Xing Xie. 2023. Prompt-Bench: A unied library for evaluation of large language models. arXiv preprintarXiv:2312.07910 (2023). Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chen-long Deng, Zhicheng Dou, and Ji-Rong Wen. 2023. Large language models forinformation retrieval: A survey. arXiv preprint arXiv:2308.07107 (2023). Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. 2023. Universaland transferable adversarialattacks on aligned language models. arXiv preprintarXiv:2307.15043 (2023).",
  "LLM Operations and Observability": "Business problems: What processes and mechanisms are impor-tant for addressing grounding and evaluation related challenges inreal-world LLM application settings in a holistic manner? How canwe monitor LLMs and other generative AI applications deployed inproduction for metrics related to quality, safety, and other responsi-ble AI dimensions? How can we anticipate and manage risks fromfrontier AI systems?",
  "Conclusion": "Given the increasing prevalence of AI technologies in our dailylives, it is crucial to integrate responsible AI methodologies into thedevelopment and deployment of Large Language Models (LLMs)and other Generative AI applications. We must understand thepotential harms these models may introduce, and leverage state-of-the-art techniques for enhancing overall quality, fairness, ro-bustness, and explainability. Addressing the responsible AI relatedharms and challenges not only reduces legal, regulatory, and rep-utational risks, but also safeguards individuals, businesses, and so-ciety as a whole. Moreover, there is a pressing need to establishways to quantitatively assess the performance, quality, and safetyof such models. Without comprehensive evaluations, establishingtrust in LLM-based applications becomes exceedingly dicult. Thegoal of this tutorial is to establish a foundation for the developmentof safer and more reliable generative AI applications in the future.",
  "shape the business problems, solution approaches, and open chal-lenges discussed in this article, and Mark Johnson and Qinlan Shenfor thoughtful feedback": "Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov,Kunal Talwar, and Li Zhang. 2016. Deep learning with dierential privacy. InCCS. Alessandro Achille, Michael Kearns, Carson Klingenberg, and Stefano Soatto.2024. AI model disgorgement: Methods and choices. Proceedings of the NationalAcademy of Sciences 121, 18 (2024). Google Cloud AI. 2024.Check grounding | Vertex AI Agent Builder. MicrosoftAzureAI.2024.Groundednessdetection. Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi.2024.Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reection. In The Twelfth International Conference on Learning Representations. Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, TomHenighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. 2021.A general language assistant as a laboratory for alignment.arXiv preprintarXiv:2112.00861 (2021). Sergul Aydore, William Brown, Michael Kearns, Krishnaram Kenthapadi, LucaMelis, Aaron Roth, and Ankit A Siva. 2021. Dierentially private query releasethrough adaptive projection. In International Conference on Machine Learning.PMLR, 457467. Xuechunzi Bai, Angelina Wang, Ilia Sucholutsky, and Thomas L Griths. 2024.Measuring implicit bias in explicitly unbiased large language models. arXivpreprint arXiv:2402.04105 (2024). Clark Barrett, Brad Boyd, Elie Bursztein, Nicholas Carlini, Brad Chen, JihyeChoi, Amrita Roy Chowdhury, Mihai Christodorescu, Anupam Datta, SoheilFeizi, et al. 2023. Identifying and mitigating the security risks of generative AI.Foundations and Trends in Privacy and Security 6, 1 (2023), 152. Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and MargaretMitchell. 2021. On the Dangers of Stochastic Parrots: Can Language ModelsBe Too Big?. In ACM Conference on Fairness, Accountability, and Transparency. Manish Bhatt, Sahana Chennabasappa, Cyrus Nikolaidis, Shengye Wan, IvanEvtimov, Dominik Gabi, Daniel Song, Faizan Ahmad, Cornelius Aschermann,Lorenzo Fontana, et al. 2023. Purple Llama CyberSecEval: A secure codingbenchmark for language models. arXiv preprint arXiv:2312.04724 (2023). Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, andAdam T Kalai. 2016. Man is to computer programmer as woman is to home-maker? Debiasing word embeddings. In NeurIPS. Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao,Leopold Aschenbrenner, Yining Chen, Adrien Ecoet, Manas Joglekar, JanLeike, et al. 2023. Weak-to-strong generalization: Eliciting strong capabilitieswith weak supervision. arXiv preprint arXiv:2312.09390 (2023). Aylin Caliskan. 2021. Detecting and mitigating bias in natural language pro-cessing. Brookings Institution (2021). Aylin Caliskan, Joanna J Bryson, and Arvind Narayanan. 2017. Semantics de-rived automatically from language corpora contain human-like biases. Science356, 6334 (2017). Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag,Florian Tramr,Borja Balle, Daphne Ippolito, and Eric Wallace. 2023. Extractingtraining data from diusion models. arXiv preprint arXiv:2301.13188 (2023). Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, ArielHerbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, UlfarErlingsson, Alina Oprea, and Colin Rael. 2021. Extracting Training Data fromLarge Language Models. In USENIX Security Symposium, Vol. 6. Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, JrmyScheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pe-dro Freire, et al. 2023. Open problems and fundamental limitations of reinforce-ment learning from human feedback. arXiv preprint arXiv:2307.15217 (2023). Chi-Min Chan, Chunpu Xu, Ruibin Yuan, Hongyin Luo, Wei Xue, Yike Guo,and Jie Fu. 2024. RQ-RAG: Learning to Rene Queries for Retrieval AugmentedGeneration. arXiv:2404.00610 Jiefeng Chen, Jinsung Yoon, Sayna Ebrahimi, Sercan Arik, Tomas Pster, andSomesh Jha. 2023. Adaptation with Self-Evaluation to Improve Selective Pre-diction in LLMs. In Findings of the Association for Computational Linguistics:EMNLP 2023. Tong Chen, Hongwei Wang, Sihao Chen, Wenhao Yu, Kaixin Ma, Xinran Zhao,Hongming Zhang, and Dong Yu. 2023. Dense X Retrieval: What Retrieval Gran-ularity Should We Use? arXiv:2312.06648 Maria De-Arteaga, Alexey Romanov, Hanna Wallach, Jennifer Chayes, Chris-tian Borgs, Alexandra Chouldechova, Sahin Geyik, Krishnaram Kenthapadi,and Adam Tauman Kalai. 2019. Bias in bios: A case study of semantic rep-resentation bias in a high-stakes setting. In FAccT. Leon Derczynski, Erick Galinkin, Jerey Martin, Subho Majumdar, and NannaInie. 2024. garak: A Framework for Security Probing Large Language Models. (2024). Kate Donahue, Alexandra Chouldechova, and Krishnaram Kenthapadi. 2022.Human-algorithm collaboration: Achieving complementarity and avoiding un-fairness. In Proceedings of the 2022 ACM Conference on Fairness, Accountability,and Transparency. Yi Dong, Zhilin Wang, Makesh Sreedhar, Xianchao Wu, and Oleksii Kuchaiev.2023. SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternativeto RLHF. In Findings of the Association for Computational Linguistics: EMNLP2023. 1127511288. Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Je Johnson, Gergely Szil-vasy, Pierre-Emmanuel Mazar, Maria Lomeli, Lucas Hosseini, and Herv J-gou. 2024. The Faiss library. (2024). arXiv:cs.LG/2401.08281 Shahul Es, Jithin James, Luis Espinosa Anke, and Steven Schockaert. 2024. RA-GAs: Automated Evaluation of Retrieval Augmented Generation. In Proceedingsof the 18th Conference of the European Chapter of the Association for Computa-tional Linguistics: System Demonstrations. 150158. Jaiden Fairoze, Sanjam Garg, Somesh Jha, Saeed Mahloujifar, Mohammad Mah-moody, and Mingyuan Wang. 2023. Publicly detectable watermarking for lan-guage models. arXiv preprint arXiv:2310.18491 (2023). Shangbin Feng, Weijia Shi, Yike Wang, Wenxuan Ding, Vidhisha Balachandran,and Yulia Tsvetkov. 2024. Dont Hallucinate, Abstain: Identifying LLM Knowl-edge Gaps via Multi-LLM Collaboration. arXiv:2402.00367 Isabel O Gallegos, Ryan A Rossi, Joe Barrow,Md Mehrab Tanjim, Sungchul Kim,Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen K Ahmed. 2024. Biasand fairness in large language models: A survey. Computational Linguistics(2024), 179. Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai,Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse,et al. 2022. Red teaming language models to reduce harms: Methods, scalingbehaviors, and lessons learned. arXiv preprint arXiv:2209.07858 (2022). Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Cha-ganty, Yicheng Fan, Vincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, andKelvin Guu. 2023. RARR: Researching and Revising What Language ModelsSay, Using Language Models. In Proceedings of the 61st Annual Meeting of theAssociation for Computational Linguistics. 1647716508. L Gao, J Tow, B Abbasi, S Biderman, S Black, A DiPo, C Foster, L Golding,J Hsu, A Le Noach, et al. 2023. A framework for few-shot language modelevaluation. Zenodo (2023). Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023. Enabling LargeLanguage Models to Generate Text with Citations. In Proceedings of the 2023Conference on Empirical Methods in Natural Language Processing. 64656488. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai,Jiawei Sun, Meng Wang, and Haofen Wang. 2024. Retrieval-Augmented Gen-eration for Large Language Models: A Survey. arXiv:2312.10997 Nikhil Garg, Londa Schiebinger, Dan Jurafsky, and James Zou. 2018. Wordembeddings quantify 100 years of gender and ethnic stereotypes. Proceedingsof the National Academy of Sciences 115, 16 (2018), E3635E3644. Sebastian Gehrmann, Elizabeth Clark, and Thibault Sellam. 2023. Repairing thecracked foundation: A survey of obstacles in evaluation practices for generatedtext. Journal of Articial Intelligence Research 77 (2023), 103166. Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart,and Jonathan Herzig. 2024. Does Fine-Tuning LLMs on New Knowledge En-courage Hallucinations? arXiv:2405.05904 Chenxi Gu, Chengsong Huang, Xiaoqing Zheng, Kai-Wei Chang, and Cho-JuiHsieh. 2022. Watermarking pre-trained language models with backdooring.arXiv preprint arXiv:2210.07543 (2022). Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Csar Teodoro Mendes, AllieDel Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kaumann, Gustavo deRosa, Olli Saarikivi, et al. 2023. Textbooks are all you need. arXiv preprintarXiv:2306.11644 (2023). Anisha Gunjal, Jihan Yin, and Erhan Bas. 2024. Detecting and preventing hal-lucinations in large vision language models. In AAAI. Zishan Guo, Renren Jin, Chuang Liu, Yufei Huang, Dan Shi, Linhao Yu, Yan Liu,Jiaxuan Li, Bojian Xiong, Deyi Xiong, et al. 2023. Evaluating large languagemodels: A comprehensive survey. arXiv preprint arXiv:2310.19736 (2023). Gyandev Gupta, Bashir Rastegarpanah, Amalendu Iyer, Joshua Rubin, and Kr-ishnaram Kenthapadi. 2023. Measuring Distributional Shifts in Text: The Ad-vantage of Language Model-Based Embeddings. arXiv preprint arXiv:2312.02337(2023). Suchin Gururangan, Ana Marasovi, Swabha Swayamdipta, Kyle Lo, Iz Beltagy,Doug Downey, and Noah A. Smith. 2020. Dont Stop Pretraining: Adapt Lan-guage Models to Domains and Tasks. In Proceedings of the 58th Annual Meetingof the Association for Computational Linguistics. Amit Haim, Alejandro Salinas, and Julian Nyarko. 2024. Whats in a Name?Auditing Large Language Models for Race and Gender Bias. arXiv preprint"
}