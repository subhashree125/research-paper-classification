{
  "ABSTRACT": "In Continual Learning (CL) contexts, concept drift typically refers to the analysis of changes in datadistribution. A drift in the input data can have negative consequences on a learning predictor andthe systems stability. The majority of concept drift methods emphasize the analysis of statisticalchanges in non-stationary data over time. In this context, we consider another perspective, wherethe concept drift also integrates substantial changes in the topological characteristics of the datastream. In this article, we introduce a novel framework for monitoring changes in multi-dimensionaldata streams. We explore variations in the topological structures of the data, presenting anotherangle on the standard concept drift. Our developed approach is based on persistent entropy andtopology-preserving projections in a CL scenario. The framework operates in both unsupervisedand supervised environments. To show the utility of the proposed framework, we analyze the modelacross three scenarios using data streams generated with MNIST samples. The obtained results revealthe potential of applying topological data analysis for shift detection and encourage further researchin this area.",
  "Introduction": "In continual learning scenarios, designing a machine learning (ML) model that is robust to distribution shifts is acrucial objective. Traditional ML methods are susceptible to data perturbations, and shifts in input data distributioncan significantly affect the models performance. Concept drift detectors encompass a family of techniques developedto analyze and detect distribution changes in the context of streaming data and time series. The concept is basedon changes in the statistical characteristics of the data over time . However, there are scenarios where certainmodifications in the distributions are not relevant. For example, simple translations or scaling of the data may notprovide meaningful information in some contexts. Therefore, it would be beneficial to define concept drift detectorsthat can detect distribution changes regardless of certain distortions or rotations of the data. There are objects thatare essentially equivalent to each other if we consider equivalence in the sense that it is possible to define a simplecontinuous transformation that approximates one object to the other. The essence of an object remains unchanged undersimple transformations, such as rotation, translation, scaling, and other types of continuous transformations . On theother hand, there are objects that are essentially different, as it is not feasible to find any continuous transformation totransform one object into another . Or at least, it is not easy to find such a transformation with low computationalresources. The field of Topological Data Analysis (TDA), specifically through the mathematical formalism of algebraictopology, defines these concepts of equivalences and differences between objects. Persistent Entropy (PE) is a measure",
  "Unsupervised Assessment of Landscape Shifts Based on Persistent Entropy and Topological Preservation": "Sebastin Basterrech, Jan Plato, Gerardo Rubino, and Micha Wozniak. Experimental Analysis on DissimilarityMetrics and Sudden Concept Drift Detection. In Ajith Abraham, Sabri Pllana, Gabriella Casalino, Kun Ma, andAnu Bajaj, editors, Intelligent Systems Design and Applications, pages 190199, Cham, 2023. Springer NatureSwitzerland. Sebastin Basterrech and Michal Wozniak. Tracking changes using Kullback-Leibler divergence for the continuallearning. In 2022 IEEE International Conference on Systems, Man, and Cybernetics (SMC), pages 32793285,2022.",
  "Background": "Drift detection techniques constitute a family of computational methods for detecting changes in the distribution of timeseries and data streams. Shifts in data distribution can occur in different forms, probably the most accepted categoriesare: sudden, gradual, and incremental shifts . A majority of drift detection techniques employ a classifier tocategorize incoming instances, and the predictor generates a class label for each input instance, which is then comparedto the actual class label. Subsequently, the accuracy is assessed and utilized as a tool to determine whether a drifthas occurred. When the classifiers accuracy significantly decreases, then it is assumed that the data distributionhas changed . In this scenario, the effectiveness of various ensemble classifiers has been examined .However, this approach can be applied only in a supervised context, and it requires the presence of ground truth labels,which are not always available. Another set of methods relies on empirically estimated distributions and statistical testsdirectly over the raw data . These approaches may be sensitive to outliers and noise, and raw data analysis (e.g.,applying density estimation) may also be affected by the curse of dimensionality . Several studies propose to",
  "Molding": ": Examples of objects that can be deformed and transformed into another object, equivalent in terms of topology.However, some shapes cannot be considered equivalent because it is not possible to define a sequence of simplecontinuous transformations to deform an original shape into another one while maintaining the original structure. Forinstance, from any of the digits at the top of the figure, we cannot form any of the digits at the bottom of the figure. compare summary of statistics and aggregation metrics of the raw data, for instance, Cumulative Sum and ExponentiallyWeighted Moving Average . For a more comprehensive review of the latest advancements in the use of datadescriptors for concept drift detection, see . Persistent homology is a key instrument in TDA as it may be used to describe the inherent structure of complex objectssuch as manifolds . Specifically, persistent homology studies the evolution of k-dimensional topological features(often referred to as holes) along a sequence of high-dimensional complex objects (named simplicial complexes) .We understand topological features as shapes or data that remain unchanged under certain continuous transformations,such as connected components, independent cycles, and holes . This process can be seen as tracking changes acrossfiltrations at multiple scales, following a specific algorithm that analyzes the connectivity information among the datapoints . Persistent Entropy, based in Shannon entropy, provides a summary of the information derived from persistenthomology . It is a measure for finding significant differences in the geometrical distribution of data points .For a comprehensive and detailed exploration about TDA and persistent homology, see .",
  "Methodology": "This section outlines the contributions made in this brief article. First, we discuss the approach for transformingthe input patterns into a different landscape that simplifies the analysis of drifts. Next, we introduce the process forestimating geometric changes between data points in different chunks. Finally, we present the sequence of modules thatcompose the developed framework.",
  "Creation of the latent space": "Monitoring and detecting distribution shifts is specially harder in the case of high-dimensional data. Although someattempts have been introduced in the literature for sparse multivariate time series , the scalability of existingalgorithms remains an issue. In particular, methods based on probability mass distribution encounter significantchallenges in high-dimensional spaces. In addition, the computation of distances between vectors also has limitationsin a high-dimensional space (e.g. Euclidean norm) , and a similarity analysis in the original input space may becomputationally expensive. Consequently, it is often more resource-efficient to first convert the data into a latent space,and then carry out the similarity analysis. For this reason, a common approach is to transform the input data into a latentspace, instead of making the analysis directly in the original space. We analyze an approach that projects the inputpoints into a latent space using dimensionality reduction (DR) techniques, which is a common method for handlingdata in high dimensions. Here, we investigate the projections generated by Self Organizing Maps (SOMs) (also calledKohonen networks), and we compare the results with other two popular DR models (a linear projection (PCA) and the",
  "Kernel PCA). The selection of an adequate data descriptor is crucial for ensuring a proper geometry in the latent spacepreserving the main features of the original space": "SOM is a bio-inspired method that combines concepts from Hebbian learning, vector quantization, and competitivelearning . Real-world data most often contain redundancies and inherent correlations among the variables. SOMis a two-layered neural network that transforms intricate relationships among high-dimensional data into straightforwardgeometric relationships on a standard lattice, typically a two-dimensional grid . Despite its simplicity, the SOMmodel is effective as a DR method, a clustering method, and a visualization tool for high-dimensional data .Another advantage is that the method is applicable to unsupervised problems and has the capability to preserve the mostimportant topological features of the reference data .",
  "Assessing shifts in the latent space": "Recently, it was introduced a clustering method based in SOM for assessing distribution changes in data streams withhigh dimensional data . SOM is used for projecting the input data into a latent space, then the analysis is donein the latent space, where the authors computed a distance matrix between the input pattern and cluster centers. Theassessment of the distribution shifts is done by applying a statistical summary. This approach of using a data descriptorwas also applied in , and is commonly used in methods based on kernel projections . Here, we modify theframework introduced in , which is based on distances and statistical summaries of the points in the latent space, toan approach that assesses topological changes according to the homological characteristics of the points in the latentspace. Once the DR mapping is done, a distance matrix is computed. The distance matrix has the information betweenthe projected point and the cluster centers. We use relative locations instead of directly working with the coordinatesof points in the latent space generated by the DR method. The coordinates are arbitrarily selected and often do notconsider any property of the data itself. There are even problems in cases where the coordinates are not natural inany sense . Therefore, the relative locations of the point cloud in the latent space are computed by calculating thedistances between the mass centers and the projected points. Hence, our focus is on the geometric properties of thelatent space, independent of the chosen coordinates in the latent space. This methodological approach is illustrated in. Note that the approach is general in the sense that any type of DR technique can be used.",
  "Case C:{1, 2, 3, 5, 7}{8}{0, 4, 6, 9}{1, 2, 3, 5, 7}": ": Creation of synthetic case studies. Data streams were generated with the MNIST samples interchangedamong the different topological types. The graphics illustrate the transition between the sequence of images from onetopological type to another type. iii) Embedding of the geometrical properties in the latent space: For each projected data point, the distancebetween the projected point and each centroid is computed. By centroid, we refer to the mass center of a cluster,which can be thought of as the average position of all the points within the cluster, or a representative data pointthat best exemplifies the clusters characteristics. Then, for each chunk, a matrix is created (with dimensionsof number of clusters Chunk size) where the column vector has the distances between a projected point andthe centroids. iv) Representation of topological features: A persistent diagram is created using the distance matrix described inthe previous step. The persistence diagram summarizes the information in the distance matrix (note that eachprojected point is computed a distance matrix). Then, the persistent entropy is computed from the collectionof persistence diagrams within the chunk, ignoring the infinity bar . The representation of topologicalfeatures returns a real vector with a number of elements equal to the size of the chunk. v) Statistical analysis. We compute a final index that corresponds to a comparison between the representation oftopological features in the current chunk and the representation of topological features in a reference chunk. Inour experiments, we compare two consecutive chunks. The comparison between these two vectors is doneusing a non-parametric test. In our experiments, we applied the Mann-Whitney U test. However, analysis ofsimilarity between these two vectors can be done using other tools. The statistical test provides a p-value score,which we use for monitoring changes between a current chunk of data and a reference chunk of data. Finally, when the global procedure is applied online, it generates a sequence of p-value scores. This sequence providesinformation about significant changes in the topological properties of the data stream. Note, an initial training phaseis performed to compute the initial clustering and its representative mass centers. An initial time window of the datastream is used for training the SOM weights and other global parameters. After this initial training phase, we continuethe learning process following a usual continual learning scenario. The procedure described previously can include anadditional step that involves adjusting the DR technique once a shift is detected. When a shift is detected, the SOMmethod can be re-trained using an arbitrary batch of data (for example, the last chunk or the last n chunks of data), withthe precautions to avoid catastrophic forgetting. This online calibration procedure was not investigated in this initialempirical exploration and could be a promising direction for future research.",
  "Experimental results": "In this section, we explore the utility of the proposed approach for monitoring and detecting topological changes in adata stream in the context of continual learning. We designed the experiments to evaluate and contrast the efficacy of thedimensionality reduction methods previously discussed: PCA, Kernel PCA, and SOM. There exists a deficiency in theavailability of extensive and diverse real-world data streams within high-dimensional spaces for analyzing the impactof distribution changes . This inconvenience is more relevant in the domain of unsupervised analysis of streamingdata. As a consequence, we created three synthetic data streams with annotated shifts. Our analysis is unsupervised(we do not use the data labels); we use the information about the annotated shifts to evaluate the performance of theproposed approach. The created stream has samples from MNIST . The annotations indicate the time stamps wherethe topological differences between two cloud of points were injected.",
  "Benchmark data": "We generated three synthetic datasets using the MNIST dataset , following the methodology outlined in .The procedure consists of creating a stream with chunks of samples that follow a specific distribution, and thenalternating these chunks with chunks of instances from a different distribution. By construction, the time stamps of thedistribution changes are arbitrary predefined. As a consequence, we have marked the exact time stamps at which a driftwas injected to evaluate the capacity of the approach in monitoring shifts. We created the data to check if the methodcan detect essential changes between sequences of digits. We divided the digits into three groups: those withoutany holes (zero-dimensional homology), those with one hole (one-dimensional homology), and those with two holes(one-dimensional homology with two loops). We denote the three experimental studies as: A, B and C. For the threecases, we analyze 20000 samples. For case studies A and B, the drift is injected every 1000 samples, and for scenarioC, the drift occurs every 500 samples. In case study A, we analyze a data stream where the changes occur betweenchunks with digits in {1, 3, 5, 7} (zero-dimensional homology) and chunks with digits in {0, 6, 9} (one-dimensionalhomology). Case study B also includes the digits 2 and 4. These digits are problematic due to variations in handwriting.Some individuals write the numbers 2 and 4 without any hole, while others write them with one hole, depending onindividual handwriting style. Then,case study B has a data stream considering the following exchanges between pointsin {1, 2, 3, 5, 7} and {0, 4, 6, 9}. Finally, in case study C, we evaluate a data stream that includes the number 8, which isnot topologically equivalent to any of the other digits. Case C exchanges samples from the three subsets {1, 2, 3, 5, 7},{0, 4, 6, 9}, and {8}. depicts how the streaming data was generated for each of the three scenarios. Case studyA and B have 20 exchanges between subsets containing digits with and without holes, and case study C involves 40exchanges between subsets containing digits without holes, with one hole, and two holes.",
  "Experimental settings": "For each of the three case studies, we used the first 20% of samples for the initial setup, and we train the parametersof the SOM algorithm. This training was made offline, as a pre-phase of the continual learning process. The SOMalgorithm has a grid with 10 10 neurons. We also analyzed three values for the chunk size parameter {50, 100, 250}.The quality assessment of the monitoring for the shifts was done using a p-value computed with the non-parametricMann-Whitney U test. We also evaluate the approach by applying PCA and Kernel-PCA instead of SOM, i.e. we applyPCA and Kernel-PCA in step (ii) of the procedure described in .3. The same initial time window used forSOM was applied to configure PCA and Kernel PCA. Then, we apply the trained DR methods to project the data pointsinto a latent space, following a CL setting.",
  "Results": "illustrates the latent space generated by the SOM projections of the data in case C. The curve (blue dots) inthe figure represents the mean distance between the projected points and the mass centers of the clusters. It shows theevolution of the mean values for each of these distance matrices. In addition, shows the results of applyingPruned Exact Linear Time (PELT) algorithm to detect changes in the sequence of these mean values. The PELTmethod is recognized for its computational efficiency, when compared with other change-point detection techniques .This experiment was done offline with the purpose of visualizing the complexity of the problem. The aim was to showthat detecting shifts in the generated data is not sufficient by merely applying a DR method and computing the meandistance to the mass centers. We consider as an illustration of the data stream characteristics for case studyC. This figure also shows the detected change points using the PELT technique. The background colors representthe changes detected by PELT, and the vertical green lines indicate the injected shifts. The visualization using PELTserves as a representation of the problems complexity. It was conducted independently of the other experiments, inwhich we compare the approaches using SOM, PCA, and Kernel-PCA. In other words, we are not comparing PELT tothe other techniques; this experiment was conducted to evaluate whether the generated data presents difficulties for awell-known technique for change point detection. The other experiments simulate a continual learning environment,making them even more complex than when the problem is addressed offline. presents the results for thethree DR techniques. These techniques were evaluated using case study C. The data stream was split into chunks of250 samples. The vertical lines (represented by green dashed lines) indicate the injected shifts. The horizontal linerepresents a p-value of 0.05. According to the results, the linear projection is unable to accurately predict the shifts. Thisobservation is consistent with other studies in the literature that discuss the limitations of linear projections in detectingdistribution shifts . The performance of SOM appears to be slightly better to that of Kernel-PCA. For instance,refer to the p-values in the chunks between 30 and 35. Chunk size is a crucial parameter for experiments conducted inonline settings. The influence of the chunk size is shown in Figures 7, 8, and 9, as well as in . showsthe results of using linear projections before the analysis of the persistent entropy. There are two graphs: the top graphshows the results for chunks with 50 instances, and the bottom graph shows the results for chunks of 100 samples. Thisfigure also highlights the limitations of linear projections for solving this specific problem, as the method provides fewalarms and detects only a low number of drifts. presents the results of Kernel PCA for chunks with 50 and100 samples. Similar graphics are depicted in where the results of SOM for chunks with 50 and 100 samplesare presented. summarizes the results of our experiments, including an evaluation of the impact of chunk size.The last two columns show the flags generated by the model using p-values with significance levels of 0.05 and 0.1.Additionally, we show the number of injected drifts in the experiment. Note that this number is an approximation dueto the anomalies that can exist in the datasets (digits with a different number of holes than expected). According tothe table, it seems that SOM may provide results using p-values with a 0.05 level of significance, while Kernel-PCAobtains better results when a p-value with a 0.1 level of significance is considered. As one might intuitively expect,smaller chunks decrease the quality of geometric pattern analysis. In contrast, larger chunks can encompass more thanone shift. The optimal chunk size should be determined through experimentation, taking this trade-off into account.",
  "Conclusions and future work": "We introduced a novel approach to concept drift detection, leveraging algebraic topology and persistent entropy. Webroaden the scope of concept drift, which is typically associated solely with statistical distribution changes in incomingdata. In the explored approach, we also integrate the definition of drift to detect significant changes in topologicalfeatures. The framework uses the SOM algorithm to transform the input space, reducing its dimensionality whilepreserving its topological characteristics. Then, the analysis of data drifts is conducted in the latent space. Weexplore the potential of persistent entropy to identify significant differences among data from consecutive chunks.We showed the performance of the method over three case studies (based on the MNIST dataset), and we comparedthe performance with PCA and Kernel-PCA. The proposed method does not make any assumption about the datadistribution. Additionally, it can be applied to both supervised and unsupervised problems. We believe that this work isan initial step towards applying TDA in the area of concept drift. A potential direction for future research could involveevaluating the framework with different types of data streams, including dynamic graphs. Furthermore, it would beinteresting to compare persistent entropy with other measures.",
  "Andrzej Lapinski, Bartosz Krawczyk, Pawel Ksieniewicz, and Michal Wozniak. An Empirical Insight IntoConcept Drift Detectors Ensemble Strategies. pages 18, 07 2018": "B. I. F. Maciel, S. G. T. C. Santos, and R. S. M. Barros. A lightweight concept drift detection ensemble. In 2015IEEE 27th International Conference on Tools with Artificial Intelligence (ICTAI), pages 10611068, Nov 2015. Isvani Inocencio Frias Blanco, Jose del Campo-Avila, Gonzalo Ramos-Jimenez, Rafael Morales Bueno, AgustinAlejandro Ortiz Diaz, and Yaile Caballero Mota. Online and non-parametric drift detection methods based onhoeffdings bounds. IEEE Trans. Knowl. Data Eng., 27(3):810823, 2015.",
  "Line H. Clemmensen and Rune D. Kjrsgaard. Data Representativity for Machine Learning and AI Systems.arXiv preprint arXiv:2203.04706, 2023": "Fabian Hinder, Andr Artelt, and Barbara Hammer. Towards Non-Parametric Drift Detection via DynamicAdapting Window Independence Drift Detection (DAWIDD). In Proceedings of the 37th International Conferenceon Machine Learning, ICML20. JMLR.org, 2020. Piotr Sobolewski and Michal Wozniak. Concept drift detection and model selection with simulated recurrence andensembles of statistical detectors. Journal of Universal Computer Science, 19(4):462483, feb 2013.",
  "Gordon J. Ross, Niall M. Adams, Dimitris K. Tasoulis, and David J. Hand. Exponentially weighted movingaverage charts for detecting concept drift. Pattern Recognition Letters, 33(2):191 198, 2012": "Kamil Faber, Roberto Corizzo, Bartlomiej Sniezynski, Michael Baron, and Nathalie Japkowicz. WATCH:Wasserstein Change Point Detection for High-Dimensional Time Series Data. In 2021 IEEE Int. Conf. on BigData (Big Data), pages 44504459, 2021. Fabian Hinder, Valerie Vaquet, and Barbara Hammer. Suitability of Different Metric Choices for Concept DriftDetection. In Tassadit Bouadi, Elisa Fromont, and Eyke Hllermeier, editors, Advances in Intelligent DataAnalysis XX, pages 157170, Cham, 2022. Springer International Publishing. Stephan Rabanser, Stephan Gunnemann, and Zachary C. Lipton. Failing loudly: An empirical study of methods fordetecting dataset shift. In 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver,Canada, 2019. Bastian Rieck, Matteo Togninalli, Christian Bock, Michael Moor, Max Horn, Thomas Gumbsch, and KarstenBorgwardt. Neural Persistence: A Complexity Measure for Deep Neural Networks Using Algebraic Topology. InInternational Conference on Learning Representations, 2019. Doruk Oner, Adlie Garin, Mateusz Kozinski, Kathryn Hess, and Pascal Fua. Persistent Homology With ImprovedLocality Information for More Effective Delineation. IEEE Transactions on Pattern Analysis and MachineIntelligence, 45(8):1058810595, 2023.",
  "Mingsheng Shang, Ye Yuan, Xin Luo, and Mengchu Zhou. An --Divergence-Generalized Recommender forHighly Accurate Predictions of Missing User Preferences. IEEE transactions on cybernetics, PP, 2022": "Jie Zhang, Zhi Wei, Zhenyu Yan, MengChu Zhou, and Abhishek Pani. Online Change-Point Detection in SparseTime Series With Application to Online Advertising. IEEE Transactions on Systems, Man, and Cybernetics:Systems, 49(6):11411151, 2019. Aleksandros Sobczyk and Mathieu Luisier. Approximate Euclidean lengths and distances beyond Johnson-Lindenstrauss. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances inNeural Information Processing Systems, volume 35, pages 1935719369. Curran Associates, Inc., 2022.",
  "Y. Le Cun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.(11):22782324, 1998": "Sebastin Basterrech, Andrzej Kasprzak, Jan Platos, and Michal Wozniak. A Continual Learning System withSelf Domain Shift Adaptation for Fake News Detection. In 10th IEEE International Conference on Data Scienceand Advanced Analytics, DSAA 2023, Thessaloniki, Greece, October 9-13, 2023, pages 110. IEEE, 2023. Lars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, Andreas Mueller, Olivier Grisel, Vlad Niculae,Peter Prettenhofer, Alexandre Gramfort, Jaques Grobler, Robert Layton, and Jake VanderPlas. Api design formachine learning software: experiences from the scikit-learn project. In ECML PKDD Workshop: Languages forData Mining and Machine Learning, pages 108122, 2013."
}