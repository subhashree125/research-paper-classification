{
  "Abstract": "The rapid growth of academic publications has exacerbated theissue of author name ambiguity in online digital libraries. Despiteadvances in name disambiguation algorithms, cumulative errorscontinue to undermine the reliability of academic systems. It is es-timated that over 10% paper-author assignments are rectified whenconstructing the million-scale WhoIsWho benchmark. Existing en-deavors to detect incorrect assignments are either semantic-basedor graph-based approaches, which fall short of making full use of therich text attributes of papers and implicit structural features definedvia the co-occurrence of paper attributes. To this end, this paperintroduces a structure-enhanced language model that combines keystructural features from graph-based methods with fine-grainedsemantic features from rich paper attributes to detect incorrectassignments. The proposed model is trained with a highly effec-tive multi-modal multi-turn instruction tuning framework, whichincorporates task-guided instruction tuning, text-attribute modal-ity, and structural modality. Experimental results demonstrate thatour model outperforms previous approaches, achieving top perfor-mance on the leaderboard of KDD Cup 2024. Our code has beenpublicly available1.",
  "Equal contribution.Work was done when Yunhe interned at Zhipu AI.Fanjin Zhang and Jie Tang are the corresponding authors.1": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from , July 2017, Washington, DC, USA 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-x-xxxx-xxxx-x/YY/MM ACM Reference Format:Yunhe Pang, Bo Chen, Fanjin Zhang, Yanghui Rao, and Jie Tang. 2024. MIND:Effective Incorrect Assignment Detection through a Multi-Modal Structure-Enhanced Language Model. In . ACM, New York, NY, USA, 10 pages.",
  "Introduction": "The past decades have witnessed a substantial proliferation of re-search papers across all fields of science, with platforms like GoogleScholar2 , Semantic Scholar3 , and AMiner4 in-dexing over 300 million papers. As a result, despite the availabilityof advanced author name disambiguation algorithms , recentstudies have highlighted significant error rates of paper-author as-signments in digital libraries, with exceeding 10% error rate whenconstructing the million-scale WhoIsWho benchmark . Theseinevitable cumulative errors from imperfect assignment methodssignificantly impact the efficacy of subsequent assignments, evenleading to distorting authors citation count and misallocation ofcredit. Therefore, an inherent self-correcting mechanism to auto-matically detect and rectify these errors is crucial for maintainingthe reliability of academic systems.In this paper, we investigate the INcorrect assignment Detection(IND) problem , which aims to detect incorrect assignments ofauthors within the paper due to the ambiguity of the author name,as shown in (a). Previous efforts typically employgraph-based algorithms, illustrated in (b), which build papersimilarity graphs by considering papers as nodes and establishingedges between papers through key co-occurrence relationshipssuch as co-authorship and co-organization, and then perform nodeclassification to detect outlier papers. However, these methods focusmore on key structural features and implicitly incorporate semantictextual features via input node features, but lack characterizing thefine-grained semantic features embodied in rich attributes of papers.Recently, large language models (LLMs) demonstrate theirstrengths across various natural language understanding and gen-eration tasks, endowed by their intrinsic capability to capture morefine-grained correlations through the self-attention mechanism on",
  "-MIND-83.511.3": "the input text. Some researchers endorse leveragingLLMs as the backbone and designing appropriate instruction tem-plates to detect incorrect assignments. An example template foraddressing the IND task is shown in (c). However, LLM-based methods suffer from modeling long contexts efficiently andeffectively, especially when an author has over 1,000 papers andeach paper possesses rich attributes (e.g., title, venue, author list,etc.). Furthermore, LLMs fail to model structural features of thegraph natively. Therefore, a unified model that captures long textattributes and graph structural patterns efficiently and effectivelyis in great demand. Present Work. Inspired by the aforementioned insights, we pro-pose a Multi-modal structural-semantic language model for INcor-rect assignment Detection (MIND) that combines the strengths ofkey structural feature extraction endowed by graph-based methodsand the fine-grained semantic feature characterized by languagemodels via effective multi-modal fusion for incorrect assignmentdetection. Practically, we adopt a multi-modal multi-turn instruc-tion tuning framework to incorporate each modality step by stepas follows: (1) Task-Guided Multi-Turn Instruction Tuning: In the firststage, we aim to align the language model backbone to tacklethe IND task via an instruction-tuning regime. Specifically, wedesign a task-specific instruction template, as shown in ,which takes the target authors papers and the target papers asinput and asks the LLMs to generate the label token supervisedby ground truths. To foster context sharing and improve train-ing/inference efficiency, we further design a multi-turn chatinstruction template, which significantly improves the accuracyand efficiency of our model. (2) Semantic Embedding Module with Rich Attributes: Lim-ited by the context length, LLMs can only take papers keyfeatures as input. To involve more informative paper attributesinto our framework, we employ a semantic embedding modulethat extracts and summarizes rich paper attributes through apre-trained language model, and then utilizes a text projectorto obtain a special text token that serves as the summarizedsemantic feature of each paper. (3) Structural Embedding Module: To endow LLMs with the ca-pacity to capture structural information, we employ a structuralembedding module that extracts and summarizes structural fea-tures from the graph-based methods, and then utilizes a graphprojector to obtain a special graph token that acts as the struc-tural feature of each paper w.r.t. corresponding target author. Through these three successive stages of multi-modal instructiontuning, both structural features and rich semantic characteristics aredynamically and effectively fused to perform accurate and robustincorrect assignment detection.Extensive experimental results highlight the superiority of ourproposed method on the KDD Cup 2024, as demonstrated in .Notably, even without intricate ensemble strategies, our modeloutperforms the previous best single model of the Top-1 methodsignificantly. With a simple ensemble strategy, our model achievesthe top position on the KDD Cup WhoIsWho-IND leaderboard5.",
  "Related Work2.1Author Name Disambiguation": "Author name disambiguation, can be categorized into three specifictasks : from-scratch name disambiguation (SND), real-time namedisambiguation (RND), and incorrect assignment detection (IND).We first briefly review methods for SND and RND, and then focuson IND methods in particular.From-scratch name disambiguation (SND), a foundationalcomponent in the construction of academic search systems, aimsto partition papers associated with the same name into disjoint",
  "MIND: Effective Incorrect Assignment Detection through a Multi-Modal Structure-Enhanced Language ModelConference17, July 2017, Washington, DC, USA": "0.65 0.70 0.75 0.80 AUC AOVT TA-base TA+sem TA+graph TO-base TO+sem TO+graph TV-base TV+sem TV+graph : Ablation studies on different paper attributes as theLLM input. A, O, V, and T denote using the author, organi-zation, venue, or title attributes from a paper as key informationinput into the LLM, respectively. Similarly, TA, TO, and TVrepresent combinations of two of these features as the key input. indicate that throughout the three stages, the Llama3-8B consis-tently achieves the best performance among the models, followedby Qwen2-7B. For all three models, a consistent stepwise outperfor-mance is observed in each stage, demonstrating the effectiveness ofour three-stage training strategy, which robustly integrates multi-source features in a progressive manner. 5.4.2Effect of different paper attributes. illustrates theimpact of paper attributes used in our framework. We first alterthe input paper attributes in the first stage of our framework (with-out semantic embedding and structural embedding module). Weobserve that the best performance for a single attribute is achievedby paper titles, followed by paper venues. The reason may lie inthat titles and venues encompass rich domain-specific information,which can be well-captured by the LLM. In contrast, organizationand author attributes are ad-hoc personal features, often used aseffective co-occurred structural features, which are arduous to beutilized by the LLMs. Furthermore, the superiority of the title at-tribute over the venue is also partly attributed to the finer-grainedtopical information conveyed in titles.To further investigate the synergistic effects between attributes,we conduct the complete training stages by employing the bestsingle attribute (i.e., title) and another paper attribute. Title+venuehas a small edge over the single usage of the paper title (+0.3%in terms of AUC), possibly due to that titles and venues embodysimilar domain-specific features. By contrast, both Title+org andTitle+author each deliver significant improvements over the sin-gle title attribute, showing +1.6% and +1.4% increase in terms ofAUC, respectively. These improvements are likely due to the com-plementary nature of the title attribute when combined with eitherorg or author. 5.4.3Effect of different text projectors. We try several types oftext projectors with increased model complexity, including a singlelinear layer, a two-layer feed-forward network FFNSwish, and Q-Former . The Q-Former is frequently used in multi-modal LLMs",
  "The IND problem is commonly viewed as a graph-based anomalydetection task . Recently, LLM-based methods have gained moreattention due to remarkable breakthroughs achieved by LLMs": "2.2.1Graph-based methods. Graph-based methods first constructa paper similarity graph for each target author based on the simi-larity between papers attributes (e.g., co-author, co-organization).Some early works employ the graph neural network (GNN)as an encoder to get papers hidden features and then perform bi-nary node classification. Recently, the IND task has been regardedas an anomaly detection task on graphs. For instance, GCCAD further employs contrastive learning to contrast abnormal nodesand normal ones, and it removes suspicious links during the mes-sage passing of GNNs to reduce the impact of erroneous edges. 2.2.2LLM-based methods. LLM-based methods regard the INDproblem as a natural language generation task. The OAG-Bench initially explores the feasibility of applying LLMs to the IND prob-lem, which defines a custom instruction to query the LLM whether atarget paper belongs to the target author (i.e., a paper list). However,this preliminary attempt solely utilizes papers titles.During KDD Cup 2024, some contest winners furtherincorporate more paper attributes via concatenation as LLM inputs.However, naively extending the input length of LLMs requires muchmore GPU memory and training/inference time. Furthermore, theyabandon graph-based features, thereby failing to effectively utilizehigh-order structural similarity features of paper similarity graphs.In contrast, our model integrates high-order similarity infor-mation derived from graph neural networks into the LLM andmakes full use of multi-source feature information in the context ofthe limited input length. Furthermore, by constructing an efficientmulti-round query strategy, we achieved significant acceleration inboth training and inference.",
  "Definition 3.2. Author. An author contains a paper set, i.e., = {1, . . . , }, where is the number of papers authored by": "Problem 3.3. Incorrect Assignment Detection (IND) . Givena conflated author entity = {, . . . , , , . . . , , . . . , , . . . , }coming from multiple authors {1, . . . ,}, where1 = {, . . . , },2 = {, . . . , }, = {, . . . , }, assuming 1 covers the high-est percentage of papers within , the objective is to detect outlierpapers owned by {2, ...,}.",
  "Model Framework": "Previous IND methods typically adopt LLM-based methods orgraph-based methods for incorrect assignment detection. Graph-based methods fall short of capturing the semantic relation acrossnumerous academic papers. By formalizing the IND problem as aquestion-answering task, LLM-based methods excel in capturingintricate semantic correlations among authors papers. However,large language models struggle to model graph structural patternsand efficiently handle rich paper attributes as long texts.In light of this, we propose MIND, a multi-modal frameworkthat integrates rich paper attributes and structural features using alarge language model (LLM) to address incorrect assignment detec-tions. MIND initially employs an effective multi-turn instructiontemplate to guide the LLM for IND tasks (see .1). Afterthat, we enhance this by introducing rich paper attributes into theIND-instructed LLM through a semantic embedding module. Thismodule summarizes the rich text attributes, projecting them assemantic tokens into the LLM(see .2). Subsequently, weintegrate node and graph features from graph neural networks toincorporate structural patterns characterized by paper-author rela-tional graphs, projecting them as structural tokens into the LLM(see .3). The progressive training algorithm is adoptedto guarantee the effective integration of different modalities (see.4). In the following sections, we delve into the details ofeach component.",
  "Task-Guided Multi-Turn Instruction Tuning": "Prior arts usually adopt graph-based methods for the INDproblem. However, recent attempts suggest that large languagemodel-based methods hold promise in this task since it can capturesemantic correlations precisely between target papers and authorprofiles via self-attention mechanisms. Inspired by this, wetransform the IND problem into a question-answering task, definean instruction template by incorporating the target paper and thetarget authors profile, and then ask the LLMs to output whetherthe target paper belongs to the target author.Due to the limitations of context length and training efficiencyin LLMs, the target paper and author must be represented by con-cise yet informative tokens. For the target paper, we choose keyattributes like titles as tokens. The target author is represented bytheir set of published papers. Following this, we define contextualpapers as textual attributes of all papers (or a randomly sampledsubset of all papers if the token length exceeds). A query is then",
  "Instruction": ": The overall framework of MIND. The entire instruction template comprises three parts of input: 1) key information (e.g. title)from each paper is used as direct input tokens of the LLM. 2) textual embedding provided by the semantic embedding module serves as asummarized embedding of the rich attributes of each paper by replacing the token <text>; 3) structural embedding from the structuralembedding module is incorporated into the LLM by replacing the token <graph> in LLM input. Our framework trains different modulesprogressively in each modality to predict a special token <label_token>. input into the LLM to determine whether the target paper is au-thored by the target author, and to prompt the LLM to predict theoutput for a custom token <label_token>.However, considering the long contexts of LLM input inducedby rich author profiles (some authors publish over 1,000 papers),each instruction predicts one target paper causes many redundantcalculations and is thus inefficient. To this end, we design a multi-turn instruction for the IND problem. An illustration example ofthe instruction template is shown in . By stacking multiplelocal queries into the input, we generate multiple predictive resultsfor multiple target papers in an auto-regressive decoding paradigm.This approach significantly reduces the time required for modeltraining and inference, while only incurring an additional lengthexpense that is negligible compared to the contextual papers. Afterutilizing the multi-turn instruction technique, the training objectiveof each instruction is defined as",
  "=1log( | context)(1)": "where denotes the stacking times of the target paper and denotes the logits of the ground-truth label of <label_token>.Note that the self-attention mechanism of LLMs usually behaves asa causal mask attention format, which ensures that latter queriesare aware of former ones, thereby enriching the context for thelatter queries. Consequently, the hidden state of the earlier queriescan serve as \"soft\" in-context demonstrations for the prediction oflabel tokens in latter queries. This mechanism enables latter queriesto predict outcomes based on both the correlations between the current target paper and the target author profile, as well as therelevance of the current query and the former queries.During the inference phase, we directly take the normalized logitof token Yes\" and token No as the final logit of <label_token>.The final logit is calculated by:",
  "Semantic Embedding Module": "Due to the limitations of the context length inherent in LLMs, aswell as the increased GPU memory and time costs of both modeltraining and inference with longer context lengths, incorporatingall the paper features into the LLM input is prohibitive. Conse-quently, we could only select a subset of key features into the LLMfor the base model in .1. To involve more informativepaper attributes into our framework, we integrate the remainingpaper features and then summarize the semantic features througha semantic embedding module. To be specific, the semantic embed-ding module is a small pre-trained language model that transformspaper attributes into a sequence of embeddings. Additionally, weuse an adaptive pooling module to summarize these embeddings.This summary is then projected by a text projector to align thefeature space of the semantic embedding module with that of LLM.",
  "= FFNSwish()(5)": "where denotes the hidden representation of token and isthe summarized paper embedding.To integrate the summarized semantic features into the LLMinput, an external token <text> is introduced and strategically po-sitioned at the beginning of each papers input text. By replacing theoriginal embedding of this token in the LLM with the embedding projected from the semantic embedding module, the supplementarytextual features are effectively incorporated into the LLM input.",
  "Structural Embedding Module": "Existing attempts demonstrate that by building a paper simi-larity graph for each target author based on the attribute similaritysuch as co-authors and co-organizations, the outlier papers can beeffectively detected by leveraging graph topology. However, theexisting LLMs are not adept at capturing structural information.Therefore, to enhance the capacity to process structural information,we first construct a paper similarity graph for each target authorbased on the aforementioned relationships. Afterward, we traina state-of-the-art GNN-based anomaly detection method, i.e., GC-CAD , to obtain the hidden node embeddings and hidden graphembeddings. The idea of GCCAD is to perform contrastive learningby comparing abnormal nodes with normal ones in terms of theirdistances to the global graph representation (e.g., the average of allnodes). In what follows, the node and graph representations areconcatenated to form the structural features of each paper. Dueto the mirrored challenge in misalignment of feature spaces like.2, we utilize a graph projector that takes the concatenatedstructural features as input and outputs the graph token embeddingfor LLM. Let A denote the adjacency matrix of the constructedpaper graph and nd denote the node input features (initializedwith PLM-encoded title embeddings). The procedure of structuralembedding module can be formalized as follows:",
  "Progressive Instruction-Tuning": "To effectively fuse multi-modal features into our framework, thetraining procedure is meticulously segmented into three phasesto progressively incorporate uni-modal information individually.Initially, as introduced in .1, we begin with training thebase LLM utilizing parameter-efficient fine-tuning .Advancing to the second phase, we freeze the parameters ofthe previously trained LLM and integrate the semantic embeddingmodule. Pre-trained language model can yield meaningful semanticfeatures. Thus, the parameters of the PLM encoder are fixed andonly the text projector is trainable at this stage.In the terminal phase, employing a similar strategy, we freezeall parameters from the antecedent stages, as well as those of thegraph encoder, and concentrate solely on training the parametersof the graph projector. This final phase is dedicated to the seamlessintegration of the structural features derived from the graph neuralnetwork into the feature space of the LLM.This progressive training approach ensures a coherent and incre-mental fusion of diverse features. At each stage, the model parame-ters of previous stages are well-trained, in the sense that featuresof previously-considered modalities are effectively fused, thus de-creasing the difficulty of training parameters of the new modality.Extensive experiments also reveal the superiority of our trainingrecipe over other alternatives.At the inference stage of MIND, for each instruction encom-passing contextual papers of the target author and multiple targetpapers, our framework integrates the outputs of the semantic em-bedding module and structural embedding module for each paperinto the LLM input, and utilize the logit of <label_token> afterdecoding as the final prediction score.",
  "GCN uses RoBERTa to embed paper titles as node features,then uses GCN as an encoder to obtain node hidden features,followed by an MLP layer for binary classification": "GCCAD employs the same graph structure and node features asthe GCN method for graph contrastive learning, which performsnode classification based on the similarity between the nodeembeddings and graph embeddings. ChatGLM-IND leverages ChatGLM3-6B-32K as thefoundation model. It employs a similar instruction template asour base model but incorporates more paper attributes such astitles, authors, and organizations in LLM inputs. This method isthe best single model in OAG-Challenge at KDD Cup 2024. Our model has three variants according to the training phase. (1)MIND-base is the model that solely utilizes key paper featuresas the LLM input as described in Sec. (4.1); (2) +sem denotes theintegration of semantic embedding module as described in Sec. (4.2);(3) +graph means adding the structural embedding module afteradding the semantic embedding module as described in Sec. (4.3). 5.1.3Evaluation Metric. We adopt AUC and MAP as the evaluationmetrics, which are broadly adopted in related anomaly detectiontasks . In addition, we follow to obtain the final resultsby weighting the outliers for all authors, where the weight is theproportion of incorrect instances of each author relative to the totalnumber of incorrect instances.",
  "Implementation Details": "As for our framework, we utilize the Llama3-8B as thefoundation model and tune the model with LoRA technique .All experiments are conducted on an 8-card 80G Nvidia A100 server.We set the maximum input length to 8K, and use FlashAttention to reduce the memory consumption during training. For thesemantic embedding module, we concatenate each papers title,organizations, authors, and venue as raw input text, and feed it intoRoBERTa-base to get the summarized paper embeddings. Weutilize the same settings as those in GCCAD for the structuralembedding module.To deal with the class imbalance issue, we perform down-samplingfor each authors papers to keep the same number of positive andnegative instances.",
  "+graph0.7840.810": "to its increased capacity to model node/paper semantics and graphstructure. GCCAD outperforms GCN since GCCAD explicitly mea-sures the deviation of nodes from the global context of the graphvia contrastive learning. LightGBM also yields decent results at thecost of sophisticated feature engineering. Despite relying solelyon textual features, ChatGLM-IND demonstrates a significant ad-vantage over GNN-based methods, highlighting the remarkablecontextual comprehension capabilities of LLMs for the IND task.It is worth noting that our base model shares similar results withChatGLM-IND despite the use of a much shorter context length(8K vs. 32K). Apart from using the different foundation models, ourbase model additionally leverages multi-turn instruction templatesto accelerate model training and inference (ablation studies shownbelow). A significant improvement is observed upon further inte-gration of more complete paper features sourced from the semanticembedding module, preliminarily indicating the benefit of fusing in-formative textual features beyond the title for this task. Ultimately,the models efficacy is further enhanced by incorporating the struc-tural embedding module, which remedies the shortcomings of LLMsfor tackling graph structural features. The main results basicallyvalidate the effectiveness of our framework. In the following, weconduct extensive ablation studies to carefully verify the rationalityof our framework.",
  ": The comparison between ensemble strategy andfrom-scratch training w.r.t. training loss and AUC curve": "graph projector jointly; (2) Ensemble of two-modal training: inject-ing the semantic embedding module and structural embedding mod-ule separately and training corresponding projectors respectively,followed by averaging their prediction scores on the validation set.As shown in , the experimental results indicate that con-currently training the text projector and graph projector yields a1.3% improvement over MIND-base, but underperforms both pro-gressive and ensemble strategies. We speculate that it is difficult toachieve optimal parameters for the two projectors simultaneouslyin from-scratch training. Rendering similar performances to ensem-ble strategy, progressive training offers a more unified and elegantmodel architecture.In , we further visualize the changes in training loss andAUC on the validation set during the training process concerningfrom-scratch training and two-modal training (with semantic orstructural embedding modules). For two-modal training, the textprojector converges at about 250 training steps while the graphprojector converges at only about 50 training steps. However, from-scratch training, in terms of both AUC and training loss, appears tohave learned predominantly from a single source of graph features.",
  "Model Efficiency": "In this subsection, we compare the time efficiency of LLM-basedmethods, including the best baseline ChatGLM-IND and our modelvariants. presents the detailed time costs for training andinference. Compared to ChatGLM-IND, our base model leverages ashorter input length as well as multi-turn instructions, achievingboth fast training and inference speed. As the semantic embed-ding module and structural embedding module are progressivelyintegrated, the required time also increases gradually. In terms ofconvergence time, the graph projector converges quickly, whilethe text projector requires more time. We conjecture that the textencoder (i.e., pre-trained language models) is not pre-trained onthe IND task, thus requiring more time to train the text projectorfor feature alignment. Despite undergoing three training stages,our model still has a significant advantage over ChatGLM-IND. Interms of inference time, our framework is nearly ten times faster.",
  "Model Ensemble": "To unlock the potential of our framework, we further perform modelensemble by using different paper attributes in our framework.As shown in , the best two-model ensemble results areobtained by combining the title attribute (T) with the title andorganization attributes (T+TO). This combined approach results inan increase of 0.8% in AUC compared to the single usage of titleattributes in the LLM input. Simultaneously, the combination ofthe title attribute and another attribute (T+TA) also yields decentresults, leading to a 0.7% increase in AUC. These improvementsindicate that incorporating raw organization texts into the LLMinputs exhibits preferable performance compared with injectingthese information into the semantic embedding module. In contrast,the combination of the title and a different attribute (T+TV) resultsin a decrease in performance compared to using the title attributealone with a 0.2% reduction in AUC.Ultimately, by integrating the best three models (T+TO+TA), weachieve the best performance in terms of AUC.",
  ": Performance comparison of different ensemblemodels by utilizing various paper attributes": "Extensive experiments reveal that our model outperforms previousbest single-model method, and demonstrate significant trainingefficiency and inference efficiency compared with previous LLM-based IND methods. Employing a simple ensemble strategy, ourmodel achieves the top position on the KDD Cup 2024 leaderboard,underscoring its robustness and accuracy in detecting incorrect as-signments in academic knowledge graphs. Our idea of task-guidedinstruction tuning on the multi-modal structure-enhanced languagemodel exhibits the potential to foster future research toward ad-dressing downstream tasks of text-rich graphs.",
  "Chenxin An, Fei Huang, Jun Zhang, Shansan Gong, Xipeng Qiu, Chang Zhou,and Lingpeng Kong. 2024. Training-free long-context scaling of large languagemodels. arXiv preprint arXiv:2402.17463 (2024)": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shot learners. Advances in neuralinformation processing systems 33 (2020), 18771901. Lei Cen, Eduard C Dragut, Luo Si, and Mourad Ouzzani. 2013. Author disambigua-tion by hierarchical agglomerative clustering with adaptive stopping criterion.In Proceedings of the 36th International ACM SIGIR conference on Research anddevelopment in information retrieval. 741744. Bo Chen, Jing Zhang, Jie Tang, Lingfan Cai, Zhaoyu Wang, Shu Zhao, HongChen, and Cuiping Li. 2020. Conna: Addressing name disambiguation on the fly.IEEE Transactions on Knowledge and Data Engineering 34, 7 (2020), 31393152. Bo Chen, Jing Zhang, Fanjin Zhang, Tianyi Han, Yuqing Cheng, Xiaoyan Li,Yuxiao Dong, and Jie Tang. 2023. Web-scale academic name disambiguation: theWhoIsWho benchmark, leaderboard, and toolkit. In Proceedings of the 29th ACMSIGKDD Conference on Knowledge Discovery and Data Mining. 38173828. Bo Chen, Jing Zhang, Xiaokang Zhang, Yuxiao Dong, Jian Song, Peng Zhang,Kaibo Xu, Evgeny Kharlamov, and Jie Tang. 2022. Gccad: Graph contrastive cod-ing for anomaly detection. IEEE Transactions on Knowledge and Data Engineering35, 8 (2022), 80378051.",
  "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:Pre-training of deep bidirectional transformers for language understanding. arXivpreprint arXiv:1810.04805 (2018)": "Yuxiao Dong, Nitesh V Chawla, and Ananthram Swami. 2017. metapath2vec:Scalable representation learning for heterogeneous networks. In Proceedings ofthe 23rd ACM SIGKDD international conference on knowledge discovery and datamining. 135144. Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, DiegoRojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, et al. 2024. ChatGLM: A Familyof Large Language Models from GLM-130B to GLM-4 All Tools. arXiv preprint",
  "Zeyu Han, Chao Gao, Jinyang Liu, Sai Qian Zhang, et al. 2024. Parameter-efficient fine-tuning for large models: A comprehensive survey. arXiv preprintarXiv:2403.14608 (2024)": "Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, SheanWang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of largelanguage models. arXiv preprint arXiv:2106.09685 (2021). Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, De-vendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,Guillaume Lample, Lucile Saulnier, et al. 2023.Mistral 7B.arXiv preprintarXiv:2310.06825 (2023). Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma,Qiwei Ye, and Tie-Yan Liu. 2017. Lightgbm: A highly efficient gradient boostingdecision tree. Advances in neural information processing systems 30 (2017).",
  "Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graphconvolutional networks. arXiv preprint arXiv:1609.02907 (2016)": "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrappinglanguage-image pre-training with frozen image encoders and large languagemodels. In International conference on machine learning. PMLR, 1973019742. Na Li, Renyu Zhu, Xiaoxu Zhou, Xiangnan He, Wenyuan Cai, Ming Gao, andAoying Zhou. 2021. On disambiguating authors: Collaboration network recon-struction in a bottom-up manner. In 2021 IEEE 37th International Conference onData Engineering (ICDE). IEEE, 888899. Xiang Lisa Li and Percy Liang. 2021. Prefix-Tuning: Optimizing ContinuousPrompts for Generation. In Proceedings of the 59th Annual Meeting of the Associa-tion for Computational Linguistics and the 11th International Joint Conference onNatural Language Processing (Volume 1: Long Papers). 45824597. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, OmerLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: Arobustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692(2019). Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel S Weld.2020. S2ORC: The Semantic Scholar Open Research Corpus. In Proceedings of the58th Annual Meeting of the Association for Computational Linguistics. 49694983.",
  "Li Tang and John Walsh. 2010. Bibliometric fingerprints: name disambiguationbased on approximate structure equivalence of cognitive maps. Scientometrics84, 3 (2010), 763784": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-AnneLachaux, Timothe Lacroix, Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, et al. 2023. Llama: Open and efficient foundation language models. arXivpreprint arXiv:2302.13971 (2023). Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin. 2017. Attention is allyou need. Advances in neural information processing systems 30 (2017).",
  "An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Cheng-peng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. 2024. Qwen2 technicalreport. arXiv preprint arXiv:2407.10671 (2024)": "Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding,Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b: An openbilingual pre-trained model. arXiv preprint arXiv:2210.02414 (2022). Chuxu Zhang, Chao Huang, Lu Yu, Xiangliang Zhang, and Nitesh V Chawla.2018. Camel: Content-aware and meta-path augmented metric learning for authoridentification. In Proceedings of the 2018 World Wide Web Conference. 709718. Fanjin Zhang, Xiao Liu, Jie Tang, Yuxiao Dong, Peiran Yao, Jie Zhang, Xiaotao Gu,Yan Wang, Evgeny Kharlamov, Bin Shao, et al. 2022. Oag: Linking entities acrosslarge-scale heterogeneous knowledge graphs. IEEE Transactions on Knowledgeand Data Engineering 35, 9 (2022), 92259239. Fanjin Zhang, Xiao Liu, Jie Tang, Yuxiao Dong, Peiran Yao, Jie Zhang, XiaotaoGu, Yan Wang, Bin Shao, Rui Li, et al. 2019. OAG: Toward linking large-scaleheterogeneous entity graphs. In Proceedings of the 25th ACM SIGKDD InternationalConference on Knowledge Discovery & Data Mining. 25852595. Fanjin Zhang, Shijie Shi, Yifan Zhu, Bo Chen, Yukuo Cen, Jifan Yu, Yelin Chen,Lulu Wang, Qingfei Zhao, Yuqing Cheng, et al. 2024. OAG-Bench: A Human-Curated Benchmark for Academic Graph Mining. In Proceedings of the 30th ACMSIGKDD Conference on Knowledge Discovery and Data Mining. 62146225.",
  "Xiaocheng Zhang, Yang Zhou, Haoru Chen, Mengjiao Bao, and Peng Yan. 2024.Enhanced Name Disambiguation via Iterative Self-Refining with LLMs. In KDD2024 OAG-Challenge Cup": "Yutao Zhang, Fanjin Zhang, Peiran Yao, and Jie Tang. 2018. Name Disambiguationin AMiner: Clustering, Maintenance, and Human in the Loop.. In Proceedings ofthe 24th ACM SIGKDD International Conference on Knowledge Discovery & DataMining. 10021011. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A surveyof large language models. arXiv preprint arXiv:2303.18223 (2023).",
  "ATraining Details": "During the training of the LLaMA model, we utilized a LoRA rank of 8, LoRA alpha value of 16, and a dropout rate of 0.05. Theper-device batch size was 1, with a gradient accumulation step of16, resulting in a global batch size of 128 for 8-card training. Weuse AdamW as the optimizer and the weight decay of 1e-3was applied. We adopted a cosine learning scheduler with a linearwarm-up for the MIND-base model, using a warm-up ratio of 0.1and a learning rate of 1e-4. In contrast, for the +sem and +graphmodels, we employed a constant scheduler with a learning rate of5e-5. The base, +sem, and +graph models were trained for 6,10, and 4 epochs, respectively, with evaluations conducted every 25global steps. During the training process, we initiated each phaseusing the best-performing parameters obtained from the evaluationof the previous phase. For the FFN layer in the semantic/structuralembedding module, we set the intermediate hidden size to twicethe hidden size of the following LLM.When using the title and title+venue (TV) as key information,we utilized a context length of 8k tokens, while for title+author (TA)and title+organization (TO), we used a context length of 20k tokens.Considering that there is no official long-text fine-tuning versionof LLaMA-3-8B, we used a training-free method to extend itstext length.For the ChatGLM3 and Qwen2 models, after some testing, weused LoRA ranks and alpha values of 128 and 256 for ChatGLM3with 32 and 64 for Qwen2."
}