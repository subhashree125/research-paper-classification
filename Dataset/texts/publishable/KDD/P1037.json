{
  "ABSTRACT": "Schema matching constitutes a pivotal phase in the data inges-tion process for contemporary database systems. Its objective is todiscern pairwise similarities between two sets of attributes, eachassociated with a distinct data table. This challenge emerges atthe initial stages of data analytics, such as when incorporating athird-party table into existing databases to inform business insights.Given its significance in the realm of database systems, schemamatching has been under investigation since the 2000s. This studyrevisits this foundational problem within the context of large lan-guage models. Adhering to increasingly stringent data securitypolicies, our focus lies on the zero-shot and few-shot scenarios: themodel should analyze only a minimal amount of customer datato execute the matching task, contrasting with the conventionalapproach of scrutinizing the entire data table. We emphasize thatthe zero-shot or few-shot assumption is imperative to safeguardthe identity and privacy of customer data, even at the potential costof accuracy. The capability to accurately match attributes undersuch stringent requirements distinguishes our work from previousliterature in this domain.",
  "Schema matching, Generative modeling, Retrieval augmented gen-eration": "ACM Reference Format:Xuanqing Liu, Luyang Kong, Runhui Wang, Patrick Song,, Austin Nevins,Henrik Johnson, Nimish Amlathe, Davor Golac. 2024. GRAM: GenerativeRetrieval Augmented Matching of Data Schemas in the Context of Data Secu-rity. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discov-ery and Data Mining (KDD 24), August 2529, 2024, Barcelona, Spain. ACM,New York, NY, USA, 11 pages.",
  "First two authors contributed equally; corresponding": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain. 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08",
  "INTRODUCTION": "Todays SaaS providers that supports diverse data suppliers withingesting, managing, and searching for potentially sensitive records,they face the challenge of dealing with inhomogeneous data schemasthat naturally occur among different suppliers. For instance, an in-surance company may have a table named BusinessProfile thatcontains columns like num_employees, mailing_address_city,business_phone, and so forth. However, when we ingest datarecords for this customer, we discover that the schema is not per-fectly aligned with our internal system, which necessitates man-ually creating mappings such as #employees num_employees,recipient_city mailing_address_city and phone# business_phone. Unfortunately, these mappings are hardly reusablefor another customer due to naming conventions and other nuances.A common scenario in data science domain is that experts spendup to several weeks to designing mappings for moderately sizedtables between 100 to 200 attributes. Even with tools like MicrosoftBizTalk or COMA 3.0 GUI , the data internalization processis typically error-prone and requires multiple rounds of trials anderrors.To address the challenge of data ingestion, the research com-munity has proposed the concept of automated schema matchingas a solution to streamline the associated processes. This conceptis visually represented in . In essence, it transforms theattribute-to-attribute mapping task into a hierarchical multi-classclassification problem. Given an input table with columns, the ap-proach involves a non-overlapping partition of the columns intosubgroups, denoted as 1, 2, . . . , columns, where =1 = .Each subgroup corresponds to a distinct object type, exemplified in by showcasing the Profile and Order object types. Foreach object type, a predefined attribute tree is established, com-prising nodes and attributes (with leaf nodes serving as aliasesfor attributes). By deploying a classifier at each non-leaf node topredict the correct child node containing the relevant attribute, themethodology simplifies the process to traversing an -nary tree.This traversal follows the direction indicated by classification re-sults at each level. As discussed later, numerous prior works alignwith and contribute to this overarching framework.Diverging from prior research efforts, we reexamine the afore-mentioned issue by harnessing the latest advancements in lan-guage understanding, specifically leveraging Large Language Mod-els (LLMs) and their adept in-context learning capabilities. By encap-sulating the previously outlined hierarchical classification problemwithin the framework of in-context learning, we proficiently repur-pose LLMs as readily available classifiers through the mechanismof few-shot prompting.",
  "Order KeyQuantityStatusTotalPrice": ": Illustration of the idea of hierarchical prediction in schema mapping. First, columns of input data table are partitionedand grouped into one or more object types, here are Profile and Order (two ellipse shapes in figure). Next, we take a columnfrom partition group, then the column traverses through the -ary tree based on the classification results at each level, untila root node is found (marked in red arrows). Each root node corresponds to a target attribute defined by target schema. Werepeat the same process for each column until all columns are mapped to target attributes.",
  "Data type: boolean Nullable: False Column meaning: A variable indicating if the order has been cancelled by customer Values: [False, False, True, ....] Length: 112,000,000": ": An example of how an individual attribute in theschema look like. We highlight the required field (columnname) with shades, and all other fields (data type, nullable,column meaning, values, length, etc.) as optional. Our primary contributions can be summarized as follows:(1) We address the automated schema matching problem withinthe context of data privacy, employing a novel perspectivethat incorporates zero-shot and few-shot predictions. (2) Our solution seamlessly integrates the recent surge in LargeLanguage Models (LLMs). We conduct a comprehensivebenchmarking exercise across various open-source and pro-prietary LLMs to assess their performance.",
  "(3) Introducing a dynamic prompt selection method based oninput characteristics, our approach not only enhances in-ference speed but also augments the in-context learningaccuracy of LLMs": "(4) Beyond the conventional scope of schema matching, our solu-tion incorporates object type detection and unique key detec-tion. These additional components transform the standaloneschema matching module into a more feature-complete data-table ingestion service. (5) We rigorously benchmark the accuracy of our methodologyagainst relevant approaches using both public and production-quality, synthetic datasets. Particularly noteworthy is theutilization of datasets designed to mirror realistic workloadsin various industrial applications.",
  "A BRIEF HISTORY OF SCHEMA MATCHINGRESEARCH2.1Pioneering solutions": "LSD. stands as one of the pioneering machine learning-based schema matching frameworks. It formulates the matchingproblem as a multi-class classification challenge. Notably, LSD em-ploys an ensemble of classifiers to enhance accuracy, incorporat-ing a nearest neighbor Whirl learner, a Naive Bayesian learner, aname matcher, and a county-name recognizer. Classifiable underthe dichotomies outlined earlier, LSD falls within the category ofone-to-one matching based on linguistic features and is trained onboth schema and instances. CUPID. is considered one of the first general-purpose schemamatching systems with a focus on feature completeness. It em-ploys linguistic features and predefined rules to match pairs orgroups of attributes. CUPIDs core idea is to determine the high-est weighted similarity () between two attributes using theformula = struct + (1 struct) , where isthe structural similarity score, and is the linguistic similarityscore. As an early work from the 2000s, CUPIDs feature extractorsare basic compared to modern language models. However, CUPIDfalls short in extracting insights from column values, missing op-portunities to address ambiguities inherent to schema-data alone.Similarity Flooding. introduces a method to transform theschema matching problem into computing the fixpoint over graphpropagation. Initially, the SQL2Graph operation converts a pair",
  "GRAM: Generative Retrieval Augmented Matching of Data Schemas in the Context of Data SecurityKDD 24, August 2529, 2024, Barcelona, Spain": "Model (LLM) inference, including intelligent decoding methods , improved memory access patterns , and model compres-sion and quantization , among others, this paper introducesan orthogonal approach specifically tailored for schema matchingacceleration, known as prompt compression. Our approach is in-spired by the observation that the inference time for transformer-based LLMs is quadratic to the input length input, i.e., = O(2input).This is because the self-attention output is computed as",
  "Modern solutions based on neural nets": "Sahay et al. presented a straightforward hybrid approachincorporating both schema data and column values, applicable toboth one-to-one and one-to-many mappings. Employing extensivefeature engineering, the authors utilize self-organizing maps or K-means clustering to cluster similar attributes. Consequently, duringtesting, an attribute is paired with the nearest cluster, and the bestattribute within that cluster is selected based on the minimumedit-distance principle. Httasch et al. introduced a neural embedding-based method,making a significant contribution with a two-level matching scheme.The first level involves table matching, followed by attribute match-ing at the second level. The matching process entails computingthe similarity, such as cosine similarity, between two embeddingsderived from textual information, including column name, datatype, comments, etc. LSM. is a schema matcher that leverages pre-trained lan-guage models. Notably relevant due to its recent development andutilization of modern transformer-based language models, LSMemploys a finetuned BERT featurizer at its core. This featurizertransforms pairs of schema information into similarity scores, con-sidering two attributes as a match if the similarity score surpassesa specified threshold. The BERT featurizer undergoes finetuningbased on human-provided labels. Once the finetuning process iscomplete, the model is prepared to generalize to new schema pairs.",
  "Given the recent surge in large language models (LLM) and gen-erative AI (GenAI), it is intuitive to explore the application of the": "\"emergent abilities\" described by Wei et al. to the realm ofschema matching. Our decision to integrate these advancementsstems from the belief that LLMs offer language understanding andreasoning capabilities approaching human levels. With this upgrade,we anticipate a transformative impact on how we conceptualizethe similarity between two data schemas. In this paper, we aim toelevate the quality and usability of schema matching systems alongthe following dimensions: Enhanced Language Understanding with Efficient Inference. Whenframing the schema mapping as a natural language processing(NLP) problem, one observes that the advancements in solutionsreviewed over the past two decades are intricately linked to theevolving landscape of language modeling. Early solutions relied onstring similarity and hand-crafted features, often complementedby shadow models such as linear classifiers, naive Bayes classifiers,or -means clustering methods. In contrast, contemporary solu-tions leverage deep learning text featurizers like Word2vec, GloVe,FastText, and BERT, extracting text similarity scores within anend-to-end paradigm. This paper benefits from superior languageunderstanding capabilities offered by open-source large languagemodels, specifically the FLAN-T5 family. Additionally, we introducemethods to expedite inference speed while preserving accuracy,a critical consideration for handling large-scale data prevalent inindustrial applications. Minimal Training Data Dependency. The conventional approachto utilizing finetuned language models, as seen in works such as, involves the collection of a substantial amount (ranging from103 to 104) of human-labeled data to calibrate the language classifierusing certain loss functions. In contrast, we adopt zero-shot andfew-shot learning, also known as in-context learning (ICL) in LargeLanguage Models (LLM) literature, reducing the dependency ondata quantity. This attribute is particularly significant in addressingcontemporary concerns regarding data security and privacy, as itobviates the need for accessing and annotating large volumes ofcustomer data. Comprehensive Feature Integration. The solution presented inthis paper transcends the boundaries of a mere schema matcher,evolving into an end-to-end service fueled by language models.This service harmonizes disparate data sources, rendering theminto uniformly searchable data records. Central to this endeavor aretwo supplementary components around the attribute mapper: theobject type detector and column key detector. Both componentsleverage language models to enhance their functionalities. Specifi-cally, the object type detector identifies the appropriate object type(target schema) for a subset of input columns; the attribute mapperestablishes connections between each input attribute and a uniquetarget attribute; and finally, the key detector designates one of theattributes as the unique key, enabling the ingestion of the entiretable with duplicates removed.",
  "BACKGROUND KNOWLEDGE3.1Large language model": "Language understanding stands as a fundamental capability toshowcase advanced artificial intelligence. Pretrained language mod-els (PLM) have proven to be a powerful and scalable approach forembedding general knowledge into transformer-based neural net-works. The conventional application of PLMs involves finetuningthem on domain-specific datasets collected from experts, leadingto the creation of one model for each task. This practice, however,limits usability in scenarios where high-quality datasets are scarce.With the growing demand for more generalized language models,researchers have identified a promising avenue. By scaling up boththe size of the pretraining corpus and the parameter count of thelanguage model, adhering to scaling-up principles , and sub-sequently finetuning the model on diverse tasks using instructionalprompts , a robust language model emerges. This modelexhibits the capability to comprehend natural instructions withstrategic prompt engineering.",
  "Retrieval augmentation": "Standalone Large Language Models (LLMs) encode world knowl-edge within their model weights, placing smaller scale models ata disadvantage when tasked with memorizing intricate languagecorpora that demand hundreds of billions of parameters. Addition-ally, Retrieval Augmented Generation (RAG) enhances modelcapacity by integrating an external knowledge search engine. RAGexcels in consolidating domain-specific knowledge that proves chal-lenging to memorize from a web corpus using LLM-based readers.In this context, RAG emerges as particularly well-suited for theschema matching task, given the often vaguely defined connectionsbetween two attributes.",
  "GRAM: GENERATIVE RETRIEVALAUGMENTED MATCHING4.1Motivating example": "To explore how instruction finetuned large language models canbe prompted with few-shot examples to effectively match simi-lar attribute pairs against unrelated ones, we have developed astraightforward demonstration using Anthropic Claude via theAWS Bedrock SDK , as illustrated in . In this instance,we instruct Claude to match the column name contact_name withan example value Amazon.com Inc. against other profile-relatedattributes, such as FirstName, LastName, HomePhoneNumber,EmailAddress, and so on. The prompt adheres to the standardin-context learning paradigm: it begins with a formulation of theproblem statement and the success goal, followed by a list of choicesand subsequently a list of examples with ground-truth labels. Fi-nally, the query example is appended at the end.It is noteworthy that this particular problem is non-trivial toanswer accurately. The column name contact_name alone can referto both FirstName, LastName, and BusinessName. The resolutionof this ambiguity is dependent on examining the example valueAmazon.com Inc., where the model deduces that BusinessNameis the sole appropriate match. Generally, the schema matchingproblem proves to be highly challenging, even for domain experts.",
  "Results": ": An illustrative example outlining the concept ofprompting Large Language Models (LLMs) to match a sourceattribute (e.g., contact_name for Amazon.com Inc.) to a listof 15 target attributes is provided for clarity. For instance, the column name state may represent a U.S. statename or serve as an equivalent to the word status, withoutadditional information discernible from the value section.We hypothesize that leveraging large language models equippedwith commonsense knowledge represents a promising approachto effectively address the challenge of schema understanding. Con-current research indicates that gigantic language models boasting100+ billion parameters demonstrate human-level reading compre-hension and near-human-level logical reasoning capabilities .This hypothesis serves as the driving force behind our decision toincorporate an instruction-finetuned large language model as thecentral component of our schema matching service.However, translating the concept illustrated in into liveproduction proves to be non-trivial. The target processing speedof schema matching service is 10 transactions per second (TPS)per host, each equipped with inference-optimized GPU devices,typically Nvidia T4 or Nvidia A10. Benchmark results reveal that,without any optimization, the naive solution achieves less than 6TPS per host.In the subsequent sections, we delve into strategies for acceler-ating inference latency, or equivalently, increasing the TPS count.While various techniques exist for optimizing Large Language",
  ",(1)": "where = in, = in, = in Rinput areattention query, key and value matrices respectively; in and outare the inputs and outputs of attention block. The bottleneck forcomputing Eq. (1) is matrix multiplication with a complexityof O2input. As a result, it is most beneficial to minimize the inputtext length input to accelerate the inference speed. At the sametime, according to the prompt structure in , we can decomposeinput to",
  "input = instruct + option + example,(2)": "where instruct is the length of instruction text, option is the averagelength of destination attribute name, example is the average lengthof each example; is the number of options in prompt, and this isequivalent to number of mapping destinations; denotes numberof examples per option (-shot prompting).Our empirical observation indicates that listing all possible match-ing destinations in each Large Language Model (LLM) query isunnecessary. Instead, by employing a combination of techniquesdetailed in the following sections, we can effectively eliminate asubstantial number of irrelevant options and examples associatedwith the source attribute. This results in a significant reduction inthe values of and in Eq. (2). Consequently, a smaller value forinput is achieved.",
  "Jim bought 300 shares of Acme Corp. in 2006": "A successful NER task would label Jim as Person, Acme Corp.as Organization, and 2006 as Time. With NER models, we canmove beyond merely matching the column data type, as seen inprior works (e.g., ), to introduce a new destination attribute filterdenoted as FNER. This filter retains only those destination attributesthat share both the same data type and data category as the sourceattribute. Mathematically,",
  "NER() = NER(),(3)": "in which is the set of all destination attributes; , is the inputdata pair containing attribute name and attribute value ; DTypeis the data type extraction operator by reading column metadata;NER denotes a named entity resolution model we trained on schemamatching tasks. To highlight the potential impact of the filter FNER, lets revisitthe prompt depicted in . Post-filtering, the available optionsare significantly reduced to just two - Account and BusinessName,down from the original 15 options. This reduction is attributedto the NER models recognition of the input value Amazon.comInc. as an organization name, while the remaining options fallinto distinct data categories such as phone numbers, person names,addresses, etc.We implemented a Named Entity Recognition (NER) model tai-lored for schema matching tasks, closely adhering to standard prac-tices outlined in the literature ( and references therein), witha few noteworthy modifications. First, we defined a more fine-grained label space. Traditional NER models are typically trainedon a coarser label space, where the target category \"address\" encom-passes street addresses, cities, states/provinces, and even countries.However, this standard practice limits usability in schema matchingtasks where the goal is to determine if a column storing zip codesmatches another column storing cities, even if both are mappedto the \"address\" category with traditional NER models. The sec-ond modification we introduced pertains to the training loss. Intraditional NER models, the loss is computed on a per-token basis,treating it as a token-level classification problem. This approach isjustified when the input is a sentence containing multiple entities,and the goal is to predict the text span encompassing all entitiesalong with their labels. In contrast, our approach computes the lossat the sequence level, treating it as a sequence-level classificationproblem. Our approach is valid under the assumption that there isonly one entity for each input sequence, a condition that is widelyapplicable to schema matching tasks.Implementation-wise, we choose RoBERTa-base as the back-bone model to initialize training. An input sequence for training orinference consists of a few samples ranging from 1 to 6 elementssampled from a column, serialized as a list of values",
  "Double-RAG filter": "The NER-based filter discussed in the previous section assessesthe coherence of two attributes based on column values. Essen-tially, two attributes can be considered a good match when theircorresponding column values are mapped to the same Named En-tity Recognition (NER) label. In this section, we adopt a differentperspective and gauge the inter-attribute coherency through the se-mantic similarity of column names. Our approach draws inspirationfrom the efficacy of the retrieval augmentation (RAG) technique inenhancing accuracy across various Large Language Model (LLM)applications (refer back to .2 for additional background).What sets our use of the RAG technique apart is that we not onlysearch for the best possible options but also seek the most suitablefew-shot examples for a particular option, giving rise to the term\"Double-RAG.\" Consequently, both the options and the few-shotexamples in the prompt dynamically change with different input",
  "KDD 24, August 2529, 2024, Barcelona, Spain.Liu and Wang, et al": "Thorvald Sorensen. 1948. A method of establishing groups of equal amplitudein plant sociology based on similarity of species content and its application toanalyses of the vegetation on Danish commons. Biologiske skrifter 5 (1948), 134. Yi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Abnar, Hyung WonChung, Sharan Narang, Dani Yogatama, Ashish Vaswani, and Donald Metzler.2021. Scale efficiently: Insights from pre-training and fine-tuning transformers.arXiv preprint arXiv:2109.10686 (2021). Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition. In Proceed-ings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003.142147. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-mine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-ale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXivpreprint arXiv:2307.09288 (2023). Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, SebastianBorgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al.2022. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682(2022). Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and SongHan. 2023. Smoothquant: Accurate and efficient post-training quantization forlarge language models. In International Conference on Machine Learning. PMLR,3808738099.",
  "12122 2............1 2": "be the database containing all available examples, in which isthe -th example of -th option. We provide examples for each ofthe options, totaling examples. To understand how membersin each database look like, we can pick a few examples from both.Suppose 1 =PhoneNumber, then we can store 11 =phone,12 =tel, 13 =phone_number and so on. In principle, we shouldcollect a diverse number of examples that give LLM enough idea ofhow the concept of option is like.Building upon the databases opt and ex, we further incorpo-rate a similarity measure sim(,) , supported by eithermachine learning models or traditional string similarity algorithms.The trade-off in this context revolves around whether the criticalfactor is the semantic understanding ability from machine learningmodels and the computational budget available in practice.For instance, we anticipate the similarity value of sim(phone, tel)to be closer to 1.0, but none of the string similarity algorithms yieldsexpected results in such cases without the aid of external thesaurusdictionaries. This is because the words \"phone\" and \"tel\" shareonly one common character, \"e\", resulting in a bi-gram Jaccardsimilarity of 0.0. In contrast, even the simplest GloVe 1 embeddingindicates a significant cosine similarity of 0.50, not to mention moresophisticated BERT-based embeddings. The experiments will revisitthe choice of similarity measures with further details.Equipped with two databases opt and ex, and a similaritymeasure sim(,) , we are ready to formulate the way weshort-listing the options together with their exemplars:",
  "paper revolves around attribute mapping, as an integral part of theoverall system, we briefly introduce their functionalities as follows": "Object Type Detector. This component serves as a preprocessorfor the attribute mapper. Its role is to partition the columns of theinput table into multiple subgroups, each representing a uniformtopic (also referred to as an object type, as illustrated in ), suchas personal profile, customer order, issue ticket, and so forth. It isimportant to note that real-life input tables can be a combinationof multiple topics, and the two databases opt/ex used in the LLMattribute mapper are determined by the topic. Hence, the systemneeds to cluster the columns and identify the topic of each clusterbefore proceeding to the attribute mapping stage. Our implemen-tation of the object type detector adheres to standard practices:we first convert the input table into CSV format, then serializeits header into a text string. Next, the entire string is tokenizedto train a BERT-based multi-class classifier with per-token levelcross-entropy loss. During the inference stage, we group columnswith the same predicted labels together into a subgroup, effectivelypartitioning the entire table. Key Detector. This component functions as a postprocessor forthe attribute mapper. Its role is to enhance the mapping resultswith a few keys for searching or de-duplication. The underlyingconcept aligns with the LLM-based attribute mapper introducedearlier; in fact, we reuse the same LLM model with a differentprompting method, thereby improving hardware utilization rates.Initially, we allow users to customize heuristic rules to filter outcolumns unlikely to serve as potent keys. A simple illustrative rulecould be any column name with the pattern *_id. Users havethe flexibility to chain multiple rules together to strike a balancebetween recall and precision. Ideally, the aim is to retain all validkeys while minimizing the list of candidates to query LLM.",
  "Workflow": "Bringing all the components together, we illustrate the entire work-flow in . At a high level, the custom data slated for ingestionfirst undergoes the object type detector, where columns are par-titioned and labeled with one of the pre-defined object types. Inthe second stage, each individual column, along with its associatedobject type, is formatted as a query to the attribute mapper. Theoutcome of stage 2 is the predicted destination attribute generatedfrom the instruction-finetuned Large Language Model (LLM). Fi-nally, in stage 3, the key detector assigns one or more keys to themapped attributes, ensuring that the ingested table is accompaniedby keys for searching and data de-duplication.",
  "(4) How does the number of -shot examples influence end-to-end accuracy?": "We initiate the process with dataset preparation, which is unde-niably one of the most challenging steps given the multi-decadehistory of schema mapping research since the 2000s. Numerousdatasets referenced in early works are either lost or unpublished.Despite these challenges, we have managed to reconstruct a sub-stantial collection of evaluation sets from diverse domains, as listedbelow. Personal Contacts: This domain revolves around personaland business profiles, which are commonly found in cus-tomer databases, employee databases, or social media records.In total, there are 1400 columns.",
  "Products: This domain comprises databases storing prod-ucts or services available in the market, including airlines,hotel rooms, groceries, etc. In total, we have collected 200columns": "Issue Tickets: This domain includes issue tickets, totaling330 columns.PII Disclosure: None of the datasets mentioned above con-tain any real identity information. This includes metadata suchas column names and/or data types (first_name(str), dob(str),zip(int32), address_line1(str), sales_amount(float32)). Thecolumn values are all synthetic or randomly generated.We have implemented and deployed our Large Language Model(LLM)-based schema matching system using PyTorch , based onthe FLAN-T5 model. For very early methods, such as LSD andCUPID , for which no first-party implementation is available,we implemented their methods following the ideas presented in the original papers. For other similar works, such as SimilarityFlooding , we were unable to replicate the algorithm due to thelack of critical details; hence, we exclude them from our experiments.When benchmarking throughput, we executed all programs onhardware comprising 4 Nvidia A10 GPUs (each with 24GB ofmemory), 24 physical CPU cores, and 192GB of memory.",
  ": Comparing the accuracy numbers across different do-mains among traditional algorithms, deep neural nets basedalgorithms, and our LLM based algorithm": "In this study, we conduct a comprehensive comparison of variousschema matching algorithms. The primary objective is to assessthe comparative advantages of machine learning (ML)-based andlarge language model (LLM)-based algorithms in comparison toconventional rule-based methods. The outcomes are presented in. Based on the experimental findings, several observationscan be made: 1) a noteworthy improvement in accuracy is evidentwhen employing an instruction-finetuned LLM, surpassing evencontemporary pretrained language model (LM) approaches, such asLSM ; 2) it is generally observed that embedding-based cosine",
  ": Comparing the testing accuracy with and withoutfilters. Filters do not change model accuracy in a consistentdirection": "We explore the impact of the Named Entity Recognition (NER)filter and the Double-RAG filter on both inference speed and ac-curacy. In principle, activating either of these filters introducesfalse negatives, as they possess the capability to exclude positiveselections and crucial instances intended to assist reasoning dur-ing test time. The discernible effect of these filters on accuracy isdetailed in . Remarkably, a consistent decline in accuracy No filter+NER+Double RAG+Both filters4 #Inference per second",
  ": Inference speedup due to NER and double-RAGfilters. With double-RAG filter, we keep opt = 4 options andex = 1 examples for each of the 4 options. Error bars areprovided but barely visible": "is not readily observable upon activating the filters. We posit thatthe incorporation of high-quality filters aids the Large LanguageModel (LLM) in decision-making by eliminating evidently incorrectoption items and unrelated few-shot examples. Despite introducingfalse negatives through occasional removal of correct options andvaluable few-shot examples, the overall impact appears to enhancethe LLMs decision-making process. Meanwhile, the filters demon-strate a noticeable acceleration in inference speed, as illustrated in across typical workloads simulated with synthetic datasets.",
  "Does larger language model perform betterin schema-matching?": "In this section, we investigate the correlation between larger LargeLanguage Models (LLMs) and enhanced accuracy in schema match-ing tasks, as observed in related works across various domains (e.g.,). To explore this relationship, we conduct an experimentcomparing downstream accuracy using FLAN-T5 as the back-end with varying LLM sizes (Small-80M, Base-250M, Large-780M,XL-3B, XXL-11B). Evaluation settings, including filter hyperparame-ters, remain consistent across all assessments. The average accuracyacross four domains plotted against model sizes is depicted in . Model parameters (M) 0.4 0.5 0.6 0.7 0.8",
  "Effect of number of shots to matchingaccuracy": "In this experiment, we explore the effect of adding more ground-truth examples in the prompt for LLM to conduct in-context learn-ing. It is widely believed that more diverse few-shot examplestypically converts to higher accuracy. However, we noticed thatthe return of adding more samples diminishs very quickly beyond1-shot setting. In we showed an significant accuracy boostmoving zero-shot (73.05%) to 1-shot (83.58%), whereas the accuracyimprovement beyond 1-shot is not significant given the error band.This finding led us to configure our model to consume just oneexample per class label.",
  "Launch strategy, user experience, andlearning": "Due to the service availability requirement, we reserved three nodeson each region with instance type ml.g5.2xlarge as well as severalback-up instance types (ml.g4dn.4xlarge etc.) so that in case oneinstance type isnt available at a certain region we will still be ableto serve the request at similar throughput. Another challenge is thevolatility of payloads: some payload consists of small schema ( 30attributes) while in extreme cases this number can be as large as 120attributes. To prevent the requests from queuing up, we distributethe attribute mapping requests originating from the same table toat least 3 hosts with an automatic scaling-up policy.",
  ": Comparing matching accuracy under varying -shot examples in the prompt": "We present the schema matching service to customers by en-abling a human-in-the-loop process: schema matcher never gener-ates the final mapping result in one shot, instead, customers havethe chance to examine the predicted mapping table as well as othermachine generated metadata (such as searchable keys) and correctany incorrect predictions on the fly. While we are not permitted torecord the user activities (e.g. number of modifications they madewhen composing the schema mapping) due to data privacy, internalstudies show that with our LLM aided schema mapping, the amountof human efforts measured by editing operations reduced by 90%.Lastly, we also learned from some negative feedback, mostlyabout the instability of prediction results. Although the modelperforms reliably on canonical input schema, it predicts wronglywhen we slightly change the column name. For instance, by addinga meaningless prefix \"XYZ_\" to all column names, the mappingaccuracy drops under certain inputs (although not very common).We attribute this as adversarial examples and we plan to focus onthis problem as the next research topic.",
  "DISCUSSION": "The schema matching task has been under investigation for overa decade. We posit that the fundamental challenge stems fromcomprehending attributes in highly heterogeneous environments.The rapid evolution of large language models has elevated languageunderstanding capabilities to unprecedented levels. In light of thisadvancement, we address the longstanding and intricate problemusing this innovative tool, yielding encouraging results. Lookingahead, our future direction involves contemplating the optimalapproach for task adaptation to the backbone model, with the aimof further enhancing matching accuracy.",
  "Benjamin Httasch, Michael Truong-Ngoc, Andreas Schmidt, and Carsten Bin-nig. 2022. Its AI Match: A Two-Step Approach for Schema Matching UsingEmbeddings. arXiv preprint arXiv:2203.04366 (2022)": "Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess,Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 (2020). Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng,Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficientmemory management for large language model serving with pagedattention. InProceedings of the 29th Symposium on Operating Systems Principles. 611626.",
  "Jing Li, Aixin Sun, Jianglei Han, and Chenliang Li. 2020. A survey on deeplearning for named entity recognition. IEEE Transactions on Knowledge and DataEngineering 34, 1 (2020), 5070": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, OmerLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: Arobustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692(2019). Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay,Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. 2023. The flan collection:Designing data and methods for effective instruction tuning. arXiv preprintarXiv:2301.13688 (2023).",
  "Sabine Massmann, Salvatore Raunich, David Aumller, Patrick Arnold, ErhardRahm, et al. 2011. Evolution of the COMA match system. Ontology Matching 49(2011), 4960": "Sergey Melnik, Hector Garcia-Molina, and Erhard Rahm. 2002. Similarity flooding:A versatile graph matching algorithm and its application to schema matching. InProceedings 18th international conference on data engineering. IEEE, 117128. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, PamelaMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022.Training language models to follow instructions with human feedback. Advancesin Neural Information Processing Systems 35 (2022), 2773027744. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, GregoryChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019.Pytorch: An imperative style, high-performance deep learning library. Advancesin neural information processing systems 32 (2019).",
  "Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. 2017.mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412(2017)": "Yunjia Zhang, Avrilia Floratou, Joyce Cahoon, Subru Krishnan, Andreas C Mller,Dalitso Banda, Fotis Psallidas, and Jignesh M Patel. 2023. Schema Matching usingPre-Trained Language Models. In 2023 IEEE 39th International Conference on DataEngineering (ICDE). IEEE, 15581571. Wenxuan Zhou, Sheng Zhang, Yu Gu, Muhao Chen, and Hoifung Poon. 2023.Universalner: Targeted distillation from large language models for open namedentity recognition. arXiv preprint arXiv:2308.03279 (2023). Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis,Harris Chan, and Jimmy Ba. 2022. Large language models are human-levelprompt engineers. arXiv preprint arXiv:2211.01910 (2022).",
  "ADICHOTOMIES OF SCHEMA-MATCHING": "Before delving into the historical overview of schema matchingresearch, it is pertinent to highlight the dichotomies that character-ize existing ideas, as elucidated in the review paper by Rahm andBernstein : Schema-only or schema+instances: A matching system is cat-egorized as schema-only when it relies solely on schemadata without considering column values. In contrast, schema+ instances matching incorporates both schema and columnvalues. In the context of modern machine learning, the for-mer is often referred to as zero-shot.",
  "Element-wise or structural matching: Element-wise matchingentails pairing individual attributes, while structural match-ing involves matching groups of attributes together": "Linguistic-based or rule-based: Linguistic-based matchingencompasses ideas that employ machine learning or non-machine learning-based text similarity metrics to determineattribute equivalence. Conversely, rule-based matching reliesmore on schema constraints, such as data types, value ranges,uniqueness, etc. One-to-one or many-to-many: A one-to-one matcher consis-tently connects one attribute to another, whereas a many-to-many matcher has the capability to associate more thanone attribute as the source or destination. Self-contained or auxiliary information: A self-containedmatcher operates autonomously, while a matcher incorporat-ing auxiliary information can leverage external knowledge,such as dictionaries, global schemas, previous matching de-cisions, and user input.",
  "BIMPLEMENTATION DETAIL OF NER FILTER": "We follow the identical modelling steps as standard BERT-basedNER . The model architecture (as well as input structure) isillustrated in . We highlight that the input sequence to the NERmodel is not a single attribute value, but a list of example values invariable length with noises (such as empty values, invalid values,etc). Adding external noise helps robustifying the model inference,as it simulates the outliers often encountered in real applications.",
  "Weights/units: Indicates the weight or unit of products, suchas 2lbs, 15ct": "FreeText: The fall-back category not captured by any of theabove labels.A majority of data categories can be synthesized by randomgeneration. Part of the data are collected from the internet / open-source datasets; while we also collected some useful examples withLLM prompting, similar to the idea of UniversalNER . In totalwe have 10,000 data entries. During training, we leverage the idea ofmixup to further augment the training dataset, in case there aremultiple different categories in the input, we also create soft-labelswhen computing the training loss."
}