{
  "ABSTRACT": "We describe lessons learned from developing and deploying ma-chine learning models at scale across the enterprise in a range offinancial analytics applications. These lessons are presented in theform of antipatterns. Just as design patterns codify best softwareengineering practices, antipatterns provide a vocabulary to describedefective practices and methodologies. Here we catalog and docu-ment numerous antipatterns in financial ML operations (MLOps).Some antipatterns are due to technical errors, while others are dueto not having sufficient knowledge of the surrounding context inwhich ML results are used. By providing a common vocabularyto discuss these situations, our intent is that antipatterns will sup-port better documentation of issues, rapid communication betweenstakeholders, and faster resolution of problems. In addition to cata-loging antipatterns, we describe solutions, best practices, and futuredirections toward MLOps maturity. ACM Reference Format:Nikil Muralidhar, Sathappan Muthiah, Patrick Butler,, ManishJain, Yu Yu, Katy Burne, Weipeng Li, David Jones, PrakashArunachalam, Hays Skip McCormick, and Naren Ramakrishnan,Department of Computer Science, Virginia Tech, Arlington, VA 22203,The Bank of New York Mellon, 240 Greenwich Street, New York, NY10286. 2021. Using AntiPatterns to avoid MLOps Mistakes. In Proceed-ings of ACM SIGKDD (KDD21). ACM, New York, NY, USA, 9 pages.",
  "INTRODUCTION": "The runaway success of machine learning models has given rise toa better understanding of the technical challenges underlying theirwidespread deployment . There is now a viewpoint encouraging the rethinking of ML as a software engineering enter-prise. MLOpsMachine Learning Operationsrefers to the body ofwork that focuses on the full lifecycle of ML model deployment, per-formance tracking, and ensuring stability in production pipelines.At the Bank of New-York Mellon (a large-scale investment bank-ing, custodial banking, and asset servicing enterprise) we havedeveloped a range of enterprise-scale ML pipelines, spanning areas Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior specific permission and/or afee. Request permissions from , Aug 2021, Singapore 2021 Association for Computing Machinery.ACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00 such as customer attrition forecasting, predicting treasury settle-ment failures, and balance prediction. In deploying these pipelines,we have encountered several recurring antipatterns that wewish to document in this paper. Just as design patterns codify bestsoftware engineering practices, antipatterns provide a vocabularyto describe defective practices and methodologies. Antipatternsoften turn out to be commonly utilized approaches that are actu-ally bad, in the sense that the consequences outweigh any benefits.Using antipatterns to desribe what is happening helps ML teamsget past any blamestorming and arrive at a refactored solutionmore quickly. While we do not provide a completed formal antipat-tern taxonomy, our intent here is to support better documentationof issues, rapid communication between stakeholders, and fasterresolution of problems.Our goals are similar to the work of that argues for the studyof MLOps through the lens of hidden technical debt. While manyof the lessons from dovetail with our own conclusions, ourperspective here is complementary, viz. we focus less on softwareengineering but more on data pipelines, how data is transducedinto decisions, and how feedback from decisions can (and should)be used to adjust and improve the ML pipeline. In particular, ourstudy recognizes the role of multiple stakeholders (beyond MLdevelopers) who play crucial roles in the success of ML systems.Our main contributions are: (1) We provide a vocabulary of antipatterns that we have en-countered in ML pipelines, especially in the financial an-alytics domain. While many appear obvious in retrospectwe believe cataloging them here will contribute to greaterunderstanding and maturity of ML pipelines. (2) We argue for a new approach that rethinks ML deploymentnot just in terms of predictive performance but in termsof a multi-stage decision making loop involving humans.This leads to a more nuanced understanding of ML objec-tives and how evaluation criteria dovetail with deploymentconsiderations. (3) Finally, similar to Model Cards , we provide several rec-ommendations for documenting and managing MLOps atan enterprise scale. In particular we describe the crucial roleplayed by model certification authorities in the enterprise.",
  "(b)": ": Forecasting US Treasury Settlement Fails. (a) Heightened market volatility during March and April from COVID-19,when traders switched to working remotely, led to difficulties for firms in making sure everything was running smoothly. Thelarger volume of securities settlements in that period contributed to a higher number of fails. (b) Settlement instructions sub-mitted between 2am and 7am NY time have a proportionally higher failure rate because the trade instructions are submittedwith less visibility into the days market conditions. up with their new owners by the time they are supposed to arrive.Such fails happen for many reasons, e.g., unique patterns in trad-ing, supply and demand imbalances, speediness of given securities,operational hiccups, or credit events. After the collapse of LehmanBrothers, which led to an increase in settlement fails, the TreasuryMarket Practices Group (TPMG) in our organization recommendeddaily penalty charges on fails to promote better market functioning.The failed-to party generally requests and recoups the TPMG failscharge from the non-delivering counterparty. After broad industryadoption, according to the Federal Reserve, the prevailing rate ofsettlement fails has fallen considerably.In the middle of the COVID-19 market crisis, demand for cashand cash-like instruments such as Treasuries was drastically higherthan normal, compounding the issue of settlement fails. ashowcases the fallout of COVID-19 on the market in the form ofsettlement fails during March and April 2020. Liquidity issues inthe Treasury market prompted the Fed to step in and buy more ofthe securities to restore calm.We have developed a machine learning service that uses intradaymetrics and other signals as early indicators of liquidity issues inspecific sets of bonds to forecast settlement failures by 1:30pmdaily NY time. The service also takes into account elements like thevelocity of trading in a given security across different time horizons,the volume of bonds circulating, a bonds scarcity, the number oftrades settled every hour and any operational issues, such as higher-than-normal cancellation rates. b showcases the daily failurerate dynamics (per hour) and characterizes the complexity of thetask that the aforementioned machine learning service is modeling.The resulting predictions help our clients, including bond dealers,to monitor their intraday positions much more closely, managedown their liquidity buffers for more effective regulatory capital treatment, and offset their risks of failed settlements. Throughthis and other ML services we have gained significant insight intoMLOps issues that we aim to showcase here.In developing and deploying this application, we encounter is-sues such as:",
  "ANTIPATTERNS": "For the most part, we present our antipatterns (summarized in) in a supervised learning or forecasting context. In a produc-tion ML context, there is typically a model that has been approvedfor daily use. Over time, such a model might be replaced by a newer(e.g., more accurate) model, or retrained with more recent data",
  "Data Leakage AntiPattern": "The separation of training and test while extolled in every ML101course can sometimes be violated in insidious ways. Data leakagerefers broadly to scenarios wherein a model makes use of informa-tion that it is not supposed to have or would not have availablein production. Data leakage leads to overly optimistic model per-formance estimates and poses serious downstream problems uponmodel deployment (specifically in high risk applications). Leakagecan happen sometimes unintentionally when feature selection isdriven by model validation or test performance or due to the pres-ence of (typically unavailable) features highly correlated with thelabel. Samala et. al. talk more about the hazards of leakage,paying particular attention to medical imaging applications. In ourdomain of financial analytics, increasingly complex features areconstantly developed such that their complexity masks underlyingtemporal dependencies which are often the primary causes of leak-age. Below are specific leakage antipatterns we have encountered. 3.1.1Peek-a-Boo AntiPattern. Many source time-series datasetsare based on reporting that lags the actual measurement. A goodexample is Jobs data which is reported in the following month. Mod-elers who are simply consuming this data may not be cognizant thatthe date of availability lags the date of the data, and unwittinglyinclude it in their models inappropriately. 3.1.2Temporal Leakage AntiPattern. When constructing train-ing and test datasets by sampling, the process by which such sam-pling is conducted can cause leakage and thus lead to not trulyindependent training and test sets. In forecasting problems, espe-cially, temporal leakage happens when the training and test splitis not carried out sequentially, thereby leading to high correlation(owing to temporal dependence and causality) between the twosets.",
  "Oversampling Leakage AntiPattern. An egregious formof leakage can be termed oversampling leakage, seen in situations": "involving a minority class. A well known strategy to use in im-balanced classification is to perform minority over-sampling, e.g.,via an algorithm such as SMOTE . In such situations, if over-sampling is performed before splitting into training and test sets,then there is a possibility of information leakage. Due to the subtlenature of this type of leakage, we showcase illustrations and per-formance characterizations of oversampling leakage, in the contextof customer churn detection in banking transactions in . 3.1.4Metrics-from-Beyond AntiPattern. This type of antipat-tern can also be seen as pre-processing leakage or hyper-parameterleakage. Often times, due to carelessness in pre-processing data,both training and test datasets are grouped and standardized to-gether leading to leakage of test data statistics. For example whenusing standard normalization, if test and train datasets are nor-malized together, then the sample mean and variance used fornormalization is a function of the test set and thus leakage hasoccurred.",
  "Act Now, Reflect Never AntiPattern": "Once models are placed in production, we have seen that predictionsare sometimes used as-is without any filtering, updating, reflection,or even periodic manual inspection. This is an issue especially insituations where we see 1) concept drift (discussed in section 3.7),2) irrelevant or easily recognisable erroneous predictions, and 3)adversarial attacks.It is important to have systems in place that can monitor, track,and debug deployed models. For instance, under such situationsit can be productive to have a meta-model that evaluates everymodel prediction and deems if it is trustworthy (or of requiredquality) to be delivered. For example, Ramakrishnan et. al. describe a meta-model called the fusion and suppression systemthat is responsible for the generation of final set of alerts froman underlying alert-stream originating from multiple ML models.The fusion and suppression system is responsible for performingduplicate detection, filling in missing values, and is also used tofine-tune precision / recall by suppressing alerts deemed to be oflow quality. A second solution could be to inspect model decisionsfurther by employing explanation frameworks like LIME . characterizes modeling decisions using meta-modeling frameworks.",
  "Tuning-under-the-Carpet AntiPattern": "Different values of hyper-parameters often prove to be significantdrivers of model performance and are expensive to tune and mostlytask specific. Hyper-parameters play such a crucial role in modelingarchitectures that entire research efforts are devoted to developingefficient hyper-parameter search strategies .The set of hyper-parameters differs for different learning algo-rithms. For instance, even a simple classification model like thedecision tree classifier, has hyper-parameters like the maximumdepth of the tree, the minimum number of samples to split an inter-nal node and the criterion to use for estimating either the impurityat a node (gini) or the information gain (entropy) at each node. En-semble models like random forest classifiers and gradient boostingmachines also have additional parameters governing the number ofestimators (trees) to include in the model. Another popular classifier,the support-vector machine which is a maximum margin classifier",
  "Attrited0.960.830.892,551.00Attrited0.480.810.60488.00acc.0.830.830.833,039.00macro avg.0.720.820.753,039.00wt. avg.0.880.830.843,039.00": ": Here we characterize the effect of oversampling in the financial application of banking customer churn (i.e., Attrition)detection. (a) Illustrates the pipeline wherein oversampling is carried out before separating the data for training and evaluation.(b) Illustrates the oversampling pipeline wherein the data for training and evaluation is first separated and only the trainingdataset is over-sampled. We can see that the pipeline in (a) shows better optimistic performance (i.e., F1 score for Attritedclass in a) than (b) (i.e., F1 score for Attrited class in b) due to leakage in information from over-sampling beforeselecting the test set",
  "MLOps with Financial ApplicationsKDD21, Aug 2021, Singapore": "requires the specification of hyper-parameters that govern the typeof kernel used (polynomial, radial-basis-function, linear etc.) aswell as the penalty for mis-classification which in-turn governs themargin of the decision boundary learned. For an exhaustive analy-sis of the effect of hyper-parameters, please refer to whereinthe authors perform a detailed analysis of the important hyper-parameters (along with appropriate prior distributions for each) fora wide range of learning models.The resurgent and recently popular learning methodology em-ploying deep neural networks also has hyper-parameters like thehidden size of intermediate layers, the types of units to employ inthe network architecture (fully connected, recurrent, convolutional),the types of activation functions (TanH, ReLU, Sigmoid), and typesof regularizations to employ (Dropout layers, Batch Normalization,Strided-Convolutions, Pooling, -norm regularization terms). Inthe context of deep learning models, this area of research is termedneural architecture search . Hyper-parameter optimization, hasbeen conducted in multiple ways, however thus far a combinationof manual tuning of hyper-parameters with grid-search approacheshave proven to be the most effective in searching overthe space of hyper-parameters. In , the authors propose thatrandom search (within a manually assigned range) of the hyper-parameter space yields a computationally cheap and an equivalentif not superior alternative to grid search based hyper-parameter op-timization. Yet other approaches pose the hyper-parameter searchas a Bayesian optimization problem over the search space. characterizes the optimization process on the learning task ofdetecting churn\" or customer attrition using their activity patternsin the context of banking transactions. The figures therein yieldan analysis of the hyper-parameter optimization process character-izing the relative importance of each hyper-parameter employedin the learning pipeline. As hyper-parameters play such a crucialrole in learning (e.g., we notice from the statistics in a thatan XGBoost model is able to achieve a 3.5% improvement in the F1score of detecting attrited customers relative to an XGBoost variantwithout hyperparameter tuning i.e., b), it is imperative thatthe part of a learning pipeline concerned with hyper-parameteroptimization be explicitly and painstakingly documented so as tobe reproducible and easily adaptable.",
  "PEST AntiPattern": "Like many applied scientific disciplines, machine learning (ML) re-search is driven by the empirical verification and validation of the-oretical proposals. Novel contributions to applied machine learningresearch comprise (i) validation of previously unverified theoreticalproposals, (ii) new theoretical proposals coupled with empiricalverification, or (iii) effective augmentations to existing learningpipelines to yield improved empirical performance. Sound empiricalverification requires a fair evaluation of the proposed approach w.r.tpreviously proposed approaches to assess empirical performance.However, it is quite often the case that empirical verification ofnewly proposed ML methodologies is insufficient, flawed, or foundwanting. In such cases, the reported empirical gains are actuallyjust an occurrence of the Perceived Empirical SuperioriTy (PEST)antipattern. For example, in , the authors question claimed advancesin reinforcement learning research due to the lack of significancemetrics and variability of results. In , the authors state thatmany years of claimed superiority in empirical performance in thefield of language modeling is actually faulty and showcase that thewell-known stacked LSTM architecture (with appropriate hyperpa-rameter tuning) outperforms other more recent and more sophisti-cated architectures. In , the authors highlight a flaw in manyprevious research works (in the context of Bayesian deep learning)wherein a well established baseline (Monte Carlo dropout) whenrun to completion (i.e., when learning is not cut-off preemptivelyby setting it to terminate after a specified number of iterations),achieves similar or superior results compared to the very same mod-els which showcased superior results when introduced. The authorsthereby motivate the need for identical experimental pipelines forcomparison and evaluation of ML models. In , authors conductan extensive comparative analysis of the supposed state-of-the-artword embedding models with a simple-word-embedding-model(SWEM) and find that the SWEM model yields performance com-parable or superior to previously claimed (and more complicated)state-of-the-art models. In our financial analytics context, we havefound the KISS principle to encourage developers to try simple mod-els first and to conduct an exhaustive comparison of models beforeadvocating for specific methodologies. Recent benchmark pipelineslike the GLUE and SQuAD benchmarks are potential waysto address the PEST antipattern.",
  "Bad Credit Assignment AntiPattern": "Another frequent troubling trend in ML modeling is the failure toappropriately identify the source of performance gains in a mod-eling pipeline. As the peer-review process encourages technicalnovelty, quite often, research work focuses on proposing empiricallysuperior, and complicated model architectures. Such empirical supe-riority is explained to be a function of the novel architecture whileit is most often the case that the performance gains are in fact afunction of clever problem formulations, data preprocessing, hyper-parameter tuning, or the application of existing well-establishedmethods to interesting new tasks as detailed by .Whenever possible, it is imperative that effective ablation stud-ies highlighting the performance gains of each component of anewly proposed learning models be included as part of the empiri-cal evaluation. There must also be a concerted effort to train andevaluate baselines and the proposed model(s) in comparable ex-perimental settings. Finally as noted in , if ablation studies areinfeasible, quantifying the error behavior and robustness ofthe proposed model can also yield significant insights about modelbehavior.",
  "Grade-your-own-Exam AntiPattern": "Usually modeling projects begin as curiosity-driven iterations toexplore for potential traction. The measure of traction is calculatedsomewhat informally without formal 3rd-party review or valida-tion. While not a problem at first, if the data science team continuesthis practice long enough, while building confidence in their re-sults, they never validate them and cannot compare unvalidatedresults against other methods. To avoid this antipattern, testing",
  "(c) Parameter Importance Plot": ": (a) Results of Attrition prediction task using XGboost classifier with hyperparameters tuned using tree structuredParzen estimator . (b) Results for the same task with XGBoost classifier trained with manually set hyperaparameters.We notice a drop in both precision (Prec.) and recall (Rec.) of the attrited customer (i.e., minority) class. (c) The parameterimportance plot depicts importance of each hyper-parameter in trained classifier; we notice learning-rate is by far the mostimportant hyperparameter to be tuned followed by n-estimators (i.e., number of trees), used in the XGboost ensemble. and evaluation data should be sampled independently, and for arobust performance analysis, should be kept hidden until modeldevelopment is complete and must be used only for final valuation.In practice, it is not uncommon for model developers to have accessto the final test set and by repeated testing against this known testset, modify their model accordingly to improve performance onthe known test set. This practice called HARKing (HypothesizingAfter Results are Known) has been detailed by Gencoglu et al. .This leads to implicit data leakage. Cawley et. al. discusses thepotential effects of not having a statistically pure test set such asover-fitting and selection bias in performance evaluation.The refactored solution here is not simple, but is essential andnecessary for effective governance and oversight. Data scienceteams must establish an independent Ground Truth system withAPIs to receive and catalog all forecasts and the data that were usedto make them. This system can provide a reliable date stamp thataccurately reflects when any data object or forecast was actuallymade available or made and can help track independent 3rd partymetrics that will stand up to audit.",
  "Set & Forget AntiPattern": "A core assumption of machine learning (ML) pipelines is that thedata generating process being sampled from (for training and whenthe model is deployed in production) generates samples that areindependent and identically distributed (i.i.d). ML pipelines pre-dominantly adopt a set & forget mentality to model training andinference. However, it is quite often the case that the statisticalproperties of the target variable that the learning model is tryingto predict change over time (concept drift ). Decision supportsystems governed by data-driven models are required to effectivelyhandle concept drift and yield accurate decisions. The primary tech-nique to handle concept drift is learning rule sets using techniquesbased on decision trees and other similar interpretable tree-basedapproaches. Domingos et al. proposed a model based on Hoeffd-ing trees. Klienberg et al. , propose sliding window and instanceweighting methods to maintain the learning model consistent withthe most recent (albeit drifted) data. Various other approaches based on rule sets, Bayesian modeling have been developed for detectionand adaptation of concept drift, details can be found in .An example of model drift adaptation can be seen in Chakraborty etal. for forecasting protest events. This work provides a use casewherein changes in surrogates can be used to detect change pointsin the target series with lower delay than just using the targetshistory.",
  "Communicate with AmbivalenceAntiPattern": "Most ML pipelines are tuned to generate predictions but little atten-tion is paid to ensure that the model can sufficiently communicateinformation about its own uncertainty. A well-calibrated model isone where the Brier score (or similar) is carefully calibrated in itsconfidence. When poorly calibrated models are placed in produc-tion, it becomes difficult to introduce compensatory or remedialpipelines when something goes wrong. Characterizing model un-certainty is thus a paramount feature for large-scale deployment.Recent work shows that in addition to explainability, conveyinguncertainty can be a significant contributor to ensuring trust in MLpipelines.",
  "Data Crisis as a Service AntiPattern": "The development of models using data manually extracted and hy-giened without recording the extraction or hygiene steps leads to amassive data preparation challenge for later attempts to validate (oreven deploy) ML models. This is often the result of sensitive datathat is selectively sanitized for the modelers by some third-partydata steward organization that cannot adequately determine therisk associated with direct data access. The data preparation stepsare effectively swept under the carpet and must be completely re-invented later, often with surprising impact on the models becausethe pipeline ends up producing different data.The refactored solution here is to: (i) ensure that your enterprisesets up a professional data engineering practice that can quicklybuild and support new data pipelines that are governed and re-silient; (ii) use assertions to track data as they move through the",
  "RETHINKING ML DEPLOYMENT": "Machine Learning (ML) models are usually evaluated with met-rics (e.g., precision, recall, confusion matrices serve as evaluationmetrics in a classification setting) that are solely focused on char-acterizing the performance of the core learning model. However,production systems are often decision guidance systems with mul-tiple additional notification (e.g., a process that raises an alert whenthe core learning model yields a particular prediction) and inter-vention (e.g., a process that carries out an appropriate action basedon the results of the notification system) layers built on top of thecore learning layer.a showcases the outcomes in a traditional (ML focused)evaluation pipeline wherein an ML model predicts a transaction tohave a Favorable or Unfavorable outcome. Depending on the appli-cation, the definition of what is considered Favorable or Unfavorablemay differ.For illustration, let us consider a fraud detection application,wherein an unfavorable outcome would be defined as a fraudulenttransaction while legitimate transactions would be considered fa-vorable outcomes. An ML model tasked with detecting fraudulenttransactions would predict whether each transaction was Favorableor Unfavorable. In this context, a indicates that the deployedML model pipeline may enter four possible states during its opera-tion life-cycle. However, b showcases a slightly more realisticML pipeline wherein notification (send alerts) and intervention(take appropriate action) layers are added on top of the ML modeldecisions to appropriately raise alerts or intervene to arrest progressof a potentially fraudulent transaction detected by the ML model.The addition of these alerting and notification mechanisms whichare imperative and ubiquitous in enterprise ML settings augmentthe number of possible states the ML pipeline may enter duringits operation. These new states the model may enter create morenuanced situations with new dilemmas which are not highlightedby a simplistic evaluation approach like the one indicated in a.For example, if we observe the state the pipeline reaches if the MLmodel predicts a transaction to be fraudulent (i.e., unfavorable) andthe notification pipeline does not notify the client of the modeldecision, then if the transaction is actually fraudulent, then thissituation is fraught with ethical ramifications. This exhaustive staterepresentation of the ML decision pipeline in c allows us toexplicitly add high penalties to such states allowing the ML, noti-fication and intervention models to be trained cognizant of suchpenalties, essentially allowing fine-grained control of the learn-ing and decision process. A more rigorous approach is to use areinforcement learning formulation to track decision making andactions as models are put in production.",
  "(5) Ensure human-in-the-loop operational capability at multiplelevels. Use our model presented for rethinking ML deploy-ment from as a basis to support interventions andcommunication opportunities": "Overall, the model development and management pipeline in ourorganization supports four classes of stakeholders: (i) the data stew-ard (who holds custody of datasets and sets performance standards),(ii) the model developer (an ML person who designs algorithms), (iii)the model engineer (who places models in production and tracksperformance), and (iv) the model certification authority (group ofprofessionals who ensure compliance with standards and risk lev-els). In particular, as ML models continue to make their way intomore financial decision making systems, the model certificationauthority within the organization is crucial to ensuring regulatorycompliance, from performance, safety, and auditability perspec-tives. Bringing such multiple stakeholder groups together ensuresa structured process where benefits and risks of ML models arewell documented and understood at all stages of development anddeployment.",
  "Disclaimer:": "BNY Mellon is the corporate brand of The Bank of New York Mellon Cor-poration and may be used to reference the corporation as a whole and/orits various subsidiaries generally. This material does not constitute a rec-ommendation by BNY Mellon of any kind. The information herein is notintended to provide tax, legal, investment, accounting, financial or otherprofessional advice on any matter, and should not be used or relied upon assuch. The views expressed within this material are those of the contributorsand not necessarily those of BNY Mellon. BNY Mellon has not indepen-dently verified the information contained in this material and makes norepresentation as to the accuracy, completeness, timeliness, merchantabilityor fitness for a specific purpose of the information provided in this material.BNY Mellon assumes no direct or consequential liability for any errors inor reliance upon this material.",
  "Joo Gama, Indre liobaite, Albert Bifet, Mykola Pechenizkiy, and AbdelhamidBouchachia. 2014. A survey on concept drift adaptation. ACM CSUR 46, 4 (2014),137": "Oguzhan Gencoglu, Mark van Gils, Esin Guldogan, Chamin Morikawa, MehmetSzen, Mathias Gruber, Jussi Leinonen, and Heikki Huttunen. 2019. HARK Sideof Deep LearningFrom Grad Student Descent to Automated Machine Learning.arXiv preprint arXiv:1904.07633 (2019). Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup,and David Meger. 2018. Deep reinforcement learning that matters. In Proceedingsof the AAAI Conference on Artificial Intelligence, Vol. 32.",
  "Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \" Why should itrust you?\" Explaining the predictions of any classifier. In ACM SIGKDD. 11351144": "Ravi K Samala, Heang-Ping Chan, Lubomir Hadjiiski, and Sathvik Koneru. 2020.Hazards of data leakage in machine learning: a study on classification of breastcancer using deep neural networks. In Medical Imaging 2020: Computer-AidedDiagnosis, Vol. 11314. International Society for Optics and Photonics, 1131416. David Sculley, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Diet-mar Ebner, Vinay Chaudhary, Michael Young, Jean-Francois Crespo, and DanDennison. 2015. Hidden technical debt in machine learning systems. NeurIPS 28"
}