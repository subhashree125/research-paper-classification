{
  "Abstract": "Large Language Models (LLMs) have revolutionized natural lan-guage processing, but their robustness against adversarial attacksremains a critical concern. We presents a novel white-box styleattack approach that exposes vulnerabilities in leading open-sourceLLMs, including Llama, OPT, and T5. We assess the impact of modelsize, structure, and fine-tuning strategies on their resistance toadversarial perturbations. Our comprehensive evaluation acrossfive diverse text classification tasks establishes a new benchmarkfor LLM robustness. The findings of this study have far-reachingimplications for the reliable deployment of LLMs in real-world ap-plications and contribute to the advancement of trustworthy AIsystems.",
  "Introduction": "In recent years, the field of artificial intelligence has witnesseda remarkable surge in the development and application of LargeLanguage Models (LLMs). These models, such as ChatGPT ,GPT-4 , and Llama-2 , have demonstrated exceptional per-formance in various natural language understanding and generationtasks . The success of LLMs can be attributed to the innovativetraining techniques employed, including instruction tuning, prompttuning, Low-Rank Adaptor (LoRA) . These advances havemade it possible to fine-tune and infer models like Llama-2-7B onconsumer-level devices, thereby increasing their accessibility andpotential for integration into daily life. Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from , July 2017, Washington, DC, USA 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-x-xxxx-xxxx-x/YY/MM However, despite their impressive capabilities, LLMs are notwithout limitations. One significant challenge is their susceptibilityto variations in input types, which can lead to inconsistencies inoutput and potentially undermine their reliability in real-worldapplications. For example, when faced with ambiguous or provoca-tive prompts, LLMs may generate inconsistent or inappropriateresponses. To address this issue, several studies have been con-ducted to assess the robustness of LLM models . However,these efforts often overlook the importance of re-fine-tuning themodels and conducting comprehensive studies of adversarial at-tacks with known adversarial sample generation mechanisms whenfull access to the model weights, architecture, and training pipelineis available .In this paper, we present an extensive study of three leading open-source LLMs: Llama, OPT, and T5. We evaluate the robustness ofvarious sizes of these models across five distinct NLP classificationdatasets. To assess their vulnerability to input perturbations, weemploy the adversarial geometry attack technique and measure theimpact on model accuracy. Furthermore, we investigate the effec-tiveness of commonly used methods in LLM training, such as LoRA,different precision levels, and variations in model architecture andtuning approaches.Our work makes several notable contributions to the field ofLLM evaluation and robustness:",
  "Related Work2.1The Evaluation of LLMs": "In recent years, the LLM domain has experienced significant ad-vances . A large number of exemplary large-scale modelssuch as GPT-4, have emerged, showcasing exceptional performanceacross various sectors.Given the remarkable capabilities and broad applications of thesemodels, evaluating their performance has become paramount. Con-sequently, a significant portion of the research is dedicated to NLPtasks. For instance, examines ChatGPTs performance in senti-ment analysis, while offers a comparative analysis with otherLLMs. Numerous studies have explored the capabilities of LLMin natural language understanding, including text classification as",
  "Conference17, July 2017, Washington, DC, USAZeyu Yang, Xiaochen Zheng, Zhao Meng, and Roger Wattenhofer": "in Sec. 3.2, evaluating its accuracy on the corresponding validationset to establish a performance baseline.2) Adversarial Attack Assessment: The fine-tuned model under-goes adversarial attacks described in Sec. 4.2, and its performanceis assessed on a test dataset altered with adversarial examples.3) Robustness Evaluation: We compare the models accuracybefore and after the adversarial attacks to assess its robustness andvulnerability to such manipulations.",
  "Robustness in NLP": "With the rapid advancement in NLP research, its applications havebecome increasingly prevalent. This ubiquity underscores the grow-ing need for reliable NLP systems that can effectively counteractmalign content and misinformation. A seminal work high-lighted the vulnerabilities of NLP systems to adversarial attacks.There are some works about various input perturbations, whichcould be categorized into three groups: character level, word level,and sentence level . At the character level, adversarial at-tacks focus on altering individual characters within a given text. introduced HotFlip, utilizing gradient information to manipulatecharacters within text. took a different approach by identifyingwords and modifying their characters. At the word level, adversarialstrategies revolve around replacing specific words within the con-tent. For instance, employs evolutionary algorithms to swap outwords with their synonyms. utilized probabilistic sampling togenerate adversarial examples. Furthermore, some researchers haveexplored adversarial tactics at the sentence level. suggesteda method that introduces an extraneous sentence to the primarycontent, aiming to mislead reading models. On the other hand, adopted a new approach, where they employed an encoder-decoderframework to rephrase entire sentences. Recently, evaluatedthe robustness and trustworthiness of GPT-3.5 and GPT-4 models,revealing vulnerabilities such as the ease of generating toxic andbiased outputs and leaking private information. Despite GPT-4s im-proved performance on standard benchmarks, it is more susceptibleto adversarial prompts, highlighting the need for rigorous trust-worthiness guarantees and robust safeguards against new adaptiveattacks. However, the landscape of NLP research is ever-evolving. Withthe introduction of more sophisticated models boasting novel ar-chitectures and training methodologies, there is an growing needto assess the robustness of these newer models. This is especiallytrue for LLMs, which present unique challenges and opportunitiesin the realm of robustness research.",
  "We apply the following fine-tuning techniques in our study": "LoRA. LoRA innovates in fine-tuning pretrained language mod-els for specific tasks, addressing the inefficiency of full fine-tuningin increasingly large models. By inserting trainable rank decom-position matrices into each layer and freezing the original modelweights, LoRA significantly reduces the number of parameters re-quiring training. Quantization. Quantization in large language models (LLMs)reduces the model size by lowering weight precision, with 8-bitprecision presenting challenges due to errors from quantizing large-value vectors. These errors are pronounced in transformer archi-tectures, requiring mixed-precision decomposition. This involvesidentifying outliers using a threshold, processing them in fp16, andquantizing the rest of the matrix at 8-bit precision. The two parts arethen combined. The approach, exemplified by LLM.int8(), aims tomake large models more accessible, trading off some performancefor significant size reduction. QLoRA. Quantized Low-Rank Adapters (QLoRA) introducean efficient technique to fine-tune large language models by sig-nificantly lowering memory requirements. QLoRA combines 4-bitquantization with Low-Rank Adapters, freezing the parameters ofa compressed pretrained language model.",
  ": The framework of our adversarial robustness assessment": "containing words, and its corresponding category label is . Ourtextual classification system is built upon LLMs, represented as (), coupled with a prompt indicating the categorization task, de-noted as . In a formal sense: = (; ), where stands forthe given answer. The prediction is accurate when equals .An adversarial attack based on word replacement processes theoriginal sample to produce an adversarial version by re-placing the -th word in with an alternative word .To ensure that the original sample and its adversarial counter-part maintain semantic similarity, prevalent methodologiestypically employ synonymous terms for replacements.",
  "Geometry Attack Methodology": "In our research, we extend the basic principles of adversarial at-tacks in the context of LLMs. Our focus is on exploiting geometricattacks to assess the vulnerability of LLMs to adversarialperturbations. We propose a systematic methodology grounded ingeometric attack insights. The following sections detail the stepsof our approach:1) Gradient Computation for Influence Analysis: We com-mence by calculating the gradients of the generation loss L withrespect to the embeddings of input sentence S. The cross entropyloss L measures the dissimilarity between the prediction and labelexamples in the output space. This computation is essential for allwords, including those segmented into sub-tokens. For such words,gradients are computed for each sub-token and subsequently av-eraged. This initial step is crucial for identifying the words thatexert significant influence on L. We determine the gradient of Lwith respect to the embedding vector . This step determines thedirection in which should be adjusted to maximize the increase inthe loss L. The resulting gradient vector is denoted as = L.2) Selection of Candidate Words: Suppose we select a tar-get word from step 1. Utilizing the DeepFool algorithm , we identify potential replacement words, forming a candidate set{1,2, . . . , }. Candidates are filtered based on their cosinesimilarity to , with those below a defined threshold being ex-cluded. This process ensures that only semantically similar andrelevant candidates are considered.3) Optimal Word Replacement and Projection Analysis:After replacing with each candidate word, we compute the newtext vectors {1,2, . . . , }. For each vector, we define the deltavector as . The projection of onto is calculated as = . The optimal replacement candidate is selected based on criterion = arg max| ||| || . This ensures that the cho-sen word induces the largest possible projection onto thegradient vector .4) Iterative Process for Enhanced Adversarial Strength: Theselected word replaces in , updating to . This itera-tive procedure is repeated for cycles, where is an adjustableparameter in our methodology. Throughout these iterations, anincrease in L should be observed, indicating a continuous en-hancement in the adversarial effectiveness of the altered input.Through this methodically structured process, our research aimsto uncover and analyze potential vulnerabilities in LLMs. We refinedour methodology to enable prompt fine-tuning for attack generationtasks, expanding its application beyond the previously limited scopeof classification tasks.",
  "Datasets": "To evaluate the models performance under various tasks and itsresilience to attacks, we employed five classification datasets, cate-gorized into binary and multi-class classifications. For binary classi-fication, the datasets include IMDB , MRPC , and SST-2 ,and for multiclass classification, AGNews and DBpedia are used. We will provide a more detailed introduction to thesetasks/datasets in Appendix A.2",
  "Experimental Results": "In this section, we conduct extensive experiments to evaluate the ro-bustness of LLMs across five different datasets. These investigationsare guided by three key research questions (RQ):RQ1: How does the robustness of variously sized models differunder adversarial attacks across distinct tasks?RQ2: Do contemporary training techniques for LLMs influencetheir performance and robustness?RQ3: How does the model architecture (e.g., fine-tuning with aclassification head vs. prompt tuning), affect the robustness of themodel?",
  "Model Size (RQ1)": "In this section, we analyze the performance metrics of various mod-els across multiple tasks. The datasets under examination includeIMDB, SST-2, MRPC, AGNews, and DBPedia. We measure the per-formance and robustness of LLMs with the metrics Acc, Acc/attack,ASR, and Replacement Rate described in . The results from the IMDB dataset in reveal distinctperformance variations among different model architectures. In theT5 Series, accuracy generally improves with increasing model size,from 60m to 11b parameters, but the relationship is nonlinear. Thissuggests that while larger models tend to be more accurate, theaccuracy does not increase uniformly with model size. Furthermore,the resilience of these models to adversarial attacks does not followa simple inverse relationship with model size. The larger T5-11bmodel shows a more noticeable decrease in accuracy under attackconditions.For the OPT models, a similar upward trend in accuracy is ob-served with increasing model size, but the Attack Success Rate(ASR) is lower, suggesting better resistance to attacks. In compari-son, the Llama models demonstrate superior performance in bothaccuracy and robustness against attacks.",
  ": MRPC Dataset Results": "the OPT series, the OPT-6.7b model stands out. However, similarto the IMDB dataset, this model also shows a significant decline inaccuracy but more robust than T5 models. One more observationin the OPT series is the overall decrease in ASR with increasingmodel size, but this trend is disrupted at the 13b parameter mark,where an anomalous increase in ASR is observed. The Llama models,demonstrate consistently high accuracy. It also presents lower ASRcompared to T5 models but similar performance to OPT models.For SST-2, the ASR of T5 models exhibit a trend entirely contraryto that observed for MRPC. It reaches its minimum at the T5-770mmodel. For the OPT models, although their ASR is much lowercompared to the T5 series, there is a consistent decrease in ASR asthe size of the OPT models increases. Regarding the Llama models,the 7b model slightly outperforms the 13b in terms of accuracy andASR.",
  ": AGNews Dataset Results": "In Tables 6 and Tables 7, analyzing results from multi-class clas-sification tasks, a distinct pattern emerges. These datasets revealenhanced stability against synonym substitution attacks. For T5models, the data shows a lower ASR on these tasks compared tobinary datasets, suggesting a better resistance to attacks in complexclassification scenarios. In contrast, OPT and Llama models exhibit",
  ": DBPedia Dataset Results": "a higher ASR on the AGNews and DBpedia14 datasets. Anotherresult is that for both T5 and OPT series, there is a marked declinein ASR around the 770 million or 1 billion parameter threshold. Thisindicates an increased robustness and better handling of adversarialattacks with the scale-up of model size. 6.1.1Analysis. When examining the accuracy of the model, weobserved a trend where the accuracy gradually increases with thegrowth in model size. However, after reaching a certain size thresh-old, the accuracy tends to saturate, stabilizing around specific val-ues. This phenomenon is particularly pronounced when tested ondatasets like DBpedia. When comparing different models operatingat the same parameter scale, their performances were found to bequite similar, without any significant disparities.However, the experiments related to robustness revealed moredistinct differences. From a, b and c, wehave more intuitive results. Observing the performance of a uniformmodel across various datasets, we made the following observations:T5 Model: As the size of the T5 model increases, its ASR graduallydecreases. This suggests that larger models, with more parameters,tend to have a deeper understanding of language. As a result, theycan maintain stronger stability in the face of various disturbances.However, on datasets like MRPC and SST-2, there were noticeablefluctuations in performance. One possible explanation for this isthat as the model size grows, the words selected based on themodels gradient become more precise and have a more significantimpact on the results. This introduces a trade-off related to modelsize.OPT Model: For the OPT model, a similar trend was observedacross most datasets. As the model size increased, its robustnessgenerally improved, aligning with the observations made for theT5 model.Llama Model: For the Llama model, the differences in performancebetween the two sizes were minimal. This suggests that the sizevariation did not significantly influence the models robustness.However, when comparing different models, the disparities be-come even more pronounced. It is obvious that the T5 models ASRand replacement rate are significantly higher than those of OPT andLlama. This indicated that Decoder-only Causal LMs have higher",
  "LLMs Fine-tuning Techniques (RQ2)": "6.2.1Instruction Tuning. To study the impact of instruction tuningon model robustness, we compared the performance of Flan-T5 withthe standard T5. The Flan-T5 is an advanced variant of T5 that hasundergone instruction tuning across over a thousand downstreamtasks. In contrast, the traditional T5 was not trained with such anextensive procedure.Based on our experimental results, as shown in the table, thereis a significant decline in accuracy for both T5 and Flan-T5 underadversarial attacks. This observation indicates that models, irrespec-tive of whether they have undergone instruction tuning, remainsusceptible to adversarial manipulations. Furthermore, consistentwith our previous findings, we noticed that as the model size in-creases, the attack success rate tends to decline.Interestingly, as shown in Fig 3, our results indicated that Flan-T5 exhibits a higher ASR than the standard T5. This suggests thatmodels subjected to instruction tuning, like Flan-T5, can be moreeasily compromised. We hypothesize the primary reason for thisobservation:The instruction tuning process for Flan-T5 encompassed datasetssimilar to IMDB. This might have rendered the model with a deeperunderstanding of tasks related to this data. As a result, attackerscould more easily pinpoint words in the input that were influentialand susceptible to replacement.",
  ": Experimental results comparing T5 and Flan-T5 afterinstruction tuning using the IMDB dataset": "6.2.2Precisions. In machine learning, balancing model size withprecision is crucial. Model size indicates capacity, and precisionaffects information granularity. Larger models typically performbetter but require more computational resources. Techniques likequantization and precision adjustments help deploy these modelsmore efficiently. We studied the impact of precision settings on therobustness of T5-770m and OPT-1.3b models by comparing theirperformance under various precisions.For the T5-770m and OPT-1.3b models, its clear that as precisionchanges from fp16 to int4, there isnt a significant drop in theirinherent accuracy. This indicates that models can handle reducedprecision without compromising their general performance drasti-cally. Whats more, across different precision settings, the attacksuccess rate for the T5-770m models remains fairly higher, which shows the same conclusion as in 6.1. However, the precision set-tings do not show a consistent pattern of influence on the ASR andreplacement rate.In essence, while different models exhibit different robustnessagainst adversarial attacks, the precision settings do not play asignificant role in this robustness.",
  ": Results of the T5-770m, OPT-1.3b, and OPT-2.7bmodels performance with and without the application ofLoRA, using the IMDB dataset": "6.2.3LoRA. As mention in Sec 3.2, LoRA has been a groundbreak-ing approach, bringing about significant reductions in memoryrequirements during model training. In this case, the potentialtrade-off in question is model robustness.For our investigation, we selected the T5-770m, OPT-1.3b, andOPT-2.7b models. Experiments were conducted under two condi-tions for each model: with and without the application of LoRA.The IMDB dataset served as our benchmark for this analysis.The experiments show that adversarial attacks significantly re-duce accuracy across all models, regardless of LoRAs use. However,crucially, both the attack success rate and replacement rate, keymeasures of resilience against adversarial tactics, were unaffectedby LoRA. This indicates that while LoRA enhances optimization,it doesnt negatively impact the models defense against adver-sarial attacks, providing optimization benefits without sacrificingrobustness.",
  "Model Architectures (RQ3)": "The architecture of a models output space significantly influencesits performance and resilience against adversarial attacks. For mod-els with a classification head, the output is simplified to a binarydecision, contrasting with OPT models without such a head, whichmust identify negative or positive labels from a vast vocabulary.This distinction impacts the models accuracy.Our data shows that smaller models with a classification headare more accurate than headless ones due to their simplified out-put space, which aids decision-making, especially in models with",
  ": The experimental results of T5 and Flan-T5 on IMDBdataset": "limited processing power. Moreover, models with a head reachtheir peak performance faster, achieving accuracy saturation morequickly.However, an intriguing observation is the heightened attack suc-cess rate for models with a classification head. On the surface, thissuggests that launching adversarial attacks against these modelsis a more straightforward task.One main factor contributes to thisvulnerability is DeepFools efficacy with last layer FFN: In suchmodels, DeepFool can more readily discern the optimal directionfor launching its attack, amplifying the ASR. This marked efficiencyunderscores a reduced robustness in these models against adver-sarial intrusions.",
  "Conclusion": "This paper utilized a novel geometric adversarial attack methodto assess the robustness of leading LLMs, utilizing advanced fine-tuning techniques for task-specific model adaptation. Our ground-breaking approach revealed that these models exhibit variable sen-sitivity to adversarial attacks, influenced by their size and architec-tural differences. This indicates inherent vulnerabilities in LLMs,yet suggests potential resilience in certain configurations. Contraryto expectations, LLM-specific techniques did not markedly reducerobustness. Future research could explore models like RLHF andmodel parallelism approaches within this framework. Additionally,the evolution of more complex adversarial attacks promises deeperinsights into LLM strengths and weaknesses.",
  "Ethics statement": "In our research, we employ adversarial attack methodologies togenerate text, aiming to evaluate the robustness of LLMs againstinputs. However, we acknowledge the ethical implications associ-ated with the use of adversarial attacks. One primary concern is thepotential generation of harmful information. This includes text thatmay be offensive, misleading, or harmful in other ways. Therefore,one should be cautious when taking such methods into practicaluse. Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Srivastava,and Kai-Wei Chang. 2018. Generating Natural Language Adversarial Examples.In Proceedings of the 2018 Conference on Empirical Methods in Natural LanguageProcessing. Association for Computational Linguistics, Brussels, Belgium, 28902896. Sren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak,and Zachary Ives. 2007. Dbpedia: A nucleus for a web of open data. In internationalsemantic web conference. Springer, 722735. Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu,Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, Jiayin Zhang, Juanzi Li, and Lei Hou.2023. Benchmarking Foundation Models with Language-Model-as-an-Examiner.arXiv:2306.04181 [cs.CL] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, BryanWilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu,and Pascale Fung. 2023. A Multitask, Multilingual, Multimodal Evaluation ofChatGPT on Reasoning, Hallucination, and Interactivity. arXiv:2302.04023 [cs.CL] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora,Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, EmmaBrunskill, et al. 2021. On the opportunities and risks of foundation models. arXivpreprint arXiv:2108.07258 (2021).",
  "Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. 2018. HotFlip: White-Box Adversarial Examples for Text Classification. arXiv:1712.06751 [cs.CL]": "Chuan Guo, Alexandre Sablayrolles, Herv Jgou, and Douwe Kiela. 2021.Gradient-based Adversarial Attacks against Text Transformers. In Proceedingsof the 2021 Conference on Empirical Methods in Natural Language Processing.Association for Computational Linguistics, 57475757. Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, LuWang, Weizhu Chen, et al. 2021. LoRA: Low-Rank Adaptation of Large LanguageModels. In International Conference on Learning Representations. Robin Jia and Percy Liang. 2017. Adversarial Examples for Evaluating ReadingComprehension Systems. In Proceedings of the 2017 Conference on Empirical Meth-ods in Natural Language Processing. Association for Computational Linguistics,Copenhagen, Denmark, 20212031. Akbar Karimi, Leonardo Rossi, and Andrea Prati. 2021.AEDA: An EasierData Augmentation Technique for Text Classification. In Findings of the As-sociation for Computational Linguistics: EMNLP 2021. Association for Com-putational Linguistics, Punta Cana, Dominican Republic, 27482754. Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting Wang. 2019. TextBugger:Generating Adversarial Text Against Real-world Applications. In Proceedings2019 Network and Distributed System Security Symposium. Internet Society. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michi-hiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar,Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove,Christopher D. Manning, Christopher R, Diana Acosta-Navas, Drew A. Hudson,Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, HuaxiuYao, Jue Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul,Mirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, PeterHenderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, SuryaGanguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary,William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. 2022.Holistic Evaluation of Language Models. arXiv:2211.09110 [cs.CL] Edward Ma. 2019. NLP Augmentation. Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng,and Christopher Potts. 2011. Learning word vectors for sentiment analysis.In Proceedings of the 49th annual meeting of the association for computationallinguistics: Human language technologies. 142150. Zhao Meng, Yihan Dong, Mrinmaya Sachan, and Roger Wattenhofer. 2022. Self-Supervised Contrastive Learning with Adversarial Perturbations for DefendingWord Substitution-based Attacks. In Findings of the Association for ComputationalLinguistics: NAACL 2022. 87101.",
  "Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. 2016.DeepFool: a simple and accurate method to fool deep neural networks.arXiv:1511.04599 [cs.LG]": "Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and DouweKiela. 2020. Adversarial NLI: A New Benchmark for Natural Language Under-standing. In Proceedings of the 58th Annual Meeting of the Association for Computa-tional Linguistics. Association for Computational Linguistics, Online, 48854901. OpenAI. 2023. GPT-4 Technical Report. (2023). arXiv:2303.08774 [cs.CL] OpenAI. 2023. OpenAI Chatbot. Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, ChristopherClark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep Contextualized Word Rep-resentations. In Proceedings of the 2018 Conference of the North American Chapterof the Association for Computational Linguistics: Human Language Technologies,Volume 1 (Long Papers). Association for Computational Linguistics, New Orleans,Louisiana, 22272237.",
  "Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga,and Diyi Yang. 2023. Is ChatGPT a General-Purpose Natural Language ProcessingTask Solver? arXiv:2302.06476 [cs.CL]": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020.Exploringthe Limits of Transfer Learning with a Unified Text-to-Text Transformer.arXiv:1910.10683 [cs.LG] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning,Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semanticcompositionality over a sentiment treebank. In Proceedings of the 2013 conference",
  "Task Description and Adversarial Example": "Dataset: IMDBModel: T5-780MOriginal: \"Choose the sentiment of this review? Expensive lunch meals. Fried pickles were good. Waitress messed up 2 orders out of 4.\"Dont\" think \"Ill\" return. Asked for no cheese waitress joked extra cheese then brought my meal with cheese. Better places to eat in area. \"Adversarial: \"Choose the sentiment of this review? Expensive brunch meals. Fried marinated were good. Waitress eventhough up 2orders out of 4. \"Dont\" imagining \"Ill\" return. Asked for no cheese miss jokingly extra cheese then brought my meal with cheese. Betterplace to devoured in area.\" Dataset: MRPCModel: OPT-2.7BOriginal: \"Do these two sentences mean the same thing? Crews worked to install a new culvert and prepare the highway so motoristscould use the eastbound lanes for travel as storm clouds threatened to dump more rain. Crews worked to install a new culvert and repavethe highway so motorists could use the eastbound lanes for travel.\"Adversarial: \"Do these two sentences mean the same thing? Crews acted to mount a nouvelle drains and prepare the avenue so chauffeurscould use the eastbound routing for voyage as stormy haze threatens to spill more rainfall. Crews worked to install a newer septic andrepave the highway so motorists could use the eastbound lanes for tours\" Dataset: AGNewsModel: Llama-7BOriginal: \"Dial M for Music Mobile-phone makers scored a surprising hit four years ago when they introduced handsets equipped withtiny digital cameras. Today, nearly one-third of the cell phones sold worldwide do double duty as cameras.\"Adversarial: \"Dial M for Melody Mobile-phone manufacturers scored a surprising hit four years ago when they unveiled handsetsequipped with tiny digital cameras. Today, nearly one-third of the cell phones sold worldwide do double duty as cameras.\"",
  "Arsene Fansi Tchango, Rishab Goel, Zhi Wen, Julien Martel, and JoumanaGhosn. 2022.DDXPlus: A New Dataset For Automatic Medical Diagnosis.arXiv:2205.09148 [cs.CL]": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-mine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-ale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucu-rull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, CynthiaGao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini,Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, IsabelKloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, ThibautLavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet,Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton,Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva,Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, RossTaylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov,Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Ro-driguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2:Open Foundation and Fine-Tuned Chat Models. arXiv:2307.09288 [cs.CL] Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. 2019.Universal Adversarial Triggers for Attacking and Analyzing NLP. In Proceedingsof the 2019 Conference on Empirical Methods in Natural Language Processing andthe 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics, 21532162. Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, ChenhuiZhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, Sang T. Truong, Sim-ran Arora, Mantas Mazeika, Dan Hendrycks, Zinan Lin, Yu Cheng, Sanmi Koyejo,Dawn Song, and Bo Li. 2023. DecodingTrust: A Comprehensive Assessment ofTrustworthiness in GPT Models. arXiv:2306.11698 [cs.CL] Boxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan, Yu Cheng, JianfengGao, Ahmed Hassan Awadallah, and Bo Li. 2022.Adversarial GLUE:A Multi-Task Benchmark for Robustness Evaluation of Language Models.arXiv:2111.02840 [cs.CL] Jindong Wang, Xixu Hu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong Wang,Linyi Yang, Haojun Huang, Wei Ye, Xiubo Geng, Binxin Jiao, Yue Zhang, and XingXie. 2023. On the Robustness of ChatGPT: An Adversarial and Out-of-distributionPerspective. arXiv:2302.12095 [cs.AI]",
  "Zengzhi Wang, Qiming Xie, Zixiang Ding, Yi Feng, and Rui Xia. 2023. Is ChatGPTa Good Sentiment Analyzer? A Preliminary Study. arXiv:2304.04339 [cs.CL]": "Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, SebastianBorgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H.Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fe-dus. 2022. Emergent Abilities of Large Language Models. arXiv:2206.07682 [cs.CL] Jason Wei and Kai Zou. 2019. EDA: Easy Data Augmentation Techniques forBoosting Performance on Text Classification Tasks. In Proceedings of the 2019Conference on Empirical Methods in Natural Language Processing and the 9thInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP).Association for Computational Linguistics, Hong Kong, China, 63826388. Huangzhao Zhang, Hao Zhou, Ning Miao, and Lei Li. 2019. Generating FluentAdversarial Examples for Natural Languages. In Proceedings of the 57th AnnualMeeting of the Association for Computational Linguistics. Association for Compu-tational Linguistics, Florence, Italy, 55645569. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, ShuohuiChen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov,Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, AnjaliSridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. OPT: Open Pre-trainedTransformer Language Models. arXiv:2205.01068 [cs.CL]",
  "Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutionalnetworks for text classification. Advances in neural information processing systems28 (2015)": "Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang,Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang,Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A Survey of LargeLanguage Models. arXiv:2303.18223 [cs.CL]",
  "AResearch MethodsA.1Experiments Setup": "In our study, we employ pretrained weights from HuggingFaceand use int8 quantization for GPU memory optimization. We alsostandardize the use of LoRA to reduce training parameters. Formodels under 3 billion parameters, experiments are conducted onan NVIDIA RTX 3090 (24GB), whereas models above 3 billion pa-rameters are tested on NVIDIA RTX A6000 (48GB) or NVIDIA A100(40GB), catering to both fine-tuning and attack simulations.",
  "A.2Dataset": "In this section, we will provide more details about the datasets usedin this work.IMDB: This dataset contains 50,000 movie reviews for sentimentanalysis, equally divided between positive and negative sentiments.SST-2: An extension of the original SST, it focuses on the binaryclassification of sentiments in movie review sentences.MRPC: A corpus for paraphrase identification, it includes sen-tence pairs from online news sources, annotated for semantic equiv-alence. AGNews: This news categorization dataset comprises 120,000training and 7,600 test samples across four categories: World, Sports,Business, and Science/Technology.DBpedia: A large-scale, multi-class dataset from the DBpediaknowledge base, featuring 560,000 training and 70,000 test samplesacross 14 categories.The statistics of these five datasets are presented in ."
}