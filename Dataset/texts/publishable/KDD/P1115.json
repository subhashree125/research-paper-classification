{
  "ABSTRACT": "The confluence of Federated Learning (FL) and Large LanguageModels (LLMs) is ushering in a new era in privacy-preserving nat-ural language processing. However, the intensive memory require-ments for fine-tuning LLMs pose significant challenges, especiallywhen deploying on clients with limited computational resources.To circumvent this, we explore the novel integration of Memory-efficient Zeroth-Order Optimization within a federated setting, asynergy we term as FedMeZO. Our study is the first to examinethe theoretical underpinnings of FedMeZO in the context of LLMs,tackling key questions regarding the influence of large parameterspaces on optimization behavior, the establishment of convergenceproperties, and the identification of critical parameters for conver-gence to inform personalized federated strategies. Our extensiveempirical evidence supports the theory, showing that FedMeZO notonly converges faster than traditional first-order methods such asFedAvg but also significantly reduces GPU memory usage duringtraining to levels comparable to those during inference. Moreover,the proposed personalized FL strategy that is built upon the theoreti-cal insights to customize the client-wise learning rate can effectivelyaccelerate loss reduction. We hope our work can help to bridgetheoretical and practical aspects of federated fine-tuning for LLMs,thereby stimulating further advancements and research in this area.",
  "INTRODUCTION": "Federated Learning (FL) has become an important approach inmodern machine learning, particularly in scenarios where datadecentralization and privacy-preserving are crucial . Central to this learning paradigm is the corroborative trainingof a global model through the aggregation of updates from multipleclients, without sharing their raw data . Corresponding author.0 Zhenqing Ling et al. | ACM 2024. This is the authors version of the work. It isposted here for your personal use. Not for redistribution. The definitive Version ofRecord was published in SIGKDD 2024, In parallel, Large Language Models (LLMs) have radically ad-vanced the field of natural language processing . Thefine-tuning of LLMs that are already pre-trained on vast corpora,has proven to be a highly effective strategy for numerous tasks,yielding models that are both versatile and capable of adapting tospecific domain narratives or aligning with human values .The tuning of LLMs requires suitable alignment data, which areoften costly to acquire . Due to the abundance of private datathat remains largely isolated and underutilized, the intersection ofFL and LLMs has sparked increasing interest among researchers. Notably, this integration presents significant com-putational challenges, especially for clients with limited resources. The scaling up of LLMs further compounds this issue, as thecomputation of gradients for backpropagation incurs substantialmemory costs, frequently surpassing the practical capabilities ofthese clients .Addressing this challenge, we turn our attention to Zeroth-OrderOptimization (ZOO), an algorithm that computes gradient approxi-mations without explicit gradient information, thus significantlyreducing memory consumption . However, the combination ofZOO and FL a research direction we refer to as ZOO-FL remainsunexplored in the literature in the context of LLMs . Our workintends to bridge this gap by harnessing the memory efficiency ofZOO within the context of federated fine-tuning of LLMs, especiallyon the following theoretical foundations:(Q1) How does the vast parameter space of LLMs influencethe behavior of ZOO-FL? (Q2) Can we establish the convergenceproperties of ZOO-FL for LLMs? (Q3) Which model parameters arecritical for convergence, and how can we leverage them to optimizeFL performance, such as via personalization?In this paper, we focus on incorporating a memory-efficient ZOOmethod, MeZO into FL, a synergy we denote as FedMeZO,and establishing its convergence properties under the large-scaleparameter space of LLMs. We analyze and present precise conver-gence rates characterized by the low effective rank of the models",
  "arXiv:2402.05926v3 [cs.LG] 17 Jun 2024": "Hessian matrices , and other typical FL parameters such asnumber of clients , number of FL rounds, iteration steps of localtraining and heterogeneity constants and . Moreover, wereveal the learning rate to be a crucial variable for convergence.Building on theoretical insights, we further propose a strategy thattailors the learning rate to each clients specific data characteristics.To validate our theoretical results, we conduct extensive exper-iments on real-world FL datasets for LLM tuning, which coverdiverse data distributions and application tasks. Our empirical find-ings corroborate the theoretical analysis, validating effective conver-gence even when scaling up to models with billions of parameters.Compared with first-order methods such as FedAvg, FedMeZOconverges faster meanwhile remarkably reducing GPU memoryrequirements. The personalized strategies guided by our theoreticalinsights, empirically show a more rapid loss reduction, as opposedto non-personalized or random learning rate assignments.In summary, our theoretical and empirical exploration validatesFedMeZO in the fine-tuning process of LLMs, providing a rigorousframework and practical insights for future applications. Our keycontributions are threefold:",
  "PRELIMINARIES2.1Background and Related Works": "Federated Fine-Tuning of Large Language Models. Large Lan-guage Models (LLMs) have demonstrated remarkable capabilitiesthat enable a variety of real-world applications . The fed-erated fine-tuning of LLMs has recently attracted attention, focusedon adapting these models to domain-specific tasks while preserv-ing the privacy of the training data. Chen et al. investigatedthe integration of LLMs within federated settings, highlighting theinherent challenges and potential opportunities. Zhang et al. furthered this research by examining instruction tuning of LLMsin a federated context, marking progress in applying FL to the spe-cialized training of LLMs. Notable frameworks such as FATE-LLMby Fan et al. and FederatedScope-LLM by Kuang et al. offer industrial-grade and comprehensive solutions for federatedfine-tuning. Our work, in contrast, investigates the fusion of Zeroth-Order Optimization (ZOO) with FL for the fine-tuning of LLMs, anarea that has yet to be fully investigated, thereby addressing a gapin the literature and providing fundamental theoretical insights. Zeroth-Order Optimization in Federated Learning. ZOO hasemerged as a viable method to address the difficulties of computinggradients in FL, especially in settings limited by computationalresources. Zhang et al. proposed a ZOO algorithm tailored forvertical FL, focusing on privacy preservation. Yi et al. and Liet al. studied ZOO-FL algorithms, with discussions on conver-gence properties with single-point perturbation and local updatesin decentralized FL, respectively. The convergence analysis is acritical aspect of FL, as illustrated by Li et al. for the FedAvgalgorithm and further developed by Fang et al. for mini-batchstochastic ZOO-FL in wireless networks. Moreover, Shu et al. proposed enhancements to query efficiency for ZOO within the FLframework. Our research sets itself apart by formulating theoret-ical convergence bounds for ZOO-FL, specifically tailored to thelarge-scale parameter space of LLMs. This builds on the preliminarywork by Malladi et al. , which confirmed the feasibility of ZOOfor LLMs in a centralized setting.",
  "For readability and brevity, we summarize the full list of intro-duced notations in Appendix A and present detailed proofs of alltheoretical results in Appendix B-D": "Federated Learning. We consider the general FL setting as ofFedAvg , with a central server and a collection of clients,indexed by 1, 2, ..., . The central server coordinates the training of aglobal model through the collaborative efforts of these clients, eachholding local data samples drawn from their respective distributionsD. The optimization problem can be formulated as:",
  "=1 (), () = EBD (, B),(1)": "where R denotes the -dimension parameter of the model,and () and () denote the global loss function on the centralserver and local loss function on client, respectively. Typically,the clients are assumed with equal importance , and the data israndomly sampled for efficiency . (, B) represents the localloss function w.r.t a specific mini-batch B drawn from D. Zeroth-Order Optimization. Zeroth-order optimization (ZOO)is a prominent technique in scenarios where gradients are difficult toobtain, which estimates gradients by forward propagations. Givena random vector and a smoothing constant , a typical one-pointgradient estimator is defined as:",
  "( + , B) ( , B),(3)": "where N (0, ) is a Gaussian random variable and is theperturbation scale. The two-point gradient estimator in Eq. (3)requires only two forward passes through the model to computethe estimation of gradient, which serves as a memory-efficientalternative to backpropagation (BP). The FedMeZO Algorithm. In this paper, we study and ana-lyze the proprieties of a practical synergy of MeZO and Fe-dAvg , which is designed to fine-tune LLMs in an efficient,privacy-preserving and personalized manner. We term this ZOO-FLapproach as FedMeZO, depicted with the following processes:In a single communication round, the central server first broad-casts the global model parameters to available clients. Once theclients have completed their local updates and uploaded their mod-els, the server aggregates the updates according to Eq. (1), formingthe basis for the subsequent round.Upon receiving the global model parameters, clients perform thefollowing steps, distinguishing FedMeZO from traditional BP-basedFedAvg algorithms in two-fold:(1) Training Memory Reduction: Clients update their models usingthe two-point ZOO gradient estimator defined in Eq. (3) as:",
  "(,)= ( (,),(,), B(,), ),(4)": "where (,) denotes the iteration within the communicationround. Unlike standard ZO-SGD algorithms that require storingthe perturbation vector at each iteration, FedMeZO resamples using random seeds in in-place implementation, thus reducingmemory usage to a level equivalent to inference .(2) Communication Cost Reduction: To mitigate the high commu-nication overhead associated with LLMs, FedMeZO leverages Low-Rank Adaptation (LoRA) , which introduces reparametriza-tion to tune two small delta matrix on the linear layers instead ofthe whole LLM weights, based on the assumption that well pre-trained LLM possess a low intrinsic dimension when adapted tonew tasks. Introducing it can help us further reduce the numberof parameters to be updated and uploaded, thereby aligning withthe practical constraints of federated settings. Detailed analysis ofcommunication cost is available in Appendix F.1.",
  "E[ (,, B, )] = ().(5)": "The Hessian matrix, which is the square matrix of second-orderpartial derivatives of the loss w.r.t the model parameters, character-izes the curvature of the loss surface . Although the size of amodels loss Hessian is often associated with the rate of fine-tuning,studies suggest that the large-scale parameters of LLMs do notnecessarily impede convergence . This paradox is addressedby recognizing that the loss Hessian often exhibits a small localeffective rank , which we capture in the following assumption:",
  "The effective rank of H (), denoted as tr(H ())/H ()op,is at most . Here tr denotes the trace of the matrix, and op denotesthe operator norm": "Assumption 1 characterizes a low effective rank in the Hessianmatrix, which demonstrates that LLM fine-tuning can occur in alow dimensional subspace ( 200 parameters) . With thisinsight, identified the bound of loss descent at each step ofcentralized ZOO, which is partially influenced by : Lemma 2.3. (Bounded Centralized Descent) Assume () is -smooth and let (,, B, ) be the unbiased zeroth-order gradientestimator from Eq. (3). If the Hessian matrix H () exhibits a localeffective rank of , and constants = (/) and = (1/) exist,then the expected decrease in loss can be bounded as follows:",
  "(+2) , =(+2)2": "(+2) (+1) , and is the number ofrandomizations.From Eq. (6), we observe that the rate of descent at a single stepdepends on the gradient related to and the gradient estimationrelated to . Following , we set to 1 in this paper.Besides, to facilitate the analysis in FL setting, we introduce fourassumptions, including Bounded Loss (Assumption 2), -smoothness(Assumption 3), mini-batch gradient error bound (Assumption 4,global-local disparities in i.i.d. and non-i.i.d. settings (Assumptions 5and 6 respectively). These assumptions are standard and founda-tional in optimization and FL literature , which wedetail in Appendix B.",
  "In Theorem 3.1, the term (2/)E | ()|2 serves as a criticalfactor that drives the decrease in the loss function, as it is the solenegative contributor in Eq. (8) such that E (+1) () 0": "Note that the presence of the factor 1 = ( 1) underscoresthe impact of the low effective rank on the convergence rate(under Assumption 1), revealing that a reduction in can accelerateconvergence independently of the high-dimensional parameterspace . Consequently, even for LLMs with expansive parameterspaces, FedMeZO can attain convergence. This addresses our firstfoundational question Q1: How does the vast parameter space ofLLMs influence the behavior of ZOO-FL?.Moreover, the terms (2/)E ()2 and (22/)are scaled by /, i.e., in (1/2), contributing a relatively smallereffect on the convergence speed compared to negative term. Thisdemonstrates that the influence on convergence speed from thezeroth-order gradient estimation is moderated by the models effec-tive low rank and dimensionality. As for the last term, (23/2),it acts as a factor slowing down the convergence rate, and we canobserve that when and are larger, this term becomes smaller.This suggests that the effect of slowing down the convergence rateis not as pronounced, and simultaneously, the perturbation step should not be excessively large. Specifically, this term indicates thatincreasing the number of clients and the number of local roundscan enhance convergence, while also emphasizing the importanceof keeping the perturbation step moderate.After gaining intuitive insights in each round of training throughthe analysis of Theorem 3.1, it is necessary to assess the convergenceperformance of FedMeZO from a global perspective. We utilize thesquared magnitude of the gradient E ()2 as a measure toassess the suboptimality of each iterate. The rapidity with whichthe algorithm approaches a stationary point serves as a crucialmetric for determining its efficacy in the context of non-convexoptimization problems .",
  "where denotes the optimal loss value": "The upper bound on the minimum squared gradient norm acrossiterations is composed of three terms in Corollary 3.2. The first termindicates that the distance to the optimal loss relies on the initialstate of optimality, while the second and third terms elucidate theinfluences of stochastic mini-batch errors and the perturbation scale inherent to ZOO, respectively. Specifically, they both reflect theimpact of the model parameters and the low effective rank onthe optimal loss. As pointed out in , by choosing an appropriatestep size, we can obtain the desired accuracy.Given that = () and = 1",
  "(10)": "The expression on the right-hand side of Eq. (10) is dominatedby O3/2()1/2. Consequently, we have derived the con-vergence rate for FedMeZO. The low effective rank significantlycontributes to lowering the convergence rate, which is also influ-enced by the number of clients , the steps of local training iteration, and the total number of communication rounds . Moreover,to satisfy the learning rate condition in Eq. (7), the values of , ,and must be suitably large.It is important to note that FedMeZO does not primarily aim toaccelerate convergence speed but rather to identify the convergencerate under assumptions pertinent to LLMs. This is intended todemonstrate that FedMeZO can achieve convergence even withina vast parameter space. In a series of studies on federated ZOO,Federated Zeroth-Order Optimization (FedZO) presents the mostcomprehensive and complete analysis with a convergence rate of",
  "compared to O": "/of DZOPA . In contrast to FedZO, ourmethod, FedMeZO, theoretically supports a faster convergence byreplacing1/2 with 3/2 and setting1 = 2 = 1. These comparisonsshow that FedMeZO addresses the challenges posed by large models,offering an efficient convergence rate that relies on .This advancement signifies progress in optimizing federatedlearning algorithms, particularly for LLMs, where the scalabilityof parameters and data heterogeneity are major challenges. Byemphasizing the low effective rank, our approach enhances boththe theoretical understanding of convergence behavior in complexsettings and the guidance insights into the settings of learning ratesand other parameters to achieve efficient convergence outcomes.However, i.i.d. data is typically encountered in idealized envi-ronments. In real-world applications, non-i.i.d. conditions are morecommon and challenging. Next, we further discuss and analyze theconvergence of FedMeZO under non-i.i.d. settings.",
  "where = + and 2 = 32 + 2": "Comparing Eq. (12) with its i.i.d. counterpart Eq. (8), the non-i.i.d.setting introduces additional terms reflecting data heterogeneity.Firstly, an additional term appears before E ()2; secondly,the original 2 change into 2; thirdly, a new term related to 2is added at the end. The term amplifies the effect of the gradi-ent norm, while 2 encapsulates both the intrinsic stochasticityand data heterogeneity. The presence of 2 indicates the impactof client data divergence on the convergence behavior, in a degreedependent on (1/2). Given that the contribution of the negativeterm accelerates the rate of decline in each round, it can be con-cluded that heterogeneity is positively correlated with convergence.Considering all the above changes, appropriate heterogeneity canaid in the model convergence.Experimental results in .3.3 confirm that a more random-ized dataset distribution leads to improved convergence, supportingour theoretical insights. Next, we present the global convergenceresult for the non-i.i.d. setting building upon Theorem 3.4.",
  "which is dominated by . Consequently,": "simplifies to 1as in the non-i.i.d. case. Compared to Corollary3.2, Corollary 3.5 introduces two changes: first, all terms on theright side of Eq. 13 include a denominator , and second, there isan additional term associated with non-i.i.d. heterogeneity, scaledwith (2/). This further demonstrates the constraining ef-fect of data heterogeneity in FedMeZO. Similar to Corollary 3.3,by setting appropriate values for and , we obtain the followingconvergence rate.",
  "2is preferred while the": "O2()1term need to increase at the same time. Conse-quently, the balance between , , and becomes a dynamictrade-off process, i.e., the heterogeneity among different clientsdirectly influences the overall convergence performance.For now, we have answered the question Q2: Can we establishthe convergence properties of ZOO-FL for LLMs? via theorems andcorollaries mentioned in this section. We also validated the nature ofconvergence under different scenarios and tasks through empiricalexperiments in .2.",
  "Implications": "The aforementioned theoretical results offer numerous insights intoparameter tuning. A critical revelation from our analysis pertainsto the constraints imposed on the learning rate, as delineated inEq. (7) and Eq. (11), which suggests that an optimal learning ratemagnitude is anchored at 1/ . Larger learning rates are not onlyineffectual but also pose a risk of destabilizing the training dynamic.In Appendix F.4, our empirical experiments corroborate this hy-pothesis, demonstrating that excessive learning rates precipitateabrupt increases in loss.Furthermore, our insights regarding the learning rate open upprospects for personalized FL, a compelling approach that usesclient-specific configurations to address heterogeneity and has at-tracted increasing interest . Specifically, we inves-tigate theory-guided personalized strategies by dynamically adjust-ing the learning rate in proportion to a quantifiable measureof data heterogeneity among clients. In light of Theorem 3.4 thata larger heterogeneity is more conducive to model convergence,it is feasible to appropriately increase the learning rate, allowingspecific clients to contribute more to the overall convergence, wepropose the following tailored adjustment strategy: Proposition 3.7. (Adaptive Learning Rate Adjustment) Let As-sumption 6 hold, the learning rate can be adjusted according to theformula to better accommodate the varied learning landscapes thannon-personalized FL:",
  "= 0(1 + ),(15)": "where 0 represents a default learning rate applicable in a i.i.d.setting, is a scaling factor that determines the sensitivity of thelearning rate and is the heterogeneity index, representing theextent of and 2. This proposition underscores the importanceof considering data heterogeneity in the design of the learning ratestrategy within personalized FL, offering a structured approach toenhance learning outcomes across diverse client datasets.In .4, we empirically confirm that a particular implemen-tation of this strategy facilitates faster convergence. It is importantto note that the data heterogeneity index cannot be determineda priori; therefore, we utilize several proxy measures during thetraining process to estimate it. Our goal is not to prescribe an exactsolution to this strategy, but rather, through analysis and empirical investigation, to enlighten further research and development ofpersonalized FedMeZO for more effective training of LLMs.These discussions and corresponding empirical support addressthe question Q3: Which model parameters are critical for conver-gence, and how can we leverage them to optimize FL performance,such as via personalization?.Besides, recall that we adopt LoRA to mitigate the communi-cation burden associated with LLMs for practical FL scenarios.Nonetheless, the influence of LoRA on the models low effectiverank, remains an open question. We thus advance the following con-jecture under Assumption 1, predicated on existing literature, to facilitate further validations: Conjecture 3.8 (Rank Correlation). The optimal reparametriza-tion rank LoRA used in Low-Rank Adaptation (LoRA) is positivelyproportional to the effective rank of the Hessian matrix H () ofthe tuned LLM. The LoRA is lower-bounded by , and can serve as anempirical proxy for .",
  ".(17)": "By substituting Eq. (3) into Eq. (17), we proceed to use theCauchy-Schwarz inequality to decompose this gradient estimatorinto two parts, each of which is a biased estimator. Recall that in ourgradient estimator, follows a Gaussian distribution. Therefore,the impact on the norm caused by a forward step and a backwardstep of the estimator is identical. Consequently, we ascertain that",
  "E (,) 2+ 3E ()2.(19)": "The first part represents the gradient difference between stages(,) and (, 0), which can be computed using Assumption 3, i.e.,the -smooth condition. The second part signifies the disparitybetween local and global aspects, calculated using Assumption 5.The third part is retained as is.Combining Equations (17), (18) and (19), we bound 2 as follow:",
  ".(21)": "In Eq. (21), E (,) 2remains unknown and we needto constrain it further. The key idea is to transform this expecta-tion term into a form related to E (,)2 and then utilize theconclusion of Eq. (18) and Eq. (19) for computation. The detailedderivation process is provided in the Appendix D.1 and we can havethe bounded result:",
  "Experimental Setup": "We utilize LLaMA-3B as the foundational model and employfour datasets covering a range of tasks and data distribution typesto provide comprehensive validation of our theoretical results .Given that our theory centers on the loss function, we primarilyfocus on analyzing loss descent in our experiments. More details ofthe used datasets are in the Appendix ().We set the total number of communication rounds to 500. Bydefault, BP-based baselines undertake local training for one epoch,whereas FedMeZO conducts local training for 30 steps. We repeatour experiments with three seeds and plot the error bars. For moredetailed implementation specifics, please refer to Appendix E.",
  "Convergence Study": "To assess the convergence of FedMeZO, we perform experiments onthree datasets using different data splitters, as specified in ,with test loss serving as the convergence metric. Our objective is toevaluate the generalization and stability of FedMeZO across diversedatasets and heterogeneity scenarios. For benchmarking purposes,we also measure the performance of BP-based FedAvg on the samedatasets. Additionally, we document the GPU memory usage duringtraining in . Representative findings are illustrated in , while all the comprehensive results are available in AppendixF.11 due to the space limitation.",
  "Dolly-Meta2657110061GSM8K-IID177719733CodeAlpaca-LDA152879569": "Two main conclusions emerge from the convergence experi-ments: First, when the learning rate complies with the require-ments discussed in .3, as stipulated by Theorem 3.1 andTheorem 3.4, FedMeZO consistently diminishes loss with each step, ultimately achieving stable convergence. Second, under equiva-lent learning rate configurations, FedMeZO decreases loss morerapidly than BP-based FedAvg, indicating a swifter convergencerate. For instance, in the Dolly-Meta figure, FedMeZO stabilizesand converges around 300 rounds, whereas BP-based FedAvgs lossis still declining at this juncture. Notably, from , we observethat the GPU memory demand for FedMeZO is roughly one-half ofthat required by BP-based FedAvg, suggesting that FedMeZO canachieve a speedier convergence with fewer resources.",
  "In this subsection, we perform a series of experiments to ascertainthe influence of various hyper-parameters, as intimated by ourtheoretical findings": "5.3.1Impact of Perturbation Scale . To corroborate the theoreticalimpacts of the perturbation step on convergence, we examine values of 5 103 and 2 104, in addition to the default =1 103. We leverage the same datasets and splitters as in .2 for robustness. shows representative outcomes, withcomprehensive results in Appendix F.9.",
  "overall influence is modest. This is evident in , where mod-ifications to within a specific range yield only slight variations.Thus, a smaller proves advantageous for model convergence": "5.3.2Impact of Local Iteration Step . To validate the theoreticalimpact of local iteration steps on convergence, we contrast = 10and = 50 with the standard = 30. Utilizing identical datasetsand splitters from .2, we present typical findings in , with all the detailed results forthcoming in Appendix F.10.These experimental results suggest that a lower engenders amore sluggish convergence pace, whereas a higher somewhatpropels convergence, mirroring the impact of as a denominator inthe theoretical convergence rate analysis. Nonetheless, an excessive may lead to instability, as depicted by the curve of = 50, whichexhibits a surge endwise in the figure. Hence, an appropriate choiceof facilitates efficient model convergence.",
  ": Effects of different local iteration steps . Moreresults are in Appendix F.10": "5.3.3Analysis of Other Hyper-parameters. We also explore theramifications of learning rate, data splitters, the number of clients on the convergence rate, the batch size and the model size. Dueto the space limit, details pertaining to these experimental settingsand results are presented in Appendices F.4, F.5, F.6, F.7 and F.8respectively.In a nutshell, as shown in , a suitable learning rate anchoredat 1/ leads to stabilized training dynamics, while larger ones ex-hibit divergence as suggested by our analysis (.3). Besides,dissimilar splitters symbolize varying extents of data heterogeneity,and we have observations revealing that augmented heterogeneityculminates in lower stabilized loss values (as shown in ). Thisintimates that a moderate degree of data heterogeneity can elevatethe models convergence proficiency. Moreover, and verifies our theoretical analysis in that an increase in helps stabilize the global convergence.",
  "Personalization Study": "To reconcile Proposition 3.7 with practical scenarios, we conductthe following subsequent experiments. To account for each clientsheterogeneity during model updates in each round, we derive threesignal quantities: (1) Round-wise Train Loss Difference: The discrep-ancy between each clients loss in the preceding training round andthe global loss. (2) Five-round Average Train Loss Difference: Theaverage loss deviation for each client relative to the global loss overthe antecedent five rounds. (3) Model Parameter Update Difference:The disparity between each clients previous round parameter up-dates and the global update magnitude. We normalize them to therange of (1, 1), serving as empirical estimates for .For the setting of the scaling factor , following the guidanceof the learning rate in .3, we designate 1.5 105 as themaximal learning rate, potentially leading to surges as per thelearning rate search network. Symmetrically, we posit the minimalvalue at 5 106, anchored on the default learning rate of 1 105,thereby assigning a value of 5 106.As counterpoints, we furnish two configuration strategies forthe learning rate adjustment: one uniformly applies a default learn-ing rate of 1 105, while the other randomly selects within[5 106, 1.5 105] for each round. The conclusive results aredepicted in . Based on the experimental results, we observethat our method achieves faster loss convergence with the secondand third types of signal quantities compared to the default and",
  "Model Update Difference": "DefaultRandomStrategy : Comparison of different strategies of learning rateadjustment. Default indicates non-personalized case, andRound-wise Loss, Five-round Loss and Model UpdateDifference indicate three signal quantities leveraged. random settings, with the third type yielding the most impressiveperformance. In contrast, the first type of signal quantity had anegligible impact. These results suggest that while round-wise lossexhibits some degree of randomness, aggregating losses over multi-ple rounds can approximate heterogeneity to a meaningful extent,thus serving as an indicator to expedite model convergence.It is also noteworthy that the third type of signal quantity alignswith the expression () =0 ( (,))2, which mostclosely reflects Assumption 6. Consequently, it demonstrates themost effective performance in the experiments, not only achiev-ing the fastest convergence but also the lowest stable loss. Thiscase study experiment substantiates the efficacy of Proposition 3.7,offering valuable insights for parameter tuning in personalized FL.",
  "CONCLUSION": "This work investigates the convergence of FedMeZO, a practicalapproach integrating Memory-efficient Zeroth-Order Optimizationwithin a federated learning setting for Large Language Models(LLMs). Extensive empirical results verified our analyses and in-dicated that FedMeZO achieves fast convergence with reducedGPU memory requirements, offering a promising alternative totraditional optimization methods. The incorporation of a person-alized learning rate adjustment, derived from theoretical analysis,has been shown to effectively enhance loss reduction. Throughthis study, we aim to enlighten more research and developmentof memory-efficient optimization techniques to address practicalchallenges associated with the fine-tuning of LLMs, particularly inresource-constrained scenarios . Alekh Agarwal, Martin J Wainwright, Peter Bartlett, and Pradeep Ravikumar.2009. Information-theoretic lower bounds on the oracle complexity of convexoptimization. Advances in Neural Information Processing Systems 22 (2009). Armen Aghajanyan, Sonal Gupta, and Luke Zettlemoyer. 2021. Intrinsic Di-mensionality Explains the Effectiveness of Language Model Fine-Tuning. InProceedings of the 59th Annual Meeting of the Association for Computational Lin-guistics and the 11th International Joint Conference on Natural Language Processing(Volume 1: Long Papers). 73197328. Jiamu Bai, Daoyuan Chen, Bingchen Qian, Liuyi Yao, and Yaliang Li. 2024. Fed-erated Fine-tuning of Large Language Models under Heterogeneous LanguageTasks and Client Resources. arXiv preprint arXiv:2402.11505 (2024).",
  "Daoyuan Chen, Dawei Gao, Weirui Kuang, Yaliang Li, and Bolin Ding. 2022.pFL-Bench: A Comprehensive Benchmark for Personalized Federated Learning.In NeurIPS": "Daoyuan Chen, Dawei Gao, Yuexiang Xie, Xuchen Pan, Zitao Li, Yaliang Li, BolinDing, and Jingren Zhou. 2023. FS-REAL: Towards Real-World Cross-Device Fed-erated Learning. In Proceedings of the 29th ACM SIGKDD Conference on KnowledgeDiscovery and Data Mining. 38293841. Daoyuan Chen, Yilun Huang, Zhijian Ma, Hesen Chen, Xuchen Pan, Ce Ge,Dawei Gao, Yuexiang Xie, Zhaoyang Liu, Jinyang Gao, Yaliang Li, Bolin Ding,and Jingren Zhou. 2024. Data-Juicer: A One-Stop Data Processing System forLarge Language Models. In International Conference on Management of Data. Daoyuan Chen, Liuyi Yao, Dawei Gao, Bolin Ding, and Yaliang Li. 2023. EfficientPersonalized Federated Learning via Sparse Model-Adaptation. In InternationalConference on Machine Learning, ICML, Vol. 202. 52345256. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de OliveiraPinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman,et al. 2021. Evaluating large language models trained on code. arXiv preprintarXiv:2107.03374 (2021). Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun,Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,et al. 2021. Training verifiers to solve math word problems. arXiv preprintarXiv:2110.14168 (2021). Mike Conover, Matt Hayes, Ankit Mathur, Xiangrui Meng, Jianwei Xie, Jun Wan,Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, et al. 2023. Free dolly:Introducing the worlds first truly open instruction-tuned llm. Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah,Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. 2023. Free Dolly:Introducing the Worlds First Truly Open Instruction-Tuned LLM. Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, andJie Tang. 2021. Glm: General language model pretraining with autoregressiveblank infilling. arXiv preprint arXiv:2103.10360 (2021). John C Duchi, Michael I Jordan, Martin J Wainwright, and Andre Wibisono. 2015.Optimal rates for zero-order convex optimization: The power of two functionevaluations. IEEE Transactions on Information Theory 61, 5 (2015), 27882806.",
  "Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. 2020. Personalized feder-ated learning: A meta-learning approach. In NeurIPS 2020": "Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, andQiang Yang. 2023. FATE-LLM: A Industrial Grade Federated Learning Frameworkfor Large Language Models. CoRR abs/2310.10049 (2023). Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, andQiang Yang. 2023. Fate-llm: A industrial grade federated learning framework forlarge language models. arXiv preprint arXiv:2310.10049 (2023). Wenzhi Fang, Ziyi Yu, Yuning Jiang, Yuanming Shi, Colin N Jones, and YongZhou. 2022. Communication-efficient stochastic zeroth-order optimization forfederated learning. IEEE Transactions on Signal Processing 70 (2022), 50585073.",
  "Kevin G Jamieson, Robert Nowak, and Ben Recht. 2012. Query complexity ofderivative-free optimization. Advances in Neural Information Processing Systems25 (2012)": "Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aurlien Bellet, Mehdi Ben-nis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode,Rachel Cummings, Rafael G. L. DOliveira, Hubert Eichner, Salim El Rouayheb,David Evans, Josh Gardner, Zachary Garrett, Adri Gascn, Badih Ghazi, Phillip B.Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo,Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail Kho-dak, Jakub Konecn, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo,Tancrde Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard Nock, Ayferzgr, Rasmus Pagh, Hang Qi, Daniel Ramage, Ramesh Raskar, Mariana Raykova,Dawn Song, Weikang Song, Sebastian U. Stich, Ziteng Sun, Ananda TheerthaSuresh, Florian Tramr, Praneeth Vepakomma, Jianyu Wang, Li Xiong, ZhengXu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao. 2021. Advances and OpenProblems in Federated Learning. Foundations and Trends in Machine Learning14, 12 (2021), 1210. Weirui Kuang, Bingchen Qian, Zitao Li, Daoyuan Chen, Dawei Gao, Xuchen Pan,Yuexiang Xie, Yaliang Li, Bolin Ding, and Jingren Zhou. 2023. Federatedscope-LLM: A comprehensive package for fine-tuning large language models in feder-ated learning. arXiv preprint arXiv:2309.00363 (2023).",
  "Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. 2018. Measuringthe Intrinsic Dimension of Objective Landscapes. In International Conference onLearning Representations": "LeiLai Li, Jianzong Wang, Xiaoyang Qu, and Jing Xiao. 2021. Communication-memory-efficient decentralized learning for audio representation. In 2021 Inter-national Joint Conference on Neural Networks (IJCNN). IEEE, 18. Mu Li, Tong Zhang, Yuqiang Chen, and Alexander J Smola. 2014. Efficient mini-batch training for stochastic optimization. In Proceedings of the 20th ACM SIGKDDinternational conference on Knowledge discovery and data mining. 661670. Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar,and Virginia Smith. 2020. Federated optimization in heterogeneous networks.Proceedings of Machine learning and systems 2 (2020), 429450.",
  "Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. 2019.On the Convergence of FedAvg on Non-IID Data. In ICLR": "Sijia Liu, Pin-Yu Chen, Bhavya Kailkhura, Gaoyuan Zhang, Alfred O Hero III,and Pramod K Varshney. 2020. A primer on zeroth-order optimization in signalprocessing and machine learning: Principals, recent advances, and applications.IEEE Signal Processing Magazine 37, 5 (2020), 4354. Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D Lee, DanqiChen, and Sanjeev Arora. 2023. Fine-Tuning Language Models with Just ForwardPasses. Advances in Neural Information Processing Systems (2023). Othmane Marfoq, Chuan Xu, Giovanni Neglia, and Richard Vidal. 2020.Throughput-Optimal Topology Design for Cross-Silo Federated Learning. InNeurIPS, H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (Eds.),Vol. 33. Curran Associates, Inc., 1947819487.",
  "Yurii Nesterov and Vladimir Spokoiny. 2017. Random gradient-free minimizationof convex functions. Foundations of Computational Mathematics 17 (2017), 527566": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, GregoryChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019.Pytorch: An imperative style, high-performance deep learning library. Advancesin neural information processing systems 32 (2019). Zhen Qin, Daoyuan Chen, Bingchen Qian, Bolin Ding, Yaliang Li, and ShuiguangDeng. 2024. Federated Full-Parameter Tuning of Billion-Sized Language Modelswith Communication Cost under 18 Kilobytes. arXiv:2312.06353 [cs.LG] Xinchi Qiu, Javier Fernandez-Marques, Pedro PB Gusmao, Yan Gao, Titouan Par-collet, and Nicholas Donald Lane. 2022. ZeroFL: Efficient On-Device Training forFederated Learning with Local Sparsity. In International Conference on LearningRepresentations.",
  "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, CarlosGuestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Stanford alpaca: Aninstruction-following llama model": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-AnneLachaux, Timothe Lacroix, Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, et al. 2023. Llama: Open and efficient foundation language models. arXivpreprint arXiv:2302.13971 (2023). Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-mine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-ale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXivpreprint arXiv:2307.09288 (2023). Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H Brendan McMa-han, Maruan Al-Shedivat, Galen Andrew, Salman Avestimehr, Katharine Daly,Deepesh Data, et al. 2021. A field guide to federated optimization. arXiv preprintarXiv:2107.06917 (2021). Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. 2021. Anovel framework for the analysis and design of heterogeneous federated learning.IEEE Transactions on Signal Processing 69 (2021), 52345249. Zhen Wang, Weirui Kuang, Yuexiang Xie, Liuyi Yao, Yaliang Li, Bolin Ding, andJingren Zhou. 2022. FederatedScope-GNN: Towards a Unified, Comprehensiveand Efficient Package for Federated Graph Learning. In Proceedings of the 28thACM SIGKDD Conference on Knowledge Discovery and Data Mining. 41104120. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,Anthony Moi, Pierric Cistac, Tim Rault, Rmi Louf, Morgan Funtowicz, et al.2020. Transformers: State-of-the-art natural language processing. In Proceedingsof the 2020 conference on empirical methods in natural language processing: systemdemonstrations. 3845. Yuexiang Xie, Zhen Wang, Dawei Gao, Daoyuan Chen, Liuyi Yao, Weirui Kuang,Yaliang Li, Bolin Ding, and Jingren Zhou. 2023. FederatedScope: A Flexible Fed-erated Learning Platform for Heterogeneity. Proceedings of the VLDB Endowment16, 5 (2023), 10591072. Zhewei Yao, Amir Gholami, Kurt Keutzer, and Michael W Mahoney. 2020. Pyhes-sian: Neural networks through the lens of the hessian. In 2020 IEEE internationalconference on big data (Big data). IEEE, 581590.",
  "Xinlei Yi, Shengjun Zhang, Tao Yang, and Karl H Johansson. 2022. Zeroth-orderalgorithms for stochastic distributed nonconvex optimization. Automatica 142(2022), 110353": "Jianyi Zhang, Saeed Vahidian, Martin Kuo, Chunyuan Li, Ruiyi Zhang, GuoyinWang, and Yiran Chen. 2023. Towards Building the Federated GPT: FederatedInstruction Tuning. arXiv preprint arXiv:2305.05644 (2023). Jianyi Zhang, Saeed Vahidian, Martin Kuo, Chunyuan Li, Ruiyi Zhang, TongYu, Guoyin Wang, and Yiran Chen. 2023. Towards Building the FederatedGPT:Federated Instruction Tuning. In International Workshop on Federated Learningin the Age of Foundation Models in Conjunction with NeurIPS 2023. Qingsong Zhang, Bin Gu, Zhiyuan Dang, Cheng Deng, and Heng Huang. 2021.Desirable companion for vertical federated learning: New Zeroth-order gradientbased algorithm. In Proceedings of the 30th ACM International Conference onInformation & Knowledge Management. 25982607. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, ShuohuiChen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt:Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068(2022). Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A surveyof large language models. arXiv preprint arXiv:2303.18223 (2023).",
  "Section E details our empirical implementations in terms ofthe datasets, platforms, and hyper-parameters": "Section F presents additional experiment results about theevaluations with common LLMs metrics (Section F.3), theconditions of learning rate (Section F.4), the impact of dataheterogeneity (Section F.5), the impact of client number (Sec-tion F.6), the impact of batch size (Section F.7), the impactof model size (Section F.8), the impact of perturbation scale(Section F.9), the impact of local iterations (Section F.10), andthe full results of convergence curves (Section F.11).",
  "where denotes the number of randomizations": "It is important to note that the last term in Eq. (32) representsthe squared norm of the true gradient, which is inconsistent withthe zeroth-order estimation method used in our approach. There-fore, a transformation is necessary to align it with the FedMeZOalgorithm. Besides, Malladi et al. detail the relationship between thetrue gradient norm and the zeroth-order gradient estimator norm,which we restate as follows:",
  "Fed-Alpaca52.0kGeneric LanguageFed-Dolly15.0kGeneric LanguageFed-GSM8K7.5kCoTFed-CodeAlpaca8.0kCode Generation": "We adopt several federated tuning datasets tailored for LLMsfrom , with different splitting strategies to simulate the het-erogeneity typical of different federated learning (FL) scenarios,including a uniform distribution of data, a Dirichlet distribution ofdata, and a splitter based on meta-information. The proportion oftest data for each dataset is 1%.Fed-Dolly: This federated corpus dataset, derived from Databricks-dolly-15k , comprises eight categories of NLP tasks: brainstorm-ing, classification, closed QA, creative writing, general QA, infor-mation extraction, open QA, and summarization. We divide thetraining set into three subsets using a three-way split and assigneach subset to a distinct client.Fed-GSM8K: Constructed from the GSM8K dataset , thiscollection is aimed at mathematical fine-tuning and consists of7.5K training problems alongside 1K test problems. By default, wepartition the training set uniformly into three subsets and allocateeach to a separate client.Fed-CodeAlpaca: This federated version of CodeAlpaca en-compasses code samples in ten programming languages, includingC, C#, C++, Go, Java, PHP, Pascal, Python, Scala, and X86-64 Assem-bly. Due to the scarcity of X86-64 Assembly samples in the originalcorpus, we exclude them. The remaining samples are then divided into three subsets using a default three-way split, with one subsetassigned to each client.Fed-Alpaca: The Alpaca dataset is designed for LLM fine-tuning and features natural language questions and responses for avariety of NLP tasks, such as text generation, translation, and openQA. It spans various domains like math, text processing, and codegeneration.",
  "E.3Default Implementation Settings": "Following the guidelines in , all approaches perform localtraining with a batch size of 1 to minimize memory usage. In aneffort to standardize the experimental conditions, both backprop-agation (BP)-based methods and our proposed method FedMeZO,train locally with specific learning rates: = 1 105 for the Fed-Dolly and Fed-Alpaca datasets, = 2 105 for the Fed-CodeAlpacadataset, and = 2.5105 for the Fed-GSM8K dataset. The rank andalpha parameters for Low-Rank Adaptation (LoRA) adapters usedby both BP-based optimization and FedMeZO are set to 128 and256, respectively. As per , the perturbation scale for FedMeZOis set to 1 103. Unless otherwise stated, in our training process,we employed the early stopping mechanism to prevent over-fittingand reduce unnecessary training time caused following previousworks . The training was stopped if there was no improvement inthe validation loss for a predefined number of consecutive epochs,known as the patience parameter. We chose a patience of 30 epochsbased on empirical evidence or prior studies. The best model was se-lected from the epoch with the lowest validation loss. Furthermore,apart from individual experiments with time constraints (experi-ments in Appendix F.4 and Appendix F.6), we conducted three setsof experiments with randomly selected seeds for the same set ofparameters, and calculated the mean as the line plot with a 90%confidence interval as the error bar.The influence of different hyper-parameters for FedMeZO hasbeen analyzed in .",
  "=1,(63)": "Note that the left side represents the clients parameters withFedAvg, while the right side corresponds to FedMeZO, and they areequivalent. By utilizing LoRA, FedMeZO achieves the same effectwith just 1.23% of parameters in our setting, totaling 42,598,400.Specifically, each parameter occupies 2 bytes under fp16, full pa-rameter transmission needs 6.39GB, while LoRA demands merely80.45MB.",
  "F.2Computational Cost of FedMeZO": "FedMeZOs single perturbation per iteration for gradient estimationsubstantially lowers computational costs versus BP-based optimiz-ers. We empirically validate it in terms of GPU memory usage andtime efficiency. illustrates GPU memory usage. After de-ducting the base models usage (8697MiB) from the total one in, we can observe that FedMeZO requires only 7.68% 13.36% ofthe usage compared to BP-based FedAvg during training. For timeefficiency, we sample the duration of ten training rounds acrossfour task and present the results in . The results indicate thatFedMeZO requires 91.35% 95.24% of the time taken by BP-basedmethods. Coupled with the faster loss decline shown in ,FedMeZO demonstrates both a quicker training speed and highercomputational efficiency.",
  "F.3Evaluations with Common LLMs Metrics": "For a more comprehensive evaluation, we examine FedMeZO withsome commonly used metrics for LLMs evaluations. We conductevaluations on Dolly with MMLU metrics , Code with OpenAI-HumanEval metrics , and GSM8K with CoT metrics. Weevaluate FedMeZO and BP-based FedAvg at model checkpoints ofrounds 0, 100, 200 and present the results in .Comparing with the results in round 0, we find that FedMeZOgains 35.48%, 1.52%, 2.3% average improvements on GSM8K, Code,and Dolly respectively after fed-tuning. By contrast, BP-based Fe-dAvg gains 29.03%, 0% and 0.38% respectively. The results verifiedFedMeZOs effectiveness again:",
  "F.5The Impact of Heterogeneity onConvergence": "We present the results of processing the same dataset with differentsplitters in . It is observable that in the Dolly and CodeAl-paca datasets, the LDA and Meta splitters perform better than IID,with Meta being the best. Noting that the data classified by Metaand LDA are non-i.i.d., this indicates that higher data heterogeneityis more conducive to model convergence.",
  ": Effects of different splitters on the same dataset": "Initially, there is no significant difference between the two dur-ing the early rounds of training. However, after 200 rounds, thetraining loss with 3 clients has dropped to its lowest and begins tofluctuate, while the training loss with 8 clients continues to steadilyconverge. This demonstrates that the model converges more stablywith more clients participating in the training. This conclusionalso corresponds to the theoretical results regarding the number ofclients discussed in , i.e., an increase in is beneficialfor reducing global convergence.",
  "For a more comprehensive investigation, we further selectedclient numbers of 3, 5, 9, 20, and 40 on the Alpaca dataset to covera broad range of scenarios. From the results in , we gleantwo insights:": "Initial convergence is quicker with fewer clients, while in-creasing clients will stabilize convergence. For instance, withthe same initial loss, by 200 rounds, the loss of 3-clients is1.48 compared to 1.51 for 9-clients. And by 400 rounds, thetraining of 3-clients fails to converge further. Beyond a certain threshold, further increasing client num-bers doesnt significantly speed up convergence. When thenumber of clients exceeds 9, the differences in loss amongdifferent clients are less than 1% throughout 500-round.",
  "F.7The Impact of Batch Size": "We present the results of altering the batch size on the Dolly-Metadataset as an example in . The results show that larger batch-size start with lower loss. For the first 200 epochs, the loss for batch-size=1 is smaller than for the other two. However, larger batch-sizedecline more slowly, and by the end, batch-size=1 has the smallesttest loss. This implies that larger batch-size are unnecessary duringtraining.",
  "F.8The Impact of Model Size": "To investigate the and versatility of FedMeZO across different modelsizes, based on LLaMA2-7B , we conducted experiments us-ing the same experimental setup on three datasets: Dolly-Meta,CodeAlpaca-LDA and GSM8K-IID. The experimental results areshown in . The results show that FedMeZO on LLaMA2-7Bretains similar trends, while starts with lower Loss than 3B-modelby 0.2 and 0.12 in Dolly-Meta and GSM8K-IID. For all tasks, 7B-models loss decreases more slowly, with reductions at 300 roundsbeing only 34%, 67%, and 41% of the 3B-models in Dolly-Meta,Code-LDA, and GSM8K-IID.",
  "F.9The Impact of Perturbation Scale": "We display the comprehensive experimental results of alteringthe perturbation scale across different datasets in . Asmentioned in .3.1, we observe that, except for CodeAlpaca,smaller values of slightly accelerate the convergence speed inthe remaining results, with their corresponding lines all positionedbelow the default setting, whereas larger values of result in slowerconvergence speeds. Through these extensive experiments, wefurther substantiate the theoretical findings.",
  "F.10The Impact of Local Iterations": "We present the complete experimental results of changing the localiteration on different datasets in . As mentioned in Sec-tion 5.3.2, we find that across all datasets and splitters, a larger significantly speeds up convergence, although in certain scenarios,such as Code-IID and Code-Meta, the loss does not reach a lowerstable convergence state, and a smaller consistently results in",
  "F.11Comprehensive Results of ConvergenceStudy": "As mentioned in .2, we have validated the convergenceof BP-based FedAvg and FedMeZO across different datasets andsplitters, with the results displayed in . We observed thatin all scenarios, FedMeZO achieves a faster loss reduction comparedto BP-based FedAvg and attains a lower loss in certain datasets,such as Code-IID and GSM-IID. This indicates that under equivalentconditions, FedMeZO is more adept at learning the characteristicsof different datasets. In the CodeAlpaca dataset, we observe thatthe BP-based FedAvg exhibits fluctuations with large error bars,showing a trend similar to that observed at the end of Code-Metain . We hypothesize that the increased fluctuation is attrib-utable to the code datas inherently disjointed nature in naturallanguage terms, compared to datasets from other domains."
}