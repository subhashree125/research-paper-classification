{
  "ABSTRACT": "Deep Neural Networks have achieved great success in some of the complex tasks that humans cando with ease. These include image recognition/classification, natural language processing, gameplaying etc. However, modern Neural Networks fail or perform poorly when trained on tasks that canbe solved easily using backtracking and traditional algorithms. Therefore, we use the architecture ofthe Neuro Logic Machine (NLM) and extend its functionality to solve a 9X9 game of Sudoku.To expand the application of NLMs, we generate a random grid of cells from a dataset of solvedgames and assign up to 10 new empty cells. The goal of the game is then to find a target valueranging from 1 to 9 and fill in the remaining empty cells while maintaining a valid configuration. Inour study, we showcase an NLM which is capable of obtaining 100% accuracy for solving a Sudokuwith empty cells ranging from 3 to 10. The purpose of this study is to demonstrate that NLMs canalso be used for solving complex problems and games like Sudoku. We also analyse the behaviour ofNLMs with a backtracking algorithm by comparing the convergence time using a graph plot on thesame problem. With this study we show that Neural Logic Machines can be trained on the tasks thattraditional Deep Learning architectures fail using Reinforcement Learning. We also aim to proposethe importance of symbolic learning in explaining the systematicity in the hybrid model of NLMs.",
  "Introduction": "The groundbreaking results of the modern deep learning models have proved that they are the ideal tools to solvecomplex problems, however the lack of systematicity in these models have been a problem for some time. Recentresearch has focussed on this issue by generating hybrid models which combine Neural Networks with SymbolicLearning. In past, researchers have attempted to create hybrid models for tasks such as Language Translation ,Curriculum Learning , Learnable Recursion Logic and the Synthesizing complex logic based on Input-Outputexample pairs , etc. By testing one such model, called the Neural Logic Machine , we emphasise on the relevanceof symbolic learning in solving complex problems on which modern deep learning methods may fail. More specifically,we validate the NLM model on a different mathematical problem to realize their true potential as well as analysetheir performance. Importantly NLMs can utilize the knowledge gained from generated rules to achieve a perfectgeneralization in several tasks. Therefore, we also aimed to test the NLMs ability to recover these lifted rules andapply them in the later stages of curriculum learning - when the complexity of the problem rises. To accomplish this,we gradually changed the number of empty cells in the grid while training. We test the architecture of Neural Logic Machines for solving a complex puzzle called Sudoku using our own setof predicates as input. In this experiment, we closely analyse the performance of this model and compare it with tradi-tional algorithms on the same problem. To perform this experiment, we completed three main tasks. First, we trained",
  "Neuro-Symbolic Sudoku SolverA PREPRINT": "the NLM on sudoku grids with pre-defined empty cells, the number of which increased as training progressed. Thisapproach , where the complexity of the problem increases over training, is known as curriculum learning. Secondly,we used symbolic learning with reinforcement rewards to award the model every time a valid configuration of theempty cells was achieved. Finally, the convergence time of the NLM and Backtracking algorithm was compared usinga graph plot. Towards the end of the experiment, we successful tested the model with random sudoku grids.",
  "Extending the Applications of Neural Logic Machines:": "The NLM is trained and tested on a completely different problem set (e.g., Sudoku puzzles) to expand its scope in wideareas of applications. In , linear space problems (e.g., sorting arrays) are used to test this model, whereas, this paperfocuses on using 2-dimensional problem set on the same model. Instead of function approximation, the reinforcementtraining algorithm REINFORCE is used to estimate the gradients using a policy-gradient mechanism and calculatethe gradients in a non-differentiable trainable setting.",
  "Testing the Robustness of the Neural Logic Machine:": "While the NLM is tested with tasks like list sorting, path finding and BlocksWorld games, we have chosen amore complex problem; solving a 9X9 sudoku puzzle with upto 10 empty cells. To sort an array, we need to compareelements with each other and swap if needed. Whereas, in Sudoku, we need to fill the gaps with appropriate numberschecking rows, columns and sub-matrices. This makes the problem more complex.",
  "Related work": "The rising demand to train Neural Networks for performing complex tasks has generated great attention among theresearchers. However, their lack of systematicity and inability to generalize for a greater set of inputs has lead them toperform poorly on more systematic tasks. To address these challenges, proposed the Neural Logic Machine, whichcan solve problems requiring systems to perform actions by following systematic sets of rules. In , NLMs utilizesthe relationships of objects obtained from quantifiers and logic predicates to solve BlocksWorld games, list sortingand path-finding tasks. The study done in highlights the difference between conventional RNNs (Recurrent NeuralNetworks) with their proposed NLM, addressing the RNNs difficulty on smaller lists, and failure to sort slightly largerlists during testing. The reason behind that is RNNs trained on smaller lists will not be able to systematically generalizeit for larger lists whereas NLM can. An alternate approach to function approximators has been used with NLM called the REINFORCE algorithm ,which is used for policy gradient optimization and estimates the gradient using a Monte-Carlo method. This is com-monly used in deep reinforcement learning where the actions are sampled and the neural network can not performbackpropagation since sampling is non-differentiable operation. In such non-differentiable settings, we instead usegradient estimation techniques. In 2019, Wang et al. proposed a novel deep-learning architecture called the SATNet, which is a differentiable max-imum satisfiability solver that uses CNNs. It is an approximate-differentiable solver which works on a fast coordinatedescent approach for solving semidefinite programs (SDP) associated with the Maximum Satisfiability problem. While the previous studies have focused on using NLMs on different problem sets or solving Sudoku puzzleswith fully Deep Learning approaches , our experiment emphasizes the combination of Symbolic Learning withDeep Learning, as well as a hybrid architecture to solve a new sets of complex problems. Lastly, this experiment alsofocuses on realizing the true potential of NLMs in different areas of applications.",
  ": Model Architecture of Neural Logic Machine": "illustrates our proposed model architecture for the Neuro-Symbolic Sudoku Solver - a Hybrid architectureof Deep Reinforcement Learning techniques and Symbolic Learning. The first half of the model constitutes the learningphase, where the Sigmoid function acts as the activation function between hidden layers, and the SoftMax functionactivates the output layer. The input layer consists of 4 neurons each accepting a certain type of parameter. Theflexibility to allocate a certain type of input to each neuron, leads to greater systematicity in the model. For instance,out of the four neurons in the input layer, the first neuron, i0, accepts only a nullary predicate; i1 accepts a unarypredicate; i2 accepts a binary predicate, and i3 receives a ternary predicate as an input. The problem of solving Sudoku puzzles requires checking for each row, column, and submatrix to maintain a validconfiguration of the Sudoku grid. In order to check this constraint, the coordinates of the rows and columns alongwith the target values are passed to the input layer. Therefore, neurons i2 and i3 receive input as binary and ternarypredicates, while neurons i0 and i1 receive Null inputs. In contrast to this experiment, shows how their problemsrequire predicates for a different set of neurons in the NLM. For example, uses i1 and i2 since unary and binarypredicates are required to solve the array sorting problem. Once the set of predicates is received by the input layer, theinput for the following layers are reduced or expanded based on the arity of the previous layer. The output from the SoftMax layer is fetched by the Reinforcement Learning (RL) module, which constitutes Phase2 of the architecture (see figure 2). The RL module takes care of three main functionalities: Allocating a +1 positivereward for a fully solved grid, a negative reward of -0.01 for every move and performing environmental resets afterchecking all target values. The RL Module will trigger an environmental reset if none of the target values from 1-9can fill an empty cell while maintaining a valid configuration of the Sudoku grid. During this reset, all filled values areemptied, the sudoku grid is reinitialized, and a new iteration begins. Given an unsolved Sudoku grid, we have implemented a multistage architecture to solve the grid step-by-step. illustrates the high-level architecture of the Neuro-Symbolic Sudoku solver, which aligns with problemsexperimented in the original NLM paper. The first step in this implementation diagram consists of calculating the boolean predicates. There are three importantrules to solve a sudoku grid: The solver needs to put a number in an empty cell such that the resulting grid configu-ration remains valid. Here, valid configurations refers to the states where each number in every row, column, and 3x3submatrix is distinct. With the help of these conceptual rules, lifted Boolean predicates may be created. Lifted rules are generalized rules which apply to a task to crack its solvability. These rules are applicable to anyexample in that task domain, irrespective of its configuration or complexity. These rules can be seen as the simplestfundamental rules of solving a system. We define the predicate isRow(r, x), which computes whether number x existsanywhere in row r. Similarly, predicate isColumn(c, x) computes whether number x exists anywhere in column c andpredicate isSubMat(r, c, x) computes whether number x exists in the 3x3 submatrix containing the cell (r, c). Based onabove definition of the predicates, isRow(r, x) and isColumn(c, x) are binary predicates and both result in shapedtensors whereas isSubMat(r, c, x) is a ternary predicate that results in a shaped tensor. It is also worth mentioning that in this study we do not input the unsolved grid into the input layer of the neuralnetwork. Instead, we compute the predicates as described above and pass those predicates as a set of input. Since there",
  ": High Level System Architecture": "are two binary predicates, we concatenate the values of isRow(r, x) and isColumn(c, x) and call the resultant tensor asa stacked binary predicate. Now, these predicates can be feeded into the input layer of the neural network. The last layer of the NLM logic machine computes the SoftMax value and provides the empty cell position, (r, c), aswell as the target value to place in the empty cell. Here comes the role of the reinforcement module. The Reinforcementmodule checks if placing x into (r, c) makes a valid sudoku grid or not. Based on the previous assertion, it generatesthe next state and computes the positive reward (if the next state is not a valid sudoku grid) or negative reward (if thenext state is not a valid sudoku grid). The reinforcement algorithm also checks if the next state generated is a solvedSudoku grid. In this case, we break from the iteration and print the output, otherwise, we repeat the same steps. However, the above strategy may take indefinite number of steps to find a solution. To prevent this, we have definedan upper bound on the number of steps the algorithm may take. The proposed algorithm yields a success rate of 1 if itsolves the grid, and a 0 otherwise.",
  "Result and Analysis": "Our experiment involves multiple settings (number of empty cells, dimensions and optimal steps) of the grid, whichare often modified during the testing phase to understand the performance of the model and obtain a pattern from theresults. To begin the experiment, the number of empty cells and the maximum steps in the sudoku grid are limited to 3and 81 respectively. These are then gradually incremented as the model trains. As these parameters change over time,the complexity to solve the problem also increases. However, our result suggests that NLMs can perfectly confrontthis complexity and yields 100% accuracy in most of the cases. To give a better understanding on how the model performs with different parameters, we have demonstrated the successrate with respect to each parameter that the model was trained on. shows the comparison and performance undereach setting that is tested on our modified version of the NLM. The model when tested with the minimum number ofempty cells (nr empty) 3 and max steps set to 81, gives a success rate of 0.94. However, when the maximum numberof steps (max steps) are increased from 81 to 150, the model receives a perfect score of 1.",
  ": Comparison of different training parameters": "A similar case is also observed when the model receives 5 empty cells with 81 and 729 max steps. With fewer optimalsteps, the model yields a comparatively lower score as compared to 3 empty cells. However, when the model does nothave a restriction on the maximum number of steps (set to a max of 729), it performs well and gets 100% accuracy.From this we conclude that success rate is directly related to the maximum number of steps allowed by the model.Another inference obtained from is that when number of empty cells increases keeping Max. steps constant,success rate drops. This shows the inverse relation between success rate and empty cells. Therefore, the success rateof NLM model is strongly determined by the relation of:",
  ": Comparison of NLM and backtracking convergence time": "In addition to fine-tuning the model, we have also drawn a time complexity analysis of the NLM with traditionalbacktracking algorithms for solving Sudoku puzzles. Both the NLM and backtracking algorithms are provided withthe same set of grids and their time to solve the complete grid is highlighted in . The motivation behind thisis to showcase the difference in the principle working of both the algorithms and analyze their convergence time (withlimited training in the case of NLMs). The backtracking algorithm takes a constant average time of nearly 0.00045 seconds to solve the same set of9X9 grids, on the other hand, the NLM takes a considerably higher amount of time to converge. (It is also worthmentioning that demonstrates the time taken by the NLM with 729 maximum number of steps). The rea-son that backtracking converges faster is due to the fact that it solves the grid in an optimal number of steps (i.e.,MaxStepsBacktracking = NumberOfEmptyCells). In , the peak time in the case of the NLM denotesthe instances in which the environment was reset due the formation of an invalid configuration of the sudoku grid.",
  ": Separate convergence time for Backtracking and NLM for same problem": "During this instance, the model first receives a negative reward through the reinforcement module and then resets theenvironment once there are no possible target values to test. In this case, the NLM again tries to fill the empty cellsbut with a different set of target values from the beginning. However, even with 10 empty cells, our modified versionof the NLM always takes less than 2.0 seconds to converge.",
  "Conclusion and Future Work": "The focus of this study is to tackle one of the drawbacks of the traditional Neural Networks i.e., systematicity. WhereNeural Networks perform poorly, NLMs can solve some the same task with 100% accuracy. NLMs have been trainedand tested by on various tasks which Deep Learning models have failed to solve or converge. In our paper, weadded to the existing applications of their architecture and solved a more complex problem to test the robustness ofNeuro Logic Machines. While the Neuro Logic Machines failed to converge for Sudoku puzzles faster than the backtracking algorithm, it isevident from this study that a Neuro Logic Machine can be trained to solve tasks where conventional Deep Learningmodels may fail. Lastly, because the NLM receives a random combination of grids and number empty cells from, weare confident that the high success rate of NLMs is not due to the models over-fitting. Thus, with this experiment,we have been able to strengthen the argument that NLM can solve tasks with 100% accuracy without relying onover-fitting. In , we also deduce that that the success rate is directly associated with the number of emptycells and the maximum number of steps that model is allowed to take. To conclude, Neuro Logic Machines can solve complex problems using a hybrid approach of Reinforcement andSymbolic Learning. In future work, we intend to cover the fine-tuning and convergence rate of the algorithm. Wepropose that the applications of NLMs can be extended further with even more games (e.g., Ken Ken puzzles) andmathematical problems (such as search tasks). We also anticipate that the architecture will cover problems where NLMhave not yielded a success rate of 100%."
}