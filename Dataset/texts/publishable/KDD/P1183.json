{
  "ABSTRACT": "With the widespread application of personalized online services,click-through rate (CTR) prediction has received more and moreattention and research. The most prominent features of CTR pre-diction are its multi-field categorical data format, and vast anddaily-growing data volume. The large capacity of neural modelshelps digest such massive amounts of data under the supervisedlearning paradigm, yet they fail to utilize the substantial data to itsfull potential, since the 1-bit click signal is not sufficient to guidethe model to learn capable representations of features and instances.The self-supervised learning paradigm provides a more promisingpretrain-finetune solution to better exploit the large amount of userclick logs, and learn more generalized and effective representations.However, self-supervised learning for CTR prediction is still anopen question, since current works on this line are only prelimi-nary and rudimentary. To this end, we propose a Model-agnosticPretraining (MAP) framework that applies feature corruption andrecovery on multi-field categorical data, and more specifically, wederive two practical algorithms: masked feature prediction (MFP)and replaced feature detection (RFD). MFP digs into feature inter-actions within each instance through masking and predicting asmall portion of input features, and introduces noise contrastiveestimation (NCE) to handle large feature spaces. RFD further turnsMFP into a binary classification mode through replacing and de-tecting changes in input features, making it even simpler and moreeffective for CTR pretraining. Our extensive experiments on two",
  "Weinan Zhang and Yanru Qu are co-corresponding authors": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 23, August 610, 2023, Long Beach, CA, USA 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0103-0/23/08...$15.00 real-world large-scale datasets (i.e., Avazu, Criteo) demonstrate theadvantages of these two methods on several strong backbones (e.g.,DCNv2, DeepFM), and achieve new state-of-the-art performancein terms of both effectiveness and efficiency for CTR prediction.",
  "CTR Prediction, Self-supervised Learning, Model Pretraining": "ACM Reference Format:Jianghao Lin, Yanru Qu, Wei Guo, Xinyi Dai, Ruiming Tang, Yong Yu,and Weinan Zhang. 2023. MAP: A Model-agnostic Pretraining Frameworkfor Click-through Rate Prediction. In Proceedings of the 29th ACM SIGKDDConference on Knowledge Discovery and Data Mining (KDD 23), August610, 2023, Long Beach, CA, USA. ACM, New York, NY, USA, 13 pages.",
  "INTRODUCTION": "Click-through rate (CTR) prediction aims to estimate the proba-bility of a users click given a specific context, andplays a fundamental role in various personalized online services,including recommender systems , display advertising , websearch , etc. Traditional CTR models (e.g., logistic regres-sion and FM-based models ) can only capture low-orderfeature interactions, which might lead to relatively inferior perfor-mance in real-world applications. With the rise of deep learningtechniques and the massive amount of user behavior data collectedonline, many delicate neural CTR models have been proposed tomodel higher-order feature interactions with different operators(e.g., product , convolution , andattention ). These works generally follow a supervisedlearning paradigm shown in (a), where a model is randomlyinitialized and trained from scratch based on the supervised signals(click or not). Nevertheless, the 1-bit click signal is not sufficient",
  "Target Task": "Target TaskPretext Task : The illustration of (a) supervised learning para-digm, and (b) self-supervised learning paradigm. The su-pervised learning paradigm directly trains a randomly ini-tialized model from scratch without pretraining. The self-supervised learning paradigm contains two stages, where wefirst pretrain the model based on the pretext task and thenfinetune it for the downstream task. enough for the model to learn capable representations of featuresand instances, resulting in suboptimal performance.Self-supervised learning provides a more powerful training par-adigm to learn more generalized and effective representations ofdata samples, and proves to be effectual in Natural Language Pro-cessing (NLP) and Computer Vision (CV) domains. Asshown in (b), they usually adopt a pretrain-finetune scheme,where we first pretrain an encoder based on pretext tasks (i.e., pre-training tasks), and then finetune a model initialized by the pre-trained encoder for downstream tasks based on specific trainingdata with supervised signals. According to the pretext tasks, self-supervised learning can be mainly classified into two categories: (1)contrastive methods and (2) generative methods . Contrastivemethods aim to learn generalized representationsfrom different views or distortions of the same input. Generativemethods reconstruct the original input sample fromthe corrupted one.Self-supervised learning has flourished in the NLP domain topretrain transformers for unlabeled sentences, making it a perfectmimicry target for sequential recommendations. Various methodsare proposed to treat user behavior sequences as sentences, andadopt language models or sentence augmentations for better user or item embeddings. Moreover, many pretrainingmethods are designed for different types of data formats (e.g., graphdata , or multi-modal data ) to further enrich therecommendation family. However, these sequential/graph/multi-modal based pretraining methods are essentially incompatible forthe CTR data format, i.e., multi-field categorical data format:",
  "(1)": "In this paper, we focus on self-supervised pretraining over multi-field categorical data for CTR prediction. There exist preliminaryworks that explore the pretraining methods for CTR data.MF4UIP leverages the BERT framework to predict themasked features for user intent prediction. However, it is non-scalable when the feature space grows, and thus suffers from severe inefficiency problem for industrial applications with million-levelfeature spaces. SCARF is a contrastive method that adopts Sim-CLR framework and InfoNCE loss to learn robust represen-tations for each data sample. Its contrastive property requires tocalculate representations from different views of the same instance,which doubles the throughput time and memory usage, leading tolow efficiency. Moreover, contrastive based methods only providecoarse-grained instance-level supervisions from sample pairs, andtherefore might get trapped in representation spaces early degen-eration problem , where the model overfits the pretext task tooearly and loses the ability to generalize.To this end, we propose a Model-agnostic Pretraining (MAP)framework that applies feature corruption and recovery towardsmulti-field categorical data for CTR pretraining. Specifically, wederive two algorithms based on different strategies of corruptionand recovery: masked feature prediction (MFP) and replaced featuredetection (RFD). MFP requires the model to recover masked fea-tures according to corrupted samples, and adopts noise contrastiveestimation (NCE) to reduce the computational overhead causedby the large feature space (million level) in CTR data. Moreover,RFD turns MFP into a binary classification mode and requires themodel to detect whether each feature in the corrupted sample isreplaced or not. Compared with MFP that only utilizes a subset offields and predict over the entire feature space, RFD is simpler yetmore effective and more efficient, which provides fine-grained andmore diverse field-wise self-supervised signals. RFD can achievebetter CTR performance with fewer parameters, higher throughputrates, and fewer pretraining epochs compared to other pretrainingmethods. Derived from the MAP framework, MFP and RFD are com-patible with any neural CTR models and can promote performancewithout altering the model structure or inference cost.Main contributions of this paper are concluded as follows: We propose a Model-agnostic Pretraining (MAP) framework thatapplies feature corruption and recovery on multi-field categor-ical data. Different pretraining algorithms could be derived bycustomizing the strategies of corruption and recovery. We derive a masked feature prediction (MFP) pretraining algo-rithm from MAP, where the model predicts the original featuresthat are replaced by <MASK> tokens. We also adopt noise con-trastive estimation (NCE) to reduce the computational overhead. We derive a replaced feature detection (RFD) pretraining algo-rithm from MAP, where the model is required to detect whetherthe feature of each field is replaced or not. RFD is simpler yetmore effective and more efficient, and it can achieve better CTRperformance with fewer computational resources. Extensive experiments on two real-world large-scale datasetsvalidate the advantages of MFP and RFD on several strong back-bones, and achieve new state-of-the-art performance in terms ofboth effectiveness and efficiency for CTR prediction.",
  "MAP: A Model-agnostic Pretraining Framework for Click-through Rate PredictionKDD 23, August 610, 2023, Long Beach, CA, USA": "and {1, 0} is the true label (click or not). For simplicity, webuild a global feature map of size , and assign a unique featureindex for each category, and thus we can represent each sampleas = [,1, . . . ,, ], where , ( = 1, . . . , ) is the index ofcorresponding feature.CTR models aim to estimate the click probability ( = 1|)for each sample. According to , the structure of most recentCTR models can be abstracted as three layers: (1) embedding layer,(2) feature interaction layer, and (3) prediction layer.Embedding layer transforms the sparse binary input intodense low-dimensional embedding vectors E = [1;2; . . . ; ] R , where is the embedding size, and each feature is repre-sented as a fixed-length vector R.Feature interaction layer, as the main functional module ofCTR models, is designed to capture the second- or higher-orderfeature interactions with various operations (e.g., product, atten-tion). This layer produces a compact representation based on thedense embedding vectors E for the sample .Prediction layer estimates the click probability = ( =1|) based on the representation generated by the feature inter-action layer. It is usually a linear layer or an MLP module followedby a sigmoid function:",
  "MAP Framework Overview": "We adopt the common pretrain-finetune scheme in self-supervisedlearning for NLP and CV , where we first pretrain a CTRmodel for a pretext task, and then finetune the pretrained modelwith click signals.We propose a Model-agnostic Pretraining (MAP) framework forthe pretraining stage. The pretext task for the model is to recoverthe original information (e.g., original features, corrupted field in-dex) from the corrupted samples. It is worth noting that MAP iscompatible with any neural CTR models, since we only corrupt theinput sample (i.e., feature corruption layer) and alter the predictionhead (i.e., feature recovery layer) for the recovery target. Finally, bycustomizing the design of feature corruption and recovery layers,we derive two specific pretraining algorithms as follows:",
  "Masked Feature Prediction": "In the masked feature prediction (MFP) pretraining stage, we firstcorrupt the original input sample with a feature masking layer,where we randomly replace a certain proportion of the features with<MASK> tokens. Then, we feed the corrupted sample throughthe embedding layer and feature interaction layer to get the compactrepresentation . Finally, the vector is inputted to a field-wiseprediction layer to predict the original feature for each <MASK>token. To ensure efficiency and practicability, we introduce noisecontrastive estimation (NCE) to allow the model to predict amonga large feature space (e.g., millions of candidate features). 3.2.1Feature Masking Layer. For an input sample with features(i.e., = [,1, . . . ,, ]), we randomly replace a part of the featureswith <MASK> tokens, resulting in a corrupted sample . Theproportion of features to be masked is a hyperparameter denotedas corrupt ratio . We represent the set of indices of masked fieldsas I. The <MASK> token is also regarded as a special feature in theembedding table, and it is shared among all the feature fields. Thatis, we do not maintain field-specific mask tokens, in order to avoidintroducing prior knowledge about the masked fields. A harderpretraining task with less prior knowledge can force the model tolearn more generalized feature representations, which benefits thedownstream task . 3.2.2Field-wise Prediction Layer. After the embedding layer andfeature interaction layer, we obtain the representation for thecorrupted sample . For each masked feature , of the -th field,we maintain an independent multi-layer perceptron (MLP) net-work followed by a softmax function to compute the predictiveprobability , R over the candidate features:",
  "=1 exp(, ,), = 1, . . . , .(5)": "We expand the predictive space (i.e., candidate features) for everymasked field from the field-specific feature space to the globalfeature space, in order to increase the difficulty of pretext task andthus benefit the downstream CTR prediction task . That is,the model has to select the original feature , out of the wholefeature space, which usually contains millions of features in recom-mender systems. Finally, we view MFP pretraining as a multi-classclassification problem and employ the multi-class cross-entropyloss for optimization:",
  "Result": "(a) MAP Framework : The illustration of (a) MAP framework, (b) masked feature prediction (MFP), (c) replaced feature detection (RFD), and(d) finetune. MFP and RFD are derived from the MAP framework by customizing the design of feature corruption and recoverylayers. In the finetuning stage, we maintain the same model structure, and load the parameters from the pretrained model toinitialize the embedding layer and feature interaction layer. 3.2.3Noise Contrastive Estimation. The MFP method introducedabove is still impractical and extremely expensive, since in Eq. 5we have to calculate the softmax function over the large globalfeature space. Such a million-level multi-class classification problemleads to a tremendous amount of memory usage and unacceptablepretraining time cost for real-world applications. To this end, weadopt noise contrastive estimation (NCE) to reduce thesoftmax overhead.NCE converts the multi-class classification problem into a binaryclassification task, where the model tries to distinguish the positivefeature (i.e., the masked feature , ) from noise features. Specifi-cally, for the -th masked field, we sample noise features from candidate features according to their frequency distribution inthe training set. Then, we employ the binary cross-entropy loss:",
  ", (7)": "where , is the output of -th MLP predictor, is the feature indexof the positive feature, and is the sigmoid function. In this way,we reduce the complexity of loss calculation from () to (),where , and is the number of masked fields. In ourexperiment, = 25 is enough to achieve a good CTR performancefor the large global feature space (e.g., = 4 millions).",
  "Replaced Feature Detection": "As shown in (c), we further propose the replaced featuredetection (RFD) algorithm to provide fine-grained and more diversepretraining signals from all the feature fields, instead of a subset offields in MFP (i.e., the masked fields).In the RFD pretraining stage, we first corrupt the original inputsample by a feature replacement layer, where we randomly re-place a certain proportion of the features with other features. Then,after obtaining the compact representation from the embedding",
  "and feature interaction layers, we employ a field-wise predictionlayer to detect whether the feature in each field is replaced or not": "3.3.1Feature Replacement Layer. For an input sample with fea-tures (i.e., = [,1, . . . ,, ]), we randomly replace a part of thefeatures, and denote the set of indices of replaced fields as I. Theproportion of features to be replaced is a hyperparameter repre-sented as corrupt ratio . Next, we replace each of these selectedfeatures by a random sampling from the empirical marginal dis-tribution (i.e., sample from the field-specific feature space by thefeature frequency distribution) of the corresponding field F in thetraining set, resulting in the corrupted sample . 3.3.2Field-wise Prediction Layer. Similar to MFP, we obtain therepresentation for the corrupted sample through the em-bedding layer and feature interaction layers. Then, we feed toan MLP predictor followed by an element-wise sigmoid function,resulting in an -length predictive vector :",
  "=1BinaryCrossEntropy(, ,, ),(9)": "where , {1, 0} ( = 1, . . . , ) is the label indicating whetherthe feature in the -th field of sample is replaced or not.Apparently, RFD serves as a simpler pretraining algorithm com-pared with MFP which requires NCE to reduce the computationaloverhead. While MFP only utilizes the masked fields (the corruptratio is usually 10% - 30%) as self-supervised signals, RFD involvesall the feature fields to provide more sufficient and more diversesignal guidance. We will evaluate the effectiveness and efficiencyof RFD later in .2 and .3, respectively.",
  "Complexity Analysis": "We analyze the time complexity of our proposed MFP and RFD,as well as two baseline pretraining algorithms (i.e., MF4UIP andSCARF). We only analyze the component above the feature inter-action layer (i.e., prediction layer and loss calculation) due to theirmodel-agnostic property.Suppose the batch size is , and we adopt a linear layer : R R to transform the compact representation (or ) to the finalpredictive place. The time complexity of MF4UIP is ( + ),where the main overhead of MF4UIP is the softmax computationover the million-level feature space of size . By adopting NCE, thetime complexity of MFP reduces to (+), where ,and is the number of masked features. By introducing the binaryclassification mode, the time complexity of RFD further reduces to( + ), where is the number of feature fields. Besides, as acontrastive algorithm with InfoNCE loss, SCARF has a quadraticcomplexity over the batch size: (2 + 42).In summary, MF4UIP and SCARF are non-scalable in terms offeature space and batch size , respectively. MFP and RFD canachieve lower complexity and is scalable for industrial applicationswith million-level feature space and large batch size.",
  "Experiment Setup": "4.1.1Datasets. We conduct extensive experiments on two large-scale CTR prediction benchmarks, i.e., Avazu and Criteo datasets.Both datasets are divided into training, validation, and test setswith proportion 8:1:1. The basic statistics of these two datasets aresummarized in . Note that the training set for the pretrainingstage and finetuning stage are the same, in order to make full useof the large-scale datasets. We describe the preprocessing for thetwo datasets as follows: Avazu originally contains 23 fields with categorical features.We remove the id field that has a unique value for each datasample, and transform the timestamp field into four new fields:weekday, day_of_month, hour_of_day, and is_weekend, resultingin 25 fields. We remove the features that appear less than 2 timesand replace them with a dummy feature <Unknown>. Criteo includes 26 anonymous categorical fields and 13 numeri-cal fields. We discretize numerical features and transform theminto categorical features by log transformation1. We remove thefeatures that appear less than 10 times and replace them with adummy feature <Unknown>.",
  "Avazu32,343,1724,042,8974,042,898254,428,327Criteo36,672,4934,584,0624,584,062391,086,794": "4.1.2Evaluation Metrics. To evaluate the performance of CTRprediction methods, we adopt AUC (Area under the ROC curve)and Log Loss (binary cross-entropy loss) as the evaluation metrics.Slightly higher AUC or lower Log Loss (e.g., 0.001) can be regardedas significant improvement in CTR prediction 4.1.3Base Models & Baselines. We evaluate the self-supervisedpretraining methods on various base CTR models with three differ-ent feature interaction operators: (1) product operator, includingDeepFM , xDeepFM , and DCNv2 ; (2) convolutionaloperator, including FiGNN and FGCNN; (3) attention oper-ator, including AutoInt and Transformer . Additionally, weadopt the classical DNN model, which proves to be a strong basemodel for self-supervised learning in our experiments. We compareour proposed MFP and RFD with two existing pretraining methods:MF4UIP and SCARF , which are chosen as representativegenerative and contrastive algorithms, respectively.",
  "Effectiveness Comparison (RQ1 & RQ2)": "We apply five training schemes on each base model, and report theresults in . In the Scratch scheme, we train the randomlyinitialized base model from scratch (i.e., supervised learning). Inother schemes, we first pretrain the base model according to thecorresponding method, and then finetune the pretrained model forCTR prediction (i.e., self-supervised learning). From , we canobtain the following observations: All the pretrain-finetune schemes can improve the performanceof base model on both metrics by a large margin compared withthe Scratch scheme, which demonstrates the effectiveness ofself-supervised learning for CTR prediction.",
  "The product based CTR models (e.g., DCNv2), together with theDNN model, win the top places among the base models, whenequipped with self-supervised learning": "Among the pretrain-finetune schemes, MFP and RFD generallygain significant improvement over the two baseline pretrainingmethods except for few cases, and RFD can consistently achievethe best performance.In addition to the performance comparison above, we also findsome interesting phenomena as follows: The pretrain-finetune scheme can greatly reduce the demand onmodel structure design for CTR prediction. To our surprise, al-though DNN model is inferior under the Scratch scheme, itgains huge improvement under the pretrain-finetune schemes(especially RFD) and wins the second place, outperforming a",
  "KDD 23, August 610, 2023, Long Beach, CA, USAJianghao Lin and Yanru Qu, et al": "designed graph data , or multi-modal data . Theyexplore the pretraining methods for better representations to fur-ther enhance the recommendation performance with enriched sideinformation. However, these sequential/graph/multi-modal meth-ods are essentially incompatible with the CTR data, i.e., multi-fieldcategorical data format.In CTR prediction, there exist works that incorporatethe self-supervised signals in a semi-supervised manner, where thecross-entropy loss is jointly optimized with an auxiliary loss inone stage. As for CTR pretraining methods, VIME proposes asemi-supervised learning algorithm to learn a predictive functionbased on the frozen pretrained encoder. MF4UIP leveragesthe BERT framework for user intent prediction. SCARF adopts SimCLR framework and InfoNCE loss to pretrainthe model in a contrastive manner. Compared with these works, ourproposed MFP and RFD methods are more scalable for industrialapplications and achieve the state-of-art performance in terms ofboth effectiveness and efficiency for CTR prediction.",
  "MF4UIPSCARFMFPRFD": ": The model size (Top) and run time per epoch (Bottom) of different pretraining methods. We perform logarithmic scaleon the axis, and denote the original value on the top of each bar. The experiment is conducted on the same server with oneGeForce RTX 3090 GPU. We only consider the learning parameters above the embedding layer for model size. We consider thewhole pretraining loop for run time per epoch, including the corruption operations as well as the backpropagation. pretrain-finetune schemes. We give one conjecture about this topichere, and leave further studies as future works. Our hypothesis isthat the pretrain-finetune scheme might prefer the bit-wise fea-ture interaction (e.g., DCNv2) to the field-wise feature interaction(e.g., Transformer). The bit-wise feature interaction enables largermodel capacity to learn better feature crossing patterns during thepretraining stage.",
  "RQ3.1 What is the complexity of each pretraining method?RQ3.2 How many pretraining epochs should a method takes toachieve a certain performance (i.e., sample efficiency)?": "For RQ3.1, we have already provided the complexity analysisin .4. Following , we further empirically compare themodel size and run time per epoch of different pretraining methodsfor different base CTR models in . The experiments are con-ducted on the same server with one GeForce RTX 3090 GPU. For faircomparison, we launch one pretraining at a time as a single processto exclusively possess all the computational resources. We maintainthe same structure of each base model for the four pretraining algo-rithms, and set the corrupt ratio = 0.3. Since different pretrainingalgorithms require different amounts of dynamic GPU memory,we choose a proper batch size from {256, 512, 1024, 2048, 4096} tomake full use of GPU memory. From , we can obtain thefollowing observations: SCARF is relatively time-consuming, though it has minimal pa-rameters. The reason is that SCARF, as a contrastive method,requires computing representations of different views from thesame instance, which doubles the run time of training loops. Although MFP maintains similar amount of learning parame-ters compared with MF4UIP, it approximately exhibits an 18speedup over MF4UIP in terms of run time per epoch since weadopt NCE to resolve the softmax overhead for million-levelpredictive spaces.",
  "lowest run time per epoch, showing its simplicity and practica-bility for industrial applications": "Next, to study RQ3.2, we investigate the sample efficiency andgive the AUC performance of base models under different pretrain-ing epochs in . We choose DCNv2, DNN, and DeepFM as therepresentative base models due to the page limitation. MF4UIP is ex-cluded due to its tremendous cost of time per epoch. Note that zeropretraining epoch indicates that we train the model from scratch",
  "NegativeLog Loss": ": The hyperparameter study on corrupt ratio . We give the AUC and negative log loss performance of DNN, DCNv2and DeepFM with MFP and RFD methods on Avazu (left two columns) and Criteo (right two columns) datasets. without pretraining. In , we can observe that MFP andRFD consistently achieve better performance over SCARF underdifferent pretraining epochs. Moreover, as illustrated by the blackdashed lines, RFD can simply achieve the best performance withlimited pretraining epochs (1030), showing its superior sampleefficiency for CTR prediction.In summary, we validate the pretraining efficiency of our pro-posed methods (especially RFD), i.e., they can achieve better CTRperformance with fewer learning parameters, higher throughputrates, and fewer pretraining epochs.",
  "Ablation & Hyperparameter Study (RQ4)": "In this section, we analyze the impact of hyperparameters or com-ponents in MFP and RFD, including the corrupt ratio for bothMFP and RFD, the number of noise features in NCE for MFP, andthe feature replacement strategy for RFD. Similarly, we select DNN,DCNv2 and DeepFM as the representative base models due to thepage limitation. 4.4.1Corrupt Ratio . We select the value of corrupt ratio from{0.1, 0.2, 0.3, 0.4, 0.5}, and show the impact in . Both MFPand RFD favor a small corrupt ratio (i.e., 0.10.3). The reason is thatthe over-corruption caused by a large corrupt ratio may change thesample semantics and disturb the model pretraining. 4.4.2The Number of Noise Samples . We select the number ofnoise samples in NCE for MFP from {10, 25, 50, 75, 100}, and showthe impact in . Surprisingly, the performance fluctuations ofboth metrics (i.e., AUC and log loss) brought by different s are allwithin 0.0003, indicating that MFP is not sensitive to the number",
  "Log Loss0.43920.43920.43900.43930.4392": "of noise samples in NCE. A small number of noise features (10noise features out of the million-level feature space) is sufficient forthe model to learn effective feature crossing patterns and benefitthe final CTR performance. 4.4.3Feature Replacement Strategy. We investigate the impact ofthe feature replacement strategy in RFD, which needs to sample areplacer for the original feature. We compare four different replace-ment strategy variants shown in , and give the results in. We observe that sampling by the feature frequency distribu-tion is relatively better than uniform sampling. Moreover, samplingfrom the global feature space can greatly hurt the CTR performance,",
  "RELATED WORK5.1Click-through Rate Prediction": "The click-through rate (CTR) prediction serves as a core functionmodule in various personalized online services, including onlineadvertising, recommender systems, and web search, etc .With the rise of deep learning, many deep neural CTR models havebeen recently proposed. The core idea of them is to capture thefeature interactions, which indicates the combination relationshipsof multiple features. The deep CTR models usually leverage bothimplicit and explicit feature interactions. While the implicit featureinteractions are captured by a deep neural network (DNN), theexplicit feature interactions are modeled by a specially designedlearning function. According to the explicit feature interactionoperators, these deep CTR models can be mainly classified intothree categories: (1) product operator, (2) convolutional operator,and (3) attention operator.Product Operator. The product-based CTR models originatefrom classical shallow models such as FM and POLY2 . FFM and NFM are variants of FM, where the second-order feature interactions are captured by the inner product offeature embeddings. DeepFM and PNN combine the FMlayer with DNNs for higher-order feature interactions. PIN further extends PNN by introducing a network-in-network struc-ture to replace the inner product interaction function. FiBiNET introduces the SENET mechanism to learn the weights of fea-tures dynamically before product interactions. Moreover, DCN ,xDeepFM , DCNv2 are proposed for the explicit high-orderfeature interaction modeling by applying product-based featureinteractions at each layer explicitly. Therefore, the order of featureinteractions to be modeled increases at each layer and is determinedby the layer depth.Convolutional Operator. Apart from the product operator,Convolutional Neural Networks (CNN) and Graph ConvolutionalNetworks (GCN) are also explored for feature interaction modelingin CTR prediction. CCPM is the first work adopts the CNNmodule for CTR prediction. However, CCPM can only learn part offeature interactions between adjacent features since it is sensitiveto the field order. FGCNN improves CCPM by introducing arecombination layer to model non-adjacent features. FiGNN treats the multi-field categorical data as a fully connected graph,where each field serves as a graph node and feature interactionsare captured via graph propagation.Attention Operator. AFM improves FM by leveraging anadditional attention network to allow feature interactions to con-tribute differently to the final CTR prediction. AutoInt utilizes amulti-head self-attentive neural network with residual connectionsto explicitly model the feature interactions with different orders.InterHAt combines a transformer network with multiple at-tentional aggregation layers for feature interaction learning. Theseattention-based CTR models can also provide explainable predictionvia attention weights.",
  "Self-supervised Learning": "Self-supervised learning has achieved great success in Nature Lan-guage Processing (NLP) and Computer Vision (CV) . Theyusually adopt a pretrain-finetune scheme, where we first pretrainan encoder based on pretext tasks, and then finetune a model ini-tialized by the pretrained encoder for downstream tasks. Accordingto the pretext task, self-supervised learning can be mainly classi-fied into two categories: (1) contrastive methods and (2) generativemethods. Contrastive methods learn a latent space to draw positivesamples together (e.g., different views of the same image) and pushapart negative samples (e.g., images from different categories) .Numerous techniques are proposed to promote the performanceof contrastive methods such as data augmentation , con-trastive losses , momentum encoders , and mem-ory banks . Generative methods reconstructthe original input sample from the corrupted one. For example,BERT requires the model to recover the masked token fromthe corrupted sentences. Besides, ELECTRA adopts an adversar-ial structure and pretrains the model as a discriminator to predictcorrupted tokens, which is proven to be more sample-efficient.In recommender systems, many works apply self-supervisedlearning to user behavior sequences , manually",
  "CONCLUSION": "In this paper, we propose a Model-agnostic Pretraining (MAP)framework that applies feature corruption and recovery on multi-field categorical data for CTR prediction. Based on different strate-gies of corruption and recovery, we derive two practical algorithms:masked feature prediction (MFP), and replaced feature detection(RFD). Extensive experiments show that MFP and RFD achievenew state-of-the-art performance in terms of both effectivenessand efficiency for CTR prediction. For future work, a promisingdirection is to explore what model structures are more suitable forself-supervised paradigm, since we find different models receivequite different performance gains combined with self-supervisedlearning in .2. Furthermore, we will investigate on the pos-sible saturation of downstream CTR performance as the pretrainingvolume grows (e.g., from million to billion or even more). The SJTU team is supported by Shanghai Municipal Science andTechnology Major Project (2021SHZDZX0102) and National NaturalScience Foundation of China (62177033). The work is also sponsoredby Huawei Innovation Research Program. We thank MindSpore for the partial support of this work.",
  ". MindSpore. Dara Bahri, Heinrich Jiang, Yi Tay, and Donald Metzler. 2021.Scarf: Self-supervised contrastive learning using random feature corruption. arXiv preprintarXiv:2106.15147 (2021)": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shot learners. Advances in neuralinformation processing systems 33 (2020), 18771901. Yin-Wen Chang, Cho-Jui Hsieh, Kai-Wei Chang, Michael Ringgaard, and Chih-JenLin. 2010. Training and testing low-degree polynomial data mappings via linearSVM. Journal of Machine Learning Research 11, 4 (2010). Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. Asimple framework for contrastive learning of visual representations. In Interna-tional conference on machine learning. PMLR, 15971607. Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey EHinton. 2020. Big self-supervised models are strong semi-supervised learners.Advances in neural information processing systems 33 (2020), 2224322255.",
  "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:Pre-training of deep bidirectional transformers for language understanding. arXivpreprint arXiv:1810.04805 (2018)": "Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and AndrewZisserman. 2021. With a little help from my friends: Nearest-neighbor contrastivelearning of visual representations. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision. 95889597. Lingyue Fu, Jianghao Lin, Weiwen Liu, Ruiming Tang, Weinan Zhang, Rui Zhang,and Yong Yu. 2023. An F-shape Click Model for Information Retrieval on Multi-block Mobile Pages. In Proceedings of the Sixteenth ACM International Conferenceon Web Search and Data Mining. 10571065. Jean-Bastien Grill, Florian Strub, Florent Altch, Corentin Tallec, PierreRichemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, ZhaohanGuo, Mohammad Gheshlaghi Azar, et al. 2020. Bootstrap your own latent-a newapproach to self-supervised learning. Advances in neural information processingsystems 33 (2020), 2127121284.",
  "Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.Deepfm: a factorization-machine based neural network for ctr prediction. InIJCAI": "Wei Guo, Can Zhang, Zhicheng He, Jiarui Qin, Huifeng Guo, Bo Chen, RuimingTang, Xiuqiang He, and Rui Zhang. 2022. Miss: Multi-interest self-supervisedlearning framework for click-through rate prediction. In 2022 IEEE 38th Interna-tional Conference on Data Engineering (ICDE). IEEE, 727740. Michael Gutmann and Aapo Hyvrinen. 2010. Noise-contrastive estimation: Anew estimation principle for unnormalized statistical models. In Proceedings ofthe thirteenth international conference on artificial intelligence and statistics. JMLRWorkshop and Conference Proceedings, 297304. Bowen Hao, Jing Zhang, Hongzhi Yin, Cuiping Li, and Hong Chen. 2021. Pre-training graph neural networks for cold-start users and items representation. InProceedings of the 14th ACM International Conference on Web Search and DataMining. 265273.",
  "Xiaoshuai Hao, Yi Zhu, Srikar Appalaraju, Aston Zhang, Wanqian Zhang, BoLi, and Mu Li. 2022. MixGen: A New Multi-Modal Data Augmentation. arXivpreprint arXiv:2206.08358 (2022)": "Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollr, and Ross Girshick.2022. Masked autoencoders are scalable vision learners. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition. 1600016009. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2020. Mo-mentum contrast for unsupervised visual representation learning. In Proceedingsof the IEEE/CVF conference on computer vision and pattern recognition. 97299738.",
  "Jie Hu, Li Shen, and Gang Sun. 2018. Squeeze-and-excitation networks. In Proceed-ings of the IEEE conference on computer vision and pattern recognition. 71327141": "Tongwen Huang, Zhiqi Zhang, and Junlin Zhang. 2019. FiBiNET: combining fea-ture importance and bilinear feature interaction for click-through rate prediction.In Proceedings of the 13th ACM Conference on Recommender Systems. 169177. Yanhua Huang, Hangyu Wang, Yiyun Miao, Ruiwen Xu, Lei Zhang, and WeinanZhang. 2022. Neural Statistics for Click-Through Rate Prediction. In Proceedingsof the 45th International ACM SIGIR Conference on Research and Development inInformation Retrieval. 18491853.",
  "Yuchin Juan, Yong Zhuang, Wei-Sheng Chin, and Chih-Jen Lin. 2016. Field-awarefactorization machines for CTR prediction. In RecSys. 4350": "Zeyu Li, Wei Cheng, Yang Chen, Haifeng Chen, and Wei Wang. 2020. Interpretableclick-through rate prediction through hierarchical attention. In Proceedings ofthe 13th International Conference on Web Search and Data Mining. 313321. Zekun Li, Zeyu Cui, Shu Wu, Xiaoyu Zhang, and Liang Wang. 2019. Fi-gnn:Modeling feature interactions via graph neural networks for ctr prediction. In Pro-ceedings of the 28th ACM International Conference on Information and KnowledgeManagement. 539548.",
  "Bin Liu, Ruiming Tang, Yingzhi Chen, Jinkai Yu, Huifeng Guo, and Yuzhou Zhang.2019. Feature generation by convolutional neural network for click-through rateprediction. In WWW. 11191129": "Qiang Liu, Feng Yu, Shu Wu, and Liang Wang. 2015. A Convolutional ClickPrediction Model. In Proceedings of the 24th ACM International on Conference onInformation and Knowledge Management. ACM, 17431746. Xiao Liu, Fanjin Zhang, Zhenyu Hou, Li Mian, Zhaoyu Wang, Jing Zhang, and JieTang. 2021. Self-supervised learning: Generative or contrastive. IEEE Transactionson Knowledge and Data Engineering (2021). Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, OmerLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: Arobustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692(2019). Yong Liu, Susen Yang, Chenyi Lei, Guoxin Wang, Haihong Tang, Juyong Zhang,Aixin Sun, and Chunyan Miao. 2021. Pre-training graph transformer with mul-timodal side information for recommendation. In Proceedings of the 29th ACMInternational Conference on Multimedia. 28532861. Zhiwei Liu, Yongjun Chen, Jia Li, Philip S Yu, Julian McAuley, and CaimingXiong. 2021. Contrastive self-supervised sequential recommendation with robustaugmentation. arXiv preprint arXiv:2108.06479 (2021). Zhuang Liu, Yunpu Ma, Matthias Schubert, Yuanxin Ouyang, and Zhang Xiong.2022. Multi-Modal Contrastive Pre-training for Recommendation. In Proceedingsof the 2022 International Conference on Multimedia Retrieval. 99108.",
  "Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, and Jun Wang.2016. Product-based neural networks for user response prediction. In ICDM": "Yanru Qu, Bohui Fang, Weinan Zhang, Ruiming Tang, Minzhe Niu, HuifengGuo, Yong Yu, and Xiuqiang He. 2018. Product-based neural networks for userresponse prediction over multi-field categorical data. TOIS 37, 1 (2018), 135. Steffen Rendle. 2010. Factorization machines. In ICDM. Steffen Rendle. 2012. Factorization machines with libfm. TIST (2012). Matthew Richardson, Ewa Dominowska, and Robert Ragno. 2007. Predictingclicks: estimating the click-through rate for new ads. In WWW. ACM, 521530. Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang,and Jian Tang. 2019. Autoint: Automatic feature interaction learning via self-attentive neural networks. In Proceedings of the 28th ACM International Conferenceon Information and Knowledge Management. 11611170. Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang.2019. BERT4Rec: Sequential recommendation with bidirectional encoder rep-resentations from transformer. In Proceedings of the 28th ACM internationalconference on information and knowledge management. 14411450.",
  "Yonglong Tian, Dilip Krishnan, and Phillip Isola. 2020. Contrastive multiviewcoding. In European conference on computer vision. Springer, 776794": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin. 2017. Attention is allyou need. In Advances in neural information processing systems. 59986008. Chen Wang, Yueqing Liang, Zhiwei Liu, Tao Zhang, and S Yu Philip. 2021. Pre-training Graph Neural Network for Cross Domain Recommendation. In 2021IEEE Third International Conference on Cognitive Machine Intelligence (CogMI).IEEE, 140145.",
  "Fangye Wang, Yingxu Wang, Dongsheng Li, Hansu Gu, Tun Lu, Peng Zhang, andNing Gu. 2022. CL4CTR: A Contrastive Learning Framework for CTR Prediction.arXiv preprint arXiv:2212.00522 (2022)": "Fangye Wang, Yingxu Wang, Dongsheng Li, Hansu Gu, Tun Lu, Peng Zhang,and Ning Gu. 2022. Enhancing CTR Prediction with Context-Aware FeatureRepresentation Learning. arXiv preprint arXiv:2204.08758 (2022). Peng Wang, Jiang Xu, Chunyi Liu, Hao Feng, Zang Li, and Jieping Ye. 2020.Masked-field Pre-training for User Intent Prediction. In Proceedings of the 29thACM International Conference on Information & Knowledge Management. 27892796.",
  "Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross networkfor ad click predictions. In Proceedings of the ADKDD17. 17": "Ruoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong,and Ed Chi. 2021. Dcn v2: Improved deep & cross network and practical lessonsfor web-scale learning to rank systems. In Proceedings of the Web Conference 2021.17851797. Yu Wang, Hengrui Zhang, Zhiwei Liu, Liangwei Yang, and Philip S Yu. 2022.ContrastVAE: Contrastive Variational AutoEncoder for Sequential Recommenda-tion. In Proceedings of the 31st ACM International Conference on Information &Knowledge Management. 20562066. Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. 2018. Unsupervisedfeature learning via non-parametric instance discrimination. In Proceedings ofthe IEEE conference on computer vision and pattern recognition. 37333742. Yunjia Xi, Jianghao Lin, Weiwen Liu, Xinyi Dai, Weinan Zhang, Rui Zhang,Ruiming Tang, and Yong Yu. 2023. A Birds-eye View of Reranking: from ListLevel to Page Level. In Proceedings of the Sixteenth ACM International Conferenceon Web Search and Data Mining. 10751083. Yunjia Xi, Weiwen Liu, Jianghao Lin, Jieming Zhu, Bo Chen, Ruiming Tang,Weinan Zhang, Rui Zhang, and Yong Yu. 2023. Towards Open-World Recom-mendation with Knowledge Augmentation from Large Language Models. arXivpreprint arXiv:2306.10933 (2023).",
  "Xu Xie, Fei Sun, Zhaoyang Liu, Jinyang Gao, Bolin Ding, and Bin Cui. 2020.Contrastive pre-training for sequential recommendation.arXiv preprintarXiv:2010.14395 (2020)": "Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing,Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. 2020. On layernormalization in the transformer architecture. In International Conference onMachine Learning. PMLR, 1052410533. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov,and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining for languageunderstanding. Advances in neural information processing systems 32 (2019). Tiansheng Yao, Xinyang Yi, Derek Zhiyuan Cheng, Felix Yu, Ting Chen, AdityaMenon, Lichan Hong, Ed H Chi, Steve Tjoa, Jieqi Kang, et al. 2021. Self-supervisedlearning for large-scale item recommendations. In Proceedings of the 30th ACMInternational Conference on Information & Knowledge Management. 43214330. Jinsung Yoon, Yao Zhang, James Jordon, and Mihaela van der Schaar. 2020. Vime:Extending the success of self-and semi-supervised learning to tabular domain.Advances in Neural Information Processing Systems 33 (2020), 1103311043.",
  "AIMPLEMENTATION DETAILS": "In this section, we describe the implementation details for our em-pirical experiments. We conduct both supervised learning (i.e., theScratch scheme) and self-supervised learning (i.e., four pretrain-finetune schemes) over different base models (i.e., CTR model).We first introduce the model configuration for each base model,and then give the training settings for supervised learning andself-supervised learning, respectively. Finally, we describe how toemploy the pretraining methods for the assembled models (e.g.,DCNv2 and DeepFM).",
  "A.1Configuration for Base Models": "We choose the embedding size from {16, 32, 64}. The dropout rateis selected from {0.0, 0.1, 0.2}. We utilize one linear layer after thefeature interaction layer to make the final CTR prediction. Un-less stated otherwise, we adopt ReLU as the activation function.The model-specific hyperparameter settings for base models are asfollows:",
  "A.2Settings for Supervised Learning": "We train each base model from scratch based on click signals with-out pretraining. We adopt the Adam optimizer with weight decayrate selected from {0.01, 0.05}. The batch size is 4096, and the learn-ing rate is chosen from {103, 7 104, 5 104} without decay.We adopt early stop if the AUC performance on the validation setstops increasing for two consecutive epochs. Finally, we choose themodel at the iteration with the highest validation AUC performancefor evaluation in the test set.",
  "For self-supervised learning paradigm, we implement four differentpretraining methods for CTR prediction. We give the settings forthe pretraining and finetuning stages as follows": "A.3.1Pretraining Stage. We adopt the Adam optimizer with theweight decay rate of 0.05. The learning rate is initialized at 103, andis scheduled by cosine decay. There is no warm-up epochs duringthe pretraining. The corrupt ratio is selected from {0.1, 0.2, 0.3}. Weadopt a two-layer MLP with 32 hidden units for the (field-wise)output layer. The method-specific settings are as follows:",
  "RFD. We set the batch size to 4096, and pretrain each base modelfor 60 epochs": "A.3.2Finetuning Stage. The batch size is set to 4096. The initiallearning rate is selected from {103, 7104, 5104}, and is sched-uled by cosine decay. The total finetuning epoch is chosen from{1, 2, 3, 4}. We adopt the Adam optimizer and choose the weightdecay rate from {0.01, 0.05}. We choose the model at the iterationwith the highest validation AUC performance for evaluation in thetest set.",
  "( = 1|) = () + MLP(),(10)": "where () is the explicit feature interaction model. For examples,DCNv2 assembles DNN and CrossNet. DeepFM assembles DNNand FM. We abstract them as assembled models that add up outputsfrom multiple ( 2) modules for final CTR prediction.Suppose we have modules to be assembled. Each of them pro-duces one vector , ( = 1, . . . , ) for the input sample beforethe last prediction layer. If the module is a shallow network that out-puts a scalar (e.g., LR, FM), we simply denote it as a vector that hasonly one dimension. Then, we get the compact representation from the assembled model by concatenating the output vectors:",
  "BADDITIONAL EXPERIMENTSB.1Finetuning Strategy": "Since the embedding layer and feature interaction (FI) layer willbe further updated during the finetuning stage. We provide anadditional ablation study to investigate the influence of differentfinetuning strategies (i.e., freezing different parts of CTR modelsduring finetuning). We choose DCNv2, DNN, and DeepFM as therepresentative models, and study the effect for both MFP and RFDtasks. The results are reported in .From , we observe that either freezing the embeddinglayer or the feature interaction layer will badly hurt the final perfor-mance for all base models and pretraining methods. This indicatesthat there exists gap between the pretraining objective and CTR pre-diction task. The pretrained parameters provide a useful warm-upinitialization, but still require further updating for the downstreamCTR prediction task.",
  "Log Loss0.43920.43800.4374": "and apply grid search to select from {0.1, 0.3, 0.5, 0.7, 0.9}. DCNv2,DNN, DeepFM are chosen as the representative base models. Theresults are reported in .Our observation and discussion towards the results in are in three folds: The joint method could achieve slightly better (or comparable)performance compared to the best pretraining method RFD.",
  "It is worth noting that the joint pretraining method consistentlyreaches the best performance with = 0.1. This indicates thatRFD surpasses MFP and mainly contributes to the performanceimprovement": "In addition, since the corruption and recovery strategies for MFPand RFD are different, the joint training method requires twoforward/backward propagations for each data instance, whichgreatly increases the training cost (e.g., GPU memory usage andrun time per epoch).Although the joint training method might achieve better perfor-mance with finer-grained hyperparameter search, we think RFD isstill a more elegant and practical pretraining method in terms ofboth effectiveness and efficiency."
}