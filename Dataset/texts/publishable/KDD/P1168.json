{
  "Environment and Information Technology": "Abstract. Cyclostationarity involves periodic statistical variations in signals and processes, commonly used in signal analysis and network se- curity. In the context of attacks, cyclostationarity helps detect malicious behaviors within network traffic, such as traffic patterns in Distributed Denial of Service (DDoS) attacks or hidden communication channels in malware. This approach enhances security by identifying abnormal pat- terns and informing Network Intrusion Detection Systems (NIDSs) to recognize potential attacks, enhancing protection against both known and novel threats. This research focuses on identifying cyclostationary malware behavior and its detection. The main goal is to pinpoint es- sential cyclostationary features used in NIDSs. These features are ex- tracted using algorithms such as Boruta and Principal Component Anal- ysis (PCA), and then categorized to find the most significant cyclosta- tionary patterns. The aim of this article is to reveal periodically changing malware behaviors through cyclostationarity. The study highlights the importance of spotting cyclostationary malware in NIDSs by using es- tablished datasets like KDD99, NSL-KDD, and the UGRansome dataset. The UGRansome dataset is designed for anomaly detection research and includes both normal and abnormal network threat categories of zero- day attacks. A comparison is made using the Random Forest (RF) and Support Vector Machine (SVM) algorithms, while also evaluating the effectiveness of Boruta and PCA. The findings show that PCA is more promising than using Boruta alone for extracting cyclostationary net- work feature patterns. Additionally, the analysis identifies the internet protocol as the most noticeable cyclostationary feature pattern used by malware. Notably, the UGRansome dataset outperforms the KDD99 and NSL-KDD, achieving 99% accuracy in signature malware detection using the RF algorithm and 98% with the SVM. The research suggests that the UGRansome dataset is a valuable choice for studying anomaly and cyclostationary malware detection efficiently. Lastly, the study recom- mends using the RF algorithm for effectively categorizing and detecting cyclostationary malware behaviors.",
  "Introduction": "The domain of Network Intrusion Detection Systems (NIDS) seeks innovative strategies to detect anomalous traffic patterns that surpass conventional malware detection methods . Recognizing cyclostationary traffic patterns holds sig- nificant potential to enhance NIDS efficiency and facilitate the implementation of pioneering frameworks . Most current NIDS solutions overlook the use of cyclostationary techniques for pattern detection, which could differenti- ate static from dynamic patterns . Detecting cyclostationary traffic patterns aids in discerning if an intrusion manifests as a long-term evolving malware, undergoing periodic changes . This study aims to evaluate the practices of long-term evolving malware from a cyclostationary perspective. The term cyclo- stationarity is employed to characterize the traffic patterns of zero-day threats , which can vary based on network attributes. This research analyzes the cyclostationarity of both known and unknown zero-day threats , driven by the dearth of cyclostationary datasets for Network Intrusion Detection Problems (NIDP) to comprehend the cyclostationarity of long-term evolving malware like zero-day threats. In contrast to previous methodologies, this article avoids un- necessary abstraction of cyclostationarity. It addresses the omission of evaluating cyclostationary traffic patterns for zero-day threat detection, a gap prevalent in previous NIDS research. Previous studies on cyclostationarity of zero-day threat behaviors primarily adopt anomaly detection techniques, which are the most comprehensible. The concept of cyclostationarity finds application in various scientific and engineering fields . For instance, Mechanical En- gineering employs periodicity and cyclostationarity to analyze the behavior of rotating and reciprocating components. Meteorology studies cyclostationarity in relation to weather prediction owing to Earths revolution and rotation influ- encing seasonal variations. In Network Communication, disciplines like radar, telemetry, and sonar capitalize on periodicity and cyclostationarity, essential for signal scanning, sampling, multiplexing, and modulation . The articles experiments provide diverse insights for the NIDP, as limited efforts concern cy- clostationarity within the Network Intrusion Detection Landscape (NIDL). The primary contribution lies in utilizing Boruta, PCA, and Supervised Learning algorithms to detect cyclostationarity in zero-day threats. The articles struc- ture encompasses theoretical explication of cyclostationarity and related works in , followed by the proposed methodology and datasets for detecting cyclostationary zero-day threats in . Experimental results concerning cyclostationary feature pattern recognition are presented in , while Sec- tion 5 furnishes the researchs conclusions.",
  "Description of a cyclostationary malware: How it gets differ over a traditional malware": "A cyclostationary malware represents a type of network threat characterized by its irregular attributes that exhibit cyclic variations over time . In the con- text of cyclostationary network traffic, the traffic can be divided into discrete segments such as T1, T2, ..., Tn, with the malwares often concealed within these anomalous traffic segments. Detecting these malwares necessitates the identi- fication of the abnormal sample, which can then be further broken down into contiguous or separate segments. These segments are subsequently leveraged for classification and in-depth analysis of the detected cyclostationary malwares. Within each segment, individual data points signify distinct cyclostationary fea- ture patterns of specific zero-day threats. To elaborate further, the term tradi- tional malwares refers to network attacks that have become obsolete and are no longer commonly used. In contrast, cyclostationary malwares employ updated protocols that are integrated into the Transmission Control Protocol (TCP) suite. The distinguishing factor lies in their protocolswhile cyclostationary malwares are aligned with modern TCP/IP suite protocols, traditional mal- wares rely on outdated protocols that have fallen out of usage. This transition underscores the evolving nature of cyber threats, with cyclostationary malwares exploiting contemporary protocols for their malicious activities.",
  "Related works": "Network traffic, whether in the long-term or short-term, often exhibits periodic behaviors. The periodic nature of malware presents an effective feature for Net- work Intrusion Detection Systems (NIDSs) design and performance assessment. Employing anomaly detection enables the establishment of thresholds for iden- tifying anomalies or recognizing cyclostationary threats based on network flow volume. However, these threshold values are subject to variation over time. Re- cent research by Yinka et al. employed threshold values to distinguish cyclo- stationary network traffic from stationary patterns. Their classification process demonstrated enhanced evaluation metrics, but the evolving thresholds remain a challenge. In another study , ensemble learning evaluated cyclostationar- ity using heterogeneous datasets. The stacking technique incorporated a feature selection method to efficiently detect relevant features and accurately identify cyclostationary traffic in the network. Vivekanandam et al. proposed an adaptable Machine Learning approach involving a genetic algorithm for feature selection. This method, combined with other algorithms, demonstrated improved performance in detecting diverse malware categories. Similarly, Mugunthan et al. introduced a cloud-based architecture using a Markov Model and the Random Forest algorithm to detect malwares in network flows, especially low-level Dis- tributed Denial of Service (DDoS) attacks. The rise of long-term evolution mal- ware, including ransomware, emphasizes cyclostationarity as a primary source",
  "Mike Wa Nkongolo": "thus conferring a structured taxonomy upon the delineated features. Our pro- posed methodology is scaffolded upon the employment of two pivotal algorithms, Boruta and Principal Component Analysis (PCA), which serve as the bedrock for feature extraction. While PCA conventionally operates as a dimensionality reduction tool, its role as a feature extraction conduit is justified by its inherent capability to both unveil the pertinence of data components and unveil their variance. The crucible of this methodology resides in the strategic amalgama- tion of these feature extractor components, poised to elicit intrinsic patterns from the datasets of interest. Once the vital features are distilled from the afore- mentioned datasets, the mantle of classification is assumed by the proficiency of the Random Forest and Support Vector Machine algorithms, entrusted with the task of meticulous stratification. These algorithms traverse the multidimen- sional feature space, forging connections and discerning relationships, ultimately ascribing individual instances to their rightful cyclostationary niches. The zenith of this methodology culminates in the meticulous evaluation of the adopted algo- rithms, specifically the Random Forest and Support Vector Machine. This phase affords insight into the algorithms efficacy and performance within the context of cyclostationary malware classification. The outcomes borne of this evaluation, replete with nuances and insights, are subsequently laid bare for comprehensive scrutiny and discourse. stands as a visual embodiment of the intri- cate choreography of steps requisite for the harmonious implementation of the proposed methodology. The journey commences with the meticulous collection of pertinent data, the lifeblood of this endeavor, and traverses through the an- alytical labyrinth, culminating in the pivotal Supervised Learning evaluation phase expounded upon in the forthcoming sections. In alignment with experi- mental rigor, the dataset was subject to rigorous cross-validation, an empirical stratagem where the dataset is partitioned into training and testing sets - an 80% allocation for training and the remaining 20% for comprehensive testing. This rigorous validation methodology serves as a robust safeguard against over- fitting and ensures the integrity of results obtained through this comprehensive process.",
  "The application of cyclostationary analysis in studying network traffic pat-": "terns remains an underutilized approach within the Network Intrusion Detec- tion Problem (NIDP) domain. To address this untapped potential, we introduce a supervised Machine Learning model , previously employed across diverse NIDPs (depicted in ). This Supervised Learning framework forms the ba- sis for cyclostationary malware detection, with a focus on legacy datasets like the Knowledge Discovery and Data Mining (KDD99) and Network Security Labora- toryKnowledge Discovery and Data Mining (NSL-KDD), as illustrated in Fig- ure 1. Our objective is to uncover cyclostationary patterns within these datasets,",
  "Title Suppressed Due to Excessive Length 5": "paralleled by the application of the cyclostationary dataset, UGRansome , to achieve the same goal. The Supervised Learning framework employs two key algorithms, the Support Vector Machine (SVM) and Random Forest (RF), se- lected for comparative analysis. Through evaluation metrics such as Confusion Matrix, Recall, F1-Score, Precision, and Accuracy, we measure the performance of these algorithms. Notably, certain methodologies to assess the cyclostationar- ity of malware evolution may require specialized skills. Unlike conventional tech- niques focused on detecting normal attacks, recognizing the unique attributes of long-term evolving malware like zero-day threats demands periodicity detection. This intricate process relies heavily on time and often necessitates rare or tran- sient process analysis. In this context, the supervised approach to cyclostation- ary malware detection presents a valuable tool for designing and implementing Network Intrusion Detection Systems (NIDSs) geared toward the detection of zero-day threats. By harnessing supervised learning techniques, we facilitate the incorporation of cyclostationarity as a discriminative factor in NIDSs, thereby enhancing their sensitivity and effectiveness in tackling the evolving landscape of network security threats.",
  "The KDD99 dataset": "The KDD Cup 99 dataset, as depicted in , was initially established as a benchmark data source for Network Intrusion Detection Systems (NIDSs), evaluated at MITs Lincoln Lab and sponsored by DARPA between 1998 and 1999. This dataset encompasses five predictive categories, including Remote to Local (R2L), Probe, User to Root (U2R), and Denial of Service (DoS), serving to categorize diverse network threats . Moreover, even the normal behaviors of different malwares are included in this dataset. It comprises 41 attributes classified into Traffic, Content, and Basic categories. The majority of network threats fall within the DoS and Normal classes, with a proportion of 98.6% . As highlighted in , the imbalanced nature of this dataset becomes ap- parent. This imbalance signifies a scenario where one class is more prevalent than another. Consequently, the data distribution tilts in favor of a specific cat- egory, potentially biasing the Machine Learning classification outcomes towards that favored class . The training set of the KDD99 dataset contains 4,898,431 rows, corresponding to 2,984,154 observations. The duplicate features are present in both the testing and training subsets . However, it is important to note that the KDD99 dataset, being outdated, might not be ideally suited for cyclo- stationarity analysis, as indicated in . To provide further insight, offers a comprehensive overview of the malware instances identified in the KDD99 and NSL-KDD datasets. This examination of the KDD Cup 99 dataset underscores the complexities and considerations tied to real-world network in- trusion detection scenarios. As technology evolves, datasets designed for earlier purposes might not seamlessly align with contemporary analysis requirements, highlighting the need for updated and contextually relevant datasets in the study of network security .",
  "The NSL- KDD dataset": "The KDD Cup 99 dataset, as illustrated in , emerged as a seminal benchmark for evaluating Network Intrusion Detection Systems (NIDSs) dur- ing its period of assessment at MITs Lincoln Lab, sponsored by DARPA from 1998 to 1999. This dataset encompasses five predictive categories - Remote to Local (R2L), Probe, User to Root (U2R), and Denial of Service (DoS) - serv- ing as classification criteria for diverse network threats . Interestingly, it also includes normal behaviors of various malwares, presenting a comprehensive per- spective. The inherent imbalance in these classes, as evidenced in , is a critical observation. This imbalance signifies a scenario where one class is dis- proportionately prevalent compared to another, potentially introducing bias in Machine Learning classification outcomes towards the overrepresented class . However, it is essential to consider that while the KDD99 dataset served as a foundational resource, its obsolescence may impact its suitability for contem- porary analysis, particularly in the context of cyclostationarity, as indicated by . This limitation arises from the datasets age and the evolving nature of network threats and behaviors. In light of this, the applicability of the KDD99 dataset to the study of cyclostationarity in malwares is constrained by its design for a different era of network security challenges. As technological landscapes transform, it becomes crucial to align datasets with the specific requirements of modern intrusion detection methodologies, including the emerging focus on cyclostationary analysis in detecting network threats. This underscores the dy- namic nature of network security research and the continual need for relevant and up-to-date data sources to effectively address contemporary cybersecurity concerns.",
  "Methodology": "The methodological approach to classifying cyclostationary malware through Supervised Learning is orchestrated in two overarching phases, each encapsulat- ing distinct objectives within the framework delineated in . The initial phase encompasses the extraction of pivotal features intrinsic to the cyclostation- ary context, whereas the ensuing phase involves their systematic classification,",
  "The cyclostationary dataset": "The experimental landscape was enriched by the incorporation of the UGRan- some dataset, a potent asset in our pursuit (). This dataset emerges as a synthesis of the UGR16 and ransomware datasets, a union yielding 207,534 distinct cyclostationary features and 14 attributes within 14 tuples . These meticulously triangulated features have been harnessed through a Data Fusion technique, conferring upon them the strategic potency to serve as adept in- struments for both anomaly detection and the identification of cyclostationary zero-day threats. A pivotal attribute of the UGRansome dataset is its nuanced stratification of various long-term malware instances into a tripartite predictive class configuration - Signature (S), Synthetic Signature (SS), and Anomaly (A) - outlined succinctly in . Furthermore, this dataset boasts a granular classification of 16 distinct ransomware families, encompassing entities such as",
  "Title Suppressed Due to Excessive Length 9": "advanced persistent threats (APT), Locky, DMALocker, SamSam, and more, all meticulously clustered and correlated for efficient computational maneuver- ing . The UGRansome dataset, a product of meticulous construction in the year 2021 , stands as a publicly accessible resource. Its core distinguishing facet rests in its inherently cyclostationary and periodic nature, epitomizing a rich, real-world portrayal of network traffic dynamics. This character becomes vividly apparent in , where anomalous malware-induced network traffic exhibits a variance linked to distinct network flags. An elegant equilibrium char- acterizes the distribution of novel malware instances, adeptly balanced across more than 100,000 Internet Protocol addresses, their assignments distributed coherently across class A, B, C, and D classifications. The versatility of the UGRansome dataset transcends its immediate realm of cyclostationary zero-day threat identification. Its expansive potential extends to facilitating the discern- ment of other malware archetypes, including SSH, Bonet, DoS, Port Scanning, NerisBonet, and Scan, widening the horizons of its applicability . To provide comprehensive insight, elucidates the intricate data structure intrinsic to the UGRansome dataset, an instrumental aid in comprehending its manifold dimensions.",
  ". The cyclostationarity of anomalous network traffic patterns": "and classification challenges . Rooted in the foundations of Decision Trees, Boruta operates via a multi-faceted approach, independently cultivating vari- ous Decision Trees across diverse samples extracted from the training corpus. This algorithm adopts a Wrapper technique, augmenting the original dataset by introducing shadow features, entities imbued with random values meticulously permuted among training observations . The crux of Borutas prowess is unveiled through its evaluation of feature relevance. This assessment unfolds through the lens of stratification accuracy, a pivotal metric in the context of supervised classification tasks . The orchestration of Borutas relevance computation unfolds in two successive steps. Initially, the stratification accuracy loss is evaluated individually within the canopy of Decision Trees, facilitating a nuanced evaluation where each Tree may assign disparate classifications to a given feature. Subsequently, the amalgamation of average and Standard Devi- ation computations bestows the essence of the stratification accuracy loss with comprehensible quantitative measures. The Z-score assumes center stage as the linchpin in Borutas calculus of feature importance . Computed by standardizing the average loss by the Standard Deviation, the Z-score serves as a conduit for evaluating and juxtaposing feature relevance. This measured rele- vance cascades into a tiered categorization of features, bifurcating their utility into three discernible strata:",
  "In pursuit of this end, the maximization of the Z-score stands as a criti-": "cal juncture. The Maximum Z-Score (MZS) observed across shadow features becomes the benchmark, against which the Z-scores of primary features are jux- taposed . Features outshining the MZS are embraced in the realm of confirmed relevance, while those trailing behind are relegated to the rejected cat- egory. This methodological orchestration ensures that Borutas scrutiny bestows upon the feature selection process an optimal mix of precision and comprehen- siveness.",
  "Feature extraction with PCA": "Principal Component Analysis (PCA) is a mathematical technique used for re- ducing the complexity of data while preserving its essential patterns. It trans- forms a set of correlated variables into a new set of uncorrelated variables called principal components. These components capture the most significant variations in the data, allowing for efficient visualization, dimensionality reduction, and fea- ture extraction . PCA is commonly applied in various fields, including data analysis, image processing, and machine learning, to reveal underlying structures and simplify data representation.In the context of Principal Component Analy- sis (PCA), the recorded features within the ith category are encapsulated by the notation yk,l. These features are organized into jth instances and represented as elements of an n p matrix denoted as Y . Prior to any analysis, it is imperative to bring the dataset into a standardized form, ensuring each column adheres to a distribution characterized by a zero mean and Standard Deviation. It is within this context that the pivotal PCA normalization process takes place, en-",
  "By undertaking this normalization, each feature is rendered dimensionless,": "removing any inherent biases stemming from variations in scales or units. Con- sequently, PCA can robustly capture the underlying patterns and variations in the dataset, leading to the extraction of principal components that succinctly encapsulate its most salient characteristics. These normalized features become the foundation for the generation of principal components, facilitating effec- tive dimensionality reduction and aiding in tasks such as feature selection and anomaly detection. The transformative power of normalization within the PCA framework extends beyond mathematical manipulation; it is a methodical way to prepare data for a journey of insightful discovery within a lower-dimensional space.",
  "Random Forest algorithm": "This algorithm constructs individualistic Decision Trees from the training sam- ple. Predictions are pooled from all Trees to make the final result of classification. In short, the Random Forest algorithm utilises a set of results to make a final prediction/ classification, and they are commonly named Ensemble Learning ap- proaches . The relevance of features is computed by using the decrease in the impurity of weighted nodes. The probability is computed by using the frequency of features in the node, subdivided by the sum of all samples . The greatest value represents the most important feature in the dataset. The total of features relevance value is computed and subdivided by the number of Trees:",
  "The Confusion Matrix is a n n matrix used in the computation of TP, TN,": "FN, and FP to calculate the evaluation metrics such as Accuracy, Precision, and Recall. In this research, the results of the classification using the aforementioned evaluation metrics will be tabulated. Lastly, the computational framework is tested with cross-validation. The computing environment representing the hard- ware and software specification framework is illustrated in . The random state of 42 is used for cross-validation.",
  "Results": "The experiment involved training the three datasets using carefully selected Su- pervised Learning algorithms within the Rstudio computing environment. The caret library was harnessed to utilize Machine Learning packages on a 64-bit Windows 10 Operating System. The Boruta algorithm was employed on the KDD99 and NSL-KDD datasets, as detailed in . It is worth mentioning that Boruta took relatively more computational time for the NSL-KDD dataset and required more iterations, leading to the rejection of a greater number of features compared to the KDD99 and UGRansome datasets. The outcomes of the PCA algorithm, applied to the UGRansome dataset, were visually illus- trated in , showcasing the recognition of network protocol (TCP) as the predominant cyclostationary feature pattern, with an occurrence of 92,157 instances. Further insights were drawn from Figure ??, which highlights the exemplary performance of the Random Forest and Support Vector Machine al- gorithms, achieving an impressive 99% Accuracy on the UGRansome dataset",
  "Title Suppressed Due to Excessive Length 15": "(). The culmination of the experiment was encapsulated by the Confu- sion Matrix presented in , which assessed the Random Forest algorithms performance. The UGRansome dataset exhibited remarkable results compared to the KDD99 and NSL-KDD datasets in effectively categorizing cyclostationary feature patterns into three distinct attack categories: Signature (S), Synthetic Signature (SS), and Anomaly (A) (). Specifically, the detection of Signa- ture malware was particularly prominent, occurring 17,891 times. In summary, this computational exploration underscores the viability of PCA for extracting and classifying cyclostationary network feature patterns. The preeminent cy- clostationary feature pattern pertains to the network protocol. Moreover, the UGRansome dataset exhibited superior performance in detecting signature mal- ware when compared to the KDD99 and NSL-KDD datasets.",
  "Conclusion": "In cybersecurity, the pressing challenge of identifying elusive zero-day attacks characterized by cyclostationary behaviors necessitates the deployment of so- phisticated methods. In response, this research endeavor delved into the intri- cate landscape of cyclostationarity using a diverse triad of datasets. The core objective was to decipher the cyclostationary nature of long-term evolution mal- ware, and to this end, a feature extraction paradigm, bolstered by the syner- gistic prowess of Boruta and Random Forest algorithms, was devised. The focal datasets, namely KDD99, NSL-KDD, and UGRansome, were meticulously scru- tinized to unearth latent cyclostationary patterns. The acquired insights were not confined to extraction alone; they extended to the realm of classification. Lever- aging the robust capabilities of the Random Forest and Support Vector Machine algorithms, the cyclostationary features were seamlessly classified. This metic- ulous classification yielded remarkable outcomesspecifically, an outstanding 98% Accuracy on the NSL-KDD dataset and an impressive 99% on both the KDD99 and UGRansome datasets. These findings stand as a testament to the potency of intelligent algorithms in accurately detecting cyclostationary patterns in evolving malware scenarios. Yet, as the landscape of cyber threats continues to evolve, so must our methodologies. While this study has significantly illuminated the path toward understanding cyclostationarity in malware behavior, it also un- derscores the need for more comprehensive approaches. The current experiment predominantly thrives within the realms of Supervised Learning, prompting an imperative exploration into the realm of Deep Learning for a more nuanced and agile analysis of cyclostationarity in long-term evolution malware. As the horizon of research expands, one intriguing avenue remains unexplored: the evaluation of feature extraction efficacy via the prism of a Genetic Algorithm applied to the UGRansome dataset. Such an exploration could potentially unravel latent insights, further refining our arsenal against the persistent threat of cyclosta- tionary malware. In the continued pursuit of securing digital landscapes, the research community must forge ahead with a multidisciplinary approach, har- nessing the power of intelligent algorithms and cutting-edge methodologies to fortify our defenses against the ever-evolving cyber frontier."
}