{
  "Abstract": "To counter the side effect brought by the proliferation of socialmedia platforms, hate speech detection (HSD) plays a vital role inhalting the dissemination of toxic online posts at an early stage.However, given the ubiquitous topical communities on social media,a trained HSD classifier can easily become biased towards specifictargeted groups (e.g., female and black people), where a high rate ofeither false positive or false negative results can significantly impairpublic trust in the fairness of content moderation mechanisms, andeventually harm the diversity of online society. Although existingfairness-aware HSD methods can smooth out some discrepanciesacross targeted groups, they are mostly specific to a narrow se-lection of targets that are assumed to be known and fixed. Thisinevitably prevents those methods from generalizing to real-worlduse cases where new targeted groups constantly emerge (e.g., newforums created on Reddit) over time. To tackle the defects of exist-ing HSD practices, we propose Generalizable target-aware Fairness(GetFair), a new method for fairly classifying each post that con-tains diverse and even unseen targets during inference. To removethe HSD classifiers spurious dependence on target-related features,GetFair trains a series of filter functions in an adversarial pipeline,so as to deceive the discriminator that recovers the targeted groupfrom filtered post embeddings. To maintain scalability and gener-alizability, we innovatively parameterize all filter functions via ahypernetwork. Taking a targets pretrained word embedding asinput, the hypernetwork generates the weights used by each target-specific filter on-the-fly without storing dedicated filter parameters.In addition, a novel semantic gap alignment scheme is imposed onthe generation process, such that the produced filter function foran unseen target is rectified by its semantic affinity with existingtargets used for training. Finally, experiments1 are conducted on",
  "Corresponding author.1The implementation of GetFair is released at": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08",
  "Hate Speech Detection; Target-aware Fairness; Debiased ContentModeration; Data Science for Social Good": "ACM Reference Format:Tong Chen, Danny Wang, Xurong Liang, Marten Risius, Gianluca De-martini, and Hongzhi Yin. 2024. Hate Speech Detection with Generaliz-able Target-aware Fairness. In Proceedings of the 30th ACM SIGKDD Con-ference on Knowledge Discovery and Data Mining (KDD 24), August 2529, 2024, Barcelona, Spain. ACM, New York, NY, USA, 11 pages.",
  "Introduction": "Many benefits of social medias liberation of communication comeat the expense of proliferating hate speech. To prevent the negativesocio-economical impact from hateful content, accurate algorithmsfor hate speech detection (HSD) have been heavily investigated byboth industry practitioners and research communities .Meanwhile, on the flip side of the coin, it has been reported that various HSD algorithms have exposed vulnerability to differenttypes of biases, including identity, annotation, and political biases.In the context of HSD, these biases are predominately associatedwith the sources (i.e., authors) of online posts, and a variety of corre-sponding solutions have also been made available. However, therealso exists bias towards the targets, or more precisely the posts tar-geted groups , which are usually an identity group (e.g., AfricanAmericans), or a particular protected user attribute (e.g., ones reli-gion). In the rest of this paper, when there is no ambiguity, we willuse target to refer to targeted groups in a post. The targets of a postcan be identified based on the main topics discussed, the context ofconversation, or the channels hosting this post (e.g., the incel forumon Reddit). Due to the inherent disparity in label distributions andlanguage styles among different targets, models trained on suchskewed data can reflect highly unstable HSD performance acrosstargets. As a result, it is commonly seen that HSD classifiers exhibitabnormally high false positive or negative rates on some targetedgroups. A high false positive rate on a specific target means that, theHSD classifier is prone to misclassifying a neutral post as hateful",
  "KDD 24, August 2529, 2024, Barcelona, SpainTong Chen et al": "all layers. Considering the vast pool of possible targets in onlineplatforms, it is unrealistic to rely on a single MLPs capacity forremoving all targets relevant information from s. Thus, a moreperformant approach is to instantiate one filter function () pertarget . However, this will inevitably lead to difficulties in scalingto numerous targets in reality and generalizing to unseen targets(e.g., an emerging buzzword) during inference.To ensure the practicality of GetFair as a plug-in debiasingmethod for HSD, we propose an efficient alternative for buildingtarget-aware filter functions. Instead of letting every filter functionhave a dedicated set of trainable parameters, we leverage a hyper-network () that can adaptively parameterize each target-specific MLP filter. Concretely, for every target , its dedicated filterparameters are generated via:",
  "Preliminaries": "In this section, we mathematically define the concept of target-aware fairness in HSD, metrics for quantifying such fairness, andour research objective w.r.t. generalizable target-aware HSD.Hate Speech Detection Tasks. Hate speech detection is com-monly defined as a classification task. In our paper, the default tasksetting is to take a social media post consisting of a sequenceof tokens as the input, and output a predicted scalar (0, 1) torepresent the likelihood of being a hate speech. In the most recentliterature , pretrained transformer models like BERT and GPT are commonly the default encoder () for generat-ing the post-level embeddings s = () given their superiority inrepresentation learning, which is also the initial feature we feedinto the HSD classifier. Note that our paper is scoped around thistext-based, binary classification setting, which is the most widelyused one in HSD. Meanwhile, with the availability of morenuanced annotated data, our findings can be easily generalized tosome emerging settings like multi-class classification with differentlevels of hatefulness or HSD with additional multimodal data(e.g., images ) given their text-focused origin.Definition 1: Targets in A Post. In our setting, each social mediapost has one or more targeted user groups being discussed. In ,those mentioned user groups are termed targets, denoted by T.If deployed in a real online environment, one can consider usingrule- or lexical-based approaches , or trainable detectors to identify the target set T from each post . Because ouremphasis is to uplift target-aware HSD fairness, we directly employdatasets (see .1 for details) that come with target labels,which are identified by human annotators from the textual clueswithin each post . The target identification task, as amore widely studied task, is not investigated in the paper.Definition 2: Target-aware Fairness Metrics. We assume a finiteset of targeted groups T used for evaluation. For posts containingeach target T, the overall group-level HSD classification perfor-mance can be measured by a specific accuracy metric. Then, fairnessacross targeted groups is reflected by the performance discrepancyamong all |T | targets. Intuitively, the smaller the target-wise dis-crepancy, the fairer the HSD classifier. In our work, to quantifythe level of algorithmic fairness, we adopt the well-established no-tion of False Positive/Negative Equality Difference suggested byGoogle Jigsaw, abbreviated as FPED/FNED. The only slight differ-ence is that we normalize their values with the number of targetsinvolved to make them comparable across different datasets, andthe normalized versions are termed nFPED and nFNED:",
  ": An overarching view of GetFair. Detailed designsof the four objective functions can be found in Sections 3.2(L), 3.3 (L), and 3.4 (L and L), respectively": "negative rates on all posts. Essentially, both nFPED and nFNEDmeasure how far on average the HSD performance on each target deviates from the global average, and ideally will converge tozero if all targets are treated evenly by the HSD classifier. Sinceit is equally important to account for both nFPED and nFNED forfairness, we further add the harmonic mean of both metrics, termedharmonic fairness (HF):",
  "nFPED + nFNED,(2)": "which jointly factors in two metrics for fairness evaluation. Withthe fairness metrics defined, we formulate our HSD task with gen-eralizable target-aware fairness below.Definition 3: Hate Speech Detection with Generalizable Target-aware Fairness. Given a set of posts S = {(, T,)}|S |=1where the -th post is associated with the binary label anda subset of known targets T T, T , our objective isto train a debiased HSD classifier HSD(). The trained classifierHSD() is evaluated on the test set S = {(, T,)}|S |=1,where T T, T . More importantly, all test-time postswill contain at least one target that is completely unseen duringtraining, i.e., T \\ T . Ideally, HSD() is expected to:(1) provide maximal classification effectiveness, reflected by largerscores in accuracy, F1, and AUC; and (2) yield minimal target-wisebias, reflected by lower scores in nFPED, nFNED, and HF.",
  "Target-specific Filter Generation withAdaptive Hypernetwork": "In GetFair, we utilize a filter function () which takes the generated-dimensional content embedding s R from an arbitrary encoder() as its input, and emits a debiased representation s = (s) R.The filter () will be trained to remove target-specific informationfrom the content embedding s, such that subsequent hate speechpredictions made with s are less reliant on it and will exhibit lessperformance bias on different targets while maintaining accuracy.Without loss of generality, () can be formulated as a multi-layerperceptron (MLP) with weights W R and biases b R at",
  "()= [W() , b()] = (t),(3)": "where () is the hypernetwork responsible for generating param-eters (i.e., weights and biases) for the -th layer of filter (), and denotes horizontal concatenation. For notation simplicity, weuse () R(+1) to denote the concatenated weight and bias ateach layer. Following common practices in hypernetworks , () firstly outputs a flat vector with 2 + dimensionality, whichis further reshaped into the matrix form of ().Target Indicators. Notably, () is shared across all targets,and the generated parameters are solely conditioned on its inputt R, which is the target indicator that informs the hypernet-work of the exact target filter to generate. To facilitate effectiveinformation filtering and target-wise generalizability, the designedtarget indicators should meet two characteristics: (1) they should bedistinguishable for different targets such that every target-specificfilter maintains uniqueness; and (2) they are capable of represent-ing an arbitrary number of unseen targets without any training.Hence, this eliminates some indicator methods commonly used inhypernetworks such as one-hot encoding and learnable embeddings. In GetFair, we take advantage of the wide availabilityof pretrained word embeddings, namely the GloVe embeddings for composing the target indicator t. We have adopted its300-dimensional version, thus = 300. Specifically, for target = {1, ..., , ..., } represented as a sequence of vocabularies,we take the mean of all corresponding word embeddings v R",
  "as its representation, i.e., t = 1": "=1 v.Low-rank Parameterization. In the default design of the adap-tive hypernetwork, the output space for each hypernetwork is ( + 1), which is substantially large (e.g., 16, 512 predictionsto make for = 128) compared with the input dimensionality.Consequently, this creates a low-dimensional bottleneck where thehypernetwork layers are not sufficiently expressive for the down-stream predictions , thus being prone to underfitting.Furthermore, although the target-specific filter parameters are nolonger stored owing to the adaptive hypernetwork, they are stillrequired for in-memory forward and backward passes during runtime, thus significantly limiting the batch size allowed and nega-tively impacting the training time and memory efficiency. As such,we adopt a low-rank formulation for the filter parameters:",
  "()= U()W()V() ,(4)": "where U() R, W() R, V() R(+1), and the rank . With this low-rank parameterization, assuming = , thememory cost for an -layer target-specific filter is dramaticallyreduced from O(2 + ) to O(2 + 2 + ), which nowconveniently supports parallelized batch computing.Multi-target Filters. It is worth mentioning that, in real appli-cations, one post might be attributed to several different targets,calling upon the necessity for a mechanism that can simultaneouslyfilter a set of targets T associated with each posts embedding s. Astraightforward way of dosing so is to remove one target-specificinformation at a time via s, = (s) for every T, then mergeall filtered embeddings of the same post (e.g., via sum pooling )into s. Though each s, removes information related to target T, it is not necessarily the case for a different s, where Tand , as the associated filter () is only dedicated to remov-ing information about . Consequently, the embedding fusion stepmay introduce unwanted target-specific information back into s.To bypass this potential defect, we design a combinatorial approachto form a multi-target filter via parameter ensemble:",
  "Regularizing Filter Parameters withSemantic Gap Alignment": "Compared with learning multiple filter functions, learning a unifiedadaptive hypernetwork brings substantially lighter parameteriza-tion, but also comes at a risk of overfitting some specific targetdistributions, e.g., the frequent ones, challenging the generlizability.Hence, the hypernetwork, if trained without regularization, canpotentially fail to generalize when generating filter weights forcompletely unseen targets. To prevent the generated filter param-eters from undesired homogeneity and thus impaired utility, weimpose an additional regularization on the distribution of the gen-erated filter parameters ()w.r.t. different targets. Specifically, forevery pair of targets , and their indicators t, t, we would like topreserve their semantic distance within the generated filter param-eters , too, i.e., (t, t) (() , () ) with a distance metric(, ). To bypass the unmatched dimensionality between t and ,we define a semantic gap alignment scheme as the regularizationloss L as follows:",
  "Target Discriminator": "Ideally, each debiased post embedding s no longer carries target-specific information that leads to the hypothetically unfair classifi-cation results. To ensure that the target filter removes the target-related information, we propose to take advantage of adversarialtraining to optimize each filter. Specifically, we set up a target dis-criminator defined as the following:",
  "p = sigmoid(MLP (s)),(8)": "where sigmoid(MLP ()) : R (0, 1)| T | is an MLP with asigmoid readout that maps the input s into a |T|-dimensionaloutput, with each entry representing the probability of having atarget associated with the post s. Note that given the existenceof multiple targets in a single post, p is essentially a collection ofindividual binary classification scores instead of a softmax distri-bution. The optimization of MLP () is facilitated by minimizingthe point-wise log loss below:",
  "log + (1 ) log ,(11)": "where {0, 1} is the binary label of each training instance.Enhancing HSD Performance via Imitation Learning. Toprevent the filtered post embedding s from the byproduct of per-formance degradation, we further enhance the training of the HSDclassifier with imitation learning. Specifically, we ask MLP (s)to mimic its response when given the unfiltered version of the postembedding s. This is achieved by aligning the predicted probabilitydistributions over the two binary classes:",
  "Adversarial Optimization via Alternation": "Alongside the fundamental goal of achieving accurate HSD, Get-Fair encompasses two additional components that have adversarialgoals. The target discriminator is designed to identify associatedtargets for each given post embedding, while the filter function () essentially tries to deceive it such that relevant targets cannotbe confidently inferred from the debiased embedding s. As such,this naturally translates to an adversarial training paradigm wherethe filter function and target discriminator are mutual adversariesto each other.To facilitate the optimization of GetFair, we put forward an alter-nating training paradigm with Algorithm 1. Specifically, in the firstmain loop (lines 5-10), mini-batch gradient descent is performedfor the target discriminator, which is trained to infer the targetlabels from the given post embeddings. In the second main loop(lines 11-16), with the discriminators parameters frozen, the filterfunction, along with the HSD classifier, are jointly trained with asynergic loss (line 16) that aims to magnify the target classificationerror while minimizing other loss terms. It is worth noting that,the pretrained transformer-based encoder () for generating s willalso be finetuned throughout this adversarial training procedure.",
  "Evaluation Datasets": "In our experiments, two publicly acquired benchmarks are in use,namely Jigsaw and MHS. To center our test around generalizabilityto unseen targets, we hold out two targeted groups for evaluationand use the rest for training and validation. For a thorough compar-ison, we have created two test settings on both datasets by choosingdifferent seen and unseen targets. In what follows, we provide abrief overview of both datasets and their target settings below.Jigsaw. This dataset was released by Googles online contentmoderation arm, namely Jigsaw3 to encourage relevant research.A subset of the posts have been tagged with a variety of identityattributes (i.e., targets) based on the identities mentioned in eachpost. For our evaluation, we collate all posts with target tags. Thetarget allocation for two test settings are as follows:",
  "Setting 1: Seen targets (training and validation) {Male,Female, Homosexual, Christian, Jewish, Black, Mental Illness};Unseen targets (test) {Muslim, White}": "Setting 2: Seen targets {Male, Homosexual, Christian, Jew-ish, Mental Illness, Muslim, White}; Unseen targets {Female,Black}MHS. The measuring hate speech (MHS) corpus was created andreleased by . MHS combines social media posts from YouTube,Twitter, and Reddit that are manually labelled by crowdsource work-ers from Amazon Mechanical Turk (AMT). The AMT annotatorshave also marked the targets associated with each post. We alsotest GetFairs generalization to unseen targets with two settings:",
  "Baselines and Metrics": "To test the effectiveness of GetFair, we compare it with five fairness-aware baseline methods that aim to debias HSD classifiers, namelyTHSD , FairReprogram , SAM , LWBC , and FEAG. We defer their details to Appendix B.Metrics. For evaluation, we cross-compare all methods from twoperspectives: (1) the HSD effectiveness, measured by classificationmetrics accuracy, F1, and Area under the ROC Curve (AUC); (2) thetarget-specific fairness, measured by nFPED, nFNED, and HF as inEq.(1) and Eq.(2).Implementation Notes. For fairness, we adopt the same pre-trained encoder, BERT4 across all the methods tested for em-bedding the texts in each post, which is a popular and performantchoice in the HSD literature . For GetFair, by default we setdiscriminator loss coefficient = 0.9 and the imitation loss weight = 3 on both datasets according to the contributions and magni-tudes of the loss terms. Also, we set to 0.9 and 0.5, and to 1and 5 respectively for Jigsaw and MHS. and are set to 1 and5 for optimizing GetFair, respectively. We use hidden dimension = 256 on both datasets. The filter depth is consistently set to 1for optimal efficiency as we do not notice a significant performance",
  "Overall Performance (RQ1)": "We start our analysis with the overall performance. revealsthe results from all debiasing methods on both datasets. In whatfollows, we present our observations and findings.Effectiveness in HSD. has shown the consistently ad-vantageous HSD effectiveness of GetFair, which has achieved thebest accuracy, F1, and AUC on Jigsaw, and is either the best orsecond-best across these three metrics on MHS. In general, the ma-jority of debiased HSD methods achieve the fairness objective bymaking a considerable sacrifice in the HSD efficacy, witnessed bythe subpar accuracy of LWBC on both Jigsaw and MHS, as well asthe low F1 of several models, e.g., THSD and FEAG on both Jigsawand MHS, as well as FairReprogram on MHS. In contrast, on top ofits capability of debiasing, GetFair maintains a high level of utility.Fairness in HSD. On Jigsaw, GetFair yields highly advantageousfairness scores under both settings. Such superiority can also beobserved on MHS with Setting 2. On MHS with Setting 1, GetFairhas scored the second-best results for both nFNED and HF, onlyfalling behind LWBC by a small margin. In the meantime, it is worthmentioning that the strong fairness of LWBC on MHS is achievedby giving up its utility its 50% accuracy implies the detectioneffectiveness is only slightly better than a random classifier. It isalso noticed that, among the two metrics nFPED and nFNED, somemethods are able to obtain a relatively low score on one of them,but tend to get a substantially higher score on the other. Examplesinclude THSD and FEAG on Jigsaw, where the nFNED scores areone magnitude higher than their nFPED scores, translating into alarger number of real hateful posts being missed out by the trainedclassifier. Meanwhile, GetFair keeps both nFPED and nFNED low,showcasing a balanced performance. More importantly, consider-ing that the test set contains two unseen targets, this reflects thesuperior generalizability of GetFair in real-world settings.Performance Summary. To showcase the effectiveness-fairnesstrade-off of all methods tested, we visualize their performance onboth datasets via a scattered plot in . Based on the visualiza-tion, GetFair is in the highest quartile among all the HSD debiasingmethods. Again, this verifies that, compared with other baselines,GetFair is able to achieve the state-of-the-art target-aware fairnessresults without hurting the real-world usability of the trained HSDclassifier. As an additional note on efficiency, with a batch size of",
  "Ablation Study (RQ2)": "We hereby answer RQ2 via ablation study, where we build variantsof GetFair by removing/modifying one key component at a time,and the new results from the variants are recorded in . We useF1 and HF for performance demonstration, and test with Setting 1on both datasets. Specifically, we are interested in the contributionsof imitation learning, semantic gap alignment, and the design ofmulti-target filters to both the HSD effectiveness and fairness. Inwhat follows, we introduce the corresponding variants and analyzetheir performance implications.Remove Imitation Learning. The imitation learning objectivedefined in Eq.(12) aims to uplift the HSD classifiers performance byaligning the predicted probability distributions generated from thesame posts filtered and unfiltered embeddings. After removing thiscomponent, GetFair has experienced a slight drop in F1 scores onboth datasets, which showcases the efficacy of imitation learningfor improving the detection accuracy. It is also noticed that whilethe HF score stays stable on MHS, there is an increase in HF onthe Jigsaw dataset. One possible reason is that, the performancedecrease has also amplified the models tendency of producing falsenegative and false positive results, thus bumping up the HF score.Remove Semantic Gap Alignment. With the semantic gapalignment regularizer (Eq.(7)) removed, the fairness of the HSD re-sults has significantly deteriorated. As the regularization essentiallyrectifies the way a target-specific filters parameters are generated,its removal has incurred a lower quality of generated filters andconsequently inferior generalizability to unseen targets. On MHS,a small improvement of F1 is observed, which may attribute to",
  ": Analysis of the impact from key hyperparameters, with effectiveness and fairness metrics F1 and HF, respectively": "the use of target-specific features as a result of less effective fil-tering. Besides, to further showcase the efficacy of the semanticgap alignment, we have visualized both the target indicators andtheir corresponding filter weights generated by the hypernetwork(with the semantic gap alignment in place) by projecting them ontoa two-dimensional space via t-SNE via . As can beseen, the distances among the target indicators (i.e., raw inputs ofthe hypernetwork) are mostly kept in their corresponding filterparameters (i.e., outputs of the hypernetwork), thus explaining thefairness boost from this semantic gap alignment scheme.Replacing Parameter Ensemble with Combinatorial Em-bedding Filters. As we put forward a filter design explicitly forthe co-existence of multiple targets within one post, we compareits performance with a straightforward counterpart, i.e., simplyperforming sum pooling on each individually filtered post embed-dings as described in .1. That is to say, Eq.(5) is updatedto s =1|T | T s,. With this change, a dramatic drop in boththe HSD accuracy and fairness is observed. This has verified that,compared with combining outputs from all single-target filters, ourparameter ensemble design in GetFair is more capable of preventingthe filtered post embedding from noises.",
  "Hyperparameter Sensitivity (RQ3)": "In this section, we examine the impact of some core hyperparam-eters, namely the discriminator weight , imitation weight , andregularization weight used by the synergic loss in Algorithm 1,as well as the rank (.1). This is done by adjusting onehyperparameter at a time and recording the new results achieved,while all other hyperparameters are kept to the default setting. Thispart of the experiments are also conducted with Setting 1 on bothdatasets, and similar trends can be observed with Setting 2.Impact of . The coefficient is tuned in {0.1, 0.3, 0.5, 0.7, 0.9}.As it essentially links to the optimization of target-specific filtering, has a stronger impact on HF than on F1, where HF benefits froma larger value of . When increases from 0.1 to 0.5, there is arapid improvement in the classification fairness as per the HF score.When is sufficiently large (i.e., 0.7 in our case), the fairnessgain remains positive but appears to be at a lower rate.Impact of . Interestingly, the variation of imitation loss weight not only causes fluctuations in the HSD effectiveness, but also",
  "RoBERTa-base0.78640.0034": "correlates to different fairness levels. As GetFair primarily putsmore value on fairness in the HSD, = 3 is a reasonable choice fora balanced fairness-accuracy trade-off.Impact of . The regularization weight controls the diversityand quality of the hypernetwork-generated target filters, thus highersensitivity to is observed on the fairness metric HF. The generaltrend is that, as grows, fairer detection outcomes are attained.When taking the corresponding F1 into account, = 0.9 and = 0.5are respectively the most sensible settings for Jigsaw and MHS.Impact of . The low-rank parameterization of the filter param-eters generated by the hypernetwork intends to lower the memorycost while ensuring an adequate level of efficacy. Intuitively, alarger is able to preserve more expressiveness for the generatedfilters. On Jigsaw, a larger generally contributes to lower F1 butbetter HF, while the trend on MHS is on the opposite side. A pos-sible reason is that, on Jigsaw, a stronger filter function removesmore target-related information from the post embedding, but alsoblocks useful features for HSD classification; while on MHS, thespurious target-related features are more implicit, hence a morecapable filter function can effectively filter out those noises fromthe post embedding to achieve a higher F1 score but also more falsepositive/negative predictions (i.e., a higher HF).",
  "Compatibility with Other Encoders (RQ4)": "GetFair is designed to be compatible with different pretrained textencoders () as backbones. To verify this compatibility, we havetested GetFair by using two other popular pretrained languagemodels (PLMs), namely DistilGPT2 and RoBERTa (base version) as (). Similar to RQ2 and RQ3, we test on both datasets underSetting 1 and report F1 and HF metrics.",
  "Hate Speech Detection with Generalizable Target-aware FairnessKDD 24, August 2529, 2024, Barcelona, Spain": "From the results in , the first conclusion drawn is thatGetFair is able to maintain its performance in HSD tasks when it ispaired with different pretrained encoders, especially the classifica-tion fairness measured by HF. Secondly, as per F1, RoBERTa yieldsthe highest HSD effectiveness among the three backbones, whileDistilGPT2 shows limited effectiveness gain compared with thedefault BERT used in GetFair. We hypothesize that this is attributedto different PLMs model capacity. As per the PLMs tested, Distil-GPT2 has a lower capacity than the BERT-base we have used (89million and 110 million parameters respectively), hence producinga similar or even lower accuracy than BERT. In the meantime, thebetter accuracy of RoBERTa aligns with its higher model size (125million parameters in the base version) and capacity.",
  "Hate Speech Detection": "Mining user-generated web content is a long-lastingresearch area with versatile applications , where hatespeech detection is one of the most representative lines of work.Hate speech on social media is commonly defined as a languagethat attacks or diminishes, that incites violence or hate againstgroups, based on specific characteristics such as physical appear-ance, religion, gender identity or other . In summary, extractingrepresentative linguistic features from the post texts lies at the coreof various HSD methods. Early practices in HSD involve the useof vocabulary dictionaries like Ortony Lexicon to pinpointpotentially hateful keywords, which then evolves to the use ofmore sophisticated linguistic features like the term frequency withinverse document frequency (TF-IDF) , -grams , and sen-timent . Those text-based features can be easily incorporatedwith the downstream classifiers for hate speech classification. Inthe most recent line of work, there has been an adoption of morecomplex features like images and social connections for the HSD task. Meanwhile, with the rise of language models,especially variations of the transformer family , featuresextracted from the raw texts (a.k.a. embeddings) by those pretrainedlanguage models are arguably the most widely adopted option inHSD . The commonly shared goal of HSD is usuallyperformance-oriented, where many new aspects in HSD are at-tracting an increasing amount of attention, such as efficiency andtimeliness of the predictions , explainability and transparencyof the classification mechanism , and robustness of the HSDclassifier to adversarial attacks or low-quality data . In whatfollows, we discuss an emerging research topic in this area, i.e.,ensuring the fairness in HSD.",
  "Fairness in Hate Speech Detection": "A series of research has uncovered that methods in natural languageprocessing (NLP) tasks are subject to a variety of fairness issues , and HSD is no exception as a typical NLP task. Technically,in the context of HSD, the biases can come from either the source (e.g., authors, annotators, and data collectors) and the target [44, 50] of online posts. In this work, our main focus is to demote biasestoward the targets of online posts.The main objective of addressing biases against targeted groupsis to diminish the emphasis placed on the inclusion/exclusion ofparticular terms and redirect attention towards the broader contextwithin the content being evaluated. As discussed in , data-centric solutions are a representative line of work. Some solutionsreweigh each training sample based on its likelihood of introducingbias into the model , while some provide additional meta-data, human annotations, or data augmentation fora more rigorous model training process. To bypass the reliance onempiric and human involvement, model-centric solutions aim atremoving information related to the spurious target-related fea-tures from learned representations of an online post. This is usuallyachieved by utilizing a dedicated filter module, which is trainedend-to-end along with the HSD classifier . It is alsoseen that some solutions are able to handle intersectional bias be-tween multiple targets . However, the aforementioned meth-ods all suffer from restricted generalizability, as they are trainedwith the assumption that all targets are seen in the training stage.Due to the distributional discrepancies among different targets,the debiasing effectiveness can hardly transfer to completely un-seen targets during inference, creating a practicality bottleneck forreal-world applications. Despite the efforts on some generalizableHSD methods , their goal is to maintain the HSD accuracyunder distributional/domain shift, thus being unable to solve thegeneralizability challenges associated with target-aware fairness.",
  "Conclusion": "In this paper, to address the deficiency of existing debiasing/fairness-aware HSD methods when handling unseen targets during training,we propose GetFair, which achieves generalizable target-aware fair-ness in HSD by adaptively generating target-specific filters via ahypernetwork instead of training individualized ones. A suite ofinnovative designs including low-rank parameterization, semanticgap alignment, and imitation learning are proposed for loweringthe memory cost, regularizing the generalizability of target-specificfilters, and enhancing the HSD classifier, respectively. Through aseries of experiments, we have validated the effectiveness, fairness,and generalizability of GetFair, proving it to be a viable solutionto ensuring target-aware fairness in HSD. Possible extensions ofGetFair in our future work include discovering time-sensitive pat-terns for dynamic debiasing, as well as developing lightweightvariants of GetFair to further enhance scalability.",
  "Acknowledgement": "This work is supported by Australian Research Council underthe streams of Discovery Project (Grant No. DP240101108 andDP240101814), Future Fellowship (No. FT210100624), DiscoveryEarly Career Researcher Award (No. DE230101033 and DE220101597),Center of Excellence (No. CE200100025) and Industrial Transfor-mation Training Centre (No. IC200100022). This work is partiallysupported by the Swiss National Science Foundation (Contract No.CRSII5_205975). Financial support from The University of Queens-land School of Business in the UQBS 2023 Research Project is grate-fully acknowledged.",
  "Lukas Hauzenberger, Shahed Masoudian, Deepak Kumar, Markus Schedl, andNavid Rekabsaz. 2023. Modular and on-demand bias mitigation with attribute-removal subnetworks. In Findings of the ACL. 61926214": "HuggingFace. 2019. DistilGPT2. (2019). Brendan Kennedy, Xisen Jin, Aida Mostafazadeh Davani, Morteza Dehghani,and Xiang Ren. 2020. Contextualizing Hate Speech Classifiers with Post-hocExplanation. In ACL. 54355442. Chris J Kennedy, Geoff Bacon, Alexander Sahn, and Claudia von Vacano. 2020.Constructing interval variables via faceted Rasch measurement and multitaskdeep learning: a hate speech application. arXiv preprint arXiv:2009.10277 (2020).",
  "Gaurav Maheshwari, Aurlien Bellet, Pascal Denis, and Mikaela Keller. 2023. FairWithout Leveling Down: A New Intersectional Fairness Definition. In EMNLP": "Sarah Masud, Subhabrata Dutta, Sakshi Makkar, Chhavi Jain, Vikram Goyal,Amitava Das, and Tanmoy Chakraborty. 2021. Hate is the new infodemic: Atopic-aware modeling of hate speech diffusion on twitter. In ICDE. 504515. Binny Mathew, Punyajoy Saha, Seid Muhie Yimam, Chris Biemann, Pawan Goyal,and Animesh Mukherjee. 2021. Hatexplain: A benchmark dataset for explainablehate speech detection. In AAAI, Vol. 35. 1486714875.",
  "AppendixADataset Statistics": "As discussed in .1, we provide the detailed statistics of ourexperimental datasets, namely Jigsaw and MHS. Both datasets areinherently imbalanced, where the non-hateful posts outnumbershateful ones. Specifically, we have balanced the binary classes ofhateful and non-hateful (neutral) posts in the validation and testsets to ensure a more rigorous evaluation. Note that there can bemore than one identified target associated with each post. Theposts in the test set are allowed to also contain targets that are seenduring training, while the training set does not have any posts thatmention the two hold-out targets. lists the key statistics of the Jigsaw dataset after beingprocessed for the two different unseen target settings. Analogously, below corresponds to the MHS dataset, also with two dif-ferent unseen target settings."
}