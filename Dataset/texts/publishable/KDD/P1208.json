{
  "ABSTRACT": "Learning multi-agent system dynamics has been extensively stud-ied for various real-world applications, such as molecular dynamicsin biology, multi-body system in physics, and particle dynamics inmaterial science. Most of the existing models are built to learn singlesystem dynamics, which learn the dynamics from observed histori-cal data and predict the future trajectory. In practice, however, wemight observe multiple systems that are generated across differentenvironments, which differ in latent exogenous factors such astemperature and gravity. One simple solution is to learn multipleenvironment-specific models, but it fails to exploit the potentialcommonalities among the dynamics across environments and offerspoor prediction results where per-environment data is sparse orlimited. Here, we present GG-ODE (Generalized Graph OrdinaryDifferential Equations), a machine learning framework for learningcontinuous multi-agent system dynamics across environments. Ourmodel learns system dynamics using neural ordinary differentialequations (ODE) parameterized by Graph Neural Networks (GNNs)to capture the continuous interaction among agents. We achievethe model generalization by assuming the dynamics across differentenvironments are governed by common physics laws that can becaptured via learning a shared ODE function. The distinct latent ex-ogenous factors learned for each environment are incorporated intothe ODE function to account for their differences. To improve modelperformance, we additionally design two regularization losses to(1) enforce the orthogonality between the learned initial states andexogenous factors via mutual information minimization; and (2)reduce the temporal variance of learned exogenous factors withinthe same system via contrastive learning. Experiments over variousphysical simulations show that our model can accurately predictsystem dynamics, especially in the long range, and can generalizewell to new systems with few observations.",
  "Information systems Physical data models; Computingmethodologies Spatial and physical reasoning": "Permission to make digital or hard copies of part or all of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for third-party components of this work must be honored.For all other uses, contact the owner/author(s).KDD 23, August 610, 2023, Long Beach, CA, USA 2023 Copyright held by the owner/author(s).ACM ISBN 979-8-4007-0103-0/23/08.",
  "INTRODUCTION": "Building a simulator that can understand and predict multi-agentsystem dynamics is a crucial research topic spanning over a varietyof domains such as planning and control in robotics , where thegoal is to generate future trajectories of agents based on what hasbeen seen in the past. Traditional simulators can be very expensiveto create and use as it requires sufficient domain knowledgeand tremendous computational resources to generate high-qualityresults1. Therefore, learning a neural-based simulator directly fromdata that can approximate the behavior of traditional simulatorsbecomes an attractive alternative.As the trajectories of agents are usually coupled with each otherand co-evolve along with the time, existing studies on learningsystem dynamics from data usually view the system as a graph andemploy Graph Neural Networks (GNNs) to approximate pair-wisenode (agent) interaction to impose strong inductive bias . Asa pioneering work, Interaction Networks (IN) decompose thesystem into distinct objects and relations, and learn to reason aboutthe consequences of their interactions and dynamics. Later work in-corporates domain knowledge , graph structure variances ,and equivariant representation learning into learning fromdiscrete GNNs, achieving state-of-the-art performance in variousdomains including mesh-based physical simulation and molec-ular prediction . However, these discrete models usually sufferfrom low accuracy in long-range predictions as (1) they approximatethe system by discretizing observations into some fixed timestampsand are trained to make a single forward-step prediction and (2)their discrete nature fails to adequately capture systems that arecontinuous in nature such as the spread of COVID-19 and themovements of an n-body system [18? ].Recently, researchers propose to combine ordinary differentialequations (ODEs) - the principled way for modeling dynamicalsystems in a continuous manner in the past, with GNNs to learncontinuous-time dynamics on complex networks in a data-driven",
  "KDD 23, August 610, 2023, Long Beach, CA, USAZijie Huang, Yizhou Sun, and Wei Wang": "AAPPENDIXA.1DatasetsWe conduct experiments over two datasets: The Water datasetand the Lennard-Jones potential dataset. As introduced in Sec 2,the edges between agents are assigned if the Euclidean distancebetween the agents positions ,= ||, , ||2 is within a(small) connectivity radius . The connectivity radius for the twodatasets is set as 0.015 and 2.5 respectively. The number of particlesis kept the same as 1000 for all trajectories in the Lennard-Jonespotential dataset, while in the Water dataset, each data sample canhave a varying number of particles, and the maximum number ofparticles is 1000.A.1.1Data Split. Our model is trained in a sequence-to-sequencemode, where we split the trajectory of each training sample intotwo parts and [+1, ]. We condition on the first part ofobservations to predict the second part. To fully utilize the datapoints within each training sample, we split each trajectory intoseveral chunks with three hyperparameters: the observation lengthand prediction length for each sample, and the interval betweentwo consecutive chunks (samples). We summarize the procedurein Algorithm 1, where is the number of trajectories and is theinput feature dimension.",
  "PROBLEM DEFINITION": "We aim to build a neural simulator to learn continuous multi-agentsystem dynamics automatically from data that can be generalizedacross environments. Throughout this paper, we use boldface up-percase letters to denote matrices or vectors, and regular lowercaseletters to represent the values of variables.We consider a multi-agent dynamical system of interactingagents as an evolving interaction graph G = {V, E }, where nodesare agents and edges are interactions between agents that canchange over time. For each dynamical system, we denote asthe environment from which the data is acquired. We denote , X as the feature matrix for all agents and ,as the feature vectorof agent at time under environment . The edges between agentsare assigned if two agents are within a connectivity radius basedon their current locations ,which is part of the node featurevector, i.e. , , . They reflect the local interactions of agentsand the radius is kept constant over time .Our model input consists of the trajectories of agents over timestamps 1:, = {1,, 2,, . . . , ,}, where the times-tamps 1,2 can have non-uniform intervals and be of anycontinuous values. Our goal is to learn a generalized simulator : 1:, +1: , that predicts node dynamics in the futurefor any environment . Here , Y represents the targeted nodedynamic information at time , and can be a subset of the inputfeatures. We use ,to denote the targeted node dynamic vectorof agent at time under environment .",
  "PRELIMINARIES AND RELATED WORK3.1Dynamical System Simulations with GraphNeural Networks (GNNs)": "Graph Neural Networks (GNNs) are a class of neural networks thatoperate on graph-structured data by passing local messages. They have been extensively employed in various applica-tions such as node classification , link prediction , andrecommendation systems . By viewing each agent asa node and interaction among agents as edges, GNNs have shownto be efficient for approximating pair-wise node interactions andachieved accurate predictions for multi-agent dynamical systems. The majority of existing studies propose discrete GNN-based simulators where they take the node features at time asinput to predict the node features at time +1. To further capturethe long-term temporal dependency for predicting future trajecto-ries, some work utilizes recurrent neural networks such as RNN,LSTM or self-attention mechanism to make prediction at time +1 based on the historical trajectory sequence within a time win-dow . However, they all restrict themselves to learna one-step state transition function. Therefore, when successivelyapply these one-step simulators to previous predictions in orderto generate the rollout trajectories, error accumulates and impairsthe prediction accuracy, especially for long-range prediction. Also,when applying most discrete GNNs to learn over multiple systemsunder different dynamical laws (environments), they usually re-train the GNNs individually for dealing with each specific systemenvironment , which yields a large computational cost.",
  "= 1, 2": ". Here R denotes the state vari-able for agent at timestamp and denotes the ODE function thatdrives the system move forward. Given the initial states 01, 0for all agents and the ODE function, any black box numerical ODEsolver such as Runge-Kuttais can solve the ODE initial-valueproblem (IVP), of which the solution can be evaluated at anydesired time as shown in Eqn 1.",
  "=0 1, 2 (1)": "Traditionally, the ODE function is usually hand-crafted basedon some domain knowledge such as in robot motion control and fluid dynamics , which is hard to specify without know-ing too much about the underlying principles. Even if the exactODE functions are given, they are usually hard to scale as theyrequire complicated numerical integration . Some recentstudies propose to parameterize it with a neural net-work and learn it in a data-driven way. They combine the expressivepower of neural networks along with the principled modeling ofODEs for dynamical systems, which have achieved promising re-sults in various applications .",
  "GraphODE for Dynamical Systems": "To model the complex interplay among agents in a dynamical sys-tem, researchers have recently proposed to combine ODE withGNNs, which has been shown to achieve superior performance inlong-range predictions . In , an encoder-processor-decoder architecture is proposed, where an encoder first computesthe latent initial states for all agents individually based on their firstobservations. Then an ODE function parameterized by a GNN pre-dicts the latent trajectories starting from the learned initial states.Finally, a decoder extracts the predicted dynamic features basedon a decoding function that takes the predicted latent states as in-put. Later on, a Graph-ODE framework has been proposed which follows the structure of variational autoencoder . They as-sume an approximated posterior distribution over the latent initialstate for each agent, which is learned based on the whole histor-ical trajectories instead of a single point as in . The encodercomputes the approximated posterior distributions for all agentssimultaneously considering their mutual influence and then samplethe initial states from them. Compared with , they are able toachieve better prediction performance, especially in the long range,and are also capable of handling the dynamic evolution of graphstructures which is assumed to be static in .We follow a similar framework to this line but aim at generalizingGraphODE to model multiple systems across environments.",
  "METHOD": "In this section, we present Generalized Graph ODE (GG-ODE ) forlearning complex system dynamics across environments. As de-picted in , GG-ODE consists of four main components thatare trained jointly: (1) an initial state encoder for inferring the la-tent initial states for all agents simultaneously; (2) an environmentencoder which learns the latent representations for exogenous fac-tors; (3) a generative model defined by a GNN-based ODE functionthat is shared across environments for modeling the continuousinteraction among agents in the latent space. The distinct latentexogenous factors learned for each environment are incorporatedinto the ODE function to account for their discrepancies, and (4)a decoder that extracts the predicted dynamic features based on adecoding function. We now introduce each component in detail.",
  "putes a posterior distribution of latent initial state0,| 1:,": "for each agent, from which 0,is sampled. The latent initial state0,for each agent determines the starting point for the predictedtrajectory. We assume the prior distribution (0, ) is a standardnormal distribution, and use KullbackLeibler divergence term inthe loss function to add significant regularization towards howthe learned distributions look like, which differs VAE from otherautoencoder frameworks . In multi-agent dynamical sys-tems, agents are highly-coupled and influence each other. Instead oflearning such distribution separately for each agent, such as usingan RNN to encode the temporal pattern for each individualtrajectory, we compute the posterior distributions for all agentssimultaneously (similar to ). Specifically, we fuse all trajectories",
  ": Time Invariance": ": The overall framework of GG-ODE consists of four modules. First, an initial state encoder computes the latent initialstates for all agents simultaneously by constructing a temporal graph from the input trajectories. Additionally, an environmentencoder computes the latent representations for exogenous factors that are distinct for each environment. Then, the generativemodel defined by a GNN-based ODE function calls the solver to output the predicted latent states for agents in the future, wherethe learned exogenous factors are incorporated into the ODE function. Finally, a decoder generates the predicted dynamics foreach agent based on the decoding likelihood determined by the latent states. Two regularization terms are added to preservethe orthogonality of two encoders and the time-invariant property of the environment encoder. as a whole into a temporal graph to consider both the temporalpatterns of individual agents and the mutual interaction amongthem, where each node is an observation of an agent at a specifictimestamp. Two types of edges are constructed, which are (1) spatialedges V that are among observations of interacting agents at eachtimestamp if the Euclidean distance between the agents positions, = ||, , ||2 is within a (small) connectivity radius ; and(2) temporal edges that preserve the autoregressive nature of eachtrajectory, defined between two consecutive observations of thesame agent. Note that spatial edges are bidirectional while tempo-ral edges are directional to preserve the autoregressive nature ofeach trajectory, as shown in . Based on the constructedtemporal graph, we learn the latent initial states for all agentsthrough a two-step procedure: (1) dynamic node representationlearning that learns the representation ,for each observationnode whose feature vector is , . (2) sequence representation learn-ing that summarizes each observation sequence (trajectory) into afixed-dimensional vector through a self-attention mechanism.",
  "/": "where () is a non-linear activation function; is the dimensionof node embeddings. The node representation is computed as aweighted summation over its neighbors plus residual connectionwhere the attention score is a transformer-based dot-productof node representations by the use of value, key, query projectionmatrices ,,. The learned attention scores are normalized",
  "via softmax across all neighbors. Here (,)is the representation": "of agent at time in the -th layer. (,)is the general repre-sentation for a neighbor which is connected either by a temporaledge (where < and = ) or a spatial edge (where = and ) to the observation (,). We add temporal encoding to each neighborhood node representation in order to distinguishthe message delivered via spatial and temporal edges respectively.Finally, we stack layers to get the final representation for eachobservation node as : ,= (,). 4.1.2Sequence Representation Learning. We then employ a self-attention mechanism to generate the sequence representation for each agent, which is used to compute the mean 0,and variance0,of the approximated posterior distribution of the agents initialstate. Compared with recurrent models such as RNN, LSTM , itoffers better parallelization for accelerating training speed and inthe meanwhile alleviates the vanishing/exploding gradient problembrought by long sequences .",
  "where trans is a simple Multilayer Perceptron (MLP) whose outputvector is equally split into two halves to represent the mean andvariance respectively": "4.2Environment EncoderThe dynamic nature of a multi-agent system can be largely affectedby some exogenous factors from its environment such as gravity,temperature, etc. These exogenous factors can span over a widerange of settings and are sometimes latent and not observable. Tomake our model generalize across environments, we design anenvironment encoder to learn the effect of the exogenous factorsautomatically from data to account for the discrepancies across envi-ronments. Specifically, we use the environment encoder to learn therepresentations of exogenous factors from observed trajectories andthen incorporate the learned vector into the ODE function whichis shared across environments and defines how the system evolvesover time. In this way, we use a shared ODE function framework tocapture the commonalities across environments while preservingthe differences among them with the environment-specific latentrepresentation, to improve model generalization performance. Italso allows us to learn the exogenous factors of an unseen environ-ment based on only its leading observations. We now introduce theenvironment encoder in detail.The exogenous factors would pose influence on all agents withina system. On the one hand, they will influence the self-evolution ofeach individual agent. For example, temperatures would affect thevelocities of agents. On the other hand, they will influence the pair-wise interaction among agents. For example, temperatures wouldalso change the energy when two particles collide with each other.The environment encoder envenc therefore learns the latent represen-tation of exogenous factors by jointly consider the trajectoriesfrom all agents, i.e. envenc : 1:, . Specifically, we learn anenvironment-specific latent vector from the aforementioned tempo-ral graph in Sec 4.1 that is constructed from observed trajectories.The temporal graph contains both the information for each individ-ual trajectory and the mutual interaction among agents throughtemporal and spatial edges. To summarize the whole temporal graphinto a vector , we attend over the sequence representation for each trajectory introduced in Sec 4.1 as:",
  ",(5)": "where is a transformation matrix and the attention weightis computed based on the average sequence representation withnonlinear transformation similar as in Eqn (3). Note that we usedifferent parameters to compute the sequence representation asopposed to the initial state encoder. The reason is that the semanticmeanings of the two sequence representations are different: one isfor the latent initial states and another is for the exogenous factors. 4.2.1Time Invariance. A desired property of the learned represen-tation for exogenous factors is that it should be time-invarianttowards the input trajectory time window. In other words, for thesame environment, if we chunk the whole trajectories into severalpieces, the inferred representations should be similar to each otheras they are describing the same environment.To achieve this, we design a contrastive learning loss to guidethe learning process of the exogenous factors. As shown in ,we force the learned exogenous factor representations to be similarif they are generated based on the trajectories from the same en-vironment (positive pairs), and to be apart from each other if theyare from different environments (negative pairs). Specifically, wedefine the contrastive leanring loss as follows:",
  "SimilarApart": ": Temporal properties of the environment encoder.We use contrastive learning loss to force the latent exoge-nous factors learned from different windows within the sameenvironment to be close to each other, and from differentenvironments to be apart from each other. 4.2.2Orthogonality. GG-ODE features two encoders that take theinput of observed trajectories 1:, for learning the latent initialstates and the latent exogenous factors respectively. As they aredesigned for different purposes but are both learned from the same",
  "JSD(,) = [ sp((,))] [sp((,))],(7)": "where is the product of the marginal distributions and isthe joint distribution. () = (1 +) and is a discriminatormodeled by a neural network to compute the score for measuringtheir mutual information.According to recent literature , the sample pair (posi-tive pairs) (,) drawn from the joint distribution are differentrepresentations of the same data sample, and the sample pair (neg-ative pairs) drawn from are different representations fromdifferent data samples. We therefore attempt to minimize the mutualinformation from the two encoders as follows LMI = E, [((0, , ))] E, [((0, , ))](8)where is a MLP-based discriminator. Specifically, we force thelatent initial states 0,for all agents from environment to be dis-similar to the learned exogenous factors . And construct negativepairs by replacing the learned exogenous factors from another en-vironment as . The generation process for positive and negativepairs can be found in Appendix A.3.2.",
  "ODE Generative Model and Decoder": "4.3.1ODE Generative Model. After describing the initial state en-coder and the environment encoder, we now define the ODE func-tion that drives the system to move forward. The future trajectoryof each agent can be determined by two important factors: thepotential influence received from its neighbors in the interactiongraph and the self-evolution of each agent. For example, in then-body system, the position of each agent can be affected bothby the force from its connected neighbors and its current velocitywhich can be inferred from its historical trajectories. Therefore,our ODE function consists of two parts: a GNN that captures thecontinuous interaction among agents and the self-evolution of thenode itself. One issue here is how can we decide the neighbors foreach agent in the ODE function as the interaction graph is evolving,the neighbors for each agent are dynamically changing based ontheir current positions, which are implicitly encoded in their latentstate representations , , , . We propose to first decode the latent",
  "node representations , , ,with a decoding function dec to ob-": "tain their predicted positions , , ,at current timestamp. Thenwe determine their connectivity based on whether their Euclideandistance , = ||,, ||2 is within the predefined radius . Thiscan be computed efficiently by using a multi-dimensional indexstructure such as the - tree. The decoding function dec is thesame one that we will use in the decoder.To incorporate the influence of exogenous factors, we furtherincorporate into the general ODE function to improve modelgeneralization ability as:",
  "NGNN(, ,, ) + self(, )": ",= env(, ||)(9)where || denotes concatenation and GNN can be any GNN thatconducts message passing among agents. self, env are implementedas two MLPs respectively. In this way, we learn the effect of latentexogenous factors from data without supervision where the latentrepresentation is trained end-to-end by optimizing the predictionloss. 4.3.2Decoder. Given the ODE function and agents initial states0,for = 1, 2 , the latent trajectories for all agents are de-termined, which can be solved via any black-box ODE solver. Fi-nally, a decoder generates the predicted dynamic features based onthe decoding probability (, |, ) computed from the decodingfunction dec as shown in Eqn 10. We implement dec as a simpletwo-layer MLP with nonlinear activation. It outputs the mean ofthe normal distribution (, |, ), which we treat as the predictedvalue for each agent.",
  "Training": "We now introduce the overall training procedure of GG-ODE . Foreach training sample, we split it into two halves along the time,where we condition on the first half in order to predict dy-namics in the second half [+1, ]. Given the observed trajectories1:,, we first run the initial state encoder to compute the latentinitial state 0,for each agent, which is sampled from the approxi- mated posterior distribution 0,| 1:,. We then generatethe latent representations of exogenous factors from the envi-ronment via the environment encoder. Next, we run the ODEgenerative model that incorporates the latent exogenous factors tocompute the latent states for all agents in the future. Finally, thedecoder outputs the predicted dynamics for each agent.We jointly train the encoders, ODE generative model, and de-coder in an end-to-end manner. The loss function consists of threeparts: (1) the evidence lower bound (ELBO) which is the addition ofthe reconstruction loss for node trajectories and the KL divergenceterm for adding regularization to the inferred latent initial statesfor all agents. We use 0, to denote the latent initial state matrix ofall N agents. The standard VAE framework is trained to maximizeELBO so we take the negative as the ELBO loss; (2) the contrastivelearning loss for preserving the time invariance properties of thelearned exogenous factors; (3) the mutual information loss thatdisentangles the learned representations from the two encoders.1, 2 are two hyperparameters for balancing the three terms. Wesummarize the whole procedure in Appendix A.4.",
  "EXPERIMENTS5.1Experiment Setup": "5.1.1Datasets. We illustrate the performance of our model acrosstwo physical simulations that exhibit different system dynamicsover time: (1) The Water dataset , which describes the fluiddynamics of water within a container. Containers can have differ-ent shapes and numbers of ramps with random positions insidethem, which we view as different environments. The dataset is sim-ulated using the material point method (MPM), which is suitablefor simulating the behavior of interacting, deformable materialssuch as solids, liquids, gases 2. For each data sample, the number ofparticles can vary but the trajectory lengths are kept the same as600. The input node features are 2-D positions of particles, and wecalculate the velocities and accelerations as additional node featuresusing finite differences of these positions. The total number of datasamples (trajectories) is 1200 and the number of environments is68, where each environment can have multiple data samples withdifferent particle initializations such as positions, velocities, andaccelerations. (2) The Lennard-Jones potential dataset , whichdescribes the soft repulsive and attractive interactions between sim-ple atoms and molecules 3. We generate data samples with differenttemperatures, which could affect the potential energy preservedwithin the whole system thus affecting the dynamics. We viewtemperatures as different environments. The total number of datasamples (trajectories) is 6500 and the number of environments is65. Under each environment, we generate 100 trajectories withdifferent initializations. The trajectory lengths are kept the sameas 100. The number of particles is 1000 for all data samples. Moredetails about datasets can be found in Appendix A.1. 5.1.2Task Evaluation and Data Split. We predict trajectory rolloutsacross varying lengths and use Mean Square Error (MSE) as theevaluation metric.Task Evaluation. The trajectory prediction task is conducted un-der two settings: (1) Transductive setting, where we evaluate thetest sequences whose environments are seen during training; (2)Inductive setting, where we evaluate the test sequences whose en-vironments are not observed during training. It helps to test themodels generalization ability to brand-new systems.Data Split. We train our model in a sequence-to-sequence settingwhere we split the trajectory of each training sample into two parts and [+1, ]. We condition on the first part of observationsto predict the second part. To conduct data split, we first randomlyselect 20% environments whose trajectories are all used to constructthe testing set Inducttestin the inductive setting. For the remainingtrajectories that cover the 80% environments, we randomly splitthem into three partitions: 80% for the training set train, 10% for the validation set val and 10% for the testing set in the transduc-tive setting transtest . In other words, we have two test sets for theinductive and transductive settings respectively, one training setand one validation set. To fully utilize the data points within eachtrajectory, we generate training and validation samples by splittingeach trajectory into several chunks that can overlap with each other,using a sliding window. The sliding window has three hyperpa-rameters: the observation length and prediction length for eachsample, and the interval between two consecutive chunks (samples).Specifically, for the Water dataset, we set the observation length as50 and the prediction length as 150. We obtain samples from eachtrajectory by using a sliding window of size 200 and setting thesliding interval as 50. For the Lennard-Jones potential dataset, weset the observation length as 20, the prediction length as 50, and theinterval as 10. The procedure is summarized in Appendix A.1.1. Dur-ing evaluations for both settings, we ask the model to roll out overthe whole trajectories without further splitting, whose predictionlengths are larger than the ones during training. The observationlengths during testing are set as 20 for the Lennard-Jones potentialdataset and 50 for the Water dataset across the two settings.",
  "Baselines": "We compare both discrete neural models as well as continuous neu-ral models where they do not have special treatment for modelingthe influence from different environments. For discrete ones wechoose: NRI which is a discrete GNN model that uses VAE toinfer the interaction type among pairs of agents and is trained viaone-step predictions; GNS , a discrete GNN model that usesmultiple rounds of message passing to predict every single step;LSTM , a classic recurrent neural network (RNN) that learns thedynamics of each agent independently. For the continuous models,we compare with NDCN and Social ODE , two ODE-basedmethods that follow the encoder-processor-decoder structure withGNN as the ODE function. The initial state for each agent is drawnfrom a single data point instead of a leading sequence. CG-ODE which has the same architecture as our model, but with two coupledODE functions to guide the evolution of systems.",
  "Performance Evaluation": "We evaluate the performance of our model based on Mean SquareError (MSE) as shown in . As data samples have varyingtrajectory lengths, we report the MSEs over three rollout percent-ages regarding different prediction horizons: 30%, 60%, 100% where100% means the model conditions on the observation sequence andpredicts all the remaining timestamps.Firstly, we can observe that GG-ODE consistently outperforms allbaselines across different settings when making long-range predic-tions, while achieving competitive results when making short-rangepredictions. This demonstrates the effectiveness of GG-ODE inlearning continuous multi-agent system dynamics across environ-ments. By comparing the performance of LSTM with other methods,we can see that modeling the latent interaction among agents canindeed improve the prediction performance compared with pre-dicting trajectories for each agent independently. Also, we canobserve the performance gap between GG-ODE and other baselinesincrease when we generate longer rollouts, showing its expressive",
  "(c) Predictions of GG-ODE": ": Visualization of the transductive prediction results for the Water dataset. Black lines are ramps within the container.The length of the observation sequence is set as 20. GNS makes less accurate predictions compared with GG-ODE. power when making long-term predictions. This may be due to thefact that GG-ODE is a continuous model trained in a sequence-to-sequence paradigm whereas discrete GNN methods are only trainedto make a fixed-step prediction. Another continuous model NDCNonly conditions a single data point to make predictions for thewhole trajectory in the future, resulting in suboptimal performance.Finally, we can see that GG-ODE has a larger performance gainover existing methods in the inductive setting than in the transduc-tive setting, which shows its generalization ability to fast adapt toother unseen systems with a few data points. visualizes",
  "the prediction results under the transductive setting for the Waterdataset": "5.3.1Ablation Studies. To further analyze the rationality behindour model design, we conduct an ablation study by consideringthree model variants: (1) We remove the contrastive learning losswhich forces the learned exogenous factors to satisfy the time invari-ance property, denoted as /Lcontra; (2) We remove the mutualinformation minimization loss which reduces the variance of thelearned exogenous factors from the same environment, denotedas /L. (3) We share the parameters of the two encodersfor computing the latent representation for each observation",
  "Generalizing Graph ODE for Learning Complex System Dynamics across EnvironmentsKDD 23, August 610, 2023, Long Beach, CA, USA": "sequence in the temporal graph, denoted as shared encoders. Asshown in , all three variants have inferior performancecompared to GG-ODE , verifying the rationality of the three keydesigns. Notably, when making long-range predictions, removingL would cause more harm to the model than removing Lcontra.This can be understood as the latent initial states are more im-portant for making short-term predictions, while the disentangledlatent initial states and exogenous factors are both important formaking long-range predictions. 5.3.2Hyperparameter Study. We study the effect of 1/2, whichare the hyperparameters for balancing the two regularization termsthat guide the learning of the two encoders, towards making pre-dictions under different horizons. As illustrated in , theoptimal ratio for making 30%, 60%, 100% rollout predictions are 2,1,0.5 respectively, under both the transductive and inductive set-tings. They indicate that the exogenous factors modeling plays amore important role in facilitating long-term predictions, which isconsistent with the prediction errors illustrated in whencomparing /L with /Lcontra. However, overly elevat-ing L would also harm the model performance, as the timeinvariance property achieved by Lcontra is also important to guar-antee the correctness of the learned latent initial states, whichdetermines the starting point of the predicted trajectories in thefuture.",
  ": Effect of 1/2 on the Lennard-Jones potentialdataset. Best results are circled in red for each setting": "5.3.3Sensitivity Analysis. GG-ODE can take arbitrary observa-tion lengths to make trajectory predictions, as opposed to existingbaselines that only condition on observations with fixed lengths.It allows the model to fully utilize all the information in the past.We then study the effect of observation lengths on making pre-dictions in different horizons. As shown in , the optimalobservation lengths for predicting the rollouts with 20, 40, and 50steps are 20, 25, 35 in the inductive setting, and 15, 25, 30 in thetransductive setting. When predicting long-range trajectories, ourmodel typically requires a longer observation sequence to get moreaccurate results. Also, for making predictions at the same lengths,the inductive setting requires a longer observation length comparedwith the transductive setting.",
  ": Effect of observation length on the Lennard-Jonespotential dataset": "dataset. We first randomly choose one data sample for each of the65 temperatures and visualize the learned representations of ex-ogenous factors. As shown in (a), the representations ofhigher temperatures are closer to each other on the right half of thefigure, whereas the lower temperatures are mostly distributed onthe left half. Among the 65 temperatures, 20% of them are not seenduring training which we circled in black. We can see those unseentemperatures are also properly distributed, indicating the great gen-eralization ability of our model. We next plot the representationsfor all data samples under temperatures 2.5 and 3.5 respectivelyas shown in (b). We can see that the learned represen-tations are clustered within the two temperatures, indicating ourcontrastive learning loss is indeed beneficial to guide the learningprocess of exogenous factors.",
  "CONCLUSION": "In this paper, we investigate the problem of learning the dynamicsof continuous interacting systems across environments. We modelsystem dynamics in a continuous fashion through graph neuralordinary differential equations. To achieve model generalization,we learn a shared ODE function that captures the commonalitiesof the dynamics among environments while design an environ-ment encoder that learns environment-specific representations for",
  "Yupeng Gu, Yizhou Sun, and Jianxi Gao. 2017. The Co-Evolution Model for SocialNetwork Evolving and Opinion Migration. In KDD17": "Ehsan Hajiramezanali, Arman Hasanzadeh, Krishna Narayanan, Nick Duffield,Mingyuan Zhou, and Xiaoning Qian. 2019. Variational Graph Recurrent NeuralNetworks. In Advances in Neural Information Processing Systems 32. 1070110711. Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and MengWang. 2020. Lightgcn: Simplifying and powering graph convolution network forrecommendation. In Proceedings of the International ACM SIGIR Conference onResearch and Development in Information Retrieval. 639648. R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, PhilBachman, Adam Trischler, and Yoshua Bengio. 2018. Learning deep represen-tations by mutual information estimation and maximization. arXiv preprintarXiv:1808.06670 (2018).",
  "Zijie Huang, Yizhou Sun, and Wei Wang. 2021. Coupled Graph ODE for LearningInteracting System Dynamics. In Proceedings of the 27th ACM SIGKDD Conferenceon Knowledge Discovery & Data Mining": "Amin Javari, Zhankui He, Zijie Huang, Raj Jeetu, and Kevin Chen-Chuan Chang.2020. Weakly Supervised Attention for Hashtag Recommendation Using GraphData. In Proceedings of The Web Conference 2020 (WWW 20). 10381048. Song Jiang, Zijie Huang, Xiao Luo, and Yizhou Sun. 2023. CF-GODE: Continuous-Time Causal Inference for Multi-Agent Dynamical Systems. In Proceedings of the29th ACM SIGKDD Conference on Knowledge Discovery & Data Mining. John Edward Jones. 1924. On the determination of molecular fields.I. Fromthe variation of the viscosity of a gas with temperature. Proceedings of the RoyalSociety of London. Series A, Containing Papers of a Mathematical and PhysicalCharacter 106, 738 (1924), 441462.",
  "Liang Qu, Huaisheng Zhu, Qiqi Duan, and Yuhui Shi. 2020. Continuous-time linkprediction via temporal dependent graph neural network. In The Web Conference(WWW). 30263032": "Yulia Rubanova, Ricky T. Q. Chen, and David K Duvenaud. 2019. Latent OrdinaryDifferential Equations for Irregularly-Sampled Time Series. In Advances in NeuralInformation Processing Systems 32. 53205330. Eduardo Hugo Sanchez, Mathieu Serrurier, and Mathias Ortner. 2020. Learningdisentangled representations via mutual information estimation. In ComputerVisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020,Proceedings, Part XXII 16. Springer, 205221. Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, JureLeskovec, and Peter W. Battaglia. 2020. Learning to Simulate Complex Physicswith Graph Networks. In Proceedings of the 37th International Conference onMachine Learning, ICML 2020, 13-18 July 2020, Virtual Event. 84598468.",
  "Petar Velikovi, Guillem Cucurull, Arantxa Casanova, Adriana Romero, PietroLi, and Yoshua Bengio. 2018. Graph Attention Networks. ICLR18 (2018)": "Ruijie Wang, Zijie Huang, Shengzhong Liu, Huajie Shao, Dongxin Liu, Jinyang Li,Tianshi Wang, Dachun Sun, Shuochao Yao, and Tarek Abdelzaher. 2021. DyDiff-VAE: A Dynamic Variational Framework for Information Diffusion Prediction.In Proceedings of the 44th International ACM SIGIR Conference on Research andDevelopment in Information Retrieval (SIGIR 21). 163172. Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019.Neural graph collaborative filtering. In Proceedings of the international ACM SIGIRconference on Research and development in Information Retrieval. 165174. Song Wen, Hao Wang, and Dimitris Metaxas. 2022. Social ODE: Multi-agentTrajectory Forecasting with Neural Ordinary Differential Equations. In ComputerVisionECCV 2022: 17th European Conference, Tel Aviv, Israel, October 2327, 2022,Proceedings, Part XXII. Springer, 217233.",
  "Chengxi Zang and Fei Wang. 2020. Neural dynamics on complex networks.In Proceedings of the 26th ACM SIGKDD International Conference on KnowledgeDiscovery & Data Mining. 892902": "Tianxiang Zhao, Xiang Zhang, and Suhang Wang. 2021. Graphsmote: Imbalancednode classification on graphs with graph neural networks. In Proceedings of theACM International Conference on Web Search and Data Mining. 833841. Wei Zhu, Haitian Zheng, Haofu Liao, Weijian Li, and Jiebo Luo. 2021. Learningbias-invariant representation by cross-sample mutual information minimization.In Proceedings of the IEEE/CVF International Conference on Computer Vision. 1500215012.",
  "end": "A.1.2Input Features and Prediction Target. For the Water dataset,the input node features are 2-D positions , , and we additionallycalculate the 2-D velocities and accelerations using finite differ-ences of these positions as ,= , 1,, = , 1,=, 21,+ 2,. For positions, velocities, and accelerations,we precompute their mean and variance across all samples and nor-malize them with z-score. For the Lennard-Jones potential dataset,the input node features are 3-D positions, velocities, and accelera-tions. We train the model to predict the future positions for eachagent along the time for both datasets.",
  "A.2Software and Experiment Environment": "We implement our model in PyTorch. All experiments are con-ducted on a GPU powered by an NVIDIA A100. For all datasets,we train over 100 epochs and select the one with the lowest vali-dation loss as the reported model. We report the average results over 10 runs. Encoders, the generative model, and the decoder arejointly optimized using Adam optimizer with a learning rate0.005. The batch size for the Water dataset is set as 128, and for theLennard-Jones potential dataset. is set as 256. Note that the batchsize denotes the number of data samples generated as in Alg 1.",
  "A.3Implementation Details": "We now introduce the implementation details of our model.A.3.1Initial State Encoder. The initial state encoder aims to inferlatent initial states for all agents simultaneously via a two-step pro-cedure: Firstly, the encoder computes the structural representationfor each observation node by the use of a spatial-temporal GNN.We set the number of GNN layers as 2 and the hidden dimensionas 64 across all datasets. LayerNorm is employed to providetraining stability in our experiment. Next, a self-attention-basedsequence representation learning procedure computes the sequencerepresentation for each agent and samples the initial state from it.We use a 2-layer MLP as trans in Eqn 4 with latent dimensions as128 and activation function as Tanh. A.3.2Environment Encoder. The environment encoder learns thelatent representations of exogenous factors based on the observedtrajectories. The architecture is the same as the initial state encoderbut are using two sets are parameters with the same hyperparametersettings introduced in Sec A.3.1.Contrastive Learning Loss Sampling. The contrastive learningloss Lcontra shown in Eqn 6 is designed to achieve the time in-variance properties of the learned exogenous factors. Specifically,we sample the positive pairs 1:2,,3:4, using two strategies:(1) The intra-sample generation, where 1:2,,3:4, are from thesame training sample but representing two different time windows.We achieve this by randomly selecting two timestamps within eachtraining sample to serve as 1,3 respectively, and then set the win-dow size as the observation length to get 2 = 1 + ,4 = 3 + .(2) The cross-sample generation, where 1:2,,3:4, are from twodifferent samples within the same environment . Specifically, foreach training sample, we first randomly choose another sampleunder the same environment. Then we generate 1,3 by randomlyselecting one timestamp for each of them. Finally, we calculate 2,4by adding the observation length. To generate negative pair 5:6, for each 1:2,, we first randomly select one another environment, from which we randomly pick one data sample. Similarly, wethen randomly select one timestamp within that data sample toserve as 5 and then obtain 6 as 6 = 5 + . The temperature scalar in Eqn 6 is set as 0.05.Mutual Information Minimization Loss Sampling. To disentan-gle the representations of the latent initial states and the exogenousfactors, we design the mutual information minimization loss inEqn 8 as a regularization term during training. We conduct thesampling procedure for positive and negative pairs as follows: Foreach training sample, we pair the latent initial states 0,of all the agents with the learned exogenous factors , thus constructing positive pairs. To generate negative pairs, we randomly selectanother environment and pair it with the latent initial states ofall agents within one training sample. Thus we obtain the same",
  "(13)": "where || denotes concatenation, 1 , 1 , 2 are two-layer MLPs withhidden dimension size of 64. We use z2(,)as output representationfor agent j at timestamp t from GNN. The self-evolution functionself and the transformation function env are also implemented astwo-layer MLPs with hidden dimension of 64. We use the fourth-order Runge-Kutta method from torchdiffeq python package asthe ODE solver, which solves the ODE systems on a time grid thatis five times denser than the observed time points. We also utilizethe Adjoint method described in to reduce memory usage."
}