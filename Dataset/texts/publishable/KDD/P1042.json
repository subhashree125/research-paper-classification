{
  "ABSTRACT": "Knowledge distillation (KD) has emerged as a promising techniquefor addressing the computational challenges associated with deploy-ing large-scale recommender systems. KD transfers the knowledgeof a massive teacher system to a compact student model, to reducethe huge computational burdens for inference while retaining highaccuracy. The existing KD studies primarily focus on one-time dis-tillation in static environments, leaving a substantial gap in theirapplicability to real-world scenarios dealing with continuously in-coming users, items, and their interactions. In this work, we delveinto a systematic approach to operating the teacher-student KDin a non-stationary data stream. Our goal is to enable efficient de-ployment through a compact student, which preserves the highperformance of the massive teacher, while effectively adapting tocontinuously incoming data. We propose Continual CollaborativeDistillation (CCD) framework, where both the teacher and the stu-dent continually and collaboratively evolve along the data stream.CCD facilitates the student in effectively adapting to new data, whilealso enabling the teacher to fully leverage accumulated knowledge.We validate the effectiveness of CCD through extensive quanti-tative, ablative, and exploratory experiments on two real-worlddatasets. We expect this research direction to contribute to narrow-ing the gap between existing KD studies and practical applications,thereby enhancing the applicability of KD in real-world systems.",
  "INTRODUCTION": "Recommender systems (RS) have been used in diverse industrialplatforms to enhance user experience, foster loyalty, and contributeto business success . In recent years, increasingly largeand sophisticated recommendation models, from graph neuralnetworks to large language models , have beendeveloped to identify users intricate preferences. Furthermore,these large-scale models are often employed concurrently throughmodel ensembles to further improve recommendation performance. However, this improved performance comes at the cost ofincreased computations, memory resources, and inference latency,which poses significant challenges for deployment in real-timeservices and resource-constrained environments .To overcome this issue, recent studies have employed knowl-edge distillation (KD) (a). KD serves as a modelcompression technique that transfers knowledge from a massivesystem (teacher) into a compact model (student), aiming to generatea model that achieves both high effectiveness and efficiency. It firstconstructs a massive teacher system, which often comprises multi-ple large-scale models, to attain high performance . Then,during the distillation process, the student is trained to replicatethe high-quality recommendations from the teacher system, whichare derived from its massive capacity. After the distillation, thestudent becomes capable of achieving comparable performance tothe teacher . Furthermore, the student has significantlyreduced inference latency, making it suitable for deployment.However, existing KD studies have focused on one-time distil-lation in static environments, overlooking real-world scenarioshandling a non-stationary data stream where new users, items,and interactions are continuously incoming. A naive approach toapplying KD in the data stream is to repeatedly update the teacherand generate a new student via distillation each time new dataarrives. However, this approach raises two critical problems: First,the massive teacher system cannot be updated frequently, as train-ing large-scale models requires significant time and resources .Consequently, the deployed student remains static until the nextteacher update cycle, failing to provide recommendations that re-flect up-to-date interactions as well as new users and items. Second,conducting the distillation independently each time fails to fullyleverage the previously generated knowledge along the data stream.This results in inefficient training and suboptimal performance.A well-established approach for training a model with a non-stationary data stream is continual learning (CL) (b). When trained with the data stream, an ideal model shouldseamlessly adapt to newly incoming data without forgetting pre-vious knowledge from historical data. CL aims to strike a balance",
  "KDD 24, August 2529, 2024, Barcelona, SpainGyuseok Lee, SeongKu Kang, Wonbin Kweon, and Hwanjo Yu": "Kian Ahrabian, Yishi Xu, Yingxue Zhang, Jiapeng Wu, Yuening Wang, and MarkCoates. 2021. Structure aware experience replay for incremental learning ingraph-based recommender systems. In CIKM. 28322836. Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and XiangnanHe. 2023. Tallrec: An effective and efficient tuning framework to align largelanguage model with recommendation. In Proceedings of the 17th ACM Conferenceon Recommender Systems. 10071014.",
  "RELATED WORK": "Knowledge distillation (KD). KD serves as a model compressiontechnique that transfers knowledge from a large teacher modelto a lightweight student model . In recent years, the in-ference costs of recommender systems (RS) have progressivelyincreased, presenting challenges for their practical deployment.Consequently, KD has gained much research attention to reduceinference costs while maintaining high recommendation perfor-mance . Earlier work transfers the point-wiseimportance of top-ranked items by the teacher, and proposesa topology distillation that transfers relational knowledge from theteacher representation space. Recent studies havedelved into list-wise distillation to transfer the ranking orders ofitems directly. They formulate the distillation as a ranking matchingproblem and train the student to preserve the teachers permutation.This approach has shown state-of-the-art performance in variousranking-oriented applications such as recommendation and document retrieval . Furthermore, considering the re-markable performance of a massive system consisting of multiplelarge-scale models, introduce distillation methods tailoredfor compressing knowledge of an ensembled system. These KDmethods have greatly alleviated the huge computational burdensof deploying a large-scale RS, thereby expanding its applicabilityto various environments.However, existing KD studies have focused on one-time distil-lation in static environments, leaving a substantial gap in theirapplication to real-world scenarios with continuously incomingdata. In this work, we delve into a systematic approach to operatingthe teacher-student KD in a non-stationary data stream.Continual learning (CL). Continual learning , alsoknown as lifelong learning or incremental learning, is a concept totrain a model with a non-stationary data stream. A major challengeis to strike a balance between plasticity and stability , whereplasticity refers to the ability to learn new knowledge, and stabil-ity focuses on retaining previous knowledge. Naively updating amodel on new data often fails to achieve this balance, resulting incatastrophic forgetting of its previous knowledge . Two popularCL approaches are regularization and experience replay. The regularization approach typically imposes regulariza-tion constraints on the parameter space, discouraging the modelparameters from drastically changing from the previously trained",
  "Continual Collaborative Distillation for Recommender SystemKDD 24, August 2529, 2024, Barcelona, Spain": "LWC-KD-PIW is the state-of-the-art regularization-basedCL method for recommendation. It applies the regularization(i.e., LWC-KD ) with personalized weights, adjusting theregularization effects considering the dynamics of each user. ReLoop2 is the state-of-the-art replay-based CL method forrecommendation. It introduces self-correcting learning using anerror memory that stores samples that the model failed to predict.For the student, we further devise two baselines that represent thebest solution that combines the existing KD and CL methods; alongthe data stream, we update the teacher using the state-of-the-artCL method (i.e., LWC-KD-PIW). Then, for each block, we generatea compact student via KD , which is subsequently updated byeither Fine-tune or LWC-KD-PIW, referred to as KD + Fine-tuneand KD + LWC-KD-PIW, respectively. Lastly, our approach is:",
  "PROBLEM FORMULATION3.1Concept Definition": "Definition 1 (Teacher-Student Knowledge Distillation). A mas-sive teacher recommender system , which typically comprisesseveral large-scale models, achieves high performance through itslarge capacity. However, it also incurs high computational costsfor inference. KD is employed to compress into a lightweightstudent model . The student model has significantly reducedinference latency, making it well-suited for real-time services andresource-constrained environments.Definition 2 (Recommendation with Data Stream). A real-worldrecommender system operates in a non-stationary data streamwhere new users, items, and interactions are continuously incoming.In many practical scenarios, a fixed-size time window of recent datais employed to update the system . Naturally, the data stream is viewed as consecutive blocks of interaction data [1, 2, ..., ]with a certain time interval . corresponds to set of interactionscollected from the timestamp 1 to , where = 1 + . Eachblock serves as training data for updating the system at each timesegment. At -th block, the system optimizes its performance on while leveraging the knowledge obtained from the previousblocks, 1, ..., 1. Note that the system only uses the most recentblock for training without directly accessing the previous blocks. The performance is evaluated across the entire timeline.Definition 3 (Teacher/Student update cycle). An update cycle fora system signifies the frequency at which the system is updated toadapt newly incoming interaction data . It is often empiricallydecided considering various factors such as available resources andthe amount of collected data. Let and denote update cyclesfor the teacher and the student, respectively. In practice, is muchlarger than , as updating the teacher system requires more timeand resources compared to the student model. For instance, onemight opt for a weekly update cycle for the teacher ( = 7 days)while allowing the student to update on a daily basis ( = 1 day).It is important to note that we define the data blocks in terms ofthe teacher system, thus the time interval is defined as = .",
  "Problem Definition": "We delve into a systematic approach to operate teacher-studentKD in a non-stationary data stream. Our goal is to enable efficientdeployment through a lightweight , preserving the high accuracyof while effectively adapting to continuously incoming data.This problem has two unique desiderata:(1) The student should effectively adapt to continuously incomingdata by itself during . That is, while the teacher remains static,the student should cope with evolving preferences as well asnew users and items. However, due to its limited capacity, naiveupdates of the student often lead to ineffective adaptation anda substantial loss of previously acquired knowledge. (2) The knowledge of teacher and student should be effectivelyaccumulated and leveraged throughout the data stream. Theconventional KD approach conducts distillation independentlyfor each time segment. This cannot fully leverage the knowledgegenerated from previous blocks, leading to inefficient trainingand suboptimal performance.Lastly, it is worth noting that we aim for a model-agnostic solution,enabling service providers to choose their preferred model for boththe teacher and the student according to their environments.",
  "METHODOLOGY4.1Overview": "We introduce a new Continual Collaborative Distillation (CCD)framework, where both the teacher and the student evolve collab-oratively along the non-stationary data stream. Assume that wehave updated using up to ( 1)-th data block. During thesubsequent teacher update cycle (i.e., during collecting -th blockfor teacher update), CCD follows three consecutive stages:(1) Student model generation (4.2). We generate a compactstudent model by compressing through KD. The studentmodel is used for deployment. (2) Continual update of student (4.3). The student model iscontinually updated using non-stationary data with the cy-cle . This process allows recommendations to aptly reflectnew users, new items, and up-to-date interactions. We intro-duce new embedding initialization and proxy-guided replaylearning strategies to improve the training while preventingthe loss of previous knowledge. (3) Teacher system update (4.4). When the teachers next updatecycle arrives, is updated using new interaction data accumulated during the cycle. We propose a new strategy forselectively harnessing the knowledge obtained from the studentside to further enhance .For each data block, CCD iterates these three consecutive stages(4.2-4.4). In the subsequent sections, we explain how CCD oper-ates for the -th data block. provides an overview of CCD.",
  ": Overview of CCD framework for -th data block": "each user-item pair, i.e., : U I R, where U and I denotethe set of users and items, respectively.In CCD framework, any off-the-shelf KD technique can be flex-ibly employed to generate the student model. In this work, weemploy the recent list-wise distillation that trains the stu-dent to emulate the item permutation (i.e., ranking orders) predictedby the teacher. Let denote the item ranking list for each user predicted by . The list-wise distillation loss is defined as thenegative log-likelihood of permutation probability 1:",
  "Stage 2: Continual update of student withincoming interactions": "As cannot be updated during its cycle , the student modelneeds to effectively adapt to continuously incoming data by itself.Specifically, the student is updated per its own cycle ( ).In this section, we explain how we update the student model.We first present how CCD learns with new data to provide rec-ommendations reflecting up-to-date interactions (4.3.1). Then, weintroduce a new proxy-guided replay learning to support effec-tive training by preventing the loss of previous knowledge (4.3.2).Lastly, we summarize the overall training objective (4.3.3). 4.3.1Learning with new interactions. We update the studentmodel using new interactions collected for the current data block.Here, an important challenge arises when dealing with new entitiesthat did not exist in the previous data blocks. In practical applica-tions, new users and new items continuously emerge. However, dueto the limited capacity, it is challenging for the student to effectivelylearn their preference from scratch.To facilitate learning for these new entities, we introduce a simpleyet effective entity embedding initialization technique. Let denotethe user-item bipartite graph, where nodes are users and items, andedges represent their interactions observed within the current data",
  "As this KD technique is not our contribution, we provide its details in Appendix A.1.Note that CCD framework is not dependent on this specific KD technique": "block. For a given node , we denote N as the set of -hop neigh-boring nodes in the graph. Recent studies have utilized the1-hop neighbors to generate embeddings for new entities. Specif-ically, the embeddings are initialized by aggregating the directlyinteracted entity embeddings, i.e., e = ({e : N1 }).However, for new entities, 1-hop interactions are typically highlylimited. To supplement limited interactions, we leverage the 2-hoprelations (e.g., items purchased together, users who bought the sameitem) based on the frequency information from the current datablock. Formally, we identify the set of prominent users and items,denoted as PU and PI, which have top interaction frequency inthe current block. These entities reveal the up-to-date prominenttrends, such as highly in-demand or trending items, assisting incomprehending newly emerging entities. We initialize embeddingsfor new users and items as follows:",
  "where denotes the sigmoid function. , = (,) denotes theranking score predicted by the student": "4.3.2Proxy-guided replay learning. Naively updating a modelwith newly incoming interactions can result in catastrophic for-getting, where the model significantly loses previously acquiredknowledge . This largely degrades the overall performanceof the model and further hinders the learning of new data. Further-more, the student model has a highly limited capacity, which makesit more challenging to update effectively.As a solution, we introduce a new proxy-guided replay learn-ing, which employs an external memory called proxy to assist thestudents learning. The learning theory in neuroscience positsthat humans do effective learning through two complementary sys-tems: a fast learning system for short-term adaptation to specific",
  "= (1 ) 1 + .(4)": "denotes the parameters of the distilled student model from 4.2.and denote the parameters for the stability and plastic-ity proxies that support training on -th block, respectively. Thescalar weights are hyperparameters following the relationship:0 < < 1. Therefore, the stability proxy is slowlyupdated with the long-term retention of previous knowledge, whilethe plasticity proxy is rapidly updated with an emphasis on recenttrends. These distinct proxies provide complementary views forthe previously acquired knowledge (). Note that proxies areupdated once per block, right after KD from the teacher system.Replay learning for forgotten knowledge. Using the proxies,we identify forgotten preference knowledge and train the student torecover it by replaying past predictions. A recommendation modelproduces a ranked list of items for each user. In the list, the top-ranked items correspond to the most probable predictions based onthe models knowledge of the user preference. In this regard, weidentify the forgotten knowledge that was previously captured butis not well-reflected in the current model based on rank disparitiesin the ranking lists; if items that were previously ranked near thetop now hold significantly lower ranks, it may suggest that themodel has forgotten knowledge related to them. Given a model , rank, denotes the rank of item in the rankinglist of user . A lower value indicates a higher ranking position, i.e.,rank, = 0 is the highest rank. For each item , the rank disparitywith respect to another model is defined as follows:",
  "(, ) = exp( (rank, rank,)),(5)": "where > 0 is a hyperparameter to control the sharpness of thedistribution. We utilize the exponential function to put a strongeremphasis on items with large rank disparities. A high value of (, ) indicates that item is ranked significantly higher bymodel compared to model .We create top- recommendation lists for each user from the stu-dent and proxy models, then construct item sets for replay learningbased on the rank disparity. Specifically, we obtain by samplingitems from the top- list of , using a probability distribution() (,). Similarly, we acquire by sampling itemsfrom the top- list of , using a distribution () (,).Then, we train the student to recover previous knowledge on theidentified items by replaying the predictions from the proxies:",
  "(,, , )).(6)": "(, ) denotes the error function between two predictions. In thiswork, we employ the simple binary cross-entropy loss. It is worthnoting that I and I are dynamically constructed based on thecurrent state of the student, enabling the effective identification andrecovery of forgotten knowledge. This is an important distinctionfrom the existing replay-based CL methods for RS , whichleverage a pre-constructed set of historical data without consideringthe current state of the student.",
  "Stage 3: Teacher system update": "Once the teacher update cycle arrives (i.e., the -th block has beencompletely collected), we update . To update a system on thedata stream, two types of losses are typically employed : (1)collaborative filtering loss to learn new user-item interactions, and(2) continual learning loss to mitigate forgetting of previous systemknowledge. We also employ these two objectives. Moreover, weintroduce an additional objective to fully leverage the knowledgeobtained from the student side. 4.4.1Leveraging the student-side knowledge. We argue that can be further improved by leveraging knowledge obtainedfrom the student-side in two aspects: First, the updated studentmodel contains up-to-date knowledge of the current data block,unlike which has not been updated for a while. In particular,using the student model, we can obtain potential interactions thatare not directly observed in the current block but are likely tobe observed in the future. Second, the proxies, which accumulateknowledge acquired throughout the data stream, serve as valuableknowledge sources to mitigate catastrophic forgetting. While it is",
  "Update the teacher system Eq.9": "possible to consider utilizing separate proxies for , given thelarge size of , employing the student-side proxies through KDis much more cost-effective in terms of both space and time.During the teacher training, we selectively leverage the mostconfident predictions from the student-side models (i.e., the stu-dent and two proxies) based on the rank disparity. Specifically, weidentify items ranked near the top by the student-side models butassigned significantly lower rankings by . From the top- listof each student-side model {,, }, we obtain bysampling items using a probability distribution () (,).The student-side knowledge is transferred to the teacher as follows:",
  "min L + L + ST LST.(9)": "denotes the training parameters of . L is the originalcollaborative filtering loss (e.g., binary cross-entropy) used to train . L is the continual learning loss. CCD does not require aspecific CL technique, and various CL methods for RS can be flexiblyemployed. In this work, we use the recently proposed method . and ST are the hyperparameters balancing the losses. As evolves during its training, we gradually reduce the impact of thestudent-side knowledge. We use a simple annealing schedule, whereits impact at -th epoch is controlled as ST = 0ST /. Here,0ST controls the initial impact, and controls the annealing speed.The overall training process is outlined in Algorithm 1.Remarks. A key aspect of CCD involves the collaborative evolutionof the teacher and student. To elaborate, we enhance the student byeffectively adapting to new entities and mitigating the forgettingproblem using proxies (4.3). This enhanced student knowledge isthen subsequently harnessed to improve the teacher (4.4), whichin turn allows for generating a more powerful student (4.2).",
  "# of interactions151,084 51,28034,50133,59337,97034,189": "(1, ..., 5), according to the temporal timestamp . For eachincremental block, we randomly divide the interactions of eachuser into train/validation/test sets in 80%/10%/10% split. presents the statistics of each block. 5.1.2Teacher-student KD setup. As our focus is on operatingthe teacher-student KD in a data stream, we closely follow the setupof the existing KD studies . For teacher, we construct amassive system by an ensemble of more than five large models .We increase the capacity of the teacher system until its performanceis no longer improved. For student, we employ two widely usedbackbone models: a matrix factorization (MF)-based model anda graph neural network (GNN)-based model . We set a smallembedding size for the student (8 for Yelp, and 16 for Gowalla),considering the teacher size for each dataset . The studentupdate cycle is set as one-tenth of the teacher cycle. Note that toensure consistent evaluation for both the teacher and student, allmodels are assessed on the same test set after training with allinteractions within each data block. Further details on KD setupincluding teacher configuration, and the number of parameters areprovided in Appendix A.2.2. 5.1.3Evaluation protocol. We closely follow the evaluationprotocol of the existing CL methods . All models arefirst trained on 0, then continually updated using the incrementalblocks. Specifically, at -th block, the model is updated using ,while access to past blocks 0, ..., 1 is forbidden .The evaluation of CL focuses on assessing how effectively amodel achieves a balance between plasticity and stability. We em-ploy two CL metrics proposed in . We construct the matrix R, where each element denotes the recommendationperformance on block after completing training on block . Aftertraining on each -th block, we report two metrics: Learning Average (LA) = 1",
  "Retained Average (RA) = 1": "=1 ,. It assesses how well amodel retains past knowledge, focusing on the stability aspect.A models overall capability is summarized by the harmonic meanof LA and RA, denoted by H-mean . To evaluate the recom-mendation performance, we employ Recall@20 and NDCG@20. We report the results from five independent runs. 5.1.4Baselines. To evaluate CCD, which trains both the teacherand student in a data stream, we compare various CL approaches. Full-Batch uses all the historical data to train the model fromscratch. We report its results solely for reference purposes.",
  "CCD trains the teacher and student collaboratively along thedata stream. We use the same KD technique for stage 1 (Lin Eq.1) and LWC-KD-PIW for stage 3 (L in Eq.9)": "5.1.5Implementation Details. We utilize PyTorch with CUDAfrom RTX A6000 and AMD EPYC 7313 CPU. All hyperparametersare tuned via grid search on the validation set. The learning rate and2 regularization for the Adam optimizer are chosen from {0.001,0.005, 0.01} and {0.0001, 0.0005, 0.001}, respectively. For hyperpa-rameters related to the list-wise KD, we follow the setup from .The impacts of continual learning is chosen from {0.001, 0.01,0.1}. For CCD, and for updating stability and plasticityproxies are chosen from {0.0, 0.1} and {0.9, 1.0}, respectively. Thenumber of item samples , , is chosen from {1, 3, 5}. Theremaining hyperparameters of CCD are set to their default values.The number of prominent users PU and items PI are set to 20.The rank disparity-based sampling is conducted from the top-50list of each model (i.e., = 50). We set = 103, = 5, = 0.5,and 0ST = 0.5. For baseline-specific hyperparameters, we followthe search ranges provided in the original papers.",
  "Performance Comparison": "5.2.1Main results. and show the overall per-formances of the teacher systems and student models optimizedby each compared method. Overall, CCD performs better than allbaselines in terms of both adapting to new data (LA) and retain-ing previous knowledge (RA), achieving a good balance betweenthem (H-mean). Also, CCD consistently improves both the teacherand student, with the improvements often gradually increasingthroughout the data stream (). We analyze the results fromvarious perspectives:Teacher-side. Teacher systems trained with CCD consistentlyachieve higher H-mean across all data blocks compared to state-of-the-art CL methods, including both regularization-based (i.e.,LWC-KD-PIW) and replay-based (i.e., ReLoop2) methods. Unlikethe CL methods that update the teacher by itself, CCD additionallyleverages the student-side knowledge. As discussed in 4.4.1, thestudent-side models provide up-to-date knowledge from the up-dated student as well as historical knowledge accumulated via twocomplementary proxies. This can aid the teacher in adapting to newdata more effectively while retaining past knowledge. Interestingly,we observe that the forgetting phenomenon is exacerbated through the ensemble for the teacher. We conjecture that this amplificationcould stem from the combined effect of individual model forgettingwithin the ensemble, leaving further investigation for future study.Student-side. Student models trained with CCD consistently achievehigher H-mean compared to both state-of-the-art CL methods andthe KD-enhanced variants (i.e., KD + Fine-Tune/LWC-KD-PIW).Overall, methods that leverage KD from the teacher systems (i.e.,KD + Fine-Tune/LWC-KD-PIW, CCD) achieve higher performancethan the remaining methods training the student by itself. Thisshows the importance of leveraging extensive teacher knowledge.Moreover, we observe that the effectiveness of the CL methods israther limited for the student models, particularly for the latterblocks (i.e., D4 and D5). The student has a highly limited capacityand learning capability, making it more challenging to update ef-fectively. This aspect has not been explicitly considered in previousCL methods. CCD introduces two new strategies, entity embeddinginitialization and proxy-guided replay learning, which effectivelysupport the students learning process.It is also noted that CCD even generally outperforms Full-Batchin terms of LA. Full-Batch is certainly the strongest baseline forRA (stability), as it learns from all observed historical data. How-ever, it is not always the strongest method for LA (plasticity), asit treats all data equally without putting emphasis on the latestinteractions. CCD effectively learns the latest interactions whilepreserving previous knowledge, which leads to enhanced perfor-mance in both LA and RA.Collaborative evolution. presents the absolute H-meangain of CCD over the best CL competitor. We report the results ofRecall@20 with the GNN-based student. In CCD, the teacher and thestudent collaboratively improve each other by mutual knowledgeexchange (4.2, 4.4). These improvements accumulate over time,progressively refining both the teacher and student models. As aresult, the performance gap between CCD and the CL competitorgenerally widens over time, which ascertains the collaborativeevolution in CCD. 5.2.2Accuracy-efficiency comparison. compares theaccuracy and efficiency aspects of teacher and student, trained bythe best CL method (i.e., LWC-KD-PIW) and CCD, respectively. Weincrease the student size until it achieves comparable performanceto the teacher. Then, we report the average H-mean (Recall@20)for all blocks, the number of parameters, and the time required fortraining and inference. We utilize PyTorch with CUDA from RTXA6000 and AMD EPYC 7313 CPU. Compared to the massive teachersystem which has significant computational costs for training andconsolidating the multiple large-scale models, the compact studentmodel trained with CCD significantly reduces the huge compu-tational burdens while maintaining the high performance of theteacher system.Lastly, we highlight that the KD process takes negligible timecompared to the teacher update. Specifically, on the Yelp dataset,KD takes about 8 minutes, whereas the teacher update takes about107 minutes. Considering the student update takes about 5 minutes,the KD process, followed by student updates, is more efficient thanrepetitive updates of the large-scale teacher, making it more suitablefor real-time applications.",
  "We provide comprehensive analyses for an in-depth understandingof CCD. We report the results with the GNN-based student on Yelp": "5.3.1Ablation study. We present the ablation study of CCD todemonstrate the effectiveness of each proposed component.Teacher side. presents the ablation results on the teacher.From 4.4, we exclude three proposed components: (1) w/o student-side knowledge excludes LST, (2) w/o proxies (student only)only uses the updated student excluding the proxies in LST, and(3) w/o annealing excludes the loss annealing. First, excludingthe student-side knowledge largely degrades the effectiveness ofCCD, and the best performance is achieved by leveraging both theupdated student and the proxies accumulating the historical knowl-edge. Also, the simple annealing that gradually reduces the impactsof the student-side knowledge effectively improves the teacher.Student side. presents the ablation results on the stu-dent. First, we examine the effects of proxy-guided replay learning(4.3.2). w/o proxy learning excludes L, and w/o S-/P-proxyexclude the stability proxy and the plasticity proxy, respectively.We observe that both proxies are indeed beneficial for the student,as they provide complementary views of the previous knowledge,as shown in . Second, we investigate the effect of and, used to update the proxies in Eq.4. We assess the extremecase for each proxy. When = 0, the stability proxy does notaccept any recent knowledge (i.e., extreme stability). Conversely,when = 1, the plasticity proxy is entirely replaced by therecent knowledge (i.e., extreme plasticity). The best performanceis achieved when both proxies are updated with an appropriatebalance. However, as long as the proxies accumulate past knowl-edge, the choice of update weights has little impact on the finalperformance. We set = 0.1 and = 0.9, respectively. 5.3.2Impacts of the replay size. presents the perfor-mance (Recall@20) of the student model trained with varying replaysizes in Eq.6. The best performance is achieved with a small sam-pling size ( 3), suggesting that our proxy-guided replay learningdoes not notably increase the training complexity. 5.3.3Analysis on dormant/new users. We further analyze therecommendation quality of the student for specific user groupshaving distinct characteristics. In , we present the recom-mendation performance for dormant users who havent used the |ISP| |IPP| LA 0.1060 0.1068 0.1076 |ISP| |IPP| RA 0.0860 0.0868 0.0876",
  "w/ random init.0.12700.05460.13870.0573w/ one-hop init.0.11160.04360.10370.0420": "system for an extended period. We select users who were active in1, remained dormant from 2 to 4, and became active again in5. We evaluate performance for the users on the test set of 5, afterupdating the student up to 4. We compare the baselines that showhigh effectiveness in previous experiments. We observe that CCDgenerates more accurate recommendations for the dormant users,indicating that CCD effectively preserves previous knowledge.In , we present the average recommendation perfor-mance for new users for each data block. CCD proposes to usethe prominent-entity information to facilitate the understanding ofnew entities (4.3.1). To assess its effectiveness, we report the resultswhen replacing the proposed technique with conventional randominitialization and the widely used 1-hop initialization . Theproposed technique effectively enhances the students ability toadapt to new entities. Moreover, as the teacher subsequently lever-ages the student-side knowledge, it leads to further improvementin teacher performance. These results collectively show that CCDachieves a good balance between stability and plasticity.",
  "CONCLUSION": "We propose CCD framework for effective and efficient deploymentthrough a compact student model having the high performance ofthe massive teacher system, while aptly adapting to continuouslyincoming data. Unlike the existing KD studies have focused onone-time distillation in static environments, CCD updates both theteacher and the student continually and collaboratively along thedata stream. CCD facilitates the students effective adaptation tonew data, while also enabling the teacher to fully leverage accu-mulated knowledge. Given its great compatibility with existingmodels, we expect that our CCD framework can provide a betteraccuracy-efficiency trade-off for practical recommender systems.",
  "ACKNOWLEDGEMENT": "This work was supported by the IITP grant funded by the MSIT(South Korea, No.2018-0-00584, RS-2019-II191906), the NRF grantfunded by the MSIT (South Korea, No.RS-2023-00217286, No.2020R1A2B5B03097210), the TIP funded by the MOTIE (South Korea, No.20014926), and the DIP grant funded by the MSIT and Daegu Metro-politan City (South Korea, No. DBSD1-07).",
  "Jaime Hieu Do and Hady W Lauw. 2023. Continual Collaborative FilteringThrough Gradient Alignment. In RecSys. 11331138": "Xiaocong Du, Bhargav Bhushanam, Jiecao Yu, Dhruv Choudhary, TianxiangGao, Sherman Wong, Louis Feng, Jongsoo Park, Yu Cao, and Arun Kejariwal.2021. Alternate model growth and pruning for efficient training of recommen-dation systems. In 20th IEEE International Conference on Machine Learning andApplications (ICMLA). IEEE, 14211428. Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2022.Recommendation as language processing (rlp): A unified pretrain, personalizedprompt & predict paradigm (p5). In RecSys. 299315.",
  "A.1List-wise distillation": "The list-wise distillation trains the student to emulatethe item permutation (i.e., ranking orders) predicted by the teacher.Specifically, they define a probability for observing each permuta-tion based on the Plackett-Luce model , then train the studentto maximize the likelihood of the teacher permutations . Foreach user , the item ranking list predicted by is denoted by . The list-wise KD loss is defined as the negative log-likelihoodof permutation probability :",
  "(10)": "() denotes the -th item in . is a hyperparameter reflectingthe length of the recommendation list. In this work, we set = 50. By minimizing the loss, the student model learns to preservethe detailed ranking orders of top- items in , while penalizingthe remaining items below the -th rank.This distillation approach has shown remarkable performancein many applications such as recommendation anddocument retrieval . Although more complicated variantshave been studied, we obtained satisfactory results with the defaultdistillation loss. We assessed the effectiveness of this distillation inour experimental setup, and the results are presented in .",
  "Gowalla29,85840,9881,027,46499.91Yelp14,95012,261342,61799.81": "A.2.2Teacher and student configuration. We provide the de-tailed configuration of the teacher and the student used in theexperiments. presents the number of learning parametersas well as the effectiveness of distillation in our setup.Teacher. Following the teacher configuration of the recent KDwork , we construct a massive teacher system by ensemblingmultiple large models. Initially, we evaluate the effectiveness of fivedifferent recommendation models for each dataset: matrix factoriza-tion (MF) , metric learning , deep neural network , graphneural network (GNN) , and variational autoencoder (VAE) .Subsequently, we select up to two models for each dataset that demonstrate high performance. Lastly, we incrementally increasethe capacity of the teacher system by augmenting the number ofmodels or dimension sizes until its performance no longer improves.For Gowalla, we construct the teacher system using five MF-basedmodels and two VAE-based models , each initialized withdistinct random seeds and set to 64 dimensions. For Yelp, we employfive GNN-based models , each initialized with distinct randomseeds and set to 128 dimensions. We adopt the ensemble schemeutilized in .Student. For the student, we employ two backbone models: MF-based model and GNN-based model . We set a small embed-ding size for the student (16 for Gowalla and 8 for Yelp), consideringthe teacher size for each dataset.Effectiveness of KD. We assessed the effectiveness of the list-wiseKD technique (A.1) on our teacher-student setup. The recommen-dation performance after the distillation is presented in ."
}