{
  "Abstract": "In this work, we provide data stream algorithms that compute optimal splits in decision treelearning. In particular, given a data stream of observations xi and their labels yi, the goal isto nd the optimal split j that divides the data into two sets such that the mean squared error(for regression) or misclassication rate and Gini impurity (for classication) is minimized. Weprovide several fast streaming algorithms that use sublinear space and a small number of passesfor these problems. These algorithms can also be extended to the massively parallel computationmodel. Our work, while not directly comparable, complements the seminal work of Domingos-Hulten (KDD 2000) and Hulten-Spencer-Domingos (KDD 2001).",
  "Introduction": "Decision trees are one of the most popular machine learning models. They serve as the base modelfor many ensemble methods such as gradient boosting machines , random forest , Ad-aBoost , XGBoost , LightGBM , and CatBoost . These methods are extremely pow-erful; they yield state-of-the-art results in many machine learning tasks, especially for tabular data.In the CART and ID3 algorithms, a decision tree is often built in a top-down manner.The tree is grown recursively by splitting the data into two parts at each node depending on dierentcriteria such as Gini impurity, misclassication rate, or entropy for classication and mean squarederror for regression.A key step in building a decision tree is to nd optimal splits. Denote [N] as the set {1,2,...,N}.The data consists of observations x1, x2,..., xm [N] and their real-valued labels y1,y2,...,ym [0, M] (for regression) or binary labels y1,y2,...,ym 1,+1 (for classication). Informally, wewant to nd the optimal split j [N] to partition the data into two sets, {xi j} and {xi > j}, suchthat the mean squared error (MSE) for regression or the misclassication rate for classication isminimized.The set [N] can be thought of as a discretization of the possible values of xi, which is almostalways the case in practice, as computers have nite precision. In fact, the values of observationsare often discretized into a nite number of intervals . However, the number ofdiscretized values N can still be very large, as observed in . This is due to the fact that sometimes *Authors are listed in alphabetical order.Hanoi University of Science and Technology. Email: University of Singapore, Singapore. Email: Diego State University, San Diego, CA, USA. Email: . This work is supported by the NationalScience Foundation under Grant No. 2342527. Corresponding Author. an attribute is a combination of multiple attributes, or an attribute is a categorical variable with highcardinality. This aects both the memory and time complexity.In the case of regression, we assume that yi is in the range [0, M] and each yi can be stored inone machine word of size O(logN) bits.In many applications, the data is too large to t in the main memory of a single machine. Oneoften streams the data from disk or network and processes the data in a streaming manner in sublinearspace and ideally in a small number of passes. In some applications, a small number of passes isallowed.Another computational model of interest is the massively parallel computation (MPC) model.In this model, we have a large number of machines with sublinear memory. In each synchronousround, each machine can communicate with other machines and perform local computations. TheMPC model is an abstraction of many distributed computing systems such as MapReduce and Spark.We rst consider a very simple problem of nding the optimal split which is the building blockof decision tree learning. This corresponds to building a depth-1 tree. In the case of classication,this is a decision stump which is often used as a weak learner for AdaBoost . We can repeatthis algorithm to build a deeper tree. We consider the case in which each observation xi representsa single attribute. Extending this to multiple attributes is straightforward, which we will discussshortly.The following formulation of nding the optimal split follows the standard approach from and . For a more recent overview, we refer to chapter 9 of . In this work, we considerstreaming and massively parallel computation settings. Compute the optimal split for regression.Consider a stream of observations x1, x2,..., xm [N] = {1,2,...,N} along with their labels y1,y2,...,ym [0, M]. Recall that for simplicity (i.e., toavoid introducing another parameter), we assume each yi can be stored in one machine word of sizeO(logN) bits.Let [E] denote the indicator variable for the event E. Furthermore, let D be the number of distinctvalues of {x1, x2,..., xm}. An important primitive to build a regression tree is to nd the optimal splitj [N] that minimizes the following quantity:",
  "x1 2 3 4 5 6 7 8 9 10": ": The left gure is an example of regression. The optimal split is j = 4 which minimizes themean squared error. The right gure is an example of classication. The optimal split is j = 4 whichminimizes the misclassication rate. In the oine model, we can store the entire data using O(m)1 space and then nd the optimalsplit j in O(m) time. Our goal is to remedy the memory footprint and to use sublinear space o(m)and o(N) in the streaming model.Our rst results are the following streaming algorithms for regression.",
  ". A 1-pass algorithm that uses O(D) space, O(1) update time, and O(D) post-processing timethat computes the optimal split. D is the number of distinct values of x1, x2,..., xm": "2. A 2-pass algorithm that with high probability 2 has the followings properties. It uses O(1/)space, O(1) update time, and O(1/) post-processing time. Furthermore, it computes a splitj such that L(j) OPT+. 3. For any (0,1), an O(1/)-pass algorithm that uses O1/2 Nspace , O1/2updatetime and post-processing time. It computes a split j such that L(j) (1 + )OPT. By setting = 1/logN, this implies an O(logN)-pass algorithm that uses O1/2space, O1/2updatetime and post processing time. It computes a split j such that L(j) (1 +)OPT. Compute the optimal split for classication.Consider a stream of observations x1, x2,..., xm [N] along with their binary labels y1,y2,...,ym {1,+1}. Our goal is to nd a split j that minimizesthe misclassication rate. For any R [N] and label u {1,+1}, we use fu,R to denote the count ofthe observations with label u in the range R. That is",
  "i=1[xi R][yi = u]": "1We use O() to suppress polylog N or polylogm factors whenever convenient. In this paper, the suppressed factors havelow orders such as 1 or 2 and are explicit in the proofs.2We make a convention that a high probability is at least 11/N(1) or 11/m(1). If we split the observations into two sets [1, j] and [j + 1,N], we classify the observations ineach set based on the majority label of that set. Then, the misclassication rate for a given j can beformally dened as follows.",
  "m min{f1,[1,j], f+1,[1,j]}+min{f1,(j,N], f+1,(j,N]}": "In a similar manner, we use OPT to denote the smallest possible misclassication rate over allpossible j. That is OPT = minj[N] L(j). The task is to nd a split j that approximately minimizesthe misclassication rate, i.e., L(j) OPT.Beside the misclassication rate, another popular loss function is based on the Gini impurity.Given a (multi)set of observations and labels S = {xi,yi}mi=1, the Gini impurity is dened as",
  ". A 1-pass algorithm that with high probability satises the following properties. It uses O(1/)space, O(1) update time, and O(1/) post-processing time. It outputs a split j such thatL(j) OPT+": "2. For any (0,1), an O(1/)-pass algorithm that uses O1/2 Nspace , O1/2updatetime and post-processing time. It computes a split j such that L(j) (1 + )OPT. By setting = 1/logN, this implies an O(logN)-pass algorithm that uses O1/2space, O1/2up-date time and post processing time. It computes a split j such that the loss based on GiniimpurityL(j) (1 +)OPT.",
  "space, O(1) update time, and O1/2post-processing time. The algorithm outputs a split jsuch that the loss based on Gini impurity LGini(j) OPT+": "Compute the optimal split for classication with categorical observations.For categorical ob-servations, we again assume that the observations x1, x2,..., xm [N]. However, we can think of [N]as not having a natural ordering (such as locations, colors, etc.).The goal is to compute a partition of [N] into two disjoint sets A and B, denoted by A B = [N],such that the misclassication rate is minimized. The misclassication rate is dened as follows:",
  "The table in summarizes our results for the data stream model": "Extending to multiple attributes.In the above, each xi is the value of a single attribute. If wehave d attributes, we can simply run our algorithm for each attribute in parallel.More formally, if we have d attributes, i.e., xi = (xi,1, xi,2,..., xi,d). In this setting, the optimalsplit is the best split over all attributes. Let Lq(j) be the misclassication rate (for classication) orthe mean squared error (for regression) for the q-th attribute at split j. Then, the optimal split isgiven by",
  "In other words, we could nd the optimal split for each attribute and then return the best splitover all d attributes. This results in an overhead factor d in the space complexity": "Massively parallel computation model.In the massively parallel computation (MPC) model, mobservations and their labels distributed among m1 machines. Each machine has memory Om.In each round, machines can communicate with all others, sending and receiving messages, andperform local computations. The complexity of an MPC algorithm is the number of rounds requiredto compute the answer. The MPC model is an abstraction of many distributed computing systemssuch as MapReduce, Hadoop, and Spark .We can adapt our streaming algorithms to the MPC model. The results are summarized in . Related work and comparison to our results.Previously, a very popular work by Domingosand Hulten 3 proposed a decision tree learning algorithm for data streams. In their study, theyconsidered the scenario where one has an innite stream of observations where each observation is",
  ": Result summary. R: Regression, C: Classication, CC: classication with categoricalattributes. 1: loss function based on misclassication rate, 2: loss function based on Gini impurity": "independently and identically distributed (i.i.d.). They quantied the number of samples required tond the optimal split and provided a bound on the probability that a sample will follow a dierentpath in the decision tree than it would in a tree built using the entire data set under some assumptions.It is worth noting that the work of Domingos and Hulten did not consider the regression problem.Subsequently, Hulten, Spencer, and Domingos provided a heuristic to build a decision treebased on most recent data in the stream. Some variant as well as improvements for this model arediscussed in .Compared to the work in , our work does not assume that the data stream isinnite. We also do not make the i.i.d. assumption. This is useful in scenarios such as when the datastream is collected in a time-dependent manner and the distribution of the data changes over time.Furthermore, we also consider the regression problem.Essentially, our work builds a depth-1 tree for both classication and regression in the streamingand MPC models. Thus, to construct a deeper tree, we can repeatedly apply our algorithm to builda deeper tree using any decision tree learning algorithm of interest such as ID3 , C4.5 , orCART with additional passes through the data stream. We remark that the problem of computingthe optimal decision tree is generally NP-Hard .If the observations and labels are i.i.d., our algorithm can be extended to build a tree by utilizingbatches of observations to determine the optimal splits as the tree grows. In this case, our approachalso accommodates the regression problem. Moreover, our work quanties the error at each nodein the tree, oering an advantage when one does not wish to benchmark against a specic treelearning algorithm as done in . Premilinaries and notation.We often use fR to denote the count of elements in the range R [1,N]. For classication, we use f1,R and f+1,R to denote the count of elements with label 1 and+1 in the range R, respectively.We also often rely on a common fact that log1+ x = O(1/ log x) for all x 1 and (0,1).Recall that we use [E] to denote the indicator variable for the event E. Handling deletions.Our algorithms can be extended to handle dynamic stream with deletions. Inparticular, instead of sampling to estimate the counts of elements or labels in a given range r [N],we can use Count-Min sketch with dyadic decomposition to estimate the counts . This will resultin an extra logN factor in the space complexity and the update time. Paper organization.In , we present the streaming algorithms for approximating theoptimal split for regression. In , we present the streaming algorithms for approximating theoptimal split for classication. Finally, outlines out how to adapt our streaming algorithmsto the massively parallel computation setting. All omitted proofs are in Appendix A.",
  "Algorithm 1 provides the pseudo-code for the rst result of Theorem 1": "Proof of Theorem 1 (1). Consider Algorithm 1. The space usage is O(D) since we only need tostore At,Bt,Ct for each distinct value t of x1, x2,..., xm. Note that when an observation arrives, weonly need to update At,Bt,Ct for the corresponding value of t; this can be done in O(1) time usinghash tables.We need to compute tj At, t>j At,tj Bt, t>j Bt, tjCt, and t>jCt for all t in O(D)time.This can be easily done using dynamic programming (e.g., tj Bt = (t<j Bt) + B j andt>j Bt = (t>j+1 Bt) + B j+1). This allows us to compute (j), (j), and L(j) for all distinct val-ues j of {x1, x2,..., xm}.Finally, we need to show that we correctly compute L(j). We have",
  "4m": "The idea is to nd a set of 1/ candidate split points S such that the number of observationsbetween any two consecutive split points is at most m. Then, we can compute L(j) for all j S andreturn the split point j with the smallest L(j). This will gives a j such that L(j) OPT+O().",
  "We now prove the second algorithm in the rst main result": "Proof of Theorem 1 (2). Consider Algorithm 2. In the rst pass, the algorithm samples O(1/logN)observations with high probability by Cherno bound. The update time is clearly O(1).Consider any interval [a,b] where a,b [N]. If the numbers of observations xi [a,b] is at leastm/2, for some suciently large constant C, the probability that we do not sample any xi [a,b] isat most1 C logN",
  "m/2 eC log N/2 < 1/N4": "Therefore, by appealing to the union bound over at mostN2 N2 choices of a,b we have thatwith probability at least 1 1/N2, for all a,b [N] if |{xi : a xi b}| > m/2, then we sample atleast one xi [a,b]. We can condition on this event since it happens with high probability.We next establish the approximation guarantee. Note that if we sample some xi = j, then weadd both j and j 1 to the candidate set. Suppose the optimal split j S then we are done sinceL(j) will be computed in the second pass. Otherwise, there exists t such that jt < j < jt+1. Thisimplies that jt < jt+1 1; hence, it must be the case that jt+1 was added to S because we samplesome xi = jt+2 and we did not sample any xi = jt+1. Therefore, the algorithm did not sample anyobservation in (jt, jt+1] (j, jt+1]. This implies",
  "4m M2 < OPT+M2 = OPT+O()": "The last equality uses the assumption that M = O(1). Finally, we need to show that Step 5 iscorrect and can be done eciently. Note that there is a trivial way to do this if we allow O(1/ logN)update time and an extra pass. We however can optimize the update time as follows. In the secondpass, we compute",
  "i=1[jt < xi jt+1]y2i": "for all jt, jt+1 S . Note that when xi,yi arrives, we can update the corresponding At,Bt,Ct in O(1)time since xi is in exactly one interval (jt, jt+1] which we can nd in O(loglogN + log1/)) timeusing binary search since S = O(1 logN) with high probability.It is easy to see that from At, Bt, and Ct, we can compute (jt) and (jt) for all jt S . That is",
  "Therefore, we can compute L(jk) for all jk S after the second pass based on the above quantitiesin O(1/ logN) time using dynamic programming. This completes the proof": "Multiplicative errror approximation.We now show how to obtain a factor 1 + approximationalgorithm for the regression problem. A multiplicative approximation is better if the mean squarederror is small; however, this comes at a cost of O(logN) passes and an extra 1/ factor in the memoryuse.The main idea is to guess the squared errors on the left and right sides of the optimal split pointand use binary search to nd an approximate solution using Lemma 4.At a high level, the algorithm guesses the left and right squared errors of the optimal split pointup to a factor of (1 + ). It then uses binary search to nd a split point satisfying that guess. Fora particular guess, the algorithm either nds a feasible split point or correctly determines that theguess is wrong.",
  "We consider the following cases:": "Case 1: Suppose Errorleft(j) zleft and Errorright(j) zright. We say j is a feasible splitfor this guess. Then, mL(j) zleft + zright. For the desired guess, this implies that mL(j) (1 +)(zleft +zright) which implies L(j) (1 +)OPT. Case 2: Suppose Errorleft(j) > zleft and Errorright(j) > zright. This guess must be wrong.Because of Lemma 4, for any split point to the right of j, the left error will larger than zleft.Similarly, for any split point to the left of j, the right error larger than zright. Therefore, wecan skip this guess since there is no feasible split. Case 3.1: Suppose Errorleft(j) > zleft and Errorright(j) zright. We know that from Lemma4, for any split point to the right of j, the left error will be larger than zleft. Therefore, we canskip all split points to the right of j and recurse on the left side. Case 3.2: Suppose Errorleft(j) zleft and Errorright(j) > zright. This case is similar to theprevious case. We know that from Lemma 4, for any split point to the left of j, the right errorwill be larger than zright. Therefore, we can skip all split points to the left of j and recurse onthe right side.",
  "We obtain the nal part of the rst main result by simulating multiple iterations of the binarysearch in fewer passes at the cost of an increased memory": "Proof of Theorem 1 (3). This result is also based on Algorithm 3. Recall that we allow O(1/2 N)space for some xed 0 < < 1. Fix a guess zleft,zright. In the rst iteration of the binary search, wecheck if the split j = (1+N)/2 is feasible. If not, in the second iteration, there are 2 possible valuesof j that the binary search can check for feasibility, depending on the outcome of the rst iteration.In general, after r iterations, the total number of possible splits j that the binary search checks forfeasibility is 1 + 2 + 4 + ...+ 2r = 2r+1 1. These values partition [N] into 2r intervals of length atmost N/2r.Note that the feasibility of a split j can be checked based on its left and right squared errors.Instead of running r iterations of binary search in 2r passes, we can compute the left and rightsquared errors of each of these 2r+1 1 splits in 2 passes using O(2r) space. The update time is O(1)by using the same trick as in Algorithm 2. This allows us to simulate r iterations of the binary searchin 2 passes. The search interval after r binary search iterations has size at most N/2r. We repeat thisprocess on the remaining search interval. After we repeat this process t times, the size of the searchinterval is at most N/2rt this implies t = O( logN",
  "that d = O log N": "with high probability. Furthermore, we only need to consider candidate splitj {j1, j2,..., jd} since if j (jt,tt+1) then fl,[1,j] = fl,[1,jt] and fl,(j,N] = fl,(jt,N] for l = 1.For u {1,+1}, we compute the number of sampled observations (xi,yi) where xi = jt and yi = udenoted by ku,[jt,jt]. For u {+1,1} and each values jt, we have the recurrence",
  "We put these together to complete the proof of Theorem 1 (3)": "Proof of Theorem 1 (3). Since for all j [N], we can approximate each term in the loss functionbased on the Gini impurity up to an additive error of , we are able to approximate the optimal splitpoint based on the Gini impurity up to an additive error of O(). Reparameterize /4 gives usthe desired error.The update time is O(1) and the space complexity is O(1/2). In the post-processing time, weonly need to consider the O(1/ logN) distinct values of j that appear in the samples. This gives usa O(1/ logN) post-processing time and completes the proof of the rst part of Theorem 1 (3). Multiplicative error approximation for the loss function based on misclassication rate.Wenow prove the second part of Theorem 2 that gives a factor 1+ approximation algorithm for ndingthe optimal split based on misclassication rate instead of an additive error. The approach is fairlysimilar to the regression case.",
  "Lemma 10. There exists an O(logN)-pass algorithm that uses O1/2space, O1/2update timeand post processing time. It computes a split j such that L(j) (1 +)OPT": "Proof of Theorem 2 (2). Consider Algorithm 6. We will show that the algorithm returns a split pointj such that L(j) (1 +)OPT.Let j be the optimal split. Let zleft and zright be the number of misclassied observations in[1, j] and in (j,N] respectively.One of the guesses zleft,zright must satisfy zleft [zleft,(1+)zleft] and zright [zright,(1+)zright]respectively. Fix a guess zleft,zright. Consider any iteration of the binary search with current searchpoint j.Let fu,[1,j] = min{f1,[1,j], f+1,[1,j]} and fv,(j,N] = min{f1,(j,N], f+1,(j,N]}. We consider the followingcases: Case 1: If fu zleft and fv zright then we know that j is a feasible split point that misclassiesat most zleft + zright observations. For the desired guess, this implies L(j) (1 + )OPT. Wesay j is a feasible split for this guess.",
  "the left of j will misclassify more than zright observations on the right. Therefore, we can skipthis guess. We say this guess is infeasible": "Case 3.1: If fu zleft and fv > zright, then we know any split to left of j will misclassify morethan zright observations on the right. Therefore, we can skip all splits to the left of j and recurseon the right side. Case 3.2: If fu > zleft and fv zright, then we know any split to right of j will misclassify morethan zleft observations on the left. Therefore, we can skip all splits to right of j and recurse onthe left side.",
  "m min{ f+1,A, f1,A}+min{ f+1,B, f1,B} OPT+/4": "Conversely, if L(A) + L(B) OPT+/4, then L(A) + L(B) OPT+. This happens with proba-bility at least 1 1/e10N for some suciently large constant C. The only dierence here is that weneed to take a union bound over 2N choices of A to get the probability bound. Lower bound.For N N, we consider the DISJ(N) problem in communication complexity. Inthis problem, Alice has input a {0,1}N and Bob has input b {0,1}N. Their goal is to determinewhether there is a index i [N] such that ai = bi = 1 or not. Specically, if there exists an index isuch that ai = bi = 1, the output is YES; otherwise, the output is NO.In the (randomized) communication setting, Alice and Bob need to communicate to determinewhether there exists an index i [N] such that ai = bi = 1 with an error probability of at most 1",
  "Theorem 11 (). The communication complexity of the DISJ(N) problem is (N), even forrandomized protocols": "Consider a stream of observations x1, x2,..., xm [N] and their labels y1,y2,...,ym {1,+1}.Using Theorem 11, we will show that any streaming algorithm for approximating the optimal mis-classication rate, denoted by OPT, with categorical observations requires at least (N) space. Toaccomplish this, we provide a reduction from the DISJ(N) problem to problem of deciding whetherOPT > 0 in the case of categorical observations. Suppose S is a streaming algorithm that decideswhether OPT = 0. The following reduction proves the second part of Theorem 3. Proof of Theorem 3 (2). For each ai = 1, Alice generates an observation label pair (i,+1) and feedsit to the streaming algorithm S. She then sends the memory state of S to Bob. For each bi = 1, Bobgenerates an observation label pair (i,1) and feeds it to S. Bob then sends the memory state of Sback to Alice and continue the next pass of the streaming algorithm.If there is no index i such that ai = bi = 1, then OPT = 0. We can choose A = {i : ai = 1} andB = {i : bi = 1}. Then, L(A,B) = 0 since all observations in A are labeled +1 and all observations inB are labeled 1. Thus, OPT = 0.If there is an index i such that ai = bi = 1, then there are two observations (i,+1) and (i,1)generated by Alice and Bob respectively. Thus, at least one of them is misclassied. Therefore,OPT > 0.Therefore, any constant-pass streaming algorithm that distinguishes between the case of OPT = 0and OPT > 0 requires at least (N) space.",
  "We now prove the second part of Theorem 13": "Proof of Theorem 13 (2). We can simulate the third algorithm in Theorem 1 in the MPC model asfollows. Recall that we try O(1/2 log2 N) guesses zleft,zright for the left and right errors of theoptimal split. Fix a guess, we run a binary search to nd a feasible split j or correctly determinethat the guess is infeasible. We can simulate r = logN iterations of the binary search by checkingthe feasibility of each of 2r+1 1 = O(N) splits. Let these splits be j1 < j2 < ... < j2r+11. FromLemma 12, we can compute the left and right errors Errorleft(jt) and Errorright(jt) for each jt inO(1) rounds which in turn allows us to determine the feasibility of each split with respect to thisguess.Therefore, we are able to simulate logN binary search iterations in O(1) rounds. Hence, thetotal number of rounds is O(1/). Note that since each machine has O1/2 mmemory, we canrun this algorithm for all guesses simulatenously.",
  "m machines. Each machine samples each xi,yi that it holds with probability p = C logN": "mfor somesuciently large constant C and sends its samples to a central machine. The central machine willthen compute the optimal split j that minimizes the misclassication rate based on the sampledobservations and labels as in the rst algorithm in Theorem 2. This requires 1 MPC-round.The third algorithm in Theorem 2 can be simulated in 1 MPC-round as well given that we have",
  "the central machine will receive O": "msamples. The central machine then computes the best splitj that minimizes the loss function based on the Gini impurity based on the samples as in the thirdalgorithm in Theorem 2; this is eectively equivalent to setting = 1/m1/4.We can also simulate the third algorithm in Theorem 2 in the MPC model. The proofs arecompletely analogous to the regression case and are omitted.",
  "In the second inequality, we apply the second inequality of Cherno bound with = m/f[a,b] 1.By taking a union bound over allN2choices of a,b [N], we have the desired result": "Proof of Lemma 12. In MPC, we can sort (xi,yi) based on xi in O(1) rounds and store them on themachines M1,M2,...,Mm in order .Let j0 = 0. Using the same argument as in the proof of Theorem 1 (2), in order to computeL(j1),L(j2),...,L(jk), it suces to compute the following quantities:",
  "xiMq[jt < xi jt+1]y2i": "and sends these quantities to the central machine. The central machine then computes At,Bt,Ctby aggregating At,q,Bt,q,Ct,q over q. We still need to argue that the central machine receives acombined message of size Om. We say that machine Mq is responsible for jt if it holds some xi such that jt1 < xi jt. It ispossible that two or more machines may be responsible for the same jt. Suppose machine Mq isresponsible for jl < jl+1 < ... < jl. Then, jl+1, jl+2,..., jl1 are only responsible for by Mq becausethe observations are stored in the machines in sorted order. We say jl and jl are the boundary splitsresponsible by Mq. Each machine has at most two boundary splits. Therefore,t,q : jt is a boundary split responsible for by M)q 2m.",
  "Tin Kam Ho. Random decision forests. In ICDAR, pages 278282. IEEE Computer Society,1995": "Dug Hun Hong, Sungho Lee, and Kyung Tae Kim. A note on the handling of fuzziness forcontinuous-valued attributes in decision tree generation. In FSKD, volume 4223 of LectureNotes in Computer Science, pages 241245. Springer, 2006. Geo Hulten, Laurie Spencer, and Pedro Domingos. Mining time-changing data streams. InProceedings of the seventh ACM SIGKDD international conference on Knowledge discoveryand data mining, pages 97106, 2001."
}