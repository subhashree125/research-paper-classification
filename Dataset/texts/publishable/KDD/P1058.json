{
  "ABSTRACT": "Adapting large language models (LLMs) to unseen tasks with in-context training samples without fine-tuning remains an importantresearch problem. To learn a robust LLM that adapts well to unseentasks, multiple meta-training approaches have been proposed suchas MetaICL and MetaICT, which involve meta-training pre-trainedLLMs on a wide variety of diverse tasks. These meta-training ap-proaches essentially perform in-context multi-task fine-tuning andevaluate on a disjointed test set of tasks. Even though they achieveimpressive performance, their goal is never to compute a truly gen-eral set of parameters. In this paper, we propose MAML-en-LLM,a novel method for meta-training LLMs, which can learn truly gen-eralizable parameters that not only performs well on disjointedtasks but also adapts to unseen tasks. We see an average increaseof 2% on unseen domains in the performance while a massive 4%improvement on adaptation performance. Furthermore, we demon-strate that MAML-en-LLM outperforms baselines in settings withlimited amount of training data on both seen and unseen domainsby an average of 2%. Finally, we discuss the effects of type of tasks,optimizers and task complexity, an avenue barely explored in meta-training literature. Exhaustive experiments across 7 task settingsalong with two data settings demonstrate that models trained withMAML-en-LLM outperform SOTA meta-training approaches.",
  "*Work done while interning at Amazon Alexa": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-x-xxxx-xxxx-x/YY/MM",
  "INTRODUCTION": "Large Language Models (LLMs) have revolutionized natural lan-guage processing (NLP) and achieved state-of-the-art performanceon a variety of diverse tasks. Recently, LLMs have been demon-strated to learn in-context , without expensive and compute-intensive fine-tuning. In-context learning (ICL) involves pre-appendingthe target sample by carefully selected task-specific exemplars -which act as conditioning for LLMs on that particular task. Learningin-context is an attractive proposition as its evaluation only requiresinference. With no gradient updates required, ICL can potentiallyimprove LLM generalization to new and diverse tasks with only afew examples.Even though out-of-the-box pre-trained LLMs show good ICL per-formance, multiple avenues of research have demonstrated improvedICL performance by warming-up out-of-the-box LLMs . Afew model warmup techniques for improved ICL performance havebeen proposed recently like MetaICL and MetaICT . Theevaluation of such models on unseen tasks proves their efficacy ingeneralization. Meta-training usually involves adapting (i.e., fine-tuning) pre-trained LLMs using a diverse set of tasks, formattedas an ICL instance by pre-appending exemplars in the promptsduring training. Once the model is meta-trained, the evaluation isusually performed on a distinct and disjoint set of tasks, never seenduring training. The process of warming up has been dubbed asmeta-training in these approaches which borrows from researchin classical machine learning literature of meta-learning which at-tempts to warm up models for faster adaptation to unseen tasks.One of the most commonly utilized techniques for meta-learningin classical literature is Model-Agnostic Meta-Learning (MAML)proposed by Finn et al. . MAML is formulated as a two-stepoptimization problem where the inner loop adapts copies of themodel parameters to various diverse tasks, while the outer loop up-dates the initial model parameters involving a second-order gradientupdate calculated on the adapted parameters. Even though LLMmeta-training approaches are eponymous to meta-learning,they do not utilize the two-step optimization framework for meta-training LLMs instead only adapting the model parameters contin-uously using a mixture of diverse tasks. For all practical purposes,the proposed meta-training approaches are analogous to multi-task",
  "KDD 24, August 2529, 2024, Barcelona, SpainSanchit Sinha, Yuguang Yue, Victor Soto, Mayank Kulkarni, Jianhua Lu, and Aidong Zhang": "Ekin Akyrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou.2023. What learning algorithm is in-context learning? Investigations with linearmodels. In The Eleventh International Conference on Learning Representations,ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Antreas Antoniou, Harrison Edwards, and Amos J. Storkey. 2019. How to trainyour MAML. In 7th International Conference on Learning Representations, ICLR2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shot learners. Advances in neuralinformation processing systems 33 (2020), 18771901. Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and He He. 2022. Meta-learning via Language Model In-context Tuning. In Proceedings of the 60thAnnual Meeting of the Association for Computational Linguistics (Volume 1: LongPapers), ACL 2022, Dublin, Ireland, May 22-27, 2022, Smaranda Muresan, PreslavNakov, and Aline Villavicencio (Eds.). Association for Computational Linguistics,719730. Budhaditya Deb, Ahmed Hassan Awadallah, and Guoqing Zheng. 2022. BoostingNatural Language Generation from Instructions with Meta-Learning. In Proceed-ings of the 2022 Conference on Empirical Methods in Natural Language Process-ing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, YoavGoldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computa-tional Linguistics, 67926808.",
  "RELATED WORK2.1In-context Learning (ICL) in LLMs": "In-context Learning (ICL) was first proposed by Brown et al. as an extremely inexpensive alternative to regular fine-tuning. ICLrequires no parameter updates as the input prompt is pre-appendedwith task-specific exemplars which are examples of the specific task to be performed. LLMs have been shown to perform exceptionallywell when prompted with a few examples of the task pre-appendedin the prompt . This behavior was first studied for LLMs inGPT-3 . Subsequent studies have shown that they can even solvecomplex problems like math and reasoning . Why LLMsare so adept at in-context learning remains an open research topicthat is attracting attention; for example Akyrek et al. comparestheir behavior to linear models. Empirical approaches like and have demonstrated that types of exemplars are important forICL performance.",
  "Meta-Learning for Generalization": "One of the most popular approaches for meta-learning was firstproposed by as Model Agnostic Meta-Learning (MAML). Thegoal of MAML was to warmup pre-trained models over a diverseset of tasks using an inner-outer minimization algorithm to learn ageneral set of parameters that can be adapted to new tasks using onlya few examples (few-shot).The inner loop usually focuses on a set of tasks, while the outerloop utilizes the learned parameters and the second-order gradientinformation from the inner loop to update the model, in turn captur-ing the direction of gradient updates. Usually, MAML requires theouter loop to perform a second-order gradient update which resultsin a higher computational burden. Even though MAML is effective,it is not without serious issues; for example, it is known to sufferfrom memorization , which entails the model repeating the datashown to it during training without learning anything. MAML hasalso been shown to be sensitive to training hyper-parameters andcomplex strategies have been proposed to enforce stability during itstraining process . MAML has also been used with some successfor training language models on particular tasks.",
  "Meta-training for improving ICL": "To improve the ICL performance of out-of-box pre-trained models,several meta-training approaches have been proposed. We discuss thetwo seminal works - MetaICT and MetaICL here. MetaICTuses BinaryCLFs and LAMA datasets to create tasks while alsopre-appending human-generated instructions to each task. MetaICLon the other hand uses a wide variety of disjoint tasks with nohuman-generated instructions to meta-train pre-trained LLMs. Bothapproaches update the model parameters on a batch from trainingtasks continuously - making them similar to fine-tuning. A recentrelated work attempts to utilize MAML to train LLMs for im-proving Prompt Tuning. However, the work is significantly differentfrom ours - as generalization is performed on learned soft embeddingof tokens and not on model parameters - a vastly different objectivefrom ours which is improving ICL performance.Comparision to existing works. We discuss the salient differenceswith MetaICL first. MetaICL discards principles of meta-learningand effectively only uses multi-task fine-tuning to meta-train theirmodels. Next, MetaICT follows an identical training process toMetaICL. Even though MetaICT attempts to compare MetaICT toMAML, they only utilize the first-order approximation and a singletask to update the model parameters in the inner optimization steppossibly due to not conducting proper investigation into a myriad ofsources of errors like optimizers and exploration states. Our work",
  "METHODOLOGY": "In this section, we first go over the standard meta-training procedurefor improving ICL performance of LLMs by discussing the problemsetup. Subsequently, we provide a mathematical overview of currentSOTA approaches and our proposed approach. Next, we providefiner details for MAML-en-LLM and its various components - taskadaptation and aggregated meta-update phases in addition to detailedoptimization perspective.",
  "Meta-training: Problem Statement": "In-context learning is an inference-only method, where exemplarsfrom the same tasks with expected output values are provided in theprompt as conditioning for the LLM. During meta-training, a similarsetup is followed where for a given train sample (,), belongingto a task C, samples (1,1),(2,2),...,(,) from the same taskC are sampled. The samples are pre-appended in the prompt withthe final train sample . The target label is used to calculate theloss and the pre-trained LLM ( ) is meta-trained over all availabletraining tasks C with standard classification training objectives ().Mathematically, a parameter update step can be represented as:",
  "T (T)LT ()(2)": "On the other hand, MAML-en-LLM aims to learn a generalizableset of parameters by training a model on a diverse variety of taskswith two distinct phases - an inner phase which controls the ex-tent of exploration - adaptation and an outer phase which controlsthe magnitude of the update - the meta-update. As a consequence,MAML-en-LLM can be represented as a bi-level optimization withan inner update step and an outer (meta) update step where it solvesa dual optimization training objective given by:",
  "T (T)LT ( LT ( ))(3)": "3.2.1Task Adaptation. The inner optimization adapts theparameters to a set of tasks. To achieve this tasks are sampledwhere each task is a set of size . The model is adapted on eachtask by performing gradient descent on the original parameters for steps also known traditionaly as support set . The number ofelements in the support set is equal to the number of steps foradaptation using gradient descent. Intuitively, the higher the number of tasks , the greater the exploration of the parameter space. Theadapted parameters are calculated using Equation 4. Mathematically,the inner loop adapts to a distribution of tasks T (T) where(T) represents a probability distribution over diverse tasks. Notethat a task represents a randomly sampled batch of training promptsas discussed in Min et al. .",
  "where T (T) is sampled from all tasks and represents themodel parameters, is the learning rate of the adaptation step and is the Cross Entropy Loss": "3.2.2Aggregated Meta-updates. The outer update utilizesa distinct query set ( ) to calculate the gradients with respect tothe adapted parameters. The size of the query set is kept the sameas the support set (i.e. = = ). In classic MAML literature,the same query set is usually used to calculate the meta-update.However MAML-en-LLM utilizes different tasks to perform themeta-updates. For each task the calculated gradients (using the queryset) are collected and averaged to perform a second-order gradientupdate on the original unadapted parameter as per Equation (5).The outer optimization performs the meta-update on the unadaptedparameter based on the second-order gradient updates of the adaptedset of parameters . Mathematically the second-order update canbe represented as:",
  "Shared Adaptive Optimizer Moments": "One of the most significant differences between MAML-en-LLM andMetaICL is the presence of a dual optimization problem in MAML-en-LLM. The dual optimization problem poses unique challengesin LLMs where the choice of optimizers affects generalization dras-tically. Note that typically, adaptive optimizers (like AdamW) arepreferred over stateless optimizers (like SGD with momentum). Fora typical adaptive optimizer, given gradients at step , observationssampled from a batch , hyperparameters 1 and 2, two movingaverages are calculated - the first moment and second moment as follows:",
  "(2 1 + (1 2) 2 )/(1 2)": "Note that the optimizer in the inner update is re-initialized afterevery meta-update, implying that the moments are re-initializedas well. This significantly changes the landscape of optimization,making inner gradient updates contribute disproportionately moreas the training continues for a longer duration, especially near theminima. To alleviate this problem, we propose optimizer parametersharing between the inner and outer optimizers - i.e., constantlyupdating shared moving averages between optimizers where only asingle set of optimizer parameters are updated.",
  "Consolidated Meta-Training usingMAML-en-LLM": "provides a schematic overview of the consolidated train-ing approach and Algorithm 1 details the exact MAML-en-LLMtraining procedure. As we utilize tasks for calculating the adaptedparameters and subsequently utilize tasks for adaptation steps andcalculating meta-updates, we represent our MAML-en-LLM termi-nology by MAML-2k-n. Hence, if the size of the support and queryset is 1 and the number of tasks during adaptations are also 1, theMAML-en-LLM setting will be represented as MAML-2-1. It is in-tuitive to see that the meta-update frequency can easily be calculatedas 2, hence for MAML-2-1, the frequency of meta-updates is 2.Similarly, for MAML-2-4, the frequency of meta-updates is 8.",
  "EXPERIMENTS4.1Dataset Description": "We utilize two datasets with a wide diversity of tasks - CROSSFIT and UNIFIEDQA consisting of total of 142 distinct tasks. Weuse the exact same task splits used by MetaICL , however due toactive developments, we utilize latest versions of tasks. All the tasksfall in the following 4 categories with increasing complexity: textclassification, natural language inference, question answering, andparaphrasing. The training tasks and the testing tasks are ensured to be disjointed. The testing tasks consist of two types of tasks -tasks with similar domains in training set and sampled from unseendomains in training set. The statistics of the dataset splits are detailedin .",
  "Training Details": "We utilize a pre-trained GPT-2 Medium consisting of 355 mil-lion parameters for all our experiments. We consider both the stan-dard models and Noisy channel models for evaluation of allexperiments. Training is performed on 8 Tesla V100 GPUs usingPytorch and pre-trained model checkpoints are taken from Hugging-Face. Training time for MetaICL models for 50k steps is about 5hours, while for MAML-en-LLM models is about 12 hours.Example prompt structure. We first visualize the prompt struc-ture during training of both standard and channel models ((Green). Next, we visualize the prompt structure during ICL infer-ence of both standard and channel models (Red). The operatorselects the label with the max probability,",
  "Non-Paraphrase59Paraphrase41": ": Statistics of number of train/test tasks in the 7 meta-training settings considered for evaluation. The number of tasksin the Unseen domain is listed in the last column. Dataset split isidentical to Min et al. same learning rate of 1x105 for both and . For MAML, thesize of support and query sets is chosen as 1 (number of batches).The number of tasks is chosen as 1 (MAML-2-1) and 4 (MAML-2-4). This implies the frequency of meta-updates is 2 and 8 respec-tively. We train both paradigms of models for 50k steps. For bothapproaches, AdamW is used as an optimizer. As MAML modelsrequire an inner and outer optimizer, we utilize AdamW for bothoptimizers with identical hyper-parameters. As every AdamWs opti-mization step depends on the last gradient update, we copy the inneroptimizers last gradient state to the outer optimizer for every meta-update. The training seed is set as 100. More details on optimizerscan be found in the Appendix5.5. Best movie ever Insipid movie very, very slow",
  "Channel Models": ": Example training and test prompts for Standard (Left)and Channel (Right) Models. The training procedure of Channelmodels learns to predict the sample, conditioned on its true label.During inference, channel models predict the target sample itselfconditioned on all possible labels for the task. In this example,the task is Sentiment Analysis and hence has only two labelsPositive and Negative. Inference. We utilize identical evaluation framework as forfair comparisons. The test sequence length is fixed at 256 or 16exemplars whichever is lower. The batch size is fixed at 16 samplesduring inference.",
  "Comparison of Baselines and Replication": "We compare our proposed methodology against a variety of differentmodels and prompt settings. We utilize 2 types of models - standardmodels and noisy channel models as proposed by Min et al. . Forstandard models, the training and inference procedure for a giveninput sample , and exemplars (1,1, ...,) is as follows(Refer problem setup in .1):",
  "= arg max (|1,1,2,2, ...,,)(7)": "where is the Cross Entropy loss. On the other hand, noisy-channelmodels (referred to as Channel models from now on) treat the train-ing and inference procedure as a generative problem rather thana classification task. During training, the labels and prompts areflipped and reordered, and the target label is appended in the promptitself, while the training instance is treated as the models gener-ated output. Mathematically, the training procedure optimizes thefollowing:",
  "(b) Performance on unseen tasks utilizing limited data setting": ": For both tables, Rows 1-3 represent the baselines and Rows 4 and 5 represent the performance of MAML settings. Similarly,Rows 6-8 represent baselines using the Channel training/inference and Rows 9 and 10 represent MAML settings on Channel models.Numbers are represented as X/Y where X represents the average performance and Y represents the worst-case performance. Theentries in bold are the best-performing models.",
  "MetaICL: We replicate the exact MetaICL setting by trainingthe model identical to . Please refer to .2, formore information": "Replication of MetaICL. We replicate the training procedure ofMetaICL and Channel MetaICL models on the latest versions of thedatasets and on the GPT-2 Medium models. Differently from theprovided approach, we do not utilize 8-bit optimization and mixedprecision training. Our replicated results agree with the reportedresults on all the data splits in the paper and even perform better thanthe reported results on GPT-2 Medium ( in ).",
  "Evaluation Criterion and Metrics": "To quantify the performance of classification tasks, macro-F1 scoreis utilized (Refer for details) which is suitable for settings withclass imbalances. For all other tasks prediction accuracy is usedinstead. We report the average and worst-case performance on fiverandom seeds (identical to ). To compare performances amongvarious methods for the same data setting, we report the win-ratesof MAML-en-LLM models. We consider a win for all cases whereboth the average and worst case performances of MAML-en-LLMmodels outperform their counterparts and are significant (discussedlater). The numbers in bold represent the best-performing methodsfor each data setting.",
  "RESULTS AND DISCUSSION5.1Experiment-1: Generalization Performance": "We evaluate our methodology on two commonly encountered datasettings in practice. These settings are useful to demonstrate theefficacy of our method on both high and low-resource settings.Complete Data Setting (High Resource). Comprised of the entiretraining set from each task dataset. We utilize the exact same taskand data splits as MetaICL. The statistics for training and testingdata are detailed in .Limited Data Setting (Low Resource). We sample 10% of trainingdata from each task dataset utilizing the same dataset seeds. To com-bat the bias introduced during sampling, we ensure the proportion of",
  "(b) Performance on all tasks utilizing limited data setting": ": For both tables, we report the performance numbers on all the tasks. Rows 1-3 represent the baselines and Rows 4 and 5represent the performance of MAML settings with 1 and 4 tasks respectively. Similarly, Rows 6-8 represent baselines using the Channeltraining/inference and Rows 9 and 10 represent MAML settings on Channel models. The numbers are represented as X/Y where Xrepresents the average performance and Y represents the worst case performance. The entries in bold are best-performing models. labels in the sampled data is the same as in the complete data setting,i.e., the sampling is equally stratified ().To compare the generalization performance of MAML-en-LLMand MetaICL, we evaluate first on test tasks in unseen trainingdomains and next on all test tasks. As ICL is sensitive to the selectedexemplars in the prompt, we consider five seeds while creatingprompts for the test set. The exemplars are sampled from the trainset of the test tasks and performance is averaged. We report both theaverage and the worst-case performances over all five seeds.Significance Analysis. For all reported results, we pay special carein determining the significance of the result. If both the averageand the worst-case performance are better, we adjudge the resultsignificant (bold numbers) based on lower standard deviation. 5.1.1Performance on tasks from unseen domains. Wereport the performance of MAML-2-1 and MAML-2-4 along withthe baselines as discussed before in (a) and (b) on onlyunseen domain of tasks. Note that unseen tasks share no task typewith the training set as well as sampled from completely disjointeddomains.Complete Data. We observe that MAML-en-LLM settings outper-form MetaICL on 5 out of the 7 task settings (win rate of 0.71)for standard models and 4 out of 7 task settings (win rate of 0.57)for channel models on complete data setting. For the task settingswhere MAML settings do not outperform MetaICL, we observe thatNon-ParaPara on standard models underperforms significantly(discussed in Subsection 5.2.3). Limited Data. Similarly, MAML-en-LLM outperforms MetaICLon 4 out of 7 settings (win rate of 0.57) using both standard andchannel models on limited data setting. These results demonstratethe efficacy of our approach in low-resource settings. For most othersettings MAML-en-LLM performs at par with MetaICL.Takeaway. The outperformance of MAML-en-LLM on the unseentask setting implies that our method learns a more generalized param-eter set. This is a direct consequence of MAML-en-LLM exploring awider set of parameter space as demonstrated in . 5.1.2Performance on all tasks. Similar to unseen tasks, wereport the performances of MAML-2-1 and MAML-2-4 settings in (a) and (b) along with the associated baselines.Complete Data. MAML-en-LLM settings outperform MetaICL onthe complete data settings for all the tasks on 4 out of 7 (win rate of0.57) data settings. Channel models perform much better, beatingMetaICL on 5 out of 7 (win rate of 0.71) data settings.Limited Data. For the limited data settings, MAML-en-LLM settingsoutperform MetaICL on 6 out of 7 (win rate of 0.85) task settings onboth Standard and Channel models. As before, MAML-en-LLM set-tings significantly outperform or are comparable with the MetaICLcounterparts on most settings.Takeaway. The results on both complete and limited task settingsshow that even though MAML-en-LLM performs meta-updates onceevery 2 batches, it is insensitive to amount of train data. Thisfurther attests to MAML-en-LLMs efficacy and wide use cases.",
  "Effect of Task Complexity on ExplorationStates (Number of Tasks)": "5.2.1Task complexity. We preface the discussion around ex-ploration states by discussing the exact task settings. Note that wehave a total of seven settings out of which - two are classification,one is natural language inference (NLI) which is similar to classifi-cation, two are question answering (QA), and one is paraphrasingwhile the last one is a mixture of all of them characterized onlyby the amount of data. We designate the set of Classification andNLI tasks as less-complex while Paraphrasing and QA are more-complex. As a direct consequence to task splits, more-complex tasksshould ideally require a wider parameter space exploration whileless-complex tasks should require relatively less exploration. 5.2.2Complexity directly affects performance of variousexploration states. We utilize MAML-en-LLM on 2 settings -with 1 task and 4 tasks represented by MAML-2-1 and MAML-2-4,respectively. The more the number of tasks, the more parameterspace is explored by the model before performing the meta-update.However, on the flip side, more exploration leads to slower conver-gence to the minima. The trend we observe is that more numberof tasks (MAML-2-4) benefit more-complex settings like QA andParaphrasing, while a smaller number of tasks (MAML-2-1) helpless-complex settings like classification and NLI. (a,3b). Wereport results on the validation set in the Appendix. 5.2.3Further discussion on performance. Even though thetask settings are chosen keeping in mind the non-overlap of tasks,not all tasks are created equal. We believe the choice behind thetask selection and splits is not explored in detail in . We attemptto shed light on why MetaICL or MAML-en-LLM do not performwell on specific tasks like Non-ParaPara. From our experiments,we observe both MetaICL and MAML-en-LLM on standard modelsactually degrade performance from out-of-box pre-trained models(Refer last column in a). This observation alludes to thefact that standard model training of MetaICL and MAML-en-LLMactually causes some forgetting to the out-of-box pre-trained modelsby disrupting model weights out of box. In other words, pre-trainedout-of-box models are already good enough to perform complextasks like paraphrasing. Hence, task type and design need to beclosely monitored to utilize meta-training approaches.",
  "Further discussion on training sets. Non-QA -> QAand Non-Para->Para require meta-training on a relatively easier sub-set (Non-QA and Non-Para sets mostly include easier classification": "and NLI tasks), which makes the test sets significantly challenging.Indeed, Meta-training on these tasks using MetaICL even causessome forgetting as compared to pre-trained models. Please referto a where RawLM outperforms MetaICL baseline on chal-lenging data settings. Refer a where both Non-Para -> Para(37.26 to 33.1) and Non-QA to QA (39 to 36.71) give a degradationin performance. This behavior is mirrored in the original paper aswell . Hence, the behavior observed using MAML-en-LLM onthese particular settings is not any different from MetaICL itself andrequires further analysis - out of the scope of this paper.",
  "Effect of Optimizer Choice": "Multiple works have demonstrated that optimizers play a crucial rolein the effective training of LLMs. Stateless optimizers like StochasticGradient Descent (SGD) underperform newer adaptive optimizerslike AdamW in LLM training. Note that MetaICL utilizesAdamW. However, MAML-en-LLM utilizes a dual optimizationproblem - which requires two separate optimizers for the inner andouter optimizations. In , we provide ablation studies withtwo different optimizers - one stateless (SGD) and one adaptive(AdamW). Due to compute limitations, we sample a 10% subsetof the training and test data across two seeds for both MetaICLand MAML-2-1. In row 4, we report the performance utilizing acombination of optimizers for inner and outer optimizations. UnderMAML-2-1, when using stateless optimizers (SGD+SGD), we donot see an increase in performance. Next, as the inner optimizer isre-initialized after every meta-update, we use SGD for inner andAdamW for outer - resulting in a significant increase in performance.Note that using a stateless optimizer in the outer loop is identical toutilizing both stateless optimizers and the performance is identicalto SGD+SGD. Lastly, we report results on using adaptive optimizersin both inner and outer optimization steps with and without momentparameter sharing (discussed in subsection 3.3).",
  "Experiment-2: Very Few Shot Adaptation": "The second experiment involves adapting the meta-trained modelsto unseen domains using only a few samples. Setup: We utilize thetraining set of test tasks for sampling both the adaptation samplesand their prompts, ensuring that the same target prompt is neverutilized in exemplars. We sample 16 exemplars or 256 sequencelengths whichever is lower for the prompts. We consider a total of16 adaptation data points. The model is adapted using the adaptationtraining samples for 16 steps (1 pass over all points) with a learning",
  "SGD + SGD36.435.6SGD + AdamW40.038.6AdamW + SGD36.4*35.6*AdamW + AdamW (d)40.038.6AdamW + AdamW42.042.8": ": Performance over two seeds for NoICL, RawLM,MetaICL and MAML-2-1 methods utilizing various combina-tions of optimizers. The optimizer column in row 4 uses thenotation X+Y, where X and Y are the inner and outer opti-mizers respectively. Note that (*) represents an identical set-ting to SGD+SGD. The AdamW+AdamW (d) setting utilizesAdamW without moment sharing. Utilizing adaptive optimizers(AdamW) with parameter sharing in inner and outer optimiza-tion yields the best results. (Sampled from Non-NLI NLI) rate of 1x107 using AdamW. The testing set remains identical tothe unseen test tasks mentioned in .Takeaway: Generalization to unseen tasks is a paramount problemin LLM literature. We report the results of very few-shot adaptedmodels in . Note that the starting unadapted standard andchannel models are identical to the complete data setting. We observefor standard models, MAML-en-LLM settings outperform MetaICLon 5 out of 7 tasks (win rate of 0.71). Similarly, channel modelsoutperform MetaICL on 5 out of 7 tasks (win rate of 0.71). Onceagain we observe that generalization to unseen domains of tasksis better captured by MAML-en-LLM as the model explores widerparameter space during training, thus learning better parameter ini-tializations for adaptation. This behavior has been well documentedin Meta-learning literature like .",
  "In this section, we detail some practical considerations to keepin mind during meta-training models using MAML-en-LLM. Forreference, details the training procedure schematically": "Model sizes: Let us assume the size of the computation graphis in order of the number of parameters of the model in ques-tion. Assume () = () = () to be the size of thecomputational graph during training. For MetaICL, as thereis no adaptation phase, the gradients are computed only once.Hence at any given instance, the maximum memory utiliza-tion of the computational graph is 2 (), where the onlyvalues are the present parameter state and the gradients. How-ever, for MAML-en-LLM, after the adaptation step the numberof parameters in the computational graph are () (unadaptedparams), () ( is number of tasks), () (meta-update)which is a total of ( + 2) () parameters. Hence the mem-ory utilization is a linear function of the number of tasks.Thus, it is important to control the number of tasks as per themeta-training strategy and the type of tasks themselves. For",
  "our purpose, GPT-2 Medium with 355 million parameters isused": "Optimizer states : MAML-en-LLM as opposed to MetaICLutilizes 2 optimizers for the adaptation and the meta-updatesteps respectively. Even though it might be tempting to utilizestate-less optimizers like SGD and Adam, LLM training hasbeen shown to work better when the updates are conditionedon the last gradient state such as with adaptive optimizerslike AdamW . Hence, we utilize AdamW for both adap-tation and meta-updates (Optim 1 and Optim 2 in ).However, after each update step, we copy the last optimizerstate back and forth to both optimizers to smoothen out thetraining procedure.",
  "We acknowledge that our method has a few major limitations inpractice. We discuss the limitations in detail below:": "Unstable Training/Performance: Due to the dual optimiza-tion procedure of MAML-based approaches, the training pro-cedure is bumpy and non-smooth, making the training andvalidation losses spiky. This observation has been noted by .This makes the training of large models even more unstableand care must be taken while training employing smoothingmethods like early stopping, gradient clipping, etc.",
  "CONCLUSION": "In this paper, we proposed a novel method MAML-en-LLM that meta-trains pre-trained out-of-box models using the principles proposedin meta-learning literature. We demonstrated that MAML-en-LLMexplores a much wider parameter space than current SOTA meta-training methods like MetaICL and MetaICT due to adaptation tomultiple sets of parameters before the actual meta-update. Empiri-cally, MAML-en-LLM outperforms MetaICL on both standard andchannel models on an extensive set of tasks in both seen and unseendomains. Subsequently, we also demonstrate that models trained us-ing MAML-en-LLM can be quickly adapted in a few-shot manner to aset of tasks in the unseen domain. Overall, MAML-en-LLM has beenempirically demonstrated to outperform MetaICL on performanceand generalization. We hope our study motivates the communityto utilize classical meta-learning principles in the meta-training ofLLMs in the future.",
  "Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-agnostic meta-learning for fast adaptation of deep networks. In International conference onmachine learning. PMLR, 11261135": "Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord,Peter Clark, and Hannaneh Hajishirzi. 2020. UnifiedQA: Crossing Format Bound-aries With a Single QA System. In Findings of the Association for ComputationalLinguistics: EMNLP 2020, Online Event, 16-20 November 2020 (Findings of ACL,Vol. EMNLP 2020), Trevor Cohn, Yulan He, and Yang Liu (Eds.). Associationfor Computational Linguistics, 18961907.",
  "Louis Kirsch, James Harrison, Jascha Sohl-Dickstein, and Luke Metz. 2022.General-purpose in-context learning by meta-learning transformers. arXiv preprintarXiv:2212.04458 (2022)": "Ilya Loshchilov and Frank Hutter. 2019. Decoupled Weight Decay Regularization.In 7th International Conference on Learning Representations, ICLR 2019, NewOrleans, LA, USA, May 6-9, 2019. OpenReview.net. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp.2022. Fantastically Ordered Prompts and Where to Find Them: OvercomingFew-Shot Prompt Order Sensitivity. In Proceedings of the 60th Annual Meetingof the Association for Computational Linguistics (Volume 1: Long Papers), ACL2022, Dublin, Ireland, May 22-27, 2022, Smaranda Muresan, Preslav Nakov, andAline Villavicencio (Eds.). Association for Computational Linguistics, 80868098. Florian Lux and Ngoc Thang Vu. 2022. Language-Agnostic Meta-Learning forLow-Resource Text-to-Speech with Articulatory Features. In Proceedings of the60th Annual Meeting of the Association for Computational Linguistics (Volume 1:Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, Smaranda Muresan,Preslav Nakov, and Aline Villavicencio (Eds.). Association for ComputationalLinguistics, 68586868. Sewon Min, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022.Noisy Channel Language Model Prompting for Few-Shot Text Classification. InProceedings of the 60th Annual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022,Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (Eds.). Associationfor Computational Linguistics, 53165330. Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2022.MetaICL: Learning to Learn In Context. In Proceedings of the 2022 Conferenceof the North American Chapter of the Association for Computational Linguistics:Human Language Technologies. 27912809.",
  "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, IlyaSutskever, et al. 2019. Language models are unsupervised multitask learners.OpenAI blog 1, 8 (2019), 9": "Ling Xiao Wang, Kevin Huang, Tengyu Ma, Quanquan Gu, and Jing Huang.2021. Variance-reduced first-order meta-learning for natural language processingtasks. In Proceedings of the 2021 Conference of the North American Chapter ofthe Association for Computational Linguistics: Human Language Technologies.26092615. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoningin large language models. Advances in Neural Information Processing Systems 35(2022), 2482424837. Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, XinyunChen, Hanxiao Liu, Da Huang, Denny Zhou, et al. 2023. Larger language modelsdo in-context learning differently. arXiv preprint arXiv:2303.03846 (2023). Qinyuan Ye, Bill Yuchen Lin, and Xiang Ren. 2021. CrossFit: A Few-shotLearning Challenge for Cross-task Generalization in NLP. In Proceedings ofthe 2021 Conference on Empirical Methods in Natural Language Processing,EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November,2021, Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.). Association for Computational Linguistics, 71637189. Mingzhang Yin, George Tucker, Mingyuan Zhou, Sergey Levine, and Chelsea Finn.2020. Meta-Learning without Memorization. In 8th International Conference onLearning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.OpenReview.net. Kang Min Yoo, Junyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang-goo Lee, and Taeuk Kim. 2022. Ground-Truth Labels Matter:A Deeper Look into Input-Label Demonstrations. In Proceedings of the 2022Conference on Empirical Methods in Natural Language Processing, EMNLP 2022,Abu Dhabi, United Arab Emirates, December 7-11, 2022, Yoav Goldberg, ZornitsaKozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, 24222437."
}