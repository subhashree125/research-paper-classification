{
  "Abstract": "Marketing optimization plays an important role to enhance userengagement in online Internet platforms. Existing studies usuallyformulate this problem as a budget allocation problem and solve itby utilizing two fully decoupled stages, i.e., machine learning (ML)and operation research (OR). However, the learning objective inML does not take account of the downstream optimization task inOR, which causes that the prediction accuracy in ML may be notpositively related to the decision quality.Decision Focused Learning (DFL) integrates ML and OR intoan end-to-end framework, which takes the objective of the down-stream task as the decision loss function and guarantees the con-sistency of the optimization direction between ML and OR. How-ever, deploying DFL in marketing is non-trivial due to multipletechnological challenges. Firstly, the budget allocation problem inmarketing is a 0-1 integer stochastic programming problem andthe budget is uncertain and fluctuates a lot in real-world settings,which is beyond the general problem background in DFL. Secondly,",
  "Both authors contributed equally to this research.Corresponding author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain. 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08 the counterfactual in marketing causes that the decision loss can-not be directly computed and the optimal solution can never beobtained, both of which disable the common gradient-estimationapproaches in DFL. Thirdly, the OR solver is called frequently tocompute the decision loss during model training in DFL, whichproduces huge computational cost and cannot support large-scaletraining data. In this paper, we propose a decision focused causallearning framework (DFCL) for direct counterfactual marketingoptimization, which overcomes the above technological challenges.Both offline experiments and online A/B testing demonstrate theeffectiveness of DFCL over the state-of-the-art methods. Currently,DFCL has been deployed in several marketing scenarios in Meituan,one of the largest online food delivery platform in the world.",
  "Causal Inference, Decision Focused Learning, Marketing Optimiza-tion": "ACM Reference Format:Hao Zhou, Rongxiao Huang, Shaoming Li, Guibin Jiang, Jiaqi Zheng, BingCheng, and Wei Lin. 2024. Decision Focused Causal Learning for DirectCounterfactual Marketing Optimization. In Proceedings of the 30th ACMSIGKDD Conference on Knowledge Discovery and Data Mining (KDD 24),August 2529, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.",
  "Introduction": "Conducting marketing campaigns is a popular and effective wayused by online Internet platforms to boost user engagement andrevenue. For example, coupons in Taobao can stimulate useractivity, dynamic pricing in Airbnb and discounts in Uberencourage users to use the products.Despite the incremental revenues, marketing campaigns couldincur significant costs. In order to be sustainable, a marketing cam-paign is usually conducted under a limited budget. In other words,only a portion of individuals (e.g., shops or goods) may receivemarketing treatments due to a limited budget. Hence, assigning theappropriate marketing treatments to different individuals is essen-tial for the effectiveness of a marketing campaign since users wouldrespond differently to various promotional offers. Such decisionproblems can be formalized as resource allocation problems andhave been investigated for decades.The mainstream solution for these problems is a two-stage method. In the first stage, the individual-level (incremental)response under different treatments is predicted using ML models.The second stage is OR, and the predictions are fed into the combi-nation optimization algorithms to achieve optimal overall revenue.However, the objectives of the two stages are not aligned: the for-mer focuses on the predictive precision of the ML models, whilethe latter focuses on the quality of decisions. The method has somedefects due to the isolation of ML and OR. First, the prediction preci-sion of ML models has no strict positive correlation with the qualityof the final decision. This is because standard loss functions (e.g.,mean square error, cross-entropy error) do not take the interplaybetween the predictions into account, which can affect decisionquality. Second, ML models often fall short of perfect precision, andthe complex operations performed on the predictions in OR lead tothe amplification or accumulation of prediction errors. Thus, thetwo-stage method usually obtains suboptimal decisions and is eveninferior to heuristic strategies in some scenarios.Recently, Decision-Focused Learning (DFL) hasreceived increasing attention as an appropriate alternative to thetwo-stage method. The paradigm integrates prediction and opti-mization into an end-to-end system, which effectively aligns theobjectives of both stages and achieves better performance on manychallenging tasks. The key idea is to train ML models using a lossfunction that directly measures the quality of the decisions obtainedfrom the predictions. Specifically, the ML models are trained un-der the predict-then-optimize framework , which (1) makespredictions based on historical data, (2) solves the optimizationproblem based on the predictions, and (3) computes the decisionloss to update the ML model parameters using stochastic gradientdescent (SGD).Nevertheless, deploying DFL in marketing is non-trivial due tothe following challenges.Uncertainty of constraints. Most prior works of DFL haveinvestigated the optimization problem where the unknown param-eters appear in the objective function. The reason behind this isthat the unknown parameters in the constraints lead to uncertaintyin the solution space, and the optimal solution derived from thepredictions may not be feasible under the real parameters. Withinthe constraints of our optimization problem, there are two distinct forms of uncertainty: intrinsic and extrinsic. The inherent uncer-tainty in the constraints refers to the costs consumed by the indi-viduals under different treatments, which can be predicted basedon historical data. Extrinsic uncertainty is the frequently changingmarketing budget, determined by the external environment. AnML model is required to guarantee superior performance underdifferent marketing budgets. Thus, our optimization objective is theeffectiveness of the decision under any budget, and the optimizationproblem is a 0-1 integer stochastic programming.Counterfactuals in marketing. Computing decision loss inmarketing is challenging due to the presence of counterfactuals.Specifically, observing the values and costs of an individual underdifferent treatments is impossible because the individual can onlyreceive one treatment, which is also called the fundamental problemof causal inference . In addition, the optimal solution of theoptimization problem cannot be obtained based on offline datadue to the counterfactuals, which disables the common gradient-estimation methods (e.g., SPO , LODL , LTR ) in DFL.Computational cost of large-scale dataset. Computationalcost is one of the major roadblocks for DFL involving large-scaleoptimization. As mentioned above, DFL integrates prediction andoptimization into an end-to-end system, where the solver will becalled frequently during training to solve the optimization problem.Therefore, the computational cost of DFL is high, leading priorworks to investigate toy-level problems with few decision variables.In real-world applications, we need to train models for tens ofmillions of data, which is unsupportable by traditional DFL.In this paper, we propose Decision-Focused Causal-Learning(DFCL) to address the above challenges. The main contributions ofthis work can be summarized as follows.Generalization. In order to address both endogenous uncer-tainty (cost of individual consumption) and exogenous uncertainty(marketing budget) in the constraints, the uncertainty constraintsare transformed into the objective function of the dual problemusing Lagrangian duality theory. The optimization objective ofthe dual problem is then used as the decision loss. Moreover, weprove that the budget of the primal problem corresponds to theLagrange multipliers of the dual problem, and thus optimizing thedual solution under different Lagrange multipliers is equivalent tooptimizing the quality of decisions under different budgets.Counterfactual Decision Loss. Optimal solution, decision loss,and gradient cannot be computed directly due to the existence ofcounterfactuals in marketing, thus we propose two solutions: (1)surrogate loss function and (2) black-box optimization based onthe Expected Outcome Metric (EOM) . Inspired by PolicyGradient in Reinforcement Learning, we transform the decisionproblem of discrete actions into the problem which maximizes ex-pected revenue under the probability distribution of the actions,and combine the Maximum Entropy Regularizer as well as theLagrangian duality theory to give two kinds of surrogate loss func-tions: Policy Learning Loss and Maximun Entropy Regularized Loss.We theoretically guarantee continuity, convexity and equivalenceof the surrogate loss functions. For black-box optimization, we em-ploy the EOM to give an unbiased estimation of the decision lossand improve the finite difference strategy to develop an efficientestimator of the gradient, which enables us to update the modelparameters using gradient descent.",
  "Decision Focused Causal Learning for Direct Counterfactual Marketing OptimizationKDD 24, August 2529, 2024, Barcelona, Spain": "Scalability. In real-world applications, we need to train modelsfor tens-of-millions of data. The surrogate functions proposed inthis paper are smooth convex loss functions with almost the samecomputational efficiency as the two-stage method. For black-boxoptimization, frequently solving the optimization problem afterperturbation incurs huge computational overhead. We acceleratethe problem solving and modify the gradient estimator using theLagrangian duality theory, which significantly improves the train-ing efficiency and reduces the training time from hour-level tosecond-level per epoch compared to the black-box method basedon the primal problem.We conduct extensive experiments to evaluate the performanceof DFCL. Both offline experiments and online A/B testing show thesuperior performance of our method over state-of-the-art baselines.DFCL is deployed to several scenarios in Meituan, an online fooddelivery platform, and achieves significant revenue.",
  "Related Works": "Two-stage Method. The mainstream solution to the resource al-location problem in marketing usually follows the two-stage para-digm , which handles the two stagesmachine learn-ing (ML) and operation research (OR)independently. In the firststage, the uplift models are deployed to predict the treatment ef-fects of individuals. Some prior works have focused on the designof uplift models, including Meta-Learners , Causal Forests, representation learning and rank model. However, standard loss functions (such as mean square errorand cross-entropy error) for training uplift models do not take thedownstream OR into account. In the second stage, the resourceallocation problem is represented as a multi-choice knapsack prob-lem (MCKP), which is NP-Hard and efficiently solved based onLagrangian duality theory .Decision-Focused Learning(DFL). DFL is considered an appro-priate alternative to the two-stage method, which integrates predic-tion and optimization into an end-to-end system. Since computingdecision loss requires solving optimization problems, which usu-ally involve non-differentiable operations, automatic differentiationin machine learning frameworks (such as Pytorch and Ten-sorflow ) cannot give the correct gradient. Three categories ofapproaches to gradient computation are proposed by prior DFLworks: analytical smoothing of optimization mappings, smooth-ing by random perturbations, and differentiation of surrogate lossfunction. The first method derives the analytic gradient of deci-sion loss by using the KKT condition or the homogenous self-dualformulation, including Optnet , DQP , QPTL , and In-tOpt . However, when the optimization problem is discrete, themethod requires a continuous relaxation of the primal problem,which results in suboptimality. A potential resolution is to considerevery optimization problem as a black-box optimization and utilizerandom perturbations, such as DBB , DPO , and I-MLE ,to generate approximate gradient. Furthermore, the decision lossis typically discontinuous and nonconvex, so some of these workssuggest convex surrogate functions, including SPO , LTR ,NCE , and LODL , for the decision loss.The most related works to ours are DRP and DPM . DRPproposes to directly learn ROI (ratio between incremental values and incremental costs) to rank and choose individuals in the binarytreatment setting. It has been shown by that the loss function inDRP is unable to converging to a stable extreme point. DPM extendthe idea to the multiple treatments setting by directly learning theunbiased estimation of the decision factor in OR. However, theconstruction of the decision factor in multi-treatment setting relieson the law of diminishing marginal utility, which does not holdstrictly in some scenarios of marketing.",
  "Problem Formulation": "In this section, we formalize the resource allocation problem andintroduce the overall optimization objective in marketing.We start with a common marketing scenario that has typesof treatments. Let and be the revenue and cost of individual under treatment , respectively. The objective is to find an opti-mal allocation strategy for a group of individuals to maximize therevenue of the platform, given a limited budget . Therefore, thebudget allocation problem with multiple treatments (MTBAP) canbe formulated as an integer programming problem (1):",
  "(1)": "where {0, 1} is the decision variable to denote whether to as-sign treatment to individual . The first constraint is the limitationof the budget and the second one requires that only one treatmentis assigned to each individual. Since the budget fluctuates a lot inreal-world settings, the objective is regared as a function of the bud-get and the overall marketing goal is to maximize revenue (, )within arbitrary given budget.Combinatorial Optimization Algorithm. When the value of and are known in advance, MTBAP is a classical multiplechoice knapsack problem (MCKP) , which remains NP-Hard.Existing studies usually solve this problem by using greedy algo-rithms or Lagrangian duality theory, both of which can provide aapproximation ratio of",
  "OPT,": "where OPT is the optimal solution. In the above equation, max refers to the revenue of one individual (e.g., one user or one shop),which is negligible compared with OPT that is the sum of therevenue of all the individuals in marketing. Therefore, it indicatesthat both greedy algorithms and Lagrangian duality theory canachieve near optimal performance, which are also the most commonalgorithms to solve MTBAP in marketing. The details can be foundin existing works, which will not be discussed in this paper.Model Prediction. However, the value of and are unknownduring decision making in real-world applications, which are usu-ally replaced with the prediction value. Therefore, how to make theprediction of and plays important roles in marketing effec-tiveness, which will be addressed in this paper. In the traditional",
  "Definition 1 (The fundamental problem of causal infer-ence). For all individuals, only one of all the potential outcomesunder different treatments can be observed in real-world data": "Therefore, L cannot be directly computed according to Eq. 2since 1 and 2 (or equivalently, 1 and 2) cannot be simul-taneously observed for any 1 2. To solve this problem, wefirst formulate the training data set and then develop a equivalentprediction loss in marketing.Data Set. Suppose that there is a data set of size collectedfrom random control trials (RCT). The -th sample is denoted by(,,, ), where is the features of individual , is theassigned treatment, and , are the revenue and the cost ofindividual under treatment . Denote the count of the samples(individuals) receiving treatment by .Prediction Loss. Given the above data set, we present the predic-tion loss in marketing in Eq. (3). Theorem 1 presents the equivalencyand the detailed proof can be found in Appendix A.",
  "Decision Loss": "As is stated in Sec. 3, the ground-truth value of and are usuallyunknown in advance, which are replaced with the prediction and during decision making. Therefore, denote the original optimiza-tion problem (, ) by (, , , ), and the solution (, , ) isobtained by solve MTBAP (, , , ), i.e.,",
  "Learning Framework": "Algorithm 1 presents the framework of decision focused causallearning (DFCL). The most crucial step in this framework is thegradient estimation of L in line 10 of Algorithm 1. However,it is non-trival in marketing due to the following technologicalchallenges, i.e., uncertainty of constraints, counterfactual and com-putation cost. In this paper, we will show how to address thesechallenges and how to deploy DFCL in marketing optimization.",
  "Gradient Estimation of DFCL": "The loss of DFCL consists of the prediction loss and the decision loss.The former is a continuously differentiable function whose gradientcan be directly computed. Hence, the gradient estimation of thedecision loss is the key focus of this section. Firstly, we introduce theequivalent dual decision loss to remove the uncertain constraintsand reduce the computation cost of combinatorial optimizationalgorithms. Secondly, we develop two surrogate loss functions andimprove the black-box optimization algorithm to provide a gradientestimation of the dual decision loss.",
  "= min0 (, ,,).(4)": "The optimal Lagrange multiplier for the dual problem (4) can beobtained by using a gradient descent algorithm or a binary searchmethod with the terminal condition of or .In addition, an approximately optimal solution for the originalproblem can be derived by maximizing (, , ,,). Theorem 2presents the relationship between the original problem (, ,,)and the dual problem (, ,,).",
  "(, ,,) 1": "The last equality holds because (, ,,) is the sum of the rev-enue of millions of individuals in marketing, which means that (, ,,) max .Therefore, instead of the original problem, the optimization ofthe dual problem (, , ,,) is taken as the learning objective,which we call the dual decision loss. Given the optimal andthe prediction value , , the solution (, , , ) is obtained bymaximizing (, , , , ), i.e.,",
  "( ) (, , ))": "Similarly, since and is irrelevant to the prediction value , , can be regarded as a constant and removed from the dualdecision loss. According to Theorem 2, is monotonic decreasingwith the increment of the budget and there is an unique forthe dual problem when given the budget . Therefore, the decisionloss L(,, , ) in the original problem under arbitrary budget can be transformed to the dual decision loss L(,, , ) underarbitrary Lagrange multiplier , i.e.,",
  "( )exp( ) exp( ) (5)": "Let (, , ) = exp( )/ exp( ) be the proba-bility of assigning treatment to individual . Take as thereward of assigning treatment to individual . Hence, minimizingL(,, , ) is equivalent to maximizing the expected reward ofpolicy = (, , ) under different Lagrange multipliers. There-fore, L is also called the policy learning loss.Due to the counterfactual in marketing, L(,, , ) cannotbe directly computed by Eq. (5) in training data sets. Instead, wepropose a surrogate loss, i.e.,",
  "Maximum Entropy Regularized Loss": "In order to obtain a differentiable closed form of (, , ) withrespect to and , we relax the discrete constraint {0, 1} to acontinuous one and add a maximum entropy regular-izer to the objective function in (, , , ). Hence, (, , , ) istransformed to a nonlinear convex function, i.e.,",
  "Improved Finite-Difference Strategy": "In addition to constructing surrogate loss functions, we can also usethe Expected Outcome Metric (EOM) to give an unbiasedestimate of the decision loss and leverage black-box optimizationfor decision-focused learning.EOM is a commonly used method for offline strategy evaluationbased on randomized dataset. Given a batch of random sam-ples and model predictions and , an arbitrary allocation strategy(, ) can be evaluated: (1) find the set of individuals whose re-ceived treatment is equal to the treatment in the allocation strategy(, ), (2) then empirically estimate their per capita revenue andper capita cost:",
  "(,,, , ) (,,, , )": "Although we avoid solving the primal MCKP, it is still necessaryto frequently evaluate the per capita revenue and per capita costafter perturbation under multiple Lagrangian multipliers. We ob-serve that the decision making is independent for each individualthanks to the decomposition of the Lagrangian duality theory. Thus,for each sample, the smallest perturbation that causes a changein the dual decison loss is first calculated, and the loss after theperturbation is obtained by correcting only the original result. Ap-pendix D provides details of the modified gradient estimator, whichgreatly reduces the computational overhead. Finally, the black-boxoptimization loss function can be rewritten as",
  "CRITEO-UPLIFT v2. This public dataset is provided by theAdTech company Criteo in the AdKDD18 workshop. Thedataset contains 13.9 million samples collected from a random": "control trial (RCT) that prevents a random part of users from be-ing targeted by advertising. Each sample has 12 features, 1 binarytreatment indicator and 2 response labels(visit/conversion). Inorder to study resource allocation problem under limited budgetusing the dataset, we follow and take the visit/conversionlabel as the cost/value respectively. We randomly sample 70%samples for training and the remaining samples for test. Marketing data. Discounting is a common marketing campaignin Meituan, an online food delivery platform. We conduct a two-week RCT to collect data in this platform. The online shops onthe platform offer daily discounts to users. Note that to avoidprice discrimination, the discount of a shop is the same for allindividuals, but it changes randomly each day and varies fromshop to shop. The data in the first week is used for training andthe other for test. The discount {0, 5, 10, 15, 20} is taken asthe treatment, where = means % off for each order whoseprice meets a given threshold. The dataset contains 2.8 millionsamples, and each sample has 107 features, 1 treatment label and2 response labels (daily cost/orders). 6.1.2Evaluation Metrics. Multiple evaluation metrics are providedfor offline evaluation in this experiment. In addition to adoptingthe evaluation metrics commonly used in two-stage models, suchas Logloss and MSE, we also use the following metrics for policyevaluation with counterfactuals, which are more significant. AUCC (Area under Cost Curve). A common metric used inexisting works , which is designed for evaluating theperformance to rank ROI of individuals in the binary treatmentsetting. We use the metric to compare the performance of differ-ent methods in CRITEO-UPLIFT v2. EOM (Expected Outcome Metric). EOM is also commonlyused in . Based on RCT data, an unbiased estimationof the expected outcome (per-capita revenue/per-capita cost) forarbitrary budget allocation policy can be obtained. The detailsof EOM are shown in Sec. 5.4. We use the metric to compare theperformance of different methods in Marketing data.",
  "Benchmarks. For each dataset in this paper, multiple modelsand algorithms are implemented and taken as benchmarks": "TSM-SL. The two-stage method is mentioned in many exstingworks. In the first stage, a well-trained S-Learnermodel is used to predict the response (revenue/cost) of individu-als under different treatments. In the second stage, we find theoptimal budget allocation solution for an MCKP formulationbased on the predictions. TSM-CF. Also a two-stage method, the difference with TSM-SLis that instead of S-learner, we use a Causal Forests to predictthe incremental response in the first stage. It is implementedhere base on EconML packages , which can support binarytreatment and multiple treatments.",
  "Implementation Details": "CRITEO-UPLIFT v2. For the baseline methods (TSM-SL, TSM-CF and DPM), we cite the results directly from. The DFCLmodel uses the same DNN architecture with a shared layer thatis a single-layer MLP of dimension 128 and four head networksthat are two-layer MLPs of dimension . Except for thefinal output layer, the remaining layers use ReLU activations.For DFCL-MER, we set the temperature = 3. Our models aretrained for 40 epochs with the Adam optimizer . In order toaccelerate the training, the first twenty epochs are warmstarting using the cross-entropy loss, and then the models are trainedusing the DFCL loss. Marketing data. In the multi-treatment experiment, the modelsneed to predict the revenue and cost under five treatments. TSM-SL, CN, CN+DFCL-PL, DFCL-PL, DFCL-MER and DFCL-IFD usethe same DNN architecture: a 4 layers MLP (64-32-32-10). Thefirst five outputs of the models are the predicted revenue, and theremaining outputs are the predicted cost. For DFCL-MER, we setthe temperature = 0.01. For DPM, a S-Learner model is trainedusing the customized loss proposed in to directly predictmarginal utility under different treatments. The DPM model has4 layers of MLP (64-32-32-1), with the last layer using a sigmoidactivation and each of the remaining layers with ReLU activations.All neural network-based models are trained for 500 epochs usingthe Adam optimizer. For TSM-CF, we set _ = 256,__ = 300 and = 24.All experiments are run on AMD EPYC 7502P Rome 32x@ 2.50GHzprocessor with 64GB memory.",
  ": Offline experiment results": "method performs best on common metrics, which minimizes MSEor Logloss on the training set. However, what we really focus on isthe decision quality of predictions. a and present thecomparison between our proposed methods and other benchmarksin CRITEO-UPLIFT v2 on AUCC , which represents the deci-sion quality under binary treatments. We can see that DFCL-IFDachieves the best performance, DFCL-PL, and DFCL-MER performsimilarly to DPM, and the two-stage methods perform the worst.In marketing data, we use EOM method to calculate per-capitaorders and per-capita budgets based on predictions. The results areshown in and b. Our models significantly outperformthe baseline models in terms of per-capita orders at all per-capitabudgets. DPM is on par with the two stage methods in the lowper-capita budgets and outperforms them in the high per-capitabudgets. CN has a marginal improvement of 0.16% compared tothe two-stage methods. Further evaluation is carried out on themodel trained with DFCL loss, which comprises MSE loss (L)and policy learning loss (L). The integration of policy learningloss yielded a notable enhancement in performance, with the con-strained network showing a significant increase of 1.26%. Thesefindings suggest that our proposed DFCL approach is versatileand can be integrated into existing methodologies. Interestingly,the constraint network combined with policy learning loss (CN +DFCL-PL) did not outperform DFCL-PL alone. We hypothesize thatthis may be due to the predefined constraints within the network,which potentially restrict the expansiveness of the decision space. 6.2.2Prediction Loss vs Decision Loss tradeoff. As mentioned above,we integrate the prediction loss as a regularizer into the trainingobjective. In this experiment, we will consider how the weightof the prediction loss affects the performance of DFCL. We set {0.1, 0.5, 1, 2, 3, 4, 5, 10} and measure the per-capita orders undera fixed per-capita budget. As shown in a, increasing in acertain range does not lead to a decrease in model performance.",
  "ModelBudgetImprovement123456": "TSM-SL1.0000 0.00231.0300 0.00221.0611 0.00231.0873 0.00221.1140 0.00201.1437 0.0021/TSM-CF1.0006 0.00071.0306 0.00061.0592 0.00041.0866 0.00061.1109 0.00031.1353 0.0008-0.19%DPM0.9983 0.00111.0366 0.00101.0720 0.00061.1050 0.00031.1305 0.00101.1594 0.00071.00%CN1.0047 0.00131.0339 0.00101.0622 0.00071.0910 0.00091.1151 0.00111.1384 0.00150.16%CN+DFCL-PL0.9995 0.00031.0366 0.00081.0739 0.00091.1071 0.00051.1367 0.00061.1650 0.00091.26%DFCL-PL1.0104 0.00051.0465 0.00061.0812 0.00041.1118 0.00071.1407 0.00111.1638 0.00181.98%DFCL-MER1.0178 0.00081.0501 0.00051.0810 0.00021.1121 0.00101.1410 0.00131.1674 0.00092.06%DFCL-IFD1.0197 0.00121.0574 0.00221.0902 0.00241.1221 0.00261.1516 0.00281.1796 0.00302.85% However, if is too large, the prediction loss dominates the trainingobjective and the model will be reduced to the two-stage method.The experiment suggests it is possible to choose a value of so thatwe can achieve better performance and more accurate predictions. 6.2.3Impact of Lagrange multiplier. Next, we would like to discussthe impact of the Lagrange multiplier on the performance ofDFCL model. Since a given Lagrange multiplier corresponds tothe MCKP for a certain budget constraint, DFCL model can learnallocation policies for different budgets simultaneously by changingor adding to the DFCL loss. We set up different combinationsof Lagrange multipliers ( {{0.1}, {0.1, 0.5}, {0.1, 0.5, 1.0}}) anduse to train DFCL models. b shows the results using DFCL-IFD models trained by combinations of Lagrange multipliers. Wecan observe that is a hyperparameter that can have a significantimpact on model performance. A small enables the model to learnthe allocation policy efficiently under high budget and vice versa.Moreover, models trained with multiple Lagrange multipliers canbalance performance with different budgets.",
  "Online A/B testing": "6.3.1Setups. We deploy DFCL, DPM and TSM-SL to support a dis-count campaign in Meituan (a food delivery platform), and conductan online A/B testing for four weeks. The experiment contains 310Konline shops and they are randomly divided every day into threegroups called G-DFCL, G-DPM and G-TSL respectively. Each shopwill be assigned a discount {0, 5, 10, 15, 20} as the treatmemt,which means % off for each order whose price meets a given thresh-old. The marketing goal is to maximize the orders by assigning anappropriate discount to each store every day for a limited budget.The online deployment of DFCL is shown in a: (1) Before the campaign starts each day, we use the DFCL model to makepredictions and allocate the appropriate discounts to each storebased on budget and other constraints in an offline environment.(2) The users visit the online shop and get discounts which willstimulate them to make purchases. (3) During model training, weuse historical random data and resource allocation optimizer toupdate the model parameters.",
  ": Online A/B testing": "6.3.2Results. b illustrates the improvement in weekly ordersfor G-DFCL and G-DPM relative to G-TSM. In order to preservedata privacy, all data points in b have been normalized that aredivided by the orders of TSM-SL in the first week. We can see thatDFCL achieves a significant average improvement of 2.17% relativeto TSM-SL and also outperforms DPM with a relative improvementof 0.85%. The detailed results can be found in Appendix E.",
  "Conclusion": "In this paper, we propose a decision focused causal learning frame-work (DFCL) for direct counterfactual marketing optimization,which overcomes the technological challenges of DFL deploymentin marketing. By designing surrogate losses and constructing black-box optimisation, we efficiently align the objectives of ML and OR.Both offline experiments and online A/B testing demonstrate theeffectiveness of DFCL over the state-of-the-art methods.",
  "KDD 24, August 2529, 2024, Barcelona, Spain.Hao Zhou et al": "Martn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al.2016. Tensorflow: Large-scale machine learning on heterogeneous distributedsystems. arXiv preprint arXiv:1603.04467 (2016). Meng Ai, Biao Li, Heyang Gong, Qingwei Yu, Shengjie Xue, Yuan Zhang, YunzhouZhang, and Peng Jiang. 2022. LBCF: A Large-Scale Budget-Constrained CausalForest Algorithm. In Proceedings of the ACM Web Conference 2022. 23102319. Javier Albert and Dmitri Goldenberg. 2022. E-commerce promotions personaliza-tion via online multiple-choice knapsack with uplift modeling. In Proceedings ofthe 31st ACM International Conference on Information & Knowledge Management.28632872.",
  "Susan Athey, Julie Tibshirani, and Stefan Wager. 2019. Generalized randomforests. (2019)": "Keith Battocchi, Eleanor Dillon, Maggie Hei, Greg Lewis, Paul Oka, MirunaOprescu, and Vasilis Syrgkanis. 2019. EconML: A Python package for ML-Basedheterogeneous treatment effects estimation. Version 0. x (2019). Quentin Berthet, Mathieu Blondel, Olivier Teboul, Marco Cuturi, Jean-PhilippeVert, and Francis Bach. 2020. Learning with differentiable pertubed optimizers.Advances in neural information processing systems 33 (2020), 95089519. Artem Betlei, Eustache Diemert, and Massih-Reza Amini. 2021. Uplift model-ing with generalization guarantees. In Proceedings of the 27th ACM SIGKDDConference on Knowledge Discovery & Data Mining. 5565.",
  "Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-mization. arXiv preprint arXiv:1412.6980 (2014)": "Sren R Knzel, Jasjeet S Sekhon, Peter J Bickel, and Bin Yu. 2019. Metalearners forestimating heterogeneous treatment effects using machine learning. Proceedingsof the national academy of sciences 116, 10 (2019), 41564165. Finn Kuusisto, Vitor Santos Costa, Houssam Nassif, Elizabeth Burnside, DavidPage, and Jude Shavlik. 2014. Support vector machines for differential prediction.In Machine Learning and Knowledge Discovery in Databases: European Conference,ECML PKDD 2014, Nancy, France, September 15-19, 2014. Proceedings, Part II 14.Springer, 5065. Jayanta Mandi, Vctor Bucarey, Maxime Mulamba Ke Tchomba, and Tias Guns.2022. Decision-focused learning: through the lens of learning to rank. In Interna-tional Conference on Machine Learning. PMLR, 1493514947.",
  "Jayanta Mandi and Tias Guns. 2020. Interior point solving for lp-based prediction+optimisation. Advances in Neural Information Processing Systems 33 (2020), 72727282": "Jayanta Mandi, James Kotary, Senne Berden, Maxime Mulamba, Victor Bucarey,Tias Guns, and Ferdinando Fioretto. 2023. Decision-focused learning: Foun-dations, state of the art, benchmark and future opportunities. arXiv preprintarXiv:2307.13565 (2023). Jayanta Mandi, Peter J Stuckey, Tias Guns, et al. 2020. Smart predict-and-optimizefor hard combinatorial optimization problems. In Proceedings of the AAAI Con-ference on Artificial Intelligence, Vol. 34. 16031610. Maxime Mulamba, Jayanta Mandi, Michelangelo Diligenti, Michele Lombardi,Victor Bucarey, and Tias Guns. 2020. Contrastive losses and solution caching forpredict-and-optimize. arXiv preprint arXiv:2011.05354 (2020).",
  "Xinkun Nie and Stefan Wager. 2021. Quasi-oracle estimation of heterogeneoustreatment effects. Biometrika 108, 2 (2021), 299319": "Mathias Niepert, Pasquale Minervini, and Luca Franceschi. 2021. Implicit MLE:backpropagating through discrete exponential family distributions. Advances inNeural Information Processing Systems 34 (2021), 1456714579. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, GregoryChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019.Pytorch: An imperative style, high-performance deep learning library. Advancesin neural information processing systems 32 (2019).",
  "Stefan Wager and Susan Athey. 2018. Estimation and inference of heterogeneoustreatment effects using random forests. J. Amer. Statist. Assoc. 113, 523 (2018),12281242": "Chao Wang, Xiaowei Shi, Shuai Xu, Zhe Wang, Zhiqiang Fan, Yan Feng, An You,and Yu Chen. 2023. A Multi-stage Framework for Online Bonus Allocation Basedon Constrained User Intent Detection. In Proceedings of the 29th ACM SIGKDDConference on Knowledge Discovery and Data Mining. 50285038. Bryan Wilder, Bistra Dilkina, and Milind Tambe. 2019. Melding the data-decisionspipeline: Decision-focused learning for combinatorial optimization. In Proceedingsof the AAAI Conference on Artificial Intelligence, Vol. 33. 16581665. Ziang Yan, Shusen Wang, Guorui Zhou, Jingjian Lin, and Peng Jiang. 2023. AnEnd-to-End Framework for Marketing Effectiveness Optimization under BudgetConstraint. arXiv preprint arXiv:2302.04477 (2023). Liuyi Yao, Sheng Li, Yaliang Li, Mengdi Huai, Jing Gao, and Aidong Zhang. 2018.Representation learning for treatment effect estimation from observational data.Advances in neural information processing systems 31 (2018). Peng Ye, Julian Qian, Jieying Chen, Chen-hung Wu, Yitong Zhou, SpencerDe Mars, Frank Yang, and Li Zhang. 2018. Customized regression model forairbnb dynamic pricing. In Proceedings of the 24th ACM SIGKDD internationalconference on knowledge discovery & data mining. 932940. Yang Zhang, Bo Tang, Qingyu Yang, Dou An, Hongyin Tang, Chenyang Xi,Xueying Li, and Feiyu Xiong. 2021. BCORLE (): An Offline ReinforcementLearning and Evaluation Framework for Coupons Allocation in E-commerceMarket. Advances in Neural Information Processing Systems 34 (2021), 2041020422. Kui Zhao, Junhao Hua, Ling Yan, Qi Zhang, Huan Xu, and Cheng Yang. 2019. Aunified framework for marketing budget allocation. In Proceedings of the 25thACM SIGKDD International Conference on Knowledge Discovery & Data Mining.18201830. Yan Zhao, Xiao Fang, and David Simchi-Levi. 2017. Uplift modeling with mul-tiple treatments and general response types. In Proceedings of the 2017 SIAMInternational Conference on Data Mining. SIAM, 588596. Hao Zhou, Shaoming Li, Guibin Jiang, Jiaqi Zheng, and Dong Wang. 2023. Directheterogeneous causal learning for resource allocation problems in marketing. InProceedings of the AAAI Conference on Artificial Intelligence, Vol. 37. 54465454.",
  "AThe Proof of Theorem 1": "Proof. First of all, we introduce some notations. Following thepotential outcome framework , let R denote the featurevector and {1, 2, . . . , } be the treatment. Let () and ()be the potential outcome of the revenue and the cost respectivelywhen the individual receives treatment . let () and () bethe predicted outcome of the revenue and the cost respectivelywhen the individual receives treatment . For L, we have",
  "DLagrangian Duality Gradient Estimator": "The decision making is independent for each individual thanksto the decomposition of the Lagrangian duality theory. Thus, foreach sample, the smallest perturbation that causes a change in thedual decison loss is first calculated, and the loss after the perturba-tion is obtained by correcting only the original result. Algorithm 2provides details of the modified gradient estimator, which greatlyreduces the computational overhead. Note that for comprehensibil-ity, Algorithm 2 is described with for loops, while in practice wework with matrix operations."
}