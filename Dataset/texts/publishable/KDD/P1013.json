{
  "Abstract": "The diversity of recommendation is equally crucial as accuracy inimproving user experience. Existing studies, e.g., DeterminantalPoint Process (DPP) and Maximal Marginal Relevance (MMR), em-ploy a greedy paradigm to iteratively select items that optimize bothaccuracy and diversity. However, prior methods typically exhibitquadratic complexity, limiting their applications to the re-rankingstage and are not applicable to other recommendation stages with alarger pool of candidate items, such as the pre-ranking and rankingstages. In this paper, we propose Contextual Distillation Model(CDM), an efficient recommendation model that addresses diver-sification, suitable for the deployment in all stages of industrialrecommendation pipelines. Specifically, CDM utilizes the candidateitems in the same user request as context to enhance the diversifi-cation of the results. We propose a contrastive context encoder thatemploys attention mechanisms to model both positive and negativecontexts. For the training of CDM, we compare each target itemwith its context embedding and utilize the knowledge distillationframework to learn the win probability of each target item under theMMR algorithm, where the teacher is derived from MMR outputs.During inference, ranking is performed through a linear combina-tion of the recommendation and student model scores, ensuringboth diversity and efficiency. We perform offline evaluations ontwo industrial datasets and conduct online A/B test of CDM on theshort-video platform KuaiShou. The considerable enhancements",
  "Equal contributions.Corresponding author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08",
  "Recommender System, Knowledge Distillation, Diversified Recom-mendation": "ACM Reference Format:Fan Li, Xu Si, Shisong Tang, Dingmin Wang, Kunyan Han, Bing Han, GuoruiZhou, Yang Song, and Hechang Chen. 2024. Contextual Distillation Modelfor Diversified Recommendation. In Proceedings of the 30th ACM SIGKDDConference on Knowledge Discovery and Data Mining (KDD 24), August2529, 2024, Barcelona, Spain. ACM, New York, NY, USA, 10 pages.",
  "Introduction": "With the prevalence of Web 2.0 and mobile devices, an increasingnumber of users are joining online feed stream platforms such asTikTok1, Douyin2, and KuaiShou3 for content sharing and con-sumption . To alleviate the issue of content homoge-nization and enhance user engagement, it is important to considerthe diversity in recommendation systems. The diversity quanti-fies the dissimilarity among recommended items based on certainpre-selected attributes (e.g., item category), which is as crucial as ac-curacy. Enhanced diverity can significantly contribute to the overalluser experience by encouraging exploration of new content anddiscovery of new interests . illustrates the stages of an industrial recommenda-tion pipeline. Existing research on diversified recommendation, such as the Determinantal Point Process (DPP)",
  ": An industry recommendation pipeline and differentitem sets for one user request. Candidate items refer to: theset of items that are to be scored one user request during theranking stage": "and Maximal Marginal Relevance (MMR) , has predominantly fo-cused on enhancing diversity in the re-ranking stage. These greedy-based approaches have successfully augmented the diversity ofre-ranking recommendation model. However, the necessity for di-versity extends beyond the re-ranking stage, encompassing earlierstages such as pre-ranking and ranking. The absence of diversity inthese preliminary stages will render subsequent efforts to enhancediversity during re-ranking ineffective against the backdrop of ahighly homogenized candidate item pool, leading to a predicamentmetaphorically described as \"You cant make bricks without straw.\"Therefore, ensuring the diversity of the entire pipeline stages iscrucial in industrial recommendation pipeline.However, the existing diversity algorithms designed for the re-ranking stage are characterized by quadratic computational com-plexity, making them unsuitable for stages involving extensivecandidate item pools, such as pre-ranking and ranking, due to theirprohibitively high computational demands. Consequently, there isa pressing need to develop an algorithm that is not only efficientin handling large sets of candidates but also capable of effectivelyenhancing diversity during the pre-ranking and ranking stages,thereby significantly elevating the overall efficacy of recommenda-tion pipelines.In this paper, we introduce Contextual Distillation Model (CDM),an efficient recommendation model designed to enhance diversifi-cation across all stages of the industrial recommendation pipeline.CDM innovatively addresses the need for both efficiency and diver-sity by training a surrogate model of the quadratic time-complexityMMR algorithm. By employing the knowledge distillation frame-work , the surrogate model is trained to estimate the probabilityof an item being selected by the MMR algorithm (i.e. the Top-Kitems). This strategy enables the deployment of an end-to-end stu-dent model that enhances diversity in the pre-ranking and rankingstages in an efficient manner.The next problem is how to architecture the student model. Weknow that diversity serves as a metric for evaluating sets. However,models in pre-ranking and ranking stages traditionally score eachitem independently (point-wise). Therefore, to introduce set infor-mation into the scoring process, CDM leverages the candidate itemswithin the same user request as context to enhance the diversity ofrecommendations. Specifically, we propose a Contrastive ContextEncoder (CCE) to model the context for each target item. Giventhe large number of candidates in pre-ranking and ranking stages, Candidate itemsHistorical interested items Positive contextNegative contextTarget item : The probability density distribution of a target item,where randomly selected from a candidate set in a single userrequest, corresponds to different sets of item embeddings.The positive context comprises the 100 candidate items mostsimilar to target item, and the negative context, the leastsimilar. its impractical to incorporate all context into calculations withthe target item, necessitating a sampling work on the context. Weposit that for any given target item, there exist two distinct types ofcontexts within the candidate pool: highly similar items (positivecontext) and markedly dissimilar items (negative context). shows the probability density distribution of a target item, whererandomly selected from a candidate set in a single user request,corresponds to different sets of item embeddings. It is essential tosample these two contexts, executed by employing a Gumbel-Top-strategy based on the target attention scores between thetarget item and the context. After that, we employ the contrastiveattention mechanism to effectively model the distinguishingrepresentations between the two opponent contexts, enhancingthe models ability to capture diverse patterns. This mechanism,initially introduced in the field of computer vision , has beenprimarily utilized for person re-identification by contrastively at-tending to person and background regions.Ultimately, by comparing each target item with its context em-bedding, we can discern whether an item surpasses the aggregateperformance, thereby estimating its probability of being selectedin the MMR algorithm . During the inference phase, rankingis performed through a linear combination of scores from boththe recommendation and student models, ensuring diversity andefficiency. This approach not only theoretically pioneers a novelsolution but also empirically validates the efficacy of CDM in ele-vating both the quality and diversity of recommendations throughoffline evaluations on two industrial datasets and an online A/Btest on the short-video platform KuaiShou.",
  "Preliminaries2.1Traditional Recommendation": "Let U = {1, ...,} and I = {1, ..., } denote the set of usersand candidate items within a single user request, respectively,where is the number of users, is the number of candidateitems. The user-item historical interactions are represented byD = {(,,)| U, I}, where {0, 1} denotes the bi-nary label (e.g., finish playing or like). The target of the traditionalrecommendation training is to learn the scoring function (,|)from D, which is capable of predicting the preference of user onitem , where is the parameters of .",
  "Diversified Recommendation": "Diversified recommendation aims to provide users with a set ofaccurate and diverse items. It can be formally defined as a subset se-lection problem. For each user U, the objective is to determinea ranked list R = {1,2, . . . , } of items from the candidate setI that maximizes a linear combination of accuracy and diversifica-tion, which follows the classical work Maximal Marginal Relevance(MMR):",
  "Approximation Calculation": "Our current challenge revolves around a discrete optimization prob-lem. Specifically, it is tasked to select a subset R of size fromcandidate items I to optimize MMR. However, existing research hasdemonstrated that maximizing a submodular function with cardi-nality constraints constitutes an NP-hard problem . Some workshave designed (11/)-approximation algorithms for this problembased on greedy techniques . That is, the construction of Rfollows an iterative process according to marginal gain:",
  "= argmaxI\\R1:1Acc(,) + Div(R1:1 |),(2)": "where represents the current selection step, and our choices areinfluenced by the outcomes of the preceding 1 steps.However, the greedy selections quadratic complexity, at(2),limits its utility primarily to the re-ranking stage. Such high compu-tational demands render it impractical for other recommendationstages, notably the pre-ranking and ranking stages, where a largerpool of candidate items necessitates more scalable solutions. Con-sequently, our objective is to achieve the benefits of the greedyapproachs optimal results while significantly reducing the compu-tational overhead in these more extensive stages.",
  "Method3.1Interest-Aware MMR": "The fundamental technique for diversity ranking is known as Max-imum Marginal Relevance (MMR) , and considerable researches have been dedicated to enhancing and extending algo-rithms grounded in MMR principles. In this section, we presentan enhancement of MMR that incorporates user interests, therebyexpanding the scope of diversity considerations. For the accuracyscore ACC(,), we directly employ the score function (,). Tocompute the diversity score, we incorporate the user interest inthe measurement of item similarity. This incorporation links di-versity measurement to the users personal perception of diversitystemming from distinct interests. Specifically,",
  "Div(R1:1 |) = 1 max R1:1(e, e,),(3)": "where e, = e u refers to the Hadamard product between theitem embedding e and the user interest vector u. In this diversityfunction, we subtract the maximal similarity between selected itemsR and the target item as the diversity score.Lets begin by addressing the selection of the initial item, referredto as 1. In this simple case, where no items have been selected yet(i.e., R = ), it simplifies to choosing the item with the highestsingle-item accuracy score, as expressed by:",
  "section, we focus on how to learn the context embeddings fromthe candidates of target item for serving as the inputs to () in theranking stage": "3.2.1Differentiable Sampling with Gumbel-Top- Reparam-eterization. During the ranking stage of industrial recommen-dation, the size of candidate items for a single user request isconsiderable, as shown in Fig 1. Incorporating all candidate itemsinto graph computations brings additional overhead and increasesundesirable noise that affects context learning. Opting for sam-pling emerges as an efficacious solution. However, simple randomsampling falls short in resolving the aforementioned issues. Upondelving into the item distribution within each user request, wediscover a significant pattern: each target item invariably has bothrelevant and competing counterparts within the candidate items.Interestingly, the distribution of relevant items tends to be denselyclustered, while that of competing ones exhibit a looser distribu-tion, as shown in Fig 2. In essence, this observation motivates us tosample from these two distributions separately, thereby modelingboth positive and negative context information for each target item.Given a candidate item set I = {1, . . . ,, . . . , } and a targetitem , we compute the attention score = (,) betweentarget item and each candidate item . However, the sampling ofw is non-differentiable and prevents the model from being trainedwell via back-propagation. To solve this issue, we adopt a differen-tiable ordered subset sampling without replacement by generalizingthe Gumbel-Softmax reparametrization to the Gumbel-Top-trick . We define the probability of as (|) = . Theperturbed weights are then obtained by adding Gumbel noise tothe log probabilities :",
  "= log( log()) Gumbel(0, 1),(6)": "where Uniform(0,1). Now, we need to consider how to turnthe perturbed weights w = [1, ..., ] into one-hot vectors a =[a1, ..., a ], s.t. a = .Following the work , we propose to use a recent top- re-laxation based on successive applications of the softmax function.Specifically, define for all candidate item indices = 1, ..., anditeration steps = 1, ...,:",
  ".(10)": "Previous work has demonstrated that as the temperature hyper-parameter 0, the expectation a1 = [P(a11 = 1), ..., P(a1 = 1)] ofthe initial sampled vector w approximates a one-hot encoding. Con-sequently, the logit update in Eq.(9) will also converge to the hardupdate from Eq.(7). By induction, this convergence trend extends toothers, resulting in a one-hot encoding. Significantly, it aligns withthe Gumbel-Softmax reparametrization when = 1. In the end,the output relaxed -hot vector a transforms into a hard encodingthat satisfies the desired property of a = as 0. Tocounterbalance the influence of model initialization and broadenparameter exploration, we initiate model training with a compara-tively larger value of for expectation computation. Subsequently,we implement a gradual annealing process that progressivelyreduces to a small constant value during training.Now, we can employ the Gumbel-Top- trick to sample relevantcandidate items I= {1,2, ...,} as positive contexts,and competing candidate items I= {1,2, ...,} asnegative contexts for each target item from its correspondingcandidate item set I.",
  "Contextual Distillation Model for Diversified RecommendationKDD 24, August 2529, 2024, Barcelona, Spain": "and MMR, which are constrained by quadratic complexity to there-ranking stage, we introduced the Contextual Distillation Model(CDM). Designed for efficient diversification across all recommenda-tion stages, CDM leverages contextual information from candidateitems and employs a contrastive context encoder to model diversecontexts effectively. Through offline and online evaluations, CDMdemonstrated significant improvements in both recommendationquality and diversity, also ensuring efficiency. This work is partially supported in part by the National NaturalScience Foundation of China (No. U2341229); the Key R&D Projectof Jilin Province (No. 20240304200SF); and the International Coop-eration Project of Jilin Province (No. 20220402009GH). Mustafa Abdool, Malay Haldar, Prashant Ramanathan, Tyler Sax, Lanbo Zhang,Aamir Manaswala, Lynn Yang, Bradley Turnbull, Qing Zhang, and ThomasLegrand. 2020. Managing diversity in airbnb search. In Proceedings of the 26thACM SIGKDD International Conference on Knowledge Discovery & Data Mining.29522960.",
  "C = softmax(w) V,(13)": "where V = [1, ...,] W3 utilizes the same parameters asused in target-attention.The separation of positive and negative context embeddingsstems from their distinct roles in relation to the target item, aim-ing to precisely model the set information of the target item. Ourobjective is to ensure that the positive context embedding C accurately captures the semantic essence associated with the targetitem, whereas the negative one C remains devoid of any con-nection to the target items attributes. In practice, we leverage theInfoNCE loss function to reinforce such differentiation.",
  "Contextual Distillation Model": "In this section, we elaborate on how the Contextual DistillationModel (Student) distills knowledge from the greedy-based Maxi-mum Marginal Relevance (Teacher) to achieve low-cost diversifiedrecommendations. We acknowledge that items selected by the MMRalgorithm (i.e., the winning item set, R), are either from niche tagsor represent the finest items within popular tags. The student modelcontrasts the target item with its associated context to discern itsalignment with the majority. Consequently, estimating the winprobability for each item facilitates the guidance of the studentmodels learning process.",
  "Discussion": "3.4.1Time Complexity. The computational efficiency of onlineserving is a critical concern in industrial recommender systems. Fora single user request, where the number of candidate items duringthe pre-ranking and ranking stage is denoted as , and itemsare to be returned. In the case of the MMR algorithm (list-wise),the time complexity for generating the Top- list is (2). Toelaborate, in each round, every remaining candidate item must becomputed with the selected items R. In contrast, the CDM approach(point-wise) provides an efficient end-to-end solution, with a timecomplexity equivalent to the cost of heap-sort algorithm to get theTop- list, which is ( log).It is worth noting that previous methods have predom-inantly found application in the final re-ranking stage, typicallyassociated with smaller and values. However, in the pre-rankingand ranking stage, where and are notably more substantial,concerns pertaining to algorithmic overhead assume heightenedimportance. CDM not only reduces the cost of inference but alsoenhances the original recommendation models ability to assimilatecandidate information and seamlessly integrate diversity. 3.4.2Scalability. To improve diversity within the ranking stage,CDM employs knowledge distillation as an efficient end-to-endmethod for learning diversity. This approach can be applied notonly to the ranking stage but also to other stages, serving as asupplementary module for any embedding-based recommendationmodel. Furthermore, we have utilized MMRs score as the supervisorsignal in this work, its important to note that alternative diversityalgorithms, such as the Determinantal Point Process (DPP) andthe Gram-Schmidt Process (GSP) , can be seamlessly substitutedto tailor the diversity learning process to specific requirements.",
  "Experiment": "In this part, we first present the offline dataset and experimentalconfiguration, as well as the reproducibility of the experiments in.1. Next, .2 and 4.3 show the evaluation results ofthe offline experiments as well as the detailed analysis. Finally, weshow the results of A/B experiments of CDM in real scenarios in.4 to further demonstrate the effectiveness of the scheme.",
  "Dataset": "JingDong : Constructed from search logs of a largeste-commerce platforms advertising system, this dataset en-compasses features such as user id, query id, and item, eachitem containing a quintuple <item id, category id, vendor id,price id>. KuaiShou4: Owing to a lack of available recommendationdatasets with candidate items from upstream pipelines, anew dataset was assembled from the KuaiShou app. It aggre-gates data from 0.5 million active users, randomly sampled,with their logs on the feed page over 5 days. The datasetincorporates user id, tab id, and item, each item presentinga triplet <item id, category id, duration id>. Release of thisdataset is anticipated post-review. 4.1.2Metrics. For evaluation, we employ a comprehensive set ofmetrics to evaluate the performance from both accuracy and diver-sity perspectives. To measure accuracy, we utilize two widely usedmetrics, namely Recall and Mean Reciprocal Rank (MRR). Re-garding recommendation diversity, we incorporate two commonlyused metrics: (1) Intra List Average Distance (ILAD), quantifyingthe dissimilarity among items within the Top- list. It measuresuser-level diversity, where a larger value indicates greater diver-sity in the Top- recommendation list. (2) Category Coverage(CC), which is the ratio of the number of categories covered by theTop- recommendations to the total number of categories in thedataset. It focuses on system-level diversity, where a higher valueindicates that a greater variety of categories is being recommended.The formal definitions of these metrics are as follows:",
  "|| ,(22)": "where denotes the Top- list, CC@ represents the set of uniquecategories within the Top- recommendation list, and CC signifiesthe set of all unique categories within the dataset. Higher valuesof ILAD@ and CC@ are indicative of greater diversity in theTop- recommendations. We report the results for = 3 and = 5on JingDong and = 20 and = 50 on KuaiShou.",
  "MMR : We implement the diversity scoring function fol-lowing MMR which use a coarse-grained item-level diversitythrough re-ranking": "4.1.4Hyper-parameters and training details. We implementthe DCN model with three hidden layers MLP, and the activationfunction is ReLU. We optimize all models with Adam optimizerwith batch sizes of {128, 256} on two datasets. We use grid searchto find the optimal hyperparameters. In the CDM framework, and, which balances accuracy and diversity, are searched in the rangeof {0.0, 0.1, ..., 0.25}. The learning rate is searched in {1 3, 1 4, 1 5}. To avoid overfitting, we set the dropout to 0.25 andthe patience of earlystop to 10 epochs.",
  "Offline Comparison": "In this section, we first evaluate all algorithms based on the accu-racy and diversity as two key metrics. presents the results ofour offline experiments on two industrial dataset. Based on these re-sults, we have the following findings: (1) Compared to the base DCNmodel, all algorithms can enhance both user-level and system-leveldiversity. (2) IPW re-weights the loss based on category distributionto improve diversity. However, it dose not consistently outperformDCN in recommendation diversity or accuracy, due to the inaccu-rate estimation and high variance of propensity scores . (3) DPPand SSD exhibit relatively similar performance. While they achievehigher diversity metrics, their accuracy drops more significantlycompared to DCN. Some previous works suggest that bothof them assume that more orthogonality of recommended itemswould result in larger diversity in recommendation results. How-ever, stronger orthogonality tends to lead to a significant decreasein accuracy, making it hard for them to effectively balance accuracyand diversity. (4) Consistent with previous findings , MMRhas the best performance of all baselines. We believe that underpersonalized recommendation, it is more appropriate to use em-bedded dot product to compute diversity scores than orthogonality.(5) Our proposed CDM effectively improved both recommenda-tion accuracy and diversity on two datasets compared to the baseDCN model. The effectiveness of CDM can be attributed to threefollowing factors: Generalization: CDM extracts diversity scores for eachuser-item pair as supervised signals from MMR, replacingthe iterative strategy of MMR with an end-to-end manner,which may have better generalization.",
  "CDM0.1205+0.0720+0.4426+0.8461+0.2219+0.0894+0.5503+0.9147+": "Capacity: However, just relying on such signals often fallsshort of surpassing teacher model. More significantly, CDMenhances its performance by modeling the distribution ofcandidate items produced by pre-rank model. This enableseach item to be aware of relevant/competing items in the can-didate set, thereby increasing the recommendation modelscapacity . Information Exchange: Additionally, the shared bottomembeddings benefit from additional update path, facilitatinginformation exchange between the recommendation modeland branch model. This, to some extent, can enhance theaccuracy of the original recommendation model.",
  "In this section, we conduct more detailed analysis experiments todemonstrate the effectiveness of CDM": "4.3.1The impact of context modeling. To simultaneously im-prove recommendation accuracy and diversity, CDM introducesan additional branch model on top of the recommendation back-bone model. Next, we investigate the impact of various componentsof CDM on the performance. Specifically, we examine the follow-ing variables of the framework and compare them through offlineexperiments, which showed in .In CDM, the Contrastive Context Encoder (CCE) categorizescandidate items into positive and negative classes and then learnspositive and negative contexts through a contrastive learning ap-proach. From , it can be observed that when the InfoNCEloss, which controls context learning, is removed, CDM experiencesa decrease in both accuracy and diversity. One possible explanationis that without this constraint, the context may capture redundantinformation, leading to reduced distinguishability between positiveand negative contexts.",
  "-CCE0.20360.07860.52230.8884": "To further investigate the CCE module, we chose to directlymodel the input sequence using the target attention, thus elimi-nating the need to distinguish context. It was found that CDMsperformance significantly deteriorated across all metrics. This sug-gests that CCEs strategy of contrastive context modeling enableseach item to perceive information about its similar or competingitems when being scored. This approach allows point-wise trainingto incorporate global candidate information, making it simpler andmore effective. 4.3.2The impact of trade-off parameter . In our framework,each item has two scores: accuracy and diversity. CDM combinestwo scores linearly, using ACC(,) + DIV(,), where is thehyper-parameter to balance accuracy and diversity. Fig 4 illustratesperformance tuning on two datasets. The y-axis represents the dif-ference between the metric value at that and its best value, math-ematically formulated as:Metric() = Metric() Metric().As ascends, RECALL demonstrates a parabolic tendency, firstelevating to a peak and then diminishing, signaling an optimal value at which RECALL is maximized, which may due to the abilityof CDM to perceive relevant/competing information. In contrast,ILAD and CC consistently exhibits an upward trajectory with the",
  ": Performance on accuracy and diversity when vary-ing": "rise of , reflecting a proportional relation that highlights the en-hanced emphasis on diversity with a larger . The results indicatethat it is crucial to introduce a proper amount of diversity into theTop- list to improve the joint utility of accuracy and diversity forfeed recommendation. Therefore, it is crucial to adjust the parame-ter to achieve the optimal balance of accuracy and diversity toprovide the best experience for users in practical applications.",
  "Online A/B Test": "To validate the efficacy of CDM, we deploy it within the feed rec-ommendation pages of two applications of KuaiShou for online A/Btest. The branch network is integrated into the online two-towerrecommendation model, which additionally receives the output (i.e.candidate items) from the pre-rank model, ensuring consistencywith the offline experiments.We evaluate the performance of CDM within a practical appli-cation context of Kuaishou, referencing primarily the subsequentthree metrics: (1) Watch Time: a crucial measure reflecting userengagement and satisfaction. (2) #Vertical Categories: it signi-fies the diversity of presented content, representing the number ofunique video categories showed to all users. (3) Clustering Coef-ficient: this measures the variability in user interaction with therecommended content, quantifying the difference of the numberof videos to the number of unique video categories per session onaverage. A lower value suggests a concentration of user interactionswithin more categories, illustrating increased diversity in contentconsumption.The A/B test spanned ten consecutive days, with the averageperformance on the three aforementioned metrics tabulated in Ta-ble 4, allowing us to formulate the ensuing conclusions. Initially,CDM manifests a marked enhancement in Watch Time, corroborat-ing the models capability to bolster user loyalty and engagement.Subsequently, the augmentation in the quantity of Vertical Cate-gories coupled with the diminution in the Clustering Coefficientof observed videos attests to the promotion of diversity within therecommended outcomes. In essence, CDM fosters both diversity inrecommendations and user engagement by strategically incorpo-rating pipeline information, enabling models in the rank stage toassimilate preferences from upstream models and data regardingcandidates. This substantiates that our framework orchestrates anoptimal balance between precision and diversity.",
  "Related Work": "The discussion on diversified recommendation has been extensivewithin the research community, highlighting the recognition of fac-tors beyond recommendation accuracy as pivotal in elevating over-all user satisfaction . This awareness has led tothe emergence of the accuracy-diversity dilemma, a scenario wherethe attainment of higher accuracy often compromises diversity, andvice versa . A primary cause of this dilemma has been thehistorical focus on accuracy . To improve user satisfaction,two distinct diversity problems have been delineated: aggregationdiversity and individual diversity . Ag-gregation diversity strives to diversify recommendations across theentire user, thereby enhancing the coverage of the recommendersystem. On the other hand, individual diversity focuses on diversify-ing recommendations for a specific user, endeavoring to harmonizeaccuracy and diversity by ensuring a range of different items withina single users recommendation list .In this paper, we focus on the issue of individual diversity. Aclassical approach is the greedy-based iterative selection algorithms. For instance, Maximal Marginal Relevance (MMR) itera-tively selects items, taking into account both accuracy and theirsimilarity to previously selected items. Determinantal point pro-cess (DPP) leverages a symmetric matrix to represent itemqualities and pairwise similarities. Sliding Spectrum Decomposi-tion (SSD) proposes a time series analysis technique to includeout-of-window items into the measurement of diversity to increasethe diversity of a long recommendation sequence and alleviate thelong tail effect as well. Feature Disentanglement Self-Balancing(FDSB) aims to improve the diversity of re-ranking stage by refin-ing MMR in relevant recommendation. proposes a general re-ranking framework named Multi-factor Sequential Re-ranking withPerception-Aware Diversification (MPAD) to jointly optimize accu-racy and diversity for feed recommendation with DPP. Furthermore,some approaches take into account the temporal dimension of rec-ommendations and utilize time series analysis techniques to modelthe diversity of recommendation sequences . These meth-ods endeavor to capture information pertaining to out-of-windowitems, thereby aligning more closely with users perception. Dif-ferent from the above approaches, our work is the first to considerboth diversity and accuracy in rank stage for recommendation witha flexible end-to-end algorithm.",
  "Azin Ashkan, Branislav Kveton, Shlomo Berkovsky, and Zheng Wen. 2015. Opti-mal Greedy Diversity for Recommendation.. In IJCAI, Vol. 15. 17421748": "Haoyue Bai, Le Wu, Min Hou, Miaomiao Cai, Zhuangzhuang He, Yuyang Zhou,Richang Hong, and Meng Wang. 2024. Multimodality Invariant Learning forMultimedia-Based New Item Recommendation. arXiv preprint arXiv:2405.15783(2024). Jaime Carbonell and Jade Goldstein. 1998. The use of MMR, diversity-basedreranking for reordering documents and producing summaries. In Proceedings ofthe 21st annual international ACM SIGIR conference on Research and developmentin information retrieval. 335336. Laming Chen, Guoxin Zhang, and Eric Zhou. 2018. Fast greedy map inferencefor determinantal point process to improve recommendation diversity. Advancesin Neural Information Processing Systems 31 (2018).",
  "Matev Kunaver and Toma Porl. 2017. Diversity in recommender systemsAsurvey. Knowledge-based systems 123 (2017), 154162": "Zihan Lin, Hui Wang, Jingshu Mao, Wayne Xin Zhao, Cheng Wang, Peng Jiang,and Ji-Rong Wen. 2022. Feature-aware diversified re-ranking with disentangledrepresentations for relevant recommendation. In Proceedings of the 28th ACMSIGKDD Conference on Knowledge Discovery and Data Mining. 33273335. Michel Minoux. 2005. Accelerated greedy algorithms for maximizing submodularset functions. In Optimization Techniques: Proceedings of the 8th IFIP Conferenceon Optimization Techniques Wrzburg, September 59, 1977. Springer, 234243.",
  "Chaofeng Sha, Xiaowei Wu, and Junyu Niu. 2016. A framework for recommendingrelevant and diverse items.. In IJCAI, Vol. 16. 38683874": "Chunfeng Song, Yan Huang, Wanli Ouyang, and Liang Wang. 2018. Mask-guidedcontrastive attention model for person re-identification. In Proceedings of theIEEE conference on computer vision and pattern recognition. 11791188. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and RuslanSalakhutdinov. 2014. Dropout: A simple way to prevent neural networks fromoverfitting. The Journal of Machine Learning Research 15, 1 (2014), 19291958. Youchen Sun, Zhu Sun, Xiao Sha, Jie Zhang, and Yew Soon Ong. 2023. Disentan-gling Motives behind Item Consumption and Social Connection for Mutually-enhanced Joint Prediction. In Proceedings of the 17th ACM Conference on Recom-mender Systems. 613624. Shisong Tang, Qing Li, Xiaoteng Ma, Ci Gao, Dingmin Wang, Yong Jiang, QianMa, Aoyang Zhang, and Hechang Chen. 2022. Knowledge-based temporal fusionnetwork for interpretable online video popularity prediction. In Proceedings ofthe ACM Web Conference 2022. 28792887. Shisong Tang, Qing Li, Dingmin Wang, Ci Gao, Wentao Xiao, Dan Zhao, YongJiang, Qian Ma, and Aoyang Zhang. 2023. Counterfactual Video Recommendationfor Duration Debiasing. In Proceedings of the 29th ACM SIGKDD Conference onKnowledge Discovery and Data Mining. 48944903.",
  "Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross networkfor ad click predictions. In Proceedings of the ADKDD17. 17": "Wenjie Wang, Fuli Feng, Xiangnan He, Xiang Wang, and Tat-Seng Chua. 2021.Deconfounded recommendation for alleviating bias amplification. In Proceedingsof the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining.17171725. Mark Wilhelm, Ajith Ramanathan, Alexander Bonomo, Sagar Jain, Ed H Chi, andJennifer Gillenwater. 2018. Practical diversified recommendations on youtubewith determinantal point processes. In Proceedings of the 27th ACM InternationalConference on Information and Knowledge Management. 21652173.",
  "Qingyun Wu, Huazheng Wang, Yanen Li, and Hongning Wang. 2019. Dynamicensemble of contextual bandits to satisfy users changing interests. In The WorldWide Web Conference. 20802090": "Jingyu Xiao, Zhiyao Xu, Qingsong Zou, Qing Li, Dan Zhao, Dong Fang, RuoyuLi, Wenxin Tang, Kang Li, Xudong Zuo, Penghui Hu, Yong Jiang, Zixuan Weng,and Michael Lyv.R. 2024. Make Your Home Safe: Time-aware UnsupervisedUser Behavior Anomaly Detection in Smart Homes via Loss-guided Mask. arXivpreprint arXiv:2406.10928 (2024). Jingyu Xiao, Qingsong Zou, Qing Li, Dan Zhao, Kang Li, Wenxin Tang, RunjieZhou, and Yong Jiang. 2023. User Device Interaction Prediction via RelationalGated Graph Attention Network and Intent-aware Encoder. In Proceedings ofthe 2023 International Conference on Autonomous Agents and Multiagent Systems(AAMAS). 16341642. Jingyu Xiao, Qingsong Zou, Qing Li, Dan Zhao, Kang Li, Zixuan Weng, Ruoyu Li,and Yong Jiang. 2023. I Know Your Intent: Graph-enhanced Intent-aware UserDevice Interaction Prediction via Contrastive Learning. Proceedings of the ACMon Interactive, Mobile, Wearable and Ubiquitous Technologies (IMUWT/UbiComp)7, 3 (2023), 128.",
  "Sang Michael Xie and Stefano Ermon. 2019. Reparameterizable subset samplingvia continuous relaxations. arXiv preprint arXiv:1901.10517 (2019)": "Mei-Feng Xu, Hong Zhang, Su Zhang, Hugh L Zhu, Hui-Min Su, Jian Liu,Kam Sing Wong, Liang-Sheng Liao, and Wallace CH Choy. 2015. A low tem-perature gradual annealing scheme for achieving high performance perovskitesolar cells with no hysteresis. Journal of Materials Chemistry A 3, 27 (2015),1442414430. Yue Xu, Hao Chen, Zefan Wang, Jianwen Yin, Qijie Shen, Dimin Wang, FeiranHuang, Lixiang Lai, Tao Zhuang, Junfeng Ge, et al. 2023.Multi-factor Se-quential Re-ranking with Perception-Aware Diversification.arXiv preprintarXiv:2305.12420 (2023).",
  "Development in Information Retrieval. Xian, China": "Xiaoying Zhang, Hongning Wang, and Hang Li. 2023. Disentangled Represen-tation for Diversified Recommendations. In Proceedings of the Sixteenth ACMInternational Conference on Web Search and Data Mining. 490498. Yin Zhang, Derek Zhiyuan Cheng, Tiansheng Yao, Xinyang Yi, Lichan Hong,and Ed H Chi. 2021. A model of two tales: Dual transfer learning framework forimproved long-tail item recommendation. In Proceedings of the web conference2021. 22202231. Kaifu Zheng, Lu Wang, Yu Li, Xusong Chen, Hu Liu, Jing Lu, Xiwei Zhao, Chang-ping Peng, Zhangang Lin, and Jingping Shao. 2022. Implicit User AwarenessModeling via Candidate Items for CTR Prediction in Search Ads. In Proceedingsof the ACM Web Conference 2022. 246255."
}