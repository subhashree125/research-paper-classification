{
  "ABSTRACT": "In recommendation systems, new items are continuously intro-duced, initially lacking interaction records but gradually accumu-lating them over time. Accurately predicting the click-through rate(CTR) for these items is crucial for enhancing both revenue and userexperience. While existing methods focus on enhancing item IDembeddings for new items within general CTR models, they tend toadopt a global feature interaction approach, often overshadowingnew items with sparse data by those with abundant interactions.Addressing this, our work introduces EmerG, a novel approachthat warms up cold-start CTR prediction by learning item-specificfeature interaction patterns. EmerG utilizes hypernetworks to gen-erate an item-specific feature graph based on item characteristics,which is then processed by a Graph Neural Network (GNN). ThisGNN is specially tailored to provably capture feature interactions atany order through a customized message passing mechanism. Wefurther design a meta learning strategy that optimizes parametersof hypernetworks and GNN across various item CTR predictiontasks, while only adjusting a minimal set of item-specific parame-ters within each task. This strategy effectively reduces the risk ofoverfitting when dealing with limited data. Extensive experimentson benchmark datasets validate that EmerG consistently performsthe best given no, a few and sufficient instances of new items.",
  "Information systems Recommender systems; Comput-ing methodologies Supervised learning; Neural networks": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08...$15.00",
  "Cold-Start Recommendation, Warm Up, Click-Through Rate Pre-diction, Few-Shot Learning, Hypernetworks, New Items": "ACM Reference Format:Yaqing Wang, Hongming Piao, Daxiang Dong, Quanming Yao, and JingboZhou. 2024. Warming Up Cold-Start CTR Prediction by Learning Item-Specific Feature Interactions. In Proceedings of the 30th ACM SIGKDD Con-ference on Knowledge Discovery and Data Mining (KDD 24), August 2529, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.",
  "INTRODUCTION": "The cold-start problem presents a significant challenge in recom-mender systems , particularly evident as new items transitionfrom having no user interactions (termed as cold-start phase) toaccumulating a few initial clicks (termed as warm-up phase) in theindustry landscape. Deep learning models, renowned for their capa-bility to capture complex feature interactions, have shown promisein improving click-through rate (CTR) predictions, a critical met-ric for assessing the likelihood of user engagement with variousitems (e.g., movies, commodities, music) . However, thesemodels typically rely on extensive datasets to achieve optimal per-formance, a requirement that poses a limitation in cold-start andwarm-up phases. With their substantial parameter size, these mod-els struggle to adapt efficiently to these phases characterized bylimited interaction records, thereby exacerbating the challenge ofmaking accurate CTR predictions and updating models withoutincurring significant costs.Recent studies have focused on enhancing the initialization ofitem ID embeddings as a strategy to mitigate the item cold-startproblem in recommender systems, which allows subsequent up-dates through gradient descent as interaction records become avail-able in the warm-up phase . Then, they leverage gen-eral CTR backbones for further processing. However, they overlooka crucial aspect: the distinctiveness of feature interaction patternsacross different users and items. This oversight limits the ability ofthese models to fully capture the nuanced dynamics of user-item in-teractions, potentially impacting the accuracy and effectiveness ofCTR predictions in scenarios where personalized recommendationsare crucial. For example, comparing high-priced luxury items with",
  "KDD 24, August 2529, 2024, Barcelona, SpainYaqing Wang, Hongming Piao, Daxiang Dong, Quanming Yao, and Jingbo Zhou": "testing AUC (%) with the number of training samples. Experimen-tal results measured by testing F1 (%) are similar. As shown, allmethods get better performance given more training samples. Theclassic CTR backbone DeepFM gradually outperforms CVAR whichequips DeepFM with additional modules to generate item ID em-beddings for new items. In contrast, EmerG consistently performsthe best and converges to better performance than the others. Thisvalidates the effectiveness of our EmerG which can nicely captureitem-specific feature interaction at different orders.",
  "We briefly review four groups of methods relevant to CTR predic-tion of newly emerging items": "A. General CTR Models. General CTR models are applied uni-versally without prioritizing underrepresented items. They mainlyfocus on modeling complex feature interactions, which is crucialfor enhancing CTR prediction accuracy . Historical advance-ments in this area show a progression from simple first-order in-teractions captured by linear models like logistic regression to second-order interactions modeled by Factorization Machines(FM) , and to high-order interactions addressed by higher-orderFMs (HOFMs) . Various deep-learning models then automatethe learning of these complex patterns. Both Wide&Deep and DeepFM employ hybrid architectures to handle second-orderand higher interactions. DIN dynamically captures user in-terests through an attention mechanism that adapts to varying adfeatures. AutoInt introduces a multi-head self-attention mech-anism to model high-order feature interactions. LorentzFM explores feature interactions in hyperbolic space to minimizeparameter size. AFN converts the power of each feature into thecoefficient to be learned. FinalMLP uses two multi-layer per-ceptron (MLP) networks in parallel, and equips them with featureselection and interaction aggregation layers. FINAL introducesa factorized interaction layer for exponential growth in featureinteraction learning. Considering that feature interactions can beconceptualized as graphs with nodes representing user and itemfeatures, graph neural network (GNN) are used. Fi-GNN learns to generate feature graphs where the edges are establishedaccording to the similarity between feature embeddings. FIVES learns a global feature graph where edges are established bydifferentiable search from large-scale CTR datasets, thus new itemswith limited data can be underrepresented. GMT models theinteractions among items, users, and their features into a large het-erogeneous graph, then feeds the local neighborhood of the targetuser-item pair for prediction. However, these general CTR models,designed for extensive datasets, often struggle during the cold-start& warm-up phases due to their substantial parameter size, whichcan lead to overfitting when adapted for new items. In contrast,EmerG introduces a specialized GNN that operates on item-specificfeature graphs, generated via hypernetworks learned from diverseCTR tasks, ensuring precise modeling of feature interactions fornew items with minimal parameter adjustments. B. Methods for New Items without Interaction Records. Severalmethods specifically address the cold-start phase, where new itemslack interaction records, while still maintaining model performanceon older, established items. DropoutNet trains neural networkswith a dropout mechanism on input samples to infer missing data.Heater employs a multi-gate mixture-of-experts approach togenerate item embeddings. GAR adopts an adversarial trainingstrategy between a generator and a recommender to produce newitem embeddings that mimic the distribution of old embeddings,deceiving the recommender systems. ALDI learns to transferthe behavioral information of old items to new items. However,these methods do not consider the incorporation of incoming inter-action records for new items and typically require re-training toaccommodate the evolving interaction history of these items. C. Methods for New Items with A Few Interaction Records. In sce-narios where only a few instances of new items are available, whichcorrespond to warm-up phases, few-shot learning present anatural solution. Few-shot learning targets at generalizing to newtasks with a few labeled samples, which has been applied to imageclassification , query intent recognition and drug discov-ery . For CTR prediction, existing works typically ap-proach the problem as a -way -shot task, where each of the new items is associated with labeled instances, then utilize theclassic gradient-based meta-learning strategy . MeLU lever-ages this strategy to selectively adapt model parameters for newitems through gradient descents. MAMO enhances adaptationby incorporating an external memory mechanism. MetaHIN",
  "EmerGKDD 24, August 2529, 2024, Barcelona, Spain": "utilizes heterogeneous information networks to exploit the rich se-mantic relationships between users and items. PAML employssocial relations to facilitate information sharing among similarusers. More recent approaches have shifted from user-specific fine-tuning via gradient descent to amortization-based methods. Thesemethods directly map user interaction histories to user-specificparameters, thus modulating the main network without iterativeadjustments. TaNP learns to modulate item-specific parametersbased on item interaction records. ColdNAS employs neuralarchitecture search to optimize the modulation function and itsapplication within the network. However, these methods struggleto handle new items that lack interaction records and cannot dy-namically incorporate additional interaction records of new itemsas they become available. D. Methods for Emerging Items with Incremental Interaction Records.To mirror the industrys dynamic evolution of new items, progress-ing from no interaction records to few and then abundant records,models are developed to manage these transitions smoothly. Ex-isting efforts primarily enhance item ID embeddings for generalCTR backbones. MetaE employs gradient-based meta-learningto train an embedding generator. MWUF transforms unstableitem ID embeddings into stable ones using meta networks. CVAR decodes new item ID embeddings from a distribution overitem side information, circumventing additional data processing.GME leverages information from neighboring old items fornew item ID embedding generation. However, these methods gen-erally optimize initial item ID embeddings while maintaining aglobal feature interaction pattern, thus failing to capture the uniquecharacteristics and interaction dynamics of these items. In contrast,our EmerG addresses new item CTR prediction by tailoring fea-ture interactions to each item with the help of hypernetworks. Bylearning with item-specific feature interactions, the risk of over-whelming new items by old items with abundant data is alleviatedand prediction accuracy is enhanced.",
  "PROBLEM FORMULATION": "Let V = {} denote a set of items where each item is associatedwith item features such as item ID, type and price. Similarly, letU = { } denote a set of users where each user is also associatedwith user features such as user ID, age and hometown. When auser clicks through an item , label , = 1. Otherwise, , = 0.During learning, the predictor is learned from a set of CTR pre-diction tasks T old = {T}=1 sampled from old items, which canrapidly generalize to predict for tasks from new items that are un-seen during training. Each task T corresponds to an old item, witha training set S = {(,,,)}=1 containing existing interaction histories associated with and a test set Q = {(,,,)}=1containing interactions to predict whether clicks through . and are the number of interactions in S and Q respectively.During testing, we consider CTR prediction for new items thatstart with no interaction records, then gradually gather a few, andeventually accumulate sufficient interaction records. Consider atask T associated with a new item which is not consideredduring training, we handle three phases:",
  "THE PROPOSED EMERG": "Aligning with the established understanding that feature inter-actions are crucial, we propose EmerG () to capture theuniqueness of items through their associated feature interactionpatterns. We design two key components in EmerG: (ii) hyper-networks shared across different tasks to generate item-specificadjacent matrices encoding feature graphs; and (i) a GNN that oper-ates on the generated item-specific feature graphs, whose messagepassing mechanism is specially tailored to provably capture featureinteractions at any order. As we consider cold-start & warm-upphases, we further design a meta learning strategy that optimizesparameters of hypernetworks and GNN across various item CTRprediction tasks, while only adjusting a small set of item-specificparameters within each task. This strategy effectively reduces therisk of overfitting when dealing with limited data.",
  "Item-Specific Feature Graph Generation": "We employ hypernetworks, following the strategy of Ha et al. ,to generate item-specific feature graphs. Hypernetworks, smallneural networks trained to generate parameters for a larger mainnetwork, present a unique challenge in their application, as theirintegration is highly problem-specific. In EmerG, hypernetworksare used to produce the initial adjacency matrix A(1), encoding theitem-specific feature graph for the first GNN layer. We streamlinethe process by allowing subsequent GNN layers to derive their adja-cency matrices from the initial A(1), optimizing storage efficiencywithout compromising the models specificity to each item.Consider task T for item . For item features 1, . . . , , itemfeature embeddings are denoted as e1,, . . . , e, respectively. Thefeature graph is a graph where each node corresponds to afeature , and the edge between two nodes records their interac-tion. We let our hypernetworks, which consists of + subnet-works, produce a dense item-specific A(1) R(+ )(+ )",
  "Deterministic Computation": ": Illustration of the proposed EmerG, designed to enhance CTR predictions of newly emerging items through thelearning of item-specific feature interaction patterns. EmerG uses hypernetworks to generate an initial item-specific adjacencymatrix for a feature graph, with nodes representing user and item features and edges denoting their interactions, based on itemfeature embeddings. Higher-order adjacency matrices for subsequent GNN layers are generated from the initial matrix, reducingboth model complexity and storage requirements. The GNNs message passing process is tailored to capture -order featureinteractions at the 1th layer, enabling nuanced integration of various interaction orders for accurate predictions. EmerGoptimizes the parameters of hypernetworks and GNN across diverse CTR prediction tasks to enhance generalization, whileutilizing minimal item-specific parameters to capture the uniqueness of new items, which are adaptable with the introductionof additional item instances.",
  "sparsify the dense A()by (4) such that only two highly relatedfeatures are connected. Further, because of the commutative lawof , we transform A()into a symmetric matrix by (5). Apart": "from the above-mentioned considerations, we expect that nodesdisconnected in low-order feature graphs to be disconnected in high-order feature graphs. For example, if the message is not propagatedfrom node 2 to node 1 in the th GNN layer, the message of 2will be not propagated to 1 in higher GNN layers. Thus, we apply(6) to obtain the final A().",
  "Customized Message Passing Process onItem-Specific Feature Graph": "Upon the learned item-specific feature graphs, we use a GNN witha customized message passing process designed to provably cap-ture feature interactions at any orders, which are then explicitlycombined into the final CTR predictions.We first describe the general mechanism of message passing. Atthe th GNN layer, node embedding h() of feature is updated as",
  "Proposition 4.1 (Efficacy of EmerG.). With the customizedmessage passing process defined in (8), the (1)th GNN layer captures-order feature interactions": "The proof is in Appendix A.1. One may consider integratingresidual connections into GNN to model arbitrary-order featureinteraction. However, as analyzed in Appendix A.2, incorporatingresidual connections will significantly elevate the maximum orderof feature interaction.With different orders of feature interactions, we then explicitlycombine all nodes embeddings of each node into the updatednode embeddings H of by multi-head attention:",
  "Learning and Inference": "To reduce the risk of overfitting when dealing with limited data,we introduce a meta learning strategy that optimizes parameters ofhypernetworks and GNN across various item CTR prediction tasks,while only adjusting a minimal set of item-specific parameterswithin each task.For simplicity, we denote hypernetworks as hyperhyper wherehyper = W is the shared trainable parameter. Then, we denote theGNN as GNNGNN, , where GNN represents shared parametersincluding parameters of embedding layers {W,}+=1, param-",
  ": end for": "By learning from a set of tasks T old, the learned GNN, hyperencode common knowledge. Consider task T of new item . When has no interaction record, namely |S | = 0, we obtain its task-specific parameter and test its performance on the test set ascold-start phase performance. Given a few new item instances, wecan update to take in the supervised information. Once isupdated, we measure performance on the test set as warm-up phaseperformance. Algorithm 2 describes the testing procedure.",
  "EXPERIMENTS5.1Experimental Settings": "Datasets. We use two benchmark datasets: (i) MovieLens :a dataset containing 1 million interaction records on MovieLens,whose item features include movie ID, title, year of release, gen-res and user features include user ID, age, gender, occupation andzip-code; and (ii) Taobao : a collection of 26 million ad clickrecords on Taobao, whose item features include ad ID, positionID, category ID, campaign ID, advertiser ID, brand, price and userfeatures include user ID, Micro group ID, cms_group_id, gender,age, consumption grade, shopping depth, occupation and city level.Following existing works , we binarize the ratings of Movie-Lens, setting rating smaller than 4 as 0 and the others as 1. Data Split. We adopt the public data split , groupitems according to their frequency: (i) old items which are itemsappearing in more than interaction records, where = 200 inMovieLens and = 2000 in Taobao; and (ii) new items whichare items appearing in less than and larger than 3 interactionrecords, where is set to 20 and 500 for MovieLens and Taobaorespectively. To mimic the dynamic process where new items aregradually clicked by more users, the interaction records associatedwith new items are sorted by timestamp. We consider three succes-sive warm-up phases, labeled as A, B, and C, each of which involvesthe introduction of a set of new interaction records for each item.The rest interaction records form testing data for evaluation. Experiment Pipeline. We adopt pipeline of existing works to assess how a model adapts to new items over time. First, weuse old item instances to pretrain the model, and directly evaluatethe model performance on testing data of new items as cold-startphase performance. Then, we measure how model performs as itlearns from a few training data in successive warm-up phases. In",
  "B Methods for new items without interaction records, includingDropoutNet and ALDI": "C Methods for new items with a few interaction records, includ-ing MeLU , MAMO , TaNP , and ColdNAS .These methods cannot incorporate new item instances dynami-cally. Therefore, to accommodate the training interaction recordsprovided in warm-up phases A, B, and C, we adopt a phasedapproach: initially, we use interaction records from phase Aas the support set to assess testing performance. Subsequently,we combine 2 records from phases A and B as the support setfor a second evaluation. Finally, we incorporate 3 records fromall three warm-up phasesA, B, and Cas the support set toconduct a third assessment of testing performance. D Methods for emerging items with incremental interaction records,which are the most relevant to ours. Existing works mainly equipgeneral CTR backbones with the ability to generate and warm-up item ID embeddings for new items, including MetaE ,MWUF , GME , and CVAR . We use the classicDeepFM as the CTR backbone. Results of equipping these meth-ods with other backbones are reported in Appendix C.1.We implement the compared methods using public codes of therespective authors. More implementation details are provided inAppendix B. Performance for Cold-Start & Warm-Up Phases. showsthe results. As shown, cold-start methods designed for cold-start& warm-up phases generally perform better. EmerG consistentlyperforms the best in all four phases, validating the effectivenessof capturing item-specific feature interaction by hypernetworks.Few-shot methods for N-way K-shot settings obtains good perfor-mance in warm-up phase A. However, as the number of samplesincreases, it is unable to achieve greater performance improvementwithout complete retraining. General CTR backbones which arefine-tuned using the training sets perform worse, where FinalMLPperforms the best. Recall that they randomly initialize item-specificparameters for new items, fine-tuning pretrained models by a smallnumber of new item instances is not enough to obtain good perfor-mance. Particularly, note that the GNN-based CTR model Fi-GNNwhich uses item-user-specific feature interaction graphs perform",
  "DropoutNet60.41(0.09)13.53(0.02)------ALDI50.10(0.18)10.93(0.05)------": "MeLU--61.37(0.17)14.09(0.17)62.48(0.04)14.34(0.05)63.07(0.07)14.64(0.11)MAMO--61.96(0.11)14.31(0.09)62.52(0.05)14.34(0.04)63.15(0.12)14.78(0.13)TaNP--55.67(0.22)11.92(0.31)55.85(0.16)12.07(0.16)56.19(0.09)12.08(0.11)ColdNAS--54.27(0.07)10.89(0.05)54.86(0.14)11.33(0.13)55.01(0.09)11.71(0.13) MetaE59.75(0.37)13.58(0.06)61.19(0.26)14.01(0.09)62.06(0.31)14.41(0.10)62.87(0.32)14.71(0.07)CVAR60.56(0.46)13.71(0.13)62.54(0.19)14.38(0.06)63.17(0.10)14.69(0.05)63.95(0.18)15.09(0.12)GME60.57(0.23)13.32(0.33)62.55(0.17)13.96(0.22)63.29(0.05)14.39(0.12)63.85(0.13)14.52(0.08)MWUF59.65(0.46)13.44(0.15)62.08(0.17)14.20(0.07)63.03(0.13)14.63(0.07)63.79(0.12)14.93(0.06)EmerG61.58(0.03)13.99(0.05)63.56(0.03)15.02(0.06)63.76(0.02)15.15(0.01)64.22(0.02)15.21(0.02) not well. This shows that too much freedom is not beneficial tocapture feature interaction patterns under cold-start & warm-upphases. While in EmerG, we utilize hypernetworks to generateitem-specific feature graphs, which is then processed by a GNNwith customized message passing mechanism to capture arbitrary-order feature interaction, and optimize parameters by meta learningstrategy. All these design considerations contributes the best per-formance obtained by EmerG. For computational overhead, EmerGis relatively more efficient in terms of both time and computationalresources. See Appendix C.2 for a detailed comparison. Performance Given Sufficient Training Samples. One might ques-tion how EmerG performs with an abundance of training samplesfor new items, referred to as the common phase, especially in com-parison to traditional CTR backbones. Here, we set aside samplesfrom original testing samples of new items (so the test set is smaller),use them to augment the experiment pipeline with more trainingsamples, and evaluate the performance on the smaller test set. Wecompare EmerG with baselines which perform the best among CTRbackbones and few-shot methods in . shows the",
  "Model Analysis": "5.3.1Ablation Study. We compare the proposed EmerG with thefollowing variants: (i) w/ random graph generates the adjacencymatrix A(1) in (8) randomly; (ii) w/o sparsification does not apply(4) to sparsify the adjacency matrices; (iii) w/o mask does not apply(6) to enforce nodes which are disconnected in low-order featuregraphs to be disconnected in higher-order feature graphs; (iv) w/shared graph employs global shared adjacency matrices for allitems, in contrast to EmerG, which utilizes item-specific adjacencymatrices; (v) w/o meta learns both GNN and hypernetworks fromold items, without forming tasks and employ a meta-learning strat-egy; and (vi) w/o inner directly uses and does not update it to within each task. shows the results. As shown, w/ random graph\" per-forms worse than EmerG which shows that the item-specific featuregraphs generated by hypernetworks is meaningful. The perfor-mance gain of EmerG over w/o sparsification\" shows that a sparsefeature graph where only closely-related nodes are connected canlet the GNN model concentrate on useful messages. Comparingw/o mask\" to EmerG, the performance drop validates our assump-tion in (6). The mask operation also prevents the adjacency matricesfrom being too dense, which can be beneficial to prune unneces-sary feature interactions and provide better explainability. We canalso observe thatw/ shared graph\" performs worse than EmerG.This validates that using global shared adjacency matrices cannotcapture the various feature interaction patterns between differentusers and items. Finally, EmerG defeats w/o meta\" and w/o inner\",which underscores the necessity of both meta-learning across tasksand inner updates within each task. 5.3.2Effect of Number of GNN Layers. As demonstrated in Propo-sition 4.1, we have customized the message passing process of theGNN to ensure that the th layer encapsulates -order feature in-teractions. Furthermore, we optimize this process by generating",
  ": Ablation study on MovieLens and Taobao": "adjacency matrices for subsequent GNN layers directly from theinitial matrix provided by hypernetworks. This approach not onlystreamlines the architecture but also facilitates the extension toadditional layers, thereby capturing higher-order feature interac-tions with ease. In this context, we investigate the influence of thenumber of GNN layers on performance across various datasets.",
  ": Varying the number of GNN layers in EmerG": "shows the results. As can be seen, EmerG with differentlayer numbers obtain the best performance on different datasets:EmerG with 2 GNN layers performs the best on MovieLens whileEmerG with 3 GNN layers achieves the best performance on Taobao.This shows that different datasets requires different number ofGNN layers: larger datasets such as Taobao may need higher-orderfeatures than smaller ones such as MovieLens. By design, EmerGcan easily meet this requirement.",
  "Case Study": "Finally, we take movie Lawnmower Man 2: Beyond Cyberspace andmovie Waiting to Exhale from MovieLens as new items, and visualizetheir adjacency matrices which record the item-specific featuregraphs in .As can be seen, EmerG learns different task-specific featuregraphs for different items. Comparing (a) with (b),we find that Lawnmower Man 2: Beyond Cyberspace has a particu-larly important second-order feature interaction genres, title. Thegenre of Lawnmower Man 2: Beyond Cyberspace is science fictionwhile the genre of Waiting to Exhale is comedy. For a science fiction,its title often reflects its world view or theme, which is the key forpeople to judge whether they are interested. As for a comedy work,whether it is interesting or not is often irrelevant to the title.Besides, EmerG can capture meaningful higher-order featureinteractions. As shown, both year, age and year, zip-code areimportant second-order feature interactions, they contribute todiscovering the third-order feature interaction year, age, zip-code.In (c), the relation between nodes of year, age and zip-codeall become relatively important although <age, zip-code> is notimportant in (b). It is easy to understand that the year ofmovies determines the age of people who are more likely to watchthem. For example, elderly people generally prefer watching oldmovies. Apart from this, location which is indicated by zip-codealso plays an important role: people in developed areas tend to bemore receptive to new things. Therefore, area changes may lead to changes in the age of people who like the same movie, whichvalidates the efficacy of the learned third-order feature interaction.We can also observe that the item-specific feature graphs gener-ated by our hypernetworks can roughly capture the feature inter-actions before seeing any training samples of new items. Althoughthey are continuously optimized using training sets of warm-upphases, the changes are not sharp. As can be seen, the second-orderfeature interaction patterns are similar in (b) and (d)to (f), with small changes to accommodate the trainingsamples. Overall, we conclude that EmerG can learn reasonableadjacency matrices to capture item-specific feature interactions atdifferent orders.",
  "CONCLUSION": "In this study, we underscore the critical role of feature interac-tions and introduce EmerG, a novel solution designed to capturethe unique interaction patterns of items, effectively handling CTRprediction of newly emerging items with incremental interactionrecords. Our approach leverages hypernetworks to construct item-specific feature graphs, with nodes representing features and edgesdenoting their interactions, thus enabling the model to discern theintricate interaction patterns that characterize each item. We incor-porate a graph neural network (GNN) equipped with a specializedmessage passing process, crafted to capture feature interactionsacross all orders, facilitating precise CTR predictions. To combatoverfitting in scenarios with sparse data, we implement a meta-learning strategy that finely tunes parameters of hypernetworksand GNN across various item CTR prediction tasks, necessitatingonly minimal modifications to item-specific parameters for eachtask. Experimental results on real-world datasets show EmerG ob-tains the state-of-the-art performance on CTR prediction for newitems that have no interaction history, a few interactions, or a sub-stantial number of interactions. We expect this approach can beused to warm-up cold-start problems in other applications such asdrug recommendation in the future. We thank the anonymous reviewers for their valuable comments.This work is supported by National Key Research and Develop-ment Program of China under Grant 2023YFB2903904 and NationalNatural Science Foundation of China under Grant No. 92270106.",
  "Olivier Chapelle, Eren Manavoglu, and Romer Rosales. 2014. Simple and scalableresponse prediction for display advertising. ACM Transactions on IntelligentSystems and Technology 5, 4 (2014), 134": "Hao Chen, Zefan Wang, Feiran Huang, Xiao Huang, Yue Xu, Yishi Lin, PengHe, and Zhoujun Li. 2022. Generative adversarial framework for cold-startitem recommendation. In International ACM SIGIR Conference on Research andDevelopment in Information Retrieval. 25652571. Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al.2016. Wide & deep learning for recommender systems. In Workshop on DeepLearning for Recommender Systems. 710.",
  "F Maxwell Harper and Joseph A Konstan. 2015. The MovieLens datasets: Historyand context. ACM Transactions on Interactive Intelligent Systems 5, 4 (2015), 119": "Feiran Huang, Zefan Wang, Xiao Huang, Yufeng Qian, Zhetao Li, and Hao Chen.2023. Aligning distillation for cold-start item recommendation. In InternationalACM SIGIR Conference on Research and Development in Information Retrieval.11471157. Hao Huang, Haihua Xu, Xianhui Wang, and Wushour Silamu. 2015. Maximum F1-score discriminative training criterion for automatic mispronunciation detection.IEEE/ACM Transactions on Audio, Speech, and Language Processing 23, 4 (2015),787797.",
  "Thomas N Kipf and Max Welling. 2017. Semi-supervised classification with graphconvolutional networks. In International Conference on Learning Representations": "Hoyeop Lee, Jinbae Im, Seongwon Jang, Hyunsouk Cho, and Sehee Chung. 2019.MeLU: Meta-learned user preference estimator for cold-start recommendation. InACM SIGKDD Conference on Knowledge Discovery and Data Mining. 10731082. Zekun Li, Zeyu Cui, Shu Wu, Xiaoyu Zhang, and Liang Wang. 2019. Fi-GNN:Modeling feature interactions via graph neural networks for CTR prediction.In ACM International Conference on Information and Knowledge Management.539548.",
  "Yuanfu Lu, Yuan Fang, and Chuan Shi. 2020. Meta-learning on heterogeneousinformation networks for cold-start recommendation. In ACM SIGKDD Conferenceon Knowledge Discovery and Data Mining. 15631573": "Kelong Mao, Jieming Zhu, Liangcai Su, Guohao Cai, Yuru Li, and Zhenhua Dong.2023. FinalMLP: An enhanced two-stream MLP model for CTR prediction. InAAAI Conference on Artificial Intelligence. 45524560. Erxue Min, Yu Rong, Tingyang Xu, Yatao Bian, Da Luo, Kangyi Lin, JunzhouHuang, Sophia Ananiadou, and Peilin Zhao. 2022. Neighbour interaction basedclick-through rate prediction via graph-masked transformer. In InternationalACM SIGIR Conference on Research and Development in Information Retrieval.353362. Wentao Ouyang, Xiuwu Zhang, Shukui Ren, Li Li, Kun Zhang, Jinmei Luo, ZhaojieLiu, and Yanlong Du. 2021. Learning graph meta embeddings for cold-start ads inclick-through rate prediction. In International ACM SIGIR Conference on Researchand Development in Information Retrieval. 11571166. Feiyang Pan, Shuokai Li, Xiang Ao, Pingzhong Tang, and Qing He. 2019. Warmup cold-start advertisements: Improving CTR predictions via learning to learn IDembeddings. In International ACM SIGIR Conference on Research and Developmentin Information Retrieval. 695704.",
  "Maksims Volkovs, Guangwei Yu, and Tomi Poutanen. 2017. DropoutNet: Ad-dressing cold start in recommender systems. In Advances in Neural InformationProcessing Systems. 49574966": "Li Wang, Binbin Jin, Zhenya Huang, Hongke Zhao, Defu Lian, Qi Liu, and EnhongChen. 2021. Preference-adaptive meta-learning for cold-start recommendation..In International Joint Conference on Artificial Intelligence. 16071614. Yaqing Wang, Abulikemu Abuduweili, Quanming Yao, and Dejing Dou. 2021.Property-aware relation networks for few-shot molecular property prediction.In Advances in Neural Information Processing Systems. 1744117454. Yaqing Wang, Song Wang, Yanyan Li, and Dejing Dou. 2022. Recognizing medicalsearch query intent by few-shot learning. In International ACM SIGIR Conferenceon Research and Development in Information Retrieval. 502512.",
  "Canran Xu and Ming Wu. 2020. Learning feature interactions with lorentzianfactorization machine. In AAAI Conference on Artificial Intelligence. 64706477": "Quanming Yao, Zhenqian Shen, Yaqing Wang, and Dejing Dou. 2024. Property-aware relation networks for few-shot molecular property prediction. IEEE Trans-actions on Pattern Analysis and Machine Intelligence (2024). Xu Zhao, Yi Ren, Ying Du, Shenzheng Zhang, and Nian Wang. 2022. Improvingitem cold-start recommendation via model-agnostic conditional variational au-toencoder. In International ACM SIGIR Conference on Research and Developmentin Information Retrieval. 25952600. Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, YanghuiYan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-throughrate prediction. In ACM SIGKDD Conference on Knowledge Discovery and DataMining. 10591068. Jieming Zhu, Qinglin Jia, Guohao Cai, Quanyu Dai, Jingjie Li, Zhenhua Dong,Ruiming Tang, and Rui Zhang. 2023. FINAL: Factorized interaction layer for CTRprediction. In International ACM SIGIR Conference on Research and Developmentin Information Retrieval. 20062010. Yongchun Zhu, Ruobing Xie, Fuzhen Zhuang, Kaikai Ge, Ying Sun, Xu Zhang,Leyu Lin, and Juan Cao. 2021. Learning to warm up cold item embeddings for cold-start recommendation with meta scaling and shifting networks. In InternationalACM SIGIR Conference on Research and Development in Information Retrieval.11671176. Ziwei Zhu, Shahin Sefati, Parsa Saadatpanah, and James Caverlee. 2020. Recom-mendation for new users and new items via randomized training and mixture-of-experts transformation. In International ACM SIGIR Conference on Research andDevelopment in Information Retrieval. 11211130.",
  "BIMPLEMENTATION DETAILS": "All results are averaged over five runs and are obtained on a 32GBNVIDIA Tesla V100 GPU. We use Adam optimizer . To searchfor the appropriate hyperparameters, we set aside 20% old itemsand form validation set using their samples. The performance isthen directly evaluated on the validation set of these items, whichcorresponds to cold-start phase. When the hyperparameters arefound by grid search, we put back samples of these old items, thenfollow the experiment pipeline described in .1 and reportthe results. The hyperparameters and their range used by EmerGare summarized in .",
  "CMORE EXPERIMENTAL RESULTSC.1Comparing with Existing MethodsEquipped with Different Backbones": "In .2, we employ DeepFM as the backbone for methodsin Group D. Despite this, results in indicate that FinalMLPgenerally surpasses DeepFM, particularly in the warm-up phases,though not in the cold-start phases. Therefore, we further integrateFinalMLP, the top-performing backbone from Group A, into meth-ods in Group D. Results are reported in . Notably, FinalMLP,",
  "number of GNN layers23 in (4)[0, 1, ,": "2 in loss function[1 2, . . . , 1]0.10.1number of heads[1, 2, . . . , 5]33embedding dimension[10, 11, . . . , 20]1616batch size512512pretraining learning rate0.0050.001pretraining epochs[1, 2, . . . , 20]21meta-training learning rate 20.0010.0001meta-training epochs[1, 2, . . . , 20]113update learning rate 1 during meta-training0.010.001update learning rate 1 during warming-up0.010.01warming-up epochs[1, 2, . . . , 20]1116 when utilized as a backbone for cold-start methods, does not exceedthe performance of configurations using DeepFM. This suggeststhat more recent CTR backbones cannot effectively handle the CTRprediction of newly emerging items."
}