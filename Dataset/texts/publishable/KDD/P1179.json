{
  "ABSTRACT": "In many recommendations, a handful of popular items (e.g., movies/ television shows, news, etc.) can be dominant in recommendationsfor many users. However, we know that in a large catalog of items,users are likely interested in more than what is popular. The domi-nance of popular items may mean that users will not see items thatthey would probably enjoy. In this paper, we propose a techniqueto overcome this problem using adversarial machine learning. Wedefine a metric to translate the user-level utility metric in termsof an advantage/disadvantage over items. We subsequently usedthat metric in an adversarial learning framework to systematicallypromote disadvantaged items. Distinctly, our method integrates asmall-capacity model to produce semantically meaningful weights,leading to an algorithm that identifies and promotes a semanticallysimilar item within the learning process. In the empirical study,we evaluated the proposed technique on three publicly availabledatasets and seven competitive baselines. The result shows thatour proposed method not only improves the coverage, but also,surprisingly, improves the overall performance. 1",
  "INTRODUCTION": "Recommender systems are used extensively in many consumer webapplications such as streaming services , video recommenda-tions , news feed recommendations , etc. The primary goalof recommender systems is to recommend appropriate items suchthat users are engaged, entertained or feel connected. The catalogof items can be movies, TV shows, news articles, videos, merchan-dise etc. Depending on how the training data is collected and howa model training is done many different types of biases can existin a recommender system ; overcoming such biases is animportant research direction in the field of recommender systemsand many approaches have been proposed to go beyond accuracyand address some of these biases (e.g., ).",
  "EAI-KDD2024, August 2525, 2024, Barcelona, Spain2024": "In this paper, we focus mainly on the popularity bias ,which is a particular type of bias where a recommender system rec-ommends many popular items at a possible disadvantage to manyother relevant items. The unpopular items, while have less userinteraction, constitute the majority of the catalog and are typicallyreferred to as the long tail of the catalog. Promoting such infrequentitems from the long tail is potentially crucial to users satisfactionand a higher chance of success to every item. Unlike popular items,these infrequent items can coincide with personalized taste, canprovide an unexpected experience other than mainstream popular-ity, motivate users to explore deeper into the catalog, and evokea sense of freshness . On the supply side of items, many moreitems have a chance to succeed on the platform such that a handfulof minority items do not suppress the chance of a vast majority ofitems.To alleviate this problem of a few items being concentrated atthe top of the recommendation, an intuitive solution is to adjustthe weight of items during learning. The weights are adjusted sothat the most frequent items are lowered while the rare ones areincreased during training. Although this practice may alleviatethe problem to some extent, there are many challenges associatedwith it. First, we need to clearly define an appropriate weight foreach item, while controlling how much we weigh one over theother. Second, it is unclear whether the frequency of the item aloneshould play such a big role in up-weighing an item, or whetherother factors should be considered.Any technique that reduces popularity bias and increases itemcoverage has the risk of reducing the overall performance of the rec-ommender system. By promoting many more less-frequent items, itis possible that the users would receive even less utility compared tothe more popular counterparts that could have been recommended.In this paper, we propose a method to decrease popularity bias,increase coverage, and, surprisingly, also improve the overall perfor-mance of recommendation systems. Rather than directly optimizingthe tail of the catalog, our technique focuses on the semanticallymeaningful tail. A semantically meaningful tail is a group of simi-lar items in the tail that exclude potential outliers. Unpopular itemson the tail can often be outliers. Excluding these outliers and onlykeeping semantically similar items helps to improve generalization.An example is illustrated in (A.b). In this figure, eachpoint visualizes an item, with blue being the unpopular ones (dis-advantaged) and orange indicating the others. The colored areaindicates assigned weights. Items in the blue area are assigned largeweights and promoted. Observe that the unpopular items in thedeep blue area are close to each other. This cluster of semanticallysimilar items in the tail forms a semantic tail. Our method tracksthese tails and assigns large weights to them. In contrast, there existtwo distant unpopular items in the green areas these are outliers.",
  "Create Smooth Weight Landscape": "Small-Weight Area (B) Weight Visualization Reweight Items and Repeat the Steps (A) Adversarial Learning Workflow(a)(b) (0,0)Origin : Figure (A) presents the adversarial learning workflow. In step 1, item advantage is quantified using an itemwise metricadapted from Recall (Eq. 7). Items are visually distinguished as blue (disadvantaged) and orange (advantaged) for illustrativeclarity. In steps 2 and 3, an adversarial model, constrained by a small Lipschitz constant, assigns a continuum of weights toitems. Driven by adversarial optimization, large weights are assigned to disadvantaged items, while small weights are assignedto advantaged ones. As this model produces smooth weight landscapes, to maximize the loss, these assigned weights naturallyfocus on the disadvantaged clusters while filtering out outliers. This process leads to the formation of semantic tails, a termthat refers to clusters of disadvantaged but semantically related items. Step 4 involves adjusting the weights of these items anditerating the process. As weights are reassigned, the semantic tails dynamically change. We repeat these steps and continuouslytrack the semantic tails via the adversarial model. Figure (B) presents a visualization of semantic tails on MovieLens, basedon Principal Component Analysis (PCA). Each dot represents an item, with the color indicating its associated weight. Rareitems appear near the origin in this representation. Observe semantic tails proximal to the center, characterized by a gradualdecrease in weights extending outward. More comparison and details can be found in Appendix D. Constrained by the capacity of the adversarial model, these outliersreceive small weights similar to those of their neighbors.(B) effectively echoes our theoretical concepts. It repre-sents MovieLens movies after PCA reduction, placing less popularitems near the origin due to sparse interactions. Each item is color-coded based on weights from the adversarial model, with the blueregion highlighting semantic tails and the promotion of less popularmovies in our approach. This smooth distribution of weights con-firms our analysis, demonstrating that similar items are assignedcomparable weights. For additional information, see Appendix D.The main contributions of our work are as follows. First, weidentify that typically in a recommender system we care about user-level ranking metrics, but for increasing item coverage, we need tobridge the gap and make sense at an item level. We thus start witha user-level metric and show how we can identify which items areat an advantage/disadvantage. Second, we use the defined metric inan adversarial model learning setting such that items at a disadvan-tage are promoted. Due to the smoothness of the adversarial model,much more semantically meaningful items are promoted ratherthan extreme outliers. Finally, we show empirical results where wecompare the proposed approach with several other baselines. Theexperiments show that while we significantly increase the coverageof items, we also improve the overall performance of recommen-dations. Finally, we show a number of empirical observations toshed light on how the proposed technique is able to achieve bothaccuracy and coverage.The remainder of the paper is organized as follows. We reviewother related work in . We define the problem setting andintroduce the notation in 3. We define a metric to define the advan-tage of an item and further use it in an adversarial learning settingto optimize it in . We briefly introduce those baselinescompared in . We show our empirical results on threepublicly available datasets in and conclude in .",
  "RELATED WORK": "Adversarial machine learning has recently gained popularity in the literature. Some of these works, such as , haveexplored the security aspects of recommendation systems. Unlikethese studies, our research focuses on improving key metrics with-out factoring in potential attacks. More recently, an instance-levelweight-based adversary was proposed in the context of su-pervised learning and was further studied in the context of rec-ommender systems . The work in improves model per-formance in users where the baseline models had difficulty. Inaddition, there have been other robust optimization approaches to improve the recommender system from a user performanceperspective. However, to the best of our knowledge, our work con-stitutes a first attempt to study an adversarial machine learningproblem from the item side on the recommender system.Popularity bias is a popular topic in the recommender systemsliterature. There have been existing works on handling popularitybias . Many of these approaches require identifying apriori which items are in the long tail\" and which ones are in theshort tail of recommendation. It is difficult for their manual designprocess to address the popularity bias problem in a systematicway. Compared to those approaches, we do not need to a prioriidentify which items are in the long-tail versus short-tail category.We compare two competitive baselines in this category, includingIPW and CVaR in and experiments. The resultsshow our significant improvement in addressing the popularitybias problem.Many approaches have been proposed for handling popular-ity bias or ensuring a degree of fairness among items in variousstudies . These methods typically focus onpost-processing rather than altering the core learning algorithm.During post-processing, they rerank the recommendations based",
  "POSIT: Promotion of Semantic Item Tail via Adversarial LearningEAI-KDD2024, August 2525, 2024, Barcelona, Spain": "x-axis and the y-axis are their second and third principal com-ponents.The close distance between the points indicates that therepresented movies are similar. We color each point according totheir assigned weight. A cool color represents a larger weight, anda warm color indicates a smaller weight. Overall, the points nearthe center tend to be unpopular items, as items with sparse inter-action are rendered close to zero. Similarly, items away from thecenter indicate popular ones. In Figure (a), we show the weightsfrom our method. Notice that our weights focus on semanticallyclose points, but much less on the popular ones. Figure(b) showsthe case where we directly use the item advantage score to definethe weight without an adversary. We can observe that differentcolors are mixed everywhere. Observe that some small weights (redpoints) are given to unpopular items in the center of semantic tails.Compared with Figure (b), our adversary in Figure (a) effectivelyapproximate a semantic tail from the advantage score and displayscontrasting but progressive colors. In Figure (c), we demonstratethe weight obtained from the best IPW model. Although it exhibitsprogressive colors, it does not effectively capture the semantic tailsas ours.",
  "PROBLEM SETTING AND PRELIMINARIES": "In this paper, we focus on recommendations in an implicit feedbacksetting. In this setting, we collect users interactions with a catalogof items. As a common assumption, the interaction of the usersindicates their preference. Unlike the explicit setting, the userswill not directly express their preferences like a rating or score.The less strict assumption of implicit setting makes it importantand popular as it closely aligns with many real-world applications,where a majority of users do not give explicit feedback.Consider a training dataset consisting of |I| items and |U| users,a user-item interaction matrix R|U||I|. User has the interac-tion history {0, 1}|I|, denoting that the user has interactedwith the items that are ones. When = 0, it indicates that theuser did not interact with the item . Such interactions can includea click, a buy, or watching a movie. The goal is to learn a recom-mendation model : {0, 1}|I| R|I|. The model gives a score forevery item that can be used for the item ranking. Given a trainedmodel, the goal then is to take a test users historically interacteditems and rank the non-interacted items that are relevant to theuser.To evaluate the performance of the recommendation system , we adopt a widely used metric - \"Recall@k\". We also considerwidely used metrics such as normalized discounted cumulative gain(NDCG) . Since these two metrics were highly correlated witheach other in our experiments, we mainly focussed on the recall.Consider the preference for the item for the user as . Therecommendation system scores each item via and ranks them bytheir scores for a particular user in descending order. Let the rankof the item for the user be . The Recall@K is defined as",
  "min(, I ).(1)": "Intuitively, it calculates the fraction of items within the top rankeditems that are actually relevant for user .In addition to recall, we use coverage to measure how well therecommendation system can cover the entire catalog of items. Con-sider a batch of 100 randomly selected users U. Coverage is definedas the unique number of items included in the top recommen-dations for these 100 users. Note that we fix the batch size 100 forfair comparisons. Choosing a larger batch size results in a similarobservation, as shown in Appendix C.",
  "(2)": "We report this number averaged among batches of 100 users. Ahigher coverage number indicates that the system can cover diver-sified items for different users. We also report the ratio of coverageagainst the theoretical upper-bound. The upper bound is attainedwhen each user is recommended with completely disjoint items.We define the coverage ratio as follows:",
  "L(;)(4)": "EASE. We consider a particular choice of loss function that wasused in EASE which is a strong baseline for recommender tasks.Unlike common matrix factorization approaches, EASE uses an itemsimilarity matrix R|I||I|. However, to avoid learning thetrivial solution of the identity matrix, EASE enforces the diagonalof the learned matrix to be 0. The optimization can be written as",
  "min 22 + 22.., diag() = 0(5)": "We adopt EASE as the base recommender because its results onthe implicit-feedback setting are very competitive. We will evalu-ate the performance of different methods applied in EASE. Whilemainly evaluating based on EASE, our method is general and canbe applied to other base recommenders as well. Although at thetime of training, a model like EASE (or Matrix Factorization) uses asquared loss, the loss is only a proxy at the time of optimization.The ultimate goal of the model trained is to still perform well on aranking metric such as NDCG or Recall that we discussed above.",
  "Identifying Items at a Disadvantage": "As we discussed in the Introduction, a typical recommender systemmay bias towards certain types of items, like popular items. In thispaper, we take an agnostic approach to such issues and adversari-ally remedy the problem by identifying semantically similar itemspotentially at a disadvantage and promoting them. We will formallymeasure the disadvantage of an item by a score whose magnitude",
  "EAI-KDD2024, August 2525, 2024, Barcelona, SpainQiuling Xu, Pannaga Shivaswamy, and Xiangyu Zhang": "Himan Abdollahpouri. Popularity bias in ranking and recommendation. InProceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, AIES 19,page 529530, New York, NY, USA, 2019. Association for Computing Machinery. Himan Abdollahpouri, Robin Burke, and Bamshad Mobasher. Controlling popu-larity bias in learning-to-rank recommendation. In Proceedings of the EleventhACM Conference on Recommender Systems, RecSys 17, page 4246, New York,NY, USA, 2017. Association for Computing Machinery.",
  "= 1 [ = 1 ] .(6)": "Next, we take the average of the score for each item. To decide thedenominator for the average, we can either count the number ofall users or the number of users with interactions with that item.The difference between these two options is whether the advantagescore should consider popularity bias or not. When using the size ofwhole users as the denominator, we essentially reflect the popularitybias in the advantage score. Since the denominator is the same forall items, items that would end up in the top for more userswould end up having a higher score. The other option removes thepopularity bias, normalizing by the number of interactions for thatitem. Therefore, Item Recall@ for item can be written as intwo versions,",
  "U(Without popularity bias)(8)": "When is high in equation (7), that means item is relevant andwould end up in the top more often showing that the item beingat an advantage given a ranking.In Appendix 6.1, we compare these two options and anotherfrequency metric. The results show that using eq.(7) for optimiza-tion yields better empirical performance. During the evaluation,different items may have very different sizes of interactions. Toreflect the performance on rare items, we include the formula eq.(8)with uniform item weights. The Item Recall@ is aggregated overall items as the follows,",
  "Adversarial Models": "Once we have a score for each item defined above, we considerthe following adversarial reweighted learning formulation. The rec-ommender optimizes a re-weighted loss from the adversary model.The adversarys goal is to identify and promote items at a disad-vantage. The adversarial model tracks the semantic tails, wheresemantic groups of disadvantaged items are implicitly identified,and creates smooth semantic weights based on the advantage scores.Semantic tails filter outliers and improve generalization during op-timization. Formally speaking, let us denotes the adversary model : {0, 1}|U| R parameterized by , the feature of item as :and the item advantage score of item defined in eq.(7) as , thenwe have the following formulation:",
  ": : (:) (:) .(11)": "We demonstrate that the desired property can be achieved byemploying a small-Lipschitz neural network. The Lipschitz constantis a measure that reflects the smoothness of the model, where asmooth model is characterized by having similar outputs for similarinputs. Specifically, a neural network with a Lipschitz constant of under the 2-norm satisfies the following relationship.",
  "Advantage Score Si": "0.0 0.2 0.4 0.6 0.8 1.0 Proposed Weight i Disadvantaged ItemsAdvantaged ItemsPotential Outliers : Illustration of Outlier Filtering during Re-weightingon Movie-Lens Dataset. This diagram reveals the relationshipbetween the weights proposed from the adversarial modeland the corresponding advantage scores, with each dot rep-resenting a specific movie. The X-axis details the advantagescore, while the Y-axis outlines the weight. For a better expla-nation, we group movies into three groups. It is worth notingthat these boundaries are drawn manually and are only forillustration purposes. The blue and orange groups symbolizedisadvantaged and advantaged movies, respectively, whichare aptly promoted or demoted according to their advantagescore. Conversely, the green group highlights potential out-liers that, despite being disadvantaged, are not adequatelypromoted due to the semantic constraints of the adversary,thereby preventing a shift in focus to dissimilar items. the weight proposal from our model. The smooth gradient meansthat the weights are generated under semantic restrictions.For robust optimization, we normalize the weights (:;) as, taking into account the varying magnitudes of the weights thatthe adversary might generate:",
  "= |I|(:;) (:;) .(13)": "The semantic weight is important for alleviating the effects ofoutliers during re-weighting. Across different datasets in our evalu-ation, we find many outliers possess a low item advantage score. Byintuition, these outliers are at disadvantages and we need to increasethe weight for those outliers in learning. However, indiscriminatelyamplifying the importance of these outliers without accounting forsimilarities among them can seriously degrade the performance ofthe recommender system. The reason is that the system is not ableto generalize to these outliers without adversely affecting the ma-jority. By enforcing a semantically meaningful weight, we promotesmoothness and assign similar weights to similar items. shows that our adversary is able to concentrate on the majorityof disadvantaged items instead of just a few outliers, which arehighlighted in green and detected by the models through seman-tic constraints. As those outliers are distant from the majority, alimited-capacity adversary must choose between promoting themajority of disadvantaged items or focusing on a small number ofdisadvantaged outliers. As optimization progresses, the strategy of focusing on the majority of items prevails to achieve the goal ofmaximization.It should be noted that the advantage score is not a fixed valueand is computed against the current model during optimization.In formulation (10) we proposed a general form of the learner andadversary that can be specialized to different recommendation basemodels. In the following, we further rewrite the training paradigmunder the baseline EASE by combining eq.(5) and eq.(10):",
  "I (),(15)": "where : is the user s interaction history and is the itemsimilarity matrix learned by the EASE model. Note that in the sum-mation and in eq.(14), we simultaneously optimize for each userand each item. This dependency on both dimensions makes it in-appropriate for optimization in batches. To accommodate batchoptimization, we further maintain an Exponentially Moving Aver-age (EMA) for values like advantage score . These approximatevalues are gradually updated during iterations.",
  "Normalization Operation": "Building upon the need to maintain a small Lipschitz constant, weintroduce a specific design to further regulate the constant, and thuscontrol the variation in the models outputs. Our design mergesboth normalization techniques and bounded activation functions(such as Sigmoid) to naturally filter outliers. This normalizationaims to center the input value ranges to zero before activation.Functions such as the sigmoid are employed to bound the outputrange, capping extremely large inputs, while having minimal impacton values near the average, which often signify outliers.We define the normalization operator as () = () () , where () represents the mean (standard deviation) and is a hyperpa-rameter. We also define the hyperbolic tangent function (), thesigmoid function (), and the fully connected layer (). Thesecombinations of operators can be manifested as () or (),depending on the desired output range. In addition, we utilize thehyperparameter to exert control over the degree of deviation ofthe inputs. A larger will filter out more outliers. For the adver-sarial component, we select () = 2 1(), anddifferent structural choices are compared in .In the algorithm 1, we show the details of the algorithm. Wesample a batch of users in line 4 and estimate the advantage scorein line 5. Line 6 normalizes the weights for stable optimization. Inlines 7-9, we follow the optimization as described in eq.(14). Wewill update the learning rate and save the best model after eachepoch.",
  "(b) Performance of Different Genres": ": In this figure, we report the Item Recall@100 for movies of different categories. We compare different methods inMovieLens. Performance is averaged for each category and sorted from the worst category to the best. A low performance onspecific categories, such as movies before 1910s and after 2010s, is due to limited data points in the dataset.",
  "I) .(16)": "CVaR. The condition value at risk (CVaR) optimizes overonly the tail of items. A tail contains the worst percent performeditems, which are measured by the training loss function. Formally,consider the loss function L(;) as in eq.(4), a positiveindicator []+ = max(0,) and an additional variable 1, we havethe optimization goal",
  ".(17)": "An optimal solution will ensure that 1 is the threshold of trainingloss which ensures that only the tail part of the loss is optimized.Rerank. Rerank promote items in the tail by post-processingduring evaluation. Suppose that a recommender ranks all itemsinto a list for some user based on relevance [1,2, ...,]. Each itemis associated with a relevance score such that 1 2... . Itpartitions this recommendation results by two thresholds and. The top recommendations are items of relevance . Themethod will keep their order the same as before. The tail recom-mendations are items of relevance . They are resortedaccording to frequency of the items occurrence. Infrequent items will rank higher than before, but still rank lower than the top rec-ommendations. Other recommendations with even less relevanceare simply ignored, as they represent irrelevant items. We fine-tunethe two thresholds during evaluation and report the best result.Causal. Causal elimination mitigates popularity bias by em-ploying a causal graph. Within this graph, popularity is introducedas an intricate interaction that includes users, items, and outcomes.They further estimate the causal effect through history and mitigatethe popularity effect by canceling part of the impacts.Post. Post-processing modifies the final recommendationscores by introducing a popularity neutral bias. This bias is derivedfrom the original recommendation score, with the intention ofshifting in favor of less popular items.MP. The Most-Popularity (MP) algorithm ranks items solelybased on their popularity, with more popular items receiving ahigher rank. This baseline is included to provide an intuitive demon-stration of the metrics on vanilla baselines.",
  "EXPERIMENTS": "In this section, we report experiments on three large-scale datasets(summarized in ), namely, MovieLens-20M , NetflixPrize and the Million Song Dataset in the baselines in-troduced in . In the comparison, we conduct parametersweeps for each baseline and report test metrics results for thebest model on validation set. We follow the same procedures forall baselines except CVaR and Rerank. They have direct trade-offsbetween performance and coverage. Reporting the model with thebest performance will only regress to the baseline EASE. Instead,we report a representative model with comparative performance.Specifically, we initialize the parameters of EASE and the firstlayer of the adversary model as 0. The reason is that these parame-ters directly take sparse inputs. Randomly initialized parameterswill prevent a stable convergence, as some of these values mayhardly get a chance to update themselves. We use SGD optimizer",
  "Standard Deviation9520.00090.0004": ": We report four metrics, including coverage and both user-side and item-side performance, for five different methods inthis table. Here, represents the standard deviation of the metric. To calculate the standard deviation of Coverage, we splitthe test set in chunks of 100 users. We report the standard deviation between chunks. For performance, we run each baseline10 times on different test sets. Best results within one standard deviation are highlighted. We found NDCG@{20,50} reflectssimilar trends as NDCG@100. Please refer to Appendix due to space limit. Causal fails on Million Song due to memory limit. for our approach. For all baselines, we fine-tune the hyperparam-eters, including the learning rate and 2-regularized . For POSIT, we also tuned the learning rate of the adversarial model and thestructure of the model (see ). For IPW, we tune the parame-ter . We tune the parameter for CVaR and and for Rerank.For Causal, we tune and . For Post, we tune and . The subparperformance of the Causal method is likely attributed to its relianceon the strong assumption that the models output carries a prob-abilistic meaning, an assumption that is incorrect under EASEsframework. Additional details can be found in Appendix A.Coverage. In we report the coverage@k from differ-ent methods on the three datasets. We report coverage@k when = 20, 50, 100 for a thorough evaluation. The calculation of cov-erage can be found in eq.(2). The result shows that our proposed method consistently achieves a competitive coverage number com-pared to other baselines on three datasets. Specifically, our methodimproves nearly 20% on Coverage@100 compared to that of EASEon MovieLens dataset.While Rerank has a coverage that is close toPOSIT when = 100, its trade-off towards coverage significantlyimpacts its performance. By comparing the results, we observe thatthe proposed method is much more effective in scenarios wherek is large or when the baseline has poor coverage. For example,the Coverage@100 of EASE on MovieLens is one-quarter of thatnumber on Million Song dataset. Consequently, the increase incoverage@100 in MovieLens is greater than on Million Song. Thebenefits of promoting tails increase as the baseline becomes moreconcentrated due to the training data.Similarly, the improvement of Coverage@100 is greater thanthat of Coverage@20 on these three datasets. It is because the",
  "Ablation Study": "In this section, we conduct ablation studies on various researchquestions(RQ) impacting the POSIT . We study one factor at a time,perform parameter sweeps for other factors, and report the bestresult. We will draw the Pareto frontier between coverage andperformance. A curve in the upper right corner is better.RQ1: How to design a good adversarial model for POSIT ?We evaluated different adversarial model structures in and found that combining bounded activation and normalizationleads to improved performance. This suggests that these operatorseffectively filter out outliers. Observe the large differences in cover-age between the 2-layer models with or without such operators in. It is worth noting that the seemingly small variation onNDCG has a big impact on semantic tails, due to their minor pres-ence. It also showed that the number of layers, or model capacity,is a crucial factor for the best performance. Specifically, a two-layerMLP provides the best results. We also find that a small model (with10 hidden units) provides good performance in . As the sizeof adversary is around 1e-3 of the size of the recommender, thetime overhead of the adversary is small.Additional studies of the research question can be found inthe Appendix C; The second research question (RQ2) investigateswhether popularity bias is essential to identify a disadvantaged item.Our findings show that even without specific information aboutpopularity, an adversary can recognize less popular items as disad-vantaged. This suggests potential common interaction patterns fordisadvantaged items other than just popularity. The third researchquestion (RQ3) explores the impact of the batch size of the user onthe definition of coverage and its effect on the results presented.Our analysis reveals a trade-off between inter-user diversity and thecoverage of rare items, depending on the number of users selectedin the batch. Furthermore, Appendix D compares the use of an ad-versary model with the advantage scores. The results demonstratea notable improvement over smooth weight assignment throughthe adoption of the adversary model in visualization.",
  "Gediminas Adomavicius and YoungOk Kwon. Toward more diverse recommen-dations: Item re-ranking methods for recommender systems. In Workshop onInformation Technologies and Systems. Citeseer, 2009": "Deepak Agarwal, Bee-Chung Chen, Rupesh Gupta, Joshua Hartman, Qi He,Anand Iyer, Sumanth Kolar, Yiming Ma, Pannagadatta Shivaswamy, Ajit Singh,and Liang Zhang. Activity ranking in linkedin feed. In Proceedings of the 20thACM SIGKDD International Conference on Knowledge Discovery and Data Mining,KDD 14, pages 16031612, New York, NY, USA, 2014. ACM. Lars Backstrom. Serving a billion personalized news feeds. In Proceedings of theNinth ACM International Conference on Web Search and Data Mining, WSDM 16,page 469, New York, NY, USA, 2016. Association for Computing Machinery.",
  "Harald Steck. Embarrassingly shallow autoencoders for sparse data. CoRR,abs/1905.03375, 2019": "Lequn Wang and Thorsten Joachims. User fairness, item fairness, and diversity forrankings in two-sided markets. In Proceedings of the 2021 ACM SIGIR InternationalConference on Theory of Information Retrieval, ICTIR 21, page 2341, New York,NY, USA, 2021. Association for Computing Machinery. Tianxin Wei, Fuli Feng, Jiawei Chen, Chufeng Shi, Ziwei Wu, Jinfeng Yi, and Xi-angnan He. Model-agnostic counterfactual reasoning for eliminating popularitybias in recommender system. CoRR, abs/2010.15363, 2020. Hongyi Wen, Xinyang Yi, Tiansheng Yao, Jiaxi Tang, Lichan Hong, and Ed H.Chi. Distributionally-robust recommendations for improving worst-case userexperience. In Proceedings of the ACM Web Conference 2022, WWW 22, page36063610, New York, NY, USA, 2022. Association for Computing Machinery. Qiuling Xu, Guanhong Tao, and Xiangyu Zhang. Bounded adversarial attack ondeep content features. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), pages 1520315212, June 2022.",
  "Yang Zhang, Fuli Feng, Xiangnan He, Tianxin Wei, Chonggang Song, GuohuiLing, and Yongdong Zhang. Causal intervention for leveraging popularity biasin recommendation. CoRR, abs/2105.06067, 2021": "Ziwei Zhu, Yun He, Xing Zhao, and James Caverlee. Popularity bias in dynamicrecommendation. In Proceedings of the 27th ACM SIGKDD Conference on Knowl-edge Discovery and Data Mining, KDD 21, page 24392449, New York, NY, USA,2021. Association for Computing Machinery. Ziwei Zhu, Yun He, Xing Zhao, Yin Zhang, Jianling Wang, and James Caverlee.Popularity-opportunity bias in collaborative filtering. In Proceedings of the 14thACM International Conference on Web Search and Data Mining, WSDM 21, page8593, New York, NY, USA, 2021. Association for Computing Machinery.",
  "AEXPERIMENT DETAILS": "In this section, we discuss the experiments configuration in details.We set training epochs = 50, = 0.9 in algorithm 1. We use abatch size of 1024 for MovieLens and 8192 for Million Song andNetflix Prize. We set the momentum of SGD optimizer as 0.9. Weuse 10k users for validation and another 10k users for testing onMovie Lens. We increase this number 10k to 40k on Netflix Prizeand 50k on Million Song dataset, respectively. Some datasets pro-vide explicit preference information, like Movie Lens. We followthe literature and convert them into implicit settings using apreference threshold. Preference score greater than this thresholdis set to 1 and otherwise it is set to 0.We conducted the experiments on machines with 64G memoryand 64 CPUs without GPU. This choice is due to budget concerns.We can finish the training on CPU as our model is relatively smalland reasonably fast enough on CPU. It typically takes 8 hours totrain POSIT on MovieLens, 1.5 days on Netflix Prize, and 1 day onMillion Song. The major overhead is due to gradient optimizationon these very large datasets. Speed can be potentially improvedvastly with GPU based training.We conduct hyperparameter searches. is an example ofthe parameter search for baseline IPW on MovieLens. Note that wemay fine-tune multiple factors and the results will not necessarilybe presented as a 2-dimensional figure. lists all the hyper-parameters that are needed to exactly reproduce our results. Wewill also release our codes upon acceptance. 1e-055e-058e-050.00010.00030.0005 1e-052e-053e-054e-055e-056e-057e-058e-059e-050.0001 0.40840.41430.41640.41720.420.4204 0.40870.41480.41670.41760.42030.4206 0.40930.41540.41710.41780.42060.4206 0.40980.41570.41740.41810.42060.4209 0.40990.41590.41770.41840.42080.4208 0.41050.41640.41810.41860.42090.4206 0.41080.41670.41830.4190.42080.4203 0.41140.41690.41840.41910.42050.42 0.41190.41720.41840.4190.41990.4192 0.41210.4170.41820.41850.41910.418 IPW NDCG@100 0.410 0.412 0.414 0.416 0.418 0.420",
  "BFREQUENCY DIVERSITY": "In table 4, we report the relative Gini Index of recommenders . It is the Gini Index on recommendations divided by the sameindex on training data. It represents the increase in popularity-bias for a recommendation system. A smaller value is better. Avalue greater than 1 indicates the learning algorithm increases thepopularity bias. We can observe that our algorithm tends to have asmaller number compared to other baselines.",
  ": Comparison of Advantage Score": "RQ2: Does the adversarial model require popularity biasto determine the weight?We compare three candidates of the advantage score on the samemodel in . After hyperparameter tuning, the results showthat the advantage score without popularity bias performs similarlyto the frequency score. Note that the frequency score is a keyindicator of popularity bias. This supports the idea that the scorederived from user-level performance without popularity bias caneffectively separate advantaged items from the disadvantaged ones.Furthermore, by combining popularity bias in eq.(7), we achievebetter results than relying solely on popularity bias.",
  "(c) Weight from the Best IPW model": ": Semantically Meaningful Weight. In this figure, we visualize the movies and their corresponding weights in theMovieLens-20M dataset obtained from different baselines. We visualize the movies by conducting Primary Component Analysis(PCA) on features of the movies. Each movie is represented as a point. We use the second and third components as the x-axisand y-axis. Note that the first component is omitted because it is less related to semantic similarity. We discuss this issue indetail in . RQ3: How will the batch size (|U|) impact the results ofCoverage@k in its definition (eq. 2)?The batch size (|U| in (eq. 2)) used to calculate Coverage@k is fixedat 100 for fair comparisons. However, different batch sizes mayemphasize different aspects of coverage. When the batch size issmall, such as 2, the metric focuses more on inter-user diversity. Asthe batch size increases larger, such as 100, the metric focuses oncoverage of rare items. To gain a better understanding of POSIT sinduction bias, we compare the Coverage@k under different userbatch sizes, as shown in . The results show that POSIT con-sistently produces competitive results in different batch sizes. Thissuggests that using batch size 100 strikes a good balance betweendissimilarity and rare-item coverage. Furthermore, we observe animprovement 60% on POSIT compared to EASE when the batchsize is 100. This highlights POSIT s ability to better promote rareitems.",
  "DVISUALIZATION": "This section explains the visualization in detail. We visualize eachmovie in MovieLens-20M dataset. We first conduct the PrincipalComponent Analysis on the interaction matrix . After PCA de-composition, we plot each movie, with the x-axis and y-axis being 1st Principle Component 2rd Principle Component 0.20.40.60.81.0 : Weight Visualization Based on The First and SecondPrincipal Component. Observe that the points are renderedblue to red from the left to right in the x-axis. The points nearthe origin on the left are unpopular items, and the pointsare the far right are popular items, due to sparsity of theinteraction matrix. Observe the x-axis (the first component)dominates the popularity, as items become more popularwhen it is closer to the right. the second and third principal components. It is worth noting thatthe first principal component is omitted as it is proportional to thepopularity. The popularity is less related to the semantic similarity;therefore, we instead use the second and third components. Thereader interested in visualization using the first component canfind such an example in .In , we visualize the weights proposed from POSIT ,Adversary Score and weights from the best IPW model. The"
}