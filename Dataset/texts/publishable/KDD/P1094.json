{
  "Abstract": "With the rapid proliferation of scientific literature, versatile aca-demic knowledge services increasingly rely on comprehensive aca-demic graph mining. Despite the availability of public academicgraphs, benchmarks, and datasets, these resources often fall shortin multi-aspect and fine-grained annotations, are constrained tospecific task types and domains, or lack underlying real academicgraphs. In this paper, we present OAG-Bench, a comprehensive,multi-aspect, and fine-grained human-curated benchmark basedon the Open Academic Graph (OAG). OAG-Bench covers 10 tasks,20 datasets, 70+ baselines, and 120+ experimental results to date.",
  "Part of the work was done when Fanjin worked at Zhipu AI.Jie Tang is the corresponding author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for third-party components of this work must be honored.For all other uses, contact the owner/author(s).KDD 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s).ACM ISBN 979-8-4007-0490-1/24/08 We propose new data annotation strategies for certain tasks andoffer a suite of data pre-processing codes, algorithm implementa-tions, and standardized evaluation protocols to facilitate academicgraph mining. Extensive experiments reveal that even advancedalgorithms like large language models (LLMs) encounter difficultiesin addressing key challenges in certain tasks, such as paper sourcetracing and scholar profiling. We also introduce the Open AcademicGraph Challenge (OAG-Challenge) to encourage community inputand sharing. We envisage that OAG-Bench can serve as a commonground for the community to evaluate and compare algorithms inacademic graph mining, thereby accelerating algorithm develop-ment and advancement in this field. OAG-Bench is accessible at",
  "KDD 24, August 2529, 2024, Barcelona, SpainFanjin Zhang et al": "Experimental Results. presents results for various match-ing models for venue alignment and affiliation alignment. The meth-ods for author alignment utilize more structural information, withcorresponding results presented in Section A.1. Venue/affiliationalignment is essentially a short text matching task. SVM performsslightly poorer than LinKG and LinKG, possibly due to the inabil-ity of SVM features to capture word order information. Conversely,LinKG and LinKG can capture the contextual dependence of wordsequence. Overall, the top-performing models on both datasets uti-lize pre-training, indicating the promising potential of pre-trainedlanguage models for entity alignment. Among all methods thatemploy pre-trained models, DeBERTa-large delivers the best perfor-mance on two tasks, particularly on affiliation alignment, indicatingthat DeBERTa-large effectively encodes additional semantic knowl-edge beyond affiliations surface names. Note that the trends ofAUC and F1 in the table are sometimes inconsistent. Given that F1requires a threshold setting, AUC is a more reliable metric whendiscrepancies arise between the two.The results of author alignment are shown in . We ob-serve that methods using authors structure information (SVM andLinKG) are significantly better than methods not using structureinformation (LinKG and LinKG). In the future, more author pairswith ambiguous names will be added to increase the difficulty ofthis task.",
  "Introduction": "The overarching goal of academic data mining is to deepen ourcomprehension of the development, nature, and trends of science.It offers the potential to unlock enormous scientific, technological,and educational value . For example, deep mining from aca-demic data can assist governments in making scientific policies,support companies in talent discovery, and help researchers acquirenew knowledge more efficiently.The landscape of academic data mining is rich with entity-centricapplications, such as paper recommendation, expert finding, andvenue recommendation. Several popular academic mining systems,such as Semantic Scholar1, ResearchGate2, and AMiner3, are allpowered by academic knowledge graphs (AKG)4. Based on differ-ent data sources, there have been multiple public academic graphsand academic benchmarks, such as Microsoft Academic Graph(MAG) and S2ORC . A comparative overview of these aca-demic resources is presented in . However, there remainseveral defects in existing popular datasets that may hinder promis-ing explorations, which are summarized as follows:",
  "Public academic graphs, such as MAG and OAG , lack multi-aspect and fine-grained annotations, impeding potential evalua-tion of downstream tasks on top of them": "Academic benchmarks, such as S2ORC and BLURB , are lim-ited to specific task types (e.g., NLP tasks) and domains (e.g.,biomedicine), which may not cover the full spectrum of academictasks, such as various graph-based tasks. Separate academic datasets, such as PubMedQA and con-cept taxonomy datasets , often do not include or align withlarge-scale and comprehensive academic graphs, resulting in adivergence from real-world scenarios. Present Work. To this end, we introduce OAG-Bench, a meticu-lously human-annotated academic benchmark for academic graphmining. OAG-Bench currently includes ten tasks, 20 datasets, 70+baseline methods, and 120+ experimental results. providesan overview of OAG-Bench. Specifically, (1) For the design principles of OAG-Bench, we aim to conductcomprehensive and fine-grained annotations on the large-scaleOAG for the full life cycle of academic graph mining. Firstly, weannotate the nodes and edges of the academic knowledge graphand identify valuable and challenging tasks during this process,such as author name disambiguation. Then, powered by theacademic graph, academic applications explore tasks beyond",
  "the academic graph itself and study knowledge acquisition andcognitive impact, such as paper source tracing (C.f. )": "(2) For the datasets in OAG-Bench, we construct various human-curated datasets for diverse tasks. We also propose new annota-tion strategies for certain tasks, such as checking inconsistentpaper assignments across sources for incorrect paper-authorassignment detection and marking the sources of papers viaonline paper reading groups. Notably, ten datasets in eight tasksare newly constructed. The dataset sizes in OAG-Bench rangefrom thousands to millions. (3) For the evaluation of OAG-Bench, OAG-Bench provides corre-sponding data processing methods, evaluation metrics, and atleast three baseline methods for each task. OAG-Bench imple-ments a wide range of methods, covering traditional machinelearning methods, shallow convolutional/recurrent/graph neu-ral networks, LLMs, etc. Experimental investigations show thatadvanced generation-based LLMs hold promise in some taskslike author name disambiguation, but they still struggle withtasks like scholar profiling and paper source tracing.To sum up, OAG-Bench makes the following contributions: First,we provide multi-aspect and fine-grained human-curated datasetsthat cover the full life cycle of academic graph mining. Second, werelease a series of data pre-processing codes, algorithm implemen-tations, and standardized evaluation protocols to assist researchersin getting started quickly in academic graph mining. Finally, basedon OAG-Bench, interested researchers or practitioners can developadvanced AKG-based algorithms, study the foundation models foracademic graph mining, and so forth.",
  "Academic Knowledge Graph": "An academic knowledge graph (AKG) is defined as a graph ={, } where each entity and each relation are associ-ated with type mapping functions () : and () : ,respectively. and represent the sets of entity and relation typeswith || > 1 and || > 1. Each entity pair 1 and 2 is linked by aspecific relation to form a tuple (1,,2).For instance, an academic graph is a heterogeneous entity graphthat encompasses multiple types of entities, such as authors, papers,and venues. The relation set, represented by , includes severalkey relationships: the authorship relation, which connects authors",
  "Academic Datasets": "Some organizations have made their academic graphs available,including MAG , OAG , AceKG , OpenAlex5, andCrossRef6. These graphs are typically large-scale, but are rarelycarefully annotated to benchmark a wide range of academic tasks.Additionally, some benchmarks based on academic corpus havebeen proposed, such as S2ORC , SciDocs , and BLURB ,but they mainly target NLP tasks and overlook the intricate struc-ture of academic graphs.To bridge the gap, our objective is to meticulously annotate large-scale academic graphs to benchmark various tasks for academicgraph mining. Our initiative, OAG-Bench, leverages the Open Aca-demic Graph (OAG)7, which was initially generated by linking twolarge academic graphs: MAG and AMiner. OAG aligned large-scaleentities in MAG and AMiner, including papers, authors, affiliations,and venues, with an accuracy of over 97%. It has made availablethe alignment relations between these two graphs alongside theirmetadata. As MAG turned down its service at the end of 2021, OAGhas expanded its data sources to include PubMed, ArXiv, CrossRef,and so forth. To date, five versions of OAG have been released,amassing around 700 million entities and 2 billion relations.",
  "OAG-Bench Framework": "In this section, we first propose the overall design principle ofOAG-Bench, and then present the detailed workflow about how toconstruct comprehensive and high-quality datasets.As depicted in , we host a series of data collection andannotation efforts to conduct multi-aspect and fined-grained la-beling based on the OAG. The framework aims to leverage high-quality academic knowledge graphs (AKG) to facilitate academic",
  ": The overall construction framework of OAG-Bench": "data mining. Therefore, the framework is structured into two typesof tasks: AKG construction and AKG-empowered academic appli-cations. AKG construction focuses on disambiguating or enrichinggraph nodes and correcting or completing graph edges, consistingof Academic Entity Construction and Academic Graph Completion.Beyond basic academic relationships, Academic applications delveinto knowledge and cognition, consisting of Academic KnowledgeAcquisition and Academic Trace and Prediction.(1) Academic Entity Construction. The construction of academicentities is fundamental to the construction of academic graphs. Thisstage mainly identifies the identical real-world entities across datasources. For notoriously ambiguous entities, i.e., authors, we furtherincorporate the author name disambiguation task.(2) Academic Graph Completion. Building upon the conflatedacademic entities, this stage aims to establish connections betweendifferent entities to complete and enrich academic graphs. Specif-ically, we engage in fine-grained scholar profiling labeling andattach concepts to entities, such as authors, papers, and concepts.(3) Academic Knowledge Acquisition. On top of high-quality aca-demic graphs, this stage focuses on the acquisition of academicknowledge and models the multifaceted relations between usersand papers. We gather user behavior records from real academicsystems to build corresponding datasets.(4) Academic Trace and Prediction. Besides the correlation be-tween academic knowledge and users, this stage aims to furtherexplore the cognitive influence exerted by papers and authors. Itinvolves retrospective analysis to pinpoint the pivotal referencesthat have inspired a research paper. Looking forward, the challengelies in forecasting impactful papers or authors.",
  "Academic Entity Construction": "To integrate academic data from multiple sources, various typesof entities need to be aligned. Thus, we first include the entityalignment task. In view of the severe ambiguity of author names,we further add the author name disambiguation task. Entity Alignment. Given two entity sets 1 and 2, the goal ofentity alignment is to generate entity matchings = {(1,2)|1 1,2 2} such that 1 and 2 refer to the same real-world en-tity. Specifically, we consider three types of entities, i.e., authors,affiliations, and venues. As for dataset annotation, we randomlysample a venue set and an affiliation set, and then manually labelvenue pairs with high similarity calculated by the Jaccard Index,and construct affiliation alignment pairs by using aliases or formernames derived from the information box of their Wikipedia entries.We utilize Wikipedia due to its high data quality. Meanwhile, itallows us to accurately obtain positive affiliation alignment pairswithout the need for manual labeling. For author alignment, wesample top-viewed computer science authors from AMiner, andthen manually pair them with DBLP authors according to theiraffiliations, published venues, and papers. As a result, we construct1,200 venue pairs, 5,000 affiliation pairs, and 10,000 author pairs. Author Name Disambiguation (AND). Aiming to disambiguatethe same-name authors, AND is a key and challenging task in aca-demic knowledge graph construction. We adopt the WhoIsWho dataset, a million-scale human-annotated dataset for author namedisambiguation. WhoIsWho breaks down the task into three sub-tasks: (1) From-scratch Name Disambiguation (SND), (2) Real-timeName Disambiguation (RND), and (3) Incorrect Assignment Detec-tion (IND). While existing research primarily concentrates on SNDand RND, the IND task has received less attention despite its grow-ing importance with the expansion of academic databases. Givenan author profile with paper lists, IND aims to detect incorrectlyassigned papers to this author. To address the IND challenge, if wewere to randomly select author profiles for annotation of their pa-per assignments, theres a high likelihood that we would encounternumerous profiles with a little ambiguity. Thus, we propose aneffective cross-checking annotation strategy. Specifically, we utilizeexisting paper alignments and author alignments between AMinerand DBLP, and then gather inconsistent paper-author assignmentsfor further expert checking. This strategy ensures that the profiles under checking have a high likelihood of inaccuracy (with a sig-nificant error rate exceeding 30% for these inconsistencies withinAMiner). Subsequently, all papers associated with AMiner authorsthat have incorrect assignments are manually checked by experts.Finally, the refined IND dataset includes 1,691 authors, 326,738 pa-pers, with an assignment error rate of 11.32% and reaching 1.5 timesthe scale of papers of the IND task in WhoIsWho.",
  "Academic Graph Completion": "Academic graph completion aims to enrich academic graphs fromtwo aspects entities and relations. To enrich entities, we includethe scholar profiling task to extract multidimensional attributes forauthors. To enrich relations, we include the entity tagging task toattach concepts to entities. Concepts are abstract entities that canendow semantics to entities. To further build a hierarchical knowl-edge structure, we also include the concept taxonomy completiontask to identify hypernyms and hyponyms for new concepts. Scholar Profiling. Profiling scholars from big data is a vital taskin scholar mining, and it becomes harder and harder due to datafragmentation, modeling lengthy texts, data noise, etc. Previousworks on scholar profiling usually extract attributes from schol-ars homepages or search engines. In OAG-Bench, besides profilingscholars from search engines, we introduce a new complex set-ting Multidimensional Scholar Profiling from Long Texts, whichaims to extract multiple attributes in lengthy texts. Each attributeextraction includes the starting and ending positions in the text.Importantly, long attributes are also taken into consideration, suchas work experience and education experience. These attributes canoften exceed 100 tokens. Traditional scholar profiling or namedentity recognition tasks seldom focus on extracting such long at-tributes. For data annotation, scholars with detailed biographicaldescriptions are randomly sampled. Then, we manually label thestarting and ending positions of each attribute in the texts. Finally,we construct 2,099 scholars with 12 attributes. Entity Tagging. Aiming at associating entities with concept la-bels, entity tagging is an important step in building semantic andhierarchical academic graphs. We introduce scholar interest extrac-tion and paper topic classification to attach concepts to scholarsand papers, respectively. Scholar interest extraction aims to ex-tract scholars research interests from their publications. Derivedfrom 2017 Open Academic Data Challenge8, the dataset of this taskcontains manually annotated 789 research interest tags for 11,357scholars and their papers. Paper topic classification aims to classifypapers into several topics based on the paper citation network. Fordataset construction, based on the DBLP paper citation network9,each paper is assigned one of nine topics10 related to computerscience based on its publication venue.",
  "TaskData Source#DatasetsData Volume#BaselinesData characteristics": "Entity alignmentAMiner/DBLP/MAG3/21K-10K5Matching heterogeneous entitiesAuthor name disambiguationAMiner3/11M10Million-scale human-annotated dataScholar profilingAMiner2/12K-9K13Long attribute extraction for long textsEntity taggingAMiner2/111K-900K12A large number of class labelsConcept taxonomy completionMAG/AMiner3/11K-300K3Professionally-labeled data for the AI fieldPaper recommendationAMiner1/010K4User click records in a real academic systemReviewer recommendationFrontiers1/1200K4Authentic public review recordsAcademic question answeringZhihu/StackExchange1/018K6Automatic QA for academic domainPaper source tracingAMiner1/12K11Careful annotations by researchersAcademic influence predictionAMiner3/21K-1M12Summarizing Test-of-Time award in CS",
  "CS: computer science": "under machine learning. The automatic construction of concepttaxonomies is a critical challenge in the fast-evolving landscapeof knowledge concepts, which is beneficial to the organization ofknowledge in the realm of big data. Given an existing concepthierarchy tree (Taxonomy) 0 and a set of new concepts , thegoal of concept taxonomy completion is to predict its hypernym() 0 and hyponym () 0 for each new concept tocomplete and expand the existing concept hierarchy tree. We adopttwo MAG taxonomies (MAG-Full and MAG-CS) as two taxon-omy datasets. These two datasets are large-scale but not carefullyverified by experts. Thus, we introduce a newly manually curateddataset covering AI sub-fields by AI researchers, with 1,335 con-cepts and 1,283 edges. The guidelines of edge construction refer torelevant textbooks and the ACM Computing Classification System.",
  "Academic Knowledge Acquisition": "Academic services based on academic graphs provide conveniencefor researchers to acquire knowledge actively or passively. For pas-sive academic recommendation, we include paper recommendationand reviewer recommendation. For active knowledge acquisition, weinclude academic question answering task. Paper Recommendation. As the volume of papers surges, re-searchers face increasing challenges in locating relevant literature.Given a user-paper bipartite graph = {, , }, where is theuser set, is the paper set, and signifies interactions (e.g., clicks)between users and papers, the goal of paper recommendation is topredict the next paper a user will interact with . We collect userbehavior data based on the real AMiner system. AMiner providesa real-time paper recommendation service for researchers on thehomepage. Researchers can offer several keywords to subscribe torelevant research papers. The back-end recommendation enginemakes recommendations based on the users historical click records.This dataset includes 5,340 users, 14,967 papers, and 163,084 inter-actions as of October 2021. To ensure quality, only users/paperswith over 10 clicks/be-clicked instances are included. Reviewer Recommendation. As the volume of submissions toacademic journals and conferences increases, reviewer recommen-dation becomes increasingly hard. Different from paper recom-mendations, reviewer recommendation aims to pair papers withproficient and willing reviewers. Given a paper submission set , areviewer set , and known paper-reviewer matches , thistask is to predict the reviewer for a new submission record , Additional information, including paper metadata and re-viewer expertise, is available. For data collection, we extract realpaper-reviewing records from the open-access platform Frontiers.After processing, it includes 210,069 reviewers and 225,478 papers,with each paper having at least 2 reviewers. Furthermore, we matchreviewers to authors in OAG using names, affiliations, and researchinterests, linking approximately half of the reviewers to the OAG.These reviewers are associated with their respective publications. Academic Question Answering. Traditional keyword-based in-formation retrieval cannot satisfy professional knowledge retrievalin the era of artificial intelligence. For instance, consider the ques-tion, Can neural networks be used to prove conjectures?. Howto retrieve answers and evidence from scholarly literature? Givenan academic question and a paper set = {1, 2, .., }, thegoal of academic question answering is to select the most relevantpapers from the candidate set . We adopt OAG-QA dataset,which is derived from academic question-answering platforms. Weretrieve question posts from StackExchange and Zhihu websites,extract the paper URL mentioned in the answer, and match it withthe paper in OAG . It comprises 17,948 question-paper pairs.Questions cover 22 disciplines and 87 topics, forming a two-levelhierarchical structure; that is, each topic belongs to a discipline.For each topic, 10,000 candidate papers, including the ground-truthpapers in the answers, are included.",
  "Academic Trace and Prediction": "Understanding the evolution of science on the cognitive level offersthe potential to predict, change, and finally invent the future. Trac-ing back to the past, we include paper source tracing task to identifythe sources of research papers. To predict future potential academicimpact, we include two tasks, i.e., paper influence prediction andauthor influence prediction. Paper Source Tracing (PST). Tracing the sources of papers iscrucial for understanding technological essence and uncoveringinnovation patterns. Given a paper (including its full text) andits references, the goal of PST is to identify the most importantreferences (termed ref-source) that largely inspired the paper interms of ideas or methods. The source papers of a given paper aredefined by the following principles: (1) the main idea of the paper is inspired by the reference; or (2) the main method of the paper comes from the reference. In other words, this paper would notcome into being without these source papers. We carefully build",
  "Since SND and IND are two widely studied tasks, we take incorrectassignment detection (IND) as an illustration for evaluation": "Baselines. We adopt graph-based anomaly detection methodsand LLM-based methods as baselines. For each author, graph-basedmethods first construct a paper similarity graph based on attributesimilarity (e.g., co-authorship, co-organization) and then detectanomalies in the graph. (1) Logistic Regression (LR): injects topeigenvectors of each graph as features to perform node classifi-cation. (2) GCN : employs graph convolutional networks asthe encoder, and then uses fully-connected layers to classify nor-mal/abnormal nodes. (3) GCCAD : leverages graph contrastivelearning and contrasts abnormal nodes with normal ones in terms",
  "of their distances to the global context. (4) ChatGLM : fine-tunes ChatGLM-6B model by inputting each authors paper list andasking the model whether one given paper is an anomaly or not": "Evaluation Metrics. Due to the imbalance between positive andnegative instances, we adopt the widely-used metric AUC. Further-more, we choose mean average precision (MAP) as another metric,which pays more attention to the rankings of incorrect instances.We take a macro average of each metric for each author. Experimental Results. shows the performance of incor-rect assignment detection. We observe that graph neural network-based methods (GCN and GCCAD) outperform the traditionalmethod (LR) based on eigenvalue decomposition. GCCAD explicitlycontrasts abnormal paper nodes with other nodes, yielding betterperformance than GCN. Surprisingly, ChatGLM outperforms graph-based anomaly detection methods, indicating the potential of theattention mechanisms in LLMs to capture the complex correlationsbetween the target paper and the overall author profile. The bestperformance of IND is not that satisfactory compared with that ofSND and RND tasks , suggesting that more attention should bepaid to the IND task for author name disambiguation in the future.",
  "In this subsection, for entity attribute enrichment, we present theevaluation of multidimensional scholar profiling from long texts": "Baselines. We select the latest NER methods based on pre-trainedmodels: (1) Han et al. : use a Biaffine decoder to generatefeatures for each start and end position and then employ CNN toclassify locations based on spatial position dependence. (2) Global-Pointer : uses a multiplicative attention mechanism to incor-porate relative positional encodings of start and end positions andalleviates class imbalance via modified loss functions. (3) UIE :is a generative pre-trained model based extraction framework withstructure extraction languages and template-specific prompts. Evaluation Metrics. Precision, Recall, and F1 are computed bycomparing predicted and annotated text segments for each attribute.These individual attribute results are then averaged to obtain theoverall evaluation result.",
  "OAG-Bench: A Human-Curated Benchmark forAcademic Graph MiningKDD 24, August 2529, 2024, Barcelona, Spain": "Experimental Results. a and b report the perfor-mance of SND and RND methods, respectively. For SND methods,despite using OAG-BERT for semantic paper representation, theliub method underperforms, suggesting the need for further explo-ration of applying large language models. In addition, given thesimilarity of the three methods frameworks, how to break awayfrom paper representations based on semantic and structural di-mensions to calculate paper similarity and then use the DBSCANalgorithm to cluster papers is also worthy of further study.For RND methods, all three methods yield good results. AlexNEattempts to introduce a graph structure to help disambiguate unas-signed papers, which is a less explored direction. Data Magicianuses time-based paper similarity features for complex name dis-ambiguation scenarios. Further research is needed for challengingsituations like co-authors with identical names or affiliation shifts,and for designing a robust paper-author matching model, consider-ing potential incorrect assignments in existing authors papers.",
  "For relation enrichment, this subsection presents the results ofscholar interest extraction": "Baselines. For scholar interest extraction, we employ competition-winning solutions and methods relying on pre-trained models.These approaches follow a common principle: they gauge the sim-ilarity between authors in the test and training sets, using theweighted interest tags of training authors for the authors in thetest set. The variations among baselines lie in how they computeauthor similarity. (1) LSI employs bag of words and TF-IDF forpaper texts, reducing dimensions with the LSI model. (2) ACA14:utilizes more paper attributes, including titles, citations, and venues,for a more nuanced author similarity calculation. (3) pre-trainingmodels: leverage models like Sentence-BERT (S-BERT) , Sim-CSE , E5 , BERT , GTE , BGE15, and Sentence-T5(S-T5) to encode paper texts for similarity measurement.",
  "| |": "where is the number of scholars, is the annotated interest setof the -th scholar, and is the predicted interest set of the -thscholar. We pick the 3 closest tags to the author for evaluation. Forpaper topic classification, we measure multi-classification accuracy. Experimental Results. presents the results of scholarinterest extraction. Initial attempts to classify scholars paper textsusing research interest tags as labels yielded unsatisfactory results,likely due to the large number of tags and limited training data. Themethods compared in rely on author similarity to calculateinterest tags, which proves more effective than text classification.Notably, encoding with pre-trained models directly is less effectivethan LSI, highlighting the effectiveness of shallow semantic models.Additionally, models focused on sentence embedding outperformgeneral pre-trained models like BERT. The ACA method, whichleverages various author attributes such as venues and citing papers,yields improved prediction results. However, the overall accuracyremains low, indicating a challenge in accurate classification witha large number of interest tags. BERT-base S-T5-xxl SimCSE GTE-large S-BERT E5-large BGE-large LSI ACA",
  "Evaluation Metrics. Like the standard recommendation task , we adopt Recall@20 and NDCG@20 as evaluation metrics forpaper/reviewer recommendation": "Experimental Results. shows the paper recommendationperformance. GF-CF outperforms other methods, highlighting theeffectiveness of graph filters. GNN-based methods exceed Mult-VAE,demonstrating the value of high-order graph structures. LightGCNperforms better than NGCF, which confirms the redundancy ofsome GNN modules in NGCF. However, there is room for improve-ment in recommendation accuracy. How to use the attributes ofpapers and users to capture the dynamic interest changes of usersis a difficult point.",
  ": Results of academic question answering": "reports the reviewer recommendation performance. GF-CF and LightGCN outperform TF-IDF, underscoring the importanceof leveraging graph structures. The performance of TPMS is un-satisfactory because TPMS is also mainly based on TF-IDF textsimilarity. However, most methods fall short of fully utilizing themultidimensional attributes of papers and reviewers research in-terests. This highlights the need for further research in the reviewerrecommendation task.",
  "For academic knowledge acquisition, this subsection introduces theresults of academic question answering": "Baselines. We adopt sparse and dense retrieval methods: (1) Sparseretrieval methods: BM25, (2) Dense retrieval methods: DPR-FT(full fine-tuning of Dense Passage Retriever (DPR) ), DPR-PT2(parameter-efficient fine-tuning of DPR with P-Tuning v2 ),ColBERT-FT (full fine-tuning of ColBERT ), ColBERT-PT2(parameter-efficient fine-tuning of ColBERT with P-Tuning v2), andLLM-Embedder (a fine-tuned LLM based on various retrieval-related tasks).",
  "Evaluation Metrics. Hit@K is used to measure retrieval accuracy,reporting if the top retrieved papers contain the correct answer.The average Hit@K across all questions is reported": "Experimental Results. presents the results of the OAG-QA dataset. Generally speaking, dense retrieval methods outper-form sparse retrieval methods. ColBERT-based methods are signifi-cantly better than DPR-based ones. This shows that by employinglate interaction patterns and multi-vector representations, ColBERTmodels the correlation between questions and papers better. Inter-estingly, efficient parameter fine-tuning methods excel over fullfine-tuning, possibly due to better knowledge retention and gener-alization from the pre-trained model. The effect of LLM-Embeddersuggests there still exists noticeable gap between LLM and academicretrieval. Overall, these methods retrieval effects are suboptimal,suggesting room for improvement.",
  ": Results of paper source tracing": "Baselines. We compare three types of methods. (1) Statistical meth-ods: Rule (employing regular expressions to extract references ap-pearing near signal words like motivated by or inspired by), andRandom Forest (RF) (following , extracting statistical featuresabout citations, citing positions, text similarity, etc., and using RFto predict the importance of references). (2) Graph-based methods:LINE and NetSMF train paper embeddings in citation net-works and then calculates the cosine similarity between the paperembedding and the reference embedding to measure the importanceof references. (3) Pre-training methods: extract the contextual textwhere each reference appears in the full texts, encode the text withthe pre-training models, and use the reference annotation results inthe training set for fine-tuning. The pre-training models consideredinclude BERT , SciBERT , Galactica-standard , andGLM . We also adopt three SOTA closed-source models: GPT-3.5 , GPT-4 , and Claude-instant . For both open-sourceand closed-source LLMs, we input the context of a referenced pa-per and query the model to assess the references significance. Forinstance, we ask, Given the context ..., is the current referenceimportant? Closed-source LLMs perform this task using zero-shotevaluation. Evaluation Metrics. A paper may have one or more ref-sources.For each reference of the paper , an importance score between needs to be output. For each paper to be traced, its referencelist is encoded as 0-1 based on the labeling results (1 if its ref-source,0 otherwise). By comparing the prediction result of each referencewith its labeling result, we compute the Mean Average Precision(MAP). The average MAP across different papers serves as theevaluation metric. Experimental Results. presents the results of papersource tracing. Among all methods, SciBERT delivers the best per-formance, indicating the efficacy of pre-trained language models.RF outperforms the Rule method, underscoring the effectivenessof feature engineering. Graph-based methods achieve average per-formance, possibly owing to the ignorance of the contextual in-formation of references. The Rule-based approachs performanceis subpar, likely due to many important references lacking sur-rounding signal words like inspired by, resulting in a low recall.Surprisingly, finetuned SciBERT and BERT-base outperform largermodels like GLM-2B, Galactica-standard, and closed-source LLMs.The reason may lie in two aspects. First, the training objective of",
  "For academic influence prediction, this subsection presents theresults of paper influence prediction": "Baselines. We select the following methods: (1) Citation: is basedon the paper citation number of known years; (2) Random Forest(RF) : defines features as the paper citation number per yearand the total number of citations; (3) GBDT : uses the samefeatures as RF; (4) PageRank : calculates papers PageRankscore based on paper citation networks; (5) GraphSAGE : per-forms semi-supervised classification on the paper citation network.Additionally, we consider graph-based node importance predictionmethods: (6) GENI and (7) RGTN . Evaluation Metrics. We predict for each venue to determinewhether a paper would be awarded, with labels being 0 or 1 indicat-ing whether the paper is awarded or not. Mean Average Precision(MAP) is calculated by comparing the predicted probability of win-ning the award with the ground truth label, and the mean MAPacross different venues is used as the evaluation metric. Experimental Results. presents results for paper influ-ence prediction. shows PageRank performing best, as itconsiders the influence of citing papers, unlike the citation methodthat treats each citing paper equally. Traditional classifiers (RFand GBDT) are inferior to methods using only total citations, in-dicating that total citations are a very important indicator. Thefeatures added by the classifier may dilute the effect of total cita-tions. GraphSAGEs poor performance may be due to its inabilityto capture paper-influence factors like citation count. GENI out-performed GraphSAGE, but both methods were less effective thanCitation and PageRank methods. This could be due to their implicitincorporation of citation statistics and the severe class imbalanceproblem (positive vs. negative < 1 : 100). Thus, identifying factorsbeyond citation count remains a challenge in predicting papersbreakthrough innovation.",
  "To promote the engagement of the research community and thedevelopment of OAG-Bench, we also introduce the Open Academic": "Data Challenge (OAG-Challenge) and set up a regular leaderboardfor up-to-date OAG-Bench16. OAG-Challenge currently containsthree challenging academic tasks: incorrect assignment detectionfor author name disambiguation (IND), academic question answer-ing (OAG-AQA), and paper source tracing (PST).Specifically, given the paper assignments of each author andpaper metadata, IND aims to detect paper assignment errors foreach author. Given professional questions and a pool of candidatepapers, OAG-AQA hopes to retrieve the most relevant papers toanswer these questions. As mentioned earlier, given the full textsof each paper, PST aims to automatically trace the most significantreferences that have inspired a given paper.OAG-Challenge was deployed at KDD Cup 2024 and attractedmore than 800 team registrations globally. Following the previ-ous successful conventions, submissions are required to providesource codes, technical reports, and contact information for bet-ter knowledge sharing and iteration. We are periodically updatingthe datasets, including annotating new assignment errors, crawl-ing new academic question and answer pairs, and collecting newreading records for PST.",
  "Conclusion": "The attention of the research community to academic benchmarksremains limited, even if academic tasks offer various challenges andapplications of immense impact. Thus, this paper introduces OAG-Bench to carefully annotate large-scale OAG for the full life cycleof academic graph mining. OAG-Bench now includes 10 tasks, 20datasets, 70+ baseline models, and 120+ experimental results. In thefuture, we plan to continually maintain and enhance OAG-Bench byupdating up-to-date datasets regularly from real scenarios, addingmore practical tasks, and exploring interactive evaluation metrics.OAG-Bench is always open for contributions from communities byadding new tasks or datasets, developing cutting-edge algorithmsor foundation models for various tasks, etc. This work is supported by Natural Science Foundation of China(NSFC) 62425601 and 62276148, Technology and Innovation Ma-jor Project of the Ministry of Science and Technology of Chinaunder Grant 2020AAA0108400, the New Cornerstone Science Foun-dation through the XPLORER PRIZE. We also thank Weibin Liao,Chao Yu, Kai Yu, and Zheng Jiang for their contribution to codereproducibility.",
  "Anthropic. 2023.Introducing Claude": "Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: A Pretrained LanguageModel for Scientific Text. In Proceedings of the 2019 Conference on EmpiricalMethods in Natural Language Processing and the 9th International Joint Conferenceon Natural Language Processing. 36153620. Leo Breiman. 2001. Random forests. Machine learning 45, 1 (2001), 532. Laurent Charlin and Richard Zemel. 2013. The Toronto paper matching system:an automated paper-reviewer assignment system. (2013). Bo Chen, Jing Zhang, Fanjin Zhang, Tianyi Han, Yuqing Cheng, Xiaoyan Li,Yuxiao Dong, and Jie Tang. 2023. Web-Scale Academic Name Disambiguation:the WhoIsWho Benchmark, Leaderboard, and Toolkit. In Proceedings of the 29thACM SIGKDD International Conference on Knowledge Discovery & Data Mining.38173828. Bo Chen, Jing Zhang, Xiaokang Zhang, Yuxiao Dong, Jian Song, Peng Zhang,Kaibo Xu, Evgeny Kharlamov, and Jie Tang. 2023. GCCAD: Graph ContrastiveLearning for Anomaly Detection. IEEE Transactions on Knowledge & Data Engi-neering 01 (2023), 114.",
  "Tianqi Chen and Carlos Guestrin. 2016. Xgboost: A scalable tree boosting system.In Proceedings of the 22nd acm sigkdd international conference on knowledgediscovery and data mining. 785794": "Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel S Weld.2020.SPECTER: Document-level Representation Learning using Citation-informed Transformers. In Proceedings of the 58th Annual Meeting of the As-sociation for Computational Linguistics. 22702282. Scott Deerwester, Susan T Dumais, George W Furnas, Thomas K Landauer, andRichard Harshman. 1990. Indexing by latent semantic analysis. Journal of theAmerican Society for Information Science 41, 6 (1990), 391407. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding. InProceedings of the 2019 Conference of the North American Chapter of the Associationfor Computational Linguistics: Human Language Technologies. 41714186.",
  "Anna Veronika Dorogush, Vasily Ershov, and Andrey Gulin. 2018. CatBoost:gradient boosting with categorical features support. (2018). arXiv:1810.11363": "Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, andJie Tang. 2022. GLM: General Language Model Pretraining with AutoregressiveBlank Infilling. In Proceedings of the 60th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers). 320335. Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang.2022. Language-agnostic BERT Sentence Embedding. In Proceedings of the 60thAnnual Meeting of the Association for Computational Linguistics (Volume 1: LongPapers). 878891.",
  "Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning fornetworks. In Proceedings of the 22nd ACM SIGKDD International Conference onKnowledge Discovery and Data Mining. 855864": "Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, XiaodongLiu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. 2021. Domain-specificlanguage model pretraining for biomedical natural language processing. ACMTransactions on Computing for Healthcare 3, 1 (2021), 123. Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representationlearning on large graphs. In Proceedings of the 31st International Conference onNeural Information Processing Systems. 10251035. Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2022. DeBERTaV3: ImprovingDeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Em-bedding Sharing. In Proceedings of the 11th International Conference on LearningRepresentations. Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and MengWang. 2020. Lightgcn: Simplifying and powering graph convolution network forrecommendation. In Proceedings of the 43rd International ACM SIGIR conferenceon research and development in Information Retrieval. 639648.",
  "Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirectional LSTM-CRF models forsequence tagging. (2015). arXiv:1508.01991": "Minhao Jiang, Xiangchen Song, Jieyu Zhang, and Jiawei Han. 2022. TaxoEnrich:Self-Supervised Taxonomy Completion via Structure-Semantic Representations.In Proceedings of the ACM Web Conference 2022. 925934. Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu.2019. PubMedQA: A Dataset for Biomedical Research Question Answering. InProceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-cessing and the 9th International Joint Conference on Natural Language Processing(EMNLP-IJCNLP). 25672577. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, SergeyEdunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-Domain Question Answering. In Proceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing. 67696781.",
  "Dawen Liang, Rahul G Krishnan, Matthew D Hoffman, and Tony Jebara. 2018.Variational autoencoders for collaborative filtering. In Proceedings of the 2018world wide web conference. 689698": "Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and JieTang. 2022. P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning AcrossScales and Tasks. In Proceedings of the 60th Annual Meeting of the Association forComputational Linguistics (Volume 2: Short Papers). 6168. Xiao Liu, Da Yin, Jingnan Zheng, Xingjian Zhang, Peng Zhang, Hongxia Yang,Yuxiao Dong, and Jie Tang. 2022. OAG-BERT: Towards a Unified BackboneLanguage Model for Academic Knowledge Services. In Proceedings of the 28thACM SIGKDD Conference on Knowledge Discovery and Data Mining. 34183428. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, OmerLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2020. RoBERTa:A Robustly Optimized BERT Pretraining Approach. In Proceedings of the 9thInternational Conference on Learning Representations. Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel S Weld.2020. S2ORC: The Semantic Scholar Open Research Corpus. In Proceedings of the58th Annual Meeting of the Association for Computational Linguistics. 49694983. Yaojie Lu, Qing Liu, Dai Dai, Xinyan Xiao, Hongyu Lin, Xianpei Han, Le Sun, andHua Wu. 2022. Unified Structure Generation for Universal Information Extraction.In Proceedings of the 60th Annual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers). 57555772. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.Distributed representations of words and phrases and their compositionality. InProceedings of the 26th International Conference on Neural Information ProcessingSystems-Volume 2. 31113119. Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith Hall, DanielCer, and Yinfei Yang. 2022. Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models. In Findings of the Association for ComputationalLinguistics: ACL 2022. 18641874.",
  "OpenAI. 2022. Introducing ChatGPT. Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. ThePageRank citation ranking: Bringing order to the web. Technical Report. StanfordInfoLab": "Aldo Pareja, Giacomo Domeniconi, Jie Chen, Tengfei Ma, Toyotaro Suzumura,Hiroki Kanezashi, Tim Kaler, Tao Schardl, and Charles Leiserson. 2020. Evolvegcn:Evolving graph convolutional networks for dynamic graphs. In Proceedings ofthe 34th AAAI Conference on Artificial Intelligence. 53635370. Namyong Park, Andrey Kan, Xin Luna Dong, Tong Zhao, and Christos Faloutsos.2019. Estimating node importance in knowledge graphs using graph neuralnetworks. In Proceedings of the 25th ACM SIGKDD international conference onknowledge discovery & data mining. 596606.",
  "empirical methods in natural language processing. 15321543": "Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Chi Wang, Kuansan Wang, and JieTang. 2019. Netsmf: Large-scale network embedding as sparse matrix factoriza-tion. In The World Wide Web Conference. 15091520. Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddingsusing Siamese BERT-Networks. In Proceedings of the 2019 Conference on EmpiricalMethods in Natural Language Processing and the 9th International Joint Conferenceon Natural Language Processing. 39823992.",
  "Emanuele Rossi, Fabrizio Frasca, Ben Chamberlain, Davide Eynard, MichaelBronstein, and Federico Monti. 2020. Sign: Scalable inception graph neuralnetworks. (2020). arXiv:2004.11198": "Jiaming Shen, Zhihong Shen, Chenyan Xiong, Chi Wang, Kuansan Wang, andJiawei Han. 2020. TaxoExpan: Self-supervised taxonomy expansion with position-enhanced graph neural network. In Proceedings of The Web Conference 2020.486497. Yifei Shen, Yongji Wu, Yao Zhang, Caihua Shan, Jun Zhang, B Khaled Letaief,and Dongsheng Li. 2021. How Powerful is Graph Convolution for Recommenda-tion?. In Proceedings of the 30th ACM International Conference on Information &Knowledge Management. 16191629. Arnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Darrin Eide, Bo-June Hsu, andKuansan Wang. 2015. An overview of microsoft academic service (mas) andapplications. In Proceedings of the 24th international conference on world wide web.243246. Jianlin Su, Ahmed Murtadha, Shengfeng Pan, Jing Hou, Jun Sun, Wanwei Huang,Bo Wen, and Yunfeng Liu. 2022. Global Pointer: Novel Efficient Span-basedApproach for Named Entity Recognition. (2022). arXiv:2208.03054 Ilya Sutskever, Ruslan Salakhutdinov, and Joshua B Tenenbaum. 2009. Modellingrelational data using Bayesian Clustered Tensor Factorization. In Proceedingsof the 22nd International Conference on Neural Information Processing Systems.18211828. Weng Lam Tam, Xiao Liu, Kaixuan Ji, Lilong Xue, Xingjian Zhang, Yuxiao Dong, Ji-ahua Liu, Maodi Hu, and Jie Tang. 2022. Parameter-efficient prompt tuning makesgeneralized and calibrated neural text retrievers. arXiv preprint arXiv:2207.07087(2022). Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei.2015. Line: Large-scale information network embedding. In Proceedings of the24th international conference on world wide web. 10671077. Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, AnthonyHartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Sto-jnic. 2022.Galactica: A large language model for science.arXiv preprintarXiv:2211.09085 (2022). Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-AnneLachaux, Timothe Lacroix, Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, et al. 2023. Llama: Open and efficient foundation language models. arXivpreprint arXiv:2302.13971 (2023).",
  "Dashun Wang and Albert-Lszl Barabsi. 2021. The science of science. CambridgeUniversity Press": "Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang,Rangan Majumder, and Furu Wei. 2022. Text embeddings by weakly-supervisedcontrastive pre-training. arXiv preprint arXiv:2212.03533 (2022). Ruijie Wang, Yuchen Yan, Jialu Wang, Yuting Jia, Ye Zhang, Weinan Zhang, andXinbing Wang. 2018. Acekg: A large-scale knowledge graph for academic datamining. In Proceedings of the 27th ACM international conference on informationand knowledge management. 14871490. Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019.Neural graph collaborative filtering. In Proceedings of the 42nd international ACMSIGIR conference on Research and development in Information Retrieval. 165174. Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and KilianWeinberger. 2019. Simplifying graph convolutional networks. In Proceedings ofthe 36th International Conference on Machine Learning. 68616871. Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. 2017.End-to-end neural ad-hoc ranking with kernel pooling. In Proceedings of the 40thInternational ACM SIGIR conference on research and development in informationretrieval. 5564. Hang Yan, Yu Sun, Xiaonan Li, and Xipeng Qiu. 2023. An Embarrassingly Easybut Strong Baseline for Nested Named Entity Recognition. In Proceedings of the61st Annual Meeting of the Association for Computational Linguistics (Volume 2:Short Papers). 14421452. Dan Zhang, Yangliao Geng, Wenwen Gong, Zhongang Qi, Zhiyu Chen, XingTang, Ying Shan, Yuxiao Dong, and Jie Tang. 2024. RecDCL: Dual ContrastiveLearning for Recommendation. In Proceedings of the ACM on Web Conference2024. 36553666. Dan Zhang, Yifan Zhu, Yuxiao Dong, Yuandong Wang, Wenzheng Feng, EvgenyKharlamov, and Jie Tang. 2023. ApeGNN: Node-Wise Adaptive Aggregationin GNNs for Recommendation. In Proceedings of the ACM Web Conference 2023.759769.",
  "SVM90.1493.67LinKG67.7266.78LinKG74.8077.58LinKG89.6297.32": "Fanjin Zhang, Xiao Liu, Jie Tang, Yuxiao Dong, Peiran Yao, Jie Zhang, Xiaotao Gu,Yan Wang, Evgeny Kharlamov, Bin Shao, et al. 2023. OAG: Linking Entities acrossLarge-scale Heterogeneous Knowledge Graphs. IEEE Transactions on Knowledgeand Data Engineering 35, 9 (2023), 92259239. Fanjin Zhang, Xiao Liu, Jie Tang, Yuxiao Dong, Peiran Yao, Jie Zhang, XiaotaoGu, Yan Wang, Bin Shao, Rui Li, et al. 2019. OAG: Toward linking large-scaleheterogeneous entity graphs. In Proceedings of the 25th ACM SIGKDD InternationalConference on Knowledge Discovery & Data Mining. 25852595.",
  "We provide the experimental results of venue alignment, affiliationalignment, and author alignment in this subsection": "Baselines. Venue alignment and affliation alignment are short textmatching tasks. We compare different types of matching methods:(1) traditional machine learning methods (SVM) using Jaccard indexand TF-IDF similarity as input features; (2) shallow neural network-based matching methods including CNN-based matching model(LinKG) and RNN-based matching model (LinKG) ;(3) matching models based on pre-trained models (Ditto withpre-trained models BERT , ALBERT , RoBERTa , De-BERTa , LaBSE , and GLM ).Author alignment can further take authors structural infor-mation into account. Apart from SVM, LinKG, and LinKG, weadditionally select LinKG model for author alignment, whichconstructs a subgraph for each candidate pair, and then uses hetero-geneous graph attention networks to learn hidden representationsfor classification. For each candidate author pair, we leverage pub-lished papers and venues of authors as features.",
  "We present the detailed task description and experiments for authorname disambiguation in this subsection. Two subtasks of authorname disambiguation are defined as follows": "Problem A.1. From-scratch Name Disambiguation (SND).Given a collection of papers associated with identically-named authors,the goal is to cluster these papers into distinct groups, where eachgroup should represent papers by the same author, while differentgroups signify papers by different authors. Problem A.2. Real-time Name Disambiguation (RND). Givena collection of unassigned papers and a set of authors (each authorincludes attributes such as affiliations, research interests, publishedpapers, etc.), the goal is to assign these papers to the correct author orreturn empty (meaning that no author can be matched). Datasets. The WhoIsWho dataset includes 72,609 authors with2459 names, and 1,102,249 associated papers. The authorship be-tween papers and authors is manually annotated. For the two sub-tasks, the training set contains the mapping relationship amongthe author name author ID paper ID, and the metadata of thepaper (such as title, author name, published venue, etc.). For theSND task, the validation set and test set contain the name to bedisambiguated and the papers associated with the name, and thegoal is to cluster the papers into different groups. For the RND task,the validation and test sets involve unassigned papers, and the goalis to assign papers to existing authors or return NIL.",
  "kingsundad93.492AlexNE93.136Data Magician92.850": "include ECNU_AIDA, Complex808, and liub. The three methodsfollow a similar framework. They first encode the semantic featuresof papers via pre-trained models. Then, they construct a heteroge-neous network according to the heterogeneous attributes of papers.Next, they use random walks based on meta-paths to generate thestructural representation of papers. The semantic representationand structural representation can separately generate two papersimilarity matrices. Finally, they use the DBSCAN clustering al-gorithm to cluster the papers to obtain the clustering result. Thedifference between the three methods lies in: (1) ECNU_AIDA andComplex808 use pre-trained Word2Vec model to obtain thesemantic representation of papers, while liub uses OAG-BERT to obtain the semantic representation of papers; (2) ECNU_AIDAand liub use co-author and co-organization relations between twopapers for random walks in heterogeneous networks, while Com-plex808 additionally introduces co-venue and co-keyword rela-tions between two papers with a certain probability for randomwalks.Compared RND methods include: (1) kingsundad: employsthree similarity features: handcrafted, OAG-BERT, and RBF-kernelinteraction matching . These features are input into multipleclassifiers like XGBoost , LightGBM , and CatBoost forensemble learning. (2) AlexNE: introduces various name encodingtechniques, like abbreviated encoding, to enhance recall. Unlikethe Kingsundad method, it uses both OAG-BERT and GloVe to generate semantic paper representations. It constructs a largegraph connecting unassigned papers to existing ones via keywordsand generates node representations using Node2Vec . (3) DataMagician: is a feature engineering-based method that uses featuresfrom keywords, affiliations, co-authors, years, and more. Notably,it defines a time-weighted paper similarity method to account forchanging research interests.",
  "This subsection presents the task description and experiments forsearch engine-based scholar profiling": "Problem A.3. Search Engine-based Scholar Profiling. Givena scholars name, affiliation, and ones search engine records (usingname + affiliation to query and extracting up to 2 search pages andup to 20 snippets), the goal is to extract the portrait information ofthe scholar, including homepage, gender, and position. Datasets. CCKS2021-En18: This dataset is an English subset ofthe CCKS 2021 scholar profiling track from AMiner. It contains9,221 scholar portraits, randomly divided into 5,557 for training,1,833 for validation, and 1,831 for testing. Baselines. Drawing from the winning solutions of the CCKS 2021and recent named entity recognition (NER) methods, we select thefollowing baselines for search engine-based scholar profiling: (1)SML: employ manual features and traditional classifiers. Specifi-cally, for gender prediction, SML-esb extracts features such as thefrequency of his and her and uses various classifiers for voting.For homepage extraction, Logistic Regression (LR) and XGBoostextract features like the appearance of signal words (such as eduand academic) for classification. (2) Rule: utilizes regular expres-sions and voting for position extraction. (3) BI-LSTM-CRF :uses a BI-LSTM layer and a CRF layer for sequence labeling inposition extraction. (4) Pre-training methods: We design differentinputs for pre-training models for each attribute. For gender pre-diction and position extraction, the scholars name, affiliation, andwebpage texts are concatenated as inputs. For homepage extrac-tion, the scholars name and the candidate URL are concatenated asinputs. We fine-tune pre-trained models for classification, includ-ing BERT , RoBERTa , DeBERTa , ALBERT ,ChatGLM19, LLaMA .",
  "SGC34.0831.44SIGN26.2524.99GraphSAGE59.5757.12": "Experimental Results. displays extraction results forsearch engine-based scholar profiling. In , pre-trained mod-els outperform traditional methods, showcasing the expressivecapacity and effectiveness of pre-trained models without manualfeature design. BI-LSTM-CRF and partial pre-trained models exhibitsimilar performance in position extraction, showing the suitabilityof both sequence labeling and pre-trained models. Large generativemodels excel in homepage extraction, though accuracy remainsmodest.",
  "Evaluation Metrics. We measure multi-classification accuracyfor paper classification": "Experimental Results. shows the results of paper topicclassification. GraphSAGE outperforms SGC and SIGN in papertopic classification due to more training parameters, expressiveability, and neighbor sampling strategy. However, its longer train-ing time poses a challenge in balancing efficiency and effectivenesson large-scale graph data. SGC performs better than SIGN, possiblybecause SIGNs graph convolution filter is less suited for paper clas-sification tasks, while SGCs simpler convolution scheme effectivelycaptures paper topic information. Current methods mainly leveragepaper citation structure for paper topic classification, yielding unsat-isfactory results. More content information could be incorporatedto enhance the fine-grained paper tagging performance.",
  "In this subsection, we provide the experimental setup and resultsfor concept taxonomy completion": "Baselines. Some of the latest concept taxonomy completion meth-ods are selected for comparison. (1) BiLinear : uses a bilin-ear model to encode new and candidate concept representations,performing binary classification to ascertain if a candidate posi-tion owns the correct hypernym and hyponym of a new concept.(2) TaxoExpan : employs a position-augmented graph neu-ral network to gauge the relationship between new concepts andcandidate concept subgraphs, using contrastive learning to bolstermodel robustness. We use SciBERT to encode concepts for a faircomparison with the next method. (3) TaxoEnrich : initiallytransforms the existing hyponymy relationship into natural lan-guage, using SciBERT to represent concepts. It then employs LSTMto encode vertical concept relationships and an attention mecha-nism for sibling relationships. Finally, a matching model calculatesthe score between a new concept and a candidate position. Evaluation Metrics. Each new concept is matched with nodes inthe existing concept hierarchy tree and sorted by similarity. Eval-uation metrics include Hit@10 and Mean Reciprocal Rank (MRR),which is the average rank of the reciprocal of actual hypernyms. Experimental Results. reports the performance of con-cept taxonomy completion. TaxoExpan outperforms other meth-ods on three datasets, indicating the effectiveness of the position-augmented graph neural network. TaxoEnrich surpasses TaxoEx-pan in its paper, likely due to its more potent pre-trained represen-tation. The BiLinear models simplicity limits its expressive power,affecting prediction accuracy. The results of OAG-AI are obtained bymaking inferences using the pre-trained model on MAG-CS, main-taining similar trends as other datasets. Hit@10 doesnt exceed 0.35on all datasets, indicating the challenge of automatic taxonomy con-struction and the potential need for more information or powerfulmodel architectures.",
  "ARIMA122523920Linear Regression56222057GBRT55321777LSTM103425409EvolveGCN96922841": "total number of papers, the H-index20, and the estimated citationnumber of the author by using author citations and paper-authorrelations. Then, we use the Linear Regression model to predict thenumber of citations of the author. (3) GBRT : uses the samefeatures as linear regression. (4) LSTM : uses the features (#cita-tions and #papers) of the author in the past 20 years as a time seriesand uses LSTM for regression prediction. (5) EvolveGCN :models author influence prediction as a node regression problemon dynamic co-author graphs.",
  "Evaluation Metrics. The root mean square error (RMSE) betweenthe predicted cited number and the actual cited number is used asthe evaluation metric": "Experimental Results. present results for author in-fluence prediction. It reveals that the GBRT method has smallerprediction errors on both datasets, demonstrating its superior fittingability over linear regression and the effectiveness of input featureslike author citations. ARIMAs poor performance suggests thattime-series-based statistical methods struggle to predict academicinfluence. In addition to being affected by past achievements, thefuture influence of scholars will also have dynamic and more com-plex factors. EvolveGCN outperforms LSTM, indicating co-authornetwork dynamics contain factors related to author influence. Thelarger prediction error on the AuthPred-2022 dataset could be dueto differences in citation statistics between AMiner and GoogleScholar, or the increased difficulty in predicting author influence in2022 due to the surge in paper numbers.",
  "BEthical Statement": "OAG-Bench involves author-centric attributes. We exclude thosesensitive attributes such as email and profile photo, making avail-able attributes publicly accessible elsewhere. For online publica-tions, OAG-Bench provides publicly available metadata and veryfew parsed full-texts of open-access papers for research purposes.For data annotation, all annotators gave their informed consent forinclusion before they participated in this study."
}