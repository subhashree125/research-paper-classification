{
  "ABSTRACT": "Recent learning-to-imitation methods have shown promising re-sults in planning via imitating within the observation-actionspace. However, their ability in open environments remains con-strained, particularly in long-horizon tasks. In contrast, traditionalsymbolic planning excels in long-horizon tasks through logicalreasoning over human-defined symbolic spaces but strugglesto handle observations beyond symbolic states, such as high-dimensional visual inputs encountered in real-world scenarios.In this work, we draw inspiration from abductive learning andintroduce a novel framework ABductive Imitation Learning (ABIL)that integrates the benefits of data-driven learning and symbolic-based reasoning, enabling long-horizon planning. Specifically, weemploy abductive reasoning to understand the demonstrations insymbolic space and design the principles of sequential consistencyto resolve the conflicts between perception and reasoning. ABILgenerates predicate candidates to facilitate the perception fromraw observations to symbolic space without laborious predicateannotations, providing a groundwork for symbolic planning. Withthe symbolic understanding, we further develop a policy ensemblewhose base policies are built with different logical objectives andmanaged through symbolic reasoning. Experiments show that ourproposal successfully understands the observations with the task-relevant symbolics to assist the imitation learning. Importantly,ABIL demonstrates significantly improved data efficiency andgeneralization across various long-horizon tasks, highlighting it asa promising solution for long-horizon planning. Project website:",
  "Both authors contributed equally to this research.Corresponding Author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 25, August 0307, 2025, Toronto, ON 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-1-4503-XXXX-X/24/06",
  "INTRODUCTION": "A long-standing goal in AI is to build agents that are flexible andgeneral, able to accomplish a diverse set of tasks in open andnovel environments, such as home robots for cooking meals orassembling furniture. These tasks generally require the agents toexecute sequential decision-making, which is often formulated as aplanning problem. Recently, the learning-based method, ImitationLearning, has achieved remarkable success via imitating expertdemonstrations, in a variety of domains, such as robotic manipula-tion , autonomous driving and language models .However, the theoretical studies reveal that imitationlearning can suffer from serious performance degradation due tothe covariate shift between limited expert demonstrations and thestate distribution actually encountered by the agents, especially inlong-horizon tasks. In traditional AI literature, symbolic plannerseffectively generalize in long-horizon decision-making, via logicalreasoning on the human-defined symbolic spaces .However, they often simplify the perception process by relyingon ground-truth symbols. Given observations and actions, purelogic-based methods struggle to map raw observations to human-defined symbolic spaces without predicate-level supervision.To address these issues, efforts are underway to merge theadvantages of learning-based and reasoning-based approaches intoneuro-symbolic planning. Xu et al. propose the regressionplanning network, learning to predict symbolic sub-goals that needto be achieved before the final goals, thereby generating a long-term symbolic plan conditioned on high-dimensional observations.",
  "Sub-Task": ": Our framework, ABductive Imitation Learning (ABIL), achieves neuro-symbolic grounding of imitation in complexscenes while showing state-of-the-art results in data efficiency, generalization, and zero-shot transfer. Konidaris et al. collect feasibility annotations and transitiondata under different symbolic operations to learn the symbolicrepresentation of different observations. The learned representationenables traditional planning in the symbolic space and allowsthe acquisition of desired low-level controllers during inference.Silver et al. formalizes operator learning for neuro-symbolicplanning, viewing operators as an abstraction model of the rawtransition and generating the high-level plan skeletons. However,most of these positive results rely on the assumption that thereare sufficient symbolic annotations to train the neural networksfor mapping high-dimensional observations to symbolic statesfor logic-based planning, or there are prior low-level controllersto achieve the expected sub-goals perfectly. Compared to real-world applications, these approaches overlook the process oflearning from demonstrations to imitate specific behaviors. Themost relevant work to ours is PDSketch , which employs neuralnetworks as the basic modules of human-specified programmingstructures and learns a transition model. This model supportsgeneric network-based representations for predicates and actioneffects. Nevertheless, its model-based planning framework tends toaccumulate errors, making it less suitable for long-horizon decision-making tasks.In this work, we borrow the idea of abductive learning andintroduce a novel framework ABductive Imitation Learning (ABIL),which combines the benefits of data-driven learning and symbolic-based reasoning, enabling long-term planning based on stateobservations. Specifically, ABIL employ abductive reasoning tohelp understand the demonstrations in symbolic space and applythe principles of sequential consistency to resolve the conflictsbetween perception and reasoning. It applies logical reasoning togenerate predicate candidates that meet constraints, eliminating theneed for laborious symbolic annotations. With the above symbolicunderstanding, we further build a policy ensemble whose basepolicies are built with different logical objectives and managed bysymbolic reasoning. The learned policy imitates specific behaviorsdirectly from demonstrations, eliminating the reliance on priorlow-level controllers used in earlier neuro-symbolic methods.Additionally, it makes decisions based on human-like cognition, which enhances its generalization capabilities. Experiments showthat our proposal successfully understands the observations withthe task-relevant symbolics to assist the imitating. Notably, ABILshows significantly improved performance in data efficiency andgeneralization settings across a variety of long-horizon tasks.",
  "RELATED WORK": "The preface work of this paper mainly includes Imitation Learning,Neuro-Symbolic Planning and Abductive Learning.Imitation Learning learns the policies from expert demon-strations to achieve sequential decision-making . There aremany works that obtain successful results on varying domains,such as robotic manipulation , autonomous driving and language models . However, learning theory reveals thatthe generalization ability of imitation learning is constrained bythe size of the expert dataset and degrades as the decision-makinghorizon increases . This issue is particularly pronounced inopen environments, where home agents need to accomplish tasksin differently arranged rooms. In such settings, the distributionshift, that is, the covariate shift between training observations andthe scenarios the agent actually encounters, presents a greaterchallenge for imitation learning .Neuro-Symbolic Planning explores to combine traditionalsymbolic planning with learning to enhance models generalizationcapabilities. To handle observations beyond symbolic states, previ-ous studies typically involve training neural networks with task-relevant predicate annotations to transform raw observations intosymbolic states for planning. For example, Xu et al. propose theRegression Planning Networks, which learns to predict sub-goalsthat need to be achieved before the final goals, enabling traditionalsymbolic regression to handle complex high-dimensional inputs,like images. Konidaris et al. collect feasibility annotations andtransition data under different symbolic operations to learn thesymbolic representation of different observations. Silver et al. formalize operator learning for neuro-symbolic planning, viewingoperators as an abstraction model of the raw transition andgenerating the high-level plan skeletons. Wang et al. leveragea pre-trained vision-language model to provide the predicate-level",
  ": Illustration of the Knowledge Base": "annotations to help imitation learning. The most related work toours is PDSketch , which utilizes neural networks as the basicmodules of human-specified programming structures and learnsan object-factored transition model that supports generic neural-network-based representations for predicates and action effects.However, we find that its planning, based on the raw-observation-action space, tends to accumulate errors and is not suitable forlong-sequence decision-making tasks. Furthermore, its applicationof logical reasoning is rather limited and usually requires largeamounts of training data to achieve neuro-symbolic grounding. Incontrast, we develop sequential consistency for abductive reasoningwhich results in a more data-efficient grounding, which assistsimitation learning in turn.Abductive Learning provides a framework that integratesmachine learning with logical reasoning . It focuseson handling the intermediate neuro-symbolic grounding, whichserves as pseudo-labels for learning and as variables for abduction.Although there have been some efforts to extend abductive learningto different applications, such as judicial sentencing andhistorical document understanding , they mainly considerthe traditional classification tasks. In this work, we focus on theplanning problems, where long-horizon sequential decision-makingtasks are mainly considered.Generally speaking, it is still challenging to implement imitationin the real world using the above technologies. Considering thathumans can relatively easily provide a knowledge base with high-level symbolic solutions for long-horizon decision-making taskslike robotic manipulation or household tasks , thiswork follows the research line of neural-symbolic methods to usea knowledge base to assist imitation learning, while also avoidingthe requirements on tedious predicate annotations.",
  "THE PROPOSED FRAMEWORK3.1Problem Formulation": "In this paper, we focus on the goal-based planning task. Follow-ing , the environment is formally defined as a tupleS, A, T, O, P, OP, S0,. Here, S represents the state space, andA represents the action space. The transitions between states andactions are governed by a deterministic transition function T:S A S. S0 denotes the distribution of initial states. Theset O consists of a finite number of task-related objects, where each object possesses a unique name, such as car and rag. Additionally,P is a finite set of task-related predicate symbols, where eachpredicate symbol has an arity that indicates the number ofarguments it takes. For example, Inside/2 has an arity of two,representing that one object is inside another. A ground atom isa predicate that only contains concrete objects, such as Inside(rag,bucket). If a state satisfies |= , it indicates that semanticallyentails the interpretation of . The set consists of ground atomsthat represent the tasks target. The task is to find an actionsequence that generates a trajectory (0,1,1, ...,, ) satisfying , |= . For simplification, we denote |= (where is aset of ground atoms) as , |= . Moreover, OP is also a setof finite predicate symbols that represent logical operators, such asclean/3 and put/2. We denote obj() to retrieve the object(s) from aground atom. For instance, obj(Put(rag, bucket)) = {rag, bucket}.The symbolic knowledge base provided by experts could beformulated as a finite-state machine with a directed graph =, . Each node in the vertex set contains a set of groundatoms of P, which can be viewed as the condition of a sub-task. Eachedge is noted as a tuple, EFF+, EFF. is a ground atom of OPrepresenting the symbolic action, e.g., Put(rag, bucket). EFF+ is theadd effect and EFF is the delete effect, each is a set of groundingatoms. A symbolic action typically requires multiple actions A to achieve the desired logical sub-goal, corresponding to asegment of the complete trajectory. For the sake of simplicity, thenotation op is utilized to denote the edge.For each node , , if there is a directed edge pointingfrom to , then = ( EFF) EFF+. We define a trajectory = (0,1,1, ...,, ) satisfying the knowledge base denotedas |= if and only if for every adjacent states pair (,+1),there exists , |= +1 |= or |= , +1 |= , , , (, ) . This indicates that the expert trajectory satisfiesthe corresponding symbolic knowledge base, such as first usinga rag and soap to clean a dusty car, and then putting them into abucket. a demonstrates an example of the knowledge baseformalized as a state machine with a directed graph.The state machine contains multiple basic structures as illus-trated in b. In the event that a singular node, denoted as1, directs towards the goal, it signifies the necessity to addressa corresponding sub-task 1. For instance, the action Clean(car,rag, soap) is imperative whenever the objective is to clean a car.Conversely, should there be a directed edge from 1 to 2, with 2",
  ": The Framework of our Abductive Imitation Learning": "subsequently pointing towards the goal, it implies that the sub-task1 must be solved before sub-task 2. In scenarios where both 1and 2 have directed edges towards the goal, either sub-task 1 or 2is required to be solved. The final configuration arises when thereis bidirectional pointing between 1 and 2, indicating that bothsub-tasks are mandatory to be solved. Utilizing the state machine,a symbolic planning solution can be derived via algorithms such as search or dynamic programming, represented as a sequence ofstates and transitions: {(0,0), (1,1) . . . (,)}.",
  "Abductive Reasoning": "The ABIL framework can be roughly divided into abductive learningwith the state machine and imitation with symbolic reasoning. Theoverall framework is illustrated and summarized in andAlgorithm 1, respectively.Given the state machine and expert demonstrations, the chal-lenge lies in establishing the perception function from observationto symbolic grounding when symbolic supervision of { } isnot available. To address this challenge, we introduce abductivereasoning to provide pseudo labels derived from the state machinesknowledge, which could be taken to optimize the perceptionfunction .",
  "The agent should complete both t1 and t2, but in anyorder. The symbolic sequence { }=1 will satisfy { }=1 |=, { }=+1 |= , (1,) or { }=1 |= , { }=+1 |=, (1,)": "With the sequential abduction, the pseudo label in Equation 1could be obtained and the perception module could be optimized.Nevertheless, the perception module plays an important role insymbolic grounding, which is crucial for the subsequent reasoning.Following , we train the perception module at theobject level. For each predicate , we have a predicate model with the object-level features as input, which could be obtainedvia an object detection model in practice. A ground atom (1,2)could be inferred by: prob((1,2)|) = ().As summarized in the left part of , our perceptionmodule is optimized via the abductive reasoning. Equation 1generates the pseudo labels based on the sequential consistencybetween the perception output and the solution of satisfyingthe knowledge base. Unlike previous works , it does notrely on the symbolic-level annotations of each observation indemonstrations, which are usually costly and difficult to obtain.",
  "Symbolic-grounded Imitation": "As a human being, one often consciously knows what he or sheis doing, such as making the action of turning left because theyrealize the destination is on the left. In this part, we thus incorporatethe reasoning of high-level operators into the original imitationlearning process, regarding the symbolic operators as an assistancesignals. Specifically, we first build the behavioral actor for eachlogical operator , e.g. and . Then we derive thedesired behavior module by the symbolic states output of perception and the corresponding abstract logical operator. Given thesolution of symbolic planning: {(0,0), (1,1) . . . (,)}.",
  "=1L( ( ,), )(4)": "As summarized in the right part of , our behavioral actors,referring to the human model of cognition before decision-making,embed high-level logical reasoning into the imitation learningprocess. The behavior ensemble learns from experience throughimitation, without relying on a pre-existing perfect controller toreach each sub-goal. Importantly, by leveraging the generalizationcapabilities of symbolic planning, the proposed actors can decom-pose diverse observations into symbolic states, facilitating morereliable decision-making.",
  "EMPIRICAL STUDY": "We evaluate our proposal in three environments, including twoneuro-symbolic benchmarks: BabyAI , Mini-BEHAVIOR ,and a robotic manipulation benchmark, CLIPort . We compareour method with three baselines: Behavior Cloning (BC) ,Decision Transformer (DT) and PDSketch . For a faircomparison, all of these methods use the same network architecturewhich is based on the Neural Logic Machine (NLM) . Specifically,we first encode the state with a two-layer NLM. For BC, we usea single linear layer, taking the state embedding as the input andoutput actions. For DT, we build a single transformer layer followingthe two-layer encoder, with the causal mask to generate futureaction with past states and actions. For PDSketch, we choose thefull mode in the original paper , which provides sufficientprior knowledge of the symbolic transition, keeping consistencywith our symbolic state machine. Following , we report thepercentage of successful planning for the desired goals, which areaveraged over 100 evaluations under three random seeds.",
  "Evaluation on BabyAI": "BabyAI provides a benchmark for grounding logical instructionswhere an agent needs to follow instructions for a series of taskslike picking up objects, or unlocking doors. Following ,we consider 5 tasks: {, ,, ,}, and conductthe generalization evaluation with different numbers of objects in the testing environments. For example, the training environmentscontain 4 objects or 4 doors, and testing environments contain 8objects or 8 doors. It simulates the challenges to the generalizationof imitation learning, where household robots in open environ-ments need to complete tasks in differently arranged rooms. Alldemonstrations of the expert dataset are generated by a script basedon A search.Results and Analysis. From , we could find that BCbaseline is efficient in the simple GotoSingle task but significantlydeteriorates on complex tasks and their generalization evaluation.PDSketch exhibits favorable performance in tasks that require fewactions to complete and generalizes well in the case of increasingobject number. Nevertheless, it struggles to solve long-horizon taskslike Put and Unlock, where experts demonstrations require 9 or10 actions to accomplish. One plausible reason is that, as a model-based method, PDSketch faces the accumulation of search errors,thus fails in long-horizon tasks. DT excels in handling tasks with asequential nature (e.g. Put and Unlock). However, in short-horizontasks, such as Goto and Pickup, DT performs weaker compared to BC.The possible reason could be its relatively high model complexitycontradicts with limited data, making it difficult to learn efficientlyfrom short-horizon demonstrations. In contrast, ABIL achievescompetitive performance compared to PDSketch and has madesignificant improvements in long-horizon tasks. It is worth notingthat ABIL exhibits stable generalization in novel environments,which supports our intention of using symbolic grounding to assistthe generalization of imitation learning.Comparison of Neuro-Symbolic Grounding. Like previ-ous neuro-symbolic methods, the efficiency of neuro-symbolicgrounding is the key to determining whether successful or not.In , we compare the accuracy of the predicates learnedby ABIL and PDSketch, under varying demonstration budgets. Wefound that in the relatively simple task, PDSketch can achieve over90% predicate accuracy with 500 demonstrations. However, in morechallenging tasks, such as open and unlock, its neuro-symbolicgrounding ability is quite poor, which also leads to unsatisfactoryperformance in . In contrast, our ABIL not only achievesreliable neuro-symbolic grounding on all tasks (with nearly perfectpredicate accuracy), but its also more data efficient than PDSketch,requiring less than 20% of their data to achieve superior neuro-symbolic grounding results. It clearly indicates the advantages ofour abductive reasoning on neuro-symbolic understanding.Comparison of Efficiency. Learning-based methods BC andDT could promptly provide responses in the inference phasebecause they do not give adequate attention to the subsequentconsiderations. The model-based planning method PDSketch needsto search for a whole sequence of actions that can achieve thegoal, which can be time-consuming, especially in cases with longersequences and a multitude of available actions. Our approachintegrates higher-level reasoning into the foundation of lower-level perception. By integrating symbolic-based planning, ourapproach enhances planning effectiveness, significantly reducingtime consumption compared to PDSketch, which requires searchingin the original observation-action space. While gaining advantagesof logical reasoning in long-horizon goals, ABIL maintains theinference efficiency of the learning-based method.",
  "Evaluation on Mini-BEHAVIOR": "Mini-BEHAVIOR is a recently proposed benchmark for embodiedAI. It contains varying 3D household tasks chosen from theBEHAVIOR benchmark, including Sorting Books, Making Tea,Cleaning A Car, and so on. Most tasks are long-horizon andheterogeneous, some of which require more than one hundreddecision-making steps to be completed. There are hundreds ofdifferent types and plenty of predicates which is challenging forneuro-symbolic grounding. In this domain, our state machine ismainly composed of several typical categories. For tasks mainlyabout tidying up the room, we split the primitive actions intoand , which is required to perform an action sequence tofinish picking or placing sub-tasks. Combined with our symbolic-grounding model , the agent will be able to distinguish when andwhere to pick and place. For tasks mainly about cleaning, we splitthe primitive actions into and , which is requiredto finish washing or putting sub-tasks. In the generalizationevaluation, we challenge the agents in environments with distractorobjects that are unseen at the training phase.Results and Analysis. The results on Mini-BEHAVIOR areprovided in . In some simple short-horizon tasks, such asInstalling a printer and Opening packages, BC shows satisfactoryperformance. Nevertheless, as the desired decision sequence grows,errors made by BC gradually accumulate, leading to an increasingdeviation from the correct solution. This becomes particularlycritical in the presence of disturbances or interferences, leading to a significant degradation on the generalization evaluation.Compared to BC, DT has achieved better performance in most tasks,however, it still performs poorly in generalization test, pointingout its vulnerability to environmental changes. In this benchmark,PDSketch failed to finish most tasks in the given time budget, dueto the significant search depth required to achieve the goal. Thishighlights the limitations of model-based planning methods in long-horizon scenarios. Ensuring the learned transition remains accurateafter numerous decision steps is challenging, making it difficult toprovide a successful termination signal for the search. In contrast,ABIL performs reasoning at the symbolic level. Even if cleaning a carrequires about 45 decision steps to complete, from the perspectiveof abstract operations, we can understand that we need to put therag and soap back into the buckets after using them. This is similarto humans behavior, where we first recognize what the logical goalto be completed is, and then achieve it step by step through actions,rather than considering the impact of each limb movement, whichwould make the entire reasoning planning path too long. In this way,our neuro-symbolic ABIL successfully incorporates logic-basedreasoning into imitation learning, achieving competitive resultsand showing good generalization under environmental change.Further Analysis with Varying Horizons. As we discussedabove, the generalization of imitation learning methods is closelyrelated to the length of the tasks horizon, especially in long-horizon tasks, where performance degradation is prone to occur.Mini-BEHAVIOR, which contains tasks that require different",
  "ABIL-DT0.977 0.0210.857 0.0250.989 0.0170.917 0.0330.382 0.0290.809 0.008": "decision steps to complete, provides an appropriate observationwindow from this perspective. As shown in , the numberof expert demonstrations required for these tasks ranges from10 to 106 steps. On the one hand, we find that the increase indecision-making length required by the task indeed makes it morechallenging, leading to a decline in the performance of almostall methods. On the other hand, we observe that the increase indecision-making length also amplifies performance differences inthe generalization evaluation of the baseline methods compared to",
  "TaskAve. LengthEvaluation": "Packing-5shapes14/7 unseen/total colorsPacking-20shapes14/7 unseen/total colorsPlacing-red-in-green211 total colorsPutting-blocks-in-bowls27 total colorsAssembling-kits510/5 total shapes/colorsSeparating-20piles77 total colors needs to be achieved via a two-step primitive where each actioninvolves a start and end-effector pose. provides the averagelength of expert demonstrations. This benchmark involves the agentmanipulating objects of various colors and shapes, reflecting therequirements in the open world, providing a greater challenge forimitation learning. Following , we represent each object withits image cropped from the observation with its pose, which could becompleted by an external detection module. All demonstrations arecollected using handcrafted oracle policies following CLIPort ,containing only successful trajectories. Since PDSketch mainlytargets discrete actions, in this environment, we compared ABILwith BC and DT baselines.Results and Analysis. The results are provided in .Although the execution length for robotic manipulation is shortercompared to household tasks in Mini-BEHAVIOR, the objects itneeds to manipulate are more complex. As illustrated in (b),in the packing-shapes task, the agent needs to manipulate objectsof the same shape but unseen colors during testing. In packing-20shapes, which experts can complete in one step, pure learning-based BC and DT only achieved a 20% success rate. However, ourABIL-BC achieved a satisfactory 94% success rate through neuro-symbolic grounding to recognize the shape of corresponding objects.These results highlight the vulnerability of pure-learning-basedmethods in open-world scenarios and demonstrate the necessity of",
  ": Accuracy of neuro-symbolic grounding undervarying data budgets in Robotic Manipulation tasks": "introducing neuro-symbolic reasoning in ABIL, which may providea promising solution for household agents.Comparison of Neuro-Symbolic Grounding. In roboticmanipulation, the search based planning policy of PDSketchonly applies to discrete symbolic operators, and therefore cannotbe compared in our main experiments with continuous actionspace. We can only compare ABIL with PDSketch from theperspective of neuro-symbolic grounding. The results are providedin . Achieving precise grounding in this environment ismore challenging. As the amount of demonstration increases, theaccuracy of PDSketch rises slowly and erratically. In contrast, ourmethod shows a rapid improvement, further demonstrating theadvantage of our ABIL in terms of neuro-symbolic groundingefficiency.",
  "Learning for Long-Horizon Planning via Neuro-Symbolic Abductive ImitationKDD 25, August 0307, 2025, Toronto, ON": "The results are provided in . First, we found thatPDSketch, the model-based planning method, achieved the bestresults on the pickup task, even with a limited 500 demonstrations.However, as shown in , PDSketch fails in the complex long-horizon tasks. Second, we could find that in the simple task, pickup,the generalization of different methods is consistently improvedwhen the data volume increases. However, in the Opening packagesand Putting away dishes tasks from Mini-BEHAVIOR, althoughthe results in basic evaluation improve with the increase of datavolume, their performance in the corresponding generalizationtests no longer grows. This also reflects the weakness of pure-learning-based methods, that is, they easily overfit to the specifictraining observations, and their performance in out-of-distributionobservation is fragile. Third, we found that our Abductive ImitationLearning framework has clearly improved the data efficiency of theBC and DT baselines, especially achieved significant generalizationimprovement in the out-of-distribution evaluation.",
  "Zero-Shot Generalization": "Symbolic reasoning excels at generalization, especially ensuringthe correctness of reasoning for any combination of logical clauses.In this subsection, we evaluate the zero-shot generalization perfor-mance in the composed tasks. In the BabyAI domain, we train thepolicies on the pickup and open task, then test them on the composedtask unlock. During training, the demonstrations from two tasks aremixed and learning in a multi-task scheme. In the Mini-BEHAVIORdomain, we primarily concentrate on generalization with the longerseries of events, which demands the agent to make use of learnedtechniques for repeatedly completing a single task to achieve thedesired goal. Take the Throwing away leftovers task as an example,we train every model in the environment with 1 leftover hamburgerto throw, while in the test environment, the agent is required tothrow 2 or 3 hamburgers. In robotic manipulation, we primarilyfocus on compositional generalization with the novel combinationof goals, which demands the agent to re-combine learned conceptsto achieve, as shown in (c).The results are provided in . Although all baselinesachieve satisfactory performance on the training tasks (pickup andopen), their performance degrades on the simple combined task(unlock). The pure-learning-based methods directly learn the actioncorresponding to the observations, lacking reasoning ability, thusunable to realize the need to first pick up a key that can open thetarget door, resulting in failure. PDSketch has reasoning ability,but its model-based planning solution accumulates errors with theincreasing length of the sequence, resulting in poor performanceand high computational overhead. In tasks with longer sequences,such as throwing away leftovers, solutions cannot be found evenafter running out of time. Our ABIL not only performs high-levelreasoning to know that sub-goals should be sequentially completedbut also can zero-shot achieve the composed tasks.",
  "In this work, we proposed a novel framework, ABductive ImitationLearning (ABIL), which integrates data-driven learning with sym-bolic reasoning to address long-horizon tasks in imitation learning": "ABIL bridges the gap between neural perception and logical rea-soning by autonomously generating predicate candidates from rawobservations with the knowledge base, enabling effective reasoningwithout requiring extensive manual annotations. Experimentsdemonstrate that ABIL significantly improves data efficiency andgeneralization across various long-horizon tasks, positioning it asa promising neuro-symbolic solution for imitation learning.Despite its contributions, ABIL has several limitations thatsuggest promising directions for future work: (1) Uncertainty andpartial observability: The current framework assumes determin-istic and fully observable environments, consistent with existingwork . However, real-world environments are oftenstochastic and partially observable. A promising direction is toexplore POMDP techniques , which would allow ABILto maintain a belief space and sample actions under uncertainty.(2) Automatic knowledge learning: Like most neuro-symbolicand abductive learning work, ABIL assumes the availability ofa symbolic solution and relies on an accurate and sufficientknowledge base . A key direction is to incorporate advancedknowledge learning techniques to reduce reliance onhuman-defined knowledge. Additionally, introducing the activelearning manner with human feedback could help correctand supplement the knowledge base, further enhancing ABILsadaptability and robustness. In summary, ABIL offers a timelyand promising solution for neuro-symbolic imitation learning,particularly for long-horizon planning. Addressing the challenges ofuncertain environments and incomplete knowledge will unlock itsfull potential, making it a reliable system for real-world applications. This research was supported by Leading-edge Technology Programof Jiangsu Science Foundation (BK20232003), Key Program ofJiangsu Science Foundation (BK20243012) and the PostgraduateResearch & Practice Innovation Program of Jiangsu Province(KYCX24_0233).",
  "Michael Bain and Claude Sammut. 1995. A Framework for Behavioural Cloning.In Machine Intelligence": "Raunak P. Bhattacharyya, Blake Wulfe, Derek J. Phillips, Alex Kuefler, JeremyMorton, Ransalu Senanayake, and Mykel J. Kochenderfer. 2023. Modeling HumanDriving Behavior Through Generative Adversarial Imitation Learning. IEEETransactions on Intelligent Transportation Systems 24, 3 (2023), 28742887. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, BenjaminChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, IlyaSutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. InAdvances in Neural Information Processing Systems 33. Virtual Event. Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, MishaLaskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. 2021. DecisionTransformer: Reinforcement Learning via Sequence Modeling. In Advances inNeural Information Processing Systems 34. 1508415097. Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems,Chitwan Saharia, Thien Huu Nguyen, and Yoshua Bengio. 2019. BabyAI: APlatform to Study the Sample Efficiency of Grounded Language Learning. In 7thInternational Conference on Learning Representations. New Orleans, LA. Wang-Zhou Dai, Qiu-Ling Xu, Yang Yu, and Zhi-Hua Zhou. 2019. BridgingMachine Learning and Logical Reasoning by Abductive Learning. In Advances inNeural Information Processing Systems 32. Vancouver, Canada, 28112822.",
  "Maria Fox and Derek Long. 2003. PDDL2.1: An Extension to PDDL for ExpressingTemporal Planning Domains. Journal of Artificial Intelligence Research 20 (2003),61124": "Tanmay Gangwani, Joel Lehman, Qiang Liu, and Jian Peng. 2019. LearningBelief Representations for Imitation Learning in POMDPs. In Proceedings of the35th Conference on Uncertainty in Artificial Intelligence, Vol. 115. Tel Aviv, Israel,10611071. En-Hao Gao, Yu-Xuan Huang, Wen-Chao Hu, Xin-Hao Zhu, and Wang-Zhou Dai.2024. Knowledge-Enhanced Historical Document Segmentation and Recognition.In Proceedings of the 38th AAAI Conference on Artificial Intelligence. Caelan Reed Garrett, Chris Paxton, Toms Lozano-Prez, Leslie Pack Kaelbling,and Dieter Fox. 2020. Online Replanning in Belief Space for Partially ObservableTask and Motion Problems. In IEEE International Conference on Robotics andAutomation. Paris, France, 56785684.",
  "Alfonso Emilio Gerevini. 2020.An Introduction to the Planning DomainDefinition Language (PDDL): Book review. Artificial Intelligence 280 (2020),103221": "Joy Hsu, Jiayuan Mao, Joshua B. Tenenbaum, and Jiajun Wu. 2023. Whats Left?Concept Grounding with Logic-Enhanced Foundation Models. In Advances inNeural Information Processing Systems 36. New Orleans, LA. De-An Huang, Danfei Xu, Yuke Zhu, Animesh Garg, Silvio Savarese, Li Fei-Fei,and Juan Carlos Niebles. 2019. Continuous Relaxation of Symbolic Plannerfor One-Shot Imitation Learning. In 2019 IEEE/RSJ International Conference onIntelligent Robots and Systems. Macau, China, 26352642. Yu-Xuan Huang, Wang-Zhou Dai, Le-Wen Cai, Stephen H. Muggleton, andYuan Jiang. 2021. Fast Abductive Learning by Similarity-based ConsistencyOptimization. In Advances in Neural Information Processing Systems 34. VirtualEvent, 2657426584. Yu-Xuan Huang, Wang-Zhou Dai, Jian Yang, Le-Wen Cai, Shaofen Cheng,Ruizhang Huang, Yu-Feng Li, and Zhi-Hua Zhou. 2020.Semi-SupervisedAbductive Learning and Its Application to Theft Judicial Sentencing. In 20thIEEE International Conference on Data Mining. Sorrento, Italy, 10701075. Yu-Xuan Huang, Zequn Sun, Guangyao Li, Xiaobin Tian, Wang-Zhou Dai, WeiHu, Yuan Jiang, and Zhi-Hua Zhou. 2023. Enabling Abductive Learning to ExploitKnowledge Graph. In Proceedings of the 32nd International Joint Conference onArtificial Intelligence. Macao, China, 38393847.",
  "Ahmed Hussein, Mohamed Medhat Gaber, Eyad Elyan, and Chrisina Jayne. 2017.Imitation learning: A survey of learning methods. Comput. Surveys 50, 2 (2017)": "Emily Jin, Jiaheng Hu, Zhuoyi Huang, Ruohan Zhang, Jiajun Wu, Li Fei-Fei, and Roberto Martn-Martn. 2023.Mini-BEHAVIOR: A ProcedurallyGenerated Benchmark for Long-horizon Decision-Making in Embodied AI. CoRRabs/2310.01824 (2023). George Dimitri Konidaris, Leslie Pack Kaelbling, and Toms Lozano-Prez. 2018.From Skills to Symbols: Learning Symbolic Representations for Abstract High-Level Planning. Journal of Artificial Intelligence Research 61 (2018), 215289. Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava,Roberto Martn-Martn, Chen Wang, Gabrael Levine, Michael Lingelbach, JiankaiSun, Mona Anvari, Minjune Hwang, Manasi Sharma, Arman Aydin, DhruvaBansal, Samuel Hunter, Kyu-Young Kim, Alan Lou, Caleb R. Matthews, Ivan Villa-Renteria, Jerry Huayang Tang, Claire Tang, Fei Xia, Silvio Savarese, HyowonGweon, C. Karen Liu, Jiajun Wu, and Li Fei-Fei. 2022.BEHAVIOR-1K: ABenchmark for Embodied AI with 1, 000 Everyday Activities and RealisticSimulation. In Conference on Robot Learning. Auckland, New Zealand, 8093. Jiayuan Mao, Toms Lozano-Prez, Josh Tenenbaum, and Leslie Pack Kaelbling.2022. PDSketch: Integrated Domain Programming, Learning, and Planning. InAdvances in Neural Information Processing Systems 35. New Orleans, LA. Luc Le Mero, Dewei Yi, Mehrdad Dianati, and Alexandros Mouzakitis. 2022. ASurvey on Imitation Learning Techniques for End-to-End Autonomous Vehicles.IEEE Transactions on Intelligent Transportation Systems 23, 9 (2022), 1412814147.",
  "Mohit Shridhar, Lucas Manuelli, and Dieter Fox. 2021. CLIPort: What and WherePathways for Robotic Manipulation. In 5th Conference on Robot Learning. London,UK, 894906": "Tom Silver, Ashay Athalye, Joshua B. Tenenbaum, Toms Lozano-Prez, andLeslie Pack Kaelbling. 2022. Learning Neuro-Symbolic Skills for Bilevel Planning.In 6th Conference on Robot Learning. Auckland, New Zealand, 701714. Tom Silver, Rohan Chitnis, Nishanth Kumar, Willie McClinton, Toms Lozano-Prez, Leslie Pack Kaelbling, and Joshua B. Tenenbaum. 2023.PredicateInvention for Bilevel Planning. In 37th AAAI Conference on Artificial Intelligence.Washington, DC, 1212012129. Tom Silver, Rohan Chitnis, Joshua B. Tenenbaum, Leslie Pack Kaelbling, andToms Lozano-Prez. 2021. Learning Symbolic Operators for Task and MotionPlanning. In IEEE/RSJ International Conference on Intelligent Robots and Systems.Prague, Czech Republic, 31823189. Chen Wang, Linxi Fan, Jiankai Sun, Ruohan Zhang, Li Fei-Fei, Danfei Xu,Yuke Zhu, and Anima Anandkumar. 2023. MimicPlay: Long-Horizon ImitationLearning by Watching Human Play. In 7th Annual Conference on Robot Learning.Atlanta, GA. Renhao Wang, Jiayuan Mao, Joy Hsu, Hang Zhao, Jiajun Wu, and Yang Gao.2023.Programmatically Grounded, Compositionally Generalizable RoboticManipulation. In The 11th International Conference on Learning Representations.Kigali, Rwanda.",
  "Grover J Whitehurst and Ross Vasta. 1975.Is language acquired throughimitation? Journal of Psycholinguistic Research 4 (1975), 3759": "Danfei Xu, Roberto Martn-Martn, De-An Huang, Yuke Zhu, Silvio Savarese, andLi Fei-Fei. 2019. Regression Planning Networks. In Advances in Neural InformationProcessing Systems 32. Vancouver, Canada, 13171327. Tian Xu, Ziniu Li, and Yang Yu. 2022. Error Bounds of Imitating Policies andEnvironments for Reinforcement Learning. IEEE Transactions on Pattern Analysisand Machine Intelligence 44, 10 (2022), 69686980. Xiaowen Yang, Jie-Jing Shao, Wei-Wei Tu, Yufeng Li, Wang-Zhou Dai, and Zhi-Hua Zhou. 2024. Safe Abductive Learning in the Presence of Inaccurate Rules. In38th AAAI Conference on Artificial Intelligence. Vancouver, Canada, 1636116369. Xiaowen Yang, Wenda Wei, Jie-Jing Shao, Yufeng Li, and Zhi-Hua Zhou. 2024.Analysis for Abductive Learning and Neural-Symbolic Reasoning Shortcuts. In41st International Conference on Machine Learning. Vienna, Austria.",
  "AEXPERIMENTAL DETAILSA.1BabyAI and Mini-BEHAVIOR": "In these two environments, each object feature is represented by itsstate and position in the room, and the robot feature is representedby its position and direction. The state representation is composedof features of all objects and the robot. The action spaces are bothdiscrete. All expert demonstrations are generated by scripts basedon search. In BabyAI, 1000 demonstrations were used for trainingper task and to obtain . In Mini-BEHAVIOR, install-a-printer,opening packages, and moving boxes to storage used 1000, whileother tasks used 3000 expert demonstrations for training.Model Architecture. For each predicate (e.g. is-dusty), we builda binary classifier, which takes a single object as argument, andreturns a scalar value from 0 to 1, indicating the classification score.All methods use the same network architecture which is based onthe Neural Logic Machine . Specifically, we first encode the statewith a two-layer NLM. For BC, we use a single linear layer, takingthe state embedding as the input and output actions. For DT, webuild a single transformer layer following the two-layer encoder,with the causal mask to generate future action with past statesand actions. For PDSketch, we choose the full mode in the originalpaper . For ABIL-BC, we implement the behavior modules usingBC model, and for ABIL-DT, we implement the behavior modulesusing DT model.",
  "A.2Robotic Manipulation": "In this environment, each object is represented as a tuple of a 3Dxyz location, and an image crop. Following , we first computethe 2D bounding box of the object in the camera plane, then cropthe image patch and resize it to 24 by 24 to obtain the image crop.The action space is continuous, each action involves a start andend-effector pose. provides the average length of expertdemonstrations. This benchmark involves the agent manipulatingobjects of various colors and shapes, reflecting the requirements inthe open world, providing a greater challenge for imitation learning.For each task, 1000 expert demonstrations were used for trainingand to obtain . All of these demonstrations are collected usingoracle policies following CLIPort , containing only successfultrajectories.Model Architecture. Image feature of each object is a 64-dimensional embedding obtained via an image encoder, whichis a 3-layer convolutional neural network followed by a lineartransformation layer. For each predicate (e.g. is-red), we build abinary classifier, which takes the image feature of an object, andreturns a scalar value from 0 to 1, indicating the classificationscore. The model implementation is same as in BabyAI and Mini-BEHAVIOR, except output continuous value as action.",
  "BSUPPLEMENTAL RESULTSB.1Study on Performance with ImperfectSymbolic Grounding": "To evaluate the influences of neuro-symbolic errors upon ABIL, wefurther conduct experiments on the Pickup and Putting-blocks-in-bowls task . Experimental results are provided in .Like human reasoning, incorrect logical objectives may leadto the failure of sequential decision-making. Neuro-symbolicerrors indeed lower the performance of ABIL. Nevertheless, ABILintegrates data-driven imitation and logical objectives in learning,it has a tolerance for neuro-symbolic errors. Even under 75%",
  "D.2Mini-BEHAVIOR": "In this domain, our state machine is mainly composed of several typical categories. For tasks mainly about tidying up the room, e.g. Throwingaway leftovers, we split the primitive actions into and , which is required to perform an action sequence to finish pickup orplace subtask. Combined with our symbolic-grounding , the agent will be able to distinguish when and where to pick and place. For tasksmainly about cleaning, e.g. Cleaning a car, we split the primitive actions into and , which is required to finish washing or puttingsubtask. In addition, some tasks involve more operators, such as install a printer. We provide detailed illustrations of these representativestate machine models."
}