{
  "ABSTRACT": "Graph Neural Networks (GNNs) have become pivotal tools for arange of graph-based learning tasks. Notably, most current GNNarchitectures operate under the assumption of homophily, whetherexplicitly or implicitly. While this underlying assumption is fre-quently adopted, it is not universally applicable, which can resultin potential shortcomings in learning effectiveness. In this paper,for the first time, we transfer the prevailing concept of one nodeone receptive field\" to the heterophilic graph. By constructing aproxy label predictor, we enable each node to possess a latent pre-diction distribution, which assists connected nodes in determiningwhether they should aggregate their associated neighbors. Ulti-mately, every node can have its own unique aggregation hop andpattern, much like each snowflake is unique and possesses its owncharacteristics. Based on observations, we innovatively introducethe Heterophily Snowflake Hypothesis and provide an effectivesolution to guide and facilitate research on heterophilic graphsand beyond. We conduct comprehensive experiments including (1)main results on 10 graphs with varying heterophily ratios across10 backbones; (2) scalability on various deep GNN backbones (SGC,JKNet, etc.) across various large number of layers (2,4,6,8,16,32 lay-ers); (3) comparison with conventional snowflake hypothesis; (4)efficiency comparison with existing graph pruning algorithms. Our",
  "Kun Wang and Guibin Zhang are co-first authors. denotes corresponding authors": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08...$15.00 observations show that our framework acts as a versatile operatorfor diverse tasks. It can be integrated into various GNN frame-works, boosting performance in-depth and offering an explainableapproach to choosing the optimal network depth. The source codeis available at",
  "Graph Neural Network, Heterophilic Graph, Graph Pruning": "ACM Reference Format:Kun Wang, Guibin Zhang, Xinnan Zhang, Junfeng Fang, Xun Wu, Guo-hao Li, Shirui Pan, Wei Huang, and Yuxuan Liang. 2024. The HeterophilicSnowflake Hypothesis: Training and Empowering GNNs for HeterophilicGraphs. In Proceedings of the 30th ACM SIGKDD Conference on KnowledgeDiscovery and Data Mining (KDD 24), August 2529, 2024, Barcelona, Spain.ACM, New York, NY, USA, 19 pages.",
  "INTRODUCTION": "Graph Neural Networks (GNNs) have become the defacto standard for various graph representation learning tasks, suchas node classification , link prediction , and graphclassification . The superior capabilities of GNNs can be at-tributed to their message passing paradigm . Through iterativeinformation aggregation and updating, the central node capturesrich information by interacting with its neighboring nodes basedon the connected graph structure .Among the various landscape of GNN architectures and designs,the homophily assumption serves as a foundational pil-lar, suggesting that edges predominantly link nodes with identicallabels and analogous node features. Despite its appealing success,",
  ": The algorithm workflow of Heterophilic Snowflake Hy-pothesis (Hetero-S) and Heterophily-aware Early Stopping (HES)": "the performance of current GNNs has dropped sharply as the ho-mophily of the graph decreases. Specifically, within heterophilicgraphs, a discrepancy is often observed between the labels of neigh-boring nodes and the central node, a phenomenon referred to asthe local structure discrepancy issue . We ascribe theobserved performance degradation to the uniform message pass-ing framework, in which that the central node initially aggregatesmessages from its local neighboring nodes, subsequently updatingthe ego node (see left hand).However, non-euclidean data frequently display heterophily,which can be observed across diverse domains. For instance, whenusers engage with content on Netflix, the people with diverse pref-erences might be subjected to similar recommendation algorithms,owing to their interaction with identical video content. The poten-tial of heterophilic graphs is vast, holding promise in both academicand industrial spheres, such as social networks , transporta-tion systems , and recommendation platforms . In aheterophilic context, this mechanism introduces two primary andchallenging limitations: In graph topology, local neighbors refer to nearby nodes, oftenoverlooking distant yet informative nodes. In heterophilic, nodessharing structural and semantic characteristics can be more dis-tantly positioned . A consistent aggregation and update method often neglects varia-tions in information from alike/unalike neighbors. In heterophilicgraphs, achieving discerning node representations necessitatescustomized message passing to capture distinctive patterns.Given the above emerging challenges, there has been a shiftingfocus towards exploring heterophily in GNNs. This research areaincludes a wide range from delving into heterophilic graph samplingto the evolution of intricate algorithms. The growing interest inheterophilic graph learning can be attributed to its vast applicability.From a macro perspective, existing heterophilic GNNs can bebroadly classified into two categories, i.e., non-local neighbor ex-tension and GNN architecture refinement . The concept ofnon-local neighbor extension in heterophilic GNNs involves broad-ening the receptive field beyond local neighbors. This is achievedthrough strategies like high-order neighbor information mixing and potential neighbor discovery , en-hancing representation by capturing distant but relevant node fea-tures. With the second class, GNN architecture refinement focuseson enhancing the expressive capability of GNNs for heterophilic graphs by optimizing AGGREGATE and UPDATE function .Through strategies such as adaptive message aggregation ,ego-neighbor separation , and layer-wise operation ,the refinement aims to produce distinguishable and discriminativenode representations.Recently, a novel paradigm, the Snowflake Hypothesis (SnoH) ,rooted in the concept of one node, one receptive field has gainedsignificant attention for its efficacy in addressing the over-smoothing and over-fitting issues in GNNs. This hypothesis drawsinspiration from the intricate and distinctive patterns exhibitedby individual snowflakes, assuming that each node can possess itsunique receptive field. It posits that for an -layer GNN, every nodein the graph harbors an optimal receptive field width denoted as (1 ). During the message passing process from 1 to hops, each node merely aggregates information from the preceding hops, after which they cease to aggregate information from theneighborhood (referred to as node early stopping).Intuitively, the snowflake hypothesis demonstrates even greatervitality and significance in the context of heterophilic graphs: (1)One concept benefits all. The idea behind the snowflake hypoth-esis can be seamlessly integrated with any heterophilic GNNs de-signs, showcasing exceptional versatility. Whether it is for non-localneighbor extension or GNN architecture refinement, the snowflakecan easily be incorporated as a plugin and demonstrates strong com-patibility. (2) Enhanced pruning requirement. In heterophilicgraphs, given the higher probability of central nodes having differ-ent labels than surrounding nodes, the need for pruning aggregationchannels becomes even more critical than in homogeneous graphs.This pruning aids nodes in selectively aggregating information andupdating themselves effectively.However, the original SnoH and its implementations appear to ex-hibit limitations when applied to heterophilic graphs. Firstly, SnoHrelies on either gradient information or cosine similarity comparisonfor node early stopping. These heterophily-unaware approachesdo not adequately integrate into heterophilic scenarios. Secondly,SnoH typically demonstrates convincing performance only in deepGNNs, which contradicts the prevailing focus on shallow designsfor heterophilic GNNs. These necessitates bespoke strategies forheterophilic graphs. To handle the distinct characteristics betweenhomophilic and heterophilic graphs, we introduce a Heterophily-aware Early Stopping (HES) strategy. HES benefits from two keyaspects, thereby overcoming the limitations of SonH: HES employs a proxy label predictor, generating pseudo-labelprobability distributions for different nodes . In this way,we can scrutinize the probability distributions of two connectednodes and determine the probabilities where two nodes are pre-dicted to have the same label. This value can further representthe homophily score between two nodes, subsequently servingas a replacement for the original adjacent matrix. By analyzing the variation in the heterophilic ratio across eachlayer, we are able to appropriately determine the depth for earlystopping. This approach contrasts with SonH, which primarilyexhibits efficacy in deeper GNN configurations.",
  "The Heterophilic Snowflake Hypothesis:Training and Empowering GNNs for Heterophilic GraphsKDD 24, August 2529, 2024, Barcelona, Spain": "of information aggregation. For the first time, we validate theexistence of snowflakes in heterophilic graphs, underscoring thesignificance of the one node, one receptive field\" paradigm in suchscenarios. We introduce a universal solution to this issue, whichstands out from previous designs due to its generality and model-agnostic nature. Remarkably, HES can seamlessly integrate withvirtually arbitrary heterophilic designs, enhancing both its trainingand inference speeds. We summarize our contributions as follows: We conceptualize one node one receptive field as heterophilicsnowflake hypothesis in the heterophilic graph scenario. Toachieve this target, we develop the heterophily-aware early stop-ping strategy, and offer theoretical analysis from the graph neuraltangent kernel (GNTK) and stochastic block model (SBM) per-spective to provide high-level insights of ours paradigm. Hetero-S finds broad applicability and HES can aid various back-bones in discovering optimal receptive fields for each node acrossdiverse datasets. We verify HES on 10 GNN backbones acrossover 16 graph benchmarks. Experiments demonstrate that for allprevailing backbones, HES can facilitate substantial performanceenhancements, ranging from 0.34% 31.86% in homophilic set-tings and from 0.68% 21.73% in heterophilic settings. Similar to the conventional snowflake hypothesis, HES can scaleup to deep GNNs effectively without any bells and whistles. Con-cretely, HES mitigates the performance degradation caused byexcessive aggregation of heterophilic information, resulting inperformance improvements ranging from 0.46% 10.37% in 16-layer configurations and from 1.13% 7.92% in 32-layer configu-rations. These experimental results demonstrate the potential ofHES to be extended to large and densely connected graphs. More observations. (I) Hetero-S has been empirically observedto exhibit comparable or even superior performance to the origi-nal snowflake hypothesis on both homophilic and heterophilicsettings. In particular, Hetero-S demonstrates performance im-provements ranging from 0.51% 8.44% on MixHop and JKNet incomparison to SnoH. (II) The snowflakes () achieves the high-est multiply-accumulate operations (MACs) saving (25% 45% ofthe baseline) compared to SOTA graph pruning algorithms ,without any performance compromise.",
  "PRELIMINARIES2.1Notations": "We consider an attributed graph denoted as G = {V, E}, whereV and E represent the sets of nodes and edges, respectively. Thefeature matrix of G is denoted as X R , where = |V|is the number of nodes in the graph and is the dimension ofnode features. We use x = X to denote the -dimensionalfeature vector corresponding to node V. An adjacency matrixA R , serves to represent the connections between nodes,where A = 1 if (, ) E and 0 otherwise. To learn the noderepresentations in a graph G, most GNNs adhere to the followingparadigm of neighborhood aggregation and message passing:",
  "vector of and h()(1 ) denotes the node embedding": "of at the -th GNN layer. AGGR and COMB represent functionsused for aggregating neighborhood information and combiningego- and neighbor-representations, respectively. In the generalnode classification setting, after the graph convolution operations,GNN uses a linear mapping to map the node embedding h()to thecorresponding prediction probability value z, and eventually getthe model prediction :",
  "VH ()node,(4)": "where N () denotes the 1-hop neighbor of and V() denotesthe 1-hop neighbors of . Specifically, H ()node represents the averageproportion of neighbors that share the same class with node , andHnode represents the global homophily by computing the averageof node homophily. Conversely, the heterophily ratio at the nodeand graph level can be expressed as H ()node = 1H ()node and Hnode =1 Hnode. In general, graphs that exhibit strong homophily tendto have Hnode values approaching 1, whereas those characterizedby pronounced heterophily often have values near 0.Its essential to note that H ()node solely reflects the heterophilywithin the 1-hop neighborhood of . Furthermore, we extend thisdefinition to the k-hop neighborhood:",
  "MOTIVATION": "In this section, we prudently introspect the heterophily in differ-ent graphs and put forward the motivation of HES strategy. Westart from the empirical observations. Concretely, we select both aheterophilic graph (Squirrel) and a homophilic graph (CS), and com-pute NH (1 6) for each node. As shown in (a,b),we list the following two observations: Obs.1: The homophily ratioof nodes in Squirrel significantly declines as the hop increases at afaster rate compared to those in CS; Obs.2: In both types of graphs,the distribution of node homophily ratios is diverse. Even when thereceptive field size extends to 6 layers (i.e., 6-hop neighborhood),there exist nodes with homophily ratios approaching 1.Insights & Reflections. These observations naturally align withthe concept of one node one receptive field.\" In both types of graphdata, the k-hop homophily distributions of different nodes exhibitsignificant variation. This prompts the question: Is it feasible to",
  "COMBh(1), , R (7)": "It is noteworthy that, beyond the R-th layer of GNN, node ceasesto aggregate information from its neighbors, thus achieving earlystopping of the receptive field. Further, (c,d) demonstratesthe performance of GCN on Squirrel and CS with different ho-mophily thresholds, which characterizes a 2.12% under = 0.3on Squirrel, and a 1.34% under = 0.2 on CS. This confirms thatearly stopping of node receptive fields can indeed assist GNNs inlearning more refined node representations.",
  "METHODOLOGY": "Considering the aforementioned observation and gained motiva-tions, we aim to identify an appropriate receptive field for eachnode. Analogous to the conventional snowflake hypothesis, weintroduce the Heterophily Snowflake Hypothesis (Hetero-S)\" forthe first time in heterophilic graphs settings: Heterophilic Snowflake Hypothesis (Hetero-S): For an -layerGNN on a heterophilic graph, each node possesses an optimal receptivefield; training the GNN by aggregating information only from neigh-bors within this optimal receptive field minimizes the inclusion ofexcessive heterophilic information, yielding optimal representations. More formally, consider an -layer GNN, when optimized with sto-chastic gradient descent (SGD) on heterophilic graphs G = {A, X},the GNN reaches a minimum validation loss and obtains a testaccuracy of . Furthermore, lets assume the existence of a prun-ing algorithm guided by the Hnode, which ensures that each node has a unique optimal receptive field size of (1 ),allowing it to be early stopped at the -th GNN layer, i.e., setN (+1) () = = N () () = . This approach helps nodesavoid over-aggregation of heterophilic information, leading to atest accuracy of . The Hetero-S posits that there exists optimal for each to satisfy that > (note as ).To achieve this target, we can naturally resort to the implementa-tion in Sec. 3. However, in practical scenarios (e.g., semi-supervisedsettings), a question arises regarding receptive field operation: whenonly partial nodes have known labels, how can we estimate the ho-mophily ratio for each node within a -hop neighborhood?",
  "Proxy Label Predictor": "As depicted above, in semi-supervised scenarios, we only know asmall fraction of labels in a heterophilic graph. Towards this end,we formulate a GNN prediction process as a combination of twoprocesses, i.e., = {AGGR COMB} . After the aggregation andcombination process, {AGGRCOMB}, nodes are mapped into a latentspace characterized by distinguishability. This mapping ensuresthat nodes with the same label are positioned in similar locations,facilitating the identification of their relational patterns. Moreover,by employing an oracle function, we can effectively predict theoutcome , leveraging the structured information encapsulatedin this latent space. This process enhances the models ability todiscern and categorize nodes, significantly improving the accuracyand efficiency of our predictions. Upon reviewing previous work, we first present a lemma: Lemma 4.1. Assuming that NH for decreases w.r.t in pro-portion to ( > 1), meaning that as the receptive field expands, aggregates more heterophilic information. Under such circumstances,when employing receptive field stopping, there exists 2 satisfy-ing the condition that ({ }() ) > ({ }(+ ) ), where () is the performance metric, { }() signifies aggregating information solely from the -hopneighborhood, and N+. With this in mind, we construct a proxy label predictor P todetermine label-wise graph aggregation . Concretely, weresort to a simple predictive model (here we can use MLP andGNN, and we place ablation results in Sec. 5.5) to obtain pseudoprobability label z with cross-entropy loss:",
  "Sparsity": "Identifying Receptive field for each node : The pipeline of our HES framework. For each node, we utilize a proxy model to evaluate the homophily strength of its edges, which isfurther used to estimate its multi-hop homophily ratio. Based on the homophily strength at each hop, we perform receptive field-level earlystopping to determine a unique receptive field for each node.",
  "Training Homophily Mask": "After obtaining the node soft labels, we proceed to define the ho-mophily mask denoted as S, where S, for edge (, ) E, representsthe homophily strength of edge (, ),i.e., the likelihood that and share the same label.To obtain the expression for S, we calculate the label distribu-tion for nodes and by computing the outer product of theirrespective predicted probability distributions.",
  "Early Stopping Based on Hop Heterophily": "After multiple rounds of training and optimization with Equation11, we obtain a relatively accurate mask, denoted as S. In the contextof GNNs, multi-layer aggregation assists the model in capturingneighbor relationships at varying distances. Here, we employ S toreplace A for capturing more distant neighbor relationships. Con-cretely, we use S() = S R to further represent the -hopneighbor aggregation expression of homophily ratio. Subsequently,we calculate the row sum values for each hop, excluding self-loops:",
  "thenDN () () = N (+1) () = = ,(13)": "where denotes the filtering threshold. While it is possible to as-sign a distinct threshold for each node, for simplicity, we employa uniform across all nodes to filter the receptive field (the sen-sitivity analysis on is placed in Appendix G). D denotes anintervention that forcefully assigns an aggregation status. We em-ploy heterophily-aware early stopping at -th layer, which canensure that each node possesses a unique receptive field size.",
  "=1(,)()(14)": "where and are indexes of nodes in the graph, and () is the setof all trainable parameters in -th layer. According to Equation (14),we need to calculate the GNTK at each layer, which follows therecursive relation: K() = G()K(1), where G() = A()( A())is -th layer propagation matrix for the GNTK with A = D1A, denotes the transpose operation. Our target is to check if thesmallest eigenvalue of GNTK is greater than zero or not. Accordingto , when the smallest eigenvalue of GNTK is greater thanzero, the training loss can be minimized to zero, implyingsuccessful optimization. Inversely, if the smallest eigenvalue of iszero, then we would say that the GNN cannot be trained successfully.To study the eigenvalue of GNTK, we introduce a generative modelnamed Stochastic Block Model (SBM) , which has been usedin the theoretical analysis of GNNs. Then the following Lemmagives the smallest eigenvalue of the GNTK: Lemma 4.2. Consider A is a probability adjacency matrix sam-pled from a SBM (, ,), wherein it is postulated that there arevarious clusters interconnected with an internal connection prob-ability of , and the inter-cluster connectivity rate is . We canconclude the expected smallest eigenvalue of the GNTK is given byEA (,,) [] = =11",
  "K() = A()( A()) A(1)( A(1)) A(1)( A(1))K(0)(15)": "where -th adjacency matrix A() is further characterized in proba-bilistic form, and the density of A() is correlated with the magni-tude of () and (). We observe that with the increasing depth ofGNN, early stopping of the receptive field can assist in the hierar-chical decrement of and . Consider a simple binary classificationscenario with a balanced distribution, where each category con-sists of nodes. The eigenvalues of matrix EA (,,) [G] are asfollows:",
  "( 1) + + 1 , 2 = 2 1 =1": "( 1) + + 1(16)Generally, the probability of connection between nodes with thesame class label is higher than that of different classes, i.e., > .Hence, the inequality (1 ) < ( 1) + 1 is alwayssatisfied. Therefore, the smallest eigenvalue of the SBM is given by1 ( 1)++1. Considering in subsequent layers, the HES algorithmis applied such that and remain attenuation while heterophilicnodes are pruned (the decay rate of is slower), the product of thesmallest eigenvalues is given by:",
  "RQ3. Can Hetero-S genuinely achieve graph sparsity and ac-celerate computations compared to mainstream graph pruningalgorithms ?": "RQ4. How sensitive is Hetero-S to its key components?To provide answers to these questions, we orchestrate the experi-ments including Main experiments, Depth scalability exper-iments, Comparative analysis with traditional SnowflakeHypotheses (SnoH) and Efficiency comparison with pruningalgorithms four parts. Detailed descriptions can be found in Ap-pendix C. Through these experiments, we anticipate drawing clearconclusions regarding the efficacy of Hetero-S.",
  "Experiment Setup": "Datasets. We verify Hetero-S across 10 graph benchmarks, includ-ing citation networks: Cora, CiteSeer, and PubMed ; WebKBnetworks: Cornell, Texas, and Wisconsin ; Wikipedia-derivednetworks: Chameleon and Squirrel ; the actor co-occurrencenetwork Actor ; the heterogenous information network DBLP. in Appendix A offers a comprehensive overview ofdataset details. Note that we choose both highly homophily graphswith Hnode > 0.8 and heterophilic graphs with Hnode > 0.1.Backbones. We select three categories of GNN designs, includ-ing the non-local neighbor extension, GNN architecture refinementas stated in , along with some general GNN backbones.Specifically, for non-local neighbor extension, we choose Mixhop and GPNN . For GNN architecture refinement designs, weopt for backbones like GAT , H2GCN , GCNII , Geom-GCN , JKNet , and MGNN . Lastly, we choose somegeneral-purpose GNNs, such as GCN and GIN , to furthervalidate the universality of our algorithm.",
  "Main Results (RQ1)": "We initially investigate the presence and identifiability of the het-erophily snowflake hypothesis (Hetero-S) through the heterophily-aware early stopping (HES) mechanism. We evaluate HES in con-junction with selected GNN backbones across 10 graph benchmarks.Our tests span not only the standard homophilic datasets but alsoextend to heterophilic graphs. From the and , welist the following Observations:Obs.1. The snowflake () broadly exist under 2 8 layer back-bones settings. As shown in , upon implementing the HESalgorithm, the model consistently achieved performance improve-ments. For instance, under the MGNN+Cora setup, the model at 8",
  "layers remarkably outperformed the 2-layer baseline by 18.08%. Thisphenomenon was also observed across MGNN+Citeseer, JKNet+Texas,": "H2GCN+Wisconsin, and many others, where performance enhance-ments ranged from 2.02%14.99% over the original 2-layer configu-rations. These findings substantiate the validity of our snowflakehypothesis\" in heterophily graphs.Obs.2. HES algorithm showcases the great flexibility to var-ious backbones and consistently presents superior perfor-mance. The introduction of HES consistently results in perfor-mance enhancements across nearly all tested models and data com-binations. Specifically, with GCN and GIN on homophily graphs likeCora, Citeseer, and PubMed, the models exhibit a 1% performanceimprovement. This trend is even more pronounced in heterophilicgraphs where, for example, GCN+Texas/Wisconsin shows an av-erage performance increase of 5.94% across 2 8 layers. Theseconsistent improvements across various configurations confirm theeffectiveness of our proposed HES.Obs.3. Graph-specific and GNN-specific analyses. Take a de-tailed look and analysis of heterophilic graphs combined with tailor-made GNN architectures, we observed that HES contributes signifi-cantly to performance gains. For instance, MGNN on 6 datasets withH < 0.5, such as Texas and Squirrel, can yield performanceimprovements close to 10%. H2GCN+Texas achieves an increaseof approximately 15% 20%. Further scrutiny of revealsthat + feature significantly bolsters the robustness of our train-ing process. Moreover, this enhancement facilitates the discoveryof superior dependable subgraphs, particularly within the deeperlayers of the network. We showcase more training visualizationsin Appendix D.",
  ": The JKNet and + results across CS, DBLP, Actor andChameleon four benchmarks on 2, 4, 8, 16, 32-layer settings": "5.3Extend Hetero-S to Deep GNNs (RQ2)To provide a scalable solution for large and densely connectedheterophilic graphs, we extend the HES algorithm to deep GNNcontexts. Specifically, we select ResGCN , JKNet , GCNII and SGC as backbones and conduct tests on 2 homophilicand 2 heterophilic graphs, assessing performance at depths of upto 32 layers. For homophilic graphs, we opt for CS and DBLP ,whose Hnode is 0.81 and 0.82, respectively. For heterophilic graphs,we choose Actor (Hnode = 0.22) and Chameleon (Hnode=0.23), Welist the following observations:Obs.4. Hetero-S consistently boost GNNs at all depths. Asillustrated in , the blue line represents the enhanced modelperformance with the addition of HES, while the red line depictsthe original baseline. We observed that incorporating the HES al-gorithm leads to performance gains in both homophilic and het-erophilic graph contexts. Notably, for the Chameleon dataset, theintegration of the HES algorithm resulted in a performance increaseof nearly 2.9% against the JKNet baseline.Obs.5. Hetero-S can assist the top-student backbones. Anintriguing observation is that JKNet does not exhibit significantperformance degradation with the deepening of both homophilicand heterophilic graphs. However, even for the specifically deep-ened network JKNet, HES still demonstrates exceptional auxiliaryperformance, further proving the importance of early stopping inreceptive fields. We have placed additional experimental results inAppendix E, from which we can draw similar conclusions.Additionally, the conventional SnoH is specially designedfor deepening GNNs on homophilic graphs, and we provide furthercomparisons between Hetero-S and SnoH in Appendix F. 5.4Compare With Pruning Methods (RQ3)In this section, we compare HES with current SOTA pruning meth-ods, UGS and DSpar . We attempt to understand whetherHetero-S can (1) achieve satisfactory sparsity without compromis-ing performance, and (2) genuinely accelerate GNN computations.As shown in and , we can list the observations:",
  "M": ": Summary of performance (y-axis) at different graph andGNN layers (x-axis) on Cora and Squirrel. The size of markers rep-resents the inference MACs (=12 FLOPS) of each sparse GCN onthe corresponding sparsified graphs. Black circles () indicate thebaseline. Blue circles () are DSpar. Orange circles () represent theUGS. Red stars () are established by Hetero-S. 0bs.7. Hetero-S consistently achieves the highest sparsity. Asshown in , with the GNN layers deepening, both UGS andDSpar experience a drastic decline in extreme sparsity. In contrast,Hetero-S demonstrates enhanced sparsity capabilities, surpassingUGS and DSpar by 23.96% and 19.96% on Squirrel+16-layer GCN.Obs.8. On both datasets, Hetero-S achieved the smallest in-ference MACs, nearly only 25%-45% of the original baseline.Specifically, on the Cora+16-layer GCN, we observe that Hetero-Scan achieve comparable or even better performance than UGS andDSpar, with only 8.04% and 4.89% MACs, respectively. Furthermore,our algorithm consistently improves performance across variousGCN depths, especially in deeper layers. Hetero-S surpassed UGSby approximately 2.3%2.8% on Cora and was able to reliably train16 layers on Squirrel, outperforming the baseline by nearly 6.2%.",
  "Avg. Rank2.83.42.21.6": "Obs.9. HES Shows limited sensitivity to proxy model selec-tion. Across all five datasets, the performance variance of snowflakesobtained using different proxy models ranged narrowly between0.78% and 1.31%. Specifically, the overall performance ranking isMLP > SGC > GCN > GAT, so we leverage a 3-layer MLP for theunified experimental setup in all experiments.",
  "RELATED WORK": "Our research primarily focuses on the domain of GNNs and is highlypertinent to two specific areas. Due to space constraints, we haveincluded the comprehensive related work in the appendix I.Graph Pooling & Clustering devote to reducing the computa-tional burden of GNNs by applying pruning or compressing meth-ods , which are highly relevant to our research.We divide existing techniques into two categories. (1) Sampling-based methods aims at selecting the most expressive nodes or edgesfrom the original graph to construct a new subgraph .Though efficient, the dropping of nodes/edges sometimes resultsin severe information loss and isolated subgraphs, which may crip-ple the performance of GNNs . (2) Clustering-based methodslearns how to cluster the whole nodes in the original graph to pro-duces a informative small graph , which can remedy theaforementioned information loss problem.Heterophilic GNNs. Existing heterophilic GNNs primarily fallinto two categories: non-local neighbor extension and GNN archi-tecture refinement . The former emphasizes expanding theneighborhood scope, achieved via high-order neighbor informationmixing and potential neighbor discovery .The latter, delves into enhancing GNNs expressive power specifi-cally for heterophilic graphs. Strategies include adaptive messageaggregation , ego-neighbor separation , and layer-wise operations to optimize node representation quality.Its worth emphasizing that our work shares similarities with thatof , as both approaches utilize proxy models to discern het-erogeneity. However, our objective is specifically geared towardspruning the receptive fields that influence aggregation, granting ourapproach greater versatility. Additionally, our method can betteraid in model storage and expedite training.",
  "CONCLUSION": "In this paper, we first propose the one node one receptive fieldconcept in heterophilic graph modeling. We further establish theheterophily snowflake hypothesis philosophy for GNNs. To achievethis, we adopt heterophily-aware early stopping to let certain nodes have their own receptive fields. In general, we consistently ob-serve snowflakes\" across numerous deep architectures. Further-more, upon testing virtually every type of heterogenous design, wehave discovered that our algorithm adeptly integrates with variousframeworks, significantly enhancing their performance.",
  "Emmanuel Abbe. 2017. Community detection and stochastic block models: recentdevelopments. The Journal of Machine Learning Research 18, 1 (2017), 64466531": "Sami Abu-El-Haija, Amol Kapoor, Bryan Perozzi, and Joonseok Lee. 2020. N-gcn: Multi-scale graph convolution for semi-supervised node classification. Inuncertainty in artificial intelligence. PMLR, 841851. Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Nazanin Alipourfard, KristinaLerman, Hrayr Harutyunyan, Greg Ver Steeg, and Aram Galstyan. 2019. Mixhop:Higher-order graph convolutional architectures via sparsified neighborhoodmixing. In international conference on machine learning. PMLR, 2129.",
  "Enyan Dai, Shijie Zhou, Zhimeng Guo, and Suhang Wang. 2022. Label-WiseGraph Convolutional Network for Heterophilic Graphs. In Learning on GraphsConference. PMLR, 261": "Simon S Du, Kangcheng Hou, Russ R Salakhutdinov, Barnabas Poczos, RuosongWang, and Keyulu Xu. 2019. Graph neural tangent kernel: Fusing graph neuralnetworks with graph kernels. Advances in neural information processing systems32 (2019). Talya Eden, Shweta Jain, Ali Pinar, Dana Ron, and C Seshadhri. 2018. Provableand practical approximations for the degree distribution using sublinear graphsamples. In Proceedings of the 2018 World Wide Web Conference. 449458.",
  "KDD 24, August 2529, 2024, Barcelona, SpainKun Wang et al": "Zijian Hu, Zhengyu Yang, Xuefeng Hu, and Ram Nevatia. 2021. Simple: Similarpseudo label exploitation for semi-supervised classification. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition. 1509915108. Wei Huang, Yayong Li, Weitao Du, Jie Yin, Richard Yi Da Xu, Ling Chen, andMiao Zhang. 2021. Towards deepening graph neural networks: A GNTK-basedoptimization perspective. arXiv preprint arXiv:2103.03113 (2021).",
  "Qimai Li, Zhichao Han, and Xiao-Ming Wu. 2018. Deeper insights into graphconvolutional networks for semi-supervised learning. In Thirty-Second AAAIConference on Artificial Intelligence": "Xiang Li, Renyu Zhu, Yao Cheng, Caihua Shan, Siqiang Luo, Dongsheng Li, andWeining Qian. 2022. Finding global homophily in graph neural networks whenmeeting heterophily. In International Conference on Machine Learning. PMLR,1324213256. Derek Lim, Felix Hohne, Xiuyu Li, Sijia Linda Huang, Vaishnavi Gupta, OmkarBhalerao, and Ser Nam Lim. 2021. Large scale learning on non-homophilousgraphs: New benchmarks and strong simple methods. Advances in Neural Infor-mation Processing Systems 34 (2021), 2088720902.",
  "Meng Liu, Zhengyang Wang, and Shuiwang Ji. 2021. Non-local graph neuralnetworks. IEEE transactions on pattern analysis and machine intelligence 44, 12(2021), 1027010276": "Zirui Liu, Kaixiong Zhou, Zhimeng Jiang, Li Li, Rui Chen, Soo-Hyun Choi, andXia Hu. 2023. DSpar: An Embarrassingly Simple Strategy for Efficient GNNTraining and Inference via Degree-Based Sparsification. Transactions on MachineLearning Research (2023). Sitao Luan, Chenqing Hua, Qincheng Lu, Jiaqi Zhu, Mingde Zhao, ShuyuanZhang, Xiao-Wen Chang, and Doina Precup. 2022. Revisiting heterophily forgraph neural networks. Advances in neural information processing systems 35(2022), 13621375.",
  "Junfu Wang, Yuanfang Guo, Liang Yang, and Yunhong Wang. 2023. Heterophily-Aware Graph Attention Network. arXiv preprint arXiv:2302.03228 (2023)": "Kun Wang, Guohao Li, Shilong Wang, Guibin Zhang, Kai Wang, Yang You, Xi-aojiang Peng, Yuxuan Liang, and Yang Wang. 2023. The Snowflake Hypoth-esis: Training Deep GNN with One Node One Receptive field. arXiv preprintarXiv:2308.10051 (2023). Kun Wang, Yuxuan Liang, Xinglin Li, Guohao Li, Bernard Ghanem, Roger Zim-mermann, Huahui Yi, Yudong Zhang, Yang Wang, et al. 2023. Brave the Windand the Waves: Discovering Robust and Generalizable Graph Lottery Tickets.IEEE Transactions on Pattern Analysis and Machine Intelligence (2023). Kun Wang, Yuxuan Liang, Pengkun Wang, Xu Wang, Pengfei Gu, Junfeng Fang,and Yang Wang. 2022. Searching Lottery Tickets in Graph Neural Networks: ADual Perspective. In The Eleventh International Conference on Learning Represen-tations. Kun Wang, Zhengyang Zhou, Xu Wang, Pengkun Wang, Qi Fang, and YangWang. 2022. A2DJP: A two graph-based component fused learning frameworkfor urban anomaly distribution and duration joint-prediction. IEEE Transactionson Knowledge and Data Engineering (2022).",
  "Shiwen Wu, Fei Sun, Wentao Zhang, Xu Xie, and Bin Cui. 2022. Graph neuralnetworks in recommender systems: a survey. Comput. Surveys 55, 5 (2022), 137": "Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, and Tieniu Tan. 2019.Session-based recommendation with graph neural networks. In Proceedings ofthe AAAI conference on artificial intelligence, Vol. 33. 346353. Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, andS Yu Philip. 2020. A comprehensive survey on graph neural networks. IEEEtransactions on neural networks and learning systems 32, 1 (2020), 424.",
  "Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How powerfulare graph neural networks? arXiv preprint arXiv:1810.00826 (2018)": "Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichiKawarabayashi, and Stefanie Jegelka. 2018. Representation learning on graphswith jumping knowledge networks. In International conference on machine learn-ing. PMLR, 54535462. Yujun Yan, Milad Hashemi, Kevin Swersky, Yaoqing Yang, and Danai Koutra.2022. Two sides of the same coin: Heterophily and oversmoothing in graphconvolutional neural networks. In 2022 IEEE International Conference on DataMining (ICDM). IEEE, 12871292.",
  "Tianmeng Yang, Yujing Wang, Zhihan Yue, Yaming Yang, Yunhai Tong, and JingBai. 2022. Graph pointer neural networks. In Proceedings of the AAAI conferenceon artificial intelligence, Vol. 36. 88328839": "Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec.2019. Gnnexplainer: Generating explanations for graph neural networks. Ad-vances in neural information processing systems 32 (2019). Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and JureLeskovec. 2018. Hierarchical graph representation learning with differentiablepooling. Advances in neural information processing systems 31 (2018).",
  "Xin Zheng, Yixin Liu, Shirui Pan, Miao Zhang, Di Jin, and Philip S Yu. 2022.Graph neural networks for graphs with heterophily: A survey. arXiv preprintarXiv:2202.07082 (2022)": "Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu,Lifeng Wang, Changcheng Li, and Maosong Sun. 2020. Graph neural networks:A review of methods and applications. AI open 1 (2020), 5781. Zhengyang Zhou, Gengyu Lin, Kuo Yang, LEI BAI, Yang Wang, et al. 2022. GReTo:Remedying dynamic graph topology-task discordance via target homophily. InThe Eleventh International Conference on Learning Representations. Jiong Zhu, Ryan A Rossi, Anup Rao, Tung Mai, Nedim Lipka, Nesreen K Ahmed,and Danai Koutra. 2021. Graph neural networks with heterophily. In Proceedingsof the AAAI conference on artificial intelligence, Vol. 35. 1116811176. Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and DanaiKoutra. 2020. Beyond homophily in graph neural networks: Current limitationsand effective designs. Advances in neural information processing systems 33 (2020),77937804.",
  "Cornell1835571,70350.13Texas1835741,70350.09Wisconsin2519161,70350.19Chameleon2,27762,7922,32550.23Squirrel5,201 396,8462,08950.22Actor7,60053,4119322000.22": "Backbone Selection for Validation. To systematically validatethe capability of our Heterophily Snowflake Hypothesis,\" we haveselected 10 backbone architectures. We classify our frameworkinto three main categories: Non-local Neighbor Extension, GNNArchitecture Refinement, and General Framework. By summarizingthese three categories of work, we can more systematically verifythe universality of the Heterophily Snowflake Hypothesis.\" Ourmodel categorization is presented as .High-order Neighbor Mixing. This approach allows nodes toconsider neighbors beyond just the immediate, one-hop neighbors.This seems especially beneficial for heterophilic graphs, allowingthem to integrate information from nodes that are more than oneedge away.",
  ": Illustration of backbones adopted in our paper": "Potential Neighbor Discovery. Instead of just looking at theinherent structure of the graph, this method redefines what a \"neigh-bor\" might be. It constructs a potential neighbor set based on somemetric function distance in a latent space, rather than just topologi-cal closeness.Adaptive Message Aggregation. Given a set of neighbors, theprimary challenge in heterophilic graphs is to effectively aggregateor combine their information. This method seems to alter the aggre-gation step by weighing the importance of each neighbor differently.The goal is to differentiate information from similar neighbors (ofthe same class) versus dissimilar neighbors (of different classes).Ego-Neighbor Separation. The concept of ego-neighbor sepa-ration emphasizes differentiating ego node representations fromaggregated neighbor nodes for clearer class label distinctions. Thisapproach involves detaching self-loop connections in aggregationand modifying the update function to favor non-mixing operations.Inter-layer Combination. Inter-layer combination in GNNsdiverges from adaptive message aggregation and ego-neighborseparation methods. Instead of focusing on individual layers, it em-phasizes layer-wise operations to enhance GNNs representationcapabilities in heterophily settings. The strategys foundation isthat while shallow GNN layers capture local information, deeperlayers grasp broader, global data through repeated neighbor interac-tions. In a heterophily context, neighbors having similar data mightspan both immediate vicinity and distant global structures. Thus,integrating representations from every layer optimally leveragesdiverse neighbor scopes, considering both localized and broad struc-tural characteristics, leading to more robust heterophilic GNNs.",
  "Backward to update GNN and proxy model P": "Main experiments (RQ1). In this setup, we integrate Hetero-Sinto mainstream heterophilic GNNs, focusing on non-local neigh-bor extensions (2 backbones), GNN architecture refinements (6backbones) and general designs (2 backbones). Depth scalability experiments (RQ2). We delve into varyingdepths of GNN architectures. The aim is to determine whether theinclusion of Hetero-S enables these GNNs to maintain or enhanceperformance as the network goes deeper, avoiding issues likevanishing gradients or over-smoothing. Comparative analysis with traditional Snowflake Hypothe-ses (RQ3). Here, we juxtapose Hetero-S with its predecessors,SnoHv1 and SnoHv2, on heterophilic graphs. The experimentis tailored to elucidate if Hetero-S presents a more harmoniousalignment with the intricacies of heterophily, potentially leadingto better model interpretations and results. Efficiency comparison with pruning algorithms (RQ4). Wecompare Hetero-S with current SOTA graph sparsification meth-ods (e.g., UGS , SNIP , DSpar ) with a focus on two keyaspects: (1) whether Hetero-S can achieve the desired sparsitywithout performance compromise, and (2) whether Hetero-S cangenuinely accelerate model computations.",
  "MGNN / +83.84/- 83.50/84.29 82.62/83.98 81.10/81.56": "As shown in , we list observations (1) Across most backbones,the incorporation of Hetero-S generally improved the predictionresults, as seen by the higher scores in the columns with the +. Thisshowcases the potential benefits of integrating Hetero-S into thesearchitectures. (2) Backbones exhibit varied performance acrossdifferent layer depths. Notably, certain architectures, such as Geom-GCN, experience significant fluctuations in scores as the layersincrease. This suggests that while increased depth might amplifythe capabilities of some models, it can be counterproductive forothers, particularly for GCNII and JKNet. Intriguingly, we observethat upon incorporating Hetero-S, the backbones performancecan even revert to optimal levels exhibited by the original fewlayers, including the 2-layer structure. This strongly validates theheterophily snowflake hypothesis we propose.We also present the performance curves for tests incorporatingHetero-S with the original backbone. As shown in , we canobserve the test performance curves for traditional backbones andthe enhancement achieved by integrating our Hetero-S framework.Each graph plots the accuracy over the number of epochs, dividedinto two scenarios: one for the original backbone (in red) and onefor the backbone with Hetero-S (in blue).For the Chameleon dataset, when comparing the 4-layer, 6-layer, and 8-layer GAT, GCN, GPNN, and JKNet, we can see aconsistent pattern. The addition of Hetero-S generally leads to animprovement in maximum test accuracy across all backbone archi-tectures and depths. This is particularly evident in scenarios wherethe original backbone has a more volatile or lower accuracy trajec-tory. For instance, in the 4-layer GAT, the peak accuracy improvesfrom 73.53% to 75.45% with the inclusion of Hetero-S. Similar im-provements are observed in 6-layer and 8-layer configurations. The6-layer GAT sees an increase from 73.67% to 76.22%, and the 8-layerfrom 85.60% to 86.93%. The GCN models also show enhancementwith Hetero-S, albeit with a more significant impact on the 4-layerand 6-layer models than on the 8-layer variant. Meanwhile, theGPNN models reflect a notable benefit from Hetero-S at all depths,with the 4-layer model showing an increase from 67.76% to 68.36%,the 6-layer from 55.29% to 58.24%, and the 8-layer from 54.58% to57.57%. JKNet architectures display similar trends, with the 4-layermodels accuracy increasing from 72.59% to 73.49%, the 6-layerfrom 71.93% to 74.59%, and the 8-layer from 74.19% to 74.37%. Theseresults suggest that Hetero-S provides a consistent and noteworthy",
  ": The performance of backbones and the results after adding Hetero-S (+)": "algorithms approach to leveraging heterogeneity in data alignsexceptionally well with the sophisticated architectures of GCNII.Furthermore, the SGC models also show improvements in perfor-mance, indicating the HES algorithms robustness across different levels of model complexity. This reaffirms its utility as a versatileenhancer of graph network effectiveness. Notably, the performanceboosts are not solely dependent on depth but also show a trend ofincremental gains with increasing complexity, up to a point where",
  "FPERFORMANCE COMPARISON WITHCONVENTIONAL SNOH": "In this section, our focus is on a systematic examination of theperformance merits and limitations of HES, in contrast to conven-tional snowflake settings. To facilitate this analysis, we choose sixdistinct heterophilic graph benchmarks as foundational models forGNNs, specifically: Texas, Wisconsin, Cornell, Squirrel, Chameleon,and Actor. Notably, the depth of heterophilic GNNs is typicallyobserved to not exceed five layers, as indicated by . Hence,within the framework of our heterophilic data approach, we delib-erately confine the depth of our network to 6 and 8 layers. Thisstrategic limitation enables us to conduct a thorough and precisecomparative analysis of the performances of SnoHv1, SnoHv2, andHetero-S. In order to conduct a comprehensive comparison, we em-ploy both a homophilic (JkNet) and a heterophilic (Mixhop) GNNas the backbones for our experimental analysis.",
  "GPARAMETER SENSITIVITY ANALYSIS": "In this section, we detail how we determine the filtering threshold in all experiments and how influences the performance of HES.In practice, we want to avoid both excessively large , as it couldlead to premature removal of too large a receptive field duringearly stopping, resulting in suboptimal model performance, andexcessively small , as they could cause central nodes to absorb toomuch heterophilic information from multi-hop neighbors. There-fore, we search the most suitable in a limited range {1e-2, 1e-4,1e-6, 1e-8, 1e-16} for all experiments. To further demonstrate howsensitive our method is to , we test the performance of HES onSquirrel/Chameleon with different filtering threshold settings. Asshown in , we can observe (1) the optimal performance ofHES requires an appropriate choice of . Generally, as increases,HESs performance often initially improves (corresponding to in-creased removal of receptive fields), then declines (correspondingto excessive removal of receptive fields); (2) Overall, HES is rela-tively insensitive to the choice of . For instance, on GCN, HESperformance varies by no more than 1.04% and 1.34%.",
  "HCASE STUDY": "In this section, we endeavor to investigate the efficacy of Hetero-Sfrom a micro-perspective through the analysis of two case studies.We choose the superpixel graphs of MNIST and Squirrel asbenchmarks to observe the visualized outcomes. As depicted in, the following observations can be made: Left. On the MNIST dataset, we observe that as the depth of theGNN increases, the edges in the black regions are progressivelypruned. At the deepest layer, the adjacency matrix aligns pre-cisely with the white regions, capturing the edge information of",
  "layer": ": (Left.) Visualization of the subgraphs extracted by applying HES to an 8-layer GCN with MNIST. Original images and graphs aredisplayed on the first and second columns. Visualization of the subgraphs extracted by applying HES to a 4-layer GCN with Texas. The central(Right.) Prediction results for the central node C (The label is 2) using different algorithms.",
  "GCN+62.2563.1163.3863.2362.34MixHop+67.7469.0468.7869.0867.22JKNet+70.1770.3872.3771.6970.55MGNN+60.1962.0961.7661.9260.83": "the prediction areas, thereby significantly aiding the models pre-dictive capability. This validates that the HES algorithm adeptlycaptures the most critical information for prediction and elimi-nates redundant information. Right. In our analysis of the Squirrel, we find that the conven-tional SnoHv2, after pruning, increases its homophily ratio from0.308 to 0.364. Conversely, Hetero-S enhances this ratio to 0.429,proving that the Hetero-S scheme improves the models likelihoodof aggregating similar labels, thereby boosting the effectivenessof information aggregation. Hetero-S achieves a dual victory ofgreater graph sparsity and accuracy compared to SnoH-v2, fur-ther attesting to the superior capabilities of HES over SnoH-v2in heterophilic graph scenarios. For instance, HES consistentlysurpasses SnoH-v2 by substantial performance margins acrossSquirrel dataset (0.8% 2.5% on accuracy and 6.1% 8.9% on graph sparsity).",
  "Graph Neural Networks (GNNs). GNNs have emerged as a promi-nent subfield in machine learning, specifically tailored to manageand analyze graph-structured data . In general, GNNs owe": "their efficacy to a distinct message-passing mechanism, whichseamlessly integrates topological structures with node characteris-tics to yield richer graph representations. This process is best de-scribed by the mathematical expression () = (, (1), ()).In this equation, () stands for the node embedding after itera-tions of GNN aggregation. Meanwhile, represents the messagepropagation function, and () denotes the trainable parameters ata given layer . The escalating enthusiasm surroundingGNNs has catalyzed the development of a myriad of propagationtechniques and model variants , whichhave significantly broadened the arsenal of tools available for graph-oriented learning and exploration.Graph Pooling & Clustering. Graph pooling and clustering de-vote to reducing the computational burden of GNNs by applyingpruning or compressing methods , which arehighly relevant to our research. We divide existing techniques intotwo categories. (1) Sampling-based methods aims at selecting themost expressive nodes or edges from the original graph to constructa new subgraph . Though efficient, the dropping ofnodes/edges sometimes results in severe information loss and iso-lated subgraphs, which may cripple the performance of GNNs .(2) Clustering-based methods learns how to cluster the whole nodesin the original graph, and produces a informative graph where theclusters are node sets , which can remedy the aforemen-tioned information loss problem.Heterophilic GNNs. Existing heterophilic GNNs primarily fallinto two categories: non-local neighbor extension and GNN archi-tecture refinement . The former emphasizes expanding theneighborhood scope, achieved via high-order neighbor informationmixing and potential neighbor discovery .The latter, delves into enhancing GNNs expressive power specifi-cally for heterophilic graphs. Strategies include adaptive messageaggregation , ego-neighbor separation , and layer-wise operations to optimize node representation quality.",
  "JPROOF": "In this subsection, we present a detailed proof. The notationEA (,) [] represents the expected pattern of the adjacencymatrix. As depicted in main part, considering in subsequent layers,the HES algorithm is applied such that remains constant whileheterophilic nodes are pruned, thus reducing , the product of thesmallest eigenvalues of EA (,,) [G] is given by:",
  "() + ()": "1 () + () + () (18)Consider the case > are both functions of ,, and suppose = 1/(( 1)2), = 1/( 2). In this scenario, as the layer depthincreases, both and undergo decay, yet maintain the condition > . Consequently, the product of the smallest eigenvalues canbe expressed as:",
  ") sin(/": "), which concludes the proof.We posit that this phenomenon is not solely confined to bi-nary classification contexts. Even in multi-class scenarios, a similarpattern is observed. Utilizing the HES algorithm, we can adeptly fa-cilitate the divergence of the GNTK, as opposed to its convergenceto zero, thereby enhancing the efficacy of network training."
}