{
  "ABSTRACT": "Conversion rate prediction is critical to many online applicationssuch as digital display advertising. To capture dynamic data dis-tribution, industrial systems often require retraining models onrecent data daily or weekly. However, the delay of conversion be-havior usually leads to incorrect labeling, which is called delayedfeedback problem. Existing work may fail to introduce the correctinformation about false negative samples due to data sparsity anddynamic data distribution. To directly introduce the correct feed-back label information, we propose an Unbiased delayed feedbackLabel Correction framework (ULC), which uses an auxiliary modelto correct labels for observed negative feedback samples. Firstly,we theoretically prove that the label-corrected loss is an unbiasedestimate of the oracle loss using true labels. Then, as there are noready training data for label correction, counterfactual labeling isused to construct artificial training data. Furthermore, since coun-terfactual labeling utilizes only partial training data, we designan embedding-based alternative training method to enhance per-formance. Comparative experiments on both public and privatedatasets and detailed analyses show that our proposed approacheffectively alleviates the delayed feedback problem and consistentlyoutperforms the previous state-of-the-art methods.",
  "Corresponding author": "Permission to make digital or hard copies of part or all of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for third-party components of this work must be honored.For all other uses, contact the owner/author(s).KDD 23, August 610, 2023, Long Beach, CA, USA 2023 Copyright held by the owner/author(s).ACM ISBN 979-8-4007-0103-0/23/08.",
  "INTRODUCTION": "Predicting the probability of users clicking or converting on ads oritems is critical to many online applications, such as digital displayadvertising and recommender systems. Take online advertising asan example. Generally, ad delivery platforms provide advertiserswith several optional billing models, such as Cost Per thousandiMpressions (CPM), Cost Per Click (CPC) and Cost Per Acquisition(CPA), in which CPA is preferred as the conversion is closer toadvertisers profits. For the CPA model, predicting the click rateand conversion rate of users to the placed advertisements is thekey to achieving more revenue, which are also known as the Click-Through Rate (CTR) and Conversion Rate (CVR) prediction tasks.These two tasks have received increasing attention from industryand academia in recent years .Model freshness is important for CTR and CVR prediction mod-els as user interests change dynamically. A common strategy tokeep fresh in the industry is to retrain the model daily or weeklyon all collected data. This simple strategy can be effective for CTRprediction. However, the delay of conversion behavior makes itchallenging to ensure the freshness of CVR models, which is calledDelayed Feedback Problem. Unlike click behavior happening quicklywithin minutes of impression, conversions occur much more slowlyafter days, sometimes taking up to weeks . This leads that theground truth of recently clicked but unconverted samples is un-known as they may convert in the future.A vanilla solution is to treat all these unconverted samples asnegative feedback, which will cause some positive samples (i.e., realconversions) to be mislabeled, leading to the false negative problem.These mislabeled samples can significantly damage the performanceof the CVR prediction model as they are important to the modelfreshness. Another obvious solution is to wait for a long time untilthe labels are accurate enough. However, this means that the datais old, which conflicts with the purpose of keeping models fresh.Thus, the delayed feedback problem reflects a trade-off betweenmodel freshness and label correctness. Therefore, handling freshunconverted data with unknown labels is an important challengefor CVR prediction.",
  "KDD 23, August 610, 2023, Long Beach, CA, USAYifan Wang et al": "Wentian Bao, Hong Wen, Sha Li, Xiao-Yang Liu, Quan Lin, and Keping Yang.2020. GMCM: Graph-Based Micro-Behavior Conversion Model for Post-ClickConversion Rate Estimation. In Proceedings of the 43rd International ACM SIGIRConference on Research and Development in Information Retrieval (Virtual Event,China) (SIGIR 20). Association for Computing Machinery, New York, NY, USA,22012210. Olivier Chapelle. 2014. Modeling Delayed Feedback in Display Advertising. InProceedings of the 20th ACM SIGKDD International Conference on KnowledgeDiscovery and Data Mining (New York, New York, USA) (KDD 14). Associationfor Computing Machinery, New York, NY, USA, 10971105. Yu Chen, Jiaqi Jin, Hui Zhao, Pengjie Wang, Guojun Liu, Jian Xu, and Bo Zheng.2022. Asymptotically Unbiased Estimation for Delayed Feedback Modeling viaLabel Correction. In Proceedings of the ACM Web Conference 2022 (Virtual Event,Lyon, France) (WWW 22). Association for Computing Machinery, New York,NY, USA, 369379. Siyu Gu, Xiang-Rong Sheng, Ying Fan, Guorui Zhou, and Xiaoqiang Zhu.2021. Real Negatives Matter: Continuous Training with Real Negatives forDelayed Feedback Modeling. In Proceedings of the 27th ACM SIGKDD Confer-ence on Knowledge Discovery & Data Mining (Virtual Event, Singapore) (KDD21). Association for Computing Machinery, New York, NY, USA, 28902898. Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.DeepFM: A Factorization-Machine Based Neural Network for CTR Prediction.In Proceedings of the 26th International Joint Conference on Artificial Intelligence(Melbourne, Australia) (IJCAI17). AAAI Press, 17251731. Siyuan Guo, Lixin Zou, Yiding Liu, Wenwen Ye, Suqi Cheng, Shuaiqiang Wang,Hechang Chen, Dawei Yin, and Yi Chang. 2021. Enhanced Doubly Robust Learn-ing for Debiasing Post-Click Conversion Rate Estimation. In Proceedings of the44th International ACM SIGIR Conference on Research and Development in Informa-tion Retrieval (Virtual Event, Canada) (SIGIR 21). Association for Computing Ma-chinery, New York, NY, USA, 275284. Yuyao Guo, Haoming Li, Xiang Ao, Min Lu, Dapeng Liu, Lei Xiao, Jie Jiang, andQing He. 2022. Calibrated Conversion Rate Prediction via Knowledge Distillationunder Delayed Feedback in Online Advertising. In Proceedings of the 31st ACMInternational Conference on Information & Knowledge Management (Atlanta, GA,USA) (CIKM 22). Association for Computing Machinery, New York, NY, USA,39833987. Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Opti-mization. In 3rd International Conference on Learning Representations, ICLR 2015,San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, Yoshua Bengioand Yann LeCun (Eds.). Shunsuke Kitada, Hitoshi Iyatomi, and Yoshifumi Seki. 2019. Conversion Predic-tion Using Multi-Task Conditional Attention Networks to Support the Creationof Effective Ad Creatives. In Proceedings of the 25th ACM SIGKDD InternationalConference on Knowledge Discovery & Data Mining (Anchorage, AK, USA) (KDD19). Association for Computing Machinery, New York, NY, USA, 20692077. Sofia Ira Ktena, Alykhan Tejani, Lucas Theis, Pranay Kumar Myana, Deepak Dilip-kumar, Ferenc Huszr, Steven Yoo, and Wenzhe Shi. 2019. Addressing DelayedFeedback for Continuous Training with Neural Networks in CTR Prediction. InProceedings of the 13th ACM Conference on Recommender Systems (Copenhagen,Denmark) (RecSys 19). Association for Computing Machinery, New York, NY,USA, 187195. Haoming Li, Feiyang Pan, Xiang Ao, Zhao Yang, Min Lu, Junwei Pan, Dapeng Liu,Lei Xiao, and Qing He. 2021. Follow the Prophet: Accurate Online Conversion RatePrediction in the Face of Delayed Feedback. In Proceedings of the 44th InternationalACM SIGIR Conference on Research and Development in Information Retrieval(Virtual Event, Canada) (SIGIR 21). Association for Computing Machinery, NewYork, NY, USA, 19151919. Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H. Chi. 2018.Modeling Task Relationships in Multi-Task Learning with Multi-Gate Mixture-of-Experts. In Proceedings of the 24th ACM SIGKDD International Conferenceon Knowledge Discovery & Data Mining (London, United Kingdom) (KDD 18).Association for Computing Machinery, New York, NY, USA, 19301939. Xiao Ma, Liqin Zhao, Guan Huang, Zhi Wang, Zelin Hu, Xiaoqiang Zhu, and KunGai. 2018. Entire Space Multi-Task Model: An Effective Approach for EstimatingPost-Click Conversion Rate. In The 41st International ACM SIGIR Conference onResearch & Development in Information Retrieval (Ann Arbor, MI, USA) (SIGIR18). Association for Computing Machinery, New York, NY, USA, 11371140. Junwei Pan, Yizhi Mao, Alfonso Lobos Ruiz, Yu Sun, and Aaron Flores. 2019.Predicting Different Types of Conversions with Multi-Task Learning in OnlineAdvertising. In Proceedings of the 25th ACM SIGKDD International Conference onKnowledge Discovery & Data Mining (Anchorage, AK, USA) (KDD 19). Association",
  "RELATED WORK2.1CVR Prediction": "The CVR prediction task shares many similarities with the widelystudied CTR prediction task. They both predict the probability ofa user performing a certain behavior on an ad or an item. Besides,their inputs are generally the same. Generally, the model structuredesigned for the CTR task can also be applied to CVR prediction.Thus, existing research on CVR prediction focuses more on thedifferences between CVR and CTR.There are three main challenges for CVR prediction. First, thedata for the CVR task are often more sparse than the CTR task. Ex-isting research mitigates this problem through multi-task learning and pre-training . Second, CVR prediction suffers fromselection bias. The CVR prediction model is trained on click samplesbut infers for all exposure samples during inference. Differencesin exposure distribution and click distribution lead to selectionbias, which existing work addresses through entire sample spacemodeling , inverse propensity score , and dou-bly robust methods . Third, conversions do not happen asimmediately as clicks, with some conversions taking days or even aweek. This could result in some positive samples that have not yetconverted being incorrectly treated as negative samples. Existingstudies address it by delay time modeling or importancesampling , which we will detail in Session 2.2. In this work,we focus on the third challenge, the delayed feedback problem, andleave the extension of our method to other problems for futurework.",
  "Delayed Feedback": "Here we only focus on the delayed feedback problem in the offlinesetting.To our knowledge, the delayed feedback problem was first stud-ied by DFM . DFM models the delay time explicitly. It assumesthat the delay time obeys an exponential distribution and thenoptimizes the maximum likelihood of the currently observed datalabels. extends this approach further by using a non-parametricapproach to modeling delay time. A drawback of the above methodsis that they try to optimize the observed conversion informationinstead of directly optimizing the true conversion information.In contrast to explicitly modeling delay time, recent work attempts to address the delay feedback problem by constructingunbiased estimates of the oracle loss that uses true labels. FSIW leverages importance sampling to construct an unbiased loss.Intuitively, it increases the weight of observed positive samples anddecreases the weight of potentially negative samples as these sam-ples may be mislabeled. Besides, nnDF assumes that the labelsof samples before a time window are accurate and then uses thesesamples to correct for the biased loss of the whole training data.A drawback of the above methods is that, despite their theoreticalguarantee of unbias, they might fail to introduce information aboutthe correct positive sample for each specific false negative sample.For FSIW, it only reduces the weight of the mislabeled samples butcannot introduce the information of the corrected sample, i.e., theweight of the corresponding correct sample is still zero. For nnDF,it does not process recent samples, and therefore cannot introduceinformation about the correct samples among them. This problem",
  "Unbiased Delayed Feedback Label Correctionfor Conversion Rate PredictionKDD 23, August 610, 2023, Long Beach, CA, USA": "Weinan Zhang, Jiarui Qin, Wei Guo, Ruiming Tang, and Xiuqiang He. 2021.Deep Learning for Click-Through Rate Estimation. In Proceedings of the ThirtiethInternational Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event /Montreal, Canada, 19-27 August 2021, Zhi-Hua Zhou (Ed.). ijcai.org, 46954703. Xiao Zhang, Haonan Jia, Hanjing Su, Wenhan Wang, Jun Xu, and Ji-Rong Wen.2021. Counterfactual Reward Modification for Streaming Recommendation withDelayed Feedback. In Proceedings of the 44th International ACM SIGIR Conferenceon Research and Development in Information Retrieval (Virtual Event, Canada)(SIGIR 21). Association for Computing Machinery, New York, NY, USA, 4150. Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, YanghuiYan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep Interest Network for Click-Through Rate Prediction. In Proceedings of the 24th ACM SIGKDD InternationalConference on Knowledge Discovery & Data Mining (London, United Kingdom)(KDD 18). Association for Computing Machinery, New York, NY, USA, 10591068.",
  "PRELIMINARIES3.1Notations": "In online advertising platforms, the user behaviors for the dis-play ads are logged to train the CVR prediction model. Supposewe collect training data D at timestamp , i.e., we can obtainall the user behaviors and corresponding features before . LetD = {(, ,,,) , = 1, 2, ...}. The notation denotes the-th sample. Each sample represents a click record of users. For the-th sample (, ,,,), denotes the feature informationof this sample. denotes the click timestamp. is a binary valuethat denotes whether the clicked ad has a further conversion beforethe observed timestamp. If = 1, will record the correspond-ing conversion timestamp. Otherwise, is empty. denotes thetime elapsed from to , i.e., .Let denote whether the-th sample will finally lead to a conver-sion. Note that we cannot wait forever for the possible conversionto happen. In practice, a long time window is applied dependingon the specific scenario, e.g., one month for Criteo . Only con-versions within the time window after clicks are considered valid.In other words, if , then = . If < , is unknown.Thus, is not included in the training data D. For test data, wecan wait enough time to obtain for evaluation.For easy reading, the notations are summarized in .",
  "Task Formulation": "The conversion rate is defined as the probability of the final conver-sion for a clicked ad, i.e., = ( = 1|). The CVR predictiontask under delayed feedback is aimed to use the training data Dcollected at to predict for the clicked ads after .Note that training samples clicked before (i.e., )can be fed directly into the model without any processing as theirlabels are correct. Since the core issue for delayed feedback is howto handle the fresh data with unknown labels, we omit these data",
  "Vanilla and Oracle Loss": "Next, we introduce the two basic loss functions in the delayedfeedback problem. Note that CVR prediction is essentially a binaryclassification problem. Generally, the cross-entropy loss is adoptedfor training the CVR model. Let (;) denote the CVR model withtrainable parameters . Suppose we can now foresee the future andobtain an ideal dataset D, which contains for each sample. Thenthe cross-entropy loss can be written as:",
  "=1( log (;) + (1 ) log(1 (;))) ,": "(1)Equation (1) is called the oracle loss L as we suppose thefinal conversion label for each click record is available.However, in practice, we cannot obtain the oracle label foreach sample at the data collection timestamp . If we ignore thedelayed feedback and replace the oracle label with the observedlabel , we can get the vanilla loss L for CVR model training:",
  "UNBIASED LABEL CORRECTION FORDELAYED FEEDBACK PROBLEM4.1Overall Framework": "We propose an Unbiased delayed feedback Label Correction frame-work (ULC), which aims to address the delay feedback problemin CVR prediction through label correction. The key idea is thatdelayed feedback leads to and only leads to incorrect labels. If weare able to identify all the incorrect labels and correct them, we candirectly calculate the oracle loss. illustrates the overall framework of ULC, which consistsof a label correction (LC) model and a CVR prediction model. TheLC model is designed to predict the probability that an observedunconverted training sample will finally convert, which is used tocalculate our proposed label-corrected loss for CVR model training.We prove in .2 that if the LC model is accurate enough,the label-corrected loss is an unbiased estimate of the oracle loss.The next question is how to learn an accurate LC model. As thereis no ready training data for the LC model, we leverage counterfac-tual labeling to generate training data. It constructs the artificialdata by imagining a counterfactual data collection time < ,the details of which will be introduced in .3. However,counterfactual labeling suffers from some problems, such as inade-quate utilization of the whole training data, which we analyze in.4. To mitigate this problem, we further apply alternativetraining to re-train these two models, enhancing the performance",
  "= [L]": "The advantage of LC loss over the previous unbiased loss (e.g.,FSIW and nnDF) is that it directly complements the information ofthe correct sample corresponding to the false negative samples, i.e., (1 ) log (;). The existing unbiased losses complementthe corresponding information in indirect ways, which is stronglyinfluenced by data sparsity and data dynamics. For example, inpractice, FSIW complements the correct information by increasingthe weights of observed positive samples similar to the false nega-tive sample. However, for some fresh false negative samples, theremay not exist similar observed positive samples due to the sparsityand dynamics of CVR data. In this case, these indirect methodscannot effectively supplement the corresponding positive sample",
  "Data Generation with CounterfactualLabeling": "For the LC model, there is no ready training data. Note that weneed samples with = 0 & = 1 as positive samples and with = 0 & = 0 as negative samples. However, there are onlysamples with = 1 & = 1 and samples with = 0 in the originaldata. To train the LC model, we need to construct artificial samples.We leverage a counterfactual method to generate trainingdata for the LC model. First, we imagine that the training datawas collected at a counterfactual deadline (CD) before the trainingdatas actual deadline (AD), i.e., . The time interval betweenthe CD and the AD is a hyperparameter. Second, the samples thatare clicked but have not converted before the CD are collected astraining data, together with as the elapsed time of these samplesat the CD. Third, we treat the samples with conversion betweenCD and AD as positive samples, i.e., = 1, and others as negativesamples, i.e., = 0. Obviously, there exist some samples convertingafter AD are ignored. Nevertheless, as increases, the proportion ofthese samples keeps getting smaller. The subsequent experimentsdemonstrate that even a relatively short can effectively alleviatethe delayed feedback problem.After data generation, we can train the LC model via the classicalbinary cross-entropy loss. Then the LC model is frozen and utilizedto infer in the above LC loss. Note that the elapsed time at theAD is used instead of when inferring for LC loss. The detaileddata generation procedure is shown in Algorithm 1 (lines 2-14).",
  "Alternative Training": "Although the training data required for the LC model can be con-structed by counterfactual labeling, this method still has somedrawbacks. First, data generation only leverages partial trainingdata, i.e., samples that are clicked before CD and converted afterCD, which may result in the suboptimal performance of LC model.Second, the LC model also suffers somewhat from delayed feedback.Some potential positive samples that have a long delay and convertafter AD may be mistreated as negative samples. Next, we proposean alternative learning based approach to alleviate the first problem.The solution to the second problem we leave to future work.Note that the CVR prediction model is trained on the whole data,and the conversion rate ( = 1|) is similar to the label correctionrate ( = 1|,, = 0). We suppose that the bottom representation(i.e., the embedding layer in ) learned by the CVR model mayfacilitate the learning of the LC model and alleviate the first prob-lem mentioned above. Therefore, we adopt an alternative learningparadigm. After training the CVR prediction model, the bottomrepresentation of the LC model is initialized using the bottom rep-resentation of the CVR prediction model, and then the LC modelis retrained. The retrained LC model can be further used for the",
  "Algorithm 1 Alternative training with data generation": "Input: training data D = {(, ,,,)}, is the timestampwhen the data D are collected, where is the feature vector, is the observed conversion label, is elapsed time since theclick timestamp , is the conversion timestamp. is ahyperparameter denoting the time interval between CD andAD. is the rounds of alternative training.",
  ": return CVR prediction model": "There are some alternatives compared to alternative trainingwith embedding transfer. For example, joint learning is also a com-mon learning paradigm that enables the LC model to leverage theknowledge of the CVR model. Moreover, in addition to utilizing thelearned representation of the CVR model, another easily thought ofoption is to leverage its prediction. It is possible to mine the misla-beled samples in the training data for the LC model using the CVRprediction model as these potential positive samples might havea high predicted CVR. We also conduct experiments and comparethese alternatives in experiment section 5.4.",
  "Dataset and Settings": "5.1.1Datasets. To our knowledge, there exists only one publicdataset widely used in the research of the delayed feedbackproblem in the offline setting. Other public CVR datasets do not havenoticeably delayed feedback or lack enough temporal information.Following the common settings in previous work of usingone public and one private dataset, we also introduce a collectedprivate production dataset.Criteo dataset. This public dataset contains clicks and the cor-responding conversions from Criteo live traffic data. Each samplecorresponds to a single click and is described by several categoricalfeatures and continuous features, with the corresponding conver-sion information, if any. It also includes the timestamps of the clickand the possible conversion behavior. We use this datasets last 23days of data to conduct our experiments. Following previous work, three consecutive weeks of data are leveraged as trainingdata, data of the 22nd day is used for validation and the last dayis for the testing. Note that this dataset tracks conversion behav-ior for each click sample, so the ground truth is available fortesting. For validation, we assume that is unknown and use thelabel-corrected loss for parameter selection. The processed datasetincludes 6,363,085 click samples with a conversion rate of 0.2294.Production dataset. This dataset is collected from a real pro-duction platform with game advertising. In-game payments aretreated as conversions. Specifically, we collected and sampled one-month consecutive user feedback logs. The data format is similarto Criteo, and the last two days are used for validation and testing,respectively. The dataset includes over 2,400,000 click samples witha conversion rate of about 0.005. Statistics are shown in .",
  "Criteo236,363,0850.2294| Production302,400,0000.0050": "5.1.2Evaluation Metrics. We adopt three metrics that are widelyused in CVR prediction tasks . The first metric is area un-der ROC curve (AUC) that measures the pairwise ranking perfor-mance of the CVR prediction model. The second is area under theprecision-recall curve (PRAUC), which also measures the pair-wise ranking performance. The third one is the log loss (LL), whichmeasures the accuracy of the absolute value of the CVR prediction.To further analyze the benefits gained by solving the delayedfeedback problem, we calculate the relative improvements (RI) tothe maximum gain (i.e., the improvement of the oracle model overthe vanilla model) on the above three metrics. For method , the rela-tive improvements on metric () is defined as ( ) ()",
  "DCNV2 : a model using deep and cross networks to learneffective explicit and implicit feature crosses": "5.1.4Implementation Details. The embedding size is 64 for all themethods. The MLP model in all the backbones is a simple three-layermodel with hidden units and Leaky ReLU activation.For AutoInt, the layer number is 3, the number of heads is 2, and theattention size is 64. For DCNV2, we use the stacked structure andone cross-layer. Adam is used as the optimizer, and the learningrate is tuned in the range of [1e-3, 5e-4, 1e-4] with L2 regularizationtuned in [0, 1e-7, 1e-6, 1e-5, 1e-4]. The batch size is set to 1024 forall the methods except nnDF. Given that the nnDF approach cannotapply to batch-wise training, we set the batch size to the size of thewhole training set. For a fair comparison, we consistently use theMLP as the auxiliary model for all methods that rely on auxiliarymodels. The additional hyperparameters for the baselines are fine-tuned. Early stopping is applied to obtain the best parameters. Werepeat each experiment 5 times with different random seeds andreport the average results and make the statistical tests. 1",
  "Overall Performance: RQ1": "From , we can observe that our proposed method ULC out-performs all the baselines and achieves state-of-the-art performanceon all the backbones. There are some further observations. First,the oracle model works significantly better than the vanilla model,which validates that the delayed feedback problem indeed hurts theperformance of CVR model. Second, FSIW performs significantlybetter than the DFM and Vanilla models, which is consistent withprevious studies . However, nnDF is significantly weaker thanVanilla method. It is because nnDF loss requires global dependencecomputation and can only be optimized using full training datawhen updating, which leads to a weaker performance than batch-wise optimization methods. Third, compared to the best baseline,our method shows a significant improvement of 0.76% in the AUCmetric, 1.02% in the PRAUC metric, and 1.85% in the LL metricon average across the four backbones, which demonstrates theeffectiveness of our proposed method.We further analyze the benefits gained by solving the delayedfeedback problem. As shown in , our method narrows the",
  "ULC(ours)0.8391**0.6519**0.4105**0.7431**0.6045**0.8004**": "gap between Vanilla and Oracle by 77.55% in the AUC metric, 66.55%in the PRAUC metric, and 83.13% in the LL metric on average acrossthe four backbones. Compared to the best baseline, our methodshows a significant improvement of 60.3% in the RI-AUC metric,81.43% in the RI-PRAUC metric, and 27.76% in the RI-LL metric onaverage across the four backbones. This shows that our methodcan effectively alleviate the delayed feedback problem. shows the offline performance on the production dataset.For limited space, we only present the results with MLP as the back-bone. It is clearly observed that our proposed method alleviates thedelayed feedback problem and outperforms the two best baselines.To guarantee the reproducibility of our work, and also due to thepage limitation, we make the following further detailed analyseson the public-available dataset.",
  "Analysis on Counterfactual Labeling: RQ2": "In counterfactual labeling, only the samples converted betweenCD and AD are treated as positive samples (i.e., = 1), whichleads to some samples converted after AD being mislabeled asnegative samples. A long time interval between CD and AD canimprove label correctness of counterfactual labeling but reduce datafreshness as only clicked data before CD are utilized. We furtheranalyze the effect of different time intervals.Experimental results using different time intervals are shown in. First, increasing the time interval can effectively increase therecall of counterfactual labeling on positive samples. Second, the VanillaDFMFSIW ULC(ours)0.77 0.78 0.79 0.80 0.81 AUC DFMFSIWULC(ours) 0.030 0.032 0.034 0.036 LL",
  ": Performance comparisons of the proposed methodwith the top two baselines on the private dataset. The back-bone model is MLP. The red dotted line in the right figuredenotes Oracle": "best on the Criteo dataset is around a week. Besides, smaller orlarger will reduce the performance of CVR model. Smaller leadsto more mislabeled samples in the training data of LC model, whichin turn leads to lower performance of CVR model. Larger values,while reducing the mislabeled samples, will make the training dataof the LC model older, which leads to its inability to correct well forfalse negative samples in the CVR training data, since these falsenegative samples are relatively fresh.",
  "Recall": "ULCRecall : Effect of different time intervals between counterfactual deadline (CD) and actual deadline (AD) on the Criteodataset with MLP as the backbone. The blue line represents the performance of ULC and the red line represents the recall ofcounterfactual labeling on positive samples. Larger recall means fewer mislabels in the training data of LC model.",
  "knowledge of CVR prediction model. We conduct experiments onthese schemes": "5.4.1Joint Learning Strategy. An obvious solution is to jointlytrain the LC model and the CVR model, which also enables the LCmodel to utilize the information learned by the CVR model. We usea simple shared-bottom structure to validate the effectivenessof this scheme. The joint loss is a linear weighting of the LC modelloss and the CVR model loss. 5.4.2Prediction-based Alternative Training Strategy. In alternativetraining, in addition to using the learned representation of theCVR model, another easily thought of option is to leverage itsprediction. Note that in .3, we mention that some potentialpositive samples that have a long delay and convert after AD maybe mislabeled as negative samples during counterfactual labeling.To alleviate this problem, we consider using the prediction of theCVR model to mine these potentially positive samples. Intuitively,samples with high predicted CVR are more likely to be potentiallypositive samples. Thus, we design three simple strategies to processthe training data of the LC model: (i) hard strategy, i.e., negativesamples ( = 0) with predicted CVR above a predefined thresholdare treated as positive samples ( = 1); (ii) soft strategy, i.e., usingpredicted CVR as the label for each negative sample; (iii) dropstrategy, i.e., dropping negative samples with predicted CVR abovea predefined threshold from the training data of the LC model, asthe labels of these samples are not reliable. 5.4.3Comparisons on Different Strategies. The results of the abovestrategies on Criteo with MLP as backbone are shown in .Results on AUC are similar to PRAUC and hence omitted. Wehave the following observations: (i) using a simple joint learningscheme cannot improve performance. Instead, there is a large loss ofperformance. The reason is that the inaccurate LC model at the earlytraining stage will mislead the CVR model, which in turn affectsthe subsequent training. (ii) the three prediction-based strategiescannot improve the performance of the CVR model and even causea slight degradation. The potential positive samples after AD havea higher delay than , and the number of these samples is verysmall compared to the number of true negative samples (about 1:50ratio). Using only predicted CVR cannot effectively discover thesesamples; instead, it introduces noise.",
  ": Performance w.r.t. the alternative training rounds.Dataset: Criteo. Backbone: MLP. Note that 0 on the axismeans no alternative training": "ULC. controls the rounds of alternative training, and = 0 meansno alternative training. We conduct experiments using differentvalues of on the Criteo dataset with MLP as the backbone. Asshown in , alternative training once can significantly improvethe performance of the CVR prediction model, which validates theeffectiveness of alternative training. Besides, one round is enough,and more rounds have little impact, which is reasonable since thefirst round that changes the initialization of the LC model fromrandom to the embeddings of CVR model brings more significantchanges than the subsequent rounds. Note that even without alter-native learning, the performance of ULC is still significantly betterthan the best baseline, which reflects the effectiveness of using",
  "The delay time is an important property of delayed feedback. Wefurther analyze the performance of CVR model and LC model onsamples with different delay time": "5.5.1CVR performance on different delay time. For a fair compari-son, we divide the positive samples in the test set into five groups inascending order based on their delay time. Each group has the samenumber of positive samples. Then, each group is combined with allthe negative samples in the test set to form test sets with differentdelay times. In this way, the number of positive and negative sam-ples is the same for different test sets. Further, since the log loss issensitive to the conversion rate, to ensure that the conversion ratein the test set is consistent with the original test set, we duplicatefive copies of each positive sample.Experiment results on the Criteo dataset with MLP as backboneare shown in . We have the following observations: (i) for theOracle model without the delayed feedback problem, its perfor-mance decreases somewhat as the sample delay time increases,which indicates that samples with a long delay time are more likelyto be hard samples. (ii) as the sample delay time increases, the Oraclemodel performs increasingly better than Vanilla, which is becausepositive samples are more likely to be false negative samples as thedelay time increases. (iii) our method significantly outperforms theVanilla model and the best baseline on samples with high delays(e.g., G3, G4, and G5), and our boost increases as the delay timeincreases, which reflects the effectiveness of our method.An interesting phenomenon is that the Vanilla model performsbetter than the Oracle model on samples with short delays (G1). Itmay be because samples with short delays have a higher percentageof observed positive samples than actual positive samples. Furtheranalysis can be found in Appendix. 5.5.2LC performance on different delay time. We further analyzethe performance of the LC model on samples with different delaytime. Similarly, we divide the false negative samples in the trainingdata into five groups in ascending order based on their delay time.Each group has the same number of false positive samples. Then,as the goal of the LC model is to distinguish between false negative",
  "AUC 0.86980.83500.81170.78960.7811PRAUC 0.13970.07570.05450.04340.0398LL 0.05490.05840.06040.06210.0628": "Experiment results on the Criteo dataset are shown in .We have the following observations: (i) AUC ranges from 0.7811to 0.8698 at different delay time, which reflects that the LC modelcan effectively recognize false negative samples from all negativesamples. (ii) the performance of the LC model decreases as the delaytime of the false negative samples increases. There are two possiblereasons for this. First, samples with longer delays are more likely tobe hard samples. Second, in counterfactual labeling, false negativesamples with longer delays are more likely to convert after ADand be recognized as true negative examples, which damages theperformance of LC model.",
  "CONCLUSIONS AND FUTURE WORK": "In this paper, we propose a framework ULC to address the delayedfeedback problem in the offline setting via unbiased label correction.The key idea is that delayed feedback leads to and only leads to in-correct labels. If the incorrect labels can be effectively corrected, thedelayed feedback problem can be well addressed. ULC uses an ad-ditional LC model to guide the CVR prediction model for unbiasedlabel correction and enhances the performance through alternativetraining. We prove theoretically that the label-corrected loss inour method is an unbiased estimate of the oracle loss. Compara-tive experiments on both public and private datasets and detailedanalyses show that ULC effectively alleviates the delayed feedbackproblem and consistently outperforms the previous state-of-the-artmethods.For future work, we are interested in the following points. First,using multiple and dynamic counterfactual deadlines is likely toexploit training data more effectively. Second, given that sampleswith a long delay time are more likely to be hard samples, we wouldlike to design approaches to enhance the model performance onlong-delay samples. Third, we are interested in the combination ofour method to selection bias in CVR prediction. This work is supported by the Natural Science Foundation of China(Grant No.U21B2026), the fellowship of China Postdoctoral ScienceFoundation (No.2022TQ0178) and Huawei (Huawei Innovation Re-search Program). We also thank MindSpore for the partial supportof this work, which is a new deep learning computing framework.",
  "for Computing Machinery, New York, NY, USA, 26892697": "Yuta Saito. 2020. Doubly Robust Estimator for Ranking Metrics with Post-ClickConversions. In Proceedings of the 14th ACM Conference on Recommender Systems(Virtual Event, Brazil) (RecSys 20). Association for Computing Machinery, NewYork, NY, USA, 92100. Yuta Saito, Gota Morisihta, and Shota Yasui. 2020. Dual Learning Algorithmfor Delayed Conversions. In Proceedings of the 43rd International ACM SIGIRConference on Research and Development in Information Retrieval (Virtual Event,China) (SIGIR 20). Association for Computing Machinery, New York, NY, USA,18491852. Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang,and Jian Tang. 2019.AutoInt: Automatic Feature Interaction Learning viaSelf-Attentive Neural Networks. In Proceedings of the 28th ACM InternationalConference on Information and Knowledge Management (Beijing, China) (CIKM19). Association for Computing Machinery, New York, NY, USA, 11611170. Yumin Su, Liang Zhang, Quanyu Dai, Bo Zhang, Jinyao Yan, Dan Wang, YongjunBao, Sulong Xu, Yang He, and Weipeng Yan. 2021. An Attention-Based Model forConversion Rate Prediction with Delayed Feedback via Post-Click Calibration.In Proceedings of the Twenty-Ninth International Joint Conference on ArtificialIntelligence (Yokohama, Yokohama, Japan) (IJCAI20). Article 487, 7 pages. Hao Wang, Tai-Wei Chang, Tianqiao Liu, Jianmin Huang, Zhichao Chen, ChaoYu, Ruopeng Li, and Wei Chu. 2022. ESCM2: Entire Space Counterfactual Multi-Task Model for Post-Click Conversion Rate Estimation. In Proceedings of the 45thInternational ACM SIGIR Conference on Research and Development in InformationRetrieval (Madrid, Spain) (SIGIR 22). Association for Computing Machinery, NewYork, NY, USA, 363372. Ruoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong,and Ed Chi. 2021. DCN V2: Improved Deep & Cross Network and Practical Lessonsfor Web-Scale Learning to Rank Systems. In Proceedings of the Web Conference2021 (Ljubljana, Slovenia) (WWW 21). Association for Computing Machinery,New York, NY, USA, 17851797.",
  "Yanshi Wang, Jie Zhang, Qing Da, and Anxiang Zeng. 2020. Delayed FeedbackModeling for the Entire Space Conversion Rate Prediction. CoRR abs/2011.11826(2020). arXiv:2011.11826": "Hong Wen, Jing Zhang, Fuyu Lv, Wentian Bao, Tianyi Wang, and Zulong Chen.2021. Hierarchically Modeling Micro and Macro Behaviors via Multi-Task Learn-ing for Conversion Rate Prediction. In Proceedings of the 44th International ACMSIGIR Conference on Research and Development in Information Retrieval (VirtualEvent, Canada) (SIGIR 21). Association for Computing Machinery, New York,NY, USA, 21872191. Hong Wen, Jing Zhang, Yuan Wang, Fuyu Lv, Wentian Bao, Quan Lin, and KepingYang. 2020. Entire Space Multi-Task Modeling via Post-Click Behavior Decom-position for Conversion Rate Prediction. In Proceedings of the 43rd InternationalACM SIGIR Conference on Research and Development in Information Retrieval(Virtual Event, China) (SIGIR 20). Association for Computing Machinery, NewYork, NY, USA, 23772386. Jia-Qi Yang, Xiang Li, Shuguang Han, Tao Zhuang, De-Chuan Zhan, Xiaoyi Zeng,and Bin Tong. 2021. Capturing Delayed Feedback in Conversion Rate Predic-tion via Elapsed-Time Sampling. In Thirty-Fifth AAAI Conference on ArtificialIntelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Ar-tificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advancesin Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021. AAAI Press,45824589.",
  "Jia-Qi Yang and De-Chuan Zhan. 2022. Generalized Delayed Feedback Modelwith Post-Click Information in Recommender Systems. In NeurIPS": "Shota Yasui and Masahiro Kato. 2022. Learning Classifiers under Delayed Feed-back with a Time Window Assumption. In Proceedings of the 28th ACM SIGKDDConference on Knowledge Discovery and Data Mining (Washington DC, USA)(KDD 22). Association for Computing Machinery, New York, NY, USA, 22862295. Shota Yasui, Gota Morishita, Fujita Komei, and Masashi Shibata. 2020. A FeedbackShift Correction in Predicting Conversion Rates under Delayed Feedback. InProceedings of The Web Conference 2020 (Taipei, Taiwan) (WWW 20). Associationfor Computing Machinery, New York, NY, USA, 27402746.",
  "AAPPENDIXA.1Further Analysis on Different Delay Time": "An interesting phenomenon in Figure.6 is that the Vanilla modelperforms better than the Oracle model on samples with short delays(G1). It may be because samples with short delays have a higher per-centage of observed positive samples than actual positive samples.Specifically, the proportion of G1 to the observed positive samplesis 25.8%, which is higher than that of G1 to the true positive samples,21.4%. Thus, the Vanilla model focuses more on these short-delaysamples and learns better about them. We analyze the trainingsamples with different delay time in the same way as above. Asshown in , the Vanilla model indeed learns better on the short-delay samples of training data than the Oracle model. Moreover,we can also observe that the samples with a longer delay time onthe training set have a larger training loss for the Oracle model. Asthe number of true positive samples is the same for these groups,this also indicates that samples with longer delays are more likelyto be hard samples. G1G2G3G4G50.4 0.5 0.6 0.7 0.8 0.9",
  "A.2Case Study": "Here are two concrete cases on the Criteo dataset with MLP as thebackbone. For each test sample, we find the ten most similar samplesin the training set, as the label correctness of these training samples might have a strong impact on the prediction of the test sample.We calculate similarity using the L2 distance of sample embeddingsin our model. As shown in and 6, it can be found that ourmethod effectively corrects the labels of false negative sampleswithout wrongly categorizing true negative samples as positiveones. Consequently, the prediction accuracy of the test sample isenhanced. : The first concrete case on the Criteo dataset withMLP as the backbone. The false negatives are in boldface.Our method effectively corrects the labels of false negativesamples (5063626, 5630493). Meanwhile, the true negativesamples (1738500, 904961, 3478664) are not corrected to posi-tive samples."
}