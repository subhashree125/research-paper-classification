{
  "Abstract": "We participated in the KDD CUP 2024 paper source tracing com-petition and achieved the 3rd place. This competition tasked par-ticipants with identifying the reference sources (i.e., ref-sources,as referred to by the organizers of the competition) of given aca-demic papers. Unlike most teams that addressed this challenge byne-tuning pre-trained neural language models such as BERT orChatGLM, our primary approach utilized closed-source large lan-guage models (LLMs). With recent advancements in LLM technol-ogy, closed-source LLMs have demonstrated the capability to tacklecomplex reasoning tasks in zero-shot or few-shot scenarios. Conse-quently, in the absence of GPUs, we employed closed-source LLMsto directly generate predicted reference sources from the providedpapers. We further rened these predictions through ensemble learn-ing. Notably, ourmethod was the only one among the award-winningapproaches that did not require the use of GPUs for model training.Code available at",
  "Introduction": "In the rapidly evolving landscape of scientic research, tracing theorigins of ideas of academic papers has become increasingly cru-cial. The ability to accurately identify the source references of a pa-per not only enhances our understanding of the pathways of scien-tic progress but also holds signicant scientic and societal value[Zhang et al. 2024a]. As the volume of scientic publications con-tinues to grow exponentially [Zha et al. 2019], there is an urgentneed for ecient algorithms that can swiftly identify the sources Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor prot or commercialadvantage and that copies bearthis notice and the full citationon the rstpage. Copyrights for third-party components of this work mustbe honored.For all other uses, contact the owner/author(s).KDD Cup 2024 OAG-Challenge Workshop, Aug 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). of papers. Such tools are particularly valuable for emerging re-searchers, enabling them to quickly grasp the developmental tra-jectory of specic technologies and situate their work within thebroader scientic context.Traditionally, researchers have used methods such as randomforests [Valenzuela et al. 2015] or a neural network training [Yin et al.2021] to identify key citations for each paper. However, these ap-proaches often require substantial time and eort in designing in-tricate features or ne-tuning pre-trained models to achieve sat-isfactory performance. The advent of large language model (LLM)technologies has opened new avenues for addressing this challenge.Recent studies have demonstrated that when the parameter countis suciently high, LLMs can solve complex problems throughzero-shot learning [Kojima et al. 2022].In this paper, we propose a novel approach that leverages pre-trained LLMs as unsupervised reasoners to identify a papers sourcereferences. Our method utilizes the papers text and related infor-mation to perform direct reasoning, eliminating the need for ex-tensive feature engineering or model ne-tuning. This approach isparticularly benecial for researchers with limited GPU resources,as it oers a balance between computational eciency and perfor-mance accuracy.The ecacy of our proposed method is validated by its successin the paper source tracing competition of KDD CUP 2024, where itsecured third place. Notably, our team was the only award-winningentry that did not rely on GPUs, underscoring the methods abil-ity to achieve competitive results with minimal computational re-sources. This achievement demonstrates that our approach eec-tively balances resource consumption and performance, making ita valuable tool for a wide range of researchers and institutions.",
  "PRELIMINARIES2.1Data Description": "The dataset we used in this study is from the Open Academic Graphdataset [Zhang et al. 2024b], which includes training and test setsfor multiple tasks under the academic paper mining theme, suchas Author Name Disambiguation, Academic question answering,and Paper source tracing. The method we employ is applicable tothe data from the Paper source tracing task in the OAG dataset[Zhang et al. 2024a], which is referred to as the PST dataset in thefollowing text. The PST dataset contains 1576 papers related tocomputer science and 55014 associated citations.",
  "Task Description": "The primary objective of our study is to identify the most signi-cant reference paper for a given academic work. To formalize thistask, we introduce the following notation. Let A denote the set of all references that have been citedat least once across the entire dataset. For each paper , we dene = as theset of its references, where each , A. We denote as the subset of references that serve assource references for paper .Our task is to assign a probability score , to each ref-erence , in , where higher values indicate a greater likelihoodof the reference being a key source for the paper. To evaluate theperformance of our model, we employ the following metrics: For each paper , we calculate the Average Precision (AP),denoted as AP. The overall model performance is assessed using the MeanAverage Precision (MAP), computed as MAP = 1",
  "METHOD": "Given the constraints of limited access to high-performance GPUs,we developed an innovative approach that diverges from traditionalmethods such as ne-tuning large pre-trained models or employ-ing graph machine learning techniques. Our methodology primar-ily leverages closed-source LLMs in conjunction with feature engi-neering and ensemble learning to address the paper source tracingproblem eectively.",
  "Feature Engineering": "While our method relies heavily on LLMs for text comprehensionand problem-solving, we found that incorporating carefully engi-neered features from the papers enhances performance. Our fea-ture extraction process focuses on the following key aspects:(1) Paper Metadata: We extract information such as the publi-cation venue (journal/conference name), citation count, andauthor details (nationality and aliation) for each paper.(2) Citation Statistics: We compute the total occurrences of eachcited reference within the paper, as well as its distributionacross dierent sections or chapters.(3) Reference Metadata: Similar to the main paper, we extractpublication venue and author information for each cited ref-erence.(4) Contextual Keywords: We count the occurrences of specickeywords (e.g., motivated by, inspired by) in the vicinity ofeach citation to gauge its importance.These features provide a rich, structured representation of thepapers and their citations, complementing the semantic understand-ing capabilities of the LLMs.",
  "Prompt Engineering. We designed a set of prompts to elicitnuanced responses from the LLMs:": "(1) Base prompt: A carefully crafted instruction set (illustratedin ) that guides the LLM in identifying source cita-tions.(2) Inspiration-focused prompt: This variant asks the LLM tocategorize citations as \"direct inspiration\", \"indirect inspira-tion\", or \"other inspiration\", emphasizing the keyword \"in-spiration\".(3) Title-enriched prompt: An extension of the base prompt thatincludes the titles of cited articles for additional context.(4) Meta-optimized prompt: We utilize GPT-4 to rene the baseprompt itself, leveraging the LLMs meta-learning capabili-ties.(5) Notes-based prompt: For papers with available \"notes\" elds(potentially containing annotators insights), we instruct theLLM to identify key citations based on these descriptions. 3.2.2Answer Generation. Let denote the number of LLMtypes and the number of prompt variants. Theoretically,for each academic paper, we could generate up to = answers, where is the maximum number of an-swers per LLM-prompt combination. However, due to practicalconstraints, we limit this to answers per paper.For each cited reference , in the citation set of paper , wegenerate a probability list ,of length , where each elementrepresents the LLM-estimated probability of , being a source ci-tation. Formally, we have:",
  "Base Models": "To complement the LLM-generated probabilities, we employ twogradient boosting frameworks: LightGBM [Ke et al. 2017] and Cat-Boost [Prokhorenkova et al. 2018]. These models operate on theengineered features described in .1.For each paper-citation pair (,,) in the training set, we con-struct a feature vector , and a corresponding binary label , {0, 1}, where , = 1 indicates that , is a reference source of .We train two classiers:",
  "(1) Using and trained on the training set, we score eachreference ,, where the score represents the probabilityof it being a source citation. This score is represented asbase,= , + ,": "(2) We modify base,using the results from LLMs. This processinvolves two main steps: First, we group the LLM outputs into groups basedon prompt type and base model type. Within each group,we ask each LLM to provide a condence score for itspredicted key citations, as represented by the prob2scorefunction (Equation 5). Additionally, we assign a weightto each group based on expert knowledge, repre-senting the importance of that LLM group. Finally, we ag-gregate the results of all LLM groups according to theirweights to obtain scorebonus,, which serves as a bonus to be added to base, . In the second step, for each candidate reference, we aggre-gate the results from all LLMs. If the score given by theLLM at the -th percentile is less than , wereduce the bonus magnitude from the previous step. Ourapproach is to divide scorebonus,by a constant .",
  "Result": "We present the performance of our proposed method on the vali-dation set in . As can be observed, using LightGBM or LLMindividually does not yield high scores. When the two approachesare combined, however, a signicant improvement in the score canbe achieved. This demonstrates the eectiveness of ensemble learn-ing techniques [Zhou 2021]. In addition, we present the congura-tion of parameters in ."
}