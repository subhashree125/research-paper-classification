{
  "ABSTRACT": "The endeavor to preserve the generalization of a fair and invariant classifier across domains, especiallyin the presence of distribution shifts, becomes a significant and intricate challenge in machine learning.In response to this challenge, numerous effective algorithms have been developed with a focus onaddressing the problem of fairness-aware domain generalization. These algorithms are designed tonavigate various types of distribution shifts, with a particular emphasis on covariate and dependenceshifts. In this context, covariate shift pertains to changes in the marginal distribution of input features,while dependence shift involves alterations in the joint distribution of the label variable and sensitiveattributes. In this paper, we introduce a simple but effective approach that aims to learn a fair andinvariant classifier by simultaneously addressing both covariate and dependence shifts across domains.We assert the existence of an underlying transformation model can transform data from one domain toanother, while preserving the semantics related to non-sensitive attributes and classes. By augmentingvarious synthetic data domains through the model, we learn a fair and invariant classifier in sourcedomains. This classifier can then be generalized to unknown target domains, maintaining bothmodel prediction and fairness concerns. Extensive empirical studies on four benchmark datasetsdemonstrate that our approach surpasses state-of-the-art methods. Code repository is available at",
  "Introduction": "While modern fairness-aware machine learning techniques have demonstrated significant success in various applications, their primary objective is to facilitate equitable decision-making, ensuring algorithmicfairness across all demographic groups characterized by sensitive attributes, such as race and gender. Nevertheless, thegeneralization of a fair classifier learned in the source domain to a target domain during inference often demonstrates",
  "Instance ClassInstance FeaturesSensitive Attributes": ": Illustration of the problem in generalizing fair classifiers across different data domains under covariateand dependence shifts simultaneously. (Left) Images in source and target domains have different styles (Photos andArts). Each data domain is linked to a distinct correlation between class labels (NC and C) and sensitive attributes(Male and Female). (Right) We consider x = [x1, x2]T a simple example of a two-dimensional feature vector. A fairclassifier f learned using source data is applied to data sampled from various types of shifted target domains, resultingin misclassification and unfairness. f represents the true classifier in the target domain.",
  "severe limitations in many state-of-the-art methods. The poor generalization can be attributed to the data distributionshifts from source to target domains, resulting in catastrophic failures": "There are two main lines of data distribution shifts : general and fairness-specific shifts. The former focuses onshifts involving input features and labels. Specifically, covariate shift and label shift refer to variations due todifferent marginal distributions over feature and class variables, respectively. Concept shift indicates \"functionalrelation change\" due to the change amongst the instance-conditional distributions . Moreover, fairness-specificshifts consider additional sensitive attributes and hence place a greater emphasis on ensuring algorithmic fairness.Demographic shift3 refers to certain sensitive population subgroups becoming more or less probable duringinference. Dependence shift captures the correlation change between the class variable and sensitive attributes.Within these distribution shifts, a trained fair classifier from source domains is directly influenced and may degradewhen adapted to target domains. To simplify, we narrow the scope of distribution shifts to two prominent ones: covariate shift, which has been extensivelyinvestigated in the context of out-of-distribution (OOD) generalization , and dependence shift, a topic thathas gained attention in recent research. In the illustrative example shown in , the source and target domainsexhibit variations stemming from different image styles (Photos and Arts) and correlations between labels (No-cookingand Cooking) and sensitive attributes (Male and Female). Specifically, in the source domain, most males in thekitchen are not cooking, whereas in the target domain, a distinct correlation is observed with most males engaging incooking. To learn a classifier that is both fair and accurate under such hybrid shifts, a variety of domain generalizationapproaches have been explored. Predominantly, these methods often exhibit two specific limitations: they (1) addresseither covariate shift or dependence shift , or (2) solely focus on covariate shift but not explicitlyindicate the existence of dependence shift . Therefore, there is a need for research that explores the problem offairness-aware domain generalization (FDG), considering both covariate and dependence shifts simultaneously acrosssource and target domains. In this paper, we introduce a novel framework, namely Fair disEntangled DOmain geneRAlization (FEDORA). Thekey idea in our framework revolves around learning a fair and accurate classifier that can generalize from given sourcedomains to target domains, which remain unknown and inaccessible during training. The variations in these domainsresult from the concurrent presence of covariate and dependence shifts. Notice that, unlike the settings in some worksinvolving covariate shift , we assert each domain possesses a distinct data style (Photos and Arts), resultingin an alternation in feature spaces. Technically, we assert the existence of a transformation model that can disentangleinput data to a semantic factor that remains invariant across domains, a style factor that characterizes covariate-relatedinformation, and a sensitive factor that captures attributes of a sensitive nature. To enhance the generalization ofthe training classifier and adapt it to unknown target domains, we augment the data by generating them through thetransformation model. It utilizes semantic factors associated with various style and sensitive factors sampled fromtheir respective prior distributions. Furthermore, we leverage this framework to systematically define the FDG problemas a semi-infinite constrained optimization problem. Theoretically, we apply this re-formulation to demonstrate thata tight approximation of the problem can be achieved by solving the empirical, parameterized dual for this problem.Moreover, we develop a novel interpretable bound focusing on fairness within a target domain, considering the domaingeneralization arising from both covariate and dependence shifts. Finally, extensive experimental results on the proposed",
  "new algorithm show that our algorithm significantly outperforms state-of-the-art baselines on several benchmarks. Ourmain contributions are summarized": "We introduce a fairness-aware domain generalization problem within a framework that accommodates inter-domainvariations arising from covariate and dependence shifts simultaneously. We also give a brief survey by comparing thesetting of related works. We reformulate the problem to a novel constrained learning problem. We further establish duality gap bounds for theempirically parameterized dual of this problem and develop a novel upper bound that specifically addresses fairnesswithin a target domain while accounting for the domain generalization stemming from both covariate and dependenceshifts.",
  "Related Works": "Domain generalization. Addressing the challenge of domain shift and the absence of OOD data has led to theintroduction of several state-of-the-art methods in the domain generalization field . These methods aredesigned to enable deep learning models to possess intrinsic generalizability, allowing them to adapt effectively fromone or multiple source domains to target domains characterized by unknown distributions . They encompass varioustechniques, such as aligning source domain distributions to facilitate domain-invariant representation learning ,subjecting the model to domain shift during training through meta-learning , and augmenting data with domain analysis, among others , and so on. In the context of the number of source domains, a significant portion of research has focused on the multi-source setting. This setting assumes the availability of multiple distinct but relevantdomains for the generalization task. As mentioned in , the primary motivation for studying domain generalizationis to harness data from multiple sources in order to unveil stable patterns. This entails learning representations invariantto the marginal distributions of data features, all while lacking access to the target data. Nevertheless, existing domaingeneralization methods tend to overlook the aspect of learning with fairness, where group fairness dependence patternsmay not change domains. Fairness learning for changing environments. Two primary research directions aim to tackle fairness-aware machinelearning in dynamic or changing environments. The first approach involves equality-aware monitoring methods, which strive to identify and mitigate unfairness in a models behavior by continuouslymonitoring its predictions. These methods adapt the models parameters or structure when unfairness is detected.However, a significant limitation of such approaches is their assumption of invariant fairness levels across domains,which may not hold in real-world applications. The second approach focuses on assessing a models fairness ina dynamic environment exclusively under dependence shifts. However, it does not consider other types of distributionshifts. In response to these limitations, this paper adopts a novel approach by attributing the distribution shift from sourceto target domains to both covariate shift and fairness dependence shift simultaneously. The objective is to train afairness-aware invariant classifier capable of effective generalization across domains, ensuring robust performancein terms of both model accuracy and the preservation of fair dependence between predicted outcomes and sensitiveattributes under both shifts.",
  "Preliminaries": "Notations. Let X Rd denote a feature space, Z = {1, 1} is a sensitive space, and Y = {0, 1} is a label spacefor classification. Let C Rc, A Ra, and S Rs be the semantic, sensitive and style latent spaces, respectively,induced from X and A by an underlying transformation model T : X Z E X Z. We use X, Z, Y, C, A, Sto denote random variables that take values in X, Z, Y, C, A, S and x, z, y, c, a, s the realizations. A domain e E isdefined as a joint distribution PeXZY = P(Xe, Ze, Y e) : X Z Y . A classifier f in a class space F denotesf F : X Y. We denote E and Es E as the set of domain labels for all domains and source domains, respectively.Superscripts in the samples denote their domain labels, while subscripts specify the indices of encoders. For example,Es(xs) denotes a sample x drawn from the s domain and encoded by a style encoder Es. Fairness notions. When learning a fair classifier f F that focuses on statistical parity across different sensitivesubgroups, the fairness criteria require the independence between the sensitive random variables Z and the predictedmodel outcome f(X) . Addressing the issue of preventing group unfairness can be framed as the formulation of aconstraint. This constraint mitigates bias by ensuring that f(X) aligns with the ground truth Y , fostering equitableoutcomes.Definition 1 (Group Fairness Notion ). Given a dataset D = {(xi, zi, yi)}|D|i=1 sampled i.i.d. from PXZY , aclassifier f F : X Y is fair when the prediction Y = f(X) is independent of the sensitive random variable Z.To get rid of the indicator function and relax the exact values, a linear approximated form of the difference betweensensitive subgroups is defined as",
  "p1 and 1 p1 are the proportion of samples in the subgroup Z = 1 and Z = 1, respectively": "Specifically, when p1 = P(Z = 1) and p1 = P(Z = 1, Y = 1), the fairness notion ( Y , Z) is defined as the differenceof demographic parity and the difference of equalized opportunity, respectively . In this paper, we will presentthe results under demographic parity (and then the expectation in Eq. (1) is over XZ), while the framework can begeneralized to multi-class, multi-sensitive attributes and other fairness notions. Strictly speaking, a classifier f is fairover subgroups if it satisfies ( Y , Z) = 0. Problem setting. Given a dataset D = {De}|E|e=1, where each De = {(xei, zei , yei )}|De|i=1 is i.i.d. sampled from a domainPeXZY and e E, we consider multiple source domains {PsXZY }|Es|s=1 and a distinct target domain PtXZY , t = s, s Es E and t E\\Es, which is unknown and inaccessible during training. Given samples {Ds}|Es|s=1 from finite sourcedomains, the goal of fairness-aware domain generalization problems is to learn a classifier f F that is generalizableacross all possible domains.",
  "Learning the Transformation ModelAugmentation in Synthetic Domains": ": (Left) A transformation model T is trained using a bi-directional reconstruction loss (data reconstructionand factor reconstruction) and a sensitiveness loss. (Right) To enhance the generalization of the classifier f to unseentarget domains, the transformation model T is used for augmentation in synthetic domains by generating data basedon invariant semantic factors and randomly sampled sensitive and style factors that encode synthetic domains. Wedemonstrate the concept using the ccMNIST dataset, where the domains are distinguished by different digit colorsand fair dependencies between class labels and sensitive attributes. Here, sensitive attributes are defined by imagebackground colors. Problem 1 (Fairness-aware Domain Generalization). Let {PsXZY }|Es|s=1 be a finite subset of source domains and assumethat, for each s Es, we have access to its corresponding dataset Ds = {(xsi, zsi , ysi )}|Ds|i=1 sampled i.i.d from PsXZY .Given a classifier set F and a loss function : Y Y R, the goal is to learn a fair classifier f F for any Ds thatminimizes the worst-case risk over all domains in {PeXZY }|E|e=1 satisfying a group fairness constraint:",
  "minfF maxeE EPsXZY (f(Xs), Y s),s.t. (f(Xs), Zs) = 0(2)": "The goal of Prob. 1 is to seek a fair classifier f that generalizes from the given finite set of source domains to give agood generalization performance on all domains. Since we do not assume data from a target domain is accessible, itmakes Prob. 1 challenging to solve. Another challenge is how closely the data distributions in unknown target domains match those in the observed sourcedomains. As discussed in Sec. 1 and Tab. 1, there are five different types of distribution shifts. In this paper, we narrowthe scope and claim the shift between source and target domains is solely due to covariate and dependence shifts. Definition 2 (Covariate Shift and Dependence Shift). In Prob. 1, covariate shift occurs when domain variationis attributed to disparities in the marginal distributions over input features PsX = PtX, s. On the other hand, Prob. 1exhibits a dependence shift when domain variation arises from alterations in the joint distribution between Y and Z,denoted PsY Z = PtY Z, s where PsY |Z = PtY |Z and PsZ = PtZ; or PsZ|Y = PtZ|Y and PsY = PtY . Underlying transformation models. Inspired by existing domain generalization endeavors , distributionshifts can characterize generalization tasks across domains through an underlying transformation model T. Themotivation behind using T lies in bolstering the robustness and adaptability of the classifier f across diverse domains.By learning a transformation model, the objective is twofold: (1) to enable the model to adapt domain-invariant datarepresentations (factors) from the input data by disentangling domain-specific variations and (2) to generate augmenteddata in new domains by perturbing existing samples with various variations. This augmentation enhances the diversityof the source data and thereby improves the ability to generalize to unseen target domains.",
  "Learning the Transformation Model": "One goal of the transformation model T = {E, G} is to disentangle an input sample from source domains into threefactors in latent spaces by learning a set of encoder E = {Ec, Ea, Es} and a decoder G : C A S X, whereEc : X C, Ea : X A, and Es : X S represent semantic, sensitive and style encoders, respectively. Assumption 1 (Multiple Latent Factors). Given dataset De = {(xei, zei , yei )}|De|i=1 sampled i.i.d. from PeXZY domaine E, we assume that each instance xei is generated from (1) a latent semantic factor c C, where C = {cy=0, cy=1};(2) a latent sensitive factor a A, where A = {az=1, az=1}; and (3) a latent style factor se, where se is specificto the individual domain e. We assume that the semantic and sensitive factors in C and A do not change acrossdomains. Each domain PeXZY is represented by a style factor se and the dependence score e = (Y e, Ze)4, denotede := (se, e), where se and e are unique to the domain PeXZY . Note that Assump. 1 is similarly related to the one made in . In our paper, with a focus on groupfairness, we expand upon the assumptions of existing works by introducing three latent factors. Under Assump. 1, iftwo instances (xei, zei, y) and (xej, zej, y) where ei, ej E, i = j share the same class label, then the latter instancecan be reconstructed by decoder G from the former using c = Ec(xei), s = Es(xej), and a = Ea(xej) through T,denoted (xej, zej) = T(xei, zei, ej). To enhance the effectiveness of the transformation model T, our overall learning loss for these encoders and decodersconsists of two main components: a bidirectional reconstruction loss and a sensitiveness loss.",
  "Fair Disentangled Domain Generalization": "Furthermore, with a trained transformation model T, to learn the fairness-aware invariant classifier f across domains,we make the following assumption.Assumption 2 (Fairness-aware Domain Shift). We assume that inter-domain variation is characterized by covariateand dependence shifts. As a consequence, we assume that the conditional distribution PeY |XZ is stable acrossdomains, e E. Given a transformation model T, it holds that PeiY |XZ = PejY |XZ, ei, ej E, i = j, where(Xej, Zej) = T(Xei, Zei, ej). In Assump. 2, the domain shift captured by T would characterize the mapping from the marginal distributions PeiX and(Y ei, Zei) over Dei to the distribution PejX and (Y ej, Zej) over Dej sampled from a different data domain PejXZY ,respectively. With this in mind and under Assump. 2, we introduce a new definition of fairness-aware invariance withrespect to the variation captured by T and satisfying the group fair constraint introduced in Defn. 1.Definition 3 (Fairness-aware T-Invariance). Given a transformation model T, a fairness-aware classifier f F isdomain invariant if it holds for all ei, ej E.",
  "Here, functions equivalently as it does in Eq. (1), by substituting Y to Y": "Defn. 3 is crafted to enforce invariance on the predictions generated by f directly. We expect a prediction to remainconsistent across various data realizations T while considering group fairness.Problem 2 (Fair Disentanglement Domain Generalization). Under Defn. 3 and Assump. 2, if we restrict F of Prob. 1 tothe set of invariant fairness-aware classifiers, the Prob. 1 is equivalent to the following problem",
  "where (Xsj, Zsj) = T(Xsi, Z, sj), si, sj Es, i = j": "Similar to , Prob. 2 is not a composite optimization problem. Moreover, acquiring domain labels is often expensiveor even unattainable, primarily due to privacy concerns. Consequently, under the assumptions of disentanglement-basedinvariance and domain shift, Prob. 1 can be approximated to Prob. 2 by removing the max operator over E. In addition, Prob. 2 offers a new and theoretically-principled perspective on Prob. 1, when data varies from domain todomain with respect to T. To optimize Prob. 2 is challenging because (1) The strict equality constraints in Prob. 2 aredifficult to enforce in practice; (2) Enforcing constraints on deep networks is known to be a challenging problem due tonon-convexity. Simply transforming them to regularization cannot guarantee satisfaction for constrained problems; and(3) As we have incomplete access to all domains, it limits the ability to enforce fairness-aware T-invariance and furthermakes it hard to estimate R(f).",
  "The FEDORA Algorithm": "In practice, we propose a simple but effective algorithm, given in Algorithm 1, which is co-trained with the transfor-mation model T. The detailed training process of T is provided in Algorithm 2 of Appendix B. In Algorithm 1, weharness the power of T to address the unconstrained dual optimization problem outlined in Eq. (9) through a series ofprimal-dual iterations. Given a finite number of observed source domains, to enhance the generalization performance for unseen target domains,the invariant classifier f is trained by expanding the dataset with synthetic domains generated by T. These syntheticdomains are created by introducing random sample style and random sensitive factors, hence a random sensitiveattribute, resulting in an arbitrary fair dependence within such domains. As described in , the sensitive factoras and the style factor ss are randomly sampled from their prior distributions N(0, Ia) and N(0, Is), respectively. Asensitive attribute zs is further predicted from as through h. Along with the unchanged semantic factor c encoded by(xs, zs, y), they are further passed through G to generate (xs, zs, y) with the unchanged class labels in an augmented",
  "(xj,zj)B g( f(xj, ), zj)": "10:L() = Lcls() + 1 Linv() + 2 Lfair()11: Adam(L(), , p)12:1 max{[1 + d (Linv() 1)], 0}, 2 max{[2 + d (Lfair() 2)], 0}13:end for14: until convergence15: procedure T(x, z, y)16:c, a, s = E(x)17:Sample a N(0, Ia), s N(0, Is)18:x = G(c, a, s), z = h(a)19:return (x, z, y)20: end procedure synthetic domain. Under Assump. 2 and Defn. 3, according to Eqs. (7) and (8), data augmented in synthetic domains arerequired to maintain invariance in terms of accuracy and fairness with the data in the corresponding original domains. Specifically, in lines 15-20 of Algorithm 1, we describe the transformation procedure that takes an example (x, z, y)as INPUT and returns an augmented example (x, z, y) from a new synthetic domain as OUTPUT. The augmentedexample has the same semantic factor as the input example but has different sensitive and style factors sampled fromtheir associated prior distributions that encode a new synthetic domain. Lines 1-14 show the main training loop forFEDORA. In line 6, for each example in the minibatch B, we apply the procedure T to generate an augmented examplefrom a new synthetic domain described above. In line 7, we consider KL-divergence as the distance metric for d[]. Allthe augmented examples are stored in the set B. The Lagrangian dual loss function is defined based on B and B in line10. The primal parameters and the dual parameters 1 and 2 are updated in lines 11-12.",
  ") is JS divergence defined based on KL divergence": "Notice that the second term in Theorem 2 becomes uncontrollable during training as it relies on the unseen target domain.Therefore, to preserve fairness across target domains, we aim to learn semantic factors that map the transformation modeT, ensuring that PsiC|XZY , si Es remains invariant across source domains. Simultaneously, we strive for the classifierf to achieve high fairness within the source domains. Proofs of Theorems 1 and 2 are provided in Appendix D.",
  "Datasets. We evaluate the performance of our FEDORA on four benchmarks. To highlight each source data and its fairdependence score s defined in Assump. 1, we summarize the statistics in Tab. 3": "(1) ccMNIST is a domain generalization benchmark created by colorizing digits and the backgrounds of the MNISTdataset . ccMNIST consists of images of handwritten digits from 0 to 9. Similar to ColoredMNIST , forbinary classification, digits are labeled with 0 and 1 for digits from 0-4 and 5-9, respectively. ccMNIST contains 70,000images divided into three data domains, each characterized by a different digit color (i.e., red, green, blue) and followedby a different correlation between the class label and sensitive attribute (digit background colors). (2) FairFace is a dataset that contains a balanced representation of different racial groups. It includes 108,501 images fromseven racial categories: Black (B), East Asian (E), Indian (I), Latino (L), Middle Eastern (M), Southeast Asian (S),and White (W). In our experiments, we set each racial group as a domain, gender as the sensitive attributes, and age( or < 50) as the class label. (3) YFCC100M-FDG is an image dataset created by Yahoo Labs and released to thepublic in 2014. It is randomly selected from the YFCC100M dataset with a total of 90,000 images. For domainvariations, YFCC100M-FDG is divided into three domains. Each contains 30,000 images from different year ranges,before 1999 (d0), 2000 to 2009 (d1), and 2010 to 2014 (d2). The outdoor or indoor tag is used as the binary class labelfor each image. Latitude and longitude coordinates, representing where images were taken, are translated into differentcontinents. The North American or non-North American continent is the sensitive attribute (related to spatial disparity).(4) NYSF is a real-world dataset on policing in New York City in 2011. It documents whether a pedestrian whowas stopped on suspicion of weapon possession would, in fact, possess a weapon. NYSF consists of records collected infive different regions: Manhattan (M), Brooklyn (B), Queens (Q), Bronx (R), and Staten (S). We use regions as differentdomains. This data had a pronounced racial bias against African Americans, so we consider race (black or non-black)as the sensitive attribute. Baselines. We compare the performance of FEDORA with 19 baseline methods that fall into two main categories: (1)12 state-of-the-art domain generalizations methods, specifically designed to address covariate shifts: ColorJitter, ERM, IRM , GDRO , Mixup , MLDG , CORAL , MMD , DANN , CDANN , DDG, and MBDG , where ColorJitter is a naive function in PyTorch that randomly changes the brightness, contrast,saturation and hue of images; and (2) 7 state-of-the-art fairness-aware domain generalizations methods, specificallydesigned to address either covariate or dependence shifts: DDG-FC, MBDG-FC, EIIL , FarconVAE , FCR, FTCS , and FATDM , where DDG-FC and MBDG-FC are two baselines that built upon DDG and",
  ":Results of accuracy-fairness tradeoff onFairface (left) and YFCC100M-FDG (right) sweepingover a range of 2": "MBDG , respectively by straightforwardly adding fairness constraints defined in Defn. 1 to the loss functions of theoriginal models. Due to space limits, we present results for 14 baselines in the main paper. Comprehensive results forall baselines can be found in Appendix E. Evaluation metrics. Three metrics are used for evaluation. Two of them are for fairness quantification, DemographicParity (DP) and the Area Under the ROC Curve (AUCfair) between predictions of sensitive subgroups .Notice that the AUCfair is not the same as the one commonly used in classification based on TPR and FPR. Theintuition behind this AUCfair is based on the nonparametric Mann-Whitney U test, in which a fair condition is definedas the classifiers prediction probability of a randomly selected sample x1 from one sensitive subgroup being greaterthan a randomly selected sample x1 from the other sensitive subgroup is equal to the probability of x1 being greaterthan x1 . A value of DP closer to 1 indicates fairness, and 0.5 of AUCfair represents zero bias effect onpredictions. Model selection. The model selection in domain generalization is intrinsically a learning problem, followed by , weuse leave-one-domain-out validation criteria, which is one of the three selection methods stated in . Specifically, weevaluate FEDORA on the held-out source domain and average the performance of |Es| 1 domains over the held-outone.",
  "Results": "Data augmentation in synthetic domains via T. We visualize the augmented samples with random variations in. The first column (Original) shows the images sampled from the datasets. In the second column (Reconstruction),we display images generated from latent factors encoded from the images in the first column. The images in the secondcolumn closely resemble those in the first column. Images in the last three columns are generated using the semanticfactors encoded from images in the first column, associated with style and sensitive factors randomly sampled from their",
  "Methods(B, 0.91)(W, 0.49)(L, 0.48)Avg": "ColorJitter0.640.26 / 0.640.15 / 93.471.560.340.09 / 0.640.02 / 92.070.550.390.10 / 0.700.02 / 91.770.610.42 / 0.66 / 92.94ERM0.670.17 / 0.580.02 / 91.891.100.390.09 / 0.610.01 / 92.820.380.570.15 / 0.620.01 / 91.960.510.51 / 0.61 / 93.08IRM0.630.12 / 0.580.01 / 93.391.030.320.19 / 0.660.01 / 90.541.560.41.021 / 0.630.05 / 92.061.890.43 / 0.62 / 92.48GDRO0.710.16 / 0.570.02 / 89.811.100.480.09 / 0.600.01 / 92.500.380.540.15 / 0.620.01 / 91.590.510.55 / 0.60 / 92.55Mixup0.580.19 / 0.590.02 / 92.460.690.430.19 / 0.610.01 / 92.980.030.550.22 / 0.610.02 / 93.432.020.51 / 0.60 / 93.19DDG0.600.20 / 0.590.02 / 91.761.030.510.07 / 0.600.01 / 91.340.800.440.17 / 0.620.02 / 93.460.320.49 / 0.61 / 92.74MBDG0.600.15 / 0.580.01 / 91.291.410.300.04 / 0.620.01 / 91.050.530.560.09 / 0.610.01 / 93.490.970.50 / 0.60 / 92.71",
  "DDG-FC0.610.06 / 0.580.03 / 92.271.650.480.15 / 0.620.02 / 92.451.550.500.25 / 0.620.03 / 92.420.300.52 / 0.61 / 93.23": "MBDG-FC0.700.15 / 0.560.03 / 92.120.430.320.07 / 0.600.03 / 91.500.570.570.23 / 0.620.02 / 91.890.810.53 / 0.60 / 92.48EIIL0.880.07 / 0.590.05 / 84.752.160.460.05 / 0.650.03 / 86.531.020.490.07 / 0.590.01 / 88.391.250.64 / 0.61 / 87.78FarconVAE0.930.03 / 0.540.01 / 89.610.640.510.07 / 0.600.01 / 86.400.420.580.05 / 0.600.05 / 88.700.710.66 / 0.58 / 88.46FCR0.810.05 / 0.590.02 / 79.660.250.390.06 / 0.630.02 / 82.330.890.380.12 / 0.660.02 / 85.222.330.54 / 0.63 / 83.68FTCS0.750.10 / 0.600.02 / 80.000.200.400.06 / 0.600.02 / 79.661.050.420.23 / 0.650.03 / 79.641.000.57 / 0.64 / 80.91FATDM0.930.03 / 0.570.02 / 92.200.360.460.05 / 0.630.01 / 92.560.310.510.16 / 0.630.02 / 93.330.200.67 / 0.61 / 92.54",
  "Methods(d0, 0.73)(d1, 0.84)(d2, 0.72)Avg": "w/o Ea0.690.13 / 0.570.02 / 43.091.450.830.08 / 0.630.02 / 89.680.600.890.05 / 0.540.03 / 87.701.690.80 / 0.58 / 73.49w/o T0.820.12 / 0.560.03 / 47.211.170.830.05 / 0.630.01 / 73.100.260.820.08 / 0.530.02 / 72.952.250.82 / 0.57 / 64.42w/o Lfair0.720.17 / 0.690.03 / 54.241.750.920.02 / 0.640.03 / 94.352.350.920.07 / 0.640.03 / 93.202.170.86 / 0.66 / 80.59",
  "FEDORA0.870.09 / 0.530.01 / 62.562.250.940.05 / 0.520.01 / 93.361.700.930.03 / 0.530.02 / 93.430.730.92 / 0.53 / 83.12": "respective Gaussian distributions. The images in the last three columns preserve the fundamental semantic informationof the corresponding samples in the first column. However, their style and sensitive attributes undergo significantchanges at random. The generated images within synthetic domains enhance the classifiers generalization (f) to unseensource domains. This demonstrates that the transformation model T effectively extracts latent factors and producesdiverse transformations of the provided data domains. Effectiveness of T. To further validate the effectiveness of T, drawing inspiration from , we train a separatetransformation model for each domain. Subsequently, we generate an output image by utilizing distinct latent factorsfrom each domain. Using ccMNIST as an example, we individually train three transformation models {T i}3i=1 withineach domain. Each T i includes unique encoders Eic, Eia, and Eis. As shown in , an output image is generatedthrough G using a semantic factor (digit class, E1c(x1)), a sensitive factor (background color, E2a(x2)), and a stylefactor (digit color, E3s(x3)) from images in different domains. As a result, the output image is constructed from the digitof x1, the background color of x2, and the digit color of x3, with given variations. This suggests that the augmenteddata with random variations in for the synthetic domain are not merely altering colors; instead, they are preciselygenerated with unchanged semantics and random sensitive and style factors. The effectiveness of FEDORA across domains in terms of predicted fairness and accuracy. Comprehensiveexperiments showcase that FEDORA consistently outperforms baselines by a considerable margin. For all tables inthe main paper and Appendix, results shown in each column represent performance on the target domain, using therest as source domains. Due to space limit, selected results for three domains of FairFace are shown in Tab. 4, butthe average results are based on all domains. Complete performance for all domains of datasets refers to Appendix E.As shown in Tab. 4, for the FairFace dataset, our method has the best accuracy and fairness level for the averageDG performance over all the domains. More specifically, our method has better fairness metrics (3% for DP, 2% forAUCfair) and comparable accuracy (0.19% better) than the best of the baselines for individual metrics. As shown in",
  "Tab. 5, for YFCC100M-FDG, our method excels in fairness metrics (8% for DP, 4% for AUCfair) and comparableaccuracy (0.35% better) compared to the best baselines": "Ablation studies. We conduct three ablation studies to study the robustness of FEDORA on FairFace. In-depthdescriptions and the pseudocodes for these studies can be found in Appendix C. More results can be found in Appendix E.(1) In w/o Ea, we modify the encoder within T by restricting its output to only latent semantic and style factors. (2)w/o T skips data augmentation in synthetic domains via T and results are conducted only based f constrained by fairnotions outlined in Defn. 1. (3) In w/o Lfair, the fair constraint on f is not included, and we eliminate the Lfair in line9 of Algorithm 1. We include the performance of such ablation studies in . The results illustrate that when data isdisentangled into three factors, and the model is designed accordingly, it can enhance generalization performance dueto covariate and dependence shifts. Generating data in synthetic domains with random fairness dependence patternsproves to be an effective approach for ensuring fairness invariance across domains. Fairness-accuracy tradeoff. In our Algorithm 1, because 2 (lines 10 and 12) is the parameter that regularizes the fairloss, we conduct additional experiments to show the change of tradeoffs between accuracy and fairness sweeping overa range of 2 [0.01, 0.05, 0.1, 1, 10]. Our results show that the larger (small) 2, the better(worse) model fairnessfor each domain as well as in average, but it gives worse (better) model accuracy. Evaluation on FairFace andYFCC100M-FDG is given in . Results in the top-right of the figure indicate good performance. This result isplotted on the average performance over all target domains.",
  "Conclusion": "In this paper, we introduce a novel approach designed to tackle the challenges of domain generalization when confrontedwith covariate shift and dependence shift simultaneously. We present a tractable algorithm and showcase its effectivenessthrough comprehensive analyses and exhaustive empirical studies. Richard Zemel, Yu Wu, Kevin Swersky, Toniann Pitassi, and Cynthia Dwork. Learning fair representations. ICML,2013. Chen Zhao, Feng Chen, and Bhavani Thuraisingham. Fairness-aware online meta-learning. ACM SIGKDD, 2021. Yongkai Wu, Lu Zhang, and Xintao Wu. On convexity and bounds of fairness-aware classification. 2019. Chen Zhao, Feng Mi, Xintao Wu, Kai Jiang, Latifur Khan, and Feng Chen. Adaptive fairness-aware onlinemeta-learning for changing environments. In Proceedings of the 28th ACM SIGKDD Conference on KnowledgeDiscovery and Data Mining, page 25652575, 2022. Chen Zhao and Feng Chen. Rank-based multi-task learning for fair regression. IEEE International Conference onData Mining (ICDM), 2019. Chen Zhao and Feng Chen. Unfairness discovery and prevention for few-shot regression. ICKG, 2020. Chen Zhao, Changbin Li, Jincheng Li, and Feng Chen. Fair meta-learning for few-shot classification. ICKG,2020. Chen Zhao, Feng Mi, Xintao Wu, Kai Jiang, Latifur Khan, Christan Grant, and Feng Chen. Towards fairdisentangled online learning for changing environments. In Proceedings of the 29th ACM SIGKDD Conferenceon Knowledge Discovery and Data Mining, pages 34803491, 2023. Chen Zhao, Feng Mi, Xintao Wu, Kai Jiang, Latifur Khan, and Feng Chen. Dynamic environment responsiveonline meta-learning with fairness awareness. ACM Transactions on Knowledge Discovery from Data, 18(6),2024. Yujie Lin, Dong Li, Chen Zhao, Xintao Wu, Qin Tian, and Minglai Shao. Supervised algorithmic fairness indistribution shifts: A survey. arXiv preprint arXiv:2402.01327, 2024. Yuji Roh, Kangwook Lee, Steven Euijong Whang, and Changho Suh. Improving fair training under correlationshifts. ICML, 2023. Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-likelihoodfunction. Journal of statistical planning and inference, 90(2):227244, 2000. Ke Wang, Senqiang Zhou, Chee Ada Fu, and Jeffrey Xu Yu. Mining changes of classification by correspondencetracing. In Proceedings of the 2003 SIAM International Conference on Data Mining, pages 95106. SIAM, 2003. Gerhard Widmer and Miroslav Kubat. Learning in the presence of concept drift and hidden contexts. Machinelearning, 23:69101, 1996. Alexander Robey, George J Pappas, and Hamed Hassani. Model-based domain generalization. Advances inNeural Information Processing Systems, 34:2021020229, 2021. Stephen Giguere, Blossom Metevier, Yuriy Brun, Bruno Castro da Silva, Philip S Thomas, and Scott Niekum.Fairness guarantees under demographic shift. In Proceedings of the 10th International Conference on LearningRepresentations (ICLR), 2022. Hanlin Zhang, Yi-Fan Zhang, Weiyang Liu, Adrian Weller, Bernhard Schlkopf, and Eric P Xing. Towardsprincipled disentanglement for domain generalization. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 80248034, 2022. David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, RemiLe Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). In InternationalConference on Machine Learning, pages 58155826. PMLR, 2021. Changdae Oh, Heeji Won, Junhyuk So, Taero Kim, Yewon Kim, Hosik Choi, and Kyungwoo Song. Learningfair representation via distributional contrastive disentanglement. In Proceedings of the 28th ACM SIGKDDConference on Knowledge Discovery and Data Mining, pages 12951305, 2022. Elliot Creager, Jrn-Henrik Jacobsen, and Richard Zemel. Environment inference for invariant learning. InInternational Conference on Machine Learning, pages 21892200. PMLR, 2021. Thai-Hoang Pham, Xueru Zhang, and Ping Zhang. Fairness and accuracy under domain generalization. Proceed-ings of the International Conference on Learning Representations, 2023. Bahar Taskesen, Viet Anh Nguyen, Daniel Kuhn, and Jose Blanchet. A distributionally robust approach to fairclassification. arXiv preprint arXiv:2007.09530, 2020. Ashkan Rezaei, Rizal Fathony, Omid Memarrast, and Brian Ziebart. Fairness for robust log loss classification. InProceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 55115518, 2020. Yujie Lin, Chen Zhao, Minglai Shao, Baoluo Meng, Xujiang Zhao, and Haifeng Chen. Pursuing counterfactualfairness via sequential autoencoder across domains. ArXiv:2309.13005, 2023. Elliot Creager, David Madras, Toniann Pitassi, and Richard Zemel. Causal modeling for fairness in dynamicalsystems. In International conference on machine learning, pages 21852195. PMLR, 2020. Ashkan Rezaei, Anqi Liu, Omid Memarrast, and Brian D Ziebart. Robust fairness under covariate shift. InProceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 94199427, 2021. Wei Du and Xintao Wu. Fair and robust classification under sample selection bias. In Proceedings of the 30thACM International Conference on Information & Knowledge Management, pages 29993003, 2021. Arpita Biswas and Suvam Mukherjee. Ensuring fairness under prior probability shifts. In Proceedings of the 2021AAAI/ACM Conference on AI, Ethics, and Society, pages 414424, 2021. Vasileios Iosifidis and Eirini Ntoutsi. Fabboo-online fairness-aware learning under class imbalance. In Interna-tional Conference on Discovery Science, pages 159174. Springer, 2020. Vasileios Iosifidis, Thi Ngoc Han Tran, and Eirini Ntoutsi. Fairness-enhancing interventions in stream classification.In Database and Expert Systems Applications: 30th International Conference, DEXA 2019, Linz, Austria, August2629, 2019, Proceedings, Part I 30, pages 261276. Springer, 2019. Candice Schumann, Xuezhi Wang, Alex Beutel, Jilin Chen, Hai Qian, and Ed H Chi. Transfer of machine learningfairness across domains. arXiv preprint arXiv:1906.09688, 2019. Nathan Kallus and Angela Zhou. Residual unfairness in fair machine learning from prejudiced data. In Interna-tional Conference on Machine Learning, pages 24392448. PMLR, 2018. Harvineet Singh, Rina Singh, Vishwali Mhasawade, and Rumi Chunara. Fairness violations and mitigation undercovariate shift. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages313, 2021. Jessica Schrouff, Natalie Harris, Sanmi Koyejo, Ibrahim M Alabdulmohsin, Eva Schnider, Krista Opsahl-Ong,Alexander Brown, Subhrajit Roy, Diana Mincu, Christina Chen, et al. Diagnosing failures of fairness transferacross distribution shift in real-world medical settings. Advances in Neural Information Processing Systems,35:1930419318, 2022. Xiao Han, Lu Zhang, Yongkai Wu, and Shuhan Yuan. Achieving counterfactual fairness for anomaly detection. InPacific-Asia Conference on Knowledge Discovery and Data Mining, pages 5566. Springer, 2023. Yatong Chen, Reilly Raab, Jialu Wang, and Yang Liu. Fairness transferability subject to bounded distributionshift. Advances in Neural Information Processing Systems, 35:1126611278, 2022. Vladimir Vapnik. The nature of statistical learning theory. Springer science & business media, 1999. Martin Arjovsky, Lon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXivpreprint arXiv:1907.02893, 2019. Riccardo Volpi, Diane Larlus, and Grgory Rogez. Continual adaptation of visual representations via domainrandomization and meta-learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 44434453, 2021. Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C Kot. Domain generalization with adversarial featurelearning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 54005409,2018. Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy Hospedales. Learning to generalize: Meta-learning for domaingeneralization. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. Kaiyang Zhou, Yongxin Yang, Timothy Hospedales, and Tao Xiang. Learning to generate novel domains fordomain generalization. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August2328, 2020, Proceedings, Part XVI 16, pages 561578. Springer, 2020. Gilles Blanchard, Gyemin Lee, and Clayton Scott. Generalizing from several related classification tasks to a newunlabeled sample. Advances in neural information processing systems, 24, 2011. Gill Kirton. Unions and equality: 50 years on from the fight for fair pay at dagenham. Employee Relations: TheInternational Journal, 41(2):344356, 2019. Sergio Alonso, Rosana Montes, Daniel Molina, Ivn Palomares, Eugenio Martnez-Cmara, Manuel Chiachio,Juan Chiachio, Francisco J Melero, Pablo Garca-Moral, Brbara Fernndez, et al. Ordering artificial intelligencebased recommendations to tackle the sdgs with a decision-making model based on surveys. Sustainability,13(11):6038, 2021. Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Rich Zemel. Fairness through awareness.CoRR, 2011. Michael Lohaus, Michael Perrot, and Ulrike Von Luxburg. Too relaxed to be fair. In ICML, 2020. Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. Multimodal unsupervised image-to-image translation.In Proceedings of the European conference on computer vision (ECCV), pages 172189, 2018. Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image translation networks. Advances inneural information processing systems, 30, 2017. Dominik Maria Endres and Johannes E Schindelin. A new metric for probability distributions. IEEE Transactionson Information theory, 49(7):18581860, 2003. Yann LeCun, Lon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to documentrecognition. Proceedings of the IEEE, 86(11):22782324, 1998. Kimmo Karkkainen and Jungseock Joo. Fairface: Face attribute dataset for balanced race, gender, and age for biasmeasurement and mitigation. In Proceedings of the IEEE/CVF Winter Conference on Applications of ComputerVision (WACV), pages 15481558, January 2021. Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth,and Li-Jia Li. Yfcc100m: The new data in multimedia research. Communications of the ACM, 59(2):6473, 2016. Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani,Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness, WeiGuo, Berton Earnshaw, Imran Haque, Sara M Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, SergeyLevine, Chelsea Finn, and Percy Liang. Wilds: A benchmark of in-the-wild distribution shifts. In ICML, 2021. Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks.International Conference on Learning Representations, 2020. Shen Yan, Huan Song, Nanxiang Li, Lincan Zou, and Liu Ren. Improve unsupervised domain adaptation withmixup training. arXiv preprint arXiv:2001.00677, 2020. Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In Europeanconference on computer vision, pages 443450. Springer, 2016. Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Franois Laviolette, MarioMarchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The journal of machinelearning research, 17(1):20962030, 2016. Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao. Deep domaingeneralization via conditional invariant adversarial networks. In Proceedings of the European Conference onComputer Vision (ECCV), pages 624639, 2018. Bang An, Zora Che, Mucong Ding, and Furong Huang. Transferring fairness under distribution shifts via fairconsistency regularization. Advances in Neural Information Processing Systems, 35:3258232597, 2022. Charles X Ling, Jin Huang, Harry Zhang, et al. Auc: a statistically consistent and more discriminating measurethan accuracy. In Ijcai, volume 3, pages 519524, 2003. Toon Calders, Asim Karim, Faisal Kamiran, Wasif Ali, and Xiangliang Zhang. Controlling attribute effect inlinear regression. ICDM, 2013. Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. arXiv preprint arXiv:2007.01434,2020. Michael Feldman, Sorelle Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubramanian. Certifyingand removing disparate impact. KDD, 2015. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville,and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139144, 2020. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. InProceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016.",
  ": Important notations and corresponding descriptions.NotationsDescriptions": "Xinput feature spaceZsensitive spaceYoutput spaceClatent space for semantic factorsSlatent space for style factorsAlatent space for sensitive factorscsemantic factorsstyle factorasensitive factord[]distance metric over outputsdist[]distance metric over distributionsDdata setxdata featuresyclass labelzsensitive attributefclassifierFclassifier spacef-parameterization of Fypredicted class labelparameter spaceg(, )fairness function| |absolute functionp1empirical estimate of the proportion of samplesin the group z = 1edomain labelsssource domain labelsEset of data labelsBsampled data batchTdomain transformation modelEencoder networkGdecoder networkLloss functionexpectation of the relaxed constrainthsensitive classifierzsensitive attributes predicted by hp, dprimal and dual learning ratedual variableempirical constant",
  "DP = k, if DP 1; DP = 1/k, otherwise": "where k = P( Y = 1|Z = 1)/P( Y = 1|Z = 1) This is also known as a lack of disparate impact . A valuecloser to 1 indicates fairness. The Area Under the ROC Curve (AUCfair) varies from zero to one, and it is symmetric around 0.5, whichrepresents random predictability or zero bias effect on predictions.",
  "B.2Details of Learning the Transformation Model": "For simplicity, we denote the transformation model T consisting of three encoders Ec, Ea, Es, and a decoder G.However, in practice, we consider a bi-level auto-encoder (see ), wherein an additional content encoder Em :X M takes data as input and outputs a content factor. Furthermore, the decoder G used in the main paper is renamedGo. Specifically, the inner level decoder is denoted as Gi : C A M. As a consequence, the transformation modelT consists of encoders E = {Em, Es, Ec, Ea} and decoders G = {Gi, Go}. Specifically, in the outer level, an instance is first encoded to a content factor m M and a style factor s S throughthe corresponding encoders Em and Es, respectively. In the inner level, the content factor m is further encodedto a content factor c C and a sensitive factor a A, through encoders Ec and Ea. Therefore, the bidirectionalreconstruction loss and the sensitiveness loss stated in Sec. 4 are reformulated.",
  "Lsens = CrossEntropy(zs, h(Ea(Em(xs))))": "Additionally, motivated by the observation that GANs can improve data quality for evaluating the disentanglementeffect in the latent spaces, we use GANs to match the distribution of reconstructed data to the same distribution.Followed by , data and semantic factors generated through encoders and decoders should be indistinguishable fromthe given ones in the same domain.",
  "We have two sets of networks. One is for ccMNIST, FairFace, and YFCC100M-FDG, and the other one is for theNYSF dataset": "For ccMNIST, FairFace, and YFCC100M-FDG datasets: All the images are resized to 224 224. Em and Ecsstructures are the same. Each of them is made of four convolution layers. The first one has 64 filters, and each of theothers has 128 filters. The kernel sizes are (7, 7), (4, 4), (3, 3), (3, 3) for layers 1 to 4, respectively. The stride of the",
  ": A two-level approach for leaning the transformation model T": "second layer is (2, 2), and the stride of all the other layers is (1, 1). The activation function of the first three layers isReLU. The last convolution layer does not have an activation function. Es and Eas structures are the same. Each ofthem is made of 6 convolution layers, and there is an adaptive average pooling layer with output size 1 between the lasttwo convolution layers. The numbers of filters are 64, 128, 256, 256, 256, and 2 for the convolution layers, respectively.The kernel sizes are (7, 7), (4, 4), (4, 4), (4, 4), (4, 4), (1, 1). And the strides are (1, 1), (2, 2), (2, 2), (2, 2), (2, 2), (1, 1).The activation function of the first five layers is ReLU. The last convolution layer does not have an activation function.Go and Gis structures are almost the same. The only difference between them is the output size, 3 for Go and 128for Gi. Each of them has two parts. The first part is made of 4 convolution layers, and there is an upsampling layerwith a scale factor 2.0 between the second convolution layer and the third convolution layer. The numbers of filtersare 128, 128, 64, and 3 for the convolution layers, respectively. The kernel sizes are (3, 3), (3, 3), (5, 5), (7, 7). Thestrides are (1, 1) for all the convolution layers. The first and the third convolution layers activation functions are ReLU.The fourth convolution layers activation function is Tanh. The second convolution layer does not have an activationfunction. The second part is made of three fully connected layers. The number of neurons is 256 and 256, respectively,and the output size is 512. The activation function of the first two layers is ReLU, and there is no activation function onthe output. Do comprises 4 convolution layers followed by an average pooling layer whose kernel size is 3, stride is 2,and padding is . The numbers of filters of the convolution layers are 64, 128, 256, 1, respectively. The kernel sizesare (4, 4) for the first three convolution layers and (1, 1) for the fourth convolution layer. The strides are (2, 2) for thefirst three convolution layers and (1, 1) for the fourth convolution layer. The first three convolution layers activationfunctions are LeakyReLU. The other layers do not have activation functions. Di is made of one fully connected layerwhose input size is 112, and the output size is 64 with the activation function ReLU. h comprises one fully connectedlayer with input size 2, output size 1, and activation function Sigmoid. f has two parts. The first part is Resnet-50 ,and the second is one fully connected layer with input size 2048 and output size 2. For the NYSF dataset: Em is made of two fully connected layers. The number of neurons is 32, and the output size is16. The activation function of the first layer is ReLU, and there is no activation function on the output. Es is made oftwo fully connected layers. The number of neurons is 32, and the output size is 2. The activation function of the firstlayer is ReLU, and there is no activation function on the output. Go is made of two fully connected layers. The numberof neurons is 32, and the output size is 51. The activation function of the first layer is ReLU, and there is no activationfunction on the output. Do is made of two fully connected layers. The number of neurons is 32, and the output size is16. The activation function of the first layer is ReLU, and there is no activation function on the output. Ec is made oftwo fully connected layers. The number of neurons is 16, and the output size is 8. The activation function of the firstlayer is ReLU, and there is no activation function on the output. Ea is made of two fully connected layers. The numberof neurons is 8, and the output size is 2. The activation function of the first layer is ReLU, and there is no activationfunction on the output. Gi is made of two fully connected layers. The number of neurons is 16, and the output size is16. The activation function of the first layer is ReLU, and there is no activation function on the output. Di is made oftwo fully connected layers. The number of neurons is 8, and the output size is 8. The activation function of the firstlayer is ReLU, and there is no activation function on the output. h comprises one fully connected layer with input size 2and output size 1. The activation function is Sigmoid. f has two parts. The first part is made of 3 fully connected layers.The number of neurons is 32, and the output size is 32. The activation function of the first two layers is ReLU, andthere is no activation function on the output. The second part is made of one fully connected layer whose input size is32, the output size is 32, and it does not have an activation function.",
  "B.4Hyperparameter Search": "We follow the same set of the MUNIT for the hyperparameters. More specifically, the learning rate is 0.0001, thenumber of iterations is 600000, and the batch size is 1. The loss weights in learning T are chosen from {1, 5, 10}. Theselected best ones are 1 = 10, 2 = 1, 3 = 1, 4 = 1. We monitor the loss of the validation set and choose the with the lowest validation loss. Forthehyperparametersinlearningtheclassifierf,thelearningrateischosenfrom{0.000005, 0.00001, 0.00005, 0.0001, 0.0005}. is chosen from {0.01, 0.05, 0.1}.is chosen from{0.01, 0.025, 0.05}. is chosen from {0.1, 1, 10, 20}. The batch size is chosen from {22, 64, 80, 128, 512, 1024, 2048}.The numbers of iterations is chosen from {500, 1000, ..., 8000} on the ccMNIST and NYSF datasets. The number ofiterations are chosen from {300, 600, ..., 7800, 8000} on the FairFace and YFCC100M-FDG datasets. The selectedbest ones are: the learning rate is 0.00005, 1 = 2 = 0.05, 1 = 2 = 0.025, 1 = 2 = 1. The batch size on theccMNIST and YFCC100M-FDG datasets is 64, and it is 22 on the FairFace dataset and 1024 on the NYSF dataset.The number of iterations on the ccMNIST dataset is 3000, 500, 7000 for domains R, G, B, respectively. The numberof iterations on the FairFace dataset is 7200, 7200, 7800, 8000, 6600, 7200, 6900 for domains B, E, I, L, M, S, W,respectively. The number of iterations on the YFCC100M-FDG dataset is 7200, 6000, 6900 for d0, d1, d2, respectively.The number of iterations on the NYSF dataset is 500, 3500, 4000, 1500, 8000 for domains R, B, M, Q, S, respectively.We monitor the accuracy and the value of fairness metrics from the validation set and select the best ones. The gridspace of the grid search on all the baselines is the same as for our method.",
  "CAblation Studies": "We conduct three ablation studies, and detailed algorithms of designed ablation studies are given in Algorithms 3 to 5.For additional ablation study results on ccMNIST, YFCC100M-FDG, and NYSF, refer to Appendix E. 1. The difference between the full FEDORA and the first ablation study (w/o Ea) is that the latter does not have theinner level when learning T. Since the inner level is used to extract the content and sensitive factors from thesemantic one, the same sensitive label of the generated images will remain due to the absence of h(). Therefore,w/o Ea is expected to have a lower level of fairness in the experiments. Results shown in the tables indicate that w/oEa has a significantly lower performance on fairness metrics. 2. The second study (w/o T) does not train the auto-encoders to generate images. All losses are computed only basedon the sampled images. Similar to w/o Ea, it is much harder to train a good classifier without the generated imagesin synthetic domains. Our results demonstrate that w/o T performs worse on all the datasets. 3. The difference between FEDORA and the third study (w/o Lfair) is that w/o Lfair does not have the fairness lossLfair in line 9 of Algorithm 1. Therefore, this algorithm only focuses on accuracy without considering fairness.Results based on w/o Lfair show that it has a good level of accuracy but a poor level of fairness.",
  "D.1Sketch Proof of Theorem 1": "Before we prove Theorem 1, we first make the following propositions and assumptions.Proposition 1. Let d be a distance metric between probability measures for which it holds that d[P, T] = 0 for twodistributions P and T if and only if P = T almost surely. Then P (0, 0) = P",
  "Proposition 2. Assuming the perturbation function P (1, 2) is L-lipschitz continuous in 1, 2. Then given Proposi-tion 1, it follows that |P P (1, 2)| L||||1, where = T": "Definition 4. Let Rp be a finite-dimensional parameter space. For > 0, a function f : X Y issaid to be an -parameterization of F if it holds that for each f F, there exists a parameter such thatEPX f(x, ) f(x) . Given an -parameterization f of F, consider the following saddle-point problem:",
  ": repeat2:for minibatch B = {(xi, zi, yi)}mi=1 Ds do3:Lcls() = (1/m) mi=1 (yi, f(xi, ))4:Initialize Linv() = 05:for each (xi, zi, yi) in the minibatch do6:(xi, yi) = T(xi, zi, yi)": "7:Linv() += d[ f(xi, ), f(xi, )]8:end for9:Linv() = Linv()/m10:L() = Lcls() + 1 Linv()11: p L()12:1 max{[1 + d (Linv() 1)], 0}13:end for14: until convergence15: procedure T(x, z, y)16:c = Ec(Em(x, m), c)17:Sample a N(0, Ia)18:Sample s N(0, Is)19:x = Go(Gi(c, a, i), s, o)20:return (x, z, y)21: end procedure",
  ")]": "Let > 0 be given, and let f be an -parameterization of F. Let the assumptions hold, and further assume that , d,and g are [0, B]-bounded and that d[P, T] = 0 if and only if P = T almost surely, and that P (1, 2) is L-Lipschitz.Then assuming that A = { f(, ) : } F has finite VC-dimension, it holds with probability 1 over the Nsamples that",
  "EAdditional Results": "Additional results including complete results with all domains and baselines on ccMNIST (Tab. 7), FairFace (Tab. 8),FairFace (Tab. 9), and NYSF (Tab. 10) are provided. We showcase the reconstruction loss using the FairFacedata in . Additional ablation study results are in Tabs. 11 to 14. Sensitive analysis on slacks 1 and 2 We show additional experiment results by choosing different 1 and 2 ofAlgorithm 1 in Tab. 15. We observe that (1) by only increasing 2, the model towards giving unfair outcomes but higheraccuracy; (2) by only increasing 1, performance on both model fairness and accuracy decreases. This may be due tothe failure of disentanglement of factors.",
  "FLimitations": "In Sec. 6 and Appendix E, we empirically demonstrate the effectiveness of the proposed FEDORA, wherein our methodis developed based on assumptions. We assume (1) data instances can be encoded into three latent factors, (2) suchfactors are independent of each other, and (3) each domain shares the same semantic space. FEDORA may not workwell when data are generated with more than three factors or such factors are correlated to each other. Studies on causallearning could be a solution to address such limitations. Moreover, our model relies on domain augmentation. While theresults demonstrate its effectiveness, it might not perform optimally when semantic spaces do not completely overlapacross domains. In such scenarios, a preferable approach would involve initially augmenting data by minimizingsemantic gaps for each class across training domains, followed by conducting domain augmentations.",
  "Methods(R, 0.11)(G, 0.43)(B, 0.87)Avg": "w/o Ea0.230.05 / 0.980.01 / 94.891.720.110.06 / 0.920.02 / 98.191.390.420.06 / 0.720.03 / 95.280.220.25 / 0.87 / 96.12w/o T0.210.12 / 0.920.01 / 96.741.150.150.08 / 0.860.02 / 96.950.930.480.06 / 0.570.02 / 96.051.170.28 / 0.79 / 96.58w/o Lfair0.220.08 / 0.910.02 / 96.630.630.440.16 / 0.750.01 / 97.900.400.970.02 / 0.610.02 / 96.010.200.54 / 0.76 / 96.85",
  "Methods(B, 0.91)(E, 0.87)(I, 0.58)(W, 0.49)(L, 0.48)": "ColorJitter0.640.26 / 0.640.15 / 93.471.56 0.410.34 / 0.680.09 / 95.621.96 0.440.21 / 0.630.05 / 92.991.00 0.340.09 / 0.640.02 / 92.070.55 0.390.10 / 0.700.02 / 91.770.61ERM0.670.17 / 0.580.02 / 91.891.10 0.430.21 / 0.640.02 / 95.692.19 0.500.19 / 0.590.03 / 93.281.61 0.390.09 / 0.610.01 / 92.820.38 0.570.15 / 0.620.01 / 91.960.51IRM0.630.12 / 0.580.01 / 93.391.03 0.320.23 / 0.630.03 / 95.120.49 0.450.06 / 0.590.02 / 92.011.13 0.320.19 / 0.660.01 / 90.541.56 0.41.021 / 0.630.05 / 92.061.89GDRO0.710.16 / 0.570.02 / 89.811.10 0.460.16 / 0.610.02 / 95.261.53 0.500.14 / 0.590.01 / 93.271.27 0.480.09 / 0.600.01 / 92.500.38 0.540.15 / 0.620.01 / 91.590.51Mixup0.580.19 / 0.590.02 / 92.460.69 0.400.04 / 0.610.02 / 93.311.42 0.420.09 / 0.590.02 / 93.422.43 0.430.19 / 0.610.01 / 92.980.03 0.550.22 / 0.610.02 / 93.432.02MLDG0.630.25 / 0.580.02 / 92.712.36 0.410.15 / 0.620.03 / 95.590.87 0.510.15 / 0.600.02 / 93.351.87 0.470.20 / 0.590.01 / 92.821.65 0.530.18 / 0.620.03 / 92.990.86CORAL0.690.19 / 0.580.01 / 92.092.03 0.340.24 / 0.640.01 / 95.911.44 0.530.05 / 0.590.02 / 93.350.26 0.500.14 / 0.600.02 / 92.472.04 0.560.23 / 0.590.03 / 92.621.11MMD0.690.25 / 0.560.01 / 93.870.14 0.450.22 / 0.570.02 / 94.680.20 0.270.18 / 0.570.03 / 89.880.22 0.390.20 / 0.680.02 / 91.751.37 0.550.16 / 0.610.02 / 92.531.41DANN0.460.07 / 0.610.02 / 91.800.64 0.530.18 / 0.850.03 / 91.542.24 0.380.18 / 0.630.01 / 90.090.60 0.110.09 / 0.660.01 / 86.801.18 0.390.21 / 0.670.01 / 90.822.44CDANN0.620.24 / 0.590.03 / 91.220.33 0.430.10 / 0.660.02 / 94.752.23 0.430.18 / 0.610.01 / 92.411.68 0.350.17 / 0.670.02 / 90.190.60 0.420.23 / 0.610.03 / 92.422.19DDG0.600.20 / 0.590.02 / 91.761.03 0.360.15 / 0.630.02 / 95.522.35 0.490.17 / 0.590.01 / 92.352.04 0.510.07 / 0.600.01 / 91.340.80 0.440.17 / 0.620.02 / 93.460.32MBDG0.600.15 / 0.580.01 / 91.291.41 0.460.19 / 0.630.01 / 95.011.39 0.520.14 / 0.580.02 / 92.772.07 0.300.04 / 0.620.01 / 91.050.53 0.560.09 / 0.610.01 / 93.490.97 DDG-FC0.610.06 / 0.580.03 / 92.271.65 0.390.18 / 0.640.03 / 95.512.36 0.450.17 / 0.580.03 / 93.380.52 0.480.15 / 0.620.02 / 92.451.55 0.500.25 / 0.620.03 / 92.420.30MBDG-FC 0.700.15 / 0.560.03 / 92.120.43 0.350.07 / 0.600.01 / 95.541.80 0.560.07 / 0.570.01 / 92.411.61 0.320.07 / 0.600.03 / 91.500.57 0.570.23 / 0.620.02 / 91.890.81EIIL0.880.07 / 0.590.05 / 84.752.16 0.690.12 / 0.710.01 / 92.861.70 0.470.08 / 0.570.01 / 86.930.89 0.460.05 / 0.650.03 / 86.531.02 0.490.07 / 0.590.01 / 88.391.25FarconVAE 0.930.03 / 0.540.01 / 89.610.64 0.720.17 / 0.630.01 / 91.501.89 0.420.24 / 0.580.03 / 87.422.14 0.510.07 / 0.600.01 / 86.400.42 0.580.05 / 0.600.05 / 88.700.71FCR0.810.05 / 0.590.02 / 79.660.25 0.600.09 / 0.690.02 / 89.221.30 0.400.06 / 0.620.02 / 79.150.56 0.390.06 / 0.630.02 / 82.330.89 0.380.12 / 0.660.02 / 85.222.33FTCS0.750.10 / 0.600.02 / 80.000.20 0.660.18 / 0.650.01 / 88.111.09 0.490.05 / 0.650.01 / 82.150.64 0.400.06 / 0.600.02 / 79.661.05 0.420.23 / 0.650.03 / 79.641.00FATDM0.930.03 / 0.570.02 / 92.200.36 0.800.02 / 0.650.02 / 92.891.00 0.520.10 / 0.600.01 / 92.221.60 0.460.05 / 0.630.01 / 92.560.31 0.510.16 / 0.630.02 / 93.330.20",
  "Methods(M, 0.87)(S, 0.39)Avg": "ColorJitter0.360.12 / 0.650.05 / 92.791.220.350.20 / 0.690.06 / 91.891.020.42 / 0.66 / 92.94ERM0.340.08 / 0.620.01 / 92.511.450.680.14 / 0.590.03 / 93.480.940.51 / 0.61 / 93.08IRM0.340.11 / 0.650.02 / 92.472.420.550.23 / 0.590.01 / 91.810.660.43 / 0.62 / 92.48GDRO0.450.14 / 0.630.02 / 91.751.110.720.14 / 0.590.01 / 93.650.670.55 / 0.60 / 92.55Mixup0.310.11 / 0.620.02 / 93.520.790.910.04 / 0.580.02 / 93.200.330.51 / 0.60 / 93.19MLDG0.350.20 / 0.620.01 / 92.450.070.710.22 / 0.570.01 / 93.850.400.51 / 0.60 / 93.39 CORAL0.430.08 / 0.630.01 / 92.230.060.740.10 / 0.580.01 / 93.771.990.54 / 0.60 / 93.21MMD0.480.25 / 0.620.02 / 91.072.000.660.18 / 0.590.03 / 92.581.630.50 / 0.60 / 92.34DANN0.650.14 / 0.880.01 / 91.460.500.800.14 / 0.570.02 / 88.201.650.47 / 0.70 / 90.10CDANN0.270.12 / 0.670.01 / 91.070.970.520.12 / 0.820.02 / 88.320.370.43 / 0.66 / 91.48DDG0.370.14 / 0.640.01 / 91.360.650.630.22 / 0.580.01 / 93.400.370.49 / 0.61 / 92.74MBDG0.380.14 / 0.640.02 / 92.231.150.670.06 / 0.560.03 / 93.120.700.50 / 0.60 / 92.71 DDG-FC0.420.09 / 0.950.03 / 92.701.490.760.21 / 0.590.02 / 93.851.790.52 / 0.61 / 93.23MBDG-FC0.490.19 / 0.630.03 / 90.670.420.740.23 / 0.570.01 / 93.240.320.53 / 0.60 / 92.48EIIL0.520.09 / 0.630.03 / 84.961.370.980.01 / 0.550.02 / 89.992.270.64 / 0.61 / 87.78FarconVAE0.540.22 / 0.580.02 / 85.621.490.920.06 / 0.560.10 / 90.000.050.66 / 0.58 / 88.46FRC0.510.08 / 0.660.02 / 82.160.780.720.10 / 0.600.01 / 88.011.000.54 / 0.63 / 83.68FTCS0.490.10 / 0.680.01 / 81.151.250.750.21 / 0.620.02 / 75.692.070.57 / 0.64 / 80.91FATDM0.550.12 / 0.650.01 / 92.231.560.920.10 / 0.570.02 / 92.360.990.67 / 0.61 / 92.54",
  "Methods(R, 0.93)(B, 0.85)(M, 0.81)(Q, 0.59)(S, 0.62)Avg": "w/o Ea0.950.02 / 0.520.01 / 55.781.010.970.01 / 0.510.01 / 55.301.080.950.03 / 0.530.01 / 58.290.800.920.06 / 0.540.02 / 57.611.300.900.02 / 0.590.02 / 52.821.200.94 / 0.53 / 55.96w/o T0.950.03 / 0.520.01 / 61.360.420.910.06 / 0.540.01 / 57.670.820.890.05 / 0.550.01 / 60.680.310.970.02 / 0.520.01 / 59.330.170.870.11 / 0.570.01 / 55.400.730.92 / 0.54 / 58.89w/o Lfair 0.950.02 / 0.520.02 / 63.720.370.870.09 / 0.550.01 / 58.860.680.890.08 / 0.540.01 / 60.610.590.830.08 / 0.570.01 / 64.170.350.890.06 / 0.580.02 / 56.510.840.89 / 0.55 / 60.77 050k100k 150k 200k 250k 300k",
  "ccMNISTFairFaceYFCC100M-FDGNYSF": "1 = 0.025, 2 = 0.250.47 / 0.79 / 97.070.53 / 0.60 / 93.990.88 / 0.55 / 88.690.86 / 0.56 / 61.711 = 0.25, 2 = 0.0250.66 / 0.75 / 88.540.62 / 0.58 / 93.060.91 / 0.54 / 81.490.86 / 0.57 / 58.031 = 0.025, 2 = 0.0250.71 / 0.66 / 96.970.70 / 0.58 / 93.420.92 / 0.53 / 83.120.97 / 0.51 / 59.95"
}