{
  "ABSTRACT": "The increasing use of Retrieval-Augmented Generation (RAG) sys-tems in various applications necessitates stringent protocols toensure RAG systems accuracy, safety, and alignment with userintentions. In this paper, we introduce VERA (Validation and Eval-uation of Retrieval-Augmented Systems), a framework designedto enhance the transparency and reliability of outputs from largelanguage models (LLMs) that utilize retrieved information. VERAimproves the way we evaluate RAG systems in two important ways:(1) it introduces a cross-encoder based mechanism that encompassesa set of multidimensional metrics into a single comprehensive rank-ing score, addressing the challenge of prioritizing individual metrics,and (2) it employs Bootstrap statistics on LLM-based metrics acrossthe document repository to establish confidence bounds, ensuringthe repositorys topical coverage and improving the overall reliabil-ity of retrieval systems. Through several use cases, we demonstratehow VERA can strengthen decision-making processes and trust inAI applications. Our findings not only contribute to the theoreti-cal understanding of LLM-based RAG evaluation metric but alsopromote the practical implementation of responsible AI systems,marking a significant advancement in the development of reliableand transparent generative AI technologies.",
  "Large Language Models, Retrieval Augmented System, Evaluation": "ACM Reference Format:Tianyu Ding, Adi Banerjee, Yunhong Li, Laurent Mombaerts, Tarik Boro-govac, and Juan Pablo De la Cruz Weinstein. 2024. VERA: Validation andEvaluation of Retrieval-Augmented systems. In Proceedings of August 2526, Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from Evaluation KDD 2024, Barcelona, Spain, 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-x-xxxx-xxxx-x/YY/MM",
  "INTRODUCTION": "The integration of Retrieval-Augmented Generation (RAG) systemswith Large Language Models (LLMs) has significantly advancedthe field of natural language processing, particularly enhancingcapabilities in areas such as open-domain question answering, fact-checking, and customer service support. These systems combineextensive data repositories with sophisticated generative capabili-ties to produce responses that are both relevant and informative.Despite recent advancements, RAG systems rely on LLMs andhence face similar challenges, such as untraceable reasoning pro-cesses, supporting evidence is not provided as part of the answers,the production of \"hallucinated\" responses and answers that arecoherent but factually incorrect or irrelevant . Furthermore, in-tegrating these systems with additional databases presents uniquechallenges. Since these databases are static, they can have limitedcoverage on topics and can lead to outdated responses. Additionally,their large volumes can result in high computational costs.Traditional methods for evaluating RAG systems involve exten-sive manual annotations and continuous human monitoring, both ofwhich are resource-intensive . To address these challenges, wehave developed VERA, a scalable RAG evaluation method that uti-lizes LLM-based evaluation mechanisms and statistical estimatorsto provide annotations and evaluation tools suitable for productionenvironments.VERA efficiently evaluates both the retrieval and generationphases of RAG systems by measuring retrieval precision and recallto ensure optimal information retrieval and assessing the faith-fulness and relevance of generated answers. Additionally, VERAenhances its evaluation by leveraging a cross-encoder that incor-porates these retrieval and generation metrics to yield a singlecomprehensive score that can be used to rank RAG systems againsteach other. This singular score enables users to quickly ascertain theperformance of their RAG systems, as well as make any engineeringdecisions related to it. For instance, whether to roll-back a deploy-ment that caused an unforeseen change to their RAG performance.Furthermore, VERA introduces an innovative method that uti-lizes bootstrap estimators to validate and assess the topicality of",
  "GenAI Evaluation KDD 2024, Barcelona, Spain,Tianyu Ding, Adi Banerjee, Yunhong Li, Laurent Mombaerts, Tarik Borogovac, and Juan Pablo De la Cruz Weinstein": "Findings are presented in and Appendix . All themetric values in both tables are reported as mean.We feed the top 5 retrieved passages for a given query into aLLM to generate the final summarized response. In this experiment,the dataset labels \"PR\" and \"IR\" stand for \"Perfectly Relevant\" and\"Irrelevant,\" respectively. To make the results more deterministicand less affected by the randomness inherent in LLMs, we haveimplemented the following settings: = 0; = 0.01.The metric values in both tables are mean of all queries results.The experimental results demonstrate that Llama3, a powerfulopen-source LLM, performs comparably to established models likeAnthropics Claude V3 Haiku. In , these models manageeffective fact-checking and capture semantic relationships well, asindicated by high faithfulness and relevance scores. Additionally,the retrieval recall and precision are reasonably high, suggestingthat the models retrieve most relevant information accurately. Inthe opposite way, the low precision score in Appendix maysuggest that the queries either fall outside the scope of the coveredtopics in the knowledge base, or that the topics within the knowl-edge base are too varied relative to the generality of the queries. TheAgg-Logit scores in the comparison between different model con-figurations highlight the nuanced performance differences acrossvarious metrics.Lastly, the performance of these powerful LLMs and embeddingmodels have been compared to that of a weaker LLM (T-5 FLANBase) and embedding model (all-MiniLM-L6-v2) as a \"baseline\", tohighlight the differences in the evaluation metrics. As seen in both and , the individual evaluation metrics as well asthe Agg-Logit scores are consistently lower when using a weakerLLM + embedding model, regardless of the scenario of evaluatingagainst a perfectly relevant or irrelevant dataset.",
  "RELATED WORK": "Traditionally, RAG models were evaluated based on their perfor-mance in specific downstream tasks, utilizing established met-rics like EM and F1 scores for entity or sentiment classification,BERTScore and MoverScore for question answering, or accuracyfor fact-checking . Tools like RALLE auto-mate this process using task-specific metrics . State-of-the-artevaluation tools such as EXAM and RAGAS propose various quan-tifications for RAG retrieval and generation effectiveness, includingcontext relevance and answer faithfulness . BARTScore andSelfCheckGPT focus on generation factuality and coherency. RAGevaluation also encompasses abilities indicative of its adaptabilityand efficiency: noise robustness, negative rejection, counterfactualrobustness, and guideline adherence .Despite developments in evaluation metrics and tools, quantify-ing different aspects in RAG remains challenging due to uncertain-ties in inputs and outputs and limitations of existing benchmarks incapturing human preferences. The Large Model Systems Organiza-tion (LMSYS) group explores the feasibility and pros/cons of usingvarious LLMs as automated judge for tasks in writing, math, andworld knowledge . Their results reveal that strong LLM judgeslike GPT-4 can match both controlled and crowdsourced humanpreferences well, achieving over 80% agreement, the same level ofagreement between humans. The G-EVAL proposed by MicrosoftCognitive Services Research group with GPT-4 as the backbonemodel achieves a Spearman correlation of 0.514 with human onsummarization task, along with other studies confirming GPTsability to achieve state-of-the-art or competitive correlation withhuman judgments . Furthermore, several initiatives leverageLLM prompting to evaluate performance across diverse tasks suchas translation, summarization, and dialogue . These studiespoint out that LLMs offer a scalable and explainable alternative tohuman evaluation, which are otherwise very expensive to obtain.Lastly, given that RAGs rely on a retrieval model to retrieverelevant documents, their performance is pegged to the efficacy ofthe semantic search within the retriever. As the quality of semanticsearch is dependent on document ingestion and chunking strategiesemployed, the retriever can be made more robust by a re-ranking mechanism. This is where cross-encoder models have emerged asa prominent architecture in Natural Language Processing (NLP)for tasks requiring semantic similarity assessment and textual re-lationship understanding . These models, often leveragingtransformer-based encoders like BERT, process text pairs and gener-ate a joint embedding that encodes their semantic connection .This functionality allows for various applications, including sen-tence retrieval, question answering, and paraphrase detection .Cross-encoders offer advantages in efficiency compared to methodslike Siamese networks, particularly for large datasets. Additionally,their ability to leverage pre-trained language models enables ef-fective performance even with limited task-specific training data.",
  "VERA METHOD": "VERA first systematically assesses the integrity of document repos-itories using LLM-based metrics, such as Retrieval Precision, Recall,Faithfulness, and Answer Relevance. It then applies advanced tech-niques like rank-based aggregation and bootstrapping to enhancethe usability, reliability, and reproducibility of these metrics. Finally,it conducts contrastive analysis to evaluate document repositorytopicality . This approach not only evaluates the relevanceand accuracy of document retrieval but also ensures the integrityand thematic consistency of the information retrieved. .1covers how VERA uses LLMs to generate integrity related metrics..2 discusses rank-based aggregation. .3 introducesthe bootstrapping technique. .4 details how contrastiveanalysis is used to assess document topicality.",
  "LLMs as Evaluators": "Recent advances in LLMs information retrieval, understanding ofnuances, and reasoning abilities have made their applications inhigh-stakes tasks such as system evaluations practical and feasible. VERA uses Anthropic Claude V3 Haiku through AmazonBedrock service as the default LLM for RAG evaluations, due toHaikus balance between cost and effectiveness. Haiku achievescompetitive performance on major reasoning dataset: 75.2% onMMLU , 89.2% on ARC-Challenge and 85.9% on HellaSwag. On each of the dataset, it has surpassed GPT-3.5 over all thosethree evaluation benchmark datasets. A different LLM can be chosenbased on the models merits, specific use cases, and costs.Like existing LLM-based RAG evaluation system such as RA-GAS or ARES , VERA has measured the following LLM-basedevaluation metrics. The prompts to create the metrics are listed inAppendix 8.1.Faithfulness: This metric evaluates whether answers are basedsolely on the provided contexts, without any fabrication. The promptwill instruct the language model to generate a binary \"yes\" or \"no\"label for each (,,) pair, where , , and represent the question,answer, and context, respectively. The faithfulness metric for a setof (,,) pairs is calculated as the average of all the binary labels.Retrieval Recall: This metric evaluates the systems effective-ness in fetching all relevant information related to a query from thegiven context, ensuring that no significant data is omitted. This met-ric is determined by assessing whether each piece of informationin the answer is explicitly supported by the context. The retrieval",
  "Aggregate RAG evaluation Metrics in cross-encoder": ": Overview of VERA: VERA begins with user queries,pairing them with retrieved and LLM summarized responsesfrom a given RAG system. These elements form the basisfor the LLM-based RAG evaluation of individual question-answer pairs, ensuring that the context relevance, answerfaithfulness, and answer relevance metrics are meticulouslyassessed. These metrics are then consolidated using a cross-encoder to generate an aggregate score, enabling users toprioritize certain metrics over others and quickly makeoutcome-oriented decisions for development. The processthen culminates with Bootstrap Statistics, which apply LLM-based metrics across the entire document repository to estab-lish confidence bounds and gauge the overall performance ofretrieval systems. This robust evaluation pipeline is essentialfor maintaining high standards of precision and trustworthi-ness in document retrieval, particularly critical in domainswhere the accuracy of information is paramount. recall metric is calculated based on the proportion of sentences inthe answer that are classified as \"[Supported by Context]\", which isused in the evaluation metric prompt in Appendix 8.1. This involves:",
  "Calculating the ratio of \"[Supported by Context]\" sentencesto the total number of sentences": "Retrieval Precision: This metrics assesses the systems abilityto focus on and retrieve the most relevant parts of the context inresponse to a query, minimizing the inclusion of irrelevant content.High precision ensures that the model considers only the informa-tion that is directly pertinent to the question. The retrieval precisionmetric is calculated by evaluating the relevance of sentences ex-tracted from the context using LLM. This involves:",
  "Calculating the precision based on the ratio of relevant sen-tences to the total number of sentences in the context": "Answer Relevance: This metrics evaluates whether the gen-erated response directly addresses the given question, ensuringalignment with both the query and the retrieved context. This met-ric penalizes responses that are incomplete, redundant, or containunnecessary information, providing a score that ranges from 0 to 1,with 1 being the highest level of relevance. The answer relevancemetric is calculated through the following steps:",
  "Consolidation of Multi-DimensionalEvaluation Metrics": "The concept of consolidating evaluation metrics into a single com-prehensive score involves integrating the utilities of each metric,allowing users to make informed decisions despite the inherentfluctuations in these metrics. Appropriate consolidation eases theburden of users having to parse through multiple metrics to thenmake a decision based on the outcome - which would improve iter-ation speed in a development cycle. Furthermore, given that eachof these multi-dimensional metrics has its nuances, the question ofhow to prioritize certain metrics over others arises (e.g. does a sys-tem with higher faithfulness and lower relevance outperform thesystem with lower faithfulness and higher relevance). This wouldassist users to swiftly take action during regression testing, to makedecisions on whether to roll-back a deployment or not.Traditional techniques like simple aggregation or rank fusionoften suffer from compensatory effects and lack clarity, as theyobscure the subtleties of individual metrics .To address these challenges, VERA utilizes cross-encoder mod-els that leverage a cross-attention mechanism for a more preciseevaluation of document relevance. Traditional cross-encoder mod-els are effective at highlighting the most relevant text segmentswithin large texts, based on capturing semantic relationships be-tween words and phrases. It generates a relevance score for everyquestion-answer pair, enabling an effective comparison and rankingof these pairs. Formally, for a user-input question q and an answer",
  "(,) = ([] [] [])": "where CE is the cross-encoder, CLS and SEP are special tokens torepresent the classifier token and the separator token, and W is alearned matrix that represents the relationship between the queryand answer representations .Recently, effective multi-dimensional retrieval models are typ-ically implemented by performing a first-stage retrieval (to effi-ciently identify a subset of relevant documents from a corpus); anda second-stage re-ranking on this subset (where additional dimen-sions of relevance may be considered) . An example of this toconduct the first stage retrieval using the BM-25 algorithm (whichis a ranking algorithm that determines a documents relevance to agiven query and ranks documents based on their relevance scores).After this, the second-stage re-ranker modifies the architecture ofexisting cross-encoders, whereby the BM-25 score obtained in thefirst-stage retrieval is fed as an input token to the cross-encoder.Mathematically, this is represented by:",
  "(,) = ([] [] 25 [] [])": "In this paper, we follow a similar process to incorporate addi-tional dimensions of relevance into a cross-encoder to re-rank eval-uation records against each other. However, instead of manipulatingthe input structure of the cross-encoder, we integrate additional \"rel-evance statements\" into each question-answer pair that is fed intothe cross-encoder . These relevance statements pertain to textsrelated to the utility of each of the multi-dimensional evaluationmetrics, as well as their actual scores. As shown in , this exerciseyields 4-5 percent improvement in Mean Average Precision, Nor-malized Discounted Cumulative Gain and Mean Reciprocal Rankmetrics against baseline cross-encoder models.The process involves two key steps: first, enhancing the inputtext with additional relevant information mentioned above, andsecond, providing the queries and enhanced answer as inputs tothe pretrained cross-encoder to obtain the final aggregated score(which can be used to rank these records against each other) .This structured approach ensures a thorough and nuanced assess-ment of document relevance.Text Enhancement: The cross-encoder requires an input ofan input and output text. Within VERA, the input text will be theuser query input to a RAG system; and the output text will bean enriched answer encapsulating the original response from theRAG system, together with supplementary information about eachevaluated metrics utility as well as their scores. For instance, aquestion-answer pair (q,a) obtains a faithfulness score of 0.7 and itsenriched answer is generated by appending the original responsefrom the RAG system with the following text:\"Faithfulness measures the factual consistency of the generatedanswer against the given context. It is considered faithful if all theclaims that are made in the answer can be inferred from the givencontext. It is measured between 0 and 1; where a lower score is given toanswers consisting of claims that are not in the context; and a higherscore indicates that the answer is using information from the contexts.For the given question, context and answer, the faithfulness score is0.7.\" Cross-Encoder Ranking: Once the text enhancement step isdone, VERA feeds in the question and the enhanced answer intothe ms-marco-MiniLM-L-12-v2 (top cross-encoder model on MTEBLeaderboard). Formally, for a user-input question q and enrichedanswer , the logit score is determined as:",
  "Bootstrap LLM-based RAG EvaluationMetrics": "Evaluating RAG systems requires measuring retrieval precision,recall, faithfulness, and relevance. However, these metrics can varydue to LLMs stochasticity, reasoning limitations, and documentrepository topicality. To address this, we used bootstrapping onpre-computed metric values. This approach enhances result reliabil-ity and reproducibility by providing a robust statistical frameworkto analyze metric variability and distribution, while also support-ing document repository topicality assessment for specific contenttypes .LLMs can produce varying outputs due to factors like randomseed values, causing traditional evaluation to capture only a snap-shot of this variability and potentially misleading performanceconclusions. By applying bootstrap directly on the metric values,we can simulate multiple runs of model evaluations, capturing abroader spectrum of possible outcomes and thus providing a morecomprehensive picture of system performance.Bootstrapping metric values allows for repeated sampling froma set of observed metric computations, essentially creating numer-ous virtual evaluation scenarios. The bootstrapping metric valuescomputation are identical for all metrics.Given a known metric : Firstly, compute its values for a docu-ment repository dataset = {1,2, ...,} as () for each docu-ment . This results in a set of metric values = {1,2, ...,}.Then, for each metric , generate bootstrap samples. Each sample is created by randomly selecting metric values from with re-placement. Each bootstrap sample for metric can be representedas = {1,2, ...,}. For each bootstrap sample, compute thedesired statistics, such as the sample mean and variance as below: Estimates the Mean and Variability: Provides a statisticallyrobust way to estimate the mean and variance of perfor-mance metrics, incorporating the inherent randomness ofLLM outputs. The mean of the bootstrap samples is esti-mated as:",
  "VERA: Validation and Evaluation of Retrieval-Augmented systemsGenAI Evaluation KDD 2024, Barcelona, Spain,": "Bootstrapping Size B versus Sample Size n: There is no strictuniversal rule for the optimal bootstrapping size relative to thesample size. However, bootstrapping tends to work well when thesample size (n) is at least 30 and ideally 50 or more for accurateestimations of standard errors and confidence intervals .Bootstrapping size is recommended to be at least 1000 and 5000or more for stable convergence and complex statistics. With largersample size, a smaller bootstrapping size is possible with similaraccuracy. It is recommended to monitor the convergence of standarderrors or other statistics as increases, to determine the optimalbootstrapping size for each use case.Unbiased Estimator: The bootstrap estimator serves as an unbi-ased estimator for metrics based on LLMs, effectively estimating theexpectation of the original estimator and its bootstrap distribution.Detailed assumptions and mathematical derivations supporting thisconclusion are outlined in Appendix 8.2.",
  "Evaluating Document Repository TopicalityUsing Contrastive Query Analysis": "Document repositories often contain diverse content, leading tohigh entropy for domain-specific information retrieval and mak-ing it challenging to ascertain the repositorys thematic topicality,especially in specialized industry domains. To address this, we im-plement a contrastive analysis framework, differentiating responsesto topic perfect relevant queries (positive instances) from responsesto unrelated queries (negative controls). Within the framework,we have proposed a bootstrap estimation approach that provides astructured statistical analysis to evaluate the repositorys thematicconsistency.The approach involves several key steps with the idea ignitedfrom contrastive learning :",
  "Query Generation: Develop two distinct sets of queries. Pos-itive queries set are relevant to a specific domain of interest,and negative queries are deliberately chosen to be unrelatedto that domain": "Retrieval and Evaluation: Utilize a large language model(LLM) or a similar retrieval system to fetch and evaluate re-sponses for each query. Evaluation metrics such as RetrievalPrecision, Recall, Faithfulness, and Answer Relevance are cal-culated to assess the quality and relevance of the responses. Bootstrap Statistics: Apply bootstrap sampling techniquesto each evaluation metrics. This involves generating numer-ous subsamples from the collected metrics and computingstatistical measures (e.g., mean, variance) for these samplesto analyze the data robustly. Comparative Analysis: Compare the distributions of thesebootstrap statistics between the positive and negative querysets. This step quantitatively assesses the repositorys con-tent alignment with the domain of interest and identifies anysignificant disparities in content handling between relevantand irrelevant queries.",
  "In this section, we present the models and data used by VERA.VERA uses both public and proprietary datasets to ensure a com-prehensive analysis. We utilize the open-source MS MARCO in": "TREC 2023 Deep Learning Track for a general knowledge . Si-multaneously, we incorporate proprietary datasets tailored to AWSsales and marketing domain-specific evaluations, reflecting theunique challenges and requirements of different industry sectors.This combination allows us to assess the general applicability andtargeted performance of our RAG systems, facilitating a thoroughunderstanding of their capabilities and areas for optimization inreal-world scenarios.",
  "Models": "For domain-specific synthetic data generation, we employ An-thropic V3 Haiku to create high-quality synthetic queries and re-sponses tailored for our experimental needs. This models advancedgenerative capabilities ensure that the synthetic datasets are bothdiverse and closely aligned with the task-specific requirements.For the evaluation of responses, we utilize Anthropic V3 Sonnet,which serves as our LLM judge. The examples of synthetic genera-tion prompts, evaluation prompts and RAG summarization promptusing Llama 3 supported in POE Web UI are in Appendix 8.1.In our experiments, we compared the performance of multipleRAG systems by pairing different combinations of LLMsspecificallyAnthropic Haiku and Llama3with a selection of advanced retriev-ers. The retrievers weve chosen include e5-mistral-7b-instruct,titan-embedding-text-G1, and bge-large-en-v1.5, all of which arerecognized as top models on the MTEB leaderboard, indicative oftheir superior performance and capability in handling complex re-trieval tasks . This diverse combination of cutting-edgeLLMs and retrievers allows us to thoroughly assess and contrastthe strengths and limitations of different RAG configurations inproducing relevant and accurate responses.",
  "Datasets": "The TREC 2023 Deep Learning Track emphasizes enhancing infor-mation retrieval with large-scale datasets suitable for deep learning,focusing on passage and document ranking tasks. It utilizes theMS MARCO dataset to analyze and develop effective retrieval andreranking systems in real-world scenarios. In this research, well fo-cus on using the smaller passage ranking data from the TREC 2023Deep Learning Track for experimental purpose. For the purpose ofour experiments, we have used all 887 unique perfectly relevantquery-passage pairs (score=3) from \"2021.qrels.docs.final.txt\" and500 randomly sampled irrelevant query-passage pairs (score=0).Additionally, we have generated 400 passages related to cloudcomputation sales and marketing topic and 100 passages relatedto basketball topic. Then, we have created 200 queries about cloudcomputation sales and marketing, 200 queries about basketball and200 random queries not related to both topics.",
  "Bootstrap Metrics for Document RepositoryTopicality Analysis": "As outlined in section 3.3, we utilized bootstrap statistics to analyzea synthetic dataset described in section 4.2 and the results arein . We used bootstrap sampling with replacement on thesynthetic query sets and the overall passage set. In the syntheticquery sets, we have 200 synthetic queries in each set and we labeledthem in Tale 3 as \"Sales\", \"Basketball\" and \"Random\" based on thetopics. This approach enabled us to calculate critical statisticalmeasures like the mean and variance, providing a robust foundationfor assessing the thematic topicality of the data repository. Weused sample size 50 and bootstrapping size 500 to ensure fairlystable convergence of the statistics for each metric and each queryset. This comparative analysis helps in quantifying the documentrepositorys content topicality to distinguish and accurately processcontent relevant to its designated domain.In our study, the use of bootstrap statistics enabled us to com-pute the mean and confidence intervals for each performance met-ric across three different synthetic query sets on the same docu-ment repository. This comparison revealed notable differences inretrieval-related metrics among the query sets regarding differenttopics. The \"Sales\" query set results are with higher values in re-call, precision, and relevance as the majority (80%) of the syntheticpassage set is related cloud computation sales and marketing data. As comparison, the \"Basketball\" query set results are much higherthan the \"Random\" query set and fairly lower than the \"Sales\" queryset, which is within expectation and validated the effectivenessof bootstrapping approach to evaluate the document repositorytopicality.",
  "CONCLUSION": "In this paper, we introduced VERA, a framework tailored for evaluat-ing Retrieval-Augmented Generation (RAG) systems. By generatingLLM-based RAG evaluation metrics such as faithfulness, answerrelevance, retrieval precision and retrieval recall, VERA can helpevaluate and validate if the response from RAG based AI assistantis accurate or not. This framework boosts the reliability and trans-parency of RAG systems and build the trust in AI applications forusers.Our findings demonstrate VERAs capacity to enhance decision-making processes effectively. The framework has been appliedacross several use cases, illustrating its ability to adapt to dynamicenvironments and maintain the integrity of data repositories. Thisadaptability makes VERA an important tool in the landscape ofmodern AI technologies, where the accuracy and relevance of in-formation are paramount.Looking forward, we aim to further refine the metrics withinVERA and expand its applicability to a broader range of domainsand languages. Continuous advancements in VERAs methodologieswill allow it to keep pace with rapid technological developmentsin AI. This evolution will ensure that emerging AI technologiesare leveraged responsibly, maximizing their potential benefits forsociety.",
  "LIMITATIONS": "This paper presents several limitations that could potentially impactthe comprehensiveness and applicability of its conclusions. Firstly,the analysis omits scenarios involving fine-tuned LLMs. Potentialenhancements or specific use-case efficiencies brought by fine-tuned models might not be fully captured. This omission could leadto an incomplete understanding of the capabilities and limitationsof the models under different experimental conditions. And theexclusion of some top proprietary LLMs in our experiments, suchas OpenAI models, limits the evaluations scope and understandingof our selected models performance against the best availableoptions.Secondly, our study does not address multilingual capabilities.The focus solely on English-language tasks may limit the gener-alizability of our conclusions to multilingual applications. Thisoversight could restrict the utility of our findings for developersand researchers working on systems intended for diverse linguis-tic environments, potentially overlooking significant performancevariations across languages.Thirdly, although our bootstrap estimators offer a more con-vincing assessment of the content complexity within a documentrepository, they are computationally intensive. We aim to developa theoretically grounded, cost-effective measurement approach byconstructing a pseudo-bootstrap strategy. This strategy will utilizepre-calculated evaluation metrics instead of relying on bootstrapsampling from queries.",
  "ModelsQuerySetFaithfulnessRelevanceRecallPrecision": "Llama3 + e5-mistral-7b-instructSales0.930.030.710.040.610.070.540.09Llama3 + titan-embedding-text-G1Sales0.940.030.700.040.620.080.550.08Llama3 + bge-large-en-v1.5Sales0.930.030.700.050.600.070.530.10Haiku + e5-mistral-7b-instructSales0.930.020.720.050.620.070.550.06Haiku + titan-embedding-text-G1Sales0.940.030.710.040.630.070.560.07Haiku + bge-large-en-v1.5Sales0.930.020.700.050.610.080.560.09 Llama3 + e5-mistral-7b-instructBasketball0.940.030.670.050.560.070.430.09Llama3 + titan-embedding-text-G1Basketball0.930.030.660.060.530.070.420.08Llama3 + bge-large-en-v1.5Basketball0.940.030.660.050.540.080.450.10Haiku + e5-mistral-7b-instructBasketball0.950.020.660.060.530.090.430.09Haiku + titan-embedding-text-G1Basketball0.940.030.650.060.520.080.450.08Haiku + bge-large-en-v1.5Basketball0.930.020.660.050.540.080.440.08 Llama3 + e5-mistral-7b-instructRandom0.930.020.230.040.130.050.090.05Llama3 + titan-embedding-text-G1Random0.930.030.210.050.150.040.100.04Llama3 + bge-large-en-v1.5Random0.940.030.160.050.110.040.080.05Haiku + e5-mistral-7b-instructRandom0.940.020.240.060.120.040.100.04Haiku + titan-embedding-text-G1Random0.920.030.220.060.140.050.090.05Haiku + bge-large-en-v1.5Random0.930.030.170.050.140.040.080.05 Lastly, our study did not analyze all popular publicly availablebenchmarks such as the Knowledge Intensive Language Tasks(KILT) benchmark, which could have provided additional insightsinto the models capabilities in retrieving, reasoning, and synthe-sizing information from knowledge bases in real-world scenarios. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-cia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, ShyamalAnadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774(2023).",
  "Ching-Yao Chuang, Joshua Robinson, Yen-Chen Lin, Antonio Torralba, and Ste-fanie Jegelka. 2020. Debiased contrastive learning. Advances in neural informationprocessing systems 33 (2020), 87658775": "Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus,Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2024.Scaling instruction-finetuned language models. Journal of Machine LearningResearch 25, 70 (2024), 153. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, CarissaSchoenick, and Oyvind Tafjord. 2018. Think you have solved question answering?try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457 (2018).",
  "Tim Hesterberg. 2011. Bootstrap. Wiley Interdisciplinary Reviews: ComputationalStatistics 3, 6 (2011), 497526": "Yasuto Hoshi, Daisuke Miyashita, Youyang Ng, Kento Tatsuno, Yasuhiro Morioka,Osamu Torii, and Jun Deguchi. 2023. Ralle: A framework for developing and evalu-ating retrieval-augmented large language models. arXiv preprint arXiv:2308.10633(2023). Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, AnkurParikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, KentonLee, et al. 2019. Natural questions: a benchmark for question answering research.Transactions of the Association for Computational Linguistics 7 (2019), 453466.",
  "Dawn Lawrie, Sean MacAvaney, James Mayfield, Paul McNamee, Douglas WOard, Luca Soldaini, and Eugene Yang. 2024. Overview of the TREC 2023 NeuCLIRTrack. arXiv preprint arXiv:2404.08071 (2024)": "Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,Naman Goyal, Heinrich Kttler, Mike Lewis, Wen-tau Yih, Tim Rocktschel,et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks.Advances in Neural Information Processing Systems 33 (2020), 94599474. Yi Liu, Lianzhe Huang, Shicheng Li, Sishuo Chen, Hao Zhou, Fandong Meng,Jie Zhou, and Xu Sun. 2023. Recall: A benchmark for llms robustness againstexternal counterfactual knowledge. arXiv preprint arXiv:2311.08147 (2023).",
  "Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 2023.Query rewriting for retrieval-augmented large language models. arXiv preprintarXiv:2305.14283 (2023)": "David Nadeau, Mike Kroutikov, Karen McNeil, and Simon Baribeau. 2024. Bench-marking Llama2, Mistral, Gemma and GPT for Factuality, Toxicity, Bias andPropensity for Hallucinations. arXiv preprint arXiv:2404.09785 (2024). Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani,Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard,et al. 2020. KILT: a benchmark for knowledge intensive language tasks. arXivpreprint arXiv:2009.02252 (2020).",
  "Robert J Tibshirani and Bradley Efron. 1993. An introduction to the bootstrap.Monographs on statistics and applied probability 57, 1 (1993), 1436": "Rishabh Upadhyay, Arian Askari, Gabriella Pasi, and Marco Viviani. 2023. Enhanc-ing Documents with Multidimensional Relevance Statements in Cross-encoderRe-ranking. arXiv preprint arXiv:2306.10979 (2023). Boxin Wang, Wei Ping, Lawrence McAfee, Peng Xu, Bo Li, Mohammad Shoeybi,and Bryan Catanzaro. 2023. Instructretro: Instruction tuning post retrieval-augmented pretraining. arXiv preprint arXiv:2310.07713 (2023). Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxiang Shi, ZhixuLi, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023. Is chatgpt a good nlg evaluator? apreliminary study. arXiv preprint arXiv:2303.04048 (2023). Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang,Rangan Majumder, and Furu Wei. 2022. Text embeddings by weakly-supervisedcontrastive pre-training. arXiv preprint arXiv:2212.03533 (2022).",
  "Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and YoavArtzi. 2019. Bertscore: Evaluating text generation with bert. arXiv preprintarXiv:1904.09675 (2019)": "Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M Meyer, and Stef-fen Eger. 2019. MoverScore: Text generation evaluating with contextualizedembeddings and earth mover distance. arXiv preprint arXiv:1909.02622 (2019). Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu,Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024. Judgingllm-as-a-judge with mt-bench and chatbot arena. Advances in Neural InformationProcessing Systems 36 (2024). Ming Zhong, Yang Liu, Yichong Xu, Chenguang Zhu, and Michael Zeng. 2022.Dialoglm: Pre-trained model for long dialogue understanding and summarization.In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 36. 1176511773.",
  "Prompt of Faithfulness Metric": "Consider the given context and following statements, then deter-mine whether they are supported by the information present inthe context. Provide a brief explanation for each statement beforearriving at the verdict (Yes/No). Provide a final verdict for eachstatement in order at the end in the given format. Do not deviatefrom the specified format. Context:Emma is a graduate student specializing in marine biology at CoastalUniversity. She has a keen interest in coral reefs and is conduct-ing her thesis on coral bleaching. Emma attends several seminars",
  "Example 1:": "Context:Isaac Newton (25 December 1642 20 March 1726/27) was an Eng-lish mathematician, physicist, astronomer, alchemist, and author.He is widely recognized as one of the most influential scientistsof all time and a key figure in the scientific revolution. His book\"Philosophi Naturalis Principia Mathematica,\" first published in1687, laid the foundations of classical mechanics. Newton madeseminal contributions to optics and shares credit with GottfriedWilhelm Leibniz for developing calculus.",
  "Example 2:": "Context:Marie Curie (7 November 1867 4 July 1934) was a Polish andnaturalized-French physicist and chemist who conducted pioneer-ing research on radioactivity. She was the first woman to win aNobel Prize, the only woman to win the Nobel prize twice, andthe only person to win the Nobel Prize in two different scientificfields. Her achievements include the development of the theory ofradioactivity, techniques for isolating radioactive isotopes, and thediscovery of two elements, polonium and radium.",
  "Examples:": "Example 1:Question: What causes the tides to rise and fall?Context: The gravitational pull of the moon and the sun causesthe tides to rise and fall. The moons gravity has a greater effectbecause it is closer to the Earth, creating high and low tides. Thesun also plays a role, but to a lesser extent.Candidate Sentences:",
  "(1) Identify Key Information: Carefully read the given answerto identify the key pieces of information. These may includedates, times, locations, events, people, etc": "(2) Formulate the Question: Create a question that specificallyasks about the key information you identified in the answer.The question should be comprehensive and direct, ensuringit covers all the important details provided in the answer. (3) Ensure Clarity and Specificity: The question should beclear and specific, leaving no ambiguity about what informa-tion it seeks. It should be framed in a way that the answerprovided directly responds to it.",
  "Enhanced Document Context for Cross-Encoder": "Question: At about what age do adults normally begin to lose bonemass?Enhanced Text: The actual answer to the following question is:Based on the given context, adults typically begin to lose bone massaround the age of 40. The key points are: - Bone mass reaches itspeak during young adulthood, and then there is a slow but steadyloss of bone beginning about age 40. - After about age 30, people canstart to lose bone faster than their body makes it, which can weakenthe bones and increase the risk of breakage. - The reduction of bonemass begins between ages 30 and 40, and continues to decline. Sothe summarized response is that adults normally begin to lose bonemass around the age of 40.The context (which refers to text that was used to answer thisquestion) is: [Age. Theres no way around it: loss of bone masscomes with age, laying the groundwork for low bone density andthe potential of osteoporosis. We typically lose bone mass startingat age 40 and one in two women and one in four men over theage of 50 will fracture a bone at some point., After about age 30,you can start to lose bone faster than your body makes it, whichcan weaken the bones and increase the risk of breakage. Somebone loss is natural as men and women age, but women are athigher risk of significant bone loss., Bone mass reaches its peakduring young adulthood. Then, after a period of stability, there isa slow but steady loss of bone beginning about age 40. In women,normal aging and menopause significantly increase susceptibility toosteoporosis., In adults, this can take ten years. Until our mid-20s,bone density is still increasing. But at 35 bone loss begins as partof the natural ageing process. This becomes more rapid in post-menopausal women and can cause the bone-thinning conditionosteoporosis., The reduction of bone mass begins between ages 30and 40, and continues to decline. Women lose about 8% of skeletalmass every decade, while men lose about 3%. Epiphyses, vertebrae,and the jaws lose more mass than other sites, resulting in fragilelimbs, reduction in height, and loss of teeth.].Answer Relevancy assesses how pertinent the actual answer isto the given context. It is measured between 0 and 1; where a lowerscore is given to answers that are incomplete or contain redundant information; and a higher score indicates better relevancy. For thegiven question, context and answer, the answer relevancy score is:0.9531866263993314.Context Precision assesses how relevant is every context towardsanswering the question. Ideally all of the text in all of the contextsshould be relevant to the question. It is measured betwen 0 and1; where a lower score is given to lower precision contexts; and ahigher score indicates more precision. For the given question andcontexts, the context precision score is: 0.06666666666666667.Context recall measures the extent to which the context alignswith the ground truth. It is computed based on attributing text inthe ground truth to the context, and is measured between 0 and 1;where a lower score is given to lower recall contexts; and a higherscore indicates better performance. For the given ground truth andcontexts, the context recall score is: 0.2727272727272727Faithfulness measures the factual consistency of the generatedanswer against the given context. It is considered faithful if all theclaims that are made in the answer can be inferred from the givencontext. It is measured between 0 and 1; where a lower score isgiven to answers consisting of claims that are not in the context;and a higher score indicates that the answer is using informationfrom the contexts. For the given question, context and answer, thefaithfulness score is: 1.0.",
  "Task:Generate a passage on a random topic unrelated to cloud computa-tion sales and marketing or basketball and a corresponding questionbased on the passage": "Example:Passage:The history of the automobile is marked by continuous innova-tion and technological advancements. From the invention of theinternal combustion engine to the development of electric cars, theautomotive industry has always been at the forefront of engineer-ing and design. Modern cars are equipped with advanced safetyfeatures, autonomous driving capabilities, and environmentallyfriendly technologies.",
  "Unbiased Estimator": "Let : be a black box function mapping from the inputspace (queries) to the output space (LLM-based metrics space).For each query , the output is a vector = (1,2,3,4) where1,2,3 and4 represent Retrieval Precision, Retrieval Recall, Faith-fulness, and Answer Relevance, respectively.Assume a dataset = {}=1 where each is a query. Associ-ated with each query is a vector of metrics = (1,2,3,4).For each metric (where = 1, 2, 3, 4 corresponding to the fourmetrics), the estimator based on observations is given by thesample mean:"
}