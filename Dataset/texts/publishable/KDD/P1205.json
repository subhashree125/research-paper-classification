{
  "ABSTRACT": "Source-free domain adaptation (SFDA) aims to adapt a pretrainedmodel from a labeled source domain to an unlabeled target domainwithout access to the source domain data, preserving source do-main privacy. Despite its prevalence in visual applications, SFDA islargely unexplored in time series applications. The existing SFDAmethods that are mainly designed for visual applications may failto handle the temporal dynamics in time series, leading to im-paired adaptation performance. To address this challenge, this pa-per presents a simple yet effective approach for source-free domainadaptation on time series data, namely MAsk and imPUte (MAPU).First, to capture temporal information of the source domain, ourmethod performs random masking on the time series signals whileleveraging a novel temporal imputer to recover the original sig-nal from a masked version in the embedding space. Second, inthe adaptation step, the imputer network is leveraged to guidethe target model to produce target features that are temporallyconsistent with the source features. To this end, our MAPU canexplicitly account for temporal dependency during the adaptationwhile avoiding the imputation in the noisy input space. Our methodis the first to handle temporal consistency in SFDA for time series",
  "Corresponding Author": "Publication rights licensed to ACM. ACM acknowledges that this contribution wasauthored or co-authored by an employee, contractor or affiliate of a national govern-ment. As such, the Government retains a nonexclusive, royalty-free right to publish orreproduce this article, or to allow others to do so, for Government purposes only.KDD 23, August 610, 2023, Long Beach, CA, USA 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0103-0/23/08...$15.00 data and can be seamlessly equipped with other existing SFDAmethods. Extensive experiments conducted on three real-worldtime series datasets demonstrate that our MAPU achieves signifi-cant performance gain over existing methods. Our code is availableat",
  "Source-free domain adaptation, time series data, temporal imputa-tion": "ACM Reference Format:Mohamed Ragab, Emadeldeen Eldele, Min Wu, Chuan-Sheng Foo, Xiaoli Li,and Zhenghua Chen. 2023. Source-Free Domain Adaptation with TemporalImputation for Time Series Data. In Proceedings of the 29th ACM SIGKDDConference on Knowledge Discovery and Data Mining (KDD 23), August610, 2023, Long Beach, CA, USA. ACM, New York, NY, USA, 10 pages.",
  "INTRODUCTION": "Deep learning has achieved impressive performance in numeroustime series applications, such as machine health monitoring, humanactivity recognition, and healthcare. However, this success heavilyrelies on the laborious annotation of large amounts of data. To ad-dress this issue, unsupervised domain adaptation (UDA) has gainedtraction as a way to leverage pre-labeled source data for training",
  "KDD 23, August 610, 2023, Long Beach, CA, USAMohamed Ragab et al": "the versatility of our method by combining it with different SFDAtechniques. Furthermore, we compare the effectiveness of our taskto other auxiliary tasks on time series data. Lastly, we examine ourmodels sensitivity to different importance weights and maskingratios. In our MAPU, we leverage SHOT as the base SFDA method.Nevertheless, our approach is not limited to SHOT and can beeffectively integrated with other SFDA methods, as demonstratedin our versatility experiments.",
  "RELATED WORK2.1Time series Domain Adaptation": "Several methods have been proposed to address the challengeof distribution shift in time series data. These methods can bebroadly categorized into two groups: discrepancy-based methodsand adversarial-based methods. Discrepancy-based methods use sta-tistical distances to align the feature representations of the sourceand target domains. For instance, AdvSKM leverages the maximummean discrepancy (MMD) distance in combination with a hybridspectral kernel to consider temporal dependencies during domainadaptation . Another example is SASA, which learns the asso-ciation structure of time series data to align the source and targetdomains . On the contrary, adversarial-based methods use adver-sarial training to mitigate the distribution shift between the sourceand target domains. For instance, CoDATS utilizes a gradient rever-sal layer (GRL) for adversarial training with weak supervision onmulti-source human activity recognition data . Furthermore,DA_ATTN couples adversarial training with an un-shared atten-tion mechanism to preserve the domain-specific information .Recently, SLARDA presents an autoregressive adversarial trainingapproach for aligning temporal dynamics across domains .Albeit promising, the design of these methods is based on theassumption that source data is available during the adaptation step.However, accessing source data may not be possible in practicalsituations due to privacy concerns or storage limitations. Differ-ently, our MAPU adapts a model pretrained on source data to newdomains without access to source data during adaptation, whichcan be a more practical solution for high-stake applications.",
  "Source-Free Domain Adaptation with Temporal Imputation for Time Series DataKDD 23, August 610, 2023, Long Beach, CA, USA": "TimeTime TimeTimeTimeTime TimeTime Capturing Source DynamicsAdapting Target Temporal Dynamics : Adaptation with Temporal Imputation. Left: A temporal imputer network is trained to predict the full sequencefrom its masked version to capture the temporal information of the source domain. Right: Once trained, the temporal imputernetwork guides the target model to produce features that are temporally consistent with the source domain. (Best in viewed incolors. Components in red color are trainable, while those in gray color are non-trainable).",
  "Recurrent Imputer": "Adaptation with Feature Imputation : Adaptation with Temporal Imputation for time series data. Left: The pretraining stage of the temporal imputernetwork to capture the temporal dynamics of the source domain. First, we perform random masking across the timedimension of the source signal. Given the original source signal and its temporally masked signal , the encoder network is used to generate the corresponding latent features and respectively. Subsequently, is updated to produce imputedfeatures from masked features using the mean square error loss. Right: The adaptation stage of the encoder network onthe target domain data. The encoder is updated to produce source-like features that are imputable by the pretrained . approach is to leverage a model pretrained on the source domainto generate synthetic source-like data during the adaptation step. Another approach is to use adversarial trainingbetween multiple classifiers to generalize well to the target classes. Another prevalent approach uses softmax scores or their corresponding entropy to prioritize confident samples for pseudo-labeling, assuming that the model should be more confident onsource samples and less confident on target samples .Despite the strong potential demonstrated by these methods,they are primarily designed for visual applications and may fail toeffectively align temporal dynamics in time series data. In contrast,our method addresses this challenge through a novel temporal",
  "METHODOLOGY3.1Problem definition": "Given a labeled source domain D = {,}=1, where Xcan be a uni-variate or multi-variate time series data with a se-quence length , while Y represents the corresponding labels.In addition, we have an unlabeled target domain D = { }=1,where X , and it also shares the same label space with D.Following the existing UDA settings, we assume a difference acrossthe marginal distributions, i.e., () ( ), while the condi-tional distributions are stable, i.e., ( |) ( | ).This work aims to address the source-free domain adaptationproblem, where access to source data is strictly prohibited duringthe adaptation phase to ensure data privacy. Furthermore, we adoptthe vendor-client source-free paradigm , which allowsthe influence of the source pretraining stage. This assumption isrealistic in use cases where there is a collaboration between variousentities, but it is not possible to share source data due to data privacy,security, or regulatory issues.",
  "Overview": "We present our MAPU to achieve source-free adaptation on timeseries temporal data while considering the temporal dependenciesacross domains. The pipeline of the proposed method is illustratedin . Given the input signal and a temporally masking signal,our method comprises two stages: (1) training an autoregressivenetwork, referred to as the imputer network, which captures thetemporal information of the source domain through a novel tem-poral imputation task, and (2) leveraging the source-pretrainedimputer network to guide the target encoder towards producingtemporally consistent target features in the adaptation stage. Next,we will first elaborate on the temporal masking procedure beforedelving into the details of each stage.",
  "Temporal Masking": "In this section, we explain our process of temporal masking. Westart by dividing the input signal, , into several blocks along thetime dimension. Then, we randomly choose some of these blocksand set their values to zero, creating a masked version of the signalcalled . This process is applied to both the source and targetdomains. Our aim is to challenge the model to use the informationfrom surrounding blocks to fill in the missing parts and capture thetemporal dependencies in the input signal. Further discussion onthe impact of the masking ratio on the adaptation performance canbe found in the experiment section.",
  "Capturing Source Temporal Dynamics": "In the pretraining stage, current methods typically map the sourcedata from the input space to the feature space using an encodernetwork, represented as : X H. The extracted featuresare then passed through a classifier network, : H Y, tomake class predictions for the source data. However, to effectivelyadapt to other time series domains, it is important to consider the temporal relations in the source domain. Using only cross-entropyfor training the source network may neglect this aspect. To addressthis, we propose a temporal imputation task that aims to recoverthe input signal from a temporally masked signal in the featurespace.The imputation task is performed by an imputer network that takes the masked signal and maps it to the original signal.The input signal and masked signal are first transformedinto their corresponding feature representations and bythe encoder . The task of the imputer network is represented as = ( ( )) = (), where is the imputed signal.The imputer network is trained to minimize the mean square errorbetween the features of the original signal and the imputed signal,which can be formulated as:",
  "Temporal Adaptation with FeatureImputation": "In the adaptation stage, the goal is to train the target encoder net-work to produce target features temporally consistent with thesource features. The target encoder network is used to extract la-tent feature representations from a target sample and its maskedversion . The fixed source-pretrained imputer network is thenused to reconstruct the features of the original signal from themasked features. However, due to domain differences, the sourceimputer may not be able to accurately reconstruct the target fea-tures. Thus, the encoder network is updated to produce targetfeatures that can be accurately reconstructed by the imputer net-work. This can be expressed as the following optimization problem:",
  "( ) ( ( )))22 ,(2)": "where = ( ) are the original target features, = ( ( ))are the adapted target features produced by the imputer networkto minimize the mean square error loss, and is the total numberof target samples. Notably, only the encoder network is optimized,producing features that can be accurately imputed by the fixedsource-pretrained imputer network. To reduce the imputation loss,the adapted target features should be temporally consistent withthe source features.Algorithm 1 illustrates the adaptation procedure via temporalimputation. The process starts by first constructing a temporallymasked version of the input target sample represented as . Next,the source-pretrained encoder is used to extract the latent featuresof both the original signal and the temporally masked signal, repre-sented as and , respectively. Finally, the encoder network isupdated to make features of the masked signal recoverable by thesource-pretrained imputer network, using the mean square errorloss in Equation 2.",
  "Classifier": "Temporal ImputationEncoder : Integrating our temporal imputation with other source-free methods (Best viewed in colors). Left: In the pretrainingstage, the source model is trained using conventional cross-entropy loss L, and the temporal imputer network is trainedusing L to impute the features of the masked signals and capture the source temporal information on the feature space.Right: In the adaptation stage, the target model is jointly trained with both a generic source-free adaptation loss L and our",
  "Integration with Other Source-free Methods": "Our proposed MAPU is generic and can be integrated with othersource-free adaptation methods. Typically, source-free adaptationinvolves a two-stage training procedure: (1) pretraining the sourcemodel with source domain data, and (2) adapting the pretrainedmodel to the target domain. As shown in , our MAPU canbe seamlessly integrated into existing SFDA methods in both stages.In the pretraining stage, MAPU operates in the feature space bytraining the temporal imputation network, , to capture the tem-poral information from the source domain. The loss associated withthe temporal imputation task does not propagate to the encodermodel, . As a result, the encoder can be trained exclusively withthe conventional cross-entropy loss, ensuring that the imputationtask does not negatively impact the pretraining performance. Thetotal pretraining loss is formalized as:",
  "where Lce = =1 1[=] log( ) represents the standard cross-entropy loss between the predicted label and the true label,": "represents the predicted probability for class and sample , andLmapu represents the training loss for our temporal imputer net-work on the source data to capture the source temporal information.In the target adaptation step, the objective is to optimize thetarget encoder, , by balancing the temporal imputation loss andthe generic source-free loss to achieve temporal consistency andperform adaptation on the target domain. This can be formalizedas follows:",
  "EXPERIMENTAL SETTINGS4.1Datasets": "We evaluate our proposed method on three real-world datasetsspanning three time series applications, i.e., machine fault diagno-sis, human activity recognition, and sleep stage classification. Theselected datasets differ in many aspects, as illustrated in ,which leads to a considerable domain shift across different domains. 4.1.1UCIHAR Dataset. This dataset focuses on human activityrecognition tasks. Three types of sensors have been used to collectthe data, i.e., accelerator sensor, gyroscope sensor, and body sensor,where each sensor provides three-dimensional readings, leading toa total of 9 channels per sample, with each sample containing 128data points. The data is collected from 30 different users and each",
  "Implementation Details": "Encoder Design. In our study, we adopt the encoder architecturepresented in existing works , which is a 1-dimensional convo-lutional neural network composed of three layers with filter sizes of64, 128, and 128 respectively. Each conventional layer was followedby the application of a rectified linear unit activation function andbatch normalization. MAPU Parameters. For the purpose of temporal masking, a mask-ing ratio of 1/8 is utilized across all datasets in our experiments. Toperform the imputation task, a single-layer recurrent neural net-work with a hidden dimension of 128 is employed for all datasets. Inaddition, our method includes a primary hyperparameter, , whichis set to 0.5 for all datasets in our evaluation. Unified Training Scheme. To provide a fair and valid comparisonwith source-free baseline methods, we adhered to their establishedimplementations while incorporating the same back-bone network and training procedures utilized in our proposedmethod. In accordance with the AdaTime framework , all themodels are trained for a total of 40 epochs, using a batch size of 32,with a learning rate of 1e-3 for UCIHAR and 1e-4 for SSC and MFD.Also, the macro F1-score (MF1) metric has been used to ensure",
  "Baseline Methods": "To evaluate the performance of our model, we compare it againstconventional UDA approaches that assume access to source dataduring adaptation. These baselines are adapted from the AdaTimebenchmark . Additionally, we compare our model against recentsource-free domain adaptation methods. To ensure fair evaluation,we re-implement all source-free baselines in our framework, whileensuring the same backbone network and training schemes. Overallthe compared methods are as follows:",
  "Source-free methods": "Source Hypothesis Transfer (SHOT) : minimizes infor-mation maximization loss with self-supervised pseudo labelsto identify target features that can be compatible with thetransferred source hypothesis. Exploiting the intrinsic neighborhood structure (NRC) :captures the intrinsic structure of the target data by formingclear clusters and encouraging label consistency among datawith high local affinity. Attracting and dispersing (AaD) : optimizes an objectiveof prediction consistency by treating SFDA as an unsuper-vised clustering problem and encouraging local neighbor-hood features in feature space to have similar predictions.",
  "Quantative Results": "To assess the efficacy of our approach, we evaluate its performanceon three different time series datasets, namely, UCIHAR, SSC, andMFD. Tables 2, 3, and 4 present results for five cross-domain sce-narios in each dataset, as well as an average performance across allscenarios (AVG). The algorithms are divided into two groups: thetraditional UDA methods are marked with , while the source-freemethods are marked with . 5.1.1Evaluation on UCIHAR Dataset. The results presented in Ta-ble 2 show the performance of our MAPU in five cross-subjectscenarios. Our method demonstrates superior performance in threeof the five scenarios, achieving an overall performance of 89.57%.This exceeds the second-best source-free method by 3%. Notably, thesource-free methods (i.e., SHOT, NRC, and AaD) perform competi-tively with conventional unsupervised domain adaptation (UDA)methods that utilize source data. This can be attributed to the two-stage training (i.e., pertaining and adaptation) scheme employed inthe source-free methods, which focuses on optimizing the targetmodel for the target domain without considering source perfor-mance . Furthermore, our MAPU, with its temporal adaptationcapability, outperforms all conventional UDA methods, surpassingthe best method (i.e., CDAN) by 2.78%. 5.1.2Evaluation on SSC Dataset. The results of the sleep stageclassification task, as presented in , demonstrate the superiorperformance of our proposed method, MAPU, over other baselinemethods. Our MAPU performs best in three out of the five cross-domain scenarios, with an overall performance of 64.05%. Thisis higher than the best source-free method, SHOT, and the bestconventional UDA method, with an improvement of 1.72% and1.27% respectively. It is worth noting that source-free methods thatrely on features clustering, i.e., NRC and AaD, perform poorly onthe SSC dataset due to its class-imbalanced nature. However, ourMAPU, with its temporal adaptation capability, is able to handlesuch imbalance and outperform all source-free methods with amaximum improvement of 4.8% in scenario 16 1. 5.1.3Evaluation on MFD Dataset. The results of the Machine FaultDiagnosis (MFD) task, presented in , showcase the superiorperformance of our MAPU when compared to all other baselines.With an average performance of 92.45%, MAPU exceeds the second-best method by a large margin of 7.85%. Additionally, MAPU sig-nificantly outperforms baseline methods in the hard transfer tasks(i.e., 01 and 10), reaching a 14.46% improvement in the latterscenario, while performing competitively with other baseline meth-ods in the easy transfer tasks (i.e., 2 3 and 3 1). Compared tosource-free methods, our MAPU achieves the best performance in",
  "AaD + Temporal87.0064.0591.11": "all cross-domains, surpassing the second-best source-free method,AaD, by 11.87%.It is worth noting that the performance improvement of ourmethod is relatively large in the MFD dataset compared to otherdatasets. This is mainly attributed to two reasons. First, the MFDdataset has the longest sequence length among all other datasets,thus, the adaptation of temporal information is more prominent andnecessary. Second, unlike other datasets, this dataset has a limitednumber of classes, i.e., 3 classes, and thus, failing to correctly classifyone class can significantly harm the performance.",
  "Ablation Study on Auxiliary Tasks": "To demonstrate the effectiveness of our proposed temporal imputa-tion auxiliary task, we conducted evaluations using various auxil-iary tasks, including rotation prediction and jigsaw puzzle .We chose three different SFDA backbones, SHOT, NRC, and AaD, forthe auxiliary tasks to eliminate the bias to a specific SFDA method. shows the average performance of five cross-domain scenar-ios for each dataset. The results show that our temporal imputationtask consistently outperforms the other tasks across all datasets,even when combined with different SFDA backbones. Meanwhile,the baseline tasks, including rotation and jigsaw, not only exhibitlimited improvement but also consistently harm the performance inmany cases across various datasets. This indicates the inadequacyof these tasks for time series data and highlights the importance ofconsidering temporal dynamics to the adaptation performance, asdemonstrated by the superior performance of our MAPU approach.",
  "Model Analysis": "5.3.1Versatility Analysis. This study investigates the effectivenessof incorporating temporal information into other SFDA methods.To achieve that, we evaluated the performance of three differentSFDA methods when used in conjunction with our proposed tempo-ral imputation task on the UCIHAR, SSC, and MFD datasets. shows the average performance of five cross-domain scenariosin each dataset. Our results indicate a significant improvement inperformance across all tested datasets through the integration of",
  ": Analysis of adaptation performance with varyingrelative weight for the temporal imputation component": "our temporal imputation task. For instance, on the UCIHAR dataset,we saw a notable 3% boost in performance for the NRC and AaDmethods. On the UCIHAR dataset, the NRC and AaD methods allexperienced a performance boost of approximately 3% upon inte-gration with our temporal imputation task. The improvements areconsistent across the SSC and MFD datasets, demonstrating ourapproachs effectiveness in providing temporal adaptation capabil-ity to existing SFDA methods that are mainly proposed for visualapplications. 5.3.2Sensitivity Analysis. This study evaluates the sensitivity ofour temporal imputation component to the relative weight whenintegrated with other SFDA methods, as illustrated in .The results indicate that our models performance is relativelystable across a range of values for the parameter. Particularly, thehighest MF1 score achieved was 89.77, while the lowest accuracywas 87.75, with a difference of only 2%. This observed stability maybe attributed to the imputation process being carried out on thefeature space rather than the input space. As such, the feature spaceprovides a more abstract representation of the data, making theimputation process free of the variations present in the input space. 0.1250.250.5",
  ": Effect of temporal masking ratio on the adaptationperformance for the three datasets": "5.3.3Impact of Masking level. Here, we systematically examine theimpact of the masking ratio on adaptation performance in the con-text of imputation tasks. Specifically, we employed three differentmasking ratios (12.5%, 25%, and 50%) and evaluated the performanceon the three benchmark datasets. The results, shown in ,reveal a clear trend of improved performance with lower maskingratios. Notably, the best performance was achieved with a maskingratio of 12.5% across all datasets. These findings suggest that exces-sive masking may negatively impact the adaptation performancein the imputation task.",
  "CONCLUSION": "This paper introduced MAsk And imPUte (MAPU), a novel methodfor source-free domain adaptation on time series data. The pro-posed method addressed the challenge of temporal consistency intime series data by proposing a temporal imputation task to re-cover the original signal in the feature space rather than the inputspace. MAPU is the first method to explicitly account for temporaldependency in a source-free manner for time series data. The effec-tiveness of MAPU is demonstrated through extensive experimentson three real-world datasets, achieving significant gains over theexisting methods. This work highlights the potential of MAPU in",
  "This work was supported by the Agency of Science Technology andResearch under its AME Programmatic (Grant No. A20H6b0151)and its Career Development Award (Grant No. C210112046)": "Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra, and Jorge LuisReyes-Ortiz. 2013. A public domain dataset for human activity recognition usingsmartphones. In European Symposium on Artificial Neural Networks. Silvia Bucci, Antonio DInnocente, Yujun Liao, Fabio M Carlucci, Barbara Caputo,and Tatiana Tommasi. 2021. Self-supervised learning across domains. IEEETransactions on Pattern Analysis and Machine Intelligence 44, 9 (2021), 55165528. Ruichu Cai, Jiawei Chen, Zijian Li, Wei Chen, Keli Zhang, Junjian Ye, ZhuozhangLi, Xiaoyan Yang, and Zhenjie Zhang. 2021. Time Series Domain Adaptation viaSparse Associative Structure Alignment. In AAAI.",
  "Tong Chu, Yahao Liu, Jinhong Deng, Wen Li, and Lixin Duan. 2022. Denoised Max-imum Classifier Discrepancy for Source-Free Unsupervised Domain Adaptation.In AAAI": "Tong Chu, Yahao Liu, Jinhong Deng, Wen Li, and Lixin Duan. 2022. DenoisedMaximum Classifier Discrepancy for Source-Free Unsupervised Domain Adap-tation. In Proceedings of the AAAI conference on artificial intelligence, Vol. 36.472480. Emadeldeen Eldele, Zhenghua Chen, Chengyu Liu, Min Wu, Chee-Keong Kwoh,Xiaoli Li, and Cuntai Guan. 2021. An Attention-based Deep Learning Approachfor Sleep Stage Classification with Single-Channel EEG. IEEE Transactions onNeural Systems and Rehabilitation Engineering (2021).",
  "Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, Chee KeongKwoh, Xiaoli Li, and Cuntai Guan. 2021. Time-Series Representation Learningvia Temporal and Contextual Contrasting. In IJCAI": "Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, HugoLarochelle, Franois Laviolette, Mario Marchand, and Victor Lempitsky. 2016.Domain-adversarial training of neural networks. JMLR (2016). Ary L Goldberger, Luis AN Amaral, Leon Glass, Jeffrey M Hausdorff, Plamen ChIvanov, Roger G Mark, Joseph E Mietus, George B Moody, Chung-Kang Peng, andH Eugene Stanley. 2000. PhysioBank, PhysioToolkit, and PhysioNet Componentsof a New Research Resource for Complex Physiologic Signals. Circulation (2000).",
  "Youngeun Kim, Donghyeon Cho, Kyeongtak Han, Priyadarshini Panda, andSungeun Hong. 2021. Domain Adaptation Without Source Data. IEEE Transactionson Artificial Intelligence 2 (2021), 508518": "Divya Kothandaraman, Rohan Chandra, and Dinesh Manocha. 2021. SS-SFDA: Self-Supervised Source-Free Domain Adaptation for Road Segmentation inHazardous Environments. 2021 IEEE/CVF International Conference on ComputerVision Workshops (ICCVW) (2021), 30423052. Jogendra Nath Kundu, Suvaansh Bhambri, Akshay Kulkarni, Hiran Sarkar, VarunJampani, and R Venkatesh Babu. 2022. Concurrent subsidiary supervision forunsupervised source-free domain adaptation. In Computer VisionECCV 2022:17th European Conference, Tel Aviv, Israel, October 2327, 2022, Proceedings, PartXXX. Springer, 177194. Jogendra Nath Kundu, Akshay R Kulkarni, Suvaansh Bhambri, Deepesh Mehta,Shreyas Anand Kulkarni, Varun Jampani, and Venkatesh Babu Radhakrishnan.2022. Balancing discriminability and transferability for source-free domainadaptation. In International Conference on Machine Learning. PMLR, 1171011728. Jogendra Nath Kundu, Naveen Venkat, R Venkatesh Babu, et al. 2020. Universalsource-free domain adaptation. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition. 45444553. Vinod K Kurmi, Venkatesh K Subramanian, and Vinay P Namboodiri. 2021.Domain impression: A source data free domain adaptation method. In Proceedingsof the IEEE/CVF Winter Conference on Applications of Computer Vision. 615625. Christian Lessmeier, James Kuria Kimotho, Detmar Zimmer, and Walter Sextro.2016. Condition monitoring of bearing damage in electromechanical drive sys-tems by using motor current signals of electric motors: A benchmark data set fordata-driven classification. In PHM Society European Conference, Vol. 3. Rui Li, Qianfen Jiao, Wenming Cao, Hau-San Wong, and Si Wu. 2020. Modeladaptation: Unsupervised domain adaptation without source data. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 96419650.",
  "Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I. Jordan. 2018.Conditional Adversarial Domain Adaptation. In NeurIPS": "Zhen Qiu, Yifan Zhang, Hongbin Lin, Shuaicheng Niu, Yanxia Liu, Qing Du, andMingkui Tan. 2021. Source-free Domain Adaptation via Avatar Prototype Gener-ation and Adaptation. In International Joint Conference on Artificial Intelligence. Mohamed Ragab, Emadeldeen Eldele, Zhenghua Chen, Min Wu, Chee-KeongKwoh, and Xiaoli Li. 2022. Self-supervised Autoregressive Domain Adaptationfor Time Series Data. IEEE Transactions on Neural Networks and Learning Systems(2022). Mohamed Ragab, Emadeldeen Eldele, Wee Ling Tan, Chuan-Sheng Foo, ZhenghuaChen, Min Wu, Chee-Keong Kwoh, and Xiaoli Li. 2023. ADATIME: A Benchmark-ing Suite for Domain Adaptation on Time Series Data. ACM Trans. Knowl. Discov.Data 17, 8, Article 106 (may 2023), 18 pages. Mohammad Mahfujur Rahman, Clinton Fookes, Mahsa Baktashmotlagh, andSridha Sridharan. 2020. On Minimum Discrepancy Estimation for Deep DomainAdaptation. Domain Adaptation for Visual Understanding (2020)."
}