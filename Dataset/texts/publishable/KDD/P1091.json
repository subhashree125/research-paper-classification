{
  "ABSTRACT": "Recent efforts have been dedicated to enhancing time series fore-casting accuracy by introducing advanced network architecturesand self-supervised pretraining strategies. Nevertheless, existingapproaches still exhibit two critical drawbacks. Firstly, these meth-ods often rely on a single dataset for training, limiting the modelsgeneralizability due to the restricted scale of the training data. Sec-ondly, the one-step generation schema is widely followed, whichnecessitates a customized forecasting head and overlooks the tem-poral dependencies in the output series, and also leads to increasedtraining costs under different horizon length settings.To address these issues, we propose a novel generative pretrainedhierarchical transformer architecture for forecasting, named GPHT.There are two aspects of key designs in GPHT. On the one hand,we advocate for constructing a mixed dataset under the channel-independent assumption for pretraining our model, comprisingvarious datasets from diverse data scenarios. This approach signif-icantly expands the scale of training data, allowing our model touncover commonalities in time series data and facilitating improvedtransfer to specific datasets. On the other hand, GPHT employs anauto-regressive forecasting approach, effectively modeling tempo-ral dependencies in the output series. Importantly, no customizedforecasting head is required, enabling a single model to forecast at ar-bitrary horizon settings. We conduct sufficient experiments on eightdatasets with mainstream self-supervised pretraining models and",
  "Mingyue Cheng is the corresponding author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain. 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08...$15.00 supervised models. The results demonstrated that GPHT surpassesthe baseline models across various fine-tuning and zero/few-shotlearning settings in the traditional long-term forecasting task, pro-viding support for verifying the feasibility of pretraining time serieslarge models. We make our codes publicly available1.",
  "INTRODUCTION": "Time series forecasting is one of the fundamental tasks in timeseries analysis, garnering significant attention over the past severalyears . Precise forecasting plays a crucial role in assistingvarious real-world applications, including climate report ,patient vital sign assessments , urban computing and stockprediction etc. Various efforts have been devoted to this area formore accurate forecasting. Notably, deep-learning-based methodshave achieved great success due to their capability to capture bothtemporal and cross-dimension dependencies .On the other hand, inspired by recent significant advancementsin pretraining methods in both the NLP and CV fields ,various pretraining-based time series analysis methods have beenproposed . The contrastive learning technique is widely em-ployed in the discriminative pretraining methods, where the models",
  "KDD 24, August 2529, 2024, Barcelona, Spain.Zhiding Liu et al": "David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. 2020.DeepAR: Probabilistic forecasting with autoregressive recurrent networks. Inter-national Journal of Forecasting 36, 3 (2020), 11811191. Mohammad Amin Shabani, Amir H Abdi, Lili Meng, and Tristan Sylvain. 2022.Scaleformer: Iterative Multi-scale Refining Transformers for Time Series Fore-casting. In The Eleventh International Conference on Learning Representations. Zezhi Shao, Zhao Zhang, Fei Wang, and Yongjun Xu. 2022. Pre-training enhancedspatial-temporal graph neural network for multivariate time series forecasting.In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery andData Mining. 15671577.",
  "RELATED WORKS2.1Time Series Forecasting": "Time series forecasting is a crucial task with broad applications,garnering significant attention in recent years. Early research pre-dominantly focuses on statistical methods such as ARIMA ,which builds an auto-regressive model and forecasts in a movingaverage fashion. However, these methods may encounter limita-tions in long-term forecasting settings. The advent of deep learninghas led to the development of numerous models capturing bothtemporal and cross-dimensional dependencies in multivariate timeseries, utilizing modern architectures .Transformer-based and MLP-based approaches have emergedas research hotspots due to their outstanding performance . Beyond model architecture, several customized techniquesrooted in time series analysis have been established. These in-clude trend-seasonal decomposition , time-frequency conver-sion , series stabilization , and patching . The integra-tion of these advanced studies enables contemporary forecastersto achieve unprecedented accuracy in predictions across diversescenarios through supervised training.",
  "Self-supervised Pretraining in Time SeriesModeling": "2.2.1Discriminative methods. Contrastive learning is widely uti-lized in the discriminative time series modeling approaches, aimingto derive crucial representations from pre-defined positive andnegative pairs. A key challenge lies in effectively constructing in-formative instance pairs.In practice, TNC takes advantage of the local smoothnessof a signals generative process to define neighborhoods in timewith stationary properties, and TS2Vec proposes to employcontrastive learning on both instance level and patch level in a hi-erarchical way for robust contextual representation learning. More-over, TS-TCC introduces a new contrastive learning task ofcross-view prediction. Subsequently, CoST comprises both timedomain and frequency domain contrastive losses to learn discrimi-native trend and seasonal representations. Note that discriminativemethods primarily concentrate on coarse-grained instance-level",
  "information, leading to unsatisfactory performance in forecastingtasks where fine-grained temporal features are essential": "2.2.2Generative methods. Generative pretraining methods usuallyfollow a paradigm of reconstruction. In the context of time seriesanalysis, the objective of masked time-series modeling has been ex-tensively explored. TST pioneers the use of traditional maskedmodeling, aiming to predict the removed time series points basedon the remaining ones. Subsequently, STEP and PatchTST expand on this concept to a sub-series level, where local infor-mation is more effectively captured, and computational costs aresignificantly reduced. Furthermore, a recent study achievessuperior fine-tuning performance by introducing a novel maskedmodeling task, involving the reconstruction of the original timeseries from multiple randomly masked series. Besides, TimeMAE significantly surpasses previous competitive baselines in clas-sification tasks by leveraging decoupled masked autoencoders tolearn robust representations through two pretext tasks: maskedcodeword classification and masked representation regression.On the other hand, the forecast-as-pretraining schema has alsobeen a subject of recent research. ForecastPFN introduces aprior-data fitted network trained on synthetic data and achievesaccurate zero-shot forecasting on univariate time series. Besides,both TimeGPT-1 and PreDcT explore the potential oftraining a foundation model for forecasting, yielding zero-shot fore-casting capability under relatively short horizon lengths. Moreover,there is another related line of work focusing on adapting tradi-tional pretrained generative language models to the time seriesdomain, either through prompting or fine-tuning . Theseapproaches have demonstrated competitive results when comparedto the traditional supervised approaches. Despite the effectiveness, existing methods exhibit two key short-comings. Firstly, these models are typically trained on a singledataset with limited scale and patterns, impeding their ability togeneralize to diverse forecasting scenarios. Secondly, both super-vised and self-supervised approaches often necessitate a customizedforecasting head, incurring multiple training costs under differenthorizon length settings. Regarding our proposed method, GPHT, itstands out as a generative self-supervised pretraining approach, dis-tinguishing itself from existing methods by effectively addressingthese issues. In detail, we investigate the feasibility of constructing amixed dataset for training our model in an auto-regressive manner.This approach ensures the superior generalizability of GPHT, al-lowing it to be seamlessly adapted to any dataset, including unseendatasets, and forecast at arbitrary horizon lengths. Notably, experi-ments demonstrate that our method surpasses the baseline modelsacross various fine-tuning and zero/few-shot learning settings inthe traditional long-term forecasting task.",
  "Problem Definition": "Given an input series X R, a time series forecasting model istasked with precisely predicting future values Y R . Here, ,, and represent the lookback window length, horizon length,and the number of channels, respectively.GPHT adopts the channel-independent assumption ,treating each multivariate time series as multiple independent uni-variate time series. In essence, GPHT conducts individual fore-casting on each variate within the input series, and the resultantforecasts are concatenated to generate the final predictions.Moreover, we extend this methodology to the construction ofthe mixed pretraining dataset, where the heterogeneity of eachvariable is discarded and no extra information is taken into account.It can be therefore seamlessly applied to more diverse scenarioswhere the covariate information may be missing and the data itselfmay be synthetic. In practice, we concatenate the training segmentsfrom various real-world datasets to constitute the training set of themixed dataset. This process is similarly applied to the validation andtesting portions. Our approach ensures that GPHT is pretrained on arich variety of temporal patterns, thereby enhancing its adaptabilityand generalization capabilities across diverse time series domains.",
  "Series Tokenization": "Given that the majority of time series originate from signals cap-tured by real-world sensors, the data inherently carry noise, andinformation is often sparsely distributed across the time points.Consequently, employing auto-regressive training on point-wisetime series might yield suboptimal performance due to the risk ofoverfitting outliers and the associated error accumulation , inaddition to incurring high computation costs.To address these challenges, we employ the series tokenizationtechnique, a proven effective approach in time series modeling. Specifically, we adopt a non-overlapping tokenizationstrategy, reshaping the input series X into a sequence of time seriestokens, R , where = , represents the tokenlength, and can be considered as the sequence length. The to-kenization strategy not only helps mitigate the impact of noiseand sparse information distribution but also enhances the modelsability to better capture the local semantics, ultimately contributingto more robust and accurate time series forecasting.Furthermore, recent research has highlighted a prevalent issuein time series data, characterized by a distribution shift which means that the mean and variance of time series changesover time. This challenge significantly hinders the generalizabilityof deep-learning-based forecasters. The impact of this phenome-non is even more pronounced in the context of pretraining on amixed dataset, where series inherently stem from distinct tempo-ral distributions. To mitigate this issue, we introduce an InstanceNormalization layer , designed to address the distribution shiftproblem by normalizing the input series using the formula:",
  "Hierarchical Transformer Blocks": "Multi-scale representation learning has demonstrated its effective-ness in various time series modeling tasks , given the multipleperiodic characteristics commonly found in real-world time seriesdata. Furthermore, to better discover commonalities hidden withinmixed datasets comprising various data scenarios, we posit theindispensability of a hierarchical encoder.In practice, drawing inspiration from the multi-rate samplingstrategy , we introduce a token-level multi-stage representa-tion learning approach within our hierarchical transformer blocks:Suppose is the input series after tokenization of stage , a max-pooling operation with a kernel size of is applied on each token,",
  "= ( ).(2)": "Here, represents a regular position embedding layer, and isa token projection layer projecting a time series token into the hid-den space of the transformer. Besides, we employ the decoder-onlytransformers as the backbone, which incorporates causal attentionmasks to prevent information leakage during the auto-regressivegenerating process. Finally, the learned hidden statesare fed into the forecastinghead, which is a linear layer, to predict future values for each token.In this regard, we adopt an up-sampling operation, which can beeither linear interpolation or MLP, to map the predictions backto the original time series tokens, yielding the predictions ofstage . This process is represented as:",
  "Iterative Residual Learning": "Utilizing the multi-stage hierarchical transformer blocks, GPHTis expected to learn coarse-to-fine representations effectively. Tofully leverage these representations, we propose a novel iterativeresidual learning strategy , transforming the forecastingprocess into an iterative approach.Specifically, as illustrated in , the input of stage + 1 isthe residual of stage s input and output, defined as:",
  "+1 = ().(4)": "Taking advantage of the auto-regressive training schema, eachtoken in can be considered as the predicted value of the next to-ken corresponding to the token in . For the regression of the firsttoken, we adopt a zero-padding for simplicity. Therefore, Equation4 effectively refines the input for the next stage, eliminatingthe redundant information in the series.",
  "Generative Pretrained Hierarchical Transformer forTime Series ForecastingKDD 24, August 2529, 2024, Barcelona, Spain": "advanced pretraining techniques. Secondly, the tokenization tech-nique emerges as a crucial factor in achieving precise forecast-ing, as evidenced by the outstanding performance of both FPTand patchTST among the baseline models. Finally, it appears thatmodern forecasters have approached the precision limits of somebenchmark datasets, likely due to inherent noise and unpredictabledistribution shifts in the data. This phenomenon results in minimaldifferences in outcomes among various models.On the other hand, in terms of our proposed GPHT, it consis-tently surpasses its competitors across most experimental settings.The tuned GPHT model establishes the most accurate forecastingunder over 65% cases with its powerful counterparts. Specifically,GPHT exhibits an average MSE reduction of 9.23% on the Ex-change dataset, 1.60% on the Traffic dataset, and 3.00% on theETTh1 dataset, in comparison with the best baseline under all ex-perimental forecasting lengths. Regarding MAE evaluation, theimprovements are more pronounced at 5.30%, 3.97%, and 5.07%respectively. Although the relative improvements under certainsettings may not be as substantial, as we claimed before, the con-sistently superior performance provides strong evidence for theeffectiveness of our approach. This further demonstrates that ourmodel can better capture temporal dependencies in various datascenarios, benefiting from its pretraining on the mixed dataset.Besides, it is noteworthy that our model exhibits superior per-formance at relatively shorter horizon lengths. Specifically, GPHTsurpasses PatchTST on every dataset when = 96, resulting in anaverage MAE reduction of 4.55%. We attribute this performanceboost to the explicit modeling of temporal dependencies in theoutput series. On the other hand, due to the inevitable error accu-mulation issue caused by the auto-regressive forecasting schema,the superiority of GPHT decreases with longer s and the theaverage reduction of MAE comes to 3.38% when = 720.Even more surprisingly, GPHT*, representing the model afterpretraining without any fine-tuning or modification, provesto be competitive with the baseline models. Specifically, GPHT*achieves the best or the second-best performance in 26 out of all64 settings. When compared to a single model, it outperforms FPTand supervised PatchTST under 44 and 40 experimental settings,respectively. This result validates the feasibility of learning the com-monalities of different time series by training on the mixed datasetwith the channel-independent assumption. It also underscores theincredible generalizability of our proposed model, primarily owingto the specially designed multi-stage hierarchical blocks and theiterative residual learning strategy.",
  "Y = ( + ) + .(6)": "To fully leverage the mixed dataset and better capture tempo-ral dependencies, we formulate the pretraining task as a standardlanguage modeling task, employing a token-wise auto-regressiveloss function as the optimization target (which is also a forecastingtask). Using the same notations as in .1, and letting = ,the optimization target of GPHT is:",
  "Inference": "Given that the pretraining task can be considered a forecasting task,the pre-trained GPHT can be directly applied to downstreamforecasting tasks without any modification. This sets our ap-proach apart from mainstream pretraining methods ,where a fine-tuning procedure is typically required.On the other hand, we argue that the performance of GPHT canalso be further enhanced through fine-tuning. In practice, to strikea balance between maintaining generalizability and improving per-formance on a specific dataset, we adopt a parameter-efficient tun-ing strategy. Specifically, only the forecasting heads described inEquation 3 are updated during the fine-tuning process. Importantly,these forecasting head parameters account for less than 0.5% of theentire model.During inference, benefiting from the aforementioned trainingschema and the channel-independent assumption, GPHT is theo-retically capable of conducting universal forecasting on any inputmulti-variate time series, regardless of arbitrary horizon lengths.The forecasting process is akin to the decoding process of a lan-guage model. Given any input X, our model can initially predictthe next first token. This predicted token is then concatenated tothe end of the input series to generate predictions for the second token. Note that a max input length exists in our model dueto the positional embeddings and heavy computation cost whenaddressing long sequences. Consequently, only the most recent tokens are fed into the model for forecasting.",
  "Experimental Setup": "4.1.1Datasets. provides detailed descriptions of the useddatasets, covering various data scenarios and scales. Among them,both ETT2 and Electricity3 mainly records the consumption onelectricity, and ETT can be further divided into 4 subsets accordingto the frequency. Besides, Exchange4 collects the daily exchangerate among 8 countries, Traffic5 contains the data of traffic loadsensors, and Weather6 is made up of 21 climate indicators likeair temperature. Following the standard protocol, we split eachdataset into training, validation and testing sets according to thechronological order. The split ratio is 6:2:2 for the ETT dataset and7:1:2 for the other datasets .",
  "ETTh1/ETTh271 Hour17420EnergyETTm1/ETTm2715 Minutes69680EnergyElectricity3211 Hour26304EnergyExchange81 Day7588FinanceTraffic8621 Hour17544TransportationWeather2110 Minutes52696Weather": "4.1.2Compared Baselines. We select various state-of-the-art mod-els as the baseline models in the experiments, encompassing bothself-supervised and supervised approaches.Among them, FPT introduces a framework that utilizesparameter-efficient fine-tuning on pretrained generative languagemodels, adapting them to time series tasks. SimMTM re-frames the standard masked time series modeling target into re-covering masked time points through the weighted aggregation ofmultiple neighbors. Additionally, TimeMAE leverages decou-pled masked autoencoders to learn robust representations throughmasked codeword classification and masked representation regres-sion. On the other hand, we include superior supervised mod-els to better demonstrate the effectiveness of GPHT, includingTransformer-based models such as PatchTST and iTrans-former , a linear model-based approach DLinear , and",
  "a convolution-based model TimesNet , which is also a multi-scale model. The performance of these methods effectively repre-sents the utmost accuracy achievable by current forecasting models": "4.1.3Implementation Details. We employ ADAM as the defaultoptimizer throughout all the experiments and use mean squared er-ror (MSE) and mean absolute error (MAE) as the evaluation metrics.A lower MSE/MAE indicates better performance. For the baselinemodels, we implement them with official codes and recommendedparameter settings.As for GPHT, we set the token length to 48, and the maxinput length is set to 7 to accommodate the lookback windowlength. The model comprises 4 stages of hierarchical transformerblocks, each with three-layer decoder-only transformers. The down-sampling ratio for each stage is set to respectively. All",
  "Main Results": "We report the long-term multi-variate forecasting results in Ta-ble 2. For a fair comparison, the lookback window length is setto 336 for every model and dataset, and the horizon length is following the standard protocol. Here, GPHT*denotes our model pretrained on the mixed dataset without anymodification, while GPHT represents the fine-tuned version foreach dataset, as described in .6.As shown in the table, we can draw some interesting conclusions.Firstly, self-supervised methods showcase competitive performancewith their supervised counterparts, underscoring the efficacy of",
  "Zero-shot Evaluation": "To further highlight GPHTs capacity to learn general knowledgeand discover common patterns from the mixed dataset, we con-duct zero-shot forecasting experiments on the Exchange, Weather,and Traffic datasets, originating from various data scenarios withdistinct scales. The zero-shot forecasting task is conceptually chal-lenging for methods that model cross-variable dependencies, hence,only the channel-independent models are considered as baselinemodels. It is important to note that ForecastPFN is specificallydesigned for zero-shot forecasting, but its performance is heavilycontingent on how the training data is synthesized, and as such, it",
  "0.3770.414 0.403 0.414 0.402 0.3920.405": "Clearly, GPHT consistently outperforms other models across var-ious settings, showcasing pronounced relative improvements. Weattribute this success primarily to the multi-stage hierarchical trans-former blocks, designed to capture diverse temporal patterns withdifferent resolutions. Consequently, GPHT exhibits better trans-ferability to unseen time series. Additionally, all models achieveforecasting accuracy at an acceptable error level, with DLinear evendemonstrating unprecedented precision on the Exchange datasetwhen = 720. These results strongly validate the feasibility ofour approach to learning commonalities in time series throughpretraining on a mixed dataset.",
  "Few-shot Evaluation": "In real-world applications, the initial observation of time seriesmay be of a limited size, posing challenges for training accurateforecasters. To evaluate the representation power of GPHT undersuch circumstances, we conduct few-shot evaluations on the ETTand Electricity dataset. In detail, only a portion (10% or 5%, followingexisting work ) of the training instances are used for trainingthe models, and we evaluate their MSE and MAE on the full testset. The results are reported in .In comparison to the selected baseline models, GPHT consis-tently achieves superior performance across various experimentalsettings, particularly on small-scale datasets. Specifically, whencompared to the best-performing baseline in the 10% setting, GPHTdemonstrates a relative average MSE reduction of 16.02% and 7.02%on the ETTh1 and ETTh2 datasets, respectively. With a further re-duction in the training data to only 5%, the relative improvementsbecome even more pronounced, reaching 30.19% and 12.49%. How-ever, it is noteworthy that GPHT exhibits poor forecasting perfor-mance on the ETTm1 dataset. We believe this observation maybe attributed to the potential heterogeneity of ETTm1 compared",
  "Ablation Study": "4.5.1Hierarchical Architecture. In this section, we explore the in-fluence of hierarchical transformer blocks on GPHTs performance.We present the averaged MSE and MAE evaluations for GPHTwith varying stages of hierarchical transformer blocks across allbenchmark datasets, considering a forecasting horizon of = 720(see ). As the number of stages increases, GPHT is theoret-ically better equipped to capture diverse temporal dependencieswithin the mixed dataset, such as different periodicities. The resultsstrongly affirm our hypothesis, as the 4-stage GPHT surpasses the1-stage GPHT (without hierarchical structures), achieving a notable2.86% reduction in MSE. 4.5.2On the Effect of Pretraining. In addition to the architecturedesign, another key aspect is the pretraining procedure on themixed dataset. Does it pose positive effects on the forecasting per-formance? We provide quantified results in . Specifically, wecompare the performance of fine-tuned GPHT with the one trainedfrom scratch. The MAE evaluations averaged on the horizon length(i.e., = 96, 192, 336, 720) are presented. From the results, we caninfer that pretraining on the mixed dataset enables the model toleverage commonalities among time series, facilitating better trans-fer to specific datasets. Compared to GPHT trained from scratch,pretraining results in an average MAE reduction of 5.75%, reachingas high as 9.65% on the ETTm2 dataset.",
  "CONCLUSION": "In this work, we proposed a generative pretrained hierarchicaltransformer model, namely GPHT, for time series forecasting. Itstands out in two key aspects. Conceptually, we explored training asingle unified forecasting model that generalizes well across diversedata scenarios and forecasting settings. Technically, we proposeda simple yet effective paradigm that treats time series originatingfrom various scopes as a whole, discarding the heterogeneity andconcatenating the values of each variable from different datasetsto form the mixed dataset for pretraining. Besides, we replacedconventional one-step generating, which is adopted by most recentforecasting methods, with auto-regressive decoding for better flexi-bility and performance. We also introduced the hierarchical struc-ture better to capture the diverse patterns in the mixed dataset. Weconducted sufficient experiments on 8 widely used datasets in com-parison with mainstream self-supervised pretraining models andsupervised models, the results demonstrated that GPHT surpassesthe baseline models across various fine-tuning and zero/few-shotlearning settings in the traditional long-term forecasting task. This research was supported by grants from the Joint ResearchProject of the Science and Technology Innovation Community inYangtze River Delta (No. 2023CSJZN0200), and the FundamentalResearch Funds for the Central Universities. This work also thankedto the support of funding MAI2022C007.",
  "George EP Box and Gwilym M Jenkins. 1968. Some recent advances in forecastingand control. Journal of the Royal Statistical Society. Series C (Applied Statistics) 17,2 (1968), 91109": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shot learners. Advances in neuralinformation processing systems 33 (2020), 18771901. Cristian Challu, Kin G. Olivares, Boris N. Oreshkin, Federico Garza, MaxMergenthaler-Canseco, and Artur Dubrawski. 2022. N-HiTS: Neural Hierar-chical Interpolation for Time Series Forecasting. arXiv:2201.12886 [cs.LG] Weiqi Chen, Wenwei Wang, Bingqing Peng, Qingsong Wen, Tian Zhou, and LiangSun. 2022. Learning to rotate: Quaternion transformer for complicated periodicaltime series forecasting. In Proceedings of the 28th ACM SIGKDD Conference onKnowledge Discovery and Data Mining. 146156. Mingyue Cheng, Qi Liu, Zhiding Liu, Zhi Li, Yucong Luo, and Enhong Chen. 2023.FormerTime: Hierarchical Multi-Scale Representations for Multivariate TimeSeries Classification. In Proceedings of the ACM Web Conference 2023. 14371445. Mingyue Cheng, Qi Liu, Zhiding Liu, Hao Zhang, Rujiao Zhang, and EnhongChen. 2023. TimeMAE: Self-Supervised Representations of Time Series withDecoupled Masked Autoencoders. arXiv preprint arXiv:2303.00320 (2023). Mingyue Cheng, Xiaoyu Tao, Qi Liu, Hao Zhang, Yiheng Chen, and ChenyiLei. 2024. Learning Transferable Time Series Classifier with Cross-DomainPre-training from Language Model. arXiv preprint arXiv:2403.12372 (2024). Mingyue Cheng, Jiqian Yang, Tingyue Pan, Qi Liu, and Zhi Li. 2024. Convtimenet:A deep hierarchical fully convolutional model for multivariate time series analysis.arXiv preprint arXiv:2403.01493 (2024).",
  "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:Pre-training of deep bidirectional transformers for language understanding. arXivpreprint arXiv:1810.04805 (2018)": "Jiaxiang Dong, Haixu Wu, Haoran Zhang, Li Zhang, Jianmin Wang, and Ming-sheng Long. 2023. SimMTM: A Simple Pre-Training Framework for MaskedTime-Series Modeling. arXiv preprint arXiv:2302.00861 (2023). Samuel Dooley, Gurnoor Singh Khurana, Chirag Mohapatra, Siddartha VenkatNaidu, and Colin White. 2023. ForecastPFN: Synthetically-Trained Zero-ShotForecasting. In Thirty-seventh Conference on Neural Information Processing Sys-tems. Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, Chee KeongKwoh, Xiaoli Li, and Cuntai Guan. 2021. Time-series representation learning viatemporal and contextual contrasting. arXiv preprint arXiv:2106.14112 (2021).",
  "Rakshitha Godahewa, Christoph Bergmeir, Geoffrey I Webb, Rob J Hyndman,and Pablo Montero-Manso. 2021. Monash time series forecasting archive. arXivpreprint arXiv:2105.06643 (2021)": "Jean-Bastien Grill, Florian Strub, Florent Altch, Corentin Tallec, PierreRichemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, ZhaohanGuo, Mohammad Gheshlaghi Azar, et al. 2020. Bootstrap your own latent-a newapproach to self-supervised learning. Advances in neural information processingsystems 33 (2020), 2127121284. Nate Gruver, Marc Anton Finzi, Shikai Qiu, and Andrew Gordon Wilson. 2023.Large Language Models Are Zero-Shot Time Series Forecasters. In Thirty-seventhConference on Neural Information Processing Systems. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollr, and Ross Girshick.2022. Masked autoencoders are scalable vision learners. In Proceedings of theIEEE/CVF conference on computer vision and pattern recognition. 1600016009.",
  "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residuallearning for image recognition. In Proceedings of the IEEE conference on computervision and pattern recognition. 770778": "Wenqiang He, Mingyue Cheng, Qi Liu, and Zhi Li. 2023. ShapeWordNet: AnInterpretable Shapelet Neural Network for Physiological Signal Classification. InInternational Conference on Database Systems for Advanced Applications. Springer,353369. Junji Jiang, Likang Wu, Hongke Zhao, Hengshu Zhu, and Wei Zhang. 2023.Forecasting movements of stock time series based on hidden state guided deeplearning approach. Information Processing & Management 60, 3 (2023), 103328. Guangyin Jin, Yuxuan Liang, Yuchen Fang, Zezhi Shao, Jincai Huang, JunboZhang, and Yu Zheng. 2023. Spatio-temporal graph neural networks for predictivelearning in urban computing: A survey. IEEE Transactions on Knowledge andData Engineering (2023). Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, andJaegul Choo. 2021. Reversible instance normalization for accurate time-seriesforecasting against distribution shift. In International Conference on LearningRepresentations. Minhao Liu, Ailing Zeng, Muxi Chen, Zhijian Xu, Qiuxia Lai, Lingna Ma, andQiang Xu. 2022. SCINet: Time Series Modeling and Forecasting with SampleConvolution and Interaction. Thirty-sixth Conference on Neural InformationProcessing Systems (NeurIPS), 2022 (2022). Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, andMingsheng Long. 2023. itransformer: Inverted transformers are effective for timeseries forecasting. arXiv preprint arXiv:2310.06625 (2023). Zhiding Liu, Mingyue Cheng, Zhi Li, Zhenya Huang, Qi Liu, Yanhu Xie, andEnhong Chen. 2023. Adaptive Normalization for Non-stationary Time SeriesForecasting: A Temporal Slice Perspective. In Thirty-seventh Conference on NeuralInformation Processing Systems. Yiwei Lou, Yu Huang, Xuliang Xing, Yongzhi Cao, and Hanpin Wang. 2022.Mts-lstdm: multi-time-scale long short-term double memory for power loadforecasting. Journal of systems architecture 125 (2022), 102443. Feng Lu, Wei Li, Zhiqiang Zhou, Cheng Song, Yifei Sun, Yuwei Zhang, YufeiRen, Xiaofei Liao, Hai Jin, Ailin Luo, et al. 2023. A composite multi-attentionframework for intraoperative hypotension early warning. In Proceedings of theAAAI Conference on Artificial Intelligence, Vol. 37. 1437414381. Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. 2022.A Time Series is Worth 64 Words: Long-term Forecasting with Transformers. InThe Eleventh International Conference on Learning Representations. Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. 2019. N-BEATS: Neural basis expansion analysis for interpretable time series forecasting.In International Conference on Learning Representations. Nikolaos Passalis, Anastasios Tefas, Juho Kanniainen, Moncef Gabbouj, andAlexandros Iosifidis. 2019. Deep adaptive input normalization for time seriesforecasting. IEEE transactions on neural networks and learning systems 31, 9 (2019),37603765.",
  "Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing Ma, Junchi Yan,and Liang Sun. 2022. Transformers in time series: A survey. arXiv preprintarXiv:2202.07125 (2022)": "Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven Hoi. 2021.CoST: Contrastive Learning of Disentangled Seasonal-Trend Representations forTime Series Forecasting. In International Conference on Learning Representations. Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and MingshengLong. 2022. TimesNet: Temporal 2D-Variation Modeling for General Time SeriesAnalysis. In The Eleventh International Conference on Learning Representations. Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. 2021. Autoformer: De-composition transformers with auto-correlation for long-term series forecasting.Advances in Neural Information Processing Systems 34 (2021), 2241922430. Zhihan Yue, Yujing Wang, Juanyong Duan, Tianmeng Yang, Congrui Huang,Yunhai Tong, and Bixiong Xu. 2022. Ts2vec: Towards universal representation oftime series. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 36.89808987.",
  "G Peter Zhang. 2003. Time series forecasting using a hybrid ARIMA and neuralnetwork model. Neurocomputing 50 (2003), 159175": "Kexin Zhang, Qingsong Wen, Chaoli Zhang, Rongyao Cai, Ming Jin, Yong Liu,James Zhang, Yuxuan Liang, Guansong Pang, Dongjin Song, et al. 2023. Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress, and Prospects.arXiv preprint arXiv:2306.10125 (2023). Yuchen Zhang, Mingsheng Long, Kaiyuan Chen, Lanxiang Xing, Ronghua Jin,Michael I Jordan, and Jianmin Wang. 2023. Skilful nowcasting of extreme precip-itation with NowcastNet. Nature 619, 7970 (2023), 526532. Yunhao Zhang and Junchi Yan. 2022. Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting. In The EleventhInternational Conference on Learning Representations. Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,and Wancai Zhang. 2021. Informer: Beyond efficient transformer for long se-quence time-series forecasting. In Proceedings of the AAAI Conference on ArtificialIntelligence, Vol. 35. 1110611115. Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. 2022.FEDformer: Frequency enhanced decomposed transformer for long-term seriesforecasting. In Proc. 39th International Conference on Machine Learning (ICML2022) (Baltimore, Maryland).",
  "ACOMPLEXITY COMPARISON": "To better illustrate the proposed GPHT models computation cost,we provide quantitative results on the Electricity dataset underlookback window = 336 and forecasting horizon = 720 in. In summary, the proposed GPHT model is medium-sizedcompared to the baseline models. Thanks to its straightforwardoptimization objective, both the pretraining and finetuning pro-cesses of GPHT are quite efficient and do not require too muchtime, especially compared with the pretraining-based approaches.A key drawback of our model might be the inference speed, whichis naturally limited by the auto-regressive decoding schema.",
  "BQUALITATIVE EVALUATION": "In this section, we provide visualizations of long-term forecastingresults to better demonstrate the performance of GPHT and theeffectiveness of the hierarchical transformer architecture.We plot a forecasting sample in , illustrating how GPHTforecasts with the hierarchical architecture and the iterative residualschema. It can be referred that the initial stages predominantlyfocus on extracting the general periodic patterns from the inputseries and the latter stages can therefore pay more attention to thespecialized trends, since the auto-regressive forecasting results ofstage 3 align more closely with the input series, albeit with lessperiodicity than the preceding stages. The results strongly verifyour assumption that the hierarchical architecture can better capturethe commonalities and specialties of the mixed pertaining dataset,and the iterative residual schema effectively refines the input forthe next stage, eliminating the redundant information in the series.Besides, we provide qualitative comparison between GPHT andmainstream supervised forecasting methods in on variousdatasets. Benefiting from the auto-regressive pertaining and thehierarchical architecture, GPHT can better capture the temporaldependencies so as to achieve better performance."
}