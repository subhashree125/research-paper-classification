{
  "Abstract": "This paper describes the winning solution of all 5 tasks for the Ama-zon KDD Cup 2024 Multi Task Online Shopping Challenge for LLMs.The challenge was to build a useful assistant, answering questionsin the domain of online shopping. The competition contained 57diverse tasks, covering 5 different task types (e.g. multiple choice)and across 4 different tracks (e.g. multi-lingual).Our solution is a single model per track. We fine-tune Qwen2-72B-Instruct on our own training dataset. As the competition re-leased only 96 example questions, we developed our own trainingdataset by processing multiple public datasets or using Large Lan-guage Models for data augmentation and synthetic data generation.We apply wise-ft to account for distribution shifts and ensemblemultiple LoRA adapters in one model. We employed Logits Proces-sors to constrain the model output on relevant tokens for the tasks.AWQ 4-bit Quantization and vLLM are used during inference topredict the test dataset in the time constraints of 20 to 140 minutesdepending on the track.Our solution achieved the first place in each individual track andis the first place overall of Amazons KDD Cup 2024.",
  "All authors contributed equally to this research": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from ., , 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-x-xxxx-xxxx-x/YY/MM",
  "Introduction": "The capabilities of Large Language Models (LLMs) have signifi-cantly improved in the last years and they have become populardue to their easiness to use. Users can interact with the systemsin natural language. The LLMs excel on a variety of tasks, suchas general reasoning, math questions, coding, etc. Many systemsare getting updated by adding a LLMs to make them easier to useand/or providing more functionality. (Online) shopping is a largedomain with billions of users and high economic output. The Ama-zon KDD Cup 2024 is designed to evaluate LLMs to be a usefulshopping assistant.",
  ": One example of the development dataset. It is amultiple choice question answering tasks for understandingshopping concepts": "Amazon developed an evaluation dataset ShopBench, contain-ing approx. 20,000 questions across 57 different tasks covering 5task types (e.g. retrieval), to test LLMs capabilites in the onlineshopping domain (see an example in ). The competitionhad 5 different tracks, which evaluates different aspects such asshopping knowledge understanding or user behavior alignment.The 5th track was the overall track containing all 20,000 questions.The competition was organized as a code competition in whichparticipants have no access to the ShopBench dataset and insteadthey have to submit their model.Our team from NVIDIA won all 5 tracks (see ). Thispaper describes our final solution and an ablation study on ourexperiments. Our solution is based on a single model per track,which shares following techniques: (1) Developing a Training Dataset: As the hosts did not pro-vide a training dataset, we processed many multiple publicdatasets and enriched it by prompting Large Language Mod-els to generate a training dataset",
  "Amazon KDDCup 2024: Multi Task OnlineShopping Challenge for LLMs": "Amazon hosted the KDD Cup 2024 for Multi Task Online ShoppingChallenge for LLMs . They developed a test dataset, called Shop-Bench, containing 20,000 questions across 57 tasks. A developmentdataset of 96 question of only 18 different tasks were shared with theparticipants for the competition. The KDD Cup 2024 is designed asa code competition. Participants submit model weights with codewhich will be evaluated on infrastructure provided by Amazon.Participants have no access to the test dataset and can build solu-tions based on the 96 development questions. They receive onlythe scores on the full ShopBench dataset via the leaderboard. Asubmission is evaluated on 4x NVIDIA T4 GPUs with each 16 GBGPU memory within a runtime limit (see ).Participants have to address following challenges:",
  "Generation: There are a diverse set of generation tasks de-pending on the task (e.g. translation). Evaluation metrics areROUGE-L, BLEU or cosine similarity of sentence embedding": "A track can contain one or multiple task types. The final scoreof a track is calculated by averaging across all questions becauseeach evaluation metric is between 0-1. The overall challenge scoreis determined by the sum of position per track.For every question, the requirement is to generate text, which isparsed by Amazons evaluation script. The solution has to followthe prompt instructions (e.g. return 3 candidates IDs separated by acomma). If the evaluation script is not able to parse the generatedtext, then the score will be 0 for this question.The competition was organized in 2 phases. The organizer sharedthat Phase 2 contains harder samples and tasks than Phase 1. Theyincreased the compute resources from 2x NVIDIA T4s to 4x NVIDIAT4s for phase 2.",
  "Training Dataset": "Amazon shared multiple eCommerce datasets with participants,which are related to the ShopBench dataset, but do not have thesame structure. We created our training dataset by processing mul-tiple datasets to have a similar structure as the 18 tasks from Shop-Bench development dataset. In addition, we developed new tasks. Fi-nally, we augmented the dataset by prompting LLaMa3-70B-Instruct and GPT-4 for more diversity or infer missing information(e.g. product type, category). A detailed overview can be found inAppendix A.",
  "Synthetic Datasets": "To further improve diversity of dataset we utilized the syntheticdata generation (SDG) pipelines. In general, we used three differentmethods.We prompt LLM to construct the tasks specific prompts from theseed data. For example, we rephrase the original tasks from NingLabECInstruct dataset. These tasks include various information aboutthe product (title, description, attributes) and we combine all ofthem into one prompt. (see Dataset No 11-19).Before constructing a task specific prompt, we extract the correctlabels from the seed data using LLM. For example, we extract theproduct type, categories or attributes first and then construct thequestion. (see Dataset No 1,20).We used GPT-4 to generate the instructions with different word-ings, and then used it to construct MC tasks from ESCI-data dataset.The correct answer was randomly selected from the E entries, theremaining options were selected from the entries with S/C/I labels(see Dataset No 21-26).",
  "Model4.1Prompt Template": "We explored both zero shot LLM models and fine-tuned LLM models.Our final winning solution achieving our best model accuracy isfine-tuned. See for a comparison.When using zero shot with an instruction tuned LLM, we foundit helpful to use both the system role and user role when formattingprompts. Designing better prompts improved the zero shot modelsperformance.When fine-tuning, we found that the prompt was not as impor-tant because the model is fine-tuned to exhibit a certain behaviorgiven whatever prompt we choose to train with.One technique of our fine-tuned models used is to include aninstruction to the model identifying which of the 5 task types themodel is solving. Then during inference, we used a heuristic ruleclassifier which determined question task type and included thisis the system roles instruction prompt. Specifically, we used thefollowing template.",
  "Fine-Tuning Qwen2": "We fine-tuned Qwen/Qwen2-72B-Instruct on our developedtraining dataset using 8x NVIDIA A100 with each 80GB GPU mem-ory. Training on 500k examples takes around 24 hours. We usedthe library axolotl 1 and bitsandbytes 2 with QLoRA with 4-bitquantization and bfloat16 . The library applies Multipack (Sam-ple Packing) 3, concatenating multiple sequences into one batch toincrease training throughput. provides an overview of thehyperparameters.We train the LLM in a supervised fine-tuning strategy. The lossis calculated only on the answer tokens (see ). A commontechnique in large language model training is to apply ReinforcementLearning from Human Feedback (RLHF) . Our hypothesis isthat supervised fine-tuning is sufficient for the competition. Manyanswers are a single number or a list of numbers, which have anexact solutions and does not require human preferences betweenmultiple possible answers.",
  "Ensemble Adapters": "Our five track solutions are created from 4 fine-tuned LoRA adapters.We call them v7, v8, v7b, and v9b. First for tracks 1,3,5 we merged v8to base model Qwen2-72B with 56% weight explained in .4.For tracks 2,4 we merged v7 to base with 100%. Version 7 adapterwas trained with 417k samples whereas v8 was trained with 462k.See for details about train data.Next we trained two more LoRA adapters named v7b and v9busing two different new subsets of 152k and 40k samples respec-tively. Our final solutions with ensemble weights and leaderboardscores to tracks 1-5 are shown in .",
  ", ,Deotte et al": "Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, KatarinaSlama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Pet-roski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B.Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle,Nick Turley, Jerry Tworek, Juan Felipe Cern Uribe, Andrea Vallone, Arun Vi-jayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang,Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, PeterWelinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter,Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, MichaelWu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba,Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng,Juntang Zhuang, William Zhuk, and Barret Zoph. 2024. GPT-4 Technical Report.arXiv:2303.08774 [cs.CL] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, PamelaMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schul-man, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Pe-ter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training languagemodels to follow instructions with human feedback. arXiv:2203.02155 [cs.CL] Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, andColin White. 2024. Smaug: Fixing Failure Modes of Preference Optimisation withDPO-Positive. arXiv preprint arXiv:2402.13228 (2024). Bo Peng, Xinyi Ling, Ziru Chen, Huan Sun, and Xia Ning. 2024. eCeLLM: Gener-alizing Large Language Models for E-commerce from Large-scale, High-qualityInstruction Data. In Forty-first International Conference on Machine Learning. Chandan K. Reddy, Llus Mrquez, Fran Valero, Nikhil Rao, Hugo Zaragoza,Sambaran Bandyopadhyay, Arnab Biswas, Anlu Xing, and Karthik Subbian. 2022.Shopping Queries Dataset: A Large-Scale ESCI Benchmark for Improving ProductSearch. (2022). arXiv:2206.06588",
  "Logits Processors": "We employed a variety of logits processors to generate outputs inspecific formats. For multiple choice, ranking, and retrieval ques-tions, we constrained our models to produce only digits and com-mas. For NER tasks, we enhanced the logits of the prompt tokens,encouraging the model to cite directly from the prompt. These logitsprocessors were particularly useful in Phase 1 when we utilized lesspowerful models. These constrains also were useful in case whenthe training dataset includes only few task types. For example, youcan finetune a model for MC tasks only and successfully apply it forRetrival or Reranking tasks. However, their importance diminishedin Phase 2 as we transitioned to larger models that more effectivelyfollowed instructions.",
  "Quantization / vLLM": "KDD Cup 2024 was a code competition meaning that we mustsubmit code plus model weights to be run on the hosts pre-definedcompute resources. Each participant could submit (to each track)a GitLab repository of maximize size 100GB to be executed on 4xNVIDIA T4 GPU each with 16 GB GPU memory within a timeconstraint.",
  "Bagel-34B-v0.5 0.70070.66090.63390.58710.6834LLaMa3-70B 0.78060.65320.66580.62370.7183Smaug-72B 0.71780.65640.64840.6975Qwen2-72B 0.79820.64070.71930.69180.7486": "The Qwen2-72B model is about 150GB at fp16. Therefore inorder to fit this into disk and memory size constraints, we used 4bitquantization which reduced its size to 40GB.Quantization plus using the library vLLM accelerated our in-ference which allowed our model to answer all the questions withinthe time limit. Tracks 1-5 had 6102, 1896, 2373, 1349, and 11720questions to be answered in 70, 20, 30, 20, 140 minutes respectively.We improved AWQ quantization accuracy by calibrating withthe 96 development questions. We compared AWQ versus GPTQquantization and found both to be about equal in speed and accu-racy.AWQ quantization for Qwen2-72B takes about 1.5 hours on1xA100 GPU to process. In order for the AWQ quantized Qwen2-72B to work with vLLM, we needed to pad the unquantized modelwith zeros to change the shape of the weights before quantization.",
  "Results": "Our quantized, fine-tuned Qwen2-72B model achieves the highestscore on each individual track (T1 - T4) and overall track T5 with asignificant lead of 0.007 to 0.026 to the 2nd place (). As weplaced 1st in each individual track, our final score is 5, the sum ofour positions, which is the highest possible score.Each submission for the individual track is based on the key con-cepts of fine-tuning a Qwen2-72B model on our developed trainingdataset and optionally, ensemble multiple versions and/or applywise-tf. The submission might differ slightly in the fine-tuning time,exact amount of training dataset and ensemble combination.We provide an ablation study in , 5 and 6. Some values aremissing in the tables due to failed submissions and the successfulsubmissions were sufficient to decide the next experiments. First,table 4 compares different base models without being fine-tuned.We observe that Qwen2-72B has the highest score except of forTrack 2, followed by LLaMa3-70B is 2nd place except of Track",
  "LoRA 2 name (M2)v9bv7bv9bv7bv9bLoRA 2 weight (W2)0.750.50.250.50.25LB B+W1xM1+W2xM20.8330.7910.7460.7610.788": "4. The performance of the models are equivalent to public LLMsbenchmarks.Next, we fine-tuned Smaug-72B and Qwen2-72B on our trainingdataset. We compare the zero-shot version (SZ) with the fine-tunedversion (FT) as seen in . Qwen2-72B SZ would scored 4thplace on Track 5, demonstrating that base model provide greatcapabilities without fine-tuning. We achieve significant gains of0.0035 to 0.15 by additional fine-tuning. summarize the effect of ensembling multiple models. Thefirst solution is to submit the base model merged with the firstLoRA adapter, scaled by weight W1. If we ensemble two adapters(LB B+W1xM1+W2xM2), we observe that the LB score improvesbetween 0.001 to 0.004.",
  "Conclusion": "The KDD Cup 2024 was a great competition with a diverse set oftasks to evaluate Large Language Models capabilities in the domainof online shopping. The code competition design ensured a faircomparison of solutions. Our team solution is a single models withmultiple optimization methods, which scored the 1st place on eachtrack. It was essential to fine-tune a base model with an additionaltraining dataset. The lack of an official training dataset was compen-sated by processing multiple public datasets and prompting LargeLanguage Models. We ensembled multiple LoRA adapater, appliedwise-ft for distribution shift and constrained the model output witha Logits Processors. We optimized inference with 4-bit quantizationand vLLM to run a 72 billion parameters model on 4x NVIDIA T4with each 16 GB GPU memory in the time constrain. In addition,we share multiple experiments as an ablation study.",
  "Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023.QLoRA: Efficient Finetuning of Quantized LLMs.arXiv:2305.14314 [cs.LG]": "Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song,and Jacob Steinhardt. 2021. Aligning AI With Shared Human Values. Proceedingsof the International Conference on Learning Representations (ICLR) (2021). Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, DawnSong, and Jacob Steinhardt. 2021. Measuring Massive Multitask Language Under-standing. Proceedings of the International Conference on Learning Representations(ICLR) (2021).",
  "Yupeng Hou, Jiacheng Li, Zhankui He, An Yan, Xiusi Chen, and Julian McAuley.2024. Bridging Language and Items for Retrieval and Recommendation. arXivpreprint arXiv:2403.03952 (2024)": "Wei Jin, Haitao Mao, Zheng Li, Haoming Jiang, Chen Luo, Hongzhi Wen, HaoyuHan, Hanqing Lu, Zhengyang Wang, Ruirui Li, Zhen Li, Monica Xiao Cheng,Rahul Goutam, Haiyang Zhang, Karthik Subbian, Suhang Wang, Yizhou Sun,Jiliang Tang, Bing Yin, and Xianfeng Tang. 2023. Amazon-M2: A MultilingualMulti-locale Shopping Session Dataset for Recommendation and Text Generation.arXiv:2307.09688 [cs.IR] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng,Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient Mem-ory Management for Large Language Model Serving with PagedAttention. InProceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, IlgeAkkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, SamAltman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Bal-com, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello,Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff,Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks,Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brit-tany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, FotisChantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, BenChess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, JeremiahCurrier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, DamienDeville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet,Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simn PosadaFishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson,Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gor-don, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang ShaneGu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton,Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele,Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, ShantanuJain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, DennyJin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, ukasz Kaiser, AliKamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kil-patrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner,Jamie Kiros, Matt Knight, Daniel Kokotajlo, ukasz Kondraciuk, Andrew Kon-drich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, MichaelLampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak MingLi, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, RyanLowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov,Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew,Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, DavidMedina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, PamelaMishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati,Oleg Murk, David Mly, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, ArvindNeelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen OKeefe, JakubPachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascan-dolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng,Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Pondede Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, TollyPowell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford,Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach,Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders,Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schul-man, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker,",
  "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, CarlosGuestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford Alpaca: AnInstruction-following LLaMA model": "Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Korn-blith, Rebecca Roelofs, Raphael Gontijo-Lopes, Hannaneh Hajishirzi, Ali Farhadi,Hongseok Namkoong, and Ludwig Schmidt. 2021. Robust fine-tuning of zero-shotmodels. arXiv:arXiv:2109.01903 An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Cheng-peng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei,Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang,Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Jun-yang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, MingfengXue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin,Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu,Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang,Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang,Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo,and Zhihao Fan. 2024. Qwen2 Technical Report.arXiv:2407.10671 [cs.CL]",
  "NoSource DatasetTaskTask TypeAdapterSizeLLMAdditional Explanation": "1Amazon-M2KDD Cup2024 Task 2multiple-choicev7, v82350YesSelect product categories given product attributes2Amazon Reviews 2023KDD Cup2024 Task 3retrievalv7, v7b, v87373YesGiven a product type and sentiment, select 3 most likely snippet a customer would write about the product3Amazon Reviews 2023KDD Cup2024 Task 7retrievalv7, v7b, v83608YesGiven a product type and a review, select 3 aspects covered by the review4Amazon Reviews 2023KDD Cup2024 Task 10multiple-choicev7, v7b, v810000YesGiven a product type, which of the following categories complement the product type best?5ESCI-dataKDD Cup2024 Task 12rankingv7, v816728No6Amazon Reviews 2023KDD Cup2024 Task 14rankingv7, v7b, v85815NoGiven a product title a customer will buy, which other product titles will he like7Amazon Reviews 2023KDD Cup2024 Task 15multiple-choicev7, v7b, v810000NoGiven a product review, estimate the rating of the review8ESCI-dataKDD Cup2024 Task 16multiple-choicev7, v810000No9Amazon-M2KDD Cup2024 Task 17generationv7, v810000No10Amazon-M2KDD Cup2024 Task 18multiple-choicev7, v810000No11NingLab/ECInstructAttribute Value Extractionnamed entity recognitionv7, v819622Yes12NingLab/ECInstructMulticlass Product Classificationmultiple-choicev7, v810000Yes13NingLab/ECInstructProduct Relation Predictionmultiple-choicev7, v810000Yes14NingLab/ECInstructQuery Product Rankretrievalv7, v810000Yes15NingLab/ECInstructSequential Recommendationmultiple-choicev7, v810000Yes16NingLab/ECInstructAnswerability Predictionmultiple-choicev7, v810000No17NingLab/ECInstructProduct Matchingmultiple-choicev7, v84044No18NingLab/ECInstructProduct Substitute Identificationmultiple-choicev7, v810000No19NingLab/ECInstructSentiment Analysismultiple-choicev7, v810000No20Amazon-M2New Ideagenerationv7, v810000YesExplain product type given title, description, and product type21ESCI-dataNew Ideamultiple-choicev7, v810000YesSelect the user query that matches the product description22ESCI-dataNew Ideamultiple-choicev7, v810000YesSelect the user query that matches the product features23ESCI-dataNew Ideamultiple-choicev7, v810000YesSelect the user query that matches the product title24ESCI-dataNew Ideamultiple-choicev7, v810000YesSelect the title for the product description25ESCI-dataNew Ideamultiple-choicev7, v810000YesSelect the title for the product features26ESCI-dataNew Ideamultiple-choicev7, v810000YesSelect the product title for the user query27ESCI-dataNew Idearetrievalv810000NoPick 3 bullet points to match product28NingLab/ECInstructNew Idearankingv85000NoRank product reviews - positive to negative29ESCI-dataNew Ideamultiple-choicev85435NoPick product to match query30ESCI-dataNew Idearankingv85000NoTask12 backwards. Given product, rank queries31KDD Cup 2023New Idearetrievalv810000NoGiven purchase pick previous clicks (similar to task 14)32ESCI-dataNew Ideamultiple-choicev810000NoGiven title pick brand33ESCI-dataNew Ideamultiple-choicev9b10000NoGiven query product pair, what is relationship? E S C I34ESCI-dataNew Idearankingv9b10000NoGiven list of query product pairs, rank which are most related to least related35ESCI-dataNew Idearetrievalv9b10000NoGiven query, select products which are exact match not substitute, complement, or irrelevent36Amazon Reviews 2023New Idearankingv9b10000NoGiven a product title and multiple reviews, rank the reviews based on the helpfulness37Alpaca CleanedNo Changesgenerationv7, v851760No38MMLUNo Changesmultiple-choicev7, v7b, v8115700No",
  "ADetails on Training Dataset": "In , we provide an overview of the different datasets we generated. We share which dataset was used as a source input. We describewhich task the resulting dataset is most similar to (column Task), the size and if a LLM was used. Finally, we provide additional explanationfor our own ideas."
}