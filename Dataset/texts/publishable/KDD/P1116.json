{
  "Leon YangSchool of ITDeakin UniversityAustraliaEmail:": "AbstractAs IoT networks become more complex and generatemassive amounts of dynamic data, it is difficult to monitorand detect anomalies using traditional statistical methods andmachine learning methods. Deep learning algorithms can pro-cess and learn from large amounts of data and can also betrained using unsupervised learning techniques, meaning theydont require labelled data to detect anomalies. This makesit possible to detect new and unknown anomalies that maynot have been detected before. Also, deep learning algorithmscan be automated and highly scalable; thereby, they can runcontinuously in the backend and make it achievable to monitorlarge IoT networks instantly. In this work, we conduct aliterature review on the most recent works using deep learningtechniques and implement a model using ensemble techniqueson the KDD Cup 99 dataset. The experimental results showcasethe impressive performance of our deep anomaly detectionmodel, achieving an accuracy of over 98%.",
  ". Introduction": "Anomalies, namely novelties or outliers, are individualsthat are far from the nominal data population. In general,an anomaly can be thought of as an observation or patternthat does not conform to the expected behaviour or followsthe same patterns as the rest of the data. Anomaly Detection(AD), is the process of identifying unusual patterns in datathat do not conform to a well-defined notion of normal data. The process involves learning the normal behaviour ofthe data and then identifying instances that deviate sig-nificantly from the model through statistical methods ormachine learning techniques. For example, in a time seriesdataset, an anomaly might be a sudden change in the trendor a spike in the data that is not consistent with the rest ofthe series. In a classification problem, an anomaly could bean observation that does not fit into any defined classes oris significantly different from the other observations.Statistically, the sparsely distributed areas indicate thatthe probability of data occurring in a certain area is relativelylow, where the data falling in can be considered to beanomalies. In , we illustrate the anomalies in two-dimensional data space, where the clusters in blue indicatenormal data and red points represent anomalies far from normal. Given a dataset X = {X1, X2, ..., Xn}, the featuredimension of each sample is D, xi RD. Deep AnomalyDetection (DAD) aims to learn a mapping function, whichmaps the original space to a new representation space ():X Z, where Z RK(K D). If the probability densityof a sample in the dataset is less than the threshold, a smallenough value, the sample is considered an anomaly and theanomaly score of the sample () can be computed in thenew space. Such sparse anomalies can be applicable in manyareas by analysing activity patterns to detect anomalous be-haviours, manage industrial resources, or ensure productionsecurity.",
  ". A simple example of anomalies in 2d data space as the red crosspoints while the blue solid points are the normal data": "Based on the availability of data labels, we usuallydivide Anomaly Detection tasks into three types: supervised,semi-supervised, and unsupervised. Supervised AD useslabels of nominal and anomalous data instances to trainbinary or multi-class classifiers; semi-supervised techniquesuse the existing normal single label to separate outlierswithout the anomalous instances involved in the trainingprocess; in training with unsupervised deep AD techniques,there are both normal and anomalous instances in the data,but the normal instances are often much larger than theanomalies. This method detects outliers based on the in-trinsic properties of the data instances and is usually used",
  "arXiv:2402.04469v1 [cs.LG] 6 Feb 2024": "for automatic labelling of unlabelled data samples. Eventhough denoting a data point with a label of normal oranomalous would be one of the ideal solutions for AD, itrequires extreme effort to obtain the labelled training data. Inaddition, due to the nature of data, for example, the dynamicbehaviours in anomalous pattern recognition would lead tofurther difficulties in labelling. In that case, a vital solutionto AD is to train a normality model from normal data in anunsupervised manner to detect anomalies through deviationsfrom the model.In the context of the Internet of Things (IoT), wheredata is transmitted between IoT devices and systems overa network, anomalies are a special type of outlier pointthat usually carries meaningful pieces of information, suchas sensor readings, device status and configuration data,and messages or commands sent between devices. The dataformat of IoT network traffic can vary depending on thespecific devices and systems being used and the type of databeing transmitted. Due to the complexity of IoT networks,identifying unexpected behaviours in the system is vital forpreventing anomalies from escalating into larger issues. Forexample, AD can help identify potential security threats,such as unauthorised access to the system or unusual datausage patterns that could indicate a cyber-attack. Especiallywith the continuous development of IoT, the data generatedby massive sensors and smart objects need to be processed innear real-time, which makes it extremely urgent to classifyand detect abnormal data.In this paper, we explore the performance of the selectedDeep Learning models for DAD tasks using the KDD Cup99 dataset. Our research questions focus on the impactof different pre-precessing techniques, model architectures,optimisation algorithms, hyperparameters, ensemble tech-niques, and comparisons with other state-of-the-art methodson the performance of these models. Additionally, we aim toinvestigate the interpretability of the models and provide acomprehensive evaluation of their strengths and limitations.",
  ". Literature Review": "From a statistical perspective, most anomaly detectionmethods are based on constructing a probability distributionmodel and considering how likely the data can be fittedinto the model . Therefore, anomalies are the datathat have a low probability in the distribution model. Toexplore the suitable approaches to ensure network securityand optimise performance, traditional and machine learningalgorithms have been both adopted and developed for real-world applications. Moreover, Deep Learning (DL) methodsfor anomaly detection have been proven to show promisingresults in learning complex patterns and features from dataand can detect anomalies in a relatively more accurateand efficient manner compared to traditional methods . Traditional machine learning methods generally re-quire more sophisticated feature engineering design, andthe cross-domain versatility is not competitive . Addi-tionally, these methods are generally aimed at time series-based anomaly detection tasks, as the correlation between multiple series is difficult to design and compute by model-driven methods, while DL models are more suitable for suchscenarios .The relationship between deep learning and anomalydetection is inseparable because DL can be used for fea-ture extraction or integrated with AD methods to learneffective representations of normal instances and even di-rectly learn scalar anomaly scores in an end-to-end fashion. AD algorithms based on deep learning methods canbe mainly divided into the following categories: Distance-based, Classification, Clustering-based, Predictability-based,and Reconstruction method.The DAD techniques have been popular due to theability of deep neural networks to learn complex patternsand representations of data. As more DL algorithms havebeen proposed, a variety of choices for anomaly detectiontasks are available nowadays, including autoencoder, deepbelief networks, recurrent neural networks, convolutionalneural networks, etc.One advantage of adopting DL algorithms for anomalydetection is the ability to handle high-dimensional andcomplex data, which is beneficial in various real-worldapplications. This is because deep learning algorithms canlearn complex data representations that capture local andglobal structures, and also handle both structured and un-structured data, making them well-suited for a wide rangeof scenarios . Additionally, DL algorithms can handlenoisy data and identify anomalies accurately by learningrobust representations of data. A comprehensive summaryof recent research work on DAD algorithms can be foundin .Despite the advantages we discussed, there are researchgaps in the field of DAD that need to be addressed to furtherimprove its effectiveness and efficiency. For example, DADmodels often require a large amount of data for training,which can be a challenge for real-world applications with alimited amount of data. Also, DAD models are sensitive tooutliers and typically designed to handle structured data, im-pacting the models performance. Moreover, with a limitedamount of labelled data, it is difficult to train DAD models,making adopting semi-supervised and unsupervised DADmodels even more desirable.",
  ". Research Approach & Methodology": "This work seeks to implement and assess existing al-gorithms from the literature within the context of deepanomaly detection. The primary objective is to execute thesealgorithms on benchmark datasets, with the ultimate goalof identifying potential enhancements. The methodologyadopted in this research involves a meticulous and system-atic exploration of deep anomaly detection.The research will adopt a hypothesis-driven approach,postulating that deep learning (DL) models can proficientlyidentify anomalies within intricate datasets. The investiga-tive process will encompass a combination of theoretical and",
  "TABLE 1. RESEARCH WORK ON DEEP ANOMALY DETECTION ALGORITHMS": "empirical methods. These include an extensive literature re-view, thorough data pre-processing, and normalization pro-cedures, as well as a rigorous implementation and trainingof models, followed by a robust evaluation and validationphase.To ensure the credibility of the findings, the research out-comes will be subject to in-depth analysis using appropriatestatistical methods. This statistical scrutiny aims to establishthe statistical significance of the results. Moreover, ethicalconsiderations will underpin the entire research process.This involves ensuring that data acquisition and utilizationadhere strictly to ethical and legal standards. Overall, thiswork aspires to contribute valuable insights to the field ofdeep anomaly detection through a methodical and ethicalresearch approach.We rely on deep learning models to detect anomalies,where the models are trained on the pre-processed dataset toidentify patterns and anomalies in the data. The performanceof the model is then evaluated using various metrics, such asaccuracy precision, recall, and F1-score. The results are thenanalysed to determine the effectiveness of the DAD methodsin solving the research problem. Finally, the findings arereported clearly and concisely, and the implications of theresults are discussed in the context of the research ques-tion and objectives. We maintain a rigorous and systematicapproach to ensure that the results are accurate and reliable.",
  ". GAN. GANs are a class of deep learning algorithmsthat have two neural networks to be trained: a generatornetwork and a discriminator network. The generator network": "learns to generate synthetic data that is similar to the normaldata, while the discriminator network learns to distinguishbetween the synthetic data generated by the generator andthe actual normal data. The generator and discriminatorare trained together in a two-player minimax game, wherethe generator tries to fool the discriminator by generatingsynthetic data that is indistinguishable from the normal data,while the discriminator accurately identifies the syntheticdata .In the training process, the generator and discriminatornetworks are trained alternatively. First, the generator net-work is updated to generate synthetic data that is more simi-lar to the normal data, and then the discriminator network isupdated to better distinguish between the synthetic data andthe normal data. The process continues until the generatornetwork can generate synthetic data that is indistinguishablefrom the normal data, and the discriminator network cannotaccurately distinguish between the two. After training, thediscriminator network can be used to score new data points,with a lower score indicating a higher likelihood of beinganomalous. Mathematically, the score for a new data pointx can be defined as:",
  "S(x) = logD(x)(1)": "Strength. GANs are unsupervised learning algorithms,which means that they can detect anomalies without labelswhen the labelled data is scarce or difficult to obtain. Theycan also generate new data samples similar to the trainingdata, which can be used in AD tasks to identify data thatis significantly different from the normal data. Moreover,GANs are suitable for handling high-dimensional data, mak-ing them well-suited for complex datasets, such as KDDCup 99. Weaknesses. GANs can be difficult to train, as thetraining process can be unstable and prone to producingsuboptimal results. Additionally, GANs are computationallyintensive algorithms, which can be a drawback in real-worldapplications where the size of the dataset is large. Thesefacts should be taken into account when processing ourselected dataset. 3.1.2. CNN. CNNs are commonly used for image classifi-cation and anomaly detection in sequential data such as timeseries or network traffic data. CNNs are trained on the pre-processed and labelled data, learning to recognise patternsthat are indicative of normal or anomalous behaviour. Withthe same advantage as RNNs that can identify complexpatterns, CNNs can also learn and adapt to changes in thedata over time, making them suitable for detecting anomaliesin dynamic environments.The convolution layer in a CNN model applies a convo-lution operation to the input data, which can be representedasC = X W + b(2) where C is the output of the convolution layer, X is the inputdata, W is the convolution kernel, and b is the bias. Theconvolution operation slides the convolution kernel over theinput data and computes a dot product between the kerneland the overlapping region of the input data. This operationresults in a feature map, which represents the most essentialfeatures in the data.After the convolution operation, an activation function isapplied to the output of the convolution layer. An activationfunction is used to introduce non-linearity into the network,allowing it to learn more complex data representations. Acommon activation function used in CNN is the rectifiedlinear unit (ReLU) function, which is defined as:",
  "f(x) = max(0, x)(3)": "After the activation function, a pooling layer is applied toreduce the size of the data, which helps to reduce overfittingand make the network computationally efficient.After multiple convolution and pooling layers, the datais fed into a fully connected layer. A fully connected layeris a type of layer in a neural network that connects all theneurons in one layer to all the neurons in the next layer. Thefully connected layer can be mathematically represented as:",
  "Y = W2P + b2(4)": "where Y is the output of the fully connected layer, W2 andb2 are the weights and biases of the layer, and P is theoutput of the pooling layer. In the last step, the data is fedinto an output layer, which can be used to predict whethera given segment of IoT network traffic is anomalous or not.Strength. CNN models can extract features from thedata through convolutional and pooling layers, which canlearn to recognise patterns in the dataset and reduce thedimensionality of the data. They also perform well and arerobust to noise in detecting anomalies in sequential data, which is an efficient tool to characterise the differencesbetween normal and anomalies.Weakness. The complexity of CNN models makes themdifficult to train and interpret and increases the computa-tional requirements of the models. Another weakness of theCNN models is their tendency to overfit the training data,which results in poor performance on unseen data. Lastly,the CNN models require large amounts of high-quality datato train effectively, which can be a challenge when dealingwith small datasets or corrupted data. 3.1.3. LSTM.LSTM models are the type of recurrentneural network that is commonly used for DAD tasks. Inan LSTM model, information is passed through a series ofmemory cells, gates, and layers to learn a temporal repre-sentation of the input data. This makes LSTM particularlywell-suited for sequential data, where the order of the datapoints is important. A memory cell is the basic unit of anLSTM model. It holds information for a certain period oftime, allowing the model to maintain its memory of pastinput even as new inputs are received. The memory cell isupdated at each time step in the sequence .Strengths. LSTM uses memory cells, which allow thenetwork to store and access information from previous timesteps. It makes LSTMs suitable for tasks that require thenetwork to remember information from the past and use itto make predictions. LSTMs are also capable of handlinglong-term dependencies. Traditional RNNs struggle withthis aspect because as the gap between relevant informa-tion and the current time step grows, the backpropagationgradient becomes weaker, and eventually vanishes. LSTMssolve this problem by using gates that control the flow ofinformation into and out of the memory cells. In addition,LSTMs also solve the problems when the gradient used forbackpropagation becomes too small to update the networkweights effectively.Weaknesses. One of the obvious weaknesses of LSTMsis their complexity, which requires a large number of pa-rameters and computations, making it time-consuming andcomputationally expensive to train and deploy the infrastruc-ture. LSTS are also prone to overfilling, making the modeldifficult to interpret. 3.1.4. AutoEncoder. AE is trained to reconstruct its inputdata. The idea behind using AEs for anomaly detection isto train the network on normal data, and then to identifydeviations from the normal behaviours.The architecture of an autoencoder-based algorithm forDAD typically consists of two parts: an encoder and adecoder. The encoder maps the input data to a lower-dimensional representation, while the decoder maps thelower-dimensional representation back to the original dataspace. The training process minimises the reconstructionerror between the input data and its reconstruction, which isobtained by passing the input data through the encoder andthe decoder.Once the AE is trained, it can be used for DAD bycomputing the reconstruction error for unseen data. If the reconstruction error for a particular instance is significantlylarger than the reconstruction errors for the normal instancesin the training set, it is considered to be an anomaly. Thethreshold for what constitutes a significant difference istypically set based on the distribution of the reconstructionerrors for the normal instances in the training set .Strengths. AEs are especially suitable for detectinganomalies in non-linear data, as they can learn complex,non-linear relationships between inputs and outputs. Theycan also be used for unsupervised AD tasks, and be adaptedto different types of data, as they can be trained on a varietyof data distributions, including high-dimensional and sparsedata. Finally, AEs are generally robust to small amounts ofnoise in the data, as they are trained to reconstruct the input,rather than to classify it into a specific category.Weaknesses. As with some other DL algorithms, AEscan also be computationally expensive, as they require alarge number of training iterations and can be slow toconverge. Also, if the dataset is small or the network archi-tecture is complex, the AEs can be prone to overfitting. Thisalso leads to the degradation of the model performance in thedata, which is significantly different from the normal data, asthe network may not be able to learn a good reconstructionfor the anomalies.",
  ". Dataset": "We use KDD Cup 99 as our dataset. The dataset is awell-known benchmark dataset for intrusion detection, andit is most adopted for evaluating the performance of deepanomaly detection models. The dataset was created as partof the Knowledge Discovery and Data Mining (KDD) Cupcompetition held in 1999 and consists of a large set ofnetwork traffic data collected by the U.S. Defense AdvancedResearch Projects Agency (DARPA). It contains data froma simulated military network and includes various types ofnetwork attacks.The dataset, consisting of over 4 million instances ofnetwork traffic, serves as an ideal choice for evaluating deeplearning-based anomaly detection methods. Key considera-tions include its representatives of real-world IoT networks,ample size for training deep neural networks, pre-processingfor data simplification, public availability for widespread re-search use, and continued relevance in addressing contempo-rary IoT security challenges due to its historical significancein network intrusion detection.In a nutshell, the KDD Cup 1999 dataset is widelyevaluated in the academic community and industry and isconsidered a benchmark dataset for evaluating the perfor-mance of anomaly detection algorithms. It is also a valuableresource for researchers studying network security and forpractitioners building intrusion detection systems. A sum-mary of the dataset can be found in . In our work,",
  ". The illustration of the experimental process in our work": "The original KDD Cup 99 dataset (10%) contains 42attributes and 494,021 entries. We first cleaned the datasetby dropping missing values and naming the attributes ac-cordingly. We then encoded the categorical attributes (pro-tocol type, service, and flag) and labels (label) andmapped the target variables into numerical values. Weadopted both one-hot encoding and label encoding methodsin our experiment, with One-hot encoding on the AE model,as it can be trained on sparse data, and labelling encodingis adopted for the rest models.Normaliser and MinMaxScaler from the sklearn packageare used to transform the dataset after splitting it intotraining and testing. In the experiments, 80% of the normalsamples were randomly selected for training, and a testing dataset from the remaining samples was generated. Weevaluate the performance of the models using metrics suchas accuracy, precision, recall, and F1-score. givesan illustration of all the steps in our experimental design. 4.2.2. Model Implementation.We adopted a variety ofpre-processing techniques, such as normalisation, featurescaling, and feature selection, to prepare the dataset. Inthe experiment phase, different hyperparameters have beenvaried for each model, such as the number of hidden layers,the size of the hidden layers, the learning rate, and thebatch size. Then we evaluated the performance of eachmodel using metrics to determine the optimal pre-processingtechniques and hyperparameters that result in the best per-formance for each model.We implemented a GAN-based algorithm that incorpo-rates an encoder that maps input samples to latent represen-tations, as well as a generator and a discriminator duringtraining. A score function is then defined to measure howunusual an example is based on a convex reconstruction lossand a combination of discriminator losses. The discrimi-nators cross-entropy or feature matching loss is used toevaluate whether the reconstructed data has similar featuresto the real samples in the discriminator. In the generator,we defined the latent space dimension of 114, with 6 hiddenlayers, a learning rate of 1e5, and an activation function oftanh. For the discriminator, we defined a 6 hidden layerarchitecture with 128 neurons per layer, a ReLU activationfunction, a learning rate of 0.00001, and 0.2 dropout. Forthe GAN network, we trained for 10 epochs with a batchsize of 512.The other algorithm utilised the AE method as a discrim-inative DNN, where the target output is similar to the input,and the number of hidden layer nodes is lower than theinput. Training is done by minimizing construction error anda regularization strategy, after which hidden layer results areconsidered as compressed representations of the input data.The method uses a deep autoencoder to encode features intodifferent feature groups and uses the feature vector of thelast hidden layer of the encoder as a representation of theattack score of the input data. The AE model is defined withone input layer, 3 hidden layers with activation functions oftanh and ReLU, and a decoder.In the proposed model, as illustrated in , weadopted the CNN+LSTM method, which contains the net-work packet filters in three layers. The KNN is used forgeneral categorisation, which classifies the input of thedataset. The second layer (CNN + LSTM) allows the modelto analyse a series of data and makes the filter check thedata against previously received packets. The outputs of thefirst two filters are then compared, and the conflicting inputis sent to the third layer if a conflict is detected. The thirdlayer is a Random Forest (RF) classifier. The RF classifierclassifies the final result of the input. In this experiment, weadopted the sparse categorical crossentropy as the lossfunction and SGD as the optimizer. We also set the check-point by 1 verbose and monitored by validation accuracy,with overwriting the current file by the maximisation of the monitored quantity. When fitting the CNN + LSTM layers,we adopted 20 epochs and a batch size of 128. After trainingthe RF classifier on conflicted instances, we combined thepre-trained models to make a final prediction on the unseendata.",
  ". Results and Evaluations": "In our proposed model, the KNN layer produced anaccuracy of 96.81%, while the CNN+LSTM model obtainedan accuracy of 97.83% in overall detection. We then evalu-ated the assembled model by combining the three layers andgave the final output for the data. An accuracy of 98.22%was acquired using the ensemble model, which is higherthan any of the previous classifiers. We also compared theresults among all three implemented models. The results canbe found in .",
  ". Conclusion": "In this work, we conducted a literature review of deepanomaly detection on IoT network traffic analysis. Thereview has shown a growing interest in using DL methodsfor the detection of anomalies and highlighted several deep-learning models categorised by the nature of the methods.The result of this literature review provides a foundation forthe following research on DAD analysis and highlights thepotential of DL methods for the task. We then proposed touse ensemble techniques in current models for deep anomalydetection in IoT network traffic analysis. The model isevaluated in terms of accuracy, precision, recall, F1-score,and confusion matrix.The result of the empirical evaluation showed that theproposed model outperformed the existing models such asAE and GAN models in terms of precision, recall, and F1-score. The proposed model was able to achieve over 98%accuracy, which is significantly higher than the other mod-els. In light of this researchs findings, we recommend usingthe ensemble model method for deep anomaly detection inIoT network traffic analysis and encourage future researchin this area to improve further the performance of DeepLearning models for this task.",
  "V. Chandola, A. Banerjee, and V. Kumar, Anomaly detection: Asurvey. acm computing surveys, vol, vol. 41, p. 15, 2009": "B. Appiah, Z. Qin, O. T. Nartey, B. Agemang, and A. J. A. Kan-pogninge, Anomaly detection based on discriminative generative ad-versarial network, International Journal of Network Security, vol. 23,no. 4, pp. 718724, 2021. F. Arellano-Espitia, M. Delgado-Prieto, A.-D. Gonzalez-Abreu,J. J. Saucedo-Dorantes, and R. A. Osornio-Rios, Deep-compact-clustering based anomaly detection applied to electromechanical in-dustrial systems, Sensors, vol. 21, no. 17, p. 5830, 2021.",
  "B. Staar, M. Lutjen, and M. Freitag, Anomaly detection with convo-lutional neural networks for industrial surface inspection, ProcediaCIRP, vol. 79, pp. 484489, 2019": "T. Tayeh, S. Aburakhia, R. Myers, and A. Shami, Distance-basedanomaly detection for industrial surfaces using triplet networks, in2020 11th IEEE Annual Information Technology, Electronics andMobile Communication Conference (IEMCON).IEEE, 2020, pp.03720377. G. Yuan, B. Li, Y. Yao, and S. Zhang, A deep learning enabledsubspace spectral ensemble clustering approach for web anomaly de-tection, in 2017 International Joint Conference on Neural Networks(IJCNN).IEEE, 2017, pp. 38963903."
}