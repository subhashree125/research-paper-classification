{
  "ABSTRACT": "Recent advancements in social bot detection have been driven bythe adoption of Graph Neural Networks. The social graph, con-structed from social network interactions, contains benign and botaccounts that influence each other. However, previous graph-baseddetection methods that follow the transductive message-passingparadigm may not fully utilize hidden graph information and arevulnerable to adversarial bot behavior. The indiscriminate messagepassing between nodes from different categories and communi-ties results in excessively homogeneous node representations, ul-timately reducing the effectiveness of social bot detectors. In thispaper, we propose SeBot, a novel multi-view graph-based con-trastive learning-enabled social bot detector. In particular, we usestructural entropy as an uncertainty metric to optimize the entiregraphs structure and subgraph-level granularity, revealing the im-plicitly existing hierarchical community structure. And we designan encoder to enable message passing beyond the homophily as-sumption, enhancing robustness to adversarial behaviors of socialbots. Finally, we employ multi-view contrastive learning to maxi-mize mutual information between different views and enhance thedetection performance through multi-task learning. Experimentalresults demonstrate that our approach significantly improves theperformance of social bot detection compared with SOTA methods.",
  "Corresponding authors": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-1-4503-9416-1/23/04",
  "social bot detection, graph neural networks, contrastive learning,structural entropy": "ACM Reference Format:Yingguang Yang, Qi Wu, Buyun He, Hao Peng, Renyu Yang, Zhifeng Hao,and Yong Liao. 2024. SeBot: Structural Entropy Guided Multi-View Con-trastive Learning for Social Bot Detection. In The 30th SIGKDD Conferenceon Knowledge Discovery and Data Mining (KDD 24), August 2529, 2024,Spain. ACM, New York, NY, USA, 12 pages.",
  "INTRODUCTION": "Social bots are automated controlled accounts that widely existon social platforms such as Twitter and Weibo, in most cases, formalicious purposes such as spreading misinformation , ma-nipulating public opinion , and influencing political elections. They will undoubtedly give rise to societal disharmony insocial network environments.Conventional methods for social bots detection primarily focuson extracting discriminative features, ranging from user attributes,, text features to structure features, which can be thenused to train classifiers in a supervised manner. Due to the con-tinuous evolution of social bots , including their ability to stealinformation from legitimate accounts and mimic normal accountbehaviors , these traditional methods turn out to be ineffectivein identifying the latest generation bots. A recent advancementin social bot detection is introducing graph neural networks that treat accounts and the interactions in-between as nodes andedges, respectively. Multi-relational heterogeneous graphs can beestablished and a Relation Graph Transformer (RGT) is respon-sible for aggregating information from neighbors. Such approachesconsider the features of each account as interdependent ones andleverage the semantic information of relationships between ac-counts to generate semantically-richer representations.While successful, there are two challenges facing the existinggraph-based methods (illustrated in ). i) How to fully exploitthe hierarchical information hidden in the graph structure? Unlikeother semi-supervised node classification tasks, some accounts in",
  ": Illustration of community structure and inter-classinteractions co-exists in social networks. The abstraction ofthe hierarchical community structure is presented in theform of an encoding tree": "social platforms tend to exhibit stronger correlations with others inthe topological structure due to shared interests, events of interest,etc. , indicating an intrinsic hierarchical community structure inthe graph constructed by social bot detectors. The existing graph-based social bot detection methods primarily focused on aggregating node-level information on theoriginal graph, neglecting comprehensive utilization of high-ordergraph structural information. As a result, the generated representa-tions only capture low-order graph information and lack high-ordersemantic information. ii) How to handle the adversarial behaviors ofbots intentionally interacting with humans to evade detection? Ex-isting bot detection methods rely on the assumption that bots andhumans exhibit stronger connections with nodes of their category.However, for social bots, especially in the case of advanced socialbot control programs, gathering human-specific information fromneighbors would be more advantageous for concealing their trueidentity. Therefore, bots intentionally tend to establish associationswith human accounts to escape detection .To tackle these two challenges, we propose SeBot, a novelStructural Entropy Guided Social Bot Detection framework throughgraph contrastive learning enhanced classification with both intra-class and inter-class edges. Motivated by minimum entropy theory, indicating systems at the minimum level of uncertainties, andstructural entropy further offers an effective measure of the in-formation embedded in an arbitrary graph and structural diversity.We construct encoding trees for both the entire input graph andthe subgraphs of each node by minimizing their structural entropy.We then use the optimal encoding trees to describe the hierarchicalstructural information of the graph and obtain cluster assignmentmatrixes for the nodes. Subsequently, node representations are ob-tained through structural entropy pooling and unpooling for nodeand graph classification. On the other hand, we designed an encoderthat can adaptively make neighbors similar or differentiate them tohandle the adversarial behavior of bots. Finally, we utilize the noderepresentations generated by the three modules to calculate bothcross-entropy loss and self-supervised contrastive learning loss.The contributions of this work can be summarized as follows:",
  "RELATED WORK": "Graph-based Social Bot Detection. Graph-based social bot de-tection has been of ultimate importance in modeling various in-teractions intrinsically existing in social networks. Previous meth-ods have focused primarily on designing informa-tion aggregation strategies for better detection performance. takes the first attempt to use graph convolutional neural networks(GCNs) for detecting social bots. Typically, BotRGCN uti-lizes relational graph convolutional networks (RGCNs) to ag-gregate neighbor information from edges of different relations. proposed RGT, which utilizes a self-attention mechanism to adap-tively aggregate information from neighbors in each relationalview of the graph. Although these methods have shown signifi-cant improvements compared to traditional feature engineeringand text-based approaches , they may not fully exploit the cru-cial semantic information concealed in the graph structure and thegraph structure obtained from sampling social networks contain asignificant amount of uncertainty and randomness.Graph Self-Supervised Learning. Self-supervised learning hasachieved great success in the fields of natural language process-ing and computer vision without the need for prohibitivelycostly labeled data. Graph contrastive learning (GCL) is a typicalparadigm of self-supervised learning on graphs, aiming at learn-ing invariant representations between different graph views. Forinstance, DGI utilizes a local-global mutual information maxi-mization approach to obtain node representations. proposesa series of graph augmentation methods including node dropping,edge perturbation, attribute masking, and so on. propose anadaptive way for graph augmentation, which assigns adaptive prob-abilities to attribute and topological perturbation. However, theseaugmentation methods inevitably suffer from the loss of essentialinformation or introduce class-redundant noise. Due to the sig-nificant impact of the quality of generated views on contrastivelearning, theoretically proves that the anchor view containingessential semantic information should have the minimum structuralentropy. Inspired by this, structural entropy is employed by us togenerate the anchor view with minimum uncertainty.Structural Entropy. After Shannon proposed information entropyto measure system uncertainty , the measurement of uncer-tainty in graph structure has been widely studied, and several meth-ods have been developed to quantify it. Among them,structural entropy has been widely used in re-cent years and has shown promising results in graph structurelearning , graph pooling, and other tasks. Since structuralentropy can be used as a metric to measure the complexity of graphhierarchical structure, previous applications have mainly focusedon minimizing the structural entropy of the constructed encod-ing tree. For instance, SEP defines MERGE, REMOVE, andFILL operations to update the constructed encoding tree based onthe principle of minimizing structural entropy. In this paper, we",
  "PRELIMINARIES": "In this section, we first illustrate the graph-based social bot detec-tion task, followed by an introduction to the definition of graphcontrastive learning.Definition 3.1. Graph-based Social bot detection. Graph-based social bot detection can be regarded as a semi-supervisednode binary classification problem on a multi-relational graph. Itinvolves treating the accounts in social platforms as nodes andthe interactions such as \"following\" and \"follower\" as edges ofdifferent relations. Constructed graph in this task can be formulatedas G = {V, E, X}, where V = { | = 1, 2, . . . ,} is the set ofall nodes, E = =1E represents the set of edges formed by relations and X is the feature matrix. Row of X represents thefeature vector of the -th node. Total detection process is to use thegraph G and the labels of training nodes Ytrain to predict the labelsof test nodes Ytest :",
  "(G, Ytrain ) Ytest .(1)": "Definition 3.2. Graph Contrastive Learning. In the generalgraph contrastive learning paradigm for node classification, twoaugmented graphs G, and G are generated using different graphaugmentation methods (such as edge dropping, feature masking,etc.) on the input graph G. Subsequently, an encoder consisting ofmulti-layer graph neural networks is employed to generate noderepresentations including topological information existing in graphstructure. During the first training stage, these representations arefurther mapped into an embedding space by a shared projection head for contrastive learning. A typical graph contrastive loss,InfoNCE , treats the same node in different views and as positive pairs and other nodes as negative pairs. The graphcontrastive learning loss L of node and total loss L can beformulated as:",
  "METHODOLOGY4.1Overview of SeBot": "The total pipeline of SeBot is illustrated in . To begin with,an attributed multi-relational graph G is constructed by represent-ing social network interactions as edges. Subsequently, it is fed intothree different modules to obtain representations at multi-grainedlevels under various receptive field scopes h = [h h h ].The two modules above are responsible for constructing encodingtrees by minimizing structural entropy for the entire graph viewG and -order subgraph view G respectively. G is formed bythe target node and surrounding neighbor nodes. The view G isgenerated by edge removing. Then, bottom-up information prop-agation is performed according to these encoding trees to obtainnode embeddings h and h . In the third module, to counteract",
  "KDD 24, August 2529, 2024, SpainYingguang Yang et at": "the intentional evasion behaviors of social bots, we devise a graphconvolutional layer that can make neighbors similar or discrimina-tive adaptively while maintaining normalization. Simultaneously,a relational channel-wise mixing (RCM) layer is proposed to inte-grate information from different relations to obtain h . Finally, thefusion of the three modules involves computing cross-entropy lossfor classification and utilizing graph self-supervised contrastivelearning loss to capture shared information between three views,enhancing the classification learning process.",
  "Community-aware Hierarchical Augment": "In social networks, some accounts may exhibit more pronouncedconnections with each other due to shared interests, events, andso on, thus forming communities. However, previous detectionmethods have not effectively leveraged the community structureinformation within social networks. To reveal the hierarchical struc-ture within the graph, we utilize structural entropy minimization toobtain fixed-height encoding trees, where the child nodes of eachnode belong to the same community. For the sake of clarity, we firstillustrate the definition of structural entropy and its minimizationalgorithm.Structural Entropy. Structural entropy is initially proposed by to measure the uncertainty of graph structure information. Thestructural entropy of a given graph G = {V, E, X} on its encodingTree is defined as:",
  "(+ ) ,(3)": "where is a node in except for root node and also stands for asubset V V, is the number of edges connecting nodes in andoutside V, + is the immediate predecessor of of and (),(+ ) and (V) are the sum of degrees of nodes in , + andV, respectively. The structural entropy of graph G is the entropy ofthe encoding tree with the minimum structural entropy: H (G) =min {H (G)}. According to this definition, structural entropycan be used to decode the hierarchical structure of a given graphinto an encoding tree as a measurement of community division. Inaddition, the generated encoding tree can be seen as the naturalmulti-grained hierarchical community division result.Minimization Algorithm. In addition to the optimal encodingtree with the minimum structural entropy, a fixed level of commu-nity partitioning is preferred for the specific scenario. Consideringthis, the -dimension structural entropy of G is defined as theoptimal encoding tree with a fixed height :",
  "H () (G) =min:Height( )={H (G)}.(4)": "The total process of generation of a encoding tree with a fixedheight can be divided into two steps: 1) construction of the full-height binary encoding tree and 2) compression of the binary en-coding tree to height . Given root node of the encoding tree ,all original nodes in graph G = (V, E) are treated as leaf nodes.We first define two iterative operations on .",
  "+ . = + . ..(6)": "The generation of the encoding tree with a fixed height pri-marily involves iterations through two operations to obtain theminimum structural entropy, which is shown in Algorithm 1. Tostart with, we initialize an encoding tree by treating all nodes in Vas children of root node . During step 1, an iterative Merge(1, 2)is conducted with the aim of minimizing structural entropy to ob-tain a binary coding tree without height limitation. In this way,selected leaf nodes are combined to form new community divisionswith minimal structural entropy. Then, to compress height to aspecific hyperparameter , Drop() is leveraged to merge smalldivisions into larger ones, and thus the height of the coding tree isreduced, which is still following the structural entropy minimiza-tion strategy. Eventually, the encoding tree with fixed height andminimal structural entropy is obtained.Obtained encoding tree in this manner can also be seen as theanchor view that includes minimal but sufficient important informa-tion in an unsupervised manner . The quality of views generatedby graph augmentation technologies plays a crucial role in learninginformative representations. According to graph information bottle-neck theory (GIB) , retaining important information in a graphview should involve maximizing mutual information between theoutput and labels while reducing mutual information between inputand output. This can be formally expressed as follows:",
  "Message Passing on Encoding Tree": "To obtain node representations and subgraph representations, themessage passing on the encoding tree is carried out bottom-up,where the generated parent nodes aggregate information from theirchild nodes. This process begins with the leaf nodes (i.e., the nodesin V) at the first layer transmitting information to their second-layer parent nodes. Specifically, given the cluster assignment matrixS R +1 where and +1 are the number of nodes andassigned clusters (i.e. nodes in the next layer) in the -th layer,each element in S equal to 1 indicates that the node belongs to acorresponding cluster. The adjacency matrix A+1 R+1+1 andthe hidden layer representations P+1 R+1 for the ( + 1)-thlayer can be obtained by matrix multiplication: SEP : A+1 = S AS;P+1 = S H,(8)where A is the adjacency matrix and H denotes the hidden fea-tures martix in the -th layer. As pooling continues, the number ofnodes decreases. To obtain representations of nodes in V, we fur-ther employ unpooling to ensure that the number of nodes matchesthe number of nodes in V: SEP-U : A+1 = SAS ;P+1 = SH,(9)where S is the same matrix used in previous pooling layers. Thenode-level representation h is obtained through multiple layersof SEP to obtain high-order community representations and multi-layer SEP-U reconstructions.On the other hand, the representation of each subgraph extractedfor every target user can be obtained by concatenating the resultsof SEP pooling layers:",
  "Relational Information Aggregation": "To alleviate graph adversarial attacks by social bots (i.e., activelyestablishing relationships with humans), we propose a relationalinformation aggregation mechanism beyond homophily and a rela-tion channel-wise mixing layer in this subsection. 4.4.1Relational graph convolution beyond resemblance limitation.Previous work has adopted RGCN to detect bot accountsand has shown promising success in modeling different relations.However, the information aggregation of RGCN is based on thehomophily assumption (i.e., nodes belonging to the same class tend to be connected), and advanced bots may consciously interactmore with humans. Considering this issue, we incorporate high-frequency information (i.e., the differences between nodes) into theinformation aggregation strategy of RGCN through the generationof negative attention coefficients. However, positive and negativeweights generated directly through the tanh activation functioncan not be normalized. To ensure the consistency of informationaggregation, we further introduce the Gumbel-Max reparametriza-tion trick to make the weights close to 1 or -1. Specifically, theattention weight ,{ }of the edge in relation and the layer is generated by: ,{ }= tanh(g{ },h{1} h{1}+ (1 ))/,(11)where g{ } R2 denotes the trainabele parameter, (0, 1)is the sampled Gumbel random variate and is a small tempera-ture used to amplify ,{ }. In this manner, when the weights areclose to 1, it retains similar information with neighbors, whereaswhen close to -1, it preserves dissimilar information. In addition,normalization can be directly performed based on the number ofneighbors:",
  "where () is the neighbors of and { }is the weight matrix.Due to the adopted parametrization tick, normalization is approxi-mately satisfied by | ()| ( ) |,{ }|": "4.4.2Relational Channel-wise Mixing. After aggregating informa-tion independently across different relations, it is necessary tomerge this relational information to acquire representations withricher semantics. Previous adaptive method is limited to gen-erating normalized weight coefficients for representations in eachrelation, inevitably introducing some noise information present inspecific relations. Inspired by , we propose a relational channel-wise adaptive fusion layer. Specifically, given the embeddings h,{ }generated in each relation and at layer , we first calculate theweight vector of the relation:",
  "Multi-Task Optimization and Learning": "After obtaining node representations from the three modules, it isnecessary to design the loss function as the learning objective. Theprimary objective of the model remains classification, to identifybot accounts. We employ a two-layer MLP as the classificationlayer for the concatenation h = [h h h ] and calculate theclassification loss using binary cross-entropy:",
  "= ((h1 + 1)2 + 2).(16)": "Additionally, as shown in , in real-world scenarios, hi-erarchical community structure and heterophily coexist in socialnetworks. Our goal is to learn unified node representations that canincorporate both types of information simultaneously. However, toour knowledge, there are no methods that capture both types ofinformation simultaneously. Therefore, we use three modules to ob-tain three representations, h , h , and h , each containing differentsemantic information. Specifically, h includes global hierarchical structure information, h includes local hierarchical structure in-formation, and h includes specific category information of theneighborhood.However, representations obtained through independent mod-ules are often one-sided. Therefore, we further use self-supervisedcontrastive learning to capture the consistency between differentrepresentations. Specifically, by maximizing the mutual informa-tion between different representations. The proposed two levelcontrastive learning losses L and L are computed basedon the representations of all nodes (i.e., H, H and H) acrossdifferent views:",
  "L = ( (H), (H)),(17)": "where () and () are defined as projection heads for node-level and subgraph-level contrastive learning, respectively. In thisway, two seemingly contradictory challenges are unified and ad-dressed in a self-supervised manner, rather than being simply solvedindependently.Ultimately, the overall loss of the proposed method is calculatedby summing the aforementioned three learning losses:",
  "return the predicted label set for the test nodes Ytest": "structural entropy minimization, thus, will be around .Besides, a large-scale social network generally has more edges thannodes, i.e., , thus the runtime of Algorithm 1 almost scaleslinearly in the number of edges.The overall time complexity of the three proposed modules is( + + ( log + )). Specifically, in .3, the timecomplexities of SEP, SEP-U, and SEP-G are all (). In .4,the proposed relational information aggregation has a time com-plexity of (( +)), where represents the number of layersand represents the number of relations.",
  "SeBot: Structural Entropy Guided Multi-View Contrastive Learning for Social Bot DetectionKDD 24, August 2529, 2024, Spain": "0.010.020.030.040.050.060.070.080.090.1 0.09 0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.010.1 86.8186.8186.6486.7386.1486.1486.5686.5686.8185.21 86.7386.8186.7386.3986.7386.5685.8085.4686.8186.05 87.1587.2486.6487.0786.0586.2286.5686.0586.6486.31 86.8186.6486.9886.6486.7386.7386.6486.0586.7386.22 86.8186.9086.6486.7386.7386.7387.0786.9886.0586.39 86.9086.6486.8186.8186.9087.0786.5686.9086.7386.64 86.1486.5687.1586.8186.7386.2286.8186.7386.2286.48 86.3986.3986.8186.8186.7386.6486.9086.4886.7386.56 86.3186.9887.1585.9786.9086.7386.6486.5686.4886.56 86.0585.9786.4885.8886.2287.1586.1486.4886.4886.73 Tree depth k=3 on TwiBot-20 0.010.020.030.040.050.060.070.080.090.1 0.09 0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.010.1 86.7386.9086.3186.8187.2485.4686.5686.1486.6486.73 86.7386.5687.2486.4885.6386.1486.3986.3986.9086.98 86.3186.4886.3186.4886.1486.5686.6486.4886.3986.64 86.9886.9086.9086.5686.9886.8186.5686.6486.3985.63 87.0786.8187.1586.9887.5786.8187.2486.6487.0786.56 86.9886.9886.8186.9086.6486.3986.6486.8185.6386.31 85.8087.0787.4087.0786.9886.4886.9086.7387.0785.80 86.4886.9086.7386.9086.3186.8186.6486.6486.3986.14 86.5687.3286.5686.6486.4886.0586.6485.4686.3985.97 86.7387.3286.3986.8186.5686.3186.5685.9786.4886.14 Tree depth k=4 on TwiBot-20 0.010.020.030.040.050.060.070.080.090.1 0.09 0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.010.1 86.7386.6486.7386.5685.8085.7185.8086.9086.8186.56 86.9087.4986.8186.9886.9086.7386.3185.7185.6385.97 87.2487.2487.2486.4886.7386.5685.8885.5586.0586.39 87.4086.8186.4885.9786.3186.3185.2185.4685.6385.88 86.9886.9086.9886.0585.0486.6486.7385.9785.3885.71 87.4086.9086.5686.2286.3185.7184.9586.5685.3886.22 87.1586.3186.7386.3186.1485.7185.1285.2986.1486.31 86.7385.9786.8186.5685.6385.8885.5585.9785.8085.97 86.5686.3186.1486.1486.6486.3185.9786.1485.9785.88 86.6486.4886.4885.1286.5685.7186.3185.8085.9786.56 Tree depth k=5 on TwiBot-20 85.0 85.5 86.0 86.5 87.0 87.5",
  "GBT calculates the empirical cross-correlation matrix usingnode representations and computes the loss using the Barlow-Twins loss function": "5.1.3Hyperparameter Setting. Hyperparameter settings of our ex-periments on TwiBot-20 and MGTAB are listed in . We usedthe AdamW optimizer to update the parameters. The learning rateis set to 0.01, which is larger compared to baseline models likeRGT, resulting in faster convergence. The dropout mechanism isemployed to prevent overfitting and maintain high generalizationcapacity. The weight of two contrastive loss 1 and 2 are set ac-cording to sensitive study results. The tree depth is set to 6, whichis a compromise between training time and accuracy. Gumbel-Maxreparametrization trick temperature is set to 0.01 to amplifyedge attention.",
  "ParameterT-20MGTABParameterT-20MGTAB": "optimizerAdamWAdamWhidden dimension3232learning rate0.010.01subgraph order 21dropout0.50.5L2 regularization3e-33e-3loss weight 10.090.05loss weight 20.030.05tree depth 66maximum epochs70200temperture 0.10.1trick temperature 0.010.01 5.1.4Implementation. Pytorch and Pytorch Geometric areleveraged to implement SeBot and other baselines. All experimentsare conducted on a cluster with 8 GeForce RTX 3090 GPUs with24 GB memory, 16 CPU cores, and 264 GB CPU memory. See ourcode1 for more details.",
  "RQ1: Performance Analysis": "To answer RQ1, we evaluate the performance of SeBot and 11 otherbaselines on two social bot detection benchmarks. The experimentalresults are presented in , which illustrates that: SeBot demonstrates superior performance compared to all otherbaselines on both datasets in terms of Accuracy and F1-score onboth datasets. Furthermore, it achieves relatively high results interms of Recall and Precision on TwiBot-20 and MGTAB, indicat-ing that SeBot is better at uncovering social bots and possessesstronger robustness. On the other hand, SeBot demonstrates thebetter generalization performance across both datasets, a featthat other methods cannot achieve. Compared to traditional GNNs (i.e., GCN, GAT, and GraphSage),SeBot not only considers the community structure but also isconscious of the adversarial structure intentionally constructedby social bots, thus exhibiting stronger detection performance.This also highlights the need for extra fine-grained designs whenapplying graph neural networks to social bot detection. Compared to GNNs beyond homophily (i.e., FAGCN, H2GCN,GPRGNN), SeBot extends further into the social bot detectionscenario, specifically on multi-relation directed graphs. In addi-tion, better performance also implies the significant importanceof considering adversarial heterophily in social bot detection. Compared with state-of-the-art graph-based social bot detectionmethods (i.e., BotRGCN and RGT), SeBot achieves the best accu-racy and F1-score as it further takes into account the structuralsemantics present in social networks, which proves to be po-tent for uncovering deeply concealed bots. Meanwhile, previousgraph-based detection methods relied on traditional informationaggregation patterns and could not fully capture the structuralinformation within the graph.",
  "SeBot87.240.1088.740.1392.971.1684.900.7990.461.4482.122.4281.732.7782.522.19OURs": "Compared with typical self-supervised graph contrastive learn-ing methods (i.e., DGI, GBT, and GRACE), SeBot retains essentialinformation within the graph by minimizing structural entropy,while other graph augmentation methods may unavoidably in-troduce noise or lead to the loss of crucial information relevant todownstream tasks. Its important to mention that class imbalancein MGTAB significantly impacts the performance of contrastivelearning techniques like DGI, leading to suboptimal results acrossvarious metrics.",
  "RQ2: Ablation Study": "To address RQ2, we conducted ablation experiments as follows. Weseparately removed the encoding tree of the entire graph, encodingtrees of subgraphs and the proposed RCM layer, and evaluated theperformance of the residual modules. We also substitute RGCN forthe encoder, and adopt different policies of graph augmentation.The results are presented in . Removing different modulesfrom SeBot resulted in performance degradation on TwiBot-20and MGTAB datasets, indicating the pivotal role in overall modeleffectiveness. This emphasizes the importance of co-consideringcommunity structure and heterophilic relations in social bot de-tection. By comparison, graph augmentation, feature masking, ordropout may disrupt the original feature distribution structure, acritical step in node classification, meanwhile, adding edges mayintroduce some additional structure noise, and thus lead to modelperformance degradation.",
  "RQ3: Sensitive Analysis": "To answer RQ3, we conducted experiments on TwiBot-20 (MGTABsee in Appendix A.1) to analyze the impact of fixed encoding treedepth and contrastive losses. We set to 6, 7, and 8, and variedhyperparameters 1 and 2 from 0.01 to 0.1 in steps of 0.01. Theresults of the experiments are presented in the form of heatmaps in. From this, we can intuitively observe that increasing thedepth of the tree improves the overall performance of the proposed",
  "Feature Mask84.930.6787.000.7090.001.2881.591.85Feature Dropping83.390.6185.530.9289.361.4880.581.27Edge Adding86.520.4287.960.5389.291.3980.602.95": "model. A greater depth enables a finer-grained community parti-tion, benefiting social bot detection, but it requires longer trainingtime. Further, as the tree depth increases, the impact of hyperpa-rameters 1 and 2 on model performance exhibits a fluctuatingpattern, initially decreasing and then increasing. Notably, when = 7, the models accuracy shows minimal variation across differ-ent hyperparameters. However, increasing or decreasing resultsin greater sensitivity of the accuracy to changes in the hyperpa-rameters. This corresponds to our assertion in .2 that, inpractical scenarios, a specific tree depth is generally preferred.",
  "RQ4: Visualization": "To address RQ4, we visually represent the 128-dimensional nodeembeddings generated by GCN, FAGCN, BotRGCN, RGT, GRACE,and SeBot on TwiBot-20 by projecting them onto a 2-dimensionalspace using T-SNE , as depicted in . The embeddingsfrom GCN and BotRGCN exhibit more scattering, whereas FAGCN,RGT, and our SeBot produce denser embeddings. GRACE embed-dings are quite uniform but suffer from the class collapse issue,where nodes from different classes are located closely to eachother due to the absence of label information. It is worth not-ing that, compared to RGT and FAGCN, our method exhibits local",
  ": Account representations visualization on TwiBot-20. Red represents bots, while blue represents humans": "clustering rather than smooth curves visible in 2-dimensional space.This implies that nodes of similar features or belonging to the samecommunity tend to aggregate together in clusters or beads. Thisalso indicates the ability of our method to capture the inherent com-munities present in the graph structure. Overall, the embeddingsgenerated by SeBot demonstrate relatively better class discrimina-tion compared to other methods, and the inclusion of reparameteri-zation techniques ensures that the representations avoid excessiveclustering.",
  "RQ5: Case Study": "To answer RQ5, we visualize a local community consisting of 3subcommunities and 7 social accounts, one of which is a bot ac-count. Due to the employment of the reparameterization technique,edge weights generated adaptively are equal to or near 1 or -1, andedges of two different colors are to represent them, respectively. Asshown in , social networks contain both the edges betweennodes of the same class and the edges between nodes of differentclasses. The edge weight adaptive mechanism proposed by us cansimultaneously model both types of edges by enabling positiveand negative edge weights. Furthermore, the fine-grained hierar-chical community structure is obtained by minimizing structuralentropy with a height constraint. Message passing on the encod-ing tree provides higher-order feature information favorable forclassification.",
  "league im doing?": ": A case study of local community structure and gen-erated edge attention. The same background color representsbelonging to the same sub-community, with the same parentnode on the constructed encoding tree. that leverage structural entropy minimization and a heterophily-aware encoder. SeBot employs self-supervised contrastive learningto unify and learn the intrinsic characteristics of nodes more ef-fectively. Comprehensive experiments show that SeBot exhibitssuperior generalizability and robustness on two real-world datasetscompared with all other baselines. This work was supported by the National Key Research and De-velopment Program of China through the grants 2022YFB3104700,2022YFB3105405, 2021YFC3300502, NSFC through grants 62322202and 61932002, Beijing Natural Science Foundation through grant4222030, Guangdong Basic and Applied Basic Research Founda-tion through grant 2023B1515120020, Shijiazhuang Science andTechnology Plan Project through grant 231130459A.",
  "Yuanmeng Yan, Rumei Li, Sirui Wang, Fuzheng Zhang, Wei Wu, and WeiranXu. 2021. ConSERT: A Contrastive Framework for Self-Supervised SentenceRepresentation Transfer. In ACL/IJCNLP (1)": "Yingguang Yang, Renyu Yang, Yangyang Li, Kai Cui, Zhiqin Yang, Yue Wang, JieXu, and Haiyong Xie. 2023. Rosgas: Adaptive social bot detection with reinforcedself-supervised gnn architecture search. TWEB (2023). Yingguang Yang, Renyu Yang, Hao Peng, Yangyang Li, Tong Li, Yong Liao, andPengyuan Zhou. 2023. FedACK: Federated Adversarial Contrastive KnowledgeDistillation for Cross-Lingual and Cross-Model Social Bot Detection. In WWW.13141323.",
  "(a) Sensitive analysis of hyperparameter 1 and 2 on TwiBot-20. (Tree depth 3-5)": "0.010.020.030.040.050.060.070.080.090.1 0.09 0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.010.1 90.0090.8890.4989.7190.2090.2990.8890.4989.5190.10 90.3989.7190.2090.6990.4990.3990.3990.8890.3990.00 90.4990.0090.3990.2990.2090.5990.1090.3990.8890.10 89.9090.3989.5190.4990.5989.6190.3990.2090.3990.49 90.5990.1090.2989.9090.6990.2090.8890.3990.6990.29 89.4190.0089.8090.4990.6990.5989.9090.2989.7190.59 90.6990.7889.7190.6989.9089.9090.2990.2990.8890.78 90.3990.2089.4189.9089.4189.7190.7889.8090.1090.49 89.7190.6990.1090.3989.7190.2990.4990.4990.2989.41 90.2090.4990.7890.2090.5989.5190.5990.0090.2090.59 Tree depth k=3 on MGTAB 0.010.020.030.040.050.060.070.080.090.1 0.09 0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.010.1 90.7890.8890.3990.2990.3990.7890.7890.6990.6990.39 89.8089.9089.7887.7891.2790.1089.7190.2990.7890.20 90.7891.1890.0090.1088.7390.1090.4990.3990.0090.69 90.2990.0090.2990.9890.2089.5191.0889.9090.2090.29 89.8090.4990.5988.4888.7890.4990.5990.2090.0090.49 90.3990.6990.6990.2090.2090.3991.2790.1089.7189.61 90.8890.2090.5991.0890.1090.2989.7190.5990.5991.18 90.7891.0890.3990.8890.8890.4990.3990.4990.7890.00 90.1089.2290.4990.5989.3189.7190.9889.8090.1090.20 90.5990.5990.3990.2990.1090.1089.6190.2090.3990.00 Tree depth k=4 on MGTAB 0.010.020.030.040.050.060.070.080.090.1 0.09 0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.010.1 90.5989.9090.2090.3990.6990.2990.6990.4990.6990.49 90.3990.5990.6990.2990.2990.6989.9090.2990.1090.10 89.7190.5989.7190.1089.9089.8090.0090.7890.4989.90 91.3790.2989.9090.1089.9089.3190.2089.7190.0090.00 90.2090.2990.4990.3990.3990.2090.2090.2990.5990.39 89.7189.9090.7890.4989.9090.5990.0089.4190.0089.61 90.4990.3989.8090.0090.6990.5989.9090.1090.1090.59 90.5989.9090.2990.4990.4989.9089.5190.2989.7190.00 90.8890.3989.7189.9089.6189.8090.7890.7890.4990.49 90.9890.3989.8089.8090.0090.1090.5990.2990.2089.90 Tree depth k=5 on MGTAB 88.0 88.5 89.0 89.5 90.0 90.5 91.0 91.5",
  "(b) Sensitive analysis of hyperparameter 1 and 2 on MGTAB. (Tree depth 3-5)": "0.010.020.030.040.050.060.070.080.090.1 0.09 0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.010.1 90.2990.4990.3990.5990.4990.2090.4990.3990.2090.39 90.1090.6990.5990.2090.0090.7890.3990.8890.5990.49 90.9890.2990.1090.3990.4990.2990.1090.2090.6990.39 89.9091.0890.2090.8890.8890.9890.3990.6990.6989.90 90.3990.3990.6990.3990.3990.9891.0890.3990.5990.39 89.6190.7889.2290.8891.3790.1090.6990.2990.1090.10 90.0090.3989.9090.4991.1890.8891.0890.5990.3990.10 90.8890.4990.5990.0090.6989.6190.1089.9090.2089.71 90.2090.0090.1090.1090.2089.3190.9890.3990.9890.59 90.2090.6990.2090.4989.9090.1090.6990.6990.4990.29 Tree depth k=6 on MGTAB 0.010.020.030.040.050.060.070.080.090.1 0.09 0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.010.1 89.7190.3990.2090.8890.9890.5990.3990.5990.1089.90 90.3990.6991.0890.7890.2090.3990.8890.8890.7890.59 90.1089.1291.0889.8089.8089.5190.2990.5990.5990.29 90.3990.5990.8889.7190.8890.8889.1291.2790.6989.31 91.1890.4990.2089.7191.5790.3990.5990.2991.1890.59 89.8090.5990.8889.9090.2990.1091.0889.0290.5989.90 90.6989.9089.9090.1090.3990.7890.4989.9090.5990.98 90.2090.9890.6990.5990.6990.5990.0090.2990.4989.31 90.1090.0090.5989.7190.2989.2291.0890.2989.8089.90 90.4990.4989.5190.2989.8090.2090.7890.4989.9090.10 Tree depth k=7 on MGTAB 0.010.020.030.040.050.060.070.080.090.1 0.09 0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.010.1 89.9090.2990.5990.0090.7890.0090.6990.5990.8890.29 90.2090.5990.7890.4990.1090.5990.2090.1091.1890.49 90.5990.0090.8890.2990.3990.2090.3990.7890.4990.29 90.2990.8890.0089.7190.4989.3189.8090.6991.2790.39 90.3989.6190.5990.4990.2990.2089.8090.7890.0090.10 90.2989.4190.7890.7890.4990.1090.2990.6990.2090.39 89.8090.0090.5990.1090.1090.5990.3990.1089.7189.31 90.1090.7890.2090.2090.2990.2990.3989.7190.4990.88 90.9890.2090.7889.9090.2990.2990.0090.0090.6989.90 89.7190.2990.3989.8090.2990.5990.0090.1090.2090.39 Tree depth k=8 on MGTAB 88.0 88.5 89.0 89.5 90.0 90.5 91.0 91.5",
  "AAPPENDIXA.1Sensitive Analysis Supplement": "In this subsection, we provide additional experimental results onthe impact of hyperparameters 1, 2 and tree depth on accuracy,as shown in . From a, when is set to 5, the trainedmodel is more sensitive to changes in hyperparameters 1 and 2,and the impact is larger. When =3, the overall performance isless affected by hyperparameters, and the differences in accuracyare smaller. We also conduct experiments on MGTAB in the sameway, which is shown in b and 6c. It is visually evident thatthe influence of hyperparameters is relatively small on MGTABcompared to TwiBot-20. Moreover, with the increase in tree depth,there isnt a significant overall improvement in performance. Evenwhen =3, satisfactory results can be achieved, indicating that the",
  "A.2Data Efficiency Study": "Current approaches to social bot detection predominantly followa supervised paradigm, heavily reliant on an adequately rich setof annotated training data. However, acquiring such a dataset is acostly endeavor, and issues such as inaccurate annotations, noise,and insufficient richness are widespread. Consequently, it becomesimperative to evaluate the performance of SeBot under conditionsof limited training data, relationships, and account features. Toaddress this, we specifically design experimental conditions bytraining solely on a subset of the data, randomly removing edges,and masking partial features. The results are illustrated in . Notably, even under the constraint of utilizing only 50% of the"
}