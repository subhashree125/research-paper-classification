{
  "Abstract": "The contextual bandit has been identified as a powerful frame-work to formulate the recommendation process as a sequentialdecision-making process, where each item is regarded as an armand the objective is to minimize the regret of rounds. In thispaper, we study a new problem, Clustering of Neural Bandits, byextending previous work to the arbitrary reward function, to strikea balance between user heterogeneity and user correlations in therecommender system. To solve this problem, we propose a novelalgorithm called M-CNB, which utilizes a meta-learner to representand rapidly adapt to dynamic clusters, along with an informativeUpper Confidence Bound (UCB)-based exploration strategy. Weprovide an instance-dependent performance guarantee for the pro-posed algorithm that withstands the adversarial context, and wefurther prove the guarantee is at least as good as state-of-the-art(SOTA) approaches under the same assumptions. In extensive exper-iments conducted in both recommendation and online classificationscenarios, M-CNB outperforms SOTA baselines. This shows theeffectiveness of the proposed approach in improving online recom-mendation and online classification performance.",
  "Introduction": "Recommender systems play an integral role in various online busi-nesses, including e-commerce platforms and online streaming ser-vices. They leverage user correlations to assist the perception of userpreferences, a field of study spanning several decades. In the past,considerable effort has been directed toward supervised-learning-based collaborative filtering methods within relatively static envi-ronments . However, the ideal recommender systems shouldadapt over time to consistently meet user interests. Consequently,it is natural to formulate the recommendation process as a sequen-tial decision-making process. In this paradigm, the recommenderengages with users, observes their online feedback (i.e., rewards),and optimizes the user experience for long-term benefits, ratherthan fitting a model on the collected static data based on super-vised learning . Based on this idea, this paper focuses onthe formulation of contextual bandits, where each item is treatedas an arm (context) in a recommendation round, and the primaryobjective is to minimize the cumulative regret over rounds andtackle the dilemma of exploitation and exploration in the sequentialdecision-making process .Linear contextual bandits model a users preference through alinear reward function based on arm contexts . However,given the substantial growth of users in recommender systems, itcan be overly ambitious to represent all user preferences with asingle reward function, and it may overlook the user correlations ifeach user is modeled as a single bandit. To address this challenge, aseries of methods known as clustering of linear bandits have emerged, which represent each cluster of users as a rewardfunction, achieving a balance between user heterogeneity and usercorrelations. Note that the cluster information is unknown in thisproblem setting. In essence, with each user being treated as a linearcontextual bandit, these methods adopt graph-based techniques todynamically cluster users, and leverage user correlations for makingarm recommendations. However, it is crucial to acknowledge thelimitations of this line of works: they all rely on linear rewardfunctions, and user clusters are represented as linear combinationsof individual bandit parameters. The assumptions of linearity inreward functions and the linear representation of clusters may nothold up well in real-world applications .",
  "KDD 24, August 2529, 2024, Barcelona, SpainYikun Ban, Yunzhe Qi, Tianxin Wei, Lihui Liu, and Jingrui He": "L. Li, W. Chu, J. Langford, and R. E. Schapire. A contextual-bandit approach topersonalized news article recommendation. In Proceedings of the 19th internationalconference on World wide web, pages 661670, 2010. S. Li, A. Karatzoglou, and C. Gentile. Collaborative filtering bandits. In Proceedingsof the 39th International ACM SIGIR conference on Research and Development inInformation Retrieval, pages 539548, 2016. S. Li, W. Chen, S. Li, and K.-S. Leung. Improved algorithm on online clusteringof bandits. In Proceedings of the 28th International Joint Conference on ArtificialIntelligence, pages 29232929. AAAI Press, 2019. Z. Lipton, X. Li, J. Gao, L. Li, F. Ahmed, and L. Deng. Bbq-networks: Efficientexploration in deep reinforcement learning for task-oriented dialogue systems.In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.",
  ") efforts to O(": "), where is the expectednumber of clusters. This also indicates the proposed algorithmcan leverage the collaborative effects among users. (3) Adversarialattack on contexts: In most neural bandit works, a common as-sumption is that the NTK matrix is non-singular, requiring thatno two observed contexts (items) are identical or parallel .This vulnerability makes their regret analysis susceptible to adver-sarial attacks and less practical in real-world scenarios. In face of this challenge, we provide an instance-dependent regret analysisthat withstands the context attack, and allows the contexts to berepeatedly observed. Furthermore, under the same assumptions asin existing works, we demonstrate that our regret upper bound is atleast as good as SOTA approaches. The above efforts to address thechallenges in the theoretical analysis is our third main contribution.Evaluations. We evaluate the proposed algorithm in two scenar-ios: Online recommendation and Online classification with banditfeedback. For the first scenario, which naturally lends itself to CNB,we assess the algorithms performance on four recommendationdatasets. Since online classification has been widely used to evaluateneural bandits , we evaluate the algorithms on eight clas-sification datasets where each class can be considered as a bandit(user), and correlations among classes are expected to be exploited.We compare the proposed algorithm with 8 strong baselines andshow the superior performance of the proposed algorithm. Addi-tionally, we offer the empirical analysis of the algorithms timecomplexity, and conduct extensive sensitivity studies to investi-gate the impact of critical hyperparameters. The above empiricalevaluation is our fourth main contribution.Next, detailed discussion regarding related works is placed in. After introducing the problem definition in , wepresent the proposed algorithm, M-CNB, in together withtheoretical analysis in . Then, we provide the experimentalresults in and conclude the paper in .",
  "Related Work": "In this section, we briefly review the related works, including clus-tering of bandits and neural bandits.Clustering of bandits. CLUB first studies collaborativeeffects among users in contextual bandits where each user hostsan unknown vector to represent the behavior based on the linearreward function. CLUB formulates user similarity on an evolvinggraph and selects an arm leveraging the clustered groups. Then,Gentile et al. , Li et al. propose to cluster users based onspecific contents and select arms leveraging the aggregated infor-mation of conditioned groups. Li et al. improves the clusteringprocedure by allowing groups to split and merge. Ban and He uses seed-based local clustering to find overlapping groups, differ-ent from global clustering on graphs. Korda et al. , Liu et al., Wang et al. , Wu et al. , Yang et al. also study clus-tering of bandits with various settings in recommender systems.However, all these works are based on the linear reward assumption,which may fail in many real-world applications.Neural bandits. Lipton et al. , Riquelme et al. adapt theThompson Sampling (TS) to the last layer of deep neural networksto select an action. However, these approaches do not provide re-gret analysis. Zhou et al. and Zhang et al. first provide theregret analysis of UCB-based and TS-based neural bandits, wherethey apply ridge regression on the space of gradients. Ban et al. studies a multi-facet bandit problem with a UCB-based explo-ration. Jia et al. perturbs the training samples for incorporatingboth exploitation and exploration. EE-Net proposes to useanother neural network for exploration with applications on activelearning and meta-learning . combines the last-layer neural network embedding with linear UCB to improve the",
  "Meta Clustering of Neural BanditsKDD 24, August 2529, 2024, Barcelona, Spain": "computation efficiency. Dutta et al. uses an off-the-shelf meta-learning approach to solve the contextual bandit problem in whichthe expected reward is formulated as Q-function. Santana et al. proposes a Hierarchical Reinforcement Learning framework forrecommendation in the dynamic experiments, where a meta-banditis used for the selected independent recommender system. Kassraieand Krause revisit Neural-UCB type algorithms and showsthe O( ) regret bound without the restrictive assumptions onthe context. Hong et al. , Maillard and Mannor study thelatent bandit problem where the reward distribution of arms areconditioned on some unknown discrete latent state and prove theO( ) regret bound for their algorithm as well. Federated bandits consider dealing with multiple bandits (agents) while preserv-ing the privacy of each bandit. Deb et al. reduce the contextualbandits to neural online regression for tighter regret upper bound.Qi et al. propose to use graph to formulate user correlationswith the adoption of graph neural networks. However, the aboveworks either focus on the different problem settings or overlookthe clustering of bandits.Other related works. study meta-learning in Thomp-son sampling and Hong et al. , Wan et al. aims to exploitthe hierarchical knowledge among hierarchical Bayesian bandits.However, they focus on the Bayesian or non-contextual bandits.",
  "Problem: Clustering of Neural Bandits": "In this section, we introduce the CNB problem, motivated by learn-ing correlations among bandits with arbitrary reward functions.Next, we will use the scenarios of personalized recommendation tostate the problem setting.Suppose there are users (bandits), = {1, . . . ,}, to serve ona platform. In the th round, the platform receives a user (unique ID for this user) and prepares the corresponding candi-date arms X = {x,1, x,2, . . . , x, }. Each arm is represented byits -dimensional feature vector x, R, [] = {1, . . . , },which will encode the information from both the user side andthe arm side . Then, the learner is expected to select an armx X and recommend it to , where refers to the target orserved user. In response to this action, will provide the platformwith a corresponding reward (feedback) . Here, since differentusers may generate different rewards towards the same arm, weuse , | to represent the reward produced by given x,. Theformal definition of arm reward is below.Given , the reward , for each candidate arm x, Xis assumed to be governed by an unknown function by",
  ", | = (x,) + ,,(1)": "where is an unknown reward function associated with , andit can be either linear or non-linear. , is a noise term with zeroexpectation E = 0. We also assume the reward , isbounded, as in many existing works . Note that previousworks on clustering of linear bandits all assume is a linearfunction with respect to arm x, .Meanwhile, users may exhibit clustering behavior. Inspired by, we consider the cluster behavior to be item-varying, i.e.,the users who have the same preference on a certain item may havedifferent opinions on another item. Therefore, we formulate a set of",
  "(2) N , s.t. N satisfies (1) and N (x,) N": "The condition (2) is to guarantee that no other clusters containsN (x,). This cluster definition allows users to agree on certainitems while disagree on others, which is consistent with the real-world scenario. Since the users from different clusters are expectedto have distinct behavior with respect to x,, we provide the fol-lowing constraint among relative clusters.",
  "N (x,), N(x,), |E[, |] E[, |]|": "For any two clusters in , we assume that they satisfy the -gap constraint. Note that such an assumption is standard in theliterature of online clustering of bandit to differentiate clusters. As a result, given an arm x,, the bandit pool can be divided into , non-overlapping clusters: N1(x,), N2(x,),. . . , N, (x,), where , . Note that the cluster informationis unknown in the platform.For the CNB problem, the goal of the learner is to minimize thepseudo regret of rounds:",
  "=1E[ | , X],(2)": "where is the reward received in round , and E[ |, X] =maxx, X (x,).Notations. Let x be the arm selected in round , and bethe corresponding reward received in round . We use x 2 torepresent the Euclidean norm. For each user , let be thenumber of rounds that user learner has been served up to round, and Tbe all of s historical data up to round . is the widthof neural network and is depth of neural network in the proposedapproach. Given a group N, all its data up to round can be denotedby {T }N = {T | N}. We use standard O and notationto hide constants.",
  "Proposed Algorithm": "In this section, we present our proposed algorithm, denoted asM-CNB, to address the formulated CNB problem. M-CNB lever-ages the potential correlations among bandits, and aims to rapidlyacquire a representation for dynamic relative clusters.For M-CNB, we utilize a meta-learner, denoted as , to rapidlyadapt to clusters, as well as represent the behavior of a cluster. Addi-tionally, there are user-learners, denoted by {} , responsiblefor learning the preference () for each user . In terms ofthe workflow, the primary role of the meta-learner is to determinerecommended arms, while the user-learners are primarily utilizedfor clustering purposes. The meta-learner and user-learners sharethe same neural network structure, denoted as . And the workflowof M-CNB is divided into three main components: User clustering,",
  ": Clustering and Meta Adaptation: Given and anarm x,, (1) M-CNB identifies cluster N (x,), and then (2)meta-learner 1 rapidly adapt to this cluster, proceedingto (3) the UCB exploration": "Meta adaptation, and UCB-based selection. Then, we proceed toelaborate their details.User clustering. Recall that in , each user is governed by an unknown function . In this case, we use aneural network (;), to estimate . In round [], let be the user to serve. Given s past data up to round 1, i.e.,T1, we can train parameters by minimizing the following loss:L ( ) = (x, )T1 ( (x; ) )2/2. Let 1 represent trained on T1 in round 1 by stochastic gradient descent (SGD).Therefore, for each , we can obtain the trained parameters1. Then, given and an arm x,, we return s estimatedcluster with respect to arm x, by",
  "| N (x, ) | N (x, ) . This can": "lead to limited representation power of the cluster learner, andtheir linear reward assumptions may not necessarily hold for realworld settings . Instead, we use the meta adaptation to updatethe meta-learner 1 according to N (x,), which can representnon-linear combinations of user-learners .UCB-based Exploration. To balance the trade-off betweenthe exploitation of the currently available information and theexploration of new matches, we introduce the following UCB-based",
  "where (x; ) incorporates the discriminative informationof meta-learner acquired from the correlations within the relativecluster N (x) and O(1) shows the shrinking confidence in-": "terval of user-learner to a specific user . Then, we select an armaccording to: x = argx, X max U, ( where U, is calculated inLine 9).In summary, Algorithm 1 depicts the workflow of M-CNB. Ineach round , given a target user and a pool of candidate arms, wecompute the meta-learner and its bound for each relative cluster(Line 6-10). Then, we choose the arm according to the UCB-typestrategy (Line 11). After receiving the reward, we update the user-learners. Note that the meta-learner has been updated in Line 8.Then, we discuss the time complexity of Algorithm 1. Here, with being the number of users, M-CNB will take O() to find thecluster for the served user. Given the detected cluster N, it takesO(| N|) to update the meta-learner by SGD. Suppose E[| N|] =/ and / . Therefore, the overall test time complexity ofAlgorithm 1 is O(( + /)). To scale M-CNB for deploymentin large recommender systems, we can rely on the assistance ofpre-processing tools: Pre-clustering of users and Pre-selection ofitems. On the one hand, we can perform pre-clustering of usersbased on the user features or other information. Then, let a pre-cluster (instead of a single user) hold a neural network, which willsignificantly reduce . On the other hand, we can conduct the pre-selection of items based on item and user features, to reduce substantially. For instance, we only consider the restaurants thatare near the serving user for the restaurant recommendation task.Furthermore, we can also control the magnitude of / by tuningthe hyperparameter based on the actual application scenario.Consequently, M-CNB can effectively serve as a core componentof large-scale recommender systems.",
  "Regret Analysis": "In this section, we provide the performance guarantee of M-CNB,which is built in the over-parameterized neural networks regime.As the standard setting in contextual bandits, all arms are nor-malized to the unit length. Given an arm x, R with x, 2 = 1, [], [], without loss of generality, we define as a fully-connected network with depth 2 and width :",
  ", = [vec(W1), vec(W2), . . . , vec(W)] R": "Note that our analysis results can also be readily generalized to otherneural architectures such as CNNs and ResNet . Then, weemploy the following initialization for and : For ,each entry of W is drawn from the normal distribution N (0, 2/);Each entry of W is drawn from the normal distribution N (0, 1/).Here, given > 0, we define the following function class:",
  "(0, ) = { R : 02 /1/4}.(5)": "The term (0, ) defines a function class ball centered at the ran-dom initialization point 0 and with a radius of . This defini-tion was originally introduced in the context of analyzing over-parameterized neural networks, and it can be found in the worksof and . Recall that , represents the number of clustersgiven x,. For the simplicity of analysis, we assume E = , [], []. Let {(x,)}=1 represent all the data in roundsand define the squared loss L () = ( (x;) )2/2. Then, weprovide the instance-dependent regret upper bound for M-CNBwith the following theorem.",
  "where =inf (0,)=1 L ()": "Theorem 5.1 provides a regret bound for M-CNB, which consistsof two main terms. The first term is instance-dependent and relatesto the squared error achieved by the function class (0, ) on thedata. The second term is a standard large-deviation error term.There are some noteworthy properties regarding Theorem 5.1.One important aspect is that it depends on the parameter , whichrepresents the expected number of clusters, rather than the numberof users . Specifically, O(",
  ") is an estimate of theregret effort for learning bandits. However, Theorem 5.1 refinesthis naive bound to O(": "), linking the regret effort to the actualunderlying clusters among users.Another advantage of Theorem 5.1 is that it makes no assump-tions about the contexts {x }=1 used in the problem. This makesTheorem 5.1 robust against adversarial attacks on the contexts andallows the observed contexts to contain repeated items. In contrast,existing neural bandit algorithms like rely on Assump-tion 5.1 for the contexts, and their regret upper bounds can bedisrupted by straightforward adversarial attacks, e.g., creating twoidentical contexts with different rewards.The term reflects the \"regression difficulty\" of fitting all thedata using a given function class, while the radius controls therichness or complexity of that function class. Its important to notethat the choice of is flexible, although its not without constraints:",
  "Assumption 5.1. There exists 0 > 0, such that H 0I": "The assumption 5.1 is generally held in the literature of neuralbandits to ensure the existence of a solutionfor NTK regression. This assumption holds true when any twocontexts in {x }=1 are not linearly dependent or parallel. Then,the SOTA regret upper bound for a single neural bandit ( = 1) is as follows:",
  "h = [1 (x1),1 (x2), . . . , (x)] R": "The purpose of the term is to provide an upper bound on theoptimal parameters in the context of NTK regression. However,its important to note that the value of becomes unbounded (i.e.,) when the matrix H becomes singular. This singularity can beinduced by an adversary who creates two identical or parallel con-texts, causing problems in their analysis.The second complexity term is the effective dimension , definedas = log det(I+H)",
  "Experiments": "In this section, we evaluate M-CNBs empirical performance onboth online recommendation and classification scenarios. Our sourcecode are anonymously available at datasets. We use four public datasets, Ama-zon , Facebook , Movielens , and Yelp 1, to evaluateM-CNBs ability in discovering and exploiting user clusters to im-prove the recommendation performance. Amazon is an E-commercerecommendation dataset consisting of 883636 review ratings. Face-book is a social recommendation dataset with 88234 links. Movie-Lens is a movie recommendation dataset consisting of 25 millionreviews between 1.6 105 users and 6 104 movies. Yelp is a shoprecommendation dataset released in the Yelp dataset challenge,composed of 4.7 million review entries made by 1.18 million userstowards 1.57 105 merchants. For these four datasets, we extractratings in the reviews and build the rating matrix by selecting thetop 10000 users and top 10000 items (friends, movies, shops) withthe most rating records. Then, we use the singular-value decompo-sition (SVD) to extract a normalized 10-dimensional feature vectorfor each user and item. The goal of this problem is to select the itemwith good ratings. Given an item and a specific user, we generatethe reward by using the users rating stars for this item. If the usersrating is more than 4 stars (5 stars total), its reward is 1; Otherwise,its reward is 0. Here, we use pre-clustering (K-means) to form theuser pool with 50 users (pre-clusters). Then, in each round, a user is randomly drawn from the user pool. For the arm pool, werandomly choose one restaurant (movie) rated by with reward1 and randomly pick the other 9 restaurants (movies) rated by with 0 reward. With each restaurant or movie corresponding to anarm, the goal for the learner is to pick the arm with the highestreward.Classification datasets. In our online classification with banditfeedback experiments, we utilized a range of well-known classifi-cation datasets, including Mnist , Notmnist , Cifar10 ,Emnist (Letter) , Fashion , as well as the Shuttle, Mush-room, and MagicTelescope (MT) datasets . Here, we providesome preliminaries for this setup. In the round [], givenan instance x R drawn from some distribution, we aim toclassify x among classes. x is first transformed into longvectors: x,1 = (x, 0, . . . , 0), x,2 = (0, x, . . . , 0), . . . , x, =(0, 0, . . . , x) R, matching classes respectively. The indexof the arm that the learner selects is the class predicted by thelearner. Then, the reward is defined as 1 if x belongs to this class;otherwise, the reward is 0. In other words, each arm represents aspecific class. For example, x,1 is only presented to Class 1; x,2",
  ": Regret comparison on recommendation datasets": "is only presented to Class 2. This problem has been studied in al-most all the neural bandit works . Compared to theseworks, we aim to learn the correlations among classes to improveperformance. Thus, we formulate one class as a user (bandit) (i.e.,a user in the recommendation scenario) and all the samples be-longing to this class are deemed as the data of this user. This setof experiments aims to evaluate M-CNBs ability to learn variousnon-linear reward functions, as well as the ability of discoveringand exploiting the correlations among classes. Additionally, weextended the evaluation by combining the Mnist and Notmnistdatasets to simulate a more challenging application scenario, giventhat both datasets involve 10-class classification problems.Baselines. We compare M-CNB with SOTA baselines as follows:(1) CLUB clusters users based on the connected componentsin the user graph and refines the groups incrementally; (2) COFIBA clusters on both the user and arm sides based on the evolvinggraph, and chooses arms using a UCB-based exploration strategy;(3) SCLUB improves the algorithm CLUB by allowing groups tomerge and split, to enhance the group representation; (4) LOCB uses the seed-based clustering and allows groups to be overlapped.Then, it chooses the best group candidates for arm selection; (5)NeuUCB-ONE uses one neural network to formulate all users,and selects arms via a UCB-based recommendation; (6) NeuUCB-IND uses one neural network to formulate one user separately(totally networks) and applies the same strategy to choose arms.(7) NeuA+U: we concatenate the arm features and user featurestogether and treat them as the input for the neural network. Notethat the user features are only available on Movielens and Yelpdatasets. Thus, we only report the results on these two datasets forNeuA+U. (8) NeuralLinear: following the existing work . Ashared neural network is built for all users to get an embedding foreach arm. which is fed into the linear bandit with the clustering pro-cedure. Since LinUCB and KernalUCB are outperformedby the above baselines, we will not include them for comparison.Configurations. We run all experiments on a server with theNVIDIA Tesla V100 SXM2 GPU. For all the baselines, they all havetwo parameters: that is to tune the regularization at initializationand which is to adjust the UCB value. To find their best perfor-mance, we conduct the grid search for and over (0.01, 0.1, 1)and (0.0001, 0.001, 0.01, 0.1) respectively. For LOCB, the number",
  ": Regret comparison on Mnist and Notmnist, Cifar10,EMNIST(Letter), and Shuttle": "of random seeds is set as 20 following their default setting. ForM-CNB, we set as 5 and as 0.4 to tune the cluster, and is setto 1. To ensure fair comparison, for all neural methods, we use thesame simple neural network with 2 fully-connected layers, and thewidth is set as 100. To save the running time, we train the neuralnetworks every 10 rounds in the first 1000 rounds and train the neu-ral networks every 100 rounds afterwards. In our implementation,we use Adam for SGD. In the end, we choose the best resultsfor the comparison and report the mean and standard deviation(shadows in figures) of 10 runs for all methods.Results. -4 reports the average regrets of all the methodson the recommendation and classification datasets. displaysthe regret curves for all the methods evaluated on the MovieLensand Yelp datasets. In these experiments, M-CNB consistently out-performs all the baseline methods, showcasing its effectiveness.Specifically, M-CNB improves performance by 5.8% on Amazon,7.7 % on Facebook, 8.1 % on MovieLens, and 2.0 % on Yelp, com-pared to the best-performing baseline. These superior results canbe attributed to two specific advantages that M-CNB offers overthe two types of baseline methods. In contrast to conventional lin-ear clustering of bandits (CLUB, COFIBA, SCLUB, LOCB), M-CNBhas the capability to learn non-linear reward functions. This flexi-bility allows M-CNB to excel in scenarios where user preferencesexhibit non-linearity in terms of arm contexts. In comparison to neu-ral bandits (NeuUCB-ONE, NeuUCB-IND, NeuA+U, NeuralLinear),M-CNB takes advantage of user clustering and leverages the corre-lations within these clusters, as captured by the meta-learner. Thisexploitation of inter-user correlations enables M-CNB to enhancerecommendation performance. By combining these advantages,M-CNB achieves substantial improvements over the MovieLensand Yelp datasets, demonstrating its prowess in addressing collab-orative neural bandit problems and enhancing recommendationsystems. Note M-CNBs regret rate decreases on these four datasets,even though the \"linear-like\" behavior in .Figures 3 and 4 show the regret comparison on ML datasets,where M-CNB outperforms all the baselines. Here, each class can bethought of as a user in these datasets. The ML datasets exhibit non-linear reward functions concerning the arms, making them challeng-ing for conventional clustering of linear bandits (CLUB, COFIBA,",
  ": Regret comparison on Mnist, Fashion-Mnist, Mush-room, and MagicTelescope": "SCLUB, LOCB). These methods may struggle to capture the non-linearity of the reward functions, resulting in sub-optimal perfor-mance. Among the neural baselines, NeuUCB-ONE benefits fromthe representation power of neural networks. However, it treats allusers (classes) as a single cluster, overlooking the variations andcorrelations among them. On the other hand, NeuUCB-IND dealswith users individually, neglecting the potential benefits of leverag-ing collaborative knowledge among users. NeuralLinear uses oneshared embedding (neural network) for all users, which may not bethe optimal solution given the user heterogeneity. M-CNBs advan-tage lies in its ability to exploit shared knowledge within clustersof classes that exhibit strong correlations. It leverages this commonknowledge to improve its performances across different tasks, as itcan efficiently adapt its meta-learner based on past clusters. Running Time(s)",
  ": Running time vs. Performance for all methods": "Running time analysis. demonstrates the trade-offbetween running time and cumulative regret on both the Movie-lens and Mnist datasets, where the unit of the x-axis is seconds. AsM-CNB is under the framework of neural bandits, we use NeuUCB-ONE as the baseline (1.0). The results indicate that M-CNB takescomparable computation costs (1.6 on Movielens and 2.9 onMnist) to NeuUCB-ONE while substantially improving performance.This suggests that M-CNB can be deployed to significantly enhanceperformance when the user correlation is a crucial factor (e.g.,recommendation tasks), with only a moderate increase in computa-tional overhead.Now, let us delve into the analysis of the running time for M-CNB. Specifically, we can break down the computational cost ofM-CNB into three main components: (1) Clustering: to form theuser cluster (Line 7 in Algorithm 1); (2) Meta adaptation: to train",
  "User-learner training0.0670.0680.0960.078": "provides the breakdown of the time cost for the threemain components of M-CNB. Clustering: This parts time costgrows linearly with the number of users because it has a timecomplexity of () for clustering. As discussed previously, lever-aging pre-clustering techniques can significantly reduce this cost.It is also important to note that all clustering methods inherentlyhave this time cost, and it is challenging to further reduce it. Metaadaptation: Due to the benefits of meta-learning, this part requiresonly a few steps of gradient descent to train a model with goodperformances. Consequently, the time cost for meta-adaptation isrelatively trivial. User-learner training: While this part may requiremore SGD steps to converge, it is important to recognize that it isprimarily used for clustering purposes. Therefore, the frequency oftraining user-learners can be reduced to decrease the cost. In sum-mary, M-CNB aims to achieve the clustering of neural bandits andcan manage to strike a good balance between the computationalcost and the model performance.",
  ": Sensitivity study for and on MovieLens Dataset": "Study for and. illustrates the performance variationof M-CNB concerning the parameters and . For the sake ofdiscussion, we will focus on but note that plays a similar role interms of controlling clustering. When is set to a value like 1.1, theexploration range of clusters becomes very narrow. In this case, theinferred cluster size in each round, | N (x,)|, tends to be small.This means that the inferred cluster N (x,) is more likely toconsist of true members of s relative cluster. However, there is adrawback regarding this narrow exploration range: it might resultin missing out on potential cluster members in the initial phases oflearning. On the other hand, setting to a larger value, like = 5,widens the exploration range of clusters. This means that thereare more opportunities to include a larger number of members inthe inferred cluster. However, continuously increasing does notnecessarily lead to improved performances, because excessivelylarge values of might result in inferred clusters that include non-collaborative users and clustering noise. Therefore, in practice, werecommend to set to a relatively large number (e.g., = 5) thatstrikes a balance between the exploration and exploitation.",
  ": Sensitivity study for on Mnist Dataset": "Study for . provides insight into the sensitivity ofM-CNB concerning the parameter in Algorithm 1. It is evidentthat M-CNB exhibits robust performance across a range of valuesfor . This robustness can be attributed to the strong discriminabil-ity of the meta-learner and the derived upper bound. Even withvarying values, the relative order of arms ranked by M-CNB ex-periences only slightly changes. This consistency in arm rankingsdemonstrates that M-CNB is capable of maintaining the robustperformance, which in turn reduces the need for extensive hyper-parameter tuning.",
  "Conclusion": "In this paper, we study the Cluster of Neural Bandits problem toincorporate correlation in bandits with generic reward assumptions.Then, we propose a novel algorithm, M-CNB, to solve this problem,where a meta-learner is assigned to represent and rapidly adapt todynamic clusters, along with an informative UCB-type explorationstrategy. Moreover, we provide the instance-dependent regret anal-ysis for M-CNB. In the end, to demonstrate the effectiveness ofM-CNB, we conduct extensive experiments to evaluate its empiri-cal performance against strong baselines on recommendation andclassification datasets.",
  "Acknowledgement": "This work is supported by National Science Foundation underAward No. IIS-2002540, and Agriculture and Food Research Initia-tive (AFRI) grant no. 2020-67021-32799/project accession no.1024178from the USDA National Institute of Food and Agriculture. Theviews and conclusions are those of the authors and should notbe interpreted as representing the official policies of the fundingagencies or the government.",
  "C. Riquelme, G. Tucker, and J. Snoek. Deep bayesian bandits showdown: Anempirical comparison of bayesian deep networks for thompson sampling. arXivpreprint arXiv:1802.09127, 2018": "M. R. Santana, L. C. Melo, F. H. Camargo, B. Brando, A. Soares, R. M. Oliveira,and S. Caetano. Contextual meta-bandit for recommender systems selection. InFourteenth ACM Conference on Recommender Systems, pages 444449, 2020. M. Simchowitz, C. Tosh, A. Krishnamurthy, D. J. Hsu, T. Lykouris, M. Dudik,and R. E. Schapire. Bayesian decision-making under misspecified priors withapplications to meta-learning. Advances in Neural Information Processing Systems,34:2638226394, 2021.",
  "AProof Details of Theorem 5.1": "Our proof technique is different from related works. are built on the classic linear bandit framework and utilizethe kernel-based analysis in the NTK regime. In contrast, we use the generalization bound of user-learner to bound the error incurred ineach round and bridge meta-learner with user-learner by bounding their distance, which leads to our final regret bound. Specifically, wedecompose the regret of rounds into three key terms (Eq. (27)), where the first term is the error induced by user learner , the secondterm is the distance between user learner and meta learner, and the third term is the error induced by the meta learner . Then, Lemma A.10provides an upper bound for the first term. Lemma A.10 is an extension of Lemma A.7, which is the key to removing the input dimension.Lemma A.7 has two terms with the complexity O( ), where the first term is the training error induced by a class of functions aroundinitialization, the second term is the deviation induced by concentration inequality for (;). Lemma A.13 bounds the distance betweenuser-learner and meta-learner. Lemma A.14 bounds the error induced by the meta learner using triangle inequality bridged by the userlearner. Bounding the three terms in Eq. (27) completes the proof.We first show the lemmas for the analysis of user-learner in Section A.1, the lemmas for meta-learner in Section A.2, the lemma to bridgebandit-learner and meta-learner in Section A.3, and the lemmas for the main workflow in Section A.4.",
  "where () is by simply discarding the last term and () is by = 2 and replacing with O(1/).The proof is completed": "Lemma A.7. For any (0, 1), > 0, and ,1,2 satisfy the conditions in Theorem 5.1. In a round where is serving user, let bethe selected arm and is the corresponding received reward. Then, with probability at least 1 over the randomness of initialization, thecumulative regret induced by up to round is upper bounded by:",
  "(x ; )": "where the expectation is taken over , conditioned on x, for each [], ,1 are intermediate user parameters introduced in Lemma A.9trained on Bayes-optimal pairs by Algorithm 1, e.g., (x1,1), and are meta parameters trained on the group N (x ) using Algorithm2. Then, the cumulative regret of rounds can be upper bounded by"
}