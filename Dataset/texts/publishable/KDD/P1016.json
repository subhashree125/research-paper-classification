{
  "ABSTRACT": "Due to the continuously improving capabilities of mobile edges,recommender systems start to deploy models on edges to allevi-ate network congestion caused by frequent mobile requests. Sev-eral studies have leveraged the proximity of edge-side to real-timedata, fine-tuning them to create edge-specific models. Despite theirsignificant progress, these methods require substantial on-edgecomputational resources and frequent network transfers to keepthe model up to date. The former may disrupt other processeson the edge to acquire computational resources, while the latterconsumes network bandwidth, leading to a decrease in user satis-faction. In response to these challenges, we propose a customizeDslImming framework for incompatiblE neTworks(DIET). DIET de-ploys the same generic backbone (potentially incompatible for aspecific edge) to all devices. To minimize frequent bandwidth usageand storage consumption in personalization, DIET tailors specificsubnets for each edge based on its past interactions, learning togenerate slimming subnets(diets) within incompatible networksfor efficient transfer. It also takes the inter-layer relationships intoaccount, empirically reducing inference time while obtaining moresuitable diets. We further explore the repeated modules within net-works and propose a more storage-efficient framework, DIETING,which utilizes a single layer of parameters to represent the entirenetwork, achieving comparably excellent performance. The exper-iments across four state-of-the-art datasets and two widely usedmodels demonstrate the superior accuracy in recommendation andefficiency in transmission and storage of our framework.",
  "The corresponding author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08",
  "INTRODUCTION": "With the advent of the era of big data, recommender systems havebecome indispensable in various aspects of our daily lives, encom-passing e-commerce, restaurants, and movies. It is well known that each user frequently interacts withrecommender systems, leading to increased latency in adverse net-work conditions or with large data volumes, due to the necessityof transmitting data back and forth to the cloud for processing.To address this issue, edge-cloud collaborative learningbegins to leverage the advancing capabilities of mobile edges to de-ploy models directly on edges. This approach allows user rerankingrequests to be processed locally, thereby reducing response latency.Furthermore, various edges have the opportunity to possess distinctmodels, allowing for differences between cloud and edge modelsand accommodating both interest and resource heterogeneity.Unfortunately, despite significant advancements in mobile edgecapabilities, conducting model training on edges for adaptationof on-edge data still requires a considerable amount of time, andlimited data on edges can easily lead to overfitting and performancedegradation. In response to this, existing research addresses thisissue without introducing much overhead for mobile edges throughtwo perspectives. One approach involves splitting the larger cloud-side model into two parts, deploying the part with a large amount ofcomputation on cloud and lightweight computation on edges.",
  "Advertise": "Same model for both devices and cloud abundant resources limited resources fast slow gradients click PromotionDailyWorld Cup : (a): There always exists data distribution shifts across cloud and edges, where cloud holds massive user historicalinteraction data and each edge only accesses their own data. (b): In each edge, users interest would change frequently due tosome other factors. This would cause cloud to send the latest models to each edge for adaptation, causing massive transmissiondelays. (c): In recommender systems, different scenarios need different models to provide services, which makes that userdevices are flooded with models. (d): Due to the difference in computing resources between cloud and edges, although themodel can quickly complete gradient updates and inference on cloud, it still takes a long time on edges. Another technique primarily focuses on providing each edge witha personalized model tailored to local data in . Most ofthem distribute gradient computation across edges,which are then gathered by cloud for aggregation. Despite theiradvancements, they may result in significant latency due to fine-tuning and transmission delays in order to keep in line with dy-namic interests. Other methods alleviate thisproblem by removing redundant parameters to minimize modelsize or expedite inference speed. Nevertheless, parameters discov-ered might be merely redundant and not compatible with someedges. Remaining incompatible parameters lead to poor results forheterogeneous edges, making them degrade a lot.In light of these considerations, we propose to investigate howto achieve higher performance on different edges under strict re-source constraints. Building on this objective, we aim to addressthree key issues in our investigation: (i) Transmission efficiency.As we discussed above, current methods would download modelsfrom cloud frequently to fit users frequently changing interests in(b). However, ongoing transmission might lead to severenetwork congestion and non-negligible transmission delay. Thus,how to design the representation and transmission of the modelis a matter worth considering. (ii) Storage consumption. On asuperplatform (e.g., Amazon and Taobao), each user has the op-portunity to access recommender systems with multiple channels,e.g., home page recommendation, recommendation after purchase,short video recommendation. Under these circumstances, as shownin (c), previous methods send substantial models to edges,resulting in significant storage pressure. Intuitively, models acrossdifferent channels are expected to share some similarities, whichmight help reduce storage occupancy on edges. (iii) Inference cost.The inference speed of edge-side models determines the waiting time after user makes a request. With faster inference speed, rec-ommender system can handle more requests within the same timeframe. It presents us with an additional obstacle in decreasing thetime required for inference on resource-constrained edges as shownin (d).",
  "Methods": "Towards this end, we propose a lightweight and efficient edge-cloud collaborative recommendation framework called DIET aim-ing at customizeD slImming for incompatiblE neTworks. DIETis dedicated to eliminating incompatible parameters within givennetworks between cloud and edges while addressing edge-side con-straints. Ideally, for distinct edges with various interests, DIET issupposed to assign customized models(including parameters andstructures) and minimize costs wherever possible to fit their localinterests. Specifically, given each users real-time sequence, DIETlearns to generate personalized diet at both the element level andthe filter level. A diet consists of a series of binary masks, each ofwhich represents whether the parameter at this location is compat-ible. In technique, we employ sequence extractor and layer-wisemask generators to extract information from user interactions and",
  "DIET: Customized Slimming for Incompatible Networks in Sequential RecommendationKDD 24, August 2529, 2024, Barcelona, Spain": "learn both element level and filter level importance in the frozennetwork. Element-level importance makes parameters transmittedbetween cloud and edges shift from a dense network to lightweightbinary masks, reducing the transmission overhead. Filter-level im-portance not only ensures the exactness of the consolidated learneddiets but also accelerates the inference speed empirically. In thissituation, for different scenarios, each edge only needs to store oneset of parameters, while the cloud can meet the diverse needs of theedge side by training different generators. Once user sends his real-time interactions, cloud will generate the corresponding diet andsend it back at once, which prevents user from fine-tuning locallyand the time for inference on cloud can be negligible. Motivated byrepeated blocks in widely-used recommenders, e.g. CNN andtransformer, DIETING is then proposed to use stacked smallblocks within the network to represent the entire network, makingit more storage-friendly for edges with limited storage. provides a brief comparison of DIET and other related methods inedge-cloud collaborative learning, demonstrating the superiorityof DIET in edge-cloud collaborative recommendation.We conduct experiments on four real-world datasets with twowidely used recommenders. Further ablation study and parameteranalyses consistently validate the generalizability and effectivenessof our approach in edge-cloud collaborative recommendation. Wesummarize the main contributions of this work as follows:",
  "RELATED WORK2.1Sequential Recommendation": "Owing to the potential to capture dynamic user interests, sequen-tial recommendation has garnered significant attention in bothacademia and industry. Initial sequential recommender systembased on Markov chains utilizes the probabilistic modeling ofuser-item transitions to predict and recommend subsequent itemsin a sequence. With the flourishing development of deep learning,a series of methods have been applied to recommender systemto better capture users long and short-term interests, includingRNN-based, transformer-based and CNN-based. Some researchers employed causal-related methods toeliminate bias in recommender systems. Additionally, someworks leveraged transferable knowledge from large lan-guage models to enhance the effectiveness of recommender system.",
  "Edge-cloud Collobrative Learning": "Recent methods of edge cloud collaborative learning can be catego-rized into model splitting and model personalization. The former divide the model into two parts, which are placed on edgesand cloud, thus preventing heavy end-side burden. EdgeRecdeploys the embedding matrix on cloud, while the remaining light-weight model is deployed on user edges. By applying multipleenvironmental constraints. Auto-Split automatically segmentseach model into cloud-side and edge-side parts, thus avoiding thedifficulty of manual segmentation. The latter commit to im-proving the personality of each edge and their local performance.Detective and focus on improving the generalization androbustness of cloud models by leveraging multiple distributions andmulti-domain data on devices. DCCL proposes an edge-cloudcollaborative framework, introducing a meta-patching mechanismthat not only alleviates the computational burden on the edge sidebut also ensures edge-side personalization. DUET and Intellec-tReq constructe a comprehensive and effective device-cloudcollaborative system, respectively addressing the issue of how andwhen models can be efficiently generalized.",
  "Network Compression": "Proper model pruning can help reduce model complexity and com-putational requirements, improving inference speed and reducingresource consumption. Based on the purpose, model pruning canbe categorized into two major types: structured pruning and un-structured pruning. Structured pruning mainly focuses onspeeding up inference through removing entire neurons, channels,or layers from the neural network, leading to a more structuredmodel. Although it improves the models inference speed, its usu-ally challenging to reduce the model size without significantly com-promising the final performance. Unstructured pruning aims toremove individual connections in the neural network, leading to amore irregular reduction of the model size. After LTH confirmedthat there exists a sparse network that can be trained to compara-ble accuracy in isolation, a variety of methods starts to prune themodels during training or even before training.",
  "Preliminary": "In the settings of edge-cloud collaborative learning in recommendersystem, suppose there is an item set = [1,2, ...,] and an edgeset = [1,2, ...,], where and are the number of items andedges, separately. Each edge has limited computation resources andcannot access data from other edges. In our paper we rigorously ac-count for model transmission delays and other constraints, aimingto alleviate the issues mentioned in . Apart from this, thereexists another cloud server with abundant computing resourcesto provide various services for each edge. The cloud server hasaccess to the historical interactions = [1,2, ..., ] of each",
  "Importance Generators": ": Overview of DIET. (a): Cloud will generate edge-specific diets(subnets) condition on real-time interactions for variousdata distribution on edges. (b): Another module on cloud to discover inter-layer filter relationships in networks, generatedfilter-level importance will post-process diets from (a) to correct less important connections. (c): A brief schematic diagram ofDIET and DIETING(the portion marked with orange dashed lines). The edges send their real-time samples and cloud customizesdiets for them. The transmission over the network consists of a series of binary masks, with all edges sharing the same network.(d): With specific diets from cloud, edges construct their own models, enabling fast adaptation. edge, where for and . Each edge alsohas its own real-time interactions = [1 ,2 , ..., ]. Our taskin edge-cloud collaborative learning is to improve the generaliz-ability of the cloud model and further make it adapt to changingenvironments and resource constraints on edge conditions on theirreal-time samples:",
  "(1)": "Here C represents the set of all edge constraints and denotes theparameters of cloud model .For a similar consideration, we adopt the local reranking settingsas in some mainstream works in recommender system.At the beginning of each session or only when user interest changesdramatically, cloud will send the newest models along with the can-didate embeddings to each edge. Those embeddings would be storedin local cache for subsequent reranking. Under these circumstances,cloud does not need to participate in edge reranking unless user in-terests change drastically. As candidate embeddings only occupy asmall amount of space, in this paper we mainly consider the storageand transmission of network parameters.",
  "Network bandwidth and storage resources for the edges continueto be in short supply despite the rapid development of technol-ogy. The former one makes it more expensive for the edges to": "acquire the latest model from cloud. The latter one may becomesome obstacles where each edge needs to keep multiple models forvarious scenarios, thereby adding burden to itself. This situation isespecially obvious in recommendation systems, where the usersinterests are changing frequently and each user may interact withrecommenders through various ways in some large e-commercesystem. Fortunately, previous work has proved the potentialof random-initialized networks, i.e. where there is a sub-networkthat can achieve comparable performance through parameter se-lection as training the entire network. Inspired by this, we proposeto discover whether this strategy can be effective in recommendersystem.Consider that the proposed model with transformers, eachwith parameters = [0 ,1 , ...,1], where [0, ) and is the number of linear layers in th transformer. For those trans-formers, the weights are frozen and the training process can beconverted to find an optim mask = [0 ,1 , ...,1] for eachlinear layer. Then the weight of each linear layer for inference willbe: = [0 0 ,1 1 , ...,1 1].(2)",
  "(0 ).(4)": "Since current networks are mainly composed of stacked blocks,we begin to conjecture whether the initialization values in thenetwork are important. In light of this, we propose another variantof DIET named DIETING, which is more lightweight than DIET.DIETING proposes that since the network consists of identicalblocks, it might be also feasible to initialize the parameters of eachlayer and use one of the layers to initialize the entire network. Thatis, for all and ,",
  "Edge-specific Diets to Enhance Personality": "Although sending binary masks between cloud and edges reducesthe latency required for transmission, fewer meaningful parameterson edges may reduce the models generalizability, as parameterssuitable for cloud may not necessarily be suitable for edges. Moti-vated by the widely used hypernetwork, which generatesthe corresponding parameters with learnable vectors, this capabil-ity can be integrated into our framework to produce personalizeddiets for each edge. Formally, the hypernetwork get an input ,then output the specific weight :",
  "= ().(6)": "Despite the personalized parameters it produces, the hypernet-work uses a random latent vector as its input, which needs to beretrained for each edge on their local data. However, for resource-constrained edges, finetuning the model on edges to get distinctive is not feasible. We thereby introduce an extra sequence extractorto learn to generate the latent vector from users recent behaviorsourselves. Each edge uploads its recent interactions and cloud canextract valuable information from them to generate personalizeddiets. Then Equation 6 becomes:",
  "where is the real-time interactions of each edge": "3.3.1Sequence Extractor. Given the length of the interaction as and its representation , with being the dimension ofthe item embedding, the sequence extractor aims to extract thoseunderlying information representing the data distributionof each edge. Though most of the DNN architectures can be usedas the sequence extractor, in our framework we use GRU as thesequence extractor which offers efficient training, reduced complex-ity, and comparable performance to LSTMs in capturing long-rangedependencies. In order to prevent mutual interference betweendifferent layers, each layer has its own independent extractor whencalculating its unique mask. 3.3.2HyperNetwork. In this section, we will outline how to utilizethe extracted sequence features and hypernetwork to generatediet for each layer. Take the linear layer as an example, the dimen-sion of it is , with and are the dimension of itsoutput and input, respectively. The hypernetwork can consist ofa fully connected layer or a complex MLP. Similarly, every layerhas its hypernetwork to ensure an adequate level of parameterpersonalization.The generated score will then replace the importance score toproduce the corresponding mask as in Equation 3:",
  "= [0 (0 (0 ())), ...,1 (1(1()))], (8)": "where1and 1are the hypernetwork and sequence extractorof the 1th linear layer in the th transfomer.Despite extra model parameters we include to generate masks,those modules wont be sent to each edge but rather kept in cloud.Hence it wont add overhead for edges. Once users upload theirreal-time samples to the cloud, the sequence extractors and hy-pernetworks of each layer will generates the personalized dietsaccording to the extracted information of real-time data samplesand send them to the edge. Multiple extractors and hypernetworkscan be executed in parallel during an inference process, thus incur-ring minimal waiting time overhead.",
  "Connections Correction for Each Diet": "Recall that when generating masks in the previous steps, eachelement in the linear layer is independently predicted and onlyelement-level dependencies are considered. Inspired by previouswork which delved into the inter-layer filter relationships andemphasized the retention of filters containing substantial infor-mation across adjacent layers, we propose to leverage inter-layerdependencies to capture more comprehensive information and en-hance the overall model performance.Similarly, we take the hypernetwork mentioned in .3 togenerate inter-layer importance. To simplify the architecture, theelement-level sequence extractor is repurposed for the filter level.In our view, those extracted common characteristics can be used forboth importance calculations. Formally, given the extracted features, another hypernetwork maps it to the row level importance , which will be expanded and multiply the generateddiets:",
  "where0 = (0 (0 ())),(10)": "the softmax function here is to prevent some outputs from beingnegative because the mask generate function will use the abso-lute value to determine whether to retain an element at a certainposition.Taking inter-layer dependencies into consideration not onlyprovides a better diet for edges but also empirically speeds up theinference on each edge. The results will be described in .2. Given the varying importance of each row/filter within a layer,elements in less important filters will receive lower scores. Thisprompts the importance of the elements within this filter to bepositioned towards the latter part of the entire layer, ultimately",
  "EXPERIMENTS": "In order to demonstrate the effectiveness and efficiency of ourmethod, we compare it with other outstanding methods on twomost widely used models and four real-world recommendationdatasets. We choose SASRec and Caser as the architectures they useare CNN and transformer respectively. All results are the averageof five experiments with five individual random seeds.",
  "Experimental Setup": "4.1.1Datasets. The experiments are conducted on four state-of-art benchmarks Movielens-1M1, Movielens-100K2, Amazon-CD andAmazon-TV 3. Detailed statistics of them are shown in . Tokeep data quality, we use 20-core setting for Movielens datasets and10-core settings for Amazon datasets according to different datasparsity of each dataset. We treat the interactions with positiveratings as positive samples. Following previous works, we sortthe user-item interactions in chronological order and treat the lastinteracted item of each user as test sample. 4.1.2Evaluation Metrics. For the edge-cloud collaborative learn-ing in recommendation, we are supposed to consider recommenderperformance, transmission latency, and inference speed simultane-ously. For the recommender performance, we use Hit and NDCG,two frequently used metrics in recommender system. For the trans-mission latency, we evaluate it with the number of bits needed totransfer when updating. For the inference speed, we use Floating-point operations (FLOPs) of a single iteration during inference.Detailed descriptions can be found in Appendix.",
  "Overall Performance": "The comparison of the recommendation performance of our methodand other baselines on four datasets is shown in . From thetable, it is evident that most of the pruning methods perform worsethan the basic recommender without any fine-tuning, indicatingthat the lottery ticket found by them does not have a guaranteeof comparable performance with base recommender. Surprisingly,those data-dependent pruning methods sometimes perform slightlyworse than random pruning. This could be attributed to the rapidchanges in user interests within recommender systems, leadingto a distribution gap between real-time data, rendering the sparsenetwork learned through training data ineffective.In a nutshell, our approach consistently outperforms the baserecommender and other baselines across two models, four datasets,and two evaluation metrics. Although it does not exhibit significantimprovement over Caser on Movielens-1M, it still yields the bestperformance among all the other baselines, which drops consider-ably compared to the original recommender. Moreover, DIET im-proves the base recommender by a large margin on various datasets.For example, on SASRec-based Movielens-100K, we achieve almost20% improvement over the base recommender and on Amazon-CDdataset, we improve the two models by more than 10% concerningNDCG and Hit through searching the specific diets.Simply learning from given networks is far from enough andit might cause inferior results. Those sparse selection-based meth-ods, like SuperMask and PEMN, become the two worst performingmethods. The heterogeneity of interests poses more challenges tothose methods, making them less likely to find the proper mask.In contrast, DIET confronts the heterogeneity of user behaviorsby generating corrected personalized diet for each edge given itsrecent interactions.We also present the number of parameters to be transferred andFLOPs for inference relative to the base recommender. Accordingto the result, we can find that all of the pruning methods reduce themodel size a lot, among which those sparse selection-based methods",
  "Improv0.31%0.06% 31.96 2.0910.42%7.32% 31.96 1.7614.84%15.09% 31.96 1.998.44%7.23% 31.96 1.20": "get more significant effects, reducing to 3% of the origin model size.Despite the substantial achievement in model compression, thesemethods do not improve the inference time, as the model still takesabout the same amount of time for inference. While Gater largelyreduces the inference time, it needs to transfer a large number ofparameters and perform poorly on some datasets. Although STTDachieve better results on Movielens-1M of Caser, it degrades a lotcompared to even the base model on other datasets. Moreover, dueto the additional computation required for the semi-tensor prod-ucts matrix multiplication, STTD requires more computationalresources for each inference. In comparison, DIET shows consis-tent improvement on both transmission and inference, especiallyachieves the best results over Caser on Amazon-CD. All the aboveresults demonstrate the capability and efficiency of DIET in edge-cloud collaborative learning.",
  "Ablation Study": "After having a full comparison between our method and others,we would like to learn more about its details and check whethereach part of this design has played its intended role. To achieve this,we incrementally incorporate various components into DIET andanalyze the impact of the resulting models individually. The ablatedmodels are presented below, and results are depicted in : Base are the two origin sequential recommenders SASRecand Caser. Both of them exhibit similar performance onMovielens-1M and Movielens-100K. However, we notice thaton Amazon-CD SASRec gains more NDCG and Hit improve-ments against Caser. This owns to the more parameters ofSASRec to capture high-order user interests. +mask neither use hypernetwork to generate personalizedmask for each user nor correct the mask. This means that wewill generate the same mask for all users. From the table wecan observe a clear performance drop of +mask over Caseron all three datasets even if it yields better performance overMovielens-100K and Amazon-CD. We attribute this to thecomplex and ever-changing user interests in recommendersystem as mentioned in .2. Not surprisingly, by em-ploying lightweight diets, the number of parameters requir-ing transfer is significantly reduced, resulting in a substantialdecrease in communication costs. +MG generate personalized masks based on the past fewinteractions but it does not consider filter-level importanceand treats each connection as independent. This model trans-mits the same number of parameters as +mask due to thesame sparse selection strategy. Whats more, we observe ahuge improvement on recommendation performance overalmost all datasets. This underscores the effectiveness ofthe personalized mask generation method based on user in-terests. Nevertheless, introducing hypernetwork does notchange the number of FLOPs during inference and it obtainssimilar results and poorer results to +mask and Base modelsindividually. DIET aims to rectify the generated mask by taking the filter-level importance into consideration, thereby creating interde-pendencies among elements. Compared to the +mask model,DIET superior results on all metrics. First, owning to thecorrected mask, the recommendation performance improves",
  "KDD 24, August 2529, 2024, Barcelona, SpainKairui Fu, Shengyu Zhang, Zheqi Lv, Jingyuan Chen, & Jiwei Li": "Yuval Alaluf, Omer Tov, Ron Mokady, Rinon Gal, and Amit Bermano. 2022.Hyperstyle: Stylegan inversion with hypernetworks for real image editing. InProceedings of the IEEE/CVF conference on computer Vision and pattern recognition.1851118521. Alia Asheralieva, Dusit Niyato, and Zehui Xiong. 2021. Auction-and-learningbased lagrange coded computing model for privacy-preserving, secure, and re-silient mobile edge computing. IEEE Transactions on Mobile Computing (2021).",
  "Influence of DIETING": "In this section, we aim to illustrate within our framework that theconnections learned during the training process take precedenceover the initial values. Through experiments conducted on theaforementioned datasets, the results are presented in . Sur-prisingly, DIETING, which initializes the model with just one layer,achieves comparable performance to DIET. This further confirmsthat the initial parameters of the model have minimal impact on thefinal results. Instead, the crucial factor lies in the customization ofdiets for each edge. The composition of elements within the neuralnetwork plays a pivotal role in determining the ultimate outcome.With DIETING, the parameters stored on edges can consist of onlyone layer, regardless of the scenario.",
  "In-depth Analysis": "4.5.1Detail performance analysis. To further elucidate the effec-tiveness of our method, we plot the NDCG and Hit when trainingin . It is evident that the personalized-based method outper-forms mask, indicating that personalization brings a huge perfor-mance improvement. The results also indicate that consistent dietsfor all users might lead to prediction oscillation, which is particu-larly severe on Caser. However, this figure also shows that simplyassigning personalized diets might degrade the performance, suchas Caser. In addition, our method with weight correction addressesthis problem with better results than base model. 4.5.2Analysis on the sparsity ratio. As the model on edges is frozen,the quantity of 0s and 1s in the mask sent from the cloud side de-termines the performance of the final model. Towards this end, wetune the sparsity of each model to gain insights into the potentialof these subnets in incompatible networks. Denote as the ratio ofthe number of elements equal to 0 in each layer to all elements. Therange of is [0.6, 0.7, 0.8, 0.9, 0.95], and we plot the correspondingchange of NDCG and Hit on Movielens-1M in . On the onehand, when the value of is small, then most of the mask is 1sand the mask learned is less effective. On the other hand, when approaches 1, only a small fraction of elements can be selected,",
  ": The variation on the test set during training": "leading to a loss of model information, thereby decreasing the per-formance. From the figure we can clearly observe the performanceof the two models on Movielens-1M first continues to increase as grows, reaching a certain point after which it starts to decrease,for reasons as discussed earlier. Moreover, the results presentedabove confirm that properly finding specific for each model issignificant to balance the inference speed and the performance asmore 0s lead to faster inference speed.",
  ": Sensitivity of each model towards": "4.5.3Analysis on the correction part. In .3, we have demon-strated that the weight correction reduces the number of floating-point operations and accelerates the inference speed of each edge.We deduce that this is due to the presence of more filters that consistentirely of zeros, which do not need to be involved in the inferenceprocess. As shown in figure 5, even with a small fraction of 1s inthe mask, there are still a few filters that do not need to participatein calculation, especially in SASRec. This situation changes a littlewithin Caser due to the small size of each filter in the convolutionallayer. It suggests that reweighting each filter in each layer reducesthe number of floating-point operations as assigning lower weightsto less important filters reduces the scores of these elements in thefinal ranking, prompting the elements in these filters to become 0s.These results empirically verified the effectiveness of connectionscorrection on both recommendation and rapid inference.",
  "DIET0.03440.04610.06690.0831": "the robustness of our score generator for those unseen user behav-ior sequences. Random 80% users are used for training and the restare for testing. We choose to use those Amazon datasets becauseof their large sparsity, this ensures that the sequence distributionof the training data and the testing data are inconsistent and helpsjustify whether models can learn accurate subnets from differentuser representations. From the table, we have the following find-ings: first, without personalized subnets for each edge, traininga consistent model for all edges mostly yields the worst perfor-mance, especially for Caser on both datasets, where both metricsdecreased by more than half. Second, those personalized models areconsistently superior to it, demonstrating the high generalizationcapability of the hypernetwork condition on each user sequence.",
  "Case Study": "We present a case study in to illustrate the concept ofcompatibility as described in our paper. To demonstrate this, weextract two interactions from the MovieLens-1M dataset. The actiondenoted as G(the green one) is utilized to create compatible masks, while the action denoted as Y(the yellow one) is used to generateincompatible masks. Our findings reveal distinct preferences: Gfavors genres such as Animation, Childrens, and Musical, whereasY leans towards Horror, Crime, and Mystery.",
  ": Case study for illustration of the compatible net-works": "Initially, we input G into SASRec, and observe that the top-3 rec-ommended items do not include the next-clicked movies, but ratherrelevant movies. However, when employing compatible masks de-rived from G using DIET, the results significantly improve, withthe next-clicked item and more relevant movies appearing in therecommended list. Conversely, when utilizing Y to generate masks,which exhibits disparate preferences compared to G, it recommendsfew relevant movies. Better performance compared to the trainedSASRec and inicompatible network highlights the superiority ofDIET, underscoring that identifying compatible parameters can notonly reduce resource costs but also yield superior results.",
  "CONCLUSION": "In this work, we investigate the problem of edge-cloud collabora-tive recommendation under communication costs and mobile edgeresource constraints. In response to potential constraints during col-laborative training, we have categorized them into three challengesand substantiated the necessity of addressing them. We proposean efficient framework named DIET, which takes both elementand filter/row level importance into consideration and searches themost suitable diets for each edge, addressing the aforementionedchallenges successfully. Based on it, we propose DIETING to uti-lize only one layer of parameters to represent the entire model butget comparable performance, thereby more storage-friendly. Fur-ther experiments on real-world datasets and widely used models,accompanied by insightful analyses, once again demonstrate theeffectiveness of our framework.",
  "ACKNOWLEDGEMENT": "This work was supported by the National Science and TechnologyMajor Project (2022ZD0119100), National Natural Science Foun-dation of China (62441605, 62376243, 62037001, U20A20387), Key Re-search and Development Program of Zhejiang Province(2024C03270),the StarryNight Science Fund of Zhejiang University Shanghai In-stitute for Advanced Study (SN-ZJU-SIAS-0010), Scientific ResearchFund of Zhejiang Provincial Education Department (Y202353679).",
  "Yue Bai, Huan Wang, Xu Ma, Yitian Zhang, Zhiqiang Tao, and Yun Fu. 2022.Parameter-Efficient Masking Networks. Advances in Neural Information Process-ing Systems 35 (2022), 1021710229": "Amin Banitalebi-Dehkordi, Naveen Vedula, Jian Pei, Fei Xia, Lanjun Wang, andYong Zhang. 2021. Auto-split: A general framework of collaborative edge-cloudAI. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &Data Mining. Zhourong Chen, Yang Li, Samy Bengio, and Si Si. 2019. You look twice: Gaternetfor dynamic filter selection in cnns. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition. 91729180. Zhengyu Chen, Teng Xiao, Kun Kuang, Zheqi Lv, Min Zhang, Jinluan Yang,Chengqiang Lu, Hongxia Yang, and Fei Wu. 2024. Learning to Reweight forGeneralizable Graph Neural Network. In Proceedings of the AAAI Conference onArtificial Intelligence, Vol. 38. 83208328. Zhengyu Chen, Ziqing Xu, and Donglin Wang. 2021. Deep transfer tensor decom-position with orthogonal constraint for recommender systems. In Proceedings ofthe AAAI Conference on Artificial Intelligence, Vol. 35. 40104018.",
  "Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014.Empirical evaluation of gated recurrent neural networks on sequence modeling.arXiv preprint arXiv:1412.3555 (2014)": "Chuntao Ding, Ao Zhou, Yunxin Liu, Rong N Chang, Ching-Hsien Hsu, andShangguang Wang. 2020. A cloud-edge collaboration framework for cognitiveservice. IEEE Transactions on Cloud Computing 10, 3 (2020), 14891499. Tan M Dinh, Anh Tuan Tran, Rang Nguyen, and Binh-Son Hua. 2022. Hyper-inverter: Improving stylegan inversion via hypernetwork. In Proceedings of theIEEE/CVF conference on computer vision and pattern recognition. 1138911398.",
  "Jonathan Frankle and Michael Carbin. 2018. The Lottery Ticket Hypothesis: Find-ing Sparse, Trainable Neural Networks. In International Conference on LearningRepresentations": "Kairui Fu, Qiaowei Miao, Shengyu Zhang, Kun Kuang, and Fei Wu. 2023. End-to-End Optimization of Quantization-Based Structure Learning and InterventionalNext-Item Recommendation. In CAAI International Conference on Artificial Intel-ligence. 415429. Madan Ravi Ganesh, Jason J Corso, and Salimeh Yasaei Sekeh. 2021. Mint: Deepnetwork compression via mutual information-based neuron trimming. In 202025th International Conference on Pattern Recognition (ICPR). IEEE, 82518258. Yu Gong, Ziwen Jiang, Yufei Feng, Binbin Hu, Kaiqi Zhao, Qingwen Liu, andWenwu Ou. 2020. EdgeRec: recommender system on edge in Mobile Taobao. InProceedings of the 29th ACM International Conference on Information & KnowledgeManagement. 24772484.",
  "Balzs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk.2015. Session-based recommendations with recurrent neural networks. arXivpreprint arXiv:1511.06939 (2015)": "Balzs Hidasi, Massimo Quadrana, Alexandros Karatzoglou, and DomonkosTikk. 2016. Parallel recurrent neural network architectures for feature-richsession-based recommendations. In Proceedings of the 10th ACM conference onrecommender systems. 241248. Yupeng Hou, Zhankui He, Julian McAuley, and Wayne Xin Zhao. 2023. Learningvector-quantized item representation for transferable sequential recommenders.In Proceedings of the ACM Web Conference 2023. 11621171.",
  "Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. 2018.Snip:Single-shot network pruning based on connection sensitivity. arXiv preprintarXiv:1810.02340 (2018)": "Haoxuan Li, Yanghao Xiao, Chunyuan Zheng, Peng Wu, and Peng Cui. 2023.Propensity matters: Measuring and enhancing balancing for recommendation.In International Conference on Machine Learning. PMLR, 2018220194. Haoxuan Li, Chunyuan Zheng, Wenjie Wang, Hao Wang, Fuli Feng, and Xiao-Hua Zhou. 2024. Debiased Recommendation with Noisy Feedback. In Proceedingsof the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. Jiacheng Li, Ming Wang, Jin Li, Jinmiao Fu, Xin Shen, Jingbo Shang, and JulianMcAuley. 2023. Text Is All You Need: Learning Language Representations forSequential Recommendation. arXiv preprint arXiv:2305.13731 (2023). Yujie Lin, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Dongxiao Yu, Jun Ma,Maarten de Rijke, and Xiuzhen Cheng. 2020. Meta matrix factorization forfederated rating predictions. In Proceedings of the 43rd International ACM SIGIRConference on Research and Development in Information Retrieval. 981990.",
  "Zhaohao Lin, Weike Pan, and Zhong Ming. 2021. FR-FMSS: Federated recom-mendation via fake marks and secret sharing. In Proceedings of the 15th ACMConference on Recommender Systems. 668673": "Peng Liu, Lemei Zhang, and Jon Atle Gulla. 2023. Pre-train, prompt and recom-mendation: A comprehensive survey of language modelling paradigm adaptationsin recommender systems. arXiv preprint arXiv:2302.03735 (2023). Shuchang Liu, Qingpeng Cai, Zhankui He, Bowen Sun, Julian McAuley, DongZheng, Peng Jiang, and Kun Gai. 2023. Generative flow network for listwise rec-ommendation. In Proceedings of the 29th ACM SIGKDD Conference on KnowledgeDiscovery and Data Mining. 15241534. Zechun Liu, Haoyuan Mu, Xiangyu Zhang, Zichao Guo, Xin Yang, Kwang-TingCheng, and Jian Sun. 2019. Metapruning: Meta learning for automatic neuralnetwork channel pruning. In Proceedings of the IEEE/CVF international conferenceon computer vision. 32963305. Zheqi Lv, Feng Wang, Shengyu Zhang, Wenqiao Zhang, Kun Kuang, and FeiWu. 2023. Parameters Efficient Fine-Tuning for Long-Tailed Sequential Recom-mendation. In CAAI International Conference on Artificial Intelligence. Springer,442459.",
  "Zheqi Lv, Wenqiao Zhang, Zhengyu Chen, Shengyu Zhang, and Kun Kuang. 2024.Intelligent model update strategy for sequential recommendation. In Proceedingsof the ACM on Web Conference 2024. 31173128": "Zheqi Lv, Wenqiao Zhang, Shengyu Zhang, Kun Kuang, Feng Wang, YongweiWang, Zhengyu Chen, Tao Shen, Hongxia Yang, Beng Chin Ooi, et al. 2023. DUET:A Tuning-Free Device-Cloud Collaborative Parameters Generation Frameworkfor Efficient Device Model Generalization. In Proceedings of the ACM Web Confer-ence 2023. 30773085. Jed Mills, Jia Hu, and Geyong Min. 2021. Multi-task federated learning forpersonalised deep neural networks in edge computing. IEEE Transactions onParallel and Distributed Systems 33, 3 (2021), 630641. Xufeng Qian, Yue Xu, Fuyu Lv, Shengyu Zhang, Ziwen Jiang, Qingwen Liu, XiaoyiZeng, Tat-Seng Chua, and Fei Wu. 2022. Intelligent request strategy design inrecommender system. In Proceedings of the 28th ACM SIGKDD Conference onKnowledge Discovery and Data Mining. 37723782. Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2010. Factor-izing personalized markov chains for next-basket recommendation. In Proceedingsof the 19th international conference on World wide web. 811820. Kartik Sreenivasan, Jy-yong Sohn, Liu Yang, Matthew Grinde, Alliot Nagle,Hongyi Wang, Eric Xing, Kangwook Lee, and Dimitris Papailiopoulos. 2022. Raregems: Finding lottery tickets at initialization. Advances in Neural InformationProcessing Systems 35 (2022), 1452914540.",
  "Suraj Srinivas, Akshayvarun Subramanya, and R Venkatesh Babu. 2017. Trainingsparse neural networks. In Proceedings of the IEEE conference on computer visionand pattern recognition workshops. 138145": "Jiaxi Tang and Ke Wang. 2018. Personalized top-n sequential recommenda-tion via convolutional sequence embedding. In Proceedings of the eleventh ACMinternational conference on web search and data mining. 565573. Shisong Tang, Qing Li, Xiaoteng Ma, Ci Gao, Dingmin Wang, Yong Jiang, QianMa, Aoyang Zhang, and Hechang Chen. 2022. Knowledge-based temporal fusionnetwork for interpretable online video popularity prediction. In Proceedings ofthe ACM Web Conference 2022. 28792887. Shisong Tang, Qing Li, Dingmin Wang, Ci Gao, Wentao Xiao, Dan Zhao, YongJiang, Qian Ma, and Aoyang Zhang. 2023. Counterfactual Video Recommendationfor Duration Debiasing. In Proceedings of the 29th ACM SIGKDD Conference onKnowledge Discovery and Data Mining. 48944903. Xiaolei Wang, Kun Zhou, Xinyu Tang, Wayne Xin Zhao, Fan Pan, Zhao Cao, andJi-Rong Wen. 2023. Improving Conversational Recommendation Systems viaCounterfactual Data Simulation. arXiv preprint arXiv:2306.02842 (2023). Xin Xia, Hongzhi Yin, Junliang Yu, Qinyong Wang, Guandong Xu, and QuocViet Hung Nguyen. 2022.On-device next-item recommendation with self-supervised knowledge distillation. In Proceedings of the 45th International ACMSIGIR Conference on Research and Development in Information Retrieval. 546555. Yuhao Yang, Chao Huang, Lianghao Xia, Yuxuan Liang, Yanwei Yu, and Chen-liang Li. 2022. Multi-behavior hypergraph-enhanced transformer for sequentialrecommendation. In Proceedings of the 28th ACM SIGKDD conference on knowledgediscovery and data mining. 22632274. Jiangchao Yao, Feng Wang, Kunyang Jia, Bo Han, Jingren Zhou, and Hongxia Yang.2021. Device-cloud collaborative learning for recommendation. In Proceedingsof the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining.",
  "Wenqiao Zhang and Zheqi Lv. 2024. Revisiting the Domain Shift and SampleUncertainty in Multi-source Active Domain Transfer. In CVPR. IEEE": "Yipeng Zhang, Xin Wang, Hong Chen, and Wenwu Zhu. 2023. Adaptive disen-tangled transformer for sequential recommendation. In Proceedings of the 29thACM SIGKDD Conference on Knowledge Discovery and Data Mining. 34343445. Zeyu Zhang, Heyang Gao, Hao Yang, and Xu Chen. 2023. Hierarchical InvariantLearning for Domain Generalization Recommendation. In Proceedings of the 29thACM SIGKDD Conference on Knowledge Discovery and Data Mining. 34703479. Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, YanghuiYan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-throughrate prediction. In Proceedings of the 24th ACM SIGKDD international conferenceon knowledge discovery & data mining. 10591068.",
  "AEVALUATION METRICS": "For the edge-cloud collaborative learning in recommendation, weare supposed to consider recommender performance, transmissionlatency and inference speed simultaneously. For the recommenderperformance, we use Hit and NDCG, two frequently used metrics inrecommender system . The first metric assesses whether the targetitems appear in the recommenders provided recommendation list.This is equivalent to recall when the number of target items is 1,as in our experiments. The second metric is used to measure thequality of recommendation lists and the accuracy of their ordering,it combines the relevance of recommended items with the impactof their ranking. Detailed calculation process is as follows:",
  "denominator @ = =min(,)=01": "2(+1) . and in the above equations are the customized length of itemson top of the recommender list and the length of the userinteraction list for evaluation (in our experiment the lastinteraction items). in the first equation denotes whetheritem in the recommender list is in the user interaction listfor evaluation.",
  "For the transmission latency, we evaluate it with the number ofbits needed to transfer when updating. For the base models like": "SASRec and Caser, the transmission cost is calculated as 32 ,where is the number of parameters in the models. Aligned withthe popular pruning settings, compressed sparse column (CSC) orcompressed sparse row (CSR) formats are used to store the pa-rameters for most baselines. They are storage formats for sparsematrices, efficiently designed for column-oriented operations bystoring values, column/row indices, and offsets. Transmission costof them is calculated as 32 2 . in the equation is theproportion of non-zero elements in the mask of each layer. ForDIET, DIETING, and those sparse-selection methods, we use binaryformat to store the masks, which is . For the inference speed, weuse Floating-point operations(FLOPs) of a single iteration duringinference.",
  "BIMPLEMENTATION DETAILS": "Due to the potential inconsistency in performance of different pre-trained models when searching for recipes, we therefore use randominitialized parameters in our experiments to get rid of the unfair-ness brought by different pre-trained model. For all the modelsmentioned above, we use Adam as the optimizer with a triangularlearning rate scheduler. The learning rate is 0.001 for Movielens-100K and 0.1 for others. The size item embedding we use in ourexperiment is 64. We use Xavier normal initialization for all themodel parameters. The number of horizontal and vertical convo-lution filters are 4 and 16 in Caser, respectively. For the activationfunctions and , we simply use tanh in our experiments. ForSASRec we use two self-attention blocks and four heads in eachblock. The dropout rate is set to zero and the max length of eachinput interaction is 5. The hyper-parameter across datasets andmodels is significantly different. In Caser is set to 0.1, 0.1, 0.2,0.1 on Movielens-1M, Movielens-100K, Amazon-CD and Amazon-Movies, while in SASRec is set to 0.3, 0.1, 0.1, 0.1 respectively.As none of SuperMask and Gater have a precise way to controlthe model sparsity, we try to modify the initialized value and thecoefficient of the penalty term as much as possible.",
  "CGENERALIZABILITY OF DIETING": "In order to further investigate the feasibility of using one layer to ob-tain all the models, we test it on those synthetic out-of-distributiondatasets, and results are shown in . From the table, we canobserve that with personalized mask generator and inter-layercorrections, storing one layer on edges still keeps reliable general-izability."
}