{
  "Abstract": "Qualitative data collection and analysis approaches, such as thoseemploying interviews and focus groups, provide rich insights intocustomer attitudes, sentiment, and behavior. However, manually an-alyzing qualitative data requires extensive time and effort to identifyrelevant topics and thematic insights. This study proposes a novelapproach to address this challenge by leveraging Retrieval Aug-mented Generation (RAG) based Large Language Models (LLMs)for analyzing interview transcripts. The novelty of this work liesin strategizing the research inquiry as one that is augmented byan LLM that serves as a novice research assistant. This researchexplores the mental model of LLMs to serve as novice qualitativeresearch assistants for researchers in the talent management space.A RAG-based LLM approach is extended to enable topic modeling ofsemi-structured interview data, showcasing the versatility of thesemodels beyond their traditional use in information retrieval andsearch. Our findings demonstrate that the LLM-augmented RAGapproach can successfully extract topics of interest, with significantcoverage compared to manually generated topics from the samedataset. This establishes the viability of employing LLMs as novicequalitative research assistants. Additionally, the study recommendsthat researchers leveraging such models lean heavily on qualitycriteria used in traditional qualitative research to ensure rigor andtrustworthiness of their approach. Finally, the paper presents keyrecommendations for industry practitioners seeking to reconcilethe use of LLMs with established qualitative research paradigms,providing a roadmap for the effective integration of these powerful,albeit novice, AI tools in the analysis of qualitative datasets withintalent management research.",
  "Introduction": "Talent management researchers frequently work backwards fromtheir customers, the employees at the organization. Understandingemployee sentiment and behavior often involves conducting deep-dive interviews, explanatory in nature e.g., demystifying the whybehind customer choices, attitudes or behaviors (e.g., ). Talentmanagement research, at its core, seeks to use science to equip every employee with resources to help them best navigate theircareers .Consequently, qualitative research methodology plays a criti-cal role in talent management. Many of the key considerationsaround employee engagement, motivation, and workforce cultureinvolve subjective, context-dependent factors that are best exploredthrough in-depth interviews, focus groups, and other qualitativedata collection approaches. Talent management professionals oftenrely on rich qualitative datasets to gain deep insights into employeeexperiences, organizational dynamics, and the nuances of humancapital. However, these qualitative paradigms can clash with themore positivist, quantitative worldview that underlies many of theanalytic tools used to evaluate talent management data. Talent man-agement researchers may find that standard statistical techniquesand data visualization approaches struggle to fully capture thecomplexities inherent in qualitative datasets, leading to potentialmisinterpretations or oversimplifications of the human elementsinvolved in managing an organizations workforce. Navigating thistension between qualitative and quantitative approaches is an on-going challenge for talent management professionals.Large language models (LLMs) like BERT, GPT-3 and PaLM havedemonstrated strong aptitude for summarization (e.g., ), classi-fication (e.g., ), and information extraction (e.g., ) for text-based data. Consequently, LLMs are also increasingly being lever-aged within talent management contexts for tasks such as interviewanalysis. However, language models are themselves designed pri-marily from a quantitative, data-driven paradigm. These models aretrained on vast troves of text data using statistical machine learn-ing techniques optimized for numerical patterns and correlations.While powerful at extracting insights from large-scale datasets,LLMs can often struggle to fully capture the nuanced, contextualnature of language , that is critical for qualitative informa-tion sourced from interviews, focus groups, and other qualitativeresearch methods common in talent management.Talent management professionals must therefore continuouslynavigate a tension between the quantitative orientation of theiranalytical tools and the qualitative richness of the human dynamicsthey seek to understand. Bridging this gap requires innovative ap-proaches that combine the opportunity for scale and speed offeredby LLM-powered analysis augmented by borrowing evaluative nu-ances of traditional qualitative techniques. Talent leaders, thus,",
  "Accepted to Talent Management and Computing (TMC) Workshop at KDD 24 at Barcelona, Spain": "must carefully select and configure their AI-powered tools to en-sure the voices and experiences of employees are authenticallyrepresented, rather than reduced to oversimplified metrics. Master-ing this balance is an ongoing challenge, but one that is critical fortalent management to yield truly holistic and impactful insights.This paper presents results from leveraging LLMs as a novicequalitative researcher to augment qualitative research workstreams,specifically for data generated through semi-structured interviews.The purpose of this paper is two-fold 1) provide an overview ofa successful implementation of a Retrieval Augmented Generation-based model for analyzing semi-structured interviews, and moreimportantly, 2) enumerate pragmatic take-aways and learningsdrawing from traditional qualitative research to help fellow in-dustry practitioners in reconciling the methodological paradigms.We posit the second purpose to be valuable to the larger discus-sion within talent management research communities on how andwhere to integrate AI capabilities across different talent manage-ment workstreams.",
  "Quantitative and Qualitative Paradigms": "Quantitative and qualitative research represent two fundamentalparadigms or philosophical frameworks that guide research strate-gies, methods, analysis, and use of results . While both method-ological approaches seek to rigorously study research problems,they are based on distinct assumptions and procedures adapted toinvestigating particular types of questions and drawing differentconclusions. Quantitative research is based on the assumptions ofpositivism, the philosophical tradition premised on the applicationof natural science methods to the study of social reality and beyond. Quantitative researchers believe that objective facts and truthsabout human behavior and society can be measured and quantifiednumerically. Quantitative methods such as surveys, structured ob-servations, and experiments aim to test hypotheses derived fromtheories by examining relationships between precisely measuredvariables statistically analyzed using large sample sizes . Thesemethods seek to minimize subjectivity and generalize findings toa population. In contrast, qualitative research aligns with inter-pretivist and constructivist philosophical traditions by embracingsubjectivity and focused meaning-making by and with researchparticipants .Qualitative researchers often use an inductive approach aimed atdiscovering and understanding processes, experiences, and world-views by collecting non-numerical data through methods like in-depth interviews, ethnographic fieldwork, and document analysis.Findings derive from themes that emerge openly from the datarather than testing predetermined hypotheses. Samples tend to besmall and purposely selected to illuminate a phenomenon in depthand detail. The aim is particularization rather than generalization,with a priority on ecological validity and multiple realities situatedin time, place, culture, and context.While debates once positioned these paradigms in opposition,contemporary mixed methods research leverages the complemen-tary strengths of quantitative and qualitative approaches . Mixedmethods investigations integrate quantitative and qualitative datacollection and analysis within a single program of inquiry by com-bining these approaches in creative ways to deepen understanding . This reconciliation of methodological perspectives of-fers opportunities to generate more robust, contextualized insightsto address complex research problems. The use of large languagemodels (LLMs) as novice qualitative research assistants, as exploredin this paper, can be considered an exercise in mixed methodsresearch design.Prior to LLMs, in previous work, Natural Language Processingbased modeling of qualitative data from social science contexts,have also been used as \"novice insight\" augmented by the more ex-pert contextualization provided by human researchers (e.g., , ).Popular traditional topic modeling techniques (e.g. Latent DirichletAllocation), however, suffer from several limitations (e.g. specifyingnumber of clusters) when compared to existing deep learning-basedmethods. They also often fail to capture the contextual nuancesand ambiguities inherent in natural language, as they rely heav-ily on predefined rules and patterns . This can make itchallenging to handle the complexities and variations present inreal-world text data, and may require domain-specific knowledgeor fine-tuning to achieve acceptable performance . Recent ad-vancements in LLMs, such as BERT and GPT, have largely overcomethese limitations by leveraging deep neural networks to learn rich,contextual representations from large amounts of text data . These powerful models can capture subtle semantic and prag-matic features of language, and demonstrate strong generalizationcapabilities through transfer learning .Further, in traditional qualitative research, thematic analysis isthe process of gathering themes across topics from qualitative data,such as interview data, through iteratively analyzing the datasetfor topics of interest . Inductive coding and deductive coding aretwo approaches to analyzing data from semi-structured interviews.Inductive coding involves starting with raw data and graduallydeveloping codes and categories based on patterns and topics thatemerge from the data as the researcher manually interacts with it . This approach is bottom-up, where the data drives thedevelopment of codes and theories . Deductive coding, on theother hand, involves starting with preconceived codes or theoriesand applying them to the data . This approach is top-down,where existing theories or frameworks guide the coding process. Researchers in industry typically work backwards from re-search question of interest. Most of the research questions in in-dustry driving qualitative data collection are also explanatory (i.e.,tend to explain the quantitative findings such as low customer sat-isfaction, low product adoption numbers), rather than exploratory(i.e., ethnography of a community of interest or a phenomenon)and as a result deductive approaches are often more popular thaninductive coding.Ultimately, by augmenting traditional deep-dive qualitative anal-ysis with the time and resource efficient pattern recognition andtext processing capabilities of LLMs, researchers can integrate quan-titative and qualitative techniques to enhance the speed, depth, andrigor of their investigations. This mental model of a novice-LLMapproach holds promise for bridging the divide between positivistand interpretive paradigms, ultimately working towards a morecomprehensive understanding of the phenomenon under study.",
  "Dataset": "We used an open-source dataset to demonstrate how an LLMprompted as a novice researcher can enhance traditional qualitativedeductive thematic coding. This dataset was originally collectedto explore educators experiences implementing open educationalpractices . The dataset contains eight transcripts each fromhour-long interviews conducted with educators to understand howthey are using openly accessible sources of knowledge and open-source tools. The original research involved a deep-dive qualitativeanalysis through using a phenomenological approach to extracttopics manually from the dataset. We chose this open-source datasetfor two reasons 1) structural match to proprietary dataset, and2) rich description and manually identified topics by an expertto serve as a gold standard to measure the efficacy of our LLMbased approach. Semi-structured interviews provide critical insightsthrough participant perspectives, making them foundational invarious industry settings. The semi-structured approach used to create this dataset is aclose match to proprietary talent management data from our or-ganization, where employees are interviewed on a particular phe-nomenon to get deeper understanding of their related sentiment,attitudes, and behaviors. Manually extracted topics serve as goldstandard for benchmarking findings from our LLM-based approach.The paper describing the dataset explains the manual pro-cess establishing how each transcript was read twice: first, for acomprehensive analysis, and subsequently, to initiate a thematicexploration. Additional reviewing continued as codes and topicsemerged and intersected among the interviews. A manual qualita-tive coding approach was applied at each iteration to reveal themes,following constant comparison methodology .We posit that our approach, as demonstrated on this samplesemi-structured interview dataset, can easily extend to multipleindustry settings in talent management research where researchersconduct interviews and focus groups.",
  "Thematic Analysis Using LLMs": "In traditional, manual qualitative research, deductive thematic anal-ysis process begins with the researcher first formulating the re-search questions. Then, upon collection of the data, such as in-terview transcripts, the researcher iterates manually through thetranscripts to identify and extract themes or topics of interest. Thislabor-intensive process involves carefully reading through the data,taking notes, and organizing the topics iteratively into broader co-herent themes that address the research questions. The researchermay go through multiple rounds of coding and analysis to refinethe themes and ensure they comprehensively capture the key in-sights from the data. Our approach finds that LLMs can quicklyuncover topics of interest from the dataset which can then be it-erated upon to garner broader themes of interest across topics.Thus, for our novice-LLM led approach, we leveraged the powerof Large Language Models (LLMs) as a novice research assistant inthe thematic analysis process. Specifically, we used the open-sourceframework called Langchain to create dynamic prompt templates,such as few-shot prompts and chain of thoughts, that guided theLLM in performing topic modeling and generating insights from theinterview transcripts. We then opted to use Anthropics Claude2model to execute these prompts and extract the relevant themes.To initiate the analysis, we first selected a main research questionand corresponding sub-questions from our dataset . We thenfed these research questions, along with the interview transcripts,into the LLM-powered Langchain framework. The model was ableto quickly identify and summarize the key topics, and iteratively,themes emerging from the data. This approach provided a quick yetrelatively comprehensive analysis that would have taken a humanresearcher significant time and effort to reproduce manually.",
  "Thematic analysis enhanced throughRetrieval Augmented Generation (RAG)": "In our LLM based approaches, we experiment with four meth-ods - zero-shot prompting, few-shot prompting, chain-of-thoughtreasoning, and Retrieval Augmented Generation based QuestionAnswering. In zero-shot prompting we provide a single promptto the model. In few-shot prompting, we provide a set of topics and anecdotes to the model as examples. In the chain of thought(COT) approach, we provide a set of instructions for the modelto follow. Finally, for Retrieval Augmented Generation (RAG) weprovide context and questions to the model, from which it extractsinformation.Zero-shot prompts are simple instructions or tasks given to anLLM that have not been specifically trained on that task. It servesas a baseline because it demonstrates the models fundamentalability to understand and respond to prompts based solely on itspre-training . In few-shot prompting, a small set of examplesillustrating the desired outcome are manually selected and providedto the LLM. These examples allow the model to understand thetasks at hand and generate similar results . Chain-of-thoughtprompting provides a set of intermediate steps to guide the LLM tomimic human-like reasoning. This significantly improves the capa-bility of the LLM to understand complex reasoning and generatebetter topics . Retrieval-augmented generation (RAG) combinesthe capabilities of an LLM with a retrieval system to source andintegrate additional information into its responses . This effortprovides contextually richer and ultimately more accurate outputs.We do this by providing all the interview transcripts to the LLMas a custom knowledge base. Two considerations helped the RAGapproach outperform the other approaches: 4.1.1Focused Analysis: In our approach, LLM searches the knowl-edge base to find and retrieve parts of documents that are mostrelevant to the question in the query. This narrows the focus to themost relevant information and ensures attention to critical topicsand nuances. 4.1.2Context Dilution/Managing Information Overload: Using alltranscripts as input in a single instance creates information over-load scenarios, ultimately leading to dilution of important topicsor nuances. If the dataset is too large or complex, LLM might losetrack of whats most relevant to specific query, leading halluci-nations. Hallucinations or inaccuracies within this context refersto instances where the model generates information which is notgrounded in input data. In our approach, the use of RAG mitigatessome of the hallucination by anchoring LLM responses relevant",
  "Findings": "In the paper describing the dataset leveraged for this work, theauthors collected and conducted a manual analysis . Their re-search led to identification of significant, recurring topics withinthe interviews. Our evaluation strategy uses these manually gen-erated topics from the papers work as gold standard to compareagainst topics generated by the LLMs-based approach. We use Pre-cision (Equation 1), Recall (Equation 2), and F1-score (Equation 3)to benchmark topics generated by our LLM-augmented qualita-tive research approach against the topics generated by the humanresearcher.",
  "(4)": "These metrics are the current evaluation standard for classifica-tion models, but they can be adapted for text generation tasks .Precision and Recall measure the proportion of correctly identifiedpositive cases. In the context of our experiment, every word frompredicted text gets matched to a word in the referenced text tocompute recall. This process is inverted to then compute precision.The precision and recall values are then combined to compute anF1 score. These metrics use cosine similarity (Equation 4) in whicheach predicted word is paired with its closest corresponding wordfrom the reference text with the aim of maximizing the similarityscore.In , the performance of various LLM prompting tech-niques including Chain of Thought, Few Shot, Zero Shot and RAG,are compared across different embedding models (Distillbert-base-uncased, Bert-base-uncased, and Roberta-large). This comparisonaims to evaluate the robustness and effectiveness of these promptingtechniques. Our results indicate that while each prompting tech-nique shows varying level of precision, recall and F1-score, RAGconsistently outperform the others on all three metrics, achievinghighest performance across all models.",
  "Learnings": "Treating large language models (LLMs) as novice research assistantsduring thematic analysis offered valuable insights for our research.By framing the LLM as a novice collaborator with little knowledgeor insight of the context, prompts can be crafted to better guide themodel and leverage its capabilities. Used prudently, similar noviceLLM-augmented approaches can significantly increase time andresource efficiency compared to traditional qualitative coding meth-ods in talent management research. The following sections explore",
  "Approaching LLMs as Novice ResearchAssistants can help prepare better prompts": "A novice is a person who, has no experience with the situationsin which they are expected to perform tasks . The novice isthus at a basic proficiency level for skill acquisition, with limitedinformation and prior experience related to a task at hand . Forlarge qualitative datasets analyzed using LLMs we propose that anovice-led approach to analysis is a good fit. In our approach thehuman behaves as an expert prompting the novice LLM to provideinsights related to topics of interest. We found this framework as ahelpful mental model to ground the primary researcher promptingthe LLM as they iteratively uncover insights from the dataset.",
  "Used prudently, LLMs can help increasetime effectiveness and resource efficiency": "LLMs have advanced the field of natural language processing withtheir ability to understand and generate responses that closelymimic human language . The strengths of LLMs extend beyondmetrics, these models are adept at processing vast amounts of textrapidly, demonstrating a level of topic modeling that can mimichuman analysis. Manual topic modeling is human labor intensiveand time inefficient . LLMs also enhance efficiency by streamlin-ing the processing of large datasets, allowing for the extraction oftopics from qualitative data more quickly. Improvisations of thesemodel using techniques like few-shot and zero-shot learning ca-pabilities further reduce the need for expensive data labeling andannotations. In a nutshell, LLMs boost speed, reduce human effort,scale to massive datasets, and lower labeling costs. However, humanexpertise is still essential for judgment, validation and end-to-endframework design.",
  "LLM augmented approaches offer significantincrease in ease and enhanced contextcompared to traditional NLP approaches": "Using a RAG approach towards an LLM-augmented qualitative re-search analyzing semi-structure interviews shows great promisecompared to natural language processing methods like Latent Dirich-let allocation (LDA). Currently, there are no widely accepted meth-ods for comparing the two approaches as there is no bridge tocompare keywords to themes, except from a human-evaluator easeof interpretability standpoint. We performed topic modeling anal-ysis on the same dataset with the broader aim of finding themes.Manually comparing both approaches, each researcher of this work-stream independently found that any of the approaches using anLLM yielded much greater context and consequently, better inter-pretability than the traditional LDA approach. This is likely because,with LDA, the model outputs a list of words and probability foreach topic. With these words, the researcher would then have tomanually define the topic. While this approach increases researcherflexibility, it remains time and resource consuming. In contrast,",
  "Recommendations": "Traditional qualitative research is evaluated based on several crite-ria that ensure quality and rigor of the research, both in terms ofmethods as well as findings. Prior research has established four cri-teria for increased rigor and trustworthiness of qualitative researchstudies around credibility, dependability, confirmability, and trans-ferability . We recommend three ways in which quality criteriafrom traditional qualitative research can be used by practitionersemploying LLM augmented analysis of qualitative data.",
  "Establishing credibility of findings byincorporating mechanism for memberchecks": "Member checks, i.e., the strategy of soliciting insights from researchparticipants on research findings, are often relied on as the goldstandard for increasing trustworthiness of qualitative research ap-proaches (e.g., ). Qualitative researchers employing LLMscan work on deepening their understanding of the research con-text using appropriate data-collection methods and tools that workbest for particular contexts, as well as conduct adequate memberchecking to ensure the accuracy of findings.",
  "Practicing increased researcher reflexivity": "Qualitative researchers are recommended that they acknowledgeand address their own biases, thus recognizing the influence oftheir own experiences and opinions on the research process .Similar exercises on reflectivity can also be helpful for researchersaugmenting qualitative data analysis through employing LLMs.Researcher reflexivity in such instances can extend to queryingthe LLM to ask for rationale on why certain topics were extracted,grounding topics in anecdotes from the transcripts, and recognizingthe influence the human researchers prior knowledge and biaseswill have on the prompts used. Future work in extending LLMs forqualitative research should continue to draw on evaluation criteriagrounded in traditional qualitative research paradigm.",
  "Increasing transparency of decisions madethroughout the research study": "Qualitative researchers are recommended to thoroughly documentall decisions that guide their analysis process by providing thickdescriptions, allowing for increased transparency. This practiceenhances reliability and reproducibility of the research . Quali-tative researchers employing LLMs should also similarly strategizemaximizing transparency through mechanisms such as document-ing changes in workflow, sharing prompts, and detailing modelpreferences.",
  "Closing Thoughts": "The approach outlined in this paper offers a promising avenuefor industry-based talent management practitioners seeking to in-crease the time and resource efficiency of qualitative interview dataanalysis. By leveraging large language models (LLMs) as novice qualitative research assistants, organizations can potentially ac-celerate the coding, categorization, and thematic synthesis of richinterview data - a critical bottleneck in many talent managementresearch initiatives.However, as the field of LLM-assisted qualitative research ma-tures, it will be essential to not only benchmark model performanceagainst traditional quantitative evaluation metrics, but also considerquality criteria more prominent within the qualitative research par-adigm. Factors such as credibility, transferability, dependability,and confirmability will need to be carefully evaluated as LLMs areintegrated into qualitative workflows. Furthermore, the ethical useof AI assistants in sensitive domains like talent management willrequire close, multi-disciplinary attention to issues at the intersec-tion of data privacy, algorithmic bias, and model transparency, forwhich researchers will have to be trained .Future research should seek to establish guidelines and bestpractices for LLM-augmented qualitative analysis that uphold therigor and trustworthiness expected within the qualitative researchcommunity. Only by doing so can talent management scholarsand practitioners unlock the full potential of these powerful lan-guage models, while respecting the epistemological foundationsof qualitative inquiry. As the field evolves, we believe that a judi-cious, ethically-grounded approach to LLM integration can yieldsubstantial gains in research efficiency and organizational impact. Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and ShmargaretShmitchell. 2021. On the dangers of stochastic parrots: Can language models betoo big?. In Proceedings of the 2021 ACM conference on fairness, accountability,and transparency. 610623.",
  "Jacob Devlin. 2018. Bert: Pre-training of deep bidirectional transformers forlanguage understanding. arXiv preprint arXiv:1810.04805 (2018)": "Alexander Dunn, John Dagdelen, Nicholas Walker, Sanghoon Lee, Andrew SRosen, Gerbrand Ceder, Kristin Persson, and Anubhav Jain. 2022. Structured in-formation extraction from complex scientific text with fine-tuned large languagemodels. arXiv preprint arXiv:2212.05238 (2022). Yogesh K Dwivedi, Nir Kshetri, Laurie Hughes, Emma Louise Slade, AnandJeyaraj, Arpan Kumar Kar, Abdullah M Baabdullah, Alex Koohang, VishnupriyaRaghavan, Manju Ahuja, et al. 2023. Opinion Paper:So what if ChatGPT wroteit? Multidisciplinary perspectives on opportunities, challenges and implicationsof generative conversational AI for research, practice and policy. InternationalJournal of Information Management 71 (2023), 102642.",
  "Traditional topic modeling using approaches such as Latent Dirich-let Allocation (LDA) often present the most representative words": "for each generated topic. For instance, for Topic 1 words such as\"students\", \"develop\", \"institution\", \"science\", etc. were found impor-tant. Attempting to interpret the underlying thematic meaning ofthese word lists can be challenging without additional contextualinformation about how those words were used within the originalcorpus. In contrast, large language models (LLMs) have demon-strated the capability to synthesize the semantically related wordsand phrases into more coherent topical representations. This abilityof LLMs to generate primitive yet formative contextual informationthreading together words and phrases of interest and thereby pro-vide researchers with a more insightful starting point for furtheranalysis and interpretation of the latent topics uncovered throughthe LDA process."
}