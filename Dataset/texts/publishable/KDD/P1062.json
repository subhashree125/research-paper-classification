{
  "ABSTRACT": "Boolean satisfiability (SAT) problems are routinely solved by SATsolvers in real-life applications, yet solving time can vary drasti-cally between solvers for the same instance. This has motivatedresearch into machine learning models that can predict, for a givenSAT instance, which solver to select among several options. Ex-isting SAT solver selection methods all rely on some hand-pickedinstance features, which are costly to compute and ignore the struc-tural information in SAT graphs. In this paper we present GraSS,a novel approach for automatic SAT solver selection based on tri-partite graph representations of instances and a heterogeneousgraph neural network (GNN) model. While GNNs have been previ-ously adopted in other SAT-related tasks, they do not incorporateany domain-specific knowledge and ignore the runtime variationintroduced by different clause orders. We enrich the graph represen-tation with domain-specific decisions, such as novel node featuredesign, positional encodings for clauses in the graph, a GNN archi-tecture tailored to our tripartite graphs and a runtime-sensitive lossfunction. Through extensive experiments, we demonstrate that thiscombination of raw representations and domain-specific choicesleads to improvements in runtime for a pool of seven state-of-the-art solvers on both an industrial circuit design benchmark, andon instances from the 20-year Anniversary Track of the 2022 SATCompetition. Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August25-29, Barcelona, Spain 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-1-4503-XXXX-X/18/06...$15.00",
  "GNN, SAT, Algorithm Selection": "ACM Reference Format:Zhanguang Zhang, Didier Chetelat, Joseph Cotnareanu, Amur Ghose, WenyiXiao, Hui-Ling Zhen, Yingxue Zhang, Jianye Hao, Mark Coates, and Mingx-uan Yuan. 2018. GraSS: Combining Graph Neural Networks with ExpertKnowledge for SAT Solver Selection. In Proceedings of the 30th ACM SIGKDDConference on Knowledge Discovery and Data Mining (KDD 24). ACM, NewYork, NY, USA, 10 pages.",
  "INTRODUCTION": "The Boolean satisfiability (SAT) problem is one of the most funda-mental computer science problems, with numerous applicationsin planning and scheduling , formal software verification and electronic circuit design . A SAT instance con-sists of a formula with Boolean variables, such as (1 2) (1 2 3) 1, and the problem involves finding an assignment ofvalues for each variable which makes the whole formula true, orproving that no such assignment exists. Although the problem isNP-complete , many SAT solvers have been designed over theyears and modern CDCL-based solvers are routinely able to solveindustrial problems within minutes .Structural differences between different SAT problems mean thatthe choice of solver can have a dramatic impact on the solving time.This has motivated the use of machine-learning based methods forselecting the optimal solver to use for a given instance, with thehope that data-driven models can see patterns where humans havebeen unsuccessful. The most influential of those has probably beenSATzilla , which has won several times the annual SAT Compe-tition . This model relies on machine learning algorithms that",
  "Loss": ": The workflow of our method. SAT instances are represented as literal-clause graphs with hand-designed attributes.Rounds of heterogeneous graph convolutions are applied, which modify the attributes. The attributes of the clause and variablenodes are then averaged, before being fed to a linear layer followed by a softmax over the various solvers. The convolutions andthe linear layer are trained to minimize by gradient descent a runtime-sensitive classification loss computed from runtimescollected on training SAT instances. require a fixed-dimensional vector of features as input, irrespectiveof the actual instance size (number of clauses and variables). Thisnecessarily implies that some aspects of a SAT problem are nottaken into account when performing solver selection.In recent years, machine learning has been revolutionized bydeep learning models trained on raw descriptions of data pointssuch as image pixels or text strings . In particular, surprisingsuccess has been found in a variety of combinatorial optimizationtasks by representing optimization problems as graphs, and feedingthem as inputs to graph neural networks . Such models are ableto take the complete representation of a problems as input, in asize-independent way, and see patterns where humans have beenunable to distinguish any.In this work, we propose GraSS (Graph Neural Network SATSolver Selector), the first graph neural network (GNN) based methodfor automatic SAT solver selection. We represent instances as literal-clause graphs , thus encoding the entirety of the informationpertaining to an instance. To improve performance further, wealso endow the graph with hand-designed features representingdomain-knowledge about which aspects of the graph should beparticularly useful for solver selection, as well as positional encod-ings for the clauses to allow for order-specific effects. Our GNNmodel consists of learned graph convolutions operating over eachtype of edge, with a node-specific pooling operation prior to a lin-ear classifier. We train our model in a supervised manner with aruntime-sensitive classification loss. The training data consists of acollection of instances for which the runtimes of multiple solvershave been collected.On both a large-scale industrial circuit design benchmark, andon instances from the Anniversary Track of the 2022 SAT Com-petition , we report improvements in performance comparedto seven competitive solvers, as well as state-of-the-art machinelearning approaches. We also perform a complete ablation study to",
  "RELATED WORK2.1SAT Solver Selection": "There exists a rich literature describing machine learning modelsfor the selection of the optimal SAT solver for a given instance. Thisapproach is sometimes referred to as portfolio-based SAT solving.A detailed summary is provided by Holden et al. [20, ].SATzilla and its successors are a family of classifica-tion models that have won multiple prizes in the SAT Competition(2007 and 2009) and the SAT Challenge (2012). SATzilla uses hand-selected features to characterize each SAT instance for best solverselection. The latest version of SATzilla consists of a feature",
  "GraSS: Combining Graph Neural Networks with Expert Knowledge for SAT Solver SelectionKDD 24, August25-29, Barcelona, Spain": "Armin Biere and Mathias Fleury. 2022. Gimsatul, IsaSAT and Kissat entering theSAT Competition 2022. In Proc. of SAT Competition 2022 Solver and BenchmarkDescriptions (Department of Computer Science Series of Publications B, Vol. B-2022-1), Tomas Balyo, Marijn Heule, Markus Iser, Matti Jrvisalo, and Martin Suda(Eds.). University of Helsinki, 1011. Bernd Bischl, Pascal Kerschke, Lars Kotthoff, Marius Lindauer, Yuri Malitsky,Alexandre Frchette, Holger Hoos, Frank Hutter, Kevin Leyton-Brown, KevinTierney, and Joaquin Vanschoren. 2016. ASlib: A benchmark library for algorithmselection. Artificial Intelligence (2016). Quentin Cappart, Didier Chtelat, Elias B Khalil, Andrea Lodi, Christopher Morris,and Petar Velikovi. 2023. Combinatorial optimization and reasoning with graphneural networks. Journal of Machine Learning Research 24 (2023), 161.",
  "Algorithm Selection": "More generally, SAT solver selection is a special case of algorithmselection, which aims to select, for a given input, the most efficientalgorithm from a set of candidate algorithms. This is particularlyimportant for computationally hard problems, where there is typi-cally no single algorithm that outperforms all others for all inputs.Besides SAT solving, algorithm selection techniques have achievedremarkable success in various applications such as Answer Set Pro-gramming (ASP) and the Traveling Salesperson Problem (TSP). A thorough literature review is provided in Kerschke et al. .Algorithm selection methods can be roughly divided betweenoffline and online methods. Offline methods, such as this work,rely on training ahead of time on a labeled dataset, whereas onlinealgorithms attempt to improve selection performance as more andmore cases are run. Although providing worse initial performance,online methods avoid the computational cost of the initial trainingphase and the problem of distribution shift between training andtest data, and methods based on based on reinforcement learning ormulti-armed bandits have been proposed for this purpose .The many design choices in algorithm selection systems posenew challenges for efficient system development. For example, Auto-Folio automatically configures the entire framework, includingbudget allocation for pre-solving schedules, pre-processing proce-dures (such as transformations and filtering) and algorithm com-ponent selection. It achieved competitive performance in multiplescenarios from the ASLib benchmark.",
  "Graph Neural Networks in SAT Solving": "Several works have explored applications of GNNs to various as-pects of SAT solving in the past, even if not specifically to the prob-lem of solver selection. In all these works, some kind of graphicalrepresentation of SAT instances is used. Multiple suggestions haveappeared in the literature: these include lossless representationslike literal-clause graphs (LCGs) and variable-clause graphs (VCGs),and lossy representations like literal-incidence graphs (LIGs) andvariable-incidence graphs (VIGs) . These different representa-tions strike a balance between graph size and information content,and have found success in various SAT-related tasks.Some prior works have explored the use of GNNs to learn localsearch heuristics in SAT solvers . Yolcu and Pczos represent SAT formulas as variable-clause graphs (VCGs) and aGNN model is trained to select variables whose sign to flip at everystep through a Markov decision process (MDP). The learned heuris-tic is shown to reduce the number of steps required to solve theproblem. Graph-Q-SAT uses a deep Q-network (DQN) withGNN architecture to learn branching heuristics in conflict drivenclause learning (CDCL) solvers. Each SAT formula is converted intoa VCG, and GNN layers are used to predict the -value of eachvariable. The variable with the highest -value for the specific as-signment is selected for branching. The learned heuristic is shownto significantly reduce the number of iterations required for SATsolving.Instead of relying on existing SAT solvers, NeuroSAT uses aGNN-based model to predict satisfiability of an instance in an end-to-end manner. Each SAT formula is represented by a literal-clausegraph (LCG), as in our work. After several steps of message-passing,the updated embedding of each literal is projected to a scalar voteto indicate the confidence that the formula is satisfiable. The votesare averaged together and passed through a sigmoid function toproduce the models probability that the instance is satisfiable. Onrandomly generated instances from a SR(40) distribution, NeuroSATsolved 70% of SAT problems with an accuracy of 85%. A subsequentwork, NeuroCore , uses a lighter NeuroSAT model to predictthe core of instance, which is the smallest unsatisfiable subset ofclauses. This prediction is then used to guide variable selection inSAT solver algorithms.Finally, graph neural networks have also been widely used forSAT instance generation. For example, G2SAT and HardSAT-GEN represent instances as LCGs, and generate new variantsfrom an iterative splitting and merging process driven by a GNN.Furthermore, W2SAT extends this approach by representinginstances as weighted graphs encoding literal co-occurence amongclauses, while using a similar generation mechanism.",
  "KDD 24, August25-29, Barcelona, SpainZhang, et al": "... Literal FeaturesClauseFeatures Clause PE : Literal-clause graph representation of a SAT in-stance used in this work. The instance is converted to CNFform, and nodes represent positive literals, negative liter-als and clauses. Edges are drawn between clauses and literalnodes if the literal participate in the clause, and edges arealso drawn between positive and negative nodes of the samevariable. The nodes are endowed with feature vectors de-scribed in Appendix A. To do so, at train-time, we are given a collection of SAT in-stances and for each instance , we are given sampled solvingtimes 1 , ..., from each solver. This dataset can be used for train-ing a machine learning model offline in a supervised manner. Attest-time, a separate collection of SAT instances are given to themodel, which selects a solver for each. The selected solver for eachinstance is run, and the average runtime over all testing instancesis used to benchmark performance of the selector model.",
  "Representation and features": "The inputs to the model are individual SAT instances. We assumethat they are formulated in conjunctive-normal form (CNF), that isas a formula 1 where each is a clause, which in turnis in the form = 1 2 . . . , where each literal stands for avariable in the problem, or its negation . Any SAT problemcan be converted into an equivalent CNF problem in linear time.Given CNF SAT instances, we input them to the machine learn-ing model as literal-clause graphs (LCGs) endowed with extrainformation in the form of node features. A representation is pro-vided in . The literal-clause graph of a SAT instance is anundirected graph with three types of node: one node per clausein the graph, and two nodes and per variable in the graph,representing itself and its negation respectively. An edge is drawnbetween a clause node and a variable node / if the variable(respectively, its negation) appears as literal in the clause. Finally,an edge is drawn between every positive and negative variablenode.In addition, to every clause node and variable node, we attachfeature vectors. Most features are hand-designed and are inspiredby those used by SATzilla . They represent expert knowledgethat is known to be critical for SAT solving process, such as thepresence of Horn clauses, which are clauses containing at mostone positive literal. These are especially important for the solvingprocess as the collection of Horn clauses can be proved withinlinear time . Besides these hand-designed features, clause nodefeatures are also enriched with a positional encoding, described in",
  "the next subsection. A complete list of the features used is providedin Appendix A": "3.2.1Clause positional embeddings. In principle, satisfiability of aSAT formula is not affected by permuting the variables or clauses,and literal-clause graphs are permutation-invariant as well. In prac-tice, however, we found solver runtimes can be sensitive to theorder in which clauses are provided as input. We conducted a studywith the popular Kissat 3.0 solver on the industrial LEC datasetdescribed further in . As can be seen in , shuf-fling clauses sometimes led to very large variations in runtime. Incontrast, shuffling variables showed limited impact.This is in line with previously reported remarks on other SATsolvers . A possible explanation could be the algorithmicdesign of modern solvers, for which the storage architecture ofvariables relies on a doubly linked list and the initial storage orderfollows the parsing order of the clauses. This results in variationsin cache miss rates depending on the provided clause ordering. Incontrast, variable ordering usually only impacts the variable labelsused by the solvers.To address the sensitivity to clause ordering, we include po-sitional encodings among the clause features. These encode theposition of a clause within the CNF formula. We follow the classicalencodings from Vaswani et al. and endow the th clause witha 10-dimensional embedding",
  "Model": "We use a graph neural network (GNN) model to predict whichsolver to use for a given instance. These models operate on graphsby repeatedly modifying node embeddings through graph convo-lution operations, and have emerged as a standard paradigm fordealing with graph-structured data, both in SAT solving andmore widely for combinatorial optimization in general . How-ever, we deviate from standard graph convolution frameworks byinterpreting our literal-clause graph as a graph with three typesof edges: (i) from clause to literal nodes; (ii) from literal to clausesnodes; and (iii) between positive and negative literal nodes. Thesegraphs can then be interpreted as heterogeneous graphs, and wecan apply heterogenous GNN methodologies .In this framework, the graph convolution steps take the followingedge-type-dependent form. Let R stand for the feature vectorat node . For every edge (,) E of type {1, . . . , } fromnode to node , we compute a message ,, = (,) where is a learnable message function. We then update the featurevector at node by the formula",
  "Training": "We train in a supervised fashion on a dataset of training SAT in-stances, for which runtimes have been collected ahead of time oneach solver of interest. Since our model produces a distributionover the possible solvers, we could treat the problem as simpleclassification with a cross-entropy loss. However, this would notbe well-aligned with our objective of minimizing solving runtime,since it would equally penalize incorrect predictions, irrespectiveof the amount of additional runtime induced by the selection of a",
  ",(1)": "where and are the model probability and runtime for instance = 1, . . . , and solver = 1, . . . , , respectively, and = min is the best time achieved by any solver on the instance. This has theadvantage of more directly optimizing final runtime, taking intoaccount that not all mistakes are equally impactful on solving time.We minimize this loss using the Adam algorithm with earlystopping.",
  "Inference": "At test-time, to predict which SAT solver to use, we convert the SATinstance into our graph representation, compute its node features,and feed the graph to our trained GNN model, which outputs aprobability distribution over the solvers of interest. The solver withhighest probability is chosen for solving the instance, with theruntime being reported.",
  "Base solvers": "We train and evaluate on a portfolio of seven top-performing solversfrom recent SAT Competitions : (a) Kissat-3.0, (b) bulky,(c) HyWalk, (d) MOSS, (e) mabgb, (f) ESA and (g) UCB. Among them,Kissat-3.0 and bulky are based on Kissat, which is the winnerof the 2020 SAT Competition and is known for its efficient datastructure design . The other five solvers are based on UCB and",
  "We train and evaluate on two datasets": "Logic Equivalence Checking (LEC). This is a proprietary datasetgenerated from logic equivalence checking steps in electronic cir-cuit design. Circuits undergo a large number of optimization stepsduring logic synthesis, and at each one of the steps, it is necessaryto verify that the circuits before and after optimization are func-tionally equivalent. This is done by verifying that the two circuitsproduce the same outputs for all possible inputs, which is equivalentto solving a SAT problem . We collected logic equivalencechecks from the optimization of 30 industrial circuits, yielding atotal of 78,727 SAT instances. A summary of dataset statistics isprovided as a. SAT Competition (SC). This is a subset of the Anniversary TrackBenchmark of the 2022 SAT Competition , which itself wascreated by collecting all instances from the Main, Crafted and Ap-plication tracks of the previous SAT competitions up to that year.We ran each instance of the Anniversary benchmark through eachof the seven solvers in the portfolio, and excluded those that couldnot be solved within 1,500 seconds by any solver, as well as thosewith more than 20,000 variables, yielding 2,088 SAT instances. Asummary of dataset statistics is provided as b.",
  "We compare our approach with the following baselines": "Best Base Solver. The individual solver among the portfolio ofseven that had the best performance on the training data, measuredin average runtime over all instances. In practice, this was the bulkysolver for both datasets. SATzilla07 . We adapt this landmark SAT solving machinelearning model, based on a linear ridge regression model trainedto predict runtimes based on global handcrafted features that sum-marize SAT instance characteristics. Since our work focuses onSAT solver selection, we omit the presolving process in the originalSATzilla pipeline. We also remove features in the original modelthat require probing. This leaves 33 global features (#1-33 in theoriginal article). The model is trained from the Ridge class in thescikit-learn library with default settings. We convert theapproach into a SAT solver selection model by selecting the solverwith shortest predicted runtime. SATzilla12 . We also adapt the updated SATzilla model from2012, which was based on random forest classification. Again, weremove the presolving process, and only use the features that donot require probing (features #1-55 in the original article). We traina random forest model between each pair of solvers, weighting eachtraining instance by the absolute difference in runtime betweenthe two solvers, for 7(7 1)/2 = 21 models in total. Each model istrained from the RandomForestClassifier class in scikit-learnwith 99 trees and 2(55) + 1 = 7 sampled features in each tree.At test-time, each model is used to vote which solver it prefers inits pair for solving an instance, and the final solver choice is madefrom the solver that has received the most votes. ArgoSmArT . This is an approach based on a -nearest neigh-bors model trained for classification. We used the same 29 featuresas in the original paper, which form a subset of the 33 features usedin our adaptation of SATzilla07. We use the KNeighborsClassifierclass in scikit-learn with = 9 neighbors. CNN . We reimplement the approach of Loreggia et al. ,where the CNF formula is interpreted as text, converted to itsASCII values and then to a grayscale image, before being resized to128x128 pixels and fed to a Convolutional Neural Network (CNN).We use the same architecture as in the paper, which we implementin the pytorch library, and train it over a cross-entropy losswith the Adam algorithm, a learning rate of 1e-3 and earlystopping.",
  "We report the following metrics for performance evaluation. Resultsare averaged over five train-test folds over the data, and the averageand standard deviation over those five folds are reported": "Average Runtime (Avg Runtime). For each instance, the methodselects a solver, and this solver is used to solve the instance, report-ing a runtime. These runtimes are then averaged over all instances.In other words, this is the average runtime that would be observedif this method were used for all instances, as a portfolio\" solver.Lower is better.",
  "(b) SC benchmark": ": Cost of wrong prediction: the runtime differencebetween predicted solver and the optimal solver, when theselector has made a mistake. Average across five folds areshown, with standard deviation as error bar. Lower is better. in percentage of problems solved within our 500s cutoff time. In-terestingly, this is true despite the method not being as accurate inselecting the optimal solver for every instance, as measured by theaccuracy metric. This suggests that an important component of itssuccess lies in its improved robustness to error: when the methodmakes mistakes, they impact runtime less than competing methods.To understand this phenomenon further, we looked at the run-time difference between the predicted solver and the optimal solver,whenever a mistake is made. As can be seen in , the averagecost of wrong prediction is substantially lower for our method thanfor competitors, especially for the SC dataset.We further analyze the performance difference between our ap-proach and the next best method in average runtime, SATzilla12. Ta-ble 3 regroups the instances by rough measure of difficulty, namely",
  "Best base solverSATzilla076.76735.932SATzilla128.643194.155ArgoSmArT6.47241.266CNN1.3532.401GraSS (ours)1.23233.862": "by grouping them by which quartile (0-25%, 25-50%, 50-75% or75-100%) their best runtime achieved on any solver falls into, andcompares the performance of SATzilla12 and GraSS on each sub-group. As can be seen, SATzilla12 performs better on easy instances,while GraSS performs better on hard instances.Finally, as described in Subsection 4.4, our timing results onlyreport the time taken by the solvers in optimizing the instances. Inparticular, this means we exclude from the numbers the time takento compute the features necessary to take the decision, which isdependent on the storage format used to save the instances. For ourexperiments, we chose to save them on a hard drive in the standard.cnf text file format , and we report for completeness the timetaken to compute the features for each method in . Otherstorage formats would lead to different timings.",
  "Ablation Study": "We extend our analysis by considering the impact of various method-ological choices on performance over the LEC benchmark.We first evaluate the impact of the graph neural network archi-tecture, by comparing our approach with a variant that uses thesame convolution weights for every edge, effectively treating itas a homogeneous graph (Homogeneous). We also compare with aNeuroSAT-style architecture (NeuroSAT variant) inspired by Selsamet al. , which was originally designed for satisfiability prediction(sat/unsat). Their model also uses a literal-clause graph to encodeinstances, although with learned initial node embeddings, and usesa very deep LSTM-GNN hybrid architecture with 26 layers andcustom graph convolution operations. We implement the same, butreplace the final layer which computes a scalar vote for everyliteral, and takes the average vote before a sigmoid activation, byan averaging of the literal embeddings, followed by a linear layerand a softmax activation. We also use 4 layers instead of 26 fortractability on our dataset, whose instances are substantially largerthan those in the original paper. As can be seen in , ourapproach improves over these alternatives in every metric.We next evaluate our choice of node features. We compare itagainst random normal values (Random), as in Selsam et al. );a one-hot vector indicating whether the node represents a clause,positive or negative literal (Node-type), as used in Li et al. , Yolcuand Pczos and You et al. ; and Laplacian Positional Encod-ings (Laplacian PE), as introduced in Dwivedi et al. . We alsocompare against a variant of our approach consisting of the same",
  "LIMITATIONS": "Although our experiments strongly establish the superiority of ourapproach in the presented scenario, several limitations can be noted.Deep learning methods are well-known to be data hungry, andperform best in regimes where training sets are large. It is plausiblethat in scenarios where a limited number of timed instances areavailable, performance would not be competitive against simplermodels. In addition, in many scenarios it might be desirable to learnonline, updating models as examples stream in: our method cannotbe readily adapted to this situation, as training requires runtimelabels on every solver for each instance, and adapting graph neuralnetworks to online learning is challenging .",
  "CONCLUSION": "This work proposed a novel supervised approach to SAT solver se-lection, based on representing instances as literal-clause graphs andtraining a graph neural network to select, from this representation,a SAT solver among a fixed portfolio so as to minimize solving run-time. The graph representations are endowed with node featuresthat encode domain knowledge, and in the case of clause nodes, alsoposition within the SAT formula. The resulting scheme is shown tooutperform competing approaches on two benchmarks, one froman industrial circuit design application and one from the annualSAT solver competitions.",
  "Stephen A Cook. 2023. The complexity of theorem-proving procedures. InLogic, Automata, and Computational Complexity: The Works of Stephen A. Cook.143152": "James M Crawford and Andrew B Baker. 1994. Experimental results on theapplication of satisfiability algorithms to scheduling problems. In Proceedings ofthe 2nd AAAI Conference on Artificial Intelligence. 10921097. Hans Degroote, Bernd Bischl, Lars Kotthoff, and Patrick De Causmaecker. 2016.Reinforcement learning for automatic online algorithm selection-an empiricalstudy. In Proceedings of the 16th ITAT Conference Information Technologies - Ap-plications and Theory.",
  "William F Dowling and Jean H Gallier. 1984. Linear-time algorithms for testing thesatisfiability of propositional Horn formulae. The Journal of Logic Programming(1984)": "Vijay Durairaj and Priyank Kalla. 2005. Variable ordering for efficient SATsearch by analyzing constraint-variable dependencies. In Proceedings of the 8thInternational Conference on the Theory and Applications of Satisfiability Testing.Springer, 415422. Vijay Prakash Dwivedi, Chaitanya K Joshi, Anh Tuan Luu, Thomas Laurent,Yoshua Bengio, and Xavier Bresson. 2023. Benchmarking graph neural networks.Journal of Machine Learning Research 24, 43 (2023), 148.",
  "Franjo Ivani, Zijiang Yang, Malay K Ganai, Aarti Gupta, and Pranav Ashar.2008. Efficient SAT-based bounded model checking for software verification.Theoretical Computer Science 404 (2008), 256274": "Serdar Kadioglu, Yuri Malitsky, Ashish Sabharwal, Horst Samulowitz, and MeinolfSellmann. 2011. Algorithm selection and scheduling. In Proceedings of the 17thInternational Conference on the Principles and Practice of Constraint Programming. Henry Kautz, David McAllester, Bart Selman, et al. 1996. Encoding plans in propo-sitional logic. In Proceedings of the Fifth International Conference on Principles ofKnowledge Representation and Reasoning. 374384.",
  "Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification withGraph Convolutional Networks. In Proceedings of the 5th International Conferenceon Learning Representations": "Vitaly Kurin, Saad Godil, Shimon Whiteson, and Bryan Catanzaro. 2020. CanQ-Learning with Graph Networks Learn a Generalizable Branching Heuristicfor a SAT Solver?. In Proceedings of the 34th International Conference on NeuralInformation Processing Systems. Yang Li, Xinyan Chen, Wenxuan Guo, Xijun Li, Wanqian Luo, Junhua Huang, Hui-Ling Zhen, Mingxuan Yuan, and Junchi Yan. 2023. HardSATGEN: Understandingthe Difficulty of Hard SAT Formula Generation and A Strong Structure-Hardness-Aware Baseline. In Proceedings of the 29th ACM SIGKDD Conference on KnowledgeDiscovery and Data Mining.",
  "Mladen Nikoli, Filip Mari, and Predrag Janii. 2013. Simple algorithm portfoliofor SAT. Artificial Intelligence Review (2013)": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, GregoryChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al.2019. Pytorch: An imperative style, high-performance deep learning library. InProceedings of the 32nd International Conference on Neural Information ProcessingSystems. Fabian Pedregosa, Gal Varoquaux, Alexandre Gramfort, Vincent Michel,Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss,Vincent Dubourg, et al. 2011. Scikit-learn: Machine learning in Python. theJournal of machine Learning research 12 (2011), 28252830. Daniel Selsam and Nikolaj Bjrner. 2019. Guiding high-performance SAT solverswith unsat-core predictions. In Proceedings of the 22nd International Conferenceon the Theory and Applications of Satisfiability Testing. Springer, 336353. Daniel Selsam, Matthew Lamm, Benedikt Bnz, Percy Liang, Leonardo de Moura,and David L Dill. 2019. Learning a SAT solver from single-bit supervision. InProceedings of the 7th International Conference on Learning Representations.",
  "Grigori S Tseitin. 1983. On the complexity of derivation in propositional calculus.Automation of Reasoning 2: Classical Papers on Computational Logic 19671970(1983), 466483": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all youneed. In Proceedings of the 30th International Conference on Neural InformationProcessing Systems. Junshan Wang, Guojie Song, Yi Wu, and Liang Wang. 2020. Streaming graph neu-ral networks via continual learning. In Proceedings of the 29th ACM InternationalConference on Information and Knowledge Management. 15151524.",
  "Emre Yolcu and Barnabs Pczos. 2019. Learning Local Search Heuristics forBoolean Satisfiability. In Proceedings of the 32nd International Conference on NeuralInformation Processing Systems": "Jiaxuan You, Haoze Wu, Clark Barrett, Raghuram Ramanujan, and Jure Leskovec.2019. G2SAT: Learning to Generate SAT Formulas. In Proceedings of the 32thInternational Conference on Neural Information Processing Systems. Chuxu Zhang, Dongjin Song, Chao Huang, Ananthram Swami, and Nitesh VChawla. 2019. Heterogeneous graph neural network. In Proceedings of the 25thACM SIGKDD international conference on knowledge discovery & data mining.793803.",
  "Clause node features": "1clause_is_hornIs the clause Horn?1clause_degreeNumber of literals in the clause,divided by the total number ofvariables in the instance.1clause_is_binaryIs the clause composed of twoliterals?1clause_is_ternaryIs the clause composed of threeliterals?1clause_pos_numNumber of positive literals di-vided by the total number of lit-erals in the clause.1clause_neg_numNumber of negative literals di-vided by the total number of lit-erals in the clause.1clause_pos_neg_ratioNumber of postivive literals, di-vided by the number of nega-tive literals in the clause plus 1.10clause_pePositional encoding (see Sub-section 3.2.1 in the main paper.)"
}