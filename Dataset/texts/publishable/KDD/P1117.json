{
  "Abstract": "Online controlled experiments are a crucial tool to allow for con-fident decision-making in technology companies. A North Starmetric is defined (such as long-term revenue or user retention),and system variants that statistically significantly improve on thismetric in an A/B-test can be considered superior. North Star met-rics are typically delayed and insensitive. As a result, the cost ofexperimentation is high: experiments need to run for a long time,and even then, type-II errors (i.e. false negatives) are prevalent.We propose to tackle this by learning metrics from short-termsignals that directly maximise the statistical power they harnesswith respect to the North Star. We show that existing approachesare prone to overfitting, in that higher average metric sensitivitydoes not imply improved type-II errors, and propose to insteadminimise the -values a metric would have produced on a log ofpast experiments. We collect such datasets from two social mediaapplications with over 160 million Monthly Active Users each, to-talling over 153 A/B-pairs. Empirical results show that we are ableto increase statistical power by up to 78% when using our learntmetrics stand-alone, and by up to 210% when used in tandem withthe North Star. Alternatively, we can obtain constant statisticalpower at a sample size that is down to 12% of what the North Starrequires, significantly reducing the cost of experimentation.",
  "Modern platforms on the web need to continuously make decisionsabout their product and user experience, which are often central to": "KDD 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.This is the authors version of the work. It is posted here for your personal use. Notfor redistribution. The definitive Version of Record was published in Proceedings ofthe 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 24),August 2529, 2024, Barcelona, Spain, the business at hand. These decisions range from design and inter-face choices to back-end technology adoption and machine learningmodels that power personalisation. Online controlled experiments,the modern web-based extension of Randomised Controlled Trials(RCTs) , provide an effective tool to allow for confident decision-making in this context (bar some common pitfalls ).A North Star metric is adopted, such as long-term revenue oruser retention, and system variants that statistically significantlyimprove the North Star metric are considered superior to the testedalternative . Proper use of statistical hypothesis testing toolssuch as Welchs -test , then allows us to define and measurestatistical significance in a mathematically rigorous manner.However effective this procedure is, it is far from efficient. Indeed,experiments typically need to run for a long time, and statisticallysignificant changes to the North Star are scarce. This can eitherbe due to false negatives (i.e. type-II error), or simply because theNorth Star is not moved by short-term experiments. In these cases,we need to resort to second-tier metrics (e.g. various types of userengagement signals) to make decisions instead. These problemsare common in industry, as evidenced by a wide breadth of relatedwork. A first line of research leverages control variates to reduce thevariance of the North Star metric, directly reducing type-II errors byincreasing sensitivity . Another focuses on identi-fying second-tier proxy or surrogate metrics that are promisingto consider instead of the North Star , or to predict long-term effects from short-term data . Finally, several workslearn metric combinations that maximise sensitivity .This paper synthesises, generalises and extends several of theaforementioned works into a general framework to learn A/B-testing metrics that maximise the statistical power they harness.We specifically extend the work of Kharitonov et al. to appli-cations beyond web search, where the North Star can be delayedand insensitive. We highlight how their approach of maximisingthe average -score does not accurately reflect downstream metricutility in our case, in that it does not penalise disagreement withthe North Star sufficiently (i.e. type-III/S errors ).Indeed: whilst this approach maximises the mean -score, it doesnot necessarily improve the median -score, and does not lead toimproved statistical power in the form of reduced type-II error as aresult.Alternatively, optimising the learnt metric to minimise -valueseither directly or after applying a log-transformation more equi-tably ditributes gains over multiple experiments, leading to morestatistically significant results instead of a few extremely signifi-cant results. Furthermore, we emphasise that learnt metrics are notmeant to replace existing metrics, but rather to complement them.As such, their evaluation should be done through multiple hypoth-esis testing (with appropriate corrections ) if any of the NorthStar, available vetted proxies and surrogates, or learnt metrics are",
  "KDD 24, August 2529, 2024, Barcelona, SpainOlivier Jeunen and Aleksei Ustimenko": "statistically significant under the considered treatment variant. Wecan then either adopt a conservative plug-in Bonferroni correctionto temper type-I errors, or analyse synthetic A/A experiments toensure the final procedure matches the expected confidence level.We empirically validate these insights through two dataset ofpast logged A/B results from large-scale short-video platforms withover 160 million monthly active users each: ShareChat and Moj.Experimental results highlight that our learnt metrics provide sig-nificant value to the business: learnt metrics can increase statisticalpower by up to 78% over the North Star, and up to 210% when usedin tandem. Alternatively, if we wish to retain constant statisticalpower as we do under the North Star, we can do so with down to12% of the original required sample size. This significantly reducesthe cost of online experimentation to the business. Our learnt met-rics are currently used for confident, high-velocity decision-makingacross ShareChat and Moj business units.",
  "Background & Problem Setting": "We deal with online controlled experiments, where two system vari-ants and are deployed to a properly randomised sub-populationof users, adhering to best practices .For every system variant, for every experiment, we measurevarious metrics that describe how users interact with the platform.These metrics include types of implicit engagement (e.g. video-plays and watch time), as well as explicit engagement (e.g. likesand shares) as well as longer-term retention or revenue signals. Foreach metric, we log empirical means, variances and covariances (ofthe sample mean). For metrics with 1 , that is:",
  "Statistical Significance Testing": "We want to assess whether the mean of metric is statisticallysignificantly higher under variant compared to variant . To thisend, we define a significance level (often 0.05), correspond-ing to the false-positive-rate we deem acceptable. Then, we applyWelchs -test. The test statistic (also known as the -score) formetric and the given variants is given by:",
  "d.(4)": "When < , we can confidently reject the null-hypothesisthat and are equivalent w.r.t. the mean of metric . Note that-scores are signed, whereas two-tailed -values are not. Indeed:relabeling the variants changes the -score but not the -value,which leaves room for faulty conclusions of directionality, knownas type-III errors or sign errors . We discuss thesephenomena in detail, further in this article.A one-tailed -value for the one-tailed null hypothesis isgiven by = 1(), and rejected when <",
  ". Throughout,we use two-tailed -values unless mentioned otherwise": "2.1.1-value corrections. The above procedure is valid for a sin-gle metric, a single hypothesis, and importantly, a single decision.Nevertheless, this is not how experiments run in practice. Withoutexplicit corrections on the -values (or corresponding -scores), vio-lations of these assumptions lead to inflated false-positive-rates. Weconsider two common cases: a (conservative) multiple testing cor-rection when an experiment has several treatments, and a sequen-tial testing correction when experiments have no predeterminedend-date or sample size at which to conclude. These correctionsare applied as experiment-level corrections, to ensure that for anymetric and variants , , the obtained -values accurately reflectwhat they should reflect, yielding the specified coverage at varyingconfidence levels . Multiple comparisons. Often, launched experiments will havemultiple treatments deployed, leading to the infamous multiplehypothesis testing problem . We apply a Bonferroni correctionto deal with this. When there are treatments, we consider atreatment to be statistically significantly different from controlwhen a two-tailed < , instead of the original < threshold.We can equivalently apply this correction on -scores instead,allowing us to directly compare -scores across experiments withvarying numbers of treatments. Recall that the percentile pointfunction is the inverse of the CDF. We obtain a one-tailed -valueas = 1 (), and we reject the one-tailed null hypothesis when <",
  ") controls type-I errors effectively": "Always-Valid-Inference and peeking. A statistical test should onlybe performed once, at the end of an experiment. When the treatmenteffect is large, this implies we may have been able to concludethe experiment earlier. To this end, sequential hypothesis testshave been proposed in the literature . Modern versions makeuse of Always-Valid-Inference (AVI) to allow for continuouspeeking at intermediate results and making decisions based on them,whilst controlling type-I errors. Here, analogously, we can apply acorrection on the -scores as follows:",
  "Learning Metrics that Maximise Sensitivity": "The observation that we can learn parameters to maximise statis-tical sensitivity is not new. Yue et al. apply such ideas specificallyfor interleaving experiments in web search . Kharitonov et al.extend this to A/B-testing in web search, aiming to learn combina-tions of metrics that maximise the average -score . Deng andShi discuss lessons learned from applying similar techniques .We introduce the approach presented by Kharitonov et al. , asour proposed improvements build on their foundations.We consider new metrics as linear transformations on :",
  "= implies that requires a factor 2 fewer samples to": "achieve the same significance level as . This can directlybe translated to the cost of experimentation, as it allows us to runexperiments for shorter time-periods or on smaller sub-populations.As such, it comes naturally to frame the objective as learningthe weights that maximise the -score on the training data. Thistraining data consists of a set of experiments with pairs of variantsE = {(, )}|E|=1. We consider three distinct relations between pairsof deployed system variants: (1) Known outcomes: (, ) E+, where ,(2) Unknown outcomes: (, ) E?, where ? ,(3) A/A outcomes: (, ) E, where .Here, implies that there is a known and vetted preferenceof variant over typically because the North star or otherguardrail metrics showed statistically significant improvements.These experiments are further validated by replicating outcomes,observing long-term holdouts, or because the experiment was partof an intentional degradation test. We denote inconclusive exper-iments as ? , implying statistically insignificant outcomes onthe North Star. In rare cases, the North star might have gone up atthe expense of important guardrail metrics, rendering conclusionsambiguous. We only include experiments into the inconclusive setfor which we have a very strong intuition that something changed(and we know the null hypothesis should be rejected), but we areunable to make a confident directionality decision. This ensuresthat we can use this set to truly measure type-II errors. Finally, represents A/A experiments, where we know the null hy-pothesis to hold true (by design). The first set of experiments isused to measure type-III/S errors. Known and unknown outcomesare used to measure type-II errors, and A/A experiments can in-form us about type-I errors. This dataset of past A/B experiments is",
  "( + + )1( ).(8)": "Here, R is a small number to ensure that the matrix to beinverted is not singular. Kharitonov et al. fix this value at = 0.01and never adjust it throughout the paper. We wish to highlightthat technique is known as Ledoit-Wolf shrinkage , andthat it can have substantial influence on the obtained direction.Indeed: it acts as a regularisation term pushing the weights closerto = (). This can be seen by observing that as inf, the",
  ", and the solution hence becomes = ( )": ".As we only care about the direction we can ignore the denominator.To ensure fair comparison, we also set = 0.01. Exploring the effectsof Ledoit-Wolf shrinkage as a regularisation technique where is ahyper-parameter, gives an interesting avenue for future work.In order to include observations from multiple experiments intoa single set of learned weights, they propose to compute the optimaldirection per experiment, normalise, and average the weights:",
  "Methodology & Contributions3.1Learning Metrics that Maximise Power": "When directly optimising -scores, an implicit assumption is madethat the utility we obtain from increased -scores is linear. This isseldom a truthful characterisation of reality, considering how wewish to actually use these metrics downstream. We provide a toyexample in , reporting -scores and one-tailed -values fortwo experiments and three possible metrics 1,2,3, inspiredby real data. In this toy example, we know that , based on ahypothetical North Star metric. Nevertheless, as we do not knowthis beforehand, we typically test for the null hypothesis withtwo-tailed -values. In the table, this means that the outcome isstatistically significant if the reported one-tailed values are <",
  ", ), (ii) the outcome isinconclusive (?), i.e. a type-II error, or (iii) the null hypothesis isrejected, but for the wrong reason ( > 1": "2 , ).This latter case is deeply problematic, as it signifies disagreementwith the North Star. Such errors have been described as type-III ortype-S errors in the statistical literature . Naturally, wewould rather have a metric that fails to reject the null than one thatconfidently declares a faulty variant to be superior. Indeed, Dengand Shi argue that both directionality and sensitivity are desirableattributes for any metric . Nevertheless, considering candidatemetrics in , type-III errors are not sufficiently penalised bythe average -score: metric 3 maximises this objective despiteyielding statistical power that is on par with a coin flip.Directly maximising power might prove cumbersome, as it isessentially a discrete step-function w.r.t. the -score, dependenton the significance level . Instead, it comes natural to minimisethe one-tailed -value reported in . Indeed, the -valuetransformation models diminishing returns for high -scores, whichallows type-III errors to be sufficiently penalised. When consideringthis objective, 3 is clearly suboptimal whilst 2 is preferred.Note that the change in objective would not affect the geomet-ric heuristic as described in .2.1. As we simply apply amonotonic transformation on the-scores, the weight direction thatmaximises the -score equivalently minimises its -value. Whenlearning via gradient descent, however, the -value transformationaffects how we aggregate and attribute gains over different inputsamples. This allows us to stop focusing on increasing sensitiv-ity for experiments that are already sensitive enough, and moreequitably consider all experiments in the training data.This change in objective provides an intuitive and efficient ex-tension to existing approaches, allowing us to directly optimisethe confidence we have to correctly reject the null-hypothesis. Forknown outcomes, the loss function is given by:",
  ": Visualising our proposed optimisation objectivesfor learnt metrics, as a function of their -score": "-values for A/A-outcomes if type-I error becomes problematic. Aswe will show empirically in .3, this is not a problem weencounter. For this reason, we set 0.Note that whilst direct optimisation of -values is an improve-ment over myopic consideration of -scores, there is another caveat:the worst-case loss of a type-III/S error is bounded at 1, whichdoes not reflect our true utility function: metrics that disagree withthe North Star are far less reliable than those that simply remaininconclusive. As such, we also consider another variant of the ob-jective, where = log(1 ). provides visual intuitionto clarify how this monotonic transformation on the -values moreheavily penalises type-III/S errors, whilst retaining the optimum.From a theoretical perspective, this function provides a convexrelaxation for minimising the number of type-III/S errors a metricproduces. As a result, we expect this surrogate to exhibit strong gen-eralisation. We refer to this objective as minimising the log-value. Note that one could envision further extensions here wherethe significance level is directly incorporated into the objectivefunction to maximise statistical power at a given significance level .Nevertheless, we conjecture that their discrete nature might hampereffective optimisation and generalisation, compared to the strictlyconvex and smooth surrogate we obtain from the log-value.",
  "= [1.0, 1.0], = [0.5, 0.5], = = .(15)": "For this low-dimensional problem, we can visualise the -scoreas a function of the metric weights in a contour plot, as shown ina. Here, it becomes visually clear that whilst the direction ofthe = vector matters, its scale does not. The consequenceis that the gradient vectors w.r.t. the objective on the right-hand plotcan lead to slow convergence, even in this concave objective. Indeed,for poor initialisation in the bottom left quadrant (e.g. = ),the gradient direction is perpendicular w.r.t. the optima.Recent work makes a similar observation for discrete scale-freeobjectives as they appear in ranking problems . They propose toadopt projected gradient descent, normalising the gradients beforeevery update. Whilst effective, in our setting we would prefer to useout-of-the-box optimisation methods for practitioners ease-of-use.Instead, we introduce a simple regularisation term that representsthe distance between the scale of the vector and a hyper-sphere:",
  "L = 222.(16)": "All optima for this objective function are also optima to the origi-nal functionbut the gradient field is more amenable to iterativegradient-based optimisation techniques. b visualises howthis transforms the loss surface. Under this regularised objective,it is visually clear that gradient-based optimisation methods arelikely to exhibit faster convergence. Our empirical results confirmthis, for a variety of initialisation weights and learning objectives.",
  "Experiments & Discussion": "To empirically validate the methods proposed in this work, werequire a dataset containing logged metrics (sample means, theirvariances and covariances), together with preference orderings overcompeting system variants that were collected from real-world A/B-tests, ideally spanning large user-bases and several weeks.Existing work on this topic used a private dataset from Yandex fo-cused on web search experiments that ran between 20132014 .They report type-I and -II errors for 8 metrics and a fixed 5% signif-icance level, over 118 A/B-tests and 472 A/A-tests.In this work, we consider more general metrics that are relevantfor use-cases beyond web search (i.a. user retention and variousengagement signals). Furthermore, we report type-I/II/III/S errorsat varying significance levels, providing insights into the learnt met-rics behaviour. For this, we leverage logs of past A/B-experimentson two large-scale short-video platforms with over 160 millionmonthly active users each: ShareChat and Moj. The datasets consistof 153 A/B-experiments (of which 58 were conclusive) total thatran in 2023, and over 25 000 A/A-pairs. In total, we have access toroughly 100 metrics detailing various types of interactions withthe platform, engagements, and delayed retention signals. Because",
  "heuristic7.313.071.88e11.18e3-score7.552.672.33e13.88e3-value5.223.084.32e21.09e3log-value4.333.175.19e28.60e4": ": Sensitivity results for learnt metrics, computedvia leave-one-out cross-validation on all experiments withknown outcomes. We observe that minimising the log-valueeffectively improves median sensitivity over alternatives. our dataset is limited in size (a natural consequence of the problemdomain), we are bound to overfit when using all available metricsas input features. As such, we limit ourselves to 10 input metrics tolearn from, and evaluate them w.r.t. the delayed North Star. Thisfeature selection step also ensures that our linear model consistsof fewer parameters, which increases practitioners and businessstakeholders trust in its output. We focus on non-delayed signals,including activity metrics such as the number of sessions and activedays, and counters for positive and negative feedback engagementsof various types. These are selected through an analysis of theirtype-I/II/III/S errors w.r.t. the North Star, as well as their -scores:focusing on metrics with high sensitivity and limited disagreement.The research questions we wish to answer empirically using thisdata, are the following: RQ1 Do learnt metrics effectively improve on their objectives?RQ2 How do learnt metrics behave in terms of type-III/S errors?RQ3 How do learnt metrics type-I/II errors behave when consideredas stand-alone evaluation metrics?",
  "Effectiveness of Learnt Metrics (RQ1)": "We learn and evaluate metrics through leave-one-out cross-validation:for every experiment, we train a model on all other experiments andevaluate the -score (Eq. 7) and -value (Eq. 3) the metric yields forthe held-out experiment. We report the mean and median -scoresand -values we obtain for all A/B-pairs with known outcomes (i.e.E+) in . Best performers for every column (either maximising-scores or minimising -values) are highlighted in boldface. Em-pirical observations match our theoretical expectations: whilst the-score objective does effectively maximise the average -score, it isthe worst performer for both mean and median -values, and eventhe median -score. Our proposed log-value objective effectivelyimproves both the median -score and -value over alternatives.",
  "(b) False-positive rate (type-I error) and false-negative rate (type-II error, 1 power) for sets of metrics, after Bonferroni correction": ": When considering only learnt metrics, we improve type-II error significantly without hurting specificity. At = 0.05, weincrease statistical power by 67% (upper plot). In conjunction with the North Star and top proxy metrics, Bonferroni correctionsare slightly conservative (type-I error < ), and allow us to improve statistical power by 133% for = 0.05 (lower plot).",
  "Learning Metrics that Maximise Power for Accelerated A/B-TestsKDD 24, August 2529, 2024, Barcelona, Spain": "that do lead to significant outcomes are costly too: by their very def-inition, a portion of user traffic interacts with a sub-optimal systemvariant. As such, we want to maximise the number of decisions wecan make based on the experiments we run, and we want to min-imise the required sample size for statistically significant outcomes.In this work, we achieve this by learning metrics that maximisethe statistical power they harness. We present novel learning ob-jectives for such metrics, and provide a thorough evaluation of theeffectiveness of our proposed approaches. Our learnt metrics arecurrently used for confident, high-velocity decision-making acrossShareChat and Moj business units.We believe our work opens several avenues for future workimproving the efficacy of learnt metrics, by e.g. relaxing the linearityconstraint we rely on. Furthermore, we wish to leverage our learntmetrics as reward signals for personalisation through machinelearning models . Susan Athey, Raj Chetty, Guido W Imbens, and Hyunseung Kang. 2019. TheSurrogate Index: Combining Short-Term Proxies to Estimate Long-Term TreatmentEffects More Rapidly and Precisely. Working Paper 26463. National Bureau ofEconomic Research. Shubham Baweja, Neeti Pokharna, Aleksei Ustimenko, and Olivier Jeunen. 2024.Variance Reduction in Ratio Metrics for Efficient Online Experiments. In Proc. ofthe 46th European Conference on Information Retrieval (ECIR 24). Springer. Roman Budylin, Alexey Drutsa, Ilya Katsev, and Valeriya Tsoy. 2018. ConsistentTransformation of Ratio Metrics for Efficient Online Controlled Experiments.In Proc. of the Eleventh ACM International Conference on Web Search and DataMining (WSDM 18). ACM, 5563. Olivier Chapelle, Thorsten Joachims, Filip Radlinski, and Yisong Yue. 2012. Large-Scale Validation and Analysis of Interleaved Search Evaluation. ACM Trans. Inf.Syst. 30, 1, Article 6 (mar 2012), 41 pages. Ed H. Chi. 2020. From Missing Data to Boltzmann Distributions and TimeDynamics: The Statistical Physics of Recommendation. In Proc. of the 13th In-ternational Conference on Web Search and Data Mining (WSDM 20). ACM, 12.",
  "Power Increase from Learnt Metrics (RQ34)": "Until now, we have leveraged experiments with known outcomesto assess sensitivity and agreement with the North Star. Now, weadditionally consider A/A-experiments (E) and experiments withunknown outcomes (E?) to measure type-I and type-II error respec-tively. We measure this for the North Star, for the best-performingproxy metric that serves as input to the learnt metrics, and for learntmetrics that exhibit no empirical disagreement with the North Star.We plot the type-I error (i.e. fraction of A/A-pairs in E that arestatistically significant at significance level ) and the type-II error(i.e. fraction of A/B-pairs in {E+ E?} that are statistically insignifi-cant at significance level ) for varying values of in a. Weobserve that we are able to significantly reduce type-II errors com-pared to the North Star (up to 78%), whilst keeping type-I errors atthe required level (i.e. ). However, we also observe that the type-IIerror we obtain when using learnt metrics does not significantlyimprove over the top proxy metric, when considered in isolation.Nonetheless, this is not how evaluation metrics are used in prac-tice. Indeed, we track several metrics and can draw conclusions ifany of them are statistically significant. As such, metrics should beevaluated on their complementary sensitivity. That is, we compute-values for a set of metrics, apply a Bonferroni correction, andassess statistical significance. The statistical power that we obtainthrough this procedure is visualised in b. We consider eitherthe North Star in isolation, the North Star in conjunction with thetop-proxy, or a further combination with any learnt metric. Here,we observe that the learnt metric provides a substantial increase instatistical power: statistical power (i.e. 1 type-II error) is increasedby up to a relative 210% compared to the North Star alone, and2530% over the North Star plus proxies. Furthermore, as the Bon-ferroni correction is slightly conservative, we observe lower thanexpected type-I error for higher significance levels . This impliesthat a more fine-grained multiple testing correction can furtherimprove statistical power. We empirically observe that this worksas expected, but its effects are negligible in practice.",
  "Relative Sample Size Decrease for Sets of Metrics (afer Bonferroni)": ": When considering learnt metrics in conjunctionwith the North Star and top proxy metrics, we require signif-icantly reduced sample sizes to obtain the same statisticalsignificance level as we would get from the North Star.4.4Cost Reduction from Learnt Metrics (RQ5) So far, we have shown that metrics learnt to minimise (log)-valuesare effective at improving sensitivity (), whilst minimisingtype-III error () and improving statistical power ().On one hand, powerful learnt metrics can lead to more con-fident decisions from statistically significant A/B-test outcomes.Another view is that we could make the same amount of decisionsbased on fewer data points, as we reach statistical significance withsmaller sample sizes. This implies a cost reduction, as we can runexperiments either on smaller portions of user traffic or for shorterperiods of time, directly impacting experimentation velocity.This reduction in required sample size is equal to the square ofthe relative -score . We visualise this quantity in ,for varying significance levels , for the same Bonferroni-correctedprocedure as b. To obtain a -score for a set of metrics, wesimply take the maximal score and apply a Bonferroni correctionto it as laid out in .1.1. Note that this procedure dependson , explaining the slope in . We observe that our learntmetrics can achieve the same level of statistical confidence as theNorth Star with up to 8 times fewer samples, i.e. a reduction downto 12.5%. This significantly reduces the cost of experimentation forthe business, further strengthening the case for our learnt metrics.",
  "Spherical Regularisation (RQ6)": "Our goal is to assess and quantify the effects of the proposed spheri-cal regularisation method in .2. We train models on all avail-able data with known outcomes E+, where we have a vetted prefer-ence over variants . We consider three weight initialisationstrategies to set init, and normalise weights to ensure Linit = 0:(i) good initialisation at init =1",
  "stant initialisation at the all-one vector init = 1, and (iii) badinitialisation at init =1": "| E+|(,)E+ . We train mod-els for all learning objectives we deal with in this paper: -scores,-values, and log-values; whilst varying the strength of the spher-ical regularisation term . As discussed, this term does not affectthe optima, but simply transforms the loss to be more amenable togradient-based optimisation methods. Thus, we expect convergenceafter fewer training iterations. All models are trained until conver-gence with the adam optimiser , initialising the learning rateat 5e4 and halving it every 1 000 steps where we do not observeimprovements. We use the radam variant to avoid convergence",
  ": Spherical regularisation significantly accelerates convergence for all considered objectives, up to 40%": "issues , and have validated that this choice does not sig-nificantly alter our obtained results and conclusions. We considera model converged if there are no improvements to the learningobjective after 10 000 steps. All methods are implemented usingPython3.9 and PyTorch . visualises the evolution of the learning objective overoptimisation steps, for all mentioned learning objectives, initialisa-tion strategies, and regularisation strengths. We observe that themethod is robust, significantly improving convergence speed for allsettings, requiring up to 40% fewer iterations until convergence isreached. This positively influences the practical utility of the learntmetric pipeline for researchers and practitioners.We provide source code to reproduce and our regularisa-tion method at github.com/olivierjeunen/learnt-metrics-kdd-2024.",
  "Insights from Learnt Metrics": "In this Section, we briefly discuss insights that arose through ourempirical evaluation of all metrics: the North Star, classical sur-rogates and proxies, as well as learnt metrics. These insights arespecific to our platforms, but we believe they can contribute to ageneral intuition and understanding of metrics for online contentplatforms and broader application areas. Ratio metrics are easily fooled. Often, important metrics can beframed as a ratio of the means (or sums) of two existing met-rics . Examples include click-through rate (i.e. clicks / im-pressions), variants of user retention (i.e. retained users / activeusers), or general engagement ratios (e.g. likes / video-plays). Weobserve that, whilst these metrics can be important from a businessperspective, they typically exhibit significant type-III/S errors w.r.t.the North Star. Indeed, in the examples above both the numeratorand denominator represent positive signals we wish to increase.Suppose an online experiment increases the number of video-playsby %, and the overall number of likes by % < %. These two positive signals will lead to a decreasing ratio, whilst we are likelyto still prefer the treatment w.r.t. the North Star if and aresubstantially large. Similar observations cautioning the use of ratiometrics have been made by Dmitriev et al. .We believe this is connected to common offline ranking evalua-tion metrics prevalent in the recommender systems field .Indeed, such metrics are cumulative in nature, optimising overallvalue instead of some notion of value-per-item . User-level aggregations conquer general counters. In the previousexample, we describe general count metrics for the number of likesand the number of video-play events. User behaviour on onlineplatforms often follows a power-law distribution: a few powerusers generate the majority of such events . As a result, suchmetrics are easily skewed, and they are not guaranteed to accuratelyreflect improvements for the full population of usersempiricallyleading to type-III/S errors w.r.t. the North Star. Aggregating suchcounters per users (e.g. count the number of days a user has at least video-plays) instead of using raw event counters, provides strongand sensitive proxies to the North Star.Interestingly, this framing is reminiscent of recall, as we effec-tively measure the coverage of users about whom we have positivesignals. Recall metrics are again strongly connected to offline eval-uation practices in recommender systems, especially in the firststage of two-stage systems common in industry .",
  "Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep Neural Networks forYouTube Recommendations. In Proc. of the 10th ACM Conference on RecommenderSystems (RecSys 16). ACM, 191198": "Alex Deng and Xiaolin Shi. 2016. Data-Driven Metric Development for OnlineControlled Experiments: Seven Lessons Learned. In Proc. of the 22nd ACM SIGKDDInternational Conference on Knowledge Discovery and Data Mining (KDD 16).ACM, 7786. Alex Deng, Ya Xu, Ron Kohavi, and Toby Walker. 2013. Improving the Sensitivityof Online Controlled Experiments by Utilizing Pre-Experiment Data. In Proc. ofthe Sixth ACM International Conference on Web Search and Data Mining (WSDM13). ACM, 123132. Pavel Dmitriev, Somit Gupta, Dong Woo Kim, and Garnet Vaz. 2017. A DirtyDozen: Twelve Common Metric Interpretation Pitfalls in Online ControlledExperiments. In Proceedings of the 23rd ACM SIGKDD International Conferenceon Knowledge Discovery and Data Mining (KDD 17). ACM, 14271436.",
  "Andrew Gelman and John Carlin. 2014. Beyond Power Calculations: AssessingType S (Sign) and Type M (Magnitude) Errors. Perspectives on PsychologicalScience 9, 6 (2014), 641651. PMID:26186114": "Graham Van Goffrier, Lucas Maystre, and Ciarn Mark Gilligan-Lee. 2023. Es-timating long-term causal effects from short-term experiments and long-termobservational data with unobserved confounding. In Proc. of the Second Con-ference on Causal Learning and Reasoning (Proc. of Machine Learning Research,Vol. 213), Mihaela van der Schaar, Cheng Zhang, and Dominik Janzing (Eds.).PMLR, 791813. Yongyi Guo, Dominic Coey, Mikael Konutgan, Wenting Li, Chris Schoener, andMatt Goldman. 2021. Machine Learning for Variance Reduction in Online Ex-periments. In Advances in Neural Information Processing Systems, Vol. 34. CurranAssociates, Inc., 86378648.",
  "Frederick Mosteller. 1948. A k-Sample Slippage Test for an Extreme Population.The Annals of Mathematical Statistics 19, 1 (1948), 5865": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, GregoryChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Des-maison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, AlykhanTejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and SoumithChintala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learn-ing Library. In Advances in Neural Information Processing Systems, H. Wallach,H. Larochelle, A. Beygelzimer, F. d'Alch-Buc, E. Fox, and R. Garnett (Eds.), Vol. 32.Curran Associates, Inc. Alexey Poyarkov, Alexey Drutsa, Andrey Khalyavin, Gleb Gusev, and PavelSerdyukov. 2016. Boosted Decision Tree Regression Adjustment for VarianceReduction in Online Controlled Experiments. In Proc. of the 22nd ACM SIGKDDInternational Conference on Knowledge Discovery and Data Mining (KDD 16).ACM, 235244.",
  "arXiv:2309.07893 [stat.ME]": "Julin Urbano, Harlley Lima, and Alan Hanjalic. 2019. Statistical SignificanceTesting in Information Retrieval: An Empirical Analysis of Type I, Type II andType III Errors. In Proc. of the 42nd International ACM SIGIR Conference on Researchand Development in Information Retrieval (SIGIR19). ACM, 505514. Aleksei Ustimenko and Liudmila Prokhorenkova. 2020. StochasticRank: GlobalOptimization of Scale-Free Discrete Functions. In Proc. of the 37th InternationalConference on Machine Learning (ICML 20, Vol. 119). PMLR, 96699679.",
  "Bernard Lewis WELCH. 1947. The Generalization of Students Problem whenSeveral Different Population Variances are Involved. Biometrika 34, 1-2 (01 1947),2835": "Huizhi Xie and Juliette Aurisset. 2016. Improving the Sensitivity of OnlineControlled Experiments: Case Studies at Netflix. In Proc. of the 22nd ACM SIGKDDInternational Conference on Knowledge Discovery and Data Mining (KDD 16).ACM, 645654. Yisong Yue, Yue Gao, Oliver Chapelle, Ya Zhang, and Thorsten Joachims. 2010.Learning More Powerful Test Statistics for Click-Based Retrieval Evaluation. InProc. of the 33rd International ACM SIGIR Conference on Research and Developmentin Information Retrieval (SIGIR 10). ACM, 507514.",
  "AAdditional Experimental Results": "To further empirically validate our theoretical insights w.r.t. theproposed methods, we repeat the experiments reported in on data collected for the Moj platform, and reproduce Figures 35.Results are visualised in . Observations match our expecta-tions, further strengthening trust in the replicability of our results.All improvements in sensitivity and statistical power are a similarorder of magnitude as those for ShareChat: learnt metrics thatminimise (log)-values can substantially reduce type-II/III errorswithout affecting type-I errors. We observe an improvement overShareChat data in d: learnt metrics for Moj exhibit a 12-fold reduction in the sample size that is required to attain constantstatistical confidence as to the North Star."
}