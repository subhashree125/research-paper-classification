{
  "ABSTRACT": "Deep learning-based RGB caries detection improves the efficiencyof caries identification and is crucial for preventing oral diseases.The performance of deep learning models depends on high-qualitydata and requires substantial training resources, making efficientdeployment challenging. Core data selection, by eliminating low-quality and confusing data, aims to enhance training efficiencywithout significantly compromising model performance. However,distance-based data selection methods struggle to distinguish de-pendencies among high-dimensional caries data. To address thisissue, we propose a Core Data Selection Method with Jensen-Shannon Divergence (JSCDS) for efficient caries image learningand caries classification. We describe the core data selection cri-terion as the distribution of samples in different classes. JSCDScalculates the cluster centers by sample embedding representationin the caries classification network and utilizes Jensen-Shannon Di-vergence to compute the mutual information between data samplesand cluster centers, capturing nonlinear dependencies among high-dimensional data. The average mutual information is calculated tofit the above distribution, serving as the criterion for constructingthe core set for model training. Extensive experiments on RGBcaries datasets show that JSCDS outperforms other data selectionmethods in prediction performance and time consumption. Notably,JSCDS exceeds the performance of the full dataset model with only",
  "The authors contributed equally to this research.The corresponding author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from acronym XX, June 0305, 2018, Woodstock, NY 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-1-4503-XXXX-X/18/06",
  "Caries Classification, Core Dataset, Jason-Shannon Divergence,Image Processing, Deep Learning": "ACM Reference Format:Peiliang Zhang, Yujia Tong, Chenghu Du, Chao Che, and Yongjun Zhu.2018. JSCDS: A Core Data Selection Method with Jason-Shannon Diver-gence for Caries RGB Images-Efficient Learning. In Proceedings of Makesure to enter the correct conference title from your rights confirmation emai(Conference acronym XX). ACM, New York, NY, USA, 5 pages.",
  "INTRODUCTION": "Dental caries is one of the chronic diseases affecting the health ofchildren and adolescents. If not treated promptly, it can lead to com-plications such as pulpitis and periapical periodontitis, adverselyaffecting the development and quality of life of adolescents .According to incomplete statistics, approximately 50% of childrenaged 6-11 have caries in their primary teeth, and over 34% of ado-lescents aged 12-19 have caries in their permanent teeth globally. This underscores the importance of early detection and treat-ment of dental caries in maintaining the oral health of children andadolescents. Currently, the most widely used methods for cariesdetection are visual inspection, probing, and X-ray examination.However, these methods depend heavily on the professional ex-pertise of dentists and increase the economic burden on patients.Therefore, efficient, accurate, and affordable caries detection meth-ods are urgently needed.Previous Work: According to different detection methods, cariesdetection can be categorized into physical examination, auxiliary ex-amination, and automated detection . Physical examinationrefers to dentists visually inspecting the tooth surface for changes",
  ": The motivation statement for JSCDS": "in color and shape to identify caries, with common methods includ-ing visual inspection and probing. While physical examinationsare cost-effective, they rely heavily on the dentists expertise, havea higher misdiagnosis rate, and require patients to visit the clinic,which is time-consuming and labor-intensive. Auxiliary examina-tions utilize modern medical equipment to scan or chemically testteeth for caries detection . Methods such as X-ray exami-nation, laser fluorescence detection, electrical impedance measure-ment, and optical coherence tomography fall under this category.Although auxiliary examinations are more accurate compared tophysical examinations, they are also significantly more expensive.Automated detection involves using automated tools to captureRGB images of teeth and intelligently determine the presence ofcaries . With the advancement of Deep Learning, various cariesrecognition and detection models have been developed, making au-tomated caries detection increasingly mainstream . Comparedto the other two methods, deep learning-based automated cariesdetection offers significant advantages in terms of portability, highaccuracy, and low cost.Limitation and Motivation: Deep learning methods based onRGB images have significantly improved the efficiency and accu-racy of caries detection while reducing its costs . However, deeplearning methods require high-quality and large training datasets.Low-quality, confusing data in the training dataset can severelyaffect the models performance. In caries detection, patients primar-ily use mobile devices to capture RGB images of their teeth. Duringthis process, due to the oral cavitys unique structure and the pho-tographers varying skill levels, the captured images often containa certain amount of unclear, mixed-boundary confusing data. Thesedata can severely impact the performance of the detection model. Data core set selection is one effective way to address thisissue. By quantifying the impact of different data on the model,core data selection identifies key data and reduces the influence ofconfusing data, thereby enhancing the performance and efficiencyof model training . As shown in (A), the Moderatemethod achieved superior performance and faster computational ef-ficiency using 70% of the core dataset, with similar results observedin (B). Therefore, how to measure the impact of differentcaries data on the detection model and selecting the core datasetaccordingly is crucial for improving caries detection performanceand computational efficiency.To address the aforementioned problems, we propose a core dataselection method with Jensen-Shannon Divergence (JSCDS) forefficient caries image learning and caries classification. Specifically, JSCDS calculates the cluster centers of different classes with sam-ple embeddings in the caries classification network. The core dataselection criterion is described as the distribution of samples withindifferent classes. We utilize Jensen-Shannon Divergence (JSD) tocompute the mutual information between data samples and clus-ter centers and use average mutual information (AvgMI) as thedata selection criterion. JSCDS selects samples close to the AvgMIas the core set for model training. Unlike distance-based data im-portance selection methods, JSCDS captures mutual informationgain between samples, enhancing the handling of high-dimensionaldata. We conduct extensive experiments using MobileNetV2 andResNet18, and the results demonstrate that JSCDS has lower timecosts in data selection and exceeds the performance of the fulldataset model using only 50% of the core data.Our main contributions are as follows: Different from distance-based data selection methods, wefocus on information theory-based data selection to iden-tify high-dimensional nonlinear dependencies in samples.We propose the concept of AvgMI to generate the core set,effectively distinguishing between different caries. As a proof of concept, we design a mutual information calcu-lation method with Jensen-Shannon Divergence. This methoddoes not depend on model structure and selects core dataonly with the neural networks embedding. The results demonstrate that JSCDS significantly outper-forms others in prediction performance and time consump-tion. Notably, JSCDS exceeds the performance of the fulldataset model using only 50% of the core data.",
  "METHODOLOGY2.1Definition": "We define the core data selection problem in caries classification. Fora given full dataset of caries training data M = {1,2, , },which contains caries images. Any caries image (F, T),where T {T |T |T } represents the images label. Weaim to design a data selection strategy Y() to construct a subsetM = {1,2, ,| } to remove confusing data, such as",
  "To construct the core set for caries, we propose JSCDS with Jensen-Shannon Divergence. The workflow is shown in . JSCDSprimarily consists of two parts: data representation and data selec-tion": "2.2.1Data Representation and Cluster Center Calculation. For theneural network model for caries classification, it can be viewed as ablack-box feature extractor. For the caries image F, the probabilitydistribution P(Z, F) of the image sample can be obtained throughthe mapping Z() = H (()). Here, () represents the finalembedding representation of the input caries image in the hiddenlayer, and H () is the feature mapping function represented by",
  ": The workflow of JSCDS. JSCDS calculates cluster centers based on neural network embeddings representation, andthen combines the AvgMI of data samples to generate the core set for model training": "(). The hidden layer embedding representation of thetraining data can be obtained through the above calculation,and from this, the hidden embedding representation of all trainingdata M can be derived, as shown in Equation (1). We calculatethe cluster center C for each data class based on the hidden layerrepresentations by Equation (2).",
  "C ==1 I[ = ] =1 I[ = ], = 1, 2, , (2)": "Here, the numerator represents the sum of the sample embeddingsin class T, and the denominator represents the number of samplesin class T. The cluster center C is the reference for subsequentdata selection processes. 2.2.2Data Selection with JSD. Relevant research primarily mea-sures the relevance of samples by calculating the distance betweenthem . While this method is computationally efficient, itheavily depends on the data size . Identifying high-dimensionalnonlinear dependencies in the training data is challenging, makingit difficult to distinguish between samples. In contrast, informa-tion theory-based methods can capture nonlinear relationshipsbetween samples, enabling effective high-dimensional data process-ing. Therefore, JSCDS designs a JSD-based data sample importanceselection strategy from the information theory perspective.Based on the hidden layer representations {1,2, , } of thetraining data and the cluster centers {C1, C2, , C } of the classes,the mutual information (, C) between each image embeddingrepresentation and its corresponding cluster center is calculated.(, C) is obtained by computing the JSD:",
  "(3)": "where and C represent the data samples and the class clustercenters, respectively. The JSD is computed by combining KL diver-gences, and denotes the embedding dimension of .The set of mutual information between each sample in the train-ing set and its class cluster center is denoted as (1), , (),sorted in descending order to obtain { ( 1), , ( )}. Weuse the average mutual information ( ()) as the metric,selecting samples close to ( ()) as the core set .",
  "( )}": "}(5)Here, represents the selection ratio of data in the core set (Toensure clarity in our descriptions, we describe as \"Fraction\" inthe experimental section.), and { , , } is the core set afterselection, which is used for model training.In information theory, the more considerable mutual informationbetween a data sample and the cluster center indicates the higherposterior probability given by the neural network for the sample.However, obtaining embedding representations in neural networksis easier than computing posterior probabilities . Therefore, ourmethod is more practical in real-world applications and requiresfewer resources, thereby improving the computational efficiencyof deep learning.",
  "EXPERIMENTAL3.1Datasets": "We used the RGB dental caries dataset labeled and processed byprofessional dentists. This dataset has been widely used in cariesobject detection and classification . It contains 5619 cariesimages, each in 24-bit JPG format. Professional dentists classifiedthe caries images into three categories: mild, moderate, and severecaries, based on the extent and severity of the lesions. Followingthe settings in previous related studies , we divided thedataset into training, validation, and test set in the ratio of 8:1:1.",
  "Experimental Setup": "3.2.1Evaluation Metrics. We constructed caries fine-grained clas-sification to verify the data selection capability of JSCDS. In thecaries classification task, Accuracy (ACC), Precision (Pre), Recall(Rec), F1-score (F1), Specificity (SPE), AUPR, and AUROC are com-monly used to evaluate model performance . AUPR andAUROC are closely related to evaluation metrics such as Pre andRec. Due to page limitations, we use ACC, Pre, Rec, F1, and SPE toevaluate model performance in this work. 3.2.2Training Details. We designed and implemented JSCDS basedon Python and PyTorch, and conducted training and testing ona server equipped with two NVIDIA GeForce RTX 4090 GPUs(each with 24GB RAM), Ubuntu operating system and an Intel(R)Core(TM) i9-12900KF CPU. The backbone networks for the experi-ments were MobileNetV2 and ResNet18 . We initialize themodel parameters using the model that has been pre-trained onthe ImageNet-1K dataset. The training parameters for JSCDS are asfollows: we train the network for 50 epochs using a learning rate of0.001, no weight decay, batch size of 64, and Adam optimizer. Every10 epochs,the core set is reselected.",
  "We compared the performance of JSCDS with four core set selectionmethods. The primary comparison methods include: random dataselection, Moderate , kCenterGreedy , and Forgetting": "To comprehensively analyze the performance of JSCDS, we con-ducted extensive experiments on the dental caries classificationdataset. The experimental results are shown in . The param-eter settings of the comparison methods all followed the principleof optimal performance in our experimental equipment. 3.3.1Comprehensive Performance Analysis. As shown in , our proposed JSCDS achieves the best fine-grained caries pre-diction results in different fractions. In predictive performance,JSCDSs performance on 50% of the core set is already close toor even exceeds the predictive results using the full dataset. Atthe 70% fraction, its predictive performance significantly surpassesthat of the full dataset, as highlighted by the red data in . further visually confirms these findings. Regarding timeoverhead, JSCDS effectively reduces the training time, substantiallyenhancing training efficiency. In summary, JSCDS effectively selectshigh-quality data from the original dataset, improving the modelsclassification performance. The main reason is that JSCDS calcu-lates the AvgMI with JSD and uses it to select the core set, capturinghigh-dimensional nonlinear dependencies among samples. 3.3.2The Performance Analysis of Different Core Set Selection Meth-ods. Compared to Moderate, kCenterGreedy, and Forgetting, JSCDSachieves the best comprehensive predictive performance in differ-ent backbone networks. Although the training time for JSCDS ismarginally longer than that for Moderate, the substantial improve-ment in performance makes this additional time investment accept-able. Moderates shorter training time can be attributed to its use of",
  "JSCDS: A Core Data Selection Method with Jason-Shannon Divergence for Caries RGB Images-Efficient LearningConference acronym XX, June 0305, 2018, Woodstock, NY": "Euclidean distance for data selection, which is computationally sim-pler. However, as the volume of selected data increases, Moderatescomputational speed significantly decreases. This experimentationresult aligns with the general knowledge that Euclidean distanceincurs higher computational overhead in large-scale datasets. Incontrast, JSCDS maintains its advantages in predictive performanceand time overhead as the data scale grows. kCenterGreedy andForgetting fail to achieve satisfactory predictive performance andtraining time, possibly due to the limited number of caries dataclasses. Moderate and JSCDS yield commendable results regardingtraining time overhead, significantly reducing the training time. 3.3.3The Performance Analysis in Different Fractions. As shown in, all models achieve optimal predictive performance whenthe fraction is 50% or 70%, with minimal performance differencesfrom the full dataset. JSCDS and Moderate even surpass the fulldatasets performance at 70% fraction, indicating their effectivenessin core data selection. As the fraction increases from 30% to 70%,the training time for JSCDS only increases by 21% and 19.5% inMobilenetV2 and Resnet18, respectively. This demonstrates thatJSCDS is efficient in data selection, with its time consumption notproportionally increasing with larger fractions. In contrast, kCen-terGreedy and Forgetting have consistently high training times,and Moderates performance in this aspect is slightly inferior toJSCDS. This result further underscores JSCDSs efficiency. The re-sults in indicate that JSCDS achieves the best performanceat a 70% fraction, with its predictive performance in both backbonenetworks showing a near-normal distribution trend. This suggeststhat the core set size significantly impacts model performance. Thecore set that is too large may include some noisy data, while a coreset that is too small lacks sufficient high-quality data, both of whichcan degrade the models predictive performance.",
  "CONCLUSION": "In this paper, we designed a core set selection method with Jensen-Shannon Divergence. By capturing high-dimensional dependenciesin caries images, we construct a high-quality core set for modeltraining to improve the models predictive performance. Extensiveexperiments on RGB caries datasets show that JSCDS outperformsother data selection methods in prediction performance and timeconsumption. Notably, JSCDS exceeds the performance of the fulldataset model with only 50% of the core data, with its performanceadvantage becoming more pronounced in the 70% of core data.",
  "Jinlin Ma, Ke Ouyang, Ziping Ma, Mingge Xia, Silong Xu, and Ke Lu. 2024.Transformer dense center network for liver tumor detection. Biomedical SignalProcessing and Control 91 (2024), 106066": "Mansheej Paul, Surya Ganguli, and Gintare Karolina Dziugaite. 2021. Deeplearning on a data diet: Finding important examples early in training. Advancesin Neural Information Processing Systems 34 (2021), 2059620607. Javier Prez de Frutos, Ragnhild Holden Helland, Shreya Desai, Line CathrineNymoen, Thomas Lang, Theodor Remman, and Abhijit Sen. 2024. AI-Dentify:deep learning for proximal caries detection on bitewing x-ray-HUNT4 OralHealth Study. BMC Oral Health 24, 1 (2024), 344. Nigel B Pitts, Domenick T Zero, Phil D Marsh, Kim Ekstrand, Jane A Weintraub,Francisco Ramos-Gomez, Junji Tagami, Svante Twetman, Georgios Tsakos, andAmid Ismail. 2017. Dental caries. Nature reviews Disease primers 3, 1 (2017), 116. Omead Pooladzandi, David Davini, and Baharan Mirzasoleiman. 2022. Adap-tive second order coresets for data-efficient machine learning. In InternationalConference on Machine Learning. PMLR, 1784817869. J Priya, S Kanaga Suba Raja, and S Sudha. 2024. An intellectual caries seg-mentation and classification using modified optimization-assisted transformerdenseUnet++ and ViT-based multiscale residual denseNet with GRU. Signal,Image and Video Processing (2024), 115. Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. 2018. Mobilenetv2: Inverted residuals and linear bottlenecks. InProceedings of the IEEE conference on computer vision and pattern recognition.45104520.",
  "Ozan Sener and Silvio Savarese. 2017. Active learning for convolutional neuralnetworks: A core-set approach. arXiv preprint arXiv (2017)": "Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler,Yoshua Bengio, and Geoffrey J Gordon. 2018. An empirical study of exampleforgetting during deep neural network learning. arXiv preprint arXiv (2018). Xianyun Wang, Sizhe Gao, Kaisheng Jiang, Huicong Zhang, Linhong Wang, FengChen, Jun Yu, and Fan Yang. 2023. Multi-level uncertainty aware learning forsemi-supervised dental panoramic caries segmentation. Neurocomputing 540(2023), 126208. Xiaobo Xia, Jiale Liu, Jun Yu, Xu Shen, Bo Han, and Tongliang Liu. 2022. Moderatecoreset: A universal method of data selection for real-world data-efficient deeplearning. In The Eleventh International Conference on Learning Representations. Peiliang Zhang, Chao Che, Bo Jin, Jingling Yuan, Ruixin Li, and Yongjun Zhu.2024. NCH-DDA: Neighborhood contrastive learning heterogeneous networkfor drugdisease association prediction. Expert Systems with Applications 238(2024), 121855. Peiliang Zhang, Jiatao Chen, Chao Che, Liang Zhang, Bo Jin, and Yongjun Zhu.2023. IEA-GNN: Anchor-aware graph neural network fused with informationentropy for node classification and link prediction. Information Sciences 634(2023), 665676. Peiliang Zhang, Yuanjie Liu, and Zhishu Shen. 2023.KSGTN-DDI: KeySubstructure-aware Graph Transformer Network for Drug-drug Interaction Pre-diction. In 2023 IEEE International Conference on Bioinformatics and Biomedicine(BIBM). IEEE, 974977."
}