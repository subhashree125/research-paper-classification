{
  "ABSTRACT": "Approaches to recommendation are typically evaluated in one oftwo ways: (1) via a (simulated) online experiment, often seen as thegold standard, or (2) via some offline evaluation procedure, wherethe goal is to approximate the outcome of an online experiment. Sev-eral offline evaluation metrics have been adopted in the literature,inspired by ranking metrics prevalent in the field of InformationRetrieval. (Normalised) Discounted Cumulative Gain (nDCG) is onesuch metric that has seen widespread adoption in empirical studies,and higher (n)DCG values have been used to present new methodsas the state-of-the-art in top- recommendation for many years.Our work takes a critical look at this approach, and investigateswhen we can expect such metrics to approximate the gold standardoutcome of an online experiment. We formally present the assump-tions that are necessary to consider DCG an unbiased estimator ofonline reward and provide a derivation for this metric from firstprinciples, highlighting where we deviate from its traditional usesin IR. Importantly, we show that normalising the metric rendersit inconsistent, in that even when DCG is unbiased, ranking com-peting methods by their normalised DCG can invert their relativeorder. Through a correlation analysis between off- and on-line ex-periments conducted on a large-scale recommendation platform,we show that our unbiased DCG estimates strongly correlate withonline reward, even when some of the metrics inherent assump-tions are violated. This statement no longer holds for its normalisedvariant, suggesting that nDCGs practical utility may be limited.",
  "Offline Evaluation; Off-Policy Evaluation; Counterfactual Inference": "ACM Reference Format:Olivier Jeunen, Ivan Potapov, and Aleksei Ustimenko. 2024. On (Normalised)Discounted Cumulative Gain as an Off-Policy Evaluation Metric for Top- Recommendation. In Proceedings of the 30th ACM SIGKDD Conferenceon Knowledge Discovery and Data Mining (KDD 24), August 2529, 2024,Barcelona, Spain. ACM, New York, NY, USA, 12 pages. KDD 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.This is the authors version of the work. It is posted here for your personal use. Notfor redistribution. The definitive Version of Record was published in Proceedings ofthe 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 24),August 2529, 2024, Barcelona, Spain,",
  "What content do we recommend to whom?": "Research and applications in the field of recommender systemshave undergone a large shift in the last few decades, moving fromrating to item prediction and, more recently, embracing aninterventionist view . Throughout this evolution, the evaluationpractices that are commonly adopted in the field have undergone aparallel shift. Indeed, whereas the continuous and explicit natureof ratings lends itself to the Root Mean Square Error (RMSE) met-ric; discrete item predictions are more often viewed in a rankingsetup where Information Retrieval (IR) metrics like Precision, Re-call, and (normalised) Discounted Cumulative Gain (nDCG) havebeen widely adopted ; and the interventionist view lends it-self particularly well to counterfactual and off-policy estimationtechniques that can be directly mapped to online metrics .Despite the fact that evaluation methods are a core topic inrecommender systems research that enjoy a significant amount ofinterest, some problems remain that are fundamentally hard to solve.Online experiments (i.e. randomised controlled trials or A/B-tests)are seen as the gold standard of evaluation practices, and deservedlyso: by leveraging interactions with users, they allow us to directlymeasure an array of online metrics for a given recommendationmodel . Nevertheless, as they are costly to conduct and theacademic research community seldom has access to platforms withreal users , offline evaluation practices are a common alternativeused to showcase newly proposed methods performance, bothin the research literature and in industry applications (often as aprecursor to an online experiment).Even though the need for offline evaluation methods that mimicthe outcome of an online experiment is clear, the reality is thatexisting methods seldom do so satisfactorily , even ifadvances in counterfactual estimation techniques have recently ledto several success stories . The reasons for this offline-onlinemismatch are manifold and some can be attributed due to offlineevaluation inconsistencies (even before we compare them to onlineresults). First, there is a fundamental mismatch between manynext-item prediction metrics (e.g. recall) and online metrics (e.g.click-through rate). Although this can be partially alleviated by theinterventionist lens , a majority of published research remainsfocused on IR-inspired metrics. Second, a myriad of evaluationoptions can lead to contrasting results , and several offlinemetrics differ in robustness and discriminative power . Third,sampled versions of these metrics have been adopted for efficiencyreasons, but are inconsistent with their unsampled counterparts andshould thus be avoided . Fourth, multiple recent works",
  "KDD 24, August 2529, 2024, Barcelona, SpainOlivier Jeunen, Ivan Potapov, and Aleksei Ustimenko": "have reported troubling trends in the reproducibility of widely citedmethods similar issues plagued the adjacent IRfield a decade earlier .This article aims to contribute to this important line of research,by focusing on the widely adopted (normalised) Discounted Cumu-lative Gain metric . Focusing on the purpose that offline metricsserve, we ask a simple question:",
  "The main scientific contributions we present in pursuit of an-swering this question, are the following:": "(1) We formally present the assumptions that are necessaryto consider DCG an unbiased estimator of online reward,providing a derivation for this metric from first principleswhilst linking it to off-policy estimation ( 4). (2) We formally prove that the widespread practice of normalis-ing the DCG metric renders it inconsistent with respect toDCG, in that the ordering given by nDCG can differ fromthat given by DCG, and provide empirical evidence ( 5).",
  "(a) the unbiased DCG metric strongly correlates with onlinemetrics over time, whereas nDCG does not ( 6.1),": "(b) whilst differences in online metrics directionally alignwith differences in both nDCG and DCG, the latter can en-joy improved sensitivity to detect statistically significantonline improvements ( 6.2). (4) We revisit the assumptions that are necessary to considerDCG an unbiased estimator: discussing when they can bereasonable and how we can relax them, giving rise to aresearch agenda and ways forward ( 7).",
  "BACKGROUND & RELATED WORK": "Offline evaluation methods for recommender systems have beenstudied for decades , and their shortcomings are widely reported.Caamares et al. provide an overview of common approaches, high-lighting how different choices (in pre-processing, metrics, splits,...)lead to contrasting results . More problematic, Ji et al. showthat common train-test-split procedures lead to data leakage issuesthat affect conclusions drawn from offline experiments . Otherrecent work shows that sampled versions of evaluation metricsthat only rank a sample of the item catalogue instead of the fullcatalogue, are inconsistent with the full metrics, leading the authorsto explicitly discourage their use . Even when we manageto steer clear from these pitfalls, biases in logged data can give riseto undesirable phenomena like Simpsons paradox . Generalde-biasing procedures have been proposed to this end , as wellas off-policy estimation techniques and methods toevaluate competing estimators .Most traditional ranking evaluation metrics stem from IR. Val-carce et al. find that nDCG offers the best discriminative poweramong them . Findings like this reinforce the communitystrust in nDCG, and it is commonly used to compare novel top-recommendation methods to the state-of-the-art, also in repro-ducibility studies . Ferrante et al. argue that whilenDCG can be preferable because it is bounded and normalised, problems can arise because the metric is not easily transformed toan interval scale . They all do not consider the consistency of(n)DCG.Other recent work highlights that commonly used online evalu-ation metrics relying on experimental data (e.g. click-through rate),differ fundamentally from commonly used offline evaluation met-rics that rely on organic interactions (e.g. hit-rate) . Deffayetet al. argue that a similar mismatch is especially pervasive whenconsidering reinforcement learning methods , and Diaz empha-sises critical issues with interpreting organic implicit feedback asa user preference signal . These works together indicate a riftbetween online and offline experiments.Several open-source simulation environments have been pro-posed as a way to bypass the need for an online ground truth re-sult , and several works have leveraged these simulatorsto empirically validate algorithmic advances in bandit learning forrecommendation . Nevertheless, whether conclu-sions drawn from simulation results accurately reflect those drawnfrom real-world experiments, is still an open research question.In this work, we focus on the core purpose that offline evaluationmetrics serve: to give rise to offline evaluation methodologies thataccurately mimic the outcome of an online experiment. To this end,we focus on the widely used (n)DCG metric, and aim to take a steptowards closing the gap between the off- and online paradigms.",
  "FORMALISING THE PROBLEM SETTING": "Throughout, we represent the domain for a random variable as X and a specific instantiation as , unless explicitly mentionedotherwise. We deal with a session-based feed recommendationsetup, describing a users journey on the platform as a trajectory .Contextual features describing a trajectory are encoded in X,which includes features describing the user U and possiblehistorical interactions they have had with items on the platform. Inline with common notation in the decision-making literature, wewill refer to these items as actions A. As is common in real-world systems, the size of the item catalogue (i.e. the action space|A|) can easily grow to be in the order of hundreds of millions,prohibiting us to score and rank the entire catalogue directly. Thisis typically dealt with through a two-stage ranking setup, where amore lightweight candidate generator stage is followed by a rankingstage that decides the final personalised order in which we presentitems to the user .We adopt generalised probabilistic notation for two-stage rankersin this work, but stress that our insights are model-agnostic anddirectly applicable to single-stage rankers as well.Let A denote all subsets of actions: A 2A : A, | | = . A candidate generation policy G defines a conditionalprobability distribution over such sets of candidate actions, given acontext: G( |) P( |, G). We will use the shorthand nota-tion G() when context allows it. Note that this general notationsubsumes other common scenarios, such as candidate generationpolicies that are deterministic, consist of ensembles, or simply yieldthe entire item catalogue A (i.e. single-stage ranking systems).After obtaining a set of candidate items for context by sampling G(), we pass them on to the ranking stage.",
  "On (Normalised) Discounted Cumulative Gain as an Off-Policy Evaluation Metric for Top- RecommendationKDD 24, August 2529, 2024, Barcelona, Spain": "been proposed to encode concepts of listwise novelty and diversityinto DCG-like formulations for top- recommendations ;we conjecture they can be extended to our unbiased setup as well.4. The examination hypothesis implies that exposure bias (through ) is the main culprit that makes a noisy indicator of . Differ-ences in exposure can then purely come from position bias (as in thePBM), but they can also be perpetuated by selection bias (as madeevident by Eq. 6) . Other sources of bias have been raised inthe literature, such as presentation or trust bias . We expectthat these types of biases can be incorporated into the theoreticalderivation of Sec. 4 to devise DCG-like formulations that remainunbiased estimators of online reward, even when additional biasesare present. Naturally, such biases are use-case-specific.5. Full support of the logging policy. The main assumption thatmakes IPS work, is that no actions with non-zero probability underthe target policy can have zero probability under the logging policy.Indeed, if a context-action pair is known not to be present in thedata, we cannot make any inferences about its reward (with guaran-tees). This is at the heart of policy-based estimation, but especiallyproblematic in real-world systems where the action space is largeand the cost of such full randomisation, even with small probabil-ities, can be high. Recent work in learning from bandit feedbackdeals with such cases empirically and theoretically ,providing a source of inspiration to (partially) alleviate these issues.",
  "AP(| = ,, R)P( = |, G).(1)": "Now, for a given context , we can obtain rankings over actionsby sampling = (1, . . . ,) R(). These rankings are thenpresented to users, who scroll through the ordered list of items andview them along the way. We will not yet restrict our setup to aspecific user model that describes how users interact with rankings,but introduce a binary random variable to indicate whether auser has viewed a given item.1 As such, we will assume that loggedtrajectories only contain items that were viewed by the user (i.e. = 1). That is, if the user abandons the feed after action shownat rank = , we do not log samples for actions , > .Users can not only view items, but they can interact with them inseveral ways. These interaction signals could be seen as the rewardor (relevance) label. Following traditional notation where rewardsare clicks we will denote them by random variable . Nevertheless,these signals are general and can be binary (e.g. likes), real-valued(e.g. revenue), or higher-dimensional to support multiple objectives(e.g. diversity, satisfaction, and fairness ).As users can interact with every item separately, we define = (1, . . . ,) as the logged rewards over all ranks. We donot place any restrictions on the reward distribution yet, so the re-ward at any given rank can be dependent on actions at other ranks:(1, . . . ,) P( |,1, . . . ,). Now, a trajectory consists ofcontextual information, as well as a sequence of user-item inter-actions with observed reward labels: = {, (1,1), . . . (,)},where || = . The true metric of interest that we care about is theexpectation of reward, over contexts sampled from an unknownmarginal distribution P(), candidates sampled from our can-didate generator G(), rankings sampled from our ranker(1, . . . ,) R(,), and rewards sampled from the unknownreward distribution (1, . . . ,) P( | = ,1 = 1, . . . , =). We will denote with the sum of rewards over all observedranks: = | |=1 . Note that this notation generalises typical eval-uation metrics that are used in real-world recommender systems,such as per-item dwell-time or counters of engagement signals.In order to obtain an estimate for our true metric of interest E[],we can perform an online experiment. Indeed, in doing so, we effec-tively sample from the above-mentioned distributions and obtainan empirical estimate of our metric by averaging observed samples.For a dataset D0 containing logged interactions under policies G0and R0, Eq. 2 shows how to obtain this empirical estimate:",
  "=1.(2)": "1Note that this problem setting deviates from traditional work dealing with web searchin IR, where multiple items are shown on screen and item-specific view events cannotbe disentangled trivially. In contrast, our items take up most of the users mobile screenwhen presented, and we are able to deduce accurate item-level view labels from loggedscrolling behaviour. See e.g. Jeunen for work dealing with this setting. As we directly measure the quantity of interest that depends on thedeployed policies G0 and R0, this online estimator is often seen asthe gold standard. Nevertheless, it is costly to obtain. Indeed, as hasbeen widely reported , online experiments require us to:(1) bring hypotheses for new policies up to production standardsfor an initial test, (2) wait several days or weeks to deduce statisticalsignificant improvements, and (3) possibly harm user experiencewhen the policies are performing subpar. Furthermore, severalpitfalls arise when setting up or interpreting results from onlineA/B experiments .For these reasons, we want to be able to perform accurate offlineexperiments. That is, given a dataset of interactions D0 that werelogged under the production policies G0 and R0 (often referred toas logging policies), we want to estimate what the reward wouldhave been if we had deployed a new policy R instead (assume Rincludes sampling candidates from G). The goal at hand is thusto devise some function that takes in a dataset of interactionscollected under the logging policy, and is able to approximate theground truth metric for a target policy R, as shown in Eq. 3:",
  "DISCOUNTED CUMULATIVE GAIN AS ANUNBIASED OFFLINE EVALUATION METRIC": "In reality, this problem can be very complex. Indeed, the rewardthat we obtain from presenting a certain ranking to a user candepend on the entire slate at once (of which there are ! versionsin a top- setting), and will be non-i.i.d. over trajectories (i.e. thereward distribution can depend on a users state, influenced byactions we have taken in the past).As is typical in machine learning research, we require assump-tions that make the problem more tractable. These assumptions areflawed, but they give us a starting point and a strong foundation tobuild upon for future iterations. Note that we will lay out the specificassumptions that are necessary to motivate the use of DiscountedCumulative Gain as an offline evaluation metricwe discuss waysof relaxing these assumptions in .",
  "The reward for a context-action pair (,) in trajectory is inde-pendent of the rankings presented in other trajectories D\\": "Assumption 2 (position-based model ). We follow theposition-based model (PBM) to describe user scrolling behaviour, im-plying that the probability of a user viewing an item is only dependenton its rank and described by P( |). Assumption 3 (reward independence across ranks). Thereward for a context-action pair (,) is independent of other actionsin the user trajectory. Formally, |, . We describethe resulting reward distribution as P( |,, ).",
  "P( |).(4)": "Asm. 1 allows us to avoid reinforcement learning scenarios, andAsm. 2 prohibits cascading behaviour (that would give rise to othermetrics, such as ERR ). Asm. 3 allows us to avoid modellingentire slates as individual actions (leading to a combinatorial ex-plosion of the action space), and through Asm. 4, Eq. 4 estimatesthe unobserved context-dependent quality of a given item fromobservable quantities alone. From Eq. 2, we can now rewrite theexpected reward we obtain under a ranking policy R as:E(1,..., )R()[]",
  "=1P( = 1| = , = ) P( = 1| = ).(5)": "Through the position-based model and the examination hypothesis,items shown at lower ranks are discounted. Because we assumedindependence of rewards across ranks, rewards observed at differentranks are cumulative. A key insight here is that the ranking policyR only affects the rank at which an item is shown, and as such, theexposure probability that is allocated to the item . We formaliseexposure as the expected number of views a target item will obtainunder a given context , for candidate generation and rankingpolicies G, R:EG,R[ | = , = ] =",
  ".(6)": "Note that this general notation accommodates recommendationscenarios where we retrieve and rank candidates but only showthe top- to the user, where > , by simply defining P( =1| = ) = 0 = + 1, . . . ,. By encoding cut-offs directly in theposition bias model, we forgo the need to consider metrics likeDCG@ .Recent work in unbiased learning-to-rank leverages similar ex-posure definitions to jointly combat selection and position biasthrough Inverse Propensity Score (IPS) weighting .",
  "(,) . (8)": "Eq. 8 provides a general unbiased estimator for the reward under(G, R), computed from data collected under (G0, R0).2 For sim-plicity of notation, but without loss of generality, we now restrictourselves to a deterministic ranking policy R and assume G is con-stant (i.e. G G0). With a slight abuse of notation, we briefly denotewith R(,) the rank at which item is placed when policy R is pre-sented with context . In doing so, we observe that the importanceweights in Eq. 8 can be simplified to: (,)",
  "(9)": "Through this derivation, two different views of the DCG metricarise. That is, we either (1) view it as a pure importance samplingestimator that reweights the exposure that is allocated to a certainitem in a certain context, or (2) we view it as a way to de-biasobserved interactions (i.e. estimate from and ), and use theposition-based model (Asm. 2) and the examination hypothesis(Asm. 4) to obtain a final estimate of the cumulative reward.If the assumptions laid out above hold, Eq. 9 provides an unbi-ased estimate of the online reward policy R will incur, based ondata collected under R0; providing a strong motivation for DCG.Even though unbiasedness is an attractive theoretical property, thisestimators variance can become problematic in cases where thelogging and target policies (R0, R) diverge. We can adopt meth-ods that were originally proposed to strike a balance between biasand variance for general IPS-based estimators, such as clipping theweights , self-normalising them , adapting the loggingpolicy , or extending the estimator with a reward model toenable doubly robust estimation . Similarly, when thelogged data that is used for offline evaluation was collected by multi-ple logging policies, ideas from multiple importance sampling are effective at reducing the variance of the final estimator .Saito et al. describe extensions for large action spaces .In the traditional IR use-case of web search, it is often assumedthat we have access to human-annotated relevance labels rel(,)for query-document pairs (,). Such crowdsourced labels are seenas a proxy to P(|,), which makes them understandably at-tractive. Nevertheless, for an offline evaluation metric to be usefulin real-world recommendation systems, access to direct relevancelabels is seldom a realistic requirement. The discount function forDCG that is most often used in practice, makes the assumption that 2Note that, for general two-stage ranking scenarios, this is a novel contribution to theresearch literature in and of itself. Existing work on off-policy corrections in two-stagerecommender systems only considers a top-1 scenario, instead of a ranking policy .",
  "P( = 1| = ) =1": "log2(+1) is a good approximation for empiricalexposure (see, e.g., ). This gives rise to the more widely recog-nisable form of DCG, as =1rel(, )log2(+1) . We note that neither one ofthese additional assumptions is likely to hold in real-world applica-tions, which imply that even if Assumptions 15 hold, this estimatoris biased. The more general form presented in Eq. 9, however, issupported by a theoretical framework that allows for counterfactualevaluation, formally describing settings for its appropriate use.",
  "NORMALISING DCG IS INCONSISTENT": "We have introduced the DCG metric from first principles, and haveshown that under several assumptions, it can be seen as an unbiasedestimator for the reward that a new ranking policy R will obtain.Nevertheless, this metric seldom appears in the research literaturein its unadulterated form. Much more prevalent is normalised DCG(nDCG), which rescales the DCG metric to be at most 1 for anideal ranking. For notational simplicity, we define ground truthquality labels as () =E[| = , = ] A. Then, we cancompute ideal DCG as the DCG obtained under an oracle rankerthat yields R sort(()). In what follows, we first show thatthis is reasonable practice under a single sample (context), in thatit retains a consistent ordering among ranking policies. We thengo on to show that: (1) nDCG yields inconsistent orderings overcompeting policies in expectation when compared to DCG, and(2) defining iDCG is problematic in the realistic setting of partialinformation (i.e. and thus () are unobservable). Lemma 5.1. The Discounted Cumulative Gain (DCG) and Nor-malised Discounted Cumulative Gain (nDCG) metrics yield consistentrelative orders over a competing set of policies that are being eval-uated for a single sample . That is,",
  "arg sortRDCG(, R) arg sortRnDCG(, R), X": "Proof. For any given context X, assume a relative orderexists between two methods R and R for a given non-negativemetric : i.e. (, R) (, R). Define () as the ideal metricvalue, i.e. the metric value that is obtained by the optimal ranking: () (, R), where R = arg maxR (, R) = sort(()).",
  "DCG is, in general, not restricted to be non-negative. However,if we assume that the ideal discounted cumulative gain is non-negative, we have that the above inequality applies for DCGand iDCG": "We believe that this (seemingly trivial) insight has led to thewidespread adoption of nDCG as an offline evaluation metric inthe recommender systems and Learning-to-Rank research fields,as normalised metric values where 1 indicates a perfect modelfacilitate comparisons of methods over different datasets. Indeed,when describing its prevalence in the literature, Ferrante et al. arguethat usually nDCG is preferred over DCG because it is bounded andnormalised . Nevertheless, nDCG does not retain consistent",
  "arg sortRDCG(, R) arg sortRnDCG(, R) in the general case": "Proof. We provide a proof by counterexample, for which thedetails are presented in . Indeed, even though the DCG andnDCG metrics align for every sample in isolation (see columns for1,2), they are inconsistent in aggregate (see columns for ). Discrepancies between (n)DCG have been touched upon in theIR literature, focused on search engine evaluation and blaminga limited number of relevance judgments . shows thatthe issue has deeper roots than this: the normalisation procedureis inconsistent. This insight is problematic, as virtually all offlineevaluation protocols consist of first aggregating evaluation metricsover sets of samples, and then inferring preferences over competingpolicies based on these metric averages. Our work shows that, whenthe assumptions laid out in are met and DCG provides anunbiased estimate of reward, this gives rise to a theoretically soundmodel selection protocol. The same statement does not hold fornDCG as it widely appears in the research literature (see e.g. [88,Eq. 8.9] and [100, Eq. 5]). We hypothesise that the normalisationformula has been widely adopted for its ease-of-use, rather thanfor its theoretical properties. This suggests that nDCG is of limitedpractical use as an offline evaluation metric, and that it should beavoided by researchers and practitioners who wish to use DCG foroffline evaluation and model selection purposes.We provide empirical evidence of discrepancies between (n)DCGon common top- recommendation evaluation tasks on publiclyavailable data in Appendix A.In the odd case where metric values that maximise at 1 are re-quired, we propose the use of a post-normalisation procedure, wherepnDCG(D, R) = DCG(D,R) iDCG(D) . Recent work on LTR in IR leveragesthis nDCG formulation . Indeed, through Lemma 5.1, one cantrivially show that this metric is consistent with respect to DCG.Nevertheless, we wish to advise against this practice altogether,as computing the ideal DCG metric implies that we must constructthe ideal ranking policy R. To do so, we require full knowledge of(), which is hardly realistic in real-world scenarios. In traditionalIR use-cases where human-annotated relevance labels are availableas ground truth, these labels can be used to inform (). In aca-demic recommendation datasets where we have explicit feedbackand full observability of the user-item matrix, this can inform ()similarly. In practical applications, however, we typically estimate() () from logged implicit feedback. Aside from the prob-lems that occur when accurately interpreting this feedback ,such logged datasets are known to be riddled with biases that com-plicate estimating () , resulting in only partial observabilityand noisy estimates that should not be taken at face value to informan optimal ranker.One final argument in favour of nDCG, is that it partially alle-viates the impact of outliers. Indeed, the normalisation procedure",
  "whereE[| = 1, = 1] = 1.0,E[| = 1, = 2] = 0.0,E[| = 2, = 1] = 1.0,E[| = 2, = 2] = 2.5,X = {1,2}": ": A proof by example that, while rankings inferred from the DCG and nDCG metrics are consistent for a single sample,they can be inconsistent when aggregated over multiple samples (i.e. DCG(, R) > DCG(, R) nDCG(, R) > nDCG(, R)). rescales the contribution of every sample, which can be preferablein cases where strong outliers are present. Nevertheless, in suchscenarios, we would propose to first devise a more appropriateonline metric than the average cumulative reward, and then derivean offline estimator for this quantity, rather than trying to repur-pose the existing DCG estimator. Exactly what such metrics andestimators would look like, is an interesting area for future work.",
  "EXPERIMENTAL RESULTS & DISCUSSION": "Until now, we have derived the theoretical conditions that are nec-essary to consider DCG an unbiased estimator of online reward, andwe have highlighted both theoretically and empirically that nDCGdeviates from this. In what follows, we wish to empirically validatewhether we can leverage the metrics effectively to estimate onlinereward for deployed ranking policies, for recommender systemsrunning on large-scale web platforms. The research questions wewish to answer with empirical evidence, are the following:",
  "RQ4 Are differences in any of the considered offline evaluation met-rics predictive of differences in online metrics?": "We focus on correlations between off- and online metrics ratherthan exact estimation error, because downstream business logicprevents the model output to exactly match online behaviour .To provide empirical evidence for research questions 14, we re-quire access to a ground truth online metric. There are two familiesof evaluation methods we can consider to obtain this: (1) simula-tion studies, or (2) online experiments. Both come with their own(dis)advantages. Indeed, simulation studies are generally repro-ducible and allow full control over the environment to investigatewhich of the assumptions laid out in the theoretical sections ofthis work are necessary to retain DCGs utility as an online metric.Nevertheless, simulations require us to make additional assump-tions about user behaviour, that are often non-trivial to validate. Asa result, they would provide no empirical evidence on real-worldvalue of the metrics, and are limited in the insights they can bring.Online experiments, on the other hand, are harder to reproduce.Notwithstanding this, they allow us to directly measure real userbehaviour and give a view of the utility of the DCG metric foroffline evaluation purposes in a real-world deployed system. Thiscan guide practitioners who need to perform such evaluations. We focus on the online family for the remainder of this work, notingthat simulations provide an interesting avenue for future research.We use data from a large-scale social media platform that utilisesa two-stage ranking system as described earlier to present userswith a personalised feed of short videos they might enjoy. The plat-form operates a hierarchical feed where users are presented witha 1st-level feed they can scroll through and engage with content,and users can enter a 2nd-level more-like-this feed via any given1st-level item.3 Because of the differences in interaction modalitiesand the user interface between the two feeds, they require sepa-rate models to estimate position bias, and we separate them in ouranalysis. The 1st-level feed adopts a recently proposed probabilis-tic position bias model , whereas the 2nd-level feed adopts anexponential form (such as the one underlying rank-biased preci-sion ). Because of this difference, the importance weights in the2nd-level feed exhibit much larger variance, and we adopt a clippingparameter for IPS which we set at 200 to compute the de-biasedDCG metric on this data (and vary it in .2). Rewards on ashort-video platform can be diverse. We collect both implicit signals(e.g. watching a video until the end) and explicit signals (e.g. likinga video), and consider both types of rewards for our on- and offlinemetrics, referring to them as Cimp and Cexp respectively.",
  "OfflineOnline Metric Correlation (RQ13)": "We collect data over a week of a deployed online experiment withover 40 million users where we deployed a change to a deterministicranking policy R, and kept the candidate generator G fixed. Thissimplifies the exposure calculation that is used in the offline evalu-ation metrics in Eq. 6 to that in Eq. 9, and ensures that the varianceof the offline estimator is lower. For every day in the experiment,we (1) aggregate online results per day as the average numberof logged positive feedback samples per session, and (2) collect adataset D0 which we use to compute offline metrics (through Eq. 9 and variants thereof). Then, we compute Pearsons correlation co-efficient between the series of online metrics from (1), and the offline estimates from (2), for competing evaluation metrics. presents results from this experiment, with a detailed descriptionin the caption. Results here align with what theory would suggest:the unbiased DCG variant that we have formally derived in Sec-tion 4 provides the strongest correlation with online reward. Bothadopting a learnt position bias model as opposed to the classicallogarithmic form, and de-biasing observed interaction labels asopposed to navely using them, have a significant effect on the per-formance of the final estimator. On the 1st-level feed, we observethat our estimator works especially well with an explicit reward",
  "impR00.100.880.580.300.480.410.850.070.930.02R0.550.340.880.050.700.190.880.050.770.13": ": Correlation between online reward as measured from an A/B-test and offline evaluation metrics. We consider bothexplicit and implicit reward signals, on two levels of a hierarchical feed structure on a short-video platform. We consider DCGwith a logarithmic discount (log) and a learnt model (pbm); using interaction signals directly () or de-biasing them to estimate = /P( |); for both a logging and target policy R0, R. We report Pearsons correlation coefficient and a two-tailed -valueusing Students correlation test . Statistically significant correlations ( < 0.05) are marked, best performers are bold. signal. This somewhat reverses for the 2nd-level feed, where theimplicit reward signal leads to a stronger correlation with onlineresults. We hypothesise that this is directly related to the degreewith which the assumptions laid out in are violated. Usersleave the 1st-level feed and enter the 2nd-level feed if they click ona 1st-level video. As such, the implicit reward Cimp of succesfullywatching a video is highly dependent on other items in the feed, vio-lating Asm. 3 (reward independence across ranks), as well as Asm. 2(absence of cascading behaviour). Note that all assumptions areexpected to be violated to some degree but the strongly positivecorrelation results presented in are promising.Somewhat surprisingly, not only does (unbiased) normalisedDCG exhibit worse correlation than DCG, it provides a stronglynegative correlation. At first sight, this seems troubling. Neverthe-less, we provide an intuitive explanation and highlight that thisresult alone does not imply that the metric cannot be useful foroffline evaluation purposes. Note that discrepancies stemming fromthe normalisation procedure have a disproportionate impact whenthe reward is unevenly distributed across sessions or days. Supposewe observe two sessions: one with a single positive label over twoimpressions, and one with 10 positive labels over 1 000 impressions.Because DCG deals with absolute numbers, the second session willbear a weight proportional to its positive reward. Normalised DCG,on the other hand, considers the relative distance to the optimalranking. If we reasonably assume that our ranking model is im-perfect, the distance to the optimal ranking is likely to be higherfor the second session, and nDCG will be lower as a result (eventhough we have higher DCG). This same argument can be madeacross different days in the experiment, explaining poor correla-tion results over time. Notwithstanding this, it does not necessarilyimply that nDCG holds no merit as an offline evaluation metric. Inwhat follows, we consider a more important question, focusing ondifferences in online metrics instead.",
  "OfflineOnline Metric Sensitivity (RQ4)": "To consider this research question, we restrict ourselves to a settingwhere we know that strong statistically significant ( 0.001)improvements in online metrics are observed for the target policies{G, R } over the logging policies {G0, R0}. We restrict our analy-sis to the 2nd-level feed, as it generates the majority of user-iteminteractions and relatively long sessions. We consider a variety ofexplicit and implicit feedback signals as rewards, all of which sawstatistically significant improvements in the online experiment. Weconsider three possible notions of alignment between on- and offlinemetrics, in increasing levels of expressivity: (1) Without consideringstatistical significance, do differences in offline metrics directionallyalign with differences in online metrics? (i.e. sign agreement) (2) Forstatistically significant improvements (as validated by the online ex-periment with 0.001), does the offline metric show statisticallysignificant improvements with < 0.01? (i.e. True-Positive-Rate,recall or sensitivity). (3) For statistically significant improvements,what confidence does the offline metric have in the improvements?(i.e. the -values). We consider 5 days of a deployed online experi-ment and 4 different reward signals, yielding 20 distinct statisticallysignificant online metric improvements. The purpose of our offlineestimators, is to reflect these statistically significant online differ-ences in their offline estimates. As offline estimators, we consider aclipped variant of the unbiased DCG metric in Eq. 8, where the fac-",
  "tor for the inverse exposure propensity is replaced with min, 1": ",for varying values of . Although this renders the metricbiased in a pessimistic way , its reduced variance can yield morefavourable performance as an offline evaluation metric. For everyvariant of the DCG metric we construct in this way, we computethe analogous normalised DCG metric. Because these metrics areaggregated over trajectories, we can use their empirical means andstandard deviations to construct normal confidence intervals for themetric values and their differences (i.e. the treatment effect). If the99% confidence interval for the metric difference is strictly positive,we say the metric indicates a statistically significant improvement",
  ": Sensitivity measures (y-axis) of (n)DCG for varying values of the capping parameter in IPS (x-axis)": "(with < 0.01). Because all online metric differences we considerwere statistically significant, we know that they are true positives,and we can compute the sensitivity or True-Positive-Rate (TPR) forthe offline metric by counting how often it indicates a statisticallysignificant offline difference for these true positives. We additionallyrecord the average -value for the null hypothesis (i.e. the hypothe-sis that the metric difference is 0), obtained from the confidenceintervals. To measure the weaker notion of sign agreement, we onlyconsider the mode of the confidence interval , and count agree-ment iff > 0. We vary the clipping hyper-parameter for IPS as {1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 2048, 4096, inf}, where = 1corresponds to directly using the interaction labels , and = infyields an unbiased estimator with higher variance.Reassuringly, all offline estimators exhibit 100% directional signagreement with the true treatment effect we observe for serv-ing personalised recommendations to users through {G, R } over{G0, R0}. Results for our sensitivity analysis are visualised in Fig-ure 1. In a, higher values indicate that the offline evaluationmetric is more likely to detect statistically significant improvementsin the online metric, averaged over the 20 settings described above.Analogously, lower values in b indicate that the metricyields more statistical confidence. Lower -values in b ad-ditionally imply that the metric requires less data to achieve thesignificance level, potentially reducing costs . We observe thatintroducing IPS weighting (i.e. > 1) to account for position biasin the logged interactions leads to improved sensitivity. This resultsholds for both DCG and nDCG, and both for the TPR metric and theaverage -values. We additionally observe that for a wide range ofclipping values, the DCG metric has a higher TPR (lower -values)than nDCG. Intuitively, this can be explained by the fact that nDCGessentially squashes a metric with a high expressive range to the domain, which can only come at a cost of discriminativepower. DCG, on the other hand, directly models the online metricswe care about (under the assumptions laid out in ).When we do not clip the IPS weights (i.e. = inf), we observefrom that the variance of the DCG metric increases to apoint where its sensitivity is harmed (even if directional alignmentis maintained). Note that the nDCG metric is not affected by this, asits values are bounded and they exhibit lower variance as a result.Intuitively, whereas low values of 0 can blow up the unbiased DCGformulation, they will also do this for ideal DCG, and their ratio (i.e.nDCG) will be less likely to suffer from this. We observe that with clipped propensities, even at large values, DCG leads to superiorsensitivity over nDCG, striking a favourable bias-variance trade-off.Our experimental results show promise in using DCG as anoffline estimator of online reward, and the bias-variance trade-offthat is clearly visualised in b helps us to tune this hyper-parameter properly. Even when all of the metrics underlyingassumptions violated to some degree, its value is apparent.",
  "PERSPECTIVES GOING FORWARD": "In what follows, we revisit the assumptions that are necessary toconsider the DCG metric an unbiased estimator of online reward.1. Reward independence across trajectories is necessary to avoidhaving to model any internal user state that is influenced by actionstaken by a ranking policy. Indeed, if we do allow this to happen,we must resort to Reinforcement Learning (RL) formulations of ourproblem, which inhibits the simple form that DCG allows. Neverthe-less, unbiased evaluation of RL policies is an active research area,which has found applications in recommendation research ,also for two-stage policies (without considering rankings) . Ieet al. can provide inspiration for learnt RL policies in top- recom-mendation domains with DCG-like reward structures .2. Position-based model (PBM). The classical PBM allows for gen-eral formulations of P( |), including the widely adopted func-tional form1 log2(+1) . The recently proposed Contextual PBM can be plugged into Eq. 6 to directly provide an unbiased DCGformulation with a context-dependent discount function, enjoyingthe same theoretical guarantees we have derived for DCG under thePBM. A variety of other click models have been proposed inthe research literature , as well as ways to evaluate them .We expect that our work provides a basis for further connections tobe drawn between click models and unbiased evaluation metrics.3. Reward independence across ranks. When we do not assumeany structure between the actions taken by the ranking policy andthe observed rewards, the problem quickly becomes intractable, aswe suffer from a combinatorial explosion of the action space. This isa well-known problem, and the independence assumption has beenadopted (either explicitly or implicitly) by a wide array of relatedwork . Note that this assumption does not simply relateto observing rewards, but to the underlying distribution of. Indeed,related work that adopts a cascading user behaviour model alsorelies on this assumption, as the cascade relates to the distribution of (and thus) rather than that of . Evaluation metrics have",
  "CONCLUSIONS & OUTLOOK": "Offline evaluation of recommender systems is a common task, andknown to be problematic. This work investigates the commonlyused (normalised) discounted cumulative gain metric and its uses inthe research literature. Specifically, we have investigated when wecan expect such metrics to approximate the gold standard outcomeof an online experiment. In a counterfactual estimation framework,we formally derived the necessary assumptions to consider DCGan unbiased estimator of online reward. Whilst it is reassuring thatsuch assumptions exist and we can directly map DCG to onlinemetrics we also highlighted how this ideal use deviates from thetraditional uses of the metric in IR, and how it often appears in the re-search literature. We then shifted our focus to normalised DCG, anddemonstrated its inconsistency, both theoretically and empiricallywith reproducible experiments. Indeed, even when all neccesaryassumptions hold and DCG provides unbiased estimates of onlinereward, nDCG cannot be used to rank competing models, as it doesdoes not preserve the rankings we would obtain from DCG.Through a correlation analysis between results obtained fromoff- and on-line experiments on a large-scale recommendation plat-form, we show that our unbiased DCG estimates strongly correlatewith online metrics in a real-world use-case. Additionally, we showhow the offline metric can be used to detect statistically significantonline improvements with high sensitivity, further highlighting itspromise for offline evaluation in both academia and industry. Nor-malised DCG, on the other hand, suffers from a weaker correlationwith online results, and lower sensitivity than DCG. These resultssuggest that nDCGs practical utility may be limited.We believe our work opens up interesting areas for future re-search, where our theoretical framework can be extended to for-mally assess the assumptions required by other commonly used",
  "A. Agarwal, X. Wang, C. Li, M. Bendersky, and M. Najork. 2019. AddressingTrust Bias for Unbiased Learning-to-Rank. In Proc. of the 2019 World Wide WebConference (WWW 19). ACM, 414": "A. Al-Maskari, M. Sanderson, and P. Clough. 2007. The Relationship betweenIR Effectiveness Measures and User Satisfaction. In Proc of. the 30th AnnualInternational ACM SIGIR Conference on Research and Development in InformationRetrieval (SIGIR 07). ACM, 773774. T. G. Armstrong, A. Moffat, W. Webber, and J. Zobel. 2009. ImprovementsThat Dont Add up: Ad-Hoc Retrieval Results since 1998. In Proc of. the 18thACM Conference on Information and Knowledge Management (CIKM 09). ACM,601610. J. Beel, M. Genzmehr, S. Langer, A. Nrnberger, and B. Gipp. 2013. A ComparativeAnalysis of Offline and Online Evaluations and Discussion of Research PaperRecommender System Evaluation. In Proc. of the International Workshop onReproducibility and Replication in Recommender Systems Evaluation (RepSys 13).714. W. Bendada, G. Salha, and T. Bontempelli. 2020. Carousel Personalizationin Music Streaming Apps with Contextual Bandits. In Proc. of the 14th ACMConference on Recommender Systems (RecSys 20). ACM, 420425.",
  "R. Caamares, P. Castells, and A. Moffat. 2020. Offline evaluation optionsfor recommender systems. Information Retrieval Journal 23, 4 (01 Aug 2020),387410": "E. Cavenaghi, G. Sottocornola, F. Stella, and M. Zanker. 2023. A Systematic Studyon Reproducibility of Reinforcement Learning in Recommendation Systems.ACM Trans. Recomm. Syst. 1, 3, Article 11 (jul 2023), 23 pages. O. Chapelle, D. Metzler, Y. Zhang, and P. Grinspan. 2009. Expected ReciprocalRank for Graded Relevance. In Proc of. the 18th ACM Conference on Informationand Knowledge Management (CIKM 09). ACM, 621630.",
  "M. Ferrante, N. Ferro, and N. Fuhr. 2021. Towards Meaningful Statements inIR Evaluation: Mapping Evaluation Measures to Interval Scales. IEEE Access 9(2021), 136182136216": "M. Ferrari Dacrema, S. Boglio, P. Cremonesi, and D. Jannach. 2021. A TroublingAnalysis of Reproducibility and Progress in Recommender Systems Research.ACM Trans. Inf. Syst. 39, 2, Article 20 (jan 2021), 49 pages. M. Ferrari Dacrema, P. Cremonesi, and D. Jannach. 2019. Are We Really MakingMuch Progress? A Worrying Analysis of Recent Neural Recommendation Ap-proaches. In Proc. of the 13th ACM Conference on Recommender Systems (RecSys19). ACM, 101109. F. Garcin, B. Faltings, O. Donatsch, A. Alazzawi, C. Bruttin, and A. Huber. 2014.Offline and Online Evaluation of News Recommender Systems at Swissinfo.Ch.In Proc. of the 8th ACM Conference on Recommender Systems (RecSys 14). 169176. A. Gilotte, C. Calauznes, T. Nedelec, A. Abraham, and S. Doll. 2018. Offline A/BTesting for Recommender Systems. In Proc. of the Eleventh ACM InternationalConference on Web Search and Data Mining (WSDM 18). ACM, 198206. A. Gruson, P. Chandar, C. Charbuillet, J. McInerney, S. Hansen, D. Tardieu,and B. Carterette. 2019. Offline Evaluation to Make Decisions About PlaylistRecommendation Algorithms. In Proc of. the 12th ACM International Conferenceon Web Search and Data Mining (WSDM 19). ACM, 420428. S. Gupta, H. Oosterhuis, and M. de Rijke. 2023. Safe Deployment for Coun-terfactual Learning to Rank with Exposure-Based Risk Minimization. In Proc.of the 46th International ACM SIGIR Conference on Research and Developmentin Information Retrieval (SIGIR 23). ACM, 249258.",
  "A. H. Jadidinejad, C. Macdonald, and I. Ounis. 2021. The Simpsons Paradox inthe Offline Evaluation of Recommendation Systems. ACM Trans. Inf. Syst. 40, 1,Article 4 (sep 2021), 22 pages": "R. Jagerman, X. Wang, H. Zhuang, Z. Qin, M. Bendersky, and M. Najork. 2022.Rax: Composable Learning-to-Rank Using JAX. In Proc. of the 28th ACM SIGKDDConference on Knowledge Discovery and Data Mining (KDD 22). ACM, 30513060. M. Jakimov, A. Buchholz, Y. Stein, and T. Joachims. 2023. Unbiased OfflineEvaluation for Learning to Rank with Business Rules. In RecSys 2023 Workshop:CONSEQUENCES Causality, Counterfactuals and Sequential Decision-Making.arXiv:2311.01828",
  "O. Jeunen, D. Rohde, and F. Vasile. 2019. On the Value of Bandit Feedback forOffline Recommender System Evaluation. arXiv:1907.12384 [cs.IR]": "O. Jeunen, D. Rohde, F. Vasile, and M. Bompaire. 2020. Joint Policy-ValueLearning for Recommendation. In Proc. of the 26th ACM SIGKDD InternationalConference on Knowledge Discovery & Data Mining (KDD 20). ACM, 12231233. O. Jeunen, K. Verstrepen, and B. Goethals. 2018. Fair Offline Evaluation Method-ologies for Implicit-feedback Recommender Systems with MNAR Data. In Proc.of the REVEAL 18 Workshop on Offline Evaluation for Recommender Systems(RecSys 18).",
  "T. Joachims, B. London, Y. Su, A. Swaminathan, and L. Wang. 2021. Recom-mendations as Treatments. AI Magazine 42, 3 (Nov. 2021), 1930": "N. Kallus, Y. Saito, and M. Uehara. 2021. Optimal Off-Policy Evaluation fromMultiple Logging Policies. In Proc. of the 38th International Conference on MachineLearning (ICML 21, Vol. 139), Marina Meila and Tong Zhang (Eds.). PMLR, 52475256. E. Kharitonov, A. Drutsa, and P. Serdyukov. 2017. Learning Sensitive Com-binations of A/B Test Metrics. In Proc. of the Tenth ACM International Con-ference on Web Search and Data Mining (WSDM 17). ACM, 651659. H. Kiyohara, Y. Saito, T. Matsuhiro, Y. Narita, N. Shimizu, and Y. Yamamoto. 2022.Doubly Robust Off-Policy Evaluation for Ranking Policies under the CascadeBehavior Model. In Proc of. the Fifteenth ACM International Conference on WebSearch and Data Mining (WSDM 22). ACM, 487497. R. Kohavi, A. Deng, and L. Vermeer. 2022. A/B Testing Intuition Busters: Com-mon Misunderstandings in Online Controlled Experiments. In Proc. of the 28thACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 22).ACM, 31683177.",
  "W. Krichene and S. Rendle. 2020. On Sampled Metrics for Item Recommendation.In Proc. of the 26th ACM SIGKDD International Conference on Knowledge Discovery& Data Mining (KDD 20). ACM, 17481757": "N. Larsen, J. Stallrich, S. Sengupta, A. Deng, R. Kohavi, and N. T. Stevens. 2023.Statistical Challenges in Online Controlled Experiments: A Review of A/BTesting Methodology. The American Statistician 0, 0 (2023), 115. D. Li, R. Jin, J. Gao, and Z. Liu. 2020.On Sampling Top-K Recommenda-tion Evaluation. In Proc of. the 26th ACM SIGKDD International Conferenceon Knowledge Discovery & Data Mining (KDD 20). ACM, 21142124.",
  "J. Ma, Z. Zhao, X. Yi, J. Yang, M. Chen, J. Tang, L. Hong, and E. H. Chi. 2020.Off-Policy Learning in Two-Stage Recommender Systems. In Proc. of the 2020World Wide Web Conference (WWW 20). ACM": "J. McInerney, B. Brost, P. Chandar, R. Mehrotra, and B. Carterette. 2020.Counterfactual Evaluation of Slate Recommendations with Sequential Re-ward Interactions. In Proc of. the 26th ACM SIGKDD International Confer-ence on Knowledge Discovery & Data Mining (KDD 20). ACM, 17791788. R. Mehrotra, J. McInerney, H. Bouchard, M. Lalmas, and F. Diaz. 2018. Towards aFair Marketplace: Counterfactual Evaluation of the Trade-off between Relevance,Fairness & Satisfaction in Recommendation Systems. In Proc. of the 27th ACMInternational Conference on Information and Knowledge Management (CIKM 18).ACM, 22432251. R. Mehrotra, N. Xue, and M. Lalmas. 2020. Bandit Based Optimization of MultipleObjectives on a Music Streaming Platform. In Proc of. the 26th ACM SIGKDDInternational Conference on Knowledge Discovery & Data Mining (KDD 20). ACM,32243233. L. Michiels, R. Verachtert, and B. Goethals. 2022. RecPack: An(Other) Experi-mentation Toolkit for Top-N Recommendation Using Implicit Feedback Data.In Proc. of the 16th ACM Conference on Recommender Systems (RecSys 22). ACM,648651.",
  "H. Oosterhuis. 2023. Doubly Robust Estimation for Correcting Position Biasin Click Feedback for Unbiased Learning to Rank. ACM Trans. Inf. Syst. 41, 3,Article 61 (feb 2023), 33 pages": "H. Oosterhuis and M. de Rijke. 2020. Policy-Aware Unbiased Learning to Rankfor Top-k Rankings. In Proc of. the 43rd International ACM SIGIR Conference onResearch and Development in Information Retrieval (SIGIR 20). ACM, 489498. A. B. Owen. 2013. Monte Carlo theory, methods and examples. J. Parapar and F. Radlinski. 2021. Towards Unified Metrics for Accuracy andDiversity for Recommender Systems. In Proc. of the 15th ACM Conference on Rec-ommender Systems (RecSys 21). ACM, 7584. S. Rendle, W. Krichene, L. Zhang, and J. Anderson. 2020. Neural CollaborativeFiltering vs. Matrix Factorization Revisited. In Proc of. the 14th ACM Conferenceon Recommender Systems (RecSys 20). ACM, 240248. S. Rendle, W. Krichene, L. Zhang, and Y. Koren. 2022.Revisiting the Per-formance of IALS on Item Recommendation Benchmarks. In Proc. of the16th ACM Conference on Recommender Systems (RecSys 22). ACM, 427435.",
  "S. Rendle, L. Zhang, and Y. Koren. 2019. On the Difficulty of Evaluating Baselines:A Study on Recommender Systems. arXiv:1905.01395 [cs.IR]": "D. Rohde, S. Bonner, T. Dunlop, F. Vasile, and A. Karatzoglou. 2018. RecoGym:A Reinforcement Learning Environment for the problem of Product Recommen-dation in Online Advertising. arXiv preprint arXiv:1808.00720 (2018). M. Rossetti, F. Stella, and M. Zanker. 2016. Contrasting Offline and OnlineResults when Evaluating Recommendation Algorithms. In Proc. of the 10thACM Conference on Recommender Systems (RecSys 16). ACM, 3134. N. Sachdeva, Y. Su, and T. Joachims. 2020. Off-Policy Bandits with DeficientSupport. In Proc of. the 26th ACM SIGKDD International Conference on KnowledgeDiscovery & Data Mining (KDD 20). ACM, 965975. Y. Saito, S. Aihara, M. Matsutani, and Y. Narita. 2021. Open Bandit Dataset andPipeline: Towards Realistic and Reproducible Off-Policy Evaluation. In Proc of.the Neural Information Processing Systems Track on Datasets and Benchmarks,Vol. 1. Y. Saito and T. Joachims. 2021. Counterfactual Learning and Evaluation forRecommender Systems: Foundations, Implementations, and Recent Advances.In Proc. of the 15th ACM Conference on Recommender Systems (RecSys 21). ACM,828830.",
  "(ICML 22, Vol. 162). PMLR, 1908919122": "Y. Saito, Q. Ren, and T. Joachims. 2023. Off-Policy Evaluation for Large Ac-tion Spaces via Conjunct Effect Modeling. In Proc. of the 40th InternationalConference on Machine Learning (ICML 23, Vol. 202). PMLR, 2973429759. Y. Saito, T. Udagawa, H. Kiyohara, K. Mogi, Y. Narita, and K. Tateno. 2021.Evaluating the Robustness of Off-Policy Evaluation. In Proc. of the 15th ACMConference on Recommender Systems (RecSys 21). ACM, 114123. O. Sakhi, S. Bonner, D. Rohde, and F. Vasile. 2020. BLOB : A Probabilistic Modelfor Recommendation that Combines Organic and Bandit Signals. In Proc. of the26th ACM Conference on Knowledge Discovery & Data Mining (KDD 20). ACM.",
  "Student. 1908. Probable Error of a Correlation Coefficient. Biometrika 6, 2/3(1908), 302310": "Y. Su, M. Dimakopoulou, A. Krishnamurthy, and M. Dudik. 2020. Doubly robustoff-policy evaluation with shrinkage. In Proc. of the 37th International Conferenceon Machine Learning (ICML 20). PMLR, 91679176. Y. Su, P. Srinath, and A. Krishnamurthy. 2020. Adaptive Estimator Selection forOff-Policy Evaluation. In Proc of. the 37th International Conference on MachineLearning (ICML 20, Vol. 119). PMLR, 91969205.",
  "A. Swaminathan and T. Joachims. 2015.The Self-Normalized Estima-tor for Counterfactual Learning. In Advances in Neural Information Pro-cessing Systems. 32313239": "A. Swaminathan, A. Krishnamurthy, A. Agarwal, M. Dudik, J. Langford,D. Jose, and I. Zitouni. 2017.Off-policy evaluation for slate recom-mendation. In Advances in Neural Information Processing Systems, Vol. 30.Curran Associates, Inc. A. D. Tucker and T. Joachims. 2023. Variance-Minimizing Augmentation Loggingfor Counterfactual Evaluation in Contextual Bandits. In Proc of. the SixteenthACM International Conference on Web Search and Data Mining (WSDM 23).ACM, 967975. T. Udagawa, H. Kiyohara, Y. Narita, Y. Saito, and K. Tateno. 2023. Policy-AdaptiveEstimator Selection for Off-Policy Evaluation. Proc. of the AAAI Conference onArtificial Intelligence 37, 8 (Jun. 2023), 1002510033. A. Ustimenko and L. Prokhorenkova. 2020. StochasticRank: Global Optimizationof Scale-Free Discrete Functions. In Proc of. the 37th International Conference onMachine Learning (ICML 20, Vol. 119). PMLR, 96699679.",
  "D. Valcarce, A. Bellogn, J. Parapar, and P. Castells. 2020. Assessing rankingmetrics in top-N recommendation. Information Retrieval Journal 23, 4 (01 Aug2020), 411448": "A. Vardasbi, H. Oosterhuis, and M. de Rijke. 2020. When Inverse PropensityScoring Does Not Work: Affine Corrections for Unbiased Learning to Rank.In Proc of. the 29th ACM International Conference on Information & KnowledgeManagement (CIKM 20). ACM, 14751484. F. Vasile, D. Rohde, O. Jeunen, and A. Benhalloum. 2020. A Gentle Introductionto Recommendation as Counterfactual Policy Learning. In Proc. of the 28th ACMConference on User Modeling, Adaptation and Personalization (UMAP 20). ACM,392393.",
  ": DCG and nDCG exhibit significant disagreementfor a standard offline evaluation setup on MovieLens-1M.AEMPIRICAL EVIDENCE OF (N)DCGINCONSISTENCY ON PUBLIC DATA": "provides a formal proof that an ordering over competing rec-ommendation (or IR) models obtained through a normalised metricis not guaranteed to be consistent with the original metric. Never-theless, one might wonder whether this single example representsa misguided pathological case, or whether metric disagreementoccurs in practice. This gives rise to the research question:RQ5 Do DCG and normalised DCG disagree when ranking recom-mendation models in typical offline evaluation setups?To answer this question, we make use of the RecPack Python pack-age and the MovieLens-1M dataset . We consider two typesof models, easer and kunn , varying their hyperparame-ters to train 192 models on a fixed 50% of the available user-iteminteractions, and assess their performance on the held-out 50%. This style of evaluation setup is prevalent in the recommendationfield . We adopt this package, dataset and methods toprovide a reproducible setup that runs in under 20 minutes on a 2021MacBook Pro. All source code, including hyperparameter ranges,is available at github.com/olivierjeunen/nDCG-disagreement.We do not de-bias the interactions (as MovieLens does not pro-vide information about exposure), and adopt the traditional loga-rithmic discount for DCG with a cut-off at rank 100. Results arevisualised in with DCG@100 on the -axis and nDCG@100on the -axis. The two metrics exhibit a linear correlation of 0.6(Pearson), and a rank correlation of 0.5 (Kendall). Whilst theyare clearly correlated, practitioners should not blindly adopt nDCGwhen DCG estimates their online metric. Indeed, DCG can be formu-lated as an unbiased estimator of the average reward per trajectory,but nDCG cannot. As can be seen from the plot, significant disagree-ment occurs between the two metrics: when randomly choosingtwo observations, the empirical probability of nDCG inverting theordering implied by DCG is roughly 25% on this example. Naturally,one would expect this type of disagreement to occur even morefrequently when considering Learning-to-Rank algorithms thatdirectly optimise listwise objectives such as (n)DCG .Note that this discrepancy would not occur if we would samplethe exact same number of held-out items for every user (as in Leave-One-Out Cross-Validation). Indeed, in such cases () is constant X, simply rescaling the metric. Whilst this practice can becommon in academic scenarios, real-world use-cases typically implyvarying numbers of relevant items per user or context.We include these results to aid in the reproducibility of theempirical phenomena we report in this work."
}