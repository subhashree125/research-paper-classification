{
  "ABSTRACT": "In this paper, we introduce a new self-supervised rationalizationmethod, called KGRec, for knowledge-aware recommender sys-tems. To effectively identify informative knowledge connections,we propose an attentive knowledge rationalization mechanism thatgenerates rational scores for knowledge triplets. With these scores,KGRec integrates generative and contrastive self-supervised tasksfor recommendation through rational masking. To highlight ratio-nales in the knowledge graph, we design a novel generative task inthe form of masking-reconstructing. By masking important knowl-edge with high rational scores, KGRec is trained to rebuild andhighlight useful knowledge connections that serve as rationales.To further rationalize the effect of collaborative interactions onknowledge graph learning, we introduce a contrastive learning taskthat aligns signals from knowledge and user-item interaction views.To ensure noise-resistant contrasting, potential noisy edges in bothgraphs judged by the rational scores are masked. Extensive experi-ments on three real-world datasets demonstrate that KGRec outper-forms state-of-the-art methods. We also provide the implementationcodes for our approach at",
  "Chao Huang is the corresponding author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 23, August 610, 2023, Long Beach, CA, USA 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0103-0/23/08...$15.00",
  "INTRODUCTION": "With the rise of information overload, recommender systems havebecome a critical tool to help users discover relevant items of inter-est . Among the leading paradigms in this field is collabora-tive filtering (CF), which assumes that users with similar interac-tions share similar interests in items . CF has proven to beeffective in a wide range of applications and has driven significantadvances in the field of recommender systems.In recent years, collaborative filtering (CF) frameworks have un-dergone significant improvements with the introduction of neuralnetworks and latent embedding for users and items, leading to ef-fective enhancements for traditional matrix factorization methods(e.g., ). Moreover, novel models that integrate variationalautoencoders, attention mechanisms, and graph neural networkshave further increased the performance of CF (e.g., ).However, the sparsity of user-item interactions fundamentally lim-its the scope of performance improvement. To address this issue,incorporating a knowledge graph (KG) as a rich information net-work for items has gained traction in collaborative filtering, leadingto knowledge graph-enhanced recommendation.The exploration of knowledge graph-enhanced recommendationbegins with embedding-based methods and path-based methods.Specifically, some studies incorporate transition-basedknowledge graph embedding, such as TransR , into item em-bedding to enrich user and item modeling. Other studies focus on extracting semantically meaningful meta-paths from theKG and perform complex modeling of users and items along thesemeta-paths. To unify embedding-based and path-based methods ina mutually beneficial manner, recent research has adopted powerfulgraph neural networks (GNNs) to capture multi-hop high-orderinformation through propagation and aggregation on the KG. Thesestate-of-the-art solutions include .Although knowledge graphs have proven effective for improv-ing recommendation systems, they can also introduce noise andsparsity issues, leading to sub-optimal performances . To ad-dress these issues, recent studies propose using contrastive learning(CL) for better knowledge-aware recommendation. For example,KGCL applies stochastic graph augmentation on the KG andperforms CL to address noisy entity and long-tail problems in theKG. design a cross-view CL paradigm between the KG anduser-item graph to improve KG representation learning with real la-bels from recommendation. However, we argue that these methodsadopt either simple random augmentation or intuitive cross-view",
  "Freediving": ": The left figure displays a distribution of attentivescores for knowledge triplets in the baseline method of KGAT,which is skewed towards the tail end. On the other hand, theright figure suggests that we can determine the rationalityof knowledge triplets for recommendation by analyzing thetraining labels of user-item interactions. information, failing to consider the important latent rationales be-tween the KG and recommendation task. presents the distribution of attention scores of knowl-edge triplets in KGAT on the left, and a motivating case on theright that illustrates the rationales in the KG emphasized by CFsignals. The distribution of attention scores in the KGAT modelshows that only a small proportion of knowledge triplets have highattention scores and are thus highly contributive to recommenda-tion as rationales. The remaining knowledge triplets exhibit a longtail of low scores in the distribution and are less informative inthe network. To better understand the relationship between KGand CF signals, we provide an example of an e-commerce platformwhere users often purchase diving glasses and underwater camerastogether. To make accurate predictions, the connections with com-mon semantics Sports/Diving will be highlighted in the KG. Thus,for the underwater cameras, the knowledge Photography andDigital Cam will be less important compared to Sports Cam. Thishighlights the importance of identifying and emphasizing relevantrationales in the KG to improve recommendation performance.In order to achieve accurate and effective knowledge graph-basedrecommendations, it is important to explicitly model the rationalesbehind the user preference learning. To address this challenge, wepropose a new knowledge graph-enhanced recommender system,called KGRec to leverage attentive knowledge rationalization togenerate task-related rational scores for knowledge triplets. KGRecproposes a self-supervised rationale-aware masking mechanismto extract useful rationales from the KG, by adaptively maskingknowledge triplets with higher rational scores. By forcing KGRec tolearn to reconstruct these important connections, we highlight task-related knowledge rationales. We also align the rational semanticsbetween the KG signals and the Collaborative Filtering (CF) signalsvia a knowledge-aware contrasting mechanism. This is achievedby filtering out low-scored knowledge that may be potential noiseby masking during graph augmentation for contrastive learning.Finally, we inject the rational scores into the knowledge aggregationfor the recommendation task, enabling knowledge rational scoresto be learned tightly from the CF labels.In summary, we make the following contributions in this paper:",
  "the knowledge graph for recommendation and align them in anoise-free and rationale-aware manner": "Our proposed rationale-aware masking mechanism allows us toidentify and highlight the most important and relevant informa-tion within the knowledge graph, while suppressing potentialnoise or irrelevant knowledge graph connections. To validate the effectiveness of our proposed model, KGRec,we conduct extensive experiments on three real-world datasets.Evaluation results provide strong evidence that our proposedmodel achieves superior performance compared with existingknowledge-aware recommender systems.",
  "PRELIMINARIES": "We begin by introducing the concepts that will be used in our paperand formally defining the KG-enhanced recommendation task.User-Item Interaction Graph. In a typical recommendation sce-nario, we have a set of users, denoted by U, and a set of items,denoted by V. Let U and V represent a single user anditem, respectively. We construct a binary graph G = (,, )to denote the collaborative signals between users and items, with = 1 if user interacted with item , and vice versa.Knowledge Graph. We represent real-world knowledge aboutitems with a heterogeneous graph consisting of triplets, denotedby G = (,,). , E are knowledge entities, and Rrepresents the semantic relation connecting them, such as (author,wrote, book). It is important to note that the item set is a propersubset of the entity set, i.e., V E. This allows us to model thecomplex relationships between items and entities in the KG.Task Formulation. Our KG-aware recommendation task can beformally described as follows: given a user-item interaction graph,denoted by G, and a knowledge graph, denoted by G, our goal is tolearn a recommender model, denoted by F (, |G, G, ), whereF represents the model architecture with learnable parameters .The output of the model is a value in the range that indicatesthe likelihood of user interacting with item .",
  "Rationale Discovery for Knowledge Graph": "To automatically distill essential semantics for recommendationfrom the complex knowledge graph, we propose a rationale weight-ing function that learns the probability of knowledge triplets beingthe underlying rationale for collaborative interactions. This ratio-nale function weighs each knowledge triplet based on a learnablegraph attention mechanism. Inspired by the heterogeneous graphtransformer (HGT) , which discriminates the importance ofheterogeneous relations, we implement the rationale weightingfunction (,,) as follows:",
  "Knowledge Aggregation": ": The overall framework of KGRec. The attentive knowledge rationalization module generates rational scores for KGtriplets based on their importance for the recommendation task. Connections with high rational scores are masked, and themodel is trained to reconstruct the important connections under relational context. Low-scored KG triplets are considered asnoise and removed for rationales contrastive learning between user-item interactions and knowledge graphs. Here, e, e, and e are embeddings for the head, relation, and tailentities, respectively. The trainable weights for attention, W andW, have dimensions of R, where is the hidden dimension-ality. To model the relational context, we use the element-wiseproduct between the relation and the tail entity , which cor-responds to the rotation of the entity embedding e to the latentspace of relation . The rationale score (,,) of a knowl-edge triplet indicates its importance in assisting user preference,as learned by the model and guided by the labels from the recom-mendation task. To ensure comparability of rationale scores acrossneighbors of the same head entity, we normalize the scores by thenumber of neighbors N using the following softmax function:",
  "Rationale-aware HeterogeneousKnowledge Aggregation": "A complex KG often contains a large number of real-world knowl-edge triplets with heterogeneous nature. Inspired by previous workssuch as , we design an aggregation layer for the knowl-edge graph that reflects the relational heterogeneity of knowledgetriplets. In particular, we focus on the rationales of knowledgetriplets, which enable dynamic weighting considering the impor-tance of neighbor entities. To build the knowledge aggregator, weinject the relational context into the embeddings of the neighboringentities, weighting them with the knowledge rationale scores.",
  "(,, )N(,,)e e(1),(3)": "where denotes the layer of the aggregator, and N G is thenode-centric sub-graph of first-order neighbors. To inject relationalcontext, we use the same element-wise product as in Equation 1 tobridge the gap between aggregation and rationale weighting. By performing such aggregation across the entire knowledge graph,we carefully consider the contextual relationships between knowl-edge entities and weight neighbor information for the head entityaccording to normalized rationale scores.Its worth noting that items are a subset of knowledge entities.Therefore, we obtain knowledge-aware item representations byaggregating paths e e1 e on the KG using Equa-tion 3. To model collaborative signals between users and items,we take into account the role of users in the interaction graph G.This allows us to generate user embeddings by aggregating theembeddings of the neighboring items in the user-item interactiongraph. Specifically, we use a neighbor aggregation method to obtainthe user embedding with the following formulas:",
  "KDD 23, August 610, 2023, Long Beach, CA, USAYuhao Yang, Chao Huang, Lianghao Xia, and Chunzhen Huang": "representations with the positive labels. The distribution of train-ing data and + distribution of positive labels are used to computethe expected value of the alignment loss.We first prove that the rational masking-reconstructing task isan explicit alignment for features. According to the generative lossin Equation 10, the optimization of L equals to:",
  "(,,) = |N| (,,) =|N| exp ( (,,))(, ,)N exp ( (,,)) . (6)": "The motivation behind this criterion is to identify the most valuableknowledge triplets across the entire KG. By using the rationalescore after softmax, we can determine the relative proportion of aknowledge triplet among its head entity neighbors N. We multiplythe rationale score with the number of head entity neighbors |N|,which makes it globally comparable. By using this approach, wecan select the most valuable knowledge triplets across the entireKG based on the value of(,,). To improve sampling robustness,we add Gumbel noise to the learned rationale scores.",
  "M = {(,,)|(,,) topk(;)},(8)": "where represents the distribution of all (,,). Finally, to createan augmented knowledge graph, denoted by G , we remove theedges M with low rationale scores from the original knowledgegraph G. In other words, G is obtained by subtracting the set ofedges M from the set of edges in G, represented by G \\ M. 3.3.2Reconstructing with Relation-aware Objective. In or-der to enable our model to recover crucial knowledge in a self-supervised way, we provide the model with entity embeddingscreated from the augmented graph G , and train the model to reconnect the masked knowledge edges. Therefore, we begin byapplying rationale-aware knowledge aggregation, as outlined inEquation 3, on G to produce entity embeddings, in which rationale edges have been removed.",
  "e = (G ;); e = (G ;),(9)": "The function () is the aggregation function on the knowledgegraph, as defined in Equation 5. At this point, the knowledge tripletswith significant rationale scores, denoted by M, which were notvisible during the aggregation stage, can be used as self-supervisionlabels for reconstruction. Given the rich relational heterogeneity inthe knowledge graph, we aim to reconstruct the important rationalconnections under relational contexts. To achieve this, we minimizethe following dot-product log-loss for the label triplets, with ()representing the sigmoid activation function:",
  "Knowledge Rationale-aware Contrasting": "3.4.1Rationale-aware Graph Augmentation. As explainedearlier, the hierarchical rationales for knowledge triplets are de-rived from the connection between the knowledge graph and user-involved recommendation labels. In order to further enhance theinterpretability of the knowledge rationalization modules, we drawinspiration from previous works . Specifically, we propose toalign the representations of the knowledge graph with collaborativefiltering signals, which allows us to explicitly model cross-viewrationales. To construct debiased contrastive views, we begin byidentifying and removing weakly task-related edges that couldpotentially introduce noise in both graphs.Regarding the knowledge graph, it is worth noting that knowl-edge triplets with lower rationale scores tend to have less impacton the recommendation task. Consequently, we aim to improvethe quality of the graph by removing the noisy triplets. This aug-mentation process ensures that the remaining triplets are moreinformative and have a higher rationale score. By doing so, wecan enhance the performance of our model and better capture theunderlying relationships between the entities in the graph.",
  "S = {(,,)|(,,) topk(; )}; G = G \\ S,(11)": "In Equation 8, we introduced the knowledge attentive scores and, which are computed with the addition of Gumbel noise. Here, represents the distribution of all values. By taking the negativeof , denoted as , we can use the top-k function to calculate theleast-k values. The hyperparameter controls the dropout ratioduring training. We also introduce the augmented knowledge graphG, which is debiased from noise with lower rationale scores.In addition to the knowledge graph, we also aim to improvethe quality of the u-i interaction graph by removing noisy interac-tions that are not conducive to cross-view alignment. Specifically,we want to retain interaction edges that clearly reflect the usersinterests and can better guide knowledge graph rationalizationthrough cross-view contrasting. Given that the semantics of itemembeddings can be influenced by their linked knowledge in theKG, we propose to weight each interaction edge by considering therationales of the knowledge triplets connected to the item. This",
  "Knowledge Graph Self-Supervised Rationalization for RecommendationKDD 23, August 610, 2023, Long Beach, CA, USA": "approach allows us to better reflect the noise associated with eachinteraction edge. To implement this, we calculate the mean value ofthe rationale scores for all the knowledge triplets linked to the item.This mean value is then used as a weight for the correspondinginteraction edge, which helps to distinguish between informativeand noisy interactions.",
  "= mean({(,,)| = = }).(12)": "A lower value implies that the knowledge entities neighboringan item in the KG are relatively less contributive to the recom-mendation task, which can lead to bias in the item representation.To address this issue, we filter our interaction edges using the score and augment the graph with only the informative interac-tions. To avoid overfitting on user and item representations, weadopt a multinomial distribution sampling strategy to de-rive more randomized samples for edge dropout. This approachhelps to ensure that the model is not overly reliant on a specific setof interactions and can generalize well to new data. Formally, theprocess can be defined as follows:",
  "=exp exp; S multinomialNR(; ),(13)": "After calculating the score for each item , which represents themean value of the rationale scores for all the knowledge tripletslinked to the item, we apply softmax to obtain a probability distri-bution over all items. The resulting distribution is used tosample a subset of items without replacement using the multino-mial distribution sampling method, denoted as multinomialNR(; ).Here, denotes the size of the sampled candidates. By followingthe previous definitions, we can generate the augmented u-i graphas the difference between the original u-i graph G and the set ofsampled interactions S, i.e., G = G \\ S. 3.4.2Contrastive Learning with Cross-View Rationales. Withthe augmented knowledge graph and u-i graph, we use pre-definedaggregators to capture the view-specific node representations foritems as the contrastive embeddings. For the u-i interaction view,we utilize the state-of-the-art LightGCN module to iterativelycapture high-order information on G.",
  "z = xT W1 + b1TW2 + b2,(16)": "where the notation , denotes view-specific representations,namely z and z. The learnable weights and bias denoted as Wand b. By doing so, we can effectively capture the complementaryinformation from both views.To ensure the alignment of cross-view item representations, weadopt a contrastive objective. To avoid over-fitting and eliminatethe false-negative effect, as inspired by , we modify the widelyused InfoNCE loss by specifying one random sample for eachview as the negative. Formally, we define our contrastive loss as:",
  "(,,)D log ,(18)": "In the BPR loss, we use the training instances D = (, , ), where is the ground-truth and is a randomly sampled negative inter-action. It is worth noting that we continue to use the entity em-beddings e from the masked graph G for the recommendationtask, rather than performing aggregation on the original knowledgegraph again. This is because the masked triplets are generally ofsmall size (e.g., 512) compared to the whole graph (e.g., millions),and this trick can greatly improve the training efficiency whileaffecting the representation learning only minimally. Moreover,according to , this setting can increase the difficulty of themain task learning and improve the optimization effect.To optimize all three loss functions, we use a joint learningapproach with the following overall loss function:",
  "(25)": "The positive pair in Equation 17 is denoted as x, y, and the negativesamples are denoted as x for brevity. The set of random negativesamples 2=1 in Equation 17 is drawn from the distribution of cross-view item representations. As a result, the lower boundof the contrastive loss function L in Equation 25 is satisfied onlyif the embeddings x, y are perfectly aligned, i.e., xTy = 1, which isequivalent to the definition of alignment in Equation 20. If the em-beddings satisfy the perfect alignment condition, the optimizationof L simplifies to a degenerate form.",
  ",(26)": "The alignment and uniformity properties in the generative lossfunction L and the contrastive loss function L can benefit repre-sentation learning by ensuring that positive pairs are in agreementand that random instances are pushed as negatives. In addition, theproposed knowledge rationalization improves the sampling distri-bution to be rational and noise-resistant, instead of using a randomdistribution as in the original forms. By exploiting rationales in theKG, we empower the alignment property with rationality-awarepositive pairing ability, which provides better gradients for modellearning. Additionally, for cross-view rationales, we remove po-tential noise to build a noise-free distribution, which eliminatesthe effect of false negative pairing and improves the contrastiveeffectiveness. Overall, our KGRec is able to derive better alignmentand uniformity compared to stochastic methods, which can lead toimproved representation for more accurate recommendations.",
  "Experimental Setup": "4.1.1Dataset. To ensure a diverse and representative evaluation,we use three distinct datasets that reflect real-life scenarios: Last-FMfor music recommendations, MIND for news recommendations, andAlibaba-iFashion for shopping recommendations. We preprocessthe datasets using the commonly adopted 10-Core approach to filterout users and items with less than 10 occurrences. To construct theknowledge graphs, we employ different methods for each dataset.For Last-FM, we map the items to Freebase entities and extractknowledge triplets, following the techniques used in and .For MIND, we collect the knowledge graph from Wikidata1 us-ing the representative entities in the original data, following theapproach proposed in . For Alibaba-iFashion, we manually con-struct the knowledge graph using category information as knowl-edge, as done in . summarizes the statistics of user-iteminteractions and knowledge graphs for three evaluation datasets. 4.1.2Evaluation Protocols. To ensure fair evaluation, we em-ploy the full-rank setting and divide our dataset into three parts:70% for training, 10% for hyperparameter tuning, and 20% for test-ing. We measure the performance of our proposed KGRec using theRecall@N and NDCG@N metrics, with N set to 20 for top-N recom-mendations. We implement KGRec using PyTorch and compare itsperformance with various baseline models using official or third-party code. To optimize the performance of KGRec, we conduct ahyperparameter search for the masking size, keeping proportionfor contrastive learning, and temperature value. Specifically, we ex-plore values of masking size from the range of {128, 256, 512, 1024},keeping proportion and from {0.4, 0.5, 0.6, 0.7, 0.8}, and tem-perature value from the range of {0.1, , 1.0}. The number ofGNN layers is set to 2 for all graph-based methods.",
  "GC-MC considers recommendation as a link prediction prob-lem on the user-item graph and proposes a graph auto-encoderframework for matrix completion": "LightGCN is a state-of-the-art recommendation methodbased on graph neural networks (GNNs), which improves per-formance by removing activation and feature transformation.SGL introduces a self-supervised learning paradigm to GNN-based recommendation by using stochastic augmentation on theuser-item graph based on the InfoNCE objective.Embedding-based Knowledge-aware Recommenders. CKE is an embedding-based KG recommender that lever-ages TransR to enrich item representations by training onstructural knowledge, thereby enhancing collaborative filtering. KTUP trains TransH using preference-injected CF andenables mutual complementation between CF and KG signals.GNN-based Knowledge Graph-enhanced Recommenders. KGNN-LS considers user preferences towards differentknowledge triplets in graph convolution and introduces labelsmoothing as regularization to force similar user preferenceweights between nearby items in the KG.",
  "RQ1: Overall Performance Comparison": "We report the performance of all the methods on three datasets in. Based on the results, we make the following observations: The proposed KGRec consistently outperforms all baseline mod-els on both metrics and all three datasets. This can be attributed tothree factors. First, by using rational masking and reconstruction,KGRec is able to capture knowledge information that is truly use-ful for the recommendation task. Second, KGRec is equipped withrational cross-view contrastive learning on augmented, noise-free graphs, which allows for better exploitation of the latent : The overall performance evaluation results forKGRec and compared baseline models on three experimenteddatasets, where the best and second-best performances aredenoted in bold and borderline, respectively.",
  "KGRec0.09430.08100.04390.03190.11880.0743": "relatedness between KG and CF signals. Third, the knowledgeaggregation layer is weighted by knowledge rational scores to re-flect the different importance of knowledge triplets. Additionally,the superior results on datasets with vastly different statisticssuggest that the proposed knowledge rationalization mechanismcan automatically discover useful knowledge related to down-stream tasks, regardless of the data characteristics. On the three datasets, there is no consistent winner among thebaseline models. Contrastive learning-based methods (e.g., MC-CLK and KGCL) are not always better than non-self-supervisedmethods (e.g., KGIN). This may be due to the limitations of ran-dom graph augmentation or intuitive handcrafted cross-viewpairing, which may fail to discover truly useful KG informationfrom the contrastive views for encoding the interests of users. GNN-based knowledge-aware recommenders can consistentlyoutperform embedding-based models. This advantage is due toGNNs ability to capture more complex and higher-order informa-tion on the KG, compared to the linear transition-based modelingadopted by embedding-based models. The introduction of knowledge graphs does not always lead tobetter performance in recommendation systems. For instance,methods such as CKE and KTUP typically perform worse thannon-KG methods like LightGCN and SGL. Even KGNN-LS andKGCN cannot consistently outperform SGL in some metrics. Thiseffect is more noticeable when the dataset has a complex KGand sparse interactions. We suggest that some KG-aware recom-menders struggle to effectively model complex relational pathsand mitigate noise in the KG, resulting in suboptimal KG repre-sentation learning and worse performances. On the other hand,LightGCN and SGL focus more on resolving the sparsity problemof user-item interactions with self-supervision signals.",
  "RQ2: Ablation Study": "4.3.1Key Module Ablation. In this study, we investigate theeffectiveness of key modules in our proposed KGRec from the per-spectives of our designed rational masked autoencoding and con-trastive learning for recommendation. To compare with the originalmethod, we built four model variants, including: w/o MAE: removing the generative SSL task of rationale-awareknowledge graph masking and reconstruction.",
  "w/o Rationale-M: replacing the rationale knowledge maskingwith random masking while keeping the masking size unchanged": "w/o CL: disabling the cross-view contrastive learning task. w/o Rationale-Aug: replacing the rational graph augmentationwith random masking while keeping the masking size unchanged.We report the results of the ablation study in and makethe following observations: i) The proposed rationale knowledgemasking and reconstruction contributes the most to performanceenhancement. This demonstrates that mask&reconstruction is aneffective strategy for exploiting highly useful knowledge tripletsfor recommendation. ii) The rational masking mechanism for bothreconstruction and contrastive learning can further improve per-formance by selecting valuable information and dropping infor-mative knowledge. iii) The contrastive learning is also beneficialfor performance. However, we observed that adding non-rationaleaugmented graph contrastive learning on the MIND dataset canhurt performance. This indicates that simple intuitive cross-viewcontrasting is not always effective due to noises in the graph.",
  ": Evaluation results on different KG proportions": "of interactions, with smaller group numbers indicating strongercold-start effects. We then separately tested the performance ofKGRec and several strong baselines in each group and reported theresults in . Our findings demonstrate that KGRec outper-forms other baseline methods in all cold-start groups, indicatingits effectiveness in addressing the cold-start problem for a diverserange of users. This can be attributed to the design of the rationaleknowledge masked autoencoding and rationale-based cross-viewcontrastive learning, which highlight useful knowledge for rep-resentation learning and contrast cross-view signals. Therefore,KGRec can effectively alleviate cold-start issue. 4.4.2Long-tail Item Recommendation. We investigate whetherKGRec can improve representation learning for long-tail items. Wecounted the occurrence of each item and divided all users into fivegroups based on the average sparsity degree of items they interactedwith. The results are reported in . Our findings demonstratethat KGRec consistently outperforms baseline models across differ-ent groups, indicating its effectiveness in addressing data scarcityproblems. This can be attributed to the design of rationale mining,which allows KGRec to better leverage external knowledge andimprove representation learning for long-tail items. 4.4.3Recommendation with small proportion of KG. Weevaluate KGRecs capacity in highlighting important task-relatedconnections from the knowledge graph. Specifically, we tested therecommendation performance of KGRec and baseline models underpartial knowledge graphs with different keeping ratios rangingfrom 40% to 70%. We randomly selected a proportion of knowledgetriplets from the original KG in the Last-FM and Alibaba-iFashiondatasets for knowledge aggregation, and the results are reported in. Our findings demonstrate that KGRec can still maintainconsiderable performance (>95% on Last-FM and >90% on Alibaba-iFashion) with only a small portion of KG. Compared to baselinemodels, KGRec shows minimal performance degradation in all cases.This can be attributed to the design of rationale knowledge masking",
  "RELATED WORK5.1Knowledge-aware Recommender Systems": "Knowledge graphs are valuable sources of side information for itemrepresentation learning and user modeling in recommender sys-tems. Currently, knowledge-aware recommendation methods canbe generally categorized into three groups: embedding-based meth-ods, path-based methods, and GNN-based methods. i) Embedding-based methods incorporate knowledge graph entityembedding into user and item representations to enhance the rec-ommendation learning. For example, CKE proposes to inte-grate the modeling of different types of side information for itemswith collaborative filtering. It encodes a knowledge graph with thetransitive KG completion method TransR as part of item repre-sentations. ii) Path-based methods focus on exploitingthe rich semantics in relational meta-paths on the KG. For instance,KPRN adopts an LSTM to model the extracted meta-paths andaggregates user preference along each path by fully-connected lay-ers. iii) GNN-based methods extend GNNs to model theKG and use the learned representations for recommendation. Forexample, KGAT proposes to use a graph attention mechanismto propagate user and item embeddings on the KG, and then apply amulti-layer perceptron to produce the final recommendation score.The line of GNN-based knowledge-aware recommenders aims to unify the two paradigms and combine theirstrengths. GNNs have a powerful ability to capture high-order in-formation, making them effective at extracting useful informationfrom the KG. KGCN samples a fixed number of neighbors asthe receptive field to aggregate item representations on the KG.KGAT leverages graph attention networks (GATs) to weight the knowledge aggregation on the KG by considering the differentimportance of knowledge neighbors. KGIN further considersuser latents towards different relations in the KG and injects rela-tional embedding in the aggregation layer to improve performance.GNN-based methods are currently the state-of-the-art solutionsdue to their ability to exploit rich semantics from the graph andtheir considerable efficiency.",
  "Self-Supervised Recommendation": "Incorporating self-supervised learning (SSL) techniques into recom-mender systems has become a new trend in the research communityto address inherent data sparsity problems by leveraging additionalsupervision signals from raw data . Existing studies have ex-plored various SSL techniques for different recommendation tasks.For large-scale industry applications, introduces contrastivelearning in the two-tower architecture for feature augmentationwith the proposed correlated feature masking strategy. SGL applies graph contrastive learning to graph collaborative filter-ing using random augmentation on graphs such as node dropout,edge dropout, and random walk to generate contrastive views andenforce agreement with InfoNCE loss. For sequential recommen-dation, S3Rec aims to augment the sequence itself by mask-ing and adopts the contrast between augmented sequences as anauxiliary task. For social recommendation, MHCN performscontrastive learning between user embedding and its social embed-ding extracted from a sub-hypergraph of the social network. Formulti-modal recommender systems, MMSSL aims to provide auniversal solution for capturing both modality-specific collabora-tive effects and cross-modality interaction dependencies, allowingfor more accurate recommendations.KGCL develops graph contrastive learning on the KG toalleviate noise and long-tail problems, while also leveraging addi-tional signals from KG agreement to guide user/item representationlearning. MCCLK employ cross-view contrastive learning be-tween the KG and interaction graph to mitigate sparse supervisionsignals. However, we argue that these methods do not sufficientlyconsider the rationales embedded in the KG. By explicitly rationaliz-ing knowledge triplets for recommendation, our KGRec achieves asignificant performance improvement compared to these methods.",
  "CONCLUSION": "In this paper, we presented a novel graph self-supervised rational-ization method (KGRec) for knowledge-aware recommendation.Our motivation is rooted in the hierarchical rationality of knowl-edge triplets. We build our method on the attentive knowledgerationalization to weight knowledge triplets, and introduce a novelrational masking and reconstruction module to emphasize ratio-nal knowledge. The rational scores were further used to facilitatethe knowledge-aware cross-view contrastive learning, where low-scored less informative knowledge was filtered out as noise. Resultsof extensive experiments validate the advantages of KGRec againststate-of-the-art solutions. In future works, we will explore morecomplex methods for knowledge graph rationalization, such asgraph structure learning and graph sparsification. This directioncan potentially provide more insights into the underlying knowl-edge graph structure.",
  "Rianne van den Berg, Thomas N Kipf, and Max Welling. 2017. Graph convolu-tional matrix completion. arXiv preprint arXiv:1706.02263 (2017)": "Yixin Cao, Xiang Wang, Xiangnan He, Zikun Hu, and Tat-Seng Chua. 2019.Unifying knowledge graph learning and recommendation: Towards a betterunderstanding of user preferences. In The Web Conference (WWW). 151161. Jingyuan Chen, Hanwang Zhang, Xiangnan He, Liqiang Nie, Wei Liu, and Tat-Seng Chua. 2017. Attentive collaborative filtering: Multimedia recommendationwith item-and component-level attention. In International Conference on Researchand Development in Information Retrieval (SIGIR). 335344.",
  "Christoph Feichtenhofer, Haoqi Fan, Yanghao Li, and Kaiming He. 2022. MaskedAutoencoders As Spatiotemporal Learners. arXiv preprint arXiv:2205.09113 (2022)": "Michael Gutmann and Aapo Hyvrinen. 2010. Noise-contrastive estimation: Anew estimation principle for unnormalized statistical models. In InternationalConference on Artificial Intelligence and Statistics (AISTATS). 297304. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollr, and Ross Girshick.2022. Masked autoencoders are scalable vision learners. In IEEE/CVF Conferenceon Computer Vision and Pattern Recognition (CVPR). 1600016009.",
  "Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. 2015. Learn-ing entity and relation embeddings for knowledge graph completion. In AAAIConference on Artificial Intelligence (AAAI)": "Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju, Haotang Deng, and PingWang. 2020. K-bert: Enabling language representation with knowledge graph. InAAAI Conference on Artificial Intelligence (AAAI). 29012908. Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.2009. BPR: Bayesian personalized ranking from implicit feedback. In InternationalConference on Uncertainty in Artificial Intelligence (UAI). 452461.",
  "Baoxu Shi and Tim Weninger. 2018. Open-world knowledge graph completion.In AAAI Conference on Artificial Intelligence (AAAI)": "Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang.2019. BERT4Rec: Sequential recommendation with bidirectional encoder rep-resentations from transformer. In International Conference on Information andKnowledge Management (CIKM). 14411450. Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. [n. d.]. RotatE: Knowl-edge Graph Embedding by Relational Rotation in Complex Space. In InternationalConference on Learning Representations (ICLR). Yu Tian, Yuhao Yang, Xudong Ren, Pengfei Wang, Fangzhao Wu, Qian Wang, andChenliang Li. 2021. Joint knowledge pruning and recurrent graph convolution fornews recommendation. In International Conference on Research and Developmentin Information Retrieval (SIGIR). 5160.",
  "Kai Wang, Yu Liu, and Quan Z Sheng. 2022. Swift and Sure: Hardness-awareContrastive Learning for Low-dimensional Knowledge Graph Embeddings. InThe Web Conference (WWW). 838849": "Tongzhou Wang and Phillip Isola. 2020. Understanding contrastive representationlearning through alignment and uniformity on the hypersphere. In InternationalConference on Machine Learning (ICML). 99299939. Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat-Seng Chua. 2019.Kgat: Knowledge graph attention network for recommendation. In InternationalConference on Knowledge Discovery & Data Mining (KDD). 950958. Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019.Neural graph collaborative filtering. In International Conference on Research andDevelopment in Information Retrieval (SIGIR). 165174. Xiang Wang, Tinglin Huang, Dingxian Wang, Yancheng Yuan, Zhenguang Liu,Xiangnan He, et al. 2021. Learning intents behind interactions with knowledgegraph for recommendation. In The Web Conference (WWW). 878887. Xiang Wang, Dingxian Wang, Canran Xu, Xiangnan He, Yixin Cao, and Tat-SengChua. 2019. Explainable reasoning over knowledge graphs for recommendation.In AAAI Conference on Artificial Intelligence (AAAI), Vol. 33. 53295336.",
  "Yuhao Yang, Chao Huang, Lianghao Xia, Chunzhen Huang, Da Luo, and KangyiLin. 2023. Debiased Contrastive Learning for Sequential Recommendation. InThe Web Conference (WWW). 10631073": "Yuhao Yang, Chao Huang, Lianghao Xia, and Chenliang Li. 2022. KnowledgeGraph Contrastive Learning for Recommendation. In International Conference onResearch and Development in Information Retrieval (SIGIR). 14341443. Tiansheng Yao, Xinyang Yi, Derek Zhiyuan Cheng, Felix Yu, Ting Chen, AdityaMenon, Lichan Hong, Ed H Chi, Steve Tjoa, Jieqi Kang, et al. 2021. Self-supervisedlearning for large-scale item recommendations. In International Conference onInformation & Knowledge Management (CIKM). 43214330. Junliang Yu, Hongzhi Yin, Jundong Li, Qinyong Wang, Nguyen Quoc Viet Hung,et al. 2021. Self-supervised multi-channel hypergraph convolutional network forsocial recommendation. In The Web Conference (WWW). 413424. Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Lizhen Cui, and Quoc Viet HungNguyen. 2022. Are graph augmentations necessary? simple graph contrastivelearning for recommendation. In International Conference on Research and Devel-opment in Information Retrieval (SIGIR). 12941303. Xiao Yu, Xiang Ren, Yizhou Sun, Quanquan Gu, Bradley Sturt, Urvashi Khandel-wal, Brandon Norick, and Jiawei Han. 2014. Personalized entity recommendation:A heterogeneous information network approach. In International Conference onWeb Search and Data Mining (WSDM). 283292. Fuzheng Zhang, Nicholas Jing Yuan, Defu Lian, Xing Xie, and Wei-Ying Ma.2016. Collaborative knowledge base embedding for recommender systems. InInternational Conference on Knowledge Discovery & Data Mining (KDD). 353362. Wayne Xin Zhao, Gaole He, Kunlin Yang, Hongjian Dou, Jin Huang, Siqi Ouyang,and Ji-Rong Wen. 2019. Kb4rec: A data set for linking knowledge bases withrecommender systems. Data Intelligence 1, 2 (2019), 121136. Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang,Zhongyuan Wang, and Ji-Rong Wen. 2020. S3-rec: Self-supervised learning forsequential recommendation with mutual information maximization. In Interna-tional Conference on Information & Knowledge Management (CIKM). 18931902. Ding Zou, Wei Wei, Xian-Ling Mao, Ziyang Wang, Minghui Qiu, Feida Zhu, andXin Cao. 2022. Multi-level Cross-view Contrastive Learning for Knowledge-awareRecommender System. In International Conference on Research and Developmentin Information Retrieval (SIGIR).",
  ": Hyperparameter Study of KGRec": "In this study, we investigate the sensitivity of KGRec to changesin key hyperparameters, including the masking size , the keepratio for CL graph augmentation , and the temperature for CL. Our analysis reveal that the optimal hyperparameter settingsare highly dependent on the characteristics of the underlying data.Specifically, we found that a masking size of 512 is ideal for MINDand Alibaba-iFashion, while 256 is optimal for Last-FM. Moreover,a CL keep ratio of 0.5 is the best choice for Last-FM and Alibaba-iFashion, while a temperature of 0.1 is recommended for MIND,0.3 for Alibaba-iFashion, and 0.9 for Last-FM. We hypothesize thatthis difference in optimal temperature is due to the sparsity ofthe datasets, with denser datasets requiring higher temperaturesto avoid false-negative samples. We suggest tuning the maskingsize and CL keep ratio in the ranges of and [0.4, 0.6],respectively, as a good starting point for tuning hyperparametersin other datasets. Although KGRec is relatively robust to smallchanges in hyperparameters, selecting the optimal settings is stillcritical for achieving the best performance.",
  "A.2Explainability Study": "In this section, we examine the interpretability of KGRecs recom-mendation results through case studies on knowledge rationaliza-tion. Specifically, we group news items in the MIND dataset bytheir preset categories and obtain the learned knowledge rationalescores for triplets connected to items within the same category. Toprovide an interpretable perspective, we calculate the average ofrationale scores by triplet sets of the same relation and present thecases in . We select cases from five popular news categories,namely sports, newspolitics, travel, finance, and tv-celebrity. For eachcategory, we showcase two of the relations with the highest aver-age global rationale scores of their associated triplets. Our analysisreveals that KGRec is capable of effectively capturing the impact ofuser interests on the KG as rationales.For instance, in the realm of sports news, users tend to focuson league categories and specific teams, and as such, these twotypes of relations in the knowledge graph are rationalized by thelabels of user preferences. Similarly, the case of newspolitics demon-strates that users political news preferences often have a strongpartisan orientation, and they are also concerned with the positionsof political figures. These examples highlight the explainabilityof our KGRec design. By explicitly modeling the hierarchical ra-tionality in the knowledge graph, our approach can differentiatetask rationales that reflect user interests. Moreover, the masking-reconstructing mechanism and cross-view rationale contrastivelearning techniques help to emphasize and strengthen the rationaleconnections. This not only enhances the models interpretabilitybut also improves its performance by leveraging user preferencesto make more accurate predictions. In summary, the rationalizedknowledge graph and the KGRec architecture provide a robustframework for personalized recommendation that considers userpreferences and interests in a structured and transparent manner."
}