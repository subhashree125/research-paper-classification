{
  "ABSTRACT": "Canonical Correlation Analysis (CCA) has been a tried and testedanalytical model which seeks to jointly embed two or more views ofthe data in a maximally correlated latent space. In addition to a pow-erful data mining tool, CCA has many implications for cutting-edgeself-supervised representation learning approaches, as one can casta number of recent approaches as variants of that model. In thiswork we explore what happens when a fundamental assumption ofthat model breaks: what if the alignment across different views ofthe data is unknown? Typically, we would first attempt to align thedifferent views, and subsequently apply CCA or any other modelin order to embed the data views. Can we do better if we alignand embed at the same time? In this work, we seek to jointly solvethe alignment and embedding by formulating and solving bothproblems under the same umbrella of Aligned Canonical Correla-tion Analysis (ACCA). We present a preliminary formulation andalternating optimization algorithm and proof-of-concept results.",
  "INTRODUCTION": "Canonical Correlation Analysis (CCA) [Harold 1936; Kettenring1971] is a classical model which, given two different views of thesame set of entities, e.g., two different bipartite graphs of (user,product) and (user, video) interactions or different feature represen-tations for those entities in general, seeks to project those entities(users) in a low-dimensional space where the different projectedviews are maximally correlated. Essentially, CCA can jointly em-bed heterogeneous datasets in a common low-dimensional space,as it can be extended to more than two views [Chen et al. 2019,2022]. Even though CCA has been in and out of the spotlight for Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior specific permission and/or afee. Request permissions from , August 07, 2023, Long Beach, CA, USA 2023 Association for Computing Machinery.ACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00 many decades and has been around for quite a long time, it is stillextremely relevant, not only as a standalone data mining tool, butalso due to the fact that cutting-edge self-supervised representationlearning techniques, such as Barlow Twins [Balestriero et al. 2023;Bielak et al. 2022; Zbontar et al. 2021; Zhang et al. 2021], can beessentially seen as variations of CCA, where the goal is to embedtwo views of the data (the original view and the augmentation) ina latent space where correlation is maximized.Traditionally, in CCA-style analysis, we assume that entitiesacross views have one-to-one correspondence across the two ormore views of the data, and there is a wealth of algorithms thatstudy different formulations for solving the problem of projectingthose views in that desired maximally correlated space, both linearlyand non-linearly [Andrew et al. 2013]. What if, however, this one-to-one correspondence is unknown? In this case, we are facedwith two problems: (1) entity alignment and (2) CCA embedding.Motivated by recent results [Wu et al. 2022] in the related problemof misaligned joint tensor factorization, it turns out that formulatingand solving the alignment and embedding problems jointly yieldsbetter results than solving each problem separately in multiplesteps, as it appears that the two sub-problems work synergisticallyto produce better quality alignment and embeddings. In this work,we explore this type of formulation for CCA and we propose a newformulation, the Aligned Canonical Correlation Analysis (ACCA),where we seek to jointly compute the alignment and the embeddingspace.The closest formulation to our proposed model is found in [Sahbi2018] where the author is considering linear transformation of thetwo views in CCA, however, is not seeking to recover the precisealignment matrix as our formulation does. In our on-going workwe will consider scenarios where we can fairly compare the twoformulations and understand pros and cons for either one.The list of contributions in this preliminary work are:",
  "minU,V,S||UX S||2 + ||VY S||2(1)": "under the constraint that SS = I which avoids the trivial solution,i.e., U = 0, V = 0, and S = 0, and ensures the latent componentsassembled in the rows of S are uncorrelated to each other. Here,the symbols and respectively stand for matrix transposeand Frobenius norm operators, and I is identity matrix with thesuitable size. The minimization problem in Eq. (1) admits globaloptimal solution: the rows of S are the eigenvectors correspondingto the top- eigenvalues of X(XX)1X + Y(YY)1Y with()1 denoting the matrix inverse operator, U = SX(XX)1, andV = SY(YY)1, e.g., [Harold 1936].",
  "PROPOSED METHOD": "The traditional CCA formulations require the entities/samples fromboth X and Y to be aligned, i.e., the -th columns of X and Y corre-spond to the two views/observations of the same latent data samplewhich is the groundtruth of the -th column of S. However, if suchentity alignment is imperfect, CCA is not able to learn the meaning-ful latent representations shared by two datasets. Toward this end,we propose a novel model, namely aligned canonical correlationanalysis ( ACCA), to jointly learn the latent representations of twoviews and recover the entity alignment between the two views.",
  "Proposed Formulation for ACCA": "Consider two centered datasets X R and Y R aretwo views in one dataset, and the alignment between the columnsof the two views, denoted as P R , is unknown. Our goalis to learn the latent component representation S and predict thealignment matrix P iteratively. Lets denote the estimation of Pas P R . Theoretically, P should be a (binary) permuta-tion matrix, and the sum of row or column is one, which showsthat P is an orthogonal matrix. Mathematically, we will minimizeUX S2 + VYP S2 under the constraints of P as well as theconstraints from CCA, i.e., SS = I. To address the computationallimitation in such optimization problem, we define a list of con-straints to describe different aspects of a permutation matrix insteadof enforcing it to be one, for a tractable optimization solution. Byrelaxing the constraints on P, the optimization formulation of ourproposed ACCA is shown as:",
  ": Output: U, V, S, P": "where , is the (, )-th entry of P, p is the -th row of P, (p)is the entropy of p by viewing the entries of p as a discreteprobability distribution, and the hyperparamters 1, 2, and arenonnegative. Enforcing the low entropy of p guarantees that thedistribution is closed to a deterministic distribution, and the secondand third terms in Eq. (2) will promote the orthogonality of P",
  "EXPERIMENTAL EVALUATION": "To validate the effectiveness of our proposed model ACCA, wewill generate synthetic data with groundtruth P and investigatethe performance of estimated P in terms of the matching accuracybetween the entities in X and Y. In all numerical tests, we set thehyperparameters 1 and 2 to be 0.0001. The initial P is obtainedby solving the optimal matching directly using X and Y withoutconsidering the canonical correlation between the two datasets, i.e.,solving the following minimization problem",
  "Synthetic Data Generation": "We first generate the groundtruth latent representation of the twodatasets, namely Z R , where the columns of Z are i.i.d. sam-ples drawn from multivariate normal distribution with zero meanand identity covariance of size . Next, two aligned datasets Xand Y R are generated from their shared latent representa-tion Z through two independent random projections: X = WZ andY = QZ where W R and Q R . For each experiment,the groundtruth P is a random permutation matrix with only oneentry in each row and column to be 1 and the rest to be 0s. Next,we have two unaligned datasets: X and Y = YP. The involved pa-rameters are set as follows: = 20, = 2, = 7, = 15, and = 10.",
  ": Loss as a function of iterations": "After setting the entropy upper bound hyperparameter to be0.1, we run 10 times of Monte Carlo experiments and report theloss of Eq. (2) for each iteration in . The curve in represents the average loss per iteration and the width of the shadestands for the standard derivation of the loss. Clearly, our proposedACCA converges to a stable point using the generated syntheticdata. K 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7",
  ": Top-k Accuracy of ACCA and Random guess": "In , we report the top- matching accuracy with meanand standard deviation, defined as the percentage of rows in theestimated permutation P whose top entries index set includes thenonzero entry index of the true permutation P, with = 1, 2, 3, 4,and 5, in comparison with such accuracy from random guess whichis /. According to our experimental records as shown in figure2, its obvious that our ACCA framework has significantly betterperformance in predicting the potential alignment between twodatasets, than that obtained from the random guess.Next, we visualize the alignment performance with respect todifferent values of the hyperparameter in where we plotthe real permutation matrix P and the estimated P as gray-scaleimages with darker grid blocks representing higher values of the corresponding entries of P or P. As uniform distribution leads to thehighest entropy, can not exceed () (= 1/ (1/)).With increasing, more nonzero entries are showing up in P asexpected. With proper setup of entropy bound hyperparameter, theperformance of ACCA will be further improved, with the compar-ison of prediction accuracies related to different entropy cases in.",
  "CONCLUSION & FUTURE WORK": "In this preliminary work we investigated the joint CCA-style em-bedding of multiview data and the simultaneous alignment of theembedded entities, by breaking the traditional assumption in CCAthat predicates a known one-to-one matching across views. Weproposed an initial formulation for Aligned Canonical CorrelationAnalysis (ACCA) and derived an alternating optimization algorithmthat produces proof-of-concept results for the viability of this for-mulation. However, there is still a lot of work to be done, and wehope that our preliminary results can serve as a stepping stone tofurther research in this direction.In our on-going and future work we will investigate variationsof the formulation and improvements of the optimization scheme,especially as it pertains to solving for the alignment matrix, which,even though has been radically simplified compared to solving fora permutation matrix, is still a major challenge both in terms ofscalability as well as in terms of finding a precise alignment matrix.Furthermore, we would like to study the alignment matrix as agraph and introduce graph-based constraints which may furtherimprove optimization. Finally, we will investigate connections be-tween our proposed Aligned Canonical Correlation Analysis modeland self-supervised representation learning models. The authors would like to thank Yunshu Wu for initial discussions.Research was supported by the National Science under CAREERgrant no. IIS 2046086 and CREST Center for Multidisciplinary Re-search Excellence in Cyber-Physical Infrastructure Systems (MECIS)grant no. 2112650 and by the US Department of Transportation un-der University Transportation Center (UTC) on Railway Safety. Anyopinions, findings, and conclusions or recommendations expressedin this material are those of the author(s) and do not necessarilyreflect the views of the funding parties.",
  "Hichem Sahbi. Learning cca representations for misaligned data. In Proceedings of theEuropean Conference on Computer Vision (ECCV) Workshops, pages 00, 2018": "Yunshu Wu, Uday Singh Saini, Jia Chen, and Evangelos E Papalexakis. Tenalign: Jointtensor alignment and coupled factorization. In 2022 IEEE International Conferenceon Data Mining (ICDM), pages 568577. IEEE, 2022. Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stphane Deny. Barlow twins:Self-supervised learning via redundancy reduction. In International Conference onMachine Learning, pages 1231012320. PMLR, 2021. Hengrui Zhang, Qitian Wu, Junchi Yan, David Wipf, and Philip S Yu. From canonicalcorrelation analysis to self-supervised graph neural networks. Advances in NeuralInformation Processing Systems, 34:7689, 2021."
}