{
  "ABSTRACT": "Recommender systems aim to fulfill the users daily demands. Whilemost existing research focuses on maximizing the users engage-ment with the system, it has recently been pointed out that howfrequently the users come back for the service also reflects thequality and stability of recommendations. However, optimizing thisuser retention behavior is non-trivial and poses several challengesincluding the intractable leave-and-return user activities, the sparseand delayed signal, and the uncertain relations between users re-tention and their immediate feedback towards each item in therecommendation list. In this work, we regard the retention signalas an overall estimation of the users end-of-session satisfaction andpropose to estimate this signal through a probabilistic flow. Thisflow-based modeling technique can back-propagate the retentionreward towards each recommended item in the user session, and weshow that the flow combined with traditional learning-to-rank ob-jectives eventually optimizes a non-discounted cumulative rewardfor both immediate user feedback and user retention. We verify theeffectiveness of our method through both offline empirical studieson two public datasets and online A/B tests in an industrial platform.The source code is accessible to facilitate replication 1.",
  "Co-first authors. * Corresponding authors": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08",
  "Recommender Systems, Generative Flow Networks, Retention Op-timization": "ACM Reference Format:Ziru Liu, Shuchang Liu, Bin Yang, Zhenghai Xue, Qingpeng Cai*, XiangyuZhao*, Zijian Zhang, Lantao Hu, Han Li, and Peng Jiang*. 2024. ModelingUser Retention through Generative Flow Networks. In Proceedings of the30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining(KDD 24), August 2529, 2024, Barcelona, Spain. ACM, New York, NY, USA,12 pages.",
  "INTRODUCTION": "In the era of information abundance, recommender systems havebecome essential tools that guide users to content that resonateswith their personal preferences . Traditional metrics used toevaluate these systems such as clicks, likes, and ratings areadept at capturing user preferences for each recommended item and are formulated as targets to guide the optimizationof the recommender systems. Despite their effectiveness, they es-sentially estimate the users immediate feedback of items and areincapable of providing a comprehensive assessment of users long-term engagement . For example, when the system findsthat items with compelling features (e.g. addictive content) canmaximize the click rate, it may decide to continuously recommendsuch items. However, these features may initially be appealing butquickly lose the users fondness. This discrepancy suggests a gapbetween the users immediate interest in an item and the sustain-able interest of the system. As a resolution, long-term metricsare adopted to offer deeper insights into users overall satisfaction.One typical example is the user retention signal that describes the",
  "KDD 24, August 2529, 2024, Barcelona, SpainZiru Liu et al": "users return-to-app behavior. This metric is one of the most criticalperformance estimators for many online services since it closelycorrelates with the pivotal business indicator, i.e. Daily Active Users(DAU) .In practice, modeling and optimizing user retention is a chal-lenging task because of its between-session nature. Specifically,the retention behavior does not happen until the user leaves thecurrent session and returns at the beginning of the next session.And it has no clear relation to any single recommendation step inprevious interactions. Furthermore, the users activity between thetwo consecutive sessions is assumed unobservable for the service,introducing extra uncertainty. To circumvent these challenges, ev-idence has found promising results using reinforcement learning(RL) to optimize the cumulative reward for the entire user interac-tion sequence as a surrogate. Intuitively, the user returnsto the platform because the systems overall impression is suffi-ciently positive and attractive, which can be partially measuredby the sum of (positive feedback related) rewards in the session.RL-based recommendation solutions solve the optimization of thiscumulative reward by formulating the user interaction sequence asa Markov Decision Process (MDP) and learning a policy that con-siders the long-term impact of each recommendation action .This allows them to dynamically tailor recommendations at eachpoint of user interaction, adapting to the evolving preferences andoptimizing the cumulative reward of the whole session.Yet, the relation between the session-level cumulative rewardand user retention is still unclear, so the authors in further showthat the RL-based method may directly integrate the cross-sessionretention signal into the long-term value estimation. Though effec-tive, this method is not designed to investigate the influence of eachinteraction on the retention signal (denoted as retention attribu-tion) and it also indirectly optimizes the retention with cumulativeimmediate rewards as a surrogate. Besides, all RL-based solutionsmay suffer from the exploration and exploitation trade-off thatlimits their performance on unstable metrics. This instability isparticularly pronounced in scenarios where user retention dynam-ics are complex, uncertain, and rapidly evolving. In general, wewant to have a stable exploratory solution that can simultaneouslyoptimize user retention and immediate rewards.Inspired by the recent development of Generative Flow Networks(GFNs), we propose an alternative approachGFN4Retention that considers the session-level recommendationas a generation task where the retention signal is directly mod-eled by the trajectory generation probability. Similar to the generalGFN formulation, a probabilistic generation process will finallyconstruct a user session (trajectory) where the target retention re-ward is matched only by the end of the process. Specifically, eachrecommendation step is considered as a conditional forward proba-bilistic flow to the next user state and each user state is associatedwith a flow estimator that represents the generation probabilityof reaching this state. During optimization, the end-of-session ter-minal state directly matches the generation probability with theretention reward. For non-terminal states, a flow matching learningobjective that incorporates an additional backward probabilisticflow is used to back-propagate the retention reward towards everystep in the sequence. This would implicitly model each recommen-dation actions retention attribution. As discussed in , GFNs have demonstrated remarkable strength in generating diverse objectsof high quality, which naturally solves the aforementioned explo-ration challenge of RL. Nevertheless, incorporating GFNs in therecommendation task with retention optimization also brings newchallenges. In its design, GFNs may find it difficult to track the nu-anced changes in user engagement of each recommendation step,since it originally assumes the absence of intermediate rewards.As a countermeasure, we derive an integrated reward system thatcan balance the immediate reward and the retention attributionin each recommendation step. The final reward of GFN4Retentionderives a refined detailed balance loss that matches the session-levelgeneration probability with the product of the retention rewardand the non-discounted cumulative reward of immediate feedback.Secondly, the recommendation space is combinatorially large andthe list-wise recommendation is generated and represented by apoint in the continuous vector space in practice. Weshow that the flow matching property still holds in this continuousspace and illustrate how to design each flow estimation componentaccordingly. To this end, we summarize our key contributions inthis paper as follows: We emphasize the importance of retention optimization in real-world RS applications and introduce GFN4Retention learningframework designed for enhancing user retention while main-taining good exploration. Our proposed solution innovatively derived an integrated rewarddesign with a refined detailed balance objective that can controlthe trade-off between the immediate reward and the retentionattribution in each recommendation step. Besides, we also pro-pose to optimize the flow-matching objectives in the continuousaction space to accommodate the large recommendation spaceof the item list. We validate the superiority of GFN4Retention through extensiveexperiments compared with state-of-the-art RL-based recommen-dation models on both offline and live experiments, and discussthe behaviors of each component with ablation study and param-eter analysis.",
  "Generative Flow Networks": "In recent years, Generative Flow Networks (GFNs) have emerged asan innovative force in generative modeling, introducing a method-ology for learning and sampling within complex distributions. Thecore of GFNs design is its ability to develop a generation policythat directs the probabilistic flow across a state space towards ter-minal states . Specifically, the general formulation considersa generation trajectory S = {1 2 } and assumes aforward probabilistic policy (+1|) that determines each actionduring the generation process. Here represents the state at step, and is the terminal step of the trajectory. The goal of GFNs isto align the generation probability (S) of each trajectory with theobserved reward by the end:",
  ": Overview of GFN4Retention Framework": "which is essentially an energy-based method since the model si-multaneously serves as the generator and evaluator.During optimization, the GFN framework introduces a flow esti-mator F () that assesses the likelihood of traversing through aspecific state . The learning process of flow in GFNs is metic-ulously calibrated to align with the target distribution, ensuringequilibrium between the sums of incoming and outgoing flows:",
  "F () (+1 | ) F (+1) ( | +1)(2)": "where (+1 | ) is the forward probability from to +1, and ( | +1) is the corresponding backward probability that modelsthe likelihood of the source state given the outcome state +1.The foundational work on GFNs has led to the development ofan optimized variant for this objective: the Detailed Balance (DB)loss that minimizes the difference between the forward view andbackward view of each step-wise joint probability (,+1). Thenthe objective follows the flow matching property in the generationprocess which minimizes the Detailed Balance (DB) loss:",
  "Problem Definition": "In our research, we focus on the problem of session-wise recom-mendation, which aims to iteratively suggest items to users andmaximize both retention and immediate feedback of a user session.The session-wise recommendation for a short video applicationscenario is illustrated in . Formally, we consider a set ofusers U and a set of items C. For each session, at any given step ,we may receive a recommendation request from user U, whichconsists of a user feature set A, the users interaction history H,.In recommender systems, the user request provides the necessarycontext information to encode the current user state . Given theuser request and the encoded state, the recommendation policy gen-erates an action that corresponds to a list of items selected from",
  "B ,(4)": "where , represents the users feedback in behavior at step ,and is the weight for behavior . By the end of the session (i.e.at ), we also observe the user retention reward R defined asthe user return frequency which is the core metric in this work.We organize each sample as the tuple (S,1, . . . ,,1, . . . ,, R)that consists of the observed states (i.e. user requests), actions,immediate rewards, and the retention reward of a session. Andwe set two goals in our problem setting: 1) find a valid rewarddesign (S) = (1, . . . ,, R) that combines the retention rewardand the cumulative immediate rewards, and helps boost the overallrecommendation performance; 2) learn a recommendation policythat explores and achieves a better combined reward of (S).",
  "User State Encoding": "While real-world online platforms heavily depend on navigatingthe intricate user dynamics as well as their static features, thechallenge of retention optimization in this scenario is not only theidentification of diverse user patterns but also the agility to adjustto rapid changes in these patterns. In our design, upon receivinga recommendation request from a user U, we consider twomajor types of input including the users feature set A and theinteraction history H,. To better employ the dynamics in the userhistory and find patterns in the items mutual influences, we firstprocess the history H, with a transformer, and the last outputembedding is considered as the history encoding. Then, a userfeature embedding generated from A is concatenated with thehistory encoding and further processed by a neural network togenerate an embedding e that forms the first part of the user state. In practice, we found that merely using a transformer to encodeuser history may over-amplify the most recent history and mayignore the feature-level interactions. As a result, we include a DNN-based context-detecting module that encodes all the contexts in theuser requests and outputs an addition embedding that forms thesecond part of the user state. The detailed framework of the featureencoder and context-detecting module is illustrated in ,which details Part (a) of .",
  "Recommendation Policy as Forward Flow": "During inference, with the encoded user state , we can generatethe action for the recommendation. As widely adopted in many rec-ommender systems, the service has to recommend a list of items foreach user request to meet the latency requirement of users frequentbrowsing behaviors. This means that it is impractical to directlyconsider the enormous item set C as the action space. Alternatively,we choose to consider a vector space for each action , this vectorcan represent the output item list through a deterministic top-Kselection module . In our design, the policy network (i.e. theforward flow estimator) will first output the statistics of the Gauss-ian distribution , = fw() and then sample the action vectoras N (, ). Note that this setting is mathematically differentfrom the original design of GFNs where a small discrete actionspace is adopted. Fortunately, the flow matching property in thecontinuous vector space still holds (explained in Appendix D) andwe can safely apply the flow estimation and the detailed balanceoptimization as we will present in the following sections. Duringtraining, the recommendation policy is regarded as the forwardflow function (+1|) which assumes that the output action determines the next state +1.",
  "Retention Flow Estimation": "Following the general design of GFNs, we include a flow estimatorF () of states and a backward flow function ( |+1) that esti-mates the posterior. In the session-level viewpoint, each observedsession S is a probabilistic trajectory generated by the recommenda-tion policy. Each state may diverge into different future states ac-cording to the sampling process in , and it may be reachable fromvarious previous states. Intuitively, the state flow estimator F ()represents how likely a state is reached. The backward function( |+1) = bw(,,+1) takes the current state-action andthe next state as input and estimates how likely the next state isgenerated by the current state. To guarantee the property F () 0and () 0, we use sigmoid activation for the network output.Then, we can match the trajectory generation likelihood with theobserved retention reward and use the flow matching objective(i.e. Eq(3)) to back-propagate this end-of-session retention signaltowards each intermediate step:",
  "(5)": "where the forward function (+1|) depends on the recommen-dation policy fw as described in the previous section and we useF() to represents that the flow estimator in this framework onlyconsiders the retention reward. As proven by , this design en-sures the learning goal (S) F( ) R. During the learningprocess, the generation may initially be random and generate lowretention reward, but the exploration effect of the policy wouldgradually discover samples with higher retention and the actionsin those sessions will receive higher generation possibility. Eventu-ally, this flow estimation learning framework helps the generationpolicy provide more diversity while maintaining high quality.",
  "Refined Detailed Balance Learning withReward Integration": "Merely optimizing the retention as in the previous section only mod-els the long-term preferences of users. In contrast, the immediaterewards in each intermediate step of the session provide valuablenuanced information that may be ignored by the long-term reten-tion reward. For example, a session that consists of a relevant itemand an irrelevant item may still receive a good retention rewardsince the user may return as long as there is relevant information.However, we should not consider the two items as equal and theusers feedback on each of the items may help us differentiate them.Intuitively, we consider the retention reward and immediate re-wards as complementary views of user preferences. In this section,we illustrate how to integrate these two rewards in the generativeflow estimation and derive a refined DB learning process.",
  "Modeling User Retention through Generative Flow NetworksKDD 24, August 2529, 2024, Barcelona, Spain": "their efficacy in tackling list-wise recommendation challenges ,showcasing their utility in recommendation systems. Our researchleverages GFNs with novel flow-matching formulations and tailoredobjective functions to refine user retention optimization. We noticethat the generative process of GFN may remind readers about thediffusion-based methods which have also been studied in sequen-tial recommendations . However, they are not designed forsparse and delayed return signal modeling since they require preciseand abundant representations for unsupervised learning. Intuitively,we believe our GFN-based solution fits better to this problem sinceit is an energy-based model that can predict the delayed retentionsignal while following the iterative recommendation process.",
  "(log F ( ) + log (+1| ) log F (+1) log ( |+1) )21 1": "(log F ( ) log R)2 = (11)where each steps immediate reward appears and only appearsin the corresponding step-wise DB loss, and the terminal state at = does not observe an immediate reward and only matches theretention flow. This learning objective is further illustrated in Part(c) of . Systematically, our design of flow F for immediaterewards is non-parametric and can naturally integrate into theDB objective with a simple extra term. In contrast, the retentionreward requires the learning of F and the flow back-propagationwith flow matching implicitly achieves the retention attribution. The overall learning framework optimizes towards the integratedgoal of (S) F ( ) = F( )(F ( )). In general, we regardretention and immediate rewards as two complementary aspects ofthe users impression of the recommendation policy. In our solution,immediate rewards provide direct guidance in each step, while theretention flow guides the policy with step-wise attribution.Besides, the values of forward probability () may approachzero deviating significantly from the valid region of the flow esti-mator in the log scale. This discrepancy can introduce high vari-ance that may influence the stability of the gradient computation.Therefore we include a bias as the hyperparameter and the cor-responding log scale estimation becomes log( () + ). Similarly,we also include to stabilize the backward function learning and to reduce the reward variance.",
  "EXPERIMENT": "In this section, we present a comprehensive performance evalu-ation of the GFN4Retention framework through experiments ona simulated user environment for two real-world datasets. Addi-tionally, we extend our evaluation to include online A/B testingconducted on a commercial platform to validate GFN4Retentionseffectiveness in a live environment. The implementation details areprovided in the Appendix A.",
  "MovieLens-1M 3, a widely-used benchmark for RSs, boasts amore extensive scale but with a sparser distribution": "The KuaiRand dataset comprises 12 feedback signals, out ofwhich we focus on six positive feedback signals: click, view time,like, comment, follow, and forward. We also consider twonegative feedback signals: hate and leave, due to their relevance.Feedback signals that occur less frequently are not included in ourstudy to maintain analytical clarity. Additionally, we extend ouranalysis to the ML-1m dataset, a widely recognized benchmark inthe field of Recommender Systems, which contains ratings from6,014 users for 3,417 movies. For the ML-1m dataset, we classifymovies rated above 3 as positive instances (indicative of a like)and the others as negative instances (indicative of a hate), thusenabling a nuanced understanding of user preferences. The statisticsof datasets are presented in .",
  "Simulated User Environment": "We have chosen the KuaiSim retention simulator for our study,which is designed to emulate long-term user behavior on shortvideo recommendation platforms. KuaiSim features two key com-ponents: a leave module, responsible for predicting the likelihoodof a user exiting a session and thereby ending an episode; and areturn module, which estimates the daily probability of a usersreturn to the platform, expressed as a multinomial distribution.",
  "Overall Performance": "To assess the effectiveness of our proposed GFN4Retention model,we conducted a comparative analysis of its overall performanceagainst five baseline models on two datasets. The results are de-tailed in . Additionally, we present the training curves ofGFN4Retention alongside selected baselines focusing on the returnday metric in . From these observations, we note that: The TD3 model registers the weakest performance in terms ofretention metrics. It exhibits higher return time across bothdatasets, suggesting increased intervals between user sessionsand, hence, lower retention rates. This model does not excel in Episode (x10) 1.5 1.6 1.7 1.8 1.9 2.0 2.1",
  "any metric, likely due to its inadequate adaptation to shifts in theenvironment distribution and a policy weakly linked to specificuser behavior patterns, resulting in suboptimal performance": "Among all the baseline models, the RLUR model stands out inretention metrics and shows commendable results in optimiz-ing immediate user feedback. This algorithms design, whichacknowledges the inherent biases of sequential recommenda-tion tasks, adeptly captures user retention dynamics. Despite itsstrengths, the RLUR suffers from training volatility and requiresmore iterations to reach convergence. The SAC model emerges as the most proficient baseline in op-timizing immediate user feedback. It boasts competitive perfor-mance across all metrics and leads in Long View Rate for theKuairend-Pure dataset. Its approach to balancing expected re-turns with policy entropy enables effective modeling of userengagement. Our GFN4Retention model surpasses all other models, includingthe best baseline models, on several crucial metrics. It achievesthe lowest return time, indicating more frequent user engage-ment, and secures the highest scores in Retention and Like Rate,with statistically significant improvements. By integrating imme-diate feedback with the final retention signal in a meticulouslystructured manner, GFN4Retention boosts user retention whilepreserving the quality of immediate user feedback. The modelsconsistency and robustness are further evidenced by the moststable training curves among all baselines. In summary, the GFN4Retention model demonstrates superior per-formance by effectively balancing immediate engagement with userretention, as indicated by its leading scores in critical metrics andsignificant improvements over the baseline models.",
  "Ablation Study": "To rigorously assess the contribution of individual modules withinour proposed GFN4Retention model, we conducted an ablationstudy focusing on the context-detection module and the rewarddesign. This study involves evaluating variations of the full modelwith the omission of particular settings to quantify their respectiveimpacts. We describe the variant models as follows: NCD (No Context Detection): This model functions withoutthe context-detection module, yet retains all other components,providing insight into the significance of context awareness inthe user state encoder.",
  "The results of the ablation study on the GFN4Retention model,employing the Kuairand-Pure dataset, are depicted in . Thisinvestigation yields several critical insights:": "The NCD variant demonstrates commendable performance inimmediate feedback metrics such as click rate and like rate.However, it exhibits an approximate 6.7% increase in returntime relative to the comprehensive GFN4Retention model. Thisoutcome can be principally ascribed to the absence of the context-detection module, which is pivotal in adaptively discerning userpreferences that are integral to retention. These findings emphat-ically highlight the significance of the context-detection modulewithin the models architecture. In contrast to the complete GFN4Retention model, the NIF variantshows a marked 45% reduction in click rate and a 5% reduction inlike rate. The lack of immediate feedback signals during the mod-eling process likely precipitates this decline. Additionally, there isa 4.7% increase in return time compared to the GFN4Retentionmodel, suggesting that only focusing on retention rewards isinsufficient for capturing user preferences. The SIF variant, which incorporates immediate feedback signalsinto the terminal session reward, outperforms the NIF variant,registering gains in both short-term engagement metrics anduser retention. This enhancement underscores the criticality ofmaintaining an equilibrium between immediate and sustaineduser engagement. The SIF variant exhibits a 40% decrease in click rate and a 4%decrease in like rate, along with a 4% increase in return time,compared to the GFN4Retention model. These comparative met-rics clearly demonstrate that our innovative reward structure,coupled with a refined detailed balance objective, significantly NCD NIF SIF GFN 1.47 1.49 1.51 1.53 1.55 1.57 1.59 1.61(a) Return Time NCD NIF SIF GFN 0.200.280.360.440.520.600.680.760.84 (b) Is_click Rate NCD NIF SIF GFN 0.780.790.800.810.820.830.840.850.86 (c) Is_like Rate",
  "Parameter Analysis": "To calibrate the significance of immediate user rewards within ourmodel, we introduce the parameter as part of the session-wisereward formulation, as delineated in Equation (6). This subsectionis devoted to analyzing the impact of this balance parameter onmodel performance, particularly how the integration of immediatefeedback signals affects outcomes. We assess the models perfor-mance across a range of values set at [0.5,0.7,0.9,1.0,1.1,1.3,1.5]with a focus on the return time and click rate metrics, the resultsof which are presented in . The following patterns emergefrom the analysis: When is reduced from 1.0 to 0.5, diminishing the weight ofimmediate user rewards, there is a marked decline in the clickrate. This drop indicates that with less optimization towards im-mediate feedback, user engagement correspondingly decreases.Intriguingly, the return time metric also exhibits a significantincrease, suggesting that the models reduced focus on user en-gagement might adversely affect its capacity to capture userretention dynamics. Conversely, as is incremented from 1.0 to 1.5, thereby increasingthe emphasis on immediate rewards, there is a slight decrease inclick rate, particularly by 2% when reaches 1.5. This nuanceddecline implies that an overly concentrated focus on immediaterewards could slightly detract from optimizing for engagement.Moreover, the heightened attention to immediate rewards seems",
  "Live Experiment": "We conduct the live evaluation of our GFN4Retention solutionthrough an A/B test on a real-world industrial video recommenda-tion platform. The system serves billions of user requests every dayand the daily candidate pool is around several million. The over-all recommender system consists of a relevant candidate retrievalstage and three rank-and-filter stages that gradually scale downthe number of selected items before it is ready for exposure. Asshown in , we deploy GFN4Retention in the ranking scoreensemble modules of two of the ranking stages (i.e. the first rankingstage and the second-ranking stage) with separately learned policymodels. The output action corresponds to the weights that fuse theranking scores from multiple score prediction models. The baselinein the first ranking stage linearly combines the input ranking scoreswith empirically fixed parameters, while the baseline in the sec-ond ranking stage adopts an RL-based solution that automaticallysearches the action space. For each experiment, we holdout 10% ofthe total online traffic for the GFN4Retention solution and 20% of the total online traffic for the baseline. For the retention signal, weuse the reciprocal return time gap as the retention reward of a usersession and the normalized watch time as an immediate reward.Correspondingly, the evaluation consists of the next-day user re-turn frequency and the average watch time which are evaluatedon daily basis. Both metrics are better if larger. We summarize theresults in which proves that the GFN4Retention method cansignificantly improve the users retention and the correspondingimmediate reward. When focusing on the target user group with rel-atively lower activity in the system, the improvement in retentionis more significant. Note that we have applied the proposed methodin the two major ranking scenarios in the system, indicating itsgeneralization ability and scalability in different stages.",
  "Reinforcement Learning Based RSs": "The application of Reinforcement Learning (RL) for recommenda-tions is justified by the underlying Markov Decision Process frame-work, which is fundamental to the RL paradigm .RLs primary benefit in this context is its focus on maximizing theexpected cumulative reward from user interactions over time, ratherthan just enhancing immediate recommendations. In environmentswith limited recommendation options, methods that utilize tabularapproaches or value function estimation have been employed to assess the long-term implications of rec-ommendations. Conversely, in situations characterized by vast ac-tion spaces, policy gradient , and actor-critic method-ologies are preferred for theirability to steer the recommendation policy toward higher-qualityoutcomes. The complexity of optimizing for multiple metrics isaddressed in the literature on multi-objective optimization, high-lighting the varying behavioral patterns among users . Tobridge the discrepancy between real-world user interactions andoffline assessments, user simulators have become a pivotal tool forresearchers . Our methodology extends this direction byperforming offline training in simulated cross-session environmentsfor user retention optimization.In parallel, Generative Flow Networks (GFNs) have surfaced as agroundbreaking approach , drawing parallels with RL but push-ing the boundaries in terms of generating diverse, high-quality sam-ples from intricate distributions. Notably, GFNs have demonstrated",
  "Retention Optimization for RSs": "Shifting away from the traditional focus on immediate feedback,the research community has started delving into strategies forenhancing users long-term satisfaction . Along thisdirection, several studies have investigated methods to boost long-term user engagement by analyzing metrics like dwell time . However, the domain of user retention-oriented optimizationremains relatively underexplored. Notably, a few pioneering effortshave aimed to predict user retention through innovative perspective. , such as the rationale contrastive multi-instancelearning approach, designed to elucidate the factors influencinguser retention and thereby augment its interpretability .A notable advancement includes leveraging decision transformer-based models to tackle user retention challenges, and ingeniouslyreframing the reinforcement learning (RL) problem as an autore-gression issue . Furthermore, some researchers argue that userretentionviewed as feedback accumulated over multiple interac-tions with the systempresents a complex challenge in attributingretention rewards to individual items or sequences . They pro-pose conceptualizing this issue using reinforcement learning tominimize the cumulative time intervals across sessions. Our workbuilds upon these insights into retention optimization, uniquelyconsidering the integration of immediate feedback within the mod-eling framework to better capture the nuances of user retentiondynamics. Moreover, our approach has demonstrated efficacy acrossboth offline datasets and large-scale online platforms.",
  "CONCLUSION": "In this work, we delve into optimizing user retention within rec-ommender systems, a critical aspect for fostering sustained userengagement. Recognizing the intricate nature of long-term userinteractions, we conceptualize the retention signal as a holisticmeasure of user satisfaction at the sessions conclusion. Our ap-proach models this comprehensive estimation through a generativeflow, ingeniously back-propagating the retention reward to eachusers immediate feedback within a session. By employing a simpli-fied flow matching technique alongside a novel DB loss function,our model optimizes long-term retention while also integratingimmediate feedback signals. The efficacy of our methodology isrigorously validated through extensive offline empirical evaluationson publicly available datasets and real-world online A/B testingon a commercial platform, demonstrating its practical applicabilityand effectiveness in enhancing user retention.",
  "ACKNOWLEDGEMENTS": "This research was partially supported by Kuaishou, Research Im-pact Fund (No.R1015-23), APRC - CityU New Research Initiatives(No.9610565, Start-up Grant for New Faculty of CityU), CityU -HKIDS Early Career Research Grant (No.9360163), Hong KongITC Innovation and Technology Fund Midstream Research Pro-gramme for Universities Project (No.ITS/034/22MS), Hong KongEnvironmental and Conservation Fund (No.88/2022), and SIRG- CityU Strategic Interdisciplinary Research Grant (No.7020046,No.7020074).",
  "BALGORITHM": "In this section, we elucidate the optimization algorithm underpin-ning our model, presented through explanatory pseudo-code. As de-lineated in Algorithm 1, the training procedure for GFN4Retentionis methodically straightforward, adhering to a clearly defined se-quence of operations that ensure the models convergence to thedesired objectives. This structured approach facilitates ease of repli-cation and verification of the results reported herein.",
  "DPROBABLISTIC FLOW IN CONTINUOUSSPACE": "For a given state, the flow estimator F () represents the gener-ation likelihood of reaching that particular state. Without loss ofgenerality, assume each sample consists of the transition informa-tion between the current state = 1 and the next state +1 = .The sampled action is denoted as = . Our forward function isassociated with a Gaussian actor , = fw where correspondingaction distribution follows N (, )F ( = 1). Then the for-ward function for the observed transition is (+1|) = ( = )The flow matching property holds at this observed point with thejoint likelihood (+1 = | = 1)F ( = 1)",
  "Shalabh Bhatnagar, Mohammad Ghavamzadeh, Mark Lee, and Richard S Sutton.2007. Incremental natural actor-critic algorithms. Advances in neural informationprocessing systems 20 (2007)": "Qingpeng Cai, Shuchang Liu, Xueliang Wang, Tianyou Zuo, Wentao Xie, BinYang, Dong Zheng, Peng Jiang, and Kun Gai. 2023. Reinforcing User Retentionin a Billion Scale Short Video Recommender System. In Companion Proceedingsof the ACM Web Conference 2023. 421426. Qingpeng Cai, Zhenghai Xue, Chi Zhang, Wanqi Xue, Shuchang Liu, RuohanZhan, Xueliang Wang, Tianyou Zuo, Wentao Xie, Dong Zheng, et al. 2023. Two-Stage Constrained Actor-Critic for Short Video Recommendation. In Proceedingsof the ACM Web Conference 2023. 865875. Wanling Cai and Li Chen. 2020. Predicting user intents and satisfaction withdialogue-based conversational recommendations. In Proceedings of the 28th ACMConference on User Modeling, Adaptation and Personalization. 3342. Lucas Augusto Montalvo Costa Carvalho and Hendrik Teixeira Macedo. 2013.Users satisfaction in recommendation systems for groups: an approach basedon noncooperative games. In Proceedings of the 22nd International Conference onWorld Wide Web. 951958. Praveen Chandar, Brian St. Thomas, Lucas Maystre, Vijay Pappu, Roberto Sanchis-Ojeda, Tiffany Wu, Ben Carterette, Mounia Lalmas, and Tony Jebara. 2022. Usingsurvival models to estimate user engagement in online experiments. In Proceed-ings of the ACM Web Conference 2022. 31863195. Haokun Chen, Xinyi Dai, Han Cai, Weinan Zhang, Xuejian Wang, Ruiming Tang,Yuzhou Zhang, and Yong Yu. 2019. Large-scale interactive recommendation withtree-structured policy gradient. In Proceedings of the AAAI Conference on ArtificialIntelligence, Vol. 33. 33123320. Li Chen, Yonghua Yang, Ningxia Wang, Keping Yang, and Quan Yuan. 2019. Howserendipity improves user satisfaction with recommendations? a large-scale userevaluation. In The world wide web conference. 240250. Minmin Chen, Alex Beutel, Paul Covington, Sagar Jain, Francois Belletti, andEd H Chi. 2019. Top-k off-policy correction for a REINFORCE recommendersystem. In Proceedings of the Twelfth ACM International Conference on Web Searchand Data Mining. 456464. Xiaocong Chen, Lina Yao, Julian McAuley, Guanglin Zhou, and Xianzhi Wang.2021. A survey of deep reinforcement learning in recommender systems: Asystematic review and future directions. arXiv preprint arXiv:2109.03540 (2021). Xiaocong Chen, Lina Yao, Aixin Sun, Xianzhi Wang, Xiwei Xu, and Liming Zhu.2021. Generative inverse deep reinforcement learning for online recommenda-tion. In Proceedings of the 30th ACM International Conference on Information &Knowledge Management. 201210.",
  "Lih-Yuan Deng. 2006. The cross-entropy method: a unified approach to combina-torial optimization, Monte-Carlo simulation, and machine learning": "Rui Ding, Ruobing Xie, Xiaobo Hao, Xiaochun Yang, Kaikai Ge, Xu Zhang, JieZhou, and Leyu Lin. 2023. Interpretable User Retention Modeling in Recom-mendation. In Proceedings of the 17th ACM Conference on Recommender Systems.702708. Gabriel Dulac-Arnold, Richard Evans, Hado van Hasselt, Peter Sunehag, TimothyLillicrap, Jonathan Hunt, Timothy Mann, Theophane Weber, Thomas Degris, andBen Coppin. 2015. Deep reinforcement learning in large discrete action spaces.arXiv preprint arXiv:1512.07679 (2015).",
  "Scott Fujimoto, Herke Hoof, and David Meger. 2018. Addressing function ap-proximation error in actor-critic methods. In International conference on machinelearning. PMLR, 15871596": "Harsha Gwalani. 2022. Studying Long-Term User Behaviour Using Dynamic TimeWarping for Customer Retention. In Proceedings of the Fifteenth ACM InternationalConference on Web Search and Data Mining. 16431643. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. Softactor-critic: Off-policy maximum entropy deep reinforcement learning with astochastic actor. In International conference on machine learning. PMLR, 18611870. Eugene Ie, Vihan Jain, Jing Wang, Sanmit Narvekar, Ritesh Agarwal, Rui Wu,Heng-Tze Cheng, Tushar Chandra, and Craig Boutilier. 2019. SlateQ: A tractabledecomposition for reinforcement learning with recommendation sets. (2019). Eugene Ie, Chih wei Hsu, Martin Mladenov, Vihan Jain, Sanmit Narvekar, JingWang, Rui Wu, and Craig Boutilier. 2019. RecSim: A Configurable SimulationPlatform for Recommender Systems. (2019). arXiv:1909.04847 [cs.LG] Komal Kapoor, Mingxuan Sun, Jaideep Srivastava, and Tao Ye. 2014. A hazardbased approach to user return time prediction. In Proceedings of the 20th ACMSIGKDD international conference on Knowledge discovery and data mining. 17191728.",
  "Zihao Li, Aixin Sun, and Chenliang Li. 2023. DiffuRec: A Diffusion Model forSequential Recommendation. ACM Trans. Inf. Syst. 42, 3, Article 66 (dec 2023),28 pages": "Feng Liu, Ruiming Tang, Xutao Li, Weinan Zhang, Yunming Ye, Haokun Chen,Huifeng Guo, and Yuzhou Zhang. 2018. Deep reinforcement learning basedrecommendation with explicit user-item interactions modeling. arXiv preprintarXiv:1810.12027 (2018). Feng Liu, Ruiming Tang, Xutao Li, Weinan Zhang, Yunming Ye, Haokun Chen,Huifeng Guo, Yuzhou Zhang, and Xiuqiang He. 2020. State representation mod-eling for deep reinforcement learning based recommendation. Knowledge-BasedSystems 205 (2020), 106170. Shuchang Liu, Qingpeng Cai, Zhankui He, Bowen Sun, Julian McAuley, DongZheng, Peng Jiang, and Kun Gai. 2023. Generative flow network for listwise rec-ommendation. In Proceedings of the 29th ACM SIGKDD Conference on KnowledgeDiscovery and Data Mining. 15241534. Shuchang Liu, Qingpeng Cai, Bowen Sun, Yuhao Wang, Ji Jiang, Dong Zheng,Peng Jiang, Kun Gai, Xiangyu Zhao, and Yongfeng Zhang. 2023. Exploration andRegularization of the Latent Action Space in Recommendation. In Proceedings ofthe ACM Web Conference 2023. 833844.",
  "Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An intro-duction. MIT press": "Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. 1999.Policy gradient methods for reinforcement learning with function approximation.Advances in neural information processing systems 12 (1999). Nima Taghipour, Ahmad Kardan, and Saeed Shiry Ghidary. 2007. Usage-basedweb recommendations: a reinforcement learning approach. In Proceedings of the2007 ACM conference on Recommender systems. 113120.",
  "In Proceedings of the 2017 ACM on Conference on Information and KnowledgeManagement. 19271936": "Xin Xin, Alexandros Karatzoglou, Ioannis Arapakis, and Joemon M Jose. 2020.Self-supervised reinforcement learning for recommender systems. In Proceedingsof the 43rd International ACM SIGIR conference on research and development inInformation Retrieval. 931940. Wanqi Xue, Qingpeng Cai, Ruohan Zhan, Dong Zheng, Peng Jiang, Kun Gai,and Bo An. 2022. ResAct: Reinforcing long-term engagement in sequentialrecommendation with residual actor. arXiv preprint arXiv:2206.02620 (2022). Zhengyi Yang, Jiancan Wu, Zhicai Wang, Xiang Wang, Yancheng Yuan, andXiangnan He. 2024. Generate What You Prefer: Reshaping Sequential Recommen-dation via Guided Diffusion. Advances in Neural Information Processing Systems36 (2024).",
  "Dinghuai Zhang, Ling Pan, Ricky TQ Chen, Aaron Courville, and Yoshua Bengio.2023. Distributional gflownets with quantile flows. arXiv preprint arXiv:2302.05793(2023)": "Qihua Zhang, Junning Liu, Yuzhuo Dai, Yiyan Qi, Yifan Yuan, Kunlun Zheng, FanHuang, and Xianfeng Tan. 2022. Multi-Task Fusion via Reinforcement Learningfor Long-Term User Satisfaction in Recommender Systems. In Proceedings ofthe 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.45104520. Kesen Zhao, Shuchang Liu, Qingpeng Cai, Xiangyu Zhao, Ziru Liu, Dong Zheng,Peng Jiang, and Kun Gai. 2023. KuaiSim: A comprehensive simulator for recom-mender systems. Advances in Neural Information Processing Systems 36 (2023),",
  "Kesen Zhao, Lixin Zou, Xiangyu Zhao, Maolin Wang, and Dawei Yin. 2023. UserRetention-oriented Recommendation with Decision Transformer. In Proceedingsof the ACM Web Conference 2023. 11411149": "Xiangyu Zhao, Long Xia, Jiliang Tang, and Dawei Yin. 2019. \" Deep reinforce-ment learning for search, recommendation, and online advertising: a survey\"by Xiangyu Zhao, Long Xia, Jiliang Tang, and Dawei Yin with Martin Vesely ascoordinator. ACM sigweb newsletter Spring (2019), 115. Xiangyu Zhao, Long Xia, Lixin Zou, Hui Liu, Dawei Yin, and Jiliang Tang. 2020.Whole-chain recommendations. In Proceedings of the 29th ACM internationalconference on information & knowledge management. 18831891. Xiangyu Zhao, Liang Zhang, Zhuoye Ding, Long Xia, Jiliang Tang, and Dawei Yin.2018. Recommendations with negative feedback via pairwise deep reinforcementlearning. In Proceedings of the 24th ACM SIGKDD International Conference onKnowledge Discovery & Data Mining. 10401048.",
  "Yufan Zhao, Donglin Zeng, Mark A Socinski, and Michael R Kosorok. 2011.Reinforcement learning strategies for clinical trials in nonsmall cell lung cancer.Biometrics 67, 4 (2011), 14221433": "Guanjie Zheng, Fuzheng Zhang, Zihan Zheng, Yang Xiang, Nicholas Jing Yuan,Xing Xie, and Zhenhui Li. 2018. DRN: A deep reinforcement learning frameworkfor news recommendation. In Proceedings of the 2018 world wide web conference.167176. Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, YanghuiYan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-throughrate prediction. In Proceedings of the 24th ACM SIGKDD international conferenceon knowledge discovery & data mining. 10591068. Lixin Zou, Long Xia, Zhuoye Ding, Jiaxing Song, Weidong Liu, and Dawei Yin.2019. Reinforcement learning to optimize long-term user engagement in recom-mender systems. In Proceedings of the 25th ACM SIGKDD International Conferenceon Knowledge Discovery & Data Mining. 28102818."
}