{
  "ABSTRACT": "Dynamic graph learning equips the edges with time attributesand allows multiple links between two nodes, which is a crucialtechnology for understanding evolving data scenarios like trafficprediction and recommendation systems. Existing works obtain theevolving patterns mainly depending on the most recent neighborsequences. However, we argue that whether two nodes will haveinteraction with each other in the future is highly correlated withthe same interaction that happened in the past. Only considering therecent neighbors overlooks the phenomenon of repeat behavior andfails to accurately capture the temporal evolution of interactions.To fill this gap, this paper presents RepeatMixer, which considersevolving patterns of first and high-order repeat behavior in theneighbor sampling strategy and temporal information learning.Firstly, we define the first-order repeat-aware nodes of the sourcenode as the destination nodes that have interacted historically andextend this concept to high orders as nodes in the destination nodeshigh-order neighbors. Then, we extract neighbors of the sourcenode that interacted before the appearance of repeat-aware nodeswith a slide window strategy as its neighbor sequence. Next, weleverage both the first and high-order neighbor sequences of sourceand destination nodes to learn temporal patterns of interactionsvia an MLP-based encoder. Furthermore, considering the varyingtemporal patterns on different orders, we introduce a time-awareaggregation mechanism that adaptively aggregates the temporalrepresentations from different orders based on the significance oftheir interaction time sequences. Experimental results demonstratethe superiority of RepeatMixer over state-of-the-art models in linkprediction tasks, underscoring the effectiveness of the proposedrepeat-aware neighbor sampling strategy.",
  "Corresponding Author": "Permission to make digital or hard copies of part or all of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for third-party components of this work must be honored.For all other uses, contact the owner/author(s).KDD 24, August 2529, 2024, Barcelona, Spain. 2024 Copyright held by the owner/author(s).ACM ISBN 979-8-4007-0490-1/24/08",
  "INTRODUCTIOIN": "Dynamic graph learning has been employed in various scenarioswith evolving graph input data, such as recommendation systems, patent applicant trend prediction and social networks. To capture the fine-grained temporal information, existingworks treat dynamic graphs as sequences of times-tamped interactions arranged in chronological order and derivenode representations from their historical neighbor sequences. : We show a dynamic graph evolves from 0 to 6in (a). Notably, some interactions occur multiple times, suchas the interaction between 6 and 1. Now we aim to predictwhether 6 will interact with 1 at timestamp 9. To generatethe temporal representations of6 and1, we obtain neighborsequences via sampling strategies (b) and (c).",
  "KDD 24, August 2529, 2024, Barcelona, Spain.Tao Zou, Yuhao Mao, Junchen Ye, and Bowen Du": "Sami Abu-El-Haija, Joshua V. Dillon, Bahare Fatemi, Kyriakos Axiotis, NeslihanBulut, Johannes Gasteiger, Bryan Perozzi, and MohammadHossein Bateni. 2023.SubMix: Learning to Mix Graph Sampling Heuristics. In UAI 2023, July 31 - 4August 2023, Pittsburgh, PA, USA, Vol. 216. PMLR, 110. Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Nazanin Alipourfard, KristinaLerman, Hrayr Harutyunyan, Greg Ver Steeg, and Aram Galstyan. 2019. MixHop:Higher-Order Graph Convolutional Architectures via Sparsified NeighborhoodMixing. In ICML 2019, 9-15 June 2019, Long Beach, California, USA, Vol. 97. PMLR,2129.",
  "PRELIMINARIES": "Definition 1. Dynamic Graph. A dynamic graph is representedas a sequence of non-decreasing chronological interactions denotedby G = {(1, 1,1), (2, 2,2), . . . , (, ,)}, where 0 1 2 . . . . In this representation, and signify the sourcenode and destination node, respectively for the -th interactionoccurring at timestamp . The set of all nodes is denoted by N.Each node N is associated with a node feature R ,and each interaction (, ,) is characterized by an edge feature, R . Here, and represent the dimensions of the nodefeature and edge feature. For non-attributed graphs, we set the nodefeature and edge feature to zero vectors, i.e., = = 0.Definition 2. Problem Formalization. Given the source node, destination node , timestamp , and historical interactions be-fore , i.e., {(, ,)| < }, representation learning on dynamicgraph aims to design a model to learn time-aware representations R for interaction with as the dimension. We validatethe effectiveness of the learned representations via dynamic link",
  "METHODOLOGY": "shows the framework of our approach, which consists ofthree components: repeat-aware neighbor sampling strategy, Re-peatMixer, and time-aware representations learning modules. Givenan interaction (, ,) that we aim to predict, we first sample the firstand high-order neighbor sequence for each node, which capturesthe pair-wise temporal patterns among historical sequences. Thesecond part introduces an MLP-like encoder to learn the temporalpatterns of interactions based on first and higher-order neighbor se-quences. Ultimately, we apply a time-aware representation learningmodule to generate the interaction representations from differentorders at timestamp according to the significance of their timeinteraction sequences. Lastly, the generated representation wouldbe used for downstream tasks in dynamic graph analysis.",
  "Repeat-aware Neighbor Sampling Strategy": "Selecting appropriate neighbors holds paramount significance acrossdiverse applications, such as graph learning, multi-hop ques-tion answering , and recommendation systems . In dynamicgraph learning, conventional approaches often focus on samplingneighbors exclusively from the nodes themselves, disregarding thesignificance of the interacted nodes. This oversight prevents modelsfrom learning repeat patterns in the neighbor sampling process.To address this limitation, we propose a novel sampling strategyexplicitly considering repeat behavior between two nodes. Thisrefined approach aims to retrieve more pertinent mutual tempo-ral patterns, presenting a solution to enhance the effectiveness ofdynamic graph learning.First-order Repeat-aware Neighbor Sampling Process. Givenan interaction (, ,), we first define repeat-aware nodes of thesource node as the historically interacted destination node. Thenwe employ a slice window strategy to select s recent neigh-bors that interacted before the appearance of repeat-aware nodesas our neighbor sequences for the source node . This is formallyexpressed by N,,1 = [|(,,) G (, ,) G 0 <, , < ], where , and , is the appeared position ofneighbor and in s historical neighbors. Similar to the sourcenode, we treat the repeat-aware nodes of the destination node asthe source node that have interacted in the past and obtain theneighbor sequence of as N,,1 . However, it is noteworthy thatsome interactions may not be present in the historical data. Hence,we acquire their most recent neighbors to form neighbor sequences.High-order Repeat-aware Neighbor Sampling Process. Inthe higher-order sampling process, we also explore the temporalpatterns and repeat behavior for interactions. To achieve this, weinitiate by retrieving the 1-th level neighbor sequences of nodes and , denoted as N,,1 and N,,1. For node , we first definethe -th level repeat-aware nodes as the destination node s 1-th level neighbors. Hence, for the node N,,1, we searchthe repeat-aware nodes in its historical neighbors and choose therecent neighbors that interacted before the appearance of repeat-aware nodes as -th neighbor sub-sequences. We then aggregateall the neighbor sub-sequences of nodes in s 1-th level as",
  "RepeatMixer": "In this section, we aim to generate the temporal representationsfor the current interaction = (, ,) from the first and higher-order neighbor sequences. Specifically, we first sample the first andhigher-order neighbor sequences of node and with our repeat-aware neighbor sampling strategy. Then we capture the correlatedtemporal information between two nodes via an MLP-based encoderbased on the sampling neighbor sequences. To capture varioustemporal patterns of neighbors at different levels, we obtain thetemporal representations from first and higher-order perspectives.Finally, we aggregate the representations from all neighbors in thesequences to obtain first and higher-order temporal representations,denoted as , and ,.Identical Information Encoding. As introduced in , adynamic graph is defined as a sequence of timestamped interactionsassociated with node features and edge features. Hence, we firstembed the identical information for nodes in first and higher-orderneighbor sequences with node feature, edge feature, and time in-terval information. For example, for node N,,1, node and edge features are obtained from the dynamic graph G, as R and, R . To embed the time interval information, we follow and apply cos() functions to map the time interval = intoa continuous vector, where is the timestamp of the interactionbetween and . By modeling time interval information, we couldlearn the evolving patterns of nodes. The equation is as follows,",
  "= =": ", following the approach outlined in .In this work, we encode the nodes in first and second-orderneighbor sequences of and . Given the greater relevance of re-cent behaviors in capturing temporal information, we select themost recent neighbors from each sequence to learn temporal in-formation in the subsequent sections. To ensure a consistent lengthof for each nodes neighbor sequence, we apply zero-paddingto neighbor sequences that have fewer than neighbors. As aresult, the embeddings of N,,1 and N,,2 are denoted as ,1 =",
  ": Framework of RepeatMixer": "3.2.1Representation Learning for First-order Temporal Information.Given an interaction = (, ,), we first obtain the embeddings oftheir first-order neighbor sequences with node features, edge inter-action features, and time interval information, represented as ,1and ,1 respectively. Then we capture the long-term dependenciesand correlated structure information between two sequences viaan MLP-like encoder. Lastly, we generate the first-order tempo-ral representations , for the interaction by aggregating theinformation from all neighbors in the sequence.Temporal Information Fusion. To capture long-term tem-poral dependencies for nodes and correlated patterns in interac-tions, we merge the sequences from and and utilize an MLP-based architecture to learn the temporal interaction infor-mation. Furthermore, to facilitate the model in discerning infor-mation from node or , we introduce trainable segment em-beddings , R for each node in N,1 and N,1, repre-sented as and for sequences N,1 and N,1. Subsequently,we concatenate the sequential encodings from and to form,1 = [,1||; ,1||] R2(+ ). Next, we derive thetemporal embeddings , via an MLP-based encoder, which isbuilt by stacking two MLP blocks. Before each block, we addLayerNorm , and after each block, we employ a residual connec-tion . Additionally, we use GeLU as the activation functionbetween fully-connected layers. The process is as follows,",
  "R , ,1 R ,,2 R , ,2 R,,1 R ,": ",1 R,,2 R and ,2 R are trainable parametersat the -th layer in the encoder. We set = and = as the dimension of hidden size. The output of the -th layer isdenoted by ,1, and we average the neighbors representationsin the concatenated sequence as local temporal representations, R, which is calculated by,",
  "=1,1[, :].(8)": "3.2.2Representation Learning for High-order Temporal Information.In the realm of static graph learning, numerous studies , leverage higher-order structures to capture intricate topologyinformation, such as triangles , motifs , and communities. In our work, we aim to capture the repeat behavior pat-terns in high-order neighbor sequences for learning the structureinformation. To strike a balance between efficiency and accuracy,we capture the second-order neighbors temporal information inour work.Temporal Information Fusion. Our definition of repeat be-haviors for node at the second-order level means that they areassociated with nodes of s first-order neighbors. Therefore, weproceed to learn the correlation between s first-order neigh-bors and s second-order neighbors as s higher-order tempo-ral information. Similar to the encoding for nodes in first-orderneighbors, we incorporate segment embeddings for the nodes inthe neighbor sequences of and . Hence, we obtain the embed-dings of s higher-order temporal information as ,2 and ,1as ,,2 = [,2||; ,1||] R2(+ ). Then we capturethe long-term sequential and higher-order topology information",
  "Repeat-Aware Neighbor Sampling for Dynamic Graph LearningKDD 24, August 2529, 2024, Barcelona, Spain": "impact on the models performance, as it effectively captures thecorrelations between nodes. The inclusion of temporal encoding in-formation provides valuable insights into the interaction frequencyand evolving patterns of nodes, leading to improved performance.Moreover, incorporating trainable embeddings to distinguish se-quences from different nodes contributes to the models ability tolearn unique temporal information.",
  "Time-aware Representations Learning": "Based on the analysis in , it indicates that time intervalsequences play a crucial role in capturing the evolving patternsof nodes. These sequences provide vital information regardingthe frequency of interactions and behavioral patterns exhibitedby the nodes. Hence, we obtain the time-aware representations ofinteraction = (, ,) by averaging the first-order representations, and higher-order representations , adaptively based on thesignificance in their time interaction sequences.Temporal Sequence Similarity. To aggregate the temporalrepresentations from both first-order and higher-order neighbors,we initially calculate the importance score between the two usinga pearson correlation coefficient (PCC) similarity function. Thisscore serves as an indicator of the similarity between the respectivesequences. Subsequently, we normalize these scores and utilizethem as weights to aggregate the first-order and higher-order rep-resentations together.",
  "= , + ,.(18)": "It is worth noticing that we could also aggregate temporal rep-resentations from higher orders beyond first and second-order.The process of calculating temporal sequence similarity amonghigher-order neighbors is similar to the second-order temporal rep-resentations based on the combination of adjacent time intervalsequences from two nodes. Lastly, we could utilize the generatedtemporal representations for downstream tasks.",
  "Experimental Settings": "Datasets. We leverage six publicly available real-world datasets,namely Wikipedia, Reddit, MOOC, LastFM, Enron, and UCI, col-lected by (refer to Appendix A.1 for detailed descriptions). Thedataset statistics are presented in . Specifically, we providethe ratios of repeat behaviors, namely as \"Ratio of Repeat Behav-iors\". Based on the statistics, it is observed that more than half ofthe interactions have occurred multiple times. Particularly note-worthy is that in the \"Reddit\" and \"Enron\" datasets, over 90 percentof interactions have been observed during the testing phase.Baselines. In our experimentation, we compare our model againstnine well-established continuous-time dynamic graph learningbaselines. These baselines span various techniques, including mem-ory networks (i.e., JODIE , DyRep , and TGN ), graphconvolutions (i.e., TGAT ), random walks (i.e., CAWN),statistics methods (i.e., EdgeBank ), MLP-based models (Graph-Mixer ), and sequential models (i.e., TCL, and DyGFormer).The descriptions of baselines are shown in Appendix A.1.Evaluation Tasks and Metrics. Our evaluation centers on thedynamic prediction task, aligning with established methodologiesin prior works . This task is characterized by two",
  "Avg. Rank8.339.836.336.335.836.337.176.174.502.672.50": "settings: 1) a transductive setting, where the objective is to pre-dict future links between nodes observed during training, and 2)an inductive setting, which aims to predict future links involvingpreviously unseen nodes. To accomplish this, we employ a multi-layer perceptron, concatenating either node representations frombaselines or edge representations from our model to predict linkprobabilities. We choose Average Precision (AP) and Area Underthe Receiver Operating Characteristic Curve (AUC-ROC) as evalua-tion metrics. Consistent with , we evaluate our work with threenegative sampling strategies: random (rnd), historical (hist), andinductive (ind). The latter two strategies are particularly challeng-ing due to their inherent complexities, as expounded upon in .Dataset splits adhere to a chronological distribution, with a 70%,15%, and 15% ratio assigned to training, validation, and testing.Implementation Details. To ensure consistent performancecomparisons, we adopt the settings and performance metrics ofthe baseline models as outlined in . The Adam optimizer is em-ployed, and training spans 100 epochs, with a patience of 20 duringearly stopping. The model achieving the best performance on thevalidation set is selected for testing. Across all datasets, the learningrate and batch size are set to 0.0001 and 200, respectively. Specifi-cally, in our sampling strategy, we select the recent 10 repeat-awarenodes in first and second-order neighbor sequences for searchingand the length of slide window is set as 5. The hyper-parameter of and is set as 0.4 and 4.0. In all models, the dimensions ofnode features and edge features are set to 172. The time encodingdimensions are consistent at 100 across all models. The remain-ing settings of the models remain unchanged as described in theirrespective papers. The experiments are executed on an Ubuntumachine featuring an Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHzwith 16 physical cores. The GPU device utilized is the NVIDIA TeslaT4 with 15 GB memory. The number of neighbors sampled in theneighbor sequences of different models is shown in Appendix A.2.Our code is available at",
  "Performance Comparison and Discussions": "Due to space limitations, we present the performance of differentmethods on the AP and AUC metrics for transductive dynamic linkprediction using three negative sampling strategies in . No-tably, the results are multiplied by 100 for improved readability. Thebest and second-best results are highlighted in bold and underlinedfonts. Additional results on AP for both transductive and inductivelink prediction tasks can be found in . It should be notedthat EdgeBank can only be evaluated for transductive dynamiclink prediction, hence its results under the inductive setting arenot presented. In our work, we perform RepeatMixer(F) and Re-peatMixer, which utilize just first-order neighbors and incorporatesecond-order temporal information for generating final temporal",
  "Effects of Repeat-aware Neighbor SamplingStrategy": "In this study, we introduce the repeat-aware Neighbor SamplingStrategy (i.e., repeat-aware NSS), which aims to capture the corre-lations between nodes. To evaluate the ability to capture temporalpatterns for interactions, we conduct three experiments from dif-ferent perspectives, including generalizability, effectiveness, andstability. To reduce the effect of high-order neighbors, we just em-ploy the neighbor sequences from the first order in this section.Generalizability of Repeat-aware Neighbor Sampling Strat-egy. Our repeat-aware neighbor sampling strategy is not only spe-cific to RepeatMixer but can also be applied to other dynamic graphlearning methods that rely on sampling recent neighbors to learntemporal information, such as TGN, TCL, and GraphMixer. Hence,we replaced the original neighbor sampling strategy (i.e., recentneighbor sampling strategy) in these methods with our repeat-aware neighbor sampling strategy. The AP on the transductive linkprediction is presented in . Interestingly, we observed thatTGN, TCL, and GraphMixer consistently achieve superior results",
  "repeat-behaviors NSS98.62 0.0392.95 0.1288.16 0.2294.76 0.08": "when using our repeat-aware neighbor sampling strategy. Thisfinding suggests that by incorporating our sampling strategy, thesemodels can capture a greater amount of correlated temporal infor-mation for each interaction. Among these three baselines, TGNand TCL both capture the temporal correlation between two nodesin the model, while GraphMixer learns the temporal patterns ofnodes via their individual neighbor sequences. Hence, GraphMixergets large improvement in these datasets by considering the cor-related information between nodes. The consistent improvementacross different methods further highlights the effectiveness andversatility of our repeat-behaviors neighbor sampling strategy.",
  ": Performance of baselines equipped with our repeat-aware neighbor sampling strategy": "Comparison with Various Neighbor Sampling Strategies.We conduct experiments to evaluate the effectiveness of our pro-posed NSS by comparing it with various neighbor sampling strate-gies, which include Recent NSS, Uniform NSS, and Time-Aware NSS.Specifically, Recent NSS selects the most recent neighbors from thehistorical neighbor sequences of nodes. Uniform NSS uniformlysamples neighbors from the historical neighbor sequences of nodes.Time-aware NSS incorporates a parameter to probabilisticallyselect neighbors from the historical sequences, giving priority toeither recent or uniform sampling. In our work, we set to 0.2. Theresults of these experiments are summarized in .Our findings demonstrate that our proposed sampling strategyeffectively captures the temporal repeat behaviors between nodesand correlations of two nodes from their neighbor sequences. Re-cent NSS samples recent neighbors to capture recent behaviors of",
  ": Effects of sampling different numbers of neighborsin neighbor sequences in RepeatMixer(F)": "Stability of Varied Length in Sampled Neighbor Sequence.In this study, we utilize a repeat-aware neighbor sequence consist-ing of recent neighbors to extract temporal representations forinteractions. To further assess the impact of different lengths ofhistorical neighbors on capturing temporal information for nodes,we conduct experiments with varying values of in the range of for transductive and inductive link predic-tion. The average precision is illustrated in .From , it reveals that our model achieves optimal per-formance when the number of historical neighbors falls withina certain range. Furthermore, the performance shows relativelystable variations across datasets such as \"Wikipedia,\" \"LastFM,\" and\"UCI\" even when the number of neighbors varies significantly. Thisobservation suggests that sampling neighbors from a pairwise per-spective enables the filtering of irrelevant neighbors, facilitating theextraction of effective temporal dependencies between two nodes.",
  "Effects of Time-aware AggregationMechanism": "In our work, we propose a time-aware aggregation mechanismthat incorporates the significance of time interaction sequencesbetween two nodes to fuse the temporal representations adaptivelyamong different orders. To evaluate the effectiveness of adaptiveaggregation according to time interaction sequences, we exper-iment with different kinds of fusion of first and high-order rep-resentations, including the summation and concatenation of tworepresentations. Specifically, summation represents summing thefirst and high-order representations directly, i.e., = , + ,,namely \"Summation\" while concatenation is implemented with a",
  ": Effects of different components in RepeatMixer(F)": "The analysis of the results reveals that in datasets requiringhigh-order temporal patterns, such as \"MOOC\" and \"LastFM,\" tra-ditional aggregation methods like summation and concatenationmay struggle to adaptively combine first and high-order temporalrepresentations due to their fixed nature of aggregation. In contrast,our mechanism excels at discerning the crucial temporal patternsbetween the first and high-order levels. This ability allows our ap-proach to dynamically adjust the aggregation ratios based on theimportance of these temporal patterns, leading to the generationof more informative representations.",
  "Ablation Study": "We conduct an ablation study to further validate the effectivenessof certain designs in RepeatMixer. To assess the impact of high-order neighbor dependencies, we conduct experiments using Re-peatMixer(F). We examine the impact of Time Encoding (TE) andSegment Encoding (SE) by removing these modules and denotingthem as \"w/o TE\" and \"w/o SE\" respectively. Besides, we also sepa-rate the neighbors sequence of the source node and the destinationnode to encode separate temporal information, denoted as \"w SepE\".The results are shown in .Our findings indicate that RepeatMixer(F) achieves the best per-formance when all components are utilized, and the performancedeteriorates when any component is removed. Particularly, the en-coding of concatenation on two sequences has the most substantial",
  "Experiments on Large Datasets": "Furthermore, we conduct a thorough evaluation of our model ontwo large datasets, \"tgbl-wiki-2\" and \"tgbl-review-v2\", designed fordynamic link prediction in . During testing, the \"tgbl-wiki-2\"dataset utilizes all nodes in the graph as negative samples. Base-line performance results are obtained from . The performanceis presented in . Considering the dynamic nature of thedatasets, which exhibit repeat behaviors and high temporal simi-larity in neighbor sequences of two nodes, our model showcasesexceptional proficiency in sampling pertinent neighbors from his-torical sequences. This unique ability empowers us to effectivelycapture the temporal correlations between nodes.",
  "RELATED WORKS5.1Dynamic Graph Learning": "In recent years, substantial studies have been proposed toexplore representation learning for dynamic graphs, helping betterunderstand evolving networks in real-life. Dynamic graph learningfalls into two categories. One branch considers the dynamicgraph as a sequence of snapshots, which treats each snapshot thatcontains the interactions up to a certain time. However, these meth-ods have to split dynamic graphs into snapshots, which fails tocapture fine-grained temporal information. To tackle the problems,continuous-time approaches treat the dynamic graphs as a flowof timestamped interactions. To learn the temporal information,they either apply temporal random walks to generate tem-poral structures for nodes or save evolving graph structures intomemorable representations . Besides, employsequential models to learn the long-term temporal dependencies.Although dynamic graph learning has succeeded, most existingmethods fail to sample repeat-aware neighbors in interaction andignore the higher-order topology structure. In this paper, we design a repeat-aware neighbor sampling strat-egy that incorporates the evolving patterns of interactions intothe sampling process, which models both the first and high-orderneighbor information simultaneously.",
  "Graph Sampling Strategy": "Graph sampling strategy is a fundamental aspect of graphanalysis and processing, which reduces computational complexitywhile preserving the essential structural properties of the graph.The graph sampling strategy is divided into three categories. Theearly works collect the neighbors of all nodes in a mini-batchand then sample the entire neighborhood for the batch and proceedrecursively layer by layer. Another branch designs node-wise sampling that modifies the neighborhood by taking a randomsubset containing at most neighbors, which learns the overalldistribution of nodes in the graph. Recent works proposea graph sampling algorithm to drop boundary nodes from otherpartitions and ensure the connectivity among minbatch nodes. Ourobservations indicate that nodes within dynamic graphs exhibitrecurrent patterns, suggesting a tendency for repeated interactionsover a given period. Hence, we design a repeat-aware neighborsampling strategy that considers the pair-wise instead of node-wisetemporal information to capture the patterns.",
  "Repeat Behavior": "Repeat consumption refers to an item that repeatedly appearedin a users historical sequences, which is important in sequentialrecommendation. For example, proposed that theitem from timesteps ago is re-consumed with a probability pro-portional to a function of . designed a KNN-based model tocapture repeated consumption behaviors while integrated apsychological theory of human cognition into re-listening musictasks. However, repeat behavior remains unexplored in dynamicgraph learning. Hence, our work analyzes the connections betweentemporal interactions and repeat behavior in dynamic graphs.",
  "CONCLUSION": "In this paper, we proposed a dynamic graph learning method Re-peatMixer with a pair-wise neighbor sampling strategy. Instead oflearning the individual temporal frequency of nodes, we concen-trated on the correlations between nodes in historical interactionsby sampling repeat-aware neighbors, which helped us learn theevolving patterns of interactions. To obtain full temporal informa-tion, we modeled the both first and high-order neighbor sequencesvia an MLP-base encoder. Besides, a time-aware aggregation mech-anism was introduced to adaptively fuse the temporal represen-tations from first and high-order neighbors. Experimental resultsshowed that our method could achieve competitive performanceby learning first- and high-order node correlations via the repeat-aware neighbor sampling strategy.",
  "Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer Normaliza-tion. CoRR abs/1607.06450 (2016)": "Maciej Besta, Raphael Grob, Cesare Miglioli, Nicola Bernold, Grzegorz Kwas-niewski, Gabriel Gjini, Raghavendra Kanakagiri, Saleh Ashkboos, Lukas Giani-nazzi, Nikoli Dryden, and Torsten Hoefler. 2022. Motif Prediction with GraphNeural Networks. In KDD 22, Washington, DC, USA, August 14 - 18, 2022. ACM,3545. Jianfei Chen, Jun Zhu, and Le Song. 2018. Stochastic Training of Graph Convo-lutional Networks with Variance Reduction. In ICML 2018, Stockholmsmssan,Stockholm, Sweden, July 10-15, 2018, Vol. 80. PMLR, 941949. Rong Chen, Jiaxin Shi, Yanzhe Chen, Binyu Zang, Haibing Guan, and HaiboChen. 2018. PowerLyra: Differentiated Graph Computation and Partitioning onSkewed Graphs. ACM Trans. Parallel Comput. 5, 3 (2018), 13:113:39. Weilin Cong, Si Zhang, Jian Kang, Baichuan Yuan, Hao Wu, Xin Zhou, HanghangTong, and Mehrdad Mahdavi. 2023. Do We Really Need Complicated ModelArchitectures For Temporal Networks?. In ICLR 2023, Kigali, Rwanda, May 1-5,2023. OpenReview.net. Alexander Dallmann, Daniel Zoller, and Andreas Hotho. 2021. A Case Studyon Sampling Strategies for Evaluating Neural Sequential Item RecommendationModels. In RecSys 21, The Netherlands, 27 September 2021 - 1 October 2021. ACM,505514.",
  "Hui Fang, Danning Zhang, Yiheng Shu, and Guibing Guo. 2020. Deep Learningfor Sequential Recommendation: Algorithms, Influential Factors, and Evaluations.ACM Trans. Inf. Syst. 39, 1 (2020), 10:110:42": "Fabrizio Frasca, Beatrice Bevilacqua, Michael M. Bronstein, and Haggai Maron.2022. Understanding and Extending Subgraph GNNs by Rethinking Their Sym-metries. In NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. William L. Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive Represen-tation Learning on Large Graphs. In Advances in Neural Information ProcessingSystems 30: Annual Conference on Neural Information Processing Systems 2017,December 4-9, 2017, Long Beach, CA, USA. 10241034.",
  "Dan Hendrycks and Kevin Gimpel. 2016. Bridging Nonlinearities and StochasticRegularizers with Gaussian Error Linear Units. CoRR abs/1606.08415 (2016)": "Haoji Hu, Xiangnan He, Jinyang Gao, and Zhi-Li Zhang. 2020. Modeling Person-alized Item Frequency Information for Next-basket Recommendation. In SIGIR2020, Virtual Event, China, July 25-30, 2020. ACM, 10711080. Shenyang Huang, Farimah Poursafaei, Jacob Danovitch, Matthias Fey, WeihuaHu, Emanuele Rossi, Jure Leskovec, Michael M. Bronstein, Guillaume Rabusseau,and Reihaneh Rabbany. 2023. Temporal Graph Benchmark for Machine Learningon Temporal Graphs. CoRR abs/2307.01026 (2023).",
  "Walks to Temporal Random Walks. In (IEEE BigData 2018), Seattle, WA, USA,December 10-13, 2018. IEEE, 10851092": "Farimah Poursafaei, Shenyang Huang, Kellin Pelrine, and Reihaneh Rabbany.2022. Towards Better Evaluation for Dynamic Link Prediction. In NeurIPS 2022,New Orleans, LA, USA, November 28 - December 9, 2022. Markus Reiter-Haas, Emilia Parada-Cabaleiro, Markus Schedl, Elham Motamedi,Marko Tkalcic, and Elisabeth Lex. 2021. Predicting Music Relistening BehaviorUsing the ACT-R Framework. In RecSys 21, 27 September 2021 - 1 October 2021.ACM, 702707.",
  "Emanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, FedericoMonti, and Michael M. Bronstein. 2020. Temporal Graph Networks for DeepLearning on Dynamic Graphs. CoRR abs/2006.10637 (2020)": "Kartik Sharma, Mohit Raghavendra, Yeon-Chang Lee, Anand Kumar M, andSrijan Kumar. 2023. Representation Learning in Continuous-Time DynamicSigned Networks. In CIKM 2023, Birmingham, United Kingdom, October 21-25,2023. ACM, 22292238. Ilya O. Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, XiaohuaZhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, JakobUszkoreit, Mario Lucic, and Alexey Dosovitskiy. 2021. MLP-Mixer: An all-MLPArchitecture for Vision. In NeurIPS 2021, December 6-14, 2021. 2426124272.",
  "Rakshit Trivedi, Mehrdad Farajtabar, Prasenjeet Biswal, and Hongyuan Zha.2019. DyRep: Learning Representations over Dynamic Graphs. In ICLR 2019, NewOrleans, LA, USA, May 6-9, 2019. OpenReview.net": "Lu Wang, Xiaofu Chang, Shuang Li, Yunfei Chu, Hui Li, Wei Zhang, XiaofengHe, Le Song, Jingren Zhou, and Hongxia Yang. 2021. TCL: Transformer-basedDynamic Graph Modelling via Contrastive Learning. CoRR abs/2105.07944 (2021). Yanbang Wang, Yen-Yu Chang, Yunyu Liu, Jure Leskovec, and Pan Li. 2021.Inductive Representation Learning in Temporal Networks via Causal AnonymousWalks. In ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. Xixi Wu, Yun Xiong, Yao Zhang, Yizhu Jiao, Caihua Shan, Yiheng Sun, YangyongZhu, and Philip S. Yu. 2022. CLARE: A Semi-supervised Community DetectionAlgorithm. In KDD 22, DC, USA, August 14 - 18, 2022. ACM, 20592069. Da Xu, Chuanwei Ruan, Evren Krpeoglu, Sushant Kumar, and Kannan Achan.2020. Inductive representation learning on temporal graphs. In ICLR 2020, AddisAbaba, Ethiopia, April 26-30, 2020. OpenReview.net. Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton,and Jure Leskovec. 2018. Graph Convolutional Neural Networks for Web-ScaleRecommender Systems. In KDD18, London, UK, August 19-23, 2018. ACM, 974983. Minji Yoon, Thophile Gervet, Baoxu Shi, Sufeng Niu, Qi He, and Jaewon Yang.2021. Performance-Adaptive Sampling Strategy Towards Fast and AccurateGraph Neural Networks. In KDD 21, Virtual Event, Singapore, August 14-18, 2021.ACM, 20462056.",
  "Le Yu, Leilei Sun, Bowen Du, and Weifeng Lv. 2023. Towards Better DynamicGraph Learning: New Architecture and Unified Library. In NeurIPS 2023, NewOrleans, LA, USA, December 10 - 16, 2023": "Wenchao Yu, Wei Cheng, Charu C. Aggarwal, Kai Zhang, Haifeng Chen, andWei Wang. 2018. NetWalk: A Flexible Deep Embedding Approach for AnomalyDetection in Dynamic Networks. In KDD 2018, London, UK, August 19-23, 2018.ACM, 26722681. Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Vik-tor K. Prasanna. 2020. GraphSAINT: Graph Sampling Based Inductive LearningMethod. In ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Kaike Zhang, Qi Cao, Gaolin Fang, Bingbing Xu, Hongjian Zou, Huawei Shen, andXueqi Cheng. 2023. DyTed: Disentangled Representation Learning for Discrete-time Dynamic Graph. In KDD 23, Long Beach, CA, USA, August 6-10, 2023. ACM,33093320.",
  "AAPPENDIXA.1Datasets and Baselines": "Datasets. Specific statistics of datasets are shown in . Specif-ically, \"#N&L Feat\" stands for the dimensions of node and link fea-tures and the ratio of repeat behaviors in training, validation, testand the whole datasets are computed in \"Ratio of Repeat Behaviors\".The detailed descriptions of datasets are shown as follows. Wikipedia: The nodes in this graph represent the users andpages, while the links indicate the connections between them.Each link is associated with a 172-dimensional LinguisticInquiry and WordCount (LIWC) feature. Furthermore, thisdataset includes dynamic labels that indicate whether usersare temporarily banned from editing. Reddit: Reddit is a bipartite graph that tracks and stores theposts made by users in various subreddits over one month.In this graph, users and subreddits are represented as nodes,while the links between them correspond to timestampedposting requests. Each link in this graph is associated with a172-dimensional LIWC feature. MOOC: MOOC is an online platform that operates as a bi-partite interaction network, consisting of two types of nodes,namely students and course content units such as videosand problem sets. The links between these nodes representa students access behavior towards a specific content unit,and each link is associated with a 4-dimensional feature. LastFM: LastFM is a bipartite system that contains dataon the songs that users listened to within a one-month pe-riod. In this setup, users and songs serve as nodes, while theconnections between them represent the listening habits ofusers.",
  "Baselines": "JODIE: JODIE is designed for temporal bipartite net-works of user-item interactions. It employs two coupledrecurrent neural networks to update the states of users anditems. A projection operation is introduced to learn the fu-ture representation trajectory of each user/item. DyRep: DyRep proposes a recurrent architecture toupdate node states upon each interaction. It also includes atemporal-attentive aggregation module to consider the tem-porally evolving structural information in dynamic graphs. TGN: TGN maintains an evolving memory for eachnode and updates this memory when the node is observedin an interaction, which is achieved by the message function,message aggregator, and memory updater. An embeddingmodule is leveraged to generate the temporal representationsof nodes.",
  "also equipped with a time encoding function for capturingtemporal patterns": "CAWN: CAWN first extracts multiple causal anony-mous walks for each node, which can explore the causalityof network dynamics and generate relative node identities.Then, it utilizes recurrent neural networks to encode eachwalk and aggregates these walks to obtain the final noderepresentation. EdgeBank: EdgeBank is a memory-based approachspecifically designed for transductive dynamic link predic-tion. Unlike models with trainable parameters, EdgeBankoperates solely on a memory unit where it stores observedinteractions. Predictions are made based on whether an inter-action is retained in the memory. If an interaction is stored, itis predicted as positive; otherwise, it is classified as negative. TCL: TCL initiates the generation of each nodes inter-action sequence by utilizing a breadth-first search algorithmon the temporal dependency interaction sub-graph. It thenintroduces a graph transformer that takes into account boththe graph topology and temporal information to learn noderepresentations. Additionally, it integrates a cross-attentionoperation to model the interdependencies between two in-teraction nodes. GraphMixer: GraphMixer demonstrates that a fixedtime encoding function outperforms the trainable version.It incorporates this fixed-function into a link encoder basedon MLP-Mixer to learn from temporal links. A nodeencoder using neighbor mean-pooling is implemented tosummarize node features. DyGFormer: DyGFormer captures the correlationsbetween the source node and target node via a neighborco-occurrence encoding method based on their historicalsequences and proposes a patching technique to effectivelyand efficiently learn longer historical neighbors by dividingthe sequence into several patches.",
  "A.4Complexity Analysis": "RepeatMixer is a sequence-based model based on a repeat-awareneighbor sampling strategy. In the neighbor sampling strategy, wefirst recognize the repeat-aware nodes in neighbor sequences, thenselect the recent neighbors before repeat-aware nodes. In the first-order sampling process, since the repeat-aware nodes are the sourcenodes and destination nodes, the complexity is () since it willcompare over the whole neighbor sequence for source nodes anddestination nodes. In the second-order sampling process, we selectthe recent one-hop neighbors of source nodes or destinationnodes as the repeat-aware nodes, hence, the complexity is (2)since we have to search all the neighbors sequences of source nodes first-order neighbors and destination nodes first-order neighbors.Therefore, the whole-time complexity of our strategy both in thefirst-order and second-order sampling process is ( +2). SinceRepeatMixer is with MLPMixer encoder, the time complexity ofencoding features is () + () = (2), where ()and () are time complexity for channel mixer and token mixer.Hence, the total complexity for our model is ( + 2 + )."
}