{
  "ABSTRACT": "Graph Neural Networks (GNNs) have revolutionized graph-basedmachine learning, but their heavy computational demands posechallenges for latency-sensitive edge devices in practical indus-trial applications. In response, a new wave of methods, collectivelyknown as GNN-to-MLP Knowledge Distillation, has emerged. Theyaim to transfer GNN-learned knowledge to a more efficient MLPstudent, which offers faster, resource-efficient inference while main-taining competitive performance compared to GNNs. However,these methods face significant challenges in situations with insuffi-cient training data and incomplete test data, limiting their applica-bility in real-world applications. To address these challenges, wepropose AdaGMLP, an AdaBoosting GNN-to-MLP Knowledge Distil-lation framework. It leverages an ensemble of diverse MLP studentstrained on different subsets of labeled nodes, addressing the issue ofinsufficient training data. Additionally, it incorporates a Node Align-ment technique for robust predictions on test data with missing orincomplete features. Our experiments on seven benchmark datasetswith different settings demonstrate that AdaGMLP outperforms ex-isting G2M methods, making it suitable for a wide range of latency-sensitive real-world applications. We have submitted our code tothe GitHub repository (",
  "Corresponding Author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior specific permission and/or afee. Request permissions from acronym XX, June 0305, 2018, Woodstock, NY 2018 Association for Computing Machinery.ACM ISBN 978-1-4503-XXXX-X/18/06...$15.00 ACM Reference Format:Weigang Lu, Ziyu Guan, Wei Zhao, and Yaming Yang. 2018. AdaGMLP:AdaBoosting GNN-to-MLP Knowledge Distillation. In Proceedings of Makesure to enter the correct conference title from your rights confirmation emai(Conference acronym XX). ACM, New York, NY, USA, 12 pages.",
  "INTRODUCTION": "Graph Neural Networks (GNNs) haverevolutionized the field of graph-based machine learning, enablingstate-of-the-art performance in various domains, including socialnetworks , recommendation systems , and bioinformat-ics . However, the neighbor-fetching operations in GNNs makeit hard for practical industrial applications, particularly when itcomes to latency constraints in numerous edge devices.The quest for more efficient alternatives to GNNs has given riseto a new generation of methods, known as Graph Neural Networkto Multi-Layer Perceptrons (MLPs) Knowledge Distillation (G2MKD) techniques . The primary idea is to transfer theknowledge learned by a GNN teacher into a MLP student via knowl-edge distillation , which is graph-agnostic. G2M methods enablefaster and less resource-intensive inference while maintaining com-petitive performance compared to GNNs.Despite their promise, G2M KD methods face two critical chal-lenges that restrict their real-world applicability: insufficient train-ing data and incomplete test data. In many real-world scenarios,acquiring labeled graph data is a costly and time-consuming processand they often contain nodes with missing or incomplete features,particularly in the context of test (unseen) data. For example, inindustries like finance and e-commerce, dealing with insufficient orincomplete data is a daily challenge since many customers refuseto provide (part of) their information. Ensuring the robustness ofstudents in the presence of insufficient training data and incompletetest data is crucial for making informed decisions.Unfortunately, the above challenges are ignored by existing G2Mmethods. In the insufficient training data case, traditional G2Mmethods employing a single MLP student can easily memorize thelimited training data rather than learn general patterns from it,inducing degraded performance on test data. It is a more seriouschallenge on G2M than GNNs since GNNs can at least fetch neigh-bor information to obtain a more general picture of the graph. Inthe incomplete test data case, current G2M methods, which are typ-ically designed for complete data, may struggle to make inferenceover the feature-missing data.",
  "Conference acronym XX, June 0305, 2018, Woodstock, NYWeigang Lu, Ziyu Guan, Wei Zhao, and Yaming Yang": "Weigang Lu, Ziyu Guan, Wei Zhao, Yaming Yang, Yuanhai Lv, Lining Xing,Baosheng Yu, and Dacheng Tao. 2023. Pseudo contrastive learning for graph-based semi-supervised learning. arXiv preprint arXiv:2302.09532 (2023). Weigang Lu, Yibing Zhan, Binbin Lin, Ziyu Guan, Liu Liu, Baosheng Yu, Wei Zhao,Yaming Yang, and Dacheng Tao. 2024. SkipNode: On Alleviating PerformanceDegradation for Deep Graph Convolutional Networks. IEEE Transactions onKnowledge and Data Engineering (2024), 114.",
  "RELATED WORKS": "In this section, we introduce the works of transferring knowledgefrom a larger GNN teacher to a smaller student GNN or MLP. Specif-ically, we represent them as G2G (GNN-to-GNN) or G2M (GNN-to-MLP) KD (Knowledge Distillation), respectively. Graph-to-Graph Knowledge Distillation. Prior researches have primarily focused on training compact stu-dent GNNs from more expansive GNNs using KD techniques .For example, methodologies like LSP and TinyGNN facili-tate the transfer of localized structural insights from teacher GNNsto student GNNs. RDD delves into the reliability aspects ofnodes and edges to enhance the G2G KD. Although the student model used in CPF is MLP, it additionally leverages label prop-agation, which still requires latency-inducing neighbor fetching.Nevertheless, these approaches still necessitate neighbor fetching,which can be impractical for applications where latency is a criticalconcern. Graph-to-MLP Knowledge Distillation. In response to latencyconcerns, recent advancements propose employing MLP students,eliminating the need for message passing during inference andshowcasing competitive performance against GNN students. Apioneer work, GLNN introduces a general G2M frameworkwithout propagations. It trains an MLP student guided by bothground-truth labels and soft labels from the GNN teacher. KRD develops a reliable sampling strategy to train MLPs with confidentknowledge. Additionally, NOSMOG combines both structuraland attribute features, which serve as inputs to MLPs, thus es-tablishing a structure-aware MLP student. Similarly, GSDN introduces topological information into student training stage. Be-sides, FF-G2M explores and provides low- and high-frequencyknowledge from the graph for the student. While traditional G2Mmethods have made notable strides in mitigating latency concernsand enabling efficient knowledge transfer, they still exhibit certainlimitations, particularly when faced with challenges related to lim-ited training data and feature missing scenarios. We will discussboth limitations in Sec. 4.",
  "PRELIMINARIES": "Notions. We denote a graph as G = (V, E), where V and E arethe node set and edge set, respectively. let represent the totalnumber of nodes. Node features are usually represented by thematrix X R , where each row x corresponds to the node s-dimensional feature vector. The adjacency matrix A R indicates neighbor connections, where A = 1 if there is an edge(, ) E, and 0 otherwise. In this paper, we use capital lettersto represent matrices, with corresponding lowercase letters usedto denote specific rows within these matrices. For example, xrepresents the -th row vector of X. Node Classification Problem Statement. The label matrix isrepresented by Y R consisting of one-hot vectors, where is the number of classes. We use the superscript and to divideall the nodes into labeled (V, X, and Y) and unlabeled parts(V , X , and Y ). The goal of node classification problem is topredict Y with A, X, and Y available. Graph Neural Networks. Generally, most GNNs follow the message-passing scheme. That is, The representation h of each node un-dergoes iterative updates within each layer by gathering messagesfrom its neighbors, denoted as N (). In the -th layer, h()is com-puted from the representation of the previous layer through anaggregation process denoted as AGGR, which is then followed byan UPDATE operation. This can be formally expressed as:",
  "AdaGMLP: AdaBoosting GNN-to-MLP Knowledge DistillationConference acronym XX, June 0305, 2018, Woodstock, NY": "In the transductive setting, AdaGMLP demonstrates superior clas-sification accuracy compared to other G2M methods and even ex-ceeds its GNN teachers on various datasets. In the inductive set-ting, AdaGMLP competes well with the SOTA methods. The averageimprovement over GLNN ( ), which is the representativemethod, varies across datasets.Its worth noting that AdaGMLP doesnt consistently outperformNOSMOG in some cases as it dose in the transductive setting. It isbecause NOSMOG benefits from access to test (unseen) node struc-tural information, which is not typically available in real-worldscenarios. Considering this, the strong performance of NOSMOGin the inductive setting should be interpreted with caution. It maynot be the best choice for real-world scenarios where structuralinformation about test nodes is unknown. In contrast, AdaGMLP per-forms competitively in the inductive setting without relying onany information about unseen nodes. This highlights its practicalapplicability and versatility, as it can handle scenarios where thetest nodes structure is not available, making it a more robust choicefor real-world applications.",
  "Citeseer": "GCNGLNNKRDNOSMOGAdaGMLP (Ours) : [Challenge 2] Incomplete Test Data. TraditionalG2M methods suffer from performance consistent dropswhen more features are missing. Our AdaGMLP consistentlymaintains a high accuracy level, outperforming other G2Mmethods as the fraction of missing features increases. [Challenge 2] Incomplete Test Data. Real-world graph data isfrequently incomplete, with missing features in test (new) nodes.However, traditional G2M KD methods ignore such situations andare trained under the complete datasets. When faced with feature-missing test data, they may yield suboptimal results due to thelack of mechanisms to effectively cope with this inherent incom-plete features issue. This limitation becomes increasingly criticalwhen making predictions on real-world graphs with incompleteinformation. In , we visualize the performance of differ-ent G2M methods under varying levels of missing features on theCora and Citeseer datasets. Unlike GCN that achieves a relativelystable performance, the performance of traditional G2M methodsgradually decrease as more features are masked since they fail toteach the MLP student how to handle feature-missing situations.Instead, AdaGMLP tends to achieve more stable performance thancounterparts due to our Node Alignment module (discussed inSec. 5).",
  "Towards Addressing these Challenges": "To address these aforementioned challenges, we propose an Ad-aBoosting GNN-to-MLP KD (AdaGMLP) framework to address thesesituations that impede the performance of existing G2M methods.Regarding Challenge 1, we tackle this by harnessing an AdaBoost-style ensemble of multiple MLP students trained ondifferent subsets of labeled nodes. This strategy encourages diver-sity in learned patterns and mitigates the risk of over-reliance on",
  "Classification": ": Illustration of AdaGMLP. In (a), for each MLP, we com-pute the KL loss using node weights, which are determined bythe difference between MLP and corresponding GNN outputs(Knowledge Distillation). Additionally, we calculate the CEloss by comparing the sampled labeled nodes with their re-spective ground-truth labels (Random Classification). In (b),we begin by obtain incomplete nodes with randomly mask-ing the features of the selected nodes and inputting theminto the MLP. Subsequently, we employ Mean Squared Error(MSE) loss to align their hidden representations and outputs(Node Alignment). In this section, we introduce AdaGMLP, a methodology designedto tackle the challenges of G2M distillation while bolstering general-ization and model capacity. AdaGMLP consists of a pre-trained GNNas the teacher and a compact student network with MLPs with layers. illustrates the architecture, showcasing three funda-mental components: Random Classification (RC), Node Alignment(NA), and AdaBoosting Knowledge Distillation (AdaKD).",
  "Random Classification": "We denote each MLP student as MS1, MS2, ..., MS. Their respectiveoutputs are represented as Z1, Z2, ..., Z R . To enhancethe student networks generalizability, we introduce randomnessinto the inputs for MS1, MS2, ..., MS1 by selecting |V |/ nodesrandomly from V without replacement. The remaining nodes areused as the input for MS, where represents the floor function.Assume the labeled node subset of MS is V , the classification",
  "Node Alignment": "The primary idea behind Node Alignment is to align the represen-tations of nodes with complete features (labeled nodes) and thosewith masked features (masked nodes) since we often encounterdatasets where labeled nodes have complete feature information,while unlabeled nodes have missing features. To illustrate this, letx R represent a complete node, and x signify a corrupted nodewith a fraction of its features randomly masked, where (0, 1).Consequently, we obtain outputs zand zas well as hidden",
  "AdaBoosting Knowledge Distillation": "We leverage AdaBoosting to obtain the collective power of multipleMLP students, further enhancing MLP studentss generalizationand performance. To achieve this, we adapt SAMME (StagewiseAdditive Modeling using a Multi-class Exponential loss function)algorithm , which is an extension of the standard two-classAdaBoost, to propose the KD-SAMME algorithm for combiningMLP students in the context of G2M.In KD-SAMME, we compute weighted error (), relying onKL-divergence for quantifying knowledge point (node) dissimilar-ity. The divergence between each node pair is denoted as ()=DKL((z ), (z)). The error () is determined as:",
  "=1 ,(11)": "where denotes the weight of -th node and > 0 controls thesensitivity to divergence between knowledge point pairs. This di-vergence captures the dissimilarity between individual knowledgepoints extracted from both the teacher and student models.Subsequently, we leverage this error information to compute acorresponding combining weight () for each MLP student as:",
  "Complexity": "AdaGMLPs computational complexity primarily derives from themultiple MLPs in the ensemble and the operations involved in NodeAlignment and AdaBoosting techniques. Assuming each MLP in theensemble comprises two layers, including a transformation from -dimensional input features to-dimensional hidden representationsand a projection from these hidden dimensions to -dimensionaloutputs, the computational complexity for each MLP is ( +).For MLPs, the combined complexity for Node Alignment amountsto (2( + )). Furthermore, the AdaBoosting process, whichupdates weights and combines predictions across MLPs, contributesadditional complexity. This aspect of the process is proportional tothe number of nodes and the ensemble size, represented as (),where is the number of nodes. Therefore, the time complexity ofour AdaGML is (2( + ) + ) for training and (( +) + ) for inference.In most cases, the hidden dimensionality often exceeds ,allowing AdaGMLPto utilize relatively lighter MLPs with smaller while still maintaining high performance. This approach not onlyenhances computational efficiency but also ensures that the modelremains robust and effective across various learning scenarios.",
  "Experiment Setting": "Dataset. Similar to , we use six public benchmark graphs, i.e.,Cora , Citeseer , Pubmed , Coauthor-CS, Coauthor-Physics, Amazon-Photo , and a large-scale graph ogbn-arxiv .The statistics of datasets are provided in Appendix C. Baselines. There are three types of baselines in this paper: (1) GNNTeachers including GCN , GraphSAGE , and GAT ; (2)SOTA G2M Methods containing GLNN , NOSMOG , andKRD ; (3) SOTA G2G Methods including CPF , RDD ,TinyGNN , GNN-SD , and LSP . The comparison betweenAdaGMLP and G2G methods is described in Appendix D. Implementation. The code of AdaGMLP is built on via DGLlibrary and we implement each MLP student with the sameconfiguration (hidden dimensionality, number of layers) as its GNNteachers. We tune {2, 3, 4} for all the experiments except for thehyper-parameter analysis. Due to the space limitation, we presentthe search spaces of other hyper-parameters in the Appendix A.",
  "Citeseer1%64.141.7262.744.3855.029.6463.165.2863.842.132%67.101.3465.546.3959.3414.5466.424.1666.462.613%69.061.8269.783.7167.303.6969.354.1369.961.94": "We conducted experiments with varying label rates (1%, 2%, and3%) on the Cora and Citeseer datasets in . The goal wasto assess how well AdaGMLP could perform compared to GCN andother G2M methods in scenarios with limited labeled data.Traditional G2M methods struggle to match the performance ofthe GCN teacher in the low-label-rate settings. This is primarilybecause these single-student methods might be easily over-fit tolimited labeled data. As a result, they tend to show higher standarddeviations compared to GCN teacher and our AdaGMLP.AdaGMLP demonstrates superior adaptability and performance inthis setting. Its ability to capture and utilize information efficientlyfrom limited labeled nodes allows it to outperform traditional G2Mmethods and even the GCN teacher in some cases. Additionally,AdaGMLPs robustness (smaller standard deviation) across differentlabel rates demonstrates its potential in real-world applicationswhere obtaining a large amount of labeled data is challenging orexpensive.",
  "Pubmed": "10%77.220.5267.643.6077.940.7374.193.4180.260.2820%76.860.3867.243.4877.440.8973.143.9379.370.4330%76.340.3565.342.5776.601.0273.092.2378.141.1640%76.140.8264.664.0675.421.1472.923.3877.420.5650%76.050.9860.014.5574.311.4772.434.2976.481.25 methods suffer from a significant drop in accuracy. This indicatestheir vulnerability to missing data, limiting their practicality inreal-world scenarios where data completeness cannot be guaran-teed. It is mainly due to three reasons: (1) Lack of Mechanismsfor Feature-missing Data: existing G2M methods are typicallytrained on complete datasets where all features are available withthe lack of mechanisms to effectively cope with missing features;(2) Limited Feature Information: these G2M methods, relyingon fixed feature vectors for prediction, cannot generalize well inthe feature-missing test data.AdaGMLP consistently outperforms other G2M methods acrossall missing rates and even exhibits better performance over GCN inalmost all the cases. It can be attributed to the Node Alignment mod-ule that teaches each MLP student to align feature-missing nodesand complete nodes. Additionally, the AdaBoost-style ensembleapproach encourages each student to collectively compensate forthe missing information by aggregating diverse knowledge fromdifferent subsets, resulting in more robust predictions.This robust-ness demonstrates AdaGMLPs ability to handle real-world scenarioswith incomplete data effectively.",
  "Hyper-parameters Analysis (Q4)": "In this section, we provide comprehensive analysis on Cora datasetto probe into three hyper-parameters in AdaGMLP, i.e., balanceweight of LRC and LAdaKD, balance weight NA of NA-H andNA-O, and sensitivity weight of knowledge point pairs divergence.To obtain more focused analysis, we remove the Node Alignmentmodule when the interested hyper-parameter is not NA.In (a), we tune from 0 to 1 with interval of 0.1 usingvarious GNN teachers. We can observe a noticeable and consistentdrop in performance across all teacher models when exceeds acertain large threshold, e.g., 0.9. This phenomenon can be explainedby considering the role of in balancing classification loss LRC and",
  ": Hyper-parameter Analysis on , NA, and": "knowledge distillation loss LAdaKD. When is set to be very large,the model places an overwhelming emphasis on minimizing theclassification error during training. This may lead to overfitting onthe training data and the teachers knowledge not being effectivelydistilled into the student. More interestingly, AdaGMLP maintainshigh performance at = 0 (complete knowledge distillation). It canbe attributed to our AdaBoost Knowledge Distillation, which allowsstudents to effectively transfer valuable knowledge from GNNs.In (b), we observe a notable phenomenon: in high featuremissing settings (e.g., 70% missing rate), smaller values of NA leadto better results, while in low feature missing scenarios (e.g., 10%missing rate), larger values of NA are more effective. With a higherfeature missing rate, retaining information through NA-H becomescrucial since the limited available features in test nodes can behardly classified. Smaller NA values emphasize NA-H and allowthe model to focus more on preserving hidden representations,which are essential in recovering information from incompletefeatures, thereby obtaining higher performance. Conversely, witha substantial portion of features available, there is ample featureinformation available for most nodes. Consequently, the modelcan exploit this rich data to generate meaningful outputs. A largerNA value allocates more importance to the alignment of nodesbased on their output representations, which acts like consistencyregularization over label information , to obtain more robustpredictions.In (c), we explore the sensitivity of different teachermodels to varying values of from 0.5 to 4 with interval of 0.5.The parameter plays a significant role in AdaBoost KnowledgeDistillation, as it controls the importance of individual instancesin the ensemble. Larger values make student more sensitive tounder-distilled node pairs. The analysis suggest that we shouldavoid using extremely small .",
  ": Ensemble Size () Analysis": "In this ensemble size () ablation experiment conducted usingAdaGMLP across various datasets and teacher models, we aim toexplore the sensitivity of to model performance. The resultsreveal following noteworthy insights.Across all datasets and teacher models, we observe that as increases, the classification accuracy generally improves. This sug-gests that increasing the ensemble size contributes positively tothe models performance. However, its essential to note that theimprovement tends to saturate as becomes larger. It indicatesthat there is an optimal point beyond which further increasing may not significantly benefit the models performance. The sensi-tivity of to model performance suggests that AdaGMLP can benefitfrom larger . Researchers can tailor the ensemble size based ontheir available computational resources and the dataset at hand.Smaller may suffice for some cases, while others may requirelarger ensembles to maximize accuracy.",
  ": Ablation Study": "In this ablation experiment, we investigate the impact of differentmodules within AdaGMLP under two different settings, includinginsufficient training data and incomplete test data. We use GCNas the teacher model and set = 0.5, NA = 0.5, = 2, = 3 asthe default setting. Different modules (RC, AdaKD, NA-O, NA-H,NA) within AdaGMLP are systematically disabled to analyze theirindividual contributions. The results (shown in ) provideinsights into the role of each module. Random Classification (RC). Removing the RC module, whichinvolves randomly sampling training data for each student, leadsto a significant drop in accuracy across all label rates and datasets.The decline is more obvious in insufficient training data setting,as shown in (a). This is because, in the absence of RC,students are trained on a fixed subset of data, potentially leadingto overfitting. The randomness introduced by RC helps mitigateoverfitting and ensures that students see diverse examples during",
  "training. It is essential for improving generalization and robustnessin the presence of insufficient training data": "AdaBoosting Knowledge Distillation (AdaKD). EliminatingAdaKD results in a noticeable performance decrease in all the cases.AdaKD contributes to improving the students knowledge by boost-ing its ability of knowledge transferring. Its role is vital for main-taining high accuracy. Moreover, it has a significant impact onperformance with insufficient training data. This is because whenthere is limited supervision, AdaKD can help student learn fromthe teachers soft labels and provide additional supervision. Node Alignment (NA). The NA module, formed by integratingNA-H and NA-O, is effective in maintaining model performance,especially under the incomplete test data setting, as shown in Fig-ure 6(b). Removing both NA-H and NA-O leads to more pronouncedperformance drops, highlighting the value of their synergy withinthe NA module. These modules enable students to recover represen-tations from the corrupted nodes, which is vital when dealing withincomplete test data. Without this alignment, students struggle tomake predictions on unseen or partially observed nodes.In summary, these modules serve complementary roles, andtheir removal impacts performance differently based on the specificchallenges posed by insufficient training data or incomplete testdata.",
  ": Accuracy vs. Inference Time (ms)": "In , we study the trade-off between performance andefficiency (inference time cost) of AdaGMLP and SOTA G2M meth-ods on Pubmed dataset. For fair comparison, we use 3-layer GCNwith 1024 hidden units as the teacher model and tune the hiddenunits in {32, 128, 1024} for all the student model(s). We fix at 3for AdaGMLP.Despite a slight increase in inference time compared to othermethods, AdaGMLP offers significantly better accuracy. This trade-off is often acceptable in real-world applications, where predictiveperformance is paramount. Besides, AdaGMLP achieves impressiveaccuracy (83.34%) even with a relatively low hidden dimensionof 128. Therefore, the increase in inference time cost due to theuse of multiple MLPs is counterbalanced because of the compactstudent model design. The MLPs in AdaGMLPare designed to becompact, with fewer parameters compared to the potentially otherMLP students which demand more parameters to maintain the expressive ability. This design choice significantly reduces the com-putational load for each MLP. Another essential practical advantageof AdaGMLP is its inherent parallelizability. AdaGMLPs architectureallows for efficient parallel computation across multiple studentmodels. This feature can significantly reduce inference time inscenarios where parallel processing is feasible.",
  "CONCLUSION": "In this work, we introduce AdaGMLP, a novel ensemble frameworkfor GNN-to-MLP Knowledge Distillation. Through an extensiveseries of experiments, we shed light on AdaGMLPs strengths byevaluating it on various scenarios, demonstrating its great potentialfor real-world applications. This work was supported in part by the National Natural ScienceFoundation of China under Grants 62133012, 61936006, 62073255,and 62303366, in part by the Innovation Capability Support Programof Shaanxi under Grant 2021TD-05, in part by the Fundamental Re-search Funds for the Central Universities under Grants QTZX23039,XJSJ23030, and in part by the Innovation Fund of Xidian Universityunder Grant YJSJ24015.",
  "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge ina neural network. arXiv preprint arXiv:1503.02531 (2015)": "Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, BowenLiu, Michele Catasta, and Jure Leskovec. 2020. Open graph benchmark: Datasetsfor machine learning on graphs. arXiv preprint arXiv:2005.00687 (2020). Chaitanya K Joshi, Fayao Liu, Xu Xun, Jie Lin, and Chuan Sheng Foo. 2022. On rep-resentation knowledge distillation for graph neural networks. IEEE Transactionson Neural Networks and Learning Systems (2022).",
  "Johannes Klicpera, Aleksandar Bojchevski, and Stephan Gnnemann. 2018. Pre-dict then propagate: Graph neural networks meet personalized pagerank. arXivpreprint arXiv:1810.05997 (2018)": "Carlos Lassance, Myriam Bontonou, Ghouthi Boukli Hacene, Vincent Gripon,Jian Tang, and Antonio Ortega. 2020. Deep geometric knowledge distillation withgraphs. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speechand Signal Processing (ICASSP). IEEE, 84848488. Weigang Lu, Ziyu Guan, Wei Zhao, Yaming Yang, and Long Jin. 2024. NodeMixup:Tackling Under-Reaching for Graph Neural Networks. Proceedings of the AAAIConference on Artificial Intelligence 38, 13 (Mar. 2024), 1417514183.",
  "Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and KilianWeinberger. 2019. Simplifying graph convolutional networks. In Internationalconference on machine learning. PMLR, 68616871": "Lirong Wu, Haitao Lin, Yufei Huang, Tianyu Fan, and Stan Z Li. 2023. ExtractingLow-/High-Frequency Knowledge from Graph Neural Networks and Injectingit into MLPs: An Effective GNN-to-MLP Distillation Framework. arXiv preprintarXiv:2305.10758 (2023). Lirong Wu, Haitao Lin, Yufei Huang, and Stan Z Li. 2022. Knowledge distillationimproves graph structure augmentation for graph neural networks. Advances inNeural Information Processing Systems 35 (2022), 1181511827.",
  "Lirong Wu, Haitao Lin, Yufei Huang, and Stan Z Li. 2023. Quantifying the Knowl-edge in GNNs for Reliable Distillation into MLPs. arXiv preprint arXiv:2306.05628(2023)": "Lirong Wu, Jun Xia, Haitao Lin, Zhangyang Gao, Zicheng Liu, Guojiang Zhao,and Stan Z Li. 2022. Teaching Yourself: Graph Self-Distillation on Neighborhoodfor Node Classification. arXiv preprint arXiv:2210.02097 (2022). Taiqiang Wu, Zhe Zhao, Jiahao Wang, Xingyu Bai, Lei Wang, Ngai Wong, andYujiu Yang. 2023. Edge-free but Structure-aware: Prototype-Guided KnowledgeDistillation from GNNs to MLPs. arXiv preprint arXiv:2303.13763 (2023).",
  "Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How powerfulare graph neural networks? arXiv preprint arXiv:1810.00826 (2018)": "Bencheng Yan, Chaokun Wang, Gaoyang Guo, and Yunkai Lou. 2020. Tinygnn:Learning efficient graph neural networks. In Proceedings of the 26th ACM SIGKDDInternational Conference on Knowledge Discovery & Data Mining. 18481856. Cheng Yang, Jiawei Liu, and Chuan Shi. 2021. Extract the Knowledge of GraphNeural Networks and Go Beyond It: An Effective Knowledge Distillation Frame-work. In Proceedings of the Web Conference 2021 (Ljubljana, Slovenia) (WWW21). Association for Computing Machinery, New York, NY, USA, 12271237. Jing Yang, Xiaoqin Zeng, Shuiming Zhong, and Shengli Wu. 2013. Effectiveneural network ensemble approach for improving generalization performance.IEEE transactions on neural networks and learning systems 24, 6 (2013), 878887. Yaming Yang, Ziyu Guan, Wei Zhao, Weigang Lu, and Bo Zong. 2022. Graphsubstructure assembling network with soft sequence and context attention. IEEETransactions on Knowledge and Data Engineering 35, 5 (2022), 48944907. Yiding Yang, Jiayan Qiu, Mingli Song, Dacheng Tao, and Xinchao Wang. 2020.Distilling knowledge from graph convolutional networks. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition. 70747083.",
  "Shichang Zhang, Yozen Liu, Yizhou Sun, and Neil Shah. 2021. Graph-less NeuralNetworks: Teaching Old MLPs New Tricks Via Distillation. In InternationalConference on Learning Representations": "Wentao Zhang, Xupeng Miao, Yingxia Shao, Jiawei Jiang, Lei Chen, Olivier Ruas,and Bin Cui. 2020. Reliable Data Distillation on Graph Convolutional Network. InProceedings of the 2020 ACM SIGMOD International Conference on Management ofData (Portland, OR, USA) (SIGMOD 20). Association for Computing Machinery,New York, NY, USA, 13991414. Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. 2003. Semi-supervisedlearning using gaussian fields and harmonic functions. In Proceedings of the 20thInternational conference on Machine learning (ICML-03). 912919.",
  "Hyper-parameters. We set the max training epochs at 500 for allthe trails. The search space of the hyper-parameters is as follows:": "Hidden Dimensionality = {128, 256, 512, 1024, 2048} Number of Layer = {2, 3} Ensemble Size = {2, 3} Balance Parameter , NA = {0.1, 0.2, , 0.9} Divergence Sensitivity Parameter = {0.5, 1, 2, 3, 4}For masking rate , we fix it at 0.1 in the normal setting and set tothe same value as the feature missing rate in the incomplete testdata setting.",
  "DPERFORMANCE COMPARISON WITH G2GMETHODS": "We compare our AdaGMLP with SOTA GNN-to-GNN (G2G) methods,i.e., CPF , RDD , TinyGNN , GNN-SD , and LSP ,in . All the methods use GCN as the teacher model. Wereuse the results of G2G methods from . is the averageimprovements across all the datasets over GCN.We can observe that AdaGMLP consistently shows better per-formance across the majority of the datasets. The improvementsin accuracy are most notable in the Cora, Citeseer, and Pubmeddatasets. On Coauthor-CS and ogbn-arxiv, AdaGMLP still demon-strates competitive performance, although not the top performer. Itmaintains robust results but with a slightly lower margin comparedto the top performer (RDD and FreeKD).AdaGMLP not only enhances efficiency but also maintains com-petitive accuracy. It achieves higher accuracy than G2G methodsacross multiple datasets. Unlike G2G methods, which require mes-sage propagation during inference, AdaGMLP operates without thisneed. This efficiency is crucial in real-world applications, espe-cially in scenarios with latency constraints and resource limita-tions, making AdaGMLP an optimal choice for such settings. Thisbalance between efficiency and accuracy is a significant advantagefor practical applications where both factors are essential.",
  "EPERFORMANCE COMPARISON WITHENSEMBLE METHODS": "In this section, we compare our AdaGMLPagainst some two well-known ensemble strategies. i.e., Vote, Bagging , and a simpleaverage ensemble strategy that uses the average predictions fromeach MLP student. All the strategies use the same configuration.We conduct experiments under three different settings, includingthe transductive setting, insufficient training data setting, and in-complete test data setting on Cora and Citeseer. The results areprovided in In the transductive setting, our AdaBoost strategies achieve thehighest accuracy on both the Cora and Citeseer datasets. Otherstrategies, such as average, vote, and bagging, perform relativelyclose to the baseline GLNN method but fall short of surpassing Ad-aBoost. This is attributed to AdaBoosts adaptive weighting of eachstudent and emphasis on unaligned knowledge points, allowing itto focus on the difficult-to-extract knowledge and improve overallpredictive performance.In the insufficient training data setting, we can see that simpleensemble strategies can also achieve better performance comparedto GLNN in some cases. However, there is still a performance gapbetween them and AdaBoost. It indicates that the AdaBoosts abil-ity to adaptively weigh weak learners is particularly effective intackling the challenges posed by limited labeled data.In the incomplete test data setting, we can observe that simpleensemble strategies can also bring performance improvement whentest data is corrupted. It demonstrates that combining multiple MLPstudents is a promising and simple way to mitigate the incompletetest data issue. Overall, the results show that AdaBoost outperforms other en-semble strategies in various settings. Its adaptability, emphasison challenging knowledge points, and weighting mechanism con-tribute to its superior performance. Additionally, the experimentshighlight the potential benefits of ensemble methods for improvingperformance of G2M."
}