{
  "ABSTRACT": "A fundamental technique of recommender systems involves mod-eling user preferences, where queries and items are widely usedas symbolic representations of user interests. Queries delineateuser needs at an abstract level, providing a high-level description,whereas items operate on a more specific and concrete level, rep-resenting the granular facets of user preference. While practical,both query and item recommendations encounter the challenge ofsparse user feedback. To this end, we propose a novel approachnamed Multiple-round Auto Guess-and-Update System (MAGUS)that capitalizes on the synergies between both types, allowing usto leverage both query and item information to form user interests.This integrated system introduces a recursive framework that couldbe applied to any recommendation method to exploit queries anditems in historical interactions and to provide recommendationsfor both queries and items in each interaction round. Concretely,MAGUS first represents queries and items through combinationsof categorical words, and then constructs a relational graph to cap-ture the interconnections and dependencies among these individualwords and word combinations. In response to each user request,MAGUS employs an offline tuned recommendation model to assignestimated scores to words representing items; and these scores aresubsequently disseminated throughout the graph, impacting eachindividual word or combination of words. Through multiple-roundinteractions, MAGUS initially guesses user interests by formulatingmeaningful word combinations and presenting them as potentialqueries or items. Subsequently, MAGUS is updated based on userfeedback, enhancing its recommendations iteratively. Empiricalresults from testing 12 different recommendation methods demon-strate that integrating queries into item recommendations via MA-GUS significantly enhances the efficiency, with which users canidentify their preferred items during multiple-round interactions.",
  "Work done during Jiarui Jins visit at University College London": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior specific permission and/or afee. Request permissions from , August X - Y, 2025, Toronto, Canada 2024 Association for Computing Machinery.ACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00 Queries concentrate on the prediction of user-search queries atan abstract level, whereas items operate on individual items at amore specific level. Existing search engine algorithms (commonlyknown as learning-to-rank algorithms) and recommendation meth-ods always utilize query information and user profiles as inputs andemploy user-browsed items as supervision signals. However, weargue that queries, being capable of describing user needs, shouldalso be employed as a form of supervision. Namely, relying exclu-sively on items as the sole basis for supervision could result insuboptimal solutions. Taking (a) as an example, a user usesquery A Milk, prompting the recommender system to return itemsA, B, and C; and all these items receive negative feedback. Previousrecommendation methods, centered solely on items, would refrainfrom suggesting additional milk items to the user. However, theseapproaches overlook the explicit indication of the users interestin milk through query A. This particular case motivates us to in-corporate query information within item recommendations, giventhat both elements fundamentally aim to describe user interests.In this paper, we propose a novel Multiple-round Auto Guess-and-Update System (MAGUS), a multiple-round recommendationframework applicable to any recommendation method. MAGUSallows a recommender system to leverage both queries and itemsin historical interactions to offer recommendations for queries anditems at each round, thus addressing the data sparsity issue inherentin user feedback. In the design of MAGUS, we are, at least, requiredto handle the following challenges. [C1] How to draw connections between queries and items? Taking as an example, a core challenge of jointly consideringqueries and items lies in creating a unified metric for evaluatingqueries and items (e.g., query B and items D, E, and F). [C2] How to model interdependence among queries? Unlike items,queries exhibit a significant degree of dependence. Here are threepossible scenarios for each query-query pair: (i) Mutual improve-ment: selecting one query increases the likelihood of selectingthe other query in the following round (e.g., selecting Milk wouldraise the probability of selecting Whole Milk in the next round). (ii)Mutual inhibition: selecting one query decreases the probabilityof selecting the other query in the following round (e.g., if a userselects Milk, it is unlikely that she would select Beef because milkand beef belong to distinct categories. (iii) Mutual Independence:the selection of one query has minimal or negligible effects onthe users decision regarding the other query (e.g., selecting Milkdoes not significantly influence the users preference for On Sale). [C3] How to efficiently make use of user feedback at each roundwithin strict online latency requirements? One of the primaryadvantages of multiple-round recommendations lies in its abilityto progressively approach user interests through iterative guesses",
  "Milk": "Item BItem C Item E : An illustrated example of MAGUS recommendingboth queries (e.g., query B) and items (e.g., items D, E, andF), as shown in (b). This framework effectively leveragesboth queries (e.g., query A) and items (e.g., items A, B, andC) present in the browsing logs, serving to alleviate the datasparsity issue of user feedback, as shown in (a).",
  "and updates. However, this recursive framework presents a chal-lenge in terms of effectively leveraging user feedback while main-taining strict online latency requirements": "We recognize that both queries and items act as representationsof user interests, albeit at different levels of granularity. In thiscontext, both queries and items can be regarded as combinations ofspecific categorical words describing user interests. As exemplifiedin (b), item A could be expressed as the combination ofwords Brand A, Milk, Semi Skimmed, 2L, and $3; and query B can beorganized as the combination of words Whole and Milk. Therefore,queries and items could be connected through the shared use ofwords, thereby addressing [C1]. To capture the dependence amongthese words, we construct a relational graph, where the nodescorrespond to individual words or combinations of words, and theedges represent three types of dependence introduced in [C2]. Weillustrate the graph in (a).In each user session, MAGUS starts with an arbitrary offlinetuned recommendation model to assign an initial prediction scoreto every node representing an item in the graph, e.g., items A,B, and C in (a). Subsequently, MAGUS propagates thesescores to associated words throughout the graph, as demonstratedin (b). During each interaction round, MAGUS selects thosenodes with the highest scores. These nodes would either compriseartificial queries or specific items.In the above multiple-round setting, MAGUS is designed to re-ceive user feedback during each interaction round. To effectivelyharness this feedback while adhering to stringent online latencyrequirements (i.e., [C3]), MAGUS employs a label propagation al-gorithm to simulate the influence of user feedback on the relevantwords, as depicted in (c). In our approach, we also introduce a feature propagation method to determine edge weights within thegraph. These weights enable us to perform weighted propagationas illustrated in (b) and (c).Given that our principal aim is to identify an item aligned withuser preferences, we have formulated an innovative multiple-roundrecommendation simulator for evaluation purposes. Within thissimulation environment, the MAGUS agent operates iteratively,providing suggestions for both queries and items in each round,while the user agent responds accordingly. This iterative cycle con-tinues until an item that sufficiently aligns with user requirements isdiscovered. Furthermore, we demonstrate that incorporating largelanguage models into the simulator can enhance its capabilities,transforming it into a conversational recommender system.Our extensive experiments, conducted on 3 real-world datasetsand incorporating 12 diverse recommendation methods, consis-tently validate that MAGUS significantly enhances recommenda-tion performance by effectively identifying items that align withuser preferences. Additionally, our results also demonstrate thatMAGUS can effectively handle cases where users do not have aclear picture of what they like.",
  "PRELIMINARIES2.1Problem Formulation": "We begin by introducing the notations used in this paper. U denotesthe set of users, V denotes the set of items, and Q denotes the set ofqueries. For convenience, we use RE(), a mapping function RE :U V R, to denote any given offline tuned recommendationmethod that assigns a predicted relevance score to each item. Wedenote the score of each item V asRE(). Our goal is to builda joint mapping function MAGUS : U A R, where A = V Qis the action space. In other words, our MAGUS, i.e., MAGUS(), iscapable of exploiting both query information and item informationto learn to assign scores to both queries and items.We highlight that the primary objective of MAGUS is to identifyan item that meets the users requirement within a session, whereMAGUS is allowed to do multiple-round recommendations of eitherqueries or items within each user session. For clarity, we presentthe description in the context of the top-1 recommendation anddiscuss its extension to the top-N recommendation in Appendix ??. Definition 2.1 (Multiple-round Recommender Systems Sup-porting both Queries and Items). Given a tuple (U, V, Q, MAX),in each user session (with user U), the objective of MAGUS()is to recommend an item satisfying s needs within MAX rounds.The action space of MAGUS() covers both the item space and thequery space, namely, A = V Q. For each -th round (where = 1, . . . , MAX), MAGUS() recommends either an item or a query,and the user should provide either positive or negative feedback.Formally, let VTARGET denote the set of items that can fulfill theusers requirements, and A signifies the recommendationmade by MAGUS() at the -th round. Our objective is:",
  "MAGUSKDD25, August X - Y, 2025, Toronto, Canada": "which, while they offer clear interpretability, can sometimes fallshort in adapting to the intricacies of user behavior that extendbeyond established norms. In comparison to purely learning-basedalgorithms, the MAGUS system offers a distinct advantage in termsof interpretability. This systems design, which incorporates a struc-tured graph with labeled nodes, facilitates a more transparent un-derstanding of its decision-making process. By examining the graphstructure and the labels assigned to individual nodes, one can morereadily deduce the rationale behind the systems actions.We also note that our MAGUS system can ensure the recommen-dations concentrate on the user feedback within the current sessionrather than previous user behaviors. In other words, our MAGUSprioritize the users current preferences, acknowledging that userinterests are subject to change over time. For instance, while ausers browsing history may indicate a significant interest in itemssuch as iPhone items, if the user clearly expresses a preference forother items such as MacBook items during the current session,the MAGUS system will dynamically adjust its recommendationsaccordingly. It will prioritize recommending MacBook items overiPhone items. This is achieved by the design of the mechanismswithin MAGUS. Specifically, according to Eqs. (5) and (6), even ifa user has shown great interest in some iPhone items in theirprevious behaviors, the prediction scores for all iPhone itemsare constrained to be smaller than 1. Furthermore, if a user clearlyexpresses a preference for MacBook items, as per Eq. (8), the pre-diction scores for MacBook items are set to 1. If all edge weightsare set to 1 for R+ (following Eq. (3)), items related to MacBookitems would receive prediction scores greater than 1 according toEq. (9). In this case, it is unlikely for our MAGUS system to respondwith an iPhone items when a user clearly prefers MacBook.",
  "Comparisons to Previous Work": "Designing multiple-round recommender systems to identify anitem meeting user requirements has been explored in the domain ofconversational recommendations . One popular direction is tocombine the recommender module and the conversational modulefrom a systematic perspective . For example, recentpapers introduces a reinforcement learning frameworkthat enables the optimizations over multiple-round interactions.However, reinforcement learning-based approaches inherently facechallenges related to the insufficient usage of labeled data and highcomplexity costs of deployment. On the contrary, MAGUS could beseamlessly integrated with any recommendation method to enableit to effectively exploit queries and items in historical interactionsand recommend both queries and items. Furthermore, it is essentialto highlight that our recommending queries differ significantlyfrom recommending attributes in conversational recommendersystems like . The key distinction lies in the substantialdependence among queries such as apple and macbook, macbookand macbook pro, which is not as prominent among attributessuch as brand and color. In addition, in contrast to our MAGUSsystem, most conversational recommendation methods usereinforcement learning techniques, whereas reinforcement learningbased methods heavily rely on meticulous reward function design.In comparison, the MAGUS system can be easily deployed into anarbitrary recommendation method, as our framework applies anoffline-tuned recommendation model in a plug-and-play manner.Our MAGUS is also connected to query recommendations (com-monly referred to as query suggestions) , aiming at improvinguser search experience by providing suggestions that attempt toguess user intentions based on their past behaviors. However, ourultimate objective differs, as we aim to identify an item rather thangenerate a query.",
  "THE MAGUS SYSTEM": "Our main idea is to recognize that queries and items both representuser interests, but they do so at varying levels of granularity. For thispurpose, we initially construct a relational graph offline to bridgequeries and items (.1). In each user session, we employan offline-tuned recommendation model as an initializer to assignan initial score to each node (.2). Subsequently, we designa label propagation algorithm to iteratively update these scoresthroughout the session (.3). Additionally, we propose afeature propagation algorithm to learn the edge weights of thegraph (.4), although its use is not mandatory in practice.",
  "To establish a connection between queries and items, we use wordsas the bridge to build a relational graph as follows": "Definition 3.1 (Words and Relational Graph). Given a tuple(V, Q) where V is the set of items and Q is the set of queries,MAGUS establishes a set denoted as W, comprised of categori-cal words, to serve as a connecting bridge between V and Q. Toelucidate this process, MAGUS commerces by extracting lexicalelements from the categorical attributes (a.k.a., features) of itemsin V, e.g., this entails the extraction of terms Brand A, Milk, SemiSkimmed, 2L, and $3 from item A in (b). This collectionof extracted terms constitutes the set W. Subsequently, MAGUSproceeds to decompose the queries within Q by matching themwith the words present in W; e.g., query B in (b) can bedeconstructed into a combination of the terms Whole and Milkbased on this matching process.As a result, MAGUS can construct a relational graph denoted asG = (E, R), where E signifies the set of nodes and R denotes theset of edges. E encompasses not only individual words from thevocabulary W but also the combinations of words that either formqueries in Q or items in V. For example, as illustrated in (a),node (macbook pro 13 grey) represents item A, and node (macbookpro 13) and node (macbook pro) are the combinations of words usedto organize item A, and node (grey) and node (macbook, apple)are individual nodes. R contains three categories of relationshipsbetween nodes, i.e., mutual improvement (denoted as R+), mutualinhibition (denoted as R), and mutual independence (denoted asR). In other words, R = R+R R. R+ delineates connectionsbetween pairs of nodes in situations where one node is subsumedwithin or constitutes a component of the other node. For instance,node (macbook) is a part of node (macbook pro). In contrast, R defines relationships between pairs of nodes that pertain to distinctitems, exemplified by node (macbook pro) and node (iphone pro)associated with item A and item C respectively. Lastly, R connectsall the other pairs of nodes, namely, it establishes connectionsbetween pairs of nodes that can be employed to represent the sameitem but are characterized by a lack of shared elements. For example,node (macbook pro) and node (gray) together form item A but donot exhibit any overlapping components. Based on Definition 3.1, all queries and items can be successfullymapped to their respective nodes within the graph, confirming thatQ E and V E. Then, for each pair of nodes, there are threepossible scenarios: (i) R+ for those where one node is a part of theother, (ii) R for those where one node is not a part of the other,but these nodes are compatible within a single item, and (iii) R for those where one node is not a part of the other node and thesenodes are not compatible.We note that the procedure for constructing this relational graph,as detailed earlier, does not consider all item features as words;instead, we focus on utilizing key features that indicate the cate-gory of items (e.g., cat_id, seller_id, and brand_id in Tmall dataset),similar to the approach in . Moreover, the extraction of key itemfeatures, such as keywords, from item descriptions aligns with es-tablished tokenization practices in the field , which falls outsidethe scope of this paper. Our graphs, derived from user interactiondata and item features, diverge from traditional knowledge graphs typically sourced from external databases, since ourgraphs are consistently accessible.",
  "Update Direction": ": An illustrated example of our relational graph organizing individual words and certain combinations of words (asshown in (a)). To maintain clarity, we have opted not to explicitly illustrate the edges representing mutual inhibition andmutual independence. In each user session, MAGUS collaborates with an offline-tuned recommendation method to initializethe node scores, where the propagation direction is from the nodes representing the items towards the other nodes (as shownin (b)). Subsequently, when a user provides her preference on a recommendation, MAGUS updates the relevant nodes by a labelpropagation algorithm whose propagation direction now operates from the recommended node towards the nodes representingitems (as shown in (c)).",
  "Recommender System as Initializer": "As described in Definition 2.1, for each user U, we have accessto her browsed items, i.e., H+ for those with positive feedback andH for those with negative feedback. Then, we can establish anoffline tuned recommendation method, denoted as RE(), trainedupon H+ H . RE() can be any recommendation method, andwe evaluate the impact of employing different RE()s in the exper-iment. In every session with user , with an offline tuned RE(),we can assign an estimated relevance score to each item V,denoted as RE(). As outlined in Definition 3.1, each item in Vcan correspond to a specific node in E. For convenience, let EITEMdenote the set of nodes representing items. Here, we use torepresent the estimated relevance score for each node E.Our initial step is to assign the estimated scores to the nodeswithin EITEM. Formally, for each node E, this process can beexpressed as follows.",
  "= RE(), EITEM,0, E/EITEM.(2)": "For convenience, we normalize all scores to fall within the range of0 to 1, namely holds for all EITEM.Next, we propagate these scores to all the other nodes in E. Forthis purpose, we assign a learnable weight to each edge in R. Forclarity, let denote the weight of the edge connecting node",
  "= 1 if , R+ R, and = 0 otherwise.(3)": "If we have access to the offline tuned representation vectors forall the items and users provided by the recommendation method,we can allocate and tune learnable weights for the edges (as laterintroduced in .4).We then initialize the remaining nodes by propagation on thegraph. As illustrated in (b), the propagation process initiatesfrom the nodes in EITEM and extends to the remaining ones. Duringthe propagation, the update function of nodes in E/EITEM can be",
  "(4)": "where E/EITEM and E, and R + and R are introducedto denote the update directions on the edges in R+ and R respec-tively. Here, R+ = R + R + and R = R R .If we have access to the set of user-searched queries H , theset of the searched queries for user , as stated in Definition 3.1,each query in H can correspond to a specific node in E. Then, letEQUERY denote the set of nodes representing the searched queries.We update the estimated scores of those nodes by:",
  ", E+,0, E,(8)": "where E+ denotes the event where provides positive feedback(e.g., clicks) to , and E denotes the event where providesnegative feedback (e.g., observations without clicks) to .We then propagate the updated score of to the remaining nodes,i.e., E\\{}. In this case, our update function can be written as: min(1, + ),, R +,max(0, ),, R ,,, R R + R .(9)We note that, in contrast to Eq. (4), which propagates informationin the direction of R + R , the propagation direction in Eq. (9) isreversed, i.e., R + R .After all nodes in E are updated in each round, we start the nextround by successively applying Eqs. (6) and (7) to generate a newMAGUS.",
  "Feature Propagation as Weight Trainer": "As mentioned in .2, for each edge in R, we can allocate alearnable weight, when we have access to the offline tuned repre-sentation vectors for all the items and users. Concretely, we denotethe representation vector of each item EITEM as RE, and therepresentation vector of each user U as RE . Since we haveaccess to all the REvectors and the sets of browsed items H+ sand H s for all user s, we can proceed with designing a featurepropagation algorithm to learn the weights.Formally, we start by initializing the representation vector ofeach node E as follows:",
  "log + (1 ) log(1 ),(13)": "where is our predicted score relevance for node regarding user, calculated by = (RE ) , and is the ground-truth labelof node . = 1 if H+ , and = 0 if H .After completing the feature propagation process as describedabove, we obtain an updated representation vector for each node.We then define the weight of each edge in R based on the similaritybetween the connected pair of nodes. This can be expressed as:",
  "= if , R+ R, and = 0 otherwise. (14)": "It is worth noting that the supervised learning process describedcan be conducted offline. In other words, our relational graph Gcan be pre-constructed following Definition 3.1, where the edgeweights can be pre-computed using either Eq. (3) or (14), and theseweights be saved within G. G can then be stored and only needs tobe updated when our recommender system RE() is updated. Thisapproach can save computational resources in online operations.",
  "EXPERIMENTS4.1Experimental Settings": "Dataset Description. We conduct extensive experiments on 3industrial real-world e-commerce datasets, namely Amazon, Ali-pay, and Tmall. For each dataset, we collect user-item interactionhistories (i.e., H+ H ) of users U, and gather all items toform V. In the process of constructing the query set Q, the pivotalstep is the assembly of the word set W, and one can then deriveQ following Definition 3.1. Concretely, we initiate the word set Wby extracting words from the categorical features of items in V.Lets take the Amazon dataset as an example. If an item has thebrand Coxlures and belongs to the categories Sports and Dance,we generate words like coxlures, sports, and dance. By iteratingthrough all items in Vm we compile the set W. For each dataset,we perform a temporal split, dividing it into training, validation,and test sets in a ratio of 6:2:2 based on time steps. We excludesequences with a length of less than 30 or those that lack itemswith positive feedback. We then randomly select 30 items to forma session. During the random selection of items, we ensure thatat least one item with positive feedback is included. We providecomprehensive details about the datasets, the data prepossessing,and the relational graph construction in Appendix C.1, C.2, C.3.",
  "KDD25, August X - Y, 2025, Toronto, CanadaJiarui Jin et al": "user 2742 (high popularity) (low popularity) (low popularity)(medium popularity) item 8632 item 1940 item 2401 (low popularity) (medium popularity) (medium popularity) (medium popularity) (low popularity) (low popularity) : An illustrated case of MAGUS providing one recom-mendation to user 2742 at each round. The number depictedover each node is its score regarding the user. The popularitylevel of each node is computed according to its frequency inthe users browsing log. This comparison effectively underscores a key point: popularity-based methods tend to overlook the variations in user preferencesand the dynamic nature of these preferences across different rounds.This drawback resembles the well-known popularity bias seenin item recommendation tasks, where popular items are favoredin recommendations without due consideration of individual userpreferences. Real-time",
  "GIPA+MAGUS+/0.8810.7850.849/0.8920.8810.934/0.9190.8320.918": "Baseline Description. Although our MAGUS framework can en-able any recommendation method to recommend queries or itemsin a multiple-round setting, the ultimate goal, as formulated inDefinition 2.1, is to identify an item satisfying user needs in eachuser session. In this context, we introduce 12 recommendation methods as base recommender systems, and our MAGUS is evalu-ated in an ablation style. These recommendation methods includeMPS (Most Popular Suggestion) , Hybrid (Hybrid Suggestion), FM (Factorization Machine) , DeepFM , PNN (Product-based Neural Network) , MMoE (Multi-gate Mixture-of-Experts), DIN (Deep Interest Network) , LSTM (Long-Short Term",
  "detailed description of our simulator and the integration of thelarge language models are provided in Appendix D.1, D.2": "Evaluation Metrics. In our evaluation, we focus on two primarytypes of metrics in effectively identifying items that match userneeds within a session. One is round-wise accuracy, denoted asRA@MAX, which evaluates the recommendation performance dur-ing each round of interaction. For every round , if a user clickson the recommendation at position , we calculate RA@MAX us-ing RA@MAX = 1/log2( + 1). If a user does not click on anyrecommendation during a round, RA@MAX is set to 0. The finalRA@MAX is determined as the average of all the RA@MAX values.The other one is session-wise accuracy, denoted as SA@MAX, whichassesses whether the system successfully recommends a target itemwithin MAX interactions in each session. If the system succeeds inrecommending a target item, SA@MAX is set to 1 for that session;otherwise, it is set to 0. The final SA@MAX is computed as theaverage of all the SA@MAX values.Additionally, to gauge the impact of combining query and itemrecommendations, we provide a single-round accuracy (denoted asSAC) metric for each baseline method. In each session, SAC is setto 1 if the recommendation list contains at least one target item,and 0 otherwise. The final SAC is determined as the average of allthe SAC values.Implementation details along with the code link are available inAppendix C.5.",
  "We present the outcomes of our experiments in . We sum-marize our findings as follows": "Performance Comparisons between Jointly using Queriesand Items and Solely using Items. As our SAC is evaluating therecommendation performance of solely considering items, we com-pare the results of single-round performance SAC against multiple-round performance RA@3, SA@3, and SA@5 of the base recom-mendation method (i.e., rows of FM, DeepFM, PNN, MMoE, DIN,LSTM, GRU, RGCN, RGAT, and GIPA). These results show that com-bining items and queries in item recommendations can significantlyimprove the recommendation performance across 12 diverse rec-ommendation approaches on all 3 datasets. This improvement alsowould be attributed to our multiple-round setting, which allows real-time user feedback and better aligns with user preferences. Notably,popularity-based methods like MPS and Hybrid perform well in",
  "terms of RA@3 but struggle with SA@3 and SA@5, likely becausehigh popularity often occurs in queries, leading to an overemphasison recommending queries": "Performance Comparisons between MAGUS (denoted as MA-GUS + X) and using Sequential Neural Networks as UpdateFormula (denoted as X). As described in .1, we extend theexisting single-round recommendation methods (i.e., the base rec-ommendation methods) to the multiple-round setting by a recurrentneural network based framework. When comparing X+MAGUS andX in terms of RA@3, SA@3, and SA@5, one can see that X+MAGUSsignificantly outperforms X. This demonstrates that MAGUS pro-vides an effective solution for jointly considering queries and items.One possible explanation for the improved performance of MA-GUS is its capability to leverage the dependence among queries anditems through the label propagation algorithm. This enables therecommender system to encode user feedback over time, resultingin more accurate recommendations. Performance Comparisons between MAGUS (denoted as X +MAGUS) and using Conversational Recommender Systems(denoted as X + CRM, X + ME, X + EAR). As described in Sec-tion 4.1, we incorporate the existing conversational recommendersystems into the base recommendation methods FM, DeepFM, andPNN. When comparing X+MAGUS and X + CRM, X + ME, X + EARin terms of RA@3, SA@3, and SA@5, one can see that X+MAGUScan consistently achieve better performance. This result can beattributed to several key differences between MAGUS and the con-versational methods, summarized as follows: (i) Unlike the MA-GUS system, conversational recommendation methods (along withtheir simulations in the experimental setting) do not take queryinformation into account. (ii) Compared to the MAGUS method,conversational recommendation methods do not explicitly modelthe rich interdependence among attributes (where attributes corre-spond to words and word combinations in the MAGUS method).(iii) Many online recommendation methods utilize reinforcementlearning techniques. While reinforcement learning methods canhandle complex states and encode multiple factors, they often facechallenges such as the need for a large number of training samples(referred to as the data insufficiency issue) and relatively smallaction space.",
  "Performance Comparisons between Graph-based Recom-mendations and other Recommendations. When considering": "SAC, sequential recommendation methods such as GRU, LSTM, andDIN often outperform other baseline methods. However, concern-ing RA@3, SA@3, and SA@5, the graph-based methods, namelyRGCN, RGAT, and GIPA, achieve comparable or superior resultscompared to the sequential recommendation methods. This sug-gests that graph-based methods, when applied to our relationalgraph, can effectively exploit the dependence among queries anditems to make recommendations. This finding also supports theadvantage of using a relational graph to bridge queries and items. Performance Comparisons between MAGUS and MAGUS+.When comparing X+MAGUS+ and X+MAGUS (where X is RGCN,RGAT, and GIPA), we can observe that MAGUS+ can bring consis-tent enhancements, which indicates that integrating these learnededge weights into the multiple-round recommendation process canhave a positive influence. This can explained as the learned edgeweights would be capable of capturing more meaningful relation-ships between queries and items, which also verifies the superiorityof using our relational graph.",
  "Hyperparameter Study": "Impact of Maximum Number of Rounds MAX. One importanthyper-parameter in MAGUS is the maximum number of rounds,denoted as MAX. We examined how the performance of MAGUSchanges when setting MAX to different values, i.e., MAX = 2, 3, 4, 5on the Amazon and the Tmall datasets. Results depicted in consistently indicate that multiple-round recommendations withmore feedback rounds from users outperform those with fewerrounds. This highlights the advantages of implementing a multiple-round recommendation service. Impact of Recommendation Length . The length of the rec-ommendation list at each round (denoted as ) is another crucialhyper-parameter in MAGUS. To investigate its impact, we exper-imented with different values, i.e., = 2, 3, 4, 5, on the Amazonand the Tmall datasets. Results replayed in show thatincreasing the length of the recommendation list can lead to im-proved performance. This improvement can be attributed to thefact that a larger list provides users with more choices and optionsto choose from. However, it is worth noting that the performancegain achieved by increasing the length of recommendation list is not as substantial as that obtained by increasing the number ofrounds MAX. This is because, compared to recommendationsin one round, recursively recommending one item in roundscould enable the model to update the predicted scores to benefitthe following recommendations, leading to a significant impact onoverall performance.",
  "Study for Deployment Feasibility": "We present a robustness study that incorporates the analysis ofambiguous user feedback within our system in Appendix E.1. Wealso derive into a complexity study in Appendix E.2. Furthermore,we offer detailed case studies in Appendix E.3. We then discussthe deployment architecture, the extension from top-1 to top-Nrecommendation, and the use case for exploratory research in Ap-pendix F.1, F.2, F.3.",
  "Jiarui Jin, Xianyu Chen, Weinan Zhang, Junjie Huang, Ziming Feng, and YongYu. 2022. Learn over Past, Evolve for Future: Search-based Time-aware Recom-mendation with Sequential Behavior Data. In WWW": "Wenqiang Lei, Xiangnan He, Yisong Miao, Qingyun Wu, Richang Hong, Min-Yen Kan, and Tat-Seng Chua. 2020. Estimation-action-reflection: Towards deepinteraction between conversational and recommender systems. In Proceedings ofthe 13th International Conference on Web Search and Data Mining. 304312. Wenqiang Lei, Gangyi Zhang, Xiangnan He, Yisong Miao, Xiang Wang, LiangChen, and Tat-Seng Chua. 2020. Interactive path reasoning on graph for conver-sational recommendation. In Proceedings of the 26th ACM SIGKDD internationalconference on knowledge discovery & data mining. 20732083. Raymond Li, Samira Ebrahimi Kahou, Hannes Schulz, Vincent Michalski, LaurentCharlin, and Chris Pal. 2018. Towards deep conversational recommendations.Advances in neural information processing systems 31 (2018).",
  "Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. 2018.Modeling task relationships in multi-task learning with multi-gate mixture-of-experts. In KDD": "Sabrina J Mielke, Zaid Alyafeai, Elizabeth Salesky, Colin Raffel, Manan Dey,Matthias Gall, Arun Raja, Chenglei Si, Wilson Y Lee, Benot Sagot, et al. 2021.Between words and characters: a brief history of open-vocabulary modeling andtokenization in nlp. arXiv preprint arXiv:2112.10508 (2021). Jessie Ooi, Xiuqin Ma, Hongwu Qin, and Siau Chuin Liew. 2015. A survey ofquery expansion, query suggestion and query refinement techniques. In 2015 4thInternational Conference on Software Engineering and Computer Systems (ICSECS).IEEE, 112117.",
  "Steffen Rendle. 2010. Factorization machines. In ICDM. Paul Resnick and Hal R Varian. 1997. Recommender systems. Commun. ACM 40,3 (1997), 5658": "Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne van den Berg, IvanTitov, and Max Welling. 2018. Modeling relational data with graph convolutionalnetworks. In European semantic web conference. Alessandro Sordoni, Yoshua Bengio, Hossein Vahabi, Christina Lioma, JakobGrue Simonsen, and Jian-Yun Nie. 2015. A hierarchical recurrent encoder-decoderfor generative context-aware query suggestion. In CIKM. Yueming Sun and Yi Zhang. 2018. Conversational recommender system. In SIGIR. Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat-Seng Chua. 2019. Kgat:Knowledge graph attention network for recommendation. In Proceedings of the25th ACM SIGKDD international conference on knowledge discovery & data mining.950958.",
  ": Update RE() using data in new H+ s and new H s": "Our approach can be regarded as a combination of non-parametricrecommendation methods relying on connections between queriesand items, and parametric recommendation methods based onuser browsing logs. Concretely, our recommendation problem canbe formulated as a node classification problem on our graph G,aiming to recommend a node to a given user. To update our pre-dictions in each -th round of interaction, we use the formula (+1) = () + (0), where is the symmetric normalizedversion of the adjacency matrix given by = 1",
  "1": "2 , and repre-sents the degree matrix of the graph . Our adjacency matrix isconstructed by integrating rules derived from the users browsingpatterns, as described in Definition 3.1. This incorporation of userbehaviors into the graphs structure provides a personalized con-text for the recommendation process. Additionally, as detailed in.2, the initial label matrix (0) is derived from an offline-tuned recommendation model. And, the weights for the graph edges{ |, R}, as discussed in .4, also can be computedusing an offline-learned recommendation model.Therefore, our MAGUS system is distinguished by its superiorgeneralization capabilities and its enhanced capacity to managecomplex user patterns that may not conform to predefined rules.This attribute sets it apart from traditional rule-based algorithms,",
  "B.2Complexity Analysis": "The MAGUS system consists of two key components: the featurepropagation part during offline training and the label propagationpart during online inference. Matrix multiplication on graph, whichis a fundamental operation in these components, has a computa-tional complexity of (|+| 2) as stated in , where |+| isthe number of nonzero entities in . The dimension of the embed-ding vector of nodes, denoted as , is significant for computationalconsiderations. In feature propagation, is the dimension of theembedding vector of nodes, while in label propagation, is thedimension of the label vector of nodes.",
  "B.3Connections to Rule-based Algorithms": "Our MAGUS system can be conceptualized as an amalgamation ofrule-based and learning-based recommendation methods. To eluci-date, our approach integrates label propagation, which embodieselements of both rule-based and learning-based strategies. Con-cretely, we delineate our framework as a node classification taskwithin the context of a relational graph, aiming to recommend anode to a given user. In each-th interaction, our interactive cycle ischaracterized by an update formula (+1) = () + (1 ) (0),where () represents the label matrix at the commencement ofthe -th iteration. Here, is the symmetric normalized version ofthe adjacency matrix given by = 1",
  "We conducted extensive experiments on 3 industrial real-worlde-commerce datasets, whose statistics are summarized as follows": "Amazon is a dataset introduced by collected from Amazon, anonline e-commerce application from May 1996 to July 2014. Thereare 1,114,563 reviews of 133,960 users and 431,827 items with anaverage sequence length of 33 and 6 feature fields. Alipay is a dataset collected by Alipay, an online paymentapplication from July 2015 to November 2015. There are 35,179,371interactions of 498,308 users and 2,200,191 items with an averagesequence length of 70 and 6 feature fields. Tmall is a dataset consisting of 54,925,331 interactions of 424,170users and 1,090,390 items. These sequential histories are collectedby Tmall e-commerce platform from May 2015 to November 2015with an average sequence length of 129 and 9 feature fields.",
  "For each dataset, we begin with organizing users and items intoa set of users U and a set of items V. Also, we can get access touser-item interaction histories to build: H+ , a set of items receiving": "positive feedback from user ; and H , a set of items receivingnegative feedback from user , for all user U. As outlined inDefinition 2.1, we need a query set Q. In the following, we takethe Amazon dataset as a concrete example of the generation ofQ. Initially, we create a word set W by extracting terms from thecategorical features of items within V. For instance, if there is anitem with brand Coxlures and belonging to categories such asSports and Dance, then we extract words coxlures, sports, anddance. We proceed by enumerating all the items in V. During thisprocess, we form combinations of words and individual words asthe nodes of our relational graph. For example, if we have an Item Awith a combination like coxlures, sport and dance, then we havenodes as individual words such as coxlures, sport, and dance,and nodes as combinations of words such as sport and dance. Allthese nodes, except for those representing items, can be consideredas queries (i.e., Q). They serve as potential query elements withinour system. Also, once we have established the nodes representingqueries and items, we can proceed to construct our relational graphG (as defined in Definition 3.1) correspondingly.",
  "C.3Description of Relational GraphConstruction": "We also emphasize the versatility of our relational graph G, whichcan be constructed across various domains. For example, whendealing with items like movies or songs, we can create these graphsusing key features extracted from raw data. For movies, featuressuch as directors and genres can be extracted, while for songs, fea-tures like singers and genres can be utilized. These features formnodes V within the graph, representing combinations of attributes(e.g., director A + romantic, director B + fiction). We definemutual improvement relationship between nodes R+ like directorA and director A + romantic, as users inclined towards the roman-tic genre may favor movies directed by director A in that genre.Conversely, we define mutual inhibition relationship R betweennodes like romantic and fiction, reflecting the understanding thatusers preferring fiction may not favor movies categorized underthe romantic genre. Additionally, we define a mutual independencerelationship between nodes R between nodes like nodes repre-senting directors and nodes representing genres, because there areno explicit relations between these two types of nodes, and the im-plicit relations are modeled by introducing nodes like director A +romantic. These relationships capture underlying interdependen-cies and refine the systems recommendations. Our primary focuslies in strategically leveraging relational graphs in multiple-roundsettings, while the extraction of relevant information, such as key-words, from item descriptions aligns with established tokenizationpractices in the field , which is out of the scope of this paper.",
  "C.4Descriptions of Adjustments of Baselines": "Consider that our task of multiple-round recommendations sup-porting queries and items (as introduced in Definition 2.1) is novel,and there are no existing approaches tailored to this specific task. Toadapt the baseline methods to our context, we introduce two mainadjustments. The first adjustment involves expanding their actionspace from just items (i.e., V) to encompass both items and queries",
  "b0f": "b1b2b3 ageprice...... user response at 1st round user response at 2nd round Last Embedding Layer of a Single-round Baseline embedding vector embedding vector embedding vector embedding vector : An illustrated example of extending a single-roundbaseline method to our multiple-round setting in the ma-sonry layout, where we introduce a recurrent neural networkto facilitate the flow of information across rounds.",
  "C.5Implementation Details": "The learning rate is decreased from the initial value 1 102 to1 105 during the training process. The dimensions of all theembedding vectors including users and words (and queries anditems) are set as 64. The batch size is set as 1000. The weight forL2 regularization term is 4 104. The dropout rate is set as 0.5.We assign the length of recommendation lists (denoted as ) as3. The maximum number of rounds (denoted as MAX) is set in theevaluation metrics (see .1 for details).",
  "The human user is supposed to respond in the following ways": "Upon querying item , the user should respond with Yes if is one of target items of users, or No otherwise. In the Yescase, the session is successfully concluded, because is one ofthe target items (i.e., the user has identified an item that meetstheir satisfaction criteria). Conversely, in the No case, the scoresassigned to each node are subjected to an update process. Upon querying query , the user should respond with Yes if is one of the elements within the word combination of thetarget items, or No otherwise. The scores assigned to each nodewould be updated accordingly. We note that in the following definition when a session encom-passes multiple target items, the conversational agents requirementis limited to identifying a single target item that meets the userscriteria. One underlying assumption under this setting is that usersalways have a clear picture of their interests, which may not holdin practice, as users often have multiple interests and some of themwould be ambiguous or not clearly defined. For instance, as de-picted in , a user may simultaneously express a preferencefor both Item A and Item B. If the conversational agent inquireabout airpods, the user agent is expected to affirm with a Yes,thereby directing the recommender system to identify Item B as apotential match. However, in practical scenarios, the user mightalso favor Item A concurrently, given that the users preferencesare not mutually exclusive.Therefore, we introduce another setting that permits the userto respond with Not Care when faced with a query from the con-versational agent that pertains to attributes or features present inmultiple preferred items. This response option acknowledges theambiguity of user preferences and allows for a more nuanced inter-action with the recommender system. The corresponding resultsunder this setting are available in Appendix E.1.",
  "D.2Empowering MAGUS with Large LanguageModels": "As depicted in , our MAGUS system integrates both queriesand items in its output layout. However, a potential concern withthis design is the occasional generation of queries that may seemartificial, potentially diverging from user-friendly formulations. Tomitigate this challenge, a feasible strategy entails harnessing thecapabilities of large language models while employing targetedprompts to steer the generation of queries toward user-friendly,natural language formulations.Initially, if the expectation is for the final queries to consist ofcombinations of keywords, the following prompt can be utilized:prompt = f\"\"\"You will be provided with text delimited by triple quotes.If words in the text can be keywords of a query, then you just return it.Otherwise, you should re-organize these words. You should just returnthe original or re-organized words, not sentences.text\"\"\"Secondly, if we anticipate the final queries to be in the form ofcompleted sentences, we can utilize the following prompt.prompt = f\"\"\"You will be provided with text delimited by triple quotes.",
  "DeepFM+MAGUS0.7590.6550.7600.7850.7490.840": "The input text can be regarded as a class of items. You should generatea sentence to ask whether the user likes this class of items or not. Youshould be gentle.text\"\"\"Next, we proceed to assess the performance of the aforemen-tioned methods as follows. The first prompt is tailored to extractkeywords from the input text while eliminating redundancy. Forinstance, if the input text is blue clothes shirt, the output would beblue shirt with clothes considered redundant due to the presenceof shirt in the query. The second prompt is designed to generatea sentence or conversation based on the query. For example, if theinput text is blue shirt, the output would be Do you like blueshirts? This output constitutes a complete sentence suitable foruse in a conversation.We also note that LLM can encode some latent features from theconversations. For example, we can add the following statementin the input prompt: If you can infer the user has a low budget,please make recommendations with low prices. In response to theuser mentioning I am a student, the system would subsequentlyprovide queries or items characterized by low prices.Considering that the above process only requires a large lan-guage model API, it is very practical to integrate a large languagemodel into our MAGUS system as later discussed in Appendix F.1.",
  "EADDITIONAL EXPERIMENTSE.1Performance Comparisons with AmbiguousUser Feedback": "As outlined in Appendix D.1, the user agent, used in .1,is designed to respond with Yes to the conversational agent ifthe query matches any of the designated target items. However,this assumption may not always be valid in practice. Consequently,an alternative configuration is presented in Appendix D.1 to ac-count for the variability and potential ambiguity inherent in userfeedback. reports the corresponding results on Amazon and Alipaydatasets. The table demonstrates that our MAGUS system consis-tently outperforms the baseline methods, even when consideringthe ambiguity of user feedback, thereby showcasing the robustnessof our system. Furthermore, a comparison between and reveals that ambiguity significantly impairs performance,particularly for X and X+ME. One possible explanation is that X,which incorporates user feedback as input, inevitably introducesmore noise, and X+ME, which emphasizes querying over item rec-ommendation, is more susceptible to user ambiguity, as ambiguityfrequently arises during the querying process.",
  "E.2Complexity Study": "We conducted an investigation into the time complexity of MAGUSand MAGUS+ based on the FM, GRU, DIN, and RGCN, recommen-dation bases on Tmall dataset. We report the training and inferencetimes for one round of the whole data in . In the trainingphase, MAGUS showed almost no significant difference in termsof computational time when compared to the baseline methods.This is because the computations introduced by MAGUS mainlyinvolve building the relational graph, which is relatively efficient.In the inference phase, the computational costs of both MAGUS andMAGUS+ are incurred by the label propagation algorithm. The useof MAGUS+ introduces some additional computational overheadcompared to MAGUS, as it involves running the one-round featurepropagation on the graph.",
  "E.3Case Study": "In order to further demonstrate the superiority of MAGUS, weconduct a case study on Amazon dataset. illustrates theprocess of MAGUS. MAGUS first initializes scores for nodes in therelational graph. These nodes represent individual words and com-binations of words, serving as queries and items. After receivingresponses from a specific user, MAGUS updates the scores accord-ingly. This dynamic process allows MAGUS to adapt to the userspreferences and provide personalized recommendations.We argue that this practical case holds significance because itcontrasts a simple and commonly used method that alwaysrecommends highly popular queries or items to users (denoted asMPS in our experiment). For clarity, we have also incorporated avisualization of each nodes popularity level in the figure. In thespecific instance, MAGUS recommends clothes as a query in thefirst round and item 1940 in the second round, while the popularity-based method would suggest food in the first round and clothesin the second round.",
  "Offline Training": "Plug-and-Play Large Language Model API : Online deployment of the MAGUS system can beseamlessly integrated into existing real-time recommenda-tion servers. The diagram below shows the workflow, wheresolid arrows represent the offline training phase, and hollowarrows represent the online update phase. Most of the compo-nents required for the MAGUS system are already present inexisting recommender systems. The main additions are themultiple relational trees, which store the relational graphused by MAGUS. We also incorporate large language modelsin a plug-and-play manner to decorate the artificial queriesgenerated by MAGUS.",
  "FDEPLOYMENT FEASIBILITYF.1System Architecture for Deployment": "Here, we discuss the feasibility of the industrial deployment of ourMAGUS system. Fortunately, integrating the MAGUS system intoan existing recommendation platform should not pose a substan-tial workload. The primary modifications introduced by MAGUS involve the organization of the relational graph and its continuousupdating based on user responses. The majority of the predictionmodel pipeline remains unchanged.Integrating the MAGUS system into an existing recommenda-tion platform should not introduce a significant computationalburden, since the primary modifications only involve organizingthe relational graph and updating the graph by user responses.To optimize the computation costs associated with using thegraph multiple times, we propose a strategy for simplification. Thisinvolves creating multiple relational trees, as illustrated in .Each of these trees originates from a root node, which typicallyrepresents an item, and then branches out to nodes representing in-dividual words. As an example, consider (a). In this case, oneof these trees extends from a node representing Item A and branchesto nodes that represent individual words like apple and grey. Theprimary benefit of this approach is the ability to parallelize com-putations and focus on localized operations within each tree. Thisparallelization can lead to significantly improved efficiency whenworking with the graph in a multiple-round recommender system.",
  "F.2Extension of MAGUS from Top-1 to Top-NRecommendations": "To extend MAGUS from top-1 recommendations to top-N recom-mendations, we only need to make the following modifications toAlgorithm 1.First, instead of selecting only one recommendation MAGUS inline 9, we now recursively select a list of MAGUSs as recommen-dations. Any one of these MAGUSs can represent either a query oran item, and they are chosen based on the top- predicted scoresfrom the recommender system RE().Second, the users response and the subsequent updates differfrom lines 10 and 11. In the case of binary user response (namely, theuser response is either positive or negative) to a recommendation,the response to top-N recommendations fails to two categories. (i)If the user clicks on one of the MAGUSs, denoted as MAGUS, we setthe score of the node representing MAGUS to 1, and the scores of thenodes representing the remaining 1 MAGUS are set to 0. Thisadjustment is accomplished by modifying Eq. (8) accordingly. (ii) Ifthe user does not click on any of the MAGUSs, we assign scoresof 0 to the nodes representing all MAGUSs. For any category ofuser responses, we subsequently apply Eq. (9) to propagate theseupdated scores to the other nodes in the graph.These modifications enable the MAGUS system to provide top-Nrecommendations and handle user responses accordingly.",
  "F.3Deployment Feasibility for ExploratorySearch": "Here, we extend the example in (b) to further illustrate howour MAGUS system would enable the exploratory search whereusers often have a vague idea of the items they seek. As introducedin , a user tends to purchase milk but lacks specific criteriasuch as the preferred brand or price range to give a well-formedquery. In such an instance, one viable solution is to provide a se-lection of recommended milk items alongside suggested queries,allowing users to engage in navigating and comparing items priorto refining their search queries. Therefore, the user initially chooses",
  "Earphone": "Brand B Item B $50 T-Shirt AppleQuery B Query CPhoneBrand C (a) Startup Page(b) Recommendation Page : An illustrated example of exploratory search iswhere users often have a vague idea of the items they seek.MAGUS provides a selection of recommended items along-side suggested queries, allowing users to engage in navigatingand comparing items prior to refining their search queries. to search for the query Milk from the startup page, comparing itwith other presented queries and items, as depicted in (a).Subsequently, MAGUS provides the user with multiple queriesand items, enabling her to compare suggested queries (at an ab-stract level) or recommended items (at a specific level) to determinewhether to search for a more specific query or directly select anitem. As a result, the user chooses item D. From the above example,we can see that achieving this requires jointly considering queriesand items; however, almost all the deployments of either queryrecommendations or item recommendations have predominantlyoccurred in isolation from one another."
}