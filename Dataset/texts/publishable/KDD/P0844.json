{
  "Abstract": "Large language models (LLMs) have been demonstrated to possessthe capabilities to understand fundamental graph properties andaddress various graph reasoning tasks. Existing methods fine-tuneLLMs to understand and execute graph reasoning tasks by spe-cially designed task instructions. However, these Text-Instructionmethods generally exhibit poor performance. Inspired by tool learn-ing, researchers propose Tool-Instruction methods to solve vari-ous graph problems by special tool calling (e.g., function, API andmodel), achieving significant improvements in graph reasoningtasks. Nevertheless, current Tool-Instruction approaches focus onthe tool information and ignore the graph structure information,which leads to significantly inferior performance on small-scaleLLMs (less than 13B). To tackle this issue, we propose GraphTool-Instruction, an innovative Instruction-tuning approach that de-composes the graph reasoning task into three distinct subtasks(i.e., graph extraction, tool name identification and tool parame-ter extraction), and design specialized instructions for each sub-task. Our GraphTool-Instruction can be used as a plug-and-playprompt for different LLMs without fine-tuning. Moreover, build-ing on GraphTool-Instruction, we develop GTools, a dataset thatincludes twenty graph reasoning tasks, and create a graph rea-soning LLM called GraphForge based on Llama3-8B. We conductextensive experiments on twenty graph reasoning tasks with dif-ferent graph types (e.g., graph size or graph direction), and wefind that GraphTool-Instruction achieves SOTA compared to Text-Instruction and Tool-Instruction methods. Fine-tuned on GTools,GraphForge gets further improvement of over 30% compared to theTool-Instruction enhanced GPT-3.5-turbo, and it performs compa-rably to the high-cost GPT-4o. Our codes and data are available at",
  "Introduction": "Although Large Language Models (LLMs) excel in fields such asnatural language processing, they encounter significant challengeswhen dealing with graph data. Graph structures exhibit high con-nectivity, rich combinatorial properties and Non-Euclidean char-acteristics, making their processing fundamentally different fromtraditional text or image data . Research shows thatLLMs possess a basic understanding of graph properties and the ca-pabilities to solve graph reasoning tasks, but their accuracy remainsconsiderably deficient because of two challenges. Challenge 1 Graph Understanding (GU): Can LLMs reallyunderstand the correct topology information of the graph withnatural language? Challenge 2 Graph Processing (GP): Can LLMs really have theability to solve the graph algorithm (e.g., Shortest Path Problem)with generative model inference?To enhance the GP ability of LLMs, researchers explore theText-Instruction approaches which initially begin with the Chainof Thought (CoT) . This method emphasizes that LLMs cansolve graph problems through step-by-step reasoning. However,the performance improvement by CoT is limited in GU challengewhen dealing with complex graph reasoning tasks. Recent studieshave proposed methods that rely on providing additional graphinformation prompts or explicit reasoning steps to en-hance the LLMs GU ability (see a). While these methodsshow improvements within their specific prompted tasks, they maycompromise their effectiveness in out-of-domain tasks.Some researchers realize that LLMs cannot deal with all pro-fessional problems with only generative model inference. Tool-former first introduces the concept of tool learning into LLMs,which not only generates text but also can call on and utilize specifictools to solve more complex and professional problems. Inspired by",
  ": (a) Text-Instruction method represented by Graph-Wiz; (b) Tool-Instruction method represented by Graph-ToolFormer; (c) GraphTool Instruction-tuning method": "Toolformer, Graph-ToolFormer is the first work that introducesthe Tool-Instruction method to solve GP challenge in graph rea-soning tasks (see b). However, Graph-ToolFormer suffersfrom GU problem, which shows limited performance on queriescontaining both the graph information and task description, such asNLGraph and GraphInstruct . Meanwhile, constrained byissues such as the diversity of tool names and tool calling formats,Graph-ToolFormer exhibits low performance on the GP challenge.Current most powerful closed-source LLMs like GPT-4o ,Claude-3-opus and GLM4-0520 have introduced a new Tool-Instruction method (i.e., Function Calling). This method allowsLLMs to act as controllers that integrate tool descriptions into theinput prompts. Function Calling improves LLMs ability to under-stand and use external tools, making them more flexible withoutadditional training . However, this method dependson the quality of the tool documentation and output text by LLMswhich may result in instances of failure or incorrect tool invoca-tion . Especially when dealing with graph reasoning tasks, theinternal Function Calling mechanism of LLMs struggles to handlethe GU challenge.In this study, we propose GraphTool-Instruction, a novelmethod designed to enhance the capabilities of LLMs in addressinggraph reasoning tasks. Compared with methods listed in ,our method first decomposes the graph reasoning task into three subtasks graph extraction, tool name identification and parameterextraction. For the GU challenge, we propose Graph-Instruction tosolve graph extraction task, which enhances the LLMs to identifyand extract graph structure information from natural language orfile paths. For the GP challenge, we decompose traditional Tool-Instruction into Task-Instruction and Parameter-Instruction. Atfirst, Task-Instruction guides LLMs to choose the right graph toolsfor solving graph reasoning tasks, along with constraints on theoutput formats of tools. Then the Parameter-Instruction retrievesthe graph tool parameter for tasks that require specific inputs, suchas the starting and ending nodes in the Shortest Path task. Ourexperiment results demonstrate that, even without fine-tuning, ourapproach achieves an average accuracy of 94% on Llama3-8B ,markedly outperforming Text-Instruction methods over 40% andGPT-3.5-turbo-FC over 30%. To further enhance the reasoning ca-pabilities of all LLMs, we have developed GTools, an Instruction-tuning dataset that includes twenty different graph reasoning taskswith 40,000 instances. Moreover, we develop an open-source LLMfor graph reasoning tasks called GraphForge based on Llama3-8Bfine-tuned with our proposed GTools. Our GraphForge achieves anaverage accuracy of over 98% on all graph reasoning tasks whichperforms comparably to the high-cost GPT-4o-FC. The contribu-tions of this paper are as follows: We summarize LLMs-based graph reasoning methods and pro-pose GraphTool-Instruction, a novel GraphTool level Instruction-tuning method which first decomposes the graph reasoning taskinto three subtasks graph extraction, tool name identification andparameter extraction with corresponding Graph-Instruction, Task-Instruction and Parameter-Instruction. We develop GTools, the first GraphTool-Instruction dataset com-prising twenty types of graph reasoning tasks. GTools excels inboth the variety of tasks and the scale of graphs, thus posing agreater challenge to LLMs in capturing graph structure informa-tion. Furthermore, we develop GraphForge based on Llama3-8Bfine-tuned with GTools. By incorporating Graph-Instruction and Parameter-Instruction,GraphTool-Instruction significantly improves the accuracy ofTool-Instruction, and achieves state-of-the-art results among allTool-Instruction methods, except being 1% behind GPT-4o-FC. We have introduced three new evaluation metrics: Graph, ToolName and Tool Parameter to enhance the reliability of our dataset.Furthermore, we utilize the accuracy rates of these three metricsto deeply analyze the factors that affect the tool execution results.",
  "Related Works": "Amidst the proliferation of LLMs, there is growing scholarly interestin integrating these computational frameworks with graph data . On the one hand, practical performance assessmentsare conducted by some researchers, investigating whether LLMshave the capabilities of reasoning on graphs such as NLGraph ,GPT4Graph and GraphInstruct . On the other hand, severalstudies, exemplified by Talk like a Graph , explore the impact ofdifferent graph description languages on LLMs understanding ofgraph data. These studies inspire researchers to develop a compre-hensive method that enables LLMs to solve various types of graphreasoning tasks.",
  "Graph-Instruction": "You will be assigned a graph reasoning task along with a graph path. Your initial task is to extract the graph information. The graph informationt is stored in an edgelist file.Input: Given a directed graph and a path, The edges are in the egdelist file, the path is: /home/GraphTools/EL_graph/Shortest/data/shortest0.edgelist. Output: path: \" /home/GraphTools/EL_graph/Shortest/data/shortest0.edgelist\"",
  "Tool Execution": "You will be given a full graph, your first mission is to extract graph in the format of I give you, the graph may have weight, so there are two types.Input:Given an weighted undirected graph, The edges are: [(0, 1, {'weight': 1}),... (0, 2, {'weight': 9})]. Output: G: [(0, 1, {'weight': 1}),... (0, 2, {'weight': 9})]Input:Given an weighted undirected graph, The edges are: [(5, 6), (6, 8), ... (1, 5)]. Output: G: [(5, 6), (6, 8), ... (1, 5)]",
  "Task-Instruction": "Given a directed graph and a path, The edges are: [(0, 1, {'weight': 15}), (0, 2, {'weight': 66}), (0, 3, {'weight': 84}), (0, 5, {'weight': 92}), (0, 7, {'weight': 27}), (1, 4, {'weight': 95}), (1, 7, {'weight': 20}), (2, 3, {'weight': 63}), (3, 1, {'weight': 67}), (3, 2, {'weight': 66}), (3, 4, {'weight': 85}), (3, 7, {'weight': 84}), (4, 1, {'weight': 41}), (4, 5, {'weight': 53}), (4, 6, {'weight': 19}), (5, 1, {'weight': 43}), (5, 2, {'weight': 37}), (5, 6, {'weight': 89}), (5, 7, {'weight': 72}), (6, 1, {'weight': 3}), (6, 4, {'weight': 70}), (7, 0, {'weight': 21}), (7, 3, {'weight': 59}), (7, 4, {'weight': 60})]. The task is: you need to determine the shortest path between two specific nodes in the graph. The nodes in question are (1, 5).",
  "Concat": ": The overview of LLM solves graph reasoning tasks based on GraphTool-Instruction. Basic Graph Analysis Task(BGA-Task) does not require additional tool parameters, whereas Parametric Graph Query Task (PGQ-Task) requires specificinput tool parameters for reasoning. WL-Graph denotes a task length within 4096 tokens, while EL-Graph is the opposite.The red arrow shows the BGA-Task reasoning process, and the blue arrow shows the PGQ-Task process, which additionallyintroduces Parameter-Instruction to enhance the accuracy of parameter extraction.",
  "Text-Instruction": "CoT is always the primary approach for solving graph rea-soning tasks. Many researchers have conducted explorations basedon this method and its variants . Results indicate thatCoT-style reasoning can improve performance on simple graph rea-soning tasks, such as Cycle Detection and Shortest Path. However,this improvement is inconsistent on more complex tasks, such asMaximum Flow and Topological Sorting . To enhancethe performance on complex graph reasoning tasks, GraphWiz employs GPT-4 to generate initial reasoning paths and utilizes amulti-sampling approach to enhance the accuracy of model outputs,demonstrating high accuracy and generalization capabilities acrossvarious graph problems. GraphInstruct enhances reasoningperformance by providing a diverse array of graph generation pro-cesses and detailed reasoning steps, as well as a step mask trainingstrategy. Although results have demonstrated their effectiveness,these methods exhibit poor performance in out-of-domain tasks.",
  "Tool-Instruction": "The integration of external tools is a novel way toboost LLMs functional capabilities. A common approach is to cre-ate Instruction-tuning datasets which includescenarios requiring tools. The LLMs are then fine-tuned to outputtext that effectively uses these tools. However, these efforts mainlyfocus on generating tool calls within real-world scenarios. In fact, graph reasoning tasks are highly suitable for reasoning by tools.Graph-ToolFormer represents the pioneering research thatenables LLMs to generate tool calls for reasoning. This approach issuitable for graph reasoning tasks from knowledge graphs, socialnetworks and recommendation systems. However, the tool callsare executed to acquire answers on an external graph which limitsits ability to generalize across graph reasoning tasks organized innatural language.Despite their potential, the use of Tool-Instruction with the cur-rent most powerful closed-source LLMs like GPT-4o , Claude-3-opus and GLM4-0520 is limited because they cant easily addnew tools. To address these issues, these LLMs have introduced anew approach called Function Calling. This approach allows LLMsto act as controllers that integrate tool descriptions right into theinput prompts, significantly enhancing LLMs ability to understandand utilize external tools, making them more functional and flexiblewithout additional training. However, this approach depends onthe quality of the tool documentation and output text by LLMs.Consequently, output text that is difficult to parse may result ininstances of failure or incorrect tool calls .",
  "Conference acronym XX, XX XXXX, 20XX, XX, XXRongzheng Wang, Shuang Liang*, Qizhi Chen, Jiasheng Zhang, and Ke Qin": "[{'name': 'shortest_path', 'description': 'Given a graph G, a source node and a target node, compute shortest paths in the graph.', 'parameters': (graph = G, path_source= , path_target= ), 'return_type': List }, .{'name': 'maximum_triangle_sum', 'description': 'Given a graph G, find the maximum sum of weights for any connected triplet of nodes.', 'parameters': (graph = G), 'return_type': Int },]",
  "GraphTool-Instruction Construction": "We select eleven classic graph reasoning tasks and generate exam-ples for both directed and undirected graphs for each task. The listof tasks is presented in and the detailed descriptions arein Appendix C. Notably, we define the Maximum Triangle Sumas finding the triangle with the largest sum of edge weights, there-fore this task is exclusive to undirected graphs. While TopologicalSorting is specific to directed graphs. In total, we identify twentydistinct graph reasoning tasks. For these graph reasoning tasks, weclassify them into two primary categories as shown in . Thefirst category is Basic Graph Analysis Task (BGA-Task), whichgenerally requires information on the graph structure and the toolname such as Cycle Detection. The second category is ParametricGraph Query Task (PGQ-Task) with the necessity for additionalparameter inputs, e.g., the Shortest Path necessitates the startingand ending nodes.We propose GraphTool-Instruction in addressing graph reason-ing tasks with three components: Graph-Instruction, Task-Instructionand Parameter-Instruction. We employ Graph-Instruction and Task-Instruction as shared components for both BGA-Task and PGQ-Task.Graph-Instruction is designed to extract the graph structure infor-mation, whereas Task-Instruction is designed to enable LLMs toidentify the tool names and extract parameters associated withthese tools. It should be noted that, while the tool names identifiedfrom Task-Instruction are highly accurate, the associated tool pa-rameters frequently suffer from issues such as omissions, incorrectorder and non-compliance with the required execution formats.Consequently, for PGQ-Tasks, we propose Parameter-Instructionto enable LLMs to output tool parameters in a more accurate andstandardized format based on the tool name provided by Task-Instruction. Therefore, these three instructions can be summarizedserving three subtasks: graph extraction, tool name identificationand tool parameter extraction.Graph-Instruction (for GU): This instruction is designed toenable LLMs to extract graph structure information from the giventasks. To assess and enhance the capabilities of LLMs in processinggraphs of various sizes, we establish a benchmark using the com-monly accepted maximum token length of 4096 for current LLMs.This threshold serves to categorize graph sizes into Within LimitGraph (WL-Graph) and Exceeds Limit Graph (EL-Graph). WL-Graph ensures that the entire graph can be directly input into LLMsin textual form. Meanwhile, EL-Graph accommodates larger graphstructures and we store the graph in files with file paths providedto LLMs.Consequently, we have elaborately crafted two types of Graph-Instruction. For WL-Graph, we employ a Two-shot Prompt of ex-tracting graph information for both weighted and unweighted",
  "Edge ExistenceNode ExistenceMaximum FlowPath ExistenceShortest Path": "graphs in a list format of NetworkX . By utilizing regular ex-pressions to parse the output text that includes the graph structureinformation, we can reconstruct the graph in tools (the regular ex-pressions can be found in Appendix B). Given the size constraintsof EL-Graph, it becomes hard to extract complete graph structureinformation based on natural language. Therefore, we replace thegraph with the file path. For EL-Graph, a one-shot prompt is usedto direct LLMs to identify and extract the file path, enabling toolsto retrieve graph structure information from the specified path.Two examples for both EL-Graph and WL-Graph can be found inAppendix E, , 7.Task-Instruction (for GP): To construct the Task-Instruction,we manually create a tool set. For each tool, we define four at-tributes: Tool Name, Tool Description, Tool Parameters and Returntype. This set is intended to inform LLMs about the appropriategraph reasoning tasks for each tool. Based on the predefined toolset, we add some general descriptions of the expected format toconstrain the output from LLMs. An example of the set of tools andgeneral descriptions is presented in Appendix E, .Parameter-Instruction (for GP): For PGQ-Tasks, we specifi-cally employ Parameter-Instruction to further standardize the for-mat of parameters extracted by Task-Instruction. At first, We pro-pose a Tool Template Retriever, which identifies the tool name basedon previous Task-Instruction and then retrieves the correspondingtool template from the tool set. Second, we combine the searchedtool template with Parameter-Instruction as a new input to gethighly accurate tool parameters. An example of the Parameter-Instruction is presented in Appendix E, .",
  "Various descriptions: For each graph reasoning task, we haveprepared five different descriptions to evaluate the task identifi-cation capabilities of LLMs": "Balanced answers: For graph reasoning tasks that determinetruth or falsehood, we ensure an even distribution of answersduring the graph generation process. This approach aims to pre-vent LLMs from developing a bias toward any particular type ofresult, thereby avoiding artificially high accuracy rates. Unique answer: For Topological Sorting, which may have mul-tiple valid solutions, we ensure the uniqueness of the answersduring the graph generation process. This facilitates the compar-ison between the LLMs results and standard labels. Graph Reasoning: Since the graph and task are determined,an accurate answer label can be obtained through the algorithmprogram. We use Llama3-8B as our base model to solve var-ious graph reasoning tasks based on GraphTool-Instruction. Theexplicit steps of reasoning are presented in . For each task,we define three subtasks: graph extraction (G), tool name identifi-cation (N) and tool parameter extraction (P) represented by a set = {G, N, P}. Each subtasks output () is generated by LLMsaccording to the corresponding instruction () and task ():",
  "where()is the actual output by LLMs, R and R": "are the low-rank matrices, is the chosen rank significantly smallerthan min (,), and controls the magnitude of the updates tothe original weight matrix W. During the learning process, onlythe matrices and are updated.The cross-entropy loss function measures the discrepancy be-tween the actual labels and the predicted labels. By comparing theLLMs output ()with the expected output (), we define the lossfunction for fine-tuning the GraphForge as:",
  "Experiments4.1Experimental Setting": "We conduct extensive experiments to evaluate our method andmodel, i.e., GraphTool-Instruction and GraphForge, covering bothin-domain and out-of-domain tasks. We implement GraphTool-Instruction on three open-source LLMs to validate the effectivenessof our method. Our experiment is designed to answer the followingthree research questions: RQ1 (Main results): How does GraphTool-Instruction performcompared to Text-Instruction and Tool-Instruction methods?",
  "GraphToolGraphToolLlama3-8B WL/ELGraphToolLlama3.1-8B WL/ELGraphToolGLM4-9B WL/EL": "Test Datasets: According to the existing generation rules ofGTools, we have created an additional 500 test instances for eachtask as our test dataset. Besides, we select Path Existence, CycleDetection, Topological Sorting, Maximum Flow, and Shortest Pathtasks from NLGraph. Since GraphForge has not been trained on NL-Graph, we directly combine the test dataset and training dataset ofNLGraph for testing. In addition, we select two tasks from NLGraph:Bipartite Graph Matching and GNN, as out-of-domain tasks to vali-date the effectiveness of GraphForge. (The GNN task in NLGraph isdefined as: Given an undirected graph and a two-dimension nodeembedding for each node, update the node embedding with thesum of all the neighbors embeddings.)Baselines: We carefully select baseline models from four cate-gories for a comprehensive evaluation. We summarize the methodsand their base models in : Closed-source LLM: We selected the most powerful closed-source LLMs currently available. In our experiments, we use theCoT method with a Two-shot prompt based on two series of",
  "closed-source LLMs: Claude and GPT. The specific versions ofthese LLMs are detailed in Appendix D": "Text-Instruction LLM: We have also implemented two recentlyreleased Text-Instruction methods, NLGraph and GraphWiz withGPT-4-turbo and Llama2-13B. Due to the inherent limitations ofthese LLMs in text-based reasoning, our experiments are exclu-sively conducted on WL-Graph. Tool-Instruction LLM: In light of recent support for Func-tion Calling officially released by some closed-source LLMs, wechoose GLM4-0520, GPT-3.5-turbo, and GPT-4o as exemplarsof the most robust Tool-Instruction LLMs. Additionally, we em-ploy the Graph-ToolFormer. Due to Graph-ToolFormer has onlyreleased a model based on GPT-J-6B, we utilize the method ofGraph-ToolFormer to fine-tune Llama3-8B on our dataset to en-sure a fair comparison of experiment results. For all the afore-mentioned LLMs, experiments will be conducted on both WL-Graph and EL-Graph. Given the Graph-ToolFormers deficiencyin graph structure extraction capabilities, we employ GraphForgeto extract graph structure information for Graph-ToolFormer onWL-Graph.",
  "Experimental Setup": "We use an NVIDIA A800 GPU to fine-tune GraphForge and Graph-Toolformer based on LoRA. For inference with all open-sourceLLMs, we employ a total of 16 NVIDIA Tesla T4 GPUs. For allclosed-source LLMs, we leverage the official API interfaces. Duringinference process, we set the max number of new tokens to 4096,with a sampling parameter top_p of 1 and a temperature of 0.7.",
  "MainResults (RQ1)": "In this experiment, we first evaluate GraphForge compared to state-of-the-art baselines based on Text-Instruction. Due to LLMs capa-bilities constraints, we conduct experiments solely on WL-Graphfor all LLMs listed in . Subsequently, we evaluate GraphForgeagainst Tool-Instruction methods. For all LLMs listed in ,experiments are conducted on both WL-Graph and EL-Graph. Weemploy answer accuracy as the evaluation metric, which calculatesthe percentage of questions that the LLM correctly predicts out ofthe total questions in the test dataset. We demonstrate ten types ofgraph reasoning tasks, and the complete experiment results are inAppendix A. We observe the following results: GraphForge demonstrates graph reasoning capabilities that sig-nificantly surpass those of all Text-Instruction methods acrossvarious tasks, achieving an average performance of 98.4%, in con-trast to the highest average of 46.2% recorded by other methods. Compared with Graph-ToolFormer, GraphForge has consistentlysurpassed by over 40% on both WL-Graph and EL-Graph. Thismarked improvement is primarily due to the inherent limita-tions of the Graph-ToolFormers method in accurately identifyinggraph reasoning tasks and extracting tool parameters.",
  "GraphTool-Instruction: Revolutionizing Graph Reasoning in LLMs through Decomposed Subtask InstructionConference acronym XX, XX XXXX, 20XX, XX, XX": "You are GraphForge, you can use many tools(APIs) to do the following task.First I will give you the task description, and your task start.You need to give your thought to analyze with a function call to actually excute your thought. Your output should follow this format:Thought:Tool_Name:Tool_Para: Remember: 1.All the thought is short, at most in 5 sentence.2. Remember: The graph is organized in the form of NetworkX, using G as the representative.----Specifically, you have access to the following Tools:[{'name': 'shortest_path', 'description': 'Given a graph G, a source node and a target node, compute shortest paths in the graph.', 'parameters': (graph = G, path_source= , path_target= ), 'return_type': List }, .{'name': 'maximum_triangle_sum', 'description': 'Given a graph G, find the maximum sum of weights for any connected triplet of nodes.', 'parameters': (graph = G), 'return_type': Int },]",
  "Overall30.832.741.735.741.446.241.594.098.4": ": Experiment with Tool-Instruction and GraphTool-Instruction methods on both WL-Graph and EL-Graph. Symbol* represents closed-source LLMs based on Function Calling (e.g., GPT-4o* represents GPT-4o-FC). For conciseness, GLM4-9B,Llama3-8B, Llama3.1-8B using GraphTool-Instruction without fine-tuning, Graph-Toolformer and GraphForge are abbreviatedas GLM4 , Llama3 , Llama3.1 , Graph-TF and GF, respectively. The best results of Tool-Instruction methods and our methodin WL-Graph are colored: Tool, Ours and for EL-Graph: Tool, Ours.",
  "Overall57.458.860.798.562.298.398.899.593.296.194.096.489.590.298.499.0": "Compared with GLM4-0520-FC and GPT-3.5-turbo-FC, Graph-Forge achieves a lead of over 30% on WL-Graph. It is importantto note that GLM4-0520-FC demonstrates poor performance onWL-Graph, with remarkably low accuracies observed in the Max-imum Triangle Sum, Maximum Flow and Shortest Path. The sameissue has also occurred with GPT-3.5-turbo-FC, where we observethe absence of graph structure information and the inability toparse tool parameters. We have detailed the error analysis in.5 and the case study is in Appendix F. Among GLM4-9B, Llama3-8B and Llama3.1-8B based on GraphTool-Instruction. We observe that all three models demonstrate a highaccuracy in most tasks. Moreover, the performance of GLM4-9B,when utilizing our approach, surpasses that of GLM4-0520-FCemploying Function Calling, thus validating the efficiency ofour method. However, we have found GLM-9B and Llama3.1-8Bexhibit lower accuracy on some specific tasks, the analysis is inAppendix A.",
  "Cycle99.086.2/86.2Triangle97.866.8/66.8Path98.852.089.051.2Shortest98.240.692.233.6": "We observe a substantial enhancement with Graph-Instructionwhich demonstrates its effectiveness in facing GU challenge. Uponexamining the outputs, we find that the absence of Graph-Instructionleads to unparseable issue about graph structure information. Thisissue arises from the LLMs inability to concurrently and accuratelyexecute three subtasks: graph extraction, tool name identificationand tool parameter extraction. The implementation of Parameter-Instruction also results in about 8% improvement in accuracy whichdemonstrates its effectiveness in facing GP challenge. This enhance-ment is because LLMs still cannot fully adhere to the instruction,often encountering issues in parameters extracting such as mis-alignment, omissions, and inconsistency with predefined formats.",
  ": Impact of graph, name and parameter accuracieson overall answer accuracy. Notably, both Cycle Detectionand Maximum Triangle Sum are BGA-Task, so there is noresult for Parameter Accuracy": "We conduct further analysis to identify factors that influence theaccuracy of answers in . For this purpose, we introduce threeaccuracy metrics: Graph Accuracy, Name Accuracy and ParameterAccuracy. Notably, for assessing whether graphs match, we directlycompare the list of edges extracted by LLMs with the standard list.We consider the graphs inconsistent if there is any discrepancyin even a single edge. The experiment is conducted on Llama3-8Bwith GraphTool-Instruction.We observe that Name Accuracy significantly surpasses the othermetrics, indicating that LLMs possess robust task identificationcapabilities. Due to the Parameter-Instruction relying on LLMsidentification of the tool name, Parameter Accuracy is slightly lowerthan Name Accuracy. For tasks that rely on the entire informationof graph structure such as Maximum Triangle Sum and ShortestPath. The Answer Accuracy depends on Graph Accuracy. Inaccurate",
  ": Error Analysis on GPT-3.5-turbo-FC, GLM-0520-FC,Graph-ToolFormer and GraphForge. Mis is short for Mis-macth": "extraction of graph structure information can lead to incorrectreasoning results, even if other parameters are extracted correctly.On the other hand, for tasks like Cycle Detection, which are lessdependent on the entire graph structure information, there is anotable disparity where Answer Accuracy surpasses Graph Accuracy.Thus, improving the precision of graph structure extraction couldbe crucial for enhancing the overall performance of LLMs.",
  "We show the statistics in and error cases in Appen-dix F. Among the four methods, the distributions of error typesare different:": "GLM4-0520-FC and GPT-3.5-turbo-FC exhibit similar distribu-tions of error types. However, there are significant differences inthe causes of these errors. For GLM4-0520-FC, during the Func-tion Calling API invocation process, even setting the API retrycount to 5 to eliminate network-related reasons, there are mul-tiple instances where the arguments return null. We speculatethat this is due to the internal parameter parsing mechanismof GLM4-0520 not handling graph structure information withweights specifically, leading to numerous Syntax Errors. WhileGPT-3.5-turbo-FC behaves differently. Both GPT-3.5-turbo-FCand GPT-4-turbo-FCs output contains significant informationgaps and omissions even based on a two-shot prompt for graphstructure extraction, leading to various Syntax Errors. However,we have discovered the impressive ability of GPT-4o-FC to follow",
  "Tool-Instruction, which achieves remarkable accuracy in variousgraph reasoning tasks": "For Graph-Toolformer, even if we mitigate its shortcomings byproviding graph structure information in advance, we observethat Graph-Toolformer has lower performance. After manuallychecking the output, we find the reason for this result is thatGraph-Toolformer often generates misalignment, omissions, andinconsistencies with the predefined tool formats. GraphForge is mainly encountered with Graph Mismatch. Thisissue tends to occur in scenarios where the number of edges inthe graph exceeds one hundred, indicating that the model demon-strates weaker graph structure extraction capabilities when deal-ing with very long inputs.",
  "Conclusion": "In this work, we propose an innovative GraphTool-Instruction toenhance LLMs graph reasoning capabilities. Experiment resultshave demonstrated the robustness of our method, which notablysurpasses existing Text-Instruction and Tool-Instruction instruc-tion methods. We have also proposed our dataset GTools, whichencompasses twenty graph reasoning tasks, further enhancing thecapabilities of LLMs in reasoning on graph tasks. Three new evalu-ation metrics: Graph, Tool Name and Tool Parameter are employedto ensure the reliability of our dataset. Furthermore, our modelGraphForge fine-tuned on the GTools, showcases outstanding per-formance, achieving an accuracy of more than 98% accuracy. Ourfuture work aims to design a framework that can accommodate agreater variety of graph tools and enhance the capabilities of LLMsto tackle real graph reasoning challenges such as Recommendationand Knowledge Graph.",
  "AI Anthropic. 2024. The Claude 3 Model Family: Opus, Sonnet, Haiku. In Claude-3Model Card": "Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski,Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Pi-otr Nyczyk, and Torsten Hoefler. 2024. Graph of Thoughts: Solving ElaborateProblems with Large Language Models. In Thirty-Eighth AAAI Conference onArtificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applica-tions of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on EducationalAdvances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver,Canada, Michael J. Wooldridge, Jennifer G. Dy, and Sriraam Natarajan (Eds.).AAAI Press, 1768217690.",
  "Jiayan Guo, Lun Du, and Hengyu Liu. 2023. GPT4Graph: Can Large LanguageModels Understand Graph Structured Data ? An Empirical Evaluation and Bench-marking. CoRR abs/2305.15066 (2023)": "Aric Hagberg, Pieter J Swart, and Daniel A Schult. 2008. Exploring networkstructure, dynamics, and function using NetworkX. Technical Report. Los AlamosNational Laboratory (LANL), Los Alamos, NM (United States). Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, SheanWang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of LargeLanguage Models. In The Tenth International Conference on Learning Representa-tions, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Xin Zhao, and Ji-Rong Wen.2023. StructGPT: A General Framework for Large Language Model to Reasonover Structured Data. In Proceedings of the 2023 Conference on Empirical Methodsin Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023,Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for ComputationalLinguistics, 92379251.",
  "Bowen Jin, Gang Liu, Chi Han, Meng Jiang, Heng Ji, and Jiawei Han. 2023. LargeLanguage Models on Graphs: A Comprehensive Survey. CoRR abs/2312.02783(2023)": "Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chlo Rolland, LauraGustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, PiotrDollr, and Ross B. Girshick. 2023. Segment Anything. In IEEE/CVF InternationalConference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023. IEEE,39924003. Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu,Zhoujun Li, Fei Huang, and Yongbin Li. 2023. API-Bank: A ComprehensiveBenchmark for Tool-Augmented LLMs. In Proceedings of the 2023 Conference onEmpirical Methods in Natural Language Processing, EMNLP 2023, Singapore, De-cember 6-10, 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Associationfor Computational Linguistics, 31023116. Zekun Li, Zhiyu Zoey Chen, Mike Ross, Patrick Huber, Seungwhan Moon, Zhao-jiang Lin, Xin Luna Dong, Adithya Sagar, Xifeng Yan, and Paul A. Crook. 2024.Large Language Models as Zero-shot Dialogue State Tracker through FunctionCalling. CoRR abs/2402.10466 (2024).",
  "Chang Liu and Bo Wu. 2023. Evaluating Large Language Models on Graphs:Performance Insights and Comparative Analysis. CoRR abs/2308.11224 (2023)": "Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu,Song-Chun Zhu, and Jianfeng Gao. 2023. Chameleon: Plug-and-Play Composi-tional Reasoning with Large Language Models. In Advances in Neural InformationProcessing Systems 36: Annual Conference on Neural Information Processing Sys-tems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, AliceOh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and SergeyLevine (Eds.). Zihan Luo, Xiran Song, Hong Huang, Jianxun Lian, Chenhao Zhang, Jinqi Jiang,Xing Xie, and Hai Jin. 2024. GraphInstruct: Empowering Large Language Modelswith Graph Understanding and Reasoning Capability. CoRR abs/2403.04483(2024). OpenAI. 2023. GPT-4 Technical Report. CoRR abs/2303.08774 (2023). Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. 2023. Gorilla:Large Language Model Connected with Massive APIs. CoRR abs/2305.15334(2023). Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin,Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, JieZhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2023. ToolLLM:Facilitating Large Language Models to Master 16000+ Real-world APIs. CoRRabs/2307.16789 (2023). Timo Schick, Jane Dwivedi-Yu, Roberto Dess, Roberta Raileanu, Maria Lomeli,Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023.Toolformer: Language Models Can Teach Themselves to Use Tools. In Advancesin Neural Information Processing Systems 36: Annual Conference on Neural Infor-mation Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December10 - 16, 2023, Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, MoritzHardt, and Sergey Levine (Eds.).",
  "alpaca": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-AnneLachaux, Timothe Lacroix, Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, Aurlien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lam-ple. 2023. LLaMA: Open and Efficient Foundation Language Models. CoRRabs/2302.13971 (2023). Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, andYulia Tsvetkov. 2023. Can Language Models Solve Graph Problems in NaturalLanguage?. In Advances in Neural Information Processing Systems 36: AnnualConference on Neural Information Processing Systems 2023, NeurIPS 2023, NewOrleans, LA, USA, December 10 - 16, 2023, Alice Oh, Tristan Naumann, AmirGloberson, Kate Saenko, Moritz Hardt, and Sergey Levine (Eds.). Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, DanielKhashabi, and Hannaneh Hajishirzi. 2023. Self-Instruct: Aligning LanguageModels with Self-Generated Instructions. In Proceedings of the 61st Annual Meetingof the Association for Computational Linguistics (Volume 1: Long Papers), ACL2023, Toronto, Canada, July 9-14, 2023, Anna Rogers, Jordan L. Boyd-Graber, andNaoaki Okazaki (Eds.). Association for Computational Linguistics, 1348413508. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, FeiXia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-Thought PromptingElicits Reasoning in Large Language Models. In Advances in Neural InformationProcessing Systems 35: Annual Conference on Neural Information Processing Systems2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, SanmiKoyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (Eds.).",
  "AComprehensive Experiment Results withBaselines": "We conduct comprehensive experiments on all twenty tasks. Dueto the weak OOD task performance of NLGraph and GraphWiz, wehave not conducted evaluation on the newly added tasks. Consid-ering these results, we have some new observations: shows that Claude and GPT demonstrate extremely strongperformance on Node Count and Edge Count tasks. This is be-cause such tasks only require models to focus on very limitedbut key information. The slightly lagging results from some Tool-Instruction methods are mainly due to incorrect tool name identi-fication, which leads to outcomes that are irrelevant to the tasks. shows that GLM-9B and Llama3.1-8B exhibit lower ac-curacy on tasks such as Maximum Triangle Sum, Degree Count,Shortest Path and Maximum Flow. Upon analysis of experimentresults, we discover that these two models tend to decomposecomplex tasks. For example, in the Shortest Path, the modelsinitially use tools to determine if a path exists between nodesbefore attempting to compute the shortest path. We considerthis to be a rational step in reasoning. However, due to the lackof a multi-step reasoning mechanism, we regard such steps asincorrect during the accuracy evaluation process.",
  "FError Analysis": "In this section, we present error cases about the Tool-Instructionmethods and our method. , 11, 12 present the errors ofGLM-0520-FC. It should be noted that due to GLM-0520-FCs inabil-ity to extract graph edges with weights, we use a one-shot promptto let GLM-0520-FC output edge list in a triplet form. shows the omission of the graph edges and tool parameters fromGPT-3.5-turbo which causes Syntax Errors. shows theMismatch and Syntax Error of Graph-Toolformer. showssome errors of GraphForge.",
  "Overall44.148.155.649.155.746.241.595.298.2": ": Experiment with Tool-Instruction and GraphTool-Instruction methods on both WL-Graph and EL-Graph. Symbol *represents model based on Function Calling. For conciseness, Llama3-8B using GraphTool-Instruction without fine-tunind andGraphForge are abbreviated as Llama and GF, respectively. The best results of Tool-Instruction methods and our method inWL-Graph are colored: Tool, Ours and for EL-Graph: Tool, Ours.",
  "GraphForge Output": ": The details of Parameter-Instruction. We first illustrate a possible error (e.g., an incorrect order causes Syntax Error)from GraphForges output based on Task-Instruction. We define a Tool Template Retriever to retrieve the predefined format ofparameters according to the Tool Name identified from Task-Instruction. Then we let GraphForge to extract the parameterbased on Parameter-Instruction concatenated with the retrieval result.",
  "Graph-Instruction for WL-Graph": "Given a directed graph and a path, The edges are: [(0, 1, {'weight': 15}), (0, 2, {'weight': 66}), (0, 3, {'weight': 84}), (0, 5, {'weight': 92}), (0, 7, {'weight': 27}), (1, 4, {'weight': 95}), (1, 7, {'weight': 20}), (2, 3, {'weight': 63}), (3, 1, {'weight': 67}), (3, 2, {'weight': 66}), (3, 4, {'weight': 85}), (3, 7, {'weight': 84}), (4, 1, {'weight': 41}), (4, 5, {'weight': 53}), (4, 6, {'weight': 19}), (5, 1, {'weight': 43}), (5, 2, {'weight': 37}), (5, 6, {'weight': 89}), (5, 7, {'weight': 72}), (6, 1, {'weight': 3}), (6, 4, {'weight': 70}), (7, 0, {'weight': 21}), (7, 3, {'weight': 59}), (7, 4, {'weight': 60})]. The task is: you need to determine the shortest path between two specific nodes in the graph. The nodes in question are (1, 5).",
  "Graph Task": "\"parameters\": { \"type\": \"object\", \"properties\": { \"G\": { \"type\": \"string\", \"description\": \"The graph G is the graph object specified in the problem, where the graph object is a list. For example: [(0, 1, {'weight': 1}), (0, 5, {'weight': 9}), (0, 7, {'weight': 10})].\" }, \"path_source\": { \"type\": \"integer\", \"description\": \"The source node of the path.\", }, \"path_target\": { \"type\": \"integer\", \"description\": \"The target node of the path.\", }, }, \"required\": [\"G\",\"path_source\",\"path_target\"], },",
  "Errors of GLM-0520-FC": ": Errors of GLM-0520-FC. For each LLM based on Function Calling, we provide the Tool List following their specifiedformat. Additionally, to ensure the accuracy of graph information extraction, we set different prompts for graph extraction. ThisFigure shows both the Name Mismatch and Syntax Error happeneded in one case. Notably, we have found that GLM-0520-FClacks the capability to process weighted graphs, as each result returned contains only a single edge. Consequently, we havemade some modifications, as shown in , specifically for GLM-0520-FC.",
  "GLM-0520-FC Output": "\"message\": { \"role\": \"assistant\", \"content\": \"To ascertain the shortest path length between two nodes in a directed graph with weighted edges, we should use the `shortest_path` API. This API will compute the shortest path between the specified source and target nodes.\\n\\nHere is the response:\\n\\nTo find the shortest path length between nodes 5 and 15 in the given graph, we will use the `shortest_path` API.\", \"function_call\": null, \"tool_calls\": [] }",
  "GPT-3.5-turbo-FC Output": "\"tool_calls\": [ { \"id\": \"call_xxxx\", \"function\": { \"name\": \"shortest_path\", \"arguments\": \"{\"G\":\"[(0, 1, {'weight': 40}), (0, 3, {'weight': 4}), , (6, 0, {'weight': 50}), (6,... 6, 7, 69), (6, 9, 61), (6, 10, 22), (6, 11, 35), (7, 1, 80), (7, 2, 22), (7, 3, 71), (7, 4, 62), (7, 5, 73), (7, 6, 12), (7, 9, 2), (7, 11, 6), (8, 0, 42), (8, 2, 81), (8, 3, 59), (8, 4, 6), (8, 5, 7), (8, 12, 54), (9, 3, 17), (9, 4, 96), (9, 6, 4), (9, 8, 33), (9, 11, 61), (10, 4, 20), (10, 5, 52), (10, 6, 25), (10, 7, 92), (10, 9, 47), (10, 11, 57), (10, 13, 61), (11, 0, 11), (11, 1, 26), (11, 4, 48), (11, 5, 16), (11, 6, 89), (11, 12, 75), (11, 13, 72), (12, 0, 24), (12, 6, 61), (12, 9, 26), (12, 10, 88)...\"}\" } } ]Syntax Error",
  "Errors of GraphForge": ": Errors of GraphForge. We have found that Graph Mismatch may occur when the input content is very long.Additionally, there is a small probability that GraphForge will have a tool misidentification, leading to a Name Mismatch.Before we implement the Parameter-Instruction, the model frequently exhibits incorrect order of parameters. This issue hasalmost disappeared after applying the Parameter-Instruction."
}