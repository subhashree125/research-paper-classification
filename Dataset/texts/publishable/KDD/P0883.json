{
  "Communication Systems Department, EURECOM, FranceTechnology Innovation Institute, Abu Dhabi, UAEDaSCI, Department of Computer Science and Artificial Intelligence, University of Granada, Spain": "AbstractThis work introduces a novel method for enhancingconfidence in anomaly detection in Intrusion Detection Systems(IDS) through the use of a Variational Autoencoder (VAE)architecture. By developing a confidence metric derived fromlatent space representations, we aim to improve the reliability ofIDS predictions against cyberattacks. Applied to the NSL-KDDdataset, our approach focuses on binary classification tasks toeffectively distinguish between normal and malicious networkactivities. The methodology demonstrates a significant enhance-ment in anomaly detection, evidenced by a notable correlation of0.45 between the reconstruction error and the proposed metric.Our findings highlight the potential of employing VAEs for moreaccurate and trustworthy anomaly detection in network security.",
  "I. INTRODUCTION": "In the era of digital advancement, the Internets rapidexpansion has been paralleled by a significant increase insophisticated cyberattacks and as a result, network securityhas become an important domain . These attacks not onlythreaten individual privacy and security but also challenge theintegrity of critical infrastructure . Against this backdrop,Intrusion Detection Systems (IDS) have emerged as an es-sential tool in the cybersecurity arsenal, designed to detectand mitigate malicious activities in network traffic . AnIDS identifies unauthorized or harmful attacks that frequentlyoccur in a network . To address vulnerable attacks, manytools and mechanisms have been developed over the years,with a significant focus on learning-aided algorithms .More recently, machine learning (ML) has taken centerstage, to make a categorization of different types of networkattacks , employing techniques ranging from supervisedlearning, where models are trained on labeled datasets torecognize specific types of attacks, to unsupervised learning, which detects anomalies without prior knowledge of attacksignatures. Another promising area involves semi-supervisedlearning, which combines elements of both to efficientlyhandle data with sparse labels . Despite the advantages thesemethods offer, each comes with limitations, ranging from highfalse positive rates in behavior-based systems to challenges ofkeeping supervised models up-to-date with new attack vectors.In this work, we extend our innovative approach to thefield of network security and intrusion detection, by lever-aging the capabilities of Variational Autoencoders (VAEs). VAEs, which are known for their proficiency in generatingnew data instances and encoding data into a compact latentspace , offer a unique advantage in identifying intricatepatterns indicative of cyberattacks . By employing VAEs,our methodology not only aims to detect anomalies but alsoto assess the reliability of unknown samples before theirevaluation. This is achieved through the development of aconfidence metric that provides insights into the expectedaccuracy of anomaly classifications, thereby addressing acritical challenge in intrusion detection research. Using theVAEs architecture, our confidence metric, based on the latentspace representations, and with the core methodology, basedon generating meaningful latent spaces, proved effective inenhancing the trustworthiness of our predictions.For instance, consider a scenario where an IDS detectspotential anomalies in network traffic. Traditional methodsmay flag these anomalies, but without a measure of confidence,the system might generate numerous false positives, leading tounnecessary alerts and wasted resources. By incorporating ourconfidence metric, derived from the Mahalanobis distance inthe latent space, we can effectively gauge the trustworthinessof each detection. Suppose an unknown sample is consideredhighly trustworthy based on its proximity to known trainingsamples in the latent space, we can reduce false positivesand increase the efficiency of our IDS without any additionalmanual tuning or complex adjustments. This straightforwardenhancement demonstrates the practical value of our approachin real-world applications.In the current study, we adapt and refine our algorithm to ad-dress the challenges of anomaly detection in IDS. By applyingour method to the NSL-KDD dataset , a benchmark datasetin network security research, we demonstrate its versatilityand effectiveness across diverse data domains. This cross-disciplinary application underscores the robustness of ourVAE-based technique in generating reliable confidence metricsfor predictions, which is a critical aspect in both environmentalstudies and cybersecurity. Through this continuity of research,we not only validate the universal applicability of our methodbut also contribute to the advancement of ML applicationsin ensuring data integrity and security in networked environ-ments. Focusing on binary classification, our work simplifiesthe complex landscape of network threats into a dichotomy ofnormal and malicious activities. This simplification allows fora more streamlined and focused approach to identifying and",
  "II. SYSTEM MODEL": "We leverage the architecture of VAEs and their proficiencyin generating novel data instances. A VAE is a Directed Prob-abilistic Graphical Model (DPGM) that has a posterior thatis approximated by a neural network having an autoencoder-like architecture . The encoder Enc() operates on theinput data X, which encompass features such as security,temporal, and protocol among others, to encode them intoa new representation projected within the latent space, i.e.,Enc(X) = Z. Subsequently, the decoder Dec() utilizes thislatent space representation Z as input to generate new dataX, i.e., Dec(Z) = X. These generated data instances areaccompanied by a reconstruction error Re.Mathematically, the VAE consists of an encoder networkq(Z|X) and a decoder network p(X|Z), where:",
  "The objective of training a VAE is to maximize the EvidenceLower Bound (ELBO), which is given by": "L(, ; x) = Eq(Z|X)[log p(X|Z)] KL(q(Z|x)p(Z))(2)where KL() denotes the Kullback-Leibler (KL) divergencebetween the approximate posterior q(Z|X) and the priorp(Z).The first term of the ELBO is the reconstruction loss, whichensures that the decoder can effectively reconstruct the inputdata from the latent representation. The second term is the KLdivergence loss, which regularizes the latent space to follow aprior distribution, typically a standard normal distribution.",
  "Ltotal = LRe + LKL(4)": "where is a hyperparameter that controls the trade-off be-tween the reconstruction loss LRe and the KL divergenceloss LKL. So it captures the trade-off between accuracy andgeneralization. By optimizing this objective function Ltotal, weensure that the VAE can effectively reconstruct normal dataand provide a reliable measure for detecting anomalies basedon the reconstruction error. B. Anomaly DetectionIn this work, we focus on a reconstruction-based anomalydetection method. Reconstruction-based methods try to iden-tify anomalies by comparing the original input data to the datareconstructed by the model. The underlying assumption is thatanomalies are not well-represented by the latent space learnedfrom the normal data, and thus will have higher reconstructionerrors.Given an input sample x, the VAE projects it to the latentspace to obtain z, and then reconstructs it back to x. Thereconstruction error Re is computed as",
  "1if Re > T0otherwise.(6)": "III. PROPOSED METHODOLOGYIn this study, our objective is to introduce a methodologythat assesses the reliability of an unknown sample for anomalydetection before subjecting it to evaluation. Our goal is toderive a confidence metric, C, which is informative regardingeach unknown instances Xun expected error e and thereforethe final classification as an anomaly or not, given the trainset Xtrain and the predictor P(). The confidence metric canbe expressed mathematically as",
  "e = |ytrue Re|.(8)": "This means that when the instance is labeled as an anomaly,we strive for a reconstruction error that is as high as possible,and when the instance is labeled as normal, we want thereconstruction error to be as low as possible. Our approachtreats this error as a continuous variable. Input Data X Reconstructed Data X? Calculation of Confidence Metric C Latent Space Z TextEncoder",
  "(x )T 1(x ).(9)": "Here, x is the latent representation of an observation, isthe mean vector of the training latent representations Ztrain,and is the covariance matrix of Ztrain.The Mahalanobis distance accounts for the distribution ofthe data by incorporating the covariance structure of thelatent representations. This metric effectively normalizes thedistances based on the variance along each dimension, makingit sensitive to the correlation between features. Consequently,it scales the distance measurements appropriately according tothe actual spread of the data.In contrast, the Euclidean distance treats all dimensionsequally, ignoring the potential variability and correlations thatexist in the data, which can lead to misleading results in high-dimensional spaces where features may have different scalesand variances.By using the Mahalanobis distance, we improve our abilityto discern how anomalous a new observation is relative to thetraining data, leading to a more robust and reliable confidencemetric for anomaly detection. This method allows us to assessthe reliability of each unknown sample before subjecting it toevaluation, ultimately improving the overall trustworthiness ofour IDS.",
  "C. Confidence Metric Calculation": "To calculate the confidence metric C using the Mahalanobisdistance, we incorporate the covariance structure of the la-tent space. First, we train the VAE model to obtain latentrepresentations of the training data, Ztrain, and the unknownobservations, Zun. Once trained, we use the model as an evaluator for the inference data we have. During evaluation,the model generates new instances based on the features of theinference data. We consider that the unknown observations thatare closer to representations of training points Ztrain are moretrustworthy.The distance C for the j-th unknown observation can bewritten as:",
  "A. The NSL-KDD Dataset": "Among the datasets employed to benchmark IDS perfor-mance, the NSL-KDD dataset has gained prominence.As an improved and standardized version of the KDD Cup99 dataset , it addresses several of its predecessorsshortcomings, offering a more reliable foundation for researchin intrusion detection and network security. The NSL-KDDdatasets widespread acceptance underscores its relevance andutility in fostering advancements within this domain.Although the NSL-KDD dataset traditionally encompassesmulti-class labels, in our experiments we focus on usingit as a binary classification task. Specifically, from the 41features that the dataset includes, regarding the attack, weconsolidate all instances of network intrusions into a singleclass labeled as 1, while normal network traffic is labeled as 0. This decision allows us to simplify the classificationtask and focus our efforts on detecting malicious activitieseffectively. Analyzing the dataset, we observe that the trainingset comprises 46.54% instances of network intrusions and53.46% instances of normal traffic. Similarly, the testing setcontains 56.92% instances of network intrusions and 43.08%instances of normal traffic, showcasing the prevalence ofmalicious activities in both training and testing data.We pay close attention to the NSL-KDD datasets details,especially how it organizes network information into differentfeatures. These features include basic details about networkconnections and more advanced information that helps usunderstand the flow of network traffic better. By looking atthese features, we can tell apart normal network activities fromthose that might be harmful.When we use the NSL-KDD dataset to separate normalactivities from possible threats, it is not just about the featuresthemselves but also about how these features interact witheach other. This interaction helps us spot suspicious patterns.Before even starting our experiments, we prepare the datasetcarefully, i.e., we make sure that all data are on a similar scaleso that our analysis is accurate.",
  "B. VAE Architecture and Training Curriculum": "The VAE architecture utilized in our study consists of twomain components: an encoder and a decoder, each structuredwith five fully connected layers. The encoder is responsiblefor transforming the input data into a lower-dimensional latentspace representation. It begins with an input layer that matchesthe dimensionality of the input data, followed by a sequence oflinear layers with respective dimensions of 512, 384, 256, and128 neurons, each activated by Rectified Linear Unit (ReLU)functions. The final layer in the encoder outputs two sets ofvalues: one for the mean and one for the log variance of thelatent space distribution, each with a dimension equal to twicethe latent dimension size to accommodate the mean and logvariance parameters.The decoder, on the other hand, reconstructs the originalinput data from the latent space representation. It starts witha linear layer that takes the latent dimension as input andexpands it to 128 neurons, followed by layers of 256, 384, and512 neurons, each activated by rectified linear unit (ReLU)functions. The final layer in the decoder maps the 512-dimensional representation back to the original input dimen-sion.Given the unsupervised nature of our task, the data consistsonly of features without explicit labels. During the trainingphase, we iterate over 30 epochs, using an initial learningrate of 0.001, dynamically adjusted by a StepLR scheduler.The optimization process employs the Adam Optimizer. TheVAEs loss function combines the Mean Squared Error (MSE)loss, which quantifies the difference between the reconstructedoutput and the original input, with the KL divergence loss. illustrates the latent space projections of bothtrain and test data, once the predictor is trained with theavailable Xtrain. The color coding in the figure helps depict",
  "C. Adjusting Model Parameters": "Two main parameters affect the performance of our confi-dence metric: (i) the dimension of the latent space, and (ii)the parameter that controls the trade-off between accuracyand generalization. The selection of those optimal parameterscan be seen as a multivariate problem and can be approachedwith alternating optimization, by optimizing iteratively oneparameter at a time.1) Latent Space Dimension: In determining the appropriatedimension for the latent space, as we can observe in TableI, we carefully consider the trade-off between computationalefficiency and the efficacy of our models performance metrics.Following a series of experiments, we conclude that settingthe latent space dimension to 20 and the default = 1 yieldsnear-optimal results.",
  "%12%52%": "The results of our correlation analysis underscore the signif-icant potential of employing the Mahalanobis distance in thelatent space for improving the reliability of anomaly detectionin IDSs. The general correlation of r = 42% between the latentspace distance and the proposed prediction error highlights theeffectiveness of the latent space representation in capturingsubstantial variability within the data. This indicates that theVAE model is proficient at distinguishing between normaland anomalous instances. The high correlation associatedwith false positives suggests that the latent space excels incapturing features of normal network traffic, which reducesthe number of benign instances misclassified as attacks. Thisis further evidenced by the models high precision score of0.90, demonstrating a strong ability to correctly identify truepositive instances of network intrusions. 2) KL weight : In our experiments, we adjust the KLdivergence weight, denoted as in equation (4), to observeits impact on the performance of the VAE. The KL weightcontrols the balance between the reconstruction loss and theregularization term in the VAEs loss function. By varying ,we aim to find the optimal trade-off that enhances the modelsability to detect anomalies.",
  ": Weights for the KL loss and correlation": "As we can observe in , the optimal value of is0.25. The results indicate that this value yields the highestcorrelation for our metric of 45%. As goes higher we cansee that the correlation drops significantly, indicating that toomuch regularization can hinder the models effectiveness incapturing meaningful patterns in the latent space for anomalydetection.This choice strikes as well a balance between computationalcomplexity and the ability of the model to effectively capturethe underlying structure of the data, resulting in favorableoutcomes across our proposed evaluation metric.",
  "D. Optimal Threshold and Anomaly Predictions": "To determine the optimal threshold for the reconstructionerror and subsequent classification, we conduct an analysisaiming to maximize the F1 score on the training data by havingthe dimension of the latent space equal to 20 and = 0.25.Through this process, we identify an optimal threshold T valueof approximately T = 0.07. We can observe the evaluationmetrics below.",
  "%7%15%19%45%": "Negatives, True Positives, and True Negatives) for the optimallatent space dimension of 20. The correlation values indicatehow well the confidence metric, derived from the latent spacedistance, aligns with the actual classification errors in theanomaly detection task. The correlation for False Positives (FP) is notably highat 57%, suggesting that the confidence metric is particularlyeffective in identifying normal instances that are incorrectlyclassified as anomalies. This high correlation indicates that themetric can help reduce false positives by providing a reliablemeasure of how similar an instance is to the training data. In contrast, the correlation for False Negatives (FN) isrelatively low at 7%, implying that the confidence metricis less effective in detecting anomalous instances that areincorrectly classified as normal. This indicates a potential areafor improvement in the models ability to correctly identifytrue anomalies. The correlations for True Positives (TP) and True Negatives(TN) are 15% and 19%, respectively, showing a moderaterelationship between the confidence metric and these correctclassifications. The general correlation across all error typesis 45%, reflecting the overall effectiveness of the confidencemetric in capturing the variability in the data for anomalydetection. These results underscore the importance of the confidencemetric in improving detection accuracy, particularly in re-ducing false positives, and highlight the need for furtherrefinement to enhance the detection of true anomalies.",
  "E. Experiment with ChoquetMahalanobis distance": "In addition to our primary approach, we conduct experi-ments comparing the effectiveness of the Mahalanobis andthe Choquet Integral Operator . The Choquet integral is asophisticated aggregation operator that can capture interactionsbetween features in a more refined manner. This method isparticularly advantageous in scenarios where feature inter-actions are complex and not easily captured by traditionaldistance measures. For this experiment, we implement theChoquet integral to aggregate the distances between the latentrepresentations of the training and test sets.Our findings indicate that the ChoquetMahalanobis dis-tance can achieve a general correlation of 45% between theconfidence metric and the prediction error. However, it isimportant to note that the Choquet integral, while effective,proved to be significantly more computationally expensivecompared to the Mahalanobis distance. The increased compu-tational cost arises from the complexity involved in computingthe Choquet integral, which requires more intensive calcula-tions to capture the interactions between features.",
  "F. Evaluation of the Confidence Metric": "To evaluate the effectiveness of our confidence metric inassessing anomaly detection, we conduct a comprehensiveanalysis across multiple dimensions. In addition to assessingthe distance in the Latent Space (LS), we also calculateddistances in the Feature Space (FS).1) Comparison of different distances: In our analysis, weevaluate the effectiveness of three different distance metrics:Mahalanobis, Euclidean, and Cosine. The Mahalanobis dis-tance, which as stated before, takes into account the covari-ance structure of the latent space, demonstrates the highestcorrelation of 45% with the error metric. This suggests thatthe Mahalanobis distance is particularly effective in capturingthe intricate patterns in the latent space that are indicativeof anomalies. The Euclidean distance, a more straightforwardmetric, shows a correlation of 38%. While it is computa-tionally less complex, it does not account for the underlyingdata distribution, which may lead to less accurate anomalydetection compared to Mahalanobis distance. The Cosinedistance, which measures the cosine of the angle betweentwo vectors, exhibits a correlation of 32%. This metric isoften used for high-dimensional spaces and can be usefulwhen the magnitude of the vectors is less important than theirdirection. However, in our context, it proves to be less effectivethan both Mahalanobis and Euclidean distances. Overall, theMahalanobis distance provided the best performance in termsof correlation with the error metric, indicating its superiorityfor our specific use case in IDS anomaly detection.",
  "Correlation45%38%32%": "2) Computational Complexities of Mahalanobis Distancein LS vs. FS: When implementing Mahalanobis distance,the computational complexity for the LS and the FS canvary significantly. This disparity primarily arises due to thedimensional differences and the covariance matrix computa-tion. In latent space, where the dimensionality is typicallymuch lower due to the compressed representations learnedby the VAE, the computation of the covariance matrix andits inverse is computationally less demanding. Consequently,calculating Mahalanobis distances in this space tends to befaster, as evidenced by the observed execution time of ap-proximately 8 minutes. In contrast, the feature space oftenretains the high-dimensional nature of the original data. Thisincreases the computational burden associated with estimatingthe covariance matrix, regularizing it, and performing matrixinversion. Additionally, the high dimensionality amplifies thecost of pairwise distance calculations, leading to a noticeableincrease in processing time, with the feature space calcula-tions taking around 37 minutes. This significant differencehighlights the practical advantage of using latent space rep-resentations for distance calculations, which not only providemeaningful lower-dimensional embeddings but also enhancecomputational efficiency.",
  "V. DISCUSSION": "This research extends an innovative use of Variational Au-toencoders to the critical field of network security, demonstrat-ing the VAEs capability not only to distill complex networktraffic data into a meaningful latent space but also to enhancethe reliability of anomaly detection in Intrusion DetectionSystems. The introduction of a reliable confidence metricderived from latent space representations marks a significantleap forward in our methodology, offering a refined approachto evaluating the trustworthiness of IDS predictions.The notable correlation observed in the latent space under-lines the VAE models profound ability to capture and interpretthe complex patterns indicative of cyberthreats. This correla-tion is not merely a numerical assessment but a reflection ofthe intrinsic data structure, which significantly contributes tothe confidence in anomaly detection. The higher correlationvalues in the latent space, compared to those in the featurespace, underscore the latent spaces critical role in identifyingand distinguishing between normal and malicious networkactivities.Our findings demonstrate the latent spaces potential beyonddimensionality reduction, establishing it as a pivotal elementfor ensuring prediction reliability in cybersecurity applications.This methodology bridges the gap between raw networkdata and actionable insights, enhancing the IDS capability tosafeguard against sophisticated cyberattacks while fosteringtrust in ML-based security solutions.Comparing our approach to traditional methods of anomalydetection, which often rely on either predefined rules or su-pervised learning models, our VAE-based methodology offersa dynamic solution capable of adapting to new and evolvingthreats. Unlike many existing models that struggle with high false positive rates or require extensive labeled datasets, ourmethod effectively utilizes unsupervised learning to identifyintricate patterns indicative of cyberattacks, as supported bythe correlation metrics presented in our results.Future research should aim to further validate and refinethe proposed methodology across a broader range of datasetsand network environments. Investigating the scalability ofthis approach to handle multi-class classification tasks andexploring ways to reduce computational overhead are criticalnext steps. Additionally, integrating our confidence metric withreal-time monitoring systems could offer new insights into itspractical applicability and effectiveness in operational settings.",
  "O. Faker and E. Dogdu, Intrusion detection using big data and deeplearning techniques, 04 2019": "B. Zhang, Y. Yu, and J. Li,Network intrusion detection based onstacked sparse autoencoder and binary tree ensemble method, in 2018IEEE International Conference on Communications Workshops (ICCWorkshops), 2018, pp. 16. J. Lansky, S. Ali, M. Mohammadi, M. K. Majeed, S. H. T. Karim,S. Rashidi, M. Hosseinzadeh, and A. M. Rahmani,Deep learning-based intrusion detection systems: A systematic review, IEEE Access,vol. 9, pp. 101574101599, 2021. A. Binbusayyis, H. Alaskar, T. Vaiyapuri, and M. Dinesh, An inves-tigation and comparison of machine learning approaches for intrusiondetection in IoMT network, The Journal of Supercomputing, vol. 78,no. 15, pp. 1740317422, Oct 2022."
}