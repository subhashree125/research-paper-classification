{
  "ABSTRACT": "Graph clustering aims at discovering a natural grouping of thenodes such that similar nodes are assigned to a common cluster.Many different algorithms have been proposed in the literature: forsimple graphs, for graphs with attributes associated to nodes, andfor graphs where edges represent different types of relations amongnodes. However, complex data in many domains can be representedas both attributed and multi-relational networks.In this paper, we propose SpectralMix, a joint dimensionalityreduction technique for multi-relational graphs with categoricalnode attributes. SpectralMix integrates all information availablefrom the attributes, the different types of relations, and the graphstructure to enable a sound interpretation of the clustering results.Moreover, it generalizes existing techniques: it reduces to spectralembedding and clustering when only applied to a single graph andto homogeneity analysis when applied to categorical data.Experiments conducted on several real-world datasets enable usto detect dependencies between graph structure and categoricalattributes, moreover, they exhibit the superiority of SpectralMixover existing methods.",
  "INTRODUCTION": "Complex data in many domains including biology, neuroscience, so-cial and collaboration networks etc. can be represented as attributedmulti-relational graphs. The nodes are often characterized by many Permission to make digital or hard copies of part or all of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for third-party components of this work must be honored.For all other uses, contact the owner/author(s).KDD 21, August 1418,2021, Virtual Event, Singapore 2021 Copyright held by the owner/author(s).ACM ISBN 978-1-4503-8332-5/21/08.",
  ": Example of a simple graph (left) and an attributed multi-relationalgraph (right). Different types of lines between nodes represent different rela-tion types": "attributes and the links can be of different types, as depicted in Fig-ure 1. In social networks, for instance, nodes represent users whoare characterized by attributes like gender, age, hometown, hob-bies etc., and edges describe the different types of links establishedby interaction on different platforms like Facebook and Twitter.In the field of neuroscience, the data obtained by neuroimagingtechniques can be modelled using nodes for the anatomical brainregions, attributes for the region size and type of tissue, and edgesfor the structural and functional connectivity links, as measured bystructural and functional Magnetic Resonance Imaging (MRI andfMRI) .Despite the importance of attributed multi-relational graphsin modeling many real-world domains, they have received littleattention, in particular in the task of clustering.Clustering is one of the main research directions in data miningtasks. It aims at discovering a natural grouping of the data such thatsimilar data objects are assigned to a common cluster. Clusteringis very helpful in order to obtain an understanding of the majorpatterns in large datasets like attributed multi-relational networks.Existing approaches are mainly designed for simple graph clus-tering , for fuzzy clustering that identifiesoverlapping clusters in which an object belongs with a membershipprobability, for clustering of networks with numerical attributednodes .Attributes and multiple types of relationships between nodes ofa graph can yield complementary information. This informationis valuable for detecting clusters of nodes with similar character-istics and can be used in different applications, e.g., targeting ads.Thus, we develop SpectralMix, a novel algorithm that providesa comprehensive approach to joint dimensionality reduction andspectral clustering of attributed multi-relational graphs with cate-gorical node attributes. Given a multi-relational attributed graph and the desired dimensionality , SpectralMix defines a mapping : R that minimizes the distance between nodes with sim-ilar attributes. The joint low-dimensional vector space generatedby SpectralMix reduces noise, emphasizes the major patterns in",
  "arXiv:2311.01840v1 [cs.LG] 3 Nov 2023": "the data, and enables subsequent data mining. By application ofthe -means algorithm we obtain a clustering inspired by spectralclustering methods. But we could also perform outlier detection orvisualize the data when we select a 2D or 3D vector space. Moreover,other very important aspects of SpectralMix are that: there is onlya single dimensionality parameter that needs tuning, and Spec-tralMix generalises other existing techniques. When consideringthe setting of a single input graph without any node attributes asinput data, our mapping is the same as Laplacian Eigenmaps .When considering only categorical data without relational infor-mation, our mapping coincides with Homogeneity Analysis , atechnique from statistics for categorical PCA .To assess its performance, we evaluate SpectralMix based onclustering and data visualization tasks. The experiments conductedon several real-world networks enable us to detect dependencies be-tween graph structure and the categorical attributes, moreover, theyexhibit the superiority of SpectralMix over alternative baselines.",
  "RELATED WORKS": "Recently, the problem of clustering graphs with node attributes hasreceived a lot of attention. The main goal in this field has been toimprove the quality of the clustering by combining both structuraland attribute properties.While it may seem a natural extension, many methods cannot be directly applied to attributed multi-relational graphs.They propose algorithms for detecting densely connected compo-nents , or for identifying clusters of objects havingsimilar connectivity but they either ignore the multipletypes of edges and/or deal with only one attributetype . A technique, that has been used frequently to overcomethese problems, is to project the attributed multi-relational graphto a single weighted graph where weights represent a combinationof attribute and structural similarities, and then apply a clusteringalgorithm for weighted graphs with the aim of performing cluster-ing and graph outlier detection or identify strong communitystructures . This projection, however, causes information lossand, as a consequence, it limits the clustering accuracy. Similarly,graph embeddings techniques have been designed mainly for singlegraphs such as FATNet which integrates the global relationsof the topology and attributes into robust representations.Different works have presented results on spectral clustering onmulti-layer networks , while very few, to the best of ourknowledge, investigate spectral clustering methods on attributedgraphs . Whereas, there are methods not designed for graphs,which exploit the fusion of the cluster-separation information fromall eigenvectors to achieve a better clustering or perform clus-tering of numerical and categorical data based on homogeneityanalysis .Recently, many works in the area of graph embedding and clus-tering have been published and they can be categorized in the threegroups: a) methods for single graphs with attributes, ANRL andDGI ; b) methods for multi-relational graphs without attributes,DMGC and CrossMNA ; and c) methods for attributed multi-relational graphs, HAN , MARINE , DMGI . ANRL is a deep neural network model designed for at-tributed networks representation learning which integrates net-work structural proximity and node attributes affinity into low-dimensional representation spaces. DGI is an approach forlearning unsupervised representations on graph-structured dataand it relies on maximizing mutual information between patchrepresentations and corresponding high-level summaries of graphsderived using established graph convolutional network architec-tures. DMGC is a deep learning based multi-relational graphclustering method which aims at performing two procedures: graphclustering and cross-graph cluster association. CrossMNA is anembedding method that integrates cross-network information torefine two embedding vectors for alignment tasks: inter-vector andintra-vector. MARINE is an unsupervised embedding methodfor both simple and multi-relational networks that preserves theproximity and attributes information in addition to various typesof relations. Furthermore, HAN is a semi-supervised graphneural network framework that can be applied to graphs with vari-ous types of nodes and edges. It employs node-level attention andsemantic-level attention simultaneously. DMGI is an unsuper-vised network embedding method for attributed multi-relationalgraphs. It is a modified approach of DGI, and it jointly integratesthe embeddings from multiple types of relations between nodesthrough a consensus regularization framework, and a universaldiscriminator.To conclude, the previous methods differ from ours in the fol-lowing ways: (i) they do not consider the existence of multiple edgetypes, (ii) they deal with only one type of attributes, (iii) their finalnode embedding is represented in higher dimensions, 100 or 200dimensions, (iv) they can perform well only on one graph category:multigraphs with attributes or multigraphs without attributes.SpectralMix outperforms the above methods because it exhibitsgood results when applied to graphs with one or many differentedge types, to graphs with or without node attributes, and a combi-nation of those. Moreover, the output from our algorithm can berepresented in 2D or 3D space and has more meaningful clusterstructure.",
  "NOTATION AND PROBLEM DEFINITION": "We consider an undirected node-attributed multi-relational net-work, (also referred to as multigraph), = (, , R) where ={1, . . . , } represents the set of nodes. Each node is character-ized by a set A = {1, . . . ,} of categorical attributes, i.e., |A| =, and is the dimensionality of the attribute space. We denotecategorical attributes by upper case letters. Let for = 1, . . . ,be a categorical attribute having distinct values, with = 2for binary attributes. For each categorical attribute, we allow for = 1, . . . , associated to it, to have a different value. More-over, . denotes the value or category of vertex in attribute. Thus, the total number of categories, denoted by , of all categorical attributes is = =1 .We denote by R the set of different relation types and, therefore,|R| denotes the dimensionality of the network. The set of edges consists of the tuples = (, ,,) where represents theweight of the edge of type , and R is the type of relation. Wedenote by set of edges of type , for = 1, . . . , |R|. Our goal is to define a mapping : R, where is thedesired dimentionality, such that the distance between connectednodes with similar attributes is minimal. We define by O and M thematrices of size and , respectively, where each entry and for = 1, . . . ,, = 1, . . . , and = 1, . . . ,, is a real valuedenoting the coordinate of the node or the categorical attributeaccording to the embedding.Our algorithm, sketched in Algorithm 1, takes as input a multi-relational graph , a categorical attribute matrix of size , andthe desired dimensionality of the embedding. It returns as outputa -dimensional feature space representation of data objects O andcategories M, and, if desired, also a clustering of the data objects.Hence, O and M are two matrices of size and, respectively.Our framework can also be applied to cluster multi-layer graphswhere there are inter-layer edges between any pair of nodes. Givena multi-layer graph, we build a multi-relational graph with the sameset of nodes and the same attributes associated to them. For the setof edges, we consider as many relations as there are layers, we alsoadd relations representing the edges from nodes in one layer to theother layers, if those edges exist.",
  "EMBEDDING MULTIDIMENSIONALATTRIBUTED GRAPHS": "Our problem setting is characterized by mixed-type data of dif-ferent modalities. In particular, we have a set of nodes which arecharacterized by multiple categorical attributes and linked by dif-ferent relations. We integrate all the different modalities by a jointvector space representation. Our objective function for this em-bedding combines the ideas of spectral embedding of graphs withhomogeneity analysis of categorical data. For nodes we derivelow-dimensional coordinates minimizing the following objectivefunction:",
  "subject to O O = .(1)": "As mentioned in the previous section, O is a matrix oflow-dimensional coordinates. Every row represents the coordinatevector for one data object (node). The first summand in Equation 1represents the contribution of the relational part of the data, i.e.,of the |R|-dimensional graph on the position of a data object. Forevery graph we consider all edges between and vertices anddetermine the coordinate matrix O such that the squared Euclideandistances are minimized. We denote by the coordinate of thenode in the dimension . As in spectral embedding techniqueswe place the vertices as close as possible to their neighbors, butnow not considering a single graph but multiple different relations.The second summand of the equation represents the categoricalinformation. Categorical data establishes a bipartite graph betweendata objects and their categories. Every categorical variable estab-lishes its own bipartite graph, where object is connected to the",
  ": Example of an attributed multi-relational graph (left) and a bipar-tite graph for its categories for attribute Hometown": "category or value of its attribute .. An example of a bipartitegraph is shown in .We now also consider the categorical graphs in determining thejoint vector space embedding. Inspired by Homogeneity Analysis,our algorithm SpectralMix will discover for every category ofevery categorical attribute a position in R. In Equation 1 wedenote the position of the category of object in dimension by. The matrix M contains these category coordinates asrow vectors.The coefficients , represent weighting factors for the rela-tions types and the categorical attributes in the input data. It ispossible to consider the unweighted objective function setting allto 1. However, the total number of edges in the multi-relationalgraph and the edges in the graphs generated by the attributes tendto be very different as the categorical data establishes bipartitegraphs where every node is connected to only and exactly onecategory. As default weighting scheme we, therefore, suggest to setthe weight factors such that every modality has the same weightin the low-dimensional representation.In order to avoid trivial solutions, we require the matrix O to becolumn-orthonormal. Without this constraint, a minimum of theobjective function would be achieved by mapping all vertices toone common location.Our algorithm, reported in Algorithm 1, starts by initializingevery element for = 1, . . . , and = 1, . . . ,, in O with a realnumber selected at random. In this matrix, every row representsthe data object associated to a vertex of the graph , and everycolumn is one of the coordinates in the embedding space.Then, we compute the value of our objective function as de-scribed in Equation 1, and at each iteration we update the coor-dinates in O and M in order to obtain a smaller value for ourobjective. If this value does not decrease after the updates, then thealgorithm converges and returns the final node embeddings for thedata objects (nodes) and the attribute categories.In particular, when we update the object coordinates we takeinto account the type of relations in the graph (lines 4 7) and theattributes (lines 8 10). For the graph contribution, we consider,for all the different relation types, the neighbors of node and the weight of these edges. We update each coordinate addingto it the product of the weighting factor , the weight ofthe edge (, ,,), the coordinate of the data object for node, and divide by () computed as the sum of the size for theneighborhood of node in relation . For the attribute contribution,we update each coordinate adding to it the product of the weighting",
  "21return O, M;": "factor for the bipartite graph generated by the category, thecoordinate of the category for attribute , and divide by ()computed as the sum of the weighting factor for category . Thenwe apply the orthogonalization algorithm.When we update the coordinates of the categories, we add toeach of them the coordinate of node and dived by the totalnumber of elements that belong to that category. We highlight thatour algorithm has only one parameter , desired dimensionality, tobe tuned by human, while all the other parameters (weights) canbe learned during optimization.We provide now a proof of convergence of our algorithm.",
  "Theorem 4.1. The SpectralMix algorithm converges in a finitenumber of iterations": "Proof. We say that our algorithm converges if the differencein the value of the objective function F between two consecutiveiterations, and +1, of our algorithm is bounded by a small positiveconstant : F+1 F < . The idea is to show that the two stepsof the algorithm, i.e., the update of the object coordinates and theupdate of the attributes coordinates, decrease the objective functionand that the difference F+1 F tends to zero. Suppose that thealgorithm proceeds from iteration to iteration + 1, it suffices toshow that :",
  "|| ||2 || ||2+1 and || ||2 || ||2+1": "In lines 4 7 of Algorithm 1, the coordinates of a node are up-dated by taking into account the position of all its neighbors in thegraph. Therefore, every node moves towards nodes that are closerin the space, reducing the distance || ||2. This observationalso holds for the attributes as they are represented as nodes inbipartite graphs, in lines 12 15 the coordinates are updated sothat these attributes moves towards the nodes to which they areassociated with, therefore the distance || ||2 decreases. Thisinformation is also considered while updating the coordinates of the objects in lines 8 10. As the algorithm updates the coordinatesof the nodes, the distance between two nodes and representedby the coordinates and , decreases but their distance to otherneighbors might increase. This does not affect the trend of the ob-jective function. We can see that by distinguishing two cases: (a)Edge = (, ,,), for a given relation , is the edge with high-est weight for both and . In this case, when the distancebetween and decreases, due to the update in line 7, it producesa decrease in the value of F . This difference compensates the dis-tances that may increase from and to any of their neighbors such that > and > . (b) Edge = (, ,,),for a given relation , is the edge with highest weight for butnot for that is connected to by an edge of weight > .In this case, the distance between and might not decrease,because as moves towards , the latter node moves towards .Even in this case, the value of the objective function will decreasebecause the factor is higher than the other values used whenupdating the coordinates.This argument holds for any dimension , any relation in R, andfor the categorical attributes represented as nodes in a bipartitegraph. Hence, the value of the objective function will decreaseat each iteration and when the decrement is smaller than , thealgorithm terminates. Complexity analysis. Lines 4 7 in Algorithm 1 compute thecontribution provided by relational information, its time complexityis (|| ), where is the number of edges. Lines 8 10compute the contribution given by the categorical data, whosetime complexity is ( ). These operations are repeated for eachnode in the graph, thus, the time complexity of lines 4 10 forupdating the object coordinates is ((|| + )). In line11 we apply the Gram-Schmidt orthonormalization algorithm withcomplexity ( 2). In lines 12 15, the algorithm updates thecoordinates of the categorical data objects, it requires ( ).Finally, in lines 17 19 we use K-Means algorithm with complexity( 2 ) setting to 1. We denote by the number ofiterations of K-Means. The total time complexity of Algorithm 1is ( ( || + 2 + ) + 2 ) where is thenumber of iterations the algorithm needs for converging.",
  "EXPERIMENTS": "In this section, we evaluate the performance of SpectralMix againsta variety of state-of-the-art methods. Our focus is on node embed-ding and clustering tasks. Due to space limitations, we only canshow a selection of the results.Datasets. To compare SpectralMix with other baselines we usefour datasets, which are multigraphs with node attributes, andtwo datasets, which are multigraphs without node attributes. Tomake fair comparisons with HAN , and DMGI , which arethe most relevant baseline methods for attributed multigraphs, weevaluate our proposed algorithm on the same datasets used in and in , i.e., ACM, and IMDB. Similarly, we use the datasets,DBLP and Flickr, that are used for evaluating the performance ofDMGC , which is the most relevant method for multi-relationalgraphs without node attributes.ACM is a multigraph with 2 types of relations where nodesrepresent papers and edges are inferred via Authors collaborations (PAP) and Subjects of the paper (PSP). The task is to cluster thedata into three clusters (Database, Wireless Communication, andData Mining).IMDB is a multigraph with 2 types of relations where nodesrepresent movies and edges are inferred via movie-actors (MAM)and movie-directors (MDM) relations. The task is to cluster the datainto three clusters (Action, Comedy, Drama) according to moviegenre.DBLP is a multigraph of 4 relations, that can be consideredas 4 graphs: a) collaboration graph consists of edges betweenauthors who have published a paper together, b) citation graph consists of edges between authors, an edge represents a citationbetween authors, c) co-citation graph - consists of edges betweenpapers, an edge represents that papers have cited the same source,and d) authorship graph consists of edges of authors and papers,an edge represents an author who has published a paper. The taskis to cluster the author and paper nodes into three cluster (AI,computer graphics, and computer networks) according to researchareas.Flickr is a dataset that does not have features and consistsof two relations: a) friendship represents an edge between userswho are friends on social network, b) tag-similarity representsan edge between users who have tag similarities among them. Thetask for this dataset is to cluster the user nodes into seven cluster,according to social groups.Brain Networks (BN) can be defined as weighted graphs, wherenodes are the brain regions, edges are connections between regions,and weights denote distances between regions. In our experiments,we use a publicly available dataset project that contains dataof 52 Typically Developed (TD) children and 49 children sufferingfrom Autism Spectrum Disorder (ASD). After preprocessing ,we obtain, for each subject, a graph with 116 nodes. We also use asecond detaset that provides structural and functional brainnetworks from 27 schizophrenic (SCZ) patients and 27 matchedhealthy adults (HL). The brain was divided into 83 approximatelyequally sized regions. For each node we are given the followingattributes: average apparent diffusion coefficient, fiber density, gen-eralized fractional anisotropy, length of fibers and number of fibers.We perform two types of experiments. For the first dataset, in orderto obtain a multi-relational graph, we aggregate the graphs of eachsubject in the groups ASD and TD. These multi-relational graphsare defined over the same set of nodes as the original graphs, whilethe weight and the type of relation of each edge are those of eachsubject in the original groups. For the second dataset, we select atrandom a subject in each group, i.e., SCZ and HL, and we build amulti-relational graph where we have two edge types: the func-tional and structural connectivity measured with MRI and fMRI.We describe this results in details in .Baseline methods. We compare SpectralMix with the state-of-the-art methods described in , i.e., ANRL , DGI ,DMGC , CrossMNA , HAN , MARINE , and DMGI .We report their main characteristics in .For fair comparisons with methods not designed for multigraphs,ANRL and DGI, we obtained the final node embedding matrix Zof a multi-relational graph by computing the average of the nodeembeddings obtained from each single graph, similarly as in .For the other two methods in the second category that ignore the",
  "SpectralMix": "node attributes, i.e., DMGC and CrossMNA, we concatenated theraw attribute matrix X to the learned node embeddings Z i.e., Z [Z;X], similarly as in .We underline that SpectralMix can operate on multi-relationalgraphs with or without attributes and it obtains a joint embeddingfor all the relation types of the graphs. SpectralMix is applicableto multi-relational graphs but it is not limited to them because thelearned node embeddings that we obtain for single graphs are thesame as the final embeddings of Laplacian Eigenmaps.Evaluation Metrics. SpectralMix is an unsupervised methodand does not need any labeled data for training. We focus on thetask of node clustering, which is a classical task for unsupervisedmethods. We compute the most commonly used metrics to assessthe quality of the clustering results : Normalized Mutual In-formation (NMI) , and Adjusted Rand Index (ARI) , where avalue equal to 1 indicates a perfect clustering.Experimental setup. The final node embeddings obtained fromSpectralMix are suitable for spectral clustering, i.e. subsequent ap-plication of K-Means and for visualization. Therefore, we apply theK-Means on the final node embeddings to perform node clusteringby setting K to the number of clusters of the dataset, and we useK-Means++ for initialization. Since the performance of K-Means isaffected by initial centroids, we repeat the process for 100 times andreport the average results. In the comparison with the alternativebaselines, we keep the same setup and parameters that are madepublicly available by the authors.Performance evaluation. The experimental results of six pub-lic datasets are shown in , the best results are marked in bold.It is easy to notice that SpectralMix outperforms all the alternativemethods. Moreover, the performance of SpectralMix improves withthe volume of data. As we can see from the results, we obtain thehighest NMI and ARI values for the ACM dataset which is the graphwith the largest number of edges and has 1, 870 attributes. Thus,SpectralMix achieves state-of-the-art performance on ACM dataset,with 5% performance lift in NMI, and 12% in ARI, compared to theresults from the competitors.SpectralMix outperforms other baselines on other real-worlddatasets too. For the IMDB dataset the improvement of SpectralMixin terms of NMI is 11% and ARI is 13%, and for multigraphs withoutattributes, DBLP and Flickr, the percentages of improvement interms of NMI are 21% and 94%, respectively, compared to the bestperformer from competitors for each dataset.Considering the results in , we observe that the flaws ofall baseline methods is that they perform well only for one category",
  "SpectralMix0.710.770.210.170.470.340.640.510.70.620.60.51": "of graphs. The strength of SpectralMix is that it is applicable todifferent graph types: multi-relational graphs with and without at-tributes. Therefore, it outperforms the most recent and well knownmethods in both categories, respectively. Our advantage lies in theimportance of considering separately the weights that each graphstructure and each category of the node attributes have in our em-bedding. Also, SpectralMix performs well when applied to singlegraphs.In particular, we observe that the DMGI performance dropswhen it is applied to multi-relational graphs without attributes, andthis is due to the crucial role that node attributes play for DMGI.Similarly, the performance of all baselines which are designed forattributed graphs, such as ANRL , HAN , and MARINE ,is poor on multi-relational graphs without attributes. Albeit, MA-RINE integrates the node attributes, we observe that very differentnode categories can impact its performance, which is the case for itsperformance drop on the IMDB dataset. Whereas, MARINE perfor-mance improves when applied to multi-relational graphs withoutattributes, and it occurs as MARINE is more focused on preservinglink structures. As expected HAN performs well on attributedmulti-relational graphs and there is a drop in performance for theother two datasets, multi-relational graphs without attributes. Thelack of node properties has affected the performance drop, since thenode level mechanism of HAN uses information from both graphstructure and node attributes.Experimental results on brain networks, are presented for Spec-tralMix and only three competitors, DMGI, HAN, and DGI. Forother methods, experiments could not be conducted as those meth-ods are not able to process the data due to their limitations, suchas not being able to process decimal or negative values for edgeweights or node attributes. In case of brain networks, the gap in per-formance is even higher between SpectralMix and other methods,and one of the factors is that baseline methods have to train andvalidate their models in order to have a satisfiable model. Since forthe brain networks the number of nodes and edges is very limited,it has an impact on the performance of these baselines methods.Visualization. To further demonstrate the effectiveness of Spec-tralMix over comparison methods, we show in the visualiza-tion of all methods on ACM dataset. For the sake of fair comparisonwe apply visualization tool t-SNE on the node embeddingsobtained from all methods. We set the same parameter values forvisualization for all embeddings. The nodes are colored according to the research area: Database, Wireless Communication, and DataMining. From , we can observe that ANRL, CrossMNA,DMGC, and MARINE do not perform well as the nodes belongingto different research areas are mixed with each other. Also, we cannote that SpectralMix, DMGI, HAN, and DGI are able to betterdistinguish different types of nodes. It is clear that SpectralMix isable to separate the nodes in different research areas with moredistinct boundaries and higher number of nodes clustered correctly.Additionally, it is noticeable on the SpectralMix node embedding inh that there is a distinct group of nodes, on the right side,which represent the noise data or outliers. These nodes are onlyconnected with each other in PAP relation type of ACM datasetand do not share the same properties or attributes with other nodes.Thus, we can see that SpectralMix is robust against outliers.It is worth mentioning that the outlier nodes represent papers inboth the types of relations in the graph of ACM, i.e., PAP and PSP.These papers have very few authors in common with other papers,i.e., they are connected be very few edges to other nodes in thePAP graph. Whereas, they are written by the same small group ofauthors, i.e., the edges between the outliers form a complete graph.We emphasize the importance of node attributes in the final nodeembeddings in . We visualize the final node embeddingswithout node attributes in a and with node attributes inb. In a the cluster structure is not clear and nodesfrom two different clusters are closer in the lower dimensionalspace. Whereas, if we integrate the available information from nodeattributes we can improve our final node embeddings, b",
  ": Visualization on ACM dataset": "yields the importance to integrate the node attributes in the finaljoint embedding, and this property of SpectralMix allows us toachieve a better performance on the clustering task.Run time evaluation. For 4 datasets analysed in , weshow the run time evaluation for all methods in . We do notinclude the brain networks on the run time evaluation as we do nothave results on these datasets for all the alternative baselines. Forthe alternative baselines, we report the run time needed to converge.It is noticeable that SpectralMix is faster than other methods onthree datasets, i.e., IMDB, Flickr, and DBLP. In the ACM dataset,",
  ": Run time evaluation": "SpectralMix is slower than other methods and this occurs due tohigher dimensionality of the output. Although, we recall that ouralgorithm is able to achieve better performance as showed in .Whereas, for other baseline methods, i.e. MARINE and CrossMNA,the main reason for their run time is the high number of edges inACM dataset.Our method, SpectralMix, outperforms also the alternative base-lines on datasets without node attributes, Flickr, and DBLP. Thestrength of SpectalMix is highlighted by these experiments. It out-performs baseline methods on different types of graphs: multi-relational graphs with node attributes and multi-relational graphswithout node attributes.Convergence. In we state that the value of our ob-jective function decreases and when the difference in the value Fbetween two consecutive iterations is smaller than , our algorithmconverges. We denote the number of these iterations by and weshow experimentally that, in practice, SpectralMix converges withfew iterations. For the two largest datasets, ACM and IMDB, isequal to 20 and 60 respectively, for = 0.0001. In we cansee that the value F of the objective function drops and remainsalmost constant even if we let the algorithm run for 5000 iterations.Similarly, the performance of SpectralMix measured by the valuesof NMI when ranges in remain constant after reachingconvergence.Parameter Analysis. As we have mentioned in , thereis only one parameter , desired dimensionality, that can be tunedfor SpectralMix1. To show the impact of dimensionality on Spec-tralMix performance, we use variations of synthetic datasets, dif-fering on number of relations between nodes. shows the",
  "The values selected for are reported in the Appendix": "NMI values and run time of SpectralMix on synthetics dataset, withdifferent dimensional embeddings as outputs. On these variationsof synthetic datasets the NMI value is 1 for all different dimen-sional embeddings with in the range , and is not affectedby different numbers of relations between nodes starting from 2 upto 10. We note that the running time increases slightly by havingmore relations between nodes and higher embeddings, but this is acommon feature for methods that use all the available informationfor computations as the complexity increases too. shows the impact of dimensionality on clustering per-formance and number of iterations SpectralMix needs to convergefor two real-world datasets. As the value of increases the drop onperformance is noticeable on IMDB dataset. For the ACM datasetwe see that the performance changes over different dimensions andthe best performance is achieved when we set dimensionality = 9.The main reasons are that: ACM is more dense and, PAP and PSP,have different number of edges (29, 281 and 2, 210, 761 respectively).Thus, this causes the runtime for ACM dataset to be higher thanfor other datasets, as shown in .Ablation study. To measure the impact of each componentof SpectralMix, we conduct ablation studies on the largest dataset,ACM, and we report the results in . We observe the followingfacts: 1) As expected, the node attributes help to improve the nodeclustering performance, therefore, node attributes have significantrole for representation learning of nodes. 2) If we set the sameweight factors for all attribute categories then the impact is veryhigh, as we see form the drop on the performance on clustering. 3)If we set the same weight factor for all different relation types, thenthe performance is affected but not in the same scale as it is forattribute categories. 4) The second summand, attribute contribution,in Eq. 1 indeed plays a significant role in optimization. Thus, theablation study demonstrates that by assigning the different weightsto relation types and categories of node attributes, SpectralMix canlearn a meaningful node embedding.Case Study: Brain Network. In we plot the resultsobtained with SpectralMix. We plot the clusters obtained for anhealthy subject G (top) and those obtained for a subject G",
  ": Parameter analysis on real-world datasets": "affected by schizophrenia (bottom) in an axial and sagittal viewof the brain. We run experiments considering different number ofclusters in the range , and we present the result with the high-est NCut value, therefore we plot the results for three clusters.We first notice that in G there is a large cluster, i.e., 1, while inG many regions in the temporal and frontal lobe belong to a dif-ferent cluster, i.e., 3. These findings are consistent with works inneuroscience affirming that some specific cognitive deficitspresent in schizophrenia can be attributed to parietal and frontallobe dysfunction. Moreover other neuroimaging studies haveprovided evidence that schizophrenia is associated with reductionsin the volume of tissue in the cerebrum and cerebellum, and a dis-ruption of the neural connectivity between them. These results, arecoherent with the clustering found by SpectarlMix where nodesin the cerebellar area belong to the same cluster in G as theyare characterized by reduced cerebral volume while in the healthysubject, the same nodes are assigned to 3 different clusters.",
  ": Clustering for the graph of a subject in the HL group (top)and for the graph of a subject in the SCZ group (bottom)": "much of the link information of each modality is preserved ineach Eigenvector. Furthermore, we are interested in answering thequestion: Are there any dense sub-graphs of a certain link typewhich are associated with a certain subspace of the attributes?Moreover, we can extend SpectralMix to perform other data miningtasks such as link prediction, node classification, and to infer labelsof unlabeled nodes in sparsely labeled multi-graphs, extending thesemi-supervised learning method in . M. Belkin and P. Niyogi. 2001. Laplacian Eigenmaps and Spectral Techniques forEmbedding and Clustering. In Proceedings of the 14th International Conference onNeural Information Processing Systems: Natural and Synthetic (Vancouver, BritishColumbia, Canada) (NIPS01). MIT Press, Cambridge, MA, USA, 585591.",
  "P. Chen and A. O. Hero. 2017. Multilayer Spectral Graph Clustering via ConvexLayer Aggregation: Theory and Algorithms. IEEE Tran. on Signal and Info. Proc.over Networks 3, 3 (2017), 553567": "X. Chu, X. Fan, D. Yao, Z. Zhu, J. Huang, and J. Bi. 2019. Cross-Network Embeddingfor Multi-Network Alignment. In The World Wide Web Conference (San Francisco,CA, USA) (WWW 19). Association for Computing Machinery, New York, NY,USA, 273284. C. Craddock, Y. Benhajali, C. Chu, F. Chouinard, A. Evans, A. Jakab, B. S. Khun-drakpam, J. D. Lewis, Q. Li, M. Milham, C. Yan, and P. Bellec. 2013. The neurobureau preprocessing initiative: Open sharing of preprocessed neuroimagingdata and derivatives. Front. Neurosci. (2013).",
  "L.Akoglu, H. Tong, B. Meeder, and C. Faloutsos. 2012. PICS: Parameter-freeIdentification of Cohesive Subgroups in Large Attributed Graphs": "J. Luo, D.and Ni, S. Wang, Y. Bian, X. Yu, and X. Zhang. 2020. Deep Multi-GraphClustering via Attentive Cross-Graph Association. In Proceedings of the ThirteenthACM International Conference on Web Search and Data Mining. ACM. J. Meier, P. Tewarie, A. Hillebrand, L. Douw, B. W. van Dijk, S. M. Stufflebeam,and P. Van Mieghem. 2016. A Mapping Between Structural and Functional BrainNetworks. Brain connectivity 6 4 (2016), 298311.",
  "W. Ye, L. Zhou, X. Sun, C. Plant, and C. Bhm. 2017. Attributed Graph Clusteringwith Unimodal Normalized Cut. In ECML/PKDD": "Z. Zhang, H. Yang, J. Bu, S. Zhou, P. Yu, J. Zhang, M. Ester, and C. Wang. 2018.ANRL: Attributed Network Representation Learning via Deep Neural Networks.In Proceedings of the Twenty-Seventh International Joint Conference on ArtificialIntelligence, IJCAI-18. 31553161. S.-Y. Zhou, M. Suzuki, T. Takahashi, H. Hagino, Y. Kawasaki, M. Matsui, H. Seto,and M. Kurachi. 2007. Parietal lobe volume deficits in schizophrenia spectrumdisorders. Schizophrenia Research 89, 1 (2007), 35 48.",
  "IMPLEMENTATION ANDREPRODUCIBILITY": "The source code of SpectralMix is publicly available 2.Our source code is implemented in Java and Python. We use Javato implement the main part of the SpectralMix algorithm and wehave used Python to run K-Means on node embeddings obtainedfrom SpectralMix and other methods too. To run the code call: javajar -SpectralMix.jar with the following argument source file name,which is the name of the Matlab file where all the informationabout adjacency matrix (matrices) and node attributes are stored.Example: java -jar SpectralMix.jar flickr.To change parameters about desired dimensionality or numberof iterations for each dataset, please use the class Sociopatterns.java(package datautil).Also, to conduct experiments on synthetic datasets and to gener-ate synthetic datasets please use the class DataGeneratorMain.java.The datasets are stored in the folder named data. Embeddingsfor each dataset are stored in the folder named embeddings.First we run the Java source code implementation, then we getthe final embeddings, and we need to run Python script for K-Meanson the final embeddings to get the cluster labels for each node, forthis purpose we run the Python script stored on K-Means pythonnotebook.All experimental data are open source. summarizes theircharacteristics. A brief description for the construction of the firstfour datasets (ACM, IMDB, DBLP, Flickr) is provided below.",
  "IMDB : The constructed heterogeneous graph includesinformation from 3, 550 movies (M), 4, 441 actors (A), and1, 726 directors (D). There are 2000 attributes for each movie": "DBLP: The collaboration graph has 2, 401 author nodesand 8, 703 edges. The citation graph has 6, 000 paper nodesand 10, 003 edges. The co-citation graph has 6, 000 papernodes and 141, 996 edges. The authorship graph has 64, 096edges. Flickr : Each relation type involves 10, 364 users as nodes.The friendship relation type has 401,302 edges. The tag-similarity relation type has 125,547 edges, where each edgerepresents the tag similarities between two users. Two nodesin these two relations(graphs) are linked if they refer to thesame user.",
  "We summarize details about the parameter settings of SpectralMix.The only tuning parameter is and here we provide the desireddimensionality for each dataset:": "ACM: Dimensionality = 9. IMDB: Dimensionality = 2. Flickr: Dimensionality = 11. DBLP: Dimensionality = 2. BrainNet-Healthy: Dimensionality = 2. BrainNet-Autism: Dimensionality = 2.Moreover, we set the number of iterations for convergence to100 for all datasets. In the experiments, we notice that the algorithmconverges with less than 100 iterations.More details on how to run experiments are available at theprovided link on the paper.For the comparison methods, we use the source code and theparameter settings provided by authors of each method and can befound in the following links:"
}