{
  "Abstract": "Hyperbolic neural networks have emerged as a powerful tool formodeling hierarchical data structures prevalent in real-world datasets.Notably, residual connections, which facilitate the direct flow ofinformation across layers, have been instrumental in the successof deep neural networks. However, current methods for construct-ing hyperbolic residual networks suffer from limitations such asincreased model complexity, numerical instability, and errors dueto multiple mappings to and from the tangent space. To addressthese limitations, we introduce LResNet, a novel Lorentzian resid-ual neural network based on the weighted Lorentzian centroid inthe Lorentz model of hyperbolic geometry. Our method enables theefficient integration of residual connections in Lorentz hyperbolicneural networks while preserving their hierarchical representationcapabilities. We demonstrate that our method can theoretically de-rive previous methods while offering improved stability, efficiency,and effectiveness. Extensive experiments on both graph and vi-sion tasks showcase the superior performance and robustness ofour method compared to state-of-the-art Euclidean and hyperbolicalternatives. Our findings highlight the potential of LResNet forbuilding more expressive neural networks in hyperbolic embeddingspace as a generally applicable method to multiple architectures,including CNNs, GNNs, and graph Transformers.",
  "Corresponding author: Menglin Yang": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from , August 3 - 7 2025, Toronto, ON, Canada 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-x-xxxx-xxxx-x/YY/MM",
  "Introduction": "In recent years, exploration of neural network architectures be-yond traditional Euclidean space has opened up new frontiers inmachine learning research . Among these, hyperbolicneural networks have gained significant atten-tion due to their inherent capabilities to model data with complexhierarchical structures. Hyperbolic spaces, characterized by theirconstant negative curvature that allows for exponential growth ofvolume, naturally align with the geometric properties of tree-likedata, offering a more fitting representation than their Euclideancounterparts that suffer from embedding distortions . Thisalignment has the potential to enhance learning efficiency andimprove representation learning for a wide range of applications,from graph-based data analysis to imageunderstanding .One of the core elements of the modern deep learning frame-work is the residual connection , a powerful mechanism thathas revolutionized the development of deep neural networks. By al-lowing layers to learn modifications to the identity mapping ratherthan complete transformations, which addresses issues such as thegradient vanishing and graph over-smoothing problems, residualconnections facilitate the training of substantially deeper networksand are widely used in many model architectures, including CNNs,GNNs, transformers, and diffusion models . However,the application of residual connections within Euclidean spaces isnot directly transferable to the complex geometry of hyperbolicspaces. This is primarily due to the curvature of hyperbolic space,where direct addition often violates geometric constraints. In thePoincar ball model, the sum could exceed the boundary of theball . Similarly, in the Lorentz model, it could cause points todeviate from the hyperboloid manifold .Existing works and their limitations. Several previous workshave explored implementing residual connections in hyperbolicspace. Poincar ResNet proposes projecting the hyperbolic em-beddings to the tangent space at a fixed point, parallel transportingit to the tangent space at the position of interest, and then utilizingthe exponential map to map them back to the hyperbolic space asshown in a. Another similar work, Riemannian ResNet ,although not confined just to hyperbolic space, employs similarconcepts for an analogous method. As a method previously usedas hyperbolic addition for aggregation, HGCN and LGCN propose first mapping to the tangent space of the origin for addition,and then projecting the sum back into hyperbolic space, as shownin b. HCNN proposes adding the Lorentzian space-likecomponent of the hyperbolic embeddings and then computing thetime-like component afterward. Each of these previous methodssuffers from at least one of the limitations listed below:",
  "(c) LResNet (Ours)": ": Visualization of hyperbolic residual connection methods. From left to right: (a) Parallel transport-based method,(b) Tangent space-based method, and (c) The proposed LResNet. Points with superscript H and T indicate their presence inhyperbolic space and tangent space, respectively. In each subfigure, zH represents the sum of points xH, yH H. PT denotesparallel Transport, and log and exp denotes the logarithmic and exponential mappings respectively. Our proposed methodLResNet overcomes limitations L(i, ii, iii, iv) by eliminating mappings and parallel transport (whose absence is shown via ),where the other two methods depend on as least one (shown via ).",
  "and from the tangent space, significantly increasing compu-tational complexity (see for runtime comparison)": "(ii) Non-Commutativity. The Euclidean residual connection pro-posed in satisfies the commutative property x + (x) = (x) + x, while the parallel transport-based residual connec-tion is non-commutative. This greatly restricting the expres-siveness and flexibility of the model (see .4). (iii) Numerical Instability. The parallel transport and tangentspace addition methods are prone to numerical instability.For curvature -1, the geodesic distance in the dimensionalPoincar ball model P and the logarithmic map in the dimensional Lorentz model L are defined by the followingformulas, respectively:",
  "(1)": "where x, y P, u, v L, = u0v0 =1 uv is theLorentzian inner product of u, v. For Poincar distance, whenx, y are near the boundary, the floating point representationof x2, y2 approaches one and the denominator of thegeodesic distance becomes zero, making parallel transportnumerically instable as the operation depends on dividing bydistance. For the Lorentz model, if u = v and u contains largecoordinate values, becomes less than one, which results inNaN because the domain of cosh1() is 1. (iv) Mapping Error. The parallel transport and tangent spacemethods require mapping a point in hyperbolic space to thetangent space, typically using the origin as the referencepoint for efficient computation. However, this introducesmapping errors for points not at the origin, especially forthose far from it . (v) Lack of Geometric Meaning. The Lorentzian space-like di-mension addition method from HCNN lacks a clear geo-metric interpretation within hyperbolic geometry, providingno motivation and justification as to why it works. In particular, the parallel transport method proposed by PoincarResNet and Riemmanian ResNet suffer from (i), (ii), (iii),(iv). The tangent space addition method used in HGCN andLGCN , suffers from (i), (ii), (iv). The space addition methodfrom HCNN suffers from (v). In , we show the visual-ization of LResNet and two of the previous methods: the paralleltransport method and tangent space. As demonstrated, these twomethods suffer from the need for multiple mappings to and fromthe tangent space while our method operates entirely on the hy-perbolic manifold. The space addition method is not shown as itdoes not have geometric interpretability.Proposed method. To overcome the above limitations, we pro-pose Lorentzian Residual Networks (LResNet), a residual neuralnetwork utilizing the weighted Lorentzian centroid. Unlike existingmethods that use parallel transport and tangent space addition, ourapproach normalizes the Euclidean weighted sum to directly projectthe output into the Lorentz model. Consequently, we avoid the needto map between tangent and hyperbolic spaces, operating directlyon the manifold. This approach addresses the limitations (i), (iii), and(iv), and ensures the commutativity of addition, thereby resolvinglimitation (ii). Unlike HCNN , our method has geometric inter-pretations and alleviates limitation (v), both in the form discussedin Proposition 4.2 and as the centroid w.r.t. to the Lorentzian dis-tance . Theoretically, we demonstrate that the proposed methodcan derive all of the discussed previous methods, offering generalapplicability while ensuring stability (see Lemma 4.1). Experimen-tally, our method achieves superior performance across multiplegraph and computer vision task datasets while being effective inaddressing the graph over-smoothing problem.We demonstrate the versatility of LResNet with adaptations toseveral model architectures, achieving significant performance im-provements: (1) GNNs, with up to 10.9% improvement over previous",
  "Lorentzian Residual Neural NetworksKDD25, August 3 - 7 2025, Toronto, ON, Canada": ": Test ROC AUC results (%) for Link Prediction (LP), F1 scores (%) for Node Classification (NC) for homophilous graph,and accuracy (%) for NC for heterophilic graph. The best performance is highlighted in bold. A lower value indicates a moretree-like dataset. The first four delta values are sourced from HGCN , and we used the same code for the remaining datasets.",
  "Related Works": "Hyperbolic representation and deep learning. Hyperbolicspaces have garnered extensive attention in recent years for rep-resentation learning and deep learning . A defininggeometric characteristic of hyperbolic spaces is their negative cur-vature, which results in an exponential growth of volume withdistance from the origin. This property closely mirrors the struc-ture of tree-like data, where the number of child nodes increasesexponentially . Consequently, hyperbolic representation learn-ing provides a strong geometric prior for hierarchical structures,tree-like data, and power-law distributed information. Significantadvancements have been achieved in various domains such as Word-Net , graphs [3, 24, 45, 50, 53? ], social networks ,and recommendation systems utilizing hyperbolicrepresentations. Moreover, hyperbolic deep learning has demon-strated impressive performance in image-related tasks, includingimage embedding and segmentation , offeringnew insights into deep learning paradigms, such as the interpre-tation of norms as reflections of uncertainty. Within the neuralnetwork domain, HNN presented the pioneering form of hy-perbolic neural networks, defining fundamental hyperbolic trans-formations, classifiers, and hyperbolic multiclass logistic regression(MLR). HNN++ further reduced the parameter count of hyper-bolic MLR and made advancements in fully connected layers, thesplitting and concatenation of coordinates, convolutional layers,and attention mechanisms within hyperbolic space. Recent studies,such as HyboNet and Hypformer propose frameworks that construct neural networks entirely within hyperbolic space, con-trasting with existing models that partially operate in Euclideantangent spaces.Residual neural networks and their hyperbolic adapta-tions. Residual connections are a fundamental module in modernneural networks , addressing the vanishing gradient problemdirectly and enabling the training of significantly deeper networks.By supporting identity mapping through skip connections, theyenhance gradient flow across layers, thereby improving trainingefficiency and model performance across a diverse set of tasks.Several works have explored the adaptation of residual connec-tions to hyperbolic spaces. Poincar ResNet leverages the con-cept of parallel transport to map points in hyperbolic space to thetangent space at a reference point and then, via parallel transport,map to the tangent space of a new point, and subsequently mapthem back using the exponential map. Riemannian ResNet pro-poses an analogous method to perform this operation. HCNN proposes to sum the Lorentzian space-like components and usespost-summation processing on the time component.However, these approaches have several limitations, as discussedin the introduction. This work aims to address these challenges.",
  "Preliminaries": "This section provides an overview of the fundamental conceptsin hyperbolic geometry, focusing on the Lorentz model. Hyper-bolic space can be formulated using several models, including thePoincar ball model , the Lorentz (Hyperboloid) model ,and the Klein model . These models are isometric, meaningthat points in one model can be smoothly mapped to another whilepreserving distances, angles, and geodesics .Lorentz model. An -dimensional Lorentz model is a Riemann-ian manifold (L, ) equipped with the Riemannian metric ten-sor = diag(1, 1, . . . , 1) and defined by a constant negativecurvature < 0, denoted as L,. Each point x L, has theparametrized form [, x] where R is called the time-likecomponent and x R is called the space-like component. L, isequipped with the Lorentzian inner product. For points x, y L,,their inner product x, yL is given by",
  "Methodology": "In this section, we introduce our proposed method for Lorentzianresidual connection, based on a generalization of the Lorentziancentroid , and analyze its theoretical properties, including nu-merical stability and representation power. We also propose theuse of an optional scaling method that helps control the norm ofthe output from the residual layer.",
  "Lorentzian residual connection": "In a standard Euclidean residual block, let x and (x) represent theinput and output from a neural network layer or series of layers.The residual connection is expressed as x + (x), or more generally,as x + (x), where and are scalar weights.Given vectors x, (x) L,, the Lorentzian residual connectionis defined as follows:",
  "where ||L =": "|L| is the Lorentzian norm and , > 0 areweights that can be learned or fixed. This formulation projects theEuclidean weighted sum directly onto L, using the normalizingdenominator to ensure it lies in the Lorentz hyperboloid.Following the general form of the residual connection in theEuclidean case, we can reformulate Equation (8) as a weighted sumof Lorentzian hyperbolic vector, given by",
  "are normalized weights": "Lorentz residual network (LResNet) The core componentof LResNet is the Lorentzian residual block, which consists of ahyperbolic layer followed by a Lorentzian residual connection. Thehyperbolic layer can be any type of layer adapted to the Lorentzmodel, such as hyperbolic fully-connected layers , hyperbolicconvolutional layers , or hyperbolic attention mechanisms .In Algorithm 1, we demonstrate LResNet applied to a classificationnetwork as an example. However, this method is not confined to thisparticular scenario. To ensure that the weights are constraints tothe feasible domain, in practice, we fix to be a positive constantand we take the absolute value of . Please see Lemma 4.1 and theaccompanying discussion for more details on the feasible domain.Next, we study the theoretical aspects of LResNet. We showthat LResNet is numerically stable in Lemma 4.1 and that LResNetgeneralizes previous methods in Proposition 4.2. We also showthat LResNet is a valid hyperbolic operation and that it satisfiesthe commutativity property. The complete proofs can be found inAppendix A.Numerical stability. TWe show that the weights , and, can always be selected to ensure that LResNet is numericallystable. Note that the only possible source of numerical instability isfrom the division, hence it suffices to ensure the dominator is lowerbounded by some constant and does not approach zero, which weshow via the following lemma.",
  "> 0 from thelemma. This lemma ensures that the denominator can always belower-bounded by a positive constant of our choosing (in this case,1/": "). Thus we never risk dividing by values close to zero, making numerically stable. Note that is still allowed to takeon any arbitrary non-negative value, which allows for any arbitraryratio /. Thus fixing = 1 does not at all restrict the valuesof the output of Equation (9) and any discussion of using arbitrary values remain applicable.Validity of LResNet. Here we show that LResNet is a validhyperbolic operation. To see that Equation (9) indeed maps to hyper-bolic space, first note that for any x, (x) L,, we can compute,x + , (x), ,x + , (x)",
  "|x + (x),x + (x)L|= 1/": "Given that ,x + , (x) is a positive time-like vector, itsLorentzian inner product with itself must be negative, specifically itmust be 1/ from above. Therefore ,x+, (x) L,,confirming that LResNet is a valid hyperbolic operation.Relation to previous methods. Our approach can theoreticallyderive the geometric meaning of the previous methods mentionedin , based on the following results in Proposition 4.2.",
  ": end for": "Proposition 4.2. Let z be the output of one of the parallel transportmethods in Equation (5), the tangent space method in Equation (6), orthe space addition method in Equation (7). Then there exists weights, R+ such that the point m = ,x + ,y lies on thegeodesic from o to z. Then by carefully selecting weights (or using trainable weights)in LResNet, we can derive previous methods using our method.This shows that LResNet has at least the representative power ofany of the previous methods, ensuring its expressiveness.Commutativity. LResNet is commutative as it is a general-ization of a weighted sum and x + y = y + x. Forcompleteness and rigor, we also include the following theoremthat demonstrates the non-commutativity of the parallel transportmethod. Theorem 4.3. Let x, y L, be points such that x = y for + 1 and x+1 = y+1. Let z = x y be the output of theparallel transport method, and let z = y x be the output in theother direction. Then z+1 = z+1. The two directions of the parallel transport addition in the abovecase are reflected over an entire axis, giving theoretical motivationfor its inflexibility and results shown in .4. For instance,when = 2, = 1, x = and y = , we havex y = and y x = .Computational Complexity. To compute the weighted sumfor LResNet in Equation (9), we calculate the Lorentzian norm in thedenominator of the weights , and ,, which takes ()time. Afterward, the remaining computation is a straightforwardEuclidean addition, allowing LResNet to be computed in () time,comparable to the computational efficiency of the Euclidean case.",
  "LResNet overcomes all of the previous methods limitations dis-cussed in . In this section, we summarize the advantagesbelow": "(i) Efficiency. Unlike previous methods that involve multiplemappings between hyperbolic and tangent spaces, LResNetis a simple weighted sum, making it significantly more effi-cient. .4 provides a more detailed runtime analysisin , where LResNet achieves over 2000 times fastercomputation than both the parallel transport and tangentspace method in the same setting. (ii) Commutativity. LResNet is commutative. In the .1and .4, we discuss the effects of the non-commutativityon the parallel transport method , showing that it is un-predictable which direction of addition achieves better per-formance. (iii) Numerical Stability. The result from Lemma 4.1 ensuresthat LResNet is computationally stable, avoiding the compu-tational instability issues mentioned in the introduction. Byeliminating the mapping between tangent and hyperbolicspaces, LResNet avoids mapping errors for points far awayfrom the origin . (iv) Geometric Interpretation. LResNet has a geometric inter-pretation, with the ability to theoretically achieve previousmethods (Proposition 4.2). By carefully selecting weights,LResNet is able to achieve the geometric meaning of previousmethods by ensuring the outputs lie on the same geodesicfrom the origin. This ensures the representation power ofLResNet and provides theoretical motivation for LResNet asopposed to the space addition method .",
  "Optional scaling": "Very often the Euclidean norm of the hyperbolic embedding hasimportant implications in model performance. For example, in im-age classification tasks, the norm tends to be positively correlatedwith classification confidence . However, previous work has shown that the unweighted Lorentzian centroid (a specific caseof LResNet) can get very close to the origin in manifolds with lowcurvature, resulting in a small Euclidean norm. Conversely, exces-sively large norms can lead to mapping errors . To help keep thenorm of the output within reasonable ranges, we propose to use anoptional scaling method after the LResNet computation when train-ing in low-curvature manifolds. For a hyperbolic vector m L,,the scaled value is",
  ",(10)": "where > 0 is the scaling constant, which can be fixed or trained.Note that since geodesics in the Klein model are Euclidean linesand the isometry K that maps from the Lorentz model to the Kleinmodel is given by K(x) = x/, this scaling simply slides m alongthe geodesic to or from the origin, ensuring the scaled output stillsatisfies Proposition 4.2. Note that the scaling also levitates thepotential limitation that the output of LResNet is always boundedby the larger of the two inputs, allowing for more expressivenessand representative power.",
  "Experiments": "To demonstrate the effectiveness and versatility of our method, weapply LResNet as the residual connection in multiple adaptationsof hyperbolic neural networks. We demonstrate that our methodachieves better results than any previous methods of residual con-nection. In each task, we focus on testing the residual connectionmethods by using a consistent base model. In .1 and Sec-tion 5.2, we test LResNet on several graph datasets, as part of hy-perbolic GNN and graph Transformer architectures to demonstrateits effectiveness. In .3, we test LResNet on image datasetsand demonstrate its effectiveness and robustness. In .4, weperform further analysis that demonstrates the efficiency and effec-tiveness of LResNet and shortcomings of previous methods. Thedatasets were selected as they exhibit low Gromovs -hyperbolicity() values, indicating highly hierarchical structures1.For all of our experiments, we used largely the same set of hyper-parameters as we did the base model without residual connectionwhenever available. Additionally, we find that the addition of aresidual layer does not add much tuning overhead. We include adiscussion of the effects of the curvature as hyperparameters inAppendix C.4, which is missing in the work of the base models. 1For homogeneous graph datasets, is taken directly from HGCN . For heteroge-neous datasets, was computed by us using the same code from HGCN . In bothcases, the values were not normalized. For image datasets, is taken from HCNN,where the value is normalized by the largest distance between two points in the datasetusing the method from .",
  "Adaptation to GNNs": "GNN model architecture. We formulate a skip-connected graphconvolutional network with LResNet as the residual connection,using the fully hyperbolic graph convolutional layer . For theoverall model architecture, we follow the ResGCN architectureoutlined in , where the output of each intermediate layerwas added to the output of the next layer via LResNet or a baselinehyperbolic residual connection method. The model architecture isshown in a, where G represents the graph convolutionallayer and arcs are residual connections.GNN experimental setup. We evaluate LResNet on both nodeclassification and link prediction tasks. We utilize several datasets:(1) homophilous graphs, including Disease, Airport, and twobenchmark citation networks, namely PubMed and Cora;(2) Heterophilic graphs, including Chameleon and Sqirrel, wherewe use the splits proposed by . For homophilous graphs, theDisease and Airport datasets exhibit a more hierarchical structure,whereas the citation networks are less so, making them suitablefor demonstrating the generalization capability of LResNet. Forevaluation, we report the F1-score for the node classification tasksand the Area Under Curve (AUC) for the link predictions task. Forheterophilic graphs, we focus on the more difficult task of nodeclassification and report accuracy percentage.We closely follow the implementation of HyboNet model ,utilizing its fully hyperbolic linear and aggregation layers. Our op-timization follows the same setup, with parameters grouped basedon their presence on the hyperbolic manifold. For the decoder, weimplemented the Fermi-Dirac decoder . For the implemen-tation of LResNet, we applied constant weights of 1 for simplicity.Baselines. We test the effectiveness of LResNet against otherhyperbolic residual methods, by applying the baselines insteadof LResNet. We consider the base HyboNet without residualconnection, and the previous residual connection methods of theparallel transport method , the tangent space method , thespace addition method .Experimental findings. We show the results in . Due tospace constraints and for readability purposes, we omit the compar-ison to results from Euclidean GCNs and other hyperbolic GCNs.These comparisons can be found in of , where the base-line HyboNet outperforms the aforementioned GCNs. As shownin , LResNet is the best performer in 8 out of the 10 tasks,for up to 4.2% in the case of node prediction for Chameleon. Com-pared to the base HyboNet without residual connection, LResNetsubstantially outperforms in 9 out of the 10 tasks, with the oneremaining task being of comparable performance. Compared to thebaseline residual connection methods, LResNet is the notably bestperformer in 8 out of the 10 tasks, especially for the more difficulttasks of node classification on heterophilic datasets, demonstrat-ing its effectiveness and generalizability to more difficult problems.LResNet is also the best performer in every node prediction task,which benefits from deeper networks, demonstrating its superiorityas a residual connection. In the more hyperbolic datasets, LResNetalways performs better and by large margins, suggesting that it ismore suitable for hyperbolic networks as it doesnt map betweenhyperbolic and tangent (Euclidean) spaces.",
  "HomophilousHeterophilicDatasetDiseaseAirportPubMedCoraChameleonSqirrelHyperbolicity = 0 = 1 = 3.5 = 11 = 2 = 1.5TaskLPNCLPNCLPNCLPNCNCNC": "HyboNet 96.8 0.496.0 1.097.3 0.390.0 1.495.8 0.278.0 1.093.6 0.380.2 1.340.1 0.834.3 0.5Parallel Transport 86.4 0.884.8 3.793.6 0.193.4 0.696.5 0.177.8 0.594.8 0.376.0 0.836.6 1.932.3 0.8Tangent Space 76.0 2.491.9 1.993.5 0.192.0 2.996.4 0.276.8 0.994.1 0.379.2 0.138.3 0.834.0 0.6Space Addition 83.1 1.288.9 2.595.8 0.390.0 1.495.5 0.275.9 0.993.2 0.278.6 0.539.4 2.034.5 0.2LResNet (ours)97.3 0.496.1 1.097.3 0.393.9 0.796.2 0.180.1 1.094.1 0.380.6 0.941.1 0.937.1 1.1",
  "Adaptation to Graph Transformers": "We also investigate the application of our method to graph Trans-formers, where we test LResNet as part of a hyperbolic adaptationof SGFormer , a recent SOTA Euclidean graph Transformer. Weconsider the same hyperbolic residual connection baselines as didin .1, namely the parallel transport method, the tangentspace method, and the space addition method.Lorentzian graph Transformer architecture. Following thenotations in SGFormer , let Z(0) be the input embedding, Zbe the output of the global attention layer, and GN(Z(0), A) bethe output of the GNN layer where A is the adjacency matrix. ForLResNet and the space addition method, we can project Z andGN(Z(0), A) directly to L,. The final embedding is computed as",
  "expo ((1 )Z)) expo (GN(Z(0), A)).(12)": "The final embedding of the model is then in hyperbolic space. Fig-ure 2c shows the visualization of the model.Experimental setup. We closely followed the setup of the baseSGFormer model . For the datasets, we consider 3 heterophilicgraphs datasets, namely Chameleon, Sqirrel, and Actor. Weuse the same splits for Actor as and the same splits fromearlier for Chameleon and Sqirrel. We report node classificationaccuracy as did in SGFormer .As for the decoder, we use the same Fermi-Dirac decoder from earlier for the final fully connected layer (we elaborate on thedetials for the decoder in the next paragraph). For the optimizer,we utilized two separate optimizers similar to HyboNet and theGCN experiments: the Euclidean Adam optimizer for non-manifoldparameters and the Riemannian Adam optimizer for manifold pa-rameters. We use trainable weights for LResNet.Here we elaborate on the decoder used for node classificationfor clarity. Specifically, let x be the final embedding precedingthe decoder layer. Then the decoder layers learn vectors in hy-perbolic space v1, . . . , v to be the boundary of the node classes.",
  "Parallel Transport 94.1 0.1372.9 0.23Tangent Space 94.0 0.1971.5 0.30Space Addition 94.3 0.1174.3 0.22LResNet (ours)94.6 0.1774.8 0.25": "Intuitively, the distance between x and v represents the negativelog-likelihood of x being in class . The classification of x is thusarg max (v, x). Please see .3 of for more details.Experimental findings. The results are presented in .LResNet consistently outperforms both the base Euclidean SG-Former and the baseline hyperbolic residual connection methodsacross all three cases, highlighting its effectiveness in Transformermodels. Moreover, in 2 of the 3 datasets, the hyperbolic SGFormernearly always surpasses the Euclidean version, supporting the ad-vantages of hyperbolic modifications.",
  "KDD25, August 3 - 7 2025, Toronto, ON, CanadaNeil He et al": ": ROC AUC (AUROC,%), AUPR(%), and FPR96(%) results for OOD detection on CIFAR-10 and CIFAR-100 with Places365,DTD, and SVHN, as OOD datasets. R20 and R32 here denote ResNet-20 and ResNet-32 architectures, both with channel widths(8, 16, 32). The best performance is highlighted in bold. For FPR95, lower is better. For AUROC and AUPR, higher is better.",
  "LResNet (ours)63.868.484.285.896.396.783.783.757.669.186.391.5": "a simple hyperbolic adaptation. For an input vector x L,, wedefine a hyperbolic layer (HL) to be a neural network layer that firstapplies the Euclidean layer on the space-like dimension of an inputvector x to obtain (x) where is the Euclidean layer, and thencalculating the time-like dimension afterward. Formally, it outputs",
  "||y ||2 1": ".For LResNet, we use trainable weights as outlined in Equation (8).We also use the scaling method outlined in Equation (10) to takeadvantage of the positive correlation between embedding norm andclassification confidence . b shows a visualization of ourresidual block, where the Lorentz variation (and subscript L) of anoperation indicates implementation with a hyperbolic layer. For thedecoder, we experienced numerical instability with the Lorentz MLRin HCNN. As a result, we use the isometry map to projectthe final embedding from the Lorentz model into the Poincar ballmodel to use the Poincar MLR from HNN++. Specifically, forthe final fully connected output layer, we first map the Lorentzianembedding to the corresponding Poincar ball model, then we usethe pooling method in Poincar ResNeT where pooling is donein Euclidean space and mapped back into the Poincar ball. ThePoincar MLR layer is then applied to the hyperbolic pooling output.For the optimizer, we used the Riemannian SGD optimizer that wasused in HCNN for classification and the Adam optimizer forODD-detection as did in Poincar ResNet .Classification accuracy experiment. To demonstrate the ef-fectiveness of LResNet over previous methods, we evaluate imageclassification performance using the ResNet-18 architecture and two datasets: CIFAR-10 and CIFAR-100. For the ex-periment, we use the structure shown in b and test ourresidual connection against previous methods. For the baseline,we test against the parallel transport method, the tangent spacemethod, and the space addition method. As parallel transport isnot commutative, we use x (x) where (x) is the output ofthe convolutional layer. We adopt the training procedure of which have been optimized for Euclidean CNNs and yielded strong results for both Euclidean and Hyperbolic ResNets. The results areshown in . LResNet performs notably better than all of theprevious methods for both datasets, demonstrating its effectivenessas a residual connection method for vision tasks.Out-of-distribution (OOD) robustness. To check that LRes-Net is robust to out-of-distribution (OOD) samples, we test the OODdetection performance of our Lorentzian ResNet-20 and ResNet-32 with channel widths (8, 16, 32), trained on either CIFAR-10 orCIFAR-100. We closely follow the experimental setup in PoincarResNet . We use the Places-365 dataset , the Texture dataset, and the SVHN dataset . The baseline we compare to arethe Euclidean and hyperbolic ResNets in Poincar ResNet .Poincar ResNet is a baseline hyperbolic ResNet model using spe-cially designed hyperbolic convolutional layers and uses the par-allel transport method as residual connection, thus it can be seenas a representative of the parallel transport baseline. As for thedetection of the OOD sample, we use the energy score which wasintroduced by and also used in . Finally, we compare ourresults against the baselines with the three commonly used metricsof FPR95, AUROC, and AUPR.OOD detection results. The results in indicate thatLResNet, when trained on CIFAR-10, significantly outperforms Eu-clidean counterparts and previous hyperbolic ResNets on almost allmetrics. On CIFAR-100, our model notably surpasses the baselinewith a ResNet-32 architecture. This demonstrates that LResNet ismore robust to OOD samples than its Euclidean and hyperboliccounterparts using parallel transport for residual connections, con-firming that LResNet is not only effective and reliable.",
  "(b) Airport": ": Comparison of ROC AUC (%) differences for linkprediction (LP) between HyboNet with and without residualconnections: orange indicates our LResNet as the residualconnection, while blue represents the tangent space methodas the residual connection. In a we show the resultson the Disease dataset and in b we show the resultson the airport dataset. examining the effects of non-commutativity on the flexibity andexpressiveness of the parallel transport method.Efficiency analysis. We compare the efficiency of LResNetwith previous multi-mapping methods, specifically the paralleltransport and tangent space method, by conducting 100 additionson randomly generated hyperbolic vectors (on the manifold withcurvature = 1) with dimensions of 2048 and sizes of 10,000, anddimensions of 4096 and sizes of 100,000, using a single RTX 3070GPU. The average runtime for each method is presented in .LResNet demonstrates significant speedup over the baselines andoffers better scalability as the size and dimension increase, for over2000 times speedup.Overcoming graph over-smoothing. Previous studies haveshown that stacking graph convolutional layers often leads to per-formance drops due to gradient vanishing or over-smoothing .Residual connections traditionally address these issues in Euclideanspace. To examine the impact of the proposed method in hyper-bolic space, we implemented the GCN architecture from .1on the Disease and Airport datasets across 4, 8, 16, 32, and 64 layers, using the tangent space method as a baseline for link pre-diction tasks. The results in depict the difference in ROCAUC scores between residual-connected HyboNets and the baseHyboNet for visual clarity. We observed that the performance gapbetween HyboNet and LResNet widens with more layers, indicat-ing the effectiveness of LResNet in countering over-smoothing.Conversely, while the gap widens for the tangent space methodas well, it shows a reduction in differences, especially at 32 layersfor Disease and 16 for Airport, highlighting LResNets stabilitywhen applied to deep neural networks. We found that in practice,parallel transport method yielded NaN values when using 16 ormore layers, reflecting its numerical instability. The results weretherefore excluded.Commutative capability. To investigate the effect of commu-tativity of the residual connection in hyperbolic models, we testthe performance on link prediction of both directions of additionfor the parallel transport method on Disease and Airport datasetsusing the architecture in a. Forward operation representsx (x) and backward operation represents (x) x, where (x) is the output of the hyperbolic graph convolutional layer. Theresults are shown in . The forward method significantlyoutperforms the backward method in link prediction tasks of theDisease dataset, while the backward method outperforms the for-ward method for the Airport dataset. This shows the limitationin the flexibility of the parallel transport method due to its non-commutativity, as it is unpredictable which direction of additionwould have the better performance.",
  "Conclusion": "In this work, we proposed LResNet, a hyperbolic residual neuralnetwork based on the weighted Lorentzian centroid in the Lorentzmodel of hyperbolic geometry. LResNet addresses the limitations ofprevious methods, offering increased efficiency, flexibility, numeri-cal stability, commutativity, and geometric information retention.We proved that LResNet can theoretically derive previous methodswhile ensuring numerical stability. Empirically, LResNet outper-forms previous methods in image classification, link prediction,and node classification tasks, demonstrating superior performanceand robustness compared to Euclidean and hyperbolic counterpartsacross multiple architectures, including GNNs, CNNs, and graphTransformers. However, it should also be noted that the perfor-mance of a residual-connected model is still conditioned upon theperformance of the base model, while there lacks an abundanceof hyperbolic models, such as in the case of diffusion models. Onedirection of future work is to apply LResNet to newly developedhyperbolic model architectures. This work was supported in part by the National Science Founda-tion (NSF) IIS Div Of Information & Intelligent Systems 2403317. Wealso gratefully acknowledge support in part from Amazon ResearchAward, Snap Research, and the Yale Office of the Provost. Addition-ally, this research has greatly benefited from the discussions andresearch talks held at the IMS-NTU Joint Workshop on AppliedGeometry for Data Sciences. We extend our sincere gratitude to thereviewers for their valuable feedback and insightful suggestions.",
  "||v||L": "By analyzing the symmetry in the construction of x, y, y, p, u, u, v,and v, it can be shown that = and = due to the identi-cal nature of the transformations applied in either direction. Thisimplies that = , leading to the conclusion that z+1 = z+1 asthe operations in the parallel transport in both directions result invectors whose ( + 1)-th components are negations of each other.In the following, we give the detailed derivation.First, since = , we have = . Let = = , then wehave,",
  "CImplementation and Training DetailsC.1Implementation details for graph datasets": "C.1.1GNN hyperparameters. For the homophilous graphs datasets,we used the largely the same hyperparmeters as in , includingdropout rate, learning rate, weight decay rate, gradient clip value,and margins for the marginal loss. For link prediction tasks, we runthe model for 3 layers. For node classification tasks, we performeda search for the number of layers on the set {3, 4, 5, 6, 7, 8}. For alltasks, we performed a search for the constant negative curvature on the set {0.1, 0.5, 1.0, 1.5, 2.0, trainable}, where the initialvalue of the trainable curvature is 1.0. For heteophilic graph datasets, we used a curvature of 1 andperformed a grid search in the following search space: dimensionwithin {16, 32, 64}; learning rate within {0.001, 0.01}; dropoutrate within {0, 0.1, 0.2, 0.3}; weight-decay rate within {0, 1 4, 1 3}; number of layers within {3, 4, 5, 6, 7, 8}. While manyvalues were searched, we find that the performance of LResNetand the baselines depend mostly on the performance of the baseHyboNet without residual connection. As a result, we used thesame hyperparameters as in .For homophilous graph datasets, we used a constant weight of 1for LResNet shown in Equation (9). For heterophilic datasets, weused trainable weights instead.",
  "C.2Implementation details for image datasets": "For hyperparameters in the classification task, we performed agrid search where learning rate is within the set {0.1, 0.01, 0.001}and weight decay is within the set {0, 5 4, 5 3}. For OOD-detection, we trained the model with the same hyperparameter asin and using the Riemmanian Adam optimizer as in aswell. In both cases, we performed search for curvature in the set{0.1, 1, 1.5} and the scaling coefficient in Equation (10) in the set{0.5, 1.0, 2.0, trainable}.",
  "C.4Analysis of curvature as hyperparameter": "We examine the sensitivity of LResNet to the choice of curva-ture of the hyperbolic manifold by conducting the link predic-tion and node classification experiments in .1 for =0.5, 1.0, 1.5, 2.0, trainable on the airport and disease datasets.The results are shown in . We do not observe the model tobe sensitive to curvature choices."
}