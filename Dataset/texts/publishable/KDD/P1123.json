{
  "Abstract": "Graph Neural Networks (GNNs) have achieved great success inKnowledge Graph Completion (KGC) by modelling how entitiesand relations interact in recent years. However, the explanation ofthe predicted facts has not caught the necessary attention. Proper ex-planations for the results of GNN-based KGC models increase modeltransparency and help researchers develop more reliable models.Existing practices for explaining KGC tasks rely on instance/subgraph-based approaches, while in some scenarios, paths can provide moreuser-friendly and interpretable explanations. Nonetheless, the meth-ods for generating path-based explanations for KGs have not beenwell-explored. To address this gap, we propose Power-Link, the firstpath-based KGC explainer that explores GNN-based models. Wedesign a novel simplified graph-powering technique, which enablesthe generation of path-based explanations with a fully parallelisableand memory-efficient training scheme. We further introduce threenew metrics for quantitative evaluation of the explanations, to-gether with a qualitative human evaluation. Extensive experimentsdemonstrate that Power-Link outperforms the SOTA baselines ininterpretability, efficiency, and scalability. The code is available at",
  "The two first authors made equal contributions.Corresponding author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08 ACM Reference Format:Heng Chang, Jiangnan Ye, Alejo Lopez-Avila, Jinhua Du, and Jia Li. 2024.Path-based Explanation for Knowledge Graph Completion. In Proceedings ofthe 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining(KDD 24), August 2529, 2024, Barcelona, Spain. ACM, New York, NY, USA,12 pages.",
  "Introduction": "Knowledge graphs (KGs) are a form of knowledge representationthat encodes structured information about real-world entities andtheir relations as nodes and edges. Each edge connects two entities,forming a triple called a fact. However, real-world KGs often sufferfrom incompleteness, which limits their usefulness and reliability.Incompleteness refers to not all possible facts being representedin the KG, either because they are unknown or unobserved. Toaddress these issues, knowledge graph completion (KGC) is a taskthat aims to predict missing relations in a KG, given the existingfacts .GNN-based KGC models have shown effectiveness on the KGCtask and attracted tremendous attention in recent years . WhileGNNs achieve remarkable success in completing KGs, how GNNspredict a given triplet candidate as factual remains unclear. Asa result, the prediction needs substantial explanations before re-searchers and end users bring them into practice, which falls intothe research scope of explainable artificial intelligence (XAI) .XAI enhances the transparency of black-box machine learning(ML) models and has been extensively investigated on GNNs fornode/graph-level tasks on homogeneous graphs . For a givendata instance, these GNN explanation methods either learn a maskto select a subgraph induced by edges or search for a sub-graph in the graph space as an explanation. However, to thebest of our knowledge, the GNN explanation of KGs, especiallyregarding the KGC task, is surprisingly few in the literature.Providing explanations for GNN-based KGC models is a noveland challenging task. Given the nature of KGs in modeling struc-tured real-world information, their end users often have little orno background in AI or ML. Therefore, the explanations need to bemore aligned with human intuitions. Existing approaches attemptto explain KGs from triplet or subgraph perspectives. Still, the prob-lem of what constitutes a good explanation of KGs is more complexthan on homogeneous graphs, especially in terms of satisfying thestakeholders desiderata . In contrast to the widely adoptedinstance/subgraph-based explanations, we focus on generating the",
  "explanations of GNN-based KGC models from the perspective ofpaths in this work:": "Can paths in the KG provide better explanations forGNN-based embedding models on the KGC task? illustrates three challenges of applying instance-basedor subgraph-based GNN explanations from homogeneous graphsto KGs: (1) Interpretability: For KGs, explanations should revealthe connections between a pair of entities given a specific relationthrough paths on KGs. However, previous approaches produce dis-connected subgraphs that do not show this connected pattern (e.g.,left from ). These subgraphs are not suitable for human inter-pretation and exploration. (2) Sufficiency: Paths contain sufficient in-formation to explain whether a given triplet is factual. From ,for explaining triplet _,_, _,the subgraph-based explanation brings in a noisy and irrelevant fact_,_, . The instance-based one only ex-tracts a related fact _,__,_but loses sufficient information for users to interpret. Therefore,neither method is suitable for KGs where concise and informa-tive semantics are preferred. (3) Scalability: Compared to generalsubgraphs, paths form a much smaller search space for optimalsubgraph explanations since paths are simpler and scale linearly inthe number of graph edges. Therefore, paths can better fit with KGsthat usually come with massive entities. To our best knowledge,there is only one approach, PaGE-Link that explores path-based solutions to the GNN explanation problem. But PaGE-Linkfocuses on heterogeneous graphs without considering the relationtypes and the scalability of the method, which makes it unsuitableto be directly applied to KGs. Therefore, the question of KGs is stillunderexplored.Given the important roles that paths play in the explanation ofKGC tasks, we propose Power-Link to tackle the aforementionedchallenges, which aims to enhance the interpretability, sufficiency,and scalability of the explanations by providing explainable pathsfor the GNN-based KGC models. To the best of our knowledge,this is the first work specifically designed to explain KGC tasks viapaths. depicts the overall framework of Power-Link. 1) Wefirst extract a k-hop subgraph around the source and target nodesand prune it to eliminate noisy neighbours. 2) A Triplet Edge Scorer",
  "HeterogeneityExplains GNNExplains KGCPath-basedScalabilityLocal Post-hoc": "(TES) is proposed to measure the importance of each edge along thecandidate paths by leveraging the node and edge type information,which is specifically designed for the KG scenario. 3) PaGE-Linkemploys the shortest-path searching algorithm to emphasize pathsin each iteration, which is computationally intensive and not ap-plicable to large-scale KGs. It also introduces the accumulation ofon-path(edges on the paths) errors during the learning process. Toalleviate this issue, we propose to replace the shortest-path search-ing algorithm with a specially-designed graph-Power-based methodto amplify explainable on-path edges. Our method is highly par-allelisable with GPUs and consumes minimal memory resources,enabling easy scaling to large KGs. The graph power-based ap-proach also provides an interface for end users to select the desiredpath length of each explanation manually. Power-Link generatesinterpretable paths to explain the prediction of GNN-based KGCmodels in an efficient manner. 5) To better measure the performanceof explanation methods on KGs from a quantitive perspective, wealso propose three proper metrics, +, , and ,that reflect how good an explanation is in the empirical evaluation.The main contributions of this paper are summarised as follows: Motivated by the need to explain the prediction on the KGCtask, we propose the first path-based methods Power-Linkfor GNN-based KGC models that efficiently generate morehuman-interpretable explanations. We propose a novel path-enforcing learning scheme based ona simplified graph-powering technique, which enjoys highparallelisation and low memory cost. This enables Power-Link to generate multiple explanatory paths precisely andefficiently. We propose three metrics to better evaluate the performanceof path-based explanation methods. We show the superiorityof Power-Link in explaining GNN-based KGC models overSOTA baselines through extensive experiments from bothquantitative and qualitative perspectives.",
  "EdgeScorer, , , , ,": ": The overall framework of the proposed Power-Link. Given a KG and a trained KGC model as inputs, we aim togenerate interpretable paths to explain why the KGC model predicts a target triplet is factual. the past decade. Please kindly refer to the survey for a thoroughreview of the recent advances in embedding-based methods. Despitethe success of GNN-based graph embedding models for KGC, theunderlying mechanisms of how these embeddings facilitate KGcompletion are still unclear. Therefore, explaining KGC modelsis an important research challenge, especially under the post-hoccircumstance, which is the main focus of our paper. Specifically, wechoose GNN-based KGC models as the targeting type of model toexplain. GNNs are recently considered unnecessary for KGC dueto their limited impact on the link prediction performance . Onthe contrary, we present Power-Link to underline the significantimpact of GNNs on the explanation of KGC.",
  "GNN and KGC Explanation": "In this section, we review existing representative methods for GNNand KGC Explanation. For better clarity, we highlight their maindifferences and advantages by comparing them with our proposedPower-Link in .GNN explanation. Explaining the prediction of GNNs is an important task for understanding and verifying theirbehaviour. A common approach is to identify a subgraph that ismost relevant to the prediction. In this vein, GNNExplainer and PGExplainer use mutual information (MI) between themasked graph and the original prediction as the relevance measureand select edge-induced subgraphs by learning masks on graphedges and node features. SubgraphX and GStarX use gametheory values, such as Shapley value and structure-aware HNvalue , as the relevance measure, and select node-induced sub-graphs by performing Monte Carlo Tree Search (MCTS) or greedysearch on nodes. PaGE-Link argues that paths are more inter-pretable than subgraphs and extends the explanation task to the linkprediction problem on heterogeneous graphs. For a review of othertypes of GNN explanations, please kindly check the surveys .Among them, our work is most closely related to PaGE-Link , aswe also advocate for path-based explanations for GNNs. However,rather than the heterogeneous graphs, we focus on a new challeng-ing task of providing path-based explanations for KGC. This couldnot be trivially tackled by the existing methods due to the lack oftriplet information and the limitation of scalability.KGC explanation. As the scope of this paper, the explanation ofKGC is rather less explored especially compared with the fruitful results on GNN explanations. Janik and Costabello proposesExamplE heuristics generate disconnected triplets as influentialexamples in latent space. Similarly, Zhao et al. leverages in-formation entropy to quantify the importance of candidates bybuilding a framework KE-X and generating explainable subgraphsaccordingly. Rossi et al. develops an independent frameworkKelpie to explain the prediction of embedding-based KGC methodsby identifying the combinations of training facts. However, thesemethods produce explanations in the form of subgraphs or triplets.We argue that paths provide better interpretability than unrestrictedsubgraphs/triplets. Thus path-based explanations are preferable onKGC tasks when the users have limited ML backgrounds.",
  "Paths on Graphs": "We briefly review the methods which leverage paths on graphs ascrucial parts of their algorithms. Paths are often used to representthe structure and semantics of graphs, such as Katz index ,graph distance , SimRank , and PathSim . They can alsoreveal the underlying relationship between a pair of nodes, andtherefore bring more explanability to the methods. proposea context path-based GNN that recursively embeds the paths con-necting nodes into the node embedding with attention mechanisms.In the same field, Zhu et al. utilizes the idea from the neuralbellman-ford algorithm to construct a general path-based frame-work NBFNet for both link prediction and KGC. Zhu et al. further enhance the scalability of NBFNet by proposing to use Aalgorithm for path constructions. Chang et al. propose to aug-ment the knowledge used in the KGC task from the counterfactualview and enhance the explainability of the completion results. Whilethese methods can generate non-trivial path-based explanations asa side-product, they cannot be applied to the black-box explanationsetting in explainable artificial intelligence (XAI), where the pre-trained KG embeddings are given. Therefore, we argue that pathsare still valuable for providing interpretable explanations for KGCunder the local post-hoc scenario.",
  "KDD 24, August 2529, 2024, Barcelona, SpainHeng Chang, Jiangnan Ye, Alejo Lopez-Avila, Jinhua Du, & Jia Li": "where E denotes the set of entities (nodes) and R is the set ofrelations (edges). (,) are -th and -th entities and is -threlation between them. The entities are indexed by , , and therelation is indexed by . The number and type of relations in a KGcan vary widely depending on the domain and the source of thedata. We also use the notation (,,) to indicate an entity pairwith a directional relation , where is the head entity and isthe tail entity.Throughout this paper, we use bold terms, or , to denotematrix/vector representations for weights and entities, respectively.And we select italic terms, or , to denote scalars. We also usea binary adjacency matrix {0, 1}| E|| E| to define whether twoentities are connected in a KG G. The (, ) entry = 1 if (,)is valid for any or otherwise = 0.",
  "GNN-Based framework for KGC": "To predict the valid but unseen triplets, the task of KGC uses knowntriplets existing in the KG G. GNN-Based framework dealing withKGC usually adopts an encoder-decoder framework , whereGNNs perform as the encoder and the KGE score functions (e.g.,TransE, DistMult, and ConvE) perform as the decoder. A KGs enti-ties and relations are first embedded by the GNN encoder throughthe message passing and neighbourhood aggregation procedures .Using the embeddings, the score function , which is also referredto as the energy function in the energy-based learning framework,learns to assign a score (,,) from either real space or com-plex space to each potential triplet (,,) E R E. Thelearned scores reflect the possibility of the existence of triplets.Because of the message passing and neighborhood aggregationmechanism in GNNs, a -layer GNN only collects messages fromthe -hop neighbors of an entity pair , to compute their rep-resentation and . Therefore, we constrain the search spaceof explaining the KGC model (G, (,,)) with an -Layer GNNencoder to the computation graph G = (E, R). G is the -hopego-graph of the predicted pair (,), i.e., the subgraph with entityset E = { E|(,) or (,) }. Therefore, theKGC result is thus fully determined by G, i.e., (G, (,,)) (G, (,,)).",
  "Problem Formulation": "In this work, we focus on a post-hoc and instance-level KGC expla-nation problem. The post-hoc assumption means the model (, )is already trained and fixed, and the explanation method does notmodify its architecture or parameters. The instance-level assump-tion means that the explanation method generates an explanationfor each individual prediction of a target triplet. We use the notation to denote the target to be explained and its related ele-ments. The target triplet is denoted by , , = (, , ). Theexplanation method aims to provide a rationale for why a giventriplet is predicted as factual by the model (, ). In a practical KGuser case, the explanation could address questions such as whya persons nationality is USA. We narrow down the scope of ex-planation to paths. The explanatory paths should be concise andinformative. It should only contain nodes and edges that are mostinfluential to the prediction. We formally define the explanationproblem for GNN-based KGC models as: Problem 1 (Path-based KGC Explanation). We focus on thetask of generating path-based explanations for a predicted fact be-tween a pair of entities and . Given a trained GNN-based KGCmodel (, ), a computation graph G of , that extracted from thetarget KG G, we find an explanation P = { | is a path withmaximum length }. By proper construction of optimization on can-didates from P, we aim to generate concise and influential pathsas explanations for the prediction. To further reduce the graph complexity and accelerate the pro-cess of finding paths, we adopt the -core pruning module from to eliminate spurious neighbours and improve speed. The -coreof a graph is defined as the unique maximal subgraph with a min-imum node degree , such that the -core of G is defined asG = (E , R ).",
  "Proposed Method: Power-Link": "Power-Link contains three components: a Triplet Edge Scorer (TES),a Path-Enforcing Learning module, and a Path Generation module.In a path-forcing way, the TES learns to create a score matrix foreach edge in the graph. The path generation module then selectsexplanatory paths according to the score matrix. Next, we introducethem in detail.",
  "Triplet Edge Scorer": "In the Power-Link, we propose to learn a Triplet Edge Scorer (TES) toleverage the entity and relation information in the KG. TES gives ascore to every edge in the computational graph. The score measuresthe importance of each edge in explaining the prediction of thetarget triplet. It can also be interpreted as the probability of theexistence of each edge for the explanation. Different from ,which only integrates node features to mark the edge scores, weconsider the local meaning of each edge triplet to the explanationof the target triplet. To be more precise, we combine the tripletembedding vectors of the local edge (,,) with the embeddingvectors of the explanation target , , . We use a Multi-LayerPerceptron(MLP) to process the combined features and generatethe edge score. The overall formula of the TES can be defined as:",
  "(,,, , , ) = (2)": "Euclidean. In the second strategy, we manually calculate the Eu-clidean norm of the difference between the corresponding head,relation, and tail embeddings of the edge triplet and the targettriplet. This results in a 3-dimensional vector. This strategy canbe more beneficial to the KGC models with energy-based scorefunctions. The Euclidean strategy can be written as:",
  "Path-Enforcing Learning": "In order to train the edge scorer to identify explanatory edges, wepropose a path-enforcing learning method relying on the simplifiedpowering process of the score matrix. Our method is simpler, faster,more efficient, more customizable, and more stable compared withPaGE-Link.We first briefly discuss the problem we find in the learning pro-cess of PaGE-Link. They optimize an explanatory weighted maskand enhance the path-forming explanations by simultaneously forc-ing the on-path (edges on the paths between the target entities)weights to increase and the off-path (edges not on the paths be-tween the target entities) weights to decrease. The potential pathsare selected using the shortest-path searching algorithm, whosecost matrix is designed based on the weighted mask and restrictionson the node degree. First, the shortest-path searching algorithm isnot parallelisable, which is slow when scaled to large KGs. Second,we argue that this optimizing strategy may introduce noise at theearly training stage. In the beginning, higher weights in the maskcan be assigned to trivial edges because of incomplete training,while meaningful edges are ignored. Therefore, when applyingshortest-path searching on the underfitting weighted mask, thealgorithm may strengthen the meaningless paths and weaken theimportant explanatory paths. This introduces noise that can bepropagated through epochs. We also observe that PaGE-Link oftengenerates similar explanations as the GNNExplainer, which is notequipped with a path-enforcing training strategy. We consider thisresult from the early perturbations in the training process, whichhinders the algorithm from finding explanatory paths.To alleviate the early perturbations, we replace the shortest-pathsearching with a graph-power-based algorithm that enhances pathsof specific lengths. The algorithm strengthens all on-path edgesduring the whole training process at a low cost of computationaltime. Different from the intuition of PaGE-Link, we find it unneces-sary to suppress the off-path edges during training. This also givesus room to significantly improve the efficiency of the algorithm.By doing so, instead of powering the whole matrix with increasingsparsity, we only need to compute the parallelisable multiplicationof a decreasing-sparsity row vector and a fixed-sparsity matrix. Themodel learns to balance the global explanatory performance andthe forcing of paths from the initial stage of the training processwithout being affected by the early noise. Next, we explain thealgorithm in detail.Path Enforcing. Intuitionally, the (,) element in the prob-ability adjacency matrix of power indicates the summation ofthe probabilities of all length- paths connecting nodes (,).Generally, we take the target element of the powered probabilityadjacency matrix and maximise it to strengthen all the edges onthe path between target nodes. Next, we explain the idea in detail.Given an extracted and pruned computational graph G = (E , R )with |E| = , we let R be the adjacency matrix of thecomputational graph. Let M = (), R be the probabilitymatrix obtained by scoring all edges in the computational graphwith TES. M can be considered as a weighted adjacency matrixM R . Let M be the edge probability of the target triplet, , , indicating the value at the targeting position (, ) of M.",
  ", = M(4)": "We suppose that we are interested in paths of length less than . Inorder to enhance paths of length (1 ), instead of poweringthe whole probability matrix, we use a simplification trick. Weonly power the target () row vector in the matrix, which we callthe power vector = M:, R1 . This is done by multiplyingM to by 1 times, yielding (). We illustrate the product ofthe power vector and a single column in M with Eq. (5). Whenmultiplying with the column of M, we are actually updatingthe value in , , with the sum of the probabilities of all pathsin length , connecting nodes ( ,).",
  "(2)4= (1)31 (1)14 + (1)32 (1)24 + (1)3 (1) 4(6)": "where indicates the probability of a single edge in the probabil-ity matrix M. The general equation of the powering process forupdating the whole power vector for iterations can be writtenas:() = (1) M1(7) We further normalize the power vector. We first divide it element-wisely by the number of paths between each node pair. This canbe obtained by simply dividing the power vector by the rowof the adjacency matrix of power 1. With the simplificationtrick, we only power the row vector . Similar to Eq. (7), wehave () = (1)1. We then elementwisely take the root of itby the path length . Now we have the average probability of theedge in the paths between the pair of nodes in each position of thematrix. The normalization process can be written as:",
  "We take the target element ()from the powered power vector,": "corresponding to the connection to the tail node in the targettriplet. The element represents the average probability of the edgeson the length paths between the target nodes (, ).We iterate the above process. After each round of powering,we record the () . Finally, we get the average probability of allon-path edges of length less than or equal to by averaging all the() , denoted by :",
  "L = log = 1 | = (M G ), , , (11)": "The Overall Loss. We combine the path loss and prediction lossby summation. Besides, we also add a regularisation term on themask (generated by TES), which is multiplied by a weight . Theregularisation term plays a crucial role in further concentrating theTES on important edges. The overall loss is defined as:",
  "Path Generation": "After we obtain the edge scorer trained on the target triplet, , , we perform a final computation of the edge score matrixM = (), R. The score matrix contains the explanatoryimportance of every edge to the target triplet. We take the inversevalues of the score matrix as the cost matrix and apply Dijkstrasalgorithm to obtain the shortest paths. The paths are the mostimportant paths supporting the prediction of the target triplet.",
  "P ( , , G , M)(13)": "Algorithm 1 describes the full process of Power-Link. The learn-ing process is highly parallelizable and can be accelerated withGPUs. Since the score matrix M and the adjacency matrix are usu-ally sparse for KGs, by utilizing the sparse matrix operations, weare able to enjoy even faster and memory-efficient computation.We also provide a complexity analysis of Power-Link and othermethods in the Experiments section.6Experiments6.1Experimental Setup Datasets. We evaluate Power-Link on the task KGC on the mostpopular datasets: FB15k-237 and WN18RR . Statistics ofdatasets can be referred to in the Appendix. We use the standardsplits for a fair comparison.Baselines. We compare Power-Link against three SOTA baselines.Two subgraph-based methods include GNNExplainer and PG-Explainer . One path-based method is PaGE-Link . Notethat GNNExplainer and PGExplainer are not designed to providepath-based explanations. To accommodate them to our task, wemodify them into GNNExplainer-Link and PGExplainer-Link byapplying Dijkstras Algorithm to their learned weighted masks toextract paths. We use these explanatory methods to explain thepredictions of three popular types of GNN-based KGC models. TheGNN encoders RGCN , WGCN and CompGCN arerespectively connected by the KGE decoders TransE, DistMult andConvE. This gives 9 KGC models for each method to explain.Implementation Details. For the sake of better generalization, wechoose the concatenation combination strategy in our experimentfor TES. We follow the common setting of only explaining edges thatthe KGC model considers to exist. We call these edges explainableedges. For the FB15K237 dataset, we consider the edges that the",
  "Algorithm 1 The overall algorithm for our proposed Power-Link": "Input: The KG G with nodes E, edges R and adjacency matrix , GNN-based KGC model trained on G, the parameteriser that estimatesthe importance of each edge for the explanation, a target triplet , , to be explained and the label , number of epochs for training theexplanation mask, the maximum length of the explanation paths.",
  ": end while18: return P ( , , G , M)": "KGC model assigns a score larger than 0.5 as explainable edges.We randomly sampled 500 explainable edges from the test set. TheWN18RR dataset is much sparser and contains fewer samples inthe test set. Just a few edges are scored higher than 0.5. Thus,we consider edges that have scores ranking at one as explainableedges and randomly sampled 200 target triplets. To ensure a faircomparison, target triplets to be explained from the same KGCmodel are identical for all explanation methods. We assume thatlonger paths contain less meaningful information for explanationand also increase the computational cost. Therefore, we choose apower order of 3 for the Power-Link throughout the experiments,which yields an enhancement on the explanatory paths of lengthless than or equal to 3. A more detailed experiment setup can befound in the Appendix.",
  "Evaluation Metrics": "Motivated by , we evaluate the learned explanation masks with+, , and . As the ranking difference isoften considered important in KG-related tasks, we compare thequality of the explanation paths by calculating the times that theranking of the target triplet drops after we delete the edges onthe paths. This is denoted as . The detailed definitions of themetrics are introduced below:Fidelity+ (F+). Given a target triplet , , and the computationalgraph G, the explanatory subgraph G is obtained by imposingthe corresponding explanation mask on the computational graph,denoted by G = M G. Let G be the output triplet score of theGNN-based KGC model propagating on G, e.g. G = ( , , , G).+ measures the soft score difference between the predictionon the computational graph G and the prediction on the computa-tional graph excluding the explanatory subgraph G\\. For test",
  "GPU Memory Usage During the Explaining Process": ": The GPU memory usage of Power-Link during theexplaining process against the number of edges in each graph.500 samples are explained. For better illustration, we onlyrecord the memory usage of the explaining module. Thememory occupied by the KGC model is ignored. Sparsity. As the Fidelity score is often positively correlated withSparsity, we consider Sparsity as one of our evaluation metrics. Inour experiment, we measured the soft sparsity of the explanatorymask. For a good explanation, the explanatory mask should behighly condensed on the meaningful edges, therefore renderinghigh sparsity.HR. Given a test triplet (,,), the computational graph G andthe explanatory path set P, we extract a test graph G by removingedges on the explanatory paths from the computational graph,denoted by G = {E, R }, R P. We let the GNN-based KGCmodel propagate on both the test graph and the computationalgraph. This gives two output scores of the same test triplet denotedby = (,,, G) and = (,,, G). measuresthe hit rate when the ranking of is smaller than . The hitindicates the ranking of the target triplet drops after we remove theexplanatory paths. A good path-based explanation should includeonly important and meaningful paths and thus cause confidencedrops to the model if we remove these paths, yielding a high hitrate. For test triplets, can be written as:",
  "Results and Analysis7.1KGC Explanation Results": "summarises the explanation results of Power-Link overthree comparable baselines. We can have the following observa-tions: (1) Our proposed Power-Link achieves consistently betterperformance over all baselines on the two representative datasets.This indicates the effectiveness of the Power-Link in generating : Runtime comparison between PaGE-Link and Power-Link. We use the two methods to explain 500 samples pre-dicted by WGCN-ConvE from FB15K237. The models are runon a single V100-32G GPU. Avg. indicates the average numberper graph.",
  "ModelF+F-SparsityHR:1HR:3HR:5": "RGCN-DistMult-MI0.3560.1680.8340.1730.3080.396WGCN-DistMult-MI0.3250.3970.6400.5990.6830.739CompGCN-DistMult-MI0.04110.02990.65560.0720.1260.194*RGCN-DistMult0.3770.1860.7840.2340.3540.432*WGCN-DistMult0.6460.2940.6720.6290.7720.836*CompGCN-DistMult0.0430.0250.5060.0920.1700.220 good explanations for the KGC task. The superiority is more obvi-ous on the FB15K237 KG, which is denser and more complicated.This reveals that the Power-Link can be more effective at explainingcomplex KGs than sparse KGs. (2) Both path-based methods PaGE-Link and Power-Link perform better than the two subgraph-basedmethods based on the path-oriented metrics. This aligns with theexpected impact of the path-enforcing learning scheme. (3) Thedifferent choices of score functions exhibit similar trends in allmethods. This indicates the stability of our Power-Link regardingscore function designs. (4) RGCN-based and WGCN-based mod-els achieve higher scores in all our metrics than CompGCN-basedones. We assume that it is generally easier to explain the GNNsthat strengthen the edge differences. This may be a future inspi-ration for designing explainability-reinforced GNNs for KGC. (5)The explanation of WN18RR is harder than FB15k-237, as we canobserve that the results on WN18RR are consistently lower. Weassume that this is mainly because of the smaller and sparser graphof WN18RR. As a result, the meaningful paths are shorter and fewer,which makes it harder to form interpretable explanations.",
  "Ablation Studies": "We conduct ablation studies using the KGC models trained on theFB15K-237 dataset to verify the impact of the design componentsin our proposed method.The path-enforcing module. As shown in , we observedan obvious drop in path explanation performance when the path-enforcing module was removed. This is as expected since there isno strengthening on the impact of paths when explanations aregenerated without the path-enforcing. We expect the gap to belarger for denser graphs.The combination method in TES. shows that using theEuclidean combination method yields better explanation perfor-mance than Concatenation on RGCN-based KGC models. The supe-riority is more obvious for RGCN-TransE, which uses the energy-based loss function during KGC training. On the contrary, Euclideandowngrades the performance on WGCN-based KGC models.",
  "Path-based Explanation for Knowledge Graph CompletionKDD 24, August 2529, 2024, Barcelona, Spain": "The mutual information loss. We test the impact of mutualinformation loss(MIL) in explaining the WGCN-Distmult model. shows that without the mutual information loss, there isa significant drop in the explanation performance. The MIL is thekey component that enables the explanation method to adjust theweighted mask of the KG according to the prediction of the KGCmodel. Without MIL, the path-enforcing module only strengthensthe edges on the paths between the head and tail nodes withoutconsidering their impacts on the models prediction.The order of powering. presents the explanation perfor-mance of Power-Link when we change the order of powering inthe path-enforcing module. Interestingly, we find that increasingthe powering order brings better performance and vice versa. Webelieve that increasing the powering order expands the range oflength of the paths that the module strengthens. This can enable thegeneration of more explanatory paths that contain more importantnodes. Even though, it is worth noting that longer paths may beless meaningful to the users, and higher powering order requiresmore computational power.Comparison with PaGE-Link on heterogeneous graphs. Sincethe path-enforcing learning module in Power-Link can also beapplied to heterogeneous graphs, for a better comparison, we re-produce the experiments in PaGE-Link with our proposed learningmodule. In both AugCitation and UserItemAttr datasets, Power-Link achieves an improvement of 0.05 on the AUC score and isaround 1.5 times faster. The result aligns with previous experi-ments, showing that Power-Link is more precise and efficient. Thespecific statistics can be found in the Appendix .",
  "RGCN-DistMult40.3670.1720.8030.2240.3760.440RGCN-DistMult20.1720.1750.9540.2000.2960.350*RGCN-DistMult30.3770.1860.7840.2340.3540.432": "explainers. We can find that only Power-Link explains why the KGCresult is factual by the most reasonable relationship _along both of the generated paths. All other baseline explainerincludes noisy edges (i.e., ,, ) that arerather irrelevant to the fact. We also find that PaGE-Link generatesthe same explanation paths as the GNNExplainer though the latteris not path-enforced. We attribute this to the noise introduced atthe early training stage of PaGE-Link.Runtime comparison. We compare the runtime of Power-Linkand PaGE-Link in . Power-Link is 3.18 times faster in totalruntime and 3.37 times faster in average runtime per graph thanPaGE-Link. This shows that the parallelisability of Power-Linksignificantly enhances the speed of the explaining process.GPU memory usage. In , we illustrate the GPU memoryusage during the explaining process against the number of edges.We can observe that the memory usage(in MB) is linearly relatedto the edge number, with a slope of around 0.01. When explaininga graph of up to 140k edges, Power-Link only takes up around 2GBGPU memory. This proves the excellent scalability of Power-Link.Human evaluation. We conduct a human evaluation to test theability of the explaining methods to improve the transparency andinteroperability of the KGC models. We randomly selected 100 sam-ples from the predictions of the WGCN-ConvE model on FB15K237.We design a user interface with a layout similar to . Theparticipants include AI researchers in NLP and recommendationsystems with limited knowledge of KG and GNNs. They are askedto select the best explanation among the given 4, where multiplechoices are allowed. 300 evaluations are collected, among which64.7% consider Power-Link as the best, 55.0% support PaGE-Link,58.7% support PGExp-Link and 56.7% support GNNExp-Link. Thehuman evaluation further shows the superiority of Power-Linkover other baseline methods.",
  "Conclusion": "In this paper, we propose Power-Link that generates explanatorypaths via a novel simplified graph-powering technique. Based onGNNs, Power-Link is the first path-based explainer for KGC tasks.Our approach sheds light on the direction of model transparency onwidely used KGs, especially given the practical importance of KGsin industrial deployment. Extensive experiments demonstrate bothquantitatively and qualitatively that Power-Link outperforms SOTAexplainers in terms of interpretability, efficiency, and scalability.We hope our work can inspire future research to design betterGNN-based frameworks that enhance the explainability of KGCmodels.",
  "Heng Chang, Jie Cai, and Jia Li. 2023. Knowledge Graph Completion withCounterfactual Augmentation. In Proceedings of the ACM Web Conference 2023.26112620": "Heng Chang, Yu Rong, Tingyang Xu, Yatao Bian, Shiji Zhou, Xin Wang, JunzhouHuang, and Wenwu Zhu. 2021. Not All Low-Pass Filters are Robust in GraphConvolutional Networks. Advances in Neural Information Processing Systems(NeurIPS) 34 (2021). Heng Chang, Yu Rong, Tingyang Xu, Wenbing Huang, Somayeh Sojoudi, JunzhouHuang, and Wenwu Zhu. 2021. Spectral graph attention network with fast eigen-approximation. In Proceedings of the 30th ACM International Conference onInformation & Knowledge Management (CIKM). 29052909. Heng Chang, Yu Rong, Tingyang Xu, Wenbing Huang, Honglei Zhang, PengCui, Wenwu Zhu, and Junzhou Huang. 2020. A restricted black-box adversarialframework towards attacking graph embedding models. In Proceedings of theAAAI conference on Artificial Intelligence (AAAI), Vol. 34. 33893396.",
  "Jia Li, Yongfeng Huang, Heng Chang, and Yu Rong. 2022. Semi-supervisedhierarchical graph classification. IEEE Transactions on Pattern Analysis andMachine Intelligence 45, 5 (2022), 62656276": "Dongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen,and Xiang Zhang. 2020. Parameterized Explainer for Graph Neural Network. InAdvances in Neural Information Processing Systems, H. Larochelle, M. Ranzato,R. Hadsell, M. F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 1962019631. Linhao Luo, Yixiang Fang, Xin Cao, Xiaofeng Zhang, and Wenjie Zhang. 2021.Detecting communities from heterogeneous graphs: A context path-based graphneural network model. In Proceedings of the 30th ACM international conferenceon information & knowledge management. 11701180. Andrea Rossi, Donatella Firmani, Paolo Merialdo, and Tommaso Teofili. 2022.Explaining link prediction systems based on knowledge graph embeddings. InProceedings of the 2022 international conference on management of data. 20622075. Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, IvanTitov, and Max Welling. 2018. Modeling relational data with graph convolutionalnetworks. In European semantic web conference. Springer, 593607. Chao Shang, Yun Tang, Jing Huang, Jinbo Bi, Xiaodong He, and Bowen Zhou.2019. End-to-end structure-aware convolutional networks for knowledge basecompletion. In Proceedings of the AAAI Conference on Artificial Intelligence,Vol. 33. 30603067.",
  "Zhanqiu Zhang, Jie Wang, Jieping Ye, and Feng Wu. 2022. Rethinking GraphConvolutional Networks in Knowledge Graph Completion. In Proceedings ofthe ACM Web Conference 2022. 798807": "Dong Zhao, Guojia Wan, Yibing Zhan, Zengmao Wang, Liang Ding, ZhigaoZheng, and Bo Du. 2023. KE-X: Towards subgraph explanations of knowledgegraph embedding based on knowledge information gain. Knowledge-BasedSystems (2023), 110772. Zhaocheng Zhu, Xinyu Yuan, Louis-Pascal Xhonneux, Ming Zhang, MaximeGazeau, and Jian Tang. 2022. Learning to Efficiently Propagate for Reasoning onKnowledge Graphs. arXiv preprint arXiv:2206.04798 (2022). Zhaocheng Zhu, Zuobai Zhang, Louis-Pascal Xhonneux, and Jian Tang. 2021.Neural bellman-ford networks: A general graph neural network framework forlink prediction. Advances in Neural Information Processing Systems 34 (2021).",
  "AComputational complexity": "In , we summarize the time complexity of Power-Link andrepresentative existing methods for explaining a prediction withcomputation graph G = (E, R) on a full graph G = (E, R). Letbe the mask learning epochs. GNNExplainer has complexity |R |as it learns a mask on R. PGExplainer has a training stage thatcovers edges in the entire graph and thus scales in (|R|). KE-Xadopts the subgraph-based approach for explanation and has similartime complexity as SubgraphX , which is (|E | 22). isthe maximum degree in G and is a manually chosen budget fornodes. For PaGE-Link, its complexity consists of two parts: linearcomplexity in |R | for the k-core pruning step and |R ||E | +|R | for the mask learning with Dijkstras algorithm and sparsematrix multiplication step. For our Power-Link, the graph poweringstep involves a vector and a sparse matrix multiplication for times for a -length path, therefore, has complexity |R |. Thek-core pruning has the same complexity, but we only need to do theshortest path searching once. Therefore, the total time complexity ofPower-Link is (|R | + |R |). Therefore, Power-Link has bettertime complexity over PaGE-Link and both are better than existingmethods since |R | is usually smaller than |R |. Both PaGE-Linkand Power-Link could converge faster, i.e., has a smaller , due tothe smaller space of candidate explanations from paths.",
  "NotationDefinition and description": "G = (E, R)a KG G, entity set E, and relation set RMThe explanatory mask contains scores for each edge in the graph.(,,)a (head, relation, tail) tripletthe adjacency matrix of KG GPthe set of pathsPthe set of paths of length L connecting a pair of nodes (, )(, )the GNN-based KGC model to explain : E R E Rthe score function used for embedding learning, , the triplet that is predicted to be fact by the KGC modelthe power vector which is the row vector in Mthe adjacency power vector which is the row vector in , the representation of entity and relation = (G, , , )the KGC prediction of the target triplet , , G = (E, R)the computation graph, i.e., L-hop ego-graph of , , G\\ = (E\\, R\\)the computational graph but excluding the subgraph"
}