{
  "ABSTRACT": "Graphs are a representation of structured data that captures therelationships between sets of objects. With the ubiquity of avail-able network data, there is increasing industrial and academic needto quickly analyze graphs with billions of nodes and trillions ofedges. A common first step for network understanding is GraphEmbedding, the process of creating a continuous representationof nodes in a graph. A continuous representation is often moreamenable, especially at scale, for solving downstream machinelearning tasks such as classification, link prediction, and cluster-ing. A high-performance graph embedding architecture leverag-ing Tensor Processing Units (TPUs) with configurable amountsof high-bandwidth memory is presented that simplifies the graphembedding problem and can scale to graphs with billions of nodesand trillions of edges. We verify the embedding space quality onreal and synthetic large-scale datasets.",
  "graph embedding, scalable algorithms, tensor processing units": "ACM Reference Format:Brandon A. Mayer, Anton Tsitsulin, Hendrik Fichtenberger, Jonathan Hal-crow, and Bryan Perozzi. 2023. HUGE: Huge Unsupervised Graph Embed-dings with TPUs. In Proceedings of the 29th ACM SIGKDD Conference onKnowledge Discovery and Data Mining (KDD 23), August 610, 2023, LongBeach, CA, USA. ACM, New York, NY, USA, 11 pages.",
  "KDD 23, August 610, 2023, Long Beach, CA, USA 2023 Copyright held by the owner/author(s).ACM ISBN 979-8-4007-0103-0/23/08": ": HUGE can learn representations on extremely largegraphs (billions of nodes) at Google. (Shown here: t-SNE pro-jection of HUGE-TPUs Papers100M embeddings)Graph can greatly vary in size in industrial applications, they oftentimes grow to billions of nodes and trillions of edges in size. Makingintelligent automated decisions with such large scale graphical datasets is extremely compute and storage intensive, making these taskshard or impossible to solve using commodity hardware.Graph embeddings1 are a common first step in graph under-standing pipelines where every node in the graph is embeddedinto a common low-dimensional space. These embeddings are thenused for graph learning tasks such as node classification, graph clus-tering, and link prediction that can be solved with standardmachine learning algorithms applied in the graph embedding spacewithout having to develop specific algorithms to directly exploit thegraph structure. For example, approximate nearest neighbor searchsystems can serve product recommendations using embeddingsof user-item graphs.Numerous methods, which we briefly review in .1, havebeen proposed in the literature. One of the most popular meth-ods, DeepWalk , proposes to embed nodes via a shallow neu-ral network trained to discriminate samples generated from ran-dom walks from random negatives. Unfortunately, the process ismemory-bound, as it requires random accesses for both randomwalk generation and updating the embedding table. At first glance,it does not scale beyond graphs larger than a couple of millions ofnodes even in the distributed setting.Or does it? Distributed data processing workflows (such asFlume or Apache Beam) can be used to execute sampling strate-gies even for graphs that are too large to represent in memory on a",
  "KDD 23, August 610, 2023, Long Beach, CA, USABrandon A. Mayer, Anton Tsitsulin, Hendrik Fichtenberger, Jonathan Halcrow, and Bryan Perozzi": "the completion of a single step, gradients across the replicas are ag-gregated and all variable copies are updated synchronously. Whilethis can accelerate computationally heavy workloads, compared tothe parameter server architecture, this design has limited use in thecontext of Graph Embedding. Replicating embedding tables acrossmultiple machines introduces unwanted redundancy and memoryconsumption. 2.4.3TPUStrategy and Accelerated TPU Embedding Tables. Train-ing a model (or graph embedding) in TensorFlow using TPU hard-ware, the TPUStrategy is very similar to the MultiWorkerMirrored-Strategy. A user defines a desired TPU topology, a slice of a PODthat can be thought of as a subset of interconnected processingunits. Under the TPUStrategy, trainable variables are copied toall TPU replicas and large batches of examples are divided intosmaller per-replica batches and distributed to available replicas andgradients are aggregated before a syncronous update. Normally,this distribution paradigm would limit the scalability of modelsthat define large embedding tables. However, TPUs are capable ofsharding embedding layers over all devices in an allocated topologyand leverage high bandwidth interconnections between replicasto support accelerated sparse look-ups and gradient updates. Ac-celerated embedding tables are exposed in TensorFlow using thetf.tpu.experimental.embedding.TPUEmbedding (TPUEmbedding)layer and are the primary mechanism for scaling DeepWalk trainingon TPUs.",
  "BACKGROUND": "In this section, we first briefly review the related work in .1.We review DeepWalk , which we use as a base for our high-performance embedding systems, in .2. We then proceedwith describing two architectures for scaling Deepwalk graph em-bedding using commodity (HUGE-CPU) and TPU (HUGE-TPU)hardware that allow us to scale to huge graphs in .3.",
  "Related Work": "We now proceed to review the two basic approaches to embeddinglarge graphs. Over the past years, tremendous amount of workintroduced various embedding methods as well as a myriad of tech-niques and hardware architectures to scale them up. We summarizethe related work in terms of the embedding approach, speeduptechniques, and its expected scalability in . 2.1.1Graph Embedding Approaches. We categorize embeddingmethods as either neural network-based or matrix factorization-based. Regardless of the approach, each method employs, some-times implicitly, a similarity function that relates each node to othernodes in the graph. The best-performing methods depart from justusing the adjacency information in the graph to some notion ofrandom walk-based similarity, for example, personalized PageR-ank (PPR) . A key insight for accelerating the computation of",
  "Open source implementation available at:": "these similarities is that they are highly localized in the graph ,meaning 23 propagation steps are enough to approximate them.Neural embedding methods view node embeddings as parame-ters of a shallow neural network. Neural methods optimize these pa-rameters with stochastic gradient descent for either adjacency ,random walk , or personalized PageRank similarity func-tions. This optimization is done via sampling, and the updates tothe embedding table are usually very sparse. Thus, random memoryaccess typically bounds the performance of these methods.An alternative to gradient-based methods is to directly factorizethe similarity matrix. There are deep connections between neuraland matrix factorization approaches essentially, for manynode similarities the optimal solutions for neural and factorization-based embeddings coincide. The main challenge to matrix-basedmethods is maintaining sparsity of intermediate representations.For large graphs, one can not afford to increase the density of theadjacency matrix nor keep too many intermediate projections. 2.1.2Scaling Graph Embedding Systems. There are several direc-tions for speeding up embedding algorithmssome are tailored toparticular methods while some are more general. We now briefly re-view the most general speedup techniques. Graph coarsening iteratively contracts the graph, learns the embeddings for the mostcompressed level, and deterministically propagates the embed-dings across the contraction hierarchy. Graph partitioning meth-ods distribute the computation across machines while at-tempting to minimize communication across machines.Early approaches to matrix factorization attempt tosparsify the random walk or PPR matrices. Unfortunately, higher-order similarity matrices are still too dense for these embeddingsmethods to scale to very large graphs. Leveraging specialized sparsenumerical linear algebra techniques proved to be a more fruitfulapproach. Implicit solvers can factorize the matrix with-out explicitly materializing it in memory. These methods are con-strained to perform linear decomposition, which is not able tosuccessfully account for structure of graphs.Two families of techniques that produce most scalable embed-ding methods are spectral propagation and matrix sketching .Spectral propagation methods first compute some truncatedeigendecomposition of the adjacency or the Laplacian matrix of agraph and then use these eigenvectors to simulate the diffusion ofinformation. Matrix sketching approaches approximate the similar-ity matrix, either iteratively or in a single pass . Thelatter option is more scalable. 2.1.3Hardware-based Embedding Acceleration. Compared to al-gorithmic advances, hardware-based acceleration has arguably re-ceived less attention. Zhu et al. proposes a hybrid system thatuses CPU for sampling and GPU for training the embeddings. Sincethe most RAM a single GPU can offer is in the order of 100 gigabytes,one can only train embeddings of 100 million node graphs on suchsystems. Wei et al. , Yang et al. address this problem withpartitioning to include more GPUs. This approach requires tensof GPUs for a billion-node graph, which is prohibitive comparedto scalable CPU-based systems, which can embed a billion-nodegraph on a single high-memory machine in hours.Efficient computation of higher-order similarity is one aspectwhere hardware acceleration is currently lacking. Wang et al. ,",
  "DeepWalk": "Before describing our TPU embedding system, it is necessary toreview DeepWalk , which is the basic method for neural graphembedding. DeepWalk adapts word2vec , a widely successfulmodel for embedding words, to graph data. DeepWalk generatesa corpus of short random walks; the objective of DeepWalk isto maximize the posterior probability of observing a neighboringvertex in a random walk within some specific window size. To max-imize this probability efficiently, it uses hierarchical softmax ,which constructs a Huffman tree of nodes based on their frequencyof appearance, or a more computationally efficient approximation,negative sampling . For each node that was observed within thewindow size from some node, DeepWalk picks uniformly atrandom as contrastive negative examples.There are several computational problems with DeepWalks ar-chitecture, which are to be solved if we are to scale DeepWalk tographs with billions of nodes:",
  "We proceed with briefly reviewing TPU architecture highlightingthe aspects critical for our graph embedding system. A detailedreview can be found in . TPUs are dedicated co-processors": "optimized for matrix and vector operations computed at half pre-cision. TPUs are organized in pods, which3 can connect a total of4096 of TPU chips with 32 GiB memory each, which together makesup to 128 TiB of distributed memory available for use. TPUs chipsinside a pod are connected with dedicated high-speed, low-latencyinterconnects organized in a 3D torus topology.",
  "Common ML Distribution Strategies": "Various methods for distributing Machine Learning workloads havebeen discussed in the literature and most Machine Learning(ML) frameworks provide consistent APIs implementing multipledistribution schemes through a consistent interface. This sectionhighlights some common distribution paradigms focusing on thetechniques used to scale DeepWalk using commodity hardware(which we refer to as HUGE-CPU) and TPUs (HUGE-TPU).TensorFlow provides the tf.distribute.Strategy abstractions toenable users to separate model creation from the training runtimeenvironment with minimal code changes. Two common strategiesare the Parameter-Server (PS) strategy and Multi-Worker MirroredStrategy. 2.4.1Parameter-Server Strategy. In the context of graph embed-ding, using a PS strategy is useful for representing a large embed-ding table. The PS strategy defines two compute pools of potentiallyheterogeneous hardware that the user can access. One pool con-tains machines labeled \"parameter servers\" and the other poolsmachines are named \"workers\". A models trainable variables aresharded across the machines in the parameter-server pool whichserve requests, potentially over a network, both for the values ofthese variables and to update them. For graph embedding, machinesin the worker pool asynchronously receive batches of examples,fetch the necessary embedding rows from parameter servers overa network, compute gradients and push updates back to parameterserver machines. 2.4.2Multi-Worker Mirrored Strategy. The Multi-Worker MirroredStrategy replicates all variables in the model on each device in auser defined pool of worker machines. A (potentially) large batch ofinput examples is divided among the multiple workers and proceedto compute gradients using their smaller per-replica batches. At",
  "METHOD": "We scale the DeepWalk algorithm to embed extremely large-scalegraphs using two methods. The first, called HUGE-CPU, uses onlycommodity hardware whereas the second, HUGE-TPU, leveragesmodern TPUs for increased bandwidth and performance gains. visualizes the parameter-server architecture of HUGE-CPU. The details of parameter-server architecture are covered insection 3.3.1. illustrates the TPU system design behindHUGE-TPU and is detailed in section 3.3.2.",
  "Preprocessing": "One key observation is that most positional graph embedding sys-tems cannot generate useful embeddings for nodes with less thantwo edges. Specifically, nodes with no edges are generally not welldefined by embedding algorithms, and similarly, positional embed-dings of nodes with only one edge are totally determined by theembedding of their single neighbor. Therefore, we typically prunethe input graph, eliminating nodes with degree less than two. Inour experiments, we only prune once though the pruning operationitself may introduce nodes that fall below the degree threshold.",
  "Sampling": "After preprocessing the graph, we run random walk sampling togenerate co-occurrence tuples that will be used as the input to thegraph embedding system.A high-level overview of the distributed random walk samplingis provided in Algorithm 1. The input to the sampling componentis the preprocessed graph and the output are TensorFlow Examplescontaining co-occurrence tuples extracted from the random walks. The implementation of the distributed random walk sampling algo-rithm is implemented using the distributed programming platformFlumeC++ .In the initialization phase, the distributed sampler takes as inputthe N nodes of the graph and replicates them times each to createthe seeds of |N| walks it will generate (Line 1). Next, the randomsampling process proceeds in an iterative fashion, performing joins which successively grow the length of each random walk(Lines 2-4). Each join combines the walk with the node at its endpoint. 4 After joining the end of the walk with its correspondingnode from the graph , sampling of the next node occurs (Line 4).We note that many kinds of sampling can be used here to selectthe next node at this step including uniform sampling, randomwalks with backtracking, and other forms of weighted sampling.For the results in this paper, we consider the case of using uniformsampling. A final GroupBy operation is used to collapse the randomwalks down to co-occurrence counts between pairs of nodes as afunction of visitation distance (Line 7).The output of the sampling pre-processing step is a sharded seriesof files encoding a triple: (source_id, destination_id, co_counts).source_id is the node ID of a starting point in the random walk, thedestination_id is a node ID that was arrived at during the ran-dom walks and co_counts is a histogram of length walk_lengthcontaining the number of times the source_id encountereddestination_id (indexed by the random walk distance of the co-occurrence).The DeepWalk model defines a graph reconstruction loss thathas a positive and negative component. The positive examplesare random walk paths that exist in the original graph. Negativeexamples are paths that do not exist in the original graph. If de-sired, the sampling step can be used to generate different varietiesof negative samples (through an additional distributed samplingalgorithm focusing on edges which do not exist). However, in prac-tice, we frequently prefer to perform approximate random negativesampling on-the-fly while training.",
  "Distributed training": "3.3.1HUGE-CPU. outlines the system design for theHUGE-CPU baseline system architecture. This system leverages dis-tributed training with commodity hardware. Two pools of machinesare defined as described in 2.4.1, a cluster of parameter-servers anda pool of workers. During initialization, trainable variables such asthe large embedding table are sharded across the machines in theparameter-server pool. Workers distribute and consume batchesof training examples from the output of the graph sampling pre-processing step, asynchronously fetch embedding activations fromparameter servers, compute a forward pass and gradients and asyn-chronously push gradient updates to the relevant activations backto the parameter servers. There is no locking or imposed order ofactivation look-ups or updates. This enables maximum through-put of the system but comes at the cost of potentially conflictinggradient updates.",
  "Parameter": "ServerAAA : System diagram for the Parameter-Server (CPU)based DeepWalk model (HUGE-CPU). Two pools of machinesare defined, parameter-servers and workers. Workers asyn-chronously fetch batches of training examples from diskand collect relevant embedding activations from parametersservers that serve requests for the sharded embedding table.Gradients are computed and updated asynchronously. after the sampling procedure is complete. The replication strategyused for TPUs in conjunction with their high FLOPS per second re-quires generating extremely large batches of training examples forevery step. The bottleneck in this system is rarely the embeddinglookup or model tuning but the input pipeline to generate the largebatch size required at every step.File shards of the sampling data are distributed over the workersin a cluster dedicated to generating input data. The workers inde-pendently deserialize the co-occurrence input data and augmentthe source_id and destination_id pairs with negative samples, repli-cating source_id and randomly sampling additional destination_idnode IDs uniformly from the embedding vocabulary.",
  "Tensors": ": System diagram for accelerated HUGE unsuper-vised graph embedding. A large embedding table is efficientlysharded over the TPU HBM using TensorFlow TPUEmbed-ding layer. A cluster of machines that read, parse and ran-domly sample the input data is leveraged to avoid an inputbottleneck. This diagram is illustrative and does not repre-sent the true connectivity of the TPU topology. : Parameters used for all HUGE-TPU and HUGE-CPUexperiments. LWSGD is Stochastic Gradient Descent witha Linear Warmpup and decay learning rate schedule. Theschedule is parameterized by four numbers, the number ofwarmup steps, the final value after warmup, the number ofdecay steps and the final value after the decay phase at whichpoint the learning rate is held constant.",
  "EXPERIMENTS4.1Experimental Details": "4.1.1Datasets. For testing the scalability of our methods, we resortto random graphs. We resort to the standard (degree-free) StochasticBlock Model , which is a generative graph model that divides vertices into classes, and then places edges between two vertices and with probability determined from the class assignments.Specifically, each vertex is given a class {1, . . . ,}, andan edge {, } is added to the edge set with probability ,",
  "Friendster65.6M3612MOGB-Papers100M111M1616MSBM-10M10M100MSBM-100M100M1000MSBM-1000M1000M10000M": "where is a symmetric matrix containing the between/within-community edge probabilities. Assortative clustering structure ina graph can be induced using the SBM by setting the on-diagonalprobabilities of higher than the off-diagonal probabilities. Forbenchmarking, we set = iff = and to otherwise.Complementing our analysis on synthetic benchmark datasets,we also study the performance of the methods on two large real-world graphs: Friendster and OGBN-Papers100m . Wereport the dataset statistics in . 4.1.2Baselines. First we compare HUGE-CPU and HUGE-TPUwith other state-of-the-art scalable graph embedding algorithms: In-stantEmbedding , PyTorch-BigGraph and LightNE onan end-to-end node classification task using the OGBN-Papers100mdataset. We further explore the embedding space quality of each al-gorithm using both the OGBN-Papers100m and Friendster datasets.Finally we compare embedding space quality metrics as a functionof training time to explore the speedups of HUGE-TPU compared toHUGE-CPU using a randomly generated graphs with 100M (SBM-100M) and 1B nodes SBM-1000M.",
  "Parameters for HUGE methods": "shows the parameters used by the HUGE-CPU and HUGE-TPU methods. The random walk sampling procedure describe in 3.2was executed sampling = 128 walks per node with a walk lengthof = 3. The set of samples were shared for all experiments involv-ing HUGE-CPU and HUGE-TPU to minimize the affect of randomsampling on the results. num_neg_per_pos is the number of ran-dom negative destinations sampled for every \"positive\" exampledrawn from the sampling pre-processing step. The global batch sizefor HUGE-TPU may be computed as per_replica_batch_size (1 + num_neg_per_pos). A step is not well defined for the HUGE-CPU algorithm since workers asynchronously pull variables andpush updates. Due to the increased computational power and highbandwidth interconnections between replicas, HUGE-TPU achievesa much higher throughput and global per-step batch size. Trainingwith extremely large batch sizes can be challenging. We have foundthat a Stochastic Gradient Descent (SGD) optimizer with a linearwarmup and ramp down gives good results. HUGE-CPU was alsotrained with a SGD optimizer but uses a fixed learning rate.",
  "PyTorch-BigGraph43.6423.016x A100 GPUsLightNE27.9040.8160 vCPUsInstantEmbedding53.153.564 vCPUsHUGE-CPU56.0315120 vCPUsHUGE-TPU56.139.94x4x4 v4 TPUs": "publicly available large graphsin our practical experience, often-times there is need to evaluate and compare different embeddingmodels in an unsupervised fashion. We propose simple unsuper-vised metrics to compare the embedding quality of different embed-dings of a graph. For the analysis, we 2-normalize all embeddings.We also report four self-directed metrics for evaluation we usein our production system to monitor the embedding quality. First,edge signal-to-noise ratio (edge SNR) defined as:",
  "E, [(, )] ,": "where we approximate the numerator term by taking a randomsub-sample of all non-edges. In our experiments, we also showthe entire distribution of edge- and non-edge distances. Theintuition behind these metrics is that the distance between nodesthat are connected in the original graph (a true edge) should becloser than nodes that are not adjacent in the input graph. Last, wecompute the sampled version of the edge recall . We sample100 nodes, pick closest nodes in the embedding space forming aset . Then, sampled recall is:",
  "Downstream Embedding Quality": "Being the fastest embedding system is not enough we also wantembedding vectors to be as useful as possible. Every percentagepoint of quality on downstream tasks directly translates to missedmonetary opportunities. Therefore, in our experience, when work-ing on scalable versions of algorithms, it is critical to maintain highembedding quality.To that end, we provide one of the first studies of embeddingscalability and performance across different hardware architectures.We compare with the fastest single-machine CPU embedding avail-able and industrial-grade GPU embedding system . Notethat these systems have a much more restrictive limit for a maxi-mum number of nodes that they can process. PyTorch-BigGraphcan not process graphs with more than 1.4 109 nodes on thecurrent hardware, assuming a system with the latest-generationGPUs and highest available memory. LightNE does not have suchrestriction, but it keeps both the graph and the embedding table in",
  ": Embedding analysis results for SBM-1000M to explore the embedding space quality as a function of training time forHUGE-CPU compared to HUGE-TPU and InstantEmbedding": "memory. Because of that, scalability into multi-billion node embed-ding territory is still an open question for that system. presents the embedding quality results on the OGB-Papers100M dataset. For measuring the embedding quality, wefollow the Open Graph Benchmark evaluation protocol with asimple logistic regression model. We skip tuning the model param-eters on the validation set, and report the accuracy of predictionson the test set. We also report the relative speedup over the CPUDeepWalk embedding implementation. HUGE-TPU is the only onethat maintains the end-to-end classification quality provided byDeepWalk and improves runtime efficiency relative HUGE-CPU byan order of magnitude.",
  "Self-directed Embedding Space Evaluation": "To better understand the differences in downstream model per-formance, we present our self-directed metric for datasets withno ground-truth labels. We analyze the embedding space qualitywith the proposed metrics comparing HUGE-CPU, HUGE-TPU,InstantEmbedding, PyTorch-BigGraph and LightNE. To that end,we report the results on 2 real and 2 synthetic datasets, presentedin Figures 4-5 and 6-7, respectively.Interestingly, the results are fairly consistent across all datasetsconsidered. We see that HUGE-TPU provides superior separationbetween the distributions of edges and non-edges, achieving avery high edge signal to noise ratio. We also see that the samplededge recall metric on is generally much harder to optimize foron very large graphs, and that HUGE-TPU meets or exceeds theperformance of its comparable baseline HUGE-CPU.",
  "Visualization": "In order to better understand our embeddings, we frequently resortto visualizations. shows a plot of the entire embeddingspace of OGBN-Papers100M dataset for HUGE-TPU and LightNE,projected via t-SNE . Compared to HUGE-TPU, the LightNEembedding demonstrated surprisingly poor global clustering struc-ture, which explains its subpar downstream task performance wecovered in .4. : The average examples per seconds processed byHUGE-CPU and HUGE-TPU for all reported experiments.HUGE-CPU used 128 Parameter Servers and 128 Workerswith 20 cores each. HUGE-TPU was configured with a v4 in a64 chip configuration. We report the total number of exam-ples processed per second relative to HUGE-CPU",
  "Discussion": "While both HUGE-CPU and HUGE-TPU can horizontally scaleaccording to the user configuration, we use the same topologiesthroughout all experiments. HUGE-CPU uses 128 Parameter Servermachines and 128 Workers with 20 cores each and 2GiB of RAM.HUGE-TPU uses a v4 TPU with 64 replicas. The total trainingexamples processed by HUGE-TPU relative to HUGE-CPU for thisconfiguration is shown in table 5. Since the throughput of HUGE-CPU and HUGE-TPU is fixed for a given topology and batch size,the throughput is constant thought all experiments.As shown in table 4, HUGE-TPU achieves the highest accu-racy in the end-to-end node classification task using the OGBN-Papers100m dataset though HUGE-CPU not far behind. However,while HUGE-CPU is able to scale horizontally to handle extremelylarge embedding spaces, in-memory and hardware acceleratedachieve orders of magnitude speedups compared to HUGE-CPU.When analyzing the embedding space quality metrics however,in terms of HUGE-TPU consistently achieves superior performance.The distribution of distances between adjacent nodes for HUGE-TPU is typically much smaller than the other methods as is reflectedby an SNR that is consitently orders of magnitude larger thanother methods. The consistently high SNR is probably due to theextremely high throughput compared with HUGE-CPU.To further explore the affect of throughput on the system, weran HUGE-CPU for multiple fixed number of training steps: 3B,12B and 36B, while fixing the TPU training time on the SBM-100M",
  "HUGE: Huge Unsupervised Graph Embeddings with TPUsKDD 23, August 610, 2023, Long Beach, CA, USA": ": Visualization of the entire embedding space of (left) HUGE-TPU and (right) LightNE embeddings of the Papers100Mdataset, projected via t-SNE with the same parameters. Colors indicate point density. We can observe much better clusteredstructure in HUGE-TPU embeddings which directly translates to significantly better downstream prediction quality. and SBM-1000M datasets. In relative terms, HUGE-CPU-3B tookapproximately half the time of HUGE-TPU, HUGE-CPU-12B wastrained for 2x the time of HUGE-TPU and HUGE-CPU-36B wastrained for 6x the allowed time of HUGE-TPU. We also comparethese results with InstantEmbedding to contrast the Deepwalk styleembeddings with a matrix factorization graph embedding method.Predictably, the results show that the HUGE-CPU will \"converge\" orat least approach, over time, the performance of InstantEmbeddingin terms of edge/non-edge distributions and recall. However, HUGE-TPU consistently outperforms both InstantEmbedding and HUGE-CPU in terms of SNR and edge and non-edge distance distributionseven when HUGE-CPU is allowed to train for more than 6x moretime than HUGE-TPU.To summarize, we comprehensively demonstrate the quality andperformance of HUGE-TPU over HUGE-CPU as well as state-of-the-art industrial-grade systems for graph embeddings. First, weshowed that on a largest-scale labelled embedding data, HUGE-TPU achieves state-of-the-art performance while being order ofmagnitude faster than comparable CPU-based system. We thenproceed with unsupervised embedding evaluations we use in de-ployed production systems at Google. We show how HUGE-TPUis competitive in embedding quality over both real and synthetictasks, only improving its performance compared to the baselinesas the size of the graphs increases.",
  "In this work we have examined the problem of scalable graphembedding from a new angle: TPU systems with large amounts of": "shared low-latency high-throughput memory. We build a system(HUGE) that does not suffer from key performance issues of theprevious work, and greatly simplifies the system design. HUGEis deployed at Google, in a variety of different graph embeddingapplications. Our experiments demonstrate the merits of usingaccelerators for graph embedding. They show that the HUGE-TPUembedding is competitive in speed with other scalable approacheswhile delivering embeddings which are more performant. In fact,the embeddings learned with HUGE-TPU are of the same qualityas running the full embedding algorithm (with no compromises forits speed).",
  "Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, BowenLiu, Michele Catasta, and Jure Leskovec. 2020. Open graph benchmark: Datasetsfor machine learning on graphs. NeurIPS (2020)": "Mubashir Imran, Hongzhi Yin, Tong Chen, Yingxia Shao, Xiangliang Zhang, andXiaofang Zhou. 2020. Decentralized embedding framework for large-scale net-works. In International Conference on Database Systems for Advanced Applications.Springer, 425441. Norman P. Jouppi, Doe Hyun Yoon, Matthew Ashcraft, Mark Gottscho, Thomas B.Jablin, George Kurian, James Laudon, Sheng Li, Peter Ma, Xiaoyu Ma, ThomasNorrie, Nishant Patil, Sushma Prasad, Cliff Young, Zongwei Zhou, and DavidPatterson. 2021. Ten Lessons From Three Generations Shaped Googles TPUv4i :Industrial Product. In 2021 ACM/IEEE 48th Annual International Symposium onComputer Architecture (ISCA). 114. Norman P. Jouppi, George Kurian, Sheng Li, Peter Ma, Rahul Nagarajan, LifengNai, Nishant Patil, Suvinay Subramanian, Andy Swing, Brian Towles, Cliff Young,Xiang Zhou, Zongwei Zhou, and David Patterson. 2023. TPU v4: An OpticallyReconfigurable Supercomputer for Machine Learning with Hardware Supportfor Embeddings. arXiv:2304.01433 [cs.AR] Norman P. Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal,Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, Rick Boyle,Pierre luc Cantin, Clifford Chao, Chris Clark, Jeremy Coriell, Mike Daley, MattDau, Jeffrey Dean, Ben Gelb, Tara Vazir Ghaemmaghami, Rajendra Gottipati,William Gulland, Robert Hagmann, C. Richard Ho, Doug Hogberg, John Hu,Robert Hundt, Dan Hurt, Julian Ibarz, Aaron Jaffey, Alek Jaworski, AlexanderKaplan, Harshit Khaitan, Andy Koch, Naveen Kumar, Steve Lacy, James Laudon,James Law, Diemthu Le, Chris Leary, Zhuyuan Liu, Kyle Lucke, Alan Lundin,Gordon MacKean, Adriana Maggiore, Maire Mahony, Kieran Miller, Rahul Nagara-jan, Ravi Narayanaswami, Ray Ni, Kathy Nix, Thomas Norrie, Mark Omernick,Narayana Penukonda, Andy Phelps, and Jonathan Ross. 2017. In-Datacenter Per-formance Analysis of a Tensor Processing Unit."
}