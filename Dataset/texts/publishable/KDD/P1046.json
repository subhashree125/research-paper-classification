{
  "ABSTRACT": "We study the problem of robust data augmentation for regressiontasks in the presence of noisy data. Data augmentation is essentialfor generalizing deep learning models, but most of the techniqueslike the popular Mixup are primarily designed for classificationtasks on image data. Recently, there are also Mixup techniques thatare specialized to regression tasks like C-Mixup. In comparison toMixup, which takes linear interpolations of pairs of samples, C-Mixup is more selective in which samples to mix based on their labeldistances for better regression performance. However, C-Mixupdoes not distinguish noisy versus clean samples, which can be prob-lematic when mixing and lead to suboptimal model performance.At the same time, robust training has been heavily studied wherethe goal is to train accurate models against noisy data throughmultiple rounds of model training. We thus propose our data aug-mentation strategy RC-Mixup, which tightly integrates C-Mixupwith multi-round robust training methods for a synergistic effect. Inparticular, C-Mixup improves robust training in identifying cleandata, while robust training provides cleaner data to C-Mixup for itto perform better. A key advantage of RC-Mixup is that it is data-centric where the robust model training algorithm itself does notneed to be modified, but can simply benefit from data mixing. Weshow in our experiments that RC-Mixup significantly outperformsC-Mixup and robust training baselines on noisy data benchmarksand can be integrated with various robust training methods.",
  "Corresponding author": "Permission to make digital or hard copies of part or all of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for third-party components of this work must be honored.For all other uses, contact the owner/author(s).KDD 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s).ACM ISBN 979-8-4007-0490-1/24/08",
  "INTRODUCTION": "Deep learning is widely used in applications that perform regres-sion tasks including manufacturing, climate prediction, and finance.However, one of the challenges is a lack of enough training data,and data augmentation techniques have been proposed as a solutionfor better generalizing the trained models. A representative tech-nique is Mixup , which mixes two samples by linearinterpolation to estimate the label of any sample in between. How-ever, Mixup is primarily designed for classification tasks mainlyon image data along with most of the other data augmentationtechniques and does not readily perform well on regressiontasks where the goal is to predict real numbers.Recently, there is an increasing literature on data augmentationdesigned for regression tasks, and C-Mixup is the state-of-the-art Mixup-based method. In order to avoid arbitrarily-incorrectlabels, each sample is mixed with a neighboring sample in terms oflabel distance where the selection follows the proposed samplingprobability calculated using a Gaussian kernel that is configuredusing a bandwidth parameter. The larger the parameter the widerthe distribution, which means that mixing can be done with distantneighbors in terms of label distance. In a sense, the bandwidthindicates whether Mixup should be performed, and to what degree.C-Mixup has theoretical guarantees for generalization and handlingcorrelation shifts in the data.At the same time, robustness against noise is becoming increas-ingly important for regression tasks. For example, in semiconductormanufacturing, the layer thickness in a 3D semiconductor needs tobe predicted for defect detection using a model. In this case, labels(e.g., layer thickness) can be noisy due to erroneous or malfunc-tioning measurement equipment, leading to a decline in predictionmodel performance and thus revenue. Hence, global semiconduc-tor companies make great efforts to ensure that their models arerobust against noise. To address this challenge, there is a recentline of multi-round robust training where noisy samplesare removed or fixed based on their loss values through multiplemodel trainings.We thus contend that integrating C-Mixup with robust trainingmethods is desirable, but this is not trivial. C-Mixup is not explicitlydesigned to be robust against noisy data and may be prone to incor-rect mixing because any out-of-distribution samples are mixed justthe same way as in-distribution samples. C-Mixup is robust againstcorrelation shifts, which assume the same data distribution, but un-fortunately cannot cope with noise in general. More fundamentally,data augmentation is to add data, while robust training is to clean(select or refurbish) data, so the two seem to even be contradictoryoperations. A nave approach is to run the two methods in sequence,e.g., run C-Mixup and then robust training or in the other ordering.",
  "a": ": RC-Mixup tightly integrates C-Mixup with multi-round robust training techniques for a synergistic effect: C-Mixupimproves robust training in identifying clean data, while robust training provides (intermediate) clean data for C-Mixup.Suppose the x-axis is the only feature, and the y-axis is the label. Also, there are two clean samples and and three noisysamples,, and . In Step 1, suppose that cleaning removes and (the exact outcome depends on the robust training technique).In Step 2, we perform C-Mixup possibly with bandwidth tuning to generate mixed samples. Here we mix the sample pairs (, )and (, ) to generate the mixed samples denoted as star shapes. Notice that C-Mixup selectively mixes samples that have closerlabels, so in this example (, ) are not mixed. In Step 3, the augmented samples can be used to train an improved regressionmodel, which can then be used for better cleaning in the next round. However, either C-Mixup will end up running on noisy data orrobust training would not benefit from augmented data.We propose the novel data augmentation strategy of tightly inte-grating C-Mixup and multi-round robust training for a synergisticeffect (see ). We call our framework RC-Mixup to emphasizethe robustness of C-Mixup. Each robust training round typicallyconsists of cleaning (Step 1) and model updating (Step 3) steps. Be-tween these two steps, we run C-Mixup (Step 2) so that it benefitsfrom the intermediate clean data that is identified by the cleaning.In addition, the model updating step now benefits from the aug-mented data produced by C-Mixup and produces a more accurateregression model that can clean data better. Another benefit of thisintegration is that it is data-centric where the robust training algo-rithm itself does not need to be modified because RC-Mixup is onlyaugmenting the data. Hence, RC-Mixup is compatible with any ex-isting multi-round robust training algorithm like Iterative TrimmedLoss Minimization (ITLM) , O2U-Net , and SELFIE . Atechnical challenge is efficiency where we would like to keep C-Mixups bandwidth up-to-date during the multiple rounds in robusttraining. We propose to periodically update the bandwidth, butdoing so reliably by evaluating candidate bandwidth values for sev-eral robust training rounds before choosing the one to use. We canoptionally speedup this process by simply updating the bandwidthin one direction with some tradeoff in performance as well.We perform extensive experiments of RC-Mixup on variousregression benchmarks and show how it significantly outperformsC-Mixup in terms of robustness against noise. While RC-Mixuputilizes a small validation set, we show it is sufficient to use therobust training to generate one from the training set. In addition,RC-Mixup also outperforms existing robust training techniquesthat do not augment their data.Summary of Contributions: (1) We propose RC-Mixup, thefirst selective mixing framework for regression tasks that is also robust against noisy data. (2) We tightly integrate the state-of-the-art C-Mixup with multi-round robust training techniques whereC-Mixup utilizes intermediate clean data and has a synergisticeffect with robust training. (3) We perform extensive experimentsand show how RC-Mixup significantly outperforms baselines byutilizing this synergy on noisy real and synthetic datasets.",
  "BACKGROUND": "Notations. Let D = {(,)} be the training set where is a -dimensional input sample, and is an -dimensionallabel. Let D = {(,)} be the validation set, where isthe distribution of the test set. Let model parameters, be a re-gression model, and the loss function that returns a performancescore comparing () with the true label using Mean SquaredError (MSE) loss. Mixup. Mixup takes a linear interpolation between any pairof samples and with the labels and to produce thenew sample + (1 ) with the label + (1 ) where (, ). According to Zhang et al. , mixing all samplesoutperforms Empirical Risk Minimization on many classificationdatasets. This strategy works well for classification where the labelsare one-hot encoded where many samples may have the same label.In regression, however, such linear interpolations are not suitableas labels are in a continuous space instead of a discrete space whereone-hot encodings cannot be used. Here two distant samples mayhave labels that are arbitrarily different, and simply mixing themcould result in intermediate labels that are very different than theactual label. C-Mixup. C-Mixup overcomes the limitation of Mixup andis the state-of-the-art approach for regression tasks on clean data. C-Mixup proposes a sampling probability distribution for each samplebased on the label distance between a neighbor using a symmetric",
  ")(1)": "where (,) is the label distance and is the bandwidth of akernel function. Since the values of a probability mass functionsum to one, C-Mixup normalizes the calculated values. The band-width is the key parameter to tune. As the bandwidth increases, theprobability distribution becomes more uniform, thereby makingC-Mixup similar to Mixup. As the bandwidth decreases, samples areonly mixed with their nearest neighbors, and C-Mixup eventuallybecomes identical to Empirical Risk Minimization (ERM). Robust Training. There is an existing literature on robust training where the goal is to perform accurate modeltraining against noisy data. While there are many approaches, wefocus on multi-round robust training where noisy data is eitherremoved or fixed progressively during the model training iterations.As a default, we use Iterative Trimmed Loss Minimization (ITLM) as a representative clean sample selection method, althoughwe can use other methods as we demonstrate in .5. ITLMrepeatedly removes a certain percentage of the training data wherethe intermediate models predictions differ from the labels andsolves the following optimization problem:",
  "()": "where D is the current clean data, is a sample in , is aloss function, and is a parameter that indicates the ratio of cleansamples in the training data. ITLM is widely used because of itssimplicity and scalability.However, any other multi-round robust training technique can be used as well. We demonstrate this point in .5where we replace ITLM with O2U-Net , which is a differentclean sample selection method, and SELFIE , which refurbisheslabels of unclean samples.",
  "C-MIXUP VULNERABILITY TO NOISY DATA": "C-Mixup assumes that all samples are clean data and performsMixup using a fixed bandwidth. Although C-Mixup is known to berobust against covariate shifts in the data where the distribution() may change, but not ( |), the data is still assumed to beclean. In comparison, we consider noisy data where ( |) maychange as well.We demonstrate how C-Mixup is vulnerable to such generalnoise in a. We experiment on the Spectrum dataset formanufacturing and add random Gaussian noise to the labels (seemore details in ). We evaluate C-Mixup with the tunedbandwidth on noisy data. This model is then evaluated on cleantest data. We compare C-Mixup with using ERM only and comparethe Root Mean Squared Error (RMSE, see definition in )results where a lower value is better. As the noise ratio increases,the RMSE of C-Mixup trained on noisy data increases. In addition,regardless of the noise ratio, the RMSE of C-Mixup increases asmuch as ERMs RMSE does, showing the vulnerability of C-Mixup.",
  "(b)": ": We evaluate C-Mixup on noisy data where we addlabel noise to the Spectrum dataset. A lower RMSE meansbetter model performance. (a) As the noise ratio increases,both ERM and C-Mixup gradually perform worse where theirperformance gap does not change much. (b) In addition, theoptimal bandwidth may actually increase where mixing di-lutes out-of-distribution data from negatively impacting themodel performance. To better understand how C-Mixup is affected by noise, wealso determine the optimal bandwidth, which results in the bestmodel performance across the different noise ratios as shown inb using the same experimental setup. As a result, as thenoise ratio increases, the optimal bandwidth also increases. Thisresult is counter-intuitive at first glance because it seems like noisydata should be mixed less. However, we suspect that mixing alsohas a dilution effect where out-of-distribution samples have lessimpact on the model training if they are mixed with other cleansamples. We do not claim that this trend always holds, and thepoint is that mixing has a non-trivial effect on noisy data. Thus,it becomes difficult to find a single bandwidth that works best forboth clean and noisy data.The C-Mixup paper also performs a robust training experi-ment against label noise, but it assumes a fixed noise ratio and doesnot necessarily show the entire story. The more extensive resultshere suggest that C-Mixup is not designed to handle noise and canbenefit from robust training methods. We thus propose a naturalextension by integrating C-Mixup with robust training.",
  "C-Mixup and Robust Training Integration": "We solve the bilevel optimization problem by interleaving C-Mixupwith the robust training rounds. This approach is common in otherbilevel optimization works . Recall that robust trainingiteratively cleans data and then updates its regression model untilthe data is clean enough. For each round, we can perform C-Mixupbetween the data cleaning and model updating steps as in .We explain how this sequence is beneficial to both methods. Data Cleaning benefits C-Mixup. We provide a simple empiricalanalysis of how data cleaning during robust training benefits C-Mixup in a. We experiment on the Spectrum dataset usedabove and add random Gaussian noise to the labels, independentlyper label. We start with a 10% noise ratio and run ITLM, whichprogressively cleans the data. For each round, we evaluate C-Mixupby training a model on the mixed data. As a result, when combiningC-Mixup with robust training, the models RMSE on the validationset clearly improves with each round, unlike when using C-Mixuponly. In addition, while using C-Mixup only has a slight perfor-mance decrease after convergence due to overfitting to the noisydata, C-Mixup with robust training does not have this problem. C-Mixup benefits Data Cleaning Performance. We also empiri-cally analyze how C-Mixup improves robust training by makingit identify clean data better in b. We use the same experi-mental setup as above and evaluate the noise detection accuracyof ITLM, which is the percent of noisy samples that are correctlyidentified by robust training. We compare ITLM with when it iscombined with C-Mixup. As a result, C-Mixup improves ITLMsnoise detection accuracy by up to 5%.",
  "As we analyzed in b, the optimal bandwidth of C-Mixupmay vary as the data is progressively cleaned via robust training, sowe would like to dynamically adjust this value both accurately and": "efficiently. Recall that the bandwidth adjusts the level of mixingwhere a larger bandwidth means that samples are mixed more withneighbors. Following the original C-Mixup setup , we considera fixed set of bandwidth candidates = {1,2, . . . ,||} and aimto select the best one.Our strategy is to update the best bandwidth after every > 0rounds of robust training. In the initial warm-up phase, we opt fora higher bandwidth to accommodate the relatively greater noiseratio. Subsequently, for each update, we evaluate bandwidth candi-dates within on the next > 0 rounds and then choose the onethat results in the best model performance. Analytically predictingmodel performance after multiple rounds is challenging becausethe cleaned data itself keeps on changing. While this strategy has anoverhead, we later show in .3 that only one or two initialupdates are needed to achieve most of the performance benefits.One way to further reduce the overhead is to avoid the bandwidthsearching altogether and instead simply decay the bandwidth by acertain ratio (say by 10%) every epochs based on the observationthat the optimal bandwidth tends to decrease for lower noise ratios.Naturally there is be a tradeoff between runtime and performance,which we show in .1. In addition, we now need to figureout the right decay rate. Nonetheless, if lowering runtime is critical,the decaying strategy can be a viable option.",
  "Overall Algorithm": "Algorithm 1 shows the overall RC-Mixup algorithm when usingITLM as the robust training method. We first initialize model pa-rameters using C-Mixup (Steps 24). The initial bandwidth is tunedon the validation set. This bootstrapping is important because thelater steps are designed to gradually update its value as the data iscleaned progressively, so we need to start from a bandwidth valuethat is optimal on the noisy data first. Next, for each round, robusttraining cleans the data, we run C-Mixup and update the modelon the cleaned data (Steps 1820). After every rounds, we alsoupdate the bandwidth of C-Mixup by evaluating the possible band-widths on rounds and choosing the one that results in the lowestvalidation set RMSE (Steps 816). We repeat the entire process untilthe model converges. Algorithm 2 shows the clean sample selectionalgorithm in ITLM, and Algorithm 3 shows the C-Mixup algorithm. Overhead on Robust Training. In comparison to robust training,RC-Mixup adds the overhead of mixing samples via C-Mixup duringeach round. For every rounds, there is also the overhead of tuningthe bandwidth using rounds of training. In our experiments, weobserve that the tuning overhead is small because the bandwidthonly needs to be updated once or twice.",
  "EXPERIMENTS": "We provide experimental results for RC-Mixup. We evaluate theregression models trained on the augmented training sets on sepa-rate test sets. We use Root Mean Squared Error (RMSE) and MeanAbsolute Percentage Error (MAPE) for measuring model perfor-mance where lower values are better. We report the mean and thestandard deviation ( in tables) for results of five random seeds.We use PyTorch , and all experiments are performed using IntelXeon Silver 4210R CPUs and NVIDIA Quadro RTX 8000 GPUs.",
  "Spectrum22642,000500NO271200200Airfoil511,000400Exchange-Rate168884,3731,518": "by applying light waves on 4-layer 3D semiconductors and measur-ing the returning wavelengths. This data is used to predict layerthickness without touching the semiconductor itself. The NO2 emis-sions dataset contains traffic and meteorological informationaround roads and is used to predict NO2 concentration. The Airfoildataset contains aerodynamic and acoustic test results for air-foil blade sections in a wind tunnel. Finally, the Exchange-Rate is a time-series dataset and contains daily exchange rates from 8countries. The three real datasets are from for a fair comparison. shows the dimension and size information of the datasets. Noise Injection. We inject noise to labels using two methods: (1)Gaussian noise, which adds to each label a value sampled from theGaussian distribution (0,22), where is the noise magnitude(see default values in the appendix), and is the standard deviationof labels of training set and (2) labeling flipping noise, which sub-tracts a maximum label value by each label as in classification .We use a noise rate of 1040% where the default is 30%. We do notset the noise ratio to be larger than 50% to prevent the noise fromdominating the data.",
  "Individual methods: performing ITLM only (Rob. train-ing) and performing C-Mixup only (C-Mixup)": "Simple combinations: performing robust training first andC-Mixup in sequence (RC), performing C-Mixup andthen robust training in sequence (CR), and performingRC-Mixup without dynamic bandwidth tuning (CR+C). Parameters. For robust training, we assume that the clean ratio is known for each dataset. If is unknown, it can be inferredwith cross validation . For the C-Mixup, CR, and CR+Cbaselines, we use a single bandwidth value and tune it using agrid search. We choose other parameters using a grid search onthe validation set or following C-Mixups setup. More details onparameters are in the appendix.",
  "Model Performance and Runtime Results": "We compare the overall performance RC-Mixup with the two typesof baselines on all the datasets as shown in . For the individ-ual method baselines, we observe that C-Mixup is indeed vulnerableto noise, and that robust training is less affected by the noise, butstill needs improvement. The simple combination baselines CRand CR+C increasingly outperform the individual methods asthey start to take advantage of both methods. However, RC-Mixupperforms the best by also dynamically tuning the bandwidth. Wealso note that our results are near-optimal. As a reference, the(RMSE, MAPE) results in an optimal setting where we only useclean data are: (6.961, 6.000) for Spectrum and (0.535, 13.209) forNO2. The RC-Mixup results are similar to these results and cannotbe further improved.We also show how RC-Mixups model training converges andhow its bandwidth is tuned in Figures 4a4d. For all four datasets,RC-Mixup clearly converges to lower RMSE values for the samenumber of rounds compared to C-Mixup and robust training. RC-Mixups RMSE values drastically decrease around the middle of eachfigure because robust training and bandwidth tuning are appliedat that point. Once robust training is applied, the model trainingis now done on cleaned data, so the model performance improvessignificantly afterwards. We then show how RC-Mixup dynamicallyadjusts its bandwidth for the four datasets with five random seedsin Figures 4e4h. For these datasets, the bandwidth values vary slightly depending on the random seed, and we show the bandwidthaveraged across five random seeds (dotted red lines). The decreasingtrends of the tuned bandwidths are consistent with b, whereas the data is cleaned, a smaller bandwidth is more effective. We donot claim this trend always holds, and the point is that RC-Mixupis able to find the right bandwidth in any situation. Runtime. The computational cost of RC-Mixup varies depend-ing on the number of bandwidth candidates or bandwidth updatefrequency. For the Spectrum dataset, the runtimes of all methodsare as follows: robust training (ITLM) only: 52s, C-Mixup only:244s, CR: 161s, CC+R: 250s, and RC-Mixup: 578s. AlthoughRC-Mixup requires roughly twice the runtime of C-Mixup, thesubstantial performance improvement of RC-Mixup justifies itsoverhead. In general, while the efficiency decreases as the searchspace grows, we can obtain performance improvements with only47 bandwidth candidates in practice. As a reference, C-Mixupalso searches for the optimal bandwidth among 610 bandwidthcandidates using a grid search. Bandwidth Decaying Results. We also show RC-Mixups perfor-mance and runtime when using bandwidth decaying in .Here we decrease the bandwidth by 10% every epochs. As a result,there is a tradeoff between runtime and performance where thedecaying strategy is 1.4-2.6x faster than the original RC-Mixup, but",
  "%C-Mixup13.3760.369 12.2570.325RC-Mixup 8.5240.3426.7840.293": "and also use label flipping. As a result, RC-Mixup consistentlyperforms better than C-Mixup as it is less affected by noise with itsbandwidth tuning. We also evaluate RC-Mixups robustness whilevarying the noise rate on the Spectrum dataset from 10% to 40% in and observe results that are consistent with when using thedefault noise rate of 30%.",
  "Parameter Analysis": "We vary the bandwidth tuning parameters and to see howRC-Mixup performs in different scenarios on the Spectrum dataset.The results on the other datasets are similar. As we decrease ,we update the bandwidth more frequently, which means that it ismore likely to be up-to-date. a indeed shows that the RMSEdecreases against the number of updates, but only for the first one ortwo updates. Increasing means that we evaluate the bandwidthsusing more rounds before selecting one of them. b showsthat a higher leads to lower RMSE, but with diminishing returns.Hence, RC-Mixup achieves sufficient performance improvementseven when both and are small, which means that the additionaloverhead for bandwidth tuning of RC-Mixup is not large.",
  "Validation Set Construction and Size": "We evaluate RC-Mixup by varying the validation set size on allthe four datasets in Figures 4i4l. As a result, RC-Mixup worksreasonably well even for smaller validation set sizes. If there is noclean validation set readily available, we can utilize the given robusttraining method to construct a validation set ourselves. In ,we compare RC-Mixups RMSE on two datasets when using (1) a",
  "AirfoilClean2.5300.3571.3980.123Cleaned with RT2.6840.4531.4710.136Noisy2.6920.2441.5470.167": "clean validation set; (2) a noisy validation set that is cleaned withrobust training first; and (3) a noisy validation set that is a subsetof the training set. We compare with (3) just as a reference. As aresult, RC-Mixup using (2) has slightly lower, but a comparableperformance as when using (1), which means that RC-Mixup canstill perform well without a given clean validation set with anadditional overhead of cleaning the noisy validation set.",
  "Other Robust Training Methods": "To show the generality of RC-Mixup, we integrate C-Mixup with thetwo other robust training methods O2U-Net and SELFIE and provide more evaluation results. RC-Mixup can be plugged intoany other multi-round robust training method as well. O2U-Net Integration. O2U-Net consists of two phases for select-ing clean samples: a pre-training phase and a cyclical training phasebetween underfitting and overfitting. During the pre-training phase,we train the model using C-Mixup. In the cyclical training phase,we further train this pre-trained model by adjusting the learningrate cyclically to obtain the clean samples. We then return the pre-trained C-Mixup model trained on the final clean samples withbandwidth tuning. SELFIE Integration. SELFIE refurbishes the labels of unclean sam-ples with low predictive uncertainty by assigning them the mostfrequent class among the previous predictions. SELFIE was de-signed for classification, so we extend it to work for regression.While SELFIE calculates uncertainty using the entropy of thepredictive categorical distribution, this approach cannot be directlyapplied in a regression setting due to the absence of the categoricaldistribution. Instead of using entropy, we quantify uncertainty by",
  "SELFIE only0.01650.0004 1.57410.0666RC-Mixup w/ SELFIE0.01540.0009 1.45350.1137": "assessing the variance of predictive labels over the past predic-tions. This approach is consistent with other common regressiontechniques , where the uncertainty corresponds to the vari-ation in predictions across multiple instances or models.When refurbishing a label of an unclean sample, we average theprevious predictions instead of finding the most frequent label. Thereason is that in regression, labels have continuous values, so itmakes less sense to find the most frequent labels as in classification.Finally, since SELFIE refurbishes the labels for every mini-batch,the label distances between two labels change, which means thesampling probabilities for C-Mixup may change as well. However,updating the distances between label pairs requires significantamounts of computation, and we thus choose not to update the sam-pling probabilities once they are initially computed. This approachis reasonable because only a fraction of labels that have low uncer-tainties are actually refurbished. Even if we do update the samplingprobabilities, our experiments show that they have a minor impacton the models performance. Nonetheless, incrementally updatingthe sampling probabilities efficiently is an interesting future work. Evaluation. We now evaluate RC-Mixup using O2U-Net andSELFIE on the four datasets in . As a result, RC-Mixupimproves both C-Mixup and robust training as in demon-strating the synergy between the two for all four datasets.",
  "RELATED WORK": "There are largely two branches of work for data augmentation inregression. One is semi-supervised regression wherethe goal is to utilize unlabeled data for training. Another branch isdata augmentation when there is no unlabeled data, which is ourresearch focus. Most data augmentation techniques are tailored toclassification and more recently a few are designed for regression. Data Augmentation for Classification. There are largely threeapproaches: generative models, policies, and Mixup techniques.Generative models including GANs and VAEs are popularin classification where the idea is to generate realistic data thatcannot be distinguished from the real data by a discriminator. An-other approach is to use policies , which specify fixing rules fortransforming the data. However, a major assumption is that thelabels of these generated samples are the same, which does notnecessarily hold in a regression setting where most samples mayhave different labels. Mixup takes the alternativeapproach of generating both data and labels together by mixingexisting samples with different labels assuming linearity betweentraining samples . Data Augmentation for Regression. There is a new and increas-ing literature on data augmentation for regression using Mixuptechniques. Although the original Mixup paper mentions that itstechniques can easily be extended to regression, the linearity of aregression model is limited where mixing samples with all othersmay not be beneficial and even detrimental to model performance.Hence, the key is to figure out how to limit the mixing. RegMix learns for each sample how many nearest neighbors in terms of datadistance it should be mixed with for the best model performanceusing a validation set. The state-of-the-art C-Mixup uses aGaussian kernel to generate a sampling probability distributionfor each sample based on label distances using a global bandwidthand selects a sample to mix based on the distribution. Anchor DataAugmentation takes a more domain-specific approach whereit clusters data points and modifies original points either towardsor away from the cluster centroids for the augmentation. Finally, R-Mixup specializes in improving model performance on biologi-cal networks. In comparison, RC-Mixup uses the domain-agnosticC-Mixup and makes it robust against noise. Robust Training. The goal of robust training is to train accuratemodels against noisy or adversarial data, and we cover the represen-tative works. While data can be problematic in various places ,most techniques assume that the labels are noisy and mitigatewith the following strategies: (1) developing model architecturesthat are less affected by the noise , (2) applying lossregularization techniques to reduce overfitting ,(3) correcting the loss function to account for the noise ,and (4) proposing sample selection techniques for se-lecting clean samples from noisy data. In comparison, RC-Mixupis mainly designed to work with the sample selection techniques,but can also complement other approaches as long as it can accessintermediate clean data within rounds.",
  "CONCLUSION": "We proposed RC-Mixup, an effective and practical data augmenta-tion strategy against noisy data for regression. The state-of-the-artdata augmentation technique C-Mixup is not designed to handlenoise, and RC-Mixup is the first to tightly integrates it with robusttraining for a synergistic effect. C-Mixup benefits from the inter-mediate clean data information identified by robust training andperforms different mixing depending on whether noisy data is be-ing used, while robust training cleans its data better with C-Mixupsdata augmentation. We also proposed dynamic tuning techniquesfor C-Mixups bandwidth. Our extensive experiments showed howRC-Mixup significantly outperforms C-Mixup and robust trainingbaselines on noisy data benchmarks and is compatible with variousrobust training methods.",
  "We explain more detailed experimental setups": "Hyperparameters. We summarize the hyperparameters for all the datasets in . For each dataset, we either use the traditional notionof Mixup or the more recent Manifold Mixup (ManiMix), whichever performs better. We use a 3-layer fully connected neural networkwith one hidden layer with 128 nodes (FCN3) for the Spectrum, NO2, and Airfoil datasets. For the Exchange-Rate dataset, we build anLST-Attn model with a horizon value of 12.",
  "A.2Noisy Versus Hard-to-train Data": "While noisy data tends to have high loss values, so does hard-to-train data. However, noisy data typically has higher losses as shown in Paulet al. . Here hard data is identified using the EL2N score, which is similar to loss. Paul et al. analyze the challenge of distinguishinghard data from noisy data and shows that data with noisy labels tends to have higher EL2N scores. If there is a label noise, data with largeloss values are commonly considered as noise ."
}