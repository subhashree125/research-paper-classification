{
  "ABSTRACT": "Retrieval Augmented Generation (RAG) is a technique used to aug-ment Large Language Models (LLMs) with contextually relevant,time-critical, or domain-specific information without altering theunderlying model parameters. However, constructing RAG systemsthat can effectively synthesize information from large and diverse Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from on Generative AI for Recommender Systems and Personalization, KDD 2024,Barcelona, Spain, 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00 set of documents remains a significant challenge. We introducea novel data-centric RAG workflow for LLMs, transforming thetraditional retrieve-then-read system into a more advanced prepare-then-rewrite-then-retrieve-then-read framework, to achieve higherdomain expert-level understanding of the knowledge base. Ourmethodology relies on generating metadata and synthetic Ques-tions and Answers (QA) for each document, as well as introducingthe new concept of Meta Knowledge Summary (MK Summary) formetadata-based clusters of documents. The proposed innovationsenable personalized user-query augmentation and in-depth infor-mation retrieval across the knowledge base. Our research makestwo significant contributions: using LLMs as evaluators and em-ploying new comparative performance metrics, we demonstratethat (1) using augmented queries with synthetic question matchingsignificantly outperforms traditional RAG pipelines that rely ondocument chunking ( < 0.01), and (2) meta knowledge-augmented",
  "Workshop on Generative AI for Recommender Systems and Personalization, KDD 2024, Barcelona, Spain,Mombaerts, et al": "proposed approach improved on state-of-the-art work. Finally, theapproach is cost-effective, costing $20 for 2000 research papers. Asa limitation, while we recognize the difficulty in crafting a set ofmetadata prior to document processing, the metadata generationcan become an iterative approach to generate the metadata upondiscovery. In addition, we left multi-hop iterative searches and im-provement of the summary of the clustered knowledge base forfuture work. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,Naman Goyal, Heinrich Kttler, Mike Lewis, Wen-tau Yih, Tim Rocktschel, et al.Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances inNeural Information Processing Systems, 33:94599474, 2020. Vladimir Karpukhin, Barlas Ouz, Sewon Min, Patrick Lewis, Ledell Wu, SergeyEdunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domainquestion answering. arXiv preprint arXiv:2004.04906, 2020. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai,Jiawei Sun, and Haofen Wang. Retrieval-augmented generation for large languagemodels: A survey. arXiv preprint arXiv:2312.10997, 2023. Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, ChenlongDeng, Zhicheng Dou, and Ji-Rong Wen. Large language models for informationretrieval: A survey. arXiv preprint arXiv:2308.07107, 2023. Wenjun Peng, Guiyang Li, Yue Jiang, Zilong Wang, Dan Ou, Xiaoyi Zeng, DerongXu, Tong Xu, and Enhong Chen. Large language model based long-tail queryrewriting in taobao search. In Companion Proceedings of the ACM on Web Confer-ence 2024, pages 2028, 2024.",
  "INTRODUCTION": "Retrieval Augmented Generation (RAG) is a standard techniqueused to augment Large Language Models (LLMs) with the capabilityto integrate contextually relevant, time-critical, or domain-specificinformation without altering the underlying model weights. Thisapproach is particularly effective for knowledge-intensive taskswhere proprietary or timely data is required to guide the languagemodels response, and has become an attractive solution for reduc-ing model hallucinations and ensuring alignment with the latestand most relevant information for the task at hand. In practice,RAG pipelines consist of several modules structured around the tra-ditional retrieve-then-read framework . Given a user question, aretriever is tasked with dynamically searching for related documentchunks and providing them as context for the LLM to predict theanswer, rather than relying solely on pre-trained model knowledge(also known as in-context learning). A simple, yet powerful andcost-effective retriever framework involves using a dual-encoderdense retrieval model to encode both the query and the documentsindividually into a high-dimensional vector space, and computingtheir inner product as a measure of similarity .However, several challenges specifically hinder the quality of theknowledge augmented context. First, knowledge base documentsmay contain substantial noise, either intrinsic for the task at handor as a result of the lack of standardization across the documents ofinterest (from various documents layouts or formats such as .pdf,.ppt, .wordx, etc.). Second, little to no human-labelled informationor relevance labels are typically available to support the documentchunking, embedding, and retrieval processes, making the overallretrieval problem a largely unsupervised approach and challengingto personalize for a given user. Third, chunking and separatelyencoding long documents pose a challenge in extracting relevantinformation for the retrieval models . Indeed, document chunksdo not conserve the semantic context of the entire document, andthe larger the chunk, the less precise the context of the chunk is maintained for further retrieval. This makes the choice of docu-ment chunking strategy non-trivial for a given use-case, althoughcritical for the quality of the subsequent steps due to potentiallysignificant information loss. Fourth, user queries are typically short,ambiguous, may contain vocabulary mismatches, or are complexenough to require multiple documents to address, making it gener-ally difficult to precisely capture the users intents and subsequentlyidentify the most appropriate documents to retrieve . Finally,there is no guarantee that the relevant information is localized inthe knowledge base, but rather spread across multiple documents.As a result, domain expert-level usage of the knowledge base ismade dramatically more challenging with automated informationretrieval systems. Such high level reasoning across the knowledgebase is a yet unsolved problem and constitutes the basis for recentLLM-based retrieval agent frameworks research.In this work, we are interested in cases where user queries re-quire the information search to be specific to users interests orprofile, are ambiguous, and require high level reasoning acrossdocuments (for example: What challenges are associated with ap-plying machine learning for marketing?\"), making recall, specificity,and depth our metrics of interest. To improve the search resultsperformance across those metrics, query augmentation has beena widely used technique in both traditional Information Retrieval(IR) use cases such as e-commerce search , as well as in the morerecent RAG frameworks leveraging LLMs . Query augmenta-tion consists in explicitly rewriting or extending the original userquery into one or more tailored queries that better match searchresults, alleviating issues related to query underspecification. Thisadjustment adds a module to the RAG framework and transforms itinto the more sophisticated rewrite-then-retrieve-then-read work-flow. Leveraging their vast underlying parametric world knowledge,LLMs constitute a fitting choice to understand and enhance usersqueries, subsequently boosting the relevance of the retrieve step. Our approach introduces a new data-centric RAG work-flow, prepare-then-rewrite-then-retrieve-then-read (PR3), where eachdocument is processed by LLMs to create both custom metadatatailored to users characteristics and QA pairs to unlock new knowl-edge base reasoning capabilities through query augmentation. Ourdata preparation and retrieval pipeline mitigates the informationloss inherent to large document chunking and embedding, as onlyQA are being encoded instead of documents chunks, while actingas a noise filtering approach for both noisy and irrelevant docu-ments for the task at hand. By introducing metadata-based clustersof QAs and Meta Knowledge Summary, our framework condition-ally augment the initial user query into multiple dedicated queries,therefore increasing the specificity, breadth, and depth of the knowl-edge base search (see ). The proposed out-of-the-shelvemethodology is easily applicable to new datasets, does not rely onmanual data labelling or model fine-tuning, and constitutes a steptowards autonomous, agent-based documents database reasoningwith LLMs, for which the literature remains limited to date .",
  "RAG Enhancement with Fine-tuning": "Methodologies that aim at improving RAG pipelines based on fine-tuning generally constitute a higher barrier to entry for performingboth the initial parameters update, and the maintenance of theaccuracy of the model over time to new documents. They requirecareful data cleaning and (often manual) curation, and manual iter-ations across training hyperparameters sets to successfully adaptthe model to the task at hand without causing catastrophic forget-ting of the pre-trained model knowledge . In addition, modeltuning may not be sustainable for frequent knowledge-base up-dates, and represents a generally higher cost due the underlyingrequirements of compute resources, despite the recent developmentof parameter-efficient fine-tuning (PEFT) techniques .In e-commerce retrieval frameworks, TaoBao created a queryrewriting framework based on company logs and rejection samplingto fine-tune a LLM in a supervised fashion, without QA generation.They further introduced a new contrastive learning methods tocalibrate query generation probability to be aligned with desiredsearch results, leading to a significant boost of merchandise volume,number of transactions and unique visitors . As an alternative,reinforcement learning methods based on black-box LLM evalua-tion has also been leveraged to train a smaller query-rewriter LLM,which showed a consistent performance improvement in open-domain and multiple-choice questions and answers (QA) in websearch . Reinforcement learning-based approaches, however, aresubject to more instability during the training phase, and require acareful investigations of the trade-offs between generalization andspecialization among downstream tasks . Other approaches havefocused on specifically improving the embedding space betweenthe user query and the documents at hand, rather than augmentingthe query itself. Authors of InPars augmented their documentsknowledge base by generating synthetic questions and answerspairs in a unsupervised fashion, and subsequently used them tofine-tune a T5 base embedding model. They showed that using thefine-tuned embedding model followed by a neural reranker such asColBERT outperformed strong baselines such as BM25 .Most recently, other types of approaches have been developedto improve the end-to-end pipeline performance, such as RAFT, which consists in specifically training a reader to differentiatebetween relevant and irrelevant documents, or QUILL thataims at entirely replacing the RAG pipeline using RAG-augmenteddistillation training of another LLM.",
  "RAG Enhancement without Fine-tuning": "As an alternative to fine-tuning LLMs or encoder models, queryaugmentation methodologies have been developed to increase theperformance of the retrievers by transforming the user query pre-encoding. These approaches can further be classified into two cate-gories: either leveraging a retrieval pass through the documents, orzero-shot (without any example document). Among the zero-shot approaches, HyDE introduced a dataaugmentation methodology that consists in generating an hypo-thetical response document to the user query by leveraging LLMs.The underlying idea is to bring closer the user query and the docu-ments of interest in the embedding space, therefore increasing theperformance of the retrieval process. Their experiments showedperformance comparable to fine-tuned retrievers across varioustasks. The generated document, however, is a nave data augmen-tation in the sense that it does not change given the underlyingembedded data for the task at hand, such that it can lead to per-formance decrease in multiple situations, for there is inevitably agap between the generated content and the knowledge base. Alter-natively, methodologies have been proposed to perform an initialpass through the embedding space of the documents first, and sub-sequently augment the initial query to perform a more informedsearch. These Pseudo Relevance Feedback (PRF) and GenerativeRelevance Feedback (GRF) modeling approaches are typicallydependent on the quality of the most highly-ranked documentsused to first condition their query augmentation to, and are there-fore prone to significant performance variation across queries, ormay even forget the essence of the original query.",
  "METHODOLOGY": "In both RAG pipeline enhancements approaches cited above, theretrievers are generally unaware of the distribution of the targetcollection of documents despite an initial pass through the retrievalpipeline. In our proposed framework, for each document and priorto inference, we create a set of dedicated metadata, and subsequentlygenerate guided QA spanning across the documents using Chainof Thoughts (CoT) prompting with Claude 3 Haiku . Thesynthetic questions are then encoded, and the metadata used forfiltering purposes. For any user-relevant combination of metadata,we create a Meta Knowledge Summary (MK Summary), leveragingClaude 3 Sonnet, which consists in a summarization of the keyconcepts available in the database for a given filter. At inferencetime, the user query is dynamically augmented by relying on thepersonalized MK Summary given the metadata of interest, thereforeproviding tailored response for this user. By doing so, we provide theretriever with the capability to reason across multiple documentswhich may have required multiple retrieval and reasoning roundsotherwise. Our goal is to ultimately increase the quality of theend-to-end retrieval pipeline across multiple metrics, such as depth,coverage, and relevancy, by enabling complex reasoning acrossthe database through tailored searches and the leverage of metaknowledge information. Importantly, our approach does not relyon any model weights update, and may very well be combinedwith any fine-tuning of either the language or the encoding modelsto any domain to further improve the performance of the end-to-end RAG pipeline . We represent our methodology pipeline in, and describe the synthetic QA generation process and theconcept of MK Summary below.",
  "Synthetic QA Generation": "First, for each document, we generate a set of metadata and subse-quent QA using CoT prompting (see Appendix A). The prompt aimsat creating a list of metadata by classifying the documents into apredefined set of categories (such as research field, or applicationstypes for our research papers benchmark). Relying on these meta-data, we generate a set of synthetic questions and answers, usingteacher-student prompting and assess the knowledge of the studenton the document. We specifically leverage Claude 3 Haiku for itslong-context reasoning abilities and potential to create syntheticQA pairs with context spanning across documents. The generatedmetadata serve both as filtering parameters for the augmentedsearch, and to select the synthetic QA used for the users queriesaugmentation in the form of meta knowledge information (MKSummary). In addition, the synthetic QA are used for the retrieval,but only questions are vectorized for downstream retrieval. For ourpublic scientific research papers use case, a total of 8657 QA weregenerated from the 2000 research documents, accounting for 5 to 6questions for 70% of cases, and 2 questions in 21% of cases. Exam-ples of synthetic questions and answers are provided in AppendixB. The total number of token generated as parts of the processingstep amount to approximately 8M output tokens, correspondingto a total of $20.17 for the entire processing pipeline of all 2000documents (including input tokens) using Amazon Bedrock .We investigated the redundancy of the generated QA across thedocument using hierarchical clustering on the embedding spaceof the questions using e5-mistral-7b-instruct , but did not de-duplicated the generated QA for our use cases due to the low QAsoverlap. QA Filtering can be application and metadata specific, andother high-dimensional approaches such as Determinantal PointProcesses (DPP) are left for future work, together with theautomated discovery of metadata topics and self-correcting QA-generation .",
  "Generation of Meta Knowledge Summary": "For a given combination of metadata, we create a Meta KnowledgeSummary (MK Summary) aiming at supporting the data augmen-tation phase for a given user query. For our research papers usecase, we limited our metadata to the specific field of research (suchas reinforcement learning, supervised vs unsupervised learning,bayesian methods, econometrics, etc.) identified during the docu-ment processing phase by Claude 3 Haiku. For this research, wecreate the MK Summary by summarizing the concepts across a setof questions tagged with the metadata of interest using Claude 3Sonnet. An alternative left for future work is that of prompt tuningto optimize for the content of the summary prompt . 1The dataset was filtered using the following categories on the Arxiv API: \"stat.ML\",\"stat.TH\", \"stat.AP\", \"stat.ME\", \"math.ST\", \"cs.AI\", \"cs.LG\", \"econ.EM\". Thank you toarXiv for use of its open access interoperability.",
  "Augmented Generation of Queries andRetrieval": "Given a user query and a set of pre-selected metadata of interests,we retrieve the corresponding pre-computed MK Summary anduse it to condition the user query augmentation into the databasesubset. For our research paper benchmark, we created a set of 20MK Summary corresponding to research fields (e.g. deep learningfor computer vision, statistical methods, bayesian analysis, etc.), re-lying on the metadata created in the processing phase. We leveragethe \"plan-and-execute\" prompting methodology to address com-plex queries, reason across documents, and ultimately improve therecall, precision, and diversity of the provided answers . Forexample, for a user query related to the Reinforcement Learningresearch topic, the pipeline will first retrieve the meta knowledge(MK Summary) about Reinforcement Learning of the database, aug-ment the user query into multiple sub queries based on the contentof the MK Summary, and perform a parallel search in the filtereddatabase relevant for manufacturing questions. For this purpose,the synthetic Questions are embedded, and replace the originaldocuments chunk-based similarity matching, therefore mitigatingthe information loss due to document chunking discontinuity. Oncethe best match of a synthetic question is found, the correspond-ing QA are retrieved, together with the original document title.Only the document title, the synthetic question, and the answerare returned as a result of the retrieval. We use JSON formattingfor downstream summarization performance. The final response ofthe RAG pipeline is obtained by providing the original query, theaugmented queries, the retrieved context and few shot examples(see ).",
  "EVALUATION4.1Generating Evaluation Queries": "To evaluate our data-centric augmented retrieval pipeline, we gener-ated 200 questions for the arXiv dataset using Claude 3 Sonnet (seeAppendix C). In addition, we compared our methodology againsttraditional document chunking, query augmentation with docu-ment chunking, and nave (not using MK Summary) augmentationwith the QA processing of the documents. As a comparison, we cre-ated documents chunks consisting of 256 tokens with 10% overlap.For our use case, traditional chunking generated 69,334 documentschunks.",
  "Evaluation Metrics and Prompts": "Without relevance labels, we use Claude 3 Sonnet as a trustedevaluator to compare the performance of all four benchmarkmethodologies considered: traditional chunking without any queryaugmentation, traditional document chunking with naive queryaugmentation, augmented search using our PR3 pipeline withoutMK Summary, and augmented search using our PR3 pipeline withMK Summary. The query augmentation prompt is provided in Ap-pendix D. We use the custom performance metrics defined belowdirectly in the prompt to compare the results of both the retrievalmodel and the final response on a scale from 0 to 100. An exampleof Claude 3 Sonnet comparison answer is provided in Appendix E.",
  "Meta Knowledge for Retrieval Augmented Large Language ModelsWorkshop on Generative AI for Recommender Systems and Personalization, KDD 2024, Barcelona, Spain,": "- Recall: evaluates the coverage of key, highly relevant informa-tion contained in the retrieved documents- Precision: evaluates the ratio of relevant documents againstirrelevant ones- Specificity: evaluates how precisely focused the final answeris on the query at hand, with clear and direct information thataddresses the question- Breadth: evaluates the coverage of all relevant aspects or areasrelated to the question, providing a complete overview- Depth: evaluates the extent to which the final answer providesa thorough understanding through detailed analysis and insightsinto the subject- Relevancy: evaluates how well-tailored the final answer isto the needs and interests of the audience or context, focusingon providing directly applicable and essential information whileomitting extraneous details that do not contribute to addressingthe specific question",
  "RESULTS": "We considered 4 cases for the evaluation of our retrieval pipeline:(1) traditional document chunking, without any augmentation, (2)traditional document chunking and augmentation, (3) QA-basedsearch and retrieval, with nave augmentation (our first proposal),and (4) QA-based search and retrieval, with the use of MK summary(our second proposal). For a single query, the computational latencyof the end-to-end-pipeline amounts to 20-25 seconds.",
  "Retrieval and End-to-end Evaluation Metrics": "For each of the synthetic user queries generated, we ran a com-parison prompt that includes the context retrieved as part of eachapproach, together with their final answers. We prompted Claude 3Sonnet to rate each of the metrics on a scale from 0 to 100, togetherwith a justification text. An example of the evaluation response isprovided in Appendix E. The obtained metrics are then averagedacross all queries and displayed below in . We observe aclear benefit across all metrics but the precision of the retrieveddocuments by our two proposed QA-based methodologies. The lackof strong improvement over the precision metric is consistent withthe usage of a single encoding model and show that few documentsare considered completely irrelevant. Specifically, we note a sig-nificant performance boost in both the breadth and the depth ofthe final LLM response. This result shows that the MK Summaryis providing additional information that is leveraged by the queryaugmentation step. Finally, the contribution of the MK summaryto the conditioning of the search itself appears statistically signifi-cant across all metrics but the precision of the retriever ( < 0.01between the augmented QA search and the MK-Augmented QAsearch)(see ). We observe that the the proposed methodologysignificantly improves the breadth of the search (by more than 20%,compared to traditional nave search with chunking approaches),which aligns to the intuition that our proposal allows for effectivelysynthetizing more information from the content of the database,and leveraging its content more extensively.",
  "CONCLUSION AND DISCUSSION": "We proposed a new data-centric RAG workflow which leveragessynthetic QA generation instead of the traditional document chunk-ing framework, and a query augmentation-based approach basedon high level summary of the content metadata-based clusters ofdocuments to improve the accuracy and quality of the end-to-endLLM augmentation pipeline. Our methodology significantly outper-forms traditional RAG pipelines relying on document chunking andnave user query augmentation. We introduced the concept of MKSummary to further boost the zero-shot search augmentation in theknowledge base, which subsequently increased the performanceof the end-to-end RAG pipeline in our test case. In essence, ourmethodology improves on simple semantic matching informationretrieval in the encoding vector space of the documents, wherewe allow for more diverse but highly relevant documents search,therefore providing more well-rounded, domain expert-level, andcomprehensive answers to the user query. On all metrics consid-ered, recall, precision, specificity, breadth, depth, and relevancy, the",
  "Rolf Jagerman, Honglei Zhuang, Zhen Qin, Xuanhui Wang, and Michael Ben-dersky. Query expansion by prompting large language models. arXiv preprintarXiv:2305.03653, 2023": "Krishna Srinivasan, Karthik Raman, Anupam Samanta, Lingrui Liao, Luca Bertelli,and Mike Bendersky. Quill: Query intent with large language models using re-trieval augmentation and multi-stage distillation. arXiv preprint arXiv:2210.15718,2022. Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang. An empir-ical study of catastrophic forgetting in large language models during continualfine-tuning. arXiv preprint arXiv:2308.08747, 2023.",
  "Alex Kulesza, Ben Taskar, et al. Determinantal point processes for machinelearning. Foundations and Trends in Machine Learning, 5(23):123286, 2012": "Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, andWilliam Yang Wang. Automatically correcting large language models: Surveyingthe landscape of diverse self-correction strategies. arXiv preprint arXiv:2308.03188,2023. Weng Lam Tam, Xiao Liu, Kaixuan Ji, Lilong Xue, Xingjian Zhang, Yuxiao Dong,Jiahua Liu, Maodi Hu, and Jie Tang. Parameter-efficient prompt tuning makesgeneralized and calibrated neural text retrievers. arXiv preprint arXiv:2207.07087,2022.",
  "We present below samples of synthetic QA generated from the 2,000 researchpapers. The questions and answers were generated using the metadata andQA generation prompt (Appendix A)": "- Question #1: \"What are the researchers conjectures about why thefuture narrative prompting approach seemed to unlock improved predictivecapabilities in ChatGPT-4 compared to direct prompting?\" (from )- Answer #1: \"The researchers conjecture that the narrative promptingapproach may leverage ChatGPT-4s capacity for \"hallucinatory\" narrativeconstruction, allowing it to more effectively synthesize and extrapolatefrom its training data to make more accurate predictions, compared to thedirect prompting approach. This suggests the models creative abilities canbe strategically utilized for predictive tasks.\" - Question #2: \"How does the distribution of preference datasets influencethe rate of model updates and the training accuracy when using the DirectPreference Optimization (DPO) objective? What are the implications forpractical alignment approaches?\" (from )- Answer #2: \"The paper provides theoretical analysis showing thatthe distribution of preference datasets, characterized by \"preference dis-tinguishability\", has a strong influence on the rate of model updates andtraining accuracy under the DPO objective. Behaviors with higher distin-guishability exhibit faster loss reduction and parameter updates. This canlead to an intricate prioritization effect, where DPO tends to prioritize learn-ing behaviors with higher distinguishability, potentially deprioritizing lessdistinguishable but crucial behaviors.\""
}