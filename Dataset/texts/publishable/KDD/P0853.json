{
  "ABSTRACT": "In product search, the retrieval of candidate products before re-ranking is more critical and challenging than other search likeweb search, especially for tail queries, which have a complex andspecific search intent. In this paper, we present a hybrid systemfor e-commerce search deployed at Walmart that combines tra-ditional inverted index and embedding-based neural retrieval tobetter answer user tail queries. Our system significantly improvedthe relevance of the search engine, measured by both offline andonline evaluations. The improvements were achieved through acombination of different approaches. We present a new techniqueto train the neural model at scale. and describe how the system wasdeployed in production with little impact on response time. Wehighlight multiple learnings and practical tricks that were used inthe deployment of this system.",
  "Work done while at Walmart": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 22, August 1418, 2022, Washington, DC, USA. 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-1-4503-9385-0/22/08...$15.00 ACM Reference Format:Alessandro Magnani, Feng Liu, Suthee Chaidaroon, Sachin Yadav, PraveenReddy Suram, Ajit Puthenputhussery, Sijie Chen, Min Xie, Anirudh Kashi,Tony Lee, and Ciya Liao. 2022. Semantic Retrieval at Walmart. In Proceedingsof the 28th ACM SIGKDD Conference on Knowledge Discovery and DataMining (KDD 22), August 1418, 2022, Washington, DC, USA. ACM, NewYork, NY, USA, 9 pages.",
  "INTRODUCTION": "Search is one of the most important channels for customers todiscover products on an e-commerce website such as Walmart.com.Given our huge catalog which contains millions of products, helpingusers find relevant products for their queries is a very challengingproblem . Existing literature on information retrieval focusesmostly on web search . While product search shares manycommon challenges with web search, there are many unique aspectsof product search. Like other search, product search usually involvestwo steps: the first step is to retrieve all relevant products from thecatalog that form the recall set; these candidate products then gothrough a re-rank step to identify which products are the best toreturn to the customer.One major difference with web search is that the retrieval stepin product search is a more critical and challenging problem . This is because product titles (the main search-able text) aregenerally much shorter than web documents. Also, while many webdocuments may contain the same information, a specific productfrom a seller rarely has a duplicate. Retrieving a specific productwhile matching on shorter text is a challenging problem.Traditionally, retrieval is based on text match between queriesand documents, utilizing a heuristic score function like OkapiBM25 and an inverted index like Apache Lucene1. Text",
  "KDD 22, August 1418, 2022, Washington, DC, USA.Alessandro Magnani et al": "match between queries and documents suffers from vocabulary mis-match , which can be more problematic in product search . For example, synonyms and hypernyms are difficult tohandle . Many existing works aim to solve this problem byincorporating knowledge graph or having a dedicated queryunderstanding component . However, these approaches needa huge amount of domain expertise, and the cost of maintainingthese components is high, since the catalog and product vocabularyfrequently change in e-commerce.More recently, neural retrieval systems have been proposed and deployed in production systems and have showngreat success in bridging the vocabulary gap. However, neuralsystems are limited by the fact that the embedding size cannot betoo large due to latency concerns. This is problematic when dealingwith rare tokens .In this paper, we describe the hybrid system used in productionat Walmart.com and how it overcomes the individual limitationsof traditional text-match retrieval and neural retrieval. We demon-strate the benefit of such a system for tail queries and highlightthe learnings we had in the process of bringing the system to pro-duction. These include various challenges related to training themodel, as well as engineering challenges in deploying the modelto production while keeping the cost-to-serve low. We describe asolution that strikes a good balance between retrieval performanceand model complexity in the context of product search.The novelty of the paper are as follows:",
  "RELATED WORK": "Neural information retrieval (NIR) has been a popular topic in thesearch community recently. It leverages a set of sub-topics such asunsupervised learning of text embeddings like word2vec , deepSiamese models based on query logs like CLSM and DSSM, and query document interaction-based models like kernelpooling . A good summary of the field can be obtained in .In the authors consider the tradeoff between sparse and denseretrieval and propose a multi embedding approach.Multiple companies have described their production systemsleveraging semantic retrieval. In , a two-tower neural modelis trained using a mixed negative sampling in addition to batchrandom negatives. Baidu described a production system thatleverages multiple model pre-training strategies. In Face-book presented a system that combines an inverted index withsemantic retrieval; the presented architecture includes multiple product features like images and titles. Taobao proposed away to better learn relevance from user engagement data. Searsused embeddings to represent products for a recommendation sys-tem . In , Amazon presented a retrieval system based on abag-of-words model; similar to our system, semantic retrieval wasused in combination with a standard inverted index system. In a residual-based learning framework was used to learn embeddingsthat compensate for shortcomings of the inverted index. In ourapproach the two systems are created separately.Negative item selection. Multiple papers have investigated theproblem of selecting negative samples to be used during train-ing. Several works considered caching embeddings for the entiredataset . In , an iterative approach was suggested to findnegative samples. We follow a similar approach, and extend it toimprove the results in the common situation where not all relativeitems are known for a query. In , a streaming negative cachewas used, but it cannot work for dual encoder training.Multiple embeddings. There are also approaches where queriesand items are represented by multiple embeddings for retrieval andranking . SPLADE produces a sparse represen-tation at the token level that improves storage requirements, andCOIL produces a token-level representation only for matchingbetween query and document.Training strategies. In , they showed for a question-answeringtask how a simple training strategy can effectively beat a state-of-the-art system. In , they used a teacher cross-interaction model,to help the training and selection of true negatives.",
  "ARCHITECTURE": "We propose a hybrid architecture that leverages the advantages ofboth traditional inverted index and neural retrieval. A traditionalinverted index is still state of the art for retrieving documents withrare tokens such as product ids and model numbers. Moreover,our production inverted index has capabilities like facet navigationand category filtering, which are hard to replicate using semantic re-trieval alone. On the other hand, semantic retrieval helps bridge thevocabulary gap especially for tail queries; it helps with synonyms,misspellings, and other query variants that users type. Semanticretrieval also helps to better understand the semantics of longerqueries that might contain a nuanced intent from the user.The overall architecture is shown in . When a user typesa query, it is directed to the Query Planner to generate a queryplan for inverted-index retrieval as well as a query embeddingvector sent to the ANN Fetcher. The query embedding is then sentto an approximate nearest neighbor (ANN) index to retrieve theitems with the closest embeddings. The ANN index contains theembeddings of all the products currently available in our catalog.When new products become available, a dedicated pipeline feeds theproduct information to the product embedding model to generatethe embeddings. The embeddings are then stored in the updatedANN index.Both the inverted index and ANN index retrieve a set of productswhich are merged to become the recall set. Finally, the retrieveditems are ranked by our re-ranking system to produce the final listof products to be shown to the customers.",
  "Semantic Retrieval at WalmartKDD 22, August 1418, 2022, Washington, DC, USA": "item embedding (). Due to system-level limitation, itemembedding storage is updated through a publish-subscribe basedmessaging pipeline that does not accept partial updates. Reducingthe size of embedding is beneficial as it allows the item embeddingand the ANN index to be refreshed more frequently. The cost forupdating the ANN index, which depends on the storage size of theinput, is also taken into account.We therefore investigated two different strategies to reduce thesize of the embeddings. In the first approach, we added a linearprojection layer to reduce the embedding size to 368, 256, 128, and 64.In the second approach we used a transformer architecture that hasa smaller embedding size. We specifically picked the MiniLM architecture with 12 layers and an embedding size of 368 and theXtremeDistil architecture with 6 layers and an embedding sizeof 368. In both cases, the implementation and the checkpoint wereprovided by Huggingface . As shown in the result ,the linear projection is very effective in reducing the size of theembedding with very little performance cost.",
  "SEMANTIC MODEL": "The semantic model architecture is a two tower structure as shownin . Each tower produces an embedding for query andproduct respectively. The score of a query and product pair is thecosine similarity of the embeddings. We experimented with theinner product of the embedding as well (see ).The product information consists of a title, description, and anumber of attribute values. Attribute values are, for example, color,brand, material. Attributes are not always available for all products.We experimented with different number of attributes in .There are two main classes of model used. The first one is atraditional bag of words model (BoW) and the second isbased on a BERT architecture. The BERT based model is farsuperior for this application and it will be our main focus. We reportthe performance of the BoW model as a baseline.",
  "Models": "We have experimented with different transformer architecture byleveraging HuggingFace pre-trained models repository 2. Specifi-cally we have used the BaseBERT with 12 layers and 1024 embeddingsize and DistillBERT with 6 layers and 768 embedding size. We re-port below on the performance of different model architectures. Weuse the pre-trained tokenizer, and all training is done by startingwith the pre-trained model. Our experiments use identical towers(Siamese network). For most experiments, we use the embeddingvector corresponding to the special token \"[CLS]\".In our experiments in , titles provide most of the sig-nal for retrieval. We use a baseline model with only title as input.We then added more attributes to the input. Each attribute is con-catenated to the title by using a prefix which is an unused tokenselected specifically for the attribute. For example, when addingthe color red to a product, the input looks like \"[title tokens] [colortoken] red\". This technique allows the model to determine whichattributes have been concatenated. We experiment with four com-mon attributes (product category, brand, color and gender) andwith a longer list of 26 attributes including the basic ones.",
  "Loss function": "We used a sampled softmax loss where for each query we haveboth relevant and irrelevant products with a corresponding score.As described in section 5.2, selecting negative items is essential forgood performance. We also notice that allowing multiple relevantproducts during training helps. Since there are, in general, manyrelevant products for a given query, we sample a few relevantproducts for each epoch.",
  "exp cos , /(1)": "Equation 1 shows the loss contribution of query , where isthe number of products under consideration, is the score ofproduct for query , and , are the embedding for query and product respectively. is a temperature factor that is trainedtogether with all the model parameters. Other loss functions havebeen evaluated including pointwise and pairwise losses, but thesoftmax loss outperforms them and is very robust during training.The number of products considered for each query is = 20. Thereis a trade-off between and the batch size. Increasing reducesthe batch size and therefore also the in-batch negatives.We will discuss in how the scores are selected andhow the products are selected in more detail.",
  "DATA": "The training of the model is performed on engagement data col-lected at Walmart.com over a one year period. The data containsthe top 2 million queries based on number of impressions. For eachquery, we consider the products that were shown to the customers,and construct labels based on the corresponding number of pur-chases, clicks, and impressions. Note that we do not account forpresentation bias, since in a retrieval model, the order in whichitems are returned is not relevant. In our experience correctingfor presentation bias adds more complexity for a negligible perfor-mance improvement.",
  "Labeling": "For each query and product pair we assign a score between 0and 10. Since the loss is insensitive up to a multiplicative factor,the range of score can be selected arbitrarily. Query-product pairswith purchases are assigned the highest scores between 10 and 8.Products that only received clicks are assigned scores between 7and 5, and if products only received impressions scores between4 and 2. We assigned scores of 0 to negative items as described insection 5.2.Ordered products are assigned a score based on a smoothedestimation of their order rate =+",
  "+ where is thesmoothing factor. The product with the highest order rate receivesa score of 10, while the product with the lowest order rate receivesa score of 8 following equation = (10 8) min()": "maxmin + 8. Asimilar approach is used for products that received only clicks.Although this labeling strategy is arbitrary, it has been shownto be effective in practice. In particular differentiating the scoresof items that have been purchased from items that have only beenclicked is in our experience essential to create effective retrievalmodels. This is consistent with even though we use a differentloss.",
  "during training and hard negatives selected offline for a giventrained model": "5.2.1In batch negatives. Selecting negative items from a batch isa common technique that reduces the computation because theembeddings are already computed for all the products in the batch.Since for memory constraints, it is not possible to select all productsin a batch as negative, we experimented with two ways of samplingthem. The first approach is a random selection. The second approachis to perform a hard in batch selection . This approach focuseson only the hardest samples in a training batch. For a given queryin a batch the negatives are generated by using all the productsfrom other queries in the batch as a pool and selecting only theones which receive the highest cosine similarity. 5.2.2Hard negative search. Out of the several negative construc-tion approaches available, we used ANCE to which we addednovel strategies that cause the model to retrieve more relevantresults, thereby boosting the recall metric. Unlike in the originalpaper where the relevant items are known, in the e-commercesetting, there can be many relevant products for a given query. For example, a query like \"red shoes\" can have hundreds of relevantproducts. Since it is impractical to editorially evaluate all of them,we introduce a set of different heuristics to overcome this limitation.The procedure to generate hard negatives is as follows: 1) Gener-ate top-k results for each query in training corpus using a partiallytrained model 2) Select negatives from top-k results based on aselection strategy (see below) 3) inject the hard negatives into thetraining corpus and resume training 4) repeat the steps 1-3.We explored three strategies to select negatives out of the top-kresults. PT match: For each item in the top-k results of a given query,if the item is not in the training data and matches the producttype (PT) attribute of the top-m items in the training data, itis removed from the set of negative candidates. Token match: The negative candidates generated using thisstrategy are more strict. We take PT match candidates as aninput to this and for each candidate item find its overlap withquery tokens. If the overlap score is below threshold t, wekeep the item; otherwise discard it. We have experimentedwith different thresholds and found that t=0.5 gave bestresults. Student-Teacher: We trained a separate model (Teacher) whereinthe query tokens can directly attend to product tokens. Theteacher model is a MonoBERT-based single encodernetwork that concatenates query and the product informa-tion together as input. The top-k items are generated viathis model, and the above PT match filter is applied to getthe final negative candidates that are then injected into theembedding model (Student).",
  "Multi Embeddings": "E-commerce queries often have multiple interpretations. For exam-ple, the query \"apple\" could refer to the fruit or to the electronicsbrand. For this reason, we explore the possibility of having multipleembeddings to represent a query or product. For a query, this meansto generate embeddings, and make calls to the ANN service.Denoting as the th embedding of the th query, the product score is maxcos( , )for 1 . . .. Following , we let the embeddings correspond to the first tokens. This is a naturalextension of using the embedding corresponding to the first token\"[CLS]\". For multiple embeddings on the product side, like in ,there are embeddings, and we therefore store in the ANN service times the number of products.",
  "RE-RANK STAGE": "In this section, we describe how the two recall sets from the invertedindex and semantic retrieval are merged and ranked. A key aspectof e-commerce search is the presence of useful ranking features thatcapture the different query and product attributes. The featuresused in the ranking model can be organized into the followinggroups:",
  "Query Features: These features capture the different attributesand properties learnt from the query. For example, queryattributes like product type, brand, query length": "Item Features: These features capture the different productattributes as well as engagement features computed at theproduct level. For example, title attributes, title length, userratings, user reviews, product sales, product department, etc. Query-Item Features: These features capture the relationsrelated to the query-item pair. For example, BM25 text matchscore, query-item engagement, query-item attribute matchscore. also shows the re-rank architecture with the query-product BERT embedding feature. The query is encoded to thequery embedding at runtime. The item embedding for all the itemsin the catalog are pre-generated and saved to the item embeddingdatastore. For all the items from both the inverted index and ANN,the corresponding item embeddings are fetched from the item em-bedding datastore.At the re-rank layer, the cosine similarity between query anditem embeddings is included as a query-item feature. The re-rankmodel, which is a Gradient Boosted Decision Tree (GBDT) model,ranks all items in the merged list.",
  "ANN service": "One major challenge for online neural product retrieval is the trade-off among accuracy, speed, and memory. Brute force algorithms thatretrieve the exact closest vectors of a query vector from millionsof item vectors with respect to a pre-defined distance cannot beused in the production setup due to their high time complexity.Some amelioration can be obtained by first compressing the datasize so that it may be easier for the vectors to be fed into memory.Such techniques include but are not limited to locality sensitivehashing (LSH), quantization, and product quantization (PQ). Forfaster retrieval, not all indices will be scanned when a query isexecuted. Candidate vectors are usually split into multiple clusters,and only vectors in the closest few clusters will be scanned - if notfurther reduced by other pruning methods.Tools and services that support approximate nearest neighbor(ANN) search have emerged in the past few years. One popular toolamong developer communities and has the potential to be produc-tionized is FAISS3. However, to reduce the cost of maintenance andto minimize the system level risk, as the real traffic might surgeduring certain periods of time, we use a managed ANN serviceavailable commercially.As with all other ANN algorithms, hyperparameter tuning isnecessary to achieve the desired recall quality within an acceptablelevel of latency, storage, and computation cost. Our experimentsshow that with normalized vectors of dimension 256, the ANNservices can yield 99% for recall@20, evaluated against the fullnearest neighborhood search, with an average latency around 13 ms;",
  "Runtime Implementation": "The in-house search engine accepts queries from customers. If aquery is eligible for neural retrieval (i.e. a tail query), the queryplanner sends its embedding vector, which is the output of thequery encoder, to the ANN index. The results of the ANN indexare cached with a preset time-to-alive (TTL) to reduce latency andcost.The recall federation framework retrieves products from boththe ANN index and the inverted index, and then de-duplicates andmerges the product sets before sending them to the rerank phase.",
  "Query modeling latency and consideration": "In real production scenarios, a large portion of queries submittedby users are not predictable and hence cannot be vectorized offlinebeforehand to reduce the overall runtime latency. For simple modelssuch as the Bag-of-Words (BoW) model, the computation is fast andusually does not raise any concern. However, the power of suchsimple models is also limited for this application. In contrast, BERT-based models describe in section 4 and their variations have drawna lot of attentions to their capacity in semantic understanding, aswell as to its application in solving search ranking problems, andhave shown in our experiments there much higher performancealong multiple metrics. Unfortunately, BERT-based models alsorequire more computation resources and may increase responsetime of the runtime system.We integrate the BERT-based query encoder as part of our queryunderstanding module. We found that with the same capacity ofcomputing clusters with CPUs, the 6-layer Distilled BERT modelalmost doubled the P99 latency of the query understanding module.Our original plan was to reduce the number of layers from 6 to 2.However, we found that the latency was not reduced linearly withthe number of layers and the contribution of the token embeddinglookup was substantial.Our model is exported from Torch checkpoints into ONNX4 format and is served in a Java codebase. The embedding lookupoperation implementation in the ONNX backend seems to be highlyinefficient. Therefore, in our experiments, we tried to move thisembedding lookup to Java hashmap and to feed the model with",
  ": Impact on latency of the query understanding mod-ule": "gathered token embedding matrices. Furthermore, we found thatsetting a dynamic query length was causing extra latency. For thisreason, we opted for fixing the maximum length of the query andusing padding. As shown in , this improve latency evenfurther.The impact of different model variations on the latency of ourquery understanding module are logged in , with the originalBERT 6-layer model normalized as 100%.To further minimize the latency impact, we deployed the modelsto individual computing clusters with GPUs for remote serving.Using servers with 4 cores of Nvidia Tesla T4 GPU, we managedto eliminate the extra latency introduced by our 6-layer DistilledBERT model on top of our existing query understanding module.",
  "EXPERIMENTS AND RESULTS": "All modeling effort has been performed using a dataset of 2 millionqueries collected over a period of one year from Walmart.comlogs. The dataset was divided between a training and validationdataset of size 90% and 10% respectively. The test dataset contains140 thousand queries disjoint from both training and validationfor which a set of relevant items has been identified using userengagement and editorial feedback.The training was performed using PyTorch and Hugging-Face. Adam was used to train with a learning rate of 105. Thebatch size was 40, and the number of products during training foreach query was set to 20.",
  "Offline Metrics": "We evaluate the models using a recall metric which measures thepercentage of relevant products retrieved for a golden dataset. Thegolden dataset contains a set of relevant products for 140 thousandqueries out of a set of around 7 million products. Since scoringall products for a given query is not possible, only a subset of therelevant products is known.It has been clear that the recall metric alone does not fully capturethe performance of the semantic search. We noticed that the modelcould have a relatively high recall while at the same time retrieving aset of completely unrelated products. This is in part due to the smalluser engagement available for some products as well as the presencein the catalog of products with noisy titles. For this reason, wedefine a new metric that tries to measure the approximate numberof irrelevant products. Since in e-commerce, each product has aproduct category associated with it (e.g. dining chairs, toothpaste),",
  ": Offline model performance by number of layers andembedding size, with random negatives": "we make the simplifying assumptions that all relevant products fora query should have the same product category of at least one of therelevant products in the golden dataset. This is the same assumptionmade in our approach to select negative items in .2. Themetric that we call category recall, is defined as the percentage ofproducts in the recall set that has the same product category of atleast one product in the golden dataset. Clearly, this is not a perfectmetric because often there could be more product categories thatare relevant for a query and not all of them are represented in thegolden dataset. Moreover, product category can often be incorrectlyassigned adding noise to this metric. Nevertheless, we have foundthis metric very useful in driving our modeling effort and capable ofcapturing what our manual inspection had found. During training,we compute the recall metric only on the batch as a proxy for theoverall recall, and we use it to terminate the training.",
  "Offline model results": "In this section, we report the results of our modeling effort basedon the two metrics described in .1. All the results are re-ported with respect to a baseline model specified for each set ofexperiments. We also report as baseline the performance of a simpleinverted index implementation where only the title of the productis indexed. In , we report all the main findings with respectto number of layers and embedding size. We observed that theBoW model had lower offline numbers compared to inverted indexlookup. After switching to BERT based model, we were able to beatthe baseline by 8.25% in Recall@40 and switching to DistilBERTmodel gave an extra lift of 4%. We experimented with includingproduct attributes, and got a further 6% boost after including at-tributes like product category, brand, color and gender to the inputfor a product. But, when we added even more product attributes(like description etc.), we did not see any further improvement inmodel performance. Notice how Category Recall@40 is dramati-cally lower without the use of negatives.In , the performance of different negative selection tech-niques are shown. The first row corresponds to the best model in. When we added hard negatives to the training data, de-scribed in section 5.2, we observed 0.87% lift in Recall@40 and 18%lift Category Recall@40 when using only product category match.When combining that with token match, the improvement on recallis 2.8%. On the other hand, our student-teacher negative selection",
  ": Offline model performance for different embeddingsize reduction": "does not work as well as the other approaches. Finally, adding hardin-batch negatives improved the recall by around 5%.In we show the results of the different embedding sizereductions described in section 5.3 The linear projection matrixperforms at a very similar performance than the original modelup to a size of 256 before dropping in performance. Moreover, at asimilar embedding size, a projection layer is superior to architecturethat have smaller sizes like the MiniLM. Therefore, 256 is the sizeused in production. shows the results of the multiple embeddings using 3embeddings. As we can see the multiple embedding doesnt seemto bring any advantage over a single embedding in our experi-ments. Considering the higher cost of the solution, it has not beenimplemented. shows that freezing the token embeddings during train-ing provides a small improvement and seems to confirm the ideathat the model can better generalize. Finally shows theeffect of different pooling approaches. The average pooling anddefault pooling on [CLS] tokens have almost identical performance.Max pooling has the worst performance.",
  "Live Experiments": "8.3.1Manual Evaluation Results. We evaluated the performanceof our proposed architecture by using human assessors to evaluatethe top-10 ranking results of the proposed architecture and thecurrent production at Walmart. The candidate model uses an em-bedding size of 256 and uses the \"[CLS]\" pooling. For a query, thehuman assessors are shown the product image, title and price ofthe product along with the product link in Walmart website. Theyrate the relevance of the product on a 3-point grading scale as notrelevant, relevant with missing attributes, and perfect match. Thequeries are selected via random sample of the search traffic fromthe tail segment. As shown in , the proposed architecturesignificantly improves the relevance of tail queries. Note that BERTembedding with dimension 256 performed similarly or even betterthan dimension 768. 8.3.2Interleaving Results. We assessed the user engagement per-formance of our proposed architecture compared with the currentproduction at Walmart using interleaving . We evaluate twodifferent models that have the same DistillBERT architecture but inone case they have a linear projection layer to an embedding size of256. Interleaving is an online evaluation approach where each useris presented with a combination of ranking results from both thecontrol and variation. We observe the add-to-carts (ATC) betweenthe control and variation ranking. The metric measured is ATC@40which is the count of add-to-carts in the top 40 position betweencontrol and variation ranking models. The results shown in demonstrate the effectiveness of the proposed architecture inimproving the user engagement performance. We notice a similar",
  "LESSONS LEARNED": "Among the many things that we learned while creating this system,we would like to highlight a few of them.Inner product vs. cosine similarity. During the model training,we experimented with inner product. The inner product is morestable during training and does not require the temperature factor shown in Eq. 1. This removes the need to select and in generalproduces better results. Unfortunately, inner product was muchharder to optimize when creating the ANN index, compared tocosine similarity. For this reason, we eventually focused on cosinesimilarity only.Blending features. Many text fields are generally available for eachproduct. These include different descriptions and many attributes.There was a common belief that the description of the productwould help improve the recall performance. In all our experiments,we could not extract any boost in performance. This is probablybecause descriptions can contain a lot of irrelevant text that simplyadds noise.Model complexity. As seen in the , for this application,a very deep model or very large embedding size is not necessaryto achieve top performance. This is probably because queries andproduct titles are not very complex from a semantic perspective.",
  "Thorsten Joachims et al. 2003. Evaluating Retrieval Performance Using Click-through Data": "Shubhra Kanti Karmaker Santu, Parikshit Sondhi, and ChengXiang Zhai. 2017. Onapplication of learning to rank for e-commerce search. In Proceedings of the 40thinternational ACM SIGIR conference on research and development in informationretrieval. 475484. Vladimir Karpukhin, Barlas Ouz, Sewon Min, Patrick Lewis, Ledell Wu, SergeyEdunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906 (2020).",
  "Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-mization. arXiv preprint arXiv:1412.6980 (2014)": "Yiqun Liu, Kaushik Rangadurai, Yunzhong He, Siddarth Malreddy, Xunlong Gui,Xiaoyi Liu, and Fedor Borisyuk. 2021. Que2search: Fast and accurate query anddocument understanding for search at facebook. In Proceedings of the 27th ACMSIGKDD Conference on Knowledge Discovery & Data Mining. 33763384. Yi Luan, Jacob Eisenstein, Kristina Toutanova, and Michael Collins. 2021. Sparse,Dense, and Attentional Representations for Text Retrieval. Transactions of theAssociation for Computational Linguistics 9 (2021), 329345. Sean MacAvaney, Franco Maria Nardini, Raffaele Perego, Nicola Tonellotto, NazliGoharian, and Ophir Frieder. 2020. Efficient document re-ranking for transform-ers by precomputing term representations. In SIGIR. 4958.",
  "Discovery & Data Mining. 28762885": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, GregoryChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Des-maison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, AlykhanTejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and SoumithChintala. 2019. PyTorch: An Imperative Style, High-Performance Deep LearningLibrary. In Advances in Neural Information Processing Systems 32, H. Wallach,H. Larochelle, A. Beygelzimer, F. d'Alch-Buc, E. Fox, and R. Garnett (Eds.). Cur-ran Associates, Inc., 80248035. Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxi-ang Dong, Hua Wu, and Haifeng Wang. 2021. RocketQA: An optimized trainingapproach to dense passage retrieval for open-domain question answering. InNAACL-HLT. Fatemeh Sarvi, Nikos Voskarides, Lois Mooiman, Sebastian Schelter, and Maartende Rijke. 2020. A Comparison of Supervised Learning to Match Methods forProduct Search. SIGIR Workshop on eCommerce (2020).",
  "Thomas Wolf et al. 2020. Transformers: State-of-the-Art Natural LanguageProcessing. In EMNLP": "Ji Xin, Rodrigo Nogueira, Yaoliang Yu, and Jimmy Lin. 2020. Early exiting BERTfor efficient document ranking. In Proceedings of SustaiNLP: Workshop on Simpleand Efficient Natural Language Processing. 8388. Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. 2017.End-to-end neural ad-hoc ranking with kernel pooling. In Proceedings of the 40thInternational ACM SIGIR conference on research and development in informationretrieval. 5564.",
  "Chenyan Xiong, Russell Power, and Jamie Callan. 2017. Explicit Semantic Rankingfor Academic Search via Knowledge Graph Embedding. In WWW, 2017. 12711279": "Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett,Junaid Ahmed, and Arnold Overwijk. 2020. Approximate nearest neighbor nega-tive contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808(2020). Ji Yang, Xinyang Yi, Derek Zhiyuan Cheng, Lichan Hong, Yang Li, Simon Xiaom-ing Wang, Taibai Xu, and Ed H Chi. 2020. Mixed negative sampling for learningtwo-tower neural networks in recommendations. In Companion Proceedings ofthe Web Conference 2020. 441447. Shaowei Yao, Jiwei Tan, Xi Chen, Keping Yang, Rong Xiao, Hongbo Deng, andXiaojun Wan. 2021. Learning a Product Relevance Model from Click-ThroughData in E-Commerce. In Proceedings of the Web Conference. 28902899. Lixin Zou, Shengqiang Zhang, Hengyi Cai, Dehong Ma, Suqi Cheng, ShuaiqiangWang, Daiting Shi, Zhicong Cheng, and Dawei Yin. 2021. Pre-trained languagemodel based ranking in Baidu search. In Proceedings of the 27th ACM SIGKDDConference on Knowledge Discovery & Data Mining. 40144022."
}