{
  "Abstract": "Graph-based models and contrastive learning have emerged as promi-nent methods in Collaborative Filtering (CF). While many exist-ing models in CF incorporate these methods in their design, thereseems to be a limited depth of analysis regarding the foundationalprinciples behind them. This paper bridges graph convolution, apivotal element of graph-based models, with contrastive learningthrough a theoretical framework. By examining the learning dy-namics and equilibrium of the contrastive loss, we oer a freshlens to understand contrastive learning via graph theory, empha-sizing its capability to capture high-order connectivity. Buildingon this analysis, we further show that the graph convolutional lay-ers often used in graph-based models are not essential for high-order connectivity modeling and might contribute to the risk ofoversmoothing. Stemming from our ndings, we introduce SimpleContrastive Collaborative Filtering (SCCF), a simple and eectivealgorithm based on a naive embedding model and a modied con-trastive loss. The ecacy of the algorithm is demonstrated throughextensive experiments across four public datasets. The experimentcode is available at",
  "This work was done at Universit de Montral": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor prot or commercial advantage and that copies bear this notice and the full cita-tion on the rst page. Copyrights for components of this work owned by others thanthe author(s) must be honored. Abstracting with credit is permitted. To copy other-wise, or republish, to post on servers or to redistribute to lists, requires prior specicpermission and/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08 ACM Reference Format:Yihong Wu, Le Zhang, Fengran Mo, Tianyu Zhu, Weizhi Ma, and Jian-YunNie. 2024. Unifying Graph Convolution and Contrastive Learning in Col-laborative Filtering. In Proceedings of the 30th ACM SIGKDD Conferenceon Knowledge Discovery and Data Mining (KDD 24), August 2529, 2024,Barcelona, Spain. ACM, New York, NY, USA, 12 pages.",
  "Introduction": "Recently, contrastive learning has become the state-of-the-art methodfor self-supervised learning, marking signicant achievements acrosscomputer vision , natural language processing , andmulti-modality . Given its success in various domains, thereis burgeoning interest in harnessing contrastive learning withinCollaborative Filtering (CF) . Wu et al. exploredthe eectiveness of the Sampled SoftMax loss function within CF.Chen et al. examined the impact of the temperature parameteron a models performance in contrastive learning. Both Zhou et al. and Zhang et al. highlighted the ability of contrastive lossto address bias in recommendation applications. Zhou et al. employed contrastive learning to address cold-start problems inrecommendation. Yet, none of these studies address the fundamen-tal question: How does contrastive learning operate within collabora-tive ltering? While studies such as Wang and Isola and Wanget al. have elucidated that contrastive learning serves to alignthe embeddings of interacted user-item pairs and to uniformly dis-tribute embeddings across a hypersphere, the mechanism and equi-librium of these forces remain veiled.Beyond contrastive learning, graph-based methods constituteanother burgeoning research avenue in CF. Drawing inspirationfrom the success of Graph Convolutional Networks (GCN) , CFresearchers have discovered that integrating graph convolutionallayers with basic learnable embeddings signicantly enhances rec-ommendation quality . While instances exist where researcherssimultaneously employ contrastive learning and graph-based ap-proaches , these domains are often treated as distinct andseparate, resulting in a lack of comprehensive exploration. Moti-vated by the recent breakthrough on a theoretical link betweencontrastive learning and graph theory , this paper",
  "KDD 24, August 2529, 2024, Barcelona, SpainYihong Wu et al": "David H Ackley, Georey E Hinton, and Terrence J Sejnowski. 1985. A learningalgorithm for Boltzmann machines. Cognitive science 9, 1 (1985), 147169. Xuheng Cai, Chao Huang, Lianghao Xia, and Xubin Ren. 2023. LightGCL: Sim-ple yet eective graph contrastive learning for recommendation. arXiv preprintarXiv:2302.08191 (2023). Jiawei Chen, Junkang Wu, Jiancan Wu, Sheng Zhou, Xuezhi Cao, and XiangnanHe. 2023. Adap-tau: Adaptively Modulating Embedding Magnitude for Recom-mendation. arXiv preprint arXiv:2302.04775 (2023). Ting Chen, Simon Kornblith, Mohammad Norouzi, and Georey Hinton. 2020.A simple framework for contrastive learning of visual representations. In Inter-national conference on machine learning. PMLR, 15971607. Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. Simcse: Simple contrastivelearning of sentence embeddings. arXiv preprint arXiv:2104.08821 (2021). Xue Geng, Hanwang Zhang, Jingwen Bian, and Tat-Seng Chua. 2015. Learningimage and user features for recommendation in social networks. In Proceedingsof the IEEE international conference on computer vision. 42744282. Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George EDahl. 2017. Neural message passing for quantum chemistry. In Internationalconference on machine learning. PMLR, 12631272. Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MITPress. Jean-Bastien Grill, Florian Strub, Florent Altch, Corentin Tallec, PierreRichemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, ZhaohanGuo, Mohammad Gheshlaghi Azar, et al. 2020. Bootstrap your own latent-a newapproach to self-supervised learning. Advances in neural information processingsystems 33 (2020), 2127121284. Je Z HaoChen, Colin Wei, Adrien Gaidon, and Tengyu Ma. 2021. Provable guar-antees for self-supervised deep learning with spectral contrastive loss. Advancesin Neural Information Processing Systems 34 (2021), 50005011. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.2020. Momen-tum contrast for unsupervised visual representation learning. In Proceedings ofthe IEEE/CVF conference on computer vision and pattern recognition. 97299738. Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and MengWang. 2020. Lightgcn: Simplifying and powering graph convolution network forrecommendation. In Proceedings of the 43rd International ACM SIGIR conferenceon research and development in Information Retrieval. 639648. Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-SengChua. 2017. Neuralcollaborative ltering. In Proceedings of the 26th internationalconference on world wide web. 173182. Thomas Hofmann, Bernhard Schlkopf, and Alexander J Smola. 2008. Kernelmethods in machine learning. (2008). Elvin Isu, Fernando Gama, David I Shuman, and Santiago Segarra. 2024. Graphlters for signal processing and machine learning on graphs. IEEE Transactionson Signal Processing (2024). Thomas N Kipf and Max Welling. 2016.Semi-supervised classication withgraph convolutional networks. arXiv preprint arXiv:1609.02907 (2016). Yehuda Koren. 2008. Factorization meets the neighborhood: a multifaceted col-laborative ltering model. In Proceedings of the 14th ACM SIGKDD internationalconference on Knowledge discovery and data mining. 426434. Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization tech-niques for recommender systems. Computer 42, 8 (2009), 3037. Dongha Lee, SeongKu Kang, Hyunjun Ju, Chanyoung Park, and Hwanjo Yu.2021. Bootstrapping user and item representations for one-class collaborative l-tering. In Proceedings of the 44th International ACM SIGIR Conference on Researchand Development in Information Retrieval. 317326. Omer Levy and Yoav Goldberg. 2014. Neural word embedding as implicit matrixfactorization. Advances in neural information processing systems 27 (2014). Dawen Liang, Rahul G Krishnan, Matthew D Homan, and Tony Jebara. 2018.Variational autoencoders for collaborative ltering. In Proceedings of the 2018world wide web conference. 689698. Kelong Mao, Jieming Zhu, Jinpeng Wang, Quanyu Dai, Zhenhua Dong, Xi Xiao,and Xiuqiang He. 2021. SimpleX: A simple and strong baseline for collaborativeltering. In Proceedings of the 30th ACM International Conference on Information& Knowledge Management. 12431252. Kelong Mao, Jieming Zhu, Xi Xiao, Biao Lu, Zhaowei Wang, and Xiuqiang He.2021. UltraGCN: ultra simplication of graph convolutional networks for rec-ommendation. In Proceedings of the 30th ACM International Conference on Infor-mation & Knowledge Management. 12531262. Bibek Paudel, Fabian Christoel, Chris Newell, and Abraham Bernstein. 2016.Updatable, accurate, diverse, and scalable recommendations for interactive ap-plications. ACM Transactions on Interactive Intelligent Systems (TiiS) 7, 1 (2016),134. Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, and Jie Tang. 2018.Network embedding as matrix factorization: Unifying deepwalk, line, pte, andnode2vec. In Proceedings of the eleventh ACM international conference on websearch and data mining. 459467. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sand-hini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.2021. Learning transferable visual models from natural language supervision. InInternational conference on machine learning. PMLR, 87488763. Raksha Ramakrishna, Hoi-To Wai, and Anna Scaglione. 2020. A user guide tolow-pass graph signal processing and its applications: Tools and applications.IEEE Signal Processing Magazine 37, 6 (2020), 7485. Steen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2012. BPR: Bayesian personalized ranking from implicit feedback. arXivpreprint arXiv:1205.2618 (2012). Aliaksei Sandryhaila and Jos MF Moura. 2013. Discrete signal processing ongraphs. IEEE transactions on signal processing 61, 7 (2013), 16441656. Yifei Shen, Yongji Wu, Yao Zhang, Caihua Shan, Jun Zhang, B Khaled Letaief,and Dongsheng Li. 2021. How powerful is graph convolution for recommenda-tion?. In Proceedings of the 30th ACM international conference on information &knowledge management. 16191629. Zhiquan Tan, Yifan Zhang, Jingqin Yang, and Yang Yuan. 2023.Con-trastive Learning Is Spectral Clustering On Similarity Graph. arXiv preprintarXiv:2303.15103 (2023). Hanghang Tong, Christos Faloutsos, and Jia-Yu Pan. 2006. Fast random walkwith restart and its applications. In Sixth international conference on data mining(ICDM06). IEEE, 613622. Ferdinand Verhulst. 2006. Nonlinear dierential equations and dynamical systems.Springer Science & Business Media. Chenyang Wang, Yuanqing Yu, Weizhi Ma, Min Zhang, Chong Chen, Yiqun Liu,and Shaoping Ma. 2022. Towards representation alignment and uniformity incollaborative ltering. In Proceedings of the 28th ACM SIGKDD Conference onKnowledge Discovery and Data Mining. 18161825. Feng Wang and Huaping Liu. 2021. Understanding the behaviour of contrastiveloss. In Proceedings of the IEEE/CVF conference on computer vision and patternrecognition. 24952504. Tongzhou Wang and Phillip Isola. 2020. Understanding contrastive representa-tion learning through alignment and uniformity on the hypersphere. In Interna-tional Conference on Machine Learning. PMLR, 99299939. Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019.Neuralgraph collaborative ltering. In Proceedings of the 42nd international ACMSIGIR conference on Research and development in Information Retrieval. 165174. Xiang Wang, Hongye Jin, An Zhang, Xiangnan He, Tong Xu, and Tat-Seng Chua.2020. Disentangled graph collaborative ltering. In Proceedings of the 43rd in-ternational ACM SIGIR conference on research and development in informationretrieval. 10011010. Yifei Wang, Qi Zhang, Tianqi Du, Jiansheng Yang, Zhouchen Lin, and YisenWang. 2023. A message passing perspective on learning dynamics of contrastivelearning. arXiv preprint arXiv:2303.04435 (2023). Yifei Wang, Qi Zhang, Yisen Wang, Jiansheng Yang, and Zhouchen Lin. 2022.Chaos is a ladder: A new theoretical understanding of contrastive learning viaaugmentation overlap. arXiv preprint arXiv:2203.13457 (2022). Jiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun Lian, andXing Xie. 2021. Self-supervised graph learning for recommendation. In Proceed-ings of the 44th international ACM SIGIR conference on research and developmentin information retrieval. 726735. Jiancan Wu, Xiang Wang, Xingyu Gao, Jiawei Chen, Hongcheng Fu, Tianyu Qiu,and Xiangnan He. 2022. On the eectiveness of sampled softmax loss for itemrecommendation. arXiv preprint arXiv:2201.02327 (2022). Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. 2018. Unsupervisedfeature learning via non-parametric instance discrimination. In Proceedings ofthe IEEE conference on computer vision and pattern recognition. 37333742. Jheng-Hong Yang, Chih-Ming Chen, Chuan-Ju Wang, and Ming-Feng Tsai. 2018.HOP-rec: high-order proximity for implicit recommendation. In Proceedings ofthe 12th ACM conference on recommender systems. 140144. Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Lizhen Cui, and Quoc Viet HungNguyen. 2022. Are graph augmentations necessary? simple graph contrastivelearning for recommendation. In Proceedings of the 45th international ACM SIGIRconference on research and development in information retrieval. 12941303. Hongyuan Zha, Xiaofeng He, Chris Ding, Horst Simon, and Ming Gu. 2001. Bi-partite graph partitioning and data clustering. In Proceedings of the tenth inter-national conference on Information and knowledge management. 2532. An Zhang, Wenchang Ma, Xiang Wang, and Tat-Seng Chua. 2022. Incorporat-ing Bias-aware Margins into Contrastive Loss for Collaborative Filtering. arXivpreprint arXiv:2210.11054 (2022). Wayne Xin Zhao, Shanlei Mu, Yupeng Hou, Zihan Lin, Yushuo Chen, XingyuPan, Kaiyuan Li, Yujie Lu, Hui Wang, Changxin Tian, et al. 2021. Recbole: To-wards a unied, comprehensive and ecient framework for recommendationalgorithms. In Proceedings of the 30th ACM International Conference on Informa-tion & Knowledge Management. 46534664. Chang Zhou, Jianxin Ma, Jianwei Zhang, Jingren Zhou, and Hongxia Yang. 2021.Contrastive learning for debiased candidate generation in large-scale recom-mender systems. In Proceedings of the 27th ACM SIGKDD Conference on Knowl-edge Discovery & Data Mining. 39853995.",
  "Preliminaries2.1Task Denition": "Consider a classic CF setting with implicit feedback for Top-K rec-ommendation, where only binary values indicating the interactionsbetween users and items are available. Our goal is to model a sim-ilarity function (,) reecting the degree of interest user hasin item . In a Top-K recommendation scenario, the recommenderpresents a user with K items, selected based on their similarity tothe users demonstrated interests.",
  "We dene some necessary notations for this paper. Let U repre-sent the set of users and I denote the set of items. Let dataset": "D = {(1,1), (2,2), ..., (,)} denote the set of observed inter-actions between users and items, D = {1,2, . . . ,} denote themultiset of users observed in the dataset, D = {1,2, . . . ,} de-note the multiset of items, |D| = be the number of interactions.The interactions between users and items are represented by a ma-trix R R|U||I| as follows:",
  ".(2)": "Let D = (1,2, . . . ,) denote the degree matrix where isthe degree of node . Let L = D A denotes the Laplacian matrix.We denote the embedding of user and item as and , respec-tively, regardless of the encoder and use the inner product functionto decode the similarity from embeddings, i.e., (,) = .",
  "Graph Convolution": "Consider a graph G = (V, E) with nodes, where V is the setof vertices and E is the set of edges, and its adjacency matrix A R. A graph signal is a function : V R mapping a node toa real value. Intuitively, a smooth signal should share similar val-ues across connected nodes. Consequently, we use the normalizedgraph quadratic form to measure the smoothness of a graphsignal dened as () = L/2 = , 2 /2.A low value () indicates a smooth signal . Since the Lapla-cian matrix L is real and symmetric, the eigendecomposition yieldsL = UU, where = (1, 2, . . . , ) is a diagonal matrixwhose entries are eigenvalues with 1 2 , U =[1, 2, . . . , ] is a matrix of eigenvectors, R is the uni-tary eigenvector corresponding to eigenvalue . Observing that() = , it can be inferred that eigenvectors associated withsmaller eigenvalues tend to be smoother. This observation moti-vates us to dene the graph frequencies as the Laplacian eigen-values and the Graph Fourier Transformation (GFT) based on theeigenvectors U . The GFT of signal is represented as = U.This transformation maps any signal into the graph frequencyspace, in which serves as the new coordinate of and reectsthe importance of frequencies. To end this section, we give the def-inition of graph lter and graph convolution.",
  "Unifying Graph Convolution andContrastive Learning": "In this section, we begin by dening the contrastive loss functionfor analysis. Subsequently, we examine the learning dynamics ofthe contrastive loss and its relationship to graph convolution. Fol-lowing this, we identify the conditions necessary to achieve equi-librium within the contrastive loss framework. The section con-cludes with a discussion on alternative forms of loss functions.",
  "Denition of Contrastive Loss": "In the context of contrastive learning, dening positive and nega-tive samples with respect to a given anchor is crucial. In the eld ofComputer Vision, positive samples are typically generated by aug-menting the same image, whereas augmentations from dierentimages are considered negative pairs . Conversely, in CF, pos-itive samples for a user are identied as items with which the userhas interacted, while items with no interaction history are deemednegative. Perhaps the most widely adopted contrastive loss func-tion in the domain of collaborative ltering is the Sampled SoftMax(SSM) function :",
  "D exp( ) .(4)": "Assuming the joint probability of a user showing interest in anitem , denoted as (,), is proportional to exp( ), the expres-sion exp( )/ D exp( ) can be interpreted as the con-ditional probability of user interacting with item , given theuser, (,|). This formulation implies that the SSM function isdesigned to maximize the log-likelihood of this conditional proba-bility for a given user. To enable a more straightforward analysis,we propose an alternative loss function that directly maximizes thelog-likelihood of the joint probability from the observed data:",
  "(5)": "In Equation (5), for a positive pair (,), we consider all other possi-ble combinations between D and D as negative pairs. This nega-tive sampling strategy is equivalent to the batch negative samplingtrick in the case of unlimited batch size. Moreover, the denomina-tor (,)D D exp can be streamlined by focusing onunique user-item pairs and their co-occurrence frequencies. Thisleads to a simplied expression (,)UI exp , whichaccounts for the redundancy of pairs in the original formulation.",
  "Learning Dynamics of Contrastive Loss": "In this section, we will derive the learning dynamic for the con-trastive loss. According to a recent analysis on contrastive learn-ing , a contrastive loss encompasses two principal components:alignment and uniformity. The alignment component is designedto minimize the distance between positive user-item pairs withinthe embedding space, thereby enhancing their similarity. Conversely,the uniformity component aims to increase the distance among negative pairs, thereby preventing the embeddings from converg-ing to a singular point. Furthermore, this decomposition simpliesour discussion since the analysis of both the alignment and unifor-mity components are congruent. We decompose Equation (5) asfollows:",
  "(,)UI exp () ()(14)": "if (, ) is a user-item or item-user pair; or A () = 0 if (, )is a user-user or item-item pair. Equation (12) elucidates anothermessage-passing mechanism across the complete graph weightedby their respective degrees and embeddings. Now we combine Equa-tion (11) and Equation (13) to get the embedding update for Equa-tion (5):",
  "Proposition 3.1. Given a graph and its corresponding Laplacianmatrix L, eigenvalues of L such that 1 2 , 0 < <1/, graph lter I L is a low-pass lter and graph lter I +L isa high-pass lter": "Proposition 3.2. For any graph signal, low-pass lter H, high-pass lter H, graph convolution with low-pass lter H increasessignals smoothness on the graph, i.e., (H) (). Graph con-volution with high-pass lter H decreases signals smoothness, i.e.,(H) (). All the proofs can be found in . By denition, a low-pass l-ter retains the low-frequency components of a graph signal whilesuppressing the high-frequency ones. As previously discussed insection 2.3, low-frequency components correspond to eigenvectorsassociated with smaller eigenvalues, thereby leading to smoothersignals. Recall that we use the graph quadratic form () to mea-sure the smoothness of graph signal . Applying a low-pass lter toa graph signal consequently results in a smoother outcome, withthe converse holding for high-pass lters. Interested readers areencouraged to consult for a more comprehensive under-standing of graph lters. Having established these foundationalpropositions, we arrive at a theorem that articulates the equiva-lence between contrastive loss and graph convolution.",
  "Theorem 3.3. For a small enough learning rate , graph lterI+A/|D| increases signals smoothness on the user-item interactiongraph; graph lter I A() decreases signals smoothness on theanity graph": "The proof can be found in Appendix A. With Theorem 3.3, wededuce that the alignment loss, Equation (11), functionally acts asa graph convolution to enhance the smoothness of embeddings onthe user-item interaction graph. Conversely, the uniformity loss,Equation (13), operates as a graph convolution to reduce the smooth-ness of embeddings on the anity graph.",
  "Equilibrium of Contrastive Learning": "We demonstrate that the equilibrium in contrastive learningtheconvergence of the learning processnecessitates the alignmentof the models estimation with the empirical distribution derivedfrom the data. When the system stabilizes into this equilibrium, itadheres to the condition: E() = E() + AE(). From this, itfollows that: AE() = 0. Since E cannot be a zero matrix 0, it fol-lows that either A is 0 or E is a null solution for A. Summarizingthe above reasoning, we have the following theorem:",
  ",(18)": "A Boltzmann distribution is a probability distribution estimatedby embeddings through an energy function exp( ). Theorem3.4 articulates that achieving equilibrium necessitates the align-ment of the models estimated probability with the empirical prob-ability. Moreover, by solving Equation A = A/|D|, we obtain anexpression for the similarity between the user and item =logA",
  "Discussion": "In summary, we describe the learning dynamics of the contrastiveloss and identify its equivalence with graph convolutions. More-over, the alignment loss corresponds to the graph convolution smooth-ing embeddings on the user-item interaction graph while the uni-formity loss corresponds to the graph convolution dispersing em-beddings on the anity graph. Lastly, we demonstrate that theequilibrium of contrastive loss requires the models estimation tomatch the empirical distribution and that the optimization via thecontrastive loss is an implicit matrix factorization.Furthermore, ouranalysis paradigm (learning dynamics and equi-librium) is not limited to Equation (5). It could also be applied tothe SSM function (Equation (4)), the Mean Squared Error (MSE)loss (Matrix Factorization ), the Bayesian Personalized Rank-ing (BPR) loss , or even the recently proposed DirecAU loss which adopted negative samples as user-user and item-itempairs.",
  "High-Order Connectivity Modeling": "High-Order Connectivity (HOC) (or High-Order Proximity )modeling is a desired property for CF methods and has been themain motivation for graph-based CF methods. The HOC modelingrequires the similarity between node embeddings should reecttheir closeness on the graph. Traditional methods such as MatrixFactorization (MF) and BPR are often considered inad-equate for modeling HOC. This perceived limitation stems from",
  "Unifying Graph Convolution and Contrastive Learning in Collaborative FilteringKDD 24, August 2529, 2024, Barcelona, Spain": "these methods primary focus on distinguishing observed itemsfrom unobserved ones, while largely neglecting to explicitly modelthe latent relationships among unobserved entities . To mit-igate this limitation, earlier graph-based collaborative ltering ap-proaches have employed random walks to derive HOC scores. Thesescores are then directly utilized to inform recommendations . HOR-rec enhances the BPR loss function by integratingHOC through weighted coecients, alongside an expanded set ofpositive and negative pairs derived from random walks. Subsequently,NGCF and subsequent graph-based methods drawupon the Graph Convolutional Network (GCN) framework ,employing multiple convolutional layers to facilitate the propaga-tion of embeddings. These GCN-inspired models represent the cut-ting edge in CF, underscoring the critical role of graph convolu-tional layers in modeling HOC.However, this paper posits dierent perspectives that the inte-gration of the contrastive loss function within any encoder canequivalently achieve HOC modeling, from the previous theoreticalanalysis, and that a careless use of the convolutional layer mightlead to suboptimal results.",
  "I + A() E(0),(20)": "eectively stacking graph convolution operations. To a certainextent, E() in the equationmirrors a-layered GCN modelalbeitdevoid of linear transformations and nonlinear activations betweenlayers. Notably, LightGCN demonstrated that eliminating theseelements enhances model performance in the CF context. Typi-cally, is considerably large, which implies the application of amultitude of graph convolution operations on the embeddings. Suchoperations foster message exchange between nodes . With mas-sive convolutions, not only is information from a node propagatedto its high-order neighbors, but also the message exchange recursuntil an equilibrium is attained. If we posit that the embeddings ef-fectively capture high-order connectivity, then it is reasonable toanticipate that the embedding of a single node can assimilate in-formation from its high-order neighbors. While traditional graph-based methods utilize convolutional layers to assimilate neighborinformation, contrastive learning implicitly incorporates convolu-tions within its learning process. Viewed from this angle, we ad-vocate that contrastive learning bestows the capability of embed-dings to model high-order connectivity.",
  "On The Necessity of Graph Convolutionallayers": "Given that contrastive learning can eectively capture high-orderconnectivity, relevant questions arise: How does the combinationof a contrastive objective and a graph-based model perform? and Isthere a genuine need for graph convolutional layer designs in CF? Toprobe these concerns, we explore LightGCN , a state-of-the-art : The performance of naive embedding and LightGCN withthe DirectAU loss on the Yelp2018 dataset. # T.L. denotes thenumber of training layers and # I.L. denotes the number of infer-ence layers. No.S. denotes the experiment setting number.",
  "where A = D1/2AD1/2": "4.2.2Empirical Results. We compare the naive embedding model(LightGCN without linear lter) with the LightGCN model of dif-ferent layers to examine their respective performance with the re-cently proposed contrastive loss DirectAU , Equation (21). Toinvestigate its impact, we manipulate the number of layers appliedto the learnable embeddings during both the training and inferencephases. Specically, \"training layers\" refer to those utilized duringthe model training process, whereas inference layers are appliedto derive the results.As depicted in , there are two groups of experiments: set-ting 0, 1, 2, and 3 are the rst group they are in a consistent settingthat the numbers of layers in both stages are the same. Setting 3,4, 5, and 6 are the second group they have the same number oflayers in training but dierent number of layers in inference. In therst group, setting 0, 1, 2, 3, the naive model (No.0) marginally out-performs consistently layered LightGCN setups (No.1, No.2, No.3).In the second group, comparing No.3 with No.5 and No.6, withexactly the same learnable embeddings, the 1-layered inferencemodel outperforms the 3-layered one. All these results indicate anincrease in layers does not always enhance performance. He et al.",
  "A simple and eective approach": "In preceding discussions, we presented a theoretical analysis un-derscoring the HOC modeling potential of contrastive loss and em-pirically elucidated the inherent risks associated with graph con-volutional layers. To corroborate this theoretical understanding,we introduce Simple Contrastive Collaborative Filtering (SCCF),a model based on a naive embedding model, without embeddingpropagation, and a rened contrastive loss function. One notableadvantage of our method is its time complexity. The time complex-ity of SCCF for obtaining a single node embedding is (1), as itrelies on a look-up table. In contrast, GNN-based models have atime complexity of (), where represents the greatest node de-gree, and denotes the number of layers. In the following section,we provide a detailed elaboration on the SCCF method.Let B = {(1,1), . . . , (,)} be a collection of data sampledfrom D uniformly with batch size and B = {(,) | , =1, . . . ,} be the collection of all possible user-item pair of batch B.We design the following learning objective:",
  ",": "(25)where is identied as the temperature parameter , and || ||2represents the 2 norm. Intriguingly, Equation (25) can be inter-preted as a mixture of two exponential kernels . It is pivotal topinpoint some nuances dierentiating Equation (5) from Equation(25): rst, the introduction of the temperature parameter ; second,the shift from the inner product to cosine similarity; third, the in-corporation of a second-order cosine similarity to transcend merelinearity.First, the integration of the temperature parameter has been em-pirically shown to be crucial as it modulates the relative",
  "Amazon-Beauty22,36312,101198,50299.93%Gowalla29,85840,8911,027,37099.92%Yelp201831,66838,0481,561,40699.87%Pinterest55,1879,9121,445,62299.74%": "disparities between samples . Equation (5) can be interpretedas a special case where equals 1. Second, the cosine similaritycan be considered as the inner product between embeddings with2 normalization. This normalization not only acts as a regulariza-tion but also imposes constraints on gradients . Moreover, the2 normalization establishes a link between the inner product andthe Mean Squared Error (MSE), which is evident from the relation-ship: ( )2 = 2 for ||||2 = ||||2 = 1. Given the softmaxfunctions invariant nature , the normalized inner product iscongruent with MSE. This alignment relates our contrastive objec-tive with the radial basis function kernel . Third, the inclusionof the second-order cosine similarity aims to infuse non-linearityinto our learning objective. As substantiated by , the mixtureof varied kernels (similarity functions) augments performance.",
  "Experiments": "This section is organized as follows: Initially, the experimental set-tings are given. Subsequently, we show that our proposed model,SCCF, demonstrates equivalent or superior performance in com-parison to several state-of-the-art methods. Later, we present evi-dence indicating that the incorporation of graph convolution maylead to a suboptimal performance. Lastly, we conduct ablation stud-ies to analyze the impact of various components in SCCF.",
  "Experimental Settings": "6.1.1Datasets. We utilize four real-world datasets for our experi-ments: Amazon-Beauty1 comprises users online shopping recordson the Amazon website; Gowalla2 consists of users check-in in-formation from a social networking website; Yelp20183 includesinformation about businesses, reviews, and user data for academicpurposes; Pinterest4 originally is proposedin and later adoptedby for image recommendation. Dataset statistic information isprovided in . 6.1.2Evaluation Protocols. For each dataset, we randomly spliteach users interactions into training/validation/test sets with aratio of 80%/10%/10%. For the evaluation of Top-K recommenda-tion performance, we employ two metrics: Recall and NormalizedDiscounted Cumulative Gain (NDCG). Recall@K assesses whetherthe test ground-truth items are present in the retrieved Top-K list.NDCG@K evaluates the position of the ground-truth items in theTop-K list, considering the relevance and rank positions.",
  "Comparison with Other Baselines": "provides a comparison of the dierent models and lossfunctions in the metric of Recall@20 and NDCG@20. A compre-hensive table, , is provided in the Appendix for a detailedcomparison. Across all evaluated datasets, our method, SCCF, con-sistently outperforms competing approaches, achieving the bestperformance. Among the alternative models, LGCN-D and SGLdemonstrate substantial ecacy. Notably, despite SGLs applica-tion of three data augmentation techniquesedge drop, node drop,and random walkto bolster training, our SCCF model, withoutany augmentations, still manages to exceed SGLs performance.This outcome underscores the eectiveness of the SCCF approach.Furthermore, we can categorize the evaluated methods into twodistinct groups. The rst group, comprising BPR, DAU, and SCCF, implements a naive embedding model with specically designedloss functions. The second group includes all graph-based methods:LGCN-B, LGCN-D, NGCF, DGCF, SGL, and SCCF. Within the rstgroup, the primary distinction lies in the design of the loss function.Our proposed loss functions, Equation (24), and DirectAUs loss,Equation (21), dierentiate themselves by incorporating multiplenegative samples, in contrast to BPRs single negative sample ap-proach. This multiplicity of negative samples may explain the supe-rior performance of our and DirectAUs loss functions over BPRs.Specically, our loss function utilizes user-item pairs as negativesamples, whereas DirectAU employs user-user and item-item pairs.This distinction suggests that our method, by directly expandingthe distances between user and item embeddings, may oer ad-vantages over DirectAU, which indirectly achieves this objectiveby expanding distances between user-user and item-item. Theseobservations highlight the design of the loss function.In the second group, all graph-based models implement graphconvolutional layers as a fundamental component. In contrast, ourSCCF model eschews graph convolutional layers yet achieves su-perior performance across these graph-based approaches. This ob-servation suggests that contrastive loss provides a more adaptablemechanism for embedding propagation, potentially surpassing thecapabilities of graph convolutional layers. Furthermore, consider-ing the prevalent assumption that graph convolutional layers areeective in modeling HOC, the outperformance of SCCF invitesa reevaluation of this premise. Specically, if SCCF, which relieson contrastive loss, surpasses graph-based models in performance,it implies that contrastive loss is either equally capable of model-ing HOC or challenges the notion that HOC is advantageous forCF tasks. The ndings from this group indicate that graph convo-lutional layers are not indispensable for HOC modeling. Instead,they highlight the ecacy of contrastive loss in achieving, and po-tentially exceeding, the modeling capabilities attributed to graphconvolutions.",
  "Hyperparameter and Ablation Study": "6.4.1Impact of 2 normalization. displays the eective-ness of various similarities during the training and inference stages.Remarkably, employing cosine similarity during training and theinner product during inference yields the most superior results.The optimization using cosine similarity in training is more chal-lenging than the inner product because it disregards the magni-tude of the embeddings. Practically speaking, popular items or ac-tive users generally exhibit larger magnitudes and subsequentlyhave higher scores than their inactive counterparts. By disregard-ing magnitude, we eectively mitigate popularity bias, enablingthe mining of patterns beyond mere frequency. Conversely, duringthe inference stage, the magnitude of embeddings becomes pivotalas it reects the popularity of user/item, playing a crucial role inrecommendations. 6.4.2Impact of temperature. As illustrated in , the perfor-mance of the model exhibits variation with respect to the tempera-ture parameter. The temperature parameter controls the smooth-ness of the similarity distribution, thereby regulating the impact ofnegative samples. A smaller value of makes the model more sen-sitive to hard negative samples, as they contribute signicantly tothe loss. Conversely, as increases, the model becomes less sensi-tive to individual samples and focuses more on the overall distribu-tion. Noticing that the optimal value of may vary across datasets,",
  "indicates the importance of selecting the appropriate temperaturebased on the specic characteristics of the dataset being used": "6.4.3Impact of embedding size. As shows, the modelsperformance improves with the increase of embedding dimensionand is saturated when the dimension reaches a certain degree, whichaligns with our common sense that more parameters, better perfor-mance. The preference for lower dimensions can be attributed notonly to computational eciency but also to early works on MF likeFunks SVD5 and SVD++ , which essentially are low-rank ap-proximations of the interaction matrix. The foundational assump-tion of these methods is that low-rank decomposition, specicallythose leading singular values and their corresponding singular vec-tors, suces to approximate the original matrix accurately. At rst",
  "Related Work": "Collaborative Filtering. Collaborative Filtering (CF) is a funda-mental and important algorithm for recommender systems. Model-based methods have gained more popularity over memory-basedmethods since the latter often relies on heuristics. A notable ex-ample of model-based methods is Matrix Factorization , whichdecomposes the interaction matrix into two low-dimensional ma-trices. NeuMF and Multi-VAE are two representative non-linear methods for CF. Recently, GNN-based embedding modelshave gained much attention in CF. Inspired by GCN , NGCF utilizes graph convolutional layers to propagate embeddings.LightGCN removes linear transformations and non-linear ac-tivations in graph convolutional layers to improve performances.Additionally, Shen et al. provided a unied analysis throughlow-pass ltering, and proposed an eective graph ltering model,GF-CF. Existing GNN-based models in CF employ graph convolu-tional layers to capture high-order connectivity. Nonetheless, ourndings reveal naive embedding with a contrastive objective demon-strates comparable capability in modeling such connectivity. Contrastive Learning in CF. The BPR loss function is a pio-neer of contrastive learning in CF. CLRec uses the contrastiveloss function to reduce exposure bias through inverse propensityweighting. Drawing insights from BYOL , BUIR incorpo-rates a momentum update, enabling the training of embedding en-coders devoid of negative samples. Wang and Isola identiedtwo properties for contrastive learning, the alignment for positivesamples and the uniformity of features distribution. Inspired bythis idea, Wang et al. proposed the DirectAU loss as a means",
  "to enhance the alignment and uniformity of embeddings in CF": "Graph Contrastive Learning in CF. Another signicant topic isgraph contrastive learning, which aims to learn invariant represen-tations through graph perturbation. SGL proposedthree graphaugmentation methods: node dropout, edge dropout, and randomwalk. SimGCL introduced the idea of adding uniform noiseto node embeddings. LightGCL aligned node embeddings withtheir SVD-augmented counterparts. However, all of these methodsrely on graph convolutional layers to implement graph augmen-tation. For simplicity and generality, our discussion on the con-trastive objective does not involve any graph augmentation and re-mains independent of specic models. The investigation of model-agnostic augmentation methods is left for future work. Theory of Contrastive Learning. HaoChen et al. elucidatedthe role of contrastive learning in performing spectral clusteringon the augmentation graph. Taking the concept of augmentationoverlap into account, Wang et al. illustrated how aligned dataaugmentations facilitate the clustering of intra-class samples. Wanget al. proposed that the learning dynamics of contrastive learn-ing resonates with message-passing mechanisms on the augmen-tation graphs and anity graph. While our analysis bears similar-ities to that of Wang et al. , there are distinct dierences infocus. Wang et al. concentrates on general contrastive learn-ing, incorporating data augmentation techniques. In contrast, ourresearch specically targets the CF setting, eschewing data aug-mentation in favor of a deeper exploration of graph theory.",
  "Conclusion": "In this study, we reexamine graph convolution and contrastivelearning in the context of collaborative ltering and reveal theequivalence between them. This equivalence oers a new perspec-tive to analyze contrastive learning via graph theory. By doing so,we show the capacity of contrastive learning for high-order con-nectivity modeling. Moreover, we examine whether it is necessaryto add graph convolutional layers to model high-order connectiv-ity. We show that this is unnecessary.Based on the above analysis, we propose a simple and eec-tive algorithm using a new contrastive loss, enabling the model toproduce equivalent or even superior performance compared withother graph-based methods. This further conrms that a model us-ing contrastive loss can successfully capture high-order connectiv-ity, which was believed to be obtained only with graph convolu-tion.This paper is a rst step in trying to better understand CF al-gorithms using graph convolution and contrastive learning. Webelieve the problem should be further investigated to gain moreinsight into the models for designing better CF algorithms.",
  "I + D2()": "The rst and the third equality are obtained by the denition of(). The less than sign is obtained by the fact that (Dmax D)Lis a semi-positive matrix. The last equality is due to the fact that() = ((I + Dmax)). Replacing I with I L/|D| and D withD/|D| in Equation (26), we have",
  "BProof for Theorem 3.4": "When the system stabilizes into equilibrium, it adheres to the con-dition: E() = E() + AE(). It follows that AE() = 0.Since E cannot be a zero matrix 0, either A is 0 or E is a non-zerosolution for AE = 0 when A 0. We will demonstrate thatin the second case, the non-zero solution E is unstable; any per-turbation at this stationary point causes the system to move awayfrom it. Consequently, the embedding system by the contrastiveobjective reaches its equilibrium if and only if A = 0.To determine the stability of the stationary points, we turn tothe knowledge of dynamical systems . WhenE/ = AE = 0,the stability of E is determined by A: the embeddings are stable if and only if all the eigenvalues of A are equal to or less than0. Since our graph has no self-loop, the diagonal of A are zeros.This zero trace indicates two cases: (1) all the eigenvalues are zeros;(2) there exist at least one positive and one negative eigenvalues.Since A is symmetric, A is not defective; i.e., it has full eigenvec-tors corresponding to its number of rows. It is impossible for A tohave all zero eigenvalues unless A = 0, which contradicts our as-sumption that A 0. Therefore, there must be some eigenvaluesthat are non-zero. Recall the fact that those non-zero eigenvaluesmust sum up to zero, then at least there exists one positive eigen-value which makes the stationary point unstable."
}