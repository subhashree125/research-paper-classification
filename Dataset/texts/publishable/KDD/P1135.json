{
  "Abstract": "Traffic prediction is essential for intelligent transportation systemsand urban computing. It aims to establish a relationship betweenhistorical traffic data and future traffic states by employingvarious statistical or deep learning methods. However, the relationsof are often influenced by external confounders that si-multaneously affect both and , such as weather, accidents, andholidays. Existing deep-learning traffic prediction models adoptthe classic front-door and back-door adjustments to address theconfounder issue. However, these methods have limitations in ad-dressing continuous or undefined confounders, as they depend onpredefined discrete values that are often impractical in complex,real-world scenarios. To overcome this challenge, we propose theSpatial-Temporal sElf-superVised confoundEr learning (STEVE)model. This model introduces a basis vector approach, creatinga base confounder bank to represent any confounder as a linearcombination of a group of basis vectors. It also incorporates self-supervised auxiliary tasks to enhance the expressive power of thebase confounder bank. Afterward, a confounder-irrelevant relationdecoupling module is adopted to separate the confounder effectsfrom direct relations. Extensive experiments across fourlarge-scale datasets validate our models superior performance inhandling spatial and temporal distribution shifts and underscoreits adaptability to unseen confounders. Our model implementationis available at",
  "These authors contributed equally to this work.Corresponding author:": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 25, August 37, 2025, Toronto, ON, Canada 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-1245-6/25/08 ACM Reference Format:Jiahao Ji, Wentao Zhang, Jingyuan Wang, and Chao Huang. 2025. Seeing theUnseen: Learning Basis Confounder Representations for Robust Traffic Pre-diction. In Proceedings of the 31st ACM SIGKDD Conference on Knowledge Dis-covery and Data Mining V.1 (KDD 25), August 37, 2025, Toronto, ON, Canada.ACM, New York, NY, USA, 12 pages.",
  "Introduction": "Traffic prediction, a key technology in intelligent transportationsystems and urban computing , has long been a promi-nent research area in spatiotemporal data mining . A high-performance and robust traffic prediction model is crucial for effi-cient urban traffic management and safe city operations . Typ-ically, it uses historical traffic states as inputs to predict futuretraffic states, denoted as , in upcoming time slots . Inthe literature, numerous models have been proposed to capturethe dependency relationship between and , including shallowstatistical methods, such as ARIMA , SVR , and Kalman fil-tering , as well as deep learning-based methods in recent years.For instance, using recurrent neural networks , temporal convo-lutional networks , and transformers to model temporalcorrelations, as well as using convolutional neural networks and graph neural networks to capture spatial dependencies.While significant efforts have been made in previous works, mostcan be classified under the same modeling paradigm from the per-spective of causal modeling, namely, modeling the directed causalrelation (see (a)). This paradigm assumes a stable anddirect causal relationship between and , allowing for effectivemodeling of this relationship through a data-driven approach. How-ever, this assumption does not always hold in urban traffic systems.Spatiotemporal dependencies between and can be influenced byvarious external factors such as rain, traffic accidents, holidays, andother events. In the causal modeling theory, these external factorscan be expressed as confounders , which simultaneously affectthe states of and , causing shifts in the relationship(see (b)). This issue limits the generalization of existing trafficprediction models under extreme weather or emergency situations,compromising the resilience of cities. For instance, heavy snow (atype of confounder) can lead to more cautious driving behavior,resulting in severe congestion during non-peak hours and alteringthe relationship between and . If this changing relationship isignored, the model cannot be expected to perform well in trafficmanagement during snowy days.",
  ": Structural causal model for traffic forecasting": "Classic approaches in causal modeling theory to address theconfounder issue include front-door adjustment and back-dooradjustment . The front-door adjustment aims to identify a me-diator variable that lies on the causal pathway between and and is not influenced by any other confounders . In contrast, theback-door adjustment controls the confounder to estimate thecausal effect of under different confounder values. In recentyears, these adjustment approaches have also been incorporatedinto deep learning models for traffic forecasting. These methodsexplore potential confounders or mediators and use deep learn-ing to extract their representations to achieve deep learning-basedfront-door and back-door adjustments. For example, STNSCM uses time and location as mediators, learning their representationsto achieve front-door adjustment for deep learning-based bike flowprediction. CaST , on the other hand, learns the representationof an environment codebook to implement back-door adjustment,removing the influence of environment confounders. It also repre-sents spatial context as a mediator of front-door adjustment, thuseliminating the impact of spatial location confounders.Although these methods have been effective in addressing theconfounder issue, there remain two significant challenges that needto be solved in real-world traffic prediction applications. First, exist-ing methods require traversing all possible values of confounders ormediators, necessitating that their values must be discrete. However,in real-world scenarios, many confounders and mediators are withcontinuous value. An approximate method is to quantize them as adiscrete value . However, setting the correct quantization stepsize is very difficult. Too small a quantization step results in insuffi-cient data for each condition, while too large a quantization stepfails to fully eliminate the effects of confounders. Second, existingmethods require confounders or mediators to be predefined. How-ever, traffic prediction is a complex open scenario that is influencedby many unknown factors that cannot be predefined , such asperiodicity and rhythm in time, spatial location and land function,and even some uncertainties, such as major events, weather and etc.Therefore, it is very difficult to represent all possible confoundersusing an explicit way.To overcome the above challenges, we propose a Spatial-TemporalsElf-superVised confoundEr learning (STEVE) model that adoptsa self-supervised method to learn representations of implicit con-founders in traffic forecasting. Our model employs a basis vectorapproach to address the challenge of traversing all possible con-founders in back-door adjustment. Specifically, we use neural net-works to learn a set of basis vectors for confounders, termed thebase confounder bank, rather than targeting specific confounders.Using the base confounder bank, we can represent any confounder,whether continuous or discrete, predefined or not, as a linear com-bination of these basis vectors. The combination weights are adap-tively produced by performing cross-attention between the inputsample and the base confounder bank. Next, to ensure that thebase confounder bank has adequate expressive capacity to handlevarious types of confounders, we propose three self-supervised auxiliary tasks for its training. The tasks include spatial locationclassification, temporal index identification, and traffic load predic-tion for incorporating spatial, temporal, and semantic informationabout confounders, respectively. Finally, we adopt a confounder-irrelevant relation decoupling module to separate the confoundereffects from the direct relations. It includes an adversarialdisentanglement component for semantic separation and mutualinformation minimization loss for distribution separation. The con-founder representations from the base confounder bank and the relation representations are then transformed into corre-sponding traffic state predictions, followed by a fusion module thatcombines these predictions to generate the final results.Extensive experiments on four large-scale traffic datasets demon-strate the superiority of our STEVE in scenarios with data distribu-tion shifts due to spatial and temporal confounders. A further studyon the weather confounder highlights our models adaptability andgeneralizability to unseen confounders. To our knowledge, this isthe first work to extend the principle of back-door adjustment tohandle continuous or unknown confounders in deep-learning-basedtraffic prediction.",
  "We use a traffic graph to model the dynamic states of urban traffic": "Definition 1 (Traffic Graph). Given a set of traffic entities (e.g.,spatial regions, road segments), denoted as V = {|1 }, wedefine a traffic graph as G = (V, ). Here, R is a binaryadjacent matrix for the graph, where , = 1 when there is an edgefrom the node to .",
  "Over the traffic graph, we define dynamic traffic states": "Definition 2 (Traffic State). Given a traffic entity , assum-ing it has traffic state features, such as average speed, traffic inflow,and traffic outflow, we denote the traffic states of at the -th timeslice as , R . The traffic states for all the entities are de-noted as a matrix of R . The historical traffic states during + 1 to are expressed as X = (+1, . . . , ) R ,and +1 = +1 denotes the future traffic states.",
  "+1 = (X; ) ,(1)": "where () is the forecasting function, +1 is the prediction for+1 and is the parameters to learn.To reveal the underlying mechanism of traffic data dynamics,we adopt the Structural Causal Model (SCM) to describe therelations between the elements in the traffic prediction problem.We denote the history traffic states as and the future traffic statesto be predicted as . There are two types of effects that can causecorrelations between and (as illustrated in ):",
  "Adversarial": "Mutual Info Minimization+ : The pipeline of our STEVE model. Repr: Repre-sentation. TCL: Temporal Convolutional Layer. GCL: GraphConvolutional Layer. Info: Information. COSSL: Confounder-Oriented Self-Supervised Learning. We omit the sample indexof all variables for simplicity. illustrates the details ofthe confounder extractor. However, most existing works only model the first relation thatis irrelevant to confounders, which can be denoted as Pr() ( |).This is due to the problem definition in Eq. (1) does not explicitlydescribe the influence of the confounder. Such a definition inducesresearchers to ignore the effect of confounders in model design andlimits the generalizability of the learned model. If we consider sucheffects, the corresponding conditional probability Pr() ( |) canbe expressed as",
  "The goal of this component is to generate the confounder repre-sentations through the input traffic data. To achieve this, we firstutilize a Traffic Sequence Representation Learner module to embed": "dynamic temporal dependencies and variant spatial relations into ahidden representation. Then, we introduce a learnable ConfounderExtractor to extract complex dynamic confounder representationsfrom the hidden representation adaptively. Lastly, the confounderrepresentations will be refined to represent the desired confoundersby Confounder-Oriented Self-Supervised Learning in .2. 3.1.1Traffic Sequence Representation Learner. The TSRL moduleaims to transform the input traffic sequence X R into ahidden representation Z R . Temporal and graph convo-lutional layers are employed in TSRL to model temporal patternsand spatial dependencies between different locations. Temporal Convolutional Layer (TCL). We take traffic state se-quence X = (+1, . . . , ) R as the input of theTCL. We employ 1D convolution along the time dimension toimplement the TCL, which outputs time-aware traffic embeddings:",
  "Graph Convolutional Layer (GCL). We take the output of TCL asinput. Our GCL is implemented by a graph-based message-passingnetwork : = GCL(, ),(7)": "where is the adjacency matrix of the corresponding network. Byapplying GCL to each time-aware representation , we obtain therefined traffic representations (1+1, . . . , ).To jointly model temporal and spatial dependencies, we drawinspiration from and construct TSRL by two blocks, each ofwhich shows like: TCL GCL TCL, as in . The finaloutput of TSRL is Z R :",
  "Confounder Extractor. This section aims to implement func-tion () () in Eq. (3) that extracts confounder representations fromhistorical traffic state data": "Motivation and Idea. The function () () is used for approxi-mating Pr(|) in Eq. (2), where and are random variables ofconfounder and historical traffic data. However, directly approxi-mating Pr(|) is a non-trivial task that involves two main obsta-cles: (1) has a complex distribution mixing different conditions;(2) could take on an infinite number of values. To address thesechallenges, we draw inspiration from and introduce a series ofbase variables to represent it:",
  "=1 () () |,(9)": "where is the number of base variables, is the membershipdegree that belongs to the -th variable, and =1 () = 1. ( () | ) denotes the probability function of the -th base variablegiven . We can treat ( (1) | ), . . . , ( () | ) as different base",
  ": The architecture of our confounder extractor. Avg:Average. Att: Attention. For simplicity, the sample index for Z and is omitted": "conditions that form complex confounder with learnable weights = ( (1), . . . , () ). Moreover, since the change in weights is con-tinuous, we can theoretically express an infinite number of con-founders. For example, suppose we have base conditions such asrush hours, rainfall, holidays, etc, by assigning different weightsto them, we can express any complex confounders such as rushhours on a rainy workday, etc.",
  "= MLP(Flatten2(Z )).(10)": "Flatten2() denotes the operation that flattens the first two dimen-sions of the input tensor Z R . MLP() is employed inthe first dimension of input data to generate with shape .Then, we use to update . A direct approach is real-time up-dating, i.e., = , which preserves sufficient environmentalinformation of the current traffic data sample but loses that ofprevious samples. This is inconsistent with our expectation that should encompass more environmental information when per-ceiving the current environment. To tackle this issue, we adopt amomentum update mechanism as follows:",
  "=1 (), ().(14)": ", is the -th row of R , which is the confounder rep-resentation for input sample X with hidden representation Z.When the ( + 1)-th sample X+1 comes, we can similarly feed itsZ+1 into Eq. (10) and repeat the procedure until Eq. (14). Remark: After the training phase, our base confounder bank defines a vector space, where each confounder can be regarded asa point (in this space) that possesses a unique coordinate definedby weights ( (1), . . . , ()). The existence of an infinite numberof points in space indicates that our base confounder bank canrepresent any possible confounder in the learned space, whethercontinuous or discrete, predefined or not. Furthermore, when test-ing, a new traffic sample can slightly shape the confounder spaceaccording to its hidden environment. This enhances the ability ofour method to generalize to new unseen confounders.",
  "Confounder-Oriented SSL": "The Confounder-Oriented Self-Supervised Learning (COSSL) com-ponent aims to refine representation by using self-supervisedsignals relevant to confounders. Since it is hard to enumerate allconfounders explicitly, we propose to use some representative onesas self-supervised signals to inject confounder information into 1.Specifically, we categorize potential factors that affect trafficstates into three classes from conceptually different perspectives,i.e., temporal, spatial, and semantic, based on the unique propertiesof ST traffic data. We then carefully select representative and easilycollected confounders from each class, including temporal index,spatial location, and traffic capacity. These selected factors willserve as self-supervised signals in the following three tasks.Task #1: Spatial Location Classification. The spatial locationof a traffic entity reflects its surroundings, which may appear as aconfounder and vary by location, altering the dependency of pastand future data (e.g., (+1, . . . , ) +1). For example, suchdependency in a transportation hub can significantly differ fromthat in a working area. Therefore, we propose a spatial location clas-sification task to perceive the surroundings of each region. Firstly,for traffic entity V, we utilize the node ID to assign it a uniqueone-hot location label, (1) {0, 1} , where its item (1), = 1 if = else 0. We optimize the task by a cross-entropy loss as",
  "=1(1), log(1),,(15)": "where , is the predicted probability of the -th entity belongingto category , and it is the -th item of vector (1)= 1() R .1() is a two-layer MLP followed by a softmax activation, while is the -th row of confounder representation .Task #2: Temporal Index Identification. Time-varying con-founders like weather and holidays can shape the traffic data dis-tribution. For instance, holidays flatten the curves of morning andevening rush hours, resulting in a very different distribution fromthe workday rush hours. To utilize such information, we propose a",
  "Seeing the Unseen: Learning Basis Confounder Representations for Robust Traffic PredictionKDD 25, August 37, 2025, Toronto, ON, Canada": "temporal index identification task. Specifically, we divide the dayinto 24 time slots, each of which is a category. We use differentcategories to distinguish between workdays and holidays, so thereis a total of = 48 temporal indexes. For a given traffic state sample(X, ), we use the temporal index of as ground truth. It is denotedby a one-hot vector (2) {0, 1} . The optimization objective ofthe temporal index identification task is",
  "=1(2)log ( (2) ),(16)": "where is the SoftMax activation. (2) =1=1 2 () is thepredicted temporal index vector, where 2 is a two-layer MLP usedfor enhancing the confounder representation .Task #3: Traffic Load Prediction. The traffic load is a kindof semantic information describing the congestion level of trafficentities. It acts as a confounder and has an impact on the change offuture traffic. For example, when the load reaches saturation, thetraffic is more likely to be congested, causing traffic speeds andinflow/outflow to drop in subsequent time slots. Therefore, we pro-pose a traffic load prediction task to inject dynamic load informationinto confounder representations. Specifically, we approximate theload capacity of the -th node by using the historical maximum traf-fic flow, i.e., = max({, }1 ) R . denotes the number of timeslots in the training set. max() extracts the maximum value of eachfeature. Then, we divide flow volume into 6 load levels and calculatethe traffic load of the -th node via (3)= 5/ {0, . . . , 5} ,where is the label data in the main traffic prediction problem.Since load states are quite imbalanced in practice, we adopt theMean Square Error (MSE) to optimize this task:",
  ",(17)": "where 3() is the load prediction head implemented by a two-layerMLP, and is the confounder representation. It is worth notingthat though quantized as a discrete value, traffic load has a relativesize relationship. Regression loss like MSE is more suitable thanclassification loss since MSE can perceive size differences.Lastly, we jointly minimize all three self-supervised loss func-tions to train representation , making it fuse information of vari-ous confounders. The target loss of confounder-orient self-supervisedlearning is defined as",
  ": Adversarial learning is achieved by inserting a GRLbetween the generator and the discriminator . The for-ward pass is indicated by arrows while the backward pass isindicated by dashed arrows": "confounder-irrelevant relations should involve minimal informa-tion about the confounder, we propose to disentangle confounder-irrelevant representations and confounder representations fromthe semantics and distribution perspectives.Recalling the hidden representation H R producedfor confounder-irrelevant relation modeling in .1.1, it isthen transformed into R by applying the TCL definedin Eq. (6) along the temporal dimension . Next, we elaborate onhow to refine into a confounder-irrelevant representation distin-guished from the confounder representation . Note we omit thesample index of and for convenience. 3.3.1Adversarial Disentanglement. To push all confounder infor-mation away from , we introduce an adversarial learning-baseddisentanglement module as shown in . Concretely, the genera-tor , consisting of a TSRL and a TCL, aims to produce . It is thenfed into the discriminator for confounder-related self-supervisedtasks in .2. Different from the learning pipeline of , weinsert a Gradient Reversal Layer (GRL) between the generatorand discriminator in the pipeline of to disentangle and .The forward pass of GRL directly outputs the input without anytransform: = GRL (). However, during the backward pass, itmultiples the incoming gradient back from the discriminator by anegative factor : GRL",
  "= ,(19)": "where is the identity matrix. The operation reverses the gradientdirection passed back to the generator, pushing it away from theoptimization direction of the confounder discriminator. This resultsin to be confounder-irrelevant, i.e., the semantics of in differentconfounder environments are as similar as possible.Mathematically, we can define the loss function that is beingminimized as",
  ",(22)": "where Pr(), Pr(), and Pr(, ) correspond to the marginal andjoint distributions of and .Due to the unknown closed-form expressions of the marginaland joint distributions, direct computation of the MI in Eq. (22) isnot feasible. Therefore, we adopt an approximation method as analternative. Specifically, we use the CLUB method to calculatethe upper bound on MI of and as",
  "=1log | ,(23)": "where is the sample size, and , denote representations pro-duced by the -th data sample. The ( |) in Eq. (23) is a vari-ational estimation of the conditional probability Pr( |), whichfollows a Gaussian distribution as N( |2 ). Here the mean and variance 2 are estimated using an MLP network:",
  "()+1 = MLP( ).(26)": "In real scenarios, confounder factors affect regions to differentdegrees. Taking the rush hours factor as an example, it mainlyaffects the traffic states in office and residential areas, but exhibitsless influence in parks and entertainment areas. Moreover, despitebeing in the same area, the influence on different state channels(e.g., inflow, outflow) is also distinct. Inspired by these phenomena,we propose a heterogeneity-aware fusion method as follows:",
  "Experiment4.1Experimental Setting": "4.1.1Dataset and Baseline. To evaluate our proposed method, weconduct experiments on four real-world traffic datasets includingNYCTaxi, NYCBike1, NYCBike2, and BJTaxi , which record thebike rental demands and taxi orders, respectively. We divide alldatasets into training, validation, and test sets in a ratio of 7:1:2.We choose Mean Absolute Error (MAE) and Mean AbsolutePercentage Error (MAPE) as evaluation metrics, which are widelyused in ST traffic prediction . A lower metric value in-dicates a better performance. We selected 13 methods as base-lines and categorized them into distinct groups: ) Spatiotemporalprediction methods based on GNNs: STGCN, GMAN, AST-GNN, and HimNet; ) Disentanglement-based spatiotem-poral methods: COST, ST-Norm, STWA, and SCNN;) Models considering distribution shift: AdaRNN, CIGA,STNSCM, CauSTG, and CaST. The final model parame-ters are chosen by the optimal effect of the validation set. Detaileddescriptions of datasets and baselines are in Appendix A.1. 4.1.2Implementation Protocols. Our STEVE is implemented withPyTorch 1.10.2 on an Ubuntu server with an NVIDIA RTX 3090. Bothtemporal and spatial convolution kernel sizes in TSRL are set to 3.The hidden dimension is searched over {16, 32, 64, 128}. For thebase confounder number , we search it from {16, 32, 64, 128, 256}.For the momentum coefficient in the confounder extractor, wetest it from 0.1 to 0.9. Our model is trained using Adam optimizerwith a learning rate of 0.001 and a batch size of 32. Task balancingcoefficients 1,2,3 are trained via a dynamic weight-averagingstrategy with initial values 1.0. Detailed model setting andparameter sensitivity are in Appendix A.1 and A.2.4. Since hiddenconfounder data are unavailable, we assess the models robustnesson distribution shift via simulated environments. Specifically, weconsider two scenarios that are common in the real world: (1) Tem-poral Distribution Shift (TDS): we split the temporal distributioninto workdays and holidays, which is roughly 5:2 in the training set.It is then shifted to 1:0 and 0:1 to imitate TDS to the maximum ex-tent. (2) Spatial Distribution Shift (SDS): To simulate real-worldsemantics of traffic entities, we cluster them into different groupsvia -means algorithm. {0, . . . ,} denote the clustering results,where entities with smaller id are usually located in less popularareas and thus have lower traffic. There is a mixed distribution",
  "Overall Performance": "We run all models five times and report the mean results in Tab. 1.The details of NYCBike2 and BJTaxi are in Appendix A.2.1. FromTab. 1, we have four key findings: (1) STEVE consistently outper-forms all competing baselines across every task on four datasets (ac-cording to the Nemenyi test at level 0.05 in Appendix A.2.3), whilethe second-best model is not consistent across all cases. This showsthat STEVE offers more stable and reliable results, highlighting itsrobustness and adaptability to various distribution-shift scenarios.(2) There is no significant uplift of unsupervised disentanglement-based methods w.r.t. classical ST prediction methods, indicating thatdecoupling without supervised signals does not effectively improvethe model performance. That is why we incorporate self-supervisedsignals with disentanglement. (3) Some of the models against distri-bution shift yield unsatisfactory results. For instance, AdaRNN andCIGA fail to fully capture spatial and temporal dependencies. Mean-while, CauSTG primarily focuses on learning invariant relationsacross different confounders, overlooking the importance of cap-turing variant relations in spatiotemporal prediction. Additionally,CaSTs forced discretization of a continuous temporal environment disrupts the intrinsic structure of spatiotemporal data, increasingmodeling difficulty. This confirms our models effectiveness in mod-eling dynamic confounders in a basis vector approach without theneed for discretization. (4) While baseline models such as AGCRNand STNSCM can achieve runner-up MAE performance in certaincases, they exhibit a large margin in MAPE compared to STEVE.This demonstrates that STEVE not only delivers small absoluteerror but also showcases superior relative error across differentcases. The relative error is usually a better indicator of the modelsgeneralizability to various cases than the absolute one as it allowsassessment of the precision of a result independently of the datascale. In addition, our model also achieves decent training efficiencyand scalability (see .3.5 and 4.3.6), making it well-suitedfor practical applications in real-world scenarios.",
  "Further Analysis of STEVE": "4.3.1Ablation Study. To verify our model design, we carry out ab-lation experiments on the following variants: (a) w/o cfd removesthe confounder bank and takes as the confounder representation;(b) w/o ssl removes the SSL tasks in Eq. (18); (c) w/o ad disables theadversarial disentanglement module in Eq. (20); (d) w/o mi doesnot use the mutual information regularization in Eq. (22). The MAEresults of all datasets are shown in Tab. 2. We can observe that all",
  ": Confounder distribution of distinct locations atdifferent time periods. JS Div: JensenShannon divergence": "four components contribute to the models overall performance.Specifically, variants (a) and (b) show a great decrease, indicatingthat our proposed confounder bank can effectively extract con-founder representation from limited observed ST data with theaid of SSL injecting representative confounder information. Be-sides, the impact of removing these components on performance ismore pronounced for NYCTaxi and BJTaxi compared to NYCBike1and NYCBike2 as taxi data possess more complex spatiotemporalrelations and are more sensitive to confounder modeling. 4.3.2Analysis of Confounder Learning. As introduced in .1.2, our confounder extractor can generate meaningful confounderembedding for each sample via a unique weight vector and con-founder bank. To verify this, we visualize the weight distribution ofdifferent samples from NYCTaxi in . We have two key observa-tions: (1) The divergence in the weight distributions for the workingarea and traffic hub (especially in peak hours) suggests that ourmodel learned the differences in the environments correspondingto these two types of zones. (2) From noon off-peak to eveningpeak, the distribution of working areas gradually concentrates on afew base confounders, while that of traffic hubs remains dispersed.This shows that the learned environments at traffic hubs are con-sistently complex while working areas can be more regular in theevening peak, highlighting the ability of our model to capture thecharacteristics of both types of zones. 4.3.3Generalization to Unseen Confounders. In the COSSL modulein .2, we subtly selected representative self-supervisedsignals to guide our model in capturing information about latentconfounders, thereby improving the robustness and generalizabilityof the model in these latent environments. To evaluate this, wecollect the weather data of NYCTaxi, which is not exposed to modeltraining. We compare STEVE with three spatiotemporal models thatdo not employ self-supervised signals in solving the distributionshift problem, and the results are displayed in (a). We canobserve that STEVE consistently beats other baselines in all three",
  ": Visualization of confounder-related representation (red circle marker) and confounder-irrelevant representa-tion (purple triangle marker). is Silhouette Score": "weather conditions. This demonstrates the effectiveness of self-supervised signals enhancement in confounder representations,making them robust to confounders not seen before. Next, we diveinto the weather confounder representations learned by our model.The weights of these confounders are scattered in (b) by usingt-SNE algorithm . From the results, we have two findings: (1)The confounder representations for the same type of weather arerelatively close. Moreover, the representations of snowy days aremore compact compared to those of sunny days, indicating snowydays characterize a more homogeneous confounder environment.(2) In the representation space, the sunny confounder is closerto cloudy and farther from snowy. This highlights our modelscapability to extract informative confounder representations withthe guidance of self-supervised signals. 4.3.4Representation Visualization. As introduced in .3.2,we propose to use Mutual Information Minimization (MIM) lossto disentangle representations and from the distribution per-spective. To verify this, we randomly took some samples from thetest set to generate the corresponding and , and then used thet-SNE algorithm to convert them as two-dimensional embed-ding vectors for visualization. As depicted in , w/o MIM andw/ MIM denote the results without and with MIM loss, respectively.It can be seen that MIM facilitates the separation of distribution of and in the representation space, thus enhancing distributionalindependence for disentangled representations. 4.3.5Model Efficiency. In this section, we assess the efficiency ofour model. Specifically, we measure the per-epoch training/inferencetime of all methods on all datasets, and the results are summarizedin Tab. 3. To ensure fairness, all experiments are conducted on anUbuntu server with an NVIDIA RTX 3090 with the same batch size.From the results, we can observe that our model reduces the train-ing and inference time by 73.7% and 81.9% on average compared tothe best baseline AGCRN. While some baselines such as STGCNand COST surpass our model in time cost, our model achieves awin-win situation in terms of performance and training efficiencyby combining the performance results in Tab. 1.",
  ": Scalability performance vs. cardinality": "4.3.6Model Scalability. In the section, we explore the scalabilityperformance of STEVE compared with AGCRN (the best baseline),focusing on their ability to handle variations in dataset size andgraph size. The evaluation employs the BJTaxi dataset that containstraffic data from 1024 graph nodes over 4 months. depicts theexperimental results. Regarding the dataset size, 25% denotes a one-month dataset, 50% denotes a two-month dataset, and so on. Forthe graph size, we decompose the input graph into four connectedsubgraphs with the same node number. Here, 25% implies usingnodes from the first subgraph to extract an adjacency matrix fromthe original one, 50% involves nodes from the first two subgraphs,and so on. From , we can observe that the prediction time forboth models increases as the dataset and graph size scale. However,the increasing trend of AGCRN is sharp, while STEVEs trend ismore stable. This demonstrates our models potential scalability inlarge-scale ST forecasting.",
  "Related Work": "Spatial-Temporal Traffic Forecasting has received increasingattention due to its pivotal role in intelligent transportation man-agement . Early contributions emerged from the timeseries community and predominantly utilized the ARIMA family tomodel traffic data . However, these methods usually rely onstationary assumptions, leading to limited representation power fortraffic data. Recent advancements have introduced a variety of deeplearning techniques that do not rely on stationary assumptions,enabling the capture of complex traffic dependencies more effec-tively. For instance, methods like recurrent neural networks and temporal convolutional networks are employed tocapture temporal dependencies. Regarding spatial dependencies, convolutional neural networks are used for grid-based spa-tiotemporal data, while graph neural networks andattention mechanism are explored to incorporate roadnetwork information. Recently, several studies have investigated theconfounder issue, concentrating on invariant relation learning ,front-door adjustment , or a combination of both front-doorand back-door adjustments . However, these methods rely onpredefined discrete confounder values that are often impracticalin real-world scenarios. Consequently, they struggle to addresscontinuous and unknown confounders, which is our primary focus.Self-Supervised Learning aims to distill valuable informationfrom input data to enhance the quality of representations . Thefundamental paradigm involves initially augmenting input data andsubsequently employing self-supervised tasks to serve as pseudolabels for the purpose of representation learning . Thesetasks are usually infused with domain knowledge to encouragerepresentations to exhibit specific characteristics. This approachhas achieved remarkable success within various data such as textdata , image data , and audio data . Motivated by theseworks, we devise customized self-supervised tasks tailored to infusevarious information into confounder representations.Disentangled Representation Learning aims to learn iden-tifying and disentangling the underlying factors hidden in the ob-servable data in representation form , which has been verifiedto increase the model generality . It was initially used to analyzevisual data and has recently been introduced to the field of spa-tiotemporal prediction . Some studies focus on disentanglingfrom the time dimension, e.g., seasonal-trend disentanglement andfrequency disentanglement . Some work focuses on struc-tural disentanglement from the spatial dimension . How-ever, they are mainly unsupervised disentangling methods, whichproved to be unable to disentangle from the corresponding under-lying factors . In contrast, this paper utilizes self-supervisedsignals to ensure the effectiveness of disentanglement.",
  "Conclusion and Future Work": "This paper presented the first attempt to extend back-door ad-justment to handle continuous or unknown confounders in deep-learning traffic prediction. By utilizing a basis vector approach, weproposed a STEVE model that creates a base confounder bank torepresent any confounder as an adaptive linear combination of agroup of basis confounder representations, with the aid of threeself-supervised auxiliary tasks. Then, we decoupled the confounder-irrelevant relations from confounder effects and used both types ofrelations for robust traffic prediction. Extensive experiments overfour datasets verified the effectiveness, robustness, and scalabilityof our model. In the future, we plan to extract representations ofcommon confounders (such as weather and holidays) to quantifythe quantitative impact of these confounders on traffic states andmake counterfactual traffic predictions under intervention settings. Prof. Jingyuan Wangs work was partially supported by the Na-tional Natural Science Foundation of China (No. 72222022, 72171013,72242101), and the Special Fund for Health Development Researchof Beijing (2024-2G-30121).",
  "ASupplementary MaterialA.1Experimental Setting": "A.1.1Datasets. We conducted experiments on four commonlyused real-world large-scale datasets released by . These datasetsare generated by millions of taxis or bikes on average and containthousands of time steps and hundreds of regions. The statisticalinformation is in Tab. 4. Two of them are bike datasets, while theothers are taxi datasets. Bike data record bike rental demands. Taxi",
  "Time interval30 min1 hour30 min30 min# regions102016810203232# taxis/bikes22m+6.8k+2.6m+34k+# samples2880439228805596": "data record the number of taxis coming to and departing from aregion given a specific time interval, i.e., inflow and outflow.We give more detailed descriptions of the four datasets as fol-lows. NYCTaxi measures the 30-minute level taxi flow from1/Jan/2015 to 01/Mar/2015. NYCBike series datasets consist of hourlylevel dataset from 1/Apr/2014 to 30/Sept/2014 (NYCBike1 )and one 30-minute level dataset from 1/Jul/2016 to 29/Aug/2016(NYCBike2 ). BJTaxi is also a 30-minute level taxi datasetfrom 01/Mar/2015 to 30/Jun/2015, collected in Beijing city. For alldatasets, the traffic network is constructed by the adjacency relationof regions. For a prediction sample at time slot , we use two typesof past data as inputs: ) data from 4 hours before , and ) datafrom 2 hours before and after the time slots , 2, and 3, where is the number of time slots in one day. Thesecond type incorporates periodicity information into the predic-tion. We adopt a sliding window strategy to generate samples, andthen split each dataset into the training, validation, and test setswith a ratio of 7:1:2. A.1.2Baselines. Since traditional statistical models and shallowmachine learning methods have proven difficult to effectively modelST traffic data , we compare STEVE with recent state-of-the-art baselines as follows.) Spatial-temporal prediction methods based GNNs:",
  "(e) Critical Difference (CD) diagram of the Nemenyi test": ": (a)-(d): Spatial clustering results of all datasets. Thecluster identification (ID) is next to the color bar. A largercluster ID means a higher level of popularity in the corre-sponding region. (e): CD diagram of the Nemenyi test. Thehorizontal axis depicts the average ranking of each modelacross all scenarios of both metrics. Bold black lines con-nect two models when their ranking difference is below theCD value (at a 5% significance level), indicating statisticalinsignificance. Otherwise, they are significantly different.",
  "CaST : it leverages a causal lens to handle the temporaldistribution shift issue by back-door adjustment and captures thedynamic spatial causation via edge-level graph convolution": "A.1.3Parameter Setting for STEVE. We conducted a grid searchto optimize the hyperparameters of our model across all datasets,focusing on parameters such as hidden dimension , momentumcoefficient , number of base confounders , batch size, kernelsizes in TCL and GCL, and learning rate. The best kernel size is3 for all datasets. The batch sizes of NYCTaxi and NYCBike2 are64, while those of NYCBike1 and BJTaxi are 32. The rest of thehyper-parameter settings are in Appendix A.2.4.",
  ": Parameter sensitivity of STEVE using MAE metric": "areas. Since there is no function label, we use -means cluster-ing algorithm to label the regions. The best is determined bythe Silhouette Coefficient metric . The input of -means is(,, ) of each regions historical traf-fic flows. (b)-9(d) presents the clustering results of all datasets.The clustering results exhibit some meaningful patterns, e.g., the",
  "clusters of the BJTaxi dataset imply the suburbs (ID 0) and ringroads (ID 3)": "A.2.3Significance Test. To further emphasize the substantial im-provement of our STEVE over the baseline models, we draw thecritical difference (CD) diagram to conduct a Nemenyi significancetest. As shown in (e), we can observe that our STEVE outper-forms the best baseline significantly at a 5% significance level. A.2.4Impact of Hyper-parameters. In this part, we conduct ex-periments to analyze the impacts of critical hyper-parameters: themomentum coefficient , the number of base confounders , andthe hidden dimension , with results in . Firstly, the effectof is shown in (a), where we vary it from 0.1 to 0.9 individ-ually and omit some values for better plotting. The results indicatethat 0.4 is the optimal setting for the NYCTaxi dataset, 0.7 is op-timal for the NYCBike1 dataset, 0.6 is optimal for the NYCBike2dataset, and 0.1 is optimal for the BJTaxi dataset. The variation inoptimal settings across datasets is attributed to the distinct impactof confounders. Secondly, the effect is shown in (b). Wecan observe that a setting of 64 is optimal for the NYCTaxi andBJTaxi datasets, while a setting of 128 is optimal for the NYCBike1and NYCBike2 datasets. Thirdly, the effect of hidden dimension isgiven in (c), where we vary it in the set {16, 32, 64, 128}. Theresults indicate 64 as the optimal settings for NYCTaxi, NYCBike1,and NYCBike2 datasets and 32 for BJTaxi. Since different datasetshave different spatiotemporal dependencies, it is reasonable to usedifferent hidden dimensions for them."
}