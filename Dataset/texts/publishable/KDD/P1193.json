{
  "editing of the manuscript and idea discussion": "1.Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Preprint at (2019). 2.Radford, A., Narasimhan, K., Salimans, T. & Sutskever, I. Improving Language Understanding by Generative Pre-Training. 3.Sun, C., Qiu, X., Xu, Y. & Huang, X. How to Fine-Tune BERT for Text Classification? Preprint at (2020). 4.Xu, H., Liu, B., Shu, L. & Yu, P. S. BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis. Preprint at (2019). 5.Dathathri, S. et al. Plug and Play Language Models: A Simple Approach to Controlled Text Generation. Preprint at (2020). 6.Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q. & Artzi, Y. BERTScore: Evaluating Text Generation with BERT. Preprint at (2020). 7.Shi, P. & Lin, J. Simple BERT Models for Relation Extraction and Semantic Role Labeling. Preprint at (2019). 8.Lee, J. et al. BioBERT: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics 36, 12341240 (2020). 9.Huang, K., Altosaar, J. & Ranganath, R. ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission. Preprint at (2020). 10. Yang, W. et al. End-to-End Open-Domain Question Answering with BERTserini. in Proceedings of the 2019 Conference of the North 7277 (2019). doi:10.18653/v1/N19-4013. 11. Qu, C. et al. BERT with History Answer Embedding for Conversational Question Answering. in Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval 11331136 (ACM, 2019). doi:10.1145/3331184.3331341. 12. Peng, Y., Yan, S. & Lu, Z. Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets. Preprint at (2019). 13. Tinn, R. et al. Fine-tuning large neural language models for biomedical natural language processing. Patterns 4, 100729 (2023). 14. A Study of Abbreviations in Clinical Notes - PMC. 15. Reisman, M. EHRs: The Challenge of Making Electronic Data Usable and Interoperable. Pharm. Ther. 42, 572575 (2017). 16. Zhou, S., Wang, N., Wang, L., Liu, H. & Zhang, R. CancerBERT: a cancer domain-specific language model for extracting breast cancer phenotypes from electronic health records. J. Am. Med. Inform. Assoc. JAMIA 29, 12081216 (2022). 17. Gu, Y. et al. Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing. ACM Trans. Comput. Healthc. 3, 123 (2022). 18. Johnson, A. E. W. et al. MIMIC-III, a freely accessible critical care database. Sci. Data 3, 160035 (2016). 19. Lafferty, J. D., McCallum, A. & Pereira, F. C. N. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. in Proceedings of the Eighteenth International Conference on Machine Learning 282289 (Morgan Kaufmann Publishers Inc., 2001). 20. Hochreiter, S. & Schmidhuber, J. Long Short-Term Memory. Neural Comput. 9, 17351780 (1997). 21. The genetic association database - PubMed. 22. Konen, J. et al. Federated Learning: Strategies for Improving Communication Efficiency. Preprint at (2017). 23. Peng, L. et al. Evaluation of federated learning variations for COVID-19 diagnosis using chest radiographs from 42 US and European hospitals. J. Am. Med. Inform. Assoc. 30, 5463 (2023). 24. Long, G., Tan, Y., Jiang, J. & Zhang, C. Federated Learning for Open Banking. Preprint at (2021). 25. Nguyen, A. et al. Deep Federated Learning for Autonomous Driving. Preprint at (2022). 26. Liu, M. et al. Federated Learning Meets Natural Language Processing: A Survey. Preprint at (2021). 27. Lin, B. Y. et al. FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks. Preprint at (2022). 28. Sui, D. et al. FedED: Federated Learning via Ensemble Distillation for Medical Relation Extraction. in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) 21182128 (Association for Computational Linguistics, 2020). doi:10.18653/v1/2020.emnlp-main.165. 29. Liu, D. & Miller, T. Federated pretraining and fine tuning of BERT using clinical notes from multiple silos. Preprint at (2020). 30. McMahan, B., Moore, E., Ramage, D., Hampson, S. & Arcas, B. A. y. Communication-Efficient Learning of Deep Networks from Decentralized Data. in Proceedings of the 20th International Conference on Artificial Intelligence and Statistics 12731282 (PMLR, 2017). 31. Li, T. et al. Federated Optimization in Heterogeneous Networks. 32. Chen, Q. et al. Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations. 33. Yang, X. et al. A large language model for electronic health records. Npj Digit. Med. 5, 19 (2022). 34. Large language models encode clinical knowledge | Nature. 35. Yang, Q., Liu, Y., Chen, T. & Tong, Y. Federated Machine Learning: Concept and Applications. ACM Trans. Intell. Syst. Technol. 10, 119 (2019). 36. Federated Learning With Differential Privacy: Algorithms and Performance Analysis | IEEE Journals & Magazine | IEEE Xplore. 37. Zhang, C. et al. BatchCrypt: Efcient Homomorphic Encryption for Cross-Silo Federated Learning. 38. Henry, S., Buchan, K., Filannino, M., Stubbs, A. & Uzuner, O. 2018 n2c2 shared task on adverse drug events and medication extraction in electronic health records. J. Am. Med. Inform. Assoc. 27, 312 (2020). 39. Smith, L. et al. Overview of BioCreative II gene mention recognition. Genome Biol. 9, S2 (2008). 40. Krallinger, M. et al. The CHEMDNER corpus of chemicals and drugs and its annotation principles. J. Cheminformatics 7, S2 (2015). 41. Collier, N., Ohta, T., Tsuruoka, Y., Tateisi, Y. & Kim, J.-D. Introduction to the Bio-entity Recognition Task at JNLPBA. in Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (NLPBA/BioNLP) 7378 (COLING, 2004). 42. Doan, R. I., Leaman, R. & Lu, Z. NCBI disease corpus: A resource for disease name recognition and concept normalization. J. Biomed. Inform. 47, 110 (2014). 43. van Mulligen, E. M. et al. The EU-ADR corpus: Annotated drugs, diseases, targets, and their relationships. J. Biomed. Inform. 45, 879884 (2012). 44. [1508.01991] Bidirectional LSTM-CRF Models for Sequence Tagging. 45. Alsentzer, E. et al. Publicly Available Clinical BERT Embeddings. Preprint at (2019). 46. Radford, A. et al. Language Models are Unsupervised Multitask Learners. 47. Brown, T. et al. Language Models are Few-Shot Learners. in Advances in Neural Information Processing Systems vol. 33 18771901 (Curran Associates, Inc., 2020). 48. OpenAI. GPT-4 Technical Report. Preprint at (2023). 49. Anil, R. et al. PaLM 2 Technical Report. Preprint at (2023). 50. Chowdhery, A. et al. PaLM: Scaling Language Modeling with Pathways. Preprint at (2022)."
}