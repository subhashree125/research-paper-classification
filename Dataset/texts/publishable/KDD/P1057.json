{
  "ABSTRACT": "Attributed bipartite graphs (ABGs) are an expressive data modelfor describing the interactions between two sets of heterogeneousnodes that are associated with rich attributes, such as customer-product purchase networks and author-paper authorship graphs.Partitioning the target node set in such graphs into disjoint clus-ters (referred to as -ABGC) finds widespread use in various do-mains, including social network analysis, recommendation systems,information retrieval, and bioinformatics. However, the majorityof existing solutions towards -ABGC either overlook attributeinformation or fail to capture bipartite graph structures accurately,engendering severely compromised result quality. The severity ofthese issues are accentuated in real ABGs, which often encompassmillions of nodes and a sheer volume of attribute data, renderingeffective -ABGC over such graphs highly challenging.In this paper, we propose TPO, an effective and efficient approachto-ABGC that achieves superb clustering performance on multiplereal datasets. TPO obtains high clustering quality through two majorcontributions: (i) a novel formulation and transformation of the -ABGC problem based on multi-scale attribute affinity specialized forcapturing attribute affinities between nodes with the considerationof their multi-hop connections in ABGs, and (ii) a highly efficientsolver that includes a suite of carefully-crafted optimizations forsidestepping explicit affinity matrix construction and facilitatingfaster convergence. Extensive experiments, comparing TPO against19 baselines over 5 real ABGs, showcase the superior clusteringquality of TPO measured against ground-truth labels. Moreover,compared to the state of the arts, TPO is often more than 40 fasterover both small and large ABGs.",
  "Work done while at Hong Kong Baptist University": "KDD 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s).This is the authors version of the work. It is posted here for your personal use. Notfor redistribution. The definitive Version of Record was published in Proceedings ofthe 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 24),August 2529, 2024, Barcelona, Spain,",
  "clustering, bipartite graphs, attributes, eigenvector": "ACM Reference Format:Renchi Yang, Yidu Wu, Xiaoyang Lin, Qichen Wang, Tsz Nam Chan, and Jiem-ing Shi. 2024. Effective Clustering on Large Attributed Bipartite Graphs:Technical Report. In Proceedings of the 30th ACM SIGKDD Conference onKnowledge Discovery and Data Mining (KDD 24), August 2529, 2024, Barcelona,Spain. ACM, New York, NY, USA, 14 pages.",
  "INTRODUCTION": "Bipartite graphs are an indispensable data structure used to modelthe interplay between two sets of entities from heterogeneoussources, e.g., author-publication associations, customer-merchanttransactions, query-webpage pairing, and various user-item interac-tions on social media, e-commerce platforms, search engines, etc. Inthe real world, such graphs are often associated with rich attributes,e.g., the user profile in social networks, web page content in webgraphs, hallmarks of pathways in cancer signaling networks, andpaper keywords in academic graphs, which are termed AttributedBipartite Graphs (hereinafter ABGs).Given an ABG G with two disjoint node sets U and V, -Attributed Bipartite Graph Clustering (-ABGC), a fundamental taskof analyzing ABGs, seeks to partition the nodes in the node set of in-terest, e.g., U or V, into non-overlapping clusters C1, C2, , C,such that nodes within the same cluster C are close to each otherin terms of both their attribute similarity and topological prox-imity in G. Due to the omnipresence of ABGs, -ABGC has seena wide range of practical applications in social network analy-sis, recommender systems, information retrieval, and bioinformat-ics, such as user/content tagging , market basket analysis, document categorization , identification of proteincomplexes, disease genes, and drug targets , and many oth-ers .As reviewed in , existing solutions towards -ABGCprimarily rely on bipartite graph co-clustering (BGCC), attributedgraph clustering (AGC), and attributed network embedding (ANE)techniques. Amid them, BGCC has been extensively investigatedin the literature for clustering non-attributedbipartite graphs, whose basic idea is to simultaneously group nodesin U and V merely based on their interactions in G, instead ofclustering them severally. As pinpointed in prior works , theattributes present rich information to characterize the propertiesof nodes and hence, can complement scant topological information",
  "KDD 24, August 2529, 2024, Barcelona, Spain": "MSA of any two nodes in the same cluster will be maximized; inother words, nodes within the same cluster are tight-knit.According to , Eq. (8) is an NP-complete combinatorial op-timization problem. Hence, the exact solution to Eq. (8) is compu-tationally infeasible when U contains a large number of nodes.Moreover, the direct optimization of Eq. (8) demands materializing(,) of every node pairs in U U. As such, deriving an approx-imate solution by optimizing Eq. (8) with even a handful of epochsentails an (|E| |U| U) computational cost and a quadraticspace overhead (|U|2), rendering it incompetent for large ABGs.",
  "The continuous version of Y satisfying Eq. (11)": "demonstrate that TPO consistently attains superior or comparableclustering quality at a fraction of the cost compared to the state-of-the-art methods. For instance, on the largest Amazon dataset withover 10 million nodes and 22 million edges, TPO obtains the bestclustering accuracy within 3 minutes, whereas the state-of-the-artdemands more than 4 hours to terminate.",
  "PROBLEM FORMULATION2.1Notation and Terminology": "We denote matrices using bold uppercase letters, e.g., M R,and the -th row (resp. the -th column) of M is represented as M[](resp. M[:, ]). Accordingly, M signifies the entry at the -throw and -th column of M. For each vector M[], we use M[] torepresent its 2 norm and M to represent the Frobenius normof M.Let G = (U V, E, XU, XV) symbolize an attributed bipartitegraph (ABG), where E is composed of edges connecting nodes intwo disjoint node sets U and V and each edge (, ) is associatedwith an edge weight (, ). Each node U (resp. V) ofG is characterized by a length-U (resp. length-V) attribute vec-tor XU [] (resp. XV []). Further, we denote by BU R|U||V| the adjacency matrix of G from the perspective of U, in whichBU = (, ) if (, ) E and 0 otherwise. Let DU (resp.DV) be a |U| |U| (resp. |V| |V|) diagonal matrix wherein thediagonal entry DU (resp. DV ) stands for the sum of theweights of edges incident to (resp. ), i.e., (, )E (, )(resp. (, )E (, )). lists the frequently used nota-tions throughout the paper.The overarching goal of -ABGC is formalized in Definition 2.1",
  "Multi-Scale Attribute Affinity (MSA)": "Notice that Definition 2.1 cannot directly guide the generation ofclusters, as it lacks a concrete optimization objective that quantifiesnode affinities. To this end, we first delineate our novel affinity mea-sure MSA for nodes in terms of both graph structure and attributes,before formally introducing our objective in .3. MSA formulation. We first assume that each node U can berepresented by a feature vector ZU [], which characterizes boththe attributes as well as the rich semantics hidden in the bipartitegraph topology. Following the popular Skip-gram model andits extension to graphs , we can model pair-wise affinity ofnodes as a softmax unit parametrized by a dot product of theirfeature vectors. Rather than using the vanilla softmax function,we adopt a symmetric softmax function and formulate the MSA(,) between any two nodes , in U as follows:",
  "MSA (,) is symmetric, i.e., (,) = (,) , U.Additionally, by imposing a normalization, 1 ZU []ZU [] 1, U, and hence, the MSA values w.r.t. any node U arescaled to a similar range": "Optimization Objective for ZU. Next, we focus on the obtain-ment of the feature vector ZU [] for each node U. A favor-able choice might be graph neural networks (GNNs) , which,however, cannot be readily applied to ABGs as existing GNNs areprimarily designed for general graphs, and it is rather costly totrain classic GNNs. As demystified by recent studies ,many popular GNNs models can be unified into an optimizationframework from the perspective of numeric optimization, whichessentially produces node feature vectors being smooth on nearbynodes in terms of the underlying graph. Inspired by this finding,we extend this optimization framework to ABGs. More specifically,its objective is as follows:minZU(1 ) O + O,(3) which includes a non-negative coefficient and two terms:(i) a fitting term O in Eq. (4) aiming at ensuring ZU is close to theinput attribute vectors XU,O = ZU XU 2(4)and (ii) a regularization term O in Eq. (5) constraining the feature",
  "+ 1": "4, where the denominators 3, 2, and 4 corre-spond to the numbers of authors in papers 3, 4 and 5. Accordingly,(3,4) evaluates the overall contributions of 3,4 to their col-laborated research works in V. Thus, the O term in Eq. (3) is tominimize the distance of feature vectors of researchers who haveextensively collaborated with each other with high contributions.The hyper-parameter balances the attribute and topology in-formation encoded into ZU. In particular, when = 0, featurevectors ZU = XU, and at the other extreme, i.e., = 1, ZU isentirely dependent on the topology of G. Closed-form Solution of ZU. Given an , Lemma 2.21 indicatesthat the optimal feature vectors ZU to Eq. (3) can be computed viaiterative sparse matrix multiplications in Eq. (6) without undergoingexpensive training.",
  "V .(7)": "In practice, we set in Eq. (6) to a finite number (typically 5) forefficiency. Intuitively, the computation of ZU essentially aggregatesattributes from other homogeneous nodes as per their multi-scaleproximities (e.g., the strength of connections via multiple hops (atmost hops)) in G. As such, the feature vectors of nodes withnumerous direct or indirect linkages will be more likely to be close,yielding a high MSA in Eq. (1).",
  "Lemma 3.1. Eq. (8) is equivalent to the following objective:minY0,H0 R YH2 s.t. Y is an NCI matrix.(9)": "Specifically, if we can identify a matrix R such that R[] R[] =(,) . U, the computation of non-overlapping clus-ters C1, C2, , C towards optimizing Eq. (8) is equivalent todecomposing R into two non-negative matrices Y and H, where Yrepresents a normalized cluster indicator (NCI) matrix Y R|U|,as defined in Eq. (10).",
  "According to Eq. (10), for each node U, its correspondingvector Y[] in the NCI matrix comprises solely one non-zero entryY indicating the clustering membership of , and the valueshould be 1/": "|C |. This characteristic ensures that Y is column-orthogonal, i.e., YY = I. However, this constraint on Y renders thefactorization of R hard to converge. Instead of directly computingthe exact Y, we employ a two-step approximation strategy. Morespecifically, TPO first builds a |U| matrix (a continuous versionof Y) which minimizes the factorization loss in Eq. (11):min0,H0 R H2 s.t. = I,(11) in which the constraint on Y in Eq. (9) is relaxed to be 0 and = I. Afterward, the task is to transform into an NCI matrixY by minimizing their difference about Eq. (9).As outlined in , given an ABG G, the number of ofclusters, and the node set U to be partitioned as input, TPO outputsan approximate solution to the -ABGC problem in Eq. (8) throughthree phases: (i) constructing a low-dimensional matrix R such thatR[] R[] (,) . U without explicitly materializingthe MSA of all node pairs (Algorithm 1, .2); (ii) factorizingR as per Eq. (11) to create a U non-negative column-orthogonalmatrix (Algorithm 2, .3); and (iii) effectively converting into an NCI Y (Algorithm 3, .4). In what follows, weelaborate on the algorithmic details of these three subroutines. Dueto space limit, we defer the complexity analysis of them and TPO to",
  "Algorithm 1 illustrates the pseudo-code of linearizing the approxi-mate computation of MSA in Eq. (1) as the matrix product R R": "with matrix R. The fundamental idea is to leverage and tweak therandom features technique designed for approximating theGaussian kernel xy2/2 of any vectors x and y.After taking as input the ABG G and parameters,, Algorithm 1 begins by calculating LU according to Eq. (7) and initializing ZUas XU (Lines 1-2). At Lines 3-4, we update ZU via iterations ofthe following matrix multiplication:ZU (XU + LU (LUZU)).(12)Particularly, we structure the matrix multiplication LULUZU asLU (LUZU) in Eq. (12) to boost the computation efficiency. Sub- sequently, Algorithm 1 transforms ZU into ZU by applying an 2normalization to each row in ZU (Line 5) and then proceeds toconstructing R (Lines 6-9).To be specific, we first generate a U U Gaussian random ma-trix G with every entry sampled independently from the standardnormal distribution (Line 6) and then apply a QR decompositionover it to get a U U orthogonal matrix Q (Line 7). The matrixQ is distributed uniformly on the Stiefel manifold, i.e., the space ofall orthogonal matrices . Next, Algorithm 1 calculates R by",
  "return R;": "3.2.1SVD-based Attribute Dimension Reduction. AlthoughAlgorithm 1 circumvents the need to construct the MSA for allnode pairs, it remains tenaciously challenging when dealing withABGs with vast attribute sets, i.e., U being large. Recall that themajor computation expenditure in Algorithm 1 lies at Lines 3-4 andLines 7-8, which need ( |E| U) and (3U + |U| 2U) time,respectively. As a result, when U is high, e.g., U = (|U|), thecomputational complexity of Algorithm 1 increases dramatically tobe cubic, rendering it impractical for large-scale ABGs.To address this, we refine the input attribute vectors XU byreducing their dimension from U to a much smaller constant ( U). This approach aims to ensure that the -dimensionalapproximation XU of XU still accurately preserves the MSA as perEq. (1). This adjustment reduces the computational cost to a lineartime complexity of ( |E| + |U|) since is a constant. To realizethis idea, we first apply the top- singular value decomposition(SVD) over XU to produce the decomposition result XU .Utilizing the column-orthogonal (semi-unitary) property of , i.e., = I, we have XUXU = 2, implying XU = R|U|,(15)which can be employed as a low-dimensional substitute of XU inputto Algorithm 1. Along this line, we can derive a low-dimensionalversion ZU of feature vectors ZU using the iterative process atLines 2-4 in Algorithm 1 by simply replacing XU as XU, i.e.,",
  "holds for every two nodes , U, where +1 is the ( + 1)-thlargest singular value of XU": "Lemma 3.3 establishes the approximation guarantee of ZU, whichtheoretically assures the accurate approximation of the MSA de-fined in Eq. (1). Aside from the capabilities of preserving MSA andreducing computation load, this SVD-based trick can surprisinglydenoise attribute data for enhanced clustering by its close connec-tion to principal component analysis (PCA), as validated by our",
  "Greedy Orthogonal NMF": "Upon constructing R R|U|2 (with = U if the dimensionreduction from .2.1 is not applied) in Algorithm 1, TPOpasses it to the second phase, i.e., conducting an orthogonal non-negative matrix factorization (NMF) of R as in Eq. (11) to create. The pseudo-code of our solver to this problem is presented inAlgorithm 2, iteratively updating and H using an alternativeframework towards optimizing the objective function in Eq. (11).(Lines 3-5). Specifically, given the number of iterations and initialguess of H and , in each iteration, we first update each (, )-entry(1 2 and 1 ) in H following Eq. (16) while fixing, and then update for U and 1 as in Eq. (17)with H fixed.",
  "( ( (RH)))(17)": "The above update rules for solving Eq. (11) can be derived byutilizing the auxiliary function approach with Lagrangian mul-tipliers in convex optimization, whose convergence is guaranteedby the monotonicity theorem . Note that we reorder the matrixmultiplications H and RH in Eq. (16) and (17) to H ()and ((RH)), respectively, so as to avert materializing 2|U|dense matrix H and |U| |U| dense matrix . As such, thecomputational complexities of updating H and in Eq. (16) and(17) are reduced to (|U| + |U|2) per iteration.The aforementioned computation is still rather costly due to thenumerous iterations needed for the convergence of and H, espe-cially when and H are initialized randomly. We resort to a greedyseeding strategy to expedite convergence, as in many optimizationproblems. That is, we carefully select a good initialization of andH in a fast but theoretically grounded manner. As described in Lines1-2 in Algorithm 1, we set and H as follows: = , H = ,(18)where and are the top- left and right singular vectors of R,respectively, and is a diagonal matrix whose diagonal entriesare top- singular values of R, which are obtained by invoking thetruncated randomized SVD algorithm with R and . Note thatthis routine consumes (|U| + (U + )2) time and can bedone efficiently in practice in virtue of its randomized algorithmicdesign as well as the highly-optimized libraries (LAPACK and BLAS)for matrix operations under the hood.",
  "return Y;": "Given the fact that singular vectors = are column-orthogonal,i.e., = I, the Eckart-Young Theorem (Theorem A.2 inAppendix A.5) pinpoints that Eq. (18) offers the optimal solution toEq. (11) when the non-negative constraints over and H are relaxed.In simpler terms, Eq. (18) immediately gains a rough solution toour optimization objective in Eq. (11), thereby drastically curtailingthe number of iterations needed for Lines 3-5.",
  "Effective NCI Generation": "In its final stage, TPO generates an NCI matrix Y by minimizingthe difference between returned by Algorithm 2 and the tar-get NCI matrix Y. Recall from Eq. (9), our original objective isto find a |U| NCI matrix Y and a 2 non-negative Hsuch that the total squared reconstruction error R YH2 = U=1 (R Y[] H[])2 is minimized. Considering is a continuous version of Y (relaxing the constraint in Eq. (10)),R H2 is capable of attaining a strictly lower reconstructionerror compared to RYH2(9) ensures that R YH2 . Therefore, an ideal solution Y to Eq. closely approximates R H2 inEq. (11). Mathematically, the conversion from matrix into the NCImatrix Y can be formulated as the minimization of the differenceof their reconstruction errors, i.e.,R YH2 R H2 =trace((YY ) RR) by Lemma 3.4.",
  "Further, we reformulate the problem as follows:min,Y Y2 s.t. = I and Y is an NCI matrix,(20)": "which implies that, if the NCI matrix Y and the row-orthogonalmatrix minimize Y2, YY 0holds and the objective loss in Eq. (19) is therefore minimized.To solve Eq. (20), we develop Algorithm 3 in TPO, which obtainsthe NCI matrix Y through an iterative framework wherein andY are refined in an alternative fashion till convergence. Initially,Algorithm 3 starts by taking as input the matrix and the number of iterations and initializing as a identity matrix (Line 1).It then launches an iterative process at Lines 2-7 to jointly refine Yand . Specifically, in each of the iterations, TPO first determinesthe cluster id of each node U via (Line 4) = arg max1 [] [:, ](21)",
  "U Y2 = 1,(23)": "in accordance with the NCI constraint in Eq. (10) (Line 6). In anutshell, Lines 3-6 optimizes Eq. (20) by updating Y with fixed. Toexplain, recall the constraint of the NCI matrix Y stated in Eq. (10),each row of Y has merely one non-zero entry. Hence, by locating thecolumn id whose corresponding entry () is maximumin the -th row of (i.e., Eq. (21)) and meanwhile updating Y[]as Eqs. (22) and (23) as Lines 5-6, the distance between and Yin Eq. (20) is naturally minimized.With the refined Y at hand, the subsequent work turns intoupdating the matrix towards optimizingmin Y2 s.t. = I. Given Y, the minimizer to this problem is = Y by utilizingLemma 4.14 in . Therefore, is updated to Y at Line 7.After repeating the above procedure for iterations, TPO returnsY as the final clustering result. Practically, a dozen iterations aresufficient to yield high-caliber Y, as validated in .3.",
  "EXPERIMENTS": "In this section, we experimentally evaluate our proposed -ABGCmethod TPO against 19 competitors over five real ABGs in terms ofclustering quality and efficiency. All the experiments are conductedon a Linux machine powered by 2 Xeon Gold 6330 @2.0GHz CPUsand 1TB RAM. For reproducibility, the source code and datasets areavailable at",
  "Experimental Setup": "Datasets. lists the statistics of the five datasets used in theexperimental study. |U|, |V|, and |E| denote the cardinality of twodisjoint node sets U, V, and edge set E of G, respectively, whileU(resp. V) stands for the dimensions of attribute vectors of nodes inU (resp. V). The number of ground-truth clusters of nodes U in Gis . Citeseer and Cora are synthesized from real citation graphs in by dividing nodes in each cluster into two equal-sized partitions(i.e., U and V) and removing intra-partition edges and isolatednodes as in . In particular, nodes represent publications, edgesdenote their citation relationships, and labels correspond to thefields of study. The well-known MovieLens dataset comprisesuser-movie ratings, where clustering labels are users occupationsin U. Google and Amazon are extracted from the Google Maps",
  "Bipartite Graph Clustering: SCC , SBC , InfoCC , Spec-MOD , CCMOD , DeepCC , and HOPE": "Unless otherwise specified, on all datasets, we set the numbersand of iterations required by Algorithms 2 and 3 in our proposedTPO to 5 and 20, respectively. Regarding parameters and , bydefault, we set = 0.6, = 6 on CiteSeer and MovieLens, = 0.9, =10 on Cora and Google, and = 0.5, = 1 on Amazon, respectively.To deal with the high attribute dimensions U of the CiteSeer, Cora,Google, and Amazon datasets, we set their new attribute dimensions in .2.1 to 32, 128, 32, and 64, respectively. We refer to theversion of TPO without the attribute dimension reduction modulein .2.1 as TPO ( = U). More implementation details ofour method and baselines are in Appendix A.3. Evaluation Metrics. Following convention, we adopt three widelyused measures to assess the clustering quality,namely (i) Clustering Accuracy (ACC), (ii) Normalized Mutual In-formation (NMI), and (3) Adjusted Rand Index (ARI), for measuringthe quality of clusters produced by each evaluated method in thepresence of the ground-truth clusters of the tested dataset. Particu-larly, ACC and NMI scores range from 0 to 1.0, whilst ARI rangesfrom 0.5 to 1.0. For each of these metrics, higher values indicatebetter clustering quality. Regarding efficiency evaluation, we re-port the running time in seconds (measured in wall-clock time) ofeach method on each dataset, excluding the time for input (loadingdatasets) and output (saving clustering results). The formulas forevaluation metrics are in Appendix A.3.",
  "Clustering Performance": "This set of experiments reports the clustering quality achievedby TPO and all competitors over the five datasets, as well as theirrespective running times. We omit a method if it cannot report theresults within three days or incur out-of-memory (OOM) errors.Since TPO is randomized, we repeat it five times and report theaverage performance. Clustering Quality. shows the ACC, NMI, and ARI scoresof all methods on five ABGs, and their overall average performancerankings. We highlight the top-3 best clustering results on eachdataset in gray with darker shades indicating higher quality. TPOconsistently outperforms the 17 competitors on the CiteSeer, Movie-Lens, and Google datasets in terms of ACC, NMI, and ARI, by sub-stantial margins of up to 9.9% for ACC, 4.5% for NMI, and 12% forARI, respectively. The only exceptions are on Cora and Amazon,where TPO achieves the highest ACC and ARI results but inferiorNMI scores compared to PANE or AGCC. In addition, TPO ( = U)exhibits competitive clustering effectiveness, which either is secondonly to TPO or obtains the third best clustering results in most cases.Specifically, TPO ( = U) is comparable to TPO on Cora, Movie-Lens, and Amazon with a performance degradation at most 0.9%in ACC, 1.0% in NMI, and 1.3% in ARI. Over all datasets, TPO andTPO ( = U) attain the best and second best average performancerank (smaller rank is better), respectively. The evident superiorityof TPO and TPO ( = U) manifests the accuracy of our proposedMSA model in .2 in preserving the attribute similarity andtopological connections between nodes, as well as the effectivenessof theoretically-grounded three-phase optimization framework in-troduced in .At this point, a keen reader may wonder why TPO with attributedimension reduction outperforms TPO ( = U) on most datasets,especially CiteSeer and Google, as it seems that the former is anapproximate version of the latter. Notice that TPO and TPO ( = U)output identical results, as dimension reduction is not needed onMovieLens and TPO turns to be TPO ( = U). Recall that the onlydifference between TPO and TPO ( = U) is that TPO employs atruncated SVD over the input attribute vectors XU of a node in U",
  ": Clustering accuracy when varying parameters": "for dimension reduction as stated in .2.1. Aside from thecrucial theoretical assurance offered by this SVD-based approachin the MSA approximation, it implicitly conducts a PCA on theattribute vectors, extracting key features from the input attributeswhile eradicating noisy ones. In brief, the SVD-based trick in .2.1 grants TPO the additional ability to denoise the attribute data,thus elevating the results quality. Efficiency. For clarity, we compare the empirical efficiency of TPOand TPO ( = U) only against competitors ranked in the top7 for clustering quality, as shown in . plots thecomputation times required by each of these methods on Cora,MovieLens, Google, and Amazon. The -axis is the running time(seconds) in the log scale. On each of the diagrams in Figures 3(a),3(b), 3(c), and 3(d), all the bars are displayed from left to right in anascending order w.r.t. their average performance rank in .Accordingly, except the first two bars from the left for TPO and TPO( = U), the third bars (from the left) in these figures illustratethe running times of the best competitors, i.e., AGCC on Cora, andPANE on MovieLens, Google, and Amazon, respectively. As we cansee, TPO is consistently faster than the state-of-the-art approaches,AGCC or PANE, on four datasets, often by orders of magnitude. Forinstance, on Cora, Google, and Amazon, TPO takes 0.47, 28.7, and 178seconds, respectively, whereas the best baselines AGCC or PANEcost around 19 seconds, 23 minutes, and 4.1 hours, respectively,attaining 40, 48, and 83 speedup. In addition, TPO also enjoysa considerable efficiency gain of up to 19.9 over TPO ( = U),attributed to the SVD-based dimension reduction (.2.1). Onthe MovieLens dataset, the input attribute dimensionU = 30 is low,and the attribute dimension reduction is therefore disabled, makingTPO and TPO ( = U) yield the same running time, which is 3.46over the best competitor PANE. Although NMF, KMeans, and SCCrun much faster than TPO on some datasets by either neglecting thegraph topology or discarding the attribute data, their result qualityis no match for our solution TPO. In summary, TPO consistently delivers superior results for -ABGC tasks over ABGs with various volumes while offering highpractical efficiency, which corroborates the efficacy of our novelobjective function based on MSA in and the optimizationsolver with careful algorithmic designs developed in .",
  "Parameter Analysis": "In these experiments, we empirically investigate the impact of fivekey parameters in TPO: ,, ,, and . For each of them, we runTPO over CiteSeer, Cora, MovieLens, and Google, respectively, byvarying the parameter with others fixed as in .1. Varying and . Figures 4(a) shows that on Cora and Google,TPOs clustering performance markedly improves as increasesfrom 0.1 to 0.9, indicating the importance of graph structure inthese datasets. On CiteSeer and MovieLens, setting = 0.6 willbe a favorable choice, which results in an optimal combination ofattributes and graph topology and hence the highest ACC scores.Figures 4(b) depicts the ACC scores when increases from 0 to10. When = 0, the graph structure is disregarded in TPO, namelyZU = XU. It can be observed on all datasets that the clusteringquality rises with increasing except CiteSeer and MovieLens, wherethe ACC results reach a plateau after 6. This is consistent withthe fact that a larger produces a more accurate solution ZU tothe objective in Eq. (3), and thus, higher clustering quality. Varying and . Figures 4(c) presents the ACC scores whenthe of iterations in Algorithm 2 is varied from 0 to 20. We canconclude that our greedy seeding strategy described in .3 is highly effective in enabling swift convergence, as additionaloptimization iterations merely bring minor gains in clustering per-formance. On Cora and CiteSeer, the ACC scores see an uptick whenvarying from 0 to 10, followed by a pronounced downturn. Sucha performance decline is caused by overfitting in solving Eq. (11).From Figures 4(d) reporting clustering performance changes when",
  "Effective Clustering on Large Attributed Bipartite GraphsKDD 24, August 2529, 2024, Barcelona, Spain": "varying from 0 to 20, we can make analogous observations onthe four datasets. The evaluation scores first experience a sharpincrease as increases from 0 to 5. After that, the ACC remaininvariant with increasing. The results manifest the effectivenessof our solver developed in .4 in fast NCI generation. Varying . Intuitively, a large may lead to accurate preservationof MSA as per Lemma 3.3 and further improve clustering quality.However, in practice, the original attribute vectors XU embodynoises, especially when U is high. As pinpointed and validated inSections 3.2.1 and 4.2, our SVD-based dimension reduction inher-ently applies a PCA over XU for noise elimination, considerablyupgrading the empirical result quality. That is to say, the choice of strikes a balance between capturing MSA and removing noisy data,consistent with our empirical results in (e). In particular,on CiteSeer, Cora, and Google, picking 32, 128, and 32 for dimension, respectively, can strike a good balance between MSA preserva-tion and noisy reduction for superior clustering performance. OnMovieLens, the attribute dimension reduction is not enabled when 32 since its original dimension U = 30. We refer interestedreaders to Appendix A.4 for NMI and efficiency results.",
  "RELATED WORK": "Bipartite Graph Clustering. A classic methodology for bi-partite graph clustering first projects a bipartite graph G into aunipartite graph by connecting every two nodes from the samepartition U if they share common neighbors in G. Then, a standardgraph clustering algorithm for node clustering can be adopted onthe constructed unipartite graph. However, the projection oftenleads to unipartite graphs (|U|2) edges, which is intolerable foreven medium-sized graphs. In our previous work , we addressthis problem by transforming it into a two-stage approximationframework.Unlike the projection-based methods, another line of researchfocuses on simultaneously clustering two disjoint sets of nodes(i.e., U and V) in a bipartite graph. These co-clustering techniqueshave been extensively investigated in the literature and spana variety of applications in bioinformatics and text mining. Sev-eral attempts are made to extend spectral clustering tobipartite graphs. Analogously, Ailem et al. and Dhillon et al. propose generating co-clusters by extending and optimizingclassic metrics of modularity and mutual information on bipartitegraphs, respectively. DeepCC creates low-dimension instancesand features using a deep autoencoder, then assigns clusters usinga variant of the Gaussian mixture model. To handle the resolutionlimit in prior works as well as incorporate attribute information,Kim et al. designed ABC, which incurs a severe efficiency issuedue to its quadratic running time (|U|2 + |V|2). Attributed Graph Clustering. As surveyed in , thereis a large body of work on attributed graph clustering (AGC). Ac-cording to , existing AGC techniques can be categorized intofour groups: edge-weigh-based methods , distance-basedmethods , statistics-based models , and graphlearning-based methods . Among them, graphlearning-based approaches have achieved state-of-the-art performance, as reported in . These methods obtain high clustering quality on attributed graphs at the cost of costlyneural network training, thus incurring poor scalability on largegraphs. To our knowledge, the statistical-model-based solution,ACMin , is the only AGC method that scales to massive graphswith millions of nodes and billions of edges, while attaining highresult quality. However, none of them are custom-made for ABGs,producing compromised result quality for -ABGC. Network Embedding. In recent years, network embedding, whichconverts each node in a graph into an embedding vector capturingthe surrounding structures, has been employed in a wide rangeof graph analytics tasks, and has seen remarkable success .In particular, by simply feeding them into data clustering meth-ods, e.g., KMeans, such embedding vectors can be utilized to copewith -ABGC. However, the majority of network embedding works are designed for graphs in the ab-sence of node attributes. To bridge this gap, a series of efforts have been made towards incorporat-ing node attributes into embedding vectors for enhanced resultutility. These approaches still suffer from sub-optimal clusteringperformance as they fall short of preserving the hidden seman-tics underlying bipartite graphs. To learn effective node embed-dings over ABGs, extend SkipGram models to ABGs bypicking node-pair samples with consideration of both their intra-partition/inter-partition proximities and attribute similarities. Atharet al. project the ABG into two homogeneous graphs based ontopological connections and attribute similarities, and then invokeunsupervised GNNs on the constructed graphs for embedding gen-eration. Moreover, Zhang et al. propose IGE for learningnode embeddings on dynamic ABGs with a focus on temporaldependence of edges rather than the bipartite graph structures.These works either fall short of preserving multi-hop relationshipsbetween nodes or struggle to cope with large ABGs due to thesignificant expense of training.",
  "CONCLUSION": "This paper presents TPO, an effective and efficient solution for -ABGC tasks. TPO achieves remarkable performance, attributed toa novel problem formulation based on the proposed multi-scaleattribute affinity measure for nodes in ABGs, and a well-thought-out three-phase optimization framework for solving the problem.Through a series of theoretically-grounded efficiency techniquesdeveloped in this paper, TPO is able to scale to large ABGs withmillions of nodes and hundreds of millions of edges while offeringstate-of-the-art result quality. The superiority of TPO over 19 base-lines is experimentally validated over 5 real ABGs in terms of bothclustering quality and empirical efficiency. Renchi Yang is supported by the NSFC YSF grant (No. 62302414)and Hong Kong RGC ECS grant (No. 22202623). Qichen Wang issupported by Hong Kong RGC Grants (Project No. C2004-21GFand C2003-23Y). Tsz Nam Chan is supported by the NSFC grant62202401. Jieming Shi is supported by Hong Kong RGC ECS (No.25201221) and NSFC 62202404.",
  "AAPPENDIXA.1Illustrative Examples": "exemplifies an ABG G with 7 researchers 1-7 in U, 8research publications 1-8 in V, and the authorships in E. Addi-tionally, each researcher possesses a collection of attributes, includ-ing nationality, work institution, and academic qualifications. In, 1,2 share identical attributes and close collaboration,indicating they should be grouped in the same cluster. Similar obser-vations can be made for node pairs (3,4) and (6,7), where thedifference is that their attributes are partially analogous. Despitelimited connections related to 5, 5 and 4 are likely to be groupedtogether due to their identical attributes, with 4 being the solecollaborator of 5. Overall, given = 3, an intuitive and ideal solu-tion for -ABGC in Definition 2.1 is to divide the 7 researchers into3 clusters, i.e., C1 = {1,2}, 2 = {3,4,5}, and C3 = {6,7},with considering both their connectivity (collaboration) in G andattribute homogeneity.Example A.1 provides an intuitive understanding of our opti-mization objective function in Eq. (2.3). Example A.1 (A Running Example). Suppose that the ABGG in is unweighted, i.e., all edge weights (,) are 1.During preprocessing, the attributes of nodes in U are convertedinto 3-dimensional vectors XU as in (a). Assume that theparameters and in Eq. (6) are 0.5 and 5, respectively, and thenumber of clusters is 3. (b) displays the feature vectors forresearchers 1 to 7 obtained by adopting the attribute aggregationin Eq. (6) and imposing the normalization in Eq. (2). We obtainthe MSA values of every two researchers in (c) using Eq.(1). From (c), it can be observed that the nodes with thehighest MSA w.r.t. 1-7 (excluding themselves) are 2, 1, 4, 5,4, 7, 6, respectively. This implies a partition of researchers 1-7into: C1 = 1,2, C2 = 3,4,5, and C3 = 6,7, optimizing theobjective in Eq. (8) (i.e., a maximization of the average intra-clusterMSA and a minimization of the average inter-cluster MSA).",
  "A.2Complexity Analysis": "Algorithm 1. As mentioned in .2.1, Lines 3-4 and Lines7-8 in Algorithm 1 need (|E| +3 + |U| 2) time in total. Asfor other operations in Algorithm 1, their processing overheads aredetermined by the number of entries in L, XU, ZU, and R, whichcan be bounded by (|E| + |U| +2). Accordingly, the total costentailed by Algorithm 1 is (|E| + 3 + |U| 2).",
  "R as well as rounds of and H updates at Lines 3-5, whichdemands (|U| + (U +) 2) time by and (|U| +|U|2 ) time according to the analysis in .3, respectively": "Algorithm 3. In Algorithm 3, the computation cost is dominatedby Eq. (21) ((|U| 2) time for all nodes per iteration) and thesparse matrix multiplication at Line 7 ((|U|) time per iteration),leading to (|U| 2) time for iterations in sum. TPO. Overall, the asymptotic computational complexity of TPO is(|E| + 3 + |U| 2 + |U| + |U| 2), which canbe simplified as (|E| + |U| 2) if , , and are regardedas constants. In addition to the space overhead of (|E| + |U| U) for the storage of G, TPO requires materializing ZU and Rin Algorithm 1, amounting to a space consumption of (|U| ).Hence, the space complexity of TPO is bounded by (|E|+|U|U)since U.",
  "A.3Experimental Setup Details": "Implementation Details. For KMeans, NMF, SpecClust, SCC,and SBC, we use their standard implementations from the machinelearning library scikit-learn with default parameters. We alsoadopt the implementations of InfoCC, SpecMOD, and CCMODfrom the Coclust package . As for the rest of the competitors, wecollect their source codes from the respective authors and use theparameter settings suggested in their papers. The data clusteringmethods and bipartite graph co-clustering algorithms are conductedon the XU and the graph data (e.g., adjacency matrix) withoutXU, respectively, owing to their inherent designs. The codes of allcompetitors are collected from their respective authors or popularopen-source libraries, and all are implemented in Python.",
  "A.4Parameter Analysis in Efficiency": "illustrates the running time of TPO when varying parame-ters , , , and . It can be observed that is the most impactfulparameter on the computational time of TPO as its time complexityis proportional to 2 analyzed in Section A.2. Regarding and ,they affect the efficiency of Algorithms 1 and 2, respectively, whichengender slight runtime growth. In contrast, (c) showsthat the running time of TPO almost stays steady, demonstratingthe superb efficiency of Algorithm 3 compared to the other twoprocedures. presents the NMI scores when varying parameters ofTPO as in .3, which are quantitatively similar to the accu-racy results in ."
}