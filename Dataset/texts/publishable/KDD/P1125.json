{
  "ABSTRACT": "While large language models (LLMs) have taken great strides to-wards helping humans with a plethora of tasks, hallucinationsremain a major impediment towards gaining user trust. The flu-ency and coherence of model generations even when hallucinatingmakes detection a difficult task. In this work, we explore if theartifacts associated with the model generations can provide hintsthat the generation will contain hallucinations. Specifically, weprobe LLMs at 1) the inputs via Integrated Gradients based tokenattribution, 2) the outputs via the Softmax probabilities, and 3) theinternal state via self-attention and fully-connected layer activa-tions for signs of hallucinations on open-ended question answeringtasks. Our results show that the distributions of these artifacts tendto differ between hallucinated and non-hallucinated generations.Building on this insight, we train binary classifiers that use theseartifacts as input features to classify model generations into hallu-cinations and non-hallucinations. These hallucination classifiersachieve up to 0.80 AUROC. We also show that tokens precedinga hallucination can already predict the subsequent hallucinationeven before it occurs.",
  "INTRODUCTION": "Past few years have witnessed a growing adoption of Large Lan-guage Models (LLMs) as interactive assistants . For in-stance, search engines are increasingly powered by LLMs. Instead ofissuing keyword-based search queries, users are starting to interactwith search engines in a conversational manner . One keyrequirement for conversational models is the ability to accuratelyretrieve factual knowledge .Recent evidence suggests that LLMs can indeed an-swer factual questions by completing prompts like Tsar Peter I wasborn in (Moscow) and The capital of Germany is(Berlin). This fact retrieval ability, however, is still far from beingreliable in practice. For instance, the HELM benchmark showsthat the best performing model in their setup, text-davinci-002,has an accuracy of mere 38.5% in answering facts from Wikipedia.Given this modest accuracy, it is important to detect when aLLM is correctly retrieving facts v.s. when it is hallucinating, sothat the end-users and downstream applications can be appropri-ately cautioned. However, detecting hallucinations is a challengingproblem since hallucinated generations can look very similar tonon-hallucinated ones in terms of coherence and fluency . Con-sider the output of the Falcon-7B model on two prompts. The firstcompletion is correct while the second is a hallucination, i.e., con-tains factually inaccurate information. The composition of the textprovides no clues on the correctness of the completion.",
  "Prompt: Tsar Peter I was born inCompletion: 1984 and graduated from the DepartmentofContemporaryArtHistoryandTheoryoftheUniversity of Rijeka (Croatia) in 2007": "In this paper, we draw inspiration from neural machine trans-lation (NMT) literature and ask: Can artifacts associated withthe model generation provide clues on hallucinations? Whilethe generated text is often the only entity the end-users see, thereare several other artifacts associated with the generation. Our ques-tion is based on the insight that while the generated text mightlook similar between hallucinations and non-hallucinations, theseartifacts might provide signals on hallucinations.",
  "KDD 24, August 2529, 2024, Barcelona, SpainBen Snyder, Marius Moisescu, and Muhammad Bilal Zafar": ": [TriviaQA dataset] The 2D TSNE distribution of the self-attention scores. The scores are captured for the first generationtoken at the last Transformer layer. The distributions are different between hallucinated v.s. non-hallucinated generationsthough the differences are more pronounced for some models than the others. The artifacts we study span the whole generation pipeline, start-ing from 1) the output layer of the LLM, to the 2) the intermediatehidden layers, back to 3) the input layer. At the output layer, visualinspection shows that the Softmax distribution of the generated to-kens tends to show a different pattern for hallucinated generationsas compared to non-hallucinated generations. Similarly, at the hid-den layers, both the self-attention scores as well as the activationsat the fully-connected component of the Transformer layers aredifferent between hallucinations and non-hallucinations (Azariaand Mitchell also make the same insight but leverage a differentdetection setup; see 2). We see similar trends at the input layer,where we use Integrated Gradients token attribution scores.Interestingly, we notice that these differences appear even at thefirst generation position, i.e., the point where the input is processedby the model but the first token is not yet generated. In other words,the model provides clues on whether it will hallucinate even beforeit hallucinates (Kadavath et al. shows a similar insight whenfine-tuning models to detect hallucinations but consider a slightlydifferent setup; see 2). shows an example.Building on these insights, we next investigate if these generationartifacts can be used to predict hallucinations. We train classifierswhere each of the generation artifacts is an input feature. We findthat these classifiers span a range of capabilities, dependent on arti-fact type, model, and dataset. Classification performance reaches ashigh as 0.81 AUROC for Falcon 40B using self-attention scores and0.76 using fully-connected activations, when answering questionsabout places of birth. Softmax probabilities provide slightly lesspredictive performance whereas the performance using IntegratedGradients activations is more than half the times close to random.To summarize, we: 1) develop a set of tests for hallucinations inopen-ended question answering using token attributions, Softmaxprobabilities, self-attention, and fully-connection activations; 2)show that hallucinations can be detected with significantly betterthan random accuracy even before they occur; and 3) show thatwhile the behavior varies from one dataset/LLM pair to another,self-attention scores and fully-connected activations provide morethan 0.70 AUROC for most pairs.",
  "RELATED WORK": "Hallucinations before LLMs. Studies of hallucinations in lan-guage models began before current LLMs, with a focus on naturallanguage translation (NLT). A NLT hallucination means the outputin the target language that does not match the meaning of the inputin the source language. Popular examples of NLT hallucinationemerged in 2018 with online translation tools outputting unrelatedreligious sounding phrases, perhaps due to over-reliance on reli-gious texts as training material for less common languages . Types of hallucinations in natural language generation. Whatconstitutes a hallucination is highly task-specific. A recent surveyby Ji et al. divides hallucinations in natural language gen-eration in two broad categories, intrinsic and extrinsic. Intrinsichallucinations occur when a model generates output that directlycontradicts the source input. Examples include inaccurate summa-rization (e.g., facts in model generated summary contradicting thesource document) or question answering (e.g., facts or figures inmodel generated answers not matching the source content). Ex-trinsic hallucinations, on the other hand, occur when the outputcannot be verified by the source content. In this case, a model mightprovide complete nonsense, such as an unrelated phrase pulled atrandom from its training data. While Ji et al. classify factualquestion answering hallucinations as intrinsic, they also note thatFor the generative question answering (GQA) task, the explorationof hallucination is at its early stage, so there is no standard defini-tion or categorization of hallucination yet. A more recent workby Huang et al. dives deeper into the hallucination taxonomy.They differentiate between factuality hallucinations (discrepancybetween generated content and verifiable real-world facts) andfaithfulness hallucinations (divergence of generated content fromuser instructions and/or the context). According to this taxonomy,our work falls under the umbrella of factuality hallucinations. Hallucination detection in questions answering. Kadavathet al. study whether LLMs are able to detect when they arehallucinating. They fine-tune the model to predict the probabilitythat it knows the correct answer and find that it leads to promising",
  "On Early Detection of Hallucinations in Factual Question AnsweringKDD 24, August 2529, 2024, Barcelona, Spain": "While each of the artifacts can be computed for each generatedtoken, we focus on the first generated token only. We also ranpreliminary experiments combining the artifacts over all the gen-erated tokens (e.g., via averaging Softmax probabilities over thetokens) and the final predicted token but did not notice meaningfulimprovements. Classifier architecture. The classifiers using IG attributions con-sist of a 4 layer Gated Recurrent Unit network with 25% dropoutat each layer. We use a recurrent network instead of feed-forwardbecause the dimensionality of attributions is different for each input(one attribution score for each token in the question), and insteadof a Transformer because of the relatively small amount of trainingdata. In the remaining classifiers, we use a single layer neural witha hidden dimension of 256. For each dataset, we train and evaluateon a random 80/20 split.",
  "Mitigating hallucinations. Pagnoni et al. propose bench-marks for hallucination in summarization tasks. Their benchmark": "involves human annotators reviewing model-generated summaries,and comparing them to inputs.Efforts to reduce LLM hallucinations have shown some success.Reinforcement learning with human feedback (RLHF) uses ahuman in the loop strategy to reduce hallucinations. Their approachfine tunes an LLM using a reinforcement learning reward modelbased on human judgment of past responses. Because of RLHFsrelatively high cost, others have proposed fine tuning models on alimited set of specially curated prompts . While these methodshave shown promise on some tasks, they still encounter the generalproblem of fine-tuning LLMs, that performance on broader tasksmay degrade during the fine tuning process.",
  "Setup": "We consider a generative QA setting where the users prompt theTransformer-based LLM. Let the question be a sequence consistingof tokens, that is, Q = [1,2, . . . ,]. We refer to the resultinggeneration as the model answer A = [1,2, . . . , ]. We also as-sume that each question is accompanied by a ground truth answer,commonly referred to as reference answer R . Let the modelvocabulary consist of tokens. At each generation step, the modelcan generate one of tokens. We denote the Softmax probabilitydistribution at generation step by P( |Q,1, . . . 1) . Factual Hallucinations. We consider the model response to be ahallucination if the generated answer A is factually incorrect. Thisdefinition is consistent with prior works in generative questionanswering that considers factually incorrect statements to be aform of hallucinations .LLM responses can be quite verbose so an exact string matchwith R is insufficient to establish the correctness of A . ConsiderQ = What is the capital of Germany? with R = Berlin. BothA = Berlin and A = Berlin, which is also its most populouscity are correct answers. To account for the response verbosity,we consider an answer to be correct if the reference answer iscontained within the generation A, that is, if R A. We convert alltokens to lowercase before performing the comparison.Given the reference answer R, establishing the correctness ofthe model response A is an open research problem with humanannotations being the only reliable source of truth . A manualanalysis of a subset of model generations in 4.5 shows a highagreement between the R A heuristic and human annotations.",
  "We consider four model artifacts for detecting hallucinations": "3.2.1Softmax probabilities. We posit that the Softmax probabil-ity distribution can be used to detect hallucinations. Specifically,following the analysis in , we hypothesize that the Softmaxdistribution has a higher entropy when the model is hallucinating.A higher entropy means that the model is less sure about its pre-diction. The model generation consists of answer tokens so onecould consider different probability distributions. For simplicity,",
  "(b) IG attribution": ": [Falcon-40B on TriviaQA dataset] Model artifactsdiffer between hallucinated (top row) and non-hallucinatedoutputs (bottom row). a shows the Softmax distribu-tion for top-10 tokens at the first generation position. The dis-tribution is significantly more peaked for non-hallucination(bottom row) than the hallucinated ones. b shows theinput feature attribution scores computed using IG methodfor the same pair of hallucinated (top) and non-hallucinated(bottom) outputs. Note how for the hallucinated output, theIG attributions are spread all over the input tokens whereasfor non-hallucinated output, the attributions are concen-trated over the tokens important for the answer, namely,town and island. we mainly focus on the distribution at the first generated token,but analyze the effect of focusing on other tokens in 5.2.a shows an example of difference in Softmax probabilities.The figure is generated by passing two questions from the Trivi-aQA dataset through the Falcon-40B model. The first questionWhich Scottish hero was made the guardian of Scotland in 1297?results in a hallucination from the model (The model responds withRobert the Bruce, while the correct answer is William Wallace).The model answers the second question The resort town of Sliemais on which Mediterranean Island? correctly (Malta). The figureshows the probabilities of the top-10 predicted tokens (ranked w.r.t.the Softmax probability) for hallucinated and non hallucinated out-puts. The distribution of the probabilities are notably different.Kadavath et al. also test a similar hypothesis that the entropyin the generated tokens could be a signal of hallucinations. However,their approach is slightly different from ours. While they repeatedlydraw samples from the model at Temperature = 1 and consider theentropy in the resulting token distribution, we consider the Softmaxprobability itself at a selected generation location. 3.2.2Feature attributions. Feature attributions, that is, how impor-tant a feature was towards a certain prediction are often used toinspect the behavior of the model and find potential problematicpatterns . Building on these insights, we posit that whenanswering the questions correctly, the model would focus on few input tokens. For example, when answering Berlin to What isthe capital of Germany? the model would focus on Germany. Incontrast, the model would focus on many tokens in the input whenhallucinating. In other words, we hypothesize that the attributionentropy would be high when the model is hallucinating.Formally, let R be the feature attribution of the answertoken . Then we can use the attributions {}=1 to detect halluci- nations. The entry ()denotes the importance of the questiontoken in predicting the answer token .There is a plethora of methods for generating . In thiswork, we use the Integrated Gradients (IG) . We select IG for thefollowing reasons: It provides attractive theoretical properties, e.g.,efficiency meaning that the sum of all feature attributions equals theoutput Softmax probability. By leveraging gradients, it runs fasterthan related methods like Kernel SHAP . Unlike methods likeLayerwise Relevance Propagation that require architecture-specific implementations, IG can operate on any architecture aslong as the model gradients are available.We show the IG input token attributions for a hallucinated andnon-hallucinated generations in b. The figure shows a cleardifference in distributions: For the non-hallucinated output, theLLM focuses on key tokens in the input (town and island). Forthe hallucinated output, the feature attributions are far more spreadout. 3.2.3Self-attention and Hidden Activations. Finally, in a mannersimilar to that of Softmax probabilities and feature attributions,we posit that the internal states of the model would also differbetween hallucinated and non-hallucinated responses. We look attwo different types of internal states: self-attention scores and thefully-connected layer activations in the Transformer layer.Formally, given a Transformer language model with layers, letS denote the self-attention at layer 1, . . . , and H denote thefully-connected activations. We focus specifically on S(,1)and H(,1)which represent the self-attention and fully-connectedactivations between the final token of the input question Q and thefirst token of the response 1. Also, unless mentioned explicitly, wefocus on the last Transformer layer only, that is, = . This choicewas made based on the preliminary experiments that showed thelast layer to provide the most promising performance (see 5.2).",
  "Hallucination Detection Classifiers": "Since our goal is to assign a hallucination / non-hallucination labelto the model generations, we now describe how to use the artifactsdetailed in 3.2 to arrive at this binary label.Given a QA dataset D, we split it into a train and test sets, Dtrainand Dtest and train four binary classifiers to detect hallucinations,each consisting of a different set of input features. The input featuresof these classifiers are:",
  "We use the following two QA datasets: the T-REx dataset theTriviaQA dataset": "4.1.1T-REx. The T-REx dataset consists of relationship tripletscontaining pairs of entities and their relationships, e.g., (France,Paris, Capital of) and (Tsar Peter I, Moscow, Born in). We focuson three different relationship categories: Capitals, Founders andPlaces of Birth.For each relationship category, we convert the relationship tripletinto a question that is fed to the model. Here are example questionsfrom each category:",
  "(1) Capitals: What is the capital of England?(2) Founders: Who founded Amazon?(3) Birth Place: Where was Tsar Peter I born?": "We found that the T-REx corpus consists of several cases wheremultiple subject/relationship pairs share the same object, e.g., (Geor-gia, Atlanta, Capital of), and (Georgia, Tbilisi, Capital of). We mergesuch triplets such that either of Atlanta, or Tbilisi is considereda correct answer.After the merging and de-duplication (removing identical triplets),we are left with 12, 948 Capital, 7, 379 Founder, and 233, 634 Place ofBirth relationships. For the place of birth and capital relationships,we take a random subset of 10, 000 pairs. 4.1.2TriviaQA. TriviaQA is a reading comprehension dataset con-sists of a set of 650, 000 trivia question, answer and evidence tuples.Evidence documents contain supporting information about the an-swer. We only use the closed book setting where the model isonly provided with questions without any supporting information.Some example questions from the dataset are:",
  "Models": "We analyze responses from three different models: OpenLLaMA(LAM),1 OPT (OPT)2 and Falcon (FAL).3 All three models come withdifferent size-based variants. We consider two different sizes foreach model: LAM-7B (7 billion parameters) and LAM-13B; OPT-6.7Band OPT-30B; FAL-7B and FAL-40B. We consider different variantsto study the effect of model size on the hallucination detectionperformance.",
  "Infrastructure and Parameters": "All experiments were ran on an Amazon SageMaker ml.g5.12xlargeinstance with 192 GB RAM and 4x NVIDIA A10G GPUs. Self-attention and fully-connected activations were captured using Ama-zon SageMaker Debugger .Generations are performed by sampling the most likely tokenaccording to the Softmax probability. This corresponds to using aTemperature = 0. We continue generating until an <end of text>tokens is generated or the generation is 100 tokens long.We compute the Integrated Gradients attribution using Cap-tum . We use a baseline of all 0s and the number of IG iterationsis set of 50. Given an output token, the corresponding IG attribu-tions for each input token are a vector of the same dimensionalityas the input token embeddings. We convert the vector scores tothe a per token scalar score by using the L2 norm reduction, whichhas been shown to provide similar or better performance to otherreduction strategies .Hallucination classifiers are trained with a batch size of 128 for1, 000 iterations. We use Adam optimizer with a learning rate of104 and weight decay of 102.",
  "Question Answering Accuracy": "Before moving on to hallucination detection, we first analyze theperformance of the models in correctly answering the questionsi.e., how often the models hallucinate. shows the accuracyof different models. The models showed a range of performanceacross each task. All models performed best on the TriviaQA tasks,and worst on the Birth Place task. Surprisingly, larger models didnot always perform better. On the subject specific tasks from T-REx,smaller models often performed as well or better than their largercounterparts (6 out of 9 times). On the more general TriviaQA task,larger models consistently performed better. On all tasks, FAL-40Bsignificantly outperformed all other models. Further, while largermodels on average performed better at general knowledge tasks,variation in performance is more strongly correlated with modeltype rather than size. For example, while LAM-13B outperformed itssmaller variants LAM-7B, and similarly OPT-30B outperformed itssmaller variant, both LAM models outperformed both OPT models.",
  "(b) Small": ": Accuracy of different models in answering the questions. We consider two model size variants: large (left table) andsmall (right table). Models tend to perform the best on TriviaQA dataset and the worst on Birth Place dataset. Performancecorrelates more with the model type than with model sizewith FAL performing the best.",
  "Accuracy of the hallucination heuristic": "Recall from 3.1 that we consider a model answer A to be a halluci-nation if R A. To evaluate this heuristic, we randomly sample 100TriviaQA generations with FAL-40B. For each generation, each ofthe three authors independently labeled it as (non-) hallucination.Based on the majority vote, the heuristic was correct in 98/100cases. The cases where the heuristic was incorrect are: Question: In Greek mythology, one of the 12 Laboursof Hercules was to produce what item belonging toAmazonian queen Hippolyte?Ref. Answer: Magical GirdleModel Answer: The girdle of Hippolyte was a girdlethat was worn by the Amazonian queen Hippolyte. Itwas one of the 12 Labours of Hercules.Heuristic label: HallucinationHuman label: Not a hallucination Question:WhichBritishPrimeMinisterwasthe1st Earl of Stockton?Ref. Answer: Harold Macmillan or Earl of StocktonModel Answer: Which British Prime Minister was the1st Earl of Stockton?Heuristic label: Not a hallucinationHuman label: Hallucination",
  "Qualitative analysis": "Recall our hypotheses that hallucinating and non-hallucinating gen-erations differ have different distributions of generation artifacts,namely, Softmax probabilities, IG attributions, self attention andfully-connected activations (3.2). Examining FAL-40B on the Triv-iaQA dataset (), we find that hallucinated outputs indeedtend to be distributed differently from non-hallucinated outputswhen considering Softmax probabilities, self-attention scores andfully-connected activations. The IG attributions however do notshow much difference in distributions.The results when comparing the entropy of distributions, andother datasets and models are mixed (figures omitted due to lack ofspace). The distributions of input token entropy showed a strongercorrelation to dataset type than to model. For topic specific datasets,entropy is on average lower for hallucinated results, while it isslightly higher for hallucinated results on TriviaQA. The entropiesof the Softmax outputs are less stable across both datasets and mod-els. While most cases show slightly higher entropy for hallucinatedresults, the difference is inconsistent. Further, in both the inputtoken and output Softmax cases, the differences, while present, arerelatively small. We find similarly small difference in entropy alongall generated tokens (see ). 2-dimensional TSNE plots alsoshow similarly mixed trends.While mixed, the results show that in many cases, thereduced distributions of model artifacts (either to 2 dimen-sions via TSNE or to 1 dimension via entropy) show visu-ally discernible differences between hallucinated and non-hallucinated outputs. Next we investigate if we can train accuratehallucination detectors by using these artifacts in their original formi.e., without reductions.",
  "Hallucination Classifiers": "Tables 2 and 3 shows the test AUROC when detecting hallucinations.We opt for AUROC instead of binary classification accuracy becauseof high class imbalance (). We include the results for binaryclassification accuracy in Appendix A.Results show that the IG attribution does slightly better thanrandom chance on the TriviaQA dataset, but no better than randomon the subject specific datasets. Softmax consistently does betterthan random for all tasks. Self-attention scores and fully-connectedactivations outperform both IG and Softmax. Interestingly, theseresults hold even though 96% of generated responses start withthe newline character, indicating that the classifier is not simply learning tokens that correlate with hallucination, but that evenwith the same token, the model internal state between hallucinatedand non-hallucinated results differs.While overall accuracy correlates with model size within eachmodel type, the trend is less consistent for a given models ability toidentify its own hallucinations. In the case of LAM, the larger variantconsistently performed better at identifying hallucination throughour classifier. OPT and FAL, on the other hand, showed no consistentcorrelation to size in their ability to capture hallucinations. In boththe large and small model variants, the fully-connected and selfattention activation internal states provided the best performance atidentifying hallucinations, followed by the models Softmax output.",
  "AUROC": "Attribution ROCSoftmax ROCBaseline ROCFully Connected ROCAttention ROC : [FAL-40B on TriviaQA dataset] AUROC of the hallu-cination detectors using self-attention and fully-connectedactivations at different layers. The performance is better atlater layers but has diminishing returns. A classifier trained on all four datasets combined (labeled asCombined in Tables 2 and 3) tended to perform slightly better thanthe individual datasets. However, a classifier trained on all modelartifacts combined (Softmax, IG, self-attention and fully-connectedactivations) showed no improvement beyond models trained oneach artifact individually (). There could be several reasonsfor this: the input dimensionality of this combined classifier is muchhigher than the individual classifiers and it is architecturally morecomplex as it consists of both dense and recurrent units. Trainingthis mixed architecture might require special considerations. Weleave the detailed analysis to a follow up work.In summary we note that different model artifacts provide dif-ferent level of accuracy in detecting hallucinations and in mostcases, self-attention scores and fully-connected activationsprovide over 0.70 AUROC in detecting hallucinations over arange of datasets and models.",
  "We take a closer look at various hyperparameters and also conducta performance comparison with an existing method": "Effect of Transformer layer choice. The results in 5.2 werebased on using the self-attention and fully-connected activationsfrom the last Transformer layer. We also investigate how the per-formance would change as a results of a change in layer number.Results in show that the performance of our classifier im-proves with depth of the model layers. Early layers do only slightlybetter than random chance, while later layers shows significantimprovement beyond random chance. Effect of classifier hyperparameters. Given the simplicity ofour hallucination detection classifiers, the only hyperparametersavailable were batch size, learning rate, and weight decay. We hand-selected the defaults in 4.3. We also tried (TriviaQA with FAL-40B)a range of different options for these hyperparameters where wevaried batch size from 4 to 256, learning rate from 106 to 102 and weight decay from 104 to 101. We found that the smallestbatch size and largest learning rate performed worse. But smalleradjustments (batch size 128 vs 256, learning rate 1104 v.s. 2104)made no discernible difference in performance.We also tested larger models for both our GRU and MLP archi-tectures. On the GRU, we tested from 4 to 12 gated recurrent layers.On our MLP, we tested up to 8 layers, with widths from 32 to 256.In both cases, we found that larger models provided no benefitin either AUROC or accuracy. For example, for the GRU modelFAL-40B with TriviaQA, expanding to 12 recurrent layers yieldedan AUC of 0.46, slightly worse than the 0.48 reported with 4 layersreported in . For MLP, expanding to 8 layers yielded 0.72,only slightly better than the 0.71 we reported with the single layermodel.A systematic study of these hyperparameter choices, and under-standing the effect of more advanced detector architectures (e.g.,Transformers, LSTMs) on the detection performance is a promisingfuture direction. Comparison with SelfCheckGPT. We compare the performanceof our method to SelfCheckGPT . Results are shown in for larger model variants and for smaller model variants.The performance is generally worse than our classifiers. We usetwo variants of SelfCheckGPT: BERTScore and n-gram. We use 20generations per input and set the temperature to 0.1. Preliminaryanalysis in shows that setting the temperature to 1.0 assuggested in the paper does not lead to better performance.",
  "CONCLUSION, DISCUSSION & LIMITATIONS": "We address the problematic behavior where large language mod-els hallucinate incorrect facts. By extracting different generationartifacts such as Integrated Gradients feature attribution scoresand self-attention scores at various Transformer layers we buildsimple classifiers for detecting hallucinations. The detection canalready be performed at the start of the response generation, thatis, factual hallucinations can often be detected even before theyoccur. We show that the ability to identify hallucinations persistsacross different topics like Capitals, Founders and Birth Place; andin a broader trivia knowledge context. Attaching classifiers of thistype to deployed LLMs can serve as an effective method of flaggingpotentially incorrect information before it reaches the end-users ordownstream applications.Surprisingly, the classifiers are able to detect hallucinations evenwhen the first generated token is a seemingly uninformative tokenlike a formatting character. With the exception of Falcon modelson the Birth Place dataset, the newline character is the first tokenin 99.5% of generated responses, with the remaining 0.5% being apunctuation. In the Falcon / Birth Place case, the newline characterstarts 82% of responses, while the word where starts the other 18%.Two factors likely contribute to this surprising effectiveness of suchuninformative tokens. First, while the first token \\n is a singlecharacter, the associated artifacts like hidden states and softmax probabilities contain much more information as these are vectorswith thousands of dimensions. Second, we are experimenting withautoregressive models. At the first generation location, the modelhas ingested all the external information (that is, the input) and theupcoming generation only depends on the internals of the modelitself (minus the effect of the generation strategy which in ourcase is the most likely next token). Nonetheless, this finding meritsfurther investigation and is a promising avenue for future work.We only considered factual hallucinations in this work. It wouldbe interesting to study if the proposed method also extends to otherforms of hallucinations . We also considered a rather simplesetup where the model output is supposed to contains a singlefact only. Extending our method to probe multiple hallucinations(e.g., multiple facts in a biography like place of birth, date of birth,alma mater) in a single generation is also an important follow updirection.Our method cannot be used with blackbox models that donot reveal model internals and gradients to the users. Text-onlymethods like SelfCheckGPT are well-suited for such situations butcompensate for the lack of internal access by querying the modelseveral times. In our case, all except one artifact can be computedfor free during the forward pass. Nonetheless, our method canstill be deployed by the model providers internally. Experimentingwith more elaborate retrieval settings (e.g., those using retrievalaugmented generation , instruction tuned models and in-context examples ), a wider range of datasets and model typesis also a promising avenue for future work.Finally, the goal of this paper was not to extract the maximumpossible detection performance, but to test if certain artifacts canbe promising in detecting hallucinations. For this reason, we userelatively simple detection architectures. We leave a more in-depthanalysis of detection architectures to a future study.The code repository for the paper is available at:",
  "Finale Doshi-Velez and Been Kim. 2017. Towards a rigorous science of inter-pretable machine learning. arXiv preprint arXiv:1702.08608": "Hady Elsahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, JonathonHare, Frederique Laforest, and Elena Simperl. 2018. T-REx: a large scale align-ment of natural language with knowledge base triples. In Proceedings of theEleventh International Conference on Language Resources and Evaluation (LREC2018). European Language Resources Association (ELRA), Miyazaki, Japan,(May 2018). Javier Ferrando, Gerard I. Gllego, Belen Alastruey, Carlos Escolano, andMarta R. Costa-juss. 2022. Towards opening the black box of neural machinetranslation: source and target interpretations of the transformer. In Proceedingsof the 2022 Conference on Empirical Methods in Natural Language Processing.Association for Computational Linguistics, Abu Dhabi, United Arab Emirates,(Dec. 2022), 87568769. doi: 10.18653/v1/2022.emnlp-main.599. Marina Fomicheva, Shuo Sun, Lisa Yankovskaya, Frdric Blain, FranciscoGuzmn, Mark Fishel, Nikolaos Aletras, Vishrav Chaudhary, and Lucia Spe-cia. 2020. Unsupervised quality estimation for neural machine translation.Transactions of the Association for Computational Linguistics, 8, 539555. Leilani H Gilpin, David Bau, Ben Z Yuan, Ayesha Bajwa, Michael Specter, andLalana Kagal. 2018. Explaining explanations: an overview of interpretability ofmachine learning. In 2018 IEEE 5th International Conference on data science andadvanced analytics (DSAA). IEEE, 8089. Nuno M. Guerreiro, Elena Voita, and Andr Martins. 2023. Looking for a needlein a haystack: a comprehensive study of hallucinations in neural machinetranslation. In Proceedings of the 17th Conference of the European Chapter ofthe Association for Computational Linguistics. Association for ComputationalLinguistics, Dubrovnik, Croatia, (May 2023), 10591075. Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, FoscaGiannotti, and Dino Pedreschi. 2018. A survey of methods for explaining blackbox models. ACM computing surveys (CSUR), 51, 5, 142.",
  "Saurav Kadavath et al. 2022. Language models (mostly) know what they know.arXiv preprint arXiv:2207.05221": "Ehsan Kamalloo, Nouha Dziri, Charles Clarke, and Davood Rafiei. 2023. Evalu-ating open-domain question answering in the era of large language models.In Proceedings of the 61st Annual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers). Anna Rogers, Jordan Boyd-Graber, andNaoaki Okazaki, (Eds.) Association for Computational Linguistics, Toronto,Canada, (July 2023), 55915606. doi: 10.18653/v1/2023.acl-long.307.",
  "Long Ouyang et al. 2022. Training language models to follow instructionswith human feedback. Advances in Neural Information Processing Systems, 35,2773027744": "Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Under-standing factuality in abstractive summarization with frank: a benchmark forfactuality metrics. arXiv preprint arXiv:2104.13346. Fabio Petroni, Tim Rocktschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin,Yuxiang Wu, and Alexander Miller. 2019. Language models as knowledge bases?In Proceedings of the 2019 Conference on Empirical Methods in Natural LanguageProcessing and the 9th International Joint Conference on Natural LanguageProcessing (EMNLP-IJCNLP). Association for Computational Linguistics, HongKong, China, (Nov. 2019), 24632473. doi: 10.18653/v1/D19-1250.",
  "Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic attributionfor deep networks. In International conference on machine learning. PMLR,33193328": "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, CarlosGuestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Alpaca: a strong, repli-cable instruction-following model. Stanford Center for Research on FoundationModels. stanford. edu/2023/03/13/alpaca. html, 3, 6, 7. Timm Teubner, Christoph M Flath, Christof Weinhardt, Wil van der Aalst, andOliver Hinz. 2023. Welcome to the era of chatgpt et al. the prospects of largelanguage models. Business & Information Systems Engineering, 65, 2, 95101. Elena Voita, Rico Sennrich, and Ivan Titov. 2021. Analyzing the source andtarget contributions to predictions in neural machine translation. In Proceedingsof the 59th Annual Meeting of the Association for Computational Linguistics andthe 11th International Joint Conference on Natural Language Processing (Volume1: Long Papers). Association for Computational Linguistics, Online, (Aug. 2021),11261140. doi: 10.18653/v1/2021.acl-long.91.",
  "Founders": "0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 Place of Birth 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.000.250.500.751.00 0.0 0.2 0.4 0.6 0.8 1.0"
}