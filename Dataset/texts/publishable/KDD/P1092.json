{
  "ABSTRACT": "Heterogeneous graph learning aims to capture complex relation-ships and diverse relational semantics among entities in a hetero-geneous graph to obtain meaningful representations for nodes andedges. Recent advancements in heterogeneous graph neural net-works (HGNNs) have achieved state-of-the-art performance byconsidering relation heterogeneity and using specialized messagefunctions and aggregation rules. However, existing frameworksfor heterogeneous graph learning have limitations in generalizingacross diverse heterogeneous graph datasets. Most of these frame-works follow the \"pre-train\" and \"fine-tune\" paradigm on the samedataset, which restricts their capacity to adapt to new and unseendata. This raises the question: Can we generalize heterogeneousgraph models to be well-adapted to diverse downstream learningtasks with distribution shifts in both node token sets and relationtype heterogeneity? To tackle those challenges, we propose HiGPT,a general large graph model with Heterogeneous graph instruction-tuning paradigm. Our framework enables learning from arbitraryheterogeneous graphs without the need for any fine-tuning pro-cess from downstream datasets. To handle distribution shifts inheterogeneity, we introduce an in-context heterogeneous graphtokenizer that captures semantic relationships in different heteroge-neous graphs, facilitating model adaptation. We incorporate a largecorpus of heterogeneity-aware graph instructions into our HiGPT,enabling the model to effectively comprehend complex relation het-erogeneity and distinguish between various types of graph tokens.Furthermore, we introduce the Mixture-of-Thought (MoT) instruc-tion augmentation paradigm to mitigate data scarcity by generatingdiverse and informative instructions. Through comprehensive eval-uations conducted in various settings, our proposed frameworkdemonstrates exceptional performance in terms of generalizationperformance, surpassing current leading benchmarks. ACM Reference Format:Jiabin Tang1, Yuhao Yang2, Wei Wei2, Lei Shi3,, Long Xia3, Dawei Yin3 andChao Huang1,2. 2024. HiGPT: Heterogeneous Graph Language Model. InProceedings of ACM Conference (Conference17). ACM, New York, NY, USA,19 pages.",
  "Conference17, July 2017, Washington, DC, USA2024. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00": "that engage in a multitude of interactions, enabling a comprehen-sive representation of complex systems . The focus of hetero-geneous graph learning is to derive meaningful representationsfor the nodes and edges within such graphs . Theserepresentations aim to capture the intricate relationships and di-verse relational semantics that exist within the graph, facilitating adeeper understanding of the underlying structural heterogeneity.In recent years, there has been a growing recognition of the sig-nificant potential of heterogeneous graph neural networks (HGNNs)in capturing the intricate and diverse information that resideswithin heterogeneous graph structures . HGNNs leveragethe expressive capabilities of high-order message passing tech-niques, enabling them to effectively model the complex relation-ships, diverse entity types, and heterogeneous semantics present inthese graphs. By aggregating and propagating information acrossvarious node and edge types, HGNNs facilitate a deeper under-standing and analysis of the intricate inter-dependencies that existwithin heterogeneous graph structures. Some notable examplesof HGNNs include: i) Metapath-based GNNs such as HAN and MAGNN ; ii) Transformer-enhanced GNNs like HGT .Furthermore, heterogeneous graph self-supervised learning, in-cluding contrastive methods (e.g., DMGI and HeCo ), andgenerative (e.g., HGMAE ) methods, showcase effectiveness inalleviating data scarcity in real-world heterogeneous graph data.Despite the demonstrated effectiveness of current frameworksfor heterogeneous graph learning, they possess limitations when itcomes to generalizing across diverse heterogeneous graph datasets.These frameworks commonly adopt the \"pre-train\" and \"fine-tune\"paradigm, where they are initially trained on a specific dataset andsubsequently fine-tuned on the same dataset . However,this approach presents challenges in adapting and achieving optimalperformance on new and unseen data. The heavy reliance on thecharacteristics and patterns of the original training dataset hinderstheir ability to effectively handle the intricacies and complexities in-herent in different heterogeneous graph datasets. As a result, theseframeworks may encounter difficulties in effectively addressing thediverse nuances and variations present in various heterogeneousgraph datasets, particularly when applied to downstream tasks.This research aims to push the boundaries of heterogeneousgraph models by addressing a fundamental question: \"Can we de-velop highly adaptable and versatile heterogeneous graph modelscapable of effectively addressing diverse downstream learning tasks,even when faced with distribution shifts in node token sets andrelation type heterogeneity?\" To tackle this challenge, we introduceHiGPT as a novel and general solution. Our model is specifically de-signed to overcome key challenges associated with generalizationacross various downstream heterogeneous graph learning tasks.",
  "arXiv:2402.16024v2 [cs.CL] 19 May 2024": "C1. Relation Type Heterogeneity Shift. One of the primary chal-lenges we focus on in this research is the shift in relation type het-erogeneity within various heterogeneous graph structures. In suchgraphs, entities are connected by various types of relations, andthese relation types can differ significantly across diverse datasets.To illustrate this, lets consider two examples. In a recommendationsystem, a heterogeneous graph may involve node-wise relationshipsbetween users and items. The relation types in this scenario could in-clude \"click,\" \"favorite,\" \"review,\" and \"purchase.\" On the other hand,in an academic graph, the relations could involve \"paper-paper,\"\"author-paper,\" and \"paper-venue.\" These examples demonstratehow different heterogeneous graphs can exhibit diverse relationheterogeneity with distinct semantics across domains.Solution: In-Context Heterogeneous Graph Tokenizer. Toachieve adaptability in a wide range of heterogeneous graph sce-narios with varying node and edge types, we introduce the in-context heterogeneous graph tokenizer. This tokenizer captures thediverse semantic relationships found in different heterogeneousgraphs, providing a unified approach. It comprises two essentialcomponents: the in-context parameterized heterogeneity projector,which utilizes language to encode distinct node and edge types,facilitating effective model adaptation, and the parameter allocator,which dynamically assigns tailored parameters to the tokenizer. Tooptimize performance and integrate the tokenizer seamlessly intothe HiGPT framework, we employ pre-training with a lightweighttext-graph contrastive alignment paradigm. This pre-training pro-cess directly incorporates the tokenizer into HiGPT, enhancing itscapabilities and ensuring smooth functionality within the overallmodel architecture, including integration with the language model.C2. Complex Heterogeneous Graph Structures. The primaryfocus of this study is to tackle the challenge of integrating largelanguage models (LLMs) into heterogeneous graph learning, withthe goal of enhancing model generalization. Our specific objectiveis to develop a graph-oriented language model that excels in com-prehending the intricate structural information inherent in complexheterogeneous graph structures. In doing so, we strive to equip thegraph model with the ability to not only recognize the heterogene-ity of relations among different types of nodes, but also capture thedistinct characteristics of entities belonging to the same type.Solution: Heterogeneous Graph Instruction-Tuning. We intro-duce a novel heterogeneous graph instruction-tuning frameworkthat integrates inter-type and intra-type token matching tasks tofine-tune large language models (LLMs). Our framework specificallytargets the enhancement of LLMs understanding of both hetero-geneous relation awareness and homogeneous relation awareness.By utilizing these tasks, our aim is to bolster the LLMs capabilitiesin the following areas: (i) distinguishing between different typesof graph tokens, (ii) comprehending intricate relationships withinheterogeneous graphs, (iii) preserving the distinctive attributes ofentities within homogeneous graphs, and (iv) effectively harnessingdiverse graph instructions during the training process.C3. Data Scarcity for Model Fine-Tuning. In practical scenariosinvolving heterogeneous graph learning, one of the key challengesis the limited availability of data . This scarcity of data posesa significant obstacle when fine-tuning models for real-world ap-plications. For instance, when utilizing heterogeneous graphs to model cold-start users or items in recommendation systems, thesparse nature of user interaction data restricts the availability ofsupervised signals . This data scarcity hampers the effec-tiveness of task-specific model fine-tuning and necessitates thedevelopment of novel techniques to overcome this challenge.Solution: Mixture-of-Thought Augmentation. Our approachintroduces a novel mechanism for augmenting graph instructions,emphasizing the use of Mixture-of-Thought (MoT) combined withvarious prompting techniques. This integration enables us to gen-erate a diverse and comprehensive set of informative task-specificinstructions. By seamlessly incorporating these augmented graphinstructions into our framework, we anticipate that our model en-hancement will effectively address the challenge of data sparsity.Experiments. To assess the efficacy of our proposed approach,we perform a comprehensive set of experiments to showcase theimpressive generalization capabilities of our heterogeneous graphlanguage model across diverse settings. We thoroughly investigatethe design rationales, effectiveness, and efficiency of our model.",
  "PRELIMINARIES": "Heterogeneous Graph. A heterogeneous graph is a graph denotedas G(V, E, A, T, R, X). It consists of nodes represented by V, edgesrepresented by E, and an adjacency matrix A that captures therelationships between nodes. The sets T and R signify the typesof nodes and edges, respectively. Additionally, the feature matrixX = { R|V | } contains attributes associated with eachnode. Here, refers to a specific node type, while representsthe dimensionality of the corresponding node attributes.Meta Relation. In a heterogeneous graph, a meta relation is arepresentation of the relationship between different types of nodesconnected by an edge. Specifically, for an edge that links a sourcenode of type to a target node of type , the meta relationof = (, ) is denoted as < (), (),() >. Here, () and ()represent the node types of and respectively, drawn from theset of node types T, while () denotes the relation type of theedge , which is selected from the set of relation types R. This metarelation provides a concise representation of the heterogeneousconnections in the graph, capturing the types of nodes involved.Heterogeneous Graph Neural Networks (HGNNs). In the con-text of a heterogeneous graph G, Heterogeneous Graph NeuralNetworks (HGNNs) employ message passing and aggregation tech-niques to incorporate neighbor information based on different nodeand edge categories. This enables the modeling of heterogeneousstructural semantic relationships, as expressed below:",
  "Propagate(1);(1),(1)": "Here, N () represents all the source nodes connected to node, and E(, ) denotes the set of edges connecting node andnode . In most HGNNs, the parameters of the Propagate () andAggregate () functions depend on the types of nodes and , aswell as the edge . However, this implies that HGNNs are limitedto modeling the specific heterogeneous graph they were trainedon and cannot be effectively applied to new heterogeneous graphs",
  "METHODOLOGY3.1In-Context Heterogeneous Graph Tokenizer": "To make our HiGPT adaptable to a wide range of heterogeneousgraph scenarios with varying node and edge types, we propose anin-context heterogeneous graph tokenizer. This method capturesthe diverse semantic relationships present in different heteroge-neous graphs, ensuring a unified approach. It comprises two es-sential components: the in-context parameterized heterogeneityprojector and the parameter allocator. The adaptive in-context pro-jector utilizes language to encode the distinct node and edge typeswithin the heterogeneous graphs, facilitating model adaptation.Meanwhile, the parameter allocator dynamically assigns param-eters tailored specifically for the tokenizer. To optimize the tok-enizers performance and seamlessly integrate it within the HiGPTframework, we conduct pre-training using a simplified text-graphcontrastive learning framework. This pre-training process directlyincorporates the tokenizer into the HiGPT framework and effec-tively integrates it with the Large Language Model (LLM). This ap-proach enhances the tokenizers capabilities and ensures its smoothfunctioning within the overall model architecture. 3.1.1Graph Tokenization with Meta Projector. Given a het-erogeneous graph G with a feature matrix X = { R|V | , T } and an adjacency matrix A, the goal of the heterogeneousgraph tokenizer is to encode the hidden representations of theheterogeneous graph, denoted as H = { R|V | , T }.This is achieved through the function H = HG-Tokenizer(X, A),where represents the hidden dimension of node type . TheHG-Tokenizer() can be implemented using various backbone HGNNarchitectures, such as HetGNN , HAN , or HGT .However, the generalization capability of these heterogeneousGNNs is constrained by their inherent design, which includespre-defined parameter learning tailored to specific heterogeneousgraphs. As a result, the trained HGNNs cannot be readily appliedto other unseen heterogeneous graphs, which goes against the ob-jective of achieving unified encoding with the HG-Tokenizer. Toillustrate, lets consider HGT as an example. In HGT, the calcula-tion of ()involves utilizing functions such as Attention() andMessage() to process information from the source nodes:",
  "(1)W ()2(3)": "Adaptive Parameterized Heterogeneity Projector. To makeour HiGPT adaptable to a wide range of heterogeneous graphswith varying graph heterogeneity settings, and to eliminate therequirement of pre-defining the number of type-specific projectionsin advance, we propose the design of a type-aware parameterizedprojector. This projector dynamically and automatically encodes therelation heterogeneity into latent representations. More specifically,the type-aware projectors with the parameters F ()and W ()are generated automatically according to the following procedure:",
  "(4)": "F is a fully-connected layer with parameters , while T () andT () are the features associated with node type () and edge type(), respectively. It is important to note that the example providedshowcases the usage of the in-context parameterized heterogeneityprojector within the heterogeneous graph transformer framework.However, our HiGPT is designed to be versatile and adaptable,allowing for the integration of diverse heterogeneous GNNs.Language-Enriched Heterogeneity Representation. We lever-age natural language as a means to generate universal heterogeneityrepresentations for nodes and edges based on their respective types.For instance, in the heterogeneous IMDB dataset, we can describea \"movie\" node as \"This node represents a movie\" using naturallanguage. Similarly, the edge (\"movie\", \"to\", \"director\") can be ex-pressed as \"The movie is directed by the director\". To encode thesenatural language descriptions of nodes and edges, we employ apre-trained language model such as Sentence-BERT to obtaintype representations. To ensure distinguishability and diversityamong different types, we utilize multiple languages to describethe same type. The encoded representations from the pre-trainedlanguage models are averaged to derive the final representation.This process can be defined as follows:",
  ": The overall architecture of our HiGPT": "3.1.2Lightweight Text-Graph Contrastive Alignment. Build-ing upon recent advancements in aligning cross-modality seman-tics , we draw inspiration to employ a text-graph contrastivealignment paradigm for pre-training the proposed heterogeneousgraph tokenizer. This approach aims to align the modeling capa-bilities of language and heterogeneous structures, enabling bet-ter collaboration between the tokenizer and the language mod-els. To begin, we consider raw textual contents represented asC = R , 1 , where denotes the total number ofheterogeneous graph nodes X = R|V | . Here, repre-sents the length of the textual content associated with the -th node.In our approach, we adopt a lightweight text-graph contrastivealignment paradigm formally presented as follows:",
  "Heterogeneous Graph Instruction Tuning": "The objective of HiGPT is to empower language models to directlygenerate predictions for downstream tasks with the unseen het-erogeneous graph and corresponding instructions. The naturallanguage instruction is first encoded by a tokenizer into text em-beddings, denoted as XI = LM-tokenizer(instruction). To alignthe dimensions, we employ a projector that maps graph tokens tothe same dimension as the text embeddings, given by XG = P(H),which can be as simple as a linear layer. For a sequence of length ,we determine the probability of generating the target output XO:",
  "where represents the learnable parameters within HiGPT": "3.2.1Instruction Tuning with Heterogeneous Graph Corpus.To enable the Language Model (LLM) to effectively differentiate be-tween different types of input heterogeneous graph tokens and thespecific nodes within each type, based on natural language instruc-tions, we propose instruction pre-training using a large \"corpus\" consisting of heterogeneous graph-instruction pairs. This approachequips the fine-tuned HiGPT with a comprehensive understandingof both homogeneous and heterogeneous graph structures. Heterogeneous Relation Awareness. Our objective is to en-hance the language models proficiency in distinguishing be-tween specific types of nodes within a heterogeneous context,taking into account the intricate relationships. This is achievedby leveraging the information encoded in the graph tokens. Homogeneous Relation Awareness. Our aim is to equip themodel with the ability to establish a significant correspondencebetween sequences of graph tokens that belong to the samecategory and their corresponding natural language descriptions.Heterogeneous Graph Instruction. In our graph instruction, weincorporate a heterogeneous subgraph generated through randomneighbor sampling, accompanied by a question generated by ahuman. To enhance the diversity of the heterogeneous graph corpus,we conduct multiple samplings focusing on nodes from differenttypes. Additionally, we introduce the <graph> token as a graphindicator within the human question. i) To achieve heterogeneousrelation awareness, we introduce the inter-type token matchingtask. This task involves providing the Language Model (LLM) withencoded sequences of graph tokens from different types, enablingit to differentiate between the various types. ii) For homogeneousrelation awareness, we design the intra-type matching task, wherethe LLM receives encoded sequences of graph tokens from a specifictype, allowing it to establish correspondence with the relevantdescriptions. Further details regarding the instruction template atthis stage are illustrated in and Appendix Section A.4. 3.2.2Heterogeneity-aware Fine-Tuning. To customize the rea-soning abilities of the language model for specific downstreamtasks on heterogeneous graphs, we propose Heterogeneity-awareFine-Tuning. This approach entails conducting supervised learningwith task-specific instructions following the initial instruction pre-training phase with heterogeneous graph corpus. It allows us torefine the LLMs performance and adapt it to the specific require-ments of the targeted tasks on heterogeneous graphs.In this stage, we incorporate a randomly sampled heterogeneoussubgraph centered around the target node, along with a human-generated question. Given that the previous phase of instructionpre-training has already equipped the model with heterogeneous",
  "(b) Homogenous Instruction Pre-training": "central_nodes: (\"paper\": [1, ...,n]), num_neighbors: Given a heterogeneous graph about papers, there are 4 types of nodes: <DESC>. ..., a heteroge-neous subgraph is obtained. The nodes for \"paper\" are: <graph>. Also, a list of textual descriptionsfor the papers are: <DESC>. Please reorder the text list based on the order of graph tokens.",
  "(c) Heterogeneous Supervised Fine-Tuning": "central_nodes: (\"movie\": [i]),num_neighbors: Given a heterogeneous graph about movies, there are 3 types of nodes: <DESC>. ..., a heteroge-neous subgraph is obtained. There are nodes of different types: \"movie\" nodes: <graph>, <DESC>where the 0-th node is the central node. \"actor\" nodes: <graph>; \"director\" nodes: <graph>. Whichof the following classes does this movie belong to: action, comedy, drama?",
  ": Mixture-of-Thought (MoT) Augmentation": "and homogeneous relation awareness, we design human questionsthat are rich in heterogeneity. These questions contain sequencesof graph tokens from different types, indicated by multiple occur-rences of the <graph> token. Additionally, the human questionincludes pertinent auxiliary information pertaining to the targetnode. The designs of these instructions are presented in .3.3Mixture-of-Thought (MoT) for GraphInstruction Augmentation In practical scenarios of heterogeneous graph learning, data scarcityoften poses a challenge. This is especially true when using hetero-geneous graphs to model cold-start users/items in recommendationsystems, where sparse user interaction data limits the availabil-ity of supervised signals. To address the issue of data sparsity, wepropose enhancing our HiGPT by incorporating augmented graphinstructions. Drawing inspiration from previous works , we in-troduce a novel method for instruction augmentation in the contextof heterogeneous graph instruction tuning. This method utilizesprompt engineering techniques, particularly Mixture-of-Thought(MoT), to generate diverse and informative instructions. The goalis to effectively overcome the challenges posed by data scarcity. Byincorporating augmented graph instructions, we expect our modelenhancement to effectively handle data sparsity.",
  "Mixture-of-Thought (MoT) Prompting. Our focus is ondesigning and optimizing prompts to effectively utilize languagemodels . We employ several techniques to enhance": "language models: i) Chain-of-Thought (CoT) : CoT promptsintroduce intermediate steps, enabling complex reasoning and so-phisticated capabilities. ii) Tree-of-Thought (ToT) : ToT main-tains a tree structure of coherent language sequences called thoughts.These thoughts serve as systematic intermediate steps for problem-solving. iii) PanelGPT : PanelGPT incorporates panel discus-sions among language models, enhancing the prompt engineer-ing process through collaboration. iv) Generated KnowledgePrompting (GKP) : GKP involves incorporating additionalknowledge into prompts for enhancement. By leveraging thesetechniques, our objective is to augment heterogeneous graph in-structions, especially in scenarios with limited data availability. 3.3.2Instruction Augmentation with Priori Knowledge. Weutilize seven instruction augmentation strategies, each generatingseven augmented instructions for every question-answer pair, incor-porating the characteristics of Mixture-of-Thought (MoT). However,closed-source language models such as ChatGPT may produce in-correct answers, resulting in flawed guidance. To overcome thisissue, we propose incorporating prior knowledge, specifically thecorrect answer, into the prompt. It enables LLM to simulate gener-ating the correct answer and produce intermediate reasoning stepsusing different MoT methods, as shown in and Appendix.",
  "Experimental Settings": "4.1.1Experimental Datasets. The experiments were conductedon three benchmark datasets, i.e., IMDB , DBLP , and ACM .IMDB is an extensive dataset that focuses on online movies andtelevision programs. It encompasses 4278 movies, 2081 directors, and 5257 actors. Each movie is categorized into one of three classes:Action, Comedy, or Drama. DBLP, on the other hand, consists of4057 authors, 14328 papers, 7723 terms, and 20 publication venues.This dataset was gathered from a computer science bibliographywebsite and the authors are distributed among four research ar-eas: Database, Data Mining, Artificial Intelligence, and InformationRetrieval. Lastly, the ACM dataset comprises 3025 papers, 5835authors, and 56 subjects. The papers are classified into three classes:Database, Wireless Communication, and Data Mining.4.1.2Evaluation Protocols. To ensure consistency in the featuredimension of nodes across all datasets, we utilize a pre-trainedSentence-BERT to encode nodes of all types from each datasetinto a standardized dimension. For the supervised few-shot nodeclassification, we randomly select 1, 3, 5, 10, 20, 40, or 60 labelednodes per class as our training set. Additionally, we reserve 1,000nodes for validation and another 1,000 nodes for testing purposes.Our evaluation metrics encompass Micro-F1, Macro-F1, and AUC.4.1.3Compared Baseline Methods. For our comprehensive per-formance comparison, we evaluate various state-of-the-art methodsfrom three different categories: i) The first category consists of repre-sentative homogeneous graph neural networks, including SAGE and GAT . ii) The second category includes approaches uti-lizing message-passing mechanisms in heterogeneous graph neu-ral networks. This category features models such as HAN ,HGT , and HetGNN . iii) The third category focuses onself-supervised techniques for heterogeneous graph learning. Thiscategory incorporates generative strategies like HGMAE , aswell as contrastive schemes such as DMGI and HeCo .4.1.4Implementation Details. In Appendix Sec A.5.1, we offercomprehensive descriptions of the implementation details, includ-ing the datasets used, training hyperparameters, configurations ofthe base LLM, and more. These details provide a deeper understand-ing of our implementation approach.",
  "Overall Performance Comparison (RQ1)": "We performed node classification tasks on three datasets, explor-ing both few-shot and zero-shot settings. In the few-shot settings,our model was trained on the IMDB dataset with shot numbersranging from 1 to 60, and evaluated on the IMDB test set of 1,000samples . For the zero-shot settings, the model was trainedon the IMDB dataset with the same shot numbers, and tested onseparate test sets from the DBLP and ACM datasets, each containing1,000 samples. To enable cross-dataset transferability in supervisedheterogeneous Graph Neural Networks (GNNs), we unified nodeand edge categories, and utilized a classifier trained with transferdata to accommodate variations in class quantities across datasets.For self-supervised methods focused on learning embeddingsfor downstream heterogeneous graph nodes, we excluded the zero-shot settings. The overall performance is partially shown in ,with detailed results in in the appendix. \"-std\" and \"-cot\"notations represent the standard test prompt with direct answersand the prompt with a Chain-of-Thought (CoT) feature, respec-tively. These details provide insights into our node classificationexperiments in both supervised and zero-shot settings.Superiority of HiGPT in Few-Shot Settings. HiGPT outper-forms state-of-the-art baselines consistently in supervised settings, even with only one sample per class. The success can be attributedto our effective instruction-tuning on a large-scale heterogeneousgraph corpus. This enables the LLM to extract valuable and trans-ferable heterogeneous structural information from graph tokens,resulting in a significant performance boost in downstream tasks.Additionally, our proposed MoT graph instruction augmentationmethod enhances the LLM with diverse mixed reasoning capabili-ties without the need for additional supervision signals. As a result,it effectively mitigates the data scarcity in few-shot scenarios.Zero-shot Superiority of HiGPT. In zero-shot settings, our HiGPTsurpasses baselines with significant improvements. Unlike tradi-tional models constrained by training graph types, our approachleverages an in-context heterogeneous graph tokenizer. This tok-enizer adapts tokenization based on the input graph, allowing theLLM to seamlessly combine graph tokens that capture higher-orderstructural features with its semantic understanding. As a result, ourmodel effectively overcomes the limitation of graph heterogeneityshift, performing exceptionally well even in cross-domain scenarios.This showcases the remarkable adaptability of our HiGPT.Effectiveness of Mixture-of-Thought Augmentation. Throughthe implementation of the MoT approach, our model harnesses thevaried reasoning capabilities of the formidable LLM (specifically,GPT-3.5) and seamlessly integrates them into our more compactlanguage model. This integration serves to bolster our modelsability to effectively navigate data scarcity and elevate its perfor-mance in situations characterized by limited supervised signals.The MoT technique assumes a pivotal role in generating dynamicand diverse instructions, thereby offsetting the dearth of data andempowering our model to make notably precise predictions acrossboth supervised and zero-shot settings.",
  "Model Ablation Test (RQ2)": "To evaluate the proposed modules effectiveness, we individuallyremove the key techniques in HiGPT. The results are summarizedin . Here are the ablated variants and the key conclusions: Effect of Heterogeneous Graph Instruction-Tuning. To vali-date the effectiveness of instruction tuning in the tuning stage onthe large heterogeneous graph corpus, we generated the \"w/o S1\"variant by directly tuning the instructions solely on the down-stream task data. Through experiments with different epoch set-tings (15, 50, and 100), we observed that models tuned solely onthe downstream task data failed to provide complete and accurateanswers in all cases. However, our HiGPT achieved state-of-the-art performance within just 15 epochs across all settings. Thissuccess can be attributed to the fact that our HiGPT learns froma vast heterogeneous graph context corpus, enabling it to under-stand and extract crucial structural information. As a result, inthe second stage, our HiGPT requires only a minimal amount ofsupervised data (even in a 1-shot scenario) to quickly align withthe downstream task. Conversely, directly aligning LLMs withsparse supervised data proves to be challenging. Effect of In-Context Heterogeneous Graph Tokenizer. Wetested the necessity of incorporating heterogeneous graph struc-tural information from our in-context tokenizer. By excludingthe introduction of heterogeneous graph tokens and solely train-ing the LLMs embeddings weights on the downstream data, : Performance comparison on node classification tasks in both few-shot and zero-shot settings. However, since SSLmethods focus on learning embeddings from downstream graphs, we excluded the zero-shot settings for them (\"-\").",
  "IMDB-20DBLP-1000fail0.58340.49950.5907": "we obtained a variant called \"w/o HG\". Our HiGPT consistentlyoutperformed this variant across different shot settings, espe-cially in scenarios with limited samples (e.g., 1 or 3 shots). Thisimprovement is attributed to the introduction of graph tokens,which enable the LLM to extract high-dimensional heterogeneousstructural information from the in-context graph tokenizer. Thisenhanced understanding significantly improves the LLMs accu-racy, particularly with sparse supervised signals. Effect of MoT Instruction Augmentation. To verify the effec-tiveness of the MoT graph instruction augmentation strategy, wetrained the variant \"-IA\" using only direct-answer instructions.Results showed a significant drop in model performance without instruction augmentation, highlighting its importance in tacklingthe scarcity of labels in downstream tasks. Additionally, HiGPTssuperior performance in zero-shot settings can be attributed toits enhanced reasoning ability, acquired through training withdiverse reasoning instructions. This improved capacity enableseffective cross-dataset and cross-domain transfer.",
  "Graph In-Context Learning (RQ3)": "In-context learning (ICL) is a method for adapting large lan-guage models (LLMs) to new tasks without gradient updates, usinga prompt with task examples. In this subsection, we explore theimpact of Graph In-Context Learning on improving HiGPTs per-formance. We conduct comprehensive tests by adding prefatoryexamples from the training set to models trained with differentshots of IMDB data. We randomly sampled training examples cor-responding to the test data. \"-ICL-1\" and \"-ICL-2\" denote one andtwo prefatory examples, respectively. \"-ICL-DBLP\" signifies the in-clusion of DBLP examples before the ACM test prompt. The results,depicted in , reveal the following observations:1-shot Beat 60-shot with Graph ICL in HiGPT. Results showthat, even with just a single example, most 1-shot models usingGraph ICL consistently outperform 60-shot models without furthertraining in both supervised and zero-shot settings. Increasing thenumber of examples enhances the effect of in-context learning. Thisimprovement can be attributed to HiGPTs two-stage instructiontuning process, which enables it to understand and analyze hetero-geneous graph tokens, benefiting downstream tasks. By providingquestion-and-answer examples with graph tokens, the model gainsa deeper understanding of the graph-text relationship. Analyzingand emulating these examples leads to more accurate responses. Number of Training Shots on IMDB 50% 55% 60% 65% 70% Testing Mi-F1 on IMDB 0.6550.6530.646 0.6540.655 0.6640.658 OursOurs-ICL-1Ours-ICL-2 Number of Training Shots on IMDB 50% 55% 60% 65% 70% Testing Ma-F1 on IMDB 0.6500.6480.641 0.6490.653 0.6610.654 OursOurs-ICL-1Ours-ICL-2",
  ": Graph In-Context Learning of our HiGPT": "Enhanced Transferability with our Graph ICL. The advantagesof our Graph ICL in HiGPT are particularly evident in zero-shottransfer scenarios. This indicates that the Graph ICL approachsignificantly improves HiGPTs transferability without the need tooptimize model parameters. Our HiGPT does not simply overfit toa single dataset but develops the ability to analyze text alongsideheterogeneous graph tokens. By incorporating graph examplesfrom other datasets, the model effortlessly transfers this analyticalcapability, demonstrating strong transfer learning capacity.Benefit of Irrelevant Graph Examples. We experimented withHiGPT with irrelevant graph examples, like using DBLP Q&A ex-amples for testing on the ACM dataset. Surprisingly, using DBLPgraph examples yielded the best results. Despite different targettasks, our HiGPT effectively leverages in-context information fromheterogeneous graph tokens, enhancing downstream tasks. Thisconfirms that our HiGPT learns valuable information from hetero-geneous graph structures, rather than relying solely on text. UsingACMs own examples did not perform as well due to a deficiencyin encoding the ACM graph in the alignment and stage 1 process.However, the DBLP examples mitigated this issue to some extent.",
  "Case Study": "We perform a case study to showcase our HiGPTs robust gener-alization in understanding complex graph structures with diversenodes and connections. Our model generates graph-aware predic-tions and responses, demonstrating its profound comprehensionand awareness of graph-related aspects. Furthermore, we validatethe positive impact of our MoT instruction augmentation. For morecomprehensive details, please refer to Appendix Section A.5.2.",
  "RELATED WORK": "Heterogeneous Graph Neural Networks. Heterogeneous GraphNeural Networks (HGNNs) capture complex relationships and di-verse semantics among entities in a heterogeneous graph .They use specialized message functions and aggregation rules tomodel relation heterogeneity. Existing models, such as MAGNN and HetGNN , leverage metapaths to capture compositerelations and guide neighbor selection. Heterogeneous graph con-volution frameworks like HeteGCN draws inspiration fromgraph convolutional networks. Heterogeneous graph attention net-works, including HAN , HGT , and HGAT , use attentionmechanisms to effectively capture and aggregate information fromdifferent node types. However, most HGNNs require sufficient la-bels to learn accurate graph representations.Heterogeneous Graph Self-Supervised Learning. Recent re-search has addressed the limited availability of labeled data by in-corporating self-supervised learning techniques into heterogeneousgraph modeling . Contrastive and generative approacheshave proven effective in augmenting data. Contrastive learningmethods like DMGI and HeCo bring similar instancescloser and push dissimilar instances apart in a latent space, captur-ing relevant patterns and structure. Generative learning approachessuch as HGMAE use masked autoencoders to reconstruct het-erogeneous graphs. However, these approaches still have limitationsin handling relation heterogeneity shift across downstream tasks,potentially leading to poor generalization ability.Large Language Models for Graph Data. Recent research hascombined large language models (LLMs) and graph models to un-derstand complex relationships in graph data . Twoprimary approaches integrate graph structural information: utiliz-ing textual prompts and incorporating graph embeddings as inputtokens. Chen et al. craft tailored prompts for graph learningtasks, while InstructGLM and GraphGPT propose to inte-grate prompt instructions with graph embeddings for fine-tuningLLM. Moreover, advancements have introduced LLMs to improvethe reasoning capabilities of models when working with graph-structured data. Prominent examples include the works of Fatemi etal. and Chai et al. . However, existing LLM-enhanced graphmodels have primarily focused on homogeneous graphs, overlook-ing the inherent heterogeneity in real-world graphs. This calls forfurther exploration of heterogeneous graph language models withstrong generalization abilities across diverse downstream tasks.",
  "CONCLUSION": "This work introduce HiGPT, a general and versatile graph modelthat offers the ability to learn from diverse heterogeneous graphswithout the need for downstream fine-tuning processes. To ad-dress distribution shifts in heterogeneity, we propose an in-contextheterogeneous graph tokenizer that captures semantic relation-ships across different heterogeneous graphs, facilitating seamlessmodel adaptation. By incorporating the heterogeneity-aware graphinstructions into our HiGPT, the model becomes proficient in com-prehending intricate relation heterogeneity and accurately discern-ing between various types of graph tokens. Our proposed frame-work has undergone extensive evaluations across diverse scenarios,demonstrating outstanding generalization performance.",
  "H. Linmei, T. Yang, C. Shi, H. Ji, and X. Li. Heterogeneous graph attentionnetworks for semi-supervised short text classification. In EMNLP, pages 48214830, 2019": "J. Liu, A. Liu, X. Lu, S. Welleck, P. West, R. L. Bras, Y. Choi, and H. Hajishirzi.Generated knowledge prompting for commonsense reasoning. In ACL (1), pages31543169. Association for Computational Linguistics, 2022. Q. Lv, M. Ding, Q. Liu, Y. Chen, W. Feng, S. He, C. Zhou, J. Jiang, Y. Dong, andJ. Tang. Are we really making much progress? revisiting, benchmarking andrefining heterogeneous graph neural networks. In KDD, pages 11501160, 2021. A. Ma, X. Wang, J. Li, C. Wang, T. Xiao, Y. Liu, H. Cheng, J. Wang, Y. Li, Y. Chang,et al. Single-cell biological network inference using a heterogeneous graphtransformer. Nature Communications, 14(1):964, 2023.",
  "AAPPENDIX": "In the supplementary materials, we provide detailed informationpertaining to our experiments. First, in Section A.1, we present thestatistical information of the datasets used. Next, in Section A.2, weprovide a comprehensive description of the baselines employed. InSection A.3, we outline the specific descriptions of nodes and edgesin our text-enriched heterogeneity representations. Section A.4 elab-orates on the templates for instructions and prompts used in ourHiGPT, including a detailed explanation of MoT graph instructionaugmentation, an instruction construction template for two-stageinstruction tuning, and an instruction construction template forgraph in-context learning. Additionally, Section A.5 presents addi-tional experimental results, covering implementation details of ourmodel, model case studies, overall performance, and comprehensiveresults of graph in-context learning.",
  "A.2Detailed Descriptions of Baselines": "To conduct a thorough evaluation, our baseline set includes thefollowing methods, which are presented below:1) Homogeneous Graph Neural Networks SAGE :It was developed to facilitate the learning of inductiverepresentations on large-scale homogeneous graphs, allowing forthe generation of highly effective node embeddings for unseendata. However, we made adaptations and modifications to tailorit specifically to the unique demands of heterogeneous graphs. GAT : It utilizes graph attention networks, which overcomethe limitations of graph convolutional networks (GCN) by in-corporating masked self-attention layers. What sets this methodapart is its ability to selectively weigh the aggregated informationfrom different nodes, thereby enhancing the message passingmechanism and refining the overall process. 2) Heterogeneous Graph Neural Encoders. HAN : It is a heterogeneous graph neural encoder that incor-porates hierarchical attention mechanisms, including node-leveland semantic-level attentions, to address the complexity of het-erogeneous graphs with various types of nodes and links, thereby",
  "improving the representation and interpretability of node embed-dings through feature aggregation based on meta-path neighbors": "HGT : HGT is an advanced graph neural network frameworkdesigned to model the complexities of large-scale heterogeneousgraphs, featuring type-dependent parameters for nodes and edgesto enable heterogeneous attention, a relative temporal encodingto capture dynamic relationships, and an efficient graph samplingalgorithm for scalable training. HetGNN : HetGNN is a powerful heterogeneous graph neu-ral network model that seamlessly integrates both the structuralinformation and diverse content attributes of nodes. It achievesthis by employing a two-module architecture for feature aggre-gation and incorporating a well-designed random walk samplingprocess. This comprehensive approach enables HetGNN to gen-erate meaningful and informative node embeddings. 3) SSL-enhanced Heterogeneous Graph Learning Approaches. HGMAE : HGMAE is a generative SSL approach that ad-dresses the challenges of capturing complex structures, incor-porating diverse node attributes, and encoding node positionsin heterogeneous graphs. It achieves this through innovativemasking techniques and tailored training strategies, utilizing aheterogeneous graph masked autoencoder. HGMAE efficientlylearns to generate meaningful representations while effectivelypreserving the rich information present in heterogeneous graphs. DMGI : It is an effective unsupervised network embeddingmethod for attributed multiplex networks that maximizes mutualinformation between graph patches and a global representation,integrating multiple relation-type embeddings with a consensusregularization framework and a universal discriminator, furtherenhanced by an attention mechanism to weigh relation types. HeCo : This is a self-supervised heterogeneous graph neu-ral network framework that employs a co-contrastive learningmechanism across two views (network schema and meta-path) tocapture both local and high-order structures, with a view maskmechanism for effective cross-view supervision, enhanced byextensions for generating higher-quality negative samples.",
  "A.4Instruction and Prompting Templates": "A.4.1Detailed prompt of MoT graph instruction augmenta-tion. In , we display all MoT graph instruction augmentationstrategies that incorporate various prompt engineering techniques,including the prompting template to prompt GPT-3.5, and the tem-plate for constructing Instructions after obtaining the results. A.4.2Instruction construction template for two stage in-struction tuning. In , we showcase the instruction tem-plate for the second stage of node classification for IMDB, wherethe prompting suffix allows our instruction to be combined witha variety of different prompt techniques for instruction tuning,thereby distilling multiple reasoning abilities for the powerful GPT-3.5. presents two types of instruction templates for the first",
  "(a) IMDB": "\"Movie\"{\"This node represents a movie\",\"This is an action movie\",\"This is a comedy movie\",\"This is a drama movie\"}\"Director\"{\"This node represents a director\", \"This is an action film director\", \"The director specializes in action\", \"This is a comedyfilm director\", \"The director specializes in comedy\", \"This is a drama film director\", \"The director specializes in dram\"}\"Actor\"{\"This node represents an actor\", \"This is an action film actor\", \"The actor specializes in action\", \"This is a comedy filmactor\", \"The actor specializes in comedy\", \"This is a drama film actor\", \"The actor specializes in drama\"}(\"movie\", \"to\", \"director\"){\"The movie is directed by the director\", \"The film features direction by the director\", \"The movies direction was in thehands of the director\", \"The movie was helmed by the director\", \"The film is a directorial effort by the director\", \"Themovie bears the directorial signature of the director\"}(\"movie\", \"to\", \"actor\"){\"The movie has the actor\", \"The movie features the actor\", \"The film includes the actor in its lineup\", \"The movieshowcases the talent of the actor\", \"The films cast includes the actor\", \"The movie presents the actor on its roster\"}(\"director\", \"to\", \"movie\"){\"This director is responsible for the films direction\", \"The director take the helm for the movie\", \"The director steersthe production of the movie\", \"The director in question crafts the narrative of the film\", \"The director provides thecreative direction for the film\", \"The director orchestrates the making of the movie\"}(\"actor\", \"to\", \"movie\"){\"The actor appears in the movie\", \"The actor is part of the movies cast\", \"The actor stars in the movie\", \"The actor isfeatured in the film\", \"The actor has a role in the movie\"}",
  "(b) DBLP": "\"paper\"{\"This node represents a paper\", \"A paper in the area of Database\", \"A paper in the area of Data Mining\", \"A paper inthe area of AI\", \"A paper in the area of Information Retrieval\", \"A paper published in a conference\", \"A paper in the areaof computer science\"}\"conference\"{\"This node represents a conference\", \"A conference in the area of Database\", \"A conference in the area of Data Mining\",\"A conference in the area of AI\", \"A conference in the area of Information Retrieval\", \"A conference about computerscience\"}\"author\"{\"This node represents an author\", \"An author in the area of Database\", \"An author in the area of Data Mining\", \"Anauthor in the area of AI\", \"An author in the area of Information Retrieval\", \"An author in the area of computer science\"}\"term\"{\"This node represents a key term related to a paper\", \"The term is included in a paper\", \"The term is related to Database\",\"The term is related to Data Mining\", \"The term is related to AI\", \"The term is related to Information Retrieval\"}(\"author\", \"to\", \"paper\"){\"The author has the paper\", \"The author publishes the paper\", \"The author writes the paper\", \"The author is the authorof the paper\", \"The author releases the paper\", \"The author issues the paper\", \"The author disseminates the paper\"}(\"paper\", \"to\", \"author\"){\"The paper has the author\", \"The paper is published by the author\", \"The paper is written by the author\", \"The paperlists the author\", \"The paper is put forth by the author\", \"The paper is made public by the author\"}(\"paper\", \"to\", \"term\"){\"The paper has the term\", \"The paper includes the term\", \"The paper contains the term\", \"The paper encompasses theterm\", \"The paper lists the term\"}(\"paper\",\"to\",\"confer-ence\"){\"The paper is published in the conference\", \"The paper is accepted by the conference\", \"The paper is included in theproceedings of the conference\", \"The paper is presented at the conference\", \"The paper appears in the conferenceproceedings\", \"The paper is part of the conferences official record.\", \"The paper makes its debut at the conference\",\"The paper is documented in the conferences scholarly collection\"}(\"term\", \"to\", \"paper\"){\"The term is related to the paper\", \"The term is included in the paper\", \"The term is in the paper\", \"The term is featuredin the paper\", \"The term is incorporated into the paper\", \"The term is part of the papers content\", \"The term is foundwithin the paper\", \"The term appears in the paper\"}(\"conference\", \"to\", \"pa-per\"){\"The conference has the paper\", \"The conference includes the paper\", \"The conference accepts this paper\", \"Theproceedings of the conference includes the paper\", \"The conference features the publication of the paper\", \"Theconference includes the paper in its publications\", \"The conference serves as the platform for the papers publication\"} IMDB-ICL-1:Q: Given a heterogeneous graph about internet movie {Human Question}A: {Ground Truth Answer&Reasoning}Q: Given a heterogeneous graph about internet movie {Human Question}IMDB-ICL-2:Q: Given a heterogeneous graph about internet movie {Human Question}A: {Ground Truth Answer&Reasoning}Q: Given a heterogeneous graph about internet movie {Human Question}A: {Ground Truth Answer&Reasoning}Q: Given a heterogeneous graph about internet movie {Human Question}ACM-ICL-DBLP:Q: Given a heterogeneous academic network graph about computer science from DBLP website {Human Question}A: {Ground Truth Answer&Reasoning}Q: Given a heterogeneous academic network graph about computer science collected from ACM website {Human Question}",
  "(c) ACM": "\"paper\"{\"This node represents a paper\", \"A paper in the area of Database\", \"A paper in the area of Wireless Communication\", \"Apaper in the area of Data Mining\", \"A paper published in a conference (one of KDD, SIGMOD, SIGCOMM, MobiCOMM,and VLDB)\", \"A paper in the area of computer science\"}\"subject\"{\"This node represents a subject\", \"The subject is related to Database\", \"The subject is related to Wireless Communication\",\"The subject is related to Data Mining\", \"The subject is related to computer science\"}\"author\"{\"This node represents an author\", \"An author in the area of Database\", \"An author in the area of Data Mining\", \"Anauthor in the area of AI\", \"An author in the area of Information Retrieval\", \"An author in the area of computer science\"}\"term\"{\"This node represents a key term related to a paper\", \"The term is included in a paper\", \"The term is related to Database\",\"The term is related to Wireless Communication\", \"The term is related to Data Mining\", \"The term is related to computerscience\"}(\"author\", \"to\", \"paper\"){\"The author has the paper\", \"The author publishes the paper\", \"The author writes the paper\", \"The author is the authorof the paper\", \"The author releases the paper\", \"The author issues the paper\", \"The author disseminates the paper\"}(\"paper\", \"to\", \"author\"){\"The paper has the author\", \"The paper is published by the author\", \"The paper is written by the author\", \"The paperlists the author\", \"The paper is put forth by the author\", \"The paper is made public by the author\"}(\"paper\", \"to\", \"term\"){\"The paper has the term\", \"The paper includes the term\", \"The paper contains the term\", \"The paper encompasses theterm\", \"The paper lists the term\"}(\"term\", \"to\", \"paper\"){\"The term is related to the paper\", \"The term is included in the paper\", \"The term is in the paper\", \"The term is featuredin the paper\", \"The term is incorporated into the paper\", \"The term is part of the papers content\", \"The term is foundwithin the paper\", \"The term appears in the paper\"}(\"paper\", \"to\", \"subject\"){\"The paper pertains to the subject\", \"The paper is concerned with the subject\", \"The paper addresses this subjectsmatter\", \"The paper contributes to the discourse on the subject\", \"The paper explores the subject in depth\", \"The paperexamines the subject\", \"The paper is dedicated to the analysis of the subject\", \"The papers content is relevant to thesubject\", \"The paper provides insights into the subject\", \"The paper discusses the subject comprehensively\"}(\"subject\", \"to\", \"paper\"){\"The subject serves as the focus for the paper\", \"The subject is the central theme of the paper\", \"The subject formsthe basis of the papers inquiry\", \"The subject underpins the scholarly work presented in the paper\", \"The subjectinforms the papers research focus\", \"The subject delineates the scope of the papers investigation\", \"The subject is thecornerstone of the papers theoretical foundation\"}(\"paper\", \"cite\", \"paper\"){\"The paper cites the paper\", \"The paper includes references to a previous paper\", \"The manuscript references earlierresearch in its bibliography\", \"The study attributes findings to an antecedent scholarly paper\", \"The paper assimilatesinsights from a previously published study\", }(\"paper\", \"ref\", \"paper\"){\"The paper is cited by the paper\", \"The paper receives a citation from a subsequent publication\", \"The article isreferenced within the bibliography of another scholarly work\", \"The manuscript is acknowledged by another study inits references\", \"The document is listed in the citations of another academic article\"} with one example, \"-ICL-2\" with two examples, and \"-ICL-DBLP\"where examples from ACM are concatenated with DBLP exam-ples. We represent the examples and the final question using aQ:...A:...Q:... sequence.",
  "A.5Supplementary Experimental Results": "A.5.1Implementation Details. In the implementation of ourHiGPT, we employ heterogeneous graph data with correspondingtextual contents from IMDB and DBLP to conduct text-graph con-trastive alignment and obtain a heterogeneous graph tokenizer. Inthe first phase, we utilize a heterogeneous graph corpus consistingof IMDB and DBLP for instruction tuning. We set the batch size to1 per GPU and train for 1 epoch with the learning rate 25, thewarmup ratio 32 and the weight decay 14. The projector ob-tained from the first phase training is used as the initial state for thesecond phase, where we set the epochs to 15 and further performinstruction tuning on downstream tasks. The base model used inboth stages is vicuna-7B-v1.5, with the maximum context length setto 2048. And For the evaluation of most baselines, we utilize their",
  "publicly available code. We employ a grid-search strategy basedon default hyperparameter settings to ensure a fair evaluation. Forfurther details, please refer to our released source code": "A.5.2Model Case Study. In this subsection, we explore the be-havior of our HiGPT under different prompting techniques. Specifi-cally, we utilize various prompting techniques to prompt the 10-shotIMDB model, obtaining six different responses, and the predictioncases for different categories of HiGPT are shown in Tables 10, 11,and 12, respectively. The parts showing the final answers are high-lighted in pink. We make the following observations: Obs.1 OurHiGPT, after instruction tuning with the MoT graph instruction aug-mentation strategy, can dynamically respond accurately to differentprompts. Obs.2 The CoT prompt in , which is unformat-ted, also shows a certain format (highlighted in yellow), which isattributed to the fact that mixing a variety of instructions can alsobenefit different prompting techniques. Obs.3 As highlighted ingreen in multiple cases, our HiGPT, after our designed two-stagegraph instruction tuning, is consciously considering issues froma graph perspective, further proving that our model is not only",
  "{Human Question} {GPTsAnswer&Reasoning}": "GKP-1I have a question as below: {Human Question} ; and the answer is {Ground Truth},please generate some knowledge that can assist in formulating an answer, including,but not limited to: distinctions between the four categories. Imagine that you havearrived at the correct answer based on the provided information and knowledge, andpresent a step-by-step reasoning. Using the following format: Knowledge: ... Answer: ...Reason: ...",
  "{HumanQuestion}+{GPTsKnowledge} {GPTs An-swer&Reasoning}": "GKP-2Please generate some knowledge that can assist in formulating an answer, including, butnot limited to: explanations of some technical terms present in the given information.Imagine that you have arrived at the correct answer based on the provided informationand knowledge, and present a step-by-step reasoning. Using the following format:Knowledge: ... Answer: ... Reason: ...",
  "solving downstream problems from a textual perspective but hasalso developed a certain level of graph-awareness": "A.5.3Comprehensive Results of Overall Performance Com-parison. showcases the results of our HiGPT in bothfew-shot and zero-shot settings, covering scenarios with 1, 3, 5, 10,20, 40, and 60 shots. The results clearly indicate that our modelconsistently outperforms state-of-the-art models in all cases. A.5.4Comprehensive Results of Graph In-Context Learning.The performance of our model on all metrics, across different shotscenarios and datasets, under the graph in-context learning is illus-trated in . The results clearly demonstrate that our graphin-context learning approach significantly improves the modelsperformance without any modifications to the model parameters.",
  "Node Classification": "Input: Given a heterogeneous graph about internet movie, there are three types of nodes, namely: movie, actor, director. The relationships (meta paths) betweendifferent nodes include: [movie is directed by director], [movie has actor]. By performing random sampling of 2-hop 10 neighbors centered on the target movienode, a heterogeneous subgraph is obtained. In the subgraph, \"movie\" nodes: <graph>, where the 0-th node is the central node that represents a moive withthe following information: Name: {movie name} Directors name: {director name} Actors names: {actor name} Plot keywords: {plot keywords} \"actor\"nodes: <graph>; \"director\" nodes: <graph>. Question: Which of the following classes does this movie belong to: action, comedy, drama? {Prompting Suffix}",
  "Prompting Suffix of Different Prompting Techniques": "Standard with the direct answer: Give likely categories directly.CoT without Format Constraint: Please think about the categorization in a step by step manner and avoid making false associations. Then provide yourreasoning.CoT with Format Constraint: Please think about the categorization in a step by step manner and avoid making false associations. Then provide your reasoning.Using the following format: Answer: [The answer] Reason: ...ToT: Imagine three different experts are answering this question. All experts will write down 1 step of their thinking, then share it with the group. Then allexperts will go on to the next step, etc. If any expert realises theyre wrong at any point then they leave.PanelGPT: 3 experts are discussing the question with a panel discussion, trying to solve it step by step, and make sure the result is correct and avoid penalty.",
  "Instruction for Heterogeneous Relation Awareness": "Input: Given a heterogeneous graph about internet movie, there are three types of nodes, namely: movie, actor, director. The relationships (meta paths) betweendifferent nodes include: [movie is directed by director], [movie has actor]. By performing random sampling of 2-hop 10 neighbors centered on the target movienode, a heterogeneous subgraph is obtained. In the subgraph, there are several sequences of heterogeneous graph nodes of different types as follows:1. <graph> 2.<graph> 3. <graph> Question: please sequentially provide the categories for the node sequences. Example: 1. actor 2. movie 3. director.",
  "Instruction for Homogeneous Relation Awareness": "Given a heterogeneous graph about internet movie, there are three types of nodes, namely: movie, actor, director. The relationships (meta paths) between differentnodes include: [movie is directed by director], [movie has actor]. By performing random sampling of 2-hop 10 neighbors centered on the target movie node, aheterogeneous subgraph is obtained. In the subgraph, there are \"movie\" nodes: <graph>. Here is a list of movies names, their directors names, their actorsnames and their plot keywords: {movie lists} Question: please reorder the list of movies according to the order of graph tokens (i.e., complete the matching ofgraph tokens and movies)",
  ": Visualization of our HiGPTs response with different prompting engineering techniques on IMDB for action genre": "Question: Given a heterogeneous graph about internet movie, there are three types of nodes, namely: movie, actor, director. The relationships (meta paths)between different nodes include: [movie is directed by director], [movie has actor]. By performing random sampling of 2-hop 10 neighbors centered on thetarget movie node, a heterogeneous subgraph is obtained. In the subgraph, \"movie\" nodes: <graph>, where the 0-th node is the central node that represents amovie with the following information: Name: Live Free or Die Hard; Directors name: Len Wiseman; Actors names: Bruce Willis|Jonathan Sadowski|CyrilRaffaelli; Plot keywords: fbi|hacker|independence day|police|terrorist; \"actor\" nodes: <graph>; \"director\" nodes: <graph>. Question: Which of the followingclasses does this movie belong to: action, comedy, drama?",
  "+ Give likely categories directly. HiGPT (Direct Response): Action": "+ Please think about the categorization in a step by step manner and avoid making false associations. Then provide your reasoning.HiGPT (CoT Response without Format Constraint): Answer: Action Reasoning: The movie \"Live Free or Die Hard\" has several plot keywords thatsuggest an action genre, such as \"fbi\", \"hacker\", \"independence day\", \"police\", and \"terrorist\". These keywords imply that the movie is likely to have a lot ofaction scenes, chases, and possibly some thrilling moments. Additionally, the director Len Wiseman has directed other action movies in the past, such as\"Underworld\" and \"Total Recall\". This further supports the idea that \"Live Free or Die Hard\" belongs to the action genre. Finally, the actors in the movie alsocontribute to the action genre. Bruce Willis, who plays the lead role, is known for his roles in action movies such as \"Die Hard\", \"Pulp Fiction\", and \"The FifthElement\". Jonathan Sadowski and Cyril Raffaelli, who also appear in the movie, have acted in other action movies and TV shows. Overall, based on the plotkeywords, the directors past work, and the actors roles, it is reasonable to conclude that \"Live Free or Die Hard\" belongs to the action genre. + Please think about the categorization in a step by step manner and avoid making false associations. Then provide your reasoning. Using the followingformat: Answer: [The answer] Reason: ...HiGPT (CoT Response with Format Constraint): Answer: Action Reason: Based on the plot keywords provided, the movie seems to involve elementsof action, such as the FBI, hackers, police, and terrorists. These keywords suggest that the movie may have a high level of action and suspense, which is acommon characteristic of action movies. Additionally, the presence of Bruce Willis, a well-known action star, further supports the classification of this movieas an action film. + Imagine three different experts are answering this question. All experts will write down 1 step of their thinking, then share it with the group. Then allexperts will go on to the next step, etc. If any expert realizes theyre wrong at any point then they leave.HiGPT (ToT Response): Expert 1: First, I will analyze the plot keywords of the movie. The keywords \"fbi\", \"hacker\", \"independence day\", \"police\", and\"terrorist\" suggest that the movie might belong to the action genre. However, the presence of the word \"independence day\" could also indicate that the moviemight be a combination of action and drama. Expert 2: Next, I will look at the directors name. Len Wiseman is known for directing the \"Underworld\" filmseries, which are primarily action movies. Therefore, it is likely that \"Live Free or Die Hard\" belongs to the action genre. Expert 3:Finally, I will examine theactors names. Bruce Willis is an action star, and Jonathan Sadowski and Cyril Raffaelli are relatively unknown actors. Therefore, it is more likely that themovie belongs to the action genre. Based on the analysis of the plot keywords, the directors name, and the actors names, it is likely that \"Live Free or DieHard\" belongs to the action genre. + 3 experts are discussing the question with a panel discussion, trying to solve it step by step, and make sure the result is correct and avoid penaltyHiGPT (Panel Discussion Response): Expert 1: Lets start by analyzing the plot keywords. The keywords \"fbi\", \"hacker\", \"independence day\", \"police\", and\"terrorist\" suggest that this movie might belong to the action genre. Expert 2: Thats a good point. However, we should also consider the directors previousworks. Len Wiseman is known for directing the \"Underworld\" series, which are primarily action movies. Therefore, its likely that this movie also belongs tothe action genre. Expert 3: I agree with both of you. However, we should also consider the actors names. Bruce Willis is known for his roles in action movies,and Jonathan Sadowski and Cyril Raffaelli are relatively unknown actors. Therefore, its possible that this movie might also belong to the drama genre. Expert1: Thats a good point. We should consider all possible genres. Lets perform a random sampling of 2-hop 10 neighbors centered on the target movie node toobtain a heterogeneous subgraph. Expert 2: Great idea. Lets see what we can find. Expert 3: Agreed. After performing the random sampling, the expertsfound that the movie \"Live Free or Die Hard\" belongs to the action genre.",
  ": Visualization of our HiGPTs response with different prompting engineering techniques on IMDB for comedy genre": "Question: Given a heterogeneous graph about internet movie, there are three types of nodes, namely: movie, actor, director. The relationships (metapaths) between different nodes include: [movie is directed by director], [movie has actor]. By performing random sampling of 2-hop 10 neighbors centeredon the target movie node, a heterogeneous subgraph is obtained. In the subgraph, \"movie\" nodes: <graph>, where the 0-th node is the central node thatrepresents a movie with the following information: Name: Caddyshack; Directors name: Harold Ramis; Actors names: Bill Murray|Rodney Dangerfield|BrianDoyle-Murray; Plot keywords: caddy|country club|golf course|gopher|nudity; \"actor\" nodes: <graph>; \"director\" nodes: <graph>. Question: Which of thefollowing classes does this movie belong to: action, comedy, drama?",
  "+ Give likely categories directly. HiGPT (Direct Response): Comedy": "+ Please think about the categorization in a step by step manner and avoid making false associations. Then provide your reasoning.HiGPT (CoT Response without Format Constraint): Based on the information provided in the subgraph, the movie \"Caddyshack\" belongs to the comedygenre. The plot keywords \"caddy|country club|golf course|gopher|nudity\" suggest that the movie is a comedy that involves a golf course, caddy, and potentiallysome nudity. Additionally, the director Harold Ramis is known for his work in comedy films, such as \"Groundhog Day\" and \"Ghostbusters.\" To further supportthe classification of \"Caddyshack\" as a comedy, we can compare it to other movies in the same subgraph that are also directed by Harold Ramis. For example,\"Groundhog Day\" is also a comedy, and it shares some of the same actors, such as Bill Murray, with \"Caddyshack.\" Therefore, it is reasonable to assume that\"Caddyshack\" also belongs to the comedy genre. In summary, the movie \"Caddyshack\" belongs to the comedy genre based on its plot keywords, directorHarold Ramis previous work in comedy films, and its similarity to other comedies in the same subgraph. + Please think about the categorization in a step by step manner and avoid making false associations. Then provide your reasoning. Using the followingformat: Answer: [The answer] Reason: ...HiGPT (CoT Response with Format Constraint): Answer: Comedy Reason: The movie \"Caddyshack\" is a comedy film, as indicated by its plot keywordssuch as \"caddy\", \"country club\", \"golf course\", and \"gopher\", which suggest a light-hearted and humorous tone. Additionally, the director Harold Ramis has ahistory of directing comedies, further supporting the classification of this movie as a comedy. + Imagine three different experts are answering this question. All experts will write down 1 step of their thinking, then share it with the group. Then allexperts will go on to the next step, etc. If any expert realizes theyre wrong at any point then they leave.HiGPT (ToT Response): Based on the plot keywords, \"caddy,\" \"country club,\" \"golf course,\" and \"nudity,\" it seems like the movie \"Caddyshack\" belongs tothe comedy genre. Expert 1: The movie has keywords like \"caddy,\" \"country club,\" and \"golf course,\" which are commonly associated with comedy movies.Expert 2: The presence of Bill Murray, Rodney Dangerfield, and Brian Doyle-Murray, who are known for their comedic roles, also suggests that the movie is acomedy. Expert 3: The plot keywords \"nudity\" may be a red herring, as it is not a common characteristic of comedy movies. Therefore, the movie likelybelongs to the comedy genre. + 3 experts are discussing the question with a panel discussion, trying to solve it step by step, and make sure the result is correct and avoid penaltyHiGPT (Panel Discussion Response): To solve this problem, we need to analyze the plot keywords and the directors name to determine the genre ofthe movie. The plot keywords suggest that the movie is likely to be a comedy, as it contains keywords such as \"caddy,\" \"country club,\" \"golf course,\" and\"gopher,\" which are commonly associated with comedic situations in movies. The directors name, Harold Ramis, also supports this conclusion. He is knownfor directing comedies such as \"Groundhog Day\" and \"Ghostbusters,\" which were both critical and commercial successes. Therefore, we can conclude that themovie \"Caddyshack\" belongs to the comedy genre. As for the sorting of the \"actor\" nodes, we can use the plot keywords to determine the order. The actorsnames suggest that the movie has a mix of well-known and lesser-known actors, but the plot keywords suggest that the movie is primarily focused on theactors interactions with the golf course and gopher. Therefore, we can sort the actors names based on their prominence in the movie, with Bill Murray andRodney Dangerfield being more well-known and therefore higher up in the list, and Brian Doyle-Murray being lesser-known but still important to the plot. Sothe sorted list of actors names would be: 1. Bill Murray 2. Rodney Dangerfield 3. Brian Doyle-Murray As for the sorting of the \"movie\" nodes, we can use theplot keywords to determine the order. The plot keywords suggest that the movie is primarily focused on the golf course and gopher, so we can sort the movienodes based on their prominence in the movie, with \"Caddyshack\" being the central node and the other two movies being lesser-known and therefore lowerdown in the list. So the sorted list of movie nodes would be: 1. Caddyshack 2. Happy Gilmore 3. The Greatest Game Ever Played Therefore, the final sorted listof nodes would be: 1. Caddyshack 2. Harold Ramis 3. Bill Murray 4. Rodney Dangerfield 5. Brian Doyle-Murray 6. Happy Gilmore 7. The Greatest Game EverPlayed Note that the order of the \"actor\" nodes and the \"movie\" nodes may vary depending on the specific criteria used to sort them. However, the overallconclusion that \"Caddyshack\" belongs to the comedy genre and is the central node in the subgraph remains the same.",
  ": Visualization of our HiGPTs response with different prompting engineering techniques on IMDB for drama genre": "Question: Given a heterogeneous graph about internet movie, there are three types of nodes, namely: movie, actor, director. The relationships (meta paths)between different nodes include: [movie is directed by director], [movie has actor]. By performing random sampling of 2-hop 10 neighbors centered on thetarget movie node, a heterogeneous subgraph is obtained. In the subgraph, \"movie\" nodes: <graph>, where the 0-th node is the central node that represents amovie with the following information: Name: The Life of David Gale; Directors name: Alan Parker; Actors names: Kevin Spacey|Kate Winslet|Matt Craven;Plot keywords: activist|death|death row|murder|texas\"actor\" nodes: <graph>; \"director\" nodes: <graph>. Question: Which of the following classes does thismovie belong to: action, comedy, drama?",
  "+ Give likely categories directly. HiGPT (Direct Response): Drama": "+ Please think about the categorization in a step by step manner and avoid making false associations. Then provide your reasoning.HiGPT (CoT Response without Format Constraint): Based on the plot keywords provided, the movie \"The Life of David Gale\" belongs to the dramagenre. The plot keywords include \"activist,\" \"death,\" \"death row,\" \"murder,\" and \"texas,\" which suggest a serious and emotional storyline. These keywords arecommonly associated with drama films, rather than action or comedy films. Therefore, the movie \"The Life of David Gale\" belongs to the drama genre. + Please think about the categorization in a step by step manner and avoid making false associations. Then provide your reasoning. Using the followingformat: Answer: [The answer] Reason: ...HiGPT (CoT Response with Format Constraint): Answer: Drama Reason: The movie \"The Life of David Gale\" is a drama, as it deals with serious andemotional themes such as activism, death row, and murder. The plot keywords also suggest a dramatic tone. + Imagine three different experts are answering this question. All experts will write down 1 step of their thinking, then share it with the group. Then allexperts will go on to the next step, etc. If any expert realizes theyre wrong at any point then they leave.HiGPT (ToT Response): Expert 1: Based on the plot keywords, this movie seems to be a drama. Expert 2: I agree with Expert 1. The plot keywords such as\"activist,\" \"death,\" \"death row,\" and \"murder\" suggest a serious and emotional theme. Expert 3: I also agree with Expert 1 and 2. The plot keywords and the factthat the movie is directed by Alan Parker, who is known for his dramatic films, further support the classification of this movie as a drama. Final answer: Themovie belongs to the drama class. + 3 experts are discussing the question with a panel discussion, trying to solve it step by step, and make sure the result is correct and avoid penaltyHiGPT (Panel Discussion Response): 1. Expert 1: Lets start by analyzing the plot keywords. We have \"activist,\" \"death,\" \"death row,\" \"murder,\" and \"texas.\"Based on these keywords, I would say this movie belongs to the drama genre. 2. Expert 2: I agree with Expert 1. The plot keywords suggest a serious andemotional storyline. However, we also have \"actor\" nodes in the graph, which could indicate that there are comedic moments or performances. 3. Expert3: I would like to add that we should also consider the directors name, Alan Parker. He has directed both dramatic and comedic movies, such as \"TheCommitments\" and \"Birdy.\" Therefore, its difficult to determine the genre solely based on the plot keywords and directors name. 4. Panel Discussion: Basedon the analysis so far, we can conclude that the movie \"The Life of David Gale\" belongs to the drama genre, but it may also have comedic elements due to thepresence of actor nodes in the graph. : Comprehensive performance comparison on node classification tasks in both few-shot and zero-shot settings. However,since SSL methods focus on learning embeddings from downstream graphs, we excluded the zero-shot settings for them (\"-\").",
  "IMDB-60ACM-10000.50850.00410.44390.00780.44730.00260.50700.00440.50370.0082---0.58010.00510.63990.0058": "Number of Training Shots on IMDB 50% 55% 60% 65% 70% Testing Mi-F1 on IMDB 0.6550.6530.646 0.6540.655 0.6640.658 OursOurs-ICL-1Ours-ICL-2 Number of Training Shots on IMDB 50% 55% 60% 65% 70% Testing Ma-F1 on IMDB 0.6500.6480.641 0.6490.653 0.6610.654 OursOurs-ICL-1Ours-ICL-2 Number of Training Shots on IMDB 50% 55% 60% 65% 70% Testing AUC on IMDB 0.7500.7490.7440.7480.7500.7570.753 OursOurs-ICL-1Ours-ICL-2",
  "(a) IMDB-IMDB@Mi-F1, Ma-F1, AUC": "Number of Training Shots on IMDB 35% 40% 45% 50% 55% Testing Mi-F1 on DBLP 0.387 0.4690.475 0.542 0.510 0.5330.540 OursOurs-ICL-1Ours-ICL-2 Number of Training Shots on IMDB 35% 40% 45% 50% 55% Testing Ma-F1 on DBLP 0.374 0.4610.467 0.535 0.505 0.5280.539 OursOurs-ICL-1Ours-ICL-2 Number of Training Shots on IMDB 50% 55% 60% 65% 70% Testing AUC on DBLP 0.586 0.6340.639 0.691 0.667 0.680 0.689 OursOurs-ICL-1Ours-ICL-2",
  "(b) IMDB-DBLP@Mi-F1, Ma-F1, AUC": "Number of Training Shots on IMDB 50% 55% 60% 65% 70% Testing Mi-F1 on ACM 0.6340.627 0.679 0.620 0.517 0.578 0.634 OursOurs-ICL-1Ours-ICL-DBLP Number of Training Shots on IMDB 50% 55% 60% 65% 70% Testing Ma-F1 on ACM 0.6310.627 0.679 0.619 0.505 0.570 0.629 OursOurs-ICL-1Ours-ICL-DBLP Number of Training Shots on IMDB 50% 55% 60% 65% 70% Testing AUC on ACM 0.7240.721 0.760 0.717 0.646 0.687 0.729 OursOurs-ICL-1Ours-ICL-DBLP"
}