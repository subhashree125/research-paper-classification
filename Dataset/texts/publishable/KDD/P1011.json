{
  "Abstract": "In this paper, we tackle a new problem of multi-source unsuperviseddomain adaptation (MSUDA) for graphs, where models trained onannotated source domains need to be transferred to the unsuper-vised target graph for node classification. Due to the discrepancyin distribution across domains, the key challenge is how to se-lect good source instances and how to adapt the model. Diversegraph structures further complicate this problem, rendering previ-ous MSUDA approaches less effective. In this work, we present theframework Selective Multi-source Adaptation for Graph (SelMAG),with a graph-modeling-based domain selector, a sub-graph nodeselector, and a bi-level alignment objective for the adaptation. Con-cretely, to facilitate the identification of informative source data,the similarity across graphs is disentangled and measured withthe transferability of a graph-modeling task set, and we use it asevidence for source domain selection. A node selector is furtherincorporated to capture the variation in transferability of nodeswithin the same source domain. To learn invariant features foradaptation, we align the target domain to selected source data bothat the embedding space by minimizing the optimal transport dis-tance and at the classification level by distilling the label function.Modules are explicitly learned to select informative source dataand conduct the alignment in virtual training splits with a meta-learning strategy. Experimental results on five graph datasets showthe effectiveness of the proposed method.",
  "graph neural networks, transfer learning, domain adaptation": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08 ACM Reference Format:Tianxiang Zhao, Dongsheng Luo, Xiang Zhang, and Suhang Wang. 2024.Multi-source Unsupervised Domain Adaptation on Graphs with Trans-ferability Modeling. In Proceedings of the 30th ACM SIGKDD Conferenceon Knowledge Discovery and Data Mining (KDD 24), August 2529, 2024,Barcelona, Spain. ACM, New York, NY, USA, 11 pages.",
  "Introduction": "Graph neural networks (GNNs) have shown great ability in rep-resentation learning on graphs, especially the node classificationtask . Nevertheless, the success of GNNs heavily relies on la-bel information; while for many real-world applications, obtaininglabel information is costly and time-consuming . The lackof label information challenges many existing GNNs. In practice,one often has access to multiple annotated domains in training. Forexample, social networks may be collected from different platforms,communities of different ethnicities, and users speaking differentlanguages. For a newly-collected social network, typically nodelabels are unknown and we want to classify them with modelstrained from those source domains. Recent years have featured atrend toward transferring knowledge across datasets to alleviatethe lack of supervision , which motivates us to explore theapplication of GNNs trained on annotated datasets (source domains)to the new unlabeled dataset (target domain) (the induc-tive setting). This scenario can be generalized to a new learningproblem: multi-source unsupervised domain adaptation (MSUDA) forgraphs as shown in .There are some significant challenges in solving the MSUDAtask : (1) the mismatch between data distributions of thesource and target domains, which requires an implicit or explicitadaptation of the trained model ; (2) A clear discrepancy alsoexists among multiple source domains, hampering the effectivenessof mainstream single-source domain adaptation methods .Explorations have been made addressing these challenges. Existingmethods differentiate the effects of source domains on the targetone by computing domain similarities from the lens of differentperspectives, such as conditional distribution probability ,and estimation with an adversarial discriminator , etc. Then,they can transform it into a single-source problem via re-weightingand conduct alignment for the adaptation . Furthermore, [3,",
  "KDD 24, August 2529, 2024, Barcelona, SpainTianxiang Zhao, Dongsheng Luo, Xiang Zhang, and Suhang Wang": ": An example of MSUDA for graphs. We want to trans-fer knowledge from annotated source domains {1 , 2 , 3 } to. Regions of the same color denote similar node attributes.It can be observed that source domains and sub-graphs ofeach domain are of different importance in adapting to w.r.t node distributions. 16, 41] provide a performance bound of adapting from a weightedcombination of source domains to the target domain.However, all the aforementioned methods are designed for in-dependent and identically distributed (i.i.d) data; while the com-plex graph structures bring new challenges to MSUDA. In graph-structured data, nodes are interconnected with edges, and the num-ber of possible topology structures grows exponentially with thegraph size. This rich and highly-diverse input space offers increasedfreedom for the mapping in <graph structure, node annotation>pairs. As a result, computing domain similarities in the input orembedding space and taking it as the estimation of informativenessfor node classification could be unreliable, and it is unsafe to assumethat nodes of the same source domain would carry a similar levelof knowledge for this transfer, as the example in . It is difficultto identify subsets of source domains containing discriminativeknowledge to transfer to the target domain due to the intricatenature of graphs. Furthermore, complex graph structures also risedifficulties in alleviating discrepancy among domains, rendering theconventional approach of aligning only in the embedding domainless-effective .To address the aforementioned challenges, we propose to com-prehensively depict the similarity between graphs from multipleviews, such as distribution of node attributes, edge existences, topo-logical structures, etc. The informativeness of source graphs shouldfurther be modeled in aware of the downstream node classificationtask, and at both graph and node levels to prevent sub-optimal adap-tations. As shown in , the performance of domain adaptation isbounded by the discrepancy in distribution between the source andthe target data set, and a tighter adaptation bound can be derivedby identifying subsets of source data that are more similar to thetarget domain . With beneficial source data identified, we canachieve the adaptation through aligning the embedding learningand label prediction processes.Concretely, we design the first framework, SelMAG, for MSUDAon graphs. First, a set of graph modeling tasks are selected to capturethe graph distribution of each domain. The transferability of modelstrained on these tasks provides a numeric estimation of domainsimilarity from different perspectives. Then, based on obtainedsimilarity measurements, a source-graph selector is designed toidentify informative source domains towards the target domain onthe target downstream task. Furthermore, a sub-graph node selector is adopted to assign different important scores to nodes of the samesource domain. It can help conduct more fine-grained selectionsand meanwhile alleviate the target shift problem (like when classproportions are different) . Virtual training splits are designedto explicitly optimize these selectors in capturing transferabilityacross graph domains with Meta-learning . Finally, an optimal-transport-based alignment is conducted both in the embeddingspace and the classification space. Our main contributions are:",
  "We study a novel problem of MSUDA for graph-structured data,by selecting informative source data and adapting in both theembedding and classification space": "To identify sub-graphs of source domains that can transferdiscriminative knowledge, we design a modeling-based graphselector and a sub-graph node selector, and explicitly train themon the selection-and-adapt pipeline with meta-learning. We design an adaptation objective by conducting bi-level align-ments with optimal transport and knowledge distillation, whichcan intrinsically incorporate the learned informativeness ofsource domains simultaneously. Experimental evaluations show that SelMAG achieves state-of-the-art performance on five datasets, validating its design. Casestudies further show the ability of SelMAG in capturing theinformativeness of source domains.",
  "Related Work2.1Graph Neural Network": "With the increasing need for learning on relational data struc-tures , various graph neural networks have been designed,which can be categorized into spectral approaches andspatial approaches . Despite their differences, most GNNsfit within the framework of message-passing , in which nodesare iteratively updated by aggregating messages from local neigh-borhoods. For instance, GCN passes messages from neighbor-ing nodes with fixed weights, GAT applies the self-attentionmechanism to learn different attention scores and selects neighbor-hood messages dynamically. Some works augment GNNswith explicit prototypes to hierarchically model the motif structuresand increase the data efficiency. Other works proposeto uncover latent groups of nodes or edges and pass messages onthe disentangled graph. Recently, explorations have also been madeover the trustworthiness of graph neural networks and their explainability .Despite the great success of GNNs, their success usually hingesupon the availability of labeled training data, especially for thetask of node classification . However, in practice, for anewly-forming or low-resource target graph, its nodes are usu-ally unlabeled. The lack of labeled data challenges many existingGNN classifiers. Thus, we often need to adapt a model trained frommature source domains which have abundant label informationto the target domain. However, distribution shifts often exist be-tween the source domains and the target domain, which calls forthe development of domain adaptation algorithms on graph .Particularly, as there are usually multiple labeled source domainscan be exploited, we propose to study a novel problem of multi-source domain unsupervised adaptation on graphs, which aims to",
  "Unsupervised Domain Adaptation": "Unsupervised domain adaptation (UDA) aims to transfer the knowl-edge learned from labeled source domains to the unlabeled targetdomain . To adapt the knowledge from the source do-mains to the target domain, one major challenge is how to addressthe distribution shift between source and target domains. Mostrecent UDA methods focus on aligning source and target domainsby learning domain-invariant features , encouraging fea-tures to follow the same distribution regardless of which domainthey come from . To reduce the distribution discrepancy, someworks propose to minimize a divergence that measuresthe distance between distributions. propose to learn repre-sentations that can reconstruct the data distribution of the targetdomain. Other works use adversarial learning to fool adiscriminator which is trained to differentiate between two distri-butions. The aforementioned approaches are all designed for i.i.ddata. Recently, there are few works on domain adaptation on otherdata structures, e.g., graph data , text data ,etc. For example, UDA-GCN extends adversary alignment to graphs with an attentive feature extractor to learn the invariantfeatures. Overall, it is non-trivial to formulate graph distributionand design adaptive models considering its highly-diverse topology,graph size, and node features .Particularly, this work is related to multi-source unsuperviseddomain adaptation. This task is complicated further by distribu-tion discrepancy among source domains. Theoreticalanalyses have been provided w.r.t the performance bound of multi-source unsupervised domain adaptation , showing the im-portance of selecting important source domains. Explorations havebeen made in measuring domain similarities with conditional distri-bution probability from the smoothness assumption . Zhaoet al. uses an adversarial discriminator and conducts the worst-case alignment, and Nguyen et al. adopts a model-based simi-larity estimation. However, all these methods are designed for i.i.ddata and have difficulty in applying to graphs. In this work, ad-dressing the highly-diverse graph structures, we propose to designa domain selector based on disentangled similarity measurementswith graph-modeling tasks and further conduct sub-domain se-lection with a node-level selector, which is better in estimatinginformativeness of source domains w.r.t node classification of thetarget graph.",
  "Preliminary3.1Notations and Problem Definition": "Semi-supervised Node Classification. We focus on the node-level classification task in this work. Specifically, we use G =(V, E; , ) to denote a graph, where V is the node set and E V V is the set of edges. Nodes are accompanied by an attributematrix R|V|, and -th row of is the -dimensional at-tributes of the corresponding node. E is described by an adjacencymatrix R|V||V|. = 1 if there is an edge between node and ; otherwise, = 0. R|V| is the class information fornodes in G, obtained with an unknown labeling function , and ()is the number of classes. During training, only a subset of , is available, containing the labels for the training node set. Basedon those labeled nodes, a hypothesis model is trained to recoverthe unknown function and to predict node classes:",
  "(; G) , V.(1)": "Multi-source Unsupervised Graph Adaptation. We use domainto define a distribution over the graph generation and its latentlabeling function, denoted as , . In this multi-source graphadaptation, we have partially labeled graphs collected from sourcedomains as G = {G , }=1 and an unsupervised graph G from the target domain. Each source graph G is generated following thedistribution and its is obtained with latent labeling function . The objective is to build a hypothesis classifier that workswell on the target domain , , predicting classes of nodes inG accurately. Concretely, the task can be formalized as: Given partially labeled graphs {G , }=1 from different sourcedomains and an unsupervised graph G from the target domain, , we aim to train a node classification model to simulate with a small loss L( (G), (G)) on the target graph G.",
  "Optimal Transport for DA": "Optimal Transport (OT) provides a theoretic tool for computingdistances between probability distributions and for alignment-baseddomain adaptation , on which our method SelMAG is developed.In this section, we provide an introduction to its key concepts andsome important results used in the following sections. The OTproblem searches for a plan with the minimum cost to transforma distribution (over space ) to another distribution (overspace ). The cost of transforming each element is measured bya cost function : R+. Following the Monge masstransfer problem , the search of transport plan with minimumtotal transportation cost () can be expressed as:",
  "(, )(, ),(4)": "where is defined to be the set of all probabilistic couplings in( ) with marginals and . can be understood as ajoint probability measure for the transportation plan.To expose the connection between optimal transport and domainadaptation, following previous analysis , the error bound ofapplying source models to the target domain can be summarized",
  "=1, ),(5)": "where the first term represents errors on source domains, =min{ () + =1 ()} is the optimal joint error and is aconstant, whileH(=1 , ) measures H-divergence acrosstwo distributions. To achieve a low error on the target domain, thediscrepancy between the distribution of the target domain and thatof source domains needs to be minimized (the third term), which canbe achieved by aligning them in the embedding space, correspondsto learning a generalizable feature extractor across domains andminimizing the optimal transportation cost min () on it .",
  "Learning of Source Models": "As we have labels for source domains, we can train a set of sourcemodels { }=1, one for each domain, using the labeled data. Thosemodels will be used to train a target model later. Specifically, eachhypothesis model is composed of two parts, a feature extractionmodule ext (with stacked GNN layers) and a classifier module cls(as an MLP). Taking node from G as an example, the featureextraction module first learns node representation of as:",
  "=11( == ) log( []),(8)": "where V denotes the labeled node set of G. To promote the align-ment in the embedding space, we adopt a hard parameter sharing onthe feature extractor of all source domains and leave the classifiermodules domain-specific.The target model is learned upon two objectives: (1) aligningembeddings of the target graph to those of selected informativesubsets of source graphs with a loss derived from optimal transporttheory, and (2) the classification loss on pseudo labels generatedwith source models through a weighted distillation strategy. Detailsof them will be introduced in the following sections.",
  "Estimate Transferability of Source Domains": "Distribution shifts result in different levels of transferability fromsource domains to G, and sub-graphs of the same domain wouldalso vary in importance for transferring discriminative features. Toidentify informative subsets of source graphs that can be transferredto the target domain, we adopt a coarse-to-fine paradigm. First, agraph-level selector is designed conditioned on factorized similaritymeasurements and then a node-level selector is proposed to captureinformative subsets of each source graph. We will go into detailabout them in this section. 4.2.1Modeling-based Graph Selector. In this part, we introducethe design of the graph-level selector based on similarities that arefactorized into the transferability of different graph modeling tasks.Generally, if prediction models trained on graph G can performwell on G, then G and G are similar in some ways. A set of self-supervision tasks can be designed to model the graph distributionfrom different perspectives, and we can train on G and test theperformance on G to obtain disentangled similarity measurements.Various self-supervision tasks can be adopted. In this paper, weadopt the following three self-supervision tasks related to graphstructure/properties, which can better model graph distribution formeasuring the transferability. We leave the exploration of othertasks as future work.",
  "Edge Prediction . We randomly sample a set of edges fortraining, and the objective is to predict the existence of edges fora random pair of nodes": "Context Prediction . Based on node attributes, we cluster thenodes of those graphs into multiple groups with the K-Meansalgorithm. Then, for each node, we obtain its so-called contextas the group distribution of its direct neighborhood. The objectiveis to predict obtained context for each node. It can be seen that these self-supervised learning (SSL) tasks focuson different aspects of the graph and have different reliance upontypical graph elements: nodes, edges, and neighborhood topology.The transferability of these tasks between each source graph and thetarget graph can depict their similarities from different perspectives,which could be used to evaluate the informativeness of each sourcegraph towards the downstream node classification task.We use to denote the prediction module w.r.t SSL task on graph G, as shown in . Concretely, first, we train mod-ules { } T w.r.t these graph modeling tasks (denoted as T) for",
  "mance into the input, which can expose relative performancedrop and help evaluate the distribution shift": "4.2.2Sub-graph Node Selector. The modeling-based selector as-signs a weight to each source graph encoding its importance as awhole. However, for each source graph, some of its sub-graphs maybe more important than others in adapting to the target domain, . To conduct a more fine-grained selection and identify in-formative node groups of each source graph, we further design asubgraph-level node selector in this part. Without loss of generality,we implement it as a 2-layer MLP. For each candidate node of thesource graph G, we concatenate its embedding with the globalrepresentation of target graph G as the input:",
  ",local= localsel , poolingG (),(11)": "where denotes the embedding of node from graph G obtainedwith its feature extractor following Eq. 6. We take pooled nodeembeddings of G as its graph-level representation. Specifically,we adopt both max-pooling and mean-pooling and concatenatethem together to preserve both distinct parts and global patterns ofG . This selector will give similar weights to source data withsimilar embeddings in effect.However, it is challenging to learn these two selectors with back-propagation on the adaptation performance due to the unsupervisedtarget domain. Hence, we propose to optimize them with meta-levelupdates, which will be introduced in Sec. 4.4.",
  "Alignment-based Domain Adaptation": "With transferability between source graphs and the target graphencoded, in this section, we introduce our strategy to incorporate itinto the adaptation process to train the classifier that works for G.Previous analysis shows that the error of cross-domain adaptation isbounded by both the global divergence across two domains and theclass-wise distribution shifts (which can be measured as the optimaljoint error) . Therefore, we design the learning objectives ofmodel by mapping nodes of G into the same space as selecteddata of source domains for aligning embeddings and imitating classi-fication behavior of selected source models for aligning the labeling functions. Specifically, we design an optimal-transport-based algo-rithm that is able to utilize extracted transferability intrinsicallyalong with a weighted knowledge distillation mechanism. Detailsare provided in the following parts. 4.3.1Selective Optimal Transport for Adaptation. Based on theanalysis in Sec. 3.2 and the error bound presented in Eq. 5, adaptingthe GNN model requires minimizing the distance between the targetand source distributions, which can be achieved by reducing theminimum total transportation cost in Eq. 4. To obtain a smoothertransport plan and increase the optimization efficiency , anentropy-based regularization is added and the alignment losscan be formulated as:",
  "where () =": "(, ) log(, ) is the negen-tropy of transport plan . This loss can guide the adaptation of thetarget model to reduce the optimal transport distance between theembedding of source graphs and that of the target graph.However, this objective neglects the difference in informative-ness among source instances toward the target data. In multi-sourcegraph adaptation, different source graphs and sub-graphs of thesame graph may contribute differently to the learning of the tar-get model. Addressing this problem, we incorporate the predictedtransferability global and local into the selective OT-based adap-tation objective by augmenting the transport cost. Our basic ideais that the alignment should be focused on source data that havehigh importance for knowledge transfer to the target graph, andcontrarily for the rest of the source data to prevent the problemof negative transfer. Hence, for node from source graph G, thetransport cost measurement w.r.t target graph G can be calculatedas:sel(, ) = (, ) global ,local ,(13) sel(, ) is used to replace the original cost measurement(, )in Eq. 12. This design enables us to select and highlight informativeparts of the source graph set during the embedding alignment formodel adaptation. For tractable optimization, we can obtain its dualform with the Fenchel-Rockafellar theorem :",
  "in which () is a scoring function and can be simulated with anetwork. We set as a small positive number following": "4.3.2Weighted Knowledge Distillation. As the domain adaptationperformance is bounded both by the shifts on the embedding spaceand the shifts on the learned labeling functions , to transfer theclassification information from source models, we further adopt aknowledge distillation loss to provide training signals in the labelspace. Concretely, based on the importance score global of eachsource model , the soft pseudo label of node in G can beobtained as, =",
  "=1, [] log (; G)[],(16)": "where () is the number of classes, same as the dimension of ,. (; G) is the output of target model for node of G, with its-th dimension as predicted probabiliy of falling into class . Thiscross-entropy loss will distill the knowledge of learned labelingfunctions of source domains to the target domain.",
  "Optimization with Meta-learning": "The adaptation objective in Eq. 17 emphasizes reducing the align-ment distance. Directly optimizing the proposed selectors on this do-main adaptation task is improper and may result in trivia solutions,as it provides no signals over transferring class-discriminative infor-mation and has the danger of highlighting uniformly distributed butnon-informative (sub-)graphs. To improve the selective knowledgetransfer for domain adaptation, we design a meta-learning-basedoptimization strategy by simulating the unsupervised domainadaptation setting and guiding the learning of selectors based onperformance after adaptation. This learning to learn pipeline canprovide explicit learning signals for selectors.Concretely, to make the meta-training process in conformitywith the adaptation process and directly optimize it, we propose todirectly learn to select from source graphs by iteratively conductingtwo learning steps, inner update, and outer update. Below, we willshow how these two steps are designed. Inner Update. To guarantee consistency, setting on the inner up-date is designed to be also a multi-source graph adaptation task.For available source graph set G, in each iteration, we sample apseudo target graph G and use its complement as pseudo sourcegraphs G. The target model is updated for T steps on G fol-lowing Eq. 17, simulating the process of adaptation. At -th step,parameters are updated with:",
  "where is the parameter of model and is the optimization stepsize. Losses LSelOT and LKD are calculated on the pseudo sourceand target graphs": "Outer Update. Nodes of G are labeled and can be utilized to eval-uate the performance of graph adaptation. Gradients from adapted can be back-propagated to the weights global, local of pseudosource graphs G, and further, be utilized to optimize the two se-lectors. Concretely, sel can be optimized accordingly to explicitlyimprove upon learning to select as:",
  ": return Trained model for the target graph G": "where sel is the parameter of model globalseland localsel , is thelearning rate, and Lcls is node classification loss on labeled nodesof G, with model parameters learned from inner update.These two steps are performed iteratively and can be summarizedin Alg. 1. With inner and outer updates, both selectors are trainedto predict informative subsets of source graphs for the target graphw.r.t the downstream node classification task. Therefore, they canbe trusted to be applied to G afterward this meta-training phase.Following meta-training steps, in adapting to G, we will fine-tunethe parameter of following Eq. 17, as shown in Alg. 1.",
  "Experiment Settings": "5.2.1Baselines. The proposed method is compared with represen-tative and state-of-the-art single-source adaptation approaches andmulti-source adaptation methods. For empirical comparisons, weuse the following baselines: Direct, MMD , Reverse , Ad-versarial , OptimalT , UDA-GCN ,DistMDA ,MDAN , and MLDG . Details are provided in Appendix B. 5.2.2Configurations. In the experiments, for each dataset, we fixthe first 5 graphs as source domains and use the last one as the targetdomain, and label 10% of nodes for training. All methods share thesame backbone network structure of a two-layer GCN and aretrained until convergence with the maximum epoch number setto 2, 000. For meta-learning, the maximum epoch number of theouter update is also set to 2, 000 and the inner update step T is setto 5 following . The Adam optimizer is adopted for all methods,with the learning rate initialized to 0.01 and weight decay as 5e-4.For a fair comparison, for all baselines, we use grid search to settheir hyperparameters. In SelMAG, the hyper-parameter is set to0.3 and is set to 0.001 if not stated otherwise. 5.2.3Evaluation Metrics. We evaluate the adapted model on thetarget graph and adopt widely used evaluation metrics for nodeclassification, including accuracy (ACC), macro AUC-ROC (AU-ROC) , and macro F1 (MacroF). Both MacroAUC and MacroF arecalculated and averaged per class, hence are more indicative of theexistence of class imbalance.",
  "Graph Adaptation Performance": "To answer RQ1, we evaluate the performance of different methodsafter adapting to the target domain on all five datasets. Each experi-ment is conducted for 5 times with random parameter initializationand train/test splits of source domains, and we report the averageresults in . Both the mean and standard deviation w.r.t eachmetric are presented, with the best performance emboldened. Fromthe results, we can make the following observations: Our proposed method, SelMAG, achieves the best performanceacross all these datasets and outperforms baselines with a clearmargin. For example, on dataset Citation and Twitch, it shows animprovement of 1.2% and 4.1% in terms of accuracy compared tothe best baseline. Adaptation by taking all source domains as equal and neglectingtheir varying transferability to the target domain often showsweak performances, and may even result in a performance dropcompared to no adaptation at all. For example, on dataset Citation,all single-source domain alignment methods show a performancedrop in both accuracy and Macro F score compared to the baselineDirect, which trains a global source model and directly applies itto the target domain. Compared to other multi-source adaptation methods like Dist-MDA, SelMAGachieves further improvement, which supportsour motivation to capture transferability between graph domainsto cope with the highly diverse topology structure.",
  "Ablation Study": "In order to examine the importance of each component and an-swer RQ2, in this subsection, we conduct a set of ablation studiesby removing different parts of SelMAG and test the performance.All hyper-parameters are left unchanged, and each experiment isconducted three times on datasets Twitch, Yelp, and Cora_full withrandom initialization and train/test splits of source domains. Meanperformance and standard deviations are summarized in . 5.4.1Source-Graph Selectors. To evaluate the importance of differ-ent weights assigned to source domains, we test the performanceafter removing model-based graph selector (w/o globalsel) and sub-graph node selector (w/o localsel ). These two variants correspond toremoving global and local in Eq. 13, respectively. From , it",
  ": Influence of the weight of domain alignment loss": "is shown that both selectors play a positive influence in this multi-source adaptation. Particularly, the model-based graph selector ismore important than the sub-graph node selector in most cases,with the variant w/o globalselshowing the worst accuracy in all threedatasets. The reason could lie in the utilization of graph modelingtasks which provides clearer evidence for estimating transferability. 5.4.2Domain Alignment Objectives. To evaluate the importanceof aligning the classification function, we implement a variant byremoving the weighted knowledge distillation loss LKD in Eq. 17and annotate it as w/o KD. To test the generalizability of SelMAGwith different domain alignment objectives, we further implementa variant SelAdv by replacing OT-based alignment in LSelOT withadversary-based alignment . Learned weights of source do-mains can be incorporated into the adversarial aligning process viare-weighting. From , it can be observed that the weightedknowledge distillation loss is also helpful for the adaptation process,with w/o KD showing a drop in adaptation performance. Besides,it is shown that our method can also work for the adversarial align-ment with a moderate performance drop, validating its robustnessacross alignment strategies.",
  "Sensitivity Analysis": "5.5.1Alignment Weight. To evaluate the domain adaptation objec-tive and partially answer RQ2, we analyze the sensitivity of the Sel-MAG on hyper-parameters , which controls the balance betweenthe embedding alignment loss LSelOT and the knowledge distil-lation loss LKD in Eq. 17. We vary it as {0.1, 0.2, 0.3, 0.5, 0.6, 0.8},and experiments are conducted on Twitch and Yelp, with otherconfigurations remaining the same as the main experiment. Eachexperiment is conducted 3 times, and we report the average accu-racy and Macro F score in . It is shown that is better setto the range [0.3, 0.6] for both datasets. Setting it too small wouldweaken the embedding alignment signal while setting it too highwould fail to explicitly align the classifier. 0.7900 0.7925 0.7950 0.7975 score ACCF1",
  ": Influence of the hyper-parameter , which controlsoptimal transport in Eq. 14": "5.5.2Optimal Transport Configuration. To analyze the sensitivityof SelMAG on the optimal transport configuration, in this partwe vary as {0.0001, 0.001, 0.01, 0.1, 0.2} which regularizes thetransport in Eq. 14. A larger encourages the transport plan to besmoother . Again, experiments are conducted on Twitch andYelp for 3 times with all other configurations unchanged. The aver-age results are presented in Fig 4. It is shown that SelMAG achievesrelatively stable performance with a small within [0, 0.1], in ac-cordance with previous observations .",
  "Analysis on Global and Local Weights": "In this subsection, we provide some analysis over learned weightsglobal and local of source graph domains towards the target graphin order to answer RQ3. However, there is no ground-truth trans-ferability between graph pairs w.r.t the downstream node classifica-tion task. Therefore, we take a surrogate strategy by evaluating thesingle-source graph adaptation performance. Single-source graphadaptation is conducted by taking only one source graph as avail-able and using optimal transport for embedding alignment. Theaccuracy obtained on the target domain after adaptation is reportedas DA-ACC. Experiments are conducted on Yelp and Citation, withoptimization hyper-parameters set to be the same as introduced inSec. 5.2.2. For ease of analysis, we apply only the graph selector orthe sub-graph node selector respectively to exclude the influenceof the other one. Results are summarized in . For local, wereport its mean across nodes of the corresponding domain.From , we can observe that SelMAG tends to assign a higherweight to source domains that show a strong performance in single-source UDA. For example on Yelp, the largest global and localweights are both assigned to 4-th graph, which also shows thestrongest UDA performance. On Citation, the 5-th source graph isgenerated from the same dataset as G, and its weights are also high.For important source domains, their average local node weights arealso high. These results validate the ability of SelMAG in estimatingtransferability between graph domains.",
  ": Analysis on weights assigned to source domains byglobalselandlocalsel, by comparing them to the accuracy obtainedwith single-source domain adaptation.6Conclusion": "In this work, we propose a novel framework to identify informativesub-graphs for knowledge transfer in the multi-source graph adap-tation task. We depict graph similarities from three perspectives,each captured by a self-supervised graph modeling task, and esti-mate task-specific cross-domain transferability with a meta-learnedselector. An optimal-transport objective and a weighted knowledgedistillation objective are designed to incorporate obtained selec-tion scores into the domain alignment process. Experiments onfive datasets show that SelMAG outperforms existing methods forMSDA on graphs. In the future, we plan to extend SelMAG to workwith emerging new classes. In adapting source models to the targetdomain, there could be shifts in the label space, like novel classesunseen during training. This scenario calls for the development ofa new algorithm.",
  "Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. 2013. Spectral net-works and locally connected networks on graphs. arXiv preprint arXiv:1312.6203(2013)": "Rita Chattopadhyay, Qian Sun, Wei Fan, Ian Davidson, Sethuraman Panchanathan,and Jieping Ye. 2012. Multisource domain adaptation and its application to earlydetection of fatigue. ACM Trans. Knowl. Discov. Data 6, 4 (2012), 18:118:26. Bo Chen, Wai Lam, Ivor Tsang, and Tak-Lam Wong. 2009. Extracting discrimi-native concepts for domain adaptation in text mining. In Proceedings of the 15thACM SIGKDD international conference on Knowledge discovery and data mining.179188. Dong Chen, Hongqing Zhu, Suyi Yang, and Yiwen Dai. 2023. Unsupervised multi-source domain adaptation with graph convolution network and multi-alignmentin mixed latent space. Signal, Image and Video Processing 17, 3 (2023), 855863. Nicolas Courty, Rmi Flamary, Amaury Habrard, and Alain Rakotoma-monjy. 2017.Joint distribution optimal transportation for domain adapta-tion. In Advances in Neural Information Processing Systems 30: Annual Con-ference on Neural Information Processing Systems 2017, December 4-9, 2017,Long Beach, CA, USA, Isabelle Guyon, Ulrike von Luxburg, Samy Ben-gio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and RomanGarnett (Eds.). 37303739.",
  "William L. Hamilton, Zhitao Ying, and J. Leskovec. 2017. Inductive RepresentationLearning on Large Graphs. In NIPS": "Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu,Michele Catasta, and Jure Leskovec. 2020. Open graph benchmark: Datasets formachine learning on graphs. Advances in neural information processing systems33 (2020), 2211822133. Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande,and Jure Leskovec. 2019. Strategies for Pre-training Graph Neural Networks. InInternational Conference on Learning Representations. Srikanth Jagabathula, Dmitry Mitrofanov, and Gustavo Vulcano. 2022. Personal-ized retail promotions through a directed acyclic graphbased representation ofcustomer preferences. Operations Research 70, 2 (2022), 641665.",
  "Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graphconvolutional networks. arXiv preprint arXiv:1609.02907 (2016)": "Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy Hospedales. 2018. Learning togeneralize: Meta-learning for domain generalization. In Proceedings of the AAAIconference on artificial intelligence, Vol. 32. Yunsheng Li, Lu Yuan, Yinpeng Chen, Pei Wang, and Nuno Vasconcelos. 2021.Dynamic transfer for multi-source domain adaptation. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition. 1099811007. Shuai Lin, Chen Liu, Pan Zhou, Zi-Yuan Hu, Shuojia Wang, Ruihui Zhao, YefengZheng, Liang Lin, Eric Xing, and Xiaodan Liang. 2022. Prototypical graph con-trastive learning. IEEE Transactions on Neural Networks and Learning Systems(2022).",
  "Haitao Mao, Lun Du, Yujia Zheng, Qiang Fu, Zelin Li, Xu Chen, Shi Han, andDongmei Zhang. 2021. Source free unsupervised graph domain adaptation. arXivpreprint arXiv:2112.00955 (2021)": "Tuan Nguyen, Trung Le, He Zhao, Quan Hung Tran, Truyen Nguyen, and DinhPhung. 2021. Most: Multi-source domain adaptation via optimal transport forstudent-teacher learning. In Uncertainty in Artificial Intelligence. PMLR, 225235. Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang.2019. Moment matching for multi-source domain adaptation. In Proceedings ofthe IEEE/CVF international conference on computer vision. 14061415.",
  "Jeffrey Pennington, R. Socher, and Christopher D. Manning. 2014. Glove: GlobalVectors for Word Representation. In EMNLP": "Devakunchari Ramalingam and Valliyammai Chinnaiah. 2018. Fake profiledetection techniques in large-scale online social networks: A comprehensivereview. Computers & Electrical Engineering 65 (2018), 165177. Ievgen Redko, Nicolas Courty, Rmi Flamary, and Devis Tuia. 2019. Optimaltransport for multi-source domain adaptation under target shift. In The 22ndInternational Conference on Artificial Intelligence and Statistics. PMLR, 849858.",
  "Jun Wu, Jingrui He, and Elizabeth Ainsworth. 2022. Non-IID Transfer Learningon Graphs. arXiv preprint arXiv:2212.08174 (2022)": "Man Wu, Shirui Pan, Chuan Zhou, Xiaojun Chang, and Xingquan Zhu. 2020.Unsupervised Domain Adaptive Graph Convolutional Networks. In WWW 20:The Web Conference 2020, Taipei, Taiwan, April 20-24, 2020, Yennun Huang, IrwinKing, Tie-Yan Liu, and Maarten van Steen (Eds.). ACM / IW3C2, 14571467. Jiaren Xiao, Quanyu Dai, Xiaochen Xie, Qi Dou, Ka-Wai Kwok, and James Lam.2022. Domain adaptive graph infomax via conditional adversarial networks. IEEETransactions on Network Science and Engineering 10, 1 (2022), 3552.",
  "Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How powerfulare graph neural networks? arXiv preprint arXiv:1810.00826 (2018)": "Hongliang Yan, Yukang Ding, Peihua Li, Qilong Wang, Yong Xu, and WangmengZuo. 2017. Mind the Class Weight Bias: Weighted Maximum Mean Discrepancyfor Unsupervised Domain Adaptation. In 2017 IEEE Conference on ComputerVision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017.IEEE Computer Society, 945954. Luyu Yang, Yogesh Balaji, Ser-Nam Lim, and Abhinav Shrivastava. 2020. Cur-riculum manager for source selection in multi-source domain adaptation. InComputer VisionECCV 2020: 16th European Conference, Glasgow, UK, August2328, 2020, Proceedings, Part XIV 16. Springer, 608624.",
  "ADataset Description": "Citation. In this dataset, we use three citation networks collectedfrom ACM (ACM-V9), DBLP (DBLP-V7), and Microsoft AcademicGraph (Citation-V1) respectively. Each node represents a paperand its descriptions are extracted as attributes using Bog-of-words.Edges denote citations, and nodes are labeled on the paper domain.We randomly split each network into two graphs and create adataset of 6 graphs, which can increase the graph number andprovide us with some prior knowledge on the similarity betweengraph pairs at the same time. This design can also help evaluatethe effectiveness of our framework in selecting source domains.A sub-graph of Citation-V1 is used as the target. Twitch. This dataset is collected from the Twitch gamer platform,with nodes as Twitch users and edges as mutual follower relation-ships between them. This binary node classification task predictswhether a user streams explicit content. Six graphs are obtainedbased on the language used by the user: French, Spanish, Por-tuguese, German, English or Russian. The graph with languageRussian is used as the target graph. Yelp. This dataset contains user reviews on Yelp to various point-of-interests (POIs) in different cities. We transform the reviewsin each city into a graph, with each node representing a POI andeach edge representing a co-review relationship. POI featuresare obtained by averaging the word embedding of its reviews,which is taken from the pre-trained language model GLOVE .We perform classification on five classes, {Food, Shop, Home Ser-vice, Health Service, Finance}, and select six cities of differentscales: {Madison, Glendale, Gilbert, Las Vegas, Toronto, Phoenix}.City Phoenix is used as the target graph. Cora_full. It is a citation network with nodes for papers andedges for citation relations. We cluster papers into 6 groupsbased on different frequencies in selected-words usage followingGOOD to generate 6 graphs and select one as the target. Arxiv. It is a large citation network among the computer science(CS) arXiv papers. The task is to predict the subject area of eachpaper. We split it into 6 disjoint graphs based on the publishedyear of each paper , and use the most recent one as G.The main statistics of these datasets are summarized in ,including average node numbers and average edge numbers amonggraphs of each dataset.",
  "Reverse . By augmenting feed-forward models with a simplenew gradient reversal layer, this baseline can encourage discov-ering features that are not predictive towards domains": "Adversarial . To capture generalizable features that areinvariant across domains, this baseline adversarially trains a do-main discriminator to distinguish the source and target domains.Model is learned to extract features that are both discriminativetowards node labels and can fool the domain discriminator. OptimalT . This baseline assumes a non-linear transforma-tion between the joint feature/label space distributions acrossdomains, and the model is adapted by minimizing this total trans-formation cost.",
  "UDA-GCN : This method aligns source and target graphswith a domain classifier and includes classification entropy topromote classification boundaries of the target domain": "DistMDA . This method is designed for multi-source unsuper-vised domain adaptation, which makes a smoothness assumptionon data distribution and estimates the weight of each source do-main by minimizing the marginal probability difference. Afterobtaining source weights, Optimal transport is used for alignment.We extend it to graph-structured data by computing probabilitydifferences in the embedding space.",
  "MDAN . A worst-case selection strategy is used in this workfor multi-source unsupervised domain adaptation. We use itsSoft-Max version to assign weights and then conduct OT-basedadaptation": "MLDG . Meta-learning with synthesized virtual testing do-mains is utilized in this method in order to explicitly learn togeneralize to the unseen domain with multiple source domainsavailable. We implement it in the same setting as our SelMAG,with MAML for the conduction of meta-learning part.Note that MMD, Reverse, Adversarial, OptimalT, and UDA-GCN areoriginally designed for single-source domain adaptation. We extendthem to MSUDA by taking all source domains as one, equivalent togiving the same weights to all source graphs.",
  "CTime Complexity": "In this part, we provide some analysis over the additional computa-tion cost of our method. Note that during model adaptation process,at each batch, both baselines and our method have the same back-bone GNN architectures and the domain distance estimator foralignment. In addition to these backbone modules, our algorithmneeds an additional source domain selector. Denoting number ofGNN layers as , number of edges in the dataset as , number ofnodes as and embedding dimension as , for the backbone model,time complexity of GNN is ( + 2) and time complexity ofdistance estimator is (2). For our model, the additional domainselector has the time complexity of(2). Therefore, the proposed will not result in a significant computation increase inadaptation. During learning, the time-consuming part of our modelis to pretrain the graph modeling tasks. For a concrete example, ondataset Arxiv, which has 169,338 nodes, the pretraining step takesaround one hour. Note that this pretraining step is only needed onceacross different running, hence its cost would not pose a severeproblem."
}