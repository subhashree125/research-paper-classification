{
  "ABSTRACT": "Sum-product networks (SPNs) are probabilistic models characterized by exact and fast evaluation offundamental probabilistic operations. Its superior computational tractability has led to applications inmany fields, such as machine learning with time constraints or accuracy requirements and real-timesystems. The structural constraints of SPNs supporting fast inference, however, lead to increasedlearning-time complexity and can be an obstacle to building highly expressive SPNs. This studyaimed to develop a Bayesian learning approach that can be efficiently implemented on large-scaleSPNs. We derived a new full conditional probability of Gibbs sampling by marginalizing multiplerandom variables to expeditiously obtain the posterior distribution. The complexity analysis revealedthat our sampling algorithm works efficiently even for the largest possible SPN. Furthermore, weproposed a hyperparameter tuning method that balances the diversity of the prior distribution andoptimization efficiency in large-scale SPNs. Our method has improved learning-time complexity anddemonstrated computational speed tens to more than one hundred times faster and superior predictiveperformance in numerical experiments on more than 20 datasets.",
  "Introduction": "Probabilistic machine learning can account for uncertainty, but many important inference tasks often encountercomputational difficulties due to high-dimensional integrals. Simple probabilistic models, such as factorized andmixture models, can compute fundamental probabilistic operations exactly in polynomial time to the model size, e.g.,likelihood, marginalization, conditional distribution, and moments as shown in . However, these models oftensuffer from insufficient expressive power due to the lack of scalability to complex structures. Consequently, the primaryfocus of current probabilistic modeling is to achieve both expressiveness and computational tractability. A sum-product network (SPN) [Poon and Domingos, 2011] has received much attention in recent years thanksto its tractability by design. SPNs are considered a deep extension of the factorized and mixture models whilemaintaining their tractability. The tractability of SPNs is a notable characteristic, markedly distinct from modelsusing deep neural networks (DNNs) such as generative adversarial networks [Goodfellow et al., 2014], variationalautoencoders [Kingma and Welling, 2014], and normalizing flows [Rezende and Mohamed, 2015], where manyoperations are approximated. SPNs have many possible uses for machine learning tasks that require fast and exactinference, e.g., image segmentation [Yuan et al., 2016, Rathke et al., 2017], speech processing [Peharz et al., 2014],language modeling [Cheng et al., 2014], and cosmological simulations [Parag and Belle, 2022]. SPNs have also beeninvestigated for real-time systems such as activity recognition [Amer and Todorovic, 2016], robotics [Pronobis and Rao,2017], probabilistic programming [Saad et al., 2021], and hardware design [Sommer et al., 2018]. Learning an SPN is relatively more complicated and time consuming than inference. Conventional approacheslike gradient descent [Poon and Domingos, 2011, Gens and Domingos, 2012] and an expectation-maximization",
  "Deep structure": "algorithm [Poon and Domingos, 2011, Hsu et al., 2017] often suffer from overfitting, mode collapse, and instability inmissing data. One way to prevent these problems is Bayesian learning. Bayesian moment matching [Jaini et al., 2016]and collapsed variation inference [Zhao et al., 2016] were proposed, and in recent years, Gibbs sampling has beenremarkable for generating samples from the posterior distribution. Vergari et al. obtained the samples from thefull conditional distribution using ancestral sampling that recursively traces the graph from the leaf nodes toward theroot, and Trapp et al. [2019a] showed that the model parameters and the network structure can be learned simultaneouslyin a theoretically consistent Bayesian framework. However, although these studies validated the flexibility of Bayesianlearning, little attention has been paid to the computational complexity. The structural constraints of sum and product nodes guarantee the inference-time tractability of SPNs but lead toincreased time complexity during posterior sampling. The scope of a node, typically feature dimensions included inits children, is constrained by decomposability and completeness conditions. SPNs still possess high expressivenessdespite these constraints, although the graph shape is restricted. In particular, the height of SPNs has a strict upper limit,so the graph tends to be broad. Ko et al. reported that existing structural learning methods [Gens and Pedro,2013, Vergari et al., 2015, Zhao et al., 2017, Butz et al., 2017] are more likely to generate wide graphs, which can be aperformance bottleneck for inference. It is also computationally disadvantageous for existing posterior sampling thatrelies on bottom-up traversal of the entire graph. This problem is currently an obstacle to building highly expressivelarge-scale SPNs. This study aims to extend the efficiency of SPNs to Bayesian learning. To enable Bayesian learning of SPNs in areasonable time for large network sizes, we solve the three most critical problems from theory to practice: 1. A new fullconditional probability of Gibbs sampling is derived by marginalizing multiple random variables to obtain the posteriordistribution. 2. To efficiently sample from the derived probability, we propose a novel sampling algorithm, top-downsampling algorithm, based on the MetropolisHastings method considering the complexity of SPNs. It reduces theexponent by one in learning-time complexity and achieves tens to more than one hundred times faster runtime innumerical experiments. 3. For the increasing hyperparameters of large-scale SPNs, a new tuning method is proposedfollowing an empirical Bayesian approach. This paper is structured as follows. quickly reviews SPNs. We describe an existing latent variable model andGibbs sampling and illustrate how they are impaired by large-scale SPNs. reveals the complexity of SPNsand introduces our marginalized posterior, fast sampling algorithm, and hyperparameter tuning method and discussestheir theoretical computational complexity. demonstrates the computational speed, sample correlation, andpredictive performance of the proposed method through numerical experiments on more than 20 datasets. Finally, concludes the study.",
  "Sum-Product Network": "An SPN is a probabilistic model with a computational graph represented as a rooted directed acyclic graph (DAG) com-posed of sum, product, distribution nodes, and edges. left illustrates a typical computational graph of an SPN.The product node corresponds to the factorized model, i.e., a product of probability distributions p(x) =",
  ": Overview of SPNs. Left: computational graph representing the structure and weights of the SPN. Center:evaluation of the density of input data x. Right: structural constraints ensuring tractability": "results. SPNs can stack factorized and mixture models in a complex manner by repeating the product and sum nodesalternately. The distribution node at the leaves consists of a probability distribution and its parameters for a simple,typically one-dimensional variable. The modeling probability of SPNs corresponds to the bottom-up evaluation on the computational graph. centerdepicts an example of evaluating density p(x) of D = 2 input data x = (x1, x2): 1. The input vector x is dividedinto simple variables x1 and x2 and input to the corresponding distribution nodes. The input value is evaluated by theprobability of the distribution node and outputs a scalar probability value. 2. The intermediate node that receives thechildrens output computes their convex combination or product and outputs the scalar value. This procedure is repeatedbottom-up toward the root. 3. All leaves and intermediate nodes are computed, and the final output value of the SPN iscomputed at the root node. Two simple constraints on the structure of the graph streamline the fundamental probabilistic operations during inference. right represents the structural constraints of SPNs. The children of product nodes must have mutually exclusivevariables, called decomposability. Decomposability simplifies the integration of a product node into the integration ofits children. The children of sum nodes must include common variables, called completeness. Completeness ensuresefficient model counting and minimization of cardinality [Darwiche, 2001] and simplifies the integration of a sum nodeinto the integration of its children. SPNs satisfying both conditions can compute any marginalization in linear timewith respect to the number of nodes [Peharz et al., 2015]. The exact evaluation of density, marginalization, conditionalprobability, and moment [Zhao and Gordon, 2017] can be performed efficiently.",
  "Latent Variable Model": "An SPN can be interpreted within the Bayesian framework by considering it as a latent variable model. Let us introducea categorical latent variable z that indicates a mixture component of the sum node. The latent state z = (z1, . . . , zS) ofthe SPN is determined by specifying the state zs = c where c (1, . . . , Cs) for each sum node s (1, . . . , S).",
  "Ldj T (zn)Ldjxdn | djn,dj pdj | djj d.(2)": "The assumption of conjugacy is inherited from existing studies [Vergari et al., 2019, Trapp et al., 2019a], and it isnoted that even in non-conjugate cases, one-dimensional leaf distributions can be easily approximated numerically.The probability distribution Ldj is typically chosen by the variable type (continuous or discrete) and its domain (realnumbers, positive numbers, finite interval , etc.). Alternatively, the model can automatically choose the appropriate",
  "distribution by configuring the distribution node as a heterogeneous mixture distribution [Vergari et al., 2019]. Thedependency between the random variables and hyperparameters is summarized in left": "As indicated by the bold edges in center and right, a subgraph from the root to the leaves is obtained byremoving the unselected edges from the graph. This is called an induced tree [Zhao et al., 2017]. We denote theinduced tree determined by the state z as T(z). The induced tree always includes one distribution node for each featuredimension, Ldj T(z) d (1, . . . , D), due to the decomposability and completeness.",
  "ps(zsn = c | xn, , W) ws,c pc(xn | , W).(5)": "center illustrates joint sampling of latent vector zn using ancestral sampling. The algorithm runs bottom-up,where the output is determined first [Poon and Domingos, 2011, Vergari et al., 2019, Trapp et al., 2019a]. The statesexcluded from the induced tree are sampled from the prior [Peharz et al., 2017]. Posterior sampling using this approach is often computationally challenging for large SPNs. Ancestral samplingrequires a traversal of the entire graph, which is too costly to be executed inside the most critical loop of Gibbs sampling.The larger SPNs typically require more iterations to reach a stationary state, making the problem more serious.",
  "Proposed Method": "This section reveals the complexity of SPNs and the structures that highly expressive SPNs tend to form. Then wepropose a novel posterior sampling method for the SPNs and discuss its theoretical advantages. The proposed methodis explained from three perspectives: a new full conditional probability of Gibbs sampling that marginalizes multiplerandom variables, a top-down sampling algorithm that includes new proposal and rejection steps, and hyperparametertuning using an empirical Bayesian approach.",
  "Complexity of SPNs": "Both top-down and bottom-up algorithms are applicable to DAG-structured SPNs. However, the ensuing discussionon computational complexity confines SPNs to tree structures. Tree-structured SPNs are not necessarily compactrepresentations [Trapp, 2020], but they are frequently used in previous studies [Gens and Pedro, 2013, Vergari et al.,2015, 2019, Trapp et al., 2019a,b, Peharz et al., 2020]. This study aims to propose a sampling method that efficiently operates even on SPNs of theoretically maximum size.For computational considerations, we assume that SPNs fulfill the following conditions: 1. They satisfy the structuralconstraints of completeness and decomposability. 2. The outdegrees of sum and product nodes are Cs, Cp 2,respectively. 3. The graph exhibits a tree structure, implying that the nodes do not have overlapping children. 4. Theroot is a sum node. These assumptions enable subsequent algorithmic discussions to cover the worst-case computationalcomplexity under given D, Cs, and Cp. The structural constraints are important for making SPNs tractable for fundamental probability operations, but theyrestrict the graph shape. shows the maximum possible size of an SPN. For simplicity, it only presents caseswhere the SPN is a complete tree, i.e., the product nodes evenly distribute children, resulting in logCp D being aninteger, or conversely, where the children of product nodes are maximally skewed, with D1 Cp1 being an integer. Othertree structures fall within the intermediate of these cases. In either case, the number of all nodes V , distribution nodes L,induced trees, and graph breadth are larger than the other variables. Note that the sizes may differ if one considers SPNswith different conditions. For example, V and S are proportional in Zhao et al. , while they are asymptoticallydifferent by a degree of one in our case. The height and breadth of the SPN should be noted. Increasing Cs can widen the graph, whereas increasing Cpreduces the height. Since the product nodes must have children of different feature dimensions for decomposability, thenumber of product nodes is restricted by D. Each time a product node is passed through on the graph, it consumes Cpdimensions from D. When the SPN is a complete tree, the product node satisfying decomposability can only be usedup to logCp D times from a leaf to the root. For the SPN where sum and product nodes alternate, the graph height islimited to 2 logCp D + 2, including the root and leaves. When the consumption is minimized, i.e., Cp = 2, the graphheight is maximized to 2 log2 D + 2. In contrast, the outdegree Cs of sum nodes can be increased while satisfyingcompleteness. Accordingly, the graph breadth can increase as Cs (CpCs)logCp D without limitation. left shows the possible graph structures of an SPN for D = 2 data. While the height is limited to a maximum of4 when Cp = 2, the breadth can expand at 2C2s by increasing Cs. right depicts the density of an SPN modelinga two-dimensional artificial dataset with a strong correlation. As Cs increases and the SPN becomes broader, it cancapture the complex distribution of spiral data. SPNs are highly expressive even for complicated data by combiningmultiple one-dimensional distribution nodes. However, the breadth of SPNs significantly impacts this. In order forSPNs to have high representational power under the structural constraints, Cs must be increased.",
  "Marginalized Posterior Distribution": "The existing studies [Vergari et al., 2019, Trapp et al., 2019a] use Gibbs sampling that updates all random variableszn (n = 1, . . . , N), W, and alternately in Equation (4), resulting in strong correlation between consecutive samples.In such cases, the mixing can be slow due to potential barriers that cannot be overcome unless multiple random variablesare updated simultaneously. We solve this problem by marginalization.",
  "p(Z | X, , ) p(X | Z, )p(Z | ).(6)": "Sampling W and can be omitted entirely during learning. It reduces the number of variables that need to be sampled,thereby reducing the sample correlation and the number of iterations. The parameters W and are required duringinference, so they are sampled immediately before use. Since the learning and inference processes are usually separatedin Gibbs sampling, this delayed evaluation approach works efficiently. In the algorithm, the sampling of W and is replaced by deterministic computation of sufficient statistics, which isconstantly referenced during sampling Z. The problem here is that marginalization can sometimes result in compleximplementation, leading to decreased performance. We will explain the new sampling algorithm designed from theviewpoint of computational complexity.",
  ".(8)": "The network determines the first factor, i.e., the weights of the sum nodes. The leaves determine the second factor, i.e.,the probability distributions of the distribution nodes. These factors have different properties and thus require differentapproaches for efficient computation. We design our algorithm to have calculations related to S and D, where thedegree of increase is relatively small from the complexity of SPNs in .",
  "xdn | xd\\n, Z\\n, dj=Ldjxdn | djpdj | xd\\n, Z\\n, djddj .(12)": "The induced tree has one distribution node per feature dimension d in the complete and decomposable SPN. Referring toonly D distribution nodes is sufficient in the leaf factor, which is less costly than calculating all distribution nodes. Theproblem is that the combination {Ldj}Dd=1 depends on the graph structure of the SPN. The joint sampling for {Ldj}Dd=1cannot be reduced to simple per-dimension calculations. Enumerating the possible patterns of induced trees requires",
  "The network factors in the numerator and denominator cancel out each other and do not need to be evaluated": "By computing the sufficient statistics of the distribution nodes beforehand, Equation (11) can be evaluated in O(D),independent of the number of datapoints N. Since pLdj is a one-dimensional probability distribution, the sufficientstatistics can be easily updated in constant time for each sample. As shown in , D is tiny compared to otherdimensionalities, so the acceptance probability can be efficiently obtained. Furthermore, the evaluation of predictivedistributions can be omitted for the dimension d where the candidate component does not change (bd = cd). Itaccelerates Gibbs sampling even when the samples are correlated.",
  "Algorithm and Complexity": "The efficient implementation of this algorithm involves traversing the computational graph top-down while referring tothe candidate state zn = c generated by the network factor, as shown in right. Ignoring the nodes outside theinduced tree, only the distribution nodes required in Equation (13) are evaluated. Compared to the existing methodtraversing the entire graph bottom-up in center, the proposed method can be more efficiently executed byaccessing a subgraph consisting of a limited number of nodes. Intuitively, the top-down method is more efficient forwider graphs. compares the time complexity of the algorithms. The entire algorithm of the top-down method isshown as Appendix B. The parameters W and marginalized by the proposed method are not updated during Gibbs sampling. These valuesneed to be updated to the latest ones in inference time. This pre-processing is identical to what is done during eachiteration of Gibbs sampling in the bottom-up algorithm (Equation (4)) and is typically performed at checkpoints duringtraining or upon completion of training. Since these are not significant in terms of time complexity, the inference timecomplexity of the proposed method does not change as shown in .",
  "Empirical Bayesian Hyperparameter Tuning": "Another challenge in large-scale SPNs is hyperparameter optimization, an essential task in Bayesian learning. Thehyperparameters of SPNs include Cs, Cp, , and . In particular, the appropriate dj induces diversity in eachdistribution node Ldj and helps SPNs choose suitable component distributions. As shown in , there are asymptotically OClogCp D+1sdistribution nodes and is proportional to them. The number of hyperparameters caneasily exceed hundreds to thousands in a real-world dataset. Applying currently common hyperparameter optimizationmethods such as tree-structured Parzen estimator (TPE) [Bergstra et al., 2011] is not advised in terms of the number oftrials. We must consider tuning proxy parameters instead of directly tuning . In the empirical Bayesian approach, hyperparameters are obtained by maximizing the marginal likelihood. Althoughcalculating the marginal likelihood p(X | , ) of an SPN is difficult, maximizing it with respect to a hyperparameterdj can be reduced to maximizing the mixture of leaf marginal likelihoods pLdj | dj:",
  "This is derived from the fact that SPNs can be considered a mixture of induced trees [Zhao et al., 2017] and inducedtrees are factorized leaf models for each feature dimension (.2). The summation": "XcX is taken over allpossible subset Xc of dataset X. When there are N datapoints in the dataset, the summation size is 2N. The coefficientc is complicated and depends on the other hyperparameters \\ dj . Whereas the exact evaluation of Equation (14) iscomputationally impossible, it gives an essential insight that the marginal likelihood of distribution nodes over subsetdata gives the empirical Bayes estimate. We consider approximating Equation (14) by a significant term of the leaf marginal likelihood with specific subsetdata. Our goal is not to identify the optimal subset Xc directly but to find the subsampling ratio rd (0, 1] that Xcshould contain from the dataset X for each feature dimension d. The empirical Bayes estimate of hyperparameter dj isapproximated with subset data xdc = subsamplexd, rdby",
  "dj = arg maxdjpLdjxdc | dj.(15)": "By tuning rd using hyperparameter optimization methods, distribution nodes with appropriate hyperparameters areexpected to become the main component of the leaf mixture. When rd = 1, it is empirical Bayes over the full dataset,and all the distribution nodes Ld have the same hyperparameters for d. As rd approaches 0, the intersection of the",
  "Contrast with Prior Work": "Zhao et al. employed collapsed variational inference to approximate the posterior distribution. Their algorithmis optimization-based and significantly different from our method based on posterior sampling. Also, it integrated outZ, which is a unique approach different from many other variational inferences. It is also distinct from our posteriordistribution marginalizing W and . The proposed method only discusses Bayesian parameter learning and does not mention structural learning. In theexperiments in , we use a heterogeneous leaf mixture similar to Vergari et al. , and an appropriate proba-bility distribution is selected by weighting from multiple types of distribution nodes. It is possible to perform Bayesianstructural learning similar to [Trapp et al., 2019a], but pre-processing is required for the marginalized parameters fora conditional probability in structural inference. The speed benefits of our method may be compromised depending onthe frequency of structural changes. Vergari et al. , Trapp et al. [2019a] assume that all distribution nodes have the same hyperparameters for eachfeature dimension and distribution type. This simple setting always assumes the same prior distribution, so it does notinduce diversity in distribution nodes like our empirical Bayesian approach on subset data. When the hyperparametersare selected to maximize the marginal likelihood, the results are expected to be similar to the proposed method withsubsampling proportion rd = 1 d.",
  "Experiments": "In this section, we compare the empirical performance of top-down and bottom-up sampling methods. We used a totalof 24 datasets, where 18 datasets were from the UCI repository [Kelly et al.] and OpenML [Vanschoren et al., 2013]and 6 datasets were from the previous studies (Abalone, Breast, Crx, Dermatology, German, Wine). Appendix D showsthe dimensions of each dataset. The datasets were divided into 8:1:1 for training, validation, and testing. To preventirregular influence on the results, k-nearest neighbor based outlier detection was performed as pre-processing. We fixedCp = 2 and investigated the effect of different Cs. Each configuration was tested 10 times with different random seedsand the mean and standard deviation were plotted. The leaf distribution node consisted of a heterogeneous mixture ofone-dimensional probability distributions similar to Vergari et al. (cf. Appendix C). The hyperparameters of theleaf distributions were obtained by the empirical Bayes approach described in .4 optimized by conducting100 trials of TPE. The code was written in Julia and iterated as much as possible within 12 hours (including a 6-hourburn-in period) on an Intel Xeon Bronze 3204 CPU machine.",
  "Computational Efficiency": "We first measured the elapsed time required for each sampling method to perform one iteration of Gibbs samplingand confirmed how much acceleration was achieved. shows the elapsed time required for each samplingmethod to perform one iteration of Gibbs sampling. The results consistently show that the top-down method is superiorand generally several orders of magnitude shorter in execution time. The bottom-up method calculates outputs at allnodes and propagates them using the logsumexp algorithm, whereas the top-down method only identifies the memoryaddresses of induced leaves. Therefore, the actual computation time is significantly faster than the theoretical timecomplexity suggests. In particular, the difference is significant as Cs increases. These results show that the top-downmethod is tens to more than one hundred times faster, supporting the efficiency of our method.",
  "Cs = 2Cs = 4DatasetTop-downBottom-upTop-downBottom-up": "Abalone19.75 0.4520.39 0.4017.36 0.2819.77 0.17Ailerons14.11 0.2616.74 0.1817.92 0.07-Airfoil Self-Noise20.22 0.2820.47 0.5517.89 0.5220.49 0.15Breast20.37 0.1320.56 0.1520.51 0.0420.40 0.04Computer Hardware20.32 0.4320.69 0.2419.52 0.2420.57 0.03cpu_act14.37 0.6320.23 0.2215.50 0.70-cpu_small12.94 0.9919.71 0.3317.79 0.1620.18 0.13Crx18.67 0.6420.63 0.1318.84 0.0719.93 0.03Dermatology15.56 0.2520.57 0.0619.07 0.0620.10 0.03elevators15.05 0.2218.01 0.3416.99 0.06-Forest Fires18.86 0.4520.61 0.1818.60 0.1120.53 0.03German17.34 0.3420.60 0.1118.94 0.0719.35 0.03Housing17.31 1.2220.56 0.1417.43 0.2420.50 0.02Hybrid Price20.38 0.6620.54 0.9720.51 0.3920.57 0.36kin8nm18.15 2.1420.75 0.3016.83 0.1319.45 0.18LPGA200820.67 0.6020.46 0.2420.47 0.1720.50 0.11LPGA200920.52 0.3920.48 0.3120.37 0.1520.54 0.05Parkinsons Telemonitoring (motor)16.43 2.4120.29 0.2917.48 0.3617.07 0.06Parkinsons Telemonitoring (total)16.54 2.5220.19 0.3618.46 0.6216.96 0.10Vote for Clinton16.97 0.9319.95 0.5817.25 0.2619.41 0.10Wine Quality Red17.27 2.2520.43 0.3117.04 0.1619.72 0.05Wine Quality White15.81 1.2120.54 0.2916.70 0.1419.37 0.10Wine20.51 0.3520.53 0.1318.12 0.3020.55 0.03Yacht Hydrodynamics20.03 1.1620.69 0.2920.19 0.2120.57 0.11",
  "Finally, we evaluated the overall predictive performance of the top-down sampling method, which reveals the trade-offbetween fast iteration speed and sample correlation": "compares the log-likelihood on the test set for different Cs, showing the results after the burn-in period. Also, illustrates the temporal evolution of predictive performance, showing the differences between the methods byoptimizing Cs. For practical interest, this experiment also compares the results with those of the collapsed variationalBayes [Zhao et al., 2016] based on optimization by the gradient descent method. The top-down method is almostconsistently superior to the others, achieving the same or higher likelihood in most configurations. These results suggestthat the algorithm speedup outweighs the impact of sample correlation. While collapsed VB is memory efficientbecause it does not need to store latent variables for each data point, it requires two full network traversals for outputpropagation and gradient calculation, resulting in a time complexity of the same order as the bottom-up method. For amore comprehensive set of results, please refer to Appendix E. From the above experiments, we conclude that 1. the top-down method is tens to more than one hundred times fasterthan the bottom-up method, 2. the sample correlation is sufficiently small for both methods, and 3. as a result, thetop-down method can achieve higher predictive performance than the bottom-up method in many cases.",
  "Conclusion": "Prior work has interpreted SPNs as latent variable models and introduced Gibbs sampling as a Bayesian learning method.This has made it possible to automatically discriminate different distribution types for data and learn network structuresin a manner consistent with the Bayesian framework. However, the bottom-up posterior sampling approach based onthe entire graph evaluation had a computational difficulty. The shape of the computational graph was a bottleneck dueto the structural constraints. This study aimed to accomplish a fast Bayesian learning method for SPNs. First, we investigated the complexity ofSPNs when the outdegrees of the sum and product nodes are given and discussed the graph shape of SPN with highrepresentational power. We also derived the new full conditional probability that marginalizes multiple variables toimprove sample mixing. For the complexity and the marginalized posterior distribution, we proposed the top-downsampling algorithm based on the carefully designed proposal and rejection steps of the MetropolisHastings. Our optimization efforts resulted in a time complexity reduction from OClogCp D+1sdown to OClogCp Ds. In numericalexperiments on more than 20 datasets, we demonstrated a speedup of tens to more than one hundred times and improvedpredictive performance.",
  "Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In Proceedings of the 32ndInternational Conference on Machine Learning, ICML 2015, pages 15301538, 2015": "Zehuan Yuan, Hao Wang, Limin Wang, Tong Lu, Shivakumara Palaiahnakote, and Chew Lim Tan. Modeling spatiallayout for scene image understanding via a novel multiscale sum-product network. Expert Syst. Appl., 63(C):231240,2016. Fabian Rathke, Mattia Desana, and Christoph Schnrr. Locally adaptive probabilistic models for global segmentationof pathological oct scans. In International Conference on Medical Image Computing and Computer-AssistedIntervention, volume 10433 of Lecture Notes in Computer Science, pages 177184. Springer, 2017. Robert Peharz, Georg Kapeller, Pejman Mowlaee, and Franz Pernkopf. Modeling speech with sum-product networks:Application to bandwidth extension. In 2014 IEEE International Conference on Acoustics, Speech and SignalProcessing (ICASSP), pages 36993703, 2014. Wei-Chen Cheng, Stanley Kok, Hoai Vu Pham, Hai Leong Chieu, and Kian Ming A. Chai. Language modeling withsum-product networks. In Fifteenth Annual Conference of the International Speech Communication Association,2014.",
  "Andrzej Pronobis and Rajesh P. N. Rao. Learning deep generative spatial models for mobile robots. 2017 IEEE/RSJInternational Conference on Intelligent Robots and Systems (IROS), 2017": "Feras A. Saad, Martin C. Rinard, and Vikash K. Mansinghka. SPPL: probabilistic programming with fast exact symbolicinference. In Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Designand Implementation, pages 804819. Association for Computing Machinery, 2021. Lukas Sommer, Julian Oppermann, Alejandro Molina, Carsten Binnig, Kristian Kersting, and Andreas Koch. Automaticmapping of the sum-product network inference problem to fpga-based accelerators. In ICCD, pages 350357. IEEEComputer Society, 2018.",
  "Robert Gens and Pedro Domingos. Discriminative learning of sum-product networks. In Advances in Neural InformationProcessing Systems 25, NIPS 2012, pages 32393247, 2012": "Wilson Hsu, Agastya Kalra, and Pascal Poupart. Online structure learning for sum-product networks with gaussianleaves. In The 5th International Conference on Learning Representations (Workshop), ICLR 2017, 2017. Priyank Jaini, Abdullah Rashwan, Han Zhao, Yue Liu, Ershad Banijamali, Zhitang Chen, and Pascal Poupart. Online al-gorithms for sum-product networks with continuous variables. In Proceedings of the Eighth International Conferenceon Probabilistic Graphical Models, volume 52 of Proceedings of Machine Learning Research, pages 228239, 2016. Han Zhao, Tameem Adel, Geoff Gordon, and Brandon Amos. Collapsed variational inference for sum-product networks.In Proceedings of the 33rd International Conference on Machine Learning, ICML 2016, pages 13101318, 2016. Antonio Vergari, Alejandro Molina, Robert Peharz, Zoubin Ghahramani, Kristian Kersting, and Isabel Valera. Automaticbayesian density analysis. Proceedings of the AAAI Conference on Artificial Intelligence, 33(01):52075215, 2019.",
  "Top-Down Bayesian Posterior Sampling for Sum-Product NetworksA PREPRINT": "Antonio Vergari, Nicola Di Mauro, and Floriana Esposito. Simplifying, regularizing and strengthening sum-productnetwork structure learning. In Proceedings of the 2015th European Conference on Machine Learning and KnowledgeDiscovery in Databases - Volume Part II, ECMLPKDD15, pages 343358, 2015. Han Zhao, Pascal Poupart, and Geoff Gordon. A Unified Approach for Learning the Parameters of Sum-ProductNetworks. In Advances in Neural Information Processing Systems 30, NIPS 2017, pages 433441, 2017.",
  "Han Zhao and Geoff Gordon. Linear time computation of moments in sum-product networks. In Advances in NeuralInformation Processing Systems 30, NIPS 2017, pages 68976906, 2017": "Robert Peharz, Robert Gens, Franz Pernkopf, and Pedro Domingos. On the latent variable interpretation in sum-productnetworks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(10):20302044, 2017. Martin Trapp, Robert Peharz, and Franz Pernkopf. Optimisation of Overparametrized Sum-Product Networks. In 3rdWorkshop of Tractable Probabilistic Modeling at the International Conference on Machine Learning, 2019b. Robert Peharz, Antonio Vergari, Karl Stelzner, Alejandro Molina, Xiaoting Shao, Martin Trapp, Kristian Kersting, andZoubin Ghahramani. Random sum-product networks: A simple and effective approach to probabilistic deep learning.In Proceedings of the 35th Uncertainty in Artificial Intelligence Conference, volume 115 of Proceedings of MachineLearning Research, pages 334344, 2020.",
  "DatasetSPN (Cs = 2)SPN (Cs = 4)DNSVSV": "Abalone93, 8501173511, 0975, 485Ailerons4112, 6142, 5177, 551111, 177555, 885Airfoil Self-Noise61, 398531593291, 645Breast102771494471, 6098, 045Computer Hardware91921173511, 0975, 485cpu_act227, 4867252, 17516, 96984, 845cpu_small137, 4862457353, 14515, 725Crx166533411, 0234, 68123, 405Dermatology353581, 7495, 24762, 025310, 125elevators1915, 2485331, 59910, 82554, 125Forest Fires134742457353, 14515, 725German211, 0006611, 98314, 92174, 605Housing144612778313, 65718, 285Hybrid Price4146216373365kin8nm97, 6881173511, 0975, 485LPGA20087145692074572, 285LPGA2009121352136392, 63313, 165Parkinsons Telemonitoring (motor)175, 4234051, 2156, 72933, 645Parkinsons Telemonitoring (total)175, 4124051, 2156, 72933, 645Vote for Clinton102, 4701494471, 6098, 045Wine Quality Red121, 4692136392, 63313, 165Wine Quality White124, 4952136392, 63313, 165Wine141782778313, 65718, 285Yacht Hydrodynamics7288692074572, 285",
  "Cs = 2Cs = 4DatasetTop-downBottom-upSpeedupTop-downBottom-upSpeedup": "Abalone0.076 0.0041.574 0.043210.367 0.01917.38 0.69947Ailerons2.938 0.12196.28 2.63133181.2 28.063282 105.918Airfoil Self-Noise0.010 0.0010.303 0.024300.021 0.0012.047 0.14397Breast0.005 0.0000.170 0.016340.014 0.0011.938 0.079138Computer Hardware0.003 0.0000.098 0.011330.007 0.0000.943 0.078135cpu_act0.525 0.01616.80 0.7713211.72 2.503588.0 15.4650cpu_small0.204 0.0055.592 0.206272.059 0.11999.17 3.38248Crx0.023 0.0010.722 0.013310.232 0.00614.21 0.45861Dermatology0.034 0.0021.887 0.042561.932 0.16292.19 3.15048elevators0.759 0.02524.87 0.6353317.62 3.630804.7 35.4546Forest Fires0.015 0.0010.405 0.021270.101 0.0045.809 0.19158German0.050 0.0032.095 0.064421.298 0.09976.78 3.33859Housing0.009 0.0000.445 0.038490.114 0.0056.616 0.25058Hybrid Price0.001 0.0000.015 0.000150.001 0.0000.061 0.00761kin8nm0.099 0.0053.216 0.326320.696 0.03433.76 1.65349LPGA20080.001 0.0000.046 0.002460.002 0.0000.318 0.034159LPGA20090.002 0.0000.118 0.014590.011 0.0011.347 0.061123Parkinsons Telemonitoring (motor)0.207 0.0126.516 0.189313.273 0.300157.8 5.86948Parkinsons Telemonitoring (total)0.211 0.0116.631 0.226303.207 0.332156.4 5.91149Vote for Clinton0.028 0.0021.413 0.153500.315 0.01115.43 0.51049Wine Quality Red0.024 0.0021.100 0.079460.294 0.01215.51 0.87153Wine Quality White0.088 0.0063.215 0.249370.979 0.04449.60 2.45551Wine0.004 0.0000.196 0.020490.028 0.0042.576 0.08892Yacht Hydrodynamics0.003 0.0000.088 0.012290.005 0.0000.603 0.064121",
  ": Log-likelihood (mean std)Cs = 2Cs = 4DatasetTop-downBottom-upTop-downBottom-up": "Abalone4.47 0.311.10 0.156.97 0.302.94 0.12Ailerons110.24 0.87108.22 0.36106.95 0.83106.26 0.99Airfoil Self-Noise12.57 1.2513.06 1.396.57 0.207.91 0.27Breast6.56 0.187.29 0.326.45 0.116.90 0.16Computer Hardware49.19 4.6151.94 5.7632.35 0.7134.28 0.64cpu_act101.13 1.49103.49 1.0399.91 0.1397.79 0.20cpu_small85.03 0.7687.06 2.0382.52 0.6282.09 0.32Crx49.87 11.2052.13 13.0130.33 2.2330.08 2.18Dermatology21.87 2.9922.26 1.5725.92 0.5546.70 0.26elevators29.07 0.1529.02 0.1429.34 0.1629.14 0.19Forest Fires30.81 0.4932.24 0.6027.46 0.2528.45 0.15German17.52 0.4519.00 0.3215.33 0.6014.08 0.23Housing24.07 1.3327.02 0.9518.40 0.9420.80 0.21Hybrid Price22.58 0.4222.74 0.2921.42 0.3622.61 0.19kin8nm10.23 0.1010.26 0.0710.10 0.0410.13 0.02LPGA200815.68 0.4215.93 0.4314.68 0.2014.84 0.09LPGA200928.29 0.4029.63 0.2127.39 0.4429.72 0.20Parkinsons Telemonitoring (motor)43.85 0.6541.94 0.2543.50 0.5642.08 0.06Parkinsons Telemonitoring (total)43.37 0.5741.68 0.1342.82 0.8341.53 0.07Vote for Clinton49.61 0.1449.84 0.0349.77 0.0949.89 0.05Wine Quality Red3.62 0.304.35 0.372.73 0.282.95 0.11Wine Quality White2.96 0.573.59 0.562.57 0.142.61 0.10Wine19.90 0.6322.44 0.4618.41 0.4420.58 0.14Yacht Hydrodynamics2.22 1.831.06 1.858.25 0.644.98 0.15"
}