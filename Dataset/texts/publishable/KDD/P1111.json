{
  "ABSTRACT": "Cloud solutions have gained significant popularity in the technol-ogy industry as they offer a combination of services and tools totackle specific problems. However, despite their widespread use, thetask of identifying appropriate company customers for a specifictarget solution to the sales team of a solution provider remains acomplex business problem that existing matching systems haveyet to adequately address. In this work, we study the B2B solutionmatching problem and identify two main challenges of this scenario:(1) the modeling of complex multi-field features and (2) the limited,incomplete, and sparse transaction data. To tackle these challenges,we propose a framework CAMA, which is built with a hierarchicalmulti-field matching structure as its backbone and supplemented bythree data augmentation strategies and a contrastive pre-trainingobjective to compensate for the imperfections in the available data.Through extensive experiments on a real-world dataset, we demon-strate that CAMA outperforms several strong baseline matchingmodels significantly. Furthermore, we have deployed our match-ing framework on a system of Huawei Cloud. Our observationsindicate an improvement of about 30% compared to the previ-ous online model in terms of Conversion Rate (CVR), whichdemonstrates its great business value.",
  "Multi-field Matching, Contrastive Learning, Cloud Solutions": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08...$15.00 ACM Reference Format:Haonan Chen, Zhicheng Dou, Xuetong Hao, Yunhao Tao, Shiren Song,and Zhenli Sheng. 2024. Enhancing Multi-field B2B Cloud Solution Match-ing via Contrastive Pre-training. In Proceedings of the 30th ACM SIGKDDConference on Knowledge Discovery and Data Mining (KDD 24), August2529, 2024, Barcelona, Spain. ACM, New York, NY, USA, 11 pages.",
  "INTRODUCTION": "Cloud solutions, referring to a combination of various cloud-basedtechnologies, tools, and services, have become increasingly pop-ular among companies these years. They are designed to addressspecific business needs or solve particular problems, such as datastorage, application development, customer relationship manage-ment (CRM), etc. For solution providers, it is crucial to have aneffective matching system that can guide sales teams in identifyingpotential enterprises that can buy the solution. This is particularlyimportant due to the marketing value of solutions and the high costof human resources in Business-to-Business (B2B) scenarios.While there have been some studies focusing on designing ef-fective matching systems , none of theseworks have explored the matching of cloud solutions and their cus-tomers, which holds significant business value. In Huawei Cloud,the scenario is manual-driven, wherein our model identifies a listof the top matching companies to the sales team associated witha specific solution. The sales team then manually reviews this listand proceeds with promoting the solution to those companies. Thisspecific scenario can be considered a matching problem, with theprimary goal being the identification of appropriate companies(customers) for the sales teams to target in their promotion efforts.In this work, we focus on this specific scenario of B2B solutionmatching and identify two main challenges: (1) The features ofsolutions and companies are complex and often comprised ofmultiple fields. As presented in , the features consist of text,categorical, and numeric features, which consist of multiple fields.Modeling these different types of features can pose challenges, suchas different encoding paradigms for texts and other features, po-tential interference between different text fields, and interactionsbetween different types of features. (2) The available transac-tion data, which include recorded successful purchases, are",
  "KDD 24, August 2529, 2024, Barcelona, SpainHaonan Chen, Zhicheng Dou, Xuetong Hao, Yunhao Tao, Shiren Song, and Zhenli Sheng": "Ali Mamdouh Elkahky, Yang Song, and Xiaodong He. 2015. A Multi-View DeepLearning Approach for Cross Domain User Modeling in Recommendation Sys-tems. In Proceedings of the 24th International Conference on World Wide Web,WWW 2015, Florence, Italy, May 18-22, 2015, Aldo Gangemi, Stefano Leonardi,and Alessandro Panconesi (Eds.). ACM, 278288. Shubhashri G, Unnamalai N, and Kamalika G. 2018. LAWBO: a smart lawyerchatbot. In Proceedings of the ACM India Joint International Conference on DataScience and Management of Data, COMAD/CODS 2018, Goa, India, January 11-13,2018. ACM, 348351. Leilei Gan, Baokui Li, Kun Kuang, Yi Yang, and Fei Wu. 2022. Exploiting Con-trastive Learning and Numerical Evidence for Improving Confusing Legal Judg-ment Prediction. CoRR abs/2211.08238 (2022). arXiv:2211.08238 Huifeng Guo, Bo Chen, Ruiming Tang, Weinan Zhang, Zhenguo Li, and Xi-uqiang He. 2021.An Embedding Learning Framework for Numerical Fea-tures in CTR Prediction. In KDD 21: The 27th ACM SIGKDD Conference onKnowledge Discovery and Data Mining, Virtual Event, Singapore, August 14-18,2021, Feida Zhu, Beng Chin Ooi, and Chunyan Miao (Eds.). ACM, 29102918. Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.DeepFM: A Factorization-Machine based Neural Network for CTR Prediction.In Proceedings of the Twenty-Sixth International Joint Conference on ArtificialIntelligence, IJCAI 2017, Melbourne, Australia, August 19-25, 2017, Carles Sierra(Ed.). ijcai.org, 17251731.",
  "RELATED WORK2.1B2B Application Scenario": "Business-to-Business (B2B) systems differ from Business-to-Consumersystems typically employed on e-commerce platforms. B2B systemsare designed for more complex scenarios and are primarily deployedwithin a company for internal usage. Consequently, the existingliterature on B2B matching is limited. Zhang et al. was anearly work introducing B2B matching scenario. Some works com-bined several techniques to design hybrid systems . Forinstance, Pande et al. combined case-based reasoning (CBR),Collaborative Filtering (CF), and Random Walk for Consultancy.Furthermore, some applied tree-based algorithm and graph-based methods to model relationships of customers. Forexample, Henna et al. utilized a graph convolution network(GCN) for B2B customer relationship management. These worksall make great contributions to this field. However, none of thesestudies have explored the valuable solution-company matchingscenario and its associated challenges.",
  "Matching Models": "In our scenario, the problem can be considered a matching problemof solutions and companies, i.e., helping salesmen contact the com-panies that match the solution. In neural text matching, researchersfocused on two kinds of models, representation-based and interaction-based .Representation-based models convert sentences into hidden vec-tors, whereas interaction-based models match texts on word-level",
  "(')": ": The illustration of CAMA. The scale encoding module incorporates the usage of look-up embedding and the AutoDisencoder to effectively model categorical and numeric features, respectively. Furthermore, two pre-trained BERT encoders areemployed along with field-aware embeddings to capture token-level interactions within two distinct groups of text pairs. At ahigher level, a Transformer encoder is utilized to model field-level interactions among various feature groups. interactions. Because of the high cost of human marketing, there isa high requirement for B2B matching systems accuracy. Thus, weneed to model the interactions of the texts.Some researchers have studied multi-aspect text matching forNews Recommendation , Document Ranking , andDense Retrieval . For instance, Kong et al. rep-resented multiple aspects of a query using different embeddings.Shan et al. designed an attribute-guided representation learn-ing framework to couple the query and item representation learningtogether. This framework can also identify the most relevant itemfeatures for item representation. To facilitate text matching, wealso encode the scale features of companies. Some works have stud-ied to incorporate side information into the text-matching processfor Document Ranking and Recommender Systems .Wide&Deep combined a wide linear model and a DNN to captureboth sparse features and dense embeddings. Some works of EntityMatching have developed matching frameworks that can be poten-tially applied to our cloud solution matching problem .For example, HierGAT developed a hierarchical graph attentiontransformer that utilizes both self-attention and graph-attentionmechanisms.",
  "METHODOLOGY": "This study focuses on the matching of solutions and companiesin the B2B scenario, which holds significant commercial value buthas received limited attention in previous research. To addressthe challenges of this scenario, we first propose a HierarchicalMulti-field Matching framework to model the complex multi-fieldfeatures of solutions and companies. Specifically, by consideringthree aspects of the modeling process, i.e., scale features encod-ing, fine-grained token-level interaction, and field-level inter-groupinteraction, we can compute matching scores from different per-spectives. Furthermore, recognizing the issue of limited, incomplete,and sparse transaction data, we devise several data augmentationstrategies to generate supplemental solution-company data pairs.",
  "Problem Definition": "Before shedding light on our model, we first give a concise definitionof the problem we study. Our objective is to identify potentialbuyers (companies) to the sales teams based on the outcomes ofour framework. Specifically, we denote the solutions as and thecompanies as. For each solution , we need to rank based onthe matching scores between and every company , denoted as(,). As presented in , the fields of and are divided intothree groups: description texts (d, d), attribute texts (a, a), andscale features (s) (only has categorical and numerical featuresrepresenting its scale). We categorize the text features into twodistinct groups based on their meanings and structures: (1) Thedescription texts are typically long natural language texts consistingof general descriptions of and . On the other hand, attributetexts include keywords or tags that represent attributes of and .Due to the distinct nature of these two types of texts, their token-level interactions can interfere with each other (demonstrated in.2). (2) The text features of solutions and companies areheterogeneous. In other words, the features of and are not exactmatches. Thus, it is not feasible to treat each field as a group andperform field-to-field matching between and .The top-ranked company list of will be distributed to thesales team responsible for promoting the corresponding solution,who will subsequently contact these companies and pursue poten-tial sales opportunities. By leveraging the multi-field features, themodel aims to learn the patterns of matching between and andsubsequently rank the companies most likely to purchase as highas possible within the generated lists.",
  "Hierarchical Multi-field Matching": "In our B2B cloud solution matching scenario, we have identifieda significant and unexplored challenge: the modeling of complexmulti-field feature interactions. Specifically, the fields of solutionsand companies are comprised of two main kinds of features: scalefeatures and text features. Consequently, we devise distinct modelsto effectively capture and analyze these different types of features. 3.3.1Scale Encoding. Since only has scale features that representits scale, our goal here is to encode these features into a representa-tion rather than modeling interactions. Instead of using s directly,we encode it to better interact with the textual representations.Besides, s is comprised of both categorical (e.g., whether is listed)and numerical (e.g., registration capital) fields. Therefore, we en-code these two types of features separately and fuse them into aunified representation, as illustrated in the lower left part of .Suppose s contains categorical fields and numerical fields:s = [v1, v2, . . . , v;1, 2, . . . , ], where v is the one-hot vectorof the value of the -th categorical field, and is the scalar valueof the -th field of the numerical features.For the categorical fields, we apply a look-up embedding tech-nique to encode the one-hot vectors. Specifically, for the -th cate-gorical field, we obtain its embedding: e = E v, where E R sis the embedding matrix for the -th field to look-up, is the fieldsize, and s is the embedding size of all scale features.To handle the numerical fields, we employ an automatic embed-ding learning technique based on soft discretization (AutoDis ).The utilization of soft discretization within our end-to-end learn-ing framework allows for the optimization of this process. First,the scalar value is discretized into buckets by a two-layer neu-ral network with skip-connection h = Leaky_ReLU(w),v =Wh + h, where w R1 and W R are learnableparameters that automatically discretized into the projection outputs of buckets: v = [1,2, . . . , ], where is the pro-jection of scalar value on the -th bucket. This projection isthen normalized by the Softmax() function into a weight on thecorresponding bucket: = Softmax( ). Subsequently, a set of meta-embeddings ME R s is de-signed for the -th field. The softly discretized results v representsthe relevance between the -th field of the numeric features and thebuckets of meta-embeddings. Thus, we can leverage a weighted-average technique to aggregate the meta-embeddings and theircorresponding weights into a representation for each numericalfeature: enum= =1 ME .After the scale features are embedded into continuous vectors, weemploy a Multi-Layer Perceptron (MLP) to fuse them into a unifiedrepresentation cs Rs that captures the scale features of : cs =MLP[e1, e2, . . . , e; enum1, enum2, . . . , enum]. We can get a score byapplying a linear projection 1() to map this representation into a(scalar) score: scale(,) = 1(cs). 3.3.2Token-level Interaction. In this part, we attempt to modelthe token-level interactions of (,). Pre-trained language models,such as BERT , have gained significant popularity in varioustasks, including Recommender Systems and InformationRetrieval . To capture the fine-grained token-level interac-tions of (d, d) and (a, a), we leverage BERT as the underlyingencoder. We employ special tokens to concatenate the fields in thedescription texts, resulting in the following sequence:",
  "d = [CLS]d1 [SEP] . . .dsd [SEP][SEP]d1 . . .dcd [SEP][SEP],": "where is the number of text fields, and are the -th and-th fields that consist of many tokens of the solution and thecompany, respectively, [CLS] is the token used for representingthe sequence, [SEP] is the separator token. We append a [SEP]token after each field to indicate the end of a field and anotherone at the end of each sequence of fields. Moreover, to distinguishthe multiple tags in attribute texts (e.g., different industry tags),we further utilize [EOS] tokens to separate these tags beforeconcatenating them (as shown in the lower center part of ):",
  "a = [CLS]a1[SEP] . . .asa [SEP][SEP]a1 . . .aca [SEP][SEP],": "where a is the -th attribute field of the company and is thenumber of the tags contained in this field.Furthermore, we design a set of field-aware embeddings thathelp the encoders to distinguish different fields during the modelingof interactions. Specifically, for each text group, we initialize a field-aware embedding matrix FE R e, where is the numberof the fields and e is the size of BERTs word embeddings. The[EOS] tokens within and the [SEP] tokens after fields are alsoenhanced with corresponding field-aware embeddings (the FE of[CLS] is distinct from others). Consequently, the embedding ofeach token is comprised of both our field-aware embedding andBERT embedding (as shown in the lower right part of ): Xd =BERTd[CLS]FEd +eBERT( d), Xa = BERTa[CLS]FEa +eBERT( a),where eBERT() is the embedding layer of BERT.",
  "We can get two token-level matching scores by applying linearprojections 2() and 3() on Xd and Xa, respectively: desc(,) =2(Xd), attr(,) = 3(Xa)": "3.3.3Field-level Interaction. In the previous section, we have gath-ered information regarding token-level interactions within twodistinct text groups. However, although it has been observed thattoken-level interactions between different groups can have a detri-mental effect on model performance, we still aim to capture theinter-group interactions from a higher level, i.e., the field level. Asshown in the center part of , for each text field, we use theencoded output of the [SEP] token appended to it as its represen-tation. By doing so, we avoid using all tokens for the same reasonwe divide the features into two groups, i.e., preventing fine-grainedinterference. Additionally, we use the encoded scale representa-tion cs to facilitate the modeling of field-level interactions. Thisis because the scale of a company can influence how a solutioninteracts with it. For instance, interactions with smaller companiesmay place more emphasis on their copyright works due to theirmore focused business nature.In order to comprehensively model these representations andcapture the interactions among fields, we utilize the Transformerencoder, as proposed in the Transformer architecture. TheTransformer encoder effectively models the aforementioned repre-sentations in the following manner:",
  "sa1, . . . , sasa, ca1, . . . , caca,(1)": "where Trm() is the Transformer encoder which consists of Trans-former layers, s and c are the encoded outputs of [SEP] appendedto text fields, () is a linear projector to map cs into the latentspace of the text field representations, and p Re is a randomlyinitialized vector used for pooling.We can get a field-level matching score by applying a linearprojection 4(): field(,) = 4(Y). 3.3.4Optimization. Since the label of whether a company buys asolution is binary (0/1), we use a cross-entropy loss to optimize ourmatching model. We formulate the loss for the matching scores ofour hierarchical models, i.e., {P} = {scale, desc, attr, field}:",
  "Text Matching Enhancement": "In our study, we identify several challenges of the transaction datain our scenario. Firstly, the training data is limited attributed tothe high cost of human resources required for promoting solutions.Additionally, the data is incomplete as some solutions and com-panies lack specific tokens or fields. Moreover, the data is sparse,meaning that many potential pairs have not been discovered andrecorded. To mitigate these challenges, we attempt to enhance thegeneralization and robustness of the BERT encoders by pre-trainingthem. As shown in , we employ a contrastive objective, which",
  "aims to bring together the representations of augmented similarsequences while pushing away different ones": "3.4.1Data Augmentation Strategies. The similar sequences are gen-erated from the inputs of BERT encoders (). We design three dataaugmentation strategies to generate additional data to complementthe limited transaction data in our scenario.(1) Token Masking. Some existing works in Natural LanguageProcessing have employed token-level augmentation tech-niques to enhance the robustness of sentence representations. Thisapproach enables our BERT encoders to acquire more robust andgeneralized representations with incomplete data that may lackspecific tokens, by reducing reliance on specific tokens.To begin, we represent the token-level input sequence as atoken sequence: = [1,2, . . . , ], where denotes the totalnumber of tokens. Next, we randomly mask a proportion t ofthe tokens in : m = [1,2, . . . ,T], where is the token tobe masked and T = t . For each token , if it ismarked to be masked in m, we will replace it with a special token[token_mask], which is similar to [MASK] in BERT .(2) Field Masking. To obtain robust representations of incom-plete data pairs, our BERT encoder models should avoid relying onspecific data fields while encoding the whole matching sequences.By contrasting sequences that are augmented through the maskingof certain fields with other sequences, we can facilitate the learningof text matching for incomplete data within our BERT models.We represent as a sequence of data fields: = [1, 2, . . . , ],where is the total number of fields. We randomly mask a ratio f ofthe fields: m = [ 1, 2, . . . , T], where represents the token tobe masked and T = f . For , if it is marked to be maskedin m, we will replace it with a special token [field_mask].(3) Company Replacing. To address the data sparsity problem,we design a rule-based method to identify similar companies1 thatcan replace the company in the positive pairs. Specifically, for each",
  "sequence of positive pairs (,), we find one similar company": "to generate a pair (,). Because the company name serves as theprimary identifier, capturing information about the industry andbusiness nature, we only use the name to identify similar companies.This simplification aims to expedite computations and diminish thecomplexity of the rule. Our rule-based method randomly selects oneof the five most similar companies of company to be designatedas company . The similarity between companies is obtained bycalculating the semantic similarity of company names using thesentence-transformers package . 3.4.2Contrastive Learning Objective. Inspired by several studiesin Information Retrieval , we adopt a contrastivelearning approach to enhance our model for text matching. Inorder to carry out contrastive learning for our task, we define aloss function specifically for the contrastive prediction task. Thistask involves identifying similar augmented pairs within the set{X}, which is constructed by randomly augmenting the original se-quences of a minibatch. Suppose a minibatch contains sequences,we randomly employ two of our augmentation strategies to gen-erate {X} comprising 2 sequences. Among these sequences, thetwo sequences generated from the same solution-company pair areconsidered a similar (positive) pair, whereas the remaining 2( 1)augmented ones serve as negative samples {X}. We formulatethe contrastive learning loss for a positive pair (X, X) as follows:",
  "EXPERIMENTAL SETUP4.1Dataset": "To evaluate the effectiveness of our proposed method, we conductedextensive experiments on a real-world dataset. We first sampledfrom the real-world data and constructed an offline dataset B2BSolution Matching (BSM). BSM contains three parts: solution data,company data, and transaction data. The solution dataset stores thetext information of 27 solutions, such as solution names, detaileddescriptions, and tags of their industries. The company dataset pro-vides detailed company profiles for 533,784 companies, includingcompany names, company registration capital, and their businessscopes, etc. The transaction data are derived from the marketingfeedback of the sales teams online. Whether a company buys a so-lution is relevant to not only its suitability but also many subjectivefactors (e.g., the sales teams pitch). Consequently, unsuccessfulpurchases are often noise and disregarded.We constructed the data format as a pairwise form (solution,company) along with their corresponding label from the abovemen-tioned raw dataset. Then the constructed dataset is split into threeparts, where 70% is used for training, 10% is used for validatingand the remaining 20% is for testing. For each positive sample, we",
  "Evaluation Metrics": "We comprehensively evaluate our proposed method through bothoffline and online evaluations.(1) Offline Metrics. We use Mean Average Precision (MAP),Area Under Curve (AUC), Precision at (P@, = 10, 100, 500),and Recall at position (R@, = 10, 100, 500) as offline metrics.The results are calculated by averaging across different solutions.(2) Online Metrics. We verify the performance of our proposedmodel with Conversion Rate (CVR) as online metrics. CVR is definedas: CVR = # Purchase/# Promoted, # Purchase denotes the numberof companies that purchase the solutions and # Promoted means thenumber of companies that the sales teams market their solutions.To evaluate CAMAs effectiveness, we will compare its performanceto a previous online model of Huawei.",
  "Baseline Models": "We compare our CAMA with two kinds of baselines:(1) Text Matching Models only use the interactions of textsto get the matching score. Sentence-BERT (SBERT) encodesthe solution and company with two BERTs separately and takesthe cosine similarity of these representations as the matching score. MADR is also a representation-based model that designsaspect learning tasks to extract representations of different aspectsfrom texts and then fuses them by calculating the weighted sum. Concatenating-BERT (CBERT) is an interaction-based model thatmodels the token-level interaction of the solution and companywithout identifying different text fields.(2) Side-aware Matching Models have frameworks that can en-code side information (scale) along with the text interaction moduleto get the representations for matching. Wide & Deep (W&D) combines a wide linear model and a DNN. For its implementation,we utilize a two-tower BERT model to get text representationsand fit them into the framework with scale features. MCN conducts an element-wise matching of solutions and companies,and fuses the matching output with scale features to learn inter-active information. For the encoding modules in MCN, we use atwo-tower BERT to get the textual representations and our scaleencoding module to model scale features. We also apply some entitymatching frameworks to our solution matching problem by treatingour text and numeric fields as different entities. DeepMatcher(DM) uses the GRU-RNN model to learn the attribute embed-dings of entities, which are aggregated for matching. Ditto applies pre-trained Transformer-based language models and op-timization techniques to perform the sequence-pair classification",
  "AUC0.71350.72380.77210.80930.81410.81360.82060.84510.84800.8528": "problem. HierGAT combines Transformer attention withhierarchical graph attention to effectively learn entity embeddings.(3) Hybrid Solution-Company Matching Framework (HSCM).This is a previously online yet unpublished framework used byHuawei Cloud. Based on the number of solutions in transaction data,the algorithm categorizes solutions into three groups. It devisesunsupervised, semi-supervised, and supervised models for each cat-egory, respectively. It is a well-designed classification frameworkbased on the classical Gradient Boosting Decision Tree (GBDT)model and BERT-encoded features. Due to the companys confiden-tiality policy, it is not feasible for us to provide further elaboration.However, HCSM has the following drawbacks: (1) The frameworkmaintains a distinct model for each solution, leading to a significantwaste of resources. (2) Because of its naive structure, the results ofthe matching are unsatisfactory.",
  "Implementation Details": "We use BERT provided by Huggingface as the token-level encoder2.The size of scale embedding s is set as 64, and the parameters ofAutoDis encoder are the same as its original paper . We use = 6 Transformer layers as the field-level encoder. During the textmatching enhancement process, we set the temperatures as 0.2 and0.05 for pre-training description BERT () and attribute BERT (),respectively. The token mask ratio and field mask ratio are tunedand established as 0.2 and 0.5, respectively, for both BERTs. Thelearning rates are set as 3e-5 for the token-level encoding module,5e-4 for the scale encoding module, and 5e-5 for both field-levelencoding and pre-training. The model is trained with a batch sizeof 32 for four epochs using four Tesla P100 16G GPUs.",
  "Overall Results": "5.1.1Offline Results. Experimental results on BSM are presentedin . The results show that the performance of CAMA is sig-nificantly better than baseline models. We can make the followingobservations based on the results:(1) Our proposed method CAMA outperforms all baselines.The offline results demonstrate that CAMA performs significantlybetter than all baseline models. This indicates that our hierarchi-cal multi-field matching framework and contrastive pre-trainingtechnique are effective for matching solutions and companies.(2) Side-aware models generally perform better than themodels purely based on text matching. For example, the weakside-aware model W&D still outperforms the strong text-matchingmodel CBERT. This demonstrates the necessity of modeling thescale features. Moreover, our model still performs better than allside-aware baselines, which demonstrates the effectiveness of ourhierarchical multi-field matching framework again. 5.1.2Online Deployment Results. To assess our proposed model,CAMA, we implemented it in a real-world system over a six-monthperiod. Our method, when given a solution slated for sale, generatesa Top-K ranked list. The sales team for the solution then reviews thislist, selecting companies based on the provided matching scores andtheir own expertise. Following this, salesmen contact the chosencompanies and gather feedback to compute the Conversion Rate(CVR). Due to confidentiality constraints, we cannot reveal specificCVR figures. However, we present the relative CVR increase ratiosduring the evaluation period. Our approach involved having thesame sales teams process both the company lists generated byour model and those produced by the pre-online model HSCM.By comparing the CVR results from these two models, CAMAdemonstrated a 29.99% improvement over the HSCM model duringthe same timeframe. Moreover, we can make these observations:(1) Our proposed method CAMA achieves a superior per-formance over HSCM. The performance of CAMA surpasses theprevious online model HSCM by 29.99%, which demonstrates itsgreat commercial value.(2) Our proposed method CAMA is not only better than theprevious framework HSCM but also easy to deploy and main-tain. The online deployment results validate the effectiveness ofthe hierarchical multi-field matching structure and the contrastivepre-trained method compared with the improvement over HSCM.",
  "Ablation Studies": "To evaluate the effectiveness of each component, we perform thefollowing ablation studies on the BSM dataset: CAMA w/o. Description Texts is CAMA without the inter-actions of Description Texts, i.e., without desc and the correspond-ing field-level representations. CAMA w/o. Attribute Texts isCAMA without the interactions of Attribute Texts. CAMA w/o.Text Grouping is CAMA without Text Grouping, i.e., we do notdistinguish two types of texts and only compute one matchingscore ({desc, attr} text ). CAMA w/o. Field Embeddings isCAMA without the Field-aware Embeddings. CAMA w/o. ScaleEncoding is CAMA without the Scale Encoding component. CAMA w/o. Field-level Interaction is CAMA without the Field-level Interaction module, i.e., field. CAMA w/o. Pre-training isCAMA without the pre-training of the token-level BERT encoders.The results in clearly demonstrate that the full modeloutperforms all ablated models, indicating the effectiveness of ourcomponents. Moreover, we can make the following conclusions:(1) Both the interactions of two text groups can help thematching process. Specifically, CAMAs performance decreasesby about 28.18% and 13.59% in terms of MAP after abandoningDescription and Attribute Texts, respectively. This indicates theimportance of modeling both groups of texts.(2) The token-level interactions between two text groupsinterfere with each other. Our models performance decreases byabout 15.09% in terms of Rec@10 after we combine two text groupsand do not distinguish them in the token-level interaction module.(3) Identifying different text fields with field-aware em-beddings is effective. The decrease of CAMA after discardingField-aware Embeddings demonstrates its effectiveness.(4) Modeling the scale of companies can facilitate our hi-erarchical textual interactions. This decrease of CAMA afterabandoning Scale Encoding meets our observation in .1,that company scale plays an important role in modeling the match-ing between solutions and companies.(5) Field-level interaction is effective for capturing inter-group matching signals. The performance of CAMA decreasesafter abandoning the high-level interaction among fields of differentgroups. This indicates the effectiveness of our field-level interaction.",
  "Influence of Data Augmentation Strategies": "In our text matching enhancement module, we propose three dataaugmentation strategies and a contrastive learning objective topre-train the BERT encoders. To investigate the influence of thesestrategies, we compare CAMA with the ablated models and showthe results in . We can make the following observations:(1) Token masking strategy can help pre-train the BERTencoders. After abandoning the sequences generated by maskingspecific tokens, the performance of our model drops by about 4.86%in terms of MAP. This shows that the available transaction dataare deficient for training a robust model, and our token maskingstrategy can help CAMA get more generalized representations.(2) Pairs augmented by field masking can mitigate theproblem of incomplete data. There are some solutions and com-panies lack specific data fields. The performance of CAMA w/o.Field Masking validates this problem of our data and demonstratesthat our strategy can help mitigate this challenge.(3) Replacing the company in a matching pair with a sim-ilar company can generate data pairs for addressing datasparsity. The performance of our model decreases by about 6.65%in terms of Rec@10 after discarding Company Replacing, whichdemonstrates this technique can complement the sparse data.",
  "Influence of Hyperparameters": "5.4.1The Embedding Size of Company Scale. To model the inter-actions between scale features and text embeddings, we have toembed the scale features into a vector cs Rs. We tune the scaleembedding size s in the range with the step of 16 onthe validation set and present the performances of CAMA withdifferent s on the test set. Due to the space limitation, we presentthe performance of MAP on BSM and only show the results of fivetuned values (the henceforth displays will follow the same policy).As shown in the left part of , the performances increase to theoptimal value and then drop. This pattern indicates a trade-off: Ifthe embedding size is too low, the scale embedding can not encodesufficient information. However, it may introduce noise into CAMAif the embedding size is too high.",
  ": Performance of CAMA on the BSM dataset with different hyperparameters": "negative samples. If it is set too low, our model will concentrateon the negative pairs that are hard to distinguish. However, a highvalue of will make CAMA treat all negative samples equally. Wetune in the range [0.05, 0.5] with the step of 0.05 and in therange [0.01, 0.2] with the step of 0.01. The results presented in themiddle part of demonstrate the trade-off of this hyperparam-eter and validate our choices. 5.4.3The Mask Ratios. In our data augmentation strategies, we usemask ratios t and f to control the number of tokens/fields we maskin Token Masking and Field Masking strategies, respectively. Wetune these two ratios for pre-training two BERTs and find that thepatterns of the two BERTs are the same, thus we present the tuningresults of each ratio for both BERTs in the same figure. Specifically,we tune t and f in the range [0.0, 1.0] with the step of 0.05. Theresults shown in the right part of indicate there is also atrade-off for the masking ratios. If we set the masking ratios toohigh, the augmented pairs may not be similar to the original pair.However, too low masking ratios will introduce little knowledgeinto our pre-training process, resulting in insufficient pre-training.",
  "CONCLUSION": "In this work, we study a valuable yet understudied problem of B2Bsolution matching and identify two key challenges in this scenario.Initially, we propose a hierarchical multi-field matching frameworkto model the interactions between the complex multi-field featuresof solutions and companies. Subsequently, three data augmentationstrategies and a contrastive learning objective are proposed to dealwith the limited, incomplete, and sparse transaction data. Extensiveexperiments on a real-world dataset BSM demonstrate the effec-tiveness of CAMA. The deployment of our framework on HuaweiCloud validates the feasibility and effectiveness of our frameworkin an online scenario. Considering the generalizability of our frame-work, it can also be applied to other B2B matching scenarios thatencounter similar challenges. Zhicheng Dou is the corresponding author. This work was sup-ported by National Natural Science Foundation of China No. 62272467,the fund for building world-class universities (disciplines) of Ren-min University of China, and Public Computing Cloud, RenminUniversity of China. The work was partially done at the Engineer-ing Research Center of Next-Generation Intelligent Search andRecommendation, MOE, and Beijing Key Laboratory of Big DataManagement and Analysis Methods.",
  "Sundus Ayyaz, Usman Qamar, and Raheel Nawaz. 2018. HCF-CRS: A HybridContent based Fuzzy Conformal Recommender System for providing recommen-dations with confidence. PloS one 13, 10 (2018), e0204849": "Saeid Balaneshinkordan, Alexander Kotov, and Fedor Nikolaev. 2018. AttentiveNeural Architecture for Ad-hoc Structured Document Retrieval. In Proceedingsof the 27th ACM International Conference on Information and Knowledge Man-agement, CIKM 2018, Torino, Italy, October 22-26, 2018, Alfredo Cuzzocrea, JamesAllan, Norman W. Paton, Divesh Srivastava, Rakesh Agrawal, Andrei Z. Broder,Mohammed J. Zaki, K. Seluk Candan, Alexandros Labrinidis, Assaf Schuster, andHaixun Wang (Eds.). ACM, 11731182. Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew M. Dai, Rafal Jzefowicz,and Samy Bengio. 2016. Generating Sentences from a Continuous Space. InProceedings of the 20th SIGNLL Conference on Computational Natural LanguageLearning, CoNLL 2016, Berlin, Germany, August 11-12, 2016, Yoav Goldberg andStefan Riezler (Eds.). ACL, 1021. Haonan Chen, Zhicheng Dou, Kelong Mao, Jiongnan Liu, and Ziliang Zhao.2024. Generalizing Conversational Dense Retrieval via LLM-Cognition DataAugmentation. CoRR abs/2402.07092 (2024). arXiv:2402.07092",
  "Haonan Chen, Zhicheng Dou*, Qiannan Zhu, Xiaochen Zuo, and Ji-Rong Wen.2022. Integrating Representation and Interaction for Context-Aware DocumentRanking. ACM Trans. Inf. Syst. (2022)": "Haonan Chen, Zhicheng Dou, Yutao Zhu, Zhao Cao, Xiaohua Cheng, and Ji-RongWen. 2022. Enhancing User Behavior Sequence Modeling by Generative Tasksfor Session Search. In Proceedings of the 31st ACM International Conference onInformation & Knowledge Management. 180190. Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui Jiang, and Diana Inkpen.2017. Enhanced LSTM for Natural Language Inference. In Proceedings of the55th Annual Meeting of the Association for Computational Linguistics, ACL 2017,Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers, Regina Barzilayand Min-Yen Kan (Eds.). Association for Computational Linguistics, 16571668. Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, RohanAnil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, and Hemal Shah.2016. Wide & Deep Learning for Recommender Systems. In Proceedings of the 1stWorkshop on Deep Learning for Recommender Systems (DLRS 2016). Associationfor Computing Machinery, New York, NY, USA, 710. Andrew M. Dai and Quoc V. Le. 2015. Semi-supervised Sequence Learning.In Advances in Neural Information Processing Systems 28: Annual Conferenceon Neural Information Processing Systems 2015, December 7-12, 2015, Montreal,Quebec, Canada. 30793087. Shitong Dai, Jiongnan Liu, Zhicheng Dou, Haonan Wang, Lin Liu, Bo Long, andJi-Rong Wen. 2023. Contrastive Learning for User Sequence Representation inPersonalized Product Search. In Proceedings of the 29th ACM SIGKDD Conferenceon Knowledge Discovery and Data Mining, KDD 2023, Long Beach, CA, USA, August6-10, 2023, Ambuj Singh, Yizhou Sun, Leman Akoglu, Dimitrios Gunopulos,Xifeng Yan, Ravi Kumar, Fatma Ozcan, and Jieping Ye (Eds.). ACM, 380389. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding. InProceedings of the 2019 Conference of the North American Chapter of the Associa-tion for Computational Linguistics: Human Language Technologies, NAACL-HLT2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), JillBurstein, Christy Doran, and Thamar Solorio (Eds.). Association for Computa-tional Linguistics, 41714186. Markus Eisenbach, Jannik Lbberstedt, Dustin Aganian, and Horst-Michael Gross.2023. A Little Bit Attention Is All You Need for Person Re-Identification. In IEEEInternational Conference on Robotics and Automation, ICRA 2023, London, UK,May 29 - June 2, 2023. IEEE, 75987605.",
  "Shagufta Henna and Shyam Krishnan Kalliadan. 2021. Enterprise Analytics usingGraph Database and Graph-based Deep Learning. CoRR abs/2108.02867 (2021).arXiv:2108.02867": "Yupeng Hou, Shanlei Mu, Wayne Xin Zhao, Yaliang Li, Bolin Ding, and Ji-Rong Wen. 2022. Towards Universal Sequence Representation Learning forRecommender Systems. In KDD 22: The 28th ACM SIGKDD Conference onKnowledge Discovery and Data Mining, Washington, DC, USA, August 14 - 18,2022, Aidong Zhang and Huzefa Rangwala (Eds.). ACM, 585593. Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry P.Heck. 2013. Learning deep structured semantic models for web search usingclickthrough data. In 22nd ACM International Conference on Information andKnowledge Management, CIKM13, San Francisco, CA, USA, October 27 - November1, 2013, Qi He, Arun Iyengar, Wolfgang Nejdl, Jian Pei, and Rajeev Rastogi (Eds.).ACM, 23332338. Zhaoheng Huang, Zhicheng Dou, Yutao Zhu, and Zhengyi Ma. 2022. MCP: Self-supervised Pre-training for Personalized Chatbots with Multi-level ContrastiveSampling. In Findings of the Association for Computational Linguistics: EMNLP2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, Yoav Goldberg, Zor-nitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics,10301042. Omar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective PassageSearch via Contextualized Late Interaction over BERT. In Proceedings of the 43rdInternational ACM SIGIR conference on research and development in InformationRetrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020, Jimmy X. Huang,Yi Chang, Xueqi Cheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, and YiqunLiu (Eds.). ACM, 3948. Weize Kong, Swaraj Khadanga, Cheng Li, Shaleen Kumar Gupta, MingyangZhang, Wensong Xu, and Michael Bendersky. 2022. Multi-Aspect Dense Retrieval.In KDD 22: The 28th ACM SIGKDD Conference on Knowledge Discovery and DataMining, Washington, DC, USA, August 14 - 18, 2022, Aidong Zhang and HuzefaRangwala (Eds.). ACM, 31783186.",
  "Wenhao Lu, Jian Jiao, and Ruofei Zhang. 2020. TwinBERT: Distilling Knowledgeto Twin-Structured BERT Models for Efficient Retrieval. CoRR abs/2002.06275(2020). arXiv:2002.06275": "Kelong Mao, Zhicheng Dou, Fengran Mo, Jiewen Hou, Haonan Chen, and HongjinQian. 2023. Large Language Models Know Your Contextual Search Intent: APrompting Framework for Conversational Search. In Findings of the Associa-tion for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023,Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for ComputationalLinguistics, 12111225. Kelong Mao, Zhicheng Dou, and Hongjin Qian. 2022. Curriculum ContrastiveContext Denoising for Few-shot Conversational Dense Retrieval. In SIGIR 22:The 45th International ACM SIGIR Conference on Research and Development inInformation Retrieval, Madrid, Spain, July 11 - 15, 2022, Enrique Amig, PabloCastells, Julio Gonzalo, Ben Carterette, J. Shane Culpepper, and Gabriella Kazai",
  "(Eds.). ACM, 176186": "Sidharth Mudgal, Han Li, Theodoros Rekatsinas, AnHai Doan, Youngchoon Park,Ganesh Krishnan, Rohit Deep, Esteban Arcaute, and Vijay Raghavendra. 2018.Deep Learning for Entity Matching: A Design Space Exploration. In Proceedingsof the 2018 International Conference on Management of Data, SIGMOD Conference2018, Houston, TX, USA, June 10-15, 2018, Gautam Das, Christopher M. Jermaine,and Philip A. Bernstein (Eds.). ACM, 1934. Hamid Palangi, Li Deng, Yelong Shen, Jianfeng Gao, Xiaodong He, Jianshu Chen,Xinying Song, and Rabab K. Ward. 2016. Deep Sentence Embedding UsingLong Short-Term Memory Networks: Analysis and Application to InformationRetrieval. IEEE ACM Trans. Audio Speech Lang. Process. 24, 4 (2016), 694707.",
  "Charuta Pande, Hans Friedrich Witschel, and Andreas Martin. 2021. New HybridTechniques for Business Recommender Systems. CoRR abs/2109.13922 (2021).arXiv:2109.13922": "Ankur P. Parikh, Oscar Tckstrm, Dipanjan Das, and Jakob Uszkoreit. 2016. ADecomposable Attention Model for Natural Language Inference. In Proceedingsof the 2016 Conference on Empirical Methods in Natural Language Processing,EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, Jian Su, Xavier Carreras,and Kevin Duh (Eds.). The Association for Computational Linguistics, 22492255. Zhen Qin, Zhongliang Li, Michael Bendersky, and Donald Metzler. 2020. MatchingCross Network for Learning to Rank in Personal Search. In WWW 20: TheWeb Conference 2020, Taipei, Taiwan, April 20-24, 2020, Yennun Huang, IrwinKing, Tie-Yan Liu, and Maarten van Steen (Eds.). ACM / IW3C2, 28352841. Nils Reimers and Iryna Gurevych. 2019.Sentence-BERT: Sentence Embed-dings using Siamese BERT-Networks. In Proceedings of the 2019 Conferenceon Empirical Methods in Natural Language Processing and the 9th InternationalJoint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, HongKong, China, November 3-7, 2019, Kentaro Inui, Jing Jiang, Vincent Ng, andXiaojun Wan (Eds.). Association for Computational Linguistics, 39803990. Steffen Rendle. 2010. Factorization Machines. In ICDM 2010, The 10th IEEEInternational Conference on Data Mining, Sydney, Australia, 14-17 December 2010,Geoffrey I. Webb, Bing Liu, Chengqi Zhang, Dimitrios Gunopulos, and XindongWu (Eds.). IEEE Computer Society, 9951000.",
  "Qusai Shambour and Jie Lu. 2015. An effective recommender system by unifyinguser and item trust information for B2B applications. J. Comput. Syst. Sci. 81, 7(2015), 11101126": "Hongyu Shan, Qishen Zhang, Zhongyi Liu, Guannan Zhang, and ChenliangLi. 2023. Beyond Two-Tower: Attribute Guided Representation Learning forCandidate Retrieval. In Proceedings of the ACM Web Conference 2023, WWW 2023,Austin, TX, USA, 30 April 2023 - 4 May 2023, Ying Ding, Jie Tang, Juan F. Sequeda,Lora Aroyo, Carlos Castillo, and Geert-Jan Houben (Eds.). ACM, 31733181. Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang.2019. BERT4Rec: Sequential Recommendation with Bidirectional Encoder Repre-sentations from Transformer. In Proceedings of the 28th ACM International Con-ference on Information and Knowledge Management, CIKM 2019, Beijing, China,November 3-7, 2019, Wenwu Zhu, Dacheng Tao, Xueqi Cheng, Peng Cui, Elke A.Rundensteiner, David Carmel, Qi He, and Jeffrey Xu Yu (Eds.). ACM, 14411450. Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang,and Xing Xie. 2019. Neural News Recommendation with Attentive Multi-ViewLearning. In Proceedings of the Twenty-Eighth International Joint Conference onArtificial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019, Sarit Kraus(Ed.). ijcai.org, 38633869.",
  "Dianshuang Wu, Guangquan Zhang, and Jie Lu. 2015. A Fuzzy Preference Tree-Based Recommender System for Personalized Business-to-Business E-Services.IEEE Trans. Fuzzy Syst. 23, 1 (2015), 2943": "Jingyuan Yang, Chuanren Liu, Mingfei Teng, Ji Chen, and Hui Xiong. 2018. AUnified View of Social and Temporal Modeling for B2B Marketing CampaignRecommendation. IEEE Trans. Knowl. Data Eng. 30, 5 (2018), 810823. Jingyuan Yang, Chuanren Liu, Mingfei Teng, Hui Xiong, March Liao, and VivianZhu. 2015. Exploiting Temporal and Social Factors for B2B Marketing CampaignRecommendations. In 2015 IEEE International Conference on Data Mining, ICDM2015, Atlantic City, NJ, USA, November 14-17, 2015, Charu C. Aggarwal, Zhi-HuaZhou, Alexander Tuzhilin, Hui Xiong, and Xindong Wu (Eds.). IEEE ComputerSociety, 499508. Junhan Yang, Zheng Liu, Shitao Xiao, Chaozhuo Li, Defu Lian, Sanjay Agrawal,Amit Singh, Guangzhong Sun, and Xing Xie. 2021. GraphFormers: GNN-nestedTransformers for Representation Learning on Textual Graph. In Advances in Neu-ral Information Processing Systems 34: Annual Conference on Neural Information",
  "Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, MarcAurelioRanzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wort-man Vaughan (Eds.). 2879828810": "Dezhong Yao, Yuhong Gu, Gao Cong, Hai Jin, and Xinqiao Lv. 2022. EntityResolution with Hierarchical Graph Attention Networks. In SIGMOD 22: Inter-national Conference on Management of Data, Philadelphia, PA, USA, June 12 - 17,2022, Zachary G. Ives, Angela Bonifati, and Amr El Abbadi (Eds.). ACM, 429442. Jing Yao, Zheng Liu, Junhan Yang, Zhicheng Dou, Xing Xie, and Ji-Rong Wen.2023. CDSM: Cascaded Deep Semantic Matching on Textual Graphs LeveragingAd-hoc Neighbor Selection. ACM Trans. Intell. Syst. Technol. 14, 2 (2023), 32:132:24. Wenpeng Yin and Hinrich Schtze. 2015. MultiGranCNN: An Architecture forGeneral Matching of Text Chunks on Multiple Levels of Granularity. In Proceed-ings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing of theAsian Federation of Natural Language Processing, ACL 2015, July 26-31, 2015, Bei-jing, China, Volume 1: Long Papers. The Association for Computer Linguistics,6373. Hamed Zamani, Bhaskar Mitra, Xia Song, Nick Craswell, and Saurabh Tiwary.2018. Neural Ranking Models with Multiple Document Fields. In Proceedingsof the Eleventh ACM International Conference on Web Search and Data Mining,WSDM 2018, Marina Del Rey, CA, USA, February 5-9, 2018, Yi Chang, ChengxiangZhai, Yan Liu, and Yoelle Maarek (Eds.). ACM, 700708. Qi Zhang, Qinglin Jia, Chuyuan Wang, Jingjie Li, Zhaowei Wang, and Xiuqiang He.2021. AMM: Attentive Multi-field Matching for News Recommendation. In SIGIR21: The 44th International ACM SIGIR Conference on Research and Developmentin Information Retrieval, Virtual Event, Canada, July 11-15, 2021, Fernando Diaz,Chirag Shah, Torsten Suel, Pablo Castells, Rosie Jones, and Tetsuya Sakai (Eds.).ACM, 15881592.",
  "Xuirui Zhang and Hengshan Wang. 2005. Study on recommender systems forbusiness-to-business electronic commerce. Communications of the IIMA 5, 4(2005), 8": "Jason Zhu, Yanling Cui, Yuming Liu, Hao Sun, Xue Li, Markus Pelger, Tianqi Yang,Liangjie Zhang, Ruofei Zhang, and Huasha Zhao. 2021. TextGNN: ImprovingText Encoder via Graph Neural Network in Sponsored Search. In WWW 21: TheWeb Conference 2021, Virtual Event / Ljubljana, Slovenia, April 19-23, 2021, JureLeskovec, Marko Grobelnik, Marc Najork, Jie Tang, and Leila Zia (Eds.). ACM /IW3C2, 28482857. Yutao Zhu, Jian-Yun Nie, Zhicheng Dou, Zhengyi Ma, Xinyu Zhang, Pan Du,Xiaochen Zuo, and Hao Jiang. 2021. Contrastive Learning of User BehaviorSequence for Context-Aware Document Ranking. In CIKM 21: The 30th ACMInternational Conference on Information and Knowledge Management, VirtualEvent, Queensland, Australia, November 1 - 5, 2021. ACM, 27802791. Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chen-long Deng, Haonan Chen, Zhicheng Dou, and Ji-Rong Wen. 2023. Large Lan-guage Models for Information Retrieval: A Survey. CoRR abs/2308.07107 (2023).arXiv:2306.07401"
}