{
  "Jiawei HanUniversity of Illinois": "ABSTRACTInstead of relying on human-annotated training samples to builda classifier, weakly supervised scientific paper classification aimsto classify papers only using category descriptions (e.g., categorynames, category-indicative keywords). Existing studies on weaklysupervised paper classification are less concerned with two chal-lenges: (1) Papers should be classified into not only coarse-grainedresearch topics but also fine-grained themes, and potentially intomultiple themes, given a large and fine-grained label space; and(2) full text should be utilized to complement the paper title andabstract for classification. Moreover, instead of viewing the entirepaper as a long linear sequence, one should exploit the structuralinformation such as citation links across papers and the hierarchyof sections and paragraphs in each paper. To tackle these chal-lenges, in this study, we propose FuTex, a framework that uses thecross-paper network structure and the in-paper hierarchy structureto classify full-text scientific papers under weak supervision. Anetwork-aware contrastive fine-tuning module and a hierarchy-aware aggregation module are designed to leverage the two typesof structural signals, respectively. Experiments on two benchmarkdatasets demonstrate that FuTex significantly outperforms compet-itive baselines and is on par with fully supervised classifiers thatuse 1,000 to 60,000 ground-truth training samples.",
  "Code and Datasets are available at": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 23, August 610, 2023, Long Beach, CA, USA 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0103-0/23/08...$15.00 ACM Reference Format:Yu Zhang, Bowen Jin, Xiusi Chen, Yanzhen Shen, Yunyi Zhang, Yu Meng,and Jiawei Han. 2023. Weakly Supervised Multi-Label Classification of Full-Text Scientific Papers. In Proceedings of the 29th ACM SIGKDD Conferenceon Knowledge Discovery and Data Mining (KDD 23), August 610, 2023, LongBeach, CA, USA. ACM, New York, NY, USA, 12 pages. 1INTRODUCTIONWeakly supervised text classification aims to classifytext documents into a set of pre-defined categories without relyingon any human-labeled training documents. Instead, the classifierseeks help from various formats of weak supervision such as cat-egory names , a few category-indicative keywords, and category descriptions . This setting significantlyalleviates the burden of manual annotations, which is particularlyhelpful in some real applications such as scientific paper classifica-tion, where annotations need to be acquired from domain experts.Although existing studies on weakly supervised text classifi-cation have applied their proposed methods to scientific paperdatasets such as arXiv and DBLP , they are lessconcerned with the following two challenges in practice.A Large and Fine-Grained Label Space. One major goal of sci-entific paper classification is to help researchers track and analyzeacademic information and resources. To facilitate this goal, papersshould be classified into not only coarse-grained research fields(e.g., Machine Learning and Public Health) but also fine-grainedthemes (e.g., Large Language Models and Deltacoronavirus).Note that in a large and fine-grained label space, most papers arenaturally relevant to multiple themes. However, most existing stud-ies under the weakly supervised setting focus on classifying papersat a coarse level with 5 to 50 categories and assume each documentis relevant to only one category (or a single path from the root to aleaf if categories form a hierarchy). As far as we know, MICoL is a pioneering work that considers weakly supervised multi-labelclassification with more than 10,000 categories. Nevertheless, itsaccuracy is hampered by using limited information such as papertitles and abstracts only, which will be discussed below.The Usage of Paper Full Texts. A papers title and abstract, al-though summarized to cover its major topics, cannot capture allfine-grained aspects. For example, technique-related labels may beintroduced in more detail in the Method section; downstreamtasks may be explained in the Experiments section. To find more",
  ": The cross-paper network structure and in-paperhierarchy structure associated with scientific papers": "labels relevant to a paper, it becomes necessary to leverage its fulltext. Intuitively, when we aim to utilize paper full texts, the firstchallenge is to deal with long text. Indeed, if we check the twodatasets, MAG-CS and PubMed, used by MICoL , the averagefull-paper length exceeds 4,000 words. In comparison, most pre-trained language models (PLMs) such as BERT and SciBERT can take an input sequence with at most 512 tokens. Therefore,those PLM-based weakly supervised text classifiers cannot be directly applied to full-text scientific papers.More importantly, we would like to argue that classifying full-text papers is beyond the problem of dealing with long text. Asshown in , we adopt Longformer , which can take atmost 4,096 tokens, to classify papers in MAG-CS and PubMed underthe weakly supervised setting. (For more experiment details such ashow Longformer is used, please refer to .1.) We control themaximum input sequence length of Longformer from 512 to 4,096.Surprisingly, the more tokens Longformer takes (i.e., the more infor-mation from the full text that Longformer considers), the lower theclassification precision is. This observation has two implications:First, full texts are noisy and should not be treated the same asabstracts. Abstracts should still play a leading role in classification,while full texts provide auxiliary information . Second, to bet-ter exploit full text, we need to consider the structures in scientificpapers. The major design that enables Longformer to take a longerinput sequence is to sparsify the fully connected attention in Trans-former , where each input token only interacts with its neighbortokens and the first several tokens in the linear sequence. However,the rich structural information prevalently available inside the fulltext is not fully captured by Longformer. shows two typesof such structural information: the cross-paper network structure and the in-paper hierarchy structure. The hierarchy structure orga-nizes sections, subsections, and paragraphs into a tree, where theparent-child relation indicates that a finer text unit is included in acoarser one. Such a structure implies which paragraphs should bejointly considered when aggregating paragraph semantics to theentire paper. Moreover, by parsing the bibliographic entries in pa-per full texts, one can obtain each papers references and constructa citation network. This cross-paper network structure indicatesthe semantic proximity between two papers. To summarize, thesetwo types of structures provide additional semantic signals that arenot reflected in a linear text sequence. Contributions. Being aware of the two aforementioned challenges,in this paper, we study weakly supervised multi-label text classifi-cation of full-text scientific papers. We propose FuTex, the designof which is centered around how to use the cross-paper networkstructure and the in-paper hierarchy structure to classify scientificpapers in a large and fine-grained label space. FuTex has three majormodules: (1) Network-aware contrastive fine-tuning aims to leveragethe cross-paper network structure to fine-tune a pre-trained lan-guage model (PLM) so that it can probe fine-grained label semanticsand distinguish among similar categories. (2) Hierarchy-aware ag-gregation aims to exploit the in-paper hierarchy structure to obtainthe entire paper representation by aggregating from its paragraphs.With this aggregation process, a PLM does not need to deal withthe full-text sequence at once. (3) Self-training aims to take theinitial prediction (i.e., top-ranked categories according to the firsttwo modules) as pseudo labels to train a full-text paper classifier.Then the prediction of the trained classifier can complement theinitial prediction to improve the final classification results.We conduct experiments on two datasets (both with >10,000categories) commonly used in previous studies . Resultsshow that FuTex outperforms competitive baselines including sci-entific PLMs , weakly supervised text classifiers ,and structure-enhanced PLMs . Notably, on the MAG-CSdataset, our FuTex model, without any ground-truth training data,is on par with a supervised classifier trained on 60,000 labeledpapers. To summarize, this work makes the following contributions. We study the problem of weakly supervised multi-label classifi-cation of full-text scientific papers. Different from most previousstudies on weakly supervised text classification, it considers alarge, fine-grained label space and paper full texts.",
  "where each node V is a paper, and (,) E if and only if cites (i.e., is a bibliographic entry in )": "To describe the relationship between two papers in G, we adoptthe notation of meta-paths . To be specific, if cites , wecan say the two papers are connected via the meta-path Paper Paper (or its abbreviation ). Similarly, if two papers and share a common reference (i.e., (,) E and (,) E),we can say and are connected via the meta-path .Intuitively, if two papers are connected via a certain meta-path, theirrelevant topics are more likely to overlap. Following , given ameta-path M, we use M to denote that is connectedto via M, and the meta-path-based neighborhood NM () isdefined as { | M AND }.In-Paper Hierarchy Structure. One unique challenge we arefacing in full-text paper classification is that each paper is beyonda plain text sequence (i.e., title+abstract) and contains its internalhierarchical structure of paragraphs. As shown in , nodesrepresenting paragraphs, subsections, sections, and the entire paperform a tree, in which the parent-child relation implies a finer textunit is entailed by a coarser one. Formally, we have the definitionbelow. Definition 2.2. (In-Paper Hierarchy Structure) A full-text pa-per contains a hierarchical tree structure T. The root of Trepresents the entire paper; the leaves of T are s paragraphsP = {1, ..., }. The tree can be characterized by a mappingChild(), where Child() is the set of text units that are one levelfiner than and contained in . Putting the two structures together, we would like to classifythe nodes in a network G. Meanwhile, each node contains itsown subcomponents that form a hierarchy T. Given a paper, weneed to jointly consider its subcomponents and its proximity withneighbors to infer its categories. 2.2Problem DefinitionIn this paper, we study weakly supervised multi-label text classi-fication. By weakly supervised, we imply that we do not haveany annotated training samples for any label, and the only avail-able supervision to characterize a label is its name and severaldescriptive sentences . shows the name and descrip-tion of the label Deltacoronavirus as an example. This settingis more challenging than zero-shot multi-label text classification which assumes annotated documents are givenfor some seen classes and the trained classifier should be general-ized to predict unseen classes. Under the weakly supervised setting,all classes are unseen. This setting is also called wild zero-shot insome previous studies .By multi-label, we mean that each paper can be relevant tomore than one label. This is a natural assumption when the labelspace is fine-grained and multi-faceted. For example, a COVID-19paper can be labeled as Infections, Lung Diseases, Coronavirus,and Public Health at the same time. This assumption makes ourtask more challenging than weakly supervised single-label classifi-cation .To summarize, our task can be defined as follows.",
  "each paper has its full text and hierarchy structure T, and (2) alabel space L where each label has its name and description, ourtask is to predict the relevant labels L L for each D": "3MODELOne straightforward solution to our task is to pick a pre-trainedlanguage model (e.g., SciBERT ), use it to encode each paperscontent and each labels name/description to get their embeddings,and then perform the nearest neighbor search in the embeddingspace. However, such an approach suffers from two drawbacks:First, unfine-tuned PLMs may not be powerful enough to detectthe subtle semantic differences between two papers or two labeldescriptions, but fine-grained text classification, to a great extent,requires the classifier to distinguish among labels that are closeto each other. Second, the entire paper is long (e.g., with 4,000words on average in the Semantic Scholar Open Research Corpus(S2ORC) ), which exceeds the maximum sequence length (e.g.,512 tokens) that a PLM can handle in most cases.To overcome the aforementioned two drawbacks, we proposeto exploit the cross-paper network structure and the in-paper hi-erarchy structure, which will be introduced in Sections 3.1 and3.2, respectively. Then, in .3, we present a self-trainingstrategy, that is, how we use initial predictions as pseudo labels totrain a classifier that complements the predictions. The overviewof our proposed FuTex framework is shown in . 3.1Network-Aware Contrastive Fine-TuningThe first module in FuTex aims to utilize the cross-paper networkstructure to improve the PLMs ability to distinguish among fine-grained labels. We follow the intuition of LinkBERT that iftwo papers are connected via certain citation-based relationships(e.g., M ), then a paragraph and a paragraph are more likely to share fine-grained topics than tworandomly picked paragraphs. In LinkBERT , Yasunaga et al.propose to concatenate the two linked paragraphs together (i.e.,[CLS][SEP][SEP]) to perform masked token predictionand document relation prediction for language model pre-training. However, in this paper, we are not aiming at training a general-purpose PLM. Instead, our model only needs to judge whether twotext units are relevant to similar topics or not. Being able to dothis, during inference, the model can take a paragraph and a labeldescription as input to predict whether the paragraph is relevant tothe label. To achieve this goal, following , we adopt a contrastivefine-tuning objective to replace the language model pre-trainingobjectives in LinkBERT.",
  ": Overview of the FuTex framework": "To be specific, the PLM should be fine-tuned to distinguish be-tween linked and unlinked paragraph pairs. As shown in (left), given three papers , +, and , where + NM () and NM (). We randomly sample three paragraphs , +, and from , +, and , respectively. The PLM aims to predict thesimilarity between and + as well as that between and .",
  "sim(,) = PLM([CLS][SEP][SEP]).(4)": "However, the strategy that concatenates each paragraph and eachlabel and feeds them into one PLM (i.e., the Cross-Encoder architec-ture ) makes the inference computationally expensive becausethe representation of each text unit cannot be pre-computed. Forexample, suppose there are papers (each of which has para-graphs) and labels, then we need to call the PLM () timesduring inference. Such a cost will prohibit us from applying themodel to a large corpus and a large label space. For example, if = = 104 and = 30, then = 3 109. To alleviate the cost,we adopt the following two strategies.Adding a retrieval stage. Following , given a paper , wefirst adopt exact name matching to retrieve a small set of candidatelabels C() from the entire label space L. To be specific, if a labelsname appears in the papers content, it will be added as a candidate.The fine-tuned PLM is then applied as a reranker to score labelsfrom the retrieved candidate pool. Using Bi-Encoder for non-abstract paragraphs. As mentionedin the Introduction, the abstract and other paragraphs should notbe treated equally in text classification. The abstract is highly sum-marized to cover the major topics of the paper, while a paragraph inthe paper body may capture only one aspect and provides auxiliarytopic signals. To use the more powerful tool in the most importantcase, we only adopt the Cross-Encoder architecture when inferringthe labels of paper abstracts (i.e., Eq. (4)). For other paragraphs, weadopt the Bi-Encoder architecture . To be specific, we encodeeach paragraph and each label separately.",
  "= PLM([CLS][SEP]), = PLM([CLS][SEP]). (5)": "Here, and are the output representations of the [CLS] tokenafter PLM encoding. Then, the similarity between and can becomputed as cos(, ). One drawback of this strategy is that theparagraph and the label text cannot serve as each others contextduring PLM encoding. However, the efficiency is significantly im-proved because we can pre-compute and for all paragraphsand labels. In fact, if we combine the two proposed strategies, theinference complexity will be reduced to ( + + ), where is the average number of candidate labels picked for each paperin the retrieval stage. For example, if we assume = = 104 and = = 30, then + + = 6.1 105, which is orders ofmagnitude smaller than = 3 109. 3.2Hierarchy-Aware AggregationAlthough the similarity between each paragraph and each la-bel can be computed efficiently now, we have not figured outhow to calculate the score between an entire paper and a label .Intuitively, simply averaging all paragraph embeddings may notwork well because important signals from the paper abstract andconclusion will then be buried under the great amount of contentfrom other sections. Previously, FullMeSH proposed to check 5sections abstract, introduction, method, result, and summary ineach full paper so as to probe relevant topics from different aspects.Inspired by their idea, we utilize the in-paper hierarchy structureT to perform embedding aggregation from paragraphs to sections,and then to the entire paper.Given a non-leaf text unit T (e.g., subsections, sections, orthe entire paper), we obtain the embedding of by aggregating theembeddings from s children. Formally,",
  "score (,) = PLM([CLS][SEP][SEP]),(8)": "where stands for Cross-Encoder.Now we adopt an ensemble ranking step to jointly considerthe two scores. Given a document , we first rank all candidatelabels from C() in descending order according to score(,) andscore (,), respectively. In this way, each candidate label willhave two rank positions (|) and (|). Then, we calculatethe mean reciprocal rank (MRR) of .",
  "Finally, candidate labels are sorted according to MRR(|) as thereranking result": "3.3Self-TrainingThe retrieval stage proposed in .1 significantly improvesthe efficiency of FuTex. However, it may filter out labels that do notexplicitly appear in a paper but are implicitly relevant to the paperin the latent semantic space. To mitigate this issue, we presenta self-training strategy to utilize paper full texts and confidentpredictions (based on MRR) to train a text classifier class(). Theclassifier is then used to predict the probability that a paper isrelevant to a label (not necessarily selected in the retrieval stage),and those top-ranked labels will complement our initial MRR-basedpredictions.Since the major goal of this paper is not to invent a new fullysupervised text classifier, we propose to use an off-the-shelf model.To leverage paper full texts and meanwhile bypass the sequencelength problem, we choose a bag-of-words multi-label classifier Parabel , which, according to , has competitive perfor-mance even compared with deep learning classifiers on benchmarkdatasets. Parabel represents each training document as a |WD|-dimensional feature vector , where WD is the vocabulary ofD. Given a word WD, its corresponding entry in is thefollowing tfidf score:",
  "where tf(,) is the term frequency of in , and idf(, D) =log|D|": "|{D|}| is the inverse document frequency of .For each paper , according to MRR calculated in Eq. (9), weuse the top- predicted labels as pseudo labels to train the Parabelclassifier. (If has less than candidate labels selected in theretrieval stage, we use all of them.) Formally, the pseudo labels of is represented as an |L|-dimensional vector , where",
  "MAG-CS 96,71810,9094071.6945.915.84PubMed 251,57316,0704901.4233.388.69": "new highly-ranked labels. To be specific, the top- predictions ac-cording to MRR remain at top- in our final predictions; the otherlabels will be sorted according to Parabels output Pr( = 1|)and ranked after top-. We use the following example to explainthis process. Example 3.1. (Final Prediction after Self-Training) Suppose thereare 5 labels L = {, ,, , } and = 2. Given a paper , assume3 labels , , and are selected in the retrieval stage and their MRRscores are 2.00, 1.00, and 0.67, respectively. Then, and will bethe two pseudo labels of used for training Parabel. The trainedParabel is used to classify again and get the following scores:",
  "Pr( = 1|)0.800.850.300.600.90": "In our final prediction, and will still be the top 2, and the otherlabels will be ranked according to Parabels prediction. Therefore,the final rank will be (, , , ,). In practice, we find this strategyachieves better classification performance than reranking all labelspurely based on Pr( = 1|), the result of which is (, ,, ,).The entire procedure of FuTex is summarized in Appendix A.1. 4EXPERIMENTS4.1SetupDatasets. We use two datasets, MAG-CS and PubMed ,that are widely adopted in previous studies on scientific paper clas-sification . Originally, MAG-CS contains 705K paperspublished at 105 top computer science venues, where each paperis labeled with its related fields-of-study ; PubMed consists of899K papers published in 150 top medicine journals, where eachpaper is labeled with its related MeSH terms . However, sincethe two original datasets do not have paper full texts, we try toextract full texts from S2ORC . S2ORC has segmented eachpaper into paragraphs, marked the section that each paragraphbelongs to, and parsed the bibliographic entries. We remove theparagraphs with less than 10 words from each paper. Note that notall papers in these two datasets can be found in S2ORC, and wefinally obtain 96,718 full-text MAG-CS papers and 251,573 full-textPubMed papers. Because FuTex does not require any annotatedtraining data, all these papers are used for testing. More statisticsof these two datasets can be found in .Compared Methods. We compare our FuTex model with thefollowing baselines including scientific PLMs, structure-enhancedPLMs, and zero-shot multi-label text classification methods. SciBERT 1 is a scientific PLM trained on 1.14M scientific pa-pers from Semantic Scholar using masked language modelingand next sentence prediction tasks.",
  "SPECTER 5 is a structure-enhanced scientific PLM. It con-tinues pre-training SciBERT using a citation prediction objectivewith 684K pairs of linked papers": "The four baselines above can be used to classify either abstractsor full-text papers. When applying them to abstracts, we directlyencode each abstract and each label description to calculate thecosine similarity between the two embeddings. When applyingthem to full text, we follow the hierarchy-aware aggregation processin .2 and ensemble the results of using abstract only andusing full text. 0SHOT-TC is a zero-shot text classification method.It is a natural language inference (NLI) model that predicts towhat extent a paper (as the premise) entails the sentence thisdocument is about {label_name}. (as the hypothesis). Following, we use RoBERTa-large-mnli 6 as the NLI model. MICoL 7 is a zero-shot text classification method. It proposesa metadata-induced contrastive learning technique to fine-tunea PLM. MICoL has various configurations with the model archi-tecture and the used meta-path. According to the experimentalresults in , we choose the best-performing configuration:(Cross-Encoder, ). The two methods above adopt the Cross-Encoder architecture. Asmentioned in .1, it will be too computationally expensiveto apply them to all paragraphs. Therefore, we keep their originalusage on paper abstracts only. Longformer 8 is a PLM dealing with long documents. Itsparsifies the fully connected attention and can take 4,096 tokensat most. We adopt it to encode full-text papers by setting themaximum number of tokens as 512, 1,024, 2,048, and 4,096. Asshown in the Introduction, the 512-token version performs thebest, so we use it for performance comparison. PLM+GAT is a PLM stacked with a Graph AttentionNetwork (GAT) layer. Each paragraph is first encodedby a PLM. Then we use GAT to obtain paragraph embeddingsby aggregating PLM representations of its neighbor paragraphs + NM (). A link prediction objective (i.e., judgingwhether two paragraphs are connected via meta-path M) is thenadopted to train the PLM and GAT in an end-to-end manner. GraphFormers 9 is a GNN-nested PLM architecture, inwhich GNN layers and Transformer layers are alternately stacked.Similar to PLM+GAT, a link prediction objective is adopted totrain the model so that the PLM can be enhanced by the cross-paper network structure.The three methods above are naturally suitable for classifying paperfull texts.According to our experiments, SPECTER performs better thanSciBERT, OAG-BERT, and LinkBERT. Therefore, for MICoL, PLM+GAT, GraphFormers, and FuTex, we all use SPECTER as the basePLM. Also, the meta-path M is set as for all thesemodels for a fair comparison.Implementation and Hyperparameters. Each label from thePubMed dataset may have multiple label names, including onecanonical name and 0, 1, or several synonyms (i.e., entry terms).Following , we include as a candidate label of if any of itsnames appears in s content. On both MAG-CS and PubMed, dur-ing the retrieval stage, we use each papers title and abstract, insteadof the full text, for label name matching because this yields betterclassification performance. During network-aware contrastive fine-tuning, when feeding the two paragraphs into a Cross-Encoder,the maximum length of each paragraph is 256 tokens. The trainingbatch size is 8. We use the AdamW optimizer , warm up the",
  "Weakly Supervised Multi-Label Classification ofFull-Text Scientific PapersKDD 23, August 610, 2023, Long Beach, CA, USA": "RTX A6000 GPU, and we train the three models for the same numberof epochs on each dataset (i.e., 20 epochs on MAG-CS and 5 epochson PubMed). As for the inference stage, we notice that the majorfactor which affects each models inference efficiency is whether itis used for abstracts only or full texts. If we fix this factor, differentBERT-based baselines will have similar inference efficiency becausethey have similar model sizes and architectures. Therefore, wechoose SPECTER as a representative for comparison and report itsinference time (per testing sample) when used for abstracts onlyand full texts. The results are demonstrated in .From (a), we observe that FuTex has much less trainingtime than PLM+GAT and GraphFormers. From (b), we findthat the inference efficiency of FuTex is on par with SPECTER (full-text). In comparison with SPECTER (abstract), FuTex and SPECTER(full-text) need significantly more time to run because about 30 to45 times more paragraphs are considered. 5RELATED WORKWeakly Supervised Text Classification. Weakly supervised textclassification aims to assign relevant label(s) to each document with-out any human-annotated training samples provided. The commonformats of weak supervision include label names , a small setof category-indicative keywords , and label descriptions .Technically, earlier methods mainly utilize Explicit Semantic Anal-ysis , Latent Dirichlet Allocation , and context-freeword embeddings . Inspired by the success of BERT ina wide spectrum of text mining tasks, recent studies start to exploitthe power of PLMs in weakly supervised text classification. Forexample, ConWea uses BERT to disambiguate the providedkeywords and retrieve more category-indicative words for pseudotraining data collection; LOTClass leverages one BERT encoderto perform masked language modeling for finding more indicativewords and another BERT to perform classification; X-Class usesBERT representations of words to perform category-aware clus-tering and then aligns documents to categories; LIME adoptsBART-large-MNLI and prompts to predict pseudo labels ofeach document and then uses BERT for self-training. However,all the aforementioned methods focus on a relatively small labelspace ( 50 categories in most cases) and assume each documentis relevant to one category only (or a single path from the root toa leaf category in the hierarchical classification setting). In con-trast, FuTex studies larger and more fine-grained label spaces (e.g.,> 10, 000 categories) where each document is relevant to multiplelabels in most cases. Zero-Shot Multi-Label Text Classification. In general, similar toweakly supervised, the term zero-shot also implies that the clas-sifier does not need any annotated samples. However, in large-scaleor extreme multi-label text classification, zero-shot is interpretedin different ways in the existing literature. According to , therestrictive zero-shot setting assumes training samples are given forsome seen classes and the classifier aims to predict unseen classes, which is different from our weakly su-pervised setting; the wild zero-shot setting does not assume anyseen classes and the classifier needs to make predictions withoutrelying on any annotations . For example, TaxoClass uses RoBERT-large-MNLI to convert text classificationto an entailment task; MICoL proposes a metadata-inducedcontrastive learning method to fine-tune SciBERT . However, existing wild zero-shot classifiers still view each document as alinear sequence of paragraphs, thus cannot be directly applied tofull-text paper classification due to the maximum length limit ofPLMs. In comparison, FuTex exploits the cross-paper and in-paperstructures of scientific literature.Scientific Paper Classification. Classifying scientific papers isa common evaluation task in both text mining (e.g., )and network mining (e.g., ) studies. However, moststudies consider coarse-grained paper classification only (e.g., 50categories). To satisfy users fine-grained interests, Zhang et al. and Ye et al. propose to use paper metadata to performlarge-scale multi-label paper classification. In the biomedical do-main, MeSH indexing can also be cast as a multi-labeltext classification task to tag PubMed papers with fine-grained med-ical subject headings. Nevertheless, these studies focus on usingthe paper title and abstract only. To the best of our knowledge,FullMeSH and BERTMeSH are two representative stud-ies making use of paper full texts. However, they adopt a fullysupervised setting and are not directly applicable to our task. 6CONCLUSIONS AND FUTURE WORKWe present FuTex, a multi-label scientific paper classifier that relieson label names and descriptions as the only supervision and doesnot require any human-annotated training data. FuTex exploits pa-per full texts and consists of three modules: the contrastive learningmodule leverages the cross-paper citation network structure; thesemantic aggregation module uses the in-paper hierarchy structureof sections, subsections, and paragraphs; the self-training moduletrains a full-text classifier using pseudo labels to complement theinitial predictions. Experiments on two datasets demonstrate thesuperiority of FuTex over competitive weakly supervised baselinesand show that FuTex is on par with fully supervised classifiers withthousands of ground-truth training samples. An ablation study val-idates the usefulness of all three proposed modules. A case studyshows that FuTex can effectively extract signals from full text topredict labels not indicated by the paper title or abstract.Interesting future directions include (1) how to leverage otherin-paper structural signals, such as the relationship between para-graphs and figures/tables, to further improve the classification per-formance and (2) how to leverage large language models (e.g., GPT-4) for weakly supervised fine-grained paper classification, whereone needs to tackle the maximum input length limit given the paperfull text and tens of thousands of label names. ACKNOWLEDGMENTSWe thank anonymous reviewers for their valuable and insight-ful feedback. Research was supported in part by the IBM-IllinoisDiscovery Accelerator Institute, US DARPA KAIROS Program No.FA8750-19-2-1004 and INCAS Program No. HR001121C0165, Na-tional Science Foundation IIS-19-56151, IIS-17-41317, and IIS 17-04532, and the Molecule Maker Lab Institute: An AI Research Insti-tutes program supported by NSF under Award No. 2019897, and theInstitute for Geospatial Understanding through an Integrative Dis-covery Environment (I-GUIDE) by NSF under Award No. 2118329.Any opinions, findings, and conclusions or recommendations ex-pressed herein are those of the authors and do not necessarilyrepresent the views, either expressed or implied, of DARPA or theU.S. Government.",
  "(12)": "4.2Performance Comparison shows P@ and NDCG@ scores of compared methods onMAG-CS and PubMed. For models with randomness (i.e., MICoL,PLM+GAT, GraphFormers, and FuTex), we run each of them 5times with the average performance reported. Other models (i.e.,SciBERT, OAG-BERT, LinkBERT, SPECTER, 0SHOT-TC, and Long-former) are deterministic according to our usage. To show statisticalsignificance, we conduct a two-tailed t-test to compare FuTex witheach baseline if the baseline has randomness, and we conduct a two-tailed Z-test to compare FuTex with each deterministic baseline.The significance level is also marked in .From , we find that: (1) FuTex consistently and signif-icantly outperforms all baselines. On both datasets, MICoL is acompetitive baseline, possibly because it also uses citation linksacross papers. However, since MICoL only considers text contentfrom paper titles and abstracts, it is not as powerful as FuTex.(2) There are four baselines (i.e., SciBERT, OAG-BERT, LinkBERT,and SPECTER) that can be used in both abstract-only and full-text settings. In most cases, a full-text variant can outperform itsabstract-only counterpart. This observation validates our claimthat considering the full text is beneficial to paper classification.Besides, unlike Longformer (as shown in ), our proposedhierarchy-aware aggregation strategy, which is also used by thefull-text variants of the four baselines, can effectively use paperfull texts. Furthermore, in a few cases, a full-text variant underper-forms its abstract-only counterpart in terms of P@1 but achieveshigher P@3 and P@5. This finding implies that signals extractedby hierarchy-aware aggregation from paper full texts can betterhelp lower-ranked predictions. Comparison with a Supervised Model. We further compare Fu-Tex with a fully supervised multi-label text classifier. In accordancewith our choice in .3, we report the performance of Parabel with ground-truth training data. To be specific, for each dataset,we select 10,000 papers as testing samples and pick different num-bers of training samples from the remaining papers. showsthe P@5 score of Parabel with different numbers of ground-truth 0.4 0.42 0.44 0.46 0.48 0.5 P@5 # Ground-Truth Training Papers FUTEXSupervised Parabel",
  "(b) PubMed": ": The P@5 score of supervised Parabel with differentnumbers of ground-truth training papers. Our FuTex model,without any ground-truth training samples, is on par withParabel that uses 60,000 and 1,000 training samples on MAG-CS and PubMed, respectively. training samples in comparison with FuTex. We can observe thatour FuTex model, without relying on any annotated training data,is on par Parabel that uses 60,000 and 1,000 ground-truth trainingsamples on MAG-CS and PubMed, respectively. 4.3Ablation StudyThere are three major modules in FuTex: network-aware con-trastive fine-tuning, hierarchy-aware aggregation, and self-training.Now we conduct an ablation study to check the contribution ofeach module. To facilitate this, we create three ablation versions ofFuTex: FuTex-NoNetwork does not have the network-aware contrastivefine-tuning module. It directly uses SPECTER in the Bi-Encoderarchitecture, ensembles the results of using abstract only andusing full text, and then performs self-training.",
  "FuTex-NoSelfTrain does not have the self-training module. Ituses the MRR-based ranking list obtained in .2 as thefinal prediction": "demonstrates the performance of the full FuTex modeland the three ablation versions. We can observe that: (1) The fullFuTex model consistently and significantly outperforms the threeablation versions, indicating that all three major modules have apositive contribution to the classification performance. (2) Amongthe three ablation versions, FuTex-NoNetwork performs the worstin terms of P@1. This finding indicates that the cross-paper networkstructure is more beneficial to top-ranked predictions. By contrast,FuTex-NoSelfTrain has the lowest P@5 score on both datasets,which means that the self-training module contributes the most tolower-ranked predictions. This observation validates our claim thatself-training can find more labels that are semantically relevant toeach paper so as to complement the initial top-ranked categories. 4.4Case StudyWe now perform a case study to qualitatively demonstrate the effectof considering full text in paper classification. shows twocases, one of which is from MAG-CS and the other from PubMed.For both papers, we show their text information including the title,",
  "Title: compact modeling technique for outdoor navigationTitle: serum calprotectin: a novel diagnostic and prognostic marker in inflammatorybowel diseases": "Abstract: abstract-in this paper, a new methodology to build compact local maps inreal time for outdoor robot navigation is presented. the environment information isobtained from a 3-d scanner laser. the navigation model, which is called traversableregion model, is based on a voronoi diagram technique ... Abstract: there is an unmet need for novel blood-based biomarkers that offer timelyand accurate diagnostic and prognostic testing in inflammatory bowel diseases (ibd).we aimed to investigate the diagnostic and prognostic utility of serum calprotectin(sc) in ibd ... Full Text: this new challenge has prompted a change in robotics navigation philosophy,where path planning and modeling were always obtained a priori ... in general, inmobile robot navigation, the occupancy-based approach is one of the most commonlyused methods ... path planning in large and outdoor environments is a complex taskbecause there are a lot of parameters that define the traversability, for example, asfollows ... when the 3-d model is defined (as the one in figs. 5 and 6 , where the terrainconsidered ta is represented in blue and the nta is represented in red), the free spacecan be extracted to build a trm for the robot navigation and path planning ... Full Text: inflammatory bowel diseases (ibd), including crohns disease (cd) andulcerative colitis (uc), are chronic, debilitating inflammatory disorders of the gas-trointestinal tract affecting adults and children ... a recent meta-analysis of 13 studiesand 1,041 patients found that fc had a pooled sensitivity and specificity of 0.93 ... todetermine the accuracy of blood parameter measurements as a prognostic test capableof diagnosing ibd, receiver operating characteristic (roc) analyses were performed byplotting sensitivity against specificity ...",
  "FuTex Prediction: inflammatory bowel diseases (), leukocyte l1 antigen complex(), crohn disease (), colitis ulcerative (), sensitivity and specificity ()": "abstract, and excerpts from full text. We also show their ground-truth labels, labels predicted by FuTex, and labels predicted byMICoL (which is the most competitive baseline using titles andabstracts only). We mark a label as blue if it (or a semanticallysimilar term) appears in the paper title or abstract; we mark a labelas orange if it does not appear in the title/abstract but is mentionedin full text.In the MAG-CS case, three labels voronoi diagram, robot,and scanner explicitly appear in the paper abstract, and they arecorrectly predicted by both MICoL and FuTex. However, MICoLmisses labels such as motion planning and mobile robot navi-gation, which are not mentioned in the title/abstract. In fact, theterm outdoor robot navigation in the abstract may imply thepapers relevance to mobile robot navigation, but MICoL doesnot build the connection between them. After the papers full textis exploited, mobile robot navigation completely appears in thecontent, and the term path planning, which is semantically closeto the label motion planning, is repeatedly mentioned. As a result,both labels are accurately captured by FuTex.In the PubMed case, MICoL successfully predicts the ground-truth labels inflammatory bowel diseases and leukocyte l1 anti-gen complex (whose synonym is calprotectin11) indicated by thetitle/abstract. However, MICoL fails to predict labels such as crohndisease, colitis ulcerative, and sensitivity and specificity. As",
  ": Training and inference time of FuTex and repre-sentative baselines on MAG-CS and PubMed": "shown in , hints to these labels can be found in the full text.Note that these labels are indeed relevant to the paper rather thanjust being mentioned: Crohns disease and ulcerative colitis aretwo types of inflammatory bowel diseases studied in the paper, andthe paper extensively discusses the sensitivity and specificity ofpredicting these diseases. By leveraging the papers full text, FuTexaccurately picks these labels. 4.5EfficiencyWe now analyze the training and inference time of FuTex on MAG-CS and PubMed. To be specific, we compare the training time ofFuTex (the network-aware contrastive learning step) with that ofPLM+GAT and GraphFormers, which also use network signals fortraining. For a fair comparison, we run each model on an NVIDIA",
  "Bowen Jin, Wentao Zhang, Yu Zhang, Yu Meng, Xinyang Zhang, Qi Zhu, andJiawei Han. 2023. Patton: Language Model Pretraining on Text-Rich Networks.arXiv preprint arXiv:2305.12268 (2023)": "Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, SergeyEdunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-Domain Question Answering. In EMNLP20. 67696781. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, AbdelrahmanMohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART:Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,Translation, and Comprehension. In ACL20. 78717880.",
  "Ximing Li, Changchun Li, Jinjin Chi, Jihong Ouyang, and Chenliang Li. 2018.Dataless text classification: A topic modeling approach with document manifold.In CIKM18. 973982": "Ke Liu, Shengwen Peng, Junqiu Wu, Chengxiang Zhai, Hiroshi Mamitsuka, andShanfeng Zhu. 2015. MeSHLabeler: improving the accuracy of large-scale MeSHindexing by integrating diverse evidence. Bioinformatics 31, 12 (2015), i339i347. Xiao Liu, Da Yin, Jingnan Zheng, Xingjian Zhang, Peng Zhang, Hongxia Yang,Yuxiao Dong, and Jie Tang. 2022. OAG-BERT: Towards a Unified BackboneLanguage Model for Academic Knowledge Services. In KDD22. 34183428. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, OmerLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: Arobustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692(2019).",
  "OpenAI. 2023. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774 (2023). Seongmin Park and Jihwa Lee. 2022. LIME: Weakly-Supervised Text Classificationwithout Seeds. In COLING22. 10831088": "Shengwen Peng, Ronghui You, Hongning Wang, Chengxiang Zhai, HiroshiMamitsuka, and Shanfeng Zhu. 2016. DeepMeSH: deep semantic representationfor improving large-scale MeSH indexing. Bioinformatics 32, 12 (2016), i70i79. Yashoteja Prabhu, Anil Kag, Shrutendra Harsola, Rahul Agrawal, and ManikVarma. 2018. Parabel: Partitioned label trees for extreme classification withapplication to dynamic search advertising. In WWW18. 9931002.",
  "Wenpeng Yin, Jamaal Hay, and Dan Roth. 2019. Benchmarking Zero-shot TextClassification: Datasets, Evaluation and Entailment Approach. In EMNLP19.39053914": "Ronghui You, Yuxuan Liu, Hiroshi Mamitsuka, and Shanfeng Zhu. 2021.BERTMeSH: deep contextual representation learning for large-scale high-performance MeSH indexing with full text. Bioinformatics 37, 5 (2021), 684692. Ronghui You, Zihan Zhang, Ziye Wang, Suyang Dai, Hiroshi Mamitsuka, andShanfeng Zhu. 2019. Attentionxml: Label tree-based attention-aware deep modelfor high-performance extreme multi-label text classification. NeurIPS19 (2019),58205830.",
  "AAPPENDIXA.1The Entire Procedure of FuTexWe summarize the entire procedure of FuTex in Algorithm 1": "A.2Performance on Infrequent LabelsA large and fine-grained label space typically implies a long-tailedlabel distribution, where most categories are associated with onlya few documents. In many real applications, it is desirable to pre-dict more tail labels. For example, in scientific paper classification,predicting a paper is relevant to Lagrangian Support Vector Ma-chine is more informative than saying the paper the relevant toMachine Learning. To promote prediction of tail labels, recentstudies propose to use propensity-based P@(i.e., PSP@) and propensity-based NDCG@ (i.e., PSN@) as eval-uation metrics. PSP@ and PSN@ are formally defined as follows.",
  "The intuition behind these two metrics is to give a higher rewardto a model if it predicts an infrequent label correctly. In Eq. (13), 1": "is such a reward; is number of papers relevant to in the wholedataset D; , , > 0 are constants. In this way, the less frequenta label is, the higher reward a model can get when predicting itcorrectly. PSP@ and PSN@ scores can be viewed as a reward-weighted version of P@ and NDCG@, respectively. Followingprevious studies , we set = 0.55, = 1.5, and = (log |D| 1)( + 1). By definition, we have PSP@1 PSN@1if each paper has at least one ground-truth label. shows PSP@ and PSN@ scores of FuTex and com-petitive baselines on MAG-CS and PubMed. From , we canobserve that: (1) FuTex consistently and significantly outperforms",
  "Return L = {top- ranked labels of };": "all baselines except MICoL. (2) When comparing with MICoL, Fu-Tex has lower PSP@1 and PSN@3 but higher PSP@3, PSP@5, andPSN@5. The only statistically significant gap between FuTex andMICoL is the gap of PSP@5. This echos our finding from that considering paper full texts is more beneficial to lower-rankedpredictions. One possible reason why FuTex underperforms MICoLin terms of PSP@1 and PSN@3 is that FuTex ensembles the predic-tions of a Cross-Encoder (Eq. (8)) and a Bi-Encoder (Eq. (7)) whileMICoL is solely based on a Cross-Encoder according to our usage.In fact, as shown in , labels predicted by the Cross-Encoderarchitecture are more infrequent than those by the Bi-Encoder. A.3Performance on a Small DatasetMAG-CS and PubMed have nearly 97K and 252K papers, respec-tively, which can provide rich self-supervision during contrastivelearning and self-training. We now examine the performance ofFuTex on a small dataset and check if it can still outperform com-petitive baselines. To facilitate this, we adopt the Art dataset fromthe MAPLE benchmark . These Art papers are labeled with1,990 categories at different granularities (e.g., classics, popularmusic, and rhetorical criticism), and we manage to find 328 ofthem from S2ORC to obtain full texts. The performance ofFuTex and competitive baselines on these Art papers are demon-strated in . We can observe that FuTex performs the bestin terms of P@3, P@5, NDCG@3, and NDCG@5. For P@1, FuTex",
  "is second to 0SHOT-TC. This observation implies that even if thedataset is small (which may limit the power of contrastive learningand self-training), FuTex still works effectively": "A.4Hyperparameter StudyWe study the effect of two major hyperparameters in FuTex: thenumber of trees used in the Parabel classifier and the maximumnumber of pseudo labels per paper used for self-training (i.e., ).The P@5 scores of FuTex on MAG-CS with different hyperparam-eter values in {1, 3, 5, 10} are plotted in . We can find thatthe performance of FuTex is not quite sensitive to the two hyper-parameters. Indeed, all P@5 scores shown in outperformthose of all baselines in ."
}