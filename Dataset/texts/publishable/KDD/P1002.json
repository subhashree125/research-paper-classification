{
  "ABSTRACT": "Federated Learning (FL) is susceptible to poisoning attacks, whereincompromised clients manipulate the global model by modifyinglocal datasets or sending manipulated model updates. Experienceddefenders can readily detect and mitigate the poisoning effectsof malicious behaviors using Byzantine-robust aggregation rules.However, the exploration of poisoning attacks in scenarios wheresuch behaviors are absent remains largely unexplored for Byzantine-robust FL. This paper addresses the challenging problem of poi-soning Byzantine-robust FL by introducing catastrophic forgetting.To fill this gap, we first formally define generalization error andestablish its connection to catastrophic forgetting, paving the wayfor the development of a clean-label data poisoning attack namedBadSampler. This attack leverages only clean-label data (i.e., with-out poisoned data) to poison Byzantine-robust FL and requires theadversary to selectively sample training data with high loss to feedmodel training and maximize the models generalization error. Weformulate the attack as an optimization problem and present twoelegant adversarial sampling strategies, Top- sampling, and meta-sampling, to approximately solve it. Additionally, our formal errorupper bound and time complexity analysis demonstrate that ourdesign can preserve attack utility with high efficiency. Extensiveevaluations on two real-world datasets illustrate the effectivenessand performance of our proposed attacks.",
  "ACM Reference Format:Yi Liu, Cong Wang, and Xingliang Yuan. 2024. BadSampler: Harnessing thePower of Catastrophic Forgetting to Poison Byzantine-robust Federated": "Corresponding author. This work was supported by CityU of HK under Grants 9678146and 9678126, in part by HK RGC under Grants CityU 11218521, 11218322, R6021-20F,R1012-21, RFS2122-1S04, C2004-21G, C1029-22G, and N_CityU139/21. Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain. 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08",
  "INTRODUCTION": "Federated Learning (FL) , as an emerging distributed machinelearning framework, has provided users with many privacy-friendlypromising applications . However, it is knownthat FL is vulnerable to poisoning : a small fraction ofcompromised FL clients, who are either owned or controlled by anadversary, may act maliciously during the training process to cor-rupt the jointly trained global model, i.e., degrading global modelperformance or causing misclassification in prediction. In this con-text, various attack strategies aimed at perturbing local trainingdatasets or generating poisoned gradients have been proposed, so-called Data Poisoning Attacks (DPA) or Model Poisoning Attacks(MPA) . However, prior arts have shown that existingattacks are difficult to deploy in practice because they either rely onunrealistic assumptions (e.g., manipulating too many clients), or theeffectiveness of these attacks is greatly suppressed by state-of-the-art defensive Byzantine-robust aggregation rules (e.g., FLTrust ).In this paper, we endeavor to investigate the FL poisoning prob-lem in a more practical scenario, where the adversary needs toachieve successful poisoning of FL without resorting to establishingunrealistic attack assumptions or deploying advanced defensive ag-gregation rules. In addition, aligning with existing work ,we likewise need to push the frontiers of FL poisoning attacks byconsidering the deployment of strong defenses. In particular, wetarget Byzantine-robust FL , which represents a widely em-braced collaborative learning framework known for its resilienceagainst attacks. In the Byzantine-robust FL paradigm, the centralserver employs robust aggregation rules for the purpose of modelupdate aggregation. This facilitates the filtering of poisoned up-dates or malicious gradients, thereby preserving the overall modelutility . As a result, implementing poisoning attacks inthe context of Byzantine-robust FL without relying on unrealisticattack assumptions presents a formidable challenge.Our Contributions. To tackle the aforementioned challenges, wepresent BadSampler, which stands as an effective clean-label datapoisoning attack designed exclusively for Byzantine-robust FL. Un-like existing attacks, BadSampler leverages only clean-label datato achieve its goals. In the FL training pipeline, the widely usedStochastic Gradient Descent (SGD) typically employs uniform ran-dom sampling of training instances from the dataset. However, thisrandomness is seldom tested or enforced in practical implementa-tions . This situation presents an opportunity for adversariesto exploit the vulnerability, allowing them to devise an adversarial",
  "KDD 24, August 2529, 2024, Barcelona, Spain.Yi Liu et al": "in a training epoch: +1 = + , = (), where is the learning rate. Secondly, we delve into the sampling proce-dure of the SGD algorithm and analyze the impact of its batchingon model performance. In fact, the stochasticity of SGD comesfrom its sampling process, thus, in SGD, sampling affects how wellthe mini-batch gradient approximates the true gradient. Given thelocal data , we assume an unbiased sampling procedure, andthe expectation of the batch gradient matches the true gradient asfollows:",
  "RELATED WORK": "Existing Poisoning Attacks and Their Limitations. The cur-rent poisoning attacks in FL primarily follow two common strate-gies: (1) Directly modifying the local data ; (2) Buildingadversarially-crafted gradients . Although these attacktactics indeed pose real threats, they have evident limitations inpractical scenarios. Here we explore the limitations of these attackstrategies in a more realistic FL scenario where training operateswithin a practical range of parameters (refer to ). Firstly,DPAs necessitate the adversary possessing knowledge of the datasetor model and the capability to perturb the training dataset .Fulfilling this requirement proves challenging in practice. Experi-enced defenders, especially those equipped with anomaly detectiontechniques, can swiftly identify corrupted datasets . Moreover,studies have shown that DPAs struggle to bypass well-designeddefenses, such as Byzantine robust aggregation rules , even ifthe adversary successfully acquires dataset and model knowledgewhile circumventing the anomaly detection mechanism. Secondly,existing MPAs heavily rely on sophisticated and high-cost jointoptimization techniques among compromised clients, as describedin references . These attacks also rely on irrational parametervalues, such as an excessive percentage of compromised clients(typically > 20%) . These limitations motivate us to explore apoisoning attack that does not require dataset perturbation, utilizesa clean-label dataset, and bypasses Byzantine robust aggregation.Defenses Against Poisoning Attacks in FL. Existing defensesagainst poisoning attacks can be categorized into two main strate-gies. Firstly, there are defenses aimed at detecting and removingmalicious clients with anomalous data or model updates . Secondly, there are defenses aimed at limiting the negativeimpact of such attacks, such as using Byzantine-robust aggregationmethods that aggregate local model parameters using the Geomet-ric mean instead of the mean . In this paper, our focus",
  "BadSampler: Harnessing the Power of Catastrophic Forgetting to Poison Byzantine-robust Federated LearningKDD 24, August 2529, 2024, Barcelona, Spain": "use the Logistic Regression (LR) model as our local model; for non-convex problems, we use a simple CNN model, i.e., CNN with 2convolutional layers followed by 1 fully connected layer and theResNet-18 model. Furthermore, we use a common CNN model, i.e.,LeNet-5 model as our surrogate model.Hyperparameters. In our attacks, we consider the cross-device FLscenario and we set the number of clients = 100 and the propor-tion of client participation = 10%. Unless otherwise mentioned,we set local training epoch = 5, the learning rate = 0.001,the total number of training rounds = 250, the Gaussian functionparameter = 0.2, the meta-state size is 10 (i.e., = 5), the propor-tion of the compromised clients = {5%, 10%}, the mini-batch size {8, 16, 32, 64} and constant {1, 2, 4, 8}. summarizesour practical parameters/settings for comparing the previous work.Attacks for Comparison. We compare the following attacks withBadSampler. Label Flipping Attack (LFA) : a classic data poi-soning attack where the adversary modifies one class of true labelof the local dataset to another class. Adversarial Attack (AA) :a recent data poisoning attack, where the adversary accesses alocal training dataset to generate an adversarial dataset topoison the global model. Gaussian Attack (GA) : a classicmodel poisoning attack, where the adversary uploads its modelupdate from a Gaussian distribution with mean1 and variance 10. Zero-update Attack (ZA) : a powerful localmodel poisoning attack, where the adversary takes control of thecompromised clients and lets them send zero-sum model updates,i.e., = = 0. Local Model Poisoning Attack (LMPA) :a state-of-the-art local model poisoning attack, in which the adver-sary controls a small set of compromised clients to jointly optimizea well-designed model-directed deviation objective function to poi-son the FL model. OBLIVION : a model poisoning attack thatuses modified model weights to achieve catastrophic forgetting. DataOrdering Attack (ODA) : a data poisoning attack that uses modi-fying the order of training data to achieve catastrophic forgetting. Asmentioned above, we chose both the classic data poisoning attackand the model poisoning attack to compare the proposed attacksfairly.Defenses. To further validate the performance of BadSampler, weevaluate attacks under three anomaly-detection-based defenses, i.e.,PCA-based defense , FoolsGold , and FLDetector , andthree Byzantine robust AGRs, i.e., Trimmed Mean , Krum ,and FLTrust . Note that here we do not consider the verification setdefense scheme because the verification set is already used in FLTrust.Attack Impact Metric. We denote by the maximum ac-curacy that the global model converges without any attack. Weuse to denote the maximum accuracy the global model canachieve under a given attack. Here, we define attack impact asthe reduction in global model accuracy due to the attack, thus for agiven attack, i.e., = .",
  "BACKGROUND AND THREAT MODEL3.1Definition of the Generalization Error": "Here, we focus on elaborating on the formal definition of gener-alization error. In the machine learning domain, the bias-variancetrade-off is the basic theory for qualitative analysis of generalizationerror , which can be defined as follows: Definition 1. (Generalization Error). Let = {,} = {,}that contains training samples denote the training dataset, where denotes the -th training sample, and is the associated label. Let () denote the predictor and let (, ) denote the loss function. Thus,the formal expression of the generalization error is as follows:",
  ", (1)": "where () = E [ (|)]. According to the above definition, thebias may be smaller when the variance is larger, which means thatthe training error of the model is small and the verification error maybe large. When the above situation occurs, the model is likely to fallinto the catastrophic forgetting problem.",
  "| , () + () ,(2)": "where () and () are the bias and variance of .Obviously, when the generalization error of the sample approaches1, we can call it a hard sample; on the contrary, we can call the sample an easy sample. Therefore, for the sample , the definition of thegeneralization error can be used to represent the difficulty of samplelearning, i.e., the sample difficulty .",
  "Threat Model": "We consider a threat model in which an adversary could exploitwell-crafted adversarial samplers to poison the global model. Dur-ing training, the adversary forges a sampler that embeds a malicioussampling function. This adversarial sampler is then incorporatedinto the target FL system through system development or main-tenance . At inference time, the target FL system has indis-criminately high error rates for testing examples. Next, we discuss our threat model with respect to the attackers goals, backgroundknowledge, and capabilities.Attackers Goal. The attackers goal is to manipulate the learnedglobal model to have an indiscriminately high error rate for testingexamples, which is similar to prior studies on poisoning attacks . Given that there are many sophisticated defenders againstpoisoning attacks today , the attackers second goal is tosuccessfully evade the tracks of these defenders.Attackers Background Knowledge. We assume that the attackerholds a model (called surrogate model) that is not related to theprimary task. It should be noted that the surrogate model is utilizedby the attacker to build an adversarial sampler but has no impacton the training of the primary task. Furthermore, to make theattack more stealthy and practical, we assume that the attackeronly needs to access and read the local dataset. Although previousliterature has shown that attackers can modify and addperturbations to local datasets, our experimental results show thatsuch operations are easily detected by experienced defenders.Attackers Capability. We assume that the attacker controls theproportion of all clients , called compromised clients. Followingthe production FL settings (see Appendix A), we assume that thenumber of compromised clients is much less than the number ofbenign clients, i.e., 10%.Remark. We provide the following justifications for why our attackis realistic and distinct from other poisoning attacks: (1) In thecontext of FL, the client possesses autonomous control over thelocal training process, rendering it feasible for a compromised clientto deceitfully forge an adversarial sampler, thereby manipulatingthe local training dynamics. (2) The client is solely required tohave read permission for the local dataset and does not need writepermission. As read operations generally necessitate less privilegethan write operations, this approach is practical and helps preservethe cleanliness of the dataset by avoiding any modifications to it.",
  "=1 (),(3)": "where 0, = 1 is a user-defined term that indicates therelative influence of each client on the global model. In Eq. (3),we mainly focus on the local optimization term () which de-termines the performance of the global model. In fact, this localoptimization term can be regarded as solving a non-convex opti-mization problem with respect to the parameter , correspondingto the minimization of a given local loss function (). For the-th client, we consider that the batch size of local training is andlet be the total number of items for training, then in a singleepoch one aims to optimize: () = 1",
  "where P( = ) = 1": "and is the number of SGD steps.Observations. We emphasize that Eq. (4) occurs only as expected,and conversely, for isolated training batches, the approximationerror may be large and affect model performance. Inspired by thisfact, we investigate how adversaries exploit local sampler to disrupttraining. This means that the classical stochastic assumption in SGDopens up a new attack surface for adversaries to affect the learningperformance and process . To this end, we revisit the impactof SGD steps during local training on model performance:",
  "< (0 ) (0 ) is": "the stochastic error determined by the mini-batches. This meansthat we can simply manipulate local training mini-batches to harmlearning performance and convergence. Hence, our poisoning at-tack aims to manipulate local samplers to change the order of thelocal training batch without using poisoned data. By doing so, wecause the first and second derivatives to be misaligned with thetrue gradient step, thereby undermining the models generalizationability.",
  "Formulating Optimization Problems": "Our idea is to poison Byzantine-robust FL by utilizing adversarialtraining batches sampled by a carefully crafted adversarial samplerat each iteration. More specifically, we aim to use local trainingbatches sampled by adversarial samplers to increase the generaliza-tion error of the FL global model so that FL falls into catastrophicforgetting. Therefore, we expect to find a model-agnostic represen-tation that can provide generalization error information for adver-sarial samplers. Inspired by the theory of bias-variance tradeoff(see Defi. 1), the attackers goal translates to performing adversarialsampling among compromised clients to obtain the most appro-priate adversarial training batches to maximize Eq. (1). Withoutloss of generality, we consider that the first clients are compro-mised. Our generalization error maximization objective is to craftlocally adversarial training batches B1, B2, , B for the com-promised clients via solving the following optimization problem at",
  "Attack Implementation": "The attackers goal is formulated as an optimization problem inEq. (6). However, due to the non-convex and nonlinear nature ofEq. (6), solving it exactly is impractical. To make it amenable tooptimization, we propose an effective adversarial sampler calledBadSampler, which guides hard sampling to optimize Eq. (6). Theworkflow and taxonomy of BadSampler attacks are illustrated in . The proposed attack strategies consist of (1) the top- samplingstrategy and (2) the meta-sampling strategy. The core idea of thefirst attack strategy is to construct a training sample candidatepool with high loss and sample the hard training samples from thispool to feed the model. The insight of the second attack strategy isto treat the training and validation error distribution as the meta-state of the error optimization process, thus using meta-training tomaximize the generalization error.Top- Sampling Strategy. As mentioned above, the core idea ofthis strategy is the construction of a hard sample candidate pool. Toselect those samples that are conducive to maximizing the general-ization error, we use the sample difficulty (see Defi. 2) as a selectionmetric to select appropriate samples. Specifically, we select the top ( is a constant and is the size of the training batch) difficult-to-learn samples as the hard sample candidate pool and instructthe adversarial sampler to sample data from this candidate pool tofeed the model training. Note that sample difficulty is calculated bythe surrogate model because sample difficulty is model-agnostic andindependent of dataset size and sample space . For simplicity, letH denote the hard sample pool, then the selection process canbe formally defined as follows:",
  "H = (, , ).(7)": "Then the process of sampling hard samples from H to feed themodel training can be expressed as: (, |H). Note that weresample the samples in this candidate pool and the candidate poolis also updated iteratively.Meta Sampling Strategy. In this strategy, we adopt the concept ofmeta-learning to seek a model-agnostic representation that providesgeneralization error information for adversarial meta-samplers. Byleveraging the bias-variance trade-off, we can express the general-ization error in a form independent of task-specific properties, such",
  ": Workflow and taxonomy of our BadSampler attack": "as dataset size and feature space . Consequently, this repre-sentation enables adversarial samplers to perform across differenttasks. Drawing inspiration from Definition 1 and the idea of gradi-ent/hardness distribution , we introduce the histogramdistribution of training and validation errors as the meta-state ofthe adversarial sampler optimization process. Next, we delve intothe construction of the histogram error distribution.Meta Sampling Histogram Error Distribution. First, refer toreference , we give a formal definition of the histogram errordistribution as follows:",
  "= : R2.(9)": "In practice, the histogram error distribution reflects the fit of agiven classifier to the dataset . By adjusting in a fine-grainedmanner, it captures the distribution information of hard and easysamples, enriching the adversarial sampling process with moregeneralization error information. Moreover, in conjunction withDefi. 1, the meta-state provides the meta-sampler with insights intothe bias/variance of the current local model, thus supporting itsdecision-making. However, directly maximizing the generalizationerror solely based on this meta-state is impractical. Sampling fromlarge datasets for sample-level decisions incurs exponential com-plexity and time consumption. Additionally, blindly maximizingtraining and validation errors can be easily detected by an experi-enced defender.Meta Sampling Gaussian Sampling. To make updating themeta-state more efficient, we leverage a Gaussian function trick tosimplify the meta-sampling process and the sampler itself, reduc-ing the sampling complexity from O(||) to O(1). Specifically,we aim to construct a mapping such that an adversarial meta-sampler () outputs into a scalar interval for a giveninput meta-state , i.e., (|). In doing so, we apply a Gauss-ian function , () to the classification error of each instanceto determine its sampling weights, where , () is defined as:",
  "( ,),( ( ( |),)": "Meta Sampling Strategy Meta Training. In this section, wepresent the training process of the adversarial meta-sampler andits strategy to maximize generalization error. The adversarial meta-sampler is designed to iteratively and adaptively select adversar-ial training batches, aiming to maximize the generalization er-ror of the local model. As mentioned earlier, the sampler takesthe current meta-state as input and outputs Gaussian functionparameters to determine the sampling probability for each sam-ple. By learning and adjusting its selection strategy through thestate()action()state(new ) interaction, the adversarial meta-sampler seeks to maximize the generalization error. Consequently,the non-convex and nonlinear nature of the objective functionis naturally addressed through Reinforcement Learning (RL) .We consider the generalization error maximization process as anenvironment (ENV) in the RL setting. Therefore, in the RL set-ting, we treat the above optimization process as a Markov deci-sion process (MDP), where the MDP can be defined by the tuple(S, , , ). The definition of parameters in this tuple can beseen as follows: S is a set of state called state space, is a setof actions called the action space (where iscontinuous), , = Pr +1 = | = , = is the prob-ability that action in state at time will lead to state +1at time + 1, and (,) is the immediate reward received af-ter transitioning from state to state , due to action . Morespecifically, at each environment step, ENV provides the meta-state = [ : ], and then the action is selected by ( |).The new state +1 will be sampled .. +1 (,) in thenext round of local training. To maximize the generalization er-ror (i.e., high validation error) while maintaining good learningbehavior (i.e., low training error), we design a generalization",
  "THEORETICAL ANALYSIS5.1Error Upper Bound Analysis for BadSampler": "In analyzing the error lower bound of the proposed attack, it is cru-cial to consider that this bound can be influenced by factors such asthe experimental environment, communication status, and networkdelay. As a consequence, the practical significance of conductingsuch an analysis is limited. Therefore, while understanding thetheoretical lower bound is informative, it is essential to interpretthe results within the context of specific experimental conditionsand practical considerations. Thus, this paper provides a detailederror upper bound analysis. For simplicity, we make the requiredassumptions as follows. Assumption 1. (Bounded Gradients). For any model parameter and in the sequence [0,1, . . . ,, . . .], the norm of the gradi-ent at every sample is bounded by a constant 20, i.e., , we have:|| ()||2 20. Assumption 2. (Bounded BadSamplers Gradients). For any modelparameter and in the sequence [0,1, . . . ,, . . .], the normof the bad batchs gradient at every sample is bounded by a constant2, i.e., , we have: || ()||2 2.",
  "Complexity Analysis": "We give a formal complexity analysis of the proposed attacks. Wedenote the feature dimensions of the input sample as , the num-ber of training batches as , the size of the training batch as ,and the total computation per sample (depending on the gradientcomputation formula) as .Complexity of Top- Sampling. Let O( log) be the timecomplexity of the Top- operation and let O() (or O())be the time complexity of computing the gradient of a singlebatch (or sample), then the total time complexity of the BadSam-pler attack to complete the sorting and compute the difficulty isO( log) + O(). For the adversary, we assume that it ma-nipulates compromised clients, then the overall time complexityof the proposed attack is O( log(log + )).Complexity of Meta Sampling. Compared with the Top- sam-pling attack strategy, this attack strategy only brings additionaloverhead of performing meta-training. Therefore, we next givethe required complexity for meta-training. Likewise, we assumethat the cost of the meta-sampler to perform a single gradientupdate step is (depending on the gradient computationformula). In our attack implementation, following the SAC op-timization strategy, we need to perform actions beforeupdating the meta-sampler, and then need steps to col-lect online transitions and perform gradient updates for ().Therefore, the overall cost of meta-training can be expressed as(( + )) + (). For the adver-sary, we assume that it manipulates compromised clients, then theoverall time complexity of the proposed attack is O( log(log+)) + ( log(( + ) + )).",
  "EXPERIMENTS6.1Experiment Setup": "To evaluate the performance of our attacks, we conduct extensiveexperiments on two benchmark datasets. All designs are developedusing Python 3.7 and PyTorch 1.7 and evaluated on a server withan NVIDIA GeForce RTX2080 Ti GPU and an Intel Xeon Silver 4210CPU.Datasets. We adopt two image datasets for evaluations, i.e., Fashion-MNIST (F-MNIST) and CIFAR-10 . The datasets cover dif-ferent attributes, dimensions, and numbers of categories, allowingus to explore the poisoning effectiveness of BadSampler. To sim-ulate the IID setting of FL, we evenly distribute the two trainingdatasets to all clients, and to simulate non-IID setting of FL, wefollow to distribute the two training datasets to all clients (seeAppendix C).Models. In this experiment, we validate our BadSampler attack inboth convex and non-convex settings. For convex problems, we",
  "OursTop-76.0775.879.2143.6040.9218.1872.7356.3810.73Meta80.7872.1212.9649.1739.8019.3087.9256.7410.37": "FedAvg as AGR. For classification tasks on the Fashion-MNISTdataset, we employ the LR model, and for the CIFAR-10 dataset,we use the CNN model and ResNet-18 model. The adversarys ca-pabilities are restricted; they do not have access to global modelparameters and must carry out the attack within strict parameters.We employ the LeNet model as the surrogate model to computesample difficulty. demonstrates that despite these limita-tions, our designed attack scheme outperforms selected baselines,particularly the adversarial meta-sampling attack, where our attackachieves a remarkable performance of 19.3%. The key to our successlies in adaptively sampling and feeding the model hard samplesduring the learning process, which can destroy the models gener-alization performance and cause the model to fall into catastrophicforgetting. 6.2.2Attack Performance under Byzantine Robust Aggregationbased Defenses. To validate the effectiveness of our BadSamplerattack, we evaluate its performance against Trimmed Mean ,Krum , and FLTrust , which are server-side Byzantine ro-bust aggregation-based defenses. These defenses have previouslydemonstrated robustness against existing DPAs and MPAs. In thisexperiment, we conduct comparative evaluations using the CNNmodel on the CIFAR-10 dataset. presents the attack impact of our attack against the three defenses on CIFAR-10. Notably,our attack successfully leads to a substantial drop in global modelaccuracy, even in the presence of strong defense measures. For in-stance, when confronted with the state-of-the-art defense FLTrust,our attack reduces the convergence accuracy of the CNN model by8.98%. Byzantine robust aggregation defenses are effective againstattacks involving malicious model updates due to their reliance onspecific aggregation rules. However, these defenses face challengesin countering our attack, as they disrupt the generalization abilityof the model, making it difficult for the aggregation rules to enhancemodel generalization. 6.2.3Attack Performance under Anomaly Detection Based Defenses.To evaluate the attack performance of BadSampler and baselinesunder Byzantine-robust FL with anomaly detection defenses, we usethe PCA-based method , FoolsGold , and FLDetector defenses against the proposed attacks. Specifically, we evaluate theattack impact of BadSampler on CNN models on CIFAR-10 dataset.We fix the local training epoch = 5, the batch size = 32, andthe number of candidate adversarial training batches = 2. shows that our attacks still effectively increase the classification",
  "OursTop-54.2149.299.8154.7750.258.8555.6552.226.88Meta50.5446.2512.8551.9848.0211.0855.4150.658.45": "error of the aggregated model. For example, under the PCA-baseddefense, our attack using the CNN model as a classifier can causea 12.85% accuracy loss on the CIFAR-10 dataset. The main reasonis that anomaly detection-based defenses are less effective whenthe compromised client is not behaving maliciously. In addition,these defenses will also bring accuracy loss to the model due to thedimensionality reduction operation involved. 6.2.4Attack Performance under Different Hyperparameters. Wesummarize the experimental results as follows:Impact of the Parameter in BadSampler Attacks. shows the attack impact of different attack strategies as the per-centage of compromised clients increases on the CIFAR-10 datasetusing the CNN model. Experimental results show that our attacksignificantly degrades the aggregation models performance as theproportion of compromised clients increases. In particular, ourattack can still effectively attack the aggregated model when theproportion of compromised clients is merely 5%. Compared to othercommon data poisoning attacks, our attack is more robust and prac-tical.Impact of the Parameter in BadSampler Attacks. We explorethe effect of the poisoned sample size on Top- sampling attackperformance. In this experiment, we change the poisoned samplesize by adjusting the number of adversarial training batches ={1, 2, 4, 8}. shows that within a certain number range, themore adversarial training batches there are, the better the attackperformance is. For example, when = 4, our BadSampler attackperformance does not change significantly compared to it when = 1. The reason is that the error caused by our attack has an",
  "None82.5181.99052.3447.45036.6734.150Top- (Ours)71.8871.2910.732.9931.2016.2522.8222.3412.81Meta (Ours)80.2768.8413.1548.3635.4711.9831.9120.2913.86": "upper bound, that is, the number of adversarial training batchescannot be increased infinitely to enhance the attack performance.Impact of the Parameter in BadSampler Attacks. Here weshow that if we fix the dataset size, the larger the batch size, the lessthe number of adversarial training batches we can choose. reports the attack impact when = {16, 32, 64}. The experimentalresults show that the batch size gradually increases, and the impactof the attack gradually weakens, but it is still stronger than thelatest poisoning attacks. Furthermore, we emphasize that smallbatch size are commonly used for training in resource-constrainedscenarios . Therefore, our BadSampler attack is practical. 6.2.5Attack Performance under Non-IID Setting. Last, we evaluatethe attack performance under the non-IID data setting ( = 0.5).In this experiment, we fix the local training epoch = 5, thebatch size = 32, and the number of adversarial training batches = 2. We use the LR model for classification tasks on the FMNISTdataset and the CNN model and ResNet-18 model for classificationtasks on the CIFAR-10 dataset. shows that our attacks arestill effective in the non-IID setting. 6.2.6Effectiveness of BadSampler. Here, we will demonstrate whyBadSampler is effective from the perspective of training behavior,generalization ability evaluation, and generalization ability visual-ization. To this end, we use the following three metrics : thegradient cosine distance: Dcos, the difference in Hessian norm ( ),and the difference in Hessian direction (). We conducted fivesets of experiments and recorded the mean value of the experimen-tal results of these five sets of experiments in . The resultsshow that the training behavior of our attack is not very differentfrom that of benign clients, especially the meta-sampling attackstrategy. This means that it is difficult for sophisticated defendersto quickly determine which clients are malicious, which brings newchallenges to current advanced defenses. Furthermore, we use theparametric loss landscape visualization with Hessian featureeigenvectors ( and ) for the resulting global model be-fore and after the attack to show the impact of our attack on themodel generalization ability. From , we can clearly see thatthe loss landscape of the poisoned model changes more drasticallyand its optimization route is not smooth, even the route optimizedto the local optimum is extremely uneven. This means that thegeneralization ability of the poisoned model is greatly damaged.On the contrary, in the absence of attacks, the loss landscape of the",
  "global model changes steadily and its optimization route is smoothand flat. Please find more details in the Appendix D": "6.2.7Overhead of BadSampler. We recorded the running time ofthe attacks over ten rounds to underscore the low attack-effort char-acteristic of our approach. reveals that the proposed attacksrunning time is lower than LMPA and comparable to DOAs, whichsignifies the efficiency of our attack. This efficiency is attributed tothe utilization of Top- and RL techniques, which enable us to cir-cumvent complex optimization processes. Additional experimentalresults can be found in Appendix C.",
  "CONCLUSIONS": "In this work, we introduced BadSampler, a clean-label data poison-ing attack algorithm targeting Byzantine-robust FL. Such attacksare designed to maximize the generalization error of the model byperforming adversarial sampling on clean-label data while main-taining good training behavior during training. Furthermore, weemphasize that the attack is learnable and adaptive so that it canbe deployed to real FL scenarios and remains effective within strictparameter constraints. The proposed BadSampler attack undergoestheoretical analysis and experimental evaluation, demonstratingits effectiveness on moderate-scale public datasets with variousparameter ranges, defense strategies, and FL deployment scenarios,highlighting its significant threat to real FL systems.",
  "APRODUCTION FEDERATED LEARNING": "Production FL Settings. Prior works make severalunrealistic assumptions in FL poisoning attacks in terms of thetotal number of clients, the percentage of compromised clients,and the knowledge held by the adversary. This means that thecurrent literature is still far from practice. Our approach focuseson creating a new poisoning attack in this realistic environmentto close this gap. Briefly, the difference between the production FLenvironment and the FL environment in the literature is the range ofpractical FL parameters. Following the work , we summarize thedifferences between the practical FL parameters (called productionFL parameters ) and the ones used in existing attacks in .Nonetheless, in this paper, our attack still needs to effectively attackthe FL system in such a setting. Notably, to confirm the efficacy ofour approach, our experimental setup adheres to every practicalparameter setting in .",
  "CADDITIONAL EXPERIMENTS": "IID and Non-IID setting: (i) IID: Here we detail our IID settings.For the CIFAR-10 dataset, we allocate 860 samples to each clienton average; that is, a total of 48,000 data samples are used as thetraining set, and the server stores 12,000 samples as the test set.For the F-MNIST dataset, we allocate 1,020 data samples to eachclient on average, that is, a total of 56,000 data as the local trainingset, and the server stores 14,000 samples as the test set. (ii) Non-IID: We adopt the classic non-IID setting where eachclient can only hold at most two classes of data samples. To do this,we distribute the dataset equally to each client by data class. Forexample, for client and CIFAR-10 dataset, its local dataset onlyhas two kinds of data samples with category code 0 and 1.",
  "upload the model updates to the server": "Impact of the Non-IID Degree in BadSampler Attacks. Here,we verify the performance of BadSampler on the CIFAR-10 datasetand set the Dirichlet parameter = {0.1, 0.5, 1}. The experimentalresults are shown in .Impact of the Surrogate Models in BadSampler Attacks. Here,we add a set of additional experiments to explore the impact ofdifferent surrogate models on the attack effectiveness of the pro-posed attack. We set the batch size = 32, the number of candidateadversarial training batches = 2, and use FedAvg as AGR. Weuse the CNN model to conduct classification tasks on the CIFAR-10dataset, and we use the LeNet-5, DenseNet, and AlexNet models asour surrogate models. summarizes the experimental resultsshowing that the surrogate model does not significantly affect theperformance of the proposed attack.",
  "DWHY BADSAMPLER WORK?": "Here, we will demonstrate why BadSampler is effective from theperspective of training behavior, generalization ability evaluation,and generalization ability visualization.Training Behavior: To demonstrate the training behavior of thedesigned attack, we use the cosine distance of the gradient betweenclients to represent it. Specifically, we divide the clients into twogroups, benign and malicious, and then we calculate the cosinedistances of gradients between benign clients and between benign",
  "||g || ||g ||,(13)": "where g represents the gradient vector of the client. Here, we stipu-late that the larger the cosine distance is, the closer the directionof gradient descent between clients is, which means the closer thetraining behavior is. In the experiment, we conducted five sets ofexperiments and recorded the mean value of the experimental re-sults of these five sets of experiments in . The results showthat the training behavior of our attack is not very different fromthat of benign clients, especially the meta-sampling attack strategy.This means that it is difficult for sophisticated defenders to quicklydetermine which clients are malicious, which brings new challengesto current advanced defenses.Generalization Ability Visualization: In the machine learninggeneralization research domain , top Hessian eigenvalues() and Hessian trace ( ) are generally used as key indicatorsfor evaluating model generalization capabilities. In practice, a net-work with lower and is generally a network with strongergeneralization ability, that is, the network is less sensitive to smallperturbations. This is because lower and indicate a morebalanced loss space during training and a flatter route to the mini-mum point. Therefore, we use the parametric loss landscape visualization with Hessian feature eigenvectors ( and )for the resulting global model before and after the attack to show",
  "None49.1447.45052.3447.45054.6748.740Top- (Ours)42.8528.9818.4751.8832.1416.6022.8222.3412.81Meta (Ours)37.7526.2521.2048.3635.4711.9852.8721.3427.40": "the impact of our attack on the model generalization ability. From, we can clearly see that the loss landscape of the poisonedmodel changes more drastically and its optimization route is notsmooth, even the route optimized to the local optimum is extremelyuneven. This means that the generalization ability of the poisonedmodel is greatly damaged. On the contrary, in the absence of anattack, the loss landscape of the global model changes steadily andits optimization route is smooth and flat.Generalization Ability Evaluation: To further quantify themodels generalization error, we follow reference to compute",
  ": Visual overview of Hessian eigenvalue distributions for benign versus poisoned models": "the difference in Hessian norm ( ) and the difference in Hessiandirection () across clients. More specifically, can be usedto represent the closeness of model generalization ability betweenclients and can be used to represent the correlation of trainingperformance. We consider that the model generalization ability ofbenign clients is generally good, so the above indicators can be usedto indirectly quantify the model generalization ability of maliciousclients. The formal definition of the above indicators is as follows:",
  ",(15)": "where is the dot product, Diag (H) is the diagonal matrix of theHessian matrix, and || || is the Frobenius norm. In the experiment,we conducted five sets of experiments and recorded the mean valueof the experimental results of these five sets of experiments in . From the experimental results, we know that our attack strategycan seriously damage the generalization ability of the model, butthe correlation between the training performance of the Top-sampling strategy and the benign client is relatively different. Thismay be the reason why the Top- sampling strategy is easier todefend than the meta-sampling attack strategy."
}