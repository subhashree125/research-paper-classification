{
  "Abstract": "Temporal Graph Networks (TGNs) have demonstrated their remark-able performance in modeling temporal interaction graphs. Theseworks can generate temporal node representations by encodingthe surrounding neighborhoods for the target node. However, aninherent limitation of existing TGNs is their reliance on fixed, hand-crafted rules for neighborhood encoding, overlooking the necessityfor an adaptive and learnable neighborhood that can accommo-date both personalization and temporal evolution across differenttimestamps. In this paper, we aim to enhance existing TGNs byintroducing an adaptive neighborhood encoding mechanism. Wepresent SEAN (Selective Encoding for Adaptive Neighborhood),a flexible plug-and-play model that can be seamlessly integratedwith existing TGNs, effectively boosting their performance. Toachieve this, we decompose the adaptive neighborhood encodingprocess into two phases: (i) representative neighbor selection, and(ii) temporal-aware neighborhood information aggregation. Specifi-cally, we propose the Representative Neighbor Selector component,which automatically pinpoints the most important neighbors for thetarget node. It offers a tailored understanding of each nodes uniquesurrounding context, facilitating personalization. Subsequently, we",
  "Corresponding author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08 propose a Temporal-aware Aggregator, which synthesizes neigh-borhood aggregation by selectively determining the utilization ofaggregation routes and decaying the outdated information, allowingour model to adaptively leverage both the contextually significantand current information during aggregation. We conduct exten-sive experiments by integrating SEAN into three representativeTGNs, evaluating their performance on four public datasets andone financial benchmark dataset introduced in this paper. The re-sults demonstrate that SEAN consistently leads to performanceimprovements across all models, achieving SOTA performance andexceptional robustness.",
  "Temporal Graph Networks; Representation Learning; Data Mining": "ACM Reference Format:Siwei Zhang, Xi Chen, Yun Xiong, Xixi Wu, Yao Zhang, Yongrui Fu, Ying-long Zhao, and Jiawei Zhang. 2024. Towards Adaptive Neighborhood forAdvancing Temporal Interaction Graph Modeling. In Proceedings of the30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining(KDD 24), August 2529, 2024, Barcelona, Spain. ACM, New York, NY, USA,12 pages.",
  "At time \"!$At time \"!$": ": Comparison between fixed and adaptive neigh-borhoods during the encoding process. (a) Existing TGNs always adopt fixed rules for neighborhood en-coding, e.g., encode 2-hop neighborhoods. (b) We proposeadaptive neighborhood encoding to facilitate both the per-sonalized and temporal understanding of a target node. extensive research has been conducted on developing TemporalGraph Networks (TGNs) . These TGNs can gener-ate temporal node representations by encoding the neighborhoodsaround the target node, thus enabling downstream predictions.Despite the remarkable success of existing TGNs, a fundamentalweakness inherent in their designs is the reliance on the fixed,hand-crafted rules for neighborhood encoding, failing to accountfor the necessary personalization and the temporal evolution acrossdifferent timestamps. For example, when generating the representa-tion for a target node at a specific timestamp, as shown in a,existing TGNs, like TIGE , indiscriminately encode the current2-hop neighborhood. Different from such existing works, we em-phasize that an adaptive neighborhood encoding mechanism iscrucial for generating more expressive representations.Personalization. Personalization is essential for TIG represen-tation learning at both graph-scale and node-scale levels. Differ-ent TIGs exhibit distinct characteristics , and a universal, pre-defined rule for neighborhood encoding is inflexible . For ex-ample, while the -hop assumption may work well for low-densityTIGs, it could lead to indistinguishable representations in denserones . Additionally, even within a single TIG, the suitable neigh-borhoods for nodes can differ significantly. This hypothesis is rea-sonable due to the diverse neighborhood connectivity patterns andpotential noise. For instance, in financial networks, transactionpatterns of banks arise from different factors , and oftencontain noise irrelevant to their primary financial interests .Temporal evolution. A suitable neighborhood for a target nodeshould adapt to different timestamps to align with the temporalevolution of TIGs, necessitating a temporal-aware design for neigh-borhood encoding. For example, investors preferences may changeaccording to economic cycles, such as preferring technology stocksduring technological booms while consumer staples during eco-nomic downturns. Fixed rules for neighborhood encoding cannotaccommodate to such preference shifts in the temporal dimension,leading to suboptimal representations for effectively capturing thesechanging preferences. Based on the aforementioned motivations, as depicted in Fig-ure 1b, it becomes critical to allow for a more flexible, scalable, androbust algorithm that performs adaptive neighborhood encodingobeying both personalization and temporal awareness.In this paper, we aim to enhance existing TGNs by adaptivelyencoding personalized and temporal-aware neighborhoods throughthe introduction of a convenient plug-and-play model, referred toas SEAN (Selective Encoding for Adaptive Neighborhood). OurSEAN can significantly boost existing TGNs, and it comprises twomain components: (i) Representative Neighbor Selector. To effec-tively select personalized neighborhoods for nodes within TIGs, weintroduce Representative Neighbor Selector. It refines the neigh-borhood by choosing representative neighbors who are importantto the target node. However, an overemphasis on these neighborscan result in a homogeneous neighborhood that lacks the neces-sary diversity . Therefore, we incorporate a penalty mechanismthat penalizes the over-concentration of neighbors, maintaining abalanced and personalized neighborhood for the target node. (ii)Temporal-aware Aggregator. To achieve temporal-aware neighbor-hood encoding, we further propose the Temporal-aware Aggregatorthat automatically aggregates neighborhoods with the temporal un-derstanding of the target node. Specifically, it takes charge of neigh-borhood aggregation by explicitly determining the utilization ofeach aggregation route, facilitating the adaptive route aggregationor pruning as needed. Meanwhile, our outdated-decay mechanismstrategically de-emphasizes those outdated routes, allowing ourmodel to concentrate on more fresh and up-to-date information.In summary, our main contributions are: We focus on adaptive neighborhood encoding for temporal in-teraction graph (TIG) modeling and propose SEAN. SEAN is thefirst model proposed to automate neighborhood encoding in TIGmodeling, and it can be easily adopted to boost existing TGNs.",
  "We develop a Representative Neighbor Selector, which enablesthe model to effectively choose representative neighbors for thetarget node, achieving personalized neighborhood encoding": "We propose a Temporal-aware Aggregator, which aggregatesneighborhoods by explicitly determining the utilization of aggre-gation routes and decaying the outdated information, facilitatingtemporal-aware neighborhood encoding. We conduct extensive experiments on several TIG benchmarkdatasets to validate SEANs effectiveness. Furthermore, we in-troduce TemFin, a new financial transaction TIG benchmarkdataset in this paper. This benchmark represents a more challeng-ing environment due to its complexity in the financial domain.",
  "Related Work2.1Temporal Graph Networks (TGNs)": "Temporal Graph Networks (TGNs) are designed to generate tempo-ral node representations of TIGs by encoding the neighborhoodsfor the target node at any given timestamp . Theytypically encode their neighborhoods based on a fixed, pre-definedrule. According to how these neighborhoods are pre-defined, exist-ing TGNs can be categorized into two types: Breadth-First SearchTGNs (BFS-TGNs) and Depth-First Search TGNs (DFS-TGNs).",
  "Towards Adaptive Neighborhood for Advancing Temporal Interaction Graph ModelingKDD 24, August 2529, 2024, Barcelona, Spain": ": Average Precision (%) results on Temporal Link Prediction task under both transductive and inductive settings. Resultswith the pink background represent the performance and corresponding improvements achieved by integrating our modelinto three backbones. The best results are highlighted in bold, and * denotes the benchmark dataset introduced in this paper.",
  "Adaptive Neighborhood for Graph Learning": "Graph learning with adaptive neighborhoods aims to encode unde-tected or enhanced neighborhoods for better representation learn-ing, which has achieved remarkable performance on static graphs.Early works encode adaptive neighborhoods byutilizing Reinforcement Learning (RL). However, these works facesignificant limitations due to the immense search space and the non-differentiability of models, resulting in substantial computationaloverhead. To address this issue, GeniePath employs a differen-tiable module for adaptive neighborhood encoding, reducing themodel complexity and achieving better performance. Subsequently,an increasing number of adaptive neighborhood encoding meth-ods have been proposed, significantlyempowering the representation learning on static graphs.These existing methods cannot be directly adopted in TIGs, pri-marily because TIGs exhibit significant temporal differences com-pared to static graphs. For instance, TIGs are consistently char-acterized by a sequence of timestamped interactions, rather thanby adjacency matrices that are typical in static graphs, due to thedynamic nature of TIGs.",
  "Preliminaries3.1Problem Formulation": "Definition 3.1. Temporal Interaction Graph. Given1 a node setV = {1, ..., |V|}, a temporal interaction graph can be representedas a sequence of edge set E = {(, ,)}, where , V and > 0.Each edge (, ,) denotes an interaction between node and node that happened at time with the feature e (). The edge featurevector can depict various information about the interactions amongnodes, e.g., interaction type. Any edge (, ,) E only has accessto its historical data before time , i.e., {(, ,) E| < }. Definition 3.2. Temporal Interaction Graph Modeling. Givenan edge (, ,) E and its historical data {(, ,) E| < }, tem-poral interaction graph modeling aims to learn a mapping function : (, ,) z (), z (), where z (), z () R represent therepresentations of nodes and at time , respectively, and is therepresentation vector dimension.",
  "Temporal Embedding Module": "As a crucial component within most existing TGNs, temporal em-bedding module is responsible for generating temporal repre-sentations for any given node at time , z (), which involves a-layer temporal attention network to encode s current neighbor-hood. In this section, we generalize various temporal embeddingmodules employed in different TGNs into a cohesive framework.This unification allows us to provide a comprehensive overview ofthe neighborhood encoding process within TGNs.For the target node at time of the -th layer, {1, ..., }, tem-poral embedding module aggregates s neighborhood informationh()() and s representation from the previous layer h(1)()using a Multi-Layer Perceptron (MLP) as the aggregator:",
  "and each row v() () is the message carried from neighbor , which": "is determined by v() () = v(h(1)()). In the above equations,()( {q, k, v}) represents the encoding functions for queries,keys, and values, respectively . These functions may havedifferent specific representations for different TGNs .For simplicity, we consider a single-head attention formula in thispaper. By incorporating personalized and temporal-aware designsinto the neighborhood attention mechanism (Equation 3) and theaggregation process (Equation 1), we can elevate this module toachieve adaptive neighborhood encoding. This innovative designenables our model to serve as a seamlessly integratable plug-and-play enhancement for various existing TGNs, broadening theirflexibility and effectiveness.",
  "Methodology": "As we have mentioned, adaptive neighborhood encoding requiresboth personalized and temporal considerations. Therefore, we de-compose this process into two phases, i.e., a Representative Neigh-bor Selector first assigns distinctive attention to select importantneighbors for personalization, and our Temporal-aware Aggregatorthen aggregates the neighborhood information to ensure temporalrelevance. We will introduce these components in the followingsubsections.",
  "routes": ": Framework of the proposed plug-and-play model. Our Representative Neighbor Selector can empower the modelto pinpoint the important neighbors, who then act as the personalized representatives for the target node. Meanwhile, wepropose the neighbor diversity penalty to penalize the over-concentration of these neighbors, thus maintaining a more balancedneighborhood. Furthermore, we conduct our Temporal-aware Aggregator by LSTM aggregation, where we propose an adaptivepruning module that explicitly determines whether to aggregate information from the given route or to prune it as needed,and an outdated-decay mechanism that de-emphasizes the outdated information. Finally, we generate the temporal noderepresentations for downstream tasks by extracting the encoded neighborhood information from the -th layer of SEAN.",
  "We propose the occurrence-aware attention and neighbor diversitypenalty mechanisms to select personalized representative neigh-borhoods for the target node": "4.1.1Occurrence-aware Attention Mechanism. In Equation 3, theattention score is calculated based on the semantic correlation be-tween the target node and its neighboring nodes. However, thissemantics-based attention computation may be suboptimal to se-lect representative neighborhoods due to its lack of personalizationand adaptability . Here, we propose the occurrence-awareattention mechanism to enhance the traditional semantics-basedattention mechanism for selecting important neighbors as personal-ized representatives. Intuitively, neighbor occurrence, which quan-tifies how frequently a neighbor occurs, is a significant signal inidentifying the importance of each neighbor for a given target node.Frequently occurring neighbors are more likely to be importantand possess more relevant information, thus making them moreeffective and convincing representatives.Formally, for node at time and its historical neighbors N (),we first count how often each neighbor has occurred historicallyand derive it to a one-dimensional occurrence frequency feature,which is represented by f () R|N ( ) |1. To balance the rawcounts, we then normalize this feature by dividing its temporalnode degree d () R|N ( ) |1, which is denoted as:",
  "f () = f () d1() R|N ( ) |1,(4)": "where denotes the element-wise product operation. The nor-malized occurrence frequency feature vector both considers theinteraction frequency and mitigates the unexpected adverse im-pacts from the hub nodes (i.e., those with large degrees), which are known as degree-related biases . To enhance the expres-siveness of the occurrence information, we apply a function e()to encode our normalized occurrence frequency feature f () into ahigh-dimensional occurrence encoding as follows:",
  "R () = ef () R|N ( ) |.(5)": "Similar to DyGFormer , we implement e() by a two-layer MLPwhose input and output dimensions are 1 and , respectively.To further enrich the occurrence-aware attention computation,we incorporate both edge attributes and temporal information. Foreach neighbor node N (), we retrieve its occurrence encodingas r () = R,:() R. Our occurrence-aware attention scorefrom node to node at time , (), can be represented by:",
  "() = tanh w [r ()e ()( )] ,(6)": "where () is the commonly used time encoding function in TGNs, and w R13 represents the trainable reshaping vector.Finally, we update the traditional attention score () with ournew occurrence-aware attention score () to obtain the finalattention score as () = () + (). This combined score inte-grates both semantic relevance and occurrence frequency, enablingour model to select important neighbors as representatives. 4.1.2Neighbor Diversity Penalty. Although our occurrence-awareattention greatly helps the model in identifying the most importantneighbors as the representatives for the target node, an overempha-sis on these neighbors can result in a homogeneous neighborhoodthat lacks the necessary diversity. For example, in the world offinance, an abundance of homogeneous investment choices maylead to the undesirable phenomenon known as the Financial EchoChamber , analogous to the Information Cocoon in Recom-mender Systems . To address this issue, we propose an",
  "h() = AGG() h(1)= LSTM() h(1),h(), c().(9)": "Here, we fetch the hidden state from the -th LSTM cell h() as ouroutput representation of the-th aggregation layer AGG() (). Theinputs are three folds: the output representation (i.e., hidden state)from the previous layer h(1), the refined neighborhood informa-tion h() that incorporates our proposed attention mechanism, andthe cell state from the -th LSTM c(). To fully achieve temporal-aware encoding, we enhance our LSTM aggregator through anadaptive pruning module to facilitate selective aggregates of h(),and an outdated-decay module to enhance the temporal sensitivityof the cell state c(). Note that we simplify our notation by omittingthe subscript term and the time . In practice, for node at time ,we determine its temporal representation by extracting the outputrepresentation from the last layer, i.e., z () = h()(). 4.2.1Adaptive Pruning Module. This module not only encouragesour model to focus on incorporating more crucial information forperformance improvement (.7) but also enhances the modelinterpretability, which explicitly specifies the models decision-making process in seeking adaptive neighborhoods (.6).",
  "(+1) = () () +1 () () + max (), 1 (),(13)": "where W R is the learnable matrix, b is the learnable vector,and () is the sigmoid function. During aggregation, the aggre-gated neighborhood progressively becomes closer to the centralnode as the number of aggregation layers increases. It means thathigher aggregation layers aggregate more central neighborhoodsthat have been proven to hold more vital information . Thisformulation encodes the observation that the likelihood of aggre-gation increases with the number of aggregation layers. Wheneverthe -th layer aggregation is omitted, the pre-activation of the layerstate for the following layer (+1) is incremented by (). Con-versely, if the -th layer aggregation is performed, the accumulatedvalue will be reset and (+1) = (). In this way, our model canexplicitly determine the route aggregation or pruning as needed. 4.2.2Outdated-decay Module. Outdated-decay module aims tode-emphasize those outdated aggregation routes, ensuring the in-corporation of more fresh and up-to-date information.We believe that the long-term cell state retains important andmore enduring information, while the short-term cell state is uti-lized for processing immediate information. Therefore, we firstdecompose the -th layer cell sate c() into two elements, i.e., theshort-term cell state c()and the long-term cell state c(). We have:",
  "c()= c() c(),(15)": "where matrix W R and vector b are learnable parameters.The short-term cell state c()is initially produced by a neuralnetwork with the activation function. Subsequently, the long-termcell state c()can be distinguished from the original cell state c().To prevent from forgetting too rapidly, we keep our long-termcell state c()untouched and forget the short-term cell state c()according to the time interval = between the current time and the last interaction time of the neighbor in the correspondingroute . We apply a decay mechanism as follows:",
  "Training": "4.3.1Error Gradients. The entire model is differentiable exceptfor the round process round() in Equation 10. In this paper, weemploy the straight-through estimator to allow all parametersto be trained efficiently without the need for any extra supervisionsignals. This involves approximating the step process with identityduring gradient computation in the backward pass: round()",
  "= 1": "4.3.2Loss Function. We employ temporal link prediction as ourself-supervised task for training. Specifically, for each link (, ,),we calculate its occurrence probability () with the concatenatedtemporal representations of interaction nodes, i.e., z () and z (),through a two-layer MLP . Then, we compute the loss functionby the cross entropy of the link prediction task as follows:",
  "Complexity Analysis": "We provide a detailed complexity analysis of SEAN where we showthe complexity of both components. The Representative Neigh-bor Selector module assigns distinct attention scores to nodes andalso provides the neighbor diversity penalty loss, leading to thecomputational complexity of O(|V|), where , |V|, refer tothe total number of stacking layers, nodes, and the embeddingdimension, respectively. The Temporal-aware Aggregator is imple-mented based on the LSTM module, therefore leading to a com-plexity of O(|V|2). Therefore, the overall complexity of SEANis O(|V| + |V|2), maintaining efficiency as it scales linearlywith the number of nodes.We want to emphasize that the temporal embedding modulementioned in .2 in most existing TGNs also incorporatesan explicit attention assignment stage. In this case, integratingSEAN only leads to an additional overall complexity of O(|V|2).",
  "Experiments5.1Experimental Settings": "5.1.1Datasets. We conduct experiments with five datasets, includ-ing four public datasets and one dataset introduced in ourpaper. The four public datasets - Wikipedia, Reddit, MOOC, andLastFM - are widely used in temporal interaction graph modeling.Meanwhile, recognizing that these available datasets are social orevent networks, we release TemFin, a new TIG benchmark datasetfrom the complicated financial domain. TemFin consists of halfa month of transactions sampled from a private financial trans-fer transaction network in the Ant Finance Group.3 Details of alldatasets are described in in the Appendix due to the pagelimitations. All datasets are sequentially split according to the edgetimestamp order by 70%, 15%, and 15% for training, validation, andtesting, respectively . 5.1.2Baselines. For comparison, we choose nine existing TGNsas our baselines, and summarize the detailed description of ourbaselines as follows: DyRep : It is a notable implementation of the temporal pointprocess in its neighborhoods and introduces a projection layerthat estimates user representation in the future.",
  "Transductive settingInductive settingWikipediaRedditMOOCLastFMTemFin*WikipediaRedditMOOCLastFMTemFin*": "JODIE94.62 0.591.11 0.376.50 1.868.77 3.088.02 0.293.11 0.494.36 1.177.83 2.182.55 1.975.35 1.6TGAT95.34 0.198.12 0.260.97 0.353.36 0.187.70 0.193.99 0.396.62 0.363.50 0.755.65 0.279.37 0.2DyRep94.59 0.297.98 0.175.37 1.768.77 2.187.99 0.192.05 0.395.68 0.278.55 1.181.33 2.176.53 0.1PINT98.78 0.199.03 .0185.14 1.288.06 0.790.79 0.198.38 .0498.25 .0485.39 1.091.76 0.781.18 0.2iLoRE98.98 0.399.11 0.490.44 1.091.39 0.190.90 0.198.60 0.398.65 0.389.75 0.893.29 0.884.33 0.2GraphMixer97.95 .0397.31 .0182.78 0.267.27 2.186.85 0.196.65 .0295.26 .0281.41 0.282.11 0.477.47 0.2",
  "DyRepPINTGraghMixerTGNTGN+(imprv.)TIGETIGE+(imprv.) DyGFormer DyGFormer+ (imprv.)": "Wikipedia 84.59 2.2 87.59 0.686.80 .0187.81 0.3 87.91 0.8 (+0.10)86.92 0.7 87.16 1.5(+0.24)87.07 0.887.49 0.6(+0.42)Reddit62.91 2.4 67.31 0.264.22 .0367.06 0.9 68.26 0.8(+1.20)69.41 1.3 70.24 2.2 (+0.83)68.30 1.569.06 2.5(+0.76)MOOC67.76 0.5 68.77 1.167.21 .0269.54 1.0 72.96 1.8(+2.04)72.35 2.3 73.85 2.0(+1.50)77.89 0.578.88 3.2(+0.99)TemFin*78.56 2.9 82.22 1.281.17 0.780.93 2.2 82.39 2.5(+1.46)80.52 3.0 83.17 2.2 (+2.65)72.50 0.473.05 2.1(+0.55)",
  "Temporal Link Prediction": "We start our experiments with the Temporal Link Prediction task,which aims to predict the probability of a link occurring betweentwo specific nodes at a certain time. We utilize two settings: thetransductive setting, which performs link prediction on nodes thathave appeared during training, and the inductive setting, which predicts links between unseen nodes. We randomly sample an equalnumber of negative nodes as detailed in Equation 18, and reportthe average precision (AP) performance with all three backbones,i.e., TGN , TIGE , and DyGFormer .The results are shown in . Clearly, we can observe thatall three backbone models show improved performance across alldatasets in both transductive and inductive settings after integrat-ing our model. Meanwhile, among the improved models, at leastone has achieved SOTA results, surpassing all baseline comparisons.This observation proves the effectiveness of the adaptive neighbor-hood encoding mechanism in TGNs. Moreover, we find the mostsignificant improvement between the backbones and their corre-sponding improved models specifically on MOOC and lastFM. Thismay be owing to their high neighborhood complexity, constrainingthe backbones effectiveness. In contrast, our model introduces anadaptive neighborhood encoding mechanism that effectively em-powers these backbones to better capture and utilize the criticalinformation in such challenging environments, demonstrating ourmodels effectiveness and robustness in handling complex TIGs.",
  ": Robustness to expanded neighborhoods on MOOC": "representation of node at time, z (), into a two-layer MLP, whichmaps the representations to the dynamic labels. We carry out theexperiments on Wikipedia, Reddit, MOOC, and TemFin. LastFM isexcluded due to its lack of node labels. We employ AUROC as ourevaluation metric due to the label imbalance issue in these datasets.We report the results in . All three improved modelsdemonstrate better performance than the backbones across alldatasets, and at least one of them has achieved the SOTA results.This indicates that the learned representations from our model canbe more effectively applied to downstream tasks, proving its supe-riority once again. Notably, the most significant improvement isobserved on MOOC and TemFin. This could be due to their rela-tively less severe label imbalance issue compared to others, therebyfacilitating the performance enhancement more readily.",
  "We have mentioned that the suitable neighborhoods for nodesshould vary due to the different connectivity patterns and potentialnoise. To simulate this scenario, we complicate and pollute the": "nodes neighborhoods by introducing random noise into TIGs .We then evaluate the model performance (both w/o and w/ SEAN)under these noisy conditions. Specifically, we replace the originallinks with our randomly sampled noisy links at a certain pertur-bation rate {10%, 20%, 30%, 40%, 50%}. Meanwhile, at = 50%,we collect the attention scores assigned from a subset of randomlyselected nodes to their perturbed neighbors, visualizing the modelsability to handle noise with box plots. We emphasize that thesecomparisons are controlled and fair because the noise issue doseexist in TIGs and the introduced noise is prevalent .The results are detailed in and 4. The improved modelsexhibit exceptional robustness, even when encountering significantnoise. This demonstrates the superiority of SEAN in encodingsuitable neighborhoods, thus weakening potential noise attacks. Asfor the perturbed neighbors scores, we find an obvious reductionand greater focus in the scores when comparing improved models totheir corresponding backbones. It suggests that SEAN successfullyempowers these backbones to evaluate and mitigate these same-level random noises introduced in nodes neighborhoods.",
  "Robustness to Expanded Neighborhoods": "We compare the performance across expanded neighborhoods be-tween the backbones and improved models. Specifically, we varythe aggregation layer {1, 2, 3, 4, 5} in models and carry outthe experiments using TGN and TIGE as the backbones.DyGFormer is not included in this group of experiments due toits limitation to the first-hop neighborhoods in the original design.In , we keep the neighbor sampling size of TGN at 10to ensure a fair comparison, which is its default hyper-parameter.However, during these experiments, maintaining the sampling sizeat 10 will lead to GPU out-of-memory issues rapidly. Therefore, wereduce the size to 5 in practice.The results are depicted in . The improved models also ex-hibit significant robustness with expanded neighborhoods, and theperformance improvement compared to their corresponding back-bones progressively widens as the neighborhood expands. It high-lights SEANs capability to adaptively encode the suitable neigh-borhoods, thus avoiding the over-smoothing issue caused bythe expanded neighborhoods. Moreover, fine-tuning the accessibleneighborhoods (i.e., the layer number ) is essential for optimizingthe backbone performance. In contrast, our improved models reachtheir peak performance by employing the largest possible accessibleneighborhoods, as long as the computational resources permit.",
  "Case Study": "We conduct the case study to interpret the choice-making processand model effectiveness when SEAN seeks the neighborhoods. Asshown in , each grid displays the encoded neighborhoodfor a certain node at a particular time, and dashed lines indicatethe aggregation routes pruned by SEAN. The values within thosedashed boxes are the Evolving Node Classification prediction prob-abilities for the backbone and the improved model, respectively.Here, higher values signify better performance. For clarity, we limitthe depiction only to 2-hop accessible neighborhoods for each node.In our observations from Node 16 in , the backbone doesnot consistently show improved performance over time. Conversely,the improved model performance continues to increase, indicatingSEANs ability to manage the increasing neighborhood complexityand progressively stronger potential noise. Another notable aspectis that, even within the same node such as Node 1648 in ,the improved model shows significant neighborhood differencesat distinct timestamps. It suggests that the improved model caneffectively encode the most suitable neighborhoods across differenttimestamps to achieve optimal performance.",
  "Ablation Study": "We conduct the ablation study on the main components of ourSEAN, including Representative Neighbor Selector (RNS) in Sec-tion 4.1, Temporal-aware Aggregator (TA) in .2, NeighborDiversity Penalty (NDP) in .1.2, Adaptive Pruning Module(APM) in .2.1, and Outdated-decay Module (OM) in .2.2. To assess the impact of each component, we remove themindividually, resulting in five variants: w/o RNS, w/o TA, w/o NDP,w/o APM, and w/o OM.We report the performance of the Temporal Link Prediction taskusing both the improved TGN model and its variants as representedin . Note that the green line represents the performanceof the TGN backbone. The model performs best when utilizing allcomponents, and the removal of any single component leads to adecrease in performance, highlighting the necessity of each design.Moreover, the performance of all variants surpasses the backbone,validating the effectiveness of our components. Additionally, ourTemporal-aware Aggregator component has the most significantcontribution to model performance, reaffirming the importance ofthe temporal-aware design for neighborhood encoding.",
  "Parameter Study": "Now, we study how the controlling hyper-parameter of the Neigh-bor Diversity Penalty ( in Equation 19) affects the performance.We plot the APs of the improved models with varying values of on MOOC in . When the penalty method is too strong( = 10), the model may struggle to learn information from thoseimportant neighbors, thus leading to a decrease in model perfor-mance. Additionally, different models have distinct sensitivities to changes. For example, TIGE requires a larger for optimal resultscompared to TGN, suggesting a more severe over-concentrationissue within the TIGE model.",
  "Conclusion and future work": "In this paper, we enhance temporal interaction graph modeling viaadaptive neighborhood encoding. We propose SEAN, a plug-and-play model designed to boost existing TGNs. By introducing theRepresentative Neighbor Selector, we can select the personalizedrepresentatives for each target node. The Temporal-aware Aggrega-tor then aggregates neighborhoods in a temporal-aware manner. Asfor future work, we will consider detecting the hidden routes thatdo not originally exist during the aggregation process. Additionally,we can also consider other implements for our Temporal-awareAggregator, such as Transformer blocks. This work is funded in part by the Shanghai Science and TechnologyDevelopment Fund No.22dz1200704, the National Natural ScienceFoundation of China Projects No. U1936213, NSF through grantsIIS-1763365 and IIS-2106972, and also supported by Ant Group. Thefirst author, Dr. Zhang, also wants to thank Yifeng Wang for hisefforts and support in this work.",
  "Vctor Campos, Brendan Jou, Xavier Gir-i Nieto, Jordi Torres, and Shih-Fu Chang.2017. Skip rnn: Learning to skip state updates in recurrent neural networks.arXiv preprint arXiv:1708.06834 (2017)": "Xi Chen, Yongxiang Liao, Yun Xiong, Yao Zhang, Siwei Zhang, Jiawei Zhang,and Yiheng Sun. 2023. SPEED: Streaming Partition and Parallel Acceleration forTemporal Interaction Graph Embedding. arXiv preprint arXiv:2308.14129 (2023). Xi Chen, Siwei Zhang, Yun Xiong, Xixi Wu, Jiawei Zhang, Xiangguo Sun, YaoZhang, Yinglong Zhao, and Yulin Kang. 2024. Prompt Learning on TemporalInteraction Graphs. arXiv preprint arXiv:2402.06326 (2024). Weilin Cong, Si Zhang, Jian Kang, Baichuan Yuan, Hao Wu, Xin Zhou, HanghangTong, and Mehrdad Mahdavi. 2023. Do We Really Need Complicated ModelArchitectures For Temporal Networks? arXiv preprint arXiv:2302.11636 (2023).",
  "Nicolas Keriven. 2022. Not too little, not too much: a theoretical analysis of graph(over) smoothing. Advances in Neural Information Processing Systems 35 (2022),22682281": "Srijan Kumar, Xikun Zhang, and Jure Leskovec. 2019. Predicting dynamic em-bedding trajectory in temporal interaction networks. In Proceedings of the 25thACM SIGKDD international conference on knowledge discovery & data mining.12691278. Kwei-Herng Lai, Daochen Zha, Kaixiong Zhou, and Xia Hu. 2020. Policy-gnn:Aggregation optimization for graph neural networks. In Proceedings of the 26thACM SIGKDD International Conference on Knowledge Discovery & Data Mining.461471. Ziqi Liu, Chaochao Chen, Longfei Li, Jun Zhou, Xiaolong Li, Le Song, and YuanQi. 2019. Geniepath: Graph neural networks with adaptive receptive paths. InProceedings of the AAAI Conference on Artificial Intelligence, Vol. 33. 44244431. Yang Luo, Zehao Gu, Shiyang Zhou, Yun Xiong, and Xiaofeng Gao. 2023.Meteorology-Assisted Spatio-Temporal Graph Network for Uncivilized UrbanEvent Prediction. In 2023 IEEE International Conference on Data Mining (ICDM).IEEE, 468477.",
  "Cass R Sunstein. 2006. Infotopia: How many minds produce knowledge. OxfordUniversity Press": "Qiaoyu Tan, Xin Zhang, Ninghao Liu, Daochen Zha, Li Li, Rui Chen, Soo-HyunChoi, and Xia Hu. 2023. Bring your own view: Graph neural networks for linkprediction with personalized subgraph selection. In Proceedings of the SixteenthACM International Conference on Web Search and Data Mining. 625633. Xianfeng Tang, Huaxiu Yao, Yiwei Sun, Yiqi Wang, Jiliang Tang, Charu Aggarwal,Prasenjit Mitra, and Suhang Wang. 2020. Investigating and mitigating degree-related biases in graph convoltuional networks. In Proceedings of the 29th ACMInternational Conference on Information & Knowledge Management. 14351444.",
  "Rakshit Trivedi, Mehrdad Farajtabar, Prasenjeet Biswal, and Hongyuan Zha. 2019.Dyrep: Learning representations over dynamic graphs. In International conferenceon learning representations": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin. 2017. Attention is allyou need. Advances in neural information processing systems 30 (2017). Yiwei Wang, Yujun Cai, Yuxuan Liang, Henghui Ding, Changhu Wang, SiddharthBhatia, and Bryan Hooi. 2021. Adaptive data augmentation on temporal graphs.Advances in Neural Information Processing Systems 34 (2021), 14401452.",
  "Zhili Wang, Shimin Di, and Lei Chen. 2021. Autogel: An automated graph neuralnetwork with explicit link information. Advances in Neural Information ProcessingSystems 34 (2021), 2450924522": "Zhili Wang, Shimin Di, and Lei Chen. 2023. A Message Passing Neural NetworkSpace for Better Capturing Data-dependent Receptive Fields. In Proceedings ofthe 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.24892501. Zhili Wang, Shimin Di, and Lei Chen. 2023. A Message Passing Neural NetworkSpace for Better Capturing Data-dependent Receptive Fields. In Proceedings ofthe 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.24892501. Lanning Wei, Huan Zhao, Quanming Yao, and Zhiqiang He. 2021. Pooling archi-tecture search for graph classification. In Proceedings of the 30th ACM InternationalConference on Information & Knowledge Management. 20912100. Sheng Xiang, Mingzhi Zhu, Dawei Cheng, Enxia Li, Ruihui Zhao, Yi Ouyang,Ling Chen, and Yefeng Zheng. 2023. Semi-supervised credit card fraud detectionvia attribute-driven graph representation. In Proceedings of the AAAI Conferenceon Artificial Intelligence, Vol. 37. 1455714565.",
  "Huimin Xu, Zhicong Chen, Ruiqi Li, and Cheng-Jun Wang. 2020. The geometry ofinformation cocoon: Analyzing the cultural space with word embedding models.arXiv preprint arXiv:2007.10083 (2020)": "Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichiKawarabayashi, and Stefanie Jegelka. 2018. Representation learning on graphswith jumping knowledge networks. In International conference on machine learn-ing. PMLR, 54535462. Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He,Yanming Shen, and Tie-Yan Liu. 2021. Do transformers really perform badlyfor graph representation? Advances in Neural Information Processing Systems 34(2021), 2887728888.",
  "Chris Zhang, Mengye Ren, and Raquel Urtasun. 2018. Graph hypernetworks forneural architecture search. arXiv preprint arXiv:1810.05749 (2018)": "Siwei Zhang, Yun Xiong, Yao Zhang, Yiheng Sun, Xi Chen, Yizhu Jiao, andYangyong Zhu. 2023. RDGSL: Dynamic Graph Representation Learning withStructure Learning. In Proceedings of the 32nd ACM International Conference onInformation and Knowledge Management. 31743183. Siwei Zhang, Yun Xiong, Yao Zhang, Xixi Wu, Yiheng Sun, and Jiawei Zhang.2023. iLoRE: Dynamic Graph Representation with Instant Long-term Modelingand Re-occurrence Preservation. In Proceedings of the 32nd ACM InternationalConference on Information and Knowledge Management. 32163225. Wei Zhang, Xiaogang Wang, Deli Zhao, and Xiaoou Tang. 2012. Graph degreelinkage: Agglomerative clustering on a directed graph. In Computer VisionECCV2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13,2012, Proceedings, Part I 12. Springer, 428441. Yao Zhang, Yun Xiong, Dongsheng Li, Caihua Shan, Kan Ren, and Yangyong Zhu.2021. CoPE: Modeling Continuous Propagation and Evolution on InteractionGraph. In Proceedings of the 30th ACM International Conference on Information &Knowledge Management. 26272636.",
  "A.2Datasets": "We employ five datasets, including four public datasets and onedataset introduced in our paper.The four public datasets4 used in our paper are provided byJODIE . (i) Wikipedia is a temporal interaction graph capturingedits made by users on Wikipedia pages. (ii) Reddit is a temporalinteraction graph recording user posts in different subreddits. Inboth Wikipedia and Reddit, the edge feature dimension is 172, anduser nodes are dynamically labeled to indicate whether they havebeen banned. (iii) MOOC is a temporal interaction graph that tracksinteractions between students and online courses, with dynamiclabels on nodes indicating whether students drop out of a course.(iv) LastFM is a temporal interaction graph that logs events betweenusers and songs, but it does not include dynamic labels.Most existing temporal interaction graph datasets are primarilyfocused on social or event networks. However, interaction datain the financial domain differs significantly. It typically involvesa considerably larger number of participants, resulting in sparsernetworks. Consequently, learning tasks on such networks tendto be more challenging compared to those on existing availablenetworks. This increased difficulty provides a more rigorous testbedfor validating the capabilities of various TGNs, making a moreeffective benchmark to assess their performance and robustness.In this paper, we release TemFin, a new temporal interactiongraph benchmark dataset that records the financial transfer trans-actions between bank accounts. Within TemFin, bank accounts arerepresented as nodes, and the financial transactions with times-tamp information occurring between two accounts are modeled astimestamped interactions. Additionally, the information about thetransactions, e.g., amount and fund channel, is converted into a 154-dimensional vector through one-hot encoding, which then servesas the edge feature for each transaction. Furthermore, the sourceaccount nodes are assigned dynamic labels, indicating whether thecorresponding account is implicated in money laundering activity.In the TemFin dataset, the Temporal Link Prediction task is designedto forecast whether one account will transfer funds to another at a given future time. Meanwhile, the Evolving Node Classificationtask concentrates on determining whether a source account in aspecific transaction is implicated in money laundering activity. Thedetailed statistics of all datasets are summarized in .",
  "A.3Implementation Details": "For baselines, all baselines employ the same experimental settingsas TIGE , thus ensuring comparability in our evaluation pro-cess. The detailed default hyper-parameters can be found in itspublication.For our SEAN, we integrate it into three representative TGNs,i.e., TGN , TIGE , and DyGFormer , chosen for theirsuperior performance and widespread recognition. To guaranteefairness in our evaluation, all hyper-parameters in these three back-bones remain consistent with their original implementations. Weimplement SEAN in PyTorch based on their corresponding offi-cial implementations. Unless otherwise stated, we use the defaulthyper-parameters of SEAN summarized in .Experiments on all datasets are conducted on a single serverwith 72 cores, 32GB memory, and one Nvidia Tesla V100 GPU.",
  "We consider two main limitations for our SEAN: While SEAN enhances model performance, it inevitably intro-duces some complexity, which may be unavoidable": "Our proposed adaptive pruning module serves as a residual linkin the aggregation process and improves the overall model per-formance. However, when the neighborhood is extremely sparse,the pruning mechanism might lose some useful information.We will address these limitations in our future work by explor-ing more efficient adaptive neighborhood encoding methods intemporal interaction graph modeling.",
  "A.5Supplementary Results": "Due to space constraints in the main body of our paper, we presentthe following supplementary results: the robustness study to noisyneighborhoods is depicted in and 10, the robustness studyto expanded neighborhoods is shown in , additional casestudy with more other selected nodes are illustrated in ,and the ablation study is detailed in ."
}