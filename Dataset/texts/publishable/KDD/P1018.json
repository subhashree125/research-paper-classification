{
  "ABSTRACT": "In this paper, we present a novel method to significantly enhancethe computational efficiency of Adaptive Spatial-Temporal GraphNeural Networks (ASTGNNs) by introducing the concept of theGraph Winning Ticket (GWT), derived from the Lottery TicketHypothesis (LTH). By adopting a pre-determined star topologyas a GWT prior to training, we balance edge reduction with effi-cient information propagation, reducing computational demandswhile maintaining high model performance. Both the time andmemory computational complexity of generating adaptive spatial-temporal graphs is significantly reduced from O( 2) to O(). Ourapproach streamlines the ASTGNN deployment by eliminating theneed for exhaustive training, pruning, and retraining cycles, anddemonstrates empirically across various datasets that it is possibleto achieve comparable performance to full models with substan-tially lower computational costs. Specifically, our approach enablestraining ASTGNNs on the largest scale spatial-temporal datasetusing a single A6000 equipped with 48 GB of memory, overcom-ing the out-of-memory issue encountered during original trainingand even achieving state-of-the-art performance. Furthermore, wedelve into the effectiveness of the GWT from the perspective ofspectral graph theory, providing substantial theoretical support.This advancement not only proves the existence of efficient sub-networks within ASTGNNs but also broadens the applicability ofthe LTH in resource-constrained settings, marking a significantstep forward in the field of graph neural networks. Code is availableat",
  "Corresponding author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08",
  "spatial-temporal graph neural network, lottery ticket hypothesis,spatial-temporal data mining": "ACM Reference Format:Wenying Duan, Tianxiang Fang, Hong Rao, and Xiaoxi He. 2024. Pre-Training Identification of Graph Winning Tickets in Adaptive Spatial-TemporalGraph Neural Networks. In Proceedings of the 30th ACM SIGKDD Con-ference on Knowledge Discovery and Data Mining (KDD 24), August 2529, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.",
  "INTRODUCTION": "Spatial-Temporal Graph Neural Networks (STGNNs) have estab-lished themselves as a formidable tool for mining the hidden pat-terns present in spatial-temporal data, displaying remarkable pro-ficiency in modeling spatial dependencies via graph structures. The construction of these spatial graphs is a pivotal aspectof STGNNs, in which the complex and implicit nature of spatial-temporal relationships has paved the way for the recently emergingself-learned methods that dynamically generate graphs to capturethese intricate dependencies in a data-driven manner. AdaptiveSpatial-Temporal Graph Neural Networks (ASTGNNs), a state-of-the-art approach to spatial-temporal data processing, are partic-ularly adept at creating adaptive graphs through learnable nodeembeddings, as exemplified by models such as Graph WaveNet and AGCRN .Despite their advanced performance, ASTGNNs are encumberedby substantial computational overheads during both the trainingand inference phases, primarily due to the exhaustive calculationsrequired for learning the adaptive adjacency matrices of completegraphs, and the computationally intensive nature of the aggrega-tion phase. This presents a significant challenge when dealing withlarge-scale spatial-temporal data, where computational efficiency isparamount. Pioneering work has explored this aspect, improv-ing the efficiency of ASTGNNs during inference via sparsificationof the spatial graph. However, the sparsification of the spatial graphrelies heavily on the training framework and can only be conducted",
  "KDD 24, August 2529, 2024, Barcelona, SpainWenying Duan, Tianxiang Fang, Hong Rao, and Xiaoxi He": "Proof of Lemma 5. Assume 1, . . . , are the eigenvalues ofThe Laplacian of K , and 1, . . . , are the eigenvalues of theLaplacian of T . For i =1, = , = , satisfying / _ . For 2 1, = , = 1, satisfying / .",
  "RELATED WORK2.1Spatial-Temporal Graph Neural Networks": "The analysis of spatial-temporal data necessitates an understand-ing of dynamic interactions within time-varying signals acrossspatial domains. Spatial-Temporal Graph Neural Networks(STGNNs) are proficient in uncovering latent patterns in thesegraph-structured data . A key characteristic of STGNNs istheir capability to model spatial dependencies among nodes, effec-tively learning adjacency matrices. Depending on their approachto constructing these matrices, STGNNs can be categorized intopre-defined and self-learned methods.Pre-defined STGNNs typically employ prior knowledge to con-struct graphs. For example, ASTGNN and STGCN uti-lize road network structures for graph creation. However, thesepre-defined graphs encounter limitations due to their reliance onextensive domain knowledge and the inherent quality of the graphdata. Given the implicit and complex nature of spatial-temporal re-lationships, self-learned methods for graph generation have gainedprominence. These methods introduce innovative techniques tocapture complex spatial-temporal dependencies, thereby offeringsignificant advantages over traditional pre-defined models.Self-learned STGNNs can be further divided into two primarycategories: feature-based and randomly initialized methods. Feature-based approaches, such as PDFormer and DG , constructdynamic graphs from time-variant inputs, enhancing the accuracyof the model. On the other hand, randomly initialized STGNNs,also known as Adaptive Spatial-Temporal Graph Neural Networks(ASTGNNs), facilitate adaptive graph generation through randomlyinitialized, learnable node embeddings. Graph WaveNet intro-duced an Adaptive Graph Convolutional Network (AGCN) layer tolearn a normalized adaptive adjacency matrix. AGCRN furtherdeveloped this concept with a Node Adaptive Parameter Learningenhanced AGCN (NAPL-AGCN) to discern node-specific patterns.Owing to its remarkable performance, the NAPL-AGCN model hasbeen incorporated into various recent models .Despite the enhanced performance of ASTGNNs, they are bur-dened with considerable computational overhead. This is primarily",
  "Pre-Training Identification of Graph Winning Tickets in Adaptive Spatial-Temporal Graph Neural NetworksKDD 24, August 2529, 2024, Barcelona, Spain": "due to two factors: i) the process of learning an adaptive adjacencymatrix necessitates calculating the edge weight between each pair ofnodes, and ii) the aggregation phase of these networks is inherentlycomputationally intensive. Our research is centered on identify-ing the graph winning ticketa concept derived from the LotteryTicket Hypothesisin order to accelerate training and inferencein ASTGNNs. This approach is particularly relevant for handlinglarge-scale spatial-temporal data, where efficiency is crucial.",
  "Lottery Ticket Hypothesis": "The Lottery Ticket Hypothesis (LTH) suggests that within largeneural networks, there exist smaller sub-networks (termed \"win-ning tickets\") that, when trained in isolation from the start, canreach a similar performance level as the original network in a com-parable number of iterations. This finding has attracted lotsof research attention as it implies the potential of training a muchsmaller network to reach the accuracy of a dense, much larger net-work without going through the time and cost-consuming pipelineof fully training the dense network, pruning and then retrainingit to restore the accuracy. The \"Early Bird Lottery Ticket\" conceptbuilds on the original LTH. It suggests that winning tickets can beidentified very early in the training process, much earlier than whatwas originally proposed in LTH. This finding could further optimizethe training of neural networks by allowing significant pruningand resource reduction very early in the training phase.. Fur-ther, generalised LTH to GNNs by iteratively applying UGSto identify graph lottery tickets. GEBT discovers the existence ofgraph early-bird tickets . DGLT generalizes Dual Lottery TicketHypothesis (DLTH) to the graph to address information loss andaggregation failure issues caused by sampling-based GNN pruningalgorithms . However, the pruned GNNs are still hard to gener-alize to unseen graphs . RGLT is proposed to find more robustand generalisable GLT to tackle this issue .For extremely large models and graphs, identifying graph win-ning tickets typically necessitates a resource-intensive process in-volving training the network, followed by pruning and retraining.However, our methodology significantly streamlines the deploy-ment of ASTGNNs. It achieves this by obviating the requirementfor exhaustive cycles of training, pruning, and retraining.",
  "PRELIMINARIES3.1Notations and Problem Definition": "Frequently used notations are summarized in . Following theconventions in spatial-temporal graph neural network researches , we denote the spatial-temporal data as a sequence offrames: {X1, X2, . . . , X, . . .}, where a single frame X R isthe -dimensional data collated from different locations at time. For a chosen task time , we aim to learn a function mapping the historical observations into the future observations in the next timesteps:",
  "SymbolDescription": "GUndirected graphNumber of nodes in the graphXFeature matrix at time step Learnable node embedding matrixNode embedding dimensionVSet of nodes in a graphESet of edges in a graphKComplete graph with nodesTSpanning treeTStar Topology Spanning TreeNode embedding vector of the central nodeModel parametersAAdjacency matrixGATGraph Attention Network functionZOutput of the model at time step the corresponding set of edges and node features, respectively,where = |V| is the number of nodes, is the feature dimension.The adjacent matrix can be denoted as A = [A], where A = 1if there is an edge (, ) E and A = 0 otherwise. To accountfor the importance of neighbor nodes in learning graph structure,GAT integrates the attention mechanism into the node aggregationoperation as:",
  "where Z R , GAT() is the graph attention function": "Adaptive Graph Convolution Network. Adaptive Graph Con-volutional Network (AGCN) facilitates adaptive learning of graphstructures through randomly initialized learnable matrices. Thisapproach lays the groundwork for the evolution of Adaptive Spatial-Temporal Graph Neural Networks (ASTGNNs). Among the notableASTGNN models are Graph WaveNet and AGCRN. Within theGraph WaveNet framework, the AGCN is characterized as follows.",
  "A = GAT( K, ),(7)": "where K is the -order complete graph K with self-loops. Asthe diameter of K is 1, AGCN facilitates the aggregation of in-formation from all nodes to each individual node within K . Thischaracteristic significantly enhances the networks capability tomodel global spatial dependencies, culminating in its state-of-the-art performance in relevant tasks, as documented in .The model utilizing multi-layers of AGCN for modeling spatialdependencies is designated as ASTGNN (Adaptive Spatio-TemporalGraph Neural Network). The spatial-temporal forecasting problemwhen addressed using ASTGNN is mathematically expressed as:",
  "In this formulation, F represents the forecasting function of AST-GNN parameterised by, which predicts future values X(+1):(+ )": "based on the input sequence X(+1): and the structural infor-mation encoded in the graph K .However, a notable limitation arises during the training phase.The computational complexity associated with calculating adja-cency matrices and executing graph convolution operations oncomplete graphs is of O( 2). This significant computational de-mand imposes a constraint on the models scalability, particularly inscenarios involving large spatial-temporal datasets, where reducingcomputational complexity is crucial for practical applicability.",
  "Graph Tickets Hypothesis": "The Graph Tickets Hypothesis represents an extension of the orig-inal Lottery Tickets Hypothesis, initially introduced by UGS .UGS demonstrated that GWT (i.e., compact sub-graphs) are presentwithin randomly initialized GNNs, which can be retrained to achieveperformance comparable to, or even surpassing, that of GNNstrained on the original full graph. This finding underscores thepotential for efficiency improvements in GNN training method-ologies. However, designing a graph pruning method to identifyGWTs in ASTGNNs proves to be a nontrivial task. The state-of-theart, AGS demonstrates that spatial graphs in ASTGNNs can un-dergo sparsification up to 99.5% with no detrimental impact on testaccuracy . Nonetheless, this robustness to sparsification doesnot hold uniformly; when ASTGNNs, sparsified beyond 99%, arereinitialized and retrained on the same dataset, there is a notableand consistent decline in accuracy. This dichotomy underscores thenuanced complexity inherent in finding winning graph tickets inASTGNNs and calls for further investigation.",
  "A = GAT( G, ),(9)": "where G is a sparse subgraph of K . However, employing G alonedoes not ensure the capability to model global spatial dependencies.To maintain the global spatial modeling ability of AGCN and totrain ASTGNNs efficiently, we argue that it is essential to use aspanning tree T of K , instead of K , with a sufficient :",
  "(10)": "To mitigate the risk of excessive parameters and overfitting dueto a high number of network layers, it is crucial to minimize as much as possible. In light of this, we found that star topologyspanning trees (with diameter = 2) can function as GWTs. Wemake two notes on the star topology spanning tree: (1) Motivation: GAT is a message-passing network, and AGCNcan be viewed as modeling a fully connected GAT, allows anynode to communicate globally. A spanning tree T , as theminimum connected graph of complete graph, can achievemessage passing to all other nodes in the graph by stacking GAT layers, where k is the diameter of T . Our goal is tominimize the computational complexity of ASTGNNs, so itsnecessary to minimize . Clearly, T with a diameter of 1doesnt exist. So we start with = 2 to examine the existenceof spanning trees and we found that there exist T with adiameter of 2, uniquely forming a star topology. Well detailthis motivation in final version. (2) Theoretical Analysis: Based on spectral graph theory, ifone graph is a - of another, they have similareigensystems and properties. We can prove that T is an N-approximation of K . So K and T have similar properties,allowing T with fewer edges to effectively replace K forlearning good representations. The complete proof can befound in Appendix A.2. Hypothesis 1. Given an N-order complete spatial graph K of anASTGNN, we investigate an associated star spanning tree: T =V, E, where E = {(, ) | V \\ {}}, with desig-nated as the central node, designated as the leaf node. All suchT are Graph Winning Ticket (GWT) for the spatial graph of thecorresponding ASTGNN.To ensure the existence of the associated star spanning tree, wehave the following proposition:",
  "Efficiency: The method lacks optimal efficiency in training. Central Node Selection: The random selection of the centralnode could lead to sub-optimal performance": "Efficiency. The computational complexity of Graph Neural Net-work (GNN) training and inference encompasses two primary com-ponents: Deep Neural Network (DNN) computation and Graphconvolution operation. Considering the relaxation of the graphsdiameter from 1 to 2, an ASTGNN necessitates a minimum of twolayers of AGCN to maintain comprehensive spatial-temporal com-munication:Z = A(AXt1)2,",
  "(13)": "here,T = {V, E }, where E = < , >| V \\ {}.T ={V, E }, where E = < , >| V \\ {}. The computationalcomplexity of graph convolution operations experiences a notablereduction in Eq.(13). To elaborate, the complexity in Eq.(11) isO(2), whereas it is diminished to O() in Eq.(13).Despite this enhancement, Eq.(11) still faces limitations in termsof hardware compatibility. At the hardware level, graph convolu-tion operations are intrinsically linked to the sparse and irregularnature of graph structures. This characteristic might not be com-patible with certain hardware architectures, leading to an increasedfrequency of random memory accesses and limited opportunitiesfor data reuse. Consequently, this can result in significantly higherinference latency for graph convolutions when compared to otherneural network architectures. Then, we introduce a self-loop to the",
  "Central Node Selection. Owing to the non-uniqueness of T": "in the complete graph K , directly employing T for trainingASTGNNs presents the challenge of central node selection. Viewedthrough the lens of AGCN, the random selection of a node fromthe vertex set V is analogous to initializing the node embedding randomly. This approach, however, might introduce bias in theconstruction of the adaptive graph. To ensure that the selectedcentral node embedding vector is positioned at the physicalcenter of the node embedding space , we opt for a setting where = Mean(), a technique we refer to as averaged initialization.We empirically show that such operation provides better on theprediction accuracy (see Sec. 5.3).",
  "In this section, we conduct extensive experiments to validate ourHypothesis 1": "Neural Network Architecture. We evaluate the existence ofGWT on two quintessential ASTGNN architectures: AGCRN andGraph WaveNet (GWNET). AGCRN integrates an RNN frame-work, specifically combining AGCN layers with Gated RecurrentUnit (GRU) layers. The AGCN layers are adept at capturing spatialdependencies, whereas the GRU layers are employed to model thetemporal dependencies effectively. Conversely, GWNET representsa CNN-based ASTGNN architecture. It amalgamates AGCN, GCNlayers, and dilated 1D convolution networks. Here, both GCN andAGCN layers are instrumental in capturing spatial dependencies,whilst the dilated 1D convolution networks are utilized to modelthe temporal dependencies. AGCRN and GWENT respectivelyrepresent AGCRN and GWNET trained within T, while AGCRN",
  "and GWNET represent AGCRN and GWNET with in GWT-AGCNdescribed in Sec. 4.2, respectively": "Datasets. We conduct experiments on five of the largest knownspatial-temporal datasets. These include PEMS07, a dataset exten-sively studied , along with SD, GBA, GLA, and CA, which wererecently introduced in the LargeST dataset . summarizesthe specifications of the datasets used in our experiments. Thesedatasets were partitioned in a 6:2:2 ratio for training, validation,and testing, respectively. The traffic flow data in PEMS07 is aggre-gated into 5-minute intervals, whereas for SD, GBA, GLA, and CA,the aggregation occurs in 15-minute intervals. We implementeda 12-sequence-to-12-sequence forecast, adhering to the standardprotocol in this research domain. Implementation Details. For all evaluated models, we set thenumber of training iterations to 100. Other training-related config-urations adhere to the recommended settings provided in the re-spective code repositories. To ensure reproducibility and reliability,experiments were conducted ten times on all datasets, except for CAand GLA. Due to their substantially larger data scales, experimentson CA and GLA were limited to three repetitions. These experi-ments were performed on an NVIDIA RTX A6000 GPU, equippedwith 48 GB of memory. Metrics. Our comprehensive evaluation encompasses the followingdimensions: i) Performance: We assess the forecasting accuracyusing three established metrics: Mean Absolute Error (MAE), RootMean Square Error (RMSE), and Mean Absolute Percentage Error(MAPE), and ii) Efficiency: Model efficiency is evaluated in terms ofboth training and inference wall-clock time. Additionally, the batchsize during training is reported, reflecting the models capabilityto manage large-scale datasets. We set a maximum batch size limitof 64. If a model is unable to operate with this configuration, weprogressively reduce the batch size to the highest possible valuethat fully utilizes the memory capacity of the A6000 GPU.",
  "The experimental results are organised as follows: Test accuraciesand efficiency comparisons are reported in and ,respectively. We also compare theWe make following observations from and :": "Graph lottery tickets are existent in ASTGNNs. Specifically,AGCRN and GWENT demonstrate performance thatis comparable or even superior across all datasets. Thesefindings indicate that T is a stable graph winning lotteryticket within ASTGNNs when evaluated on datasets suchas PEMS07, SD, GBA, GLA, and CA. Our proposed approach is demonstrably scalable. The CAdataset presents substantial challenges to existing ASTGNNs,evidenced by AGCRNs inability to operate on it. However,the proposed approach facilitates the training of AGCRNon the CA dataset. This capability not only underscores thescalability of the proposed approach but also its superiority.Conventional pruning-based methods necessitate startingthe training process with a complete graph. This approachoften leads to their inadequacy in identifying graph lotterytickets in large-scale datasets like CA, a limitation that theproposed approach effectively overcomes. GWT-AGCN has the potential to be an ideal substitute forAGCN. In comparison, ASTGNN within GWT-AGCN demon-strates enhanced overall performance, particularly in termsof speed, surpassing its predecessor. GWT-AGCN significantly accelerates the training and infer-ence of the ASTGNNs. The acceleration is more prominenton AGCRNs against GWNETs because a larger portion ofthe total computation required by GWNETs is used on theirGCN layers.",
  "Convergence. illustrates the training loss and test MeanAbsolute Error (MAE) curves of the original AGCRN and AGCRN": "under identical hyper-parameter settings on the PEMS07 dataset.Similarly, presents these curves for the same models onthe SD dataset. Pre-determined GWT ensures convergence that isas consistent, rapid, and stable as a complete graph model. Thisfeature is particularly advantageous for training on large-scalespatial-temporal data, as it significantly reduces computationaloverhead without compromising the quality of convergence. Additionally, the convergence behavior of AGCRN demonstratesits robustness in capturing complex spatial-temporal dependencies.This attribute is crucial for reliable forecasting in dynamic systems,such as traffic networks, where understanding intricate patterns iskey to accuracy. AGCRN&GWNET vs. SOTAs. AGCRN and GWNET, as rep-resentative ASTGNNs introduced between 2019 and 2020, are ofsignificant interest in our study. Our objective is to evaluate the per-formance of AGCRN and GWNET, particularly when trained usingGWT, in comparison with the current state-of-the-art STGNNs. To",
  ": Training loss (a) and testing MAE (b) curve of origi-nal AGCRN and AGCRN trained on SD, respectively": "this end, we selected five advanced STGNNs as baselines: DGCRN, MegaCRN , STGODE , D2STGNN , and DSTAGNN. These models reflect the most recent trends in the field. DGCRNand MegaCRN, seen as variations of AGCRN, epitomize the latestdevelopments in ASTGNN. STGODE employs neural ordinary dif-ferential equations innovatively to effectively model the continuousdynamics of traffic signals. In contrast, DSTAGNN and D2STGNNfocus on capturing the dynamic correlations among sensors in traf-fic networks. From the results presented in , we make thefollowing observations: i) ASTGNNs such as DGCRN and MegaCRNconsistently exhibit strong performance across most benchmarks.However, their intricate model designs limit scalability, particularlyin larger datasets like GLA and CA. ii) Methods introduced fouryears ago, such as AGCRN when trained within GWT-AGCN (i.e.,AGCRN), continue to demonstrate robust performance across var-ious evaluated datasets. Remarkably, they achieve state-of-the-artperformance on specific datasets including GBA, GLA, and CA.These findings suggest that GWT-AGCN could play a crucial rolein the development of scalable ASTGNNs for future research. Impact of averaged initialization of node embedding . Inthis study, we employ AGCRN as a benchmark to evaluate theimpact of averaged initialization of node embedding . presents a comparative analysis between AGCRN and AGCRN ,i.e., random initialization of . The results indicate that AGCRN",
  "strategies for in enhancing the predictive performance of themodel": "Comparison with AGS. We compared our method with AGS, thestate-of-the-art approach, to validate its superiority. The perfor-mance of AGS with a sparsity of 99.7% is reported on PEMS07, SD,GBA and GLA, while the sparsity of our method is 99.8%/99.7%/99.99%/99.99% for PEMS07/SD/GBA/GLA. Since AGS does not pro-vide an implementation on GWNet, we only report the results forAGCRN. The lack of CA results is due to AGS encountering out-of-memory (OOM) issues. From , we can see that our methodsignificantly outperforms AGS. Perturbed T. We attribute the effectiveness of T to its robustconnectivity, which is crucial for ASTGNNs ability to model globalspatial dependencies. To further validate this perspective, we intro-duce a perturbation process illustrated in to T, resultingin T, according to the following steps:",
  "(2) Subsequently, we randomly add new edges, connectingpreviously isolated nodes": "These steps intentionally disrupt the original connectivity in T,while ensuring that the overall sparsity of the network remains con-stant. show the MAE curves of AGCRN trained via T ofa ratio from 0 to 50%. We can see that as increases, the accuracyof the model decreases. This indicates the importance of preservingthe graphs connectivity to model global spatial dependencies",
  "CONCLUSION": "This paper introduces a novel approach in the realm of ASTGNNsby leveraging the GWT concept, inspired by the Lottery TicketHypothesis. This method markedly reduces the computational com-plexity of ASTGNNs, transitioning from a quadratic to a linear scale,thereby streamlining their deployment. Our innovative strategy ofadopting a star topology for GWT, without necessitating exhaustive",
  ": Perturbation process with a perturbation ratio of ,with a pre-specified node number": "training cycles, maintains high model performance with signifi-cantly lower computational demands. Empirical validations acrossvarious datasets underscore our methods capability to achieveperformance on par with full models, but at a fraction of the compu-tational cost. This breakthrough not only underscores the existenceof efficient sub-networks of the spatial graphs within ASTGNNs,but also extends the applicability of the Lottery Ticket Hypothe-sis to scenarios where resources are limited. Consequently, thiswork represents a significant leap forward in the optimization andpractical application of graph neural networks, particularly in envi-ronments where computational resources are constrained. In thefuture, we will develop new STGNNs based on pre-determinedGWT, aimed at long-term spatial-temporal forecasting.",
  "Joshua Batson et al. 2013. Spectral sparsification of graphs: theory and algorithms.Commun. ACM 56, 8 (2013), 8794": "Chao Chen, Karl Petty, Alexander Skabardonis, Pravin Varaiya, and Zhanfeng Jia.2001. Freeway Performance Measurement System: Mining Loop Detector Data.Transportation Research Record 1748, 1 (2001), 96102. Tianlong Chen, Yongduo Sui, Xuxi Chen, Aston Zhang, and Zhangyang Wang.2021. A Unified Lottery Ticket Hypothesis for Graph Neural Networks. InProceedings of the 38th International Conference on Machine Learning, ICML 2021,18-24 July 2021, Virtual Event (Proceedings of Machine Learning Research, Vol. 139),Marina Meila and Tong Zhang (Eds.). PMLR, 16951706. Xiaohan Chen, Yu Cheng, Shuohang Wang, Zhe Gan, Zhangyang Wang, andJingjing Liu. 2021. EarlyBERT: Efficient BERT Training via Early-bird LotteryTickets. In Proceedings of the 59th Annual Meeting of the Association for Computa-tional Linguistics and the 11th International Joint Conference on Natural LanguageProcessing (Volume 1: Long Papers). 21952207. Yuzhou Chen, Ignacio Segovia-Dominguez, Baris Coskunuzer, and Yulia R. Gel.2022. TAMP-S2GCNets: Coupling Time-Aware Multipersistence KnowledgeRepresentation with Spatio-Supra Graph Convolutional Networks for Time-SeriesForecasting. In The Tenth International Conference on Learning Representations,ICLR 2022, Virtual Event, April 25-29, 2022. Jeongwhan Choi, Hwangyong Choi, Jeehyun Hwang, and Noseong Park. 2022.Graph Neural Controlled Differential Equations for Traffic Forecasting. In Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Confer-ence on Innovative Applications of Artificial Intelligence, IAAI 2022, The TwelvethSymposium on Educational Advances in Artificial Intelligence, EAAI 2022 VirtualEvent, February 22 - March 1, 2022. AAAI Press, 63676374. Wenying Duan, Xiaoxi He, Zimu Zhou, Lothar Thiele, and Hong Rao. 2023.Localised Adaptive Spatial-Temporal Graph Neural Network. In Proceedings ofthe 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,KDD 2023, Long Beach, CA, USA, August 6-10, 2023. ACM, 448458. Zheng Fang, Qingqing Long, Guojie Song, and Kunqing Xie. 2021. Spatial-temporal graph ode networks for traffic flow forecasting. In Proceedings of the27th ACM SIGKDD conference on knowledge discovery & data mining. 364373.",
  "Jonathan Frankle and Michael Carbin. 2019. The Lottery Ticket Hypothesis: Find-ing Sparse, Trainable Neural Networks. In International Conference on LearningRepresentations": "Shengnan Guo, Youfang Lin, Huaiyu Wan, Xiucheng Li, and Gao Cong. 2021.Learning dynamics and heterogeneity of spatial-temporal graph data for trafficforecasting. IEEE Transactions on Knowledge and Data Engineering 34, 11 (2021),54155428. Rongzhou Huang, Chuyin Huang, Yubao Liu, Genan Dai, and Weiyang Kong.2020. LSGCN: Long Short-Term Traffic Prediction with Graph ConvolutionalNetworks. In Proceedings of the Twenty-Ninth International Joint Conference onArtificial Intelligence, IJCAI 2020, Christian Bessiere (Ed.). ijcai.org, 23552361. Jiawei Jiang, Chengkai Han, Wayne Xin Zhao, and Jingyuan Wang. 2023.PDFormer: Propagation Delay-Aware Dynamic Long-Range Transformer forTraffic Flow Prediction. In Thirty-Seventh AAAI Conference on Artificial Intel-ligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of Arti-ficial Intelligence, IAAI 2023, Thirteenth Symposium on Educational Advancesin Artificial Intelligence, EAAI 2023, Washington, DC, USA, February 7-14, 2023,Brian Williams, Yiling Chen, and Jennifer Neville (Eds.). AAAI Press, 43654373. Renhe Jiang, Zhaonan Wang, Jiawei Yong, Puneet Jeph, Quanjun Chen, YasumasaKobayashi, Xuan Song, Shintaro Fukushima, and Toyotaro Suzumura. 2023.Spatio-Temporal Meta-Graph Learning for Traffic Forecasting. In Thirty-SeventhAAAI Conference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference onInnovative Applications of Artificial Intelligence, IAAI 2023, Thirteenth Symposiumon Educational Advances in Artificial Intelligence, EAAI 2023, Washington, DC,USA, February 7-14, 2023, Brian Williams, Yiling Chen, and Jennifer Neville (Eds.).AAAI Press, 80788086. Renhe Jiang, Zhaonan Wang, Jiawei Yong, Puneet Jeph, Quanjun Chen, Ya-sumasa Kobayashi, Xuan Song, Shintaro Fukushima, and Toyotaro Suzumura.2023. Spatio-temporal meta-graph learning for traffic forecasting. In Proceedingsof the AAAI conference on artificial intelligence, Vol. 37. 80788086. Shiyong Lan, Yitong Ma, Weikang Huang, Wenwu Wang, Hongyu Yang, andPyang Li. 2022. Dstagnn: Dynamic spatial-temporal aware graph neural networkfor traffic flow forecasting. In International conference on machine learning. PMLR,1190611917. Fuxian Li, Jie Feng, Huan Yan, Guangyin Jin, Fan Yang, Funing Sun, Depeng Jin,and Yong Li. 2023. Dynamic graph convolutional recurrent network for trafficprediction: Benchmark and solution. ACM Transactions on Knowledge Discoveryfrom Data 17, 1 (2023), 121. Xu Liu, Yutong Xia, Yuxuan Liang, Junfeng Hu, Yiwei Wang, Lei Bai, ChaoHuang, Zhenguang Liu, Bryan Hooi, and Roger Zimmermann. 2023. LargeST: ABenchmark Dataset for Large-Scale Traffic Forecasting. In Advances in NeuralInformation Processing Systems. Hao Peng, Bowen Du, Mingsheng Liu, Mingzhe Liu, Shumei Ji, Senzhang Wang,Xu Zhang, and Lifang He. 2021. Dynamic graph convolutional network forlong-term traffic flow prediction with reinforcement learning. Inf. Sci. 578 (2021),401416. Zezhi Shao, Zhao Zhang, Wei Wei, Fei Wang, Yongjun Xu, Xin Cao, and Chris-tian S Jensen. 2022. Decoupled dynamic spatial-temporal graph neural networkfor traffic forecasting. Proceedings of the VLDB Endowment 15, 11 (2022), 27332746. Kun Wang, Guohao Li, Shilong Wang, Guibin Zhang, Kai Wang, Yang You, Xiao-jiang Peng, Yuxuan Liang, and Yang Wang. 2023. The snowflake hypothesis: Train-ing deep GNN with one node one receptive field. arXiv preprint arXiv:2308.10051(2023). Kun Wang, Yuxuan Liang, Xinglin Li, Guohao Li, Bernard Ghanem, Roger Zim-mermann, Huahui Yi, Yudong Zhang, Yang Wang, et al. 2023. Brave the windand the waves: Discovering robust and generalizable graph lottery tickets. IEEETransactions on Pattern Analysis and Machine Intelligence (2023). Kun Wang, Yuxuan Liang, Pengkun Wang, Xu Wang, Pengfei Gu, Junfeng Fang,and Yang Wang. 2023. Searching Lottery Tickets in Graph Neural Networks: ADual Perspective. In The Eleventh International Conference on Learning Represen-tations. Kun Wang, Hao Wu, Guibin Zhang, Junfeng Fang, Yuxuan Liang, Yuankai Wu,Roger Zimmermann, and Yang Wang. 2024. Modeling spatio-temporal dynamicalsystems with neural discrete learning and levels-of-experts. IEEE Transactions onKnowledge and Data Engineering (2024). Hao Wu, Yuxuan Liang, Wei Xiong, Zhengyang Zhou, Wei Huang, Shilong Wang,and Kun Wang. 2024. Earthfarsser: Versatile Spatio-Temporal Dynamical SystemsModeling in One Model. In Proceedings of the AAAI Conference on ArtificialIntelligence, Vol. 38. 1590615914. Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, andS Yu Philip. 2020. A comprehensive survey on graph neural networks. IEEETransactions on Neural Networks and Learning Systems 32, 1 (2020), 424. Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, and Chengqi Zhang. 2019.Graph Wavenet for Deep Spatial-Temporal Graph Modeling. In Proceedings ofthe 28th International Joint Conference on Artificial Intelligence (Macao, China)(IJCAI19). AAAI Press, 19071913. Sijie Yan, Yuanjun Xiong, and Dahua Lin. 2018. Spatial Temporal Graph Con-volutional Networks for Skeleton-Based Action Recognition. In Proceedings ofthe Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30thinnovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Sym-posium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans,Louisiana, USA, February 2-7, 2018, Sheila A. McIlraith and Kilian Q. Weinberger(Eds.). AAAI Press, 74447452. Haoran You, Chaojian Li, Pengfei Xu, Yonggan Fu, Yue Wang, Xiaohan Chen,Yingyan Lin, Zhangyang Wang, and Richard G. Baraniuk. 2020. Drawing Early-Bird Tickets: Toward More Efficient Training of Deep Networks. In InternationalConference on Learning Representations. Haoran You, Zhihan Lu, Zijian Zhou, Yonggan Fu, and Yingyan Lin. 2022. Early-bird gcns: Graph-network co-optimization towards more efficient gcn trainingand inference via drawing early-bird lottery tickets. In Proceedings of the AAAIConference on Artificial Intelligence, Vol. 36. 89108918. Bing Yu, Haoteng Yin, and Zhanxing Zhu. 2018. Spatio-Temporal Graph Con-volutional Networks: A Deep Learning Framework for Traffic Forecasting. InProceedings of the Twenty-Seventh International Joint Conference on Artificial In-telligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden, Jrme Lang (Ed.).ijcai.org, 36343640.",
  ": Adds the new node to the native tree": "Subsequently, assuming = 1, the complete graph K1aligns with this conclusion, and the star spanning tree is T1.In the scenario where = , the original graph is equivalentto inserting a new node into T1. shows two possiblescenarios.Only in the first scenario, does the spanning tree T meet thediameter = 2. The second scenario will increase some paths thatare longer than 2. For the spanning tree Tformed in the firstscenario, it still conforms to the definition of star spanning tree inHypothesis 1.",
  "Lemma 3. The laplacian of T has eigenvalue 0 with multiplic-ity 1, eigenvalue 1 with multiplicity 2, and eigenvalue withmultiplicity 1": "Proof of Lemma 3. Applying Lemma 2.1 to vertices and + 1for 2 < , we find 2 linearly independent eigenvectors ofthe form +1, all with eigenvalue 1. As 0 is also an eigenvalue,only one eigenvalue remains to be determined. Recall that the traceof a matrix equals both the sum of its diagonal entries and the sumof its eigenvalues. We know that the trace of T is 2 2, andwe have identified 1 eigenvalues that sum to 2. So, theremaining eigenvalue must be ."
}