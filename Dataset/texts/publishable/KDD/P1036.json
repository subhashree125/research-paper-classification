{
  "ABSTRACT": "Human action recognition in dark videos is a challenging task for computer vision. Recent researchfocuses on applying dark enhancement methods to improve the visibility of the video. However,such video processing results in the loss of critical information in the original (un-enhanced) video.Conversely, traditional two-stream methods are capable of learning information from both originaland processed videos, but it can lead to a significant increase in the computational cost duringthe inference phase in the task of video classification. To address these challenges, we propose anovel teacher-student video classification framework, named Dual-Light KnowleDge Distillationfor Action Recognition in the Dark (DL-KDD). This framework enables the model to learn fromboth original and enhanced video without introducing additional computational cost during inference.Specifically, DL-KDD utilizes the strategy of knowledge distillation during training. The teachermodel is trained with enhanced video, and the student model is trained with both the original videoand the soft target generated by the teacher model. This teacher-student framework allows the studentmodel to predict action using only the original input video during inference. In our experiments, theproposed DL-KDD framework outperforms state-of-the-art methods on the ARID, ARID V1.5, andDark-48 datasets. We achieve the best performance on each dataset and up to a 4.18% improvementon Dark-48, using only original video inputs, thus avoiding the use of two-stream framework orenhancement modules for inference. We further validate the effectiveness of the distillation strategyin ablative experiments. The results highlight the advantages of our knowledge distillation frameworkin dark human action recognition.",
  "Introduction": "Action Recognition is a popular task in computer vision that can be applied in various real-world applications. Forexample, surveillance systems and autonomous vehicles . In recent years, there has been increasing researchfocusing on this task . Compared to action recognition under well-lighted conditions, recognizing actionin dark environments is more challenging due to the degradation of the information in videos. In response to thischallenge, recent studies have proposed various frameworks to achieve better performance with dark videoinputs. Common approaches include utilizing light enhancement methods such as ZeroDCE and Gamma IntensityCorrection(GIC) to improve the video feature and visibility, followed by 3D convolutional networks like R(2+1)D or 3D-ResNext as the backbone classifiers. Two main architectures to incorporate these components are: 1) directly",
  "integrating two models , 2) using a two-stream method to improve the accuracy of action prediction fromdark videos": "Recent research focuses on applying enhancements and taking enhanced video as model inputs. While suchapproaches improve the features contained in videos, the enhancement process often leads to losing original content,which can contain critical information for action recognition. On the other hand, existing methods in dark humanaction recognition that considers the importance of the original input applied traditional two-stream method,which takes both the original and enhanced video as the inputs to the model. This approach significantly increasesthe computational load, making it slower to perform predictions during inference. In contrast, using only the originalvideo as input results in a performance gap compared to previously mentioned techniques since the model can get lessinformation from the raw video without enhancement. In summary, the three main challenges for current research ondark human action recognition are",
  "Consistent Performance: Improving the performance while using only original video without enhancement as inputduring the inference phase": "According to our literature review, no existing studies fully addressed these three challenges concurrently. To addressthese challenges, we proposed a knowledge-distillation-based framework, Dual-Light KnowleDge Distillation forAction Recognition in the Dark (DL-KDD). The DL-KDD framework overcome the challenges of feature learningfrom both original and enhanced video while avoiding the additional computational cost like two-stream methods do.Knowledge distillation , in this context, serves as an effective method, helping the model to learn from the teachermodels information without increasing the input feature set. Our architecture includes a teacher model consisting of anenhancement module and an action classifier to learn from enhanced features, and a student model which contains anaction classifier to learn original features and teacher logit. Thus, during the inference phase, we only need to use theoriginal video as input without any additional enhancement or dual input. To overcome the challenge of informationcompleteness, we allow the student models to learn information from both light feature and dark videos, ensuring thatall critical information is captured by the model. For the complexity tradeoff, we use knowledge distillation instead of atwo-stream approach to allow the model to learn two types of features without including additional costs. For consistentperformance, only original videos are required for the model during inference to achieve effective results. As this is the first work that solves these three problems simultaneously, the main contributions are three-fold: 1) Ourmethod utilizes both the original video and enhanced feature for action recognition in the dark. 2) Our model can useonly original video input without additional features or enhancement during inference, thereby maintaining the modelsize while improving performance, and 3) We achieve state-of-the-art performance in dark human action recognition.",
  "Action Recognition in the Dark": "In recent developments, various model architectures have been proposed for human action recognition, includingthose based on 3D-CNN and Transformers . These technologies perform well under well-lit conditions.However, their performance degrades while facing low-light videos. As a result, innovative approaches have been introduced to address the problem, where most of them selected 3D-CNN as the backbone due to theireffectiveness. Chen et al. proposed DarkLight, which utilizes both original and frames enhanced by Gamma IntensityCorrection (GIC) for action prediction. The method represents a significant advancement in this field, demonstrating theeffectiveness of light enhancement for action recognition in the dark. The experiments also indicate that the dual-pathapproach, which utilizes both original and enhanced frames, captures more features than methods that estimate opticalflow , thereby achieving better performance. Building on these advancements, recent studies have furtherimproved the accuracy of action prediction by incorporating ZeroDCE , a light enhancement module. These studieshave integrated ZeroDCE with backbone classifiers and showed remarkable performance gains. Consequently, thearchitecture that directly utilizes enhanced video as input has emerged as the most prevalent method in the field. OurDL-KDD addresses the issue of overlooking the importance of original video in recent studies, and the increasedcomputational cost brought by two-stream methods by applying knowledge distillation with an enhancement module.",
  "Knowledge Distillation": "Enhanced Feature Extraction The teacher models training begins with enhancing the original video frames. Theenhancement module transforms the input I into I. This enhancement improve the feature visibility and informationfor action recognition in dark videos. After the enhancement, the enhanced frames I are fed into the action classifierfor prediction. The action classifier processed the input frames into logits that capture the crucial information of theaction label. A standard Cross-Entropy Loss is applied here for training the teacher model:",
  "Problem Definition": "Action recognition aims to predict action labels from given input videos. Using the method of knowledge distillation,we train a teacher model to transfer its knowledge to a separate student model. Formally, let D = {(xi, yi)}ni=0 be avideo-based dataset, where xi is a sample input and yi is the action class. During training phase, we train a teachermodel T, which consists of an enhancement module Te and a backbone classifier Tc , and a student model S separately.For teacher model T, given training samples X{x1, x2, ..., xj} from Dtrain as the input of Te, the module wouldgenerate enhanced samples X. After that, X will be served as the input of Tc, which predict on X to the probabilityof each class, denote as yt. The loss function is then applied on yt and the ground truth y.yt = T(x) = Tc(Te(x))(1) For student model S, given training samples X{x1, x2, ..., xj} from Dtrain as the input of S, the model generates theprobability of each class, denoted as ys. The loss function is then applied on ys, yt and the ground truth y.",
  "Overall Architecture": "The overall architecture is shown in . It consists of two main components: a teacher model and a student model.The teacher model includes a light enhancement module followed by an action classifier. The module first enhances theoriginal video frames to improve visibility in order to extract features that may be loss in the dark. These enhancedfeatures are then fed into the action classifier to generate feature representations. After training the teacher model, we train the student model by taking original video frames without enhancement asinput. This approach allows the student model to learn directly from the original data, which ensures that the model isnot dependent on enhanced inputs during inference. The student model is an action classifier, and the training of thestudent model involves a dual learning process:",
  ". Direct Learning: The student model learns directly from the ground truth labels": "2. Distillation Learning: The student model is trained using the logits from the teacher model as soft targets, whichallows the student model to learn from the feature representation that has been extracted from enhanced frames byteacher model. This architecture make use of both enhanced and original data, which optimizes the models performance withoutadditional computational cost of using both data or processing enhancement during inference.",
  "Where y is the class label of the input video and yt is the logit extracted by the teacher model": "Original Representation Learning Unlike the teacher model, the student model directly takes the original videoframes I as input. The action classifier is the same as that of the teacher models, but there is no enhancement modulefor the student model. The inputs are processed by the student model to generate the results ys, and a cross-entropy lossis applied here to align the student models prediction with the ground truth.",
  "LAR = CrossEntropy(y, ys)(4)": "Dual Knowledge Learning In addition to learning from the ground truth, a knowledge distillation process is applied tothe student model, where it learns from the logit yt generated by the teacher model. This is achieved by minimizing theKullback-Leibler divergence between the student models output ys and the soft targets provided by the teacher model,which indirectly transfers the enhanced feature knowledge from the teacher to the student model:",
  "Experiment Settings": "Dataset We evaluated our method on three datasets: ARID, ARID V1.5 , and Dark-48 . The ARID datasethas been a primary benchmark of dark human action recognition. It contains over 3780 video clips collected with11 action classes. To further enhance the complexity of the dataset, ARID V1.5 was introduced. The video count isexpanded to 5572, and the videos are collected from 24 scenes. The Dark-48 dataset comprises 8815 dark videosfrom more than 40 scenes, featuring 48 classes with over 100 videos each. The dataset is split into training and testingsets in a ratio of 8:2. The training and testing settings in our experiment are the same as the original work. Implementation Details Inspired by , we selected R(2+1)D followed by BERT in replace of the conventionaltemporal global average pooling layer as our backbone classifier. The backbone model was pre-trained on IG65M .For the enhancement module of the teacher model, we selected ZeroDCE to generate enhanced frames. The inputsequences were resized to 112 x 112 pixels, and the final input shape was 3 x 64 x 112 x 112 with batch size 2. Wetrained the teacher and the student model with AdamW optimizer with a learning rate 0.0001. The parameters forthe loss function of the student model were set to 1 for both and . Metrics In this task, we recorded top-1 and top-5 accuracy to evaluate the performance of the model. Since the ARID dataset contains only 11 classes and most previous works have nearly reached 100% top-5 accuracy, we primarilypresent the top-1 accuracy for ARID and ARID V1.5.",
  "Ablation Study": "In this section, we focus on an ablative comparison of the ARID dataset to demonstrate the effectiveness of our proposedframework. To illustrate the efficacy of our training method, we present results comparing the backbone model trainedwith and without our method. Additionally, comparisons between the teacher and student models are displayed toshow that the student network can achieve better results even without enhancement after the knowledge distillationtraining. provides a detailed display of the final performance of the teacher, student model of DL-KDD, and theperformance of similar architecture without knowledge distillation training method.",
  "DL-KDD-Teacher: ZeroDCE + R(2+1)D + BERT95.73DL-KDD-Student: R(2+1)D + BERT (ours)97.27": "With and without Knowledge Distillation As shown in , our knowledge distillation training method improvedthe performance of the same architecture by 4.83%, which shows the effectiveness of learning from the knowledgedistilled from the enhanced feature by the teacher model. With the additional knowledge provided by the teacher model,the student model can take advantage of enhanced representation even without enhanced feature inputs during testing. Comparison with Teacher Model The comparison between the student and teacher model shows that even the studentmodel uses a simpler architecture without enhancement, it achieves an improvement of 1.54% over the teacher model,which indicates that in addition to the distilled knowledge of enhanced features, the original video also contains criticalinformation that improves model performance. By learning from original inputs, the student model accesses additionalinformation from enhanced features, resulting in better performance than the teacher model.",
  "Comparison with State-of-the-Art": "We conduct extensive experiments to compare our work with the recent state-of-the-art methods in dark human actionrecognition, including DarkLight , DTCM , and R(2+1)D-GCN+BERT across the ARID, ARID V1.5 ,and Dark48 dataset. Partial results from previous works are collected from . and presentdetailed results for the two versions of the ARID dataset. Despite high baseline performances on these datasets, ourproposed method outperforms existing models and achieves the best results. indicates that our model reached aTop-1 accuracy of 50.86% on the Dark-48 dataset. This demonstrates a significant improvement over the best previouslyreported result on Dark-48 by 4.18%. These results illustrate that our proposed knowledge distillation frameworksuccessfully enhances the information learned by the model, which enables our model to achieve the best performancewhile using only the original video input during testing.",
  "Conclusion": "In this work, we have proposed a novel knowledge-distillation-based framework named DL-KDD for dark humanaction recognition, emphasizing the importance of utilizing both the original video and the enhanced feature to preventthe loss of original information. Moreover, the proposed framework avoids the additional cost brought by two-streammethods. We effectively distill the knowledge of light enhancement to the student model, enabling the student model touse only original videos as input during inference and achieve better results. The state-of-the-art performance on theARID and Dark-48 datasets proves the effectiveness of our method. For future work, we will continue to refine ourarchitecture for further advancement on dark human action recognition.",
  "Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition.In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019": "D. Tran, H. Wang, L. Torresani, J. Ray, Y. LeCun, and M. Paluri. A closer look at spatiotemporal convolutions foraction recognition. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages64506459, 2018. K. Hara, H. Kataoka, and Y. Satoh. Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet? In2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 65466555, 2018. Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding?In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning,volume 139 of Proceedings of Machine Learning Research, pages 813824. PMLR, 2021. Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin transformer. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 32023211,2022. Rui Chen, Jiajun Chen, Zixi Liang, Huaien Gao, and Shan Lin. Darklight networks for action recognition in thedark. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages846852, 2021. Himanshu Singh, Saurabh Suman, Badri Narayan Subudhi, Vinit Jakhetiya, and Ashish Ghosh. Action recognitionin dark videos using spatio-temporal features and bidirectional encoder representations from transformers. IEEETransactions on Artificial Intelligence, 4(6):14611471, 2023. Zhigang Tu, Yuanzhong Liu, Yan Zhang, Qizi Mu, and Junsong Yuan. Dtcm: Joint optimization of darkenhancement and action recognition in videos. IEEE transactions on image processing : a publication of the IEEESignal Processing Society, PP, 2023. Chunle Guo, Chongyi Li, Jichang Guo, Chen Change Loy, Junhui Hou, Sam Kwong, and Runmin Cong. Zero-reference deep curve estimation for low-light image enhancement. In 2020 IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), pages 17771786, 2020. Karen Simonyan and Andrew Zisserman. Two-stream convolutional networks for action recognition in videos.Advances in neural information processing systems, 27, 2014. J. Carreira and A. Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In 2017 IEEEConference on Computer Vision and Pattern Recognition (CVPR), pages 47244733, 2017. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprintarXiv:1503.02531, 2015. Fida Mohammad Thoker and Juergen Gall. Cross-modal knowledge distillation for action recognition. In 2019IEEE International Conference on Image Processing (ICIP), pages 610, 2019. Yang Liu, Keze Wang, Guanbin Li, and Liang Lin. Semantics-aware adaptive knowledge distillation for sensor-to-vision action recognition. IEEE Transactions on Image Processing, 30:55735588, 2021. Ying-Chen Lin and Vincent S. Tseng. Multi-view knowledge distillation transformer for human action recognition.arXiv preprint arXiv:2303.14358, 2023. Didik Purwanto, Rizard Renanda Adhi Pramono, Yie-Tarng Chen, and Wen-Hsien Fang. Extreme low resolutionaction recognition with spatial-temporal multi-head self-attention and knowledge distillation. In Proceedings ofthe IEEE/CVF International Conference on Computer Vision (ICCV) Workshops, 2019. Jeong-Hyeok Park, Tae-Hyeon Kim, and Jong-Ok Kim. Dual-teacher distillation for low-light image enhancement.In 2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPAASC), pages 13511355, 2022. Ziwen Li, Yuehuan Wang, and Jinpu Zhang. Low-light image enhancement with knowledge distillation. Neuro-computing, 518:332343, 2023. Ruibing Jin, Guosheng Lin, Min Wu, Jie Lin, Zhengguo Li, Xiaoli Li, and Zhenghua Chen. Unlimited knowledgedistillation for action recognition in the dark. arXiv preprint arXiv:2308.09327, 2023. Yuecong Xu, Jianfei Yang, Haozhi Cao, Kezhi Mao, Jianxiong Yin, and Simon See. Arid: A new dataset forrecognizing action in the dark. In Deep Learning for Human Activity Recognition: Second International Workshop,DL-HAR 2020, Held in Conjunction with IJCAI-PRICAI 2020, Kyoto, Japan, January 8, 2021, Proceedings 2,pages 7084. Springer, 2021. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectionaltransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Deepti Ghadiyaram, Du Tran, and Dhruv Mahajan. Large-scale weakly-supervised pre-training for video actionrecognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages1204612055, 2019. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101,2017."
}