{
  "ABSTRACT": "High-frequency trading (HFT) that executes algorithmic trading inshort time scales, has recently occupied the majority of cryptocur-rency market. Besides traditional quantitative trading methods, re-inforcement learning (RL) has become another appealing approachfor HFT due to its terrific ability of handling high-dimensional fi-nancial data and solving sophisticated sequential decision-makingproblems, e.g., hierarchical reinforcement learning (HRL) has shownits promising performance on second-level HFT by training a routerto select only one sub-agent from the agent pool to execute thecurrent transaction. However, existing RL methods for HFT stillhave some defects: 1) standard RL-based trading agents suffer fromthe overfitting issue, preventing them from making effective pol-icy adjustments based on financial context; 2) due to the rapidchanges in market conditions, investment decisions made by anindividual agent are usually one-sided and highly biased, whichmight lead to significant loss in extreme markets. To tackle theseproblems, we propose a novel Memory Augmented Context-awareReinforcement learning method On HFT, a.k.a. MacroHFT, whichconsists of two training phases: 1) we first train multiple types ofsub-agents with the market data decomposed according to variousfinancial indicators, specifically market trend and volatility, whereeach agent owns a conditional adapter to adjust its trading policyaccording to market conditions; 2) then we train a hyper-agentto mix the decisions from these sub-agents and output a consis-tently profitable meta-policy to handle rapid market fluctuations,equipped with a memory mechanism to enhance the capability ofdecision-making. Extensive experiments on various cryptocurrencymarkets demonstrate that MacroHFT can achieve state-of-the-artperformance on minute-level trading tasks. Code has been releasedin",
  "Corresponding Author": "Permission to make digital or hard copies of part or all of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for third-party components of this work must be honored.For all other uses, contact the owner/author(s).KDD 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s).ACM ISBN 979-8-4007-0490-1/24/08",
  "Reinforcement Learning, High-frequency Trading": "ACM Reference Format:Chuqiao Zong, Chaojie Wang, Molei Qin, Lei Feng, Xinrun Wang, and BoAn. 2024. MacroHFT: Memory Augmented Context-aware ReinforcementLearning On High Frequency Trading. In Proceedings of the 30th ACMSIGKDD Conference on Knowledge Discovery and Data Mining (KDD 24),August 2529, 2024, Barcelona, Spain. ACM, New York, NY, USA, 10 pages.",
  "INTRODUCTION": "The financial market, which involves over 90 trillion dollars of mar-ket capacity, has attracted a massive number of investors. Amongall possible assets, the cryptocurrency market has gained particularfavor in recent years due to its high volatility, offering opportunitiesfor rapid and substantial profit, and its around-the-clock tradingcapacity, which allows for greater flexibility and the opportunityfor traders to react immediately . To fully exploit the profit po-tential, high-frequency trading (HFT), a form of algorithmic tradingexecuted at high speeds, has occupied the majority of cryptocur-rency markets . Besides rule-based trading strategies designedby experienced human traders, reinforcement learning (RL) hasemerged as another promising approach recently due to its ter-rific ability to handle high-dimensional financial data and solvecomplex sequential decision-making problems . However,although RL has achieved great performance in low-frequency trad-ing , there remains a technical gap in developing effectivehigh-frequency trading algorithms for cryptocurrency markets be-cause of long trading horizons and volatile market fluctuations.Specifically, existing RL-based HFT algorithms for cryptocur-rency trading still suffer from some drawbacks, mainly including:1) most of the current methods tend to treat the cryptocurrencymarket as a uniform and stationary entity or distinguish mar-ket conditions only based on market trends , neglecting themarket volatility. This oversight is significant in highly dynamiccryptocurrency markets. Ignoring the differences between markets",
  "KDD 24, August 2529, 2024, Barcelona, SpainZong, et al": "the other three datasets, is set as 4320. All the sub-agentsare trained for 15 epochs and selected based on the average returnrate on the corresponding validation subsets with the same marketlabel. The coefficient of each sub-agent is tuned separately over{0, 1, 4} and selected based on the mean return rate of the validationsubset with the same label of the agent. For hyper-agent training,the embedding dimension is 32 and the policy networks dimensionis 128. The hyper-agent is trained for 15 epochs and selected basedon the return rate over the whole validation set. All the networkparameters are optimized by Adam optimizers with a learning rateof 1e-4. The coefficient is set to be 0.5, and is tuned over {1, 5}and selected based on the overall return rate of meta-policy overthe validation set. For DOTUSDT, is set as 1. For the other threedatasets, is set as 5.",
  "(2) We propose low-level policy optimization with conditionaladaptation for sub-agents, enabling efficient adjustmentsof trading policies according to market conditions": "(3) We develop a hyper-agent that provides a meta-policy toeffectively integrate diverse low-level policies from sub-agents. Utilizing a memory module, the hyper-agent canformulate a robust trading strategy by learning from highlyrelevant experiences. (4) Comprehensive experiments on 4 popular cryptocurrencymarkets demonstrate that MacroHFT can significantly out-perform many existing state-of-the-art baseline methods inminute-level HFT of cryptocurrencies.",
  "Traditional Financial Methods": "Based on the assumption that past price and volume information canreflect future market conditions, technical analysis has been widelyapplied in traditional finance trading , and quantitative tradershave designed millions of technical indicators as signals to guidethe trading execution . For instance, Imbalance Volume (IV) is developed to measure the difference between the number of buyorders and sell orders, which provides a clue of short-term market",
  ": A Snapshot of Limit Order Book (LOB)": "direction. Moving Average Convergence Divergence (MACD) is another widely used trend-following momentum indicatorshowing the relationship between two moving averages of an assetsprice, which reflects the future market trend.However, these traditional finance methods solely based on tech-nical indicators often produce false trading signals in non-stationarymarkets like cryptocurrency, which may lead to poor performance,which has been criticized in recent studies .",
  "RL-based Methods": "Other than traditional finance methods, reinforcement learningbased trading approaches have recently been another appealing ap-proach in the field of quantitative trading. Besides directly applyingstandard deep RL algorithms like Deep-Q Network (DQN) andProximal Policy Optimization (PPO) , various techniques wereused as enhancements. CDQNRP generates trading strategiesby applying a random perturbation to increase the stability of DQNtraining. CLSTM-PPO applies LSTM to enhance the state rep-resentation of PPO for high-frequency stock trading. DeepScalper uses a hindsight bonus reward and auxiliary task to improvethe agents foresight and risk management ability.Furthermore, to improve the adaptation capacity over long trad-ing horizons containing different market dynamics, HierarchicalReinforcement Learning (HRL) structures have also been applied toquantitative trading. HRPM formulates a hierarchical frame-work to handle portfolio management and order execution simul-taneously. MetaTrader trains multiple policies using differentexpert strategies and selects the most suitable one based on thecurrent market situation for portfolio management. EarnHFT trains low-level agents under different market trends with optimalaction supervisors and a router for agent selection to achieve stableperformance in high-frequency cryptocurrency trading.However, the performance of existing HRL methods suffers fromvarying degrees of overfitting problems and has difficulty in mak-ing effective policy adjustments based on financial context, whereMetaTrader and EarnHFT only choose an individual agentto perform trading at each timestamp, usually leading to one-sidedand highly biased decision execution. To solve these challenges, wedevelop MacroHFT, which is the first HRL framework that not onlyincorporates macro market information as context to assist trad-ing decision-making, but also provides a mixed policy to leveragesub-agents specialization capacity by decomposing markets usingmultiple criteria, rather than selecting an individual one.",
  "Financial Definitions": "The common financial definitions of terms in HFT have been elab-orated as follows:Limit Order is an order placed by a market participant who wantsto buy (bid) or sell (ask) a specific quantity of cryptocurrency at aspecified price, where (,) denotes a limit order to buy a totalamount of cryptocurrency at the price , and (,) denotesa limit order of selling.Limit Order Book (LOB), as shown as Fig 1, serves as an importantsnapshot to describe the micro-structure of current market ,which is the record that aggregates buy and sell limit orders of allmarket participants for a cryptocurrency at the same timestamp. Formally, we denote an -level LOB ( = 5 in our dataset)at time as = {( , ), ( , )}=1, where , denote the -th level of bid and ask prices respectively, and , are thecorresponding quantity for trading.Open-High-Low-Close-Volume (OHLCV) is the aggregated in-formation of executed market orders. At the timestamp , theOHLCV information can be denoted as = ( , , , , ),where , , , denote the open, high, low, close prices and is the corresponding total volume of these market orders.Technical Indicators are a group of features calculated from origi-nal LOB and OHLCV information by formulaic combinations, whichcan uncover the underlying patterns of the financial market. Wedenote the set of technical indicators at time as = (,, ...,+1,+1), where is the backward window length and isthe indicator calculator. Detailed calculation formulas of technicalindicators used in our MacroHFT are provided in Appendix A.Position is the amount of cryptocurrency a trader holds at a certaintime , which is denoted as , where 0, indicating that onlylong position is allowed in our trading approach.Net Value is the sum of cash and the market value of cryptocur-rency held by a trader, which can be calculated as = + ,where is the cash value and is the close price at timestamp .We highlight that the purpose of high-frequency trading is tomaximize the final net value after executing market orders on asingle cryptocurrency over a continuous period of time.",
  "MDP Formulation": "Due to the fact that high-frequency trading for cryptocurrencycan be treated as a sequential decision-making problem, we canformulate it as an MDP constructed by a tuple < ,,, , >. Tobe specific, is a finite set of states and is a finite set of actions; : is a state transition function which is composedof a set of conditional transition probabilities between states basedon the taken action; : R is a reward function measuringthe immediate reward of taking an action in a state; [0, 1) isthe discount factor. Then, a policy : will assign each state a distribution over action space , where has probability (|). The objective is to find the optimal policy so that the expected discounted reward = +=0 canbe maximized.When applying RL-based trading strategy for HFT, a single agentusually fails to learn an effective policy that can be profitable overa long time horizon because of the non-stationary characteristic incryptocurrency markets. To solve this problem, previous work has shown that formulating HFT as a hierarchical MDP could be aneffective solution on second-level HFT, where the low-level MDPoperating on second-level time scale formulates trading executionunder different market trends and the high-level MDP formulatesstrategy selection. Moving beyond second-level HFT, in this work,we focus on constructing a hierarchical MDP for minute-level HFT,where the low-level MDP formulates the process of executing actualtrading under different types of market dynamics segmented bymultiple criteria and the high-level MDP formulates the process ofaggregating different policies through incorporating macro marketinformation to construct a meta-trading strategy.Specifically, in our work, the hierarchical MDPs are operatedunder the same time scale (minute-level) so that the meta-policycan adapt more flexibly to frequent market fluctuations, which canbe formulated as (, )",
  "=< ,,, , >": "Low-level State, denoted as at time , consists of threeparts: single state features 1, low-level context features 2 and po-sition state , where 1 = 1(,) denotes single-state featurescalculated from LOB and OHLCV snapshot of the current time step,2 = 2(,, ...,+1,+1) denotes context features calcu-lated from all LOB and OHLCV information in a backward windowof length = 60, denotes the current position of the agent.Low-level Action {0, 1} is the action of sub-agent whichindicates the target position or trading process in the low-levelMDP. At timestamp , if > , an ask order of predefined sizewill be placed. If < , a bid order of a predefined size will beplaced. After that, +1 = .Low-level Reward, denoted as at time , is the net valuedifference between current time step and next one, which can becalculated as = ( (+1 ) | |) , where+1 and are close prices, is the transaction cost and is thepredefined holding size.High-level State, denoted as at time , consists of threeparts: low-level features 1, high-level context features 2 andposition state , where 1 denotes low-level features, which is thecombination of single-state features and low-level context featuresin low-level state, 2 denotes high-level context features, whichare the slope and volatility calculated over a backward window oflength as shown in .1, denotes the current positionof the agent, which is the same as low-level MDP.High-level Action, denoted as at time , is the action ofhyper-agent representing the target position of the trading processin the high-level MDP. Given a high-level state, the hyper-agentgenerates a softmax weight vector = [1, ... ], where is thenumber of sub-agents trained in low-level MDP. The final high-level",
  "Storage": "Phase II: Hyper-agent Training with Memory Augmentation train train train train train : The overview of MacroHFT. In phase I, we train multiple types of sub-agents with conditional adapters on the marketdata decomposed according to trend and volatility indicators. In phase II, we train a hyper-agent to mix decisions from allsub-agents, enhanced with a memory mechanism. action {0, 1} is still the target position which is calculatedas = arg max (=1 ) where denotes the outputQ-value estimation of -th sub-agent.High-level Reward, denoted as , at time is the net valuedifference between the current time step and the next one, whichis the same as low-level reward since our low-level and high-levelMDPs operate under the same time scale.In our hierarchical MDP formulation, for every minute, sub-agents trained under different market dynamics provide their owndecisions based on low-level states, and the hyper-agent executedin high-level MDP provides a final decision that takes all policiesprovided by sub-agents into consideration. Our goal is to trainproper sub-agents and a hyper-agent to achieve the maximumaccumulative profit.",
  "MACROHFT": "In this section, we will introduce the detailed workflow of Macro-HFT, which will be shown to be profitable in various non-stationarycryptocurrency markets. As shown in , MacroHFT mainlyconsists of two phases of RL training: 1) in phase one, MacroHFTwill use conditioned RL method to train multiple sub-agents onlow-level states tackling different market dynamics (markets of dif-ferent trends and volatilities); 2) in phase two, MacroHFT will traina hyper-agent to provide a meta policy to fully exploit the potentialof mixing diverse low-level policies based on recent market context.",
  "Because of data drifting caused by volatile cryptocurrency markets,it is usually impossible for a single RL agent to learn profitable": "trading policy from scratch over a long time period that containsvarious market conditions. We thus aim to train multiple sub-agentsto execute policies diverse enough to tackle different market dy-namics.Inspired by the market segmentation and labeling method intro-duced in , we propose a market decomposition method basedon the two most important market dynamic indicators: trend andvolatility. In practice, given the market data that is a time seriesof OHLC prices and limit order book information, we will firstsegment the sequential data into chunks of fixed length forboth the training set and validation set. Then we need to assignsuitable trend and volatility labels for each chunk so that eachsub-agent trained using data chunks belonging to the same marketcondition can handle a specific category of market dynamic. Specif-ically, 1) for trend labels, each data chunk will be first input intoa low-pass filter for noise elimination. Then, a linear regressionmodel is applied to the smoothed chunk, and the slope of the modelis regarded as the indicator of market trend; 2) for volatility labels,the average volatility is calculated over each original chunk so thatthe fluctuations are maintained.In this case, each data block will be assigned the labels of two mar-ket dynamic indicators, including one trend label and one volatilitylabel. Thus, all the data chunks can be divided into three subsets ofequal length based on the quantiles of slope indicator and also threeadditional subsets based on the quantiles of volatility indicator, re-sulting in 6 training subsets containing data from bull (positivetrend), medium (flat trend) and bear (negative trend) markets aswell as volatile (high volatility), medium (flat volatility) and sta-ble (low volatility) markets. After decomposing and labeling thetraining set, we further label the validation set using the quantile",
  "MacroHFT: Memory Augmented Context-aware Reinforcement Learning On High Frequency TradingKDD 24, August 2529, 2024, Barcelona, Spain": "cannot reply timely to the sudden fall in the ETH market whichleads to a huge loss. At the same time, MacroHFT without condi-tional adapter fails to adjust its trading strategy when the markettrend switches from flat or bear to bull, missing the chance to makemore profit. Meanwhile, our proposed MacroHFT with both con-ditional adapter and memory achieves strong performance underdifferent types of markets because of its ability to adjust policybased on market context and react promptly to abrupt fluctuations.",
  "Low-Level Policy Optimization withConditional Adaptation": "Although previous works have stated the fact that value-based RLalgorithms such as Deep Q-network have the ability to learn prof-itable policies for high-frequency cryptocurrency trading ,the trading agents performance is largely influenced by overfittingissue . To be specific, the policy network might be too sensitiveto some features or technical indicators while ignoring the recentmarket dynamics, which can lead to significant profit loss. Further-more, the optimal policy of high-frequency trading largely dependson the current position of a trader due to the commission fee. Mostexisting trading algorithms try to include position information bysimply concatenating it with state representations, but its effecton policy decision-making might be diminished because of its lowdimension compared with state inputs. To tackle these challenges,we propose low-level policy optimization with conditional adap-tation to train each sub-agent to learn adaptive low-level tradingpolicies with conditional control.For sub-agent training, we use Double Deep Q-Network (DDQN)with dueling network architecture as our backbone and usecontext features 2 as well as current position as additionalcondition input to adapt output policy. Given an input state tuple = (1,2, ) at timestamp , where 1, 2, denote singlestate features, context features and current position respectively,as defined in .2, we employ two separate fully connectedlayers to extract semantic vectors of single and context features,and also a positional embedding layer to discrete position, whichcan be formulated as:",
  "= 3() +2(2)(1)": "where 1 and 2 denote the fully connected layers, and 3 denotesthe positional embedding layer. The obtained condition representa-tion is constructed as the sum of the semantic vectors representingcontext and position information, and the single state is representedby its hidden embedding .Inspired by the Adaptive Layer Norm Block design in DiffusionTransformer , we propose to adapt the single state representa-tion based on condition feature so that the trained RL agentcan generate suitable policies based on different market conditionsand holding positions more efficiently. Thus, given the single staterepresentation , we first perform layer normalization acrossthe whole hidden dimension, and then construct scale and shiftvectors from condition vector by linear transformation:",
  "(,))(4)": "where is the value network, is the advantage network, isthe discrete action space. All network parameters are optimizedby minimizing the one-step temporal-difference error as well asthe Optimal Value Supervisor proposed in which is the Kull-backLeibler (KL) divergence between the agents Q estimationand optimal Q values () calculated from dynamic programmingof a given state. The loss function is constructed as follows:",
  "+ ( (, )||)(5)": "where is the policy network, is the target network, isthe optimal Q value, is the reward, is the discount factor and is a coefficient controlling the importance of optimal Q supervisor.Overall, in order to generate diverse policies that are suitable fordifferent market dynamics, 6 different sub-agents are trained usingthe above algorithm on 6 training subsets introduced in .1.The resulting low-level policies are further utilized to form the finaltrading policy by a hyper-agent, which will be introduced in thefollowing section.",
  "Meta-Policy Optimization with MemoryAugmentation": "After learning diverse policies tackling different market conditions,we further train a hyper-agent that takes the decisions made by allsub-agents into consideration and outputs a high-level policy thatcan comfortably handle market dynamic changes and be consis-tently profitable. Specifically speaking, for a group of optimizedsub-agents with Q-value estimators denoted as 1,2, ...,(N=6 in our setting), the hyper-agent outputs a softmax weight vec-tor = [1,2, ..., ] and aggregates decisions of sub-agents asa meta-policy function = =1 , which fully lever-ages opinions from different sub-agents and prevents the metatrading policy from being highly one-sided. Moreover, to enhancethe decision-making capability of the hyper-agent by correctly pri-oritizing sub-agents, a conditional adapter introduced in .2is also equipped, whose condition input is the slope and volatilityindicators calculated over a backward window.However, standard RL optimization under the high-level MDPframework encounters several difficulties. Firstly, because of therapid variation of cryptocurrency markets, the reward signals ofsimilar states can vary largely, preventing the hyper-agent fromlearning a stable trading policy. Secondly, the performance of ourmeta-policy can be largely affected by extreme fluctuations thatare rare and only last for a short time period, and the standardexperience replay mechanism can hardly handle these situations. Tohandle these challenging issues, we propose an augmented memorythat fully utilizes relevant experiences to learn a more robust andgeneralized meta-policy.",
  "EXPERIMENTS5.1Datasets": "To comprehensively evaluate the effectiveness of our proposedMacroHFT, experiments are conducted on four cryptocurrencymarkets, where the training, validation and test subset splittingis shown in . We first decompose and label the train andvalidation set based on market trend and volatility using the methoddescribed in .1. Then, we train a separate sub-agent ondata chunks with different labels in the training set and conduct Trading timestamp (min) (a) BTCUSDT Total Return(%) Trading timestamp (min) (b) ETHUSDT Total Return(%) Trading timestamp (min) (c) DOTUSDT Total Return(%) Trading timestamp (min) (d) LTCUSDT Total Return(%) MacroHFT (Ours)CDQNRPDQNCLSTM-PPOPPOIVMACDEarnHFT",
  "Experiment Setup": "We conduct all experiments on 4 4090 GPUs. For the trading set-ting, the commission fee rate is 0.02% for all four cryptocurrenciesfollowing the policy of Binance. For sub-agent training, the em-bedding dimension is 64 and the policy networks dimension is128. The decomposed data chunk length is explored over{360, 4320} 1. For each dataset, we conduct both training phases anddetermine based on the overall return rate of meta-policyover the validation sets. For BTCUSDT, is set as 360. For",
  "Results and Analysis": "The performance of MacroHFT and other baseline methods on 4cryptocurrencies are shown in and . It can be ob-served that our method achieves the highest profit and the highestrisk-adjusted profit in all 4 cryptocurrency markets for most ofthe evaluation metrics. Furthermore, although chasing for largerpotential profit can lead to higher risk, MacroHFT still performscompetently in risk management compared with baseline meth-ods. For baseline comparisons, value-based methods (CDQNRP,DQN) demonstrate consistent performance across a majority ofdatasets; however, they fall short in generating profit. Policy-basedmethods (PPO, CLSTM-PPO) exhibit high sensitivity during thetraining process and can easily converge to simplistic policies (e.g.buy-and-hold), resulting in poor performance, especially in bearmarkets. Certain rule-based methods (e.g., IV) can yield profit onmost of the datasets. However, their success heavily relies on theprecise tuning of the take-profit and stop-loss thresholds, whichnecessitates the input of human expertise. Nevertheless, there arealso rule-based trading strategies (e.g., MACD) that perform poorlyacross numerous datasets, leading to significant losses. The hierar-chical RL method (EarnHFT) achieves good performance on bothprofit-making and risk management over two datasets but fails tomake profits on the other datasets.To look into more detailed trading strategies of MacroHFT, wevisualize some actual trading signal examples in different cryp-tocurrency markets, which are shown in . From the tradingexample in the ETH market ((a)), it can be observed that byexecuting a potential \"breakout\" strategy, MacroHFT successfullyseizes the fleeting opportunity of making profits. This indicates thatour MacroHFT is able to respond rapidly to momentary market fluc-tuations and make profits in short intervals, which is the commongoal of high-frequency trading. From the trading example in theDOT market ((b)), it is apparent that MacroHFT executes atrend-following strategy over a long period of bull markets and exitsits position after gaining a substantial profit. It is evident that withthe help of conditional adaptation, our MacroHFT also shows greatcapacity of grabbing significant market trends and achieving betterlong-term returns. From the trading example in the LTC market((c)), it can be observed that MacroHFT executes a stop-lossaction when encountering a collapse and makes profits when the Trading timestamp (min) (a) ETHUSDT Total Return(%) Trading timestamp (min) (b) LTCUSDT Total Return(%) MacroHFTWithout MEMWithout CAMarket",
  ": Performance of original MacroHFT and two varia-tions without conditional adapter and memory": "market rebounds. In the trading example in the BTC market (Fig-ure 4(d)), MacroHFT still manages to seize the opportunity of smalladvances even in a bear market, indicating the robustness of ourmethod under adverse conditions. Furthermore, an example of thehyper-agents weight assignment of different sub-agents in the BTCmarket is also displayed. From the curves representing the averageweight changes of sub-agents in a 60-minute interval (),we can notice that MacroHFT successfully generates consistentlyprofitable trading strategies by mixing decisions reasonably fromdifferent sub-agents based on various market conditions, while itremains the ability to adjust quickly to sudden market changes.",
  "Ablation Study": "To investigate the effectiveness of our proposed conditional adapter(CA) and memory (MEM), ablation experiments are conducted byremoving respective modules and the results are displayed in Ta-ble 3. It can be observed that the original MacroHFT with bothconditional adapter and memory achieves the highest profit, thehighest risk-adjusted profit and the lowest investment risk exceptfor the MDD criterion of the ETH market. This indicates that bothconditional adapter and memory play important roles in generatingmore profitable trading strategies and controlling investment risks.For harsh trading environments such as DOTUSDT and LTCUSDTmarkets, where market values decrease by 14.85 % and 24.94% re-spectively, the removal of these two modules can cause significantdeficit.Furthermore, We can gain a more intuitive understanding of theinfluence of conditional adapter and memory modules on hyper-agents trading behavior from , which is the return ratecurves of different ablation models in ETH and LTC markets. Refer-ring to , it can be observed that MacroHFT without memory",
  "CONCLUSION": "In this paper, we propose MacroHFT, a novel memory-augmentedcontext-aware RL method for HFT. Firstly, we train different typesof sub-agents with market data decomposed according to the mar-ket trend and volatility for better specialization capacity. Agentsare also equipped with conditional adapters to adjust their tradingpolicy according to market context, preventing them from overfit-ting. Then, we train a hyper-agent to blend decisions from differentsub-agents for less biased trading strategies. A memory mechanismis also introduced to enhance the hyper-agents decision-makingability when facing precipitous fluctuations in cryptocurrency mar-kets. Comprehensive experiments across various cryptocurrencymarkets demonstrate that MacroHFT significantly surpasses multi-ple state-of-the-art trading methods in profit-making while main-taining competitive risk managing ability, and achieves superiorperformance on minute-level trading tasks.",
  "Nguyen Hoang Hung. 2016. Various moving average convergence divergencetrading strategies: A comparison. Investment Management and Financial Innova-tions 13, Iss. 2 (2016), 363369": "WU Jia, WANG Chen, Lidong Xiong, and SUN Hongyong. 2019. Quantitative trad-ing on stock market based on deep reinforcement learning. In 2019 InternationalJoint Conference on Neural Networks (IJCNN). 18. Zura Kakushadze. 2016. 101 formulaic alphas. Wilmott 2016, 84 (2016), 7281. Thomas Krug, Jrgen Dobaj, and Georg Macher. 2022. Enforcing Network Safety-Margins in Industrial Process Control Using MACD Indicators. In EuropeanConference on Software Process Improvement. Springer, 401413.",
  "Zichuan Lin, Tianqi Zhao, Guangwen Yang, and Lintao Zhang. 2018. Episodicmemory deep Q-networks. arXiv preprint arXiv:1805.07603 (2018)": "Xiao-Yang Liu, Hongyang Yang, Qian Chen, Runjia Zhang, Liuqing Yang, BowenXiao, and Christina Dan Wang. 2020. FinRL: A deep reinforcement learninglibrary for automated stock trading in quantitative finance.arXiv preprintarXiv:2011.09607 (2020). Yang Liu, Qi Liu, Hongke Zhao, Zhen Pan, and Chuanren Liu. 2020. Adaptivequantitative trading: An imitative deep reinforcement learning approach. InProceedings of the AAAI conference on artificial intelligence, Vol. 34. 21282135.",
  "Thibaut Thate and Damien Ernst. 2021. An application of deep reinforcementlearning to algorithmic trading. Expert Systems with Applications 173 (2021),114632": "Rundong Wang, Hongxin Wei, Bo An, Zhouyan Feng, and Jun Yao. 2021. Com-mission fee is not enough: A hierarchical reinforced framework for portfoliomanagement. In Proceedings of the AAAI Conference on Artificial Intelligence,Vol. 35. 626633. Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and NandoFreitas. 2016. Dueling network architectures for deep reinforcement learning. InInternational Conference on Machine Learning. 19952003."
}