{
  "ABSTRACT": "Fair graph learning plays a pivotal role in numerous practical appli-cations. Recently, many fair graph learning methods have been pro-posed; however, their evaluation often relies on poorly constructedsemi-synthetic datasets or substandard real-world datasets. In suchcases, even a basic Multilayer Perceptron (MLP) can outperformGraph Neural Networks (GNNs) in both utility and fairness. Inthis work, we illustrate that many datasets fail to provide mean-ingful information in the edges, which may challenge the neces-sity of using graph structures in these problems. To address theseissues, we develop and introduce a collection of synthetic, semi-synthetic, and real-world datasets that fulll a broad spectrum ofrequirements. These datasets are thoughtfully designed to includerelevant graph structures and bias information crucial for the fairevaluation of models. The proposed synthetic and semi-syntheticdatasets oer the exibility to create data with controllable bias pa-rameters, thereby enabling the generation of desired datasets withuser-dened bias values with ease. Moreover, we conduct system-atic evaluations of these proposed datasets and establish a uniedevaluation approach for fair graph learning models. Our extensiveexperimental results with fair graph learning methods across ourdatasets demonstrate their eectiveness in benchmarking the per-formance of these methods. Our datasets and the code for repro-ducing our experiments are available 1.",
  "These authors contributed equally to this work.Corresponding author.1": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor prot or commercial advantage and that copies bear this notice and the full cita-tion on the rst page. Copyrights for components of this work owned by others thanthe author(s) must be honored. Abstracting with credit is permitted. To copy other-wise, or republish, to post on servers or to redistribute to lists, requires prior specicpermission and/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain. 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08",
  "Graph Neural Network, Fairness, Node Classication": "ACM Reference Format:Xiaowei Qian, Zhimeng Guo, Jialiang Li, Haitao Mao, Bingheng Li, SuhangWang, and Yao Ma. 2024. Addressing Shortcomings in Fair Graph Learn-ing Datasets: Towards a New Benchmark. In Proceedings of the 30th ACMSIGKDD Conference on Knowledge Discovery and Data Mining (KDD 24),August 2529, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.",
  "INTRODUCTION": "Graph structure is ubiquitous language to model complicated rela-tionships. As much information is organized as graph structure,graph neural networks are becoming increasingly important invarious elds, including knowledge graphs , drug discov-ery and social media mining . GNNs are versatile inhandling tasks related to graphs, enhancing performance in activ-ities from node classication to link prediction and graph classication . However, accompanying the widedeployment in many critical systems , concerns about the po-tential risks associated with GNNs are growing. Research showsthat GNNs can either inherit or exacerbate bias in the data, lead-ing to unfair and biased predictions, which potentially reinforceexisting prejudices and discrimination . This issue has raisedethical and societal concerns, signicantly hindering GNNs appli-cation in sensitive decision-making areas, such as ranking job ap-plicants and predicting criminal behavior .To tackle the fairness challenge, a series of fair graph learningmodels have been developed, e.g., FairGNN , NIFTY , and ED-ITS . These methods aim to improve fairness while maintainingthe models accuracy. Building on the foundational concerns re-garding the development of fair graph learning models, it is crucialto scrutinize the existing evaluation frameworks that assess thesemodels. Upon examination, we nd that existing evaluation proto-cols suer from several pitfalls that impede our ability to properlyevaluate these methods, which are summarized as follows: The evaluation of fair graph learning models is often limited toa few poorly constructed semi-synthetic datasets convertedfrom tabular datasets and an array of real-world datasets .Specically, the graph connections in the semi-synthetic datasets",
  "KDD 24, August 2529, 2024, Barcelona, Spain.Qian and Guo, et al": "GCN: the number of layers {1, 2, 3}, the number of hidden unit 16,learning rate {1 2, 1 3, 1 4}, weight decay {1 4, 1 5},dropout {0, 0.5, 0.8}.FairGNN: the number of hidden unit 32, learning rate {1 2, 1 3, 1 4}, weight decay {1 4, 1 5}, dropout {0, 0.5, 0.8}, regu-larization coecients {4, 5, 50, 100} and {0.01, 1, 5, 20}. NIFTY: the number of hidden unit 16, project hidden unit 16, dropedge rate 0.001, drop feature rate 0.1, learning rate {12, 13, 14}, weight decay {14, 15}, dropout {0, 0.5, 0.8}, regularizationcoecient {0.2, 0.4, 0.6, 0.8}.",
  "Graph Neural Networks": "Graph neural networks (GNNs) have revolutionized the analysisof graph-structured data across various tasks, including node clas-sication , graph classication , and link prediction . These networks fall into two primary categories: spatial-basedGNNs, which utilize direct graph structures to focus on node andneighbor interactions for feature learning, and spectral-basedGNNs,which analyze graphs through the spectral domain using the graphLaplacian and its eigenvectors to grasp global graph properties.The exceptional capabilities of GNNs have broadened their applica-tion , ranging from nancial institutions using them to detectfraudulent activities in transaction networks to their integra-tion into critical decision-making systems where fairness and inter-pretability become paramount . Despite their widespread suc-cess, recent research highlights a signicant concern: GNNs canexhibit implicit biases towards dierent groups, potentially lead-ing to unfair outcomes . This issue is of particular concern insensitive applications, underscoring the urgency of incorporatingfairness into the GNN modeling process. Bias in GNNs typicallyarises from two sources: the inherent prejudices present in the in-put data, and the algorithmic tendencies of GNNs that may favorcertain patterns or connections . Consequently, theres a grow-ing movement within the research community towards developingfairer GNN models, aiming to address these biases for more equi-table graph-based applications .",
  "Fair Graph Learning": "Fairness has become a pivotal issue in machine learning, promi-nently within the Graph Neural Networks (GNNs) domain . The evaluation of model fairness encompasses various per-spectives, such as group fairness , individual fairness , andcounterfactual fairness , all of which are pertinent to GNN as-sessment . In the realm of GNN fairness, concepts like statisti-cal parity and equal opportunity are particularly promi-nent. Eorts such as FairGNN employ adversarial training toimprove fairness, aiming to prevent the model from leveraging sen-sitive attributes for predictions. However, the traditional relianceon correlation-based methods for ensuring fairness is challengedby their inability to navigate complexities such as Simpsons para-dox. This limitation has spurred a shift towards counterfactual fair-ness, rootedin causal theory, that promises a deeper, more nuancedapproach by focusing on causal relationships to circumvent biasesinduced by correlations. This shift is exemplied by recent innova-tions like NIFTY , showcasing a keen interest in counterfactualapproaches to GNNs. FairVGNN stands out by generating fairviews via the automatic identication and masking of sensitive-correlated features, adjusting to correlation changes after feature",
  "ISSUES OF EVALUATION SETTINGS": "When evaluating fair graph learning methods, we often care aboutboth the model classication performance and fairness. Speci-cally, the following metrics are often adopted for evaluation. Forevaluating the classication performance, we often utilize metricssuch as accuracy (ACC), ROC AUC, and F1-score. To quantify groupfairness, we use statistical parity (SP) and equal opportunity(EO) . The exact metrics are dened as:",
  ": end for": "not consider the trade-o between accuracy and fairness. The ef-fectiveness of Model selection 3 is highly dependent on manuallypreset thresholds, which is certainly cumbersome for evaluatingvarious methods on dierent datasets. Therefore, implementing aconsistent and equitable model selection strategy is imperative forthe benchmark of fair graph learning methods.Our model selection strategy is described in Algorithm 1. Com-pared to the existing strategies, the proposed model selection strat-egy balances utility and fairness and employs the adaptive thresh-olds. Since these graph fairness learning methods aim to sacricea small portion of utility for higher fairness, the threshold intervalis set as 90% 95% to trade-o. Additionally, using three classi-cation performance metrics ensures a fair comparison of baselineperformance. We anticipate that this standardized model selectionapproach will assist researchers in achieving a more equitable as-sessment of fair graph learning models. 4ISSUES OF POPULAR GRAPH FAIRNESSDATASETSGood datasets are essential for advancing the eld of study. How-ever, our thorough review reveals that datasets commonly used forfair graph learning suer from signicant issues that could slowprogress in this area. To verify these issues, we not only run GNNmethod and fairness-focused models, but we also include MLP asbaseline, which is not always included in existing literature. More-over, Our experiments set a ne-grained parameter search spacefor each baseline and uniformly employ our proposed model selec-tion strategy to obtain feasible comparisons. Our empirical nd-ings, detailed in Tables 3 and 4, illustrate these problems. We ex-amine both semi-synthetic and real-world datasets widely used inthe community, identifying specic concerns to be addressed.",
  "MetricNBA": "ACC ()67.32 0.5672.02 0.7070.33 0.4662.44 4.28AUC ()72.48 0.7476.95 0.1976.33 0.4569.27 1.22F1 ()71.14 2.3174.41 1.1974.50 0.7066.87 3.51SP ()4.00 0.922.03 0.861.85 1.296.21 1.88EO ()0.78 0.443.32 1.491.61 2.093.91 1.89 can be found in Appendix A.2.1. Our experiments led to severalconcerns detailed below:Obs 1: Considering predictive capabilities assessed through ACC,AUC and F1, alongside fair metrics such as dierence in SP andEO, graph-based approaches like GCN do not demonstrate supe-rior performance compared to MLP across various datasets, whichmay challenge the necessity to use graph-based methods in thesedatasets. As shown in , MLPs achieve comparable predictiveaccuracy without compromising fairness metrics on three widelyused semi-synthetic datasets. Specically, performance metrics forACC, AUC, and F1 scores for MLP and GCN are quite close acrossthese datasets. Whats worse, MLPs show a signicant advantagein terms of SP and EO, indicating a clear lead in fairness. Theresult is a strong signal that the graph structures of these semi-synthetic datasets do not contribute meaningful information forenhancing predictions. The rationale behind this is straightforward.According to the dataset generation process described in , thesedatasets originate from tabular data, with graph structures gen-erated based on feature similarity. Thus, reiterating this feature similarity through graph structures does not enrich graph-basedmethods with novel insights. Moreover, the emphasis on featuresimilarity in constructing graph structures might inadvertently in-troduce noise to graph-based models, such as GNNs, potentiallydeteriorating fairness metrics. These results raise concerns aboutthe necessity of using graph structures for these tasks and suggestthat these datasets may not be suitable for fair graph learning prob-lem.Obs 2: In the analysis of fairness-focused models (FairGNN and NIFTY)versus MLP, we observe no consistent superiority in utility and fair-ness. As detailed in , a simple MLP model outperforms thesefairness-focused modelsacross all evaluated metrics, including ACC,AUC, F1, SP, and EO, by a signicant margin. This discrepancydeepens concerns regarding the applicability of datasets for foster-ing development of fair graph learning algorithms.",
  "Addressing Shortcomings in Fair Graph Learning Datasets: Towards a New BenchmarkKDD 24, August 2529, 2024, Barcelona, Spain": "but also enhances GNN performance. Notably, variance in edgetypes leads to superior GNN outcomes for groups = 0, = 0, and = 1, = 1, albeit introducing some degree of unfairness.Syn-2 demonstrates an unbalanced group ratio (00 = 0.22, 01 =0.28, 10 = 0.28, 11 = 0.22), which engenders signicant unfair-ness in MLP predictions. Similar to Syn-1, the graph structure el-evates GNN performance. Here, denser connections yield betterresults for groups = 0, = 1, and = 1, = 0, further con-tributing to unfairness. By adjusting the Gaussian distributionsvariance, we lower MLPs baseline performance, thereby amplify-ing the graph structures benecial impact on performance.",
  "Summary": "Based on the observations, we can conclude that the primary is-sue across six existing datasets is the lack of meaningful infor-mation provided by their graph structures. Consequently, graph-based methods tend to underperform compared to MLP. If graphstructure were suciently informative, fairness-focused methodswould still attain higher accuracy than MLP, albeit model utility isslightly reduced compared to conventional GNN architectures. It isnoteworthy that fairness-focused methods also exhibit shortcom-ings in fairness compared to MLP in certain datasets, highlightingthe inadequacy of these datasets for evaluating fair graph learningmethods. Hence, future research should consider these limitationswhen selecting or creating datasets for assessing graph unfairness.",
  "NEW FAIR GRAPH LEARNING DATASETS": "In light of the issues identied with existing semi-synthetic andreal-world datasets, there is a pressing need for new datasets thatbetter benchmark fair graph learning methods. To address thesechallenges and push the boundaries of fair graph learning researchforward, we propose the introduction of new datasets specicallydesigned to overcome the limitations of current datasets. We seekto oer a more robust and challenging benchmark for developingand evaluating fair graph learning algorithms. This section out-lines the development process, characteristics, and potential im-pact of these new datasets on the eld of fair graph learning. Ourgoal is to facilitate the development of more accurate, fair, and gen-eralizable graph learning models that can navigate the intricaciesof real-world social structures and biases. In the construction ofnew datasets, we prioritize the following critical considerations: Graph Structure Utility. Graph structure must demonstrablyenhance predictive performance, i.e., helpful for prediction task. Bias Amplication through Graph Structure. Graph struc-ture should amplify the bias information. Thus, it can render the",
  "performance discrepancy for dierent fair graph learning meth-ods involving graph structure information": "These principles ensure that only models adept at leveraging graphstructure for enhanced information processing, while simultane-ously mitigating bias inherent within, will excel. Consequently,modelsrelying solely on feature-based methodologies will nd them-selves at a disadvantage due to their inability to harness the graphstructure. Similarly, methods that overlook the bias present in graphstructures will face challenges, pushing fairness-oriented modelsto innovate beyond merely identifying and correcting for bias. Thisapproach aims to foster the development of models that not onlycapitalize on informational wealth of graph structures but also nav-igate and neutralize biases eectively, setting a new standard forfairness in graph learning research. Starting from synthetic datasetsallows researchers to control utility and bias, and then transition tonew semi-synthetic datasets, and nally evaluate models on real-world datasets to provide realistic test scenarios. This progressivebenchmarking approach enables a thorough assessment of modelcapabilities across dierent stages of dataset realism, ensuring ro-bustness and eectiveness in real-world applications.",
  "This section explores the relationship between graph structuresand fairness performance, outlines the data generation process,and introduces two datasets to demonstrate our analysis frame-work": "5.1.1Interplay Between Edge Generation Probability and FairnessMetrics. We propose a comprehensive framework, illustratedin Ta-ble 5, to explore this interplay. Our focus is on scenarios with bi-nary sensitive attributes and binary labels, where the probability ofedge creation directly inuences the accuracy of dierent groups,subsequently impacting fairness metrics. This approach aids in thedesign and enhancement of synthetic and semi-synthetic datasets.The process unfolds in two pivotal steps: From Edge Generation Probability to Group Accuracy: Thecorrelation between the probability of generating edges and theaccuracy of specic groups is outlined in . For instance,if we x the edge generation probabilities for other connectionsand increase the probability for the \"S0Y0-S0Y0\" edge, we antici-pate an improvement in the accuracy for the \"S0Y0\" group. Thisstep provides a methodical way to predict group accuracy basedon edge generation dynamics. From Group Accuracy to Fairness Metrics: The fairness met-rics, such as Statistical Parity (SP) and Equal Opportunity (EO),are crucial for assessing fairness. SP gauges the variance in pre-dictive probabilities, whereas EO evaluates the accuracy discrep-ancy between groups. By examining the accuracy of various groups,we gain insights into potential changes in these fairness metrics,oering a straightforward strategy to assess and enhance fair-ness in model predictions.",
  "=0 N (, )=1 N (, ) ,": "where = 1 1 and = 2 1 represent the covariancematrices for and embeddings, respectively, with 1 and 2being scalars and 1 the identity matrix of dimension 1. Thevariance (, ) and mean (, ) parameters are adjustableto modulate the separability between the groups.(3) To construct the node attribute for each sample, concatenatethe embeddings and as follows:",
  "= | ,": "where [ | ] denotes the concatenation of the and em-beddings, resulting in a single, unied node attribute vector for each node.(4) In constructing the graph, we initiate the creation of edges byemploying independent Bernoulli distributions for each poten-tial edge type. Specically, the existence of each edge type is determined as follows:",
  "Real-world Datasets from Twitter": "We have constructed two novel datasets by leveraging the TwitterAPI, oering insights into real-world social dynamics and biases.These datasets, detailed below, serve as the foundation for our stud-ies on bias mitigation and the robustness of predictive models.Sport Dataset: Derived from Twitter, this dataset focuses on ath-letes in the NBA and MLB. We mapped players to their Twitter ac-counts, using these accounts as nodes. Edges represent followingrelationships between players. The sensitive attribute under con-sideration is the players race, categorized as either black or white.The objective is to predict the sport of a player (NBA or MLB) with-out bias inuenced by racial attributes. For node features, we aggre-gated the rst ve tweets from each players account and utilizedaverage of their BERT embeddings as feature representations.Occupation Dataset: This dataset also originates from Twitter,with nodes representing users and edges indicating follow relation-ships. The focus is on users identied within the elds of computerscience or psychology. User selection was stratied across multiplelayers: starting from a randomly chosen set of users (1st layer), weexpanded the dataset by including their followers (2nd layer) andrepeated this process up to six layers to ensure diversity. The sensi-tive attribute here is gender, with the aim to predict a users eld ofwork without gender-based bias. Node features were derived sim-ilarly to the Sport dataset, using the mean of BERT embeddingsfrom the users tweets.",
  "BENCHMARKING ON NEW DATASETS": "This section outlines our empirical investigation designed to assessthe utility and integrity of the newly developed datasets. The sta-tistics details of these new datasets are shown in . Our goalis to scrutinize the datasets through a series of experiments aimedat addressing the following pivotal questions: (RQ 1) Are the proposed datasets capable of yielding signi-cant insights and enhancing predictive performance within theirgraph structures? (RQ 2) Does the graph structure exhibit biased information, ne-cessitating a procient model that can adeptly harness the graphsstructure while also mitigating any inherent biases? (RQ 3) Can we gain insights into the commonly used methodswith our datasets? This section delineates our experimental evaluation, conductedto ascertain the ecacy of our newly introduced datasets in facili-tating fair graph benchmarking. Our experiments are designed tobenchmark existing models, thereby providing insights into theirperformance when applied to diverse and challenging scenarios.",
  "Experimental Setup": "In our benchmarking, we selected key fair graph learning methods,including FairGNN , which uses an adversarial method witha sensitive feature estimator for fairness, and NIFTY , employ-ing a novel augmentation for counterfactual fairness through con-trastive learning. Both methods are based on GCN to leveragegraph structure. We also compared these with a standard GCN to understand the role of graph topology and an MLP to gauge thebenet of incorporating graph structure.To ensure a fair and comprehensive comparison, we meticu-lously ne-tuned the hyperparameters for each model, tailored totheir optimal performance on our datasets. The specics regardingthese congurations are provided in Appendix B.1. Aligning withthe setup in NIFTY , our experiments utilize a one-layer GCN for",
  "Evaluation and Results": "presents the performance outcomes of various methodsacross the datasets we introduce. These datasets are specicallydesigned to test the capabilities of fair graph learning approaches,revealing several key attributes conducive to their evaluation. ForRQ 1, a comparison between MLP and GCN demonstrates signif-icant enhancements in predictive accuracy, as evidenced by im-provements in ACC, AUC, and F1 score across all seven datasets.This distinction is particularly notable when juxtaposed with theoutcomes from semi-synthetic and real-world datasets discussedin , underscoring the value of incorporating graph struc-tures into the analysis for augmenting model performance.For RQ 2, an analysis of MLP and GCNs performance basedon fairness metrics SP and EO reveals that incorporating graphstructures often leads to a signicant reduction in fairness acrossall seven datasets. This observation underscores the inherent trade-o in using graph data: while it can enhance model performance,it also risks compromising fairness. Nonetheless, a comparativeassessment of fairness-focused algorithms demonstrates notableimprovements in fairness metrics for most datasets. Specically,FairGNN shows comprehensive enhancements across all indica-torsACC, AUC, F1 score, SP, and EOon Syn-1 and Syn-2. Sim-ilarly, NIFTY exhibits parallel improvements on the New Germandataset. This trend suggests the feasibility of leveraging graph struc-tures to boost predictive accuracy while simultaneously mitigatingbias. Such capability is crucial for fairness benchmarking datasets,serving as a critical measure to evaluate a models ability to ex-ploit graph data benecially without sacricing fairness. Thesendings indicate that with carefully designed fair graph learningapproaches, it is possible to balance both predictive performanceand fairness objectives eectively.In the context of RQ 3, it is apparent that current fairness method-ologies struggle to consistently excel across all datasets. This chal-lenge sets a new benchmark, urging further innovation in modeldevelopment. For example, in the New Bail dataset, FairGNN lever-ages graph structures to surpass MLP in terms of ACC, AUC, andF1 scores, yet it must navigate the inherent biases within the graphdata, leading to a decrease in SP and EO. While FairGNN man-ages to mitigate bias more eectively than the baseline GCN, thisadjustment comes at the cost of reduced predictive accuracy. Con-versely, NIFTY faces diculties in optimally exploiting graph infor-mation, resulting in performance decits across all metrics, eventrailing behind MLP. Our evaluation presents a set of challengingtasks, making it dicult for any single method to excel across alldatasets. This situation oers substantial opportunities for the de-velopment of strong fair graph learning methods, paving the wayfor new milestones in the eld.",
  "CONCLUSION": "In conclusion, our exploration into fair graph learning underscoresthe critical importance of representative datasets for evaluating theperformance of fair graph learning methods. Through this work,we have identied a signicant gap in the quality and applicabilityof existing semi-synthetic and real-world datasets. Our ndings re-veal that, in many cases, simple models such as MLPs can surpassmore complex GNNs when the datasets lack meaningful graphstructures. To address these shortcomings, we have developed acomprehensive suite of synthetic, semi-synthetic, and real-worlddatasets designed with the explicit aim of facilitating a fair and rig-orous evaluation of fair graph learning methods. These datasets arecarefully crafted to encompass critical graph structures and biasinformation, challenging models to not only leverage graph struc-tures for enhanced prediction accuracy but also to eectively ad-dress and mitigate bias inherent in the data. We introduce a uniedframework for analyzing the edge generation probability to fair-ness metrics. Based on this, we provide controllable bias parame-ters in synthetic and semi-synthetic datasets, allowing researchersto tailor the datasets to specic research needs and bias considera-tions. Our systematic evaluation of these newly proposed datasetshas yielded extensive experimental insights. This work lays thefoundation for future progress in fair graph learning, promotingthe creation of models that eectively harness graph structureswhile prioritizing fairness. We introduce challenging tasks that testthe limits of any single method across diverse datasets, thereby cre-ating signicant opportunities for developing fair graph learningmethods and setting new benchmarks in the eld.",
  "ACKNOWLEDGEMENTS": "This material is based upon work supported by the National Sci-ence Foundation (NSF) Grant #2406648 and #2406647, Army Re-search Oce (ARO) under grant number W911NF-21-1- 0198, De-partment of Homeland Security (DHS) CINA under grant numberE205949D, and Cisco Faculty Research Award. Chirag Agarwal, Himabindu Lakkaraju, and Marinka Zitnik. 2021.Towardsa Unied Framework for Fair and Stable Graph Representation Learning.arXiv:2102.13186 [cs.LG] Siddhant Arora. 2020. A survey on graph neural networks for knowledge graphcompletion. arXiv preprint arXiv:2007.12374 (2020). Enyan Dai, Limeng Cui, Zhengyang Wang, Xianfeng Tang, Yinghan Wang, Mon-ica Cheng, Bing Yin, and Suhang Wang. 2023. A unied framework of graphinformation bottleneck for robustness and membership privacy. In KDD. Enyan Dai and Suhang Wang. 2021. Say no to the discrimination: Learning fairgraph neural networks with limited sensitive attribute information. In Proceed-ings of the 14th ACM International Conference on Web Search and Data Mining.680688. Enyan Dai, Tianxiang Zhao, Huaisheng Zhu, Junjie Xu, Zhimeng Guo, Hui Liu,Jiliang Tang, and Suhang Wang. [n. d.]. A Comprehensive Survey on Trustwor-thy Graph Neural Networks: Privacy, Robustness, Fairness, and Explainability.1, 1 ([n. d.]). Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:Pre-training of deep bidirectional transformers for language understanding.arXiv preprint arXiv:1810.04805 (2018). Yushun Dong, Ninghao Liu, Brian Jalaian, and Jundong Li. 2022. EDITS: Model-ing and Mitigating Data Bias for Graph Neural Networks.. In The Web Conference(WWW). ACM, 12591269. Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and RichardZemel. 2012. Fairness through awareness. In Proceedings of the 3rd innovationsin theoretical computer science conference. 214226.",
  "A.2Dataset Details": "A.2.1Detailed Existing Datasets. Here we present a detailed de-scription of six wildly-used datasets we used to validate our pro-posed issues as follows: German Credit (German): This dataset models clients asnodes, where edges reect a high similarity in credit ac-count activities. The objective is to classify individuals into high or low-credit risk categories, considering gender as thesensitive attribute. Recidivism (Bail): It comprises nodes representing defen-dants who were released on bail between 1990 and 2009.Edges are drawn between nodes with similar criminal recordsand demographic characteristics. The classication challengeinvolves predicting bail status based on the sensitive attributeof race. Credit Defaulter (Credit): In this dataset, nodes symbolizecredit card users, connected by edges that indicate similar-ity in purchasing and payment behaviors. The classicationgoal is to identify users likely to default on payments, withage serving as the sensitive attribute. Pokec: A widely recognized dataset from the Slovak socialnetwork, anonymized in 2012, segmented into two subsets:Pokec-z and Pokec-n. These subsets represent user prolesfrom two signicant regions within Slovakia, designated bytheir respective provinces. The datasets use the geographi-cal region of the users as the sensitive attribute, aiming topredict the employment sector of the users. NBA: Comprising data on roughly 400 NBA players, thisdataset uses a players nationality (categorized into U.S. ornon-U.S.) as the sensitive attribute. The dataset constructsa social graph of NBA players through their interactions onTwitter, with the predictive task focusing on determining ifa players salary is above or below the league median."
}