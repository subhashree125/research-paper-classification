{
  "ABSTRACT": "Advertising platforms have evolved in estimating Lifetime Value(LTV) to better align with advertisers true performance metricwhich considers cumulative sum of purchases a customer con-tributes over a period. Accurate LTV estimation is crucial for theprecision of the advertising system and the effectiveness of adver-tisements. However, the sparsity of real-world LTV data presents asignificant challenge to LTV predictive model(i.e., pLTV), severelylimiting the their capabilities. Therefore, we propose to utilize ex-ternal data, in addition to the internal data of advertising platform,to expand the size of purchase samples and enhance the LTV pre-diction model of the advertising platform. To tackle the issue ofdata distribution shift between internal and external platforms,we introduce an Adaptive Difference Siamese Network (ADSNet),which employs cross-domain transfer learning to prevent negativetransfer. Specifically, ADSNet is designed to learn information thatis beneficial to the target domain. We introduce a gain evaluationstrategy to calculate information gain, aiding the model in learninghelpful information for the target domain and providing the abilityto reject noisy samples, thus avoiding negative transfer. Addition-ally, we also design a Domain Adaptation Module as a bridge toconnect different domains, reduce the distribution distance betweenthem, and enhance the consistency of representation space distri-bution. We conduct extensive offline experiments and online A/B",
  "Corresponding authors": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08 tests on a real advertising platform. Our proposed ADSNet methodoutperforms other methods, improving GINI by 2%. The ablationstudy highlights the importance of the gain evaluation strategy innegative gain sample rejection and improving model performance.Additionally, ADSNet significantly improves long-tail prediction.The online A/B tests confirm ADSNets efficacy, increasing onlineLTV by 3.47% and GMV by 3.89%.",
  "Lifetime Value Prediction, Adaptive Cross-Domain Transfer Learn-ing, Computational Advertising": "ACM Reference Format:Ruize Wang, Hui Xu, Ying Cheng, Qi He, Xing Zhou, Rui Feng, Wei Xu, LeiHuang, and Jie Jiang. 2024. ADSNet: Cross-Domain LTV Prediction with anAdaptive Siamese Network in Advertising. In Proceedings of the 30th ACMSIGKDD Conference on Knowledge Discovery and Data Mining (KDD 24),August 2529, 2024, Barcelona, Spain. ACM, New York, NY, USA, 10 pages.",
  "INTRODUCTION": "The Lifetime Value (LTV) in an advertising system is defined as thecumulative sum of purchases (i.e., revenue for advertisers) a cus-tomer contributes over a given period. Given its direct correlationwith return on investment (ROI), this metric attracts significantattention from advertisers. Consequently, advertising platformshave progressively evolved to support LTV estimation ,thereby aligning more closely with the assessment requirementsof advertisers. Accurate estimation of LTV plays a crucial role inimproving the precision of advertising systems and ensuring theeffectiveness of advertisements.",
  "internal": ": Illustration of the advertising system conversionfunnel and challenges, including: (a) the sparsity of inter-nal purchase data, (b) introducing external data, (c) negativetransfer due to data distribution shift. Recently, some efforts have been made to improve the perfor-mance of LTV estimation. For example, Wang et al. proposeto model LTV as zero-inflated lognormal (ZILN) distribution toaddress heavy-tailed problem. Some methods propose tomodel user behaviors and use a feature missing-aware network toreduce the effect of the missing features while training.However, as purchase is close to the end of the advertising conver-sion funnel, data sparsity becomes an issue as conversion behaviordeepens. This sparsity of real-world LTV data presents a formidablechallenge to LTV predictive models, severely limiting their capabil-ities and having received far less attention. To address this problem,it is essential to leverage abundant external data to enhance theLTV prediction model of advertising platforms.Cross-domain transfer learning has emerged as apromising approach to bridge the gap between different domains,particularly when there is a sparsity of labeled data in the tar-get domain. This approach has been successfully applied in var-ious fields, such as computer vision, natural language process-ing, and recommender/adverisiting systems. Some methods focuson transferring knowledge from the data perspective through ad-justment and transformation of samples and features, includinginstance weighting, feature transformation.Multi-domain learning approaches propose to mix multiplesources of data for training a unified model in a multi-task manner.However, existing cross-domain transfer learning methods oftensuffer from negative transfer, which occurs when knowledge ofthe source domain is not beneficial or even harmful to the learningprocess of the target domain. Therefore, it is critical to develop amore robust and adaptive cross-domain transfer learning methodto mitigate the impact of negative transfer.In this paper, we propose an Adaptive Difference Siamese Net-work (ADSNet) to address the challenges associated with LTV esti-mation and cross-domain transfer learning. ADSNet employs a gainevaluation strategy based on a pseudo-siamese structure, effectivelylearning beneficial information for the target domain while reject-ing noisy samples and avoiding negative transfer. Furthermore, ourDomain Adaptation Module bridges different domains, reducingdistribution distance and fostering consistent representation spacedistribution. We conduct extensive experiments and perform onlineA/B tests in a real online advertising scenario. Experimental results demonstrate that our approach significantly improves performanceon the LTV prediction dataset. Further analysis highlights the ef-fectiveness of our model in rejecting negative gain samples andimproving long-tail prediction capabilities.The contributions of this paper can be summarized as follows:",
  "We introduce a novel approach to address data sparsity byintegrating external data with the internal data from the ad-vertising system for LTV estimation, utilizing a cross-domaintransfer framework": "We propose an Adaptive Difference Siamese Network (AD-SNet) to tackle negative transfer. Utilizing a pseudo-siamesestructure in conjunction with a gain evaluation strategy, wefacilitate the assimilation of beneficial external informationinto the target domain while effectively filtering out noise.Additionally, we incorporate a domain adaptation moduleto promote consistency across different domains. Extensive experiments reveal that ADSNet surpasses othermodels, substantially enhancing performance and mitigatingnegative transfer. Additionally, ADSNet effectively improveslong-tail prediction. Online A/B tests further showcase thepractical advantages of ADSNet in real-world advertisingsystems.",
  "RELATED WORK2.1Customer Lifetime Value Prediction inAdvertising": "Customer Lifetime Value (LTV) prediction has become an integralcomponent of modern advertising platforms, owing to its direct im-pact on the effectiveness of advertisement placements and overalladvertising system precision. Previous studies have focused on var-ious approaches to improve the accuracy of LTV estimation. Someprevious works explore deep learning methods for LTVprediction, emphasizing the potential of neural networks in under-standing complex user behaviors. Reddy et al. present a com-parative analysis of traditional statistical methods versus machinelearning techniques for LTV estimation. Addressing the issue ofmissing features in real-world advertising scenarios, MarfNet proposes a feature missing-aware network designed to mitigatethe impact of these missing features during training, rather thanfill them with default values. Due to LTV of customers followsa heavy long-tailed distribution with significant fraction of zerovalue, Wang et al. proposes to model the LTV as a zero-inflatedlognormal (ZILN) distribution which is described as a mixture ofzero-point mass and lognormal distribution, to address the heavy-tailed problem. In this modeling manner, the model fits the meanand deviation of the distribution. But the distribution assumptionis simple and does not meet the multimodal distribution in realscenarios, which leads to limitations. ODMN introduces anapproach that partitions the complex LTV distribution into severalsub-distributions, each trained by distribution experts.In the conversion funnel exposure->click->activation->purchase,compared to the CTR exposure->click and CVR click->activationprediction, LTV prediction is a more challenging problem due topurchase being the deepest behavior, the data on LTV is very sparse.Although the aforementioned methods obtain better performance",
  "Input": ": (a) Conventional multi-domain models incorporate source domain knowledge by simply aggregating both source andtarget domain data and training in a joint manner, which result in introducing noisy samples and even lead to the problem ofnegative transfer due to data distribution shifts. (b) Our ADSNet explores a cross-domain transfer learning method. It employsthe pseudo-siamese network to differentially evaluate the information gain provided by the source domain, which supportsthe rejection of negative gain samples, thereby helping the model learn information that is beneficial to the target domain. on LTV prediction, they struggle with the sparsity of real-worldLTV data. The amount of data determines the upper limit of themodels performance . Motivated by such constraints,we utilize external data from sources other than the advertisingplatforms internal data, which aims to increase the number ofpurchase samples and improve the LTV prediction model of theadvertising platform.",
  "Cross-domain Transfer Learning": "Cross-domain transfer learning has emerged as a powerful approachto leverage knowledge from one domain to improve learning inanother, especially in scenarios where the target domain suffersfrom data sparsity. However, a significant challenge in this fieldis the occurrence of negative transfer, where irrelevantor noisy information from the source domain hinders the learningprocess in the target domain, a key challenge in this area.The concept of domain transfer has also been explored in thecontext of multi-task learning in Click-Through Rate (CTR) and Con-version Rate (CVR) prediction scenarios. PLE basicallyfollows the gate structure and attention network for informationtransfer, similar to MoEs . It separates task-sharing andtask-specific parameters to learn shared and private representationsfor each task explicitly, and introduces a progressive routing man-ner. STAR leverages partitioned normalization (PN) to privatizenormalization for examples from different domains, and consists ofshared centered parameters and domain-specific parameters, adap-tively modulating its parameters conditioned on the domain formore refined prediction. CCTL introduces a framework to mit-igate the effects of negative transfer for different business domainsin CTR prediction scenario. It evaluates the information gain of thesource domain on the target domain using a symmetric companionnetwork. Compared to CCTL, ADSNet focuses specifically on LTVprediction and employs a gain evaluation strategy to calculate theinformation gain and reject noisy samples. Additionally, ADSNetintroduces a domain adaptation module to reduce the distributiondistance between different domains and enhance the consistencyof representation space distribution.",
  "TASK DEFINITION": "Definition 1 (Lifetime Value). Lifetime Value (LTV) in an adver-tising system is defined as the cumulative sum of purchases (i.e.,revenue for advertisers) a customer contributes over a certain pe-riod. This metric is crucial for advertisers in assessing their returnon investment (ROI). Definition 2 (Customer LTV Prediction). Within the contextof an advertising system, the goal of the LTV prediction task is toestimate the LTV for a given (user, ad) pair . In thisscenario, represents the specific (user, ad) combination, and corresponds to the LTV generated by the user for the advertisement.Specifically, given a set of samples D = {,}=1 consisting of Ndata-label pairs, we aim to compute the LTV between the user U and an advertisement A. This progress can be formulatedas following: = = (x | , ) ,(1) where denotes the parameters of the LTV estimation model. Themodel tasks sample = (,) as input, where is user featuresincluding user historical behavior (e.g., click and conversion se-quences), user profile (e.g., age, gender and purchase frequencey),and is ad features such as the tile and category. [0, ) isdenoted as LTV label.",
  "METHOD": "As illustrated in (a), most of the previous works improvemodels by integrating knowledge from the source domain via multi-domain joint learning. Regardless of these different variations ofmethods with multi-domain learning, the common issues not fullystudied are negative transfer induced by the domain shift. To ad-dress this, we present our approach ADSNet as shown in (b) and . We utilize the pseudo-siamese network to evalu-ate the information gain, to support learning information that isbeneficial to the target domain and reject noisy samples with thegain evaluation strategy. Additionally, we introduce a domain adap-tation module as a bridge to connect different domains, reduce thedistribution distance between them, and enhance the uniformity",
  "Gain Evaluation": ": Overview of our proposed ADSNet approach. A pseudo-siamese architecture (part 1) is employed to establish a metricto contrast the differences between two networks, allowing the calculation of each inputs gain and supporting the rejection ofnegative gain samples (part 2). A domain adaption module (part 3) is proposed to promote consistency across different domains. of the representation space distribution. In this section, we firstdescribe the base model(Sec. 4.1) for LTV prediction, the structureof our ADSNet including the pseudo-siamese network(Sec. 4.2),the gain evaluation strategy(Sec. 4.3) and domain adaptation mod-ule(Sec. 4.4), and then present the adaptive training process withiterative alignment strategy (Sec. 4.5).",
  "Backbone for LTV Prediction": "To better meet the complex distribution of LTV in real advertisingscenarios, we develop a deep neural network (DNN) with OrdinalClassification as our backbone for LTV prediction,which is a classic framework consisting of the encoding layer, expertlayer, and tower layer. 4.1.1Encoding Layer. We categorize the features into distinctfields according to their characteristics. For instance, the users ba-sic profile attributes, such as age, gender, and region, constitute onefield, while different user behavior sequences form another field.Given the input features , we encode features in different fieldsinto embedding vectors and employ Field-weighted FactorizationMachines (FwFM) to model the different interaction of featuresbetween different fields, and the embedding vectors are concate-nated as the final embedding representation E = (X),where X is the input feature. It is worth noting that alter-native encoding methods could also be employed, such as DeepCross Network (DCN), Learning Hidden Unit Contributions(LHUC), Transformer, etc., to either replace FwFFM or beused in conjunction with it. 4.1.2Expert Layer. The expert layer is designed to learn andrepresent various aspects of the input by incorporating multipleexperts, each of which is responsible for capturing specific patternsor characteristics within the data. Inspired by the success of Mix-ture of Experts(MoE) architecture , we employ PLE as expert layer here, which consists of a set of expert networksand a gating network. Each expert network is implemented as aMulti-Layer Perceptron (MLP), and the gating network is respon-sible for determining the contribution of each expert to the finaloutput. Given the encoded representation E from the encodinglayer, we feed it into each expert network, obtaining a set of expertoutputs = { }=1, where is the number of experts. The gatingnetwork, which is also an MLP with sofrmax function, takes thesame input and produces a set of gating weights { }=1, with eachcorresponding to the weight of expert. The final output ofthe expert layer is a weighted sum of the expert outputs, with thegating weights determining the contribution of each expert:",
  "=1 .(2)": "4.1.3Tower Layer. The tower layer takes the expert layers out-put and generates the final LTV prediction. The LTV of customers inmobile gaming typically exhibits two distinct traits: 1) a long-taileddistribution with a substantial proportion of zero values, and 2) amultimodal distribution of purchases due to standardized purchasetiers (e.g., $6, $30, $98, and $198). To this end, we extend a multi-granularity prediction module, which comprises two components:",
  "where 1{} is the indicator function, which represents the presenceof a positive sample, i.e., whether a purchase has been made": "Amount of Purchase Prediction. Different from ZILN-basedmethods which typically employ a ZILN loss to approxi-mate the complex purchase distributions mean and variance, wedevelop a multi-class classification module with ordinal classifi-cation . It divides the LTV distribution into several sub-distributions and performs prediction over each sub-distributionwith multiple binary classifiers. This helps model to learn the or-dered nature of purchase categories, and allows for the direct mod-eling of the cumulative distribution function of purchases, which ismore aligned with the inherently sequential progression of purchaseamounts. We transform continuous purchase labels into a set ofbinary classification labels to reflect rank information. Specifically,the original LTV label is assigned to a segment , which repre-sents the segment label for the LTV ranking level. The segmentsare determined by frequency equalization to maintain a relativelybalanced sample size across them. Then, each segment(rank) la-bel is expanded to 1 binary class labels {1 , . . . ,1} suchthat {0, 1} is indicates whether exceeds rank . For exam-ple, = 1 { > }. The indicator function 1{} is 1 if the innercondition is true and 0 otherwise. Each binary classifier employsa sigmoid activation function, and represents the probabilityprediction of the -th binary classifier. In the process of inference,the predicted LTV (pLTV) is calculated as:",
  "The Siamese network is a typical architecture in deeplearning, which comprises two branches with identical structures": "and uses similar and dissimilar pairs to learn similarity. In contrast,the Pseudo-Siamese network offers more flexibility thanthe Siamese network, as it allows different structures to receiveinputs from various modalities. Drawing inspiration from theseframeworks, we integrate the Pseudo-Siamese Network to assessthe information gain, to support learning information from sourcedomain, e.g., external data outside of advertising platform, thatis beneficial to the target domain and reject noisy samples. Thisselective transfer capability is crucial in practical scenarios.Specifically, our pseudo-siamese network is composed of a vanillanetwork and a gain Network. Those two networks are based on thebackbone for LTV prediction as Sec. 4.1. The gain Network receivesinputs from both external and internal samples, while the vanillaNetwork is exclusively fed with samples from the internal channel.During training, both networks will update their parameters. Thisconcurrent parameter updating allows each network to learn fromits respective data stream, with the gain network adjusting to thenuances of both the external and internal channel samples, and thevanilla network refining its understanding based on the internalchannel data alone. This process is key to enabling the pseudo-siamese network to effectively differentiate and integrate relevantinformation from diverse data sources. In the training process, wedefine the losses calculated by L as L, of external data and, of internal data from the gain network. And vanilla networkLoss of internal data loss is L = L, from vanilla network.",
  "= L, L,,(8)": "where represents the gain. If > 0, it means there is a pos-itive gain to the internal domain. It is notable that, in addition tothe aforementioned methodology, we can extend to employ rein-forcement learning by defining metrics that are pertinent to thebusiness objectives. The differential in these metrics can be utilizedas a reward signal to train the network by Adversarial RewardLearning .",
  "Domain Adaptation Module": "The domain adaptation module serves as a bridge between the gainNetwork and the vanilla Network, mitigating the disparity betweendomain distributions. To this end, an adapter layer is integrated atthe bottom of the tower module, implemented as an MLP. First, theadapter layer within the gain network is employed to estimate thesignificance of external data as , which is calculated as:",
  "Lgain = Lgain, s1( > 0) + Lgain, t(13)": "where Lgain, s is external data loss, Lgain, t is internal data loss fromgain network respectively. is the weight of external data and is the information gain. The indicator function 1( > 0)denotes the selective transfer capability of ADSNet, which permitsonly the advantageous external data to be utilized. Iterative Alignment Strategy. Considering that the performanceof the gain network is expected to improve over time, creating awidening gap between it and the vanilla network, such a scenariocould eventually render the gain evaluation strategy ineffective. Toaddress this issue, we propose to use iterative alignment strategyduring the training process to prevent divergence between the twonetworks. Specifically, the model training is divided into two stagesas in Algorithm 1: (1) Warmup Stage: Only internal samples are",
  "EXPERIMENTS5.1Experimental Setup": "5.1.1Datasets. Due to the lack of public dataset on LTV predic-tion, we construct an industry dataset to conduct offline evaluation.The dataset is collected from from a sampling of conversion logsfrom Tencents online advertising system over a span of 90 days,covering four internal traffics (i.e., business domains) and autho-rized external data from other platforms. The dataset consists ofbillions of examples, and is split according to the time axis, with 70,10, and 10 days worth of samples allocated for training, validation,and testing, respectively. The main statistics are shown in ,where the sample size is calculated as the average per day. Due tocompany privacy policy, we only disclose the purchase sample sizeand LTV. displays the sample size, and average LTV for eachdomain. As indicated in this table, different domains exhibit dis-tinct domain-specific data distribution, as reflected in the varyingLTVs. It can be observed that the domain with the highest Avg.LTV(Domain #3) is 4.82, while the domain with the lowest Avg.LTV(Domain #2) is only 0.07. The external data have a substantiallylarger average LTV, indicating a different data distribution fromthe internal data. These variations in the dataset provide a compre-hensive ground for evaluating the performance of LTV predictionmodels across different domains and data distributions. 5.1.2Metrics. we focus on evaluating the performance of a modelthat predicts customer lifetime value (LTV) by differentiating high-value customers from low-value ones. For this purpose, we employtwo evaluation metrics: the Area Under the Curve (AUC) and theNormalized Gini Coefficient (Norm GINI). The AUC is a widelyused metric for assessing the models ability to identify purchaseusers, as it measures the models classification performance. Ahigher AUC value indicates that the model can better discriminatebetween positive (purchase) and negative (non-purchase) classes.However, the AUC does not provide information about the accu-racy of the users ranking based on the predicted LTV. To addressthis issue, we further adopt the Normalized Gini Coefficient (NormGINI) . The Norm GINI is a robust measure that captures themodels ability to rank users according to their predicted LTV accu-rately. It is preferred over the Mean Squared Error (MSE) due to itsrobustness to outliers and better business interpretation. The NormGINI ranges between 0 and 1, with a value of 1 indicating perfect",
  "consistency between the ranking based on predicted LTV and theranking based on the real LTV. The lower bound of 0 correspondsto the random ordering of customers": "5.1.3Hyper-Parameter Settings. We implemented all methodsusing Tensorflow. In the training stage, we utilize the Follow-the-Regularized-Leader (FTRL) optimizer for sparse parametersand the Follow the Moving Leader (FTML) optimizer for denseparameters, in which the learning rate is set within the range of {5e-3, 1e-2}, respectively. The batch size is fixed as 512. The embeddingdimension is set to 32, which means that each input feature isrepresented by a 32-dimensional vector. The architecture of eachexpert consists of an MLP with two hidden layers with a hidden sizeof . The tower layer of all methods is designed as an MLPwith a hidden size of 32. The weight of the domain adaptation loss,denoted as , is set to 0.1 according to the grid search performedon the validation set.",
  "Comparisons with State-of-the-Arts": "illustrates the performance of various models on the pro-posed LTV prediction dataset. It is evident that our backbone model(i.e., ADSNet-Backbone) serves as a strong baseline, and our fullmodel (i.e., ADSNet) further enhances performance.In particular, our base model outperforms other single-domainmethods such as GateNet and ZILN, suggesting that the ordinal clas-sification is more suitable for modeling the complex multi-modal",
  ": Tendency of negative gain rejection rate and GINIduring training /or / the Gain Evaluation Strategy (GES)": "LTV distribution. Our full model, ADSNet, significantly outper-forms cross-domain methods that leverage external data across alldomain datasets, achieving absolute overall GINI and AUC improve-ments of 1% to 5%. This substantiates the effectiveness of integratingexternal data into the neural network. It is noteworthy that somemodels experience a decrease in performance when introducing ex-ternal data. For instance, Share-Bottom w/ZILN, compared to ZILN,shows a decline in overall GINI by 0.4%. This can be attributedto the substantial differences in data distribution across variousdomains, as illustrated in . It demonstrates that conven-tional multi-domain joint learning models fail to avoid the negativetransfer phenomenon. In contrast, our ADSNet model exhibits su-periority by supporting the rejection of negative gain samples, thuseffectively mitigating this issue.",
  "Ablation Study": "To investigate the effectiveness and illustrate the impact of differentcomponents of our proposed model, we conduct ablation studieson four ablations built upon our full model as shown in .In particular, (1) ADSNet w/o the Gain Evaluation Strategy (row 3)exhibits a noticeable decrease in GINI, dropping from 0.856 to 0.824.This suggests that the gain evaluation strategy plays a significantrole in rejecting noise samples and stressing negative transfer. (2)The removal of the Domain Adaptation Module (row 2) results inGINI dropping by an absolute 2.3%, indicating that constraining thedistribution between the gain network and the vanilla network isbeneficial to enhance the process of knowledge transfer. Further-more, (3) the removal of the Iterative Alignment Strategy (row 4)also leads to lower GINI, which indicates that this iterative processis essential for refining the models predictions in an incrementalfashion, aligning the model more closely with the target domain.Iterative alignment likely facilitates a more nuanced adaptationthat incrementally bridges the domain gap.",
  ": Comparison over the interval of sample size": "models ability to identify and reject negative gains. We define thenegative gain rejection rate, which represents the probability thatexternal data is rejected, i.e., the gain < 0. illustratesthe change in the negative gain sample rejection rate and GINIwith training steps. The X-axis represents the training steps, theleft Y-axis represents the rejection rate of negative gain samples,and the right Y-axis represents GINI. Several observations can bemade from this: (1) The GINI exhibits nonlinear growth. It improvesrapidly in the early stages and then the rate of improvement slowsdown over time. (2) The negative gain rejection rate is low in theearly stage and then increases with the progress of training. Thisindicates that the model gradually learns to identify and distinguishnegative gain samples during the training process. Ultimately, thenegative gain rejection rate stabilizes at 0.64, suggesting that a sig-nificant portion of the external samples may not be beneficial ifused directly. These observations highlight the effectiveness of thegain evaluation strategy which ensures that only external data thatcontribute positively to the target domain are utilized.",
  "Effectiveness of Improving Long-tailPrediction": "In a practical advertising system, we are struggling with the chal-lenge of data sparsity in the LTV estimation scenario. To furtherunderstand the improvement achieved by our models, we conduct amore detailed analysis. We quantify the sample sizes correspondingto each advertisement and categorize them into intervals to exam-ine the relationship between the models predictive capabilities andthe size of samples. The results are presented in , whichdemonstrates two insights. First, it intuitively shows that the perfor-mance of the model is positively correlated with the sample size. Asthe sample size increases, the performance of the model improvesaccordingly. Moreover, compared to the ADSNet-Backbone thatdoes not utilize external data, our ADSNet gains significant im-provements by 15.2% GINI in the long tail interval , whichindicates that by introducing external data, the models ability topredict long-tail advertisements is significantly enhanced.",
  "ADSNet: Cross-Domain LTV Prediction with an Adaptive Siamese Network in AdvertisingKDD 24, August 2529, 2024, Barcelona, Spain": "based on the required profit margin. Therefore, from a formulaicperspective, GMV and LTV are highly correlated.We conduct online A/B tests on the real advertising platformof Tencent Ads. Specifically, we employ UV sampling to split thetraffic, allocating 5% of the traffic to the control group and another5% to the experimental group. The base serving model is a variant ofSTAR, adapted to our business characteristics. For the experimentalgroup, we deploy our ADSNet model. Our online evaluation metricsare the LTV and Gross Merchandise Value (GMV). Due to companyprivacy policies, we only report the relative improvement. Theonline A/B test results indicate that ADSNet leads to an increasein online LTV by 3.47% and GMV by 3.89% compared with thebase model. These results underscore the practical applicability andeffectiveness of ADSNet in the LTV prediction task, demonstratingits potential to significantly enhance the performance of real-worldadvertising systems.",
  "CONCLUSION AND FUTURE WORK": "We present the Adaptive Difference Siamese Network (ADSNet) toaddress the challenges of data sparsity of LTV estimation and cross-domain transfer learning in advertising systems. We incorporateexternal data to expand the sample size. Our ADSNet utilizes cross-domain transfer learning, a gain evaluation strategy, and a domainadaptation module to learn beneficial information, reject noisysamples, and bridge different domains. Extensive experiments andonline A/B tests have demonstrated the effectiveness of ADSNet inimproving performance and mitigating negative transfer, as wellas its ability to enhance long-tail prediction capabilities. In futurework, we will extend our approach to other related advertising tasks,such as click-through rate prediction to evaluate the generalizabilityof ADSNet in broader advertising scenarios. We gratefully acknowledge the contributions of the following: PiaoYang, Ting Wang, Yucheng Hu, Chaoyue Zhao, Liwei Lin, CongQuan and Kun Bai. This work was partially supported by the Ten-cent Rhinoceros Project. Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora,Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, EmmaBrunskill, et al. 2021. On the opportunities and risks of foundation models. arXivpreprint arXiv:2108.07258 (2021).",
  "Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.DeepFM: a factorization-machine based neural network for CTR prediction. arXivpreprint arXiv:1703.04247 (2017)": "Qing Guo, Wei Feng, Ce Zhou, Rui Huang, Liang Wan, and Song Wang. 2017.Learning dynamic siamese network for visual object tracking. In Proceedings ofthe IEEE international conference on computer vision. 17631771. Sunil Gupta, Dominique Hanssens, Bruce Hardie, Wiliam Kahn, V Kumar,Nathaniel Lin, Nalini Ravishanker, and S Sriram. 2006. Modeling customerlifetime value. Journal of service research 9, 2 (2006), 139155.",
  "Jing Jiang and ChengXiang Zhai. 2007. Instance weighting for domain adaptationin NLP. ACL": "Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang,and Qun Liu. 2019. Tinybert: Distilling bert for natural language understanding.arXiv preprint arXiv:1909.10351 (2019). Kunpeng Li, Guangcui Shao, Naijun Yang, Xiao Fang, and Yang Song. 2022. Billion-user Customer Lifetime Value Prediction: An Industrial-scale Solution fromKuaishou. In Proceedings of the 31st ACM International Conference on Information& Knowledge Management. 32433251. Wen Li, Lixin Duan, Dong Xu, and Ivor W Tsang. 2013. Learning with augmentedfeatures for supervised and semi-supervised heterogeneous domain adaptation.IEEE Transactions on Pattern analysis and machine intelligence 36, 6 (2013), 11341148. Zhongqi Lu, Erheng Zhong, Lili Zhao, Evan Wei Xiang, Weike Pan, and QiangYang. 2013. Selective transfer learning for cross domain recommendation. InProceedings of the 2013 SIAM International Conference on Data Mining. SIAM,641649. Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. 2018.Modeling task relationships in multi-task learning with multi-gate mixture-of-experts. In Proceedings of the 24th ACM SIGKDD international conference onknowledge discovery & data mining. 19301939. Xiao Ma, Liqin Zhao, Guan Huang, Zhi Wang, Zelin Hu, Xiaoqiang Zhu, and KunGai. 2018. Entire space multi-task model: An effective approach for estimatingpost-click conversion rate. In The 41st International ACM SIGIR Conference onResearch & Development in Information Retrieval. 11371140. Brendan McMahan. 2011. Follow-the-regularized-leader and mirror descent:Equivalence theorems and l1 regularization. In Proceedings of the FourteenthInternational Conference on Artificial Intelligence and Statistics. JMLR Workshopand Conference Proceedings, 525533. Zhenxing Niu, Mo Zhou, Le Wang, Xinbo Gao, and Gang Hua. 2016. Ordinalregression with multiple output cnn for age estimation. In Proceedings of the IEEEconference on computer vision and pattern recognition. 49204928. Junwei Pan, Jian Xu, Alfonso Lobos Ruiz, Wenliang Zhao, Shengjun Pan, Yu Sun,and Quan Lu. 2018. Field-weighted factorization machines for click-throughrate prediction in display advertising. In Proceedings of the 2018 World Wide WebConference. 13491357.",
  "Michael Pulis and Josef Bajada. 2021. Siamese Neural Networks for Content-basedCold-Start Music Recommendation.. In Proceedings of the 15th ACM Conferenceon Recommender Systems. 719723": "Kandula Balagangadhar Reddy, Debabrata Swain, Samiksha Shukla, and LijaJacob. 2022. Prediction of Customer Lifetime Value Using Machine Learning. InProceedings of Second Doctoral Symposium on Computational Intelligence: DoSCI2021. Springer, 271278. Xiang-Rong Sheng, Liqin Zhao, Guorui Zhou, Xinyao Ding, Binding Dai, QiangLuo, Siran Yang, Jingshan Lv, Chi Zhang, Hongbo Deng, et al. 2021. One model toserve all: Star topology adaptive recommender for multi-domain ctr prediction. InProceedings of the 30th ACM International Conference on Information & KnowledgeManagement. 41044113.",
  "KDD 24, August 2529, 2024, Barcelona, SpainWang, Xu and Cheng, et al": "Xintong Shi, Wenzhi Cao, and Sebastian Raschka. 2023. Deep neural networksfor rank-consistent ordinal regression based on conditional probabilities. PatternAnalysis and Applications 26, 3 (2023), 941955. Pawel Swietojanski and Steve Renals. 2014. Learning hidden unit contributionsfor unsupervised speaker adaptation of neural network acoustic models. In 2014IEEE Spoken Language Technology Workshop (SLT). IEEE, 171176. Hongyan Tang, Junning Liu, Ming Zhao, and Xudong Gong. 2020. Progressivelayered extraction (ple): A novel multi-task learning (mtl) model for personalizedrecommendations. In Proceedings of the 14th ACM Conference on RecommenderSystems. 269278. Ali Vanderveld, Addhyan Pandey, Angela Han, and Rajesh Parekh. 2016. Anengagement-based customer lifetime value system for e-commerce. In Proceedingsof the 22nd ACM SIGKDD international conference on knowledge discovery anddata mining. 293302.",
  "VN Vapnik. 2000. The Nature of Statistical Learning Theory (Information Scienceand Statistics) Springer-Verlag. New York (2000)": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin. 2017. Attention is allyou need. Advances in neural information processing systems 30 (2017). Pablo Villalobos, Jaime Sevilla, Lennart Heim, Tamay Besiroglu, Marius Hobb-hahn, and Anson Ho. 2022. Will we run out of data? An analysis of the limits ofscaling datasets in Machine Learning. arXiv preprint arXiv:2211.04325 (2022). Hao Wang, Tai-Wei Chang, Tianqiao Liu, Jianmin Huang, Zhichao Chen, ChaoYu, Ruopeng Li, and Wei Chu. 2022. Escm2: Entire space counterfactual multi-task model for post-click conversion rate estimation. In Proceedings of the 45thInternational ACM SIGIR Conference on Research and Development in InformationRetrieval. 363372. Jindong Wang, Yiqiang Chen, Shuji Hao, Wenjie Feng, and Zhiqi Shen. 2017.Balanced distribution adaptation for transfer learning. In 2017 IEEE internationalconference on data mining (ICDM). IEEE, 11291134.",
  "Min Xiao and Yuhong Guo. 2014. Feature space independent semi-superviseddomain adaptation via kernel matching. IEEE transactions on pattern analysisand machine intelligence 37, 1 (2014), 5466": "Mingzhe Xing, Shuqing Bian, Wayne Xin Zhao, Zhen Xiao, Xinji Luo, CunxiangYin, Jing Cai, and Yancheng He. 2021. Learning Reliable User Representationsfrom Volatile and Sparse Data to Accurately Predict Customer Lifetime Value. InProceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & DataMining. 38063816. Xuejiao Yang, Binfeng Jia, Shuangyang Wang, and Shijie Zhang. 2023. FeatureMissing-aware Routing-and-Fusion Network for Customer Lifetime Value Predic-tion in Advertising. In Proceedings of the Sixteenth ACM International Conferenceon Web Search and Data Mining. 10301038.",
  "Yi Yao and Gianfranco Doretto. 2010. Boosting for transfer learning with multiplesources. In 2010 IEEE computer society conference on computer vision and patternrecognition. IEEE, 18551862": "Xianhua Zeng, Xinyu Wang, and Yicai Xie. 2023. Multiple Pseudo-SiameseNetwork with Supervised Contrast Learning for Medical Multi-modal Retrieval.ACM Transactions on Multimedia Computing, Communications and Applications(2023). Shijie Zhang, Xin Yan, Xuejiao Yang, Binfeng Jia, and Shuangyang Wang. 2023.Out of the Box Thinking: Improving Customer Lifetime Value Modelling viaExpert Routing and Game Whale Detection. In Proceedings of the 32nd ACMInternational Conference on Information and Knowledge Management. 32063215. Wei Zhang, Pengye Zhang, Bo Zhang, Xingxing Wang, and Dong Wang. 2023. ACollaborative Transfer Learning Framework for Cross-domain Recommendation.In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery andData Mining. 55765585. Jiejie Zhao, Bowen Du, Leilei Sun, Fuzhen Zhuang, Weifeng Lv, and Hui Xiong.2019. Multiple relational attention network for multi-task learning. In Proceedingsof the 25th ACM SIGKDD international conference on knowledge discovery & DataMining. 11231131."
}