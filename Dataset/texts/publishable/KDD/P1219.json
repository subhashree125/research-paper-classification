{
  "ABSTRACT": "Anomaly detection is an important field that aims to identify un-expected patterns or data points, and it is closely related to manyreal-world problems, particularly to applications in finance, man-ufacturing, cyber security, and so on. While anomaly detectionhas been studied extensively in various fields, detecting futureanomalies before they occur remains an unexplored territory. Inthis paper, we present a novel type of anomaly detection, calledPrecursor-of-Anomaly (PoA) detection. Unlike conventional anom-aly detection, which focuses on determining whether a given timeseries observation is an anomaly or not, PoA detection aims todetect future anomalies before they happen. To solve both prob-lems at the same time, we present a neural controlled differentialequation-based neural network and its multi-task learning algo-rithm. We conduct experiments using 17 baselines and 3 datasets,including regular and irregular time series, and demonstrate thatour presented method outperforms the baselines in almost all cases.Our ablation studies also indicate that the multitasking trainingmethod significantly enhances the overall performance for bothanomaly and PoA detection.",
  "This work was done when he was at Yonsei university": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 23, August 610, 2023, Long Beach, CA, USA. 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0103-0/23/08...$15.00",
  "data points and/or trends that may indicate errors, frauds, or otherabnormal situations requiring further investigations": "Novel task definition: Among many such studies, one of themost popular setting is multivariate time series anomaly detectionsince many real-world applications deal with time series, rangingfrom natural sciences, finance, cyber security, and so on. Irregulartime series anomaly detection is of utmost importancesince time series data is frequently irregular. Many existing timeseries anomaly detection designed for regular time series showsub-optimal outcomes when being applied to irregular time series.Irregular time series typically have complicated structures withuneven inter-arrival times.In addition, it is also important to forecast whether there will bean anomaly in the future given a time series input, which we callPrecursor-of-Anomaly (PoA) detection. The precursor-of-anomalydetection refers to the process of identifying current patterns/signsthat may indicate upcoming abnormal events. The goal is to de-tect these precursors before actual anomalies occur in order totake preventive actions (cf. ). The precursor-of-anomaly de-tection can be applied to various fields such as finance, medicalcare, and geophysics. For example, in finance, the precursor-of-anomaly detection can be used to identify unusual patterns in stockprices that may indicate a potential market collapse, and in geo-physics, the precursor-of-anomaly detection can detect unusualgeometric activities that can indicate earthquakes. Despite of itspractical usefulness, the precursor-of-anomaly detection has beenoverlooked for a while due to its challenging nature except for a",
  "KDD 23, August 610, 2023, Long Beach, CA, USA.Sheo Yon Jhin, Jaehoon Lee, & Noseong Park": "Institute of Information & Communications Technology Planning& Evaluation (IITP) grant funded by the Korean government (MSIT)(No. 2020-0-01361, Artificial Intelligence Graduate School Program(Yonsei University), and (No.2022-0-00857, Development of AI/data-based financial/economic digital twin platform,10%) and (No.2022-0-00113, Developing a Sustainable Collaborative Multi-modal Life-long Learning Framework, 45%),(2022-0-01032, Development ofCollective Collaboration Intelligence Framework for Internet ofAutonomous Things, 45%).",
  "RELATED WORK2.1Anomaly Detection in Time Series": "Anomaly detection in time series data has been a popular researcharea in the fields of statistics, machine learning and data mining.Over the years, various techniques have been proposed to identifyanomaly patterns in time series data, including machine learningalgorithms, and deep learning models.Machine learning algorithms and deep learning-based anomalydetection methods in time series can be categorized into 4 methodsincluding i) classical methods, ii) clustering-based methods, iii)density-estimation methods, iv) reconstruction-based methods.Classical methods find unusual samples using traditional ma-chine learning methods such as OCSVM and Isolation Forest .Clustering-based methods are a type of unsupervised machine learn-ing method for detecting anomalies in time-series data such as DeepSVDD, ITAD, and THOC . This method splits the datainto multiple clusters based on the similarity between the normaldata points and then identifies abnormal data points that do notbelong to any clusters. However, clustering-based methods are notsuitable for complex data to train. Density-estimation-based meth-ods are a type of unsupervised machine learning methodfor detecting anomalies in time series data such as LOF. In density-estimation based anomaly detection, the time-series data is trans-formed into a feature space, and the probability density function ofthe normal data points is estimated using techniques such as kerneldensity estimation or gaussian mixture models. The data points are",
  ": The architecture of NCDEs": "then ranked based on their density, and anomalies are identifiedas data points that have low density compared to the normal datapoints. However, this method may not perform well in cases wherethe data contains significant non-stationary patterns or where theanomalies are not well represented by the estimated density func-tion. Additionally, this method can also be computationally expen-sive, as it requires estimating the density function for the entiredata. Reconstruction-based methods are a representa-tive methodology for detecting anomalies in time series data suchas USAD . In this method, a deep learning model reconstructsnormal time-series data and then identifies abnormal time-seriesdata with high reconstruction errors. Also, reconstruction-basedmethods can be effective in detecting anomalies in time-series data,especially when the anomalies are not well separated from thenormal data points and the data contains complex patterns. Thereconstruction-based methods are also robust to missing values andnoisy data, as the reconstructed data can be used to fill in missingvalues and reduce the impact of noise. However, this method canbe sensitive to the choice of reconstruction model, and may notperform well in cases where the anomalies are not well representedby the reconstruction model. Additionally, this method may becomputationally expensive, as it requires training a complex modelon the data.In this paper, we mainly focus on self-supervised learning-basedanomaly detection. Anomaly detection methods based on self-supervised learning have been extensively studied .Unlike the unsupervised-learning method, self-supervised learninglearns using negative samples generated by applying data augmen-tation to the training data . Data augmentation can overcomethe limitations of existing training data. In this paper, we train ourmethod, PAD, on time series dataset generated by data augmenta-tion.",
  "Neural Controlled Differential Equations": "Recently, differential equation-based deep learning models havebeen actively researched. Neural controlled differential equations(NCDEs) are a type of machine learning method for modeling andforecasting time-series data. NCDE combines the benefits of neural networks and differential equations to create a flexible and power-ful model for time-series data. Unlike traditional time series models(e.g., RNNs), differential equation-based neural networks estimatethe continuous dynamics of hidden vectors z(t) (i.e., z(t)",
  ",(3)": "where () is a continuous time series path interpolated from{(x, t)}=0. With interpolation methods, we can obtain a con-tinuous time series path from discrete observations {(x, t)}=0.Typically, natural cubic spline is used as the interpolationmethod note that the adaptive step size solver works properly ifthe path () is twice continuously differentiable .The main difference between NODEs and NCDEs is the existenceof the continuous time series path (). NCDEs process a hiddenvector z() along the interpolated path (). (cf. ) In thisregard, NCDEs can be regarded as a continuous analogue to RNNs.Because there exists (), NCDEs can learn what NODEs cannot(See Theorem C.1 in ). Additionally, NCDEs have the ability toincorporate domain-specific knowledge into the model, by usingexpert features as inputs to the neural networks. This can improvethe performance and robustness of the model, as well as provide adeeper understanding of the relationships between the variablesin the data. Therefore, NCDEs enable us to model more complexpatterns in continuous time series.",
  "Knowledge Distillation": "Knowledge distillation is a technique used to transfer knowl-edge from large, complex models to smaller, simpler models. In thecontext of time series data, knowledge distillation can be used totransfer knowledge from complex deep learning models to smaller,more efficient models. The knowledge extraction process involvestraining complex models such as deep neural networks on largedatasets of time series data. The complex model is then used to gen-erate predictions for the same data set. These predictions are thenused to train simpler models such as linear regression models ordecision trees. In this paper, we use these predictions in precursor-of-anomaly detection. During the training process, simpler modelsare optimized to minimize the difference between their predictionsand those of complex models. This allows simple models to learnpatterns and relationships in time series data captured by complexmodels. Furthermore, knowledge distillation can also help improvethe interpretability of the model. The simpler model can be easierto understand and analyze, and the knowledge transferred from",
  "Multi-task Learning": "Multi-task learning (MTL) is a framework for learning multipletasks jointly with shared parameters, rather than learning themindependently . By training multiple tasks simultaneously, MTLcan take advantage of shared structures between tasks to improvethe generalization performance of each individual task. Severaldifferent MTL architectures have been proposed in the literature,including hard parameter sharing, soft parameter sharing, and task-specific parameter sharing . Hard parameter sharing involvesusing a shared set of parameters for all tasks, while soft parame-ter sharing allows for different parameter sets for each task, butencourages them to be similar. Task-specific parameter sharinghas their own set of task-specific parameters for each task, andlower-level parameters are shared between tasks. This approachallows for greater task-specific adaptation while still leveragingshared information at lower levels.One of the major challenges of MTL is balancing task-specific andshared representation. Using more shared representations acrosstasks yields better generalization and performance across all tasks,but at the expense of task-specific information. Therefore, thechoice of MTL architecture and the amount of information sharedbetween tasks depends on the specific problem and task relationship.In this paper, we train our model with a task-specific parametersharing architecture considering shared information and task rela-tionships. MTL is applied to various areas such as natural languageprocessing, computer vision. In natural language processing, MTLhas been used for tasks such as named entity recognition, part-of-speech tagging, and sentiment analysis . In computer vision,MTL has been used for tasks such as object recognition, objectdetection, and semantic segmentation .These architectures have demonstrated promising results onseveral benchmark datasets, and in this paper, we conduct researchon time-series anomaly detection using MTL, showing excellentperformance for all 17 baselines.",
  "Problem Statement": "In this section, we define the anomaly detection and the precursor-of-anomaly detection tasks in irregular time series settings thoseproblems in the regular time series setting can be similarly defined.In our study, we focus on multivariate time series, defined asx0: = {x0, x1, ..., x }, where is the time-length. An observationat time , denoted x, is an -dimensional vector, i.e., x R . Forirregular time-series, the time-interval between two consecutive",
  "are": "windows in total for x0: . Each window is individuallytaken as an input to our network for the anomaly or the precursor-of-anomaly detection.Given an input window w, for the anomaly detection, our neuralnetwork decides whether w contains abnormal observations ornot, i.e., a binary classification of anomaly vs. normal. On the otherhand, it is determined whether the very next window w+1 is likelyto contain abnormal observations for the precursor-of-anomalydetection, i.e., yet another binary classification.",
  "Neural Network Architecture based onCo-evolving NCDEs": "We describe our proposed method based on dual co-evolving NCDEs:one for the anomaly detection and the other for the precursor-of-anomaly (PoA) detection. Given a discrete time series sample x1: ,we create a continuous path () using an interpolation method,which is a pre-processing step of NCDEs. After that, the followingco-evolving NCDEs are used to derive the two hidden vectors h()and z():",
  "(6)": "where means the rectified linear unit (ReLU) and means thehyperbolic tangent.Therefore, our proposed architecture falls into the category oftask-specific parameter sharing of the multi-task learning paradigm.We perform the anomaly detection task and the PoA detection taskswith h() and z(), respectively. We ensure that the two NCDEsco-evolve by using the shared parameters that allow them toinfluence each other during the training process. Although thosetwo tasks goals are ultimately different, those two tasks sharecommon characteristics to some degree, i.e., capturing key patternsfrom time series. By controlling the sizes of , , and , we cancontrol the degree of their harmonization. After that, we have thefollowing two output layers:",
  ".(10)": "Well-posedness of the problem: The well-posedness of theinitial value problem of NCDEs was proved in previous work, suchas under the condition of Lipschitz continuity, which meansthat the optimal form of the last hidden state at time is uniquelydefined given an training objective. Our method, PAD, also has thisproperty, as almost all activation functions (e.g. ReLU, Leaky ReLU,Sigmoid, ArcTan, and Softsign) have a Lipschitz constant of 1 .Other neural network layers, such as dropout, batch normalization,and pooling methods, also have explicit Lipschitz constant values.Therefore, Lipschitz continuity can be fulfilled in our case.",
  "where denotes the ground-truth of anomaly detection. As shownin Eq. (11), we use the cross entropy (CE) loss between +1 and+1 to distill the knowledge of the anomaly NCDE into the PoANCDE": "Training with the adjoint method: We train our method usingthe adjoint sensitivity method , which requires amemory of O( + ) where is the integral time domain and isthe size of the NCDEs vector field. This method is used in Lines 4to 5 of Alg. 1. However, our framework uses two NCDEs, whichincreases the required memory to O(2 + + ), where and are the sizes of the vector fields for the two NCDEs, respectively.To train them, we need to calculate the gradients of each lossw.r.t. the parameters , , and . In this paragraph, we describehow we can space-efficiently calculate them. The gradient to traineach parameter can be defined as follows:",
  "Datasets": "Mars Science Laboratory: The mars science laboratory (MSL)dataset is also from NASA, which was collected by a spacecraften route to Mars. This dataset is a publicly available dataset fromNASA-designated data centers. It is one of the most widely useddataset for the anomaly detection research due to the clear distinc-tion between pre and post-anomaly recovery. It is comprised of thehealth check-up data of the instruments during the journey. Thisdataset is a multivariate time series dataset, and it has 55 dimensionswith an anomaly ratio of approximately 10.72% . Secure Water Treatment: The secure water treatment (SWaT)dataset is a reduced representation of a real industrial water treat-ment plant that produces filtered water. This data set containsimportant information about effective measures that can be im-plemented to avoid or mitigate cyberattacks on water treatmentfacilities. The data set was collected for a total of 11 days, withthe first 7 days collected under normal operating conditions andthe subsequent 4 days collected under simulated attack scenarios.SWaT has 51 different values in an observation and an anomalyratio of approximately 11.98% . Water Distribution: The water distribution (WADI) data set iscompiled from the WADI testbed, an extension of the SWaT testbed.It is measured over 16 days, of which 14 days are measured in thenormal state and 2 days are collected in the state of the attackscenario. WADI has 123 different values in an observation and ananomaly ratio of approximately 5.99% .",
  "Hyperparameters. We list all the detailed hyperparametersetting for baselines and our method in Appendix.For reproducibility, we report the following best hyperparame-ters for our method:": "(1) In MSL, we train for 300 epochs, a learning rate of 1.0 2,a weight decay of 1.0 4, and the hidden size of ,,and is 256, 512, 256, respectively. Among 256 windows, wedetect the window in which abnormal data points exist. Thelength of each window was set to 30, and the length of thepredicted window in precursor anomaly detection was setto 10. (2) In SWaT, we train for 300 epochs, a learning rate of 1.02,a weight decay of 1.0 4, and the hidden size of ,,and is 128, 64, 64, respectively. Among 256 windows, wedetect the window in which abnormal data points exist. Thelength of each window was set to 60, and the length of thepredicted window in precursor anomaly detection was setto 20. (3) In WADI, we train for 300 epochs, a learning rate of 1.02,a weight decay of 1.0 5, and the hidden size of ,,and is 128, 128, 256, respectively. Among 256 windows, wedetect the window in which abnormal data points exist. Thelength of each window was set to 100, and the length of thepredicted window in precursor anomaly detection was setto 30. 4.2.2Baselines. We list all the detailed description about baselinesin Appendix. We compare our model with the following 16 baselinesof 4 categories, including not only traditional methods but alsostate-of-the-art deep learning-based models as follows:",
  "Experimental Results on AnomalyDetection": "We introduce our experimental results for the anomaly detectionwith the following 3 datasets: MSL, SWaT, and WADI. Evaluatingthe performance with these datasets proves the competence of ourmodel in various fields. We use the Precision, Recall, and F1-score. 4.3.1Experimental Results on Regular Time Series. summa-rizes the results on the three datasets. The anomaly detection withMSL is one of the most widely used benchmark experiments. Ourmethod, PAD, shows the best F1-score. For this dataset, all classicalmethods are inferior to other baselines. For SWaT, our experimentalresults are in . As summarized, the classical methods areinferior to other baselines. However, unlike in WADI, all baselinesexcept them show similar results. Our method, PAD, shows thebest F1-score. For WADI, among the reconstruction-based methods,InterFusion shows the second-best performance. Since this datasethas the smallest anomaly ratio among the three datasets, classicalmethods and clustering-based methods are not suitable.",
  "PAD (Anomaly)95.6995.4993.4491.2193.0692.1090.8589.4190.12": "4.3.2Experimental Results on Irregular Time Series. sum-marizes the results on irregular time series. In order to create chal-lenging irregular environments, we randomly remove 30%, 50%and 70% of the observations in each sequence. Therefore, this isbasically an irregular time series anomaly detection. We compareour method, PAD, with the 4 baselines, Isolation Forest, LOF, USAD,and Anomaly Transformer other baselines are not defined forirregular time series. In addition, it is expected that the presenceof more than 30% of missing values in time series causes poor per-formance for many baselines because it is difficult to understand the highly missing input sequence. In MSL, USAD and AnomalyTransformer shows the reasonable results and also maintains an F1-score around 80% across all the dropping ratio settings. For WADI,all baselines show poor performance when the missing rate is 70%.Surprisingly, our method, PAD, performs not significantly differ-ently from the regular anomaly detection experiments. Our methodmaintains good performance in the irregular time series settingas well because PAD uses a hidden representation controlled bythe continuous path () at every time . Additionally, our methodmaintains an F1 score larger than 90% for all the dropping ratios.",
  "Experimental Results on theprecursor-of-Anomaly Detection": "In Table. 3, we introduce our experimental results for the precursor-of-anomaly detection. In order to compare the performance of ourmethod in the precursor-of-anomaly detection task newly proposedin this paper, among the baselines performed in regular time seriesanomaly detection (cf. ), we selected reconstruction-basedmethods that allow PoA experimental setting. Therefore, we selectthe 3 baselines (LSTM, LSTM-VAE, and USAD) that showed goodperformance in reconstruction-based methods. 4.4.1Experimental Results on Regular Time Series. As shown in Ta-ble 3, USAD shows reasonable performance among the 3 baselines.Especially, in MSL dataset, USAD shows a similar performance to. Our newly proposed the precursor-of-anomaly task re-quires predicting patterns or features of future data from input data.Therefore, the reconstruction-based method seems to have showngood performance. However, our method, PAD, shows the bestperformance in all the 3 datasets. shows the visualization ofexperimental results on the anomaly detection and the precursor-of-anomaly detection on all the 3 datasets. In , the part highlightedin purple is the ground truth of the anomalies, the part highlightedin red is the result of PoA detected by PAD. As shown in , ourmethod correctly predicts the precursor-of-anomalies (highlightedin red) before the abnormal parts (highlighted in purple) occur. 4.4.2Experimental Results on Irregular Time Series. showsthe experimental result on the irregular datasets. Among the base-lines, USAD has many differences in experimental results depend-ing on the experimental environment. For example, in the WADIdataset, which has a small anomaly ratio(5.99%) among the otherdatasets, it shows poor performance, and in the MSL data set, USAD",
  ": The anomaly detection and the precursor-of-anomaly detection results on 3 datasets": "shows the second-best performance, but the performance deterio-rates as the dropping ratio increases. However, our method, PAD,clearly shows the best F1-score for all dropping ratios and all 3datasets. One outstanding point in our model is that the F1-score isnot greatly influenced by the dropping ratio. Consequently, all theseresults prove that our model shows state-of-the-art performance inboth the anomaly and the precursor-of-anomaly detection.",
  "ABLATION AND SENSITIVITY STUDIES5.1Ablation Study on Multi-task Learning": "To prove the efficacy of our multi-task learning on the anomaly andthe precursor-of-anomaly detection, we conduct ablation studies.There are 2 tasks in our multi-task learning: the anomaly detection,and the precursor-of-anomaly detection tasks. We remove one taskto build an ablation model. For the ablation study on anomaly detec-tion, there are 2 ablation models in terms of the multi-task learningsetting: i) without the precursor-of-anomaly detection, and ii) withanomaly detection only. For the ablation study on the precursor-of-anomaly detection, 2 ablation models are defined in the exactlysame way. and show the results of the ablationstudies in the regular time series setting for PAD (anomaly) andPAD (PoA), respectively. When we remove task(anomaly detectionor PoA detection) from the multi-task learning, there is notabledegradation in performance. Therefore, our multi-task learningdesign is required for good performance in both the anomaly andthe precursor-of-anomaly detection.",
  "Sensitivity to Output Sequence Length": "We also compare our method with USAD, OmniAnomaly, andTHOC by varying the length of output of the precursor-of-anomalydetection, during the multi-task learning process. After fixing theinput length of MSL, SWaT, and WADI to 30, we vary the outputlength in {1, 5, 10, 15, 20}. As shown in , our proposed methodconsistently outperforms others. As the output length increases, itbecomes more difficult to predict in general, but PAD shows excel-lent performance regardless of the output length. In the MSL andWADI datasets, most baselines show similar performances regard-less of the output length. However, in SWaT, there is a performancedifference according to the output length, and this phenomenonappears similarly for the baselines.",
  "CONCLUSION": "Recently, many studies have been conducted on time series anom-aly detection. However, most of the methods have been conductedonly for existing anomaly detection methods. In this paper, we firstpropose a task called the precursor-of-anomaly (PoA) detection.We define PoA detection as the task of predicting future anomalydetection. This study is necessary in that many risks can be mini-mized by detecting risks in advance in the real world. In addition,we combine multi-task-learning and NCDE architecture to performboth anomaly detection, and PoA detection and achieve the bestperformance through task-specific-parameter sharing. Addition-ally, we propose a novel dual co-evolving NCDE structure. TwoNCDEs perform anomaly detection and PoA detection tasks. Ourexperiments with the 3 real-world datasets and 17 baseline methodssuccessfully prove the efficacy of the proposed concept. In addition,our visualization of anomaly detection results delivers how ourproposed method works. In the future work, we plan to conductunsupervised precursor-of-anomaly detection research since thetime series data augmentation method requires a pre-processingstep.",
  "Oliver D. Anderson and M. G. Kendall. 1976. Time-Series. 2nd edn. The Statistician25 (1976), 308": "Julien Audibert, Pietro Michiardi, Frdric Guyard, Sbastien Marti, and Maria AZuluaga. 2020. Usad: Unsupervised anomaly detection on multivariate time series.In Proceedings of the 26th ACM SIGKDD International Conference on KnowledgeDiscovery & Data Mining. 33953404. Emel Ay, Maxime Devanne, Jonathan Weber, and Germain Forestier. 2022. Astudy of knowledge distillation in fully convolutional network for time seriesclassification. In 2022 International Joint Conference on Neural Networks (IJCNN).IEEE, 18. Anshuman Bhardwaj, Shaktiman Singh, Lydia Sam, Pawan K Joshi, AkankshaBhardwaj, F Javier Martn-Torres, and Rajesh Kumar. 2017. A review on re-motely sensed land surface temperature anomaly as an earthquake precursor.International journal of applied earth observation and geoinformation 63 (2017),158166. Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and Jrg Sander. 2000.LOF: identifying density-based local outliers. In Proceedings of the 2000 ACMSIGMOD international conference on Management of data. 93104. Chris U. Carmona, Franois-Xavier Aubet, Valentin Flunkert, and Jan Gasthaus.2022. Neural Contextual Anomaly Detection for Time Series. In Proceedings ofthe Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22,Lud De Raedt (Ed.). International Joint Conferences on Artificial IntelligenceOrganization, 28432851. Main Track.",
  "Lifeng Shen, Zhuocong Li, and James Kwok. 2020. Timeseries anomaly detectionusing temporal hierarchical one-class network. Advances in Neural InformationProcessing Systems 33 (2020), 1301613026": "Youjin Shin, Sangyup Lee, Shahroz Tariq, Myeong Shin Lee, Okchul Jung, DaewonChung, and Simon S Woo. 2020. Itad: integrative tensor-based anomaly detectionsystem for reducing false positives of satellite systems. In Proceedings of the29th ACM international conference on information & knowledge management.27332740. Ya Su, Youjian Zhao, Chenhao Niu, Rong Liu, Wei Sun, and Dan Pei. 2019. Robustanomaly detection for multivariate time series through stochastic recurrentneural network. In Proceedings of the 25th ACM SIGKDD international conferenceon knowledge discovery & data mining. 28282837.",
  "preprint arXiv:2110.02642 (2021)": "Qing Xu, Zhenghua Chen, Mohamed Ragab, Chao Wang, Min Wu, and Xiaoli Li.2022. Contrastive adversarial knowledge distillation for deep model compressionin time-series regression tasks. Neurocomputing 485 (2022), 242251. Takehisa Yairi, Naoya Takeishi, Tetsuo Oda, Yuta Nakajima, Naoki Nishimura,and Noboru Takata. 2017. A data-driven health monitoring method for satellitehousekeeping data based on probabilistic clustering and dimensionality reduction.IEEE Trans. Aerospace Electron. Systems 53, 3 (2017), 13841401. Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, andYoungjoon Yoo. 2019. Cutmix: Regularization strategy to train strong classifierswith localizable features. In Proceedings of the IEEE/CVF international conferenceon computer vision. 60236032.",
  "(2) In SWaT, we train for 100 epochs, a learning rate of {1.0 2, 1.0 3, 1.0 4}, a weight decay of {1.0 3, 1 4, 1 5}, and a size of hidden vector size is {39, 49, 59}": "(3) In WADI, we train for 100 epochs, a learning rate of {1.0 2, 1.0 3, 1.0 4}, a weight decay of {1.0 3, 1.0 4, 1.05}, and a size of hidden vector size is {29, 39, 49, 59}.In to 15, we clarify the network architecture of the CDEfunctions, ,, and .",
  "(FC)1256 16256 256(FC)2256 256256 1, 968": "(4) Reconstruction-based methods: For LSTM, we use a learningrate of {1.0 3, 1.0 4} and a hidden vector dimen-sion of {80, 100, 120}. For LSTM-VAE, BeatGAN, and Omni-Anomaly, we use a learning rate of {1.0 4, 1.0 5}and a hidden vector dimension of {32, 64, 128}. For USAD,we use a learning rate of {1.0 3, 1.0 4} and a hiddenvector dimension of {32, 64, 128}, and follow other defaulthyperparameters in USAD. For InterFusion, a hidden vectordimension of {128, 256, 512}, and follow other default hyper-parameters in InterFusion. For Anomaly Transformer, wefollow all hyperparameters in Anomaly Transformer.",
  "(2) Secure Water Treatment: SWaT is licensed under the follow-ing license:": "(3) Water Distribution: WADI is a public dataset from NASA.WADI is licensed under the following license: our method resorts to a self-supervised multi-task learningapproach, we augmented training samples with abnormal patternsand its detailed process is in Alg. ??. The augmentation method issimilar to other popular augmentation methods for images, e.g., Cut-Mix . There is a long training sequence x0: = {x0, x1, ..., x }.We apply the augmentation method to the raw sequence x0: beforesegmenting it into windows. We randomly copy existing obser-vations from a random location to a target location . In general,the ground-truth anomaly pattern is unknown in each dataset. Al-though our copy-and-paste augmentation method is simple, ourexperimental results prove its effectiveness. At the same time, wealso believe that there will be better augmentation methods."
}