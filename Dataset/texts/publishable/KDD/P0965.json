{
  "Abstract": "The traditional evaluation of information retrieval (IR) systems isgenerally very costly as it requires manual relevance annotationfrom human experts. Recent advancements in generative artificialintelligence specifically large language models (LLMs) can gen-erate relevance annotations at an enormous scale with relativelysmall computational costs. Potentially, this could alleviate the coststraditionally associated with IR evaluation and make it applicable tonumerous low-resource applications. However, generated relevanceannotations are not immune to (systematic) errors, and as a result,directly using them for evaluation produces unreliable results.In this work, we propose two methods based on prediction-powered inference and conformal risk control that utilize computer-generated relevance annotations to place reliable confidence in-tervals (CIs) around IR evaluation metrics. Our proposed methodsrequire a small number of reliable annotations from which the meth-ods can statistically analyze the errors in the generated annotations.Using this information, we can place CIs around evaluation metricswith strong theoretical guarantees. Unlike existing approaches, ourconformal risk control method is specifically designed for rank-ing metrics and can vary its CIs per query and document. Ourexperimental results show that our CIs accurately capture both thevariance and bias in evaluation based on LLM annotations, betterthan the typical empirical bootstrapping estimates. We hope ourcontributions bring reliable evaluation to the many IR applicationswhere this was traditionally infeasible.",
  "Authors contributed equally to this work.Now at Google DeepMind.Work done while Harrie Oosterhuis was working at Google Research": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for third-party components of this work must be honored.For all other uses, contact the owner/author(s).KDD 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s).ACM ISBN 979-8-4007-0490-1/24/08",
  "Introduction": "The evaluation of information retrieval (IR) systems is an impor-tant and long-established part of the IR field . The goalof standard IR systems is to retrieve and rank documents accord-ing to their relevance to a query and user. Accordingly, standardIR evaluation metrics (e.g., precision, recall, discounted cumula-tive gain (DCG), etc.) measure how relevant the top ranked itemsare for a set of known queries . Accordingly, tradi-tional evaluation requires a dataset with examples of documents,queries and annotations that indicate the relevance of documentsto queries . Whilst documents and queries are oftengathered by logging user interactions, relevance annotations aretraditionally created through the labour of human experts, who aretrained for the specific labelling task . Consequently,creating a new dataset for IR evaluation purposes is generally verycostly, and as a result, no large datasets have been created for manyIR settings . Thus, for these low-resource settingstraditional evaluation is not available in practice.Despite the large costs involved, there has been a continuouseffort, often driven by initiatives like TREC and CLEF, to createnew datasets for different IR tasks . Since the foundational Cranfield collection , manydatasets have been created for ad-hoc retrieval .However, to match the large variety of IR-related tasks, many otherdatasets were subsequently introduced, accordingly; For example,datasets with numerical IR features for learning-to-rank ,or large collections of natural language question-answering exam-ples such as MS MARCO and BioASQ . Similarly, recentyears have seen the introduction of the TREC-DL , BEIR",
  "KDD 24, August 2529, 2024, Barcelona, SpainHarrie Oosterhuis, Rolf Jagerman, Zhen Qin, Xuanhui Wang, and Michael Bendersky": "the labelled data . If the predictions are found to be accurate onthe labelled data, then this increases our confidence that its predic-tions on unlabelled data are also accurate. As predictions becomeavailable in much larger quantities, this can increase our confidencefurther in the overall estimate. To the best of our knowledge, weare the first to apply PPI to IR evaluation.",
  "Related Work2.1Confidence intervals for IR evaluation": "Evaluation is a well-established core part of the IR field . Generally, it aims to measure how well a retrieval sys-tem can produce a list of ranked documents in response to a userquery . The most prevalent form of IR evaluation relieson datasets containing example queries, documents and human-annotated relevance labels . Accordingly, there isa long history of efforts to create such datasets in the IR commu-nity, such as TREC , CLEF , NTCIR andmany others . Despite the enor-mous importance of these datasets, they are known to have limi-tations. For instance, expert annotators can give conflicting rele-vance assessments, and the actual users of an IR application candisagree with the experts as well . Furthermore, the construc-tions of these datasets is often costly which puts constraints ontheir size . As a result, IR datasets can only representa limited slice of the queries that a real IR system receives .Accordingly, statistical approaches to IR evaluation have beendeveloped to deal with these limitations. For example, it has be-come common practice to use significance tests to ensure thatobserved differences in IR metrics are, with high probability, notthe result of random chance . Confidence intervals (CIs)have been used to express the uncertainty that comes from us-ing the dataset sample of queries to estimate performance over allqueries . Furthermore, previous work has also applied CI forrelevance annotator disagreement and missing relevanceannotations . The statistical methods used to construct CIby previous work in IR have been based on empirical bootstrappingtechniques . To the best of our knowledge, our work isthe first to consider PPI and CRC methods for IR evaluation .",
  "LLMs for relevance annotation generation": "Recent advances in LLMs have demonstrated impressive capabil-ities on a broad range of tasks . Previous work hasspecifically considered using LLMs for relevance annotation in anIR context . Thomas et al. propose using groundtruth relevance labels from human annotators, to find a prompt thatresults in the most accurate LLM generated labels. They claim thatthis method produces relevance annotations at the same qualityas third-party human assessors but at a fraction of the costs .Clarke et al. propose that LLM relevance-annotation shouldbe approached as a spectrum, since the involvement of humans canbe varied. For instance, one could delegate most work to an LLMbut add some human verification, as a compromise of reduced costsand reliability. Faggioli et al. support this approach, as they seesevere risk in blindly following LLM generated relevance labels (atleast for the current state-of-the-art LLMs). The danger foreseen byboth is that generated labels can make systematic errors that leadto incorrect and unreliable evaluation of IR systems . Ourwork addresses this problem, and is thus very related; specifically,our contribution can be seen as an approach of human verificationdesigned to quantify uncertainty stemming from LLM usage.",
  "Preliminaries3.1Evaluation metrics for retrieval systems": "The general approach to the evaluation of a retrieval system is toconsider the expected value of a ranking metric across the queriesit will receive . Standard ranking metrics assume that eachdocument has certain relevance to a query . For a set of labelsR, we use ( = | ,) to denote the probability that a humanrater would give rating R, to the combination of document andquery . We define relevance as the expected rating value over thisdistribution: ( | ) = R ( = | ,) . In standard rankingsettings, the goal is to place more relevant documents at higherranks . Ranking metrics capture this goal by giving a weight toeach rank, which indicates how much the relevance of a documentplaced at that rank should contribute to the metric . We willuse to denote our weighting function which takes the rank of adocument as its input. For example, Precision@K has the followingcorresponding weight function: Prec@K() = 1",
  "Problem setting": "In our setting, we make the standard assumption that a large setof sampled user queries and a document collection are available.However, we do not assume that there are human relevance anno-tations for every document-query pair, and instead, we assume thatground truth annotations are only available for a small subset: thefirst queries out of a total of queries. Unique to our problemsetting is that a generative model is available to predict relevanceannotations. Furthermore, our aim is not to give a point estimate ofthe true performance of a system, instead our goal is to construct areliable CI around the true value of an IR metric. Thereby, we utilizethe generated relevance annotations, but still explicitly indicate theresulting uncertainty in our evaluation with CIs.In formal terms, let be a confidence parameter, wedesire to find a lower bound low and an upper bound high s.t:",
  "D(rank( | , D)) ( | ).(6)": "As discussed in previous work , basing () on state-of-the-art LLMs could greatly reduce costs , but there are many risksinvolved in replacing human annotators . The accuracy of ()completely depends on the predictive capabilities of the generativemodel. Thus, without further knowledge about the reliability ofthe predictions, one has no indication of its trustworthiness. Ourproposed methodologies use the available ground truth query-level performances together with the many generated relevancepredictions to construct reliable CIs that quantify these risks.",
  "Method 1: Prediction-Powered Inference forInformation Retrieval Evaluation": "Our first proposed method applies the prediction-powered infer-ence (PPI) framework to IR evaluation. PPI is a very recent advance-ment in CI construction introduced by Angelopoulos et al. . Itutilizes computer-generated predictions to create smaller CI whenthese predictions are somewhat accurate. The core idea is to avoidestimating a variable on labelled data directly, and instead, build anestimate around the predictions which is then corrected based on",
  "Classical empirical mean estimation": "Before we detail our application of PPI, it is easiest to start withclassical empirical estimates. As stated in .2, our aim is toplace a reliable CI around the true performance (), and relevanceannotations are available for the first queries in . Therefore, wecan make an empirical estimate of the mean metric performancebased on these queries:",
  ", (7)": "where 2emp is the estimated variance of the empirical estimate, and () the metric value for the single query (Eq. 1). We note thatits variance is solely reflective of the ground truth data. Obviously,this estimate does not fully utilize our problems setting, as it ignoresthe queries without ground truth relevance annotations and theircorresponding computer-generated relevance annotations.",
  "more accurate, whilst 2pred shrinks as more unlabelled data be-": "comes available (as increases). Comparing 2PPI with 2emp revealsthat PPI can give a lower variance estimate, but only if predictionsare somewhat accurate. Conversely, when they are inaccurate thevariance could actually be greater.Finally, in order to turn the estimated mean and variance into aCI, we follow Angelopoulos et al. and assume PPI() followsa normal distribution. The 95% confidence interval is then:",
  "+2pred.(11)": "Accordingly, one can use a different z-score than 1.96 to choose adifferent level of confidence. We note that this implicitly assumesthe prediction error follows a symmetric distribution.This concludes our description of our PPI method. Its biggestadvantage is its simplicity and straightforward application, makingit attractive for practical usage. A limitation is that it only gives aCI of the overall performance (dataset-level). Therefore, PPI cannotbe used to place CI around individual query performances, andsimilarly, it cannot vary its confidence for different queries.",
  "Conformal prediction": "Conformal prediction provides a unique approach to uncertaintyquantification in predictions . The key characteristic of con-formal prediction is that its predictions are not individual labels butsets of labels. For instance, the most basic version of this approachconstructs a prediction set C by including all labels that have a pre-dicted probability above a threshold . Let indicate apredicted probability, contextual features and a correspondinglabel, this basic prediction set is then:",
  "Conformal risk control": "For purposes other than label prediction, there is a more generalversion of this approach: conformal risk control (CRC) . Let C( |) be an arbitrary function that constructs sets that increase with, L a bounded loss function that shrinks as C grows, and in thiscontext R, CRC aims to guarantee the expected loss is bounded:",
  ",(15)": "where is the maximum possible value of L. Under the assumptionthat the calibration data was sampled from the same distribution((,)), CRC is proven to provide the bound guarantee stated inEq. 14 . We note that it is possible that no value exists thatcan satisfy Eq. 15 because the number of data-points is too small.In this case, the method explicitly fails to provide a CI, thereby, CRCindicates when it is unable to guarantee reliable CIs. The generalityand flexibility of the CRC framework enables us to build our ownCI method for IR evaluation on top of it.",
  "Method 2: Conformal Risk Control forInformation Retrieval Evaluation": "Our second proposed method uses conformal risk control (CRC) forCI construction . In contrast with PPI, it can provide both CIaround mean performance and per query performance. It also relieson different assumptions than PPI and empirical bootstrapping.Our description of the method is divided into three parts: firstly,we introduce our C function, secondly, we describe how calibra-tion data is gathered, and thirdly, we propose our alternative dual-calibration approach specific for CIs.",
  "Optimistic and pessimistic estimation": "For our purposes, C will construct CIs for the relevance of eachindividual document, that are then translated into CIs on query anddataset-level performance. Thus, our CRC method treats each CI asa set that includes all values between its minimum and maximum.Accordingly, we must predict the boundaries of CIs on a document-level, therefore, we propose two functions: high and low, thatprovide more optimistic and pessimistic predictions than , respec-tively. We wish the optimism/pessimism to follow the confidenceof the generative model, thus, we take the predicted distribution and remove probability from the top or bottom labels:1",
  "RCRC( = | , ).(19)": "visualizes how CRC varies over different values forthree different predicted relevance distributions. We see that lowpredicted probabilities for the largest labels mean that has to begreater for CRC to reach high values, and vice versa, has to belower for low probabilities for the lowest label values to reach lowvalues. In other words, it takes more extreme values for CRC tobe heavily optimistic when the generative model is very confidentlypessimistic, and vice versa.The document-level CRC are translated to performance esti-mates following Eq. 1 & 6 but with () replaced by CRC(, ).Finally, to construct CIs, we use two parameters: high (1, 1)and low (1, 1), s.t. low < high, to obtain CRC(, low) andCRC(, high). The predicted CI is the range between the per-turbed estimates:",
  "Data sampling and bootstrapping": "In order to perform CRC calibration, a set of ground truth examplesis required to serve as calibration data. In our setting, we aim toestimate the mean over the true query-distribution Q based on thesampled set of queries . Accordingly, a set of examples of meanestimates based on sampled set from Q is required; we create examples by sampling from the queries in with ground truthannotations: {1,2, . . .}. The collection of these setsshould mimic the distribution of Q: Q = { 1, 2, . . . , }.There are many options to construct Q, for instance, one couldsample queries with or without replacement, the size of the sampledsets could be varied, etc. Moreover, if one wants to create CIs aroundthe performance of each query, they can choose the sets to containa single query: = {}. Another option is to sample queries andsubsets of the document to be ranked, to artificially increase thevariety in candidate documents available per query. Choices thatincrease the number of examples have the potential to decreaseCI width. However, if the resulting Q is no longer representativeof the true distribution Q, the reliability of the CIs will decrease.",
  "Dual-calibration for confidence intervals": "With our definition of C(, high, low) and the calibration data Q,all that remains is to calibrate high and low. However, standardCRC is designed for the calibration of a single parameter. Luckily,for the purpose of construction a CI, we can apply CRC calibrationsequentially. Because for any low < high, the following holds:( low)",
  "Overview": "Finally, we give an overview of the different components in ourCRC approach: Our CI are created with the CCRC(, high, low)function (Eq. 20), where are all available queries (no ground truthannotations required). We note that when the set contains asingle query, it produces a CI for query-level performance.The resulting CI are only reliable if high and low are properlycalibrated. We do so by first sampling a collection of query-sets Q(.2) and calibrating each parameter independently (Eq. 24).Due to the nature of CI (Eq. 22), this guarantees the CRC require-ment is met (Eq. 25), and assuming Q is representative of Q, thisguarantees that our CI are reliable with a given probability (Eq. 27).",
  "RQ4: Can CRC capture differences in uncertainty per query?": "LLM-generated relevance labels. For each query-documentpair, a prompt is constructed that asks the LLM to assess the rele-vance according to the relevance scales of the dataset, in our case:02 (Robust04) and 03 (TREC-DL). The LLM is provided withclear definitions of the different relevance labels, similar to .Specifically, instructions that give definitions for relevance labelsin each prompt. We chose prompts that mimic the instructionsfor human annotators as closely as possible, hereby, we hope to",
  "Nr. human-labeled queries": ": Width (top) and coverage (bottom) of the confidenceintervals produced by the methods. The dashed line in thebottom plots is the 95% coverage target. Shaded areas indicate95% prediction intervals over 500 independent runs. precisely simulate the manual labeling process for each dataset.The exact prompts are provided in Appendix A.To obtain relevance labels, we run the LLMs in scoring mode .That is, for each relevance label R, we compute the log-probabilityof the LLM outputting the relevance rating . The log-probabilitiesare then normalized via a softmax function so that we obtain aprobability distribution that represents the LLMs confidence inassigning each relevance label to the query-document pair.As our LLM model, we choose to use Flan-UL2 , becauseit is open source and has demonstrated strong performance on rank-ing tasks . It is worth noting that larger, more powerful, LLMsexist , and that we do not utilize any prompt-engineering .These choices were made because the goal of our experiments is notto find the best LLM-generated labels, but to confirm whether theconfidence intervals proposed by our methods accurately capturethe uncertainty in LLM-generated relevance labels. Since advance-ments in LLM techniques result in rapid changes in the state-of-the-art, we choose to focus on the established human annotatorsetting instead .Datasets. Our evaluation is based on two established benchmarkdatasets: TREC-DL and TREC-Robust04 . Both datasetsare comprised of documents and queries together with human-annotated relevance judgments. For each dataset, we perform arandom 50:50 split to obtain a validation and test set where thevalidation set is used for calibration of the methods. (A trainingset is not required in our setting.) To avoid distribution shifts, forTREC-DL, we create a stratified sample over four years (2019 -2022) that ensures each year is equally represented in each split. Asthe ranker to evaluate, we choose BM25, as the metric we chooseDCG@10 . In other words, our methods will construct CIsaround the DCG@10 of BM25 on both datasets. displaysthe ranking performance of BM25 and the LLM-generated labels.To match the gain function of DCG all labels were transformedaccordingly: = 2 1, for all performance estimations.Methods in comparison. The methods included in our com-parison are: (i) empirical bootstrapping , (ii) prediction-poweredinference (PPI) (), and (iii) conformal risk control (CRC)(). The empirical bootstrap approach acts as a baseline",
  "Oracle weight": ": Width of the confidence intervals for increasinglevels of LLM bias (, top-row) and oracle-enhanced LLM ac-curacy ( 1, bottom row) with = 112 on TREC-DL and = 125 on Robust04. Shaded areas indicate 95% predictionintervals over 500 independent runs. Coverage plots are omit-ted since all methods maintain >95% coverage. that only considers the available human-labeled data, this is a stan-dard approach in previous IR literature . All ourempirical bootstrap CI are based on 10,000 bootstrap samples. PPIis computed by applying Eq. 11 to both the validation set (the first queries) and the test set (the remaining queries), it utilizesboth human and LLM-generated labels. Finally, our CRC approachalso utilizes both, we use the validation set to calibrate the param-eters and then compute the CI using only the LLM-generated labelson the test-set. For calibration, CRC is provided = 10,000 batcheseach consisting of queries that were sampled with replacementfrom the validation set (see .2). We note that the batchsize depends on the number of available queries with human an-notations, which is varied in our experiments. For the CIs to beevaluated, the CI is applied to the entire test-set to obtain a dataset-level CI, i.e., we compute C(test, high, low) (Eq. 20). Some of ourexperiments consider CRC CIs around query-level performance, inthese cases, is not calibrated on bootstrapped batches but on batches that each contain a single query.We evaluate the CIs produced by each method by consideringtheir width and coverage. The width measures how wide and thushow informative or specific the CI is, where a smaller width is better.The coverage measures how frequent the CI covers the true perfor-mance on the test-set over 500 independently repeated experimentruns, thus the higher the better. The target for all the methods is acoverage of 95% or higher and we set = 0.05 accordingly.",
  "Results8.1Number of required human-annotations": "Our main results are displayed in . Here we see how thewidth and coverage of the different methods vary, as they are pro-vided with queries with human annotations sampled from thevalidation set. As expected, all methods provides better CIs whenprovided with larger portion human-annotated queries, i.e., as increases coverage increases and width decreases.",
  "Robust04DCG@10": ": 95% CI produced per-query by CRC using LLM predicted relevance annotations ( = 0) and oracle-enhanced LLMannotations ( > 0). The queries are sorted by their true DCG performance (according to human-annotations), indicated by redand green dots. Green dots are covered by their CI whereas red dots are not. Blue dots indicate the predicted DCG performance(according to LLM-generated annotations). Clearly, the CIs shrink considerably as annotations become more accurate ( 1). We start by considering the performance of the empirical boot-strap baseline. On the TREC-DL dataset, we see that it requiresat least 40 labeled queries to achieve 95% coverage. Furthermore,on Robust04, with 100 labeled queries it almost reaches 95% cover-age. However, the plotted prediction intervals around the reportedcoverage reveal that many of its runs did not reach 95% coverage.In contrast, both our PPI and CRC approaches have strongercoverage with less queries: PPI needs less than 20 queries on TREC-DL and less than 40 on Robust04. Similarly, CRC needs less than 30on TREC-DL and less than 50 on Robust04. In terms of width, CRCclearly provides the smallest width of all the methods, whilst PPI isworse than empirical bootstrap on TREC-DL and comparable onRobust04. This comparison is not entirely fair, i.e., there is generallya tradeoff between coverage and width, it appears PPI does betterin terms of coverage but that results in wider CIs. Thus, PPI hasa clear advantage over empirical bootstrap on Robust04 where ithas the same width but much better coverage. Nevertheless, whenCRC and PPI have the same coverage, CRC has smaller widths, withan especially large difference on TREC-DL. Therefore, it appearsthat CRC has the most informative CI, whilst PPI needs fewerqueries to reach 95% coverage. Both methods provide substantialimprovements over empirical bootstrapping.Thus we answer RQ1 as follows: both PPI and CRC require asfew as 30 human-labeled queries to produce informative and reli-able confidence intervals. Whilst empirical bootstrapping requiressignificantly more human-labeled queries to achieve similar results.",
  "Sensitivity to LLM accuracy": "Our PPI and CRC methods can benefit from accurate LLM labels,but in order to be reliable, it is also important that they are ro-bust to inaccurate labels. We investigate the effect of LLM accu-racy by adding adversarial bias to the predicted relevance dis-tributions, with , change the predictions as follows: ( = | ,) = 1 (1 ) ( = | ,) + (1 ( = | ,)),where is a normalizing factor to ensure the result is a valid proba-bility distribution. For = 0 this leaves predictions unaltered, with = 0.5 this is a uniform distribution and at = 1 it produces theinverse of the original predictions. shows the widths as is varied ( = 112 on TREC-DL and = 125 on Robust04). We do not report coverage as allmethods obtain a mean coverage of at least 95%. When < 0.5, CRCconsistently provides better widths than empirical bootstrap, whilstPPI has inconsistent improvements. As expected, when > 0.5both methods do worse than empirical bootstrap in terms of width.Thus, we can answer RQ2: the coverage of both PPI and CRC-bootstrap are robust to systematic mistakes made by the LLM,however, improvements in widths are dependent on LLM accuracy.",
  "Potential from more accurate labels": "We run additional experiments to understand how the CIs behaveunder an oracle LLM: one that can perfectly generate relevancelabels. In , we increasingly interpolate between the LLM-generated relevance labels and the true (human-annotated) rel-evance labels using a parameter . As increases, theperformance of the LLM labels becomes better. First, we note thatall methods retain a perfect 100% coverage in these scenarios, sowe omit the plots for coverage. The empirical bootstrap approachdoes not use the LLM-generated labels and its CI is thus not im-pacted by the increasingly stronger LLM labels. The PPI method isable to leverage the stronger LLM labels and is able to significantlyoutperform the empirical bootstrap method. The fact that its CI isplaced around the overall performance (dataset-level), prevents itfrom further improving the width, as it is inherently limited by thenumber of queries. The CRC approaches are able to work aroundthis limitation by efficiently identifying that the LLM-generatedlabels are more accurate as 1 on the per-document level. Theirper-query CIs correspondingly shrink and approach 0 as the LLM-generated labels become better. This answers RQ3: Both PPI andCRC benefit from improvements in label generation accuracy.",
  "Query-performance confidence intervals": "We plot the confidence intervals produced by CRC on individualqueries in . Each plot in the figure shows the true DCG(based on human-annotated relevance labels) and the predictedDCG (based on LLM-generated labels) of all queries in the test split.The queries are sorted by their true DCG, that is, queries wherethe ranker performs best appear on the left and progressively thequery performance goes down. Furthermore, we plot the per-query",
  "Reliable Confidence Intervals for Information Retrieval Evaluation Using Generative A.I.KDD 24, August 2529, 2024, Barcelona, Spain": "Mahsa Yarmohammadi, Xutai Ma, Sorami Hisamoto, Muhammad Rahman, Yim-ing Wang, Hainan Xu, Daniel Povey, Philipp Koehn, and Kevin Duh. 2019. Robustdocument representations for cross-lingual information retrieval in low-resourcesettings. In Proceedings of Machine Translation Summit XVII: Research Track.1220. Emine Yilmaz, Evangelos Kanoulas, and Javed A Aslam. 2008. A simple andefficient sampling method for estimating AP and NDCG. In Proceedings of the31st annual international ACM SIGIR conference on Research and development ininformation retrieval. 603610. Honglei Zhuang, Zhen Qin, Kai Hui, Junru Wu, Le Yan, Xuanhui Wang, andMichael Berdersky. 2023. Beyond yes and no: Improving zero-shot llm rankersvia scoring fine-grained relevance labels. arXiv preprint arXiv:2310.14122 (2023).",
  "Conclusion": "In this paper we study reliable evaluation of IR systems using LLM-generated relevance labels. Obtaining human-annotated relevancelabels is costly, especially in low-resource settings. While LLMs canhelp generate relevance labels at scale, they are prone to make sys-tematic errors and may be unreliable. We resolve this by introducingtwo methods that construct confidence intervals (CIs) around rank-ing metrics produced by LLM-generated relevance labels: PPI andCRC. These approaches require a small amount of reliable groundtruth annotations to statistically analyze the distribution of errorsand correct those errors.Our results demonstrate that the proposed methods can correcterrors in LLM-generated labels and produce reliable CIs. Comparedto other CI approaches, we can produce CIs of superior coveragewith tighter bounds, leading to more informative evaluation. Fur-thermore, the CIs produced by CRC can be computed per-query,providing further insights into low or high performing queries.Our work is not without limitations. First, we note that our meth-ods require an LLM with scoring-mode to produce a distributionover LLM labels. For LLMs without scoring-mode one could gener-ate multiple labels stochastically to approximate a predicted distri-bution. Second, our results suggest that applying some smoothingto the LLM-generated label distribution is beneficial to the resultingCIs. How to systematically optimize the amount of smoothing is anopen question. Similarly, fine-tuning or prompt-engineering couldalso lead to distributions better suited for CI construction. Third, weonly use the Flan-UL2 as an LLM labeler. Our work can be extendedto use different and potentially more powerful LLMs. Future workcould explore all of these directions further.",
  "Anastasios N Angelopoulos, Stephen Bates, Adam Fisch, Lihua Lei, and TalSchuster. 2022. Conformal risk control. arXiv preprint arXiv:2208.02814 (2022)": "Javed A Aslam, Virgil Pavlu, and Emine Yilmaz. 2006. A statistical method forsystem evaluation using incomplete judgments. In Proceedings of the 29th annualinternational ACM SIGIR conference on Research and development in informationretrieval. 541548. Peter Bailey, Nick Craswell, Ian Soboroff, Paul Thomas, Arjen P de Vries, andEmine Yilmaz. 2008. Relevance assessment: are judges exchangeable and does itmatter. In Proceedings of the 31st annual international ACM SIGIR conference onResearch and development in information retrieval. 667674.",
  "Vineeth Balasubramanian, Shen-Shyang Ho, and Vladimir Vovk. 2014. Confor-mal prediction for reliable machine learning: theory, adaptations and applications.Newnes": "Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and ShmargaretShmitchell. 2021. On the dangers of stochastic parrots: Can language modelsbe too big?. In Proceedings of the 2021 ACM conference on fairness, accountability,and transparency. 610623. Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and Rodrigo Nogueira. 2022.Inpars: Unsupervised dataset generation for information retrieval. In Proceedingsof the 45th International ACM SIGIR Conference on Research and Development inInformation Retrieval. 23872392.",
  "Olivier Chapelle and Yi Chang. 2011. Yahoo! learning to rank challenge overview.In Proceedings of the learning to rank challenge. PMLR, 124": "Charles LA Clarke, Gianluca Demartini, Laura Dietz, Guglielmo Faggioli, MatthiasHagen, Claudia Hauff, Noriko Kando, Evangelos Kanoulas, Martin Potthast, IanSoboroff, et al. 2023. 4.2 HMC: A Spectrum of HumanMachine-CollaborativeRelevance Judgment Frameworks. Frontiers of Information Access Experimentationfor Research and Education (2023), 41. Gordon V Cormack and Thomas R Lynam. 2006. Statistical precision of infor-mation retrieval evaluation. In Proceedings of the 29th annual international ACMSIGIR conference on Research and development in information retrieval. 533540. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, Ellen M Voorhees,and Ian Soboroff. 2021. TREC deep learning track: Reusable test collections in thelarge data regime. In Proceedings of the 44th international ACM SIGIR conferenceon research and development in information retrieval. 23692375. Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sen-gupta, and Anil A Bharath. 2018. Generative adversarial networks: An overview.IEEE signal processing magazine 35, 1 (2018), 5365. Jia Cui, Brian Kingsbury, Bhuvana Ramabhadran, Abhinav Sethy, Kartik Au-dhkhasi, Xiaodong Cui, Ellen Kislal, Lidia Mangu, Markus Nussbaum-Thom,Michael Picheny, et al. 2015. Multilingual representations for low resourcespeech recognition and keyword search. In 2015 IEEE workshop on automaticspeech recognition and understanding (ASRU). IEEE, 259266. Domenico Dato, Sean MacAvaney, Franco Maria Nardini, Raffaele Perego, andNicola Tonellotto. 2022. The Istella22 Dataset: Bridging Traditional and NeuralLearning to Rank Evaluation. In Proceedings of the 45th International ACM SIGIRConference on Research and Development in Information Retrieval. 30993107. Thomas Demeester, Robin Aly, Djoerd Hiemstra, Dong Nguyen, and Chris De-velder. 2016. Predicting relevance based on assessor disagreement: analysis andpractical applications for search evaluation. Information Retrieval Journal 19",
  "Fabrizio Gilardi, Meysam Alizadeh, and Mal Kubli. 2023. Chatgpt outperformscrowd-workers for text-annotation tasks. arXiv preprint arXiv:2303.15056 (2023)": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020. Generative adversarialnetworks. Commun. ACM 63, 11 (2020), 139144. Jiafeng Guo, Yixing Fan, Liang Pang, Liu Yang, Qingyao Ai, Hamed Zamani, ChenWu, W Bruce Croft, and Xueqi Cheng. 2020. A deep look into neural rankingmodels for information retrieval. Information Processing & Management 57, 6(2020), 102067.",
  "Michael E Lesk and Gerard Salton. 1968. Relevance assessments and retrievalsystem evaluation. Information storage and retrieval 4, 4 (1968), 343359": "Yue Liu, Zhengwei Yang, Zhenyao Yu, Zitu Liu, Dahui Liu, Hailong Lin, MingqingLi, Shuchang Ma, Maxim Avdeev, and Siqi Shi. 2023. Generative artificial intel-ligence and its applications in materials science: Current situation and futureperspectives. Journal of Materiomics (2023). Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay,Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. 2023. The flan collection:Designing data and methods for effective instruction tuning. arXiv preprintarXiv:2301.13688 (2023). Claudio Lucchese, Franco Maria Nardini, Raffaele Perego, Salvatore Orlando, andSalvatore Trani. 2018. Selective gradient boosting for effective learning to rank.In The 41st International ACM SIGIR Conference on Research & Development inInformation Retrieval. 155164.",
  "Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, RanganMajumder, and Li Deng. 2016. MS MARCO: A human generated machine readingcomprehension dataset. choice 2640 (2016), 660": "Kezban Dilek Onal, Ye Zhang, Ismail Sengor Altingovde, Md Mustafizur Rahman,Pinar Karagoz, Alex Braylan, Brandon Dang, Heng-Lu Chang, Henna Kim, Quin-ten McNamara, et al. 2018. Neural information retrieval: at the end of the earlyyears. Information Retrieval Journal 21 (2018), 111182. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, PamelaMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022.Training language models to follow instructions with human feedback. Advancesin Neural Information Processing Systems 35 (2022), 2773027744.",
  "Harris Papadopoulos. 2008. Inductive conformal prediction: Theory and applica-tion to neural networks. In Tools in artificial intelligence. Citeseer": "John V Pavlik. 2023. Collaborating with ChatGPT: Considering the implications ofgenerative artificial intelligence for journalism and media education. Journalism& Mass Communication Educator 78, 1 (2023), 8493. Carol Peters. 2001. Cross-Language Information Retrieval and Evaluation: Work-shop of Cross-Language Evaluation Forum, CLEF 2000, Lisbon, Portugal, September21-22, 2000, Revised Papers. Vol. 2069. Springer Science & Business Media.",
  "Mark Sanderson et al. 2010. Test collection based evaluation of informationretrieval systems. Foundations and Trends in Information Retrieval 4, 4 (2010),247375": "Mark Sanderson and Justin Zobel. 2005. Information retrieval system evaluation:effort, sensitivity, and reliability. In Proceedings of the 28th annual internationalACM SIGIR conference on Research and development in information retrieval. 162169. Mark D Smucker, James Allan, and Ben Carterette. 2007. A comparison ofstatistical significance tests for information retrieval evaluation. In Proceedingsof the sixteenth ACM conference on Conference on information and knowledgemanagement. 623632. Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Dara Bahri, Tal Schus-ter, Huaixiu Steven Zheng, Neil Houlsby, and Donald Metzler. 2022. Unifyinglanguage learning paradigms. arXiv preprint arXiv:2205.05131 (2022). Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-BaptisteAlayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth,et al. 2023. Gemini: a family of highly capable multimodal models. arXiv preprintarXiv:2312.11805 (2023). Nandan Thakur, Nils Reimers, Andreas Rckl, Abhishek Srivastava, and IrynaGurevych. 2021. Beir: A heterogenous benchmark for zero-shot evaluation ofinformation retrieval models. arXiv preprint arXiv:2104.08663 (2021).",
  "Petter Trnberg. 2023. Chatgpt-4 outperforms experts and crowd workers inannotating political twitter messages with zero-shot learning. arXiv preprintarXiv:2304.06588 (2023)": "George Tsatsaronis, Georgios Balikas, Prodromos Malakasiotis, Ioannis Partalas,Matthias Zschunke, Michael R Alvers, Dirk Weissenborn, Anastasia Krithara, Ser-gios Petridis, Dimitris Polychronopoulos, et al. 2015. An overview of the BIOASQlarge-scale biomedical semantic indexing and question answering competition.BMC bioinformatics 16, 1 (2015), 128. Julin Urbano, Harlley Lima, and Alan Hanjalic. 2019. Statistical significancetesting in information retrieval: an empirical analysis of type I, type II and type IIIerrors. In Proceedings of the 42nd International ACM SIGIR conference on Researchand development in information retrieval. 505514. Cornelis Joost Van Rijsbergen and W Bruce Croft. 1975. Document clustering: Anevaluation of some experiments with the Cranfield 1400 collection. InformationProcessing & Management 11, 5-7 (1975), 171182. Ellen Voorhees, Tasmeer Alam, Steven Bedrick, Dina Demner-Fushman,William R Hersh, Kyle Lo, Kirk Roberts, Ian Soboroff, and Lucy Lu Wang. 2021.TREC-COVID: constructing a pandemic information retrieval test collection. InACM SIGIR Forum, Vol. 54. ACM New York, NY, USA, 112.",
  "APrompts": "The exact prompts used in our experiments are listed here. Wenote that these prompts are tailored towards each dataset and usethe relevance label definitions that human labelers used for eachdataset. The {query} and {passage}/{document} are placeholdersthat are formatted with the actual query and passage/documentduring inference.We observed that the model is sensitive to the particular promptand dataset during scoring mode. For TREC-DL we score the suffixes\"0\", \"1\", \"2\" and \"3\". For Robust04 we found that scoring the suffixeswith brackets is more effective: \"\", \"\" and \"\"."
}