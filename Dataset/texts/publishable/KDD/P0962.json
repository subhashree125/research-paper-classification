{
  "Abstract": "As the size of datasets used in statistical learning continues to grow,distributed training of models has attracted increasing attention.These methods partition the data and exploit parallelism to reducememory and runtime, but suffer increasingly from communicationcosts as the data size or the number of iterations grows. Recentwork on linear models has shown that a surrogate likelihood canbe optimized locally to iteratively improve on an initial solution ina communication-efficient manner. However, existing versions ofthese methods experience multiple shortcomings as the data sizebecomes massive, including diverging updates and efficiently han-dling sparsity. In this work we develop solutions to these problemswhich enable us to learn a communication-efficient distributed lo-gistic regression model even beyond millions of features. In ourexperiments we demonstrate a large improvement in accuracy overdistributed algorithms with only a few distributed update stepsneeded, and similar or faster runtimes. Our code is available at",
  "ACM Reference Format:Fred Lu, Ryan R. Curtin, Edward Raff, Francis Ferraro, and James Holt.2024. High-Dimensional Distributed Sparse Classification with Scalable": "Publication rights licensed to ACM. ACM acknowledges that this contribution wasauthored or co-authored by an employee, contractor or affiliate of the United Statesgovernment. As such, the Government retains a nonexclusive, royalty-free right topublish or reproduce this article, or to allow others to do so, for Government purposesonly. Request permissions from owner/author(s).KDD 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08 Communication-Efficient Global Updates. In Proceedings of the 30th ACMSIGKDD Conference on Knowledge Discovery and Data Mining (KDD 24),August 2529, 2024, Barcelona, Spain. ACM, New York, NY, USA, 11 pages.",
  "Introduction": "Over the past decade, the size of datasets used in statistical andmachine learning has increased dramatically. When the numberof samples and covariates in a dataset becomes sufficiently large,even the training of linear models over the entire dataset becomescomputationally challenging. This has sparked a flurry of interestin distributed training and inference methods. By splitting the dataover multiple machines, local training processes can be run inparallel to save memory and runtime. Multiple works have studiedthe distributed training of logistic regression models ,partitioning the dataset along samples or features and iterativelycommunicating gradients or gradient surrogates.However, when many iterations are needed for convergence,the communication cost of iterative distributed algorithms startto dominate. For example, when the number of features of thedataset is massive, second-order optimization methods which needto communicate O(2) information become impractical. Yet first-order methods, while communicating only O() information at atime, have slower convergence guarantees so may be even moreinefficient due to the extra rounds of communication needed.To alleviate this bottleneck, recent works have proposed methodsto train distributed linear models with relatively little communica-tion. Such methods are first initialized with a distributed one-shotestimator across a dataset which is partitioned across multiplenodes. To do this, the linear model objective is solved locally oneach machine, and the results are transmitted to a central processorwhich merges the local solutions .From this initial estimate, such methods then obtain gradientinformation from all the partitions. The local machine can thensolve a modified objective which takes into account the global gra-dient. This process can be iterated leading to convergence to thefull data solution. Such an approach can be interpreted as an updatestep which only communicates first-order, but uses local second-",
  "KDD 24, August 2529, 2024, Barcelona, SpainFred Lu, Ryan R. Curtin, Edward Raff, Francis Ferraro, & James Holt": "(or higher-) order information to achieve better convergence rates.Many recent papers have studied variants of this underlying ap-proach, including . These all share the underlyingupdate objective, referred to as the communication-efficient surro-gate likelihood (CSL).Theory has been progressively developed for these methodsshowing a favorable rate of convergence under certain conditions.These can include, for example, nearness of the local estimatedgradient to the global gradient, and sufficient strong convexity ofthe Hessian. In practice, as the number of features or partitionsof the dataset grows, these conditions often fail to hold, leadingto diverging solutions. For high-dimensional data, having sparseand interpretable model weights is also of interest. Yet introducingsparsity further complicates the problem, as few standard solverscan solve the CSL objective with 1 regularization in an efficientmanner. These are important limitations for the practical use ofsuch methods for training large-scale academic or industry mod-els. In these real-world scenarios, the data dimensionality can beexceedingly large, while also being sparse, leading to high cor-relations among features and poor conditioning of the objective.Existing experimental results from these prior works have not as-sessed their methods on data of this size, as they have only testedon moderately-sized data with low dimensionality relative to thepartition sample size .In our work, we first show that a standard implementation ofCSL methods fails to effectively learn sparse logistic regressionmodels on these high-dimensional datasets. We next develop aneffective solver for the 1-regularized CSL objective which scalesefficiently beyond millions of features, and prove that it convergesto the correct solution. Experimentally this approach attains higheraccuracies than other methods when the solution is highly sparse.However, at low regularizations when the solution is only moder-ately sparse, the solution to the CSL objective often diverges sharply,leading to poor update performance. To address this, we develop anadaptive damping method to stabilize the CSL objective for high-dimensional solutions. Using this technique, we demonstrate acrossmultiple single-node and multi-node distributed experiments thatour method successfully performs communication-efficient updatesto improve accuracy across a wide range of sparsity settings.",
  "Background and Related Work2.1Sparse logistic regression": "We assume a dataset D = (X, Y) where X = {1, . . . , } consistsof samples in dimensions (that is, each R). The samplesare labeled by Y = {1, . . . , }, where each {0, 1}.The standard approach to obtaining a sparse logistic regressionsolution is to use L1 regularization (also known as the LASSOpenalty) . The objective of this problem is",
  "=1(, ) + 1,(1)": "where (,) = log(1 + ) . By setting appropriately, thesolution can show good performance while having few nonzeroscompared to the dimensionality of the data: 0 .While many algorithms exist to solve the problem ,iterated proximal Newton steps using coordinate descent to solve a quadratic approximation of the objective are known to be especiallyefficient, seeing wide use in popular machine and statistical learningsoftware . In particular, the newGLMNET algorithm in theLIBLINEAR library is perhaps the most commonly used solver .",
  "Distributed estimation": "As datasets grow in size, the memory and computational limit of asingle machine leads practitioners to seek distributed methods. Oneapproach uses exact iterative methods starting from scratch, whichare usually based on adding parallelism to standard single-corealgorithms. For example, LIBLINEAR-MP is a modified version ofnewGLMNET which uses multiple cores for certain inner linearalgebra operations . Alternatively, stochastic gradient methodsallow data to be partitioned across machines or to be sampled ateach iteration, reducing the memory requirement .For even larger datasets, partitioning data across multiple nodesbecomes necessary. Distributed methods which handle splitting bysamples include distributed Newton methods and ADMM, while splitting over features has been proposed in block coor-dinate descent implementations such as .An important limitation to all these approaches is the communi-cation overhead involved in transmitting information (e.g. gradi-ents) across nodes. Because of this, when data becomes especiallylarge or many iterations are needed for convergence, communica-tion costs start to dominate. As the size of data further increases,one-shot or few-shot methods become increasingly attractive byeliminating most of the communication overhead to obtain an ap-proximate solution.",
  "One-shot estimation": "Suppose the samples of D are partitioned across nodes ormachines, and let {D1, . . . , D} denote the samples on each parti-tion, with each D = (X, Y). For simplicity we assume each nodehas samples, but this is not required. We define global and localobjective functions respectively as",
  "High-Dimensional Distributed Sparse Classification with Scalable Communication-Efficient Global UpdatesKDD 24, August 2529, 2024, Barcelona, Spain": "Proof. A similar result is derived as Theorem 3.5 in Jordan et al. by using Corollary 1 of Negahban et al. ; however, in oursituation, we must also consider the proximal penalty (/2) (1) 22.Corollary 1 of Negahban et al. requires that L( ) () berestricted strongly convex. Because L1() is restricted stronglyconvex, the restricted strong convexity of L() is established by theproof of Theorem 3.5 in Jordan et al. . By moving the proximalpenalty terms (and proximal penalty gradient term) to the righthand side, we obtain",
  "Communication-efficient updates": "Non-interactive estimators are approximate and degrade in perfor-mance as the local sample size decreases, which happens whenthe number of partitions grows. As a result, our interest is inupdate procedures which can be iterated to improve the initialestimator, ideally approaching the full data solution.Similar frameworks for iterative global updates have been pro-posed in algorithms, such as DANE , ILEA , and EDSL .For simplicity, we will adopt the term communication-efficient sur-rogate likelihood (CSL) from and refer to works using thisframework as CSL-type methods.Broadly, they propose solving locally the objective",
  "L () L () +L( ) L ( )(6)": "This is motivated as optimizing a Taylor expansion of the localobjective (3), where the local gradient L ( ) is replaced with theglobal gradient L( ). The affine term can be viewed as a first-order correction for the gradient direction. To give further intuition,the higher-order derivatives beyond the Hessian are disregarded ifwe take a quadratic approximation of the local objective",
  "() ( )(8)": "which is equivalent to a quasi-Newton step using global gradientsand local Hessian.Depending on the method, the local objective can either be up-dated only on the main node (CSL) or on all nodes simultaneously(DANE). The latter requires another round of communication andaveraging per iteration. The result of the update (1) then becomesthe starting point for the next update iteration.Other strategies for communication-efficient updates have beenproposed. The ACOWA approach also seeks to reduce theimpact of many processors , by performing two rounds of com-putation and attempting to adjust the loss/increase informationsharing in those two rounds over its predecessor OWA . Single-machine-only algorithms based on lock-free parallelism as a similarissue, where the Hogwild algorithm would regularly diverge, and a lock-free approach SAUS attempted to reduce diver-gences with careful design. In contrast, our approach is iterative butdoes not need many rounds of iteration in practice and supportsboth distributed and single-machine parallelism.",
  "Challenges for scaling CSL-like methods": "Our work aims to solve practical and theoretical concerns whenapplying the CSL framework to update models on massive datasetsand highly distributed systems. We first identify challenges in theexisting methods.Sparsity. When the dimensionality of the data is enormous,model sparsity is a desirable property. For the case of 1-regularizedlinear models, the local loss term L () includes a 1 term. Toefficiently optimize this objective requires techniques specializedfor handling the non-differentiable 1-norm. While this setting hasbeen discussed or studied in , none of the prior worksproposed or specified what solver to use.Thus a practitioner must apply an out-of-the-box solver or im-plement their own. Due to the size of data involved, first-order ordual solvers such as proximal gradient descent or ADMM wouldlikely converge slowly. In our experiments we instead use OWL-QN, a sparse variant of L-BFGS , which we believe to be thefastest standard solver. Because it uses approximate second-orderinformation, it has faster convergence than the alternatives whilestill scaling up to high-dimensional data. We find that this baselineimplementation is adequate on smaller datasets, where relativelyfew features are impactful, but often fails to converge or returnsparse solutions on larger data.We note that this may not have been an issue for prior workbecause (1) their experiments were limited to lower-dimensional orsynthetic datasets, and (2) they did not measure the actual sparsityof their models at any .To address this issue, we develop an efficient and scalable solverbased on iterative proximal quasi-Newton steps, which we detailin . Using this solver, our method proxCSL successfullyconverges to the true objective as well as the right sparsity ().Divergence of the CSL objective. As the data size increases, sooften will the number of distributed partitions to facilitate the useof larger computing systems. If or grow faster than , this oftenresults in a local sample size which is comparable to or smaller.This causes the curvature of the local objective L to decrease. Inparticular the local Hessian may become poorly conditioned ornot positive definite at all. Furthermore, the affine term of the CSLobjective grows with L( )L ( ), which may also increasewhen and diverge. In fact, much of the existing convergencetheory relies on upper bounds of the above term.Under such conditions, the optimum of the CSL objective (1)",
  "(b) ember-100k, 128 partitions, = 0.0001": ": Iterated CSL updates using a standard solver (sSCL) andour method (proxCSL) quickly converge to the optimal objectivevalue (as defined by fitting on the full data) when the solutionis sparse. However, sCSL often fails to reach the correct level ofsparsity of a full data fit. Meanwhile, our specialized solver used inproxCSL attains the optimal sparsity.",
  "L( ) () L ()+L( ( ))L ( ( )) +": "2 ( ) 22(10)Refer to Fig 2 for demonstrative examples of the CSL objectivediverging and the effect of in fixing it.Baselines. For the remainder of our work we refer to two mainCSL-type baselines for distributed updates: sCSL and sDANE. Theseare sparse modifications of CSL and DANE with proximal regular-ization (Eq. 10) which we implemented in OWL-QN. These methodswere presented and named CEASE in recent work , but we use the above names to more clearly differentiate them and relate themto their predecessors.While other global update methods have been recently proposedsuch as DiSCO and GIANT , which solve approximateNewton problems with conjugate gradient, they do not producesparse models so we do not compare against them.",
  "A proximal solver for sparse CSL": "In this section we will describe our algorithm proxCSL, which solvesthe full CSL objective (10) using iterative proximal Newton steps.proxCSL converges to the global objective and true sparsity andautomatically strengthens regularization when the CSL objectivediverges. Our algorithms are summarized in Algo. 1 and Algo. 2.Proximal Newton. Proximal Newton algorithms combine second-order updates with a proximal operator to handle the 1 penalty. Forcomposite objectives of the form min () ()+1 where is convex and smooth, the algorithm approximates () with aquadratic model () and iteratively minimizes () + 1.This minimization is associated with a proximal operator, namelyprox () = arg min12 2 + 1, and has a closed formsolution. For example, if could be solved with a quasi-Newtonstep this gives the result",
  "(+1) = prox ( ) 1( ( )).(11)": "Because of large , explicitly computing , let alone invertingit, is too costly. Instead we solve the proximal minimization usingcoordinate descent, optimizing over one element of at a time. Thisstrategy has been shown to be highly efficient for large problems in and is used in LIBLINEAR . See for detailed coverageand theory on proximal Newton algorithms.For clarity, each step of the resulting algorithm first forms aquadratic approximation which we refer to as an outer step. Thisspecific approximation is then solved using inner steps of coordinatedescent, with each inner step involving a single pass over all thefeatures. These steps are iterated until convergence or until aniteration limit is reached. Following the techniques used in ,our implementation avoids ever explicitly forming and achievesan inner step complexity of O().Outer and inner steps. In the context of CSL, the above proce-dure solves a single update of CSL, which itself can be iterated.Starting with initial estimate ( ), each outer step then forms theCSL quadratic approximation",
  "if 1 1 1otherwise": "We point out that the CSL gradient and Hessian are computedonce at the start of each outer step. However, after each coordi-nate is updated, the local gradient for the next coordinate is af-fected by the previous update via a cross-term with the Hessian: = L( ( ))+(() ( ( )) ( )) where ( ) is the currentaccumulated change to (,). This can be seen by expanding (12).In our experiments we set = 10 and = 50 max outer andinner steps, respectively (see Algo. 2).Hessian caching. As the only vector that needs to be updatedduring inner steps is () (), we note that this can be donewithout forming the Hessian. The Hessian for logistic regressionis () = 1 () where () is diagonal with entries = (1), being the predicted probability of sample . The mainnode stores as a length- vector.Then the Hessian is implicitly updated by simply updating thevector R after each coordinate step. Each coordinate step adds to entry of ; therefore, we add the -th column of () times to the vector.The diagonal of the Hessian is also cached as a length- vectorfor efficiency.Linesearch. Each iteration of coordinate descent executes a passover all features, updating the candidate update vector in place.We run = 50 iterations, unless convergence is reached earlier.This is followed by a linesearch . We replace the linesearchover the local objective with the full CSL objective (10) which in-cludes the step-size regularization. This helps prevent the divergingupdates when using the unregularized CSL objective (6). We scale",
  ": return ( ) +": "by {1, , 2, . . . , }. For each , we evaluate the objectiveat point + , and we select and corresponding update vectorwhich gives the lowest loss. We fix = 0.5 and = 20.Adaptive tuning of . Divergence of the CSL objective canbe detected by sharp decrease (e.g. 20%) in the CSL objective (10)but little change or even increase in the local objective (3). This isdue to the affine term dominating the objective. For our methodwe start = 0.0001 and proceed. If after 5 iterations of coordinatedescent the above conditions are met, we scale by 10 and restart.This helps identify the minimal at a relatively minor runtime cost.Because affects the objective itself, this check is only performedduring the first outer step.",
  "with probability 1 for some > 0, where is the populationrisk minimizer": "Here, hi and lo are the maximum and minimum eigenvaluesof the Hessian of the loss at .Thus, once we have the initial solution owa, we know that it isin the neighborhood of the true solution with high probability.Once we have owa, the next step of proxCSL is to find the proximalsurrogate loss minimizer of Eq. 10.In our setting, we choose to use newGLMNET , althoughother optimization algorithms could also suffice so long as they areguaranteed to converge to the exact solution of Eq. 10.We already know that newGLMNET converges to the exact solu-tion of the logistic regression objective function (Eq. 1, AppendixA ). Using similar reasoning, we can establish that newGLM-NET also converges for the proximal surrogate loss minimizer. Let",
  "Theorem 2. The newGLMNET optimizer, on the proximal surro-gate loss L( ) () instead of the regular logistic loss (Eqn. 1), producesan exact solution": "Proof. newGLMNET is an optimizer that fits in the frameworkof Tseng and Yun , and as with the regular logistic regressionconvergence proof for newGLMNET (Appendix A, ), it sufficesto ensure the conditions required by the framework are satisfied.Firstly, convergence requires that the Hessian (or its estimate) is positive definite. When used to solve the (standard) logisticregression objective, newGLMNET uses = 2L()+I for somesmall , and the positive-definiteness of is known. However, inour case, the addition of I is not necessary. As we are optimizingthe proximal surrogate loss (Eq. 10), we instead take",
  "+ + 1 1,(17)": "where is the solution that the optimizer has found after outerstep , and is either the Hessian 2L() or an approximationthereof. Our goal at this step is to find arg min (). For theoriginal formulation, see Eq. (13), .As specified in the paper , newGLMNET uses a constant stepsize cyclic coordinate descent to solve Eqn. 17. But, this will give aninexact solution, as noted by Friedman et al. . This issue can beresolved either by pairing the coordinate descent with a line search,or by replacing the stopping condition for the inner coordinatedescent solver with the adaptive condition proposed by Lee et al. (Eq. (2.23), adapted here to our notation):",
  "L( ) () + ( + () ) L( ) () (18)": "where is the iteration number of the inner coordinate descentsolver.When using that adaptive stopping condition, so long as thestep size is under some threshold, using Theorem 3.1 of Lee et al. and the fact that the proximal surrogate loss is smooth, weobtain that the inner coordinate descent will converge to the exactminimizer of the quadratic approximation (). In addition, therate of convergence can be shown to be q-linear or q-superlinear if decays to 0 .The final condition for overall convergence is that the outer linesearch terminates in a finite number of iterations; for this, it issufficient to show that",
  "Therefore, taking = X1 X1 + 2, newGLMNET with theadaptive stopping condition for the inner coordinate descent solverconverges to the exact minimizer of the surrogate loss L( ) ()": "Consider now the error of the overall proxCSL algorithm: 2. Recall that our interest is in the high-dimensional sparseregime where may be (much) greater than , and we also expectthe solution to be (potentially highly) sparse. Considering logis-tic regression, 2 L( ) = X X for some diagonal matrix , and if > , 2 L( ) is not positive definite. This means that strong con-vexity is not satisfied, and typical large-sample analysis techniques,such as those used for CEASE cannot be used.However, although L( ) () is not strongly convex, it can bestrongly convex along certain dimensions and in a certain region.Therefore, following Negahban et al. (and ), we imposesome common assumptions for analysis of sparse algorithms.",
  "22": "(28)and therefore L( ) () is restricted strongly convex with parameter + /2. We also must show that L( ) () has restricted LipschitzHessian. Jordan et al. show that L( ) ()(/2) (1) 22(e.g., the proximal surrogate likelihood without the proximal termor, the surrogate likelihood) has restricted Lipschitz Hessian. Theproximal penalty only adds a constant term to the Hessian:",
  ": Datasets with uncompressed libsvm-format sizes": "Proof. The result is a straightforward combination of Theo-rem 3, Theorem 1, and Theorem 3.7 from Jordan et al. .Under the conditions of Theorem 3, the exact statement of The-orem 3.7 of Jordan et al. applies to the proximal surrogateL( ) (): the proximal penalty applies only a constant shift to theHessian 2 L( ) (); this does not affect any results from that result.Next, from Theorem 1, we know that with probability (1 ),",
  "Results": "We run experiments to thoroughly assess the performance of prox-CSL relative to baselines: sCSL and sDANE, the sparse variantsof CSL and DANE respectively with the CEASE modifica-tions discussed in ; and one-shot distributed estimators Naiveavg. and OWA . For the smaller datasets we also run the serialalgorithm newGLMNET from LIBLINEAR to show how closeproxCSL can get to the full data solution. We test our method onmultiple high-dimensional datasets, with details in .The datasets were obtained from the LIBSVM website , withthe exception of ember-100k and ember-1M which we built fromthe malware and benign files in the Ember2018 dataset usingthe KiloGrams algorithm . This consists of computing8-byte n-grams over the files in the dataset, and subsetting to themost frequent 100k or 1M n-grams .For each dataset, we sample a random 80/20 train-test split. Wesplit the training data across varying partitions, depending on theexperiment, and train the methods. We repeat this process across agrid of 80 logarithmically-spaced values. For each , we replicatethe distributed estimation 5 times and record the average number ofnonzeros in each solution and average test set accuracy (along with",
  "(c) url, multi-core, 128 partitions": ": Number of nonzeros vs. test set accuracy in the single-node multi-core setting over a grid of regularization values. Thedistributed methods (sDANE, sCSL, proxCSL) are initialized withthe OWA solution and updated twice. proxCSL (blue) cleanly outper-forms other distributed methods across the datasets, often matchingthe full data solution computed with LIBLINEAR (dashed grey).sCSL performs nearly as well as proxCSL on amazon7 but not onother datasets. sDANE and sCSL fail to achieve sparse solutions onember-100k even after the grid resolution was increased. standard deviations). This gives a good comparison of the methodsacross varying sparsity levels.We implemented all methods in C++ with the Armadillo linearalgebra library and mlpack machine learning library , withOpenMP and MPI to distribute the computations. For sDANE and sCSL we also use the OWL-QN implementation of libLBFGS1 Westudy two distributed settings: (1) single-node multicore, and (2)fully distributed. The first setting is relevant in modern serverswith high numbers of cores available. In our case, we used a pow-erful server with 256 cores and 4TB of RAM for our single-nodeexperiments. The communication costs are lower in this settingbecause network latency is avoided, but the fundamental alorithmworks the same way. The second setting is even larger in scale,when multiple machines are connected. Here we use a cluster with16 nodes, using up to 32 cores and 1TB of RAM on each node.While the methods can be run for many updates, due to the goalof limiting communication, we find that 2-4 iterations are sufficientto update the solution with diminishing return after (see ).Unless otherwise stated we initialize proxCSL, sCSL, and sDANEwith the OWA solution and compare them after 2 updates.For additional hyperparameter and computational detail, referto Appendix B.",
  "Test accuracy across sparsity levels": "Experimental results for the single-node setting are shown over arange of in . Across a range of datasets and sparsities, prox-CSL is able to converge to the full data solution after two updates.When this occurs, proxCSL often significantly outperforms thebaselines sCSL and sDANE, as well as the initial estimators OWAand the naive average. On the smallest dataset (amazon7) only wesee that the sCSL with OWL-QN solver approaches proxCSL in per-formance. On the other hand, DANE generally fares significantlyworse. We find that due to averaging update models across all parti-tions, sparsity is often lost. In addition, the optimization sometimesfails to converge on one or more partitions. shows similar experiments on the fully distributed setting,which we apply to two of our largest datasets (criteo and ember-1M).As before, proxCSL consistently achieves better test accuracy acrossmost sparsity levels than the other methods.Our method can generalize to other loss functions provided thetheoretical assumptions are met. For example, we can use the elasticnet-regularized logistic regression instead of the Lasso regulariza-tion with comparable performance (Appendix C).",
  "Runtime comparison": "In the next set of experiments we also compare the runtimes ofour method proxCSL with the other methods. For each method weidentify the setting that results in a model with roughly 1000 non-zeros for comparability and record the runtime. Note that updatemethods sDANE, sCSL, and proxCSL also include the initializationtime in the total. Therefore their times will generally always behigher than OWA or Naive Avg., unless the underlying setting wasat a significantly different value of .As expected, our method is quite fast even on the largest datasets.Runtimes are comparable with sCSL since OWL-QN is also knownto be a fast solver. Yet our method converges to better accuracysolutions given similar runtime. In comparison sDANE is generallyslower because the second optimization must be done on eachpartition and re-averaged, incurring additional communication andcomputational time. For more details and analysis see Appendix A.",
  "(b) ember-1M, distributed, 512 partitions": ": Number of nonzeros vs. test set accuracy in the distributed multi-node setting, after two update steps for the distributed methods(sDANE, sCSL, proxCSL). On both datasets, proxCSL (blue) outperforms the other methods across all sparsity levels. Due to the massive datasize, no full data solution is computed. On criteo, OWA diverges at low regularizations, so we initialize the distributed methods with NaiveAvg. instead. sDANE and sCSL fail to achieve sparse solutions on ember-1M even after the grid resolution was increased.",
  "Convergence to a known model": "Finally we demonstrate empirically that our method convergesto the true solution on a sparse dataset with known generatingmodel. We simulate data where X has dimension = 100000, = 1000, where each feature is mixture distribution of (0, 1) and0 values. The true solution has 100 nonzero coefficients, andY is sampled as Bernoulli from expit(). Using 64 partitionsthe data is full-rank on each partition in order to satisfy the strongconvexity assumption.In this data generation model, the assumptions of Thm 3 are satis-fied so we expect convergence in the 2-norm. We train proxCSL aswell as baselines sSCL and sDANE with set to give approximately100 nonzeros. Convergence in 2-norm and support recovery areshown in . Here proxCSL converges to a known solution vectorfaster and more accurately than the baselines.",
  "Conclusion": "In this work we present proxCSL which performs global updateson a distributed sparse logistic regression model in an efficient andscalable manner. To do this, we develop a proximal Newton solverwhich solves a CSL-type problem effectively along with adaptiveproximal regularization. We assess our method on much largerand higher-dimension datasets than prior work, and conclude thatproxCSL has much better accuracy than prior works across a widerange of model sparsities.While we have accelerated a widely used form of logistic re-gression, other bespoke or customized versions still need improve-ment or could be integrated in the future. Coresets may be a viableapproach to improving information sharing without sending alldata and areas like differentially privacy rely heavily onlogistic regression but have far more expensive and challenging op-timization problems due to the required randomness .",
  "Xueying Chen and Min-ge Xie. 2014. A split-and-conquer approach for analysisof extraordinarily large data. Statistica Sinica (2014), 16551684": "Ryan R Curtin, Marcus Edel, Omar Shrit, Shubham Agrawal, Suryoday Basak,James J Balamuta, Ryan Birmingham, Kartik Dutt, Dirk Eddelbuettel, RishabhGarg, et al. 2023. mlpack 4: a fast, header-only C++ machine learning library.Journal of Open Source Software 8, 82 (2023). Ryan R. Curtin, Sungjin Im, Benjamin Moseley, Kirk Pruhs, and Alireza Samadian.2020. Unconditional Coreset for Regularized Loss Minimization. In Proceedings ofthe 23rd International Conference on Artifical Intelligence and Statistics (AISTATS2020). 482492.",
  "Amol Khanna, Fred Lu, and Edward Raff. 2023. The Challenge of DifferentiallyPrivate Screening Rules. In Submitted to SIGIR 23": "Amol Khanna, Fred Lu, Edward Raff, and Brian Testa. 2023. Differentially PrivateLogistic Regression with Sparse Solutions. In Proceedings of the 16th ACM Work-shop on Artificial Intelligence and Security (AISec 23). Association for ComputingMachinery, New York, NY, USA, 19. Amol Khanna, Edward Raff, and Nathan Inkawhich. 2024. SoK: A Review ofDifferentially Private Linear Models For High-Dimensional Data. In 2024 IEEEConference on Secure and Trustworthy Machine Learning (SaTML). 5777.",
  "Edward Raff. 2017. JSAT: Java Statistical Analysis Tool, a Library for MachineLearning. Journal of Machine Learning Research 18, 23 (2017), 15": "Edward Raff, William Fleming, Richard Zak, Hyrum Anderson, Bill Finlayson,Charles K. Nicholas, Mark Mclean, William Fleming, Charles K. Nicholas, RichardZak, and Mark Mclean. 2019. KiloGrams: Very Large N-Grams for MalwareClassification. In Proceedings of KDD 2019 Workshop on Learning and Mining forCybersecurity (LEMINCS19). Edward Raff, Amol Ashish Khanna, and Fred Lu. 2023. Scaling Up DifferentiallyPrivate LASSO Regularized Logistic Regression via Faster Frank-Wolfe Iterations.In Thirty-seventh Conference on Neural Information Processing Systems.",
  "Edward Raff and Jared Sylvester. 2018. Linear models with many cores and cpus:A stochastic atomic update scheme. In 2018 IEEE International Conference on BigData (Big Data). IEEE, 6573": "Edward Raff, Richard Zak, Russell Cox, Jared Sylvester, Paul Yacci, Rebecca Ward,Anna Tracy, Mark McLean, and Charles Nicholas. 2016. An investigation of byten-gram features for malware classification. Journal of Computer Virology andHacking Techniques (sep 2016). Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. 2011. Hogwild!:A lock-free approach to parallelizing stochastic gradient descent. Advances inneural information processing systems 24 (2011).",
  "Conrad Sanderson and Ryan Curtin. 2016. Armadillo: a template-based C++library for linear algebra. Journal of Open Source Software 1, 2 (2016), 26": "Ohad Shamir, Nati Srebro, and Tong Zhang. 2014. Communication-efficient dis-tributed optimization using an approximate newton-type method. In Internationalconference on machine learning. PMLR, 10001008. Ilya Trofimov and Alexander Genkin. 2015. Distributed coordinate descent forl1-regularized logistic regression. In Analysis of Images, Social Networks and Texts:4th International Conference, AIST 2015, Yekaterinburg, Russia, April 911, 2015,Revised Selected Papers 4. Springer, 243254.",
  "Jialei Wang, Mladen Kolar, Nathan Srebro, and Tong Zhang. 2017. Efficientdistributed learning with sparsity. In International conference on machine learning.PMLR, 36363645": "Shusen Wang, Fred Roosta, Peng Xu, and Michael W Mahoney. 2018. Giant:Globally improved approximate newton method for distributed optimization.Advances in Neural Information Processing Systems 31 (2018). Guo-Xun Yuan, Chia-Hua Ho, and Chih-Jen Lin. 2011. An improved glmnetfor l1-regularized logistic regression. In Proceedings of the 17th ACM SIGKDDinternational conference on Knowledge discovery and data mining. 3341.",
  "Yuchen Zhang and Lin Xiao. 2018. Communication-efficient distributed optimiza-tion of self-concordant empirical loss. Large-Scale and Distributed Optimization(2018), 289341": "Yong Zhuang, Wei-Sheng Chin, Yu-Chin Juan, and Chih-Jen Lin. 2015. Distributednewton methods for regularized logistic regression. In Advances in KnowledgeDiscovery and Data Mining: 19th Pacific-Asia Conference, PAKDD 2015, Ho ChiMinh City, Vietnam, May 19-22, 2015, Proceedings, Part II 19. Springer, 690703. Yong Zhuang, Yuchin Juan, Guo-Xun Yuan, and Chih-Jen Lin. 2018. Naiveparallelization of coordinate descent methods and an application on multi-core l1-regularized classification. In Proceedings of the 27th ACM International Conferenceon Information and Knowledge Management. 11031112.",
  "BSensitivity to hyperparameters": "Although our default proxCSL sets relatively low maximum itera-tions, these values are generally sufficient to ensure convergenceof the objective function. In this analysis we increase the itera-tion counts to guarantee full convergence and show the impact isminimal. See following table.We also provide additional parameter and computational detailsnext.Optimizers. OWLQN: We use default hyperparameters on OWL-QN for the baselines with 100 max iterations. We experimentedwith changing hyperparameters and increasing iterations but theydid not affect the results.LIBLINEAR: When using LIBLINEAR to solve the initial dis-tributed models, we set 20 max outer iterations and 50 max inner",
  ": Logistic regression objective values after running a singleproxCSL update with specified hyperparameters and": "iterations. This is to obtain faster solutions since the solution isapproximate anyway. This did not affect accuracy. All other param-eters are default. For the Full Data upper bound we use all defaultparameters.System details. A single MPI experiment uses 16 machines,each of which has two AMD EPYC 7713 64-core processors. Welimit to using 32 cores per machine so that the amount of inter-machine communication is non-trivial. The machines are connectedvia Infiniband HDR and deployed with Slurm.Computational complexity. Given and , as well as datasetsizes , , our solver is () for dense data, and () forsparse, which is quite efficient. Note that here represents thenumber of non-zero elements in the dataset. This is the same com-putational complexity as newGLMNET."
}