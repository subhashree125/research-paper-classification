{
  "*Email:": "Abstract: In the autoencoder based anomaly detection paradigm, implementing the autoencoder in edge devices capable of learning in real-time is exceedingly challenging due to limited hardware, energy, and computational resources. We show that these limitations can be addressed by designing an autoencoder with low-resolution non-volatile memory-based synapses and employing an effective quantized neural network learning algorithm. We propose a ferromagnetic racetrack with engineered notches hosting a magnetic domain wall (DW) as the autoencoder synapses, where limited state (5-state) synaptic weights are manipulated by spin orbit torque (SOT) current pulses. The performance of anomaly detection of the proposed autoencoder model is evaluated on the NSL-KDD dataset. Limited resolution and DW device stochasticity aware training of the autoencoder is performed, which yields comparable anomaly detection performance to the autoencoder having floating-point precision weights. While the limited number of quantized states and the inherent stochastic nature of DW synaptic weights in nanoscale devices are known to negatively impact the performance, our hardware-aware training algorithm is shown to leverage these imperfect device characteristics to generate an improvement in anomaly detection accuracy (90.98%) compared to accuracy obtained with floating-point trained weights. Furthermore, our DW-based approach demonstrates a remarkable reduction of at least three orders of magnitude in weight updates during training compared to the floating-point approach, implying substantial energy savings for our method. This work could stimulate the development of extremely energy efficient non-volatile multi-state synapse-based processors that can perform real-time training and inference on the edge with unsupervised data.",
  ". Introduction": "In today's interconnected world, the security and integrity of computer networks are of paramount importance. By 2030, it is estimated that 500 billion devices will be connected to the internet , with a significant portion comprising Internet of Things (IoT) devices. While the rapid growth of network-based devices, applications and services offer immense convenience, it also has led to an increasing number of cyber threats and attacks . The proliferation of cybercrimes and network intrusions underscores the need for developing robust solutions that can safeguard network security. Anomaly detection plays a vital role in safeguarding these networks by identifying and mitigating abnormal or malicious activities that deviate from expected patterns . Detecting such anomalies in real-time is crucial to prevent potential damage, data breaches, and service disruptions . As we look towards the future, where the Internet of Things (IoT) and edge computing gain prominence, the need for efficient and effective anomaly detection becomes even more critical. Autoencoders have emerged as a promising approach for anomaly detection in unlabeled network traffic samples -. These neural network architectures are capable of learning meaningful representations of data by training on unlabeled examples. The essence of an autoencoder lies in its ability to encode input",
  ". Related Work": "In recent years, the increasing adoption of machine learning approaches for anomaly detection has been driven by the limitations and high costs associated with conventional signature-based intrusion detection techniques . These traditional methods prove inadequate in effectively detecting zero-day attacks, which are characterized by their unknown and previously unseen nature . Various supervised learning-based classification algorithms and hybrid models combining multiple algorithms have been explored to identify network anomalies and detect attacks with high accuracy. Notable algorithms include Support Vector Machine (SVM), Decision Tree (DT), Naive Bayes Network (NB), Naive Bayes Tree (NBTree), J48, Fuzzy logic, and Artificial Neural Networks (ANN) , -. However, the effectiveness of these algorithms hinges on accurate labels and balanced training data . The availability of such data, particularly in the realm of network intrusion detection, is limited due to factors like privacy concerns and data confidentiality . To overcome this constraint, researchers have turned to unsupervised learning techniques, such as anomaly detection algorithms based on autoencoders , . More recently, studies have been conducted on unsupervised deep learning circuits using memristors to enable real-time anomaly detection with autoencoders on low-power devices . In the emerging field of spintronic memory devices, researchers have made significant advancements in leveraging their properties for efficient computing. It has been shown that non-volatile nanomagnetic devices can be controlled efficiently using voltage-controlled magnetic anisotropy (VCMA) -, voltage-induced strain -, current control , , and a combination of both current and voltage control , , . Studies have demonstrated that, despite having imperfect device characteristics, nanomagnetic devices can be implemented as multistate synapses for deep neural networks , . However, for quantized neural network implementation, weight gradients need to be stored in high-",
  "NSL-KDD Data": "The NSL-KDD dataset is derived from the KDD Cup 1999 dataset, which represents a comprehensive collection of network traffic data containing both normal and various attack instances . Within the NSL-KDD dataset, two distinct sets of data exist: KDDTrain+ and KDDTest+. The training data (KDDTrain+) consists of 125,973 packets, each categorized into one of the 23 distinct data types. These types include malicious categories such as Ipsweep, Guess_passwd, Warezclient, Neptune, Multihop, Perl, Smurf, Phf, Rootkit, Imap, Loadmodule, Portsweep, Nmap, Back, Pod, Spy, Land, Warezmaster, Satan, Buffer_overflow, Teardrop, Ftp_write, as well as the category labeled Normal . Among these, 58,630 packets in the KDDTrain+ dataset are labeled as malicious, while the remaining 67,343 packets represent the normal packets. As for the KDDTest+ dataset, it comprises a total of 22,544 packets, with 12,833 packets categorized as malicious and the remaining 9,711 packets labeled as normal.",
  "Preprocessing Steps": "Prior to the training phase, preprocessing is applied to all packets in the dataset. First, the categorical features are converted into numerical representations. Specifically, the second position (protocol/type), third position (service), and fourth position (flag) contain categorical data. The three categorical features are modified to one-hot encoded numerical values. For instance, in the \"protocol type\" feature, the strings \"tcp\", \"udp\", and \"icmp\" are replaced with their respective one-hot encoded representations: , , and . As a result, the three categorical features in the NSL-KDD dataset, namely \"protocol type,\" \"service,\" and \"flag,\" which have 3, 70, and 11 distinct strings respectively, are transformed into a total of 84 features. This one-hot encoding process leads to a combined total of 122 features, including the selected original 38 numeric features. Additionally, the dataset undergoes normalization to ensure consistency. Each feature vector is normalized by scaling them to fit in the range of [0; 1] based on the maximum value within that feature vector. All types of malicious data are labeled as \"1,\" while normal data packets are labeled as \"0\".",
  "Autoencoder": "An autoencoder is a neural network architecture that employs unsupervised learning to reconstruct input data. Comprising multiple layers, including one or more hidden layers, the autoencoder maintains the same size for both the input and output layers. At the center of the network lies the bottleneck layer, which represents a compressed latent space representation of the input data. The encoder maps the input to the bottleneck layer representation, while the decoder reconstructs it in the output layer . illustrates the architecture of a standard autoencoder.",
  "where is the autoencoder parameters (weights and biases). Here, L represents a mean squared error (MSE) loss function": "The reconstruction error is used to determine whether a network traffic sample is normal or malicious. During the testing phase, if a network sample shows a high reconstruction error, it is likely to be considered as a malicious packet. This is because an autoencoder trained on normal network traffic packets generally has low reconstruction error for normal data.",
  "Autoencoder Model": "In this study, we use an autoencoder architecture comprising five layers. The number of nodes in each layer, ranging from the input to the output layer, is , influenced by the model architectures investigated in . However, it is important to note that our primary focus is not the impact of different autoencoder model architectures; instead, we aim to compare the performance of an autoencoder with quantized synapses to a similar autoencoder with floating-point synapses. The input features undergo a sequence of weighted sum operations and activation functions within the autoencoder model until they are reconstructed in the output layer. Specifically, we employ the sigmoid function as the activation function. The autoencoder is trained using the backpropagation algorithm in conjunction with stochastic gradient descent. This enables the adjustment of the weights and biases of the network based on the difference between the predicted output and the actual input. The performance of the autoencoder is measured using MSE as the loss function. illustrates the autoencoder architecture.",
  "Autoencoder Training Process": "While KDDTrain+ and KDDTest+ carry both normal and malicious traffic samples, only the normal traffic samples from the KDDTrain+ are used for training the quantized autoencoder. illustrates the workflow for quantized autoencoder based anomaly detection. In the training phase, the processed normal traffic samples are sent to the autoencoder, where the original features are encoded to a latent space representation. Next, output features are reconstructed from this latent space. The reconstruction error is assessed from the difference between the reconstructed traffic sample and the original sample. The reconstruction error is measured for all the traffic samples and the standard deviation is estimated, which acts as the threshold for detecting anomalies. In the inference phase, network traffic samples (carrying both normal and malicious samples) are sent to the trained autoencoder, and reconstruction error is calculated for each sample. The difference between the reconstruction error of a sample and the mean error of samples is referred to as the anomaly score (AS). The anomaly score is then compared to the threshold. If the anomaly score is higher than or equal to the threshold, then the traffic sample is inferred as malicious. Otherwise, the sample is inferred as benign.",
  ": Schematic diagram of quantized autoencoder based anomaly detection": "Quantization-aware training of the autoencoder is performed by quantizing the weights according to (4) in the feed-forward phase and using the backpropagation algorithm based on low-precision neural network training . In this process, while all weights are quantized in the forward pass, weight gradients are stored in separate high precision memory units during the backward pass to retain accuracy. However, the weight gradients need to propagate through non-differentiable quantization blocks in the backward pass. To tackle this issue, the straight through estimator (STE) is used, which provides a workaround by treating the quantization operation as a simple identity function during backpropagation . This allows the gradients to be backpropagated as if the quantization were not applied.",
  "DW Synapse Design": "In this section, we discuss the proposed low-resolution (quantized) DW synapse design for the autoencoder neurons. Being non-volatile, the DW memory retains data for a long time even in absence of power. We design the device by simulating a thin ferromagnetic racetrack with five engineered notches where DW positions can be controlled with current pulse. The racetrack has a dimension of 560 nm 60 nm 1 nm. Along the racetrack, engineered notches are incorporated at a regular interval of 100 nm. However, the first and the fifth notches are positioned at 80 nm and 480 nm respectively. illustrates the design of the nanomagnetic DW-based synaptic device. : Schematic diagram of magnetic domain wall non-volatile synapses driven by SOT current pulses: (a) A sample of micromagnetic simulations showing pinned position of the domain wall (b) Configuration of DW device with 5 notches. To control the DW position, current pulses with specific amplitude and fixed pulse duration are applied across the heavy metal layer. The current pulses generate SOT, which acts on the magnetic racetrack above it. By varying the number and the direction of current pulses, DW can be moved to different positions. An MTJ is formed by combining the racetrack (free layer), an insulator (MgO tunneling layer), and a ferromagnetic material (reference layer) as seen in b. The MTJ is used to read the state of the racetrack's magnetization. DW positions are encoded as the conductance of MTJ, thereby creating a synapse that can be programmed with current pulses.",
  "T Temperature 300 K": "a illustrates the micromagnetic configuration of the racetrack's free layer. A fixed amplitude current pulse of 85 109 A/m2 with a fixed duration of 0.5 ns is applied through the heavy metal layer to initiate the DW depinning from its initial pinned position and move it towards the intended adjacent notch. However, due to the DW tilting caused by the presence of Dzyaloshinskii-Moriya interaction (DMI) and thermal noise, the DW exhibits significant stochastic motion when driven by the SOT current pulses. As a result, the DW might get pinned at a different notch position rather than getting pinned at the intended specific notch position after the SOT current pulses are applied. illustrates the probabilistic distribution of the DW positions due to stochastic variation in the DW motion. The equilibrium pinned positions of the DWs are used to calculate the conductance of the MTJ using the subsequent equation (5) :",
  "Performance Measures": "We employ accuracy, precision, true positive rate (TPR), and F1 score to assess the performance of the autoencoder model. These performance measures can be expressed using four quantities: True Positive (TP), True Negative (TN), False Positive (FP), and False Negative (FN). TP represents the number of correctly classified malicious samples, TN represents the number of correctly classified normal samples, FP represents the number of normal samples incorrectly classified as malicious samples, and FN represents the number of malicious samples incorrectly classified as normal samples. Accuracy quantifies the overall correctness of the classification model and represents the ratio of correctly classified packets to the total number of packets. It is represented by the following equation (6):",
  "Results for Quantized Autoencoder": "In this section, we compare the performance of the proposed quantized autoencoder with 2, 3, and 5-state quantization levels to a structurally identical autoencoder with full precision floating-point (32-bit) weights. The performance evaluation is based on testing accuracy, precision, TPR, and F1 score. illustrates these performance metrics for the autoencoder with 2, 3, 5-state, and full precision floating-point synapses. a demonstrates that employing only 2 quantization levels in the autoencoder leads to random fluctuations in the resulting accuracy as the training progresses through successive epochs. Thus, training with 2 quantization levels shows occasional high accuracy with extended training cycles; however, predicting the number of epochs required to achieve high accuracy remains challenging since the accuracy does not converge over time. Similarly, training the autoencoder with 3 quantization levels yields a comparable outcome, showing random fluctuations in accuracy with extended training cycles. However, the fluctuation pattern is less random compared to the previous case. On the other hand, training the autoencoder with 5 quantization levels demonstrates a more deterministic accuracy progression, with accuracy gradually increasing over the epochs. Notably, across all epochs, the accuracy for training with 5-state weights is comparable to the accuracy for training with full precision weights. Similar conclusions can be drawn for the remaining performance metrics illustrated in b, c, and d.",
  ": Anomaly detection testing (a) accuracy (b) precision (c) TPR and (d) F1 score for autoencoder with different state (2, 3, 5-state and floating precision) synaptic weights": "From the results demonstrated in , it can be inferred that training with 5 quantization levels shows comparable performance metrics to training with full precision weights, even though 5-state quantization requires significantly lower number of computations. While the limited number of quantized states is usually regarded as detrimental to training and testing accuracy, quantization-aware training can leverage this quantization noise to generate a significant improvement in accuracy compared to the accuracy obtained by floating-point trained weights. In this case, quantization works as a regularization operation and limits overfitting of the training weights. By reducing the precision of numerical values in network parameters, such as weights and activations, quantization reduces model complexity and prevents the network from memorizing noise or outliers in the training data. This reduction in precision introduces a controlled level of noise or approximation error, which helps smooth out decision boundaries and makes the network less sensitive to small variations in the input data. By promoting a more generalized",
  ": Anomaly detection testing accuracy vs epoch for autoencoder with 5-state quantized DW synapses, 5-state synapses (without device stochasticity), and full precision floating-point weight synapses": "illustrates anomaly detection testing accuracy for different configurations of the autoencoder. From it can be inferred that when using only five quantization levels, the quantized autoencoder achieves a competitive accuracy in anomaly detection compared to the autoencoder with full precision floating-point weights. This performance can be attributed to quantization acting as a regularization operation, as explained in the previous section. Additionally, the autoencoder with quantized DW-based stochastic synapses shows higher accuracies than the autoencoder with only quantized synapses. For the autoencoder with DW-based synapses, non-volatile synapses are designed using a specific hardware technology called racetrack MTJ. The synapses can encode multiple non-volatile states and the training process considers the characteristics of the device, such as noise and stochasticity. By introducing randomness during the training process, stochasticity serves as a regularization technique. It adds noise to the model and encourages exploration of different solutions, thus reducing the risk of overfitting to specific patterns in the training data. The results obtained with the DW synapses illustrate a higher anomaly detection accuracy (90.98%) surpassing even the full precision floating-point accuracy (90.85%). The findings presented in indicate that combining stochasticity with quantization further improves the accuracy of anomaly detection. This combination acts as a better regularization process. Moreover, the fact that stochasticity arises from",
  "In this section, we conduct a comparison of the total number of programmed weights (weight updates) across different autoencoder synapse schemes": ": Comparison of the total number of weight updates versus the number of training epochs for different autoencoder types. Significantly fewer weights are updated during the training of the proposed quantized DW device based autoencoder in comparison to the floating precision weight based autoencoder with the same architecture. presents the graphical representation of the total weight updates against the number of training epochs for the quantized DW synapse-based autoencoder, as well as the 2, 3, and 5-state autoencoders (excluding stochastic devices), and the floating-point weight-based autoencoder. The data from reveals a significant distinction: the proposed DW-based approach exhibits a remarkable reduction of at least three orders of magnitude in weight updates when compared to the floating-point approach, implying substantial energy savings for the proposed method. Moreover, the 2, 3, and 5-state quantized autoencoders also demonstrate notably fewer weight updates compared to their floating-point weight-based counterparts. Among these three quantized approaches, the 5-state autoencoder requires the least number of weight updates. Furthermore, it is noteworthy that the number of weight updates for the DW-based autoencoder diminishes as the training epochs progress, in contrast to the patterns observed in the other methods. Consequently, the proposed DW device-based autoencoder demonstrates a greater degree of computational resource efficiency, resulting in reduced energy consumption. 7. Conclusion In conclusion, the state-of-the-art autoencoder based unsupervised anomaly detection methods have shown promising results in detecting network anomalies. However, implementing these methods on edge devices with limited hardware, computational resources, and energy has been a challenge. In this paper, we proposed a solution to this challenge by designing a quantized autoencoder with non-volatile memory-based synapses to detect anomalies on NSL-KDD data effectively on edge devices. Our proposed efficient"
}