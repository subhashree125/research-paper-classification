{
  "Abstract": "Large language models (LLMs) are poised to revolutionize the do-main of online fashion retail, enhancing customer experience anddiscovery of fashion online. LLM-powered conversational agentsintroduce a new way of discovery by directly interacting with cus-tomers, enabling them to express in their own ways, refine theirneeds, obtain fashion and shopping advice that is relevant to theirtaste and intent. For many tasks in e-commerce, such as findinga specific product, conversational agents need to convert their in-teractions with a customer to a specific call to different backendsystems, e.g., a search system to showcase a relevant set of prod-ucts. Therefore, evaluating the capabilities of LLMs to performthose tasks related to calling other services is vital. However, thoseevaluations are generally complex, due to the lack of relevant and Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from Evaluation KDD 24, August 2526, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. high quality datasets, and do not align seamlessly with businessneeds, amongst others. To this end, we created a multilingual evalu-ation dataset of 4k conversations between customers and a fashionassistant in a large e-commerce fashion platform to measure thecapabilities of LLMs to serve as an assistant between customersand a backend engine. We evaluate a range of models, showcasinghow our dataset scales to business needs and facilitates iterativedevelopment of tools.",
  "NLP, Generative AI, Fashion Domain, LLM Evaluation, EvaluationFramework and Dataset": "ACM Reference Format:Antonis Maronikolakis, Ana Peleteiro Ramallo, Weiwei Cheng, and ThomasKober. 2024. What should I wear to a party in a Greek taverna? Evaluationfor Conversational Agents in the Fashion Domain. In Proceedings of KDDworkshop on Evaluation and Trustworthiness of Generative AI Models (GenAIEvaluation KDD 24). ACM, New York, NY, USA, 10 pages.",
  "Introduction": "The advent of large language models (LLMs) and generative ar-tificial intelligence has transformed the landscape of natural lan-guage processing, not only in academic but also in industry settings. LLMs, developed via large-scale pretraining and Rein-forcement Learning from Human Feedback , have show-cased heightened proficiency in language comprehension. Conse-quently, they have been instrumental in streamlining customerinteractions and enhancing overall user satisfaction.In particular, large language models are ushering a wave of newsolutions in the domain of customer support services, where modelsare forming the basis of chatbots and assistant agents. Domainssuch as e-commerce and healthcare offer ample opportunity forbreakthroughs in customer support through the deployment ofmodel agents. In our work, we focus on the domain of online fashionretail and the use of conversational agents for customer support.Beyond its functional aspect of providing clothing, fashion en-ables individuals to express themselves, communicate their identityand values, as well as forge a sense of community and belonging.1 Online fashion retailers enable customers to browse a wide rangeof assortments conveniently, inspire customers to develop theirpersonal taste and provide information to empower customers tomake decisions with greater confidence.Large language models provide an opportunity to change thestatus quo in online fashion retail. Customers, instead of interactingwith a search engine that can not process abstract descriptions offashion concepts (e.g., essentials or urban), are now enabledto have a conversation to describe and refine their wants in aninteractive way, using their own words, language and fashion ideas.An assistant agent can then interface with the retailers search andrecommendation engines to show products to the customer tailoredto the conversation and their descriptions.In our work we focus on the evaluation of the capabilities ofan agent to interact with a customer and interface appropriatelywith a backend search engine, developing a dataset of conversa-tions between customers and assistant agents in the fashiondomain. With our dataset, we facilitate the iterative developmentof assistant agents, allowing for fair comparisons between differ-ent models, recording progress across model improvements andmapping performance metrics to model changes ().Our dataset is made up of 1334 conversations each for English,German and French, plus an additional 87 conversations in Greek.Data is generated through a controlled simulation environ-ment where an LLM-based customer agent interacts with anassistant agent to generate conversations. The customer agentis given the description of an item or a theme and is instructed tointeract with the assistant in order to purchase an item that fits itsgoal, while the assistant is instructed to interact with the customerto aid them through their shopping trip, assisting in providing in-formation, advising and showing items that the customer may beinterested in. All conversations are subsequently verified manuallyfor high quality. With our methodology, which leverages the power",
  "The answer to the question what should I wear to a party in a Greek taverna is,according to our best-performing agent, white shirts paired with loose jeans andcomfortable shoes for dancing": "of LLMs for fast generation of data, we ensure evaluation scales tobusiness needs and is cost-effective.We propose the use of our multilingual dataset for theevaluation of the capabilities of assistant agents to interfacebetween customer and backend engines, interacting with thecustomer to identify their fashion needs and translate them intoa format usable to a search engine. To ensure reproducibility, wepropose evaluation via the replay of conversations in a controlledsimulation environment, with the examined agent taking the roleof the assistant agent.Using our dataset we benchmark an array of open- andclosed-source models, such as Llama2, Mistral and GPT models,as well as perform a qualitative analysis to map performance acrossdifferent fashion styles and terms.In summary, our contributions are:",
  "Related Work": "There has been considerable work towards evaluation of text gen-eration, from the early days of textual overlap metrics tomethods based on the similarity of contextualized representations, as well as composite metrics to cover many aspects ofconversational capabilities .State tracking evaluation methods offer a structured way to eval-uate the capabilities of models to converse in desirable patterns andproduce responses appropriate for each stage of the conversations. Due to the highly stochastic and subjective nature of textgeneration and its evaluation, traditional metrics have been shownto oftentimes be uninterpretable . Thus, there have beenefforts to develop systems involving humans and manual qualityassessment .With the recent rise of Large Language Models and their ad-vanced conversational and reasoning capabilities ,evaluation of text generation models is moving away from singu-lar metrics that measure particular aspects of language (such asgrammaticality, readability, or similarity to a source text), towardsthe development of leaderboards based on benchmarks of a seriesof tasks and evaluations for the more reproducible comparison ofpowerful models .",
  "in two steps: (i) conversation generation through a simulationenvironment, and (ii) manual quality verification": "3.1.1Fashion Attributes.Focusing on the fashion domain, we create a dataset covering manyaspects of fashion and terminologies that a customer would usewhen looking for items online. Namely, we cover six attributes:colour, type, material, fit, brand and size. Further, we cover eightapparel types: pants, trousers, shoes, jacket, coat, sweatshirt, hoodieand jeans. We use both pants and trousers, two synonyms, toevaluate robustness to paraphrasing. The acceptable values for eachattribute are the following:",
  "Size: Acceptable values are S, M, L, small, medium and large": "Finally, we cover nine open-ended themes customers might beshopping for: (i) hiking, (ii) Christmas dinner, (iii) football in winter,(iv) rooftop summer party, (v) techno club in Berlin, (vi) coffeehouse in Vienna, (vii) party in Greek taverna, (viii) sports in thesummer, (ix) eccentric timelord book bazaar.3.1.2Generation of Conversations.Conversations are generated in a simulation environment, lever-aging the generative capabilities of GPT. We simulate customer andassistant interactions on three levels: (i) template-based customerinteractions, (ii) LLM-based customer given the description of anitem to purchase, (iii) LLM-based customer given an abstract themeto shop for. The assistant is in all cases a production-level GPT-3.5model prompted to aid customers through their fashion shoppingjourney.On a high level, customer agents are given either an item de-scription or a theme to shop for. Item descriptions are generatedvia the combination of attribute values and apparel types, such ascolour, type, material, fit, brand and size of item (for further detailson attributes, we refer the reader to .1.1). An example itemdescription is: black utility Carhartt trousers. In this example, thefollowing attribute values were given: {colour: black, genre: utility,brand: Carhartt, material: null, fit: null, size: null} for the appareltype trousers.For a template-based customer agent (i), a single messageis formatted using the template {I would like|I am looking for}item_description. Subsequently, this message is sent to the as-sistant agent and the response recorded as a single-interactionconversation. With the template-based customer agent we intro-duce conversations that are simple and brief, evaluating whether anexamined assistant agent can fulfill their task for basic interactions.With the LLM-based customer agents (ii & iii), we promptGPT instances2 to simulate more complex and interactive conversa-tions. The customer agent is given either the description of an item",
  "Specifically, gpt-3.5-turbo-0613 models": "or a theme to shop for and is instructed to interact with the shop-ping assistant to complete their shopping trip. To simulate differentcustomer behaviors for the LLM-based agents, different promptswere used, covering an array of customer behaviors and personali-ties. Agents are prompted to behave based on three personalitytraits: Casual: You are a CUSTOMER who keeps things simple. Youdo not write too much, you write just enough. You are awarethat you are speaking with a chat bot and not a human. Youuse the chat bot as a tool to get the item you need. Indecisive: You are an indecisive and timid CUSTOMER. Youdont really know what you want. Take your time whenmaking a decision and wait for the assistant to ask you forinformation before you describe fully what you want. Rude: You are a rude CUSTOMER. You are annoyed at thechat bot. Nevertheless, you want to buy clothes so you arenegotiating with the assistant to find something. Try to findsomething, you are impatient and you dont want the con-versation to go on for long.The prompt given to customer agents is given in . You an expert CUSTOMER simulator for quality controlpurposes. You are helping to test the fashion assistantchatbot. The chatbot has been trained to guide customersto buy clothes that match what they are looking for.Pretend you are a customer with a personality and traitsthat adhere to the following description: {personality}.You are looking to buy an item that fits the followingdescription: {item}",
  ": Prompt used for the LLM-based customer agent": "All types of customer agents can message only in text form,asking questions, requesting more information, declaring interestin presented items, describing their wants, among other actionsthat relate to their goal. The assistant agent can reply in one of twoways: (i) message in text form to provide details or ask clarificationquestions to identify the needs of the customer, or (ii) query abackend search engine to present a carousel of items to the customer.The conversation ends on the side of the customer agent, which isgiven the agency to end the conversation if a relevant item is foundor if there is no more progress towards finding a relevant item. Allmessages and actions taken are recorded.Sample data is shown in . Four examples are shown, inthree of them the customer agent C is given the description of anitem, while in the last one the agent is given an open-ended theme.In two of the three item-based descriptions, the customer has givenall the information required for a correct query generation. Theassistant agent A interacts with the customer to identify their needsand to show them [ITEMS] relevant to their needs. In the slimLevis jeans\" example, the customer gives information little by little,and thus the assistant needs to show three different sets of itemsto the customer. We record the search queries generated by theassistant for each of these three instances.3.1.3Manual Verification of Conversations.For the verification of the quality of the generated dataset, wewent over every conversation to ensure high standards of quality.",
  "True": "party in greektavernaC: I am going to a Greek taverna soon and I wouldlike to buy some clothes for the occasion.A: Are you looking for a specific type of outfit forthe party in the Greek taverna? For example, a casualshirt and shorts, or something more formal like alinen suit?C: I am looking for a more casual lookA: I found some casual outfit options for the party inthe Greek taverna: [ITEMS]",
  "Error messages present in a conversation precipitate thedeletion of the conversation": "Incorrect customer behavior is a cause for deletion. If thecustomer agent does not behave as specified in the prompt,the conversation will be deleted. For example, if the customeris prompted to behave in a rude manner but instead behavesneutrally, the conversation is considered invalid. Unfaithful conversations are removed. Oftentimes the cus-tomer will not strictly stick to the given item or theme de-scription and will instead ask for incorrect details or declareinterest in an item that does not fit the description. The cus-tomer agent needs to stay faithful to the original description,otherwise the whole conversation will be removed. To expand our dataset to more languages, we translate thecurated conversations to German, French and Greek. We useGPT-4 to translate both customer and assistant messages. The restof the attributes, such as queries, remain the same (we assumesearch engines take in English queries as input and therefore wedo not translate them).For German and French we translated all messages and thenproceeded to verify 100 randomly-selected conversations for eachlanguage. Performance on these examples was deemed acceptableand thus we included all 9k translated messages of each language.For Greek, we found translation performance to be subpar. There-fore, we do not evaluate on the entire dataset, but instead a smaller,more carefully curated set of 200 messages. We consider the Greeksubset of our data as a small-scale evaluation case to investigatemodel performance in lower-resource languages.",
  "(vii) queries: The queries generated by the assistant.(viii) action: The action taken by the assistant. Can either be mes-sage or search": "We report dataset statistics in . Statistics are comparableacross languages (excluding data size, where the Greek set waskept small). There are slightly more tokens in German than theother languages, both for customer and assistant messages. Eachconversation has approximately six messages in total (includingcustomer and assistant messages), with around 66% of all assistantactions being search. Note that since the German and French setswere translated from the English set, volume statistics (such asnumber of messages) are the same.",
  "Evaluation Details": "For the evaluation of performance in the case where thecustomer agent is given the description of an item (both inthe AssistantEval and QueryGenEval tasks), we comparethe expected query (i.e., item description) with the output queryusing BERTScore . BERTScore is a method proposed to evaluatenatural language generation tasks via leveraging the capabilities ofBERT to produce contextualized embeddings.",
  ": The number of times each fashion attribute appearsin the English dataset": "For the evaluation of performance in the case where thecustomer agent is given the description of a theme, word-levelsimilarity metrics (such as BERTScore) cannot be used since thereference text is a free-form description instead of specific key-words. Instead, we evaluate generated queries in a setup based onsemantic similarity. Using contextualized embeddings, we computethe representation of the generated queries as well as the open-ended theme descriptions. The contextualized embeddings we useare text-embedding-ada-002 from OpenAI.3 We assign a query tothe open-ended theme with the highest cosine similarity and wecalculate how many correct assignments are made.",
  "Models": "We evaluate a series of models using our dataset. Llama2 is anopen-source large language model pretrained on publicly availabledata. We evaluate the llama-2-7b-chat variant, with 7B parameters.Mistral is another open-source model pretrained on publiclyavailable data and further finetuned for dialogue, shown to out-perform LLama2-13B in multiple tasks. We evaluate the Mistral-7B-Instruct-v0.1 model, with 7B parameters. GPT-based models4 is a large language model and a service that has revolutionizedacademia and industry alike. Two models are evaluated: gpt-3.5-turbo-0613 and gpt-4-0613. In total, we compare three differentprompts for GPT-3.5 to evaluate a range of behaviors. One instanceis prompted to simply act as an assistant (I), another instance isprompted to actively include specific terms the customer mentions(II) and a third is prompted to both include specific terms as usedby the customer and to actively ask questions to better identify thewants of the customer (III). Full prompts are given in . Wecompare these large language models with a low-cost, off-the-shelfunsupervised keyword extractor, Yake . Yake is a keyword ex-tractor based on statistical text features, trained as a domain- andlanguage-agnostic extractor, allowing us to use the tool across allour examined languages. Finally, as a lower bound we provide apopularity and a random baseline. For the popularity baseline, weassume a model that always generates the query for the popularblack Nike shoes item (translated to German, French and Greek).For the random baseline, we assume a model that as a query alwaysgenerates Lorem ipsum dolor sit amet (in the Greek alphabet forthe Greek set).",
  "ModelPrompt": "Commercial Fashion Assistant / GPT-4/ GPT-3.5 (I) / Llama2 / MistralYou are the Fashion Assistant, responding to customers questions related to fashion, like afashion assistant in a store. You speak language when interacting with the customer.Your limitations: (i) You dont have access to the customers basket, wishlist, order history,order status or profile information like size profile or favourite brands. (ii) You dont use URLsbut describe the customer step by step what to do. (iii) You dont have access to deals.To aid the customer, you generate queries for a search engine at points in the conversationwhere this is useful. GPT-3.5 (II)You are the Fashion Assistant, responding to customers questions related to fashion, like afashion assistant in a store. You speak language when interacting with the customer.Your limitations: (i) You dont have access to the customers basket, wishlist, order history,order status or profile information like size profile or favourite brands. (ii) You dont use URLsbut describe the customer step by step what to do. (iii) You dont have access to deals.To aid the customer, you generate queries for a search engine at points in the conversationwhere this is useful. Make sure the queries contain all keywords and specific terms the customerused. GPT-3.5 (III)You are the Fashion Assistant, responding to customers questions related to fashion, like afashion assistant in a store. You speak language when interacting with the customer.Your limitations: (i) You dont have access to the customers basket, wishlist, order history,order status or profile information like size profile or favourite brands. (ii) You dont use URLsbut describe the customer step by step what to do. (iii) You dont have access to deals.To aid the customer, you generate queries for a search engine at points in the conversationwhere this is useful. Make sure the queries contain all keywords and specific terms the customerused.If the customer has not given you a lot of specifics such as colour, brand, etc., ask the customerto give you more information about their wants so that you can guide them better.",
  ": Prompts for assistant agents used in this study": "For the engineering of prompts, a smaller set of 1062 Englishconversations was used for the QueryGenEval task, separate fromthe evaluation set. All results shown in this work come from theevaluation set. In the development set, we did not use the full setof attributes and values, to ensure substantial difference in contentbetween development and evaluation set. Namely, we only used theattributes: colour (black, white, red), type (utility, athletic, formal),brand (Converse, Nike, Pier One) and size (S, M, L).",
  "AssistantEval": "We showcase () how we can use our dataset to benchmarkconversational agents as fashion shopping assistants for the Assis-tantEval task, evaluating the capabilities of models to translatethe wants of a customer into concrete queries to interface with abackend search engine.We found that the open-source models were not able to con-sistently interact with the customer agent. Often there would berole-switching (i.e., the assistant agent would behave as a customer), incorrect formatting of responses (e.g., the assistant agent wouldadd ASSISTANT: as a message prefix) or skipping query genera-tion. Our findings corroborate previous work showing how LLMs(especially open-source ones) struggle with producing formattedoutput . Deployment of open-source LLMs currently re-quires significant engineering efforts, and we are instead focusingon GPT-based models.GPT-4 performs the best for all languages, although GPT-3.5(III) performs competitively, potentially because the prompt forthat model was engineered to instruct the model to actively askquestions about specific attributes. While the other two models(GPT-3.5 (I) and (III)) are competitive in German, French and Greek,they are considerably worse for English, where GPT-4 and the morefinely-engineered GPT-3.5 (III) are superior. In we show thecorrelation between the total cost5 of a single run of evaluation andthe performance of each model. As expected GPT-4 performs thebest, although at a higher cost.6 4.1.1Fashion Attribute Performance Analysis.We perform a qualitative analysis of performance of GPT-4 (ourbest performing model) across fashion attributes. In developing auseful assistant in the fashion domain, analysing how it performsfor different attributes is vital. For this analysis, we compute theprecision of exact matches between output queries and the input",
  ": Precision of exact matches of fashion attribute valuesin output queries": "Performance for all three languages follows similar patterns, al-though performance in German is worse across almost all attributes.In attributes with clear definitions, such as colour, material andfit, performance is adequate, while for attributes such as type andapparel, which are more open-ended (e.g., type can be describedabstractly with terms such as summer or utility, and apparelcan have overlap between terms, such as jacket and coat), per-formance is worse. Notable exception is size, with very low per-formance, potentially because of a deficiency of GPT to extractcorrect size labeling (size is often denoted with a single numberor a letter, such as S or 42). Another exception is the clearerbrand attribute, where performance is also low. We hypothesizethe low performance for brands is because zero-shot named entityrecognition with GPT is subpar .4.1.2Open-ended Themes.We present a comparison of GPT models in aiding customersshop for a theme instead of concrete item descriptions.7 We assignoutput queries to the theme with the highest cosine similarity basedon text-embedding-ada-002 representations, as described in .3. We report the Precision@3 score in . GPT-4 performsthe best overall, especially in the English set, while for French andGerman performance is competitive for many models. Performancein the other languages is more uniform. In we show thatGPT-4 performs the best, albeit at a higher cost.",
  "QueryGenEval": "Evaluating the query generation capabilities of tools is crucial inthe development of assistant agents. If a customer clearly describeswhat they are looking for, any agent should be able to generateappropriate queries. In this subtask, we assume that the customerhas given all the information necessary for a successful interaction.In our dataset, we manually mark conversations that contain allthe necessary information (i.e., fashion attributes). We perform acomparison of model performance for conversations that containall the attributes mentioned in the item description. Namely, eachmodel is given as input the entire conversation and generates asingle query which is compared to the item description as usual. Wemeasure the F1 score as produced by BERTScore and show resultsin .GPT-4 performs the best in English by a margin of 1.2. The gapbetween GPT-3.5 and GPT-4 narrows in German and GPT-3.5 per-forms marginally better in French and Greek. From the open-sourcemodels, Llama2 performs the best in English, while performance inthe other languages is similar between the open-source LLMs andYake, the unsupervised keyword extraction tool. We hypothesizethat Yake performs similarly to open-source LLMs since it has beenspecifically developed to extract keywords, a task pivotal for querygeneration.In , we show a cost analysis (in dollars) for evaluatingYake, Llama2, Mistral, GPT-3.5 and GPT-4. Yake is an unsupervisedmodel that can be run with (virtually) no cost on a CPU. We runLlama2 and Mistral on an Amazon Sagemaker instance8, costing0.399 dollars per hour.9 The GPT models are called through anAPI with an associated cost. In the analysis, we show performanceof each model and how much it costs in total to evaluate for theQueryGenEval task in English. While GPT-4 performs the best, itis also the most expensive. Llama2 and Mistral perform similarly,with GPT-3.5 being the second cheapest model while at the sametime being the second in performance.",
  "Ethical Considerations": "In this section we consider the ethical implications of our work.While on a direct level our work does not impact society or peo-ples, since we do not involve human participants in our study andwe do not examine human-generated data, our work could havelarger-scale implications. Generative AI poses a threat to workersacross fields of occupation, replacing workers output with cheaper",
  "Conclusion": "With the recent transformation of the field of NLP ushered in bythe advent of large language models, a series of breakthroughsin many domains is taking place. To ensure the productive anditerative application of LLM-powered agents, efficient and scaleableevaluation is vital.In our work, we focus on the evaluation of the capabilities of as-sistant agents to interface between customers and a backend searchengine. Specifically, we showcase an application of our evaluationmethodology and framework on the domain of online fashion retail,where LLM-powered assistants can aid customers in their shoppingtrip, answering questions, recommending products and interactingwith the customer.Our evaluation aims at evaluating these capabilities across lan-guages. Data is generated via a simulation environment where acustomer and an assistant agent are interacting with each other,with subsequent messages and interactions of the agents verifiedmanually for high-quality. We propose the use of our multilingual",
  "IBM replaces jobs with AI": "dataset to evaluate fashion assistants via the replay of conversa-tions, ensuring evaluation that is fair and reproducible, and performa comparison of a series of models, from open- to closed-source.With our dataset and methodology, we facilitate the iterative devel-opment of assistant agents in a business setting. Zahra Abbasiantaeb, Yifei Yuan, Evangelos Kanoulas, and Mohammad Alianne-jadi. 2023. Let the LLMs Talk: Simulating Human-to-Human Conversational QAvia Zero-Shot LLM-to-LLM Interactions. arXiv:2312.02913 [cs.CL] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, NovaDasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, NicholasJoseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, NelsonElhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston,Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, TomBrown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan.2022. Training a Helpful and Harmless Assistant with Reinforcement Learningfrom Human Feedback. arXiv:2204.05862 [cs.CL] Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An Automatic Metric for MTEvaluation with Improved Correlation with Human Judgments. In Proceedingsof the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MachineTranslation and/or Summarization, Jade Goldstein, Alon Lavie, Chin-Yew Lin,and Clare Voss (Eds.). Association for Computational Linguistics, Ann Arbor,Michigan, 6572. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, BenjaminChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, IlyaSutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.arXiv:2005.14165 [cs.CL] Sbastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, EricHorvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, HarshaNori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. 2023. Sparks of ArtificialGeneral Intelligence: Early experiments with GPT-4. arXiv:2303.12712 [cs.CL] Pawe Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, Iigo Casanueva,Stefan Ultes, Osman Ramadan, and Milica Gai. 2018. MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling.",
  "What should I wear to a party in a Greek taverna? Evaluation for Conversational Agents in the Fashion Domain GenAI Evaluation KDD 24, August 2526, 2024, Barcelona, Spain": "In Proceedings of the 2018 Conference on Empirical Methods in Natural LanguageProcessing, Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii(Eds.). Association for Computational Linguistics, Brussels, Belgium, 50165026. Giovanni Campagna, Agata Foryciarz, Mehrad Moradshahi, and Monica Lam.2020. Zero-Shot Transfer Learning with Synthesized Data for Multi-Domain Dia-logue State Tracking. In Proceedings of the 58th Annual Meeting of the Associationfor Computational Linguistics, Dan Jurafsky, Joyce Chai, Natalie Schluter, andJoel Tetreault (Eds.). Association for Computational Linguistics, Online, 122132. Ricardo Campos, Vtor Mangaravite, Arian Pasquali, Alpio Jorge, Clia Nunes,and Adam Jatowt. 2020. YAKE! Keyword extraction from single documentsusing multiple local features. Information Sciences 509 (2020), 257289.",
  "Paul Christiano, Buck Shlegeris, and Dario Amodei. 2018. Supervising stronglearners by amplifying weak experts. arXiv:1810.08575 [cs.LG]": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding. InProceedings of the 2019 Conference of the North American Chapter of the Associationfor Computational Linguistics: Human Language Technologies, Volume 1 (Long andShort Papers), Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Associationfor Computational Linguistics, Minneapolis, Minnesota, 41714186.",
  "Tom Hosking, Phil Blunsom, and Max Bartolo. 2024. Human Feedback is notGold Standard. arXiv:2309.16349 [cs.CL]": "David M. Howcroft, Anya Belz, Miruna-Adriana Clinciu, Dimitra Gkatzia, Sadid A.Hasan, Saad Mahamood, Simon Mille, Emiel van Miltenburg, Sashank Santhanam,and Verena Rieser. 2020. Twenty Years of Confusion in Human Evaluation: NLGNeeds Evaluation Sheets and Standardised Definitions. In Proceedings of the 13thInternational Conference on Natural Language Generation, Brian Davis, YvetteGraham, John Kelleher, and Yaji Sripada (Eds.). Association for ComputationalLinguistics, Dublin, Ireland, 169182. Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and LarryHeck. 2013.Learning Deep Structured Semantic Models for Web Searchusing Clickthrough Data. ACM International Conference on Informationand Knowledge Management (CIKM). Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, De-vendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,Guillaume Lample, Lucile Saulnier, Llio Renard Lavaud, Marie-Anne Lachaux,Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothe Lacroix,and William El Sayed. 2023. Mistral 7B. arXiv:2310.06825 [cs.CL] Harry H. Jiang, Lauren Brown, Jessica Cheng, Mehtab Khan, Abhishek Gupta,Deja Workman, Alex Hanna, Johnathan Flowers, and Timnit Gebru. 2023. AI Artand its Impact on Artists. In Proceedings of the 2023 AAAI/ACM Conference onAI, Ethics, and Society (<conf-loc>, <city>Montral</city>, <state>QC</state>,<country>Canada</country>, </conf-loc>) (AIES 23). Association for Comput-ing Machinery, New York, NY, USA, 363374. Daniel Khashabi, Gabriel Stanovsky, Jonathan Bragg, Nicholas Lourie, JungoKasai, Yejin Choi, Noah A. Smith, and Daniel Weld. 2022. GENIE: Toward Re-producible and Standardized Human Evaluation for Text Generation. In Pro-ceedings of the 2022 Conference on Empirical Methods in Natural Language Pro-cessing, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Associationfor Computational Linguistics, Abu Dhabi, United Arab Emirates, 1144411458. Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, Md Amran HossenBhuiyan, Shafiq Joty, and Jimmy Huang. 2023. A Systematic Study and Com-prehensive Evaluation of ChatGPT on Benchmark Datasets. In Findings of theAssociation for Computational Linguistics: ACL 2023, Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics,Toronto, Canada, 431469. Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu,Zhoujun Li, Fei Huang, and Yongbin Li. 2023. API-Bank: A ComprehensiveBenchmark for Tool-Augmented LLMs. In Proceedings of the 2023 Conference onEmpirical Methods in Natural Language Processing, Houda Bouamor, Juan Pino,and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore,31023116.",
  "Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries.In Text Summarization Branches Out. Association for Computational Linguistics,Barcelona, Spain, 7481": "Li Lucy and David Bamman. 2021. Gender and Representation Bias in GPT-3 Gen-erated Stories. In Proceedings of the Third Workshop on Narrative Understanding,Nader Akoury, Faeze Brahman, Snigdha Chaturvedi, Elizabeth Clark, Mohit Iyyer,and Lara J. Martin (Eds.). Association for Computational Linguistics, Virtual,4855. Qingsong Ma, Johnny Wei, Ondej Bojar, and Yvette Graham. 2019. Results ofthe WMT19 Metrics Shared Task: Segment-Level and Strong MT Systems PoseBig Challenges. In Proceedings of the Fourth Conference on Machine Translation(Volume 2: Shared Task Papers, Day 1), Ondej Bojar, Rajen Chatterjee, Chris-tian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck,Antonio Jimeno Yepes, Philipp Koehn, Andr Martins, Christof Monz, MatteoNegri, Aurlie Nvol, Mariana Neves, Matt Post, Marco Turchi, and Karin Ver-spoor (Eds.). Association for Computational Linguistics, Florence, Italy, 6290. Ian Magnusson, Akshita Bhagia, Valentin Hofmann, Luca Soldaini, Ananya HarshJha, Oyvind Tafjord, Dustin Schwenk, Evan Pete Walsh, Yanai Elazar, Kyle Lo,Dirk Groeneveld, Iz Beltagy, Hannaneh Hajishirzi, Noah A. Smith, Kyle Richard-son, and Jesse Dodge. 2023. Paloma: A Benchmark for Evaluating LanguageModel Fit. arXiv:2312.10523 [cs.CL] Nitika Mathur, Timothy Baldwin, and Trevor Cohn. 2020.Tangled up inBLEU: Reevaluating the Evaluation of Automatic Machine Translation Eval-uation Metrics. In Proceedings of the 58th Annual Meeting of the Association forComputational Linguistics, Dan Jurafsky, Joyce Chai, Natalie Schluter, and JoelTetreault (Eds.). Association for Computational Linguistics, Online, 49844997. OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, IlgeAkkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, SamAltman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Bal-com, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello,Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff,Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks,Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brit-tany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, FotisChantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, BenChess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, JeremiahCurrier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, DamienDeville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet,Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simn PosadaFishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson,Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gor-don, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang ShaneGu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton,Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele,Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, ShantanuJain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, DennyJin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, ukasz Kaiser, AliKamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kil-patrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner,Jamie Kiros, Matt Knight, Daniel Kokotajlo, ukasz Kondraciuk, Andrew Kon-drich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, MichaelLampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak MingLi, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, RyanLowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov,Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew,Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, DavidMedina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, PamelaMishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati,Oleg Murk, David Mly, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, ArvindNeelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen OKeefe, JakubPachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascan-dolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng,Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Pondede Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, TollyPowell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford,Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach,Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders,Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schul-man, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker,Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, KatarinaSlama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Pet-roski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B.Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle,Nick Turley, Jerry Tworek, Juan Felipe Cern Uribe, Andrea Vallone, Arun Vi-jayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang,Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter",
  "GenAI Evaluation KDD 24, August 2526, 2024, Barcelona, SpainMaronikolakis et al": "Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter,Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, MichaelWu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba,Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng,Juntang Zhuang, William Zhuk, and Barret Zoph. 2024. GPT-4 Technical Report.arXiv:2303.08774 [cs.CL] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu:a Method for Automatic Evaluation of Machine Translation. In Proceedings ofthe 40th Annual Meeting of the Association for Computational Linguistics, PierreIsabelle, Eugene Charniak, and Dekang Lin (Eds.). Association for ComputationalLinguistics, Philadelphia, Pennsylvania, USA, 311318.",
  "Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddingsusing Siamese BERT-Networks. arXiv:1908.10084 [cs.CL]": "Joo Sedoc, Daphne Ippolito, Arun Kirubarajan, Jai Thirani, Lyle Ungar, and ChrisCallison-Burch. 2019. ChatEval: A Tool for Chatbot Evaluation. In Proceedings ofthe 2019 Conference of the North American Chapter of the Association for Compu-tational Linguistics (Demonstrations), Waleed Ammar, Annie Louis, and NasrinMostafazadeh (Eds.). Association for Computational Linguistics, Minneapolis,Minnesota, 6065. Naeha Sharif, Lyndon White, Mohammed Bennamoun, and Syed Afaq Ali Shah.2018. Learning-based Composite Metrics for Improved Caption Evaluation.In Proceedings of ACL 2018, Student Research Workshop, Vered Shwartz, JeniyaTabassum, Rob Voigt, Wanxiang Che, Marie-Catherine de Marneffe, and MalvinaNissim (Eds.). Association for Computational Linguistics, Melbourne, Australia,1420. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-mine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-ale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucu-rull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, CynthiaGao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini,Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, IsabelKloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, ThibautLavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet,Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton,Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva,Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, RossTaylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov,Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Ro-driguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2:Open Foundation and Fine-Tuned Chat Models. arXiv:2307.09288 [cs.CL]"
}