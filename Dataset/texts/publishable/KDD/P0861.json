{
  "Abstract": "Textual description of a physical location, commonly known as anaddress, plays an important role in location-based services(LBS)such as on-demand delivery and navigation. However, the preva-lence of abnormal addresses, those containing inaccuracies that failto pinpoint a location, have led to significant costs. Address rewritinghas emerged as a solution to rectify these abnormal addresses. De-spite the critical need, existing address rewriting methods are limited,typically tailored to correct specific error types, or frequently requireretraining to process new address data effectively. In this study, weintroduce AddrLLM, an innovative framework for address rewrit-ing that is built upon a retrieval augmented large language model.AddrLLM overcomes aforementioned limitations through a meticu-lously designed Supervised Fine-Tuning module, an Address-centricRetrieval Augmented Generation module and a Bias-free ObjectiveAlignment module. To the best of our knowledge, this study pioneersthe application of LLM-based address rewriting approach to solvethe issue of abnormal addresses. Through comprehensive offline test-ing with real-world data on a national scale and subsequent onlinedeployment, AddrLLM has demonstrated superior performance in in-tegration with existing logistics system. It has significantly decreasedthe rate of parcel re-routing by approximately 43%, underscoring itsexceptional efficacy in real-world applications.",
  "Computing methodologies Natural language processing; Information systems Query reformulation": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from , July 2017, Washington, DC, USA 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-x-xxxx-xxxx-x/YY/MM",
  "Introduction": "Addresses are crucial for logistics, ensuring the smooth operation ofbusiness by facilitating accurate and efficient delivery processes. Incountries like India and China , the prevalence of inaccurateor abnormal addresses poses a significant challenge. This issue arisesfrom inadequate address regulatory frameworks, intricate addressstructures , and click farming fraud . Abnormal Chineseaddresses, defined as those that cannot be parsed into the standardhierarchy (Appendix A), often include errors such as missing ad-ministrative regions, nested addresses, unofficial aliases, irrelevantwords, and misspellings (Appendix B). An illustrative example is anaddress that conflates Beijing and Nanjing, which we term a nestedaddress. These are often exploited for region-specific discounts butlead to unreliable outcomes from Location-Based Services (LBS)due to their lack of systematic recording in databases.This issue significantly impacts companies like JD Logistics, oneof the largest logistics companies in the world, which faces around25,000 daily re-routing events caused by abnormal addresses. Thesemisrouted parcels, resulting from addresses that dispatch parcelsto the wrong delivery stations, lead to additional transfers and re-routing, depicted in (red arrow). This process incurs annuallosses exceeding $2 million for JD Logistics. Address rewriting, acritical procedure that aligns user-submitted addresses with standard-ized formats, can significantly reduce dispatching errors. Empiricalevidence suggests that refining user-provided addresses throughrewriting, as illustrated in (blue arrow), can substantiallydiminish these errors. Therefore, developing a comprehensive and",
  ": Parcel dispatching, re-routing and address rewriting.Red arrow: abnormal address results in parcel re-routing. Bluearrow: address rewriting prevents re-routing": "Address rewriting is a special case within the broader field ofquery rewriting, which has gained prominence and widespread ap-plication across e-commerce , question-answering sys-tem , code search . Unlike e-commerce queriesand natural language questions, addresses posses a more structuredformat and are closely tied to geographical information. Conversely,they are less structured than code and embody more natural lan-guage semantics, positioning address rewriting as a distinct categorywithin the spectrum of rewriting tasks. Due to the close associa-tion of address rewriting with business-oriented applications such asgeocoding, navigation and point-of-interest(POI) matching, whichmay intersect with proprietary algorithms at the heart of relatedcompanies main operations, there is a scarcity of published stud-ies in this particular area. Like query rewriting, existing literatureon address rewriting can be classified into approaches based onStatistical Machine Translation(SMT) or Neural Machine Trans-lation(NMT). SMT-based methods leverage statistical models totransform an original address into its modified counterpart. Suchmethods often encounter performance constraints due to the limitedrepresentation capabilities of statistical models as observed in theearly Bing Maps address rewriting system outlined in . NMT-based methods employ an encoder-decoder architecturewhere the encoder maps original address into a latent representation,and the decoder translates this representation into the modified ad-dress. Furthermore, some works focus on extracting relatedinformation for address rewriting, which can be viewed as prerequi-site task of address rewriting. However, require re-trainingwhen new addresses are added into database, a common occurrencein the industry. necessitates explicit geographical knowledgefor generating meaningful embeddings, which may not always bereadily available. are constrained by its focus on partic-ular types of address anomalies, such as alternations, spell mistakesand alias, which represent only a fraction of the errors encounteredreal-world data stream, as illustrated in Appendix B. Consequently,there is a pressing need for a robust framework for address rewrit-ing that can operate effectively with the textual address data alone,handle adequate errors of address in a unified model and does notnecessitate constant retraining with the addition of new addresses. Retrieval-augmented Large Language Model is a potential solu-tion to the constrained scope problem and retraining problem. Firstly,with the development of LLM techniques, reasoning ca-pability of LLM shows the potential to rectify extensive errors ofaddresses, even those never considered in previous works(,in a unified model. Secondly, LLM itself has the same retrainingproblem as previous works. To solve this, Retrieval Augmented Gen-eration(RAG) decouples knowledge storage and reasoning capabilityof LLM , making it possible to generate updated answerswithout retraining LLM, when new knowledge appears.However, utilizing RAG-based LLM to solve address rewritingproblem is still challenging, because: (1) current public LLMs lackaddress comprehension and related knowledge; (2) current RAGframeworks are mainly designed for question-answering tasks andnatural language paragraphs data, whose semantics diverge far fromaddresses; (3) current objective alignment methods are mainly basedon trainable reward model, which involves additional training effortsand may introduce bias and inaccuracies.To solve these challenges, in this paper, we propose AddrLLM,an address rewriting framework based on retrieval augmented largelanguage model. We build a RAG framework specifically for addressdata, perform supervised fine-tuning for LLM on logistics relatedtasks, and design a bias-free and large-scale objective alignmentframework based on JDs LBS system.In summary, the main contributions of this work are as follows: We are the first to explore the possibility of utilizing retrieval aug-mented large language model to rewrite address and subsequentlydeploy in real-world data stream. We design a novel LLM-based framework, AddrLLM, integratingmulti-instruction supervised fine-tuning, bias-free objective align-ment and address-centric retrieval augmented generation modules, tosolve challenges associated with adopting LLM to address rewriting. By offline experiments, AddrLLM rectifies 43.9% abnormal ad-dresses, and outperforms SoTA methods by 24.2%. AddrLLM hasbeen deployed at JD Logistics in Zhejiang province for over fourmonths, and effectively reduced over 40% parcel re-routing causedby abnormal addresses among around 2 million daily parcels. 2MethodAddress rewriting aims to refine user-input addresses into a standard-ized format that aligns with the users original intent. We introducea comprehensive framework for LLM-based address rewriting, Ad-drLLM, depicted in . AddrLLM is composed of three keycomponents: Supervised Fine-tuning(SFT) module, Address-centricRetrieval Augmented Generation(RAG) module and Bias-free Ob-jective Alignment module. Initially, we leverage JDs sophisticatedLocation-Based Services(LBS) system to collect an extensive, high-quality dataset specifically for the SFT task associated with addressrewriting. Subsequently, we design the objective alignment moduleto further calibrate the generation of rewrites to desired results. Toprevent potential bias arising from reward model or manual annota-tion, we integrate the LBS system, which provides bias-free feedbackthat is directly derived from the models performance in rewritingtask. Finally, to enhance the rewriting process, we develop a cus-tomized RAG module designed to enrich the LLM with contextualinformation via targeted retrieval of relevant addresses.",
  "Multi-instruction Supervised Fine-tuning": "Because the semantics of address significantly diverges from thepre-training corpus of LLMs, directly employing these models foraddress processing may result in inaccuracies. To solve this chal-lenge, we adopted a strategy that involves aggregating a range oftasks related to address to fine-tune LLMs, thereby improving theirproficiency in understanding standard Chinese addresses. Here wedescribe the tasks and datasets for SFT. The prompt design for thesetasks are detailed in Appendix C. Address Parsing: Address parsing task involves the process ofbreaking down an address into its constituent components. Thestructure of a Chinese address is detailed in Appendix A. Our addressparsing dataset comprises 20 million <address, address components>pairs obtained from JDs LBS system. Fine-tuning LLMs on theaddress parsing task is likely to help these models understand the structure of standard addresses, enabling them to generate addressesthat conform to the standard hierarchy. Address Entity Prediction(AEP): Address Entity Prediction is toinfer absent administrative region in address. In logistics system,user-provided addresses frequently lack administrative regions, pos-ing significant challenges to package dispatching. Thus, addressentity prediction or filling is an important ability of address rewrit-ing model. For the AEP task, we collect 20 million addresses fromhistorical delivery orders in JD, and randomly delete administra-tive regions. Then the address rewriting model needs to predict themissing address entity. Address Rewriting: Our address rewriting dataset is generatedfrom two sources. The primary source is the JD LBS system. Whena user inputs an address, the LBS system may do some basic rewrit-ing, such as misspelling correction. We collect user-input addressesand rewritten ones and obtain 15 million samples from LBS system.The second source is address recommendation system integrated onJD e-commerce platform. When a user places an order with deliv-ery address on JD e-commerce platform, the system recommendssome related addresses from our standard address database. If userchooses to replace original address with a recommended address, wekeep a record. However, this data can be noisy because the originaladdress provided by user may only contain some keywords. Thus,we filter records by feeding original addresses into JD LBS system,and if geocoding succeeds(geocoding doesnt return error althoughreturned coordinates may be incorrect), we can view original addressas a complete address and put corresponding record into our addressrewriting dataset. Following this process, 5 million samples are col-lected by the second source. In total, our address rewriting datasetcontains 20 million samples. This dataset includes 77.8% samplesthat do not involve any rewriting, i.e. original address and rewrittenaddress are the same.Finally, address parsing, address entity prediction and addressrewriting datasets are mixed together to form the SFT dataset . Multi-instruction Supervised Fine-tuning (SFT): The process ofgenerating text using large language model can be viewed as auto-regressive sampling. In auto-regressive language generation, eachword is predicted one at a time, and each prediction conditions on theprompt and previously generated words. Given model input (prompt)x and standard output y, the training objective is to find the modelparameters that maximizes the conditional probability (|) ==1 ( |0:1,). The training objective can be formulated as:",
  "Bias-free Objective Alignment": "Through prior multi-instruction SFT, AddrLLM has developed under-standing of standard Chinese addresses. Because address rewritingSFT dataset is generated based on JD LBS system, i.e. these abnor-mal addresses can be identified and rectified by current system, itindicates that these abnormal addresses have different distributionfrom those cannot be rectified by LBS system. As a result, Ad-drLLMs current skill set does not extend to effectively rewritingaddresses that fall outside the correction capabilities of the JD LBS",
  "Conference17, July 2017, Washington, DC, USAQinchen Yang, Zhiqing Hong, Dongjiang Cao, Haotian Wang, Zejun Xie, Tian He, Yunhuai Liu, Yu Yang, and Desheng Zhang": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Flo-rencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, ShyamalAnadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774(2023). Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan,Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, RunjiLin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men,Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, PengWang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang,Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, ZhengYuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, ChangZhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen TechnicalReport. arXiv preprint arXiv:2309.16609 (2023).",
  "| || | , is semantics": "embedding model. At experiments, we choose pre-trained modelBERT as .Secondly, the rewritten address should be semantically close tothe address obtained by reverse-geocoding on delivery coordinates.The result of reverse-geocoding may not be exact, for example,the room number or building number may be incorrect. However,the rewritten address should be semantically close to that addressto some extent, for example, they are in the same community orroad. To directly measure the rewritten address, we design reversegeocoding score (,), formulated as:(,) = ( (), (()))(3)where is rewritten address, is coordinates of successful delivery, is reverse-geocoding service, is semantics embeddingmodel.Thirdly, we choose geocoding task to evaluate the rewriting result.Intuitively, the geocoding result of rewritten address should be closeto the coordinates of successful delivery. Thus, we design geocodingscore (,), formulated as:",
  "(4)": "where is Euclidean distance function, is JD geocod-ing system that maps from address to coordinates. We normalizegeocoding score to by weights 1 and 2. In experiments, weset 1 as 100 meters and 2 as 1000 meters. When the rewrittenaddress cannot be recognized by JD geocoding service as an address(geocoding failure), the geocoding score is 0.Finally, three scores are added together with weights 1,2,3: (,,) = 1(,) + 2(,) + 3(,)(5)In experiments, 1,2 and 3 are set as 0.2,0.2 and 0.6 respectively.RL Task Formulation: At each time step , large language model generate next token as action , based on current state , whichincludes already generated tokens. Then the model obtains an imme-diate reward by a rewarding function : R. The detailedMarkov Decision Process(MDP) formulation is in Appendix D.Training: We adopt Proximal Policy Optimization(PPO) tooptimize the large language model. The PPO algorithm can be for-mulated as:maxE(, ) [min{, (,),",
  "(6)": "where is parameters of fixed policy, is parameters of updatedpolicy, the clip function (,, 1 , 1 + ) limits the ratio , tothe range[1 , 1 + ]. is advantage function, which is formulatedbased on the estimation of value network . The value network is initialized from the policy network 0. The formulation followsGeneralized Advantage Estimation(GAE) : = (,) + (+1) ()",
  "(9)": "where is sampling set, is step numbers.For the objective alignment, we utilize 4 million <address, loca-tion> samples, where address is the user-input address and locationis the delivery coordinates reported by courier. The groundtruthaddress is unknown to us. The objective alignment module guidesLLM to learn how to rewrite these user-input addresses to standardones.",
  "Address-centric RAG": "Large Language Models (LLMs) often experience hallucination, par-ticularly when generating content in domains unfamiliar to them, ashighlighted in the studies by . Given that LLMs are trainedon datasets encompassing a broad range of scenarios, their expertisein specific tasks such as Chinese address rewriting is limited. Con-sequently, this make them prone to generating hallucinated contentwhen tasked with rewriting Chinese addresses. Meanwhile, LLMsalso suffer from misalignment , which is also significant inaddress system since address database keeps updating. To solve thesechallenges, we develop an Address-centric Retrieval-AugmentedGeneration(RAG) module, which decouples reasoning ability andaddress knowledge storage of LLM, and maintains knowledge inexternal database.In this section, we introduce the popular \"retrieve-then-read\"RAG pipeline and adapt it to our address rewriting scenario. Firstly,the retriever identifies and extracts a set of relevant addresses fromthe database. Subsequently, the generator, i.e. a fine-tuned LLM,bases its generated output on the addresses retrieved. We describethe retriever in more detail below. Retriever: The retrievers function is to identify and prioritizeall addresses relevant to the input. Formally, the retrievers role isencapsulated by the function , which maps a query and a database to a subset , such that : , where comprisesthe relevant addresses from corresponding to the input , orderedby decreasing relevance. In our scenario, is an address we wantto modify, is a set of addresses relevant to . The foundationmodel of retriever is an encoder model , which maps an address toa representation. Then a similarity score is computed between queryaddress and each sample in :(,) = ((), ()), (10) where : R R R+ is the similarity function, such as cosinesimilarity. Finally, samples with highest similarity score, i.e. , arereturned by retriever.For the sake of efficiency and scalability, retriever usually encodetextual information into embedding space, where the retriever per-forms search . In previous RAG frameworks which mainlytarget at question-answering task, relevance is defined on semanticsof paragraphs and sentences. For our address scenario, however,semantics relevance could introduce misleading information. Ourobjective necessitates a shift in the relevance criterion towards geo-graphical proximity, ensuring that the retrieved addresses are withina close spatial range to the query address. Spatial Encoding: Previous research has explored the spatial en-coding of addresses . However, these techniques prioritizeother NLP tasks like Masked Language Modeling and HierarchicalText Classification, which generalize the pre-trained model(PTM) atthe expense of its capacity for spatial encoding. Thus, fine-tuning thePTM is necessary. In our retrieval framework, we employ the widelyadopted BERT encoder architecture as foundation model . Theinitialization of our retriever leverages parameters from the text en-coder of G2PTL . Subsequently we fine-tune BERT on geocod-ing task. To achieve this, we append several fully connected(FC)layers to existing BERT structure, which are designed to transformthe embedding to geographic coordinates, namely longitude andlatitude. Our geocoding dataset comprises 200 million <address, coordinates> pairs. Given that BERT comes pre-trained while theadded FC layers are randomly initialized, we adopt a two-phasetraining strategy. In the first epoch of training, we freeze BERTparameters and concentrate on training FC layers. For subsequentepochs, we train the whole neural network, including BERT and FClayers. When retrieving addresses, the representation produced byBERT, a 768-element vector, is utilized as the spatial embedding.",
  "Experiments": "In this section, we conduct experiments to answer the followingresearch questions: RQ1: Does AddrLLM outperform other SoTA methods in offlineexperiments? RQ2: Whether and how often does AddrLLM rewrite a standardaddress to incorrect one? RQ3: How the components in AddrLLM contribute to the perfor-mance? RQ4: How long is the duration required for SFT and objectivealignment of LLM, and the responsiveness of our framework? RQ5: Does the retriever in RAG module perform well? RQ6: What improvements has the deployment of AddrLLM broughtto JDs LBS system?",
  "Testing Datasets and Evaluation Metrics": "In this section, we present the downstream applications that servedas benchmarks for assessing our model, along with the associatedmetrics. The prompts employed for the LLMs across these appli-cations are detailed in Appendix C. An overview of the datasetsfor the different tasks is provided in . While the trainingdatasets have been extensively described in , in this part,we focus exclusively on the testing datasets. To test our model, webuild three extensive datasets, each corresponding to one of the threeapplications.Address Entity Prediction: Request the model to generate addresseswith all necessary elements for those lacking administrative regions. Trigger Prediction: This metric evaluates the ability of the modelto discern which input addresses are missing administrative regionsand require completion. If the model attempts to complete the ad-dress (e.g. add some words), we identify it as successful triggerprediction, regardless of whether the completion is correct. Accuracy: Whether the model generates correct address. Thismetric is calculated as the ratio of accurately rewritten addresses tothe total number of addresses missing regions.Direct: Directly evaluate the rewritten address. Hit Rate: We derived the groundtruth for levels 1 to 4 (Appen-dix A) by reverse geocoding the delivery coordinates, complement-ing them with level 5 and 6 from the input addresses. After tokeniz-ing both the rewritten addresses and groundtruth addresses, we thenassessed the accuracy by calculating the percentage of address com-ponents predicted by the model that correspond with the groundtruthcomponents.Geocoding: Geocoding is a popular GIS service that maps textualaddress to geospatial coordinates. In JDs LBS system, parcel dis-patching relies heavily on the geocoding service. Here we rewrite theinput address by model before feeding it to JDs geocoding serviceof LBS system.",
  "Implementation Details": "We choose AdamW optimizer for LLM SFT and objectivealignment. During SFT stage, LLM is trained for 1 epoch withlearning rate 1e-5. During objective alignment stage, LLM is trainedfor 4 epochs with learning rate 1e-6. During retriever fine-tuningstage, BERT is trained for 4 epochs with learning rate initialized to5e-5 and gradually decreased during the process of training, whereAdam is the optimizer. We compare Qwen-7B and Baichuan-7B as base LLM because of their outstanding performance inChinese language related tasks. At retrieval stage, retriever select top-10 most relevant addresses from database, for which we use the opensource vector database Vearch . All of the offline experimentsare conducted on a cloud-based computational cluster including 10Nvidia H800 GPUs, 160 CPU cores of Intel Xeon 8468V.",
  "Compared Methods": "BART : A widely used transformer-based encoder-decoderPre-Trained Model(PTM), which achieve remarkable gains in NLPtasks. We fine-tune it on our address rewriting dataset to enhance itsability in rewriting addresses. BERT : A popular transformer-based encoder PTM. We appendTransformer decoder and fine-tune it on address rewriting dataset. G2PTL : The latest PTM pre-trained on logistics data andtasks. The built-in Address Entity Prediction module in G2PTLneeds missing words marked as \"[]\", which cannot be di-rectly applied on our address rewriting scenario. Thus, we utilize thetext encoder of G2PTL and append transformer decoder to returnrewritten address. We fine-tune it on address rewriting dataset. QWen-7B : LLM baseline. Prompt Qwen-7B to rewrite address. Baichuan-7B : LLM baseline. Prompt Baichuan-7B to rewrite address. SoP: Address geocoding service within JDs LBS system. AddrLLM-Qwen: Utilize QWen-7B as the base LLM instead ofBaichuan-7B. AddrLLM w/o OA: Remove the objective alignment module fromAddrLLM. AddrLLM w/o SFT: Remove the SFT module from AddrLLM. AddrLLM w/o RAG: Remove the RAG module from AddrLLM. AddrLLM-RAG: Remove SFT and objective alignment modulesfrom AddrLLM.",
  "Our experimental results on direct evaluation, address entity predic-tion and geocoding tasks are shown in . By analyzing theresults, we have the following findings:": "3.4.1Overall Performance (RQ1). Upon evaluating the accu-racy of AEP, the hit rate and geocoding accuracy, it becomes evidentthat our model, AddrLLM, outperforms all of baselines on our test-ing dataset, which consists of 90% standard and 10% abnormaladdresses. The SoTA baseline G2PTL achieves second best averageperformance. Pose the application of AddrLLM for address rewrit-ing, compared with the second best model G2PTL, we observedsignificant improvements across several metrics: trigger predictionof AEP increases by 7%, the accuracy of AEP increases by 10.8%,the hit rate increases by 15.4%, the geocoding accuracy at stationlevel increases by 7.1%. This enhancements underscore the efficacyof our LLM-based address rewriting framework in detecting and cor-recting address inaccuracies. Notably, in the context of geocoding,a critical factor for successful parcel delivery in the logistics indus-try, compared with SoP method, AddrLLM reduces station-levelinaccuracies by a substantial 43%. 3.4.2Robustness (RQ2). Since in the real-world scenario, per-centage of incorrect addresses is greatly lower than that of our testingdataset(10%), it is important to test whether model rewrites a stan-dard address to incorrect one. Thus, we design robustness metric ongeocoding task. Robustness measures the percentage of standard ad-dresses remaining correct after rewriting. From the table, AddrLLMachieves 99.9% robustness, which significantly outperforms otherbaselines. This suggests that AddrLLM is effective at recognizingcorrect addresses and avoids modifying them into incorrect ones. 3.4.3Ablation Study (RQ3). To understand the contribution ofeach component to AddrLLMs performance and to inform future en-hancements and deployment strategies, we conduct an ablation studyand meticulously analyze the experimental results. Our analysisyields the following insights:The original Baichuan and QWen models exhibit suboptimalperformance in address rewriting tasks, falling short of the SoTAbaseline established by G2PTL, by an average of 7.3% and 8.7%respectively. This indicates that generalized LLMs have limitedknowledge about standard addresses. In contrast, compared withBaichuan and QWen, AddrLLM and AddrLLM-QWen obtain im-provements by 19.5% and 19%, respectively. This indicates that theintegration of our novel RAG, SFT and objective alignment modulessignificantly enhance LLMs capacity for address rewriting.",
  "QWen-7B70.664.368.773.576.279.186.98.465.9Baichuan-7B71.465.869.275.778.979.887.510.267.3SoP---88.389.190.01000-": "AddrLLM(ours)91.890.389.791.692.794.399.943.986.8-AddrLLM-QWen89.988.386.789.490.693.899.641.484.9-AddrLLM w/o OA88.584.683.885.787.188.896.221.879.6-AddrLLM w/o SFT76.371.774.878.282.384.591.224.672.9-AddrLLM w/o RAG85.682.479.681.683.187.593.533.578.4-AddrLLM-RAG74.169.871.777.380.681.889.214.969.9 In the absence of objective alignment(OA) component, AddrLLMattains the highest performance relative to other ablated modelsexcept AddrLLM-QWen. Even when OA is removed, AddrLLMstill outshines the SoTA baselines, albeit it falls behind the SoP ingeocoding task. The SFT process relies on groundtruth data gener-ated by JDs LBS system(SoP). Therefore, without the OA module,it is challenging for AddrLLM to surpass the SoP. The OA module,however, introduces samples where SoP lacks the knowledge for thegroundtruth addresses, providing a unique training opportunity forAddrLLM to refine its performance and ultimately exceed the SoP.Excluding the Supervised Fine-Tuning (SFT) module leads to amarked reduction in AddrLLMs effectiveness, particularly affect-ing the robustness of geocoding task. This deterioration is due tothe prevalence of non-rewriting samples within the address pars-ing, rewriting and entity prediction datasets of SFT stage. Thesenon-rewriting samples are instrumental in enhancing the modelsproficiency in recognizing correct addresses and determining whennot to perform rewrites.When the RAG component is removed, there is a noticeabledownturn in AddrLLMs effectiveness, indicating the integral roleof RAG in the models overall functionality.Observing AddrLLM-RAG, where we remove SFT and OA mod-ules, our model achieves the lowest performance relative to otherablated models. This is because the original Baichuan model lacksunderstanding of standard addresses. The average performance isonly 2.6% higher than that of Baichuan-7B. This indicates thatwithout fine-tuning the base model, AddrLLM is hard to utilize therelevant addresses retrieved by RAG module.In conclusion, the synergy of SFT, OA, and RAG is essential foroptimizing AddrLLM in the specialized task of rewriting addresses. 3.4.4Complexity Analysis (RQ4). In , we present a con-solidated overview of the cumulative duration, measured in hours,required for Supervised Fine-Tuning (SFT), objective alignment, andevaluation phases. Notably, the objective alignment stage operates atan average rate of 13 samples per second. This relatively slow speedis attributed primarily to the reward calculation being executed on the CPU, whereas the training of the LLM occurs on the GPU. Con-sequently, there is an increased frequency of data transfers betweenthe CPU and GPU compared to typical neural network training pro-cedures. Additionally, this transfer and training process is inherentlydifficult to parallelize, further contributing to the reduced processingspeed.",
  "TestGeocoding3500.4": "3.4.5Spatial Encoding in RAG (RQ5). The spatial encodingfeature within the RAG module guarantees that the addresses re-trieved are spatially close and subsequently influences the infor-mation supplied to the LLM. In this section, we assess the qualityof spatial embeddings generated by our retriever in comparison toG2PTL, the most recent state-of-the-art Pre-Trained Model(PTM)for logistics addresses.Firstly, we select 12 delivery stations across Beijing and utilizethe belonged addresses to evaluate models proficiency in distin-guishing between them. We employ t-SNE to reduce dimension andvisualize the spatial embedding. Station-level classification is at afiner granularity than that of district-level, as a single district maycontain dozens of delivery stations. While G2PTL is reportedto have excellent performance in district-level categorization, ourobservations from indicate that its capabilities at the station-level do not meet surrounding addresses retrieval in RAG module.Conversely, our retriever, which has been fine-tuned with a datasetcomprising 200 million samples from nationwide geocoding data-base, demonstrates superior performance in classifying addresses atthe station level.",
  "Online Deployment (RQ6)": "AddrLLM is incorporated to JDs current LBS system and deployedin Zhejiang province, China. Specifically, all packages sending fromZhejiang will be processed by the new system. Because the hugecomputing resources needed by Large Language Models, in the de-ployed system, AddrLLM is called to rewrite addresses only whenabnormal addresses are detected . Considering daily rewritingvolume in Zhejiang province, AddrLLM is deployed on a compu-tational cluster with 4 Intel Xeon 8468V CPUs, 4 NVIDIA RTX4090D GPUs and 256 GB RAM.We present monitoring data over the course of 60 days encom-passing the \"618\", a sales event comparable to Black Friday. Dailyrewriting volume and accuracy (percentage of abnormal addresses 2024/05/19 2024/06/02 2024/06/16 2024/06/30 2024/07/14",
  ": Result of Deployment": "that can be corrected by AddrLLM) is depicted in . Onaverage, there are 754 instances of address rewriting each day. Ourspecialized address rewriting model, AddrLLM, has effectively cor-rected over 40% of these erroneous addresses, ensuring the properdelivery of corresponding parcels. Moreover, the accuracy curve re-veals that the performance of AddrLLM remains stable when appliedto real-world data streams.Results from Yulin city, Shaanxi Province and Yangjiang city,Guangdong Province are shown in Appendix E.",
  "Discussion": "Lessons Learned: We summarize key lessons learned from work: Large Language Model shows powerful ability of reasoning, butstill needs fine-tuning and objective alignment to meet the require-ments of specific domain and task. Retrieval-Augmented Generation(RAG) boosts LLMs perfor-mance in our address rewriting scenario. After deployment of LLM, RAG decreases the workload of updat-ing knowledge, especially for a rapidly updating geocoding database.Limitations: For the offline experiment, because of vast amount of training datarequired by fine-tuning LLM, its impractical to obtain a comprehen-sive analysis of all the incorrect address. For example, in ,what are percentages of these errors in real-world address query. Because of the black-box essential of LLM, we adopt a relativelyconservative way to deploy our model. Instead of directly incorpo-rating our model into core of JDs LBS system, which is the mostcritical system in a logistics company, we initially utilize it to rectifyerroneous addresses after abnormal addresses detected. Though it isa good way to examine models ability and robustness on real-worldscenario, AddrLLMs capabilities is not fully utilized.Future Work: An autonomous method to analyze different types of errors in dailyaddresses. After obtaining comprehensive analysis of erroneous addresses,train and test LLMs rewriting ability on individual type of error. Further examine and enhance LLMs robustness, decrease its in-ference latency and incorporate it into the core of JDs LBS system.",
  "Related Work5.1Address Processing": "Address Processing is a critical task in the domain of geospatialanalysis, intersecting with Natural Language Processing, informa-tion retrieval and Machine Learning. There have been significant re-searches advancing the field of address-related tasks. formulatedaddress standardization as a multi-task classification problem anddeveloped a seq-to-seq based low-latency address rewriting frame-work. infers geographic coordinates from textual addressesusing data from e-commerce logistics. develops a frameworkfor quickly detecting abnormal addresses in e-commerce by us-ing a contrastive address augmentation approach and a lightweightattention model. introduces GeoGLUE, a benchmark for eval-uating geographic natural language understanding. developsan autonomous method to detect address alias based on logisticsdata. Recently, due to the impressive success of Pre-Training Mod-els(PTM), several works developed PTMs for addressrelated tasks, based on data from logistics and map areas. However,few work has explored large language models ability in dealingwith address-related tasks.",
  "Query Rewriting": "Address rewriting can be considered as a specialized form of queryrewriting within the context of Geographic Information Systems(GIS).Address rewriting aims to bridge the gap between user-entered ad-dresses and the data indexed within a GIS. Extensive researches onquery rewriting have arisen, whose techniques are sometimes appli-cable to address rewriting. Previous works mainly view query rewrit-ing as a machine translation problem. These works can be dividedinto: Statistical Machine Translation (SMT)-based methods and Neu-ral Machine Translation (NMT)-based methods. SMT-based meth-ods use statistical models to generate translationsfrom original query to rewritten query. Around the mid-2010s, afterdeep neural network showed its power in feature learning and gener-alization , NMT-based methods earned more attention.These methods adopt encoder-decoder architecture, where encodermaps original query into hidden representation, and decoder mapsrepresentation to rewritten query. Recently, large language modelshave demonstrated remarkable capabilities in understanding andgenerating human-like text . They are adopted in query rewritingtasks to enhance search relevance and user experience .However, these works target at rewriting e-commerce product queryand user question, which are different from address in both for-mat and semantics. There are several works dealing with addressrewriting problem . Compared with our LLM-based meth-ods, these methods have some drawbacks: (1) Substitution and spellcorrection can only handle a portion of erroneous addresses; (2) Lim-ited knowledge incorporated in the model; (3) When new addressescome, model fine-tuning or re-training is needed.",
  "Large Language Models exhibits impressive proficiency but grap-ple with issues like generating inaccurate information and outdatedknowledge. Retrieval Augmented Generation(RAG)": "presents a promising strategy. This method involves retrieving infor-mation from external database prior to generating responses, thenutilizing this information as context during the generation process.Subsequent research has explored various modifications to RAG,such as the REALM model further refined the relevance ofretrieved information through an end-to-end trainable retrieval com-ponent. Atlas shows retrieval enhancement can compensate forthe shortfall in parameter size, by jointly training the retriever and thereader. Not only does RAG help in improving the factual accuracy ofanswers, but it also allows LLM to generate texts related to a specificdomain, such as Medicine , Biomimicry and Electrocardio-graphy , Linguistics , Music . However, previous studiesmainly focus on Question-answering task and retrieve informationformatted as natural language paragraphs. Whether RAG is benefi-cial to address rewriting task and how to retrieve useful informationfrom address database are seldom explored.",
  "Conclusion": "In this paper, we explore address rewriting utilizing retrieval aug-mented large language model. We design AddrLLM, an RAG-basedLLM fine-tuned on several related downstream tasks and objectivelyaligned with package delivery task. In the AddrLLM framework, weintroduce two pioneering components, namely the RAG for addressand Bias-free Objective Alignment, which are seamlessly integratedinto the LLM architecture. We subsequently conduct a comprehen-sive array of empirical investigations on offline real-world datasetand online deployment. Our empirical findings demonstrate thatAddrLLM surpasses the performance of existing methods in bothrewriting and downstream applications. On offline experiments, Ad-drLLM outperforms other baselines and alleviate package re-routingby 43%. During the online deployment phase, AddrLLM operateswith stability and effectively corrects over 40% of the abnormaladdresses.",
  "Baichuan Inc. [n. d.]. Baichuan-7B Model. Accessed: 2024-07-11": "Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, BryanWilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu,and Pascale Fung. 2023. A Multitask, Multilingual, Multimodal Evaluation ofChatGPT on Reasoning, Hallucination, and Interactivity. In Proceedings of the13th International Joint Conference on Natural Language Processing and the3rd Conference of the Asia-Pacific Chapter of the Association for ComputationalLinguistics, IJCNLP 2023 -Volume 1: Long Papers, Nusa Dua, Bali, November 1 -4, 2023. Association for Computational Linguistics, 675718.",
  "Nils Boysen, Stefan Fedtke, and Stefan Schwerdfeger. 2021. Last-mile deliveryconcepts: a survey from an operational research perspective. OR Spectr. 43, 1(2021), 158": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shot learners. Advances in neuralinformation processing systems 33 (2020), 18771901. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, HaoChen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. 2024. A survey onevaluation of large language models. ACM Transactions on Intelligent Systemsand Technology 15, 3 (2024), 145. Kyunghyun Cho, Bart Van Merrinboer, Caglar Gulcehre, Dzmitry Bahdanau,Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phraserepresentations using RNN encoder-decoder for statistical machine translation.arXiv preprint arXiv:1406.1078 (2014). Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.BERT: Pre-training of Deep Bidirectional Transformers for Language Under-standing. In Proceedings of the 2019 Conference of the North American Chap-ter of the Association for Computational Linguistics: Human Language Tech-nologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1(Long and Short Papers). Association for Computational Linguistics, 41714186. Ruixue Ding, Boli Chen, Pengjun Xie, Fei Huang, Xin Li, Qiang Zhang, and YaoXu. 2023. MGeo: Multi-Modal Geographic Language Model Pre-Training. InProceedings of the 46th International ACM SIGIR Conference on Research andDevelopment in Information Retrieval, SIGIR 2023, Taipei, Taiwan, July 23-27,2023, Hsin-Hsi Chen, Wei-Jou (Edward) Duh, Hen-Hsen Huang, Makoto P. Kato,Josiane Mothe, and Barbara Poblete (Eds.). ACM, 185194. Jianfeng Gao, Xiaodong He, and Jian-Yun Nie. 2010. Clickthrough-based transla-tion models for web search: from word models to phrase models. In Proceedingsof the 19th ACM international conference on Information and knowledge manage-ment. 11391148. Jianfeng Gao and Jian-Yun Nie. 2012. Towards concept-based translation modelsusing search logs for query expansion. In Proceedings of the 21st ACM interna-tional conference on Information and knowledge management. 110.",
  "Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang.2020. Retrieval augmented language model pre-training. In International confer-ence on machine learning. PMLR, 39293938": "Muhammad Usman Hadi, Rizwan Qureshi, Abbas Shah, Muhammad Irfan, AnasZafar, Muhammad Bilal Shaikh, Naveed Akhtar, Jia Wu, Seyedali Mirjalili, et al.2023. A survey on large language models: Applications, challenges, limitations,and practical usage. Authorea Preprints (2023). Zhiqing Hong, Guang Wang, Wenjun Lyu, Baoshen Guo, Yi Ding, Haotian Wang,Shuai Wang, Yunhuai Liu, and Desheng Zhang. 2022. CoMiner: nationwidebehavior-driven unsupervised spatial coordinate mining from uncertain deliveryevents. In Proceedings of the 30th International Conference on Advances in",
  "Geographic Information Systems. 110": "Zhiqing Hong, Heng Yang, Haotian Wang, Wenjun Lyu, Yu Yang, Guang Wang,Yunhuai Liu, Yang Wang, and Desheng Zhang. 2022. FastAddr: real-time abnor-mal address detection via contrastive augmentation for location-based services.In Proceedings of the 30th International Conference on Advances in GeographicInformation Systems. 110. Mengting Hu, Xiaoqun Zhao, Jiaqi Wei, Jianfeng Wu, Xiaosu Sun, ZhengdanLi, Yike Wu, Yufei Sun, and Yuzhi Zhang. 2023. rT5: A Retrieval-AugmentedPre-trained Model for Ancient Chinese Entity Description Generation. In CCF In-ternational Conference on Natural Language Processing and Chinese Computing.Springer, 736748. Jizhou Huang, Haifeng Wang, Yibo Sun, Yunsheng Shi, Zhengjie Huang, An Zhuo,and Shikun Feng. 2022. Ernie-geol: A geography-and-language pre-trained modeland its applications in baidu maps. In Proceedings of the 28th ACM SIGKDDConference on Knowledge Discovery and Data Mining. 30293039.",
  "Vishal Kakkar and T Ravindra Babu. 2018. Address Clustering for e-CommerceApplications.. In eCOM@ SIGIR": "Vishal Kakkar and T. Ravindra Babu. 2018. Address Clustering for e-CommerceApplications. In The SIGIR 2018 Workshop On eCommerce co-located with the41st International ACM SIGIR Conference on Research and Development in Infor-mation Retrieval (SIGIR 2018), Ann Arbor, Michigan, USA, July 12, 2018 (CEURWorkshop Proceedings, Vol. 2319), Jon Degenhardt, Giuseppe Di Fabbrizio, SuryaKallumadi, Mohit Kumar, Andrew Trotman, Yiu-Chang Lin, and Huasha Zhao(Eds.). CEUR-WS.org. Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for StochasticOptimization. In 3rd International Conference on Learning Representations, ICLR2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, YoshuaBengio and Yann LeCun (Eds.). Simone Kresevic, Mauro Giuffr, Milos Ajcevic, Agostino Accardo, Lory S Croc,and Dennis L Shung. 2024. Optimization of hepatological clinical guidelinesinterpretation by large language models: a retrieval augmented generation-basedframework. NPJ Digital Medicine 7, 1 (2024), 102. Viet Dac Lai, Chien Van Nguyen, Nghia Trung Ngo, Thuat Nguyen, Franck Der-noncourt, Ryan A. Rossi, and Thien Huu Nguyen. 2023. Okapi: Instruction-tunedLarge Language Models in Multiple Languages with Reinforcement Learning fromHuman Feedback. In Proceedings of the 2023 Conference on Empirical Methodsin Natural Language Processing, EMNLP 2023 - System Demonstrations, Singa-pore, December 6-10, 2023, Yansong Feng and Els Lefever (Eds.). Association forComputational Linguistics, 318327.",
  "Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep learning. nature521, 7553 (2015), 436444": "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, AbdelrahmanMohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART:Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,Translation, and Comprehension. In Proceedings of the 58th Annual Meetingof the Association for Computational Linguistics, ACL 2020, Online, July 5-10,2020, Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (Eds.).Association for Computational Linguistics, 78717880. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,Naman Goyal, Heinrich Kttler, Mike Lewis, Wen-tau Yih, Tim Rocktschel,et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks.Advances in Neural Information Processing Systems 33 (2020), 94599474. Dongyang Li, Ruixue Ding, Qiang Zhang, Zheng Li, Boli Chen, Pengjun Xie, YaoXu, Xin Li, Ning Guo, Fei Huang, et al. 2023. Geoglue: A geographic languageunderstanding evaluation benchmark. arXiv preprint arXiv:2305.06545 (2023).",
  "AddrLLM: Address Rewriting via Large Language Model on Nationwide Logistics DataConference17, July 2017, Washington, DC, USA": "Ilya Loshchilov and Frank Hutter. 2019. Decoupled Weight Decay Regularization.In 7th International Conference on Learning Representations, ICLR 2019, NewOrleans, LA, USA, May 6-9, 2019. OpenReview.net. Kelvin Luu, Daniel Khashabi, Suchin Gururangan, Karishma Mandyam, andNoah A Smith. 2021. Time waits for no one! analysis and challenges of temporalmisalignment. arXiv preprint arXiv:2111.07408 (2021). Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 2023. QueryRewriting in Retrieval-Augmented Large Language Models. In Proceedings ofthe 2023 Conference on Empirical Methods in Natural Language Processing,EMNLP 2023, Singapore, December 6-10, 2023, Houda Bouamor, Juan Pino,and Kalika Bali (Eds.). Association for Computational Linguistics, 53035315. Wenjun Peng, Guiyang Li, Yue Jiang, Zilong Wang, Dan Ou, Xiaoyi Zeng, DerongXu, Tong Xu, and Enhong Chen. 2024. Large Language Model based Long-tailQuery Rewriting in Taobao Search. In Companion Proceedings of the ACM onWeb Conference 2024, WWW 2024, Singapore, Singapore, May 13-17, 2024. ACM,2028. Yiming Qiu, Kang Zhang, Han Zhang, Songlin Wang, Sulong Xu, Yun Xiao,Bo Long, and Wen-Yun Yang. 2021. Query Rewriting via Cycle-ConsistentTranslation for E-Commerce Search. In 37th IEEE International Conferenceon Data Engineering, ICDE 2021, Chania, Greece, April 19-22, 2021. IEEE,24352446. Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kiant Brantley, Jack Hessel,Rafet Sifa, Christian Bauckhage, Hannaneh Hajishirzi, and Yejin Choi. 2022.Is reinforcement learning (not) for natural language processing: Benchmarks,baselines, and building blocks for natural language policy optimization. arXivpreprint arXiv:2210.01241 (2022).",
  "Stefan Riezler and Yi Liu. 2010. Query rewriting using monolingual statisticalmachine translation. Computational Linguistics 36, 3 (2010), 569582": "Stefan Riezler, Yi Liu, and Alexander Vasserman. 2008. Translating queries intosnippets for improved query expansion. In Proceedings of the 22nd internationalconference on computational linguistics (Coling 2008). 737744. Pravakar Roy, Chirag Sharma, Chao Gao, and Kumarswamy Valegerepura. 2023.Deep Query Rewriting For Geocoding. In Proceedings of the 32nd ACM Interna-tional Conference on Information and Knowledge Management. 48014807. John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel.2015. High-dimensional continuous control using generalized advantage estima-tion. arXiv preprint arXiv:1506.02438 (2015).",
  "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347(2017)": "Lei Shu, Liangchen Luo, Jayakumar Hoskere, Yun Zhu, Yinxiao Liu, Simon Tong,Jindong Chen, and Lei Meng. 2024. RewriteLM: An Instruction-Tuned LargeLanguage Model for Text Rewriting. In Thirty-Eighth AAAI Conference on Artifi-cial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applicationsof Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Ad-vances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver,Canada, Michael J. Wooldridge, Jennifer G. Dy, and Sriraam Natarajan (Eds.).AAAI Press, 1897018980. Lei Shu, Liangchen Luo, Jayakumar Hoskere, Yun Zhu, Yinxiao Liu, SimonTong, Jindong Chen, and Lei Meng. 2024. Rewritelm: An instruction-tuned largelanguage model for text rewriting. In Proceedings of the AAAI Conference onArtificial Intelligence, Vol. 38. 1897018980.",
  "Ravindra Babu Tallamraju. 2022. Geographical Address Models in the Indiane-Commerce. In Proceedings of the 31st ACM International Conference on Infor-mation & Knowledge Management. 50965097": "Qin Tian, Fu Ren, Tao Hu, Jiangtao Liu, Ruichang Li, and Qingyun Du. 2016.Using an optimized Chinese address matching method to develop a geocodingservice: a case study of Shenzhen, China. ISPRS International Journal of Geo-Information 5, 5 (2016), 65. Hanwen Tong, Chenhao Xie, Jiaqing Liang, Qianyu He, Zhiang Yue, Jingping Liu,Yanghua Xiao, and Wenguang Wang. 2022. A context-enhanced generate-then-evaluate framework for chinese abbreviation prediction. In Proceedings of the31st ACM International Conference on Information & Knowledge Management.19451954.",
  "Christopher Toukmaji and Allison Tee. 2024. Retrieval-Augmented Generationand LLM Agents for Biomimicry Design Solutions. In Proceedings of the AAAISymposium Series, Vol. 3. 273278": "Binghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan Dou, Caishuang Huang,Wei Shen, Senjie Jin, Enyu Zhou, Chenyu Shi, Songyang Gao, Nuo Xu, YuhaoZhou, Xiaoran Fan, Zhiheng Xi, Jun Zhao, Xiao Wang, Tao Ji, Hang Yan, LixingShen, Zhan Chen, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang, ZuxuanWu, and Yu-Gang Jiang. 2024. Secrets of RLHF in Large Language Models PartII: Reward Modeling. CoRR abs/2401.06080 (2024). arXiv:2401.06080",
  "and its applications in logistics system. arXiv preprint arXiv:2304.01559 (2023)": "Mengyuan Yang, Mengying Zhu, Yan Wang, Linxun Chen, Yilei Zhao, XiuyuanWang, Bing Han, Xiaolin Zheng, and Jianwei Yin. 2024. Fine-Tuning LargeLanguage Model Based Explainable Recommendation with Explainable QualityReward. In Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024,Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence,EAAI 2014, February 20-27, 2024, Vancouver, Canada, Michael J. Wooldridge,Jennifer G. Dy, and Sriraam Natarajan (Eds.). AAAI Press, 92509259. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R.Narasimhan, and Yuan Cao. 2023. ReAct: Synergizing Reasoning and Actingin Language Models. In The Eleventh International Conference on LearningRepresentations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Fanghua Ye, Meng Fang, Shenghui Li, and Emine Yilmaz. 2023.Enhanc-ing Conversational Search: Large Language Model-Aided Informative QueryRewriting. In Findings of the Association for Computational Linguistics: EMNLP2023, Singapore, December 6-10, 2023, Houda Bouamor, Juan Pino, and Ka-lika Bali (Eds.). Association for Computational Linguistics, 59856006.",
  "Han Yu, Peikun Guo, and Akane Sano. 2023. Zero-Shot ECG Diagnosis with LargeLanguage Models and Retrieval-Augmented Generation. In Machine Learning forHealth (ML4H). PMLR, 650663": "Mengxi Yu, Ziyu Liu, Yuhang Tang, and Jianfeng Jiang. 2021. Recognitionalgorithm of e-commerce click farming based on K-means technology. In 20216th International Conference on Intelligent Computing and Signal Processing(ICSP). IEEE, 103106. Hongyi Zhu, Jia-Hong Huang, Stevan Rudinac, and Evangelos Kanoulas. 2024. En-hancing Interactive Image Retrieval With Query Rewriting Using Large LanguageModels and Vision Language Models. In Proceedings of the 2024 InternationalConference on Multimedia Retrieval. 978987. Hongyi Zhu, Jia-Hong Huang, Stevan Rudinac, and Evangelos Kanoulas. 2024. En-hancing Interactive Image Retrieval With Query Rewriting Using Large LanguageModels and Vision Language Models. In Proceedings of the 2024 InternationalConference on Multimedia Retrieval. 978987.",
  ": Prompt for Objective Alignment and evaluation": "You are an address rewriting bot, please rewrite the fol-lowing address according to standard address hierarchyand related addresses. Related addresses are possiblygeographically close to the address to be rewritten. If norewrite is needed, output the original addressAddress to be rewritten:{address}Address Hierarchy:{address hierarchy}Examples:{address rewriting examples}Related Address:{addresses returned by retriever}System: {rewritten address or original address}",
  "= || (11)": "Action space : The action space equals to the vocabulary set V. Reward and Reward function : Model obtains an immediatereward at time by a rewarding function . At each time step ,policy model generate next token as action , based on currentstate . Policy model and discount factor: At time , the model choosesan action following a policy : . The initial policy model 0 isthe large language model. At each time step , the model generatesthe next token, based on current state , i.e. already generated tokens."
}