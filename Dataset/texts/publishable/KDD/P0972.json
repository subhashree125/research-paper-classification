{
  "ABSTRACT": "Personalized Federated Continual Learning (PFCL) is a new prac-tical scenario that poses greater challenges in sharing and per-sonalizing knowledge. PFCL not only relies on knowledge fusionfor server aggregation at the global spatial-temporal perspectivebut also needs model improvement for each client according tothe local requirements. Existing methods, whether in PersonalizedFederated Learning (PFL) or Federated Continual Learning (FCL),have overlooked the multi-granularity representation of knowl-edge, which can be utilized to overcome Spatial-Temporal Cata-strophic Forgetting (STCF) and adopt generalized knowledge toitself by coarse-to-fine human cognitive mechanisms. Moreover,it allows more effectively to personalized shared knowledge, thusserving its own purpose. To this end, we propose a novel conceptcalled multi-granularity prompt, i.e., coarse-grained global promptacquired through the common model learning process, and fine-grained local prompt used to personalize the generalized represen-tation. The former focuses on efficiently transferring shared globalknowledge without spatial forgetting, and the latter emphasizesspecific learning of personalized local knowledge to overcome tem-poral forgetting. In addition, we design a selective prompt fusion",
  "Corresponding Author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08 mechanism for aggregating knowledge of global prompts distilledfrom different clients. By the exclusive fusion of coarse-grainedknowledge, we achieve the transmission and refinement of com-mon knowledge among clients, further enhancing the performanceof personalization. Extensive experiments demonstrate the effec-tiveness of the proposed method in addressing STCF as well asimproving personalized performance. Our code now is available at",
  "INTRODUCTION": "Federated Continual Learning (FCL) is a new practical paradigmaiming at fusing knowledge from different times and spaces withoutcatastrophic forgetting in dynamic Federated Learning (FL) settings. Moreover, Personalized Federated Learning (PFL) tries to fuseimplicit common knowledge extracted from various clients andpersonalize the generalized knowledge for better performance onthe client side . However, to better accommodate diverse local",
  "KDD 24, August 2529, 2024, Barcelona, SpainHao Yu et al": "adapted to the characteristics of local data and effectively meet therequirements of local tasks. However, due to FCL itself, PFCL isalso susceptible to severe spatial-temporal catastrophic forgetting.Therefore, PFCL has three main objectives. The first is to formmore generalized knowledge during server knowledge fusion, avoid-ing spatial catastrophic forgetting caused by heterogeneous data.The second is for clients to adopt appropriate strategies to over-come temporal forgetting resulting from continual learning. Thethird is for clients to employ suitable personalization strategies,ensuring that the received generalized global model better adaptsto the local task requirements and characteristics of local data.Now, we extend the traditional FL to PFCL. Given clients (denoted as A = {1,2, . . . ,}), and a cen-tral server (denoted as ), each client {, 1 } has itsunique task sequence T, where each task encompasses differ-ent classes. The task sequence of client is denoted as T ={ 1 , 2 , . . . , }, where represents the total number of tasks",
  "RELATED WORK2.1Multi-Granularity Computing": "Multi-granularity computing addresses the challenge of tacklingthe coexistence of data with different granularities . Ex-tracting multi-granularity knowledge benefits our understandingof materials and their intrinsic properties.VL-PET designs a multi-granularity controlled mechanismto impose control on modular modifications of the pre-trainedlanguage model at coarse and fine granularities. constructs aquestion-answering dataset with yearly, monthly, and daily-graineddata and proposes MultiQA to address temporally multi-granularityquestion-answering. adopts the sequential three-way decisionmethod to extract knowledge of different granularities in open-topic classification tasks. decouples the objects of group re-identification tasks into individual, subgroup, and entire groupgranularities to handle the dynamic changes in group layout andmember variations. introduces granular computing in FL andachieves automatic neural architecture search to adapt the different",
  "Personalized Federated Continual Learning via Multi-granularity PromptKDD 24, August 2529, 2024, Barcelona, Spain": "We denote the small proxy dataset owned by the server as D.{,} are the samples and corresponding labels from D for thedistillation process. For the convenience of writing and understand-ing, we will only consider two global prompt pools here, denoted asP and P . P is chosen as the student pool. Initially, the input searches for the corresponding global prompt within P, and thenconcatenates to form an embedding with the prompt. Similarly, represents the embedding of the same input but concatenated",
  "Prompt-Based Continual Learning": "Continual Learning (CL) aims to overcome catastrophic forget-ting of the previous knowledge after training on new data in non-stationary task streams . Various CL techniques have been proposed to alleviate catastrophic forgetting and achieveknowledge transfer across tasks, including regularization, rehearsal,parameter isolation, and knowledge distillation.Recent works introduce prompt learning to CL to achieve moreefficient exemplar-free CL . Prompt learning is a noveltransfer learning technique applied to adapt general knowledge ofpre-trained large language or vision models to downstream tasks byoptimizing prompts . CoOp integrates learnableprompts in the vision-language model to facilitate end-to-end learn-ing where the design of task-specific prompts is fully automated.L2P applies learnable task-specific prompts to mitigate forget-ting and even outperforms exemplar-based methods in accuracyand efficiency. DualPrompt decouples the learnable promptsinto general and expert prompts, encoding task-invariant and task-specific knowledge, respectively. CODA replaces key-value pairs in the prompt selection strategy with an attention-based end-to-end scheme. Pro-KT attaches complementary prompts to apre-trained large model to efficiently transfer task-aware and task-specific knowledge. LGCL mitigates forgetting in extremelyheterogeneous task streams, where the class set of each task is dis-joint, by improving the key lookup of the prompt pool and mappingthe output feature to class-level language representation.In this paper, we design a local prompt and a global promptmechanism to extract and encode coarse-grained and fine-grainedknowledge, achieving spatial-temporal knowledge transfer.",
  "Personalized Federated Learning": "Personalized Federated Learning (PFL) focuses on training cus-tomized models to accommodate various preferences and require-ments of clients in heterogeneous FL. Existing works on PFL canbe categorized into data-based and model-based approaches .Per-FedAvg designs a Model-Agnostic Meta-Learning (MAML)framework to find a generalized global model. It trains personalizedlocal models derived from the shared global model. pFedMe integrates L2-norm regularization in the loss function to adaptivelycontrol the balance between personalization and generalizationin federated MAML. Ditto adds a regularization term in thelocal objectives as the loss function of the local adaptation processbut aggregates the models before the local adaptation to strike abalance of personalization and generalization. FedSteg enablesdomain adaptation from the shared global model to personalizedlocal models by adding a correlation alignment layer before thesoftmax layer. FedPer decouples the model into base layersand personalized layers and aggregates the shallow base layers tocapture generic knowledge while retaining the deep personalizedlayers locally to maintain personalized knowledge. FedMSplit adopts multi-task learning to fit related but personalized models forclients. FedCE clusters the clients into several groups based onthe similarity of local data distributions and trains multiple globalmodels for each group. FedCP proposes an auxiliary Condi-tional Policy Network to achieve more fine-grained personalizationwith sample-wise feature separation. conducts clustering byanalyzing the principal angles of local data in the subspaces anddelays the training stage until the clustering is accomplished. Theseworks do not explicitly explore the multi-granular knowledge inthe processes of generalization and personalization.Some recent works incorporated prompting learning methodsinto PFL. pFedPG utilizes personalized prompt generationglobally and personalized prompt adaptation locally to achieve PFLunder heterogeneous data. pFedPrompt extracts user consensusfrom linguistic space and adapts to local characteristics in visualspace in a non-parametric manner.However, extracting and fusing spatial-temporal multi-granularknowledge via prompting to overcome catastrophic forgetting anddata heterogeneity has not yet been implemented in PFCL.",
  "{C1 C2 . . . , C }": "During the training of task , the global model on the server al-ready possesses the knowledge of 1 to 1from client {, 1 }. The server then distributes it back to clients. Afterpersonalizing the received global model 1, the client con-tinually trains it on as the initial model to get the new localmodel . The local model should perform well in classifyingclasses from the set {C1 C2 . . . , C }. Finally, the server collects the local models from clients whoparticipate in FCL and obtains a new global model , which hasmore generalized knowledge of learned tasks from all clients.Clients need to adopt appropriate strategies to personalize theglobal model , enabling it to perform better locally.According to the similarity of task sequences among clients, FCLcan be initially divided into two scenarios: synchronous FCL andasynchronous FCL . We will discuss it in detail in Sec. 5.1.2.",
  "Spatial-Temporal Catastrophic Forgetting": "Catastrophic Forgetting is a fundamental challenge in CL, whichrefers to a phenomenon that a model would forget the knowledgelearned on old tasks when training on new tasks . The reason forcatastrophic forgetting is that the well-learned network parameterson the old tasks are overwritten during training on the new tasks.In the FCL setting, catastrophic forgetting exists as well. In areal-world scenario, data reaches clients consecutively throughtask streams , causing temporal catastrophic forgetting. At theaggregation stage, the central server collects local models and ag-gregates them into one global model. Then, the server distributesthe global model back to clients. Local models are trained with dif-ferent training data. Aggregating them leads to the overwriting ofcertain task-specific crucial parameters, consequently causing a de-cline in the performance of the global model on local-specific tasks.Adopting the global model consolidated such conflict knowledgeexacerbates the temporal catastrophic forgetting of each clientsprevious tasks.The fundamental reason for STCF is that the knowledge rep-resented by the models parameters is too fine-grained, leading to a lack of robustness against minor variations. Therefore, it isnecessary to represent knowledge in a multi-granularity way. Split-ting it into coarse-grained spatial-temporal-invariant knowledgeand fine-grained spatial-temporal-specific knowledge and handlingthem separately can effectively overcome STCF.We design Temporal Knowledge Retention to measure theeffectiveness of temporal knowledge transfer and Spatial Knowl-edge Retention to measure the effectiveness of spatial knowledgetransfer in PFCL.Definition 1. (Temporal Knowledge Retention) Given a fed-erated learning system with clients, the temporal knowledgeretention is defined as:",
  "(0 ; 0 ),(1)": "where ( ; 0 ) denotes the test accuracy of client s localmodel at -th round on the 0-th task and (0 ; 0 ) denotes thetest accuracy of client s local model at the initial round on the0-th task.Definition 2. (Spatial Knowledge Retention) Given a federatedlearning system with clients, the spatial knowledge retention isdefined as:",
  "MULTI-GRANULARITY PROMPT": "In this section, we elaborate on our proposed Federated Multi-Granularity Prompt (FedMGP), which introduces a multi-granularityknowledge space into PFCL for the first time to better address per-sonalized requirements and spatial-temporal forgetting.Specifically, on the client, we design prompts at two granular-ity levels for knowledge representation, namely Coarse-grainedGlobal Prompt (see Sec. 4.1) and Fine-grained Local Prompt (seeSec. 4.2). Global prompts represent coarse-grained common knowl-edge, while local prompts, built upon global prompts, representclass-wise fine-grained knowledge. Only fusing the coarse-grainedcommon knowledge facilitates the formation of generalized knowl-edge and avoids spatial forgetting caused by aggregating fine-grained knowledge. Local prompts based on global prompts aimto personalize the generalized knowledge from the server whilepreventing temporal forgetting due to class increments.On the server side, we devise a new approach for fusing globalprompts called Selective Prompt Fusion (see Sec. 4.3) without spatialforgetting. Aggregating only coarse-grained knowledge not only en-hances aggregation speed but also provides further improvementsin privacy protection.The overall framework of the proposed method is shown in ,and the algorithm is summarized in algorithm 1.",
  "Upload": ": An overview of the proposed FedMGP. Two granularities of prompts are used to capture both temporal-spatialinvariant knowledge and specific knowledge. The coarse-grained global prompt is trained through a shared ViT model, actingon the embedding layer. The fine-grained local prompt is built upon the coarse-grained prompt by introducing additionalparameters in the MSA layer, enabling the model to better adapt to local data. Moreover, selective prompt fusion is employed toaggregate global prompts on the server side, forming generalized knowledge.",
  "Coarse-grained Global Prompt": "Due to the heterogeneity of data, significant differences exist amonglocal models, leading to substantial variations in extracted knowl-edge. This also poses significant challenges for the fusion and trans-fer of knowledge, as the knowledge learned by each client is overlyfine-grained. Inspired by the cognitive processes of humans, knowl-edge transfer among humans is effective because there is a fun-damental shared cognition, enabling the meaningful exchange ofknowledge. Therefore, we assign each client with the same pre-trained ViT model as a foundational cognitive system. With ViTsparameters frozen, clients learn global prompts that operate at theinput level. Consequently, these global prompts represent coarse-grained knowledge acquired through the common model learningprocess. Furthermore, as the knowledge is extracted from the samemodel, it is more convenient to aggregate knowledge on the serverside without spatial forgetting.The training of coarse-grained global prompts is based on thefrozen ViT model. Moreover, global prompts operate at the inputlevel, not influencing the models parameters. The purpose is toextract knowledge into a common space through the same model.",
  "where [;] represents concatenation along the token length dimen-sion. The next question is how to choose global prompts": "4.1.2Global Query Function. Due to the use of the same model,similar inputs tend to select similar prompts and vice versa. Thismitigates the challenge of aggregating heterogeneous knowledgeon the server. Based on this, we have designed a key-value pair-based query strategy that dynamically selects suitable promptsby calculating the similarity between the input key and existingprompts keys.We associate each prompt in the pool with a learnable key, de-noted as {(1, 1), (2, 2), . . . , ( , )}. To ensure that similarinputs have similar keys, we use the output features of the pre-trained ViT V as the key for the input, i.e., = V(). Then, thequery process can be summarized by the following expression:",
  "=1dis( , ),(5)": "where K denotes the subset of top-N keys selected specifically forthe input, and K represents the set of keys for all global prompts.In this work, we utilize cosine similarity as the distance function tomeasure the similarity between keys. 4.1.3Optimization for Global Prompt. Each client has a globalclassification head used for training global prompts, denoted as .At the beginning of training, it is necessary to load the pre-trainedmodel with to enable it to perform the classification task, and we",
  "Fine-grained Local Prompts": "Once the training of global prompts is completed, they will be frozenand remain unchanged, including both the prompts themselves andtheir corresponding keys, until the next task training. Based on thefrozen global prompts, we further developed fine-grained class-wiselocal prompts. These prompts directly impact the models multi-head self-attention (MSA) layers, facilitating the extraction of local, fine-grained knowledge. Additionally, this fine-grainedprompting helps overcome temporal forgetting induced by classincrements. The hierarchy of prompts, from coarse to fine, simplifiesgeneralized knowledge extraction, fusion, and personalization. 4.2.1From Coarse to Fine. Similarly, a prompt pool is constructedfor local prompts. However, since it represents class-specific knowl-edge, the size of the pool depends on the number of data classes.The local prompt pool is defined as",
  "P = {(1 , 1 ), (2 , 2 ), . . . , ( , )},(7)": "where represents the number of classes. It is precisely the class-wise fine-grained knowledge that imparts significant effectivenessto our approach of personalization and addressing temporal forget-ting induced by class increments. It is proved in Sec. 5.3. 4.2.2Local Query Function. Fine-grained prompts are selectedbased on the global prompt, so we must first obtain frozen globalprompts by allowing the original input to undergo the globalquery function. Subsequently, we concatenate to form an input with selected global prompts. Then, similar to obtaining the keyfor global prompts, we acquire the key for local prompts =V(), with the only difference being that the input is now .The subsequent steps of calculating similarity and selection areanalogous to the corresponding operations for global prompts.Note that we do not employ this querying function during thetraining phase. Instead, we use mask code to select the local promptcorresponding to the data class for training.",
  "MSA = MSA(, [ ;] , [ ; ]).(9)": "Once global prompts have completed training, they freeze alongwith their corresponding keys. The input first goes through theglobal query function to find the corresponding global prompts.Subsequently, the embedding of is concatenated with the promptsto form . Then is processed by the local query function tofind the corresponding fine-grained prompts {, }. Thus, ViTmodifies its MSA layer based on and loads the local classificationhead , forming V . The local prompt training loss function is",
  "EXPERIMENTS5.1Experimental Setup": "5.1.1Datasets. We conduct extensive experiments on CIFAR-100 with 5 incremental tasks to evaluate the effectiveness of ourFedMGP in addressing the challenges of PFCL. CIFAR-100 is awidely used benchmark dataset and consists of 60,000 RGB colorimages, each of size 32x32 pixels, classified into 100 different classes.We consider two practical scenarios of FCL, namely synchronousFCL and asynchronous FCL.In the synchronous FCL settings , clients have the same tasksequences but a varied proportion of samples from each class. It is acommon setting employed in existing FCL works . The degree ofdata heterogeneity in this scenario is controlled with the Dirichletparameter, which is set to be 1 in our experiments. Specifically, wefirst partition the dataset into 5 tasks, each containing 20 classes,with no overlapping class between tasks. Then, within each task, thesamples of each class are randomly divided into the same numberof subsets as the total number of clients, ensuring that the dataamong clients is also non-overlapping.In the asynchronous FCL settings , some of the classes areaccessible to all clients while others are private to certain clients,which is derived from pathological Non-IID in static FL . In thissetting, we consider that there are 15 private classes for each client.Each task contains 8 classes. To be specific, each client first selects15 classes unique to itself, and only that client has access to the fulldata of these classes. Therefore, there are 25 classes lefted as publicclasses shared by all clients. As a result, each client has data for 40classes. The client then randomly divides these 40 classes into 5tasks, each containing 8 classes. 5.1.2Baselines and Backbones. We compare FedMGP with FedAvg, FedEWC , FedProx and GLFC on ResNet-18 .Since our method is based on ViT-B/16, we also conduct experi-ments on ViT-B/16 to compare FedMGP with FedViT. FedViT isa naive combination of FedAvg and ViT , which performs fed-erated training by locally updating and globally aggregating theparameters of the classifier heads iteratively. FedL2P and FedDualPare the adapted versions of two effective prompt-based methodsin traditional CL, L2P and DualPrompt , making themmore suitable for use in a federated environment. More detaileddescriptions are in Appendix 3. 5.1.3Implementation Details. In our setup, the federated systemconsists of five clients and one central server, and each client pos-sesses a sequence of five tasks. We repeat experiments with threerandom seeds (42,1999,2024) and report the averaged outcomes.Across all methods, we fix the number of clients at five and theinterval rounds for increments at five. We employ Adam as theoptimizer with a learning rate of 0.001. The whole training processis performed sequentially on an NVIDIA GPU RTX-3090.",
  "Expermental Results": "We use the accuracy of the aggregated global model on local testsets as the metric in . To examine the impact of differentbackbone networks on the experimental results, we employed base-line methods based on two backbones, namely ResNet-18 and thepre-trained ViT.Surprisingly, all methods generally perform better in the asyn-chronous setting than in the synchronous setting. This is attributedto the fact that in the synchronous setting, each task involves 20classes. GLFC, FedAvg, and FedProx failed in both asynchronousand synchronous FCL. As expected, methods based on ViT outper-formed those based on ResNet-18 in both scenarios. But FedAvgperforms even better than FedViT and FedDual in synchronousFCL. This indicates that in scenarios with similar data distributions,FedAvg has the ability to challenge large pre-trained models.In all methods using ViT as the backbone, FedL2P with promptsperformed better than using ViT alone. Unfortunately, FedDualPperformed even worse than the simple FedViT. We believe this isdue to the heterogeneity in the learned parameters across clients.Moreover, the performance of these methods did not show signifi-cant improvement after aggregation. In fact, FedViT experienced adecrease of 3.9% in average accuracy after aggregation in synchro-nous FCL and a decrease of 7.27% in asynchronous FCL.Our method achieved the best performance in both synchronousand asynchronous settings, with accuracies of 90.56% and 83.46%,showing the state-of-the-art performance of fusing heterogeneousknowledge. Although our method performs well on this metriceven without some components, such as Ours-w/oGP achieving89.36% and 81.06%, and Ours-w/oLp achieving 87.29% and 77.93%,the ability to retain spatial-temporal knowledge is significantlyaffected. In the following section (Sec. 5.3), we will evaluate eachmethod using new metrics, i.e., temporal knowledge retention andspatial knowledge retention, to evaluate the resistance of spatial-temporal catastrophic forgetting.",
  "Ablation Studies": "To further validate the effectiveness of the multi-granularity knowl-edge space, we conducted three different ablation experiments un-der the same experimental setup. These experiments respectively re-moved the global prompts, local prompts, and the selective promptfusion mechanism on the server. Results are shown in .In both asynchronous and synchronous settings, ViT-based meth-ods have demonstrated exceptional performance in retaining spatialknowledge. This result also confirms our hypothesis: having sim-ilar cognition is the foundation for knowledge sharing. Based onthat, the increment of spatial knowledge retention of FedAvg in thesynchronous setting is not difficult to understand, as similar data",
  "(d) Synchronous": ": Ablation Studies using temporal knowledge retention (Equation 1) spatial knowledge retention (Equation 2) in twoFCL setting. Note that, \"Ours-w/oGP\" refers to our method without global prompts representing coarse-grained knowledge.\"Ours-w/oLP\" refers to our method without local prompt to capture client/time relevant knowledge. And \"Ours-w/oSPF\" is touse FedAvg to aggregate global prompts instead of Selective Prompt Fusion. contributes to the similarity of convolutional layers. While thesemethods have effectively preserved spatial knowledge, none ofthem demonstrates resistance to temporal catastrophic forgetting.In (b) and (d), it is challenging to distinguish the differ-ence between FedL2P, FedDualP and other methods with ResNet18as the backbone network, as their temporal knowledge retentionrates are all around 20%.Our approach not only competes with other ViT methods interms of spatial knowledge retention but also achieves almost noforgetting in temporal knowledge retention, thanks to the construc-tion of the multi-granularity knowledge space. To evaluate the con-tribution of the coarse-grained global prompt and the fine-grainedlocal prompt, three different ablation experiments are conducted,which respectively removed global prompts (Ours-w/oGP), localprompts (Ours-w/oLP), and selective prompt fusion (Ours-w/oSPF).In (a), there is a slight decrease in spatial knowledge retentionwhen we remove the global prompt. The other two componentshave little impact on spatial forgetting. However, things become more complex when it comes to temporal knowledge retention.Without local prompts, it drops significantly to around 15%. Andwhen we remove global prompts, although the retention also de-creases, it is not as drastic.It concludes that fine-grained local prompts play a crucial rolein preventing temporal catastrophic forgetting, and they still needto be combined with coarse-grained knowledge to better preventspatial-temporal catastrophic forgetting and achieve personaliza-tion. Hence, multi-granularity knowledge representation is a promis-ing direction in PFCL.",
  "(b) Local": ": Sensitivity analyses of prompt length and promptpool size. Left: Global Spatial Knowledge Retention Ratio (%)w.r.t. prompt length and prompt pool size . Right: LocalSpatial Knowledge Retention Ratio (%) w.r.t. prompt length and prompt pool size . From (a), it can be observed that, regardless of the values ofprompt length and pool size, it is beneficial for spatial knowledgeof the global prompts. Additionally, under the condition of PoolSize=1 and Prompt Length=10, spatial knowledge retention is thehighest, reaching 100.37%.From (b), it can be seen that different values of promptpool size and prompt length have little effect on spatial knowledgeretention of the local prompts. It implies utilizing multi-granularityprompts is capable of training a generalized global model as wellas personalized local models. More sensitivity analyses are shownin Appendix 2.",
  "DISCUSSION": "This section will provide a preliminary analysis and discussionof the computational cost, communication overhead, and privacyprotection in federated learning for FedMGP.Computational cost. The clients have only two parts to train:coarse-grained global prompts and fine-grained local prompts. Thesize of the global prompt pool of one client is determined by thenumber of prompts, prompt length, and embedding dimension,which are set to 10, 10, and 768 in the experiments. And the sizeof prompt keys is determined by the pool size and embeddingdimension. In our experimental setup, the total size of local promptsis 4,608,000, and the size of their corresponding keys is also thesame as the global prompts keys, which is 7,680. In summary, eachclient has a total of 4,700,160 parameters to train.Moreover, the server only needs to aggregate the global prompts.This means that the training process of local prompts can proceedin parallel with the servers aggregation process.Communication overhead. Our method transmits only coarse-grained global prompts and keys, keeping communication overheadlow. The size of the global prompt pool per client is determined bythe number of prompts, prompt length, and embedding dimension(set to 10, 10, and 768 in experiments). Prompt keys size dependson pool size and embedding dimension. Thus, the total transmittedsize is 76,800 + 7,680 parameters. Although there are fine-grainedlocal parts that also need to be trained, they remain local, which significantly reduces the communication overhead compared totraditional methods, indirectly enhancing privacy.Privacy protection. Since FedMGP only transmits the coarse-grained global prompts obtained from the ViT and their keys,without uploading the original embeddings of the images and thefine-grained local prompts, FedMGP has strong privacy protec-tion, especially against gradient leakage attacks. Moreover, in ourexperimental setup, the size of global prompts is only 76,800 pa-rameters, containing much less information, which also ensuresprivacy protection.",
  "CONCLUSION": "Personalized Federated Continual Learning is a novel and practicalscenario. It not only requires the accumulation of knowledge thatevolves over time and space but also needs consideration of per-sonalized strategies to make generalized knowledge better adaptedto local requirements. Moreover, spatial-temporal catastrophic for-getting is also a key issue that needs to be addressed.In this paper, we first formulated a formal problem definitionfor PFCL and shaped the objectives of PFCL as three folds: (1)Alleviating spatial knowledge catastrophic forgetting caused bydata heterogeneity; (2) Mitigating temporal knowledge catastrophicforgetting caused by dynamic task streams; (3) Training customizedlocal models to achieve personalization.To address these issues, we proposed a multi-granularity knowl-edge space for federated continuous learning (termed as FedMGP),which has efficient fusion and personalization by representingknowledge at different granularities. Specifically, the FedMGP uti-lizes a shared ViT to construct coarse-grained global prompts andmodifies the ViT with local prompts based on these global prompts.In addition, we designed 1) global prompts on the embedding layerto learn coarse-grained knowledge continually and 2) local promptson the multi-head self-attention layer to learn fine-grained knowl-edge as a complementary to achieve personalization. Extensiveexperiments under synchronous and asynchronous FCL settingsare conducted to demonstrate the effectiveness of our method.The effectiveness of multi-granularity knowledge representationhas been experimentally proven in this work, and their comple-mentary nature significantly enhances the models resistance tospatial-temporal catastrophic forgetting. Our future research willinvestigate the multi-granularity representation of knowledge invarious federated learning scenarios such as vertical federated learn-ing and multi-objective federated learning . We will exploreits implications for privacy preservation, model performance, algo-rithm efficiency, and so on, aiming at achieving trustworthy PFCL. This work was supported by the National Natural Science Foun-dation of China (Nos. 72242106, 62176221), the Natural ScienceFoundation of Sichuan Province (No. 2022NSFSC0528), Sichuan Sci-ence and Technology Program (No. 2024YFHZ0024), Jiaozi Instituteof Fintech Innovation in Southwestern University of Finance andEconomics (Nos. kjcgzh20230103, kjcgzh20230201) and the Funda-mental Research Funds for the Central Universities (YJ202421).",
  "Manoj Ghuhan Arivazhagan, Vinay Aggarwal, Aaditya Kumar Singh, and SunavChoudhary. 2019. Federated learning with personalization layers. arXiv preprintarXiv:1912.00818 (2019)": "Luxin Cai, Naiyue Chen, Yuanzhouhan Cao, Jiahuan He, and Yidong Li. 2023.FedCE: Personalized Federated Learning Method based on Clustering Ensembles.In Proceedings of the 31st ACM International Conference on Multimedia. 16251633. Shangxuan Cai, Yunfeng Zhao, Zhicheng Liu, Chao Qiu, Xiaofei Wang, andQinghua Hu. 2022. Multi-granularity Weighted Federated Learning in Het-erogeneous Mobile Edge Computing Systems. In 2022 IEEE 42nd InternationalConference on Distributed Computing Systems. IEEE, 436446. Jiayi Chen and Aidong Zhang. 2022. FedMSplit: Correlation-adaptive federatedmulti-task learning across multimodal split networks. In Proceedings of the 28thACM SIGKDD Conference on Knowledge Discovery and Data Mining. 8796. Ziyang Chen, Jinzhi Liao, and Xiang Zhao. 2023. Multi-granularity TemporalQuestion Answering over Knowledge Graphs. In Proceedings of the 61st AnnualMeeting of the Association for Computational Linguistics. 1137811392. Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, AleLeonardis, Gregory Slabaugh, and Tinne Tuytelaars. 2021. A continual learningsurvey: Defying forgetting in classification tasks. IEEE Transactions on PatternAnalysis and Machine Intelligence 44, 7 (2021), 33663385. Jiahua Dong, Lixu Wang, Zhen Fang, Gan Sun, Shichao Xu, Xiao Wang, and QiZhu. 2022. Federated class-incremental learning. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition. 1016410173. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, GeorgHeigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformersfor image recognition at scale. arXiv preprint arXiv:2010.11929 (2020). Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. 2020. Personalized fed-erated learning with theoretical guarantees: A model-agnostic meta-learningapproach. Advances in Neural Information Processing Systems 33 (2020), 35573568.",
  "Yan Kang, Tao Fan, Hanlin Gu, Lixin Fan, and Qiang Yang. 2023. Groundingfoundation models through federated transfer learning: A general framework.arXiv preprint arXiv:2311.17431 (2023)": "Yan Kang, Hanlin Gu, Xingxing Tang, Yuanqin He, Yuzhu Zhang, Jinnan He,Yuxing Han, Lixin Fan, Kai Chen, and Qiang Yang. 2023. Optimizing privacy,utility and efficiency in constrained multi-objective federated learning. arXivpreprint arXiv:2305.00312 (2023). Muhammad Gul Zain Ali Khan, Muhammad Ferjad Naeem, Luc Van Gool, DidierStricker, Federico Tombari, and Muhammad Zeshan Afzal. 2023. Introducinglanguage guidance in prompt-based continual learning. In Proceedings of theIEEE/CVF International Conference on Computer Vision. 1146311473. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, GuillaumeDesjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, AgnieszkaGrabska-Barwinska, et al. 2017. Overcoming catastrophic forgetting in neuralnetworks. Proceedings of the National Academy of Sciences 114, 13 (2017), 35213526.",
  "Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning multiple layers of featuresfrom tiny images. (2009)": "Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The Power of Scale forParameter-Efficient Prompt Tuning. In Proceedings of the 2021 Conference onEmpirical Methods in Natural Language Processing. 30453059. Miaomiao Li, Jiaqi Zhu, Xin Yang, Yi Yang, Qiang Gao, and Hongan Wang. 2023.CL-WSTC: Continual Learning for Weakly Supervised Text Classification on theInternet. In Proceedings of the ACM Web Conference. 14891499.",
  "Lumin Liu, Jun Zhang, SH Song, and Khaled B Letaief. 2020. Client-edge-cloudhierarchical federated learning. In ICC 2020-2020 IEEE International Conferenceon Communications. IEEE, 16": "Yang Liu, Yan Kang, Tianyuan Zou, Yanhong Pu, Yuanqin He, Xiaozhou Ye, YeOuyang, Ya-Qin Zhang, and Qiang Yang. 2024. Vertical Federated Learning:Concepts, Advances, and Challenges. IEEE Transactions on Knowledge and DataEngineering (2024). Xiaosong Ma, Jie Zhang, Song Guo, and Wenchao Xu. 2022. Layer-wised modelaggregation for personalized federated learning. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition. 1009210101.",
  "Zheda Mai, Ruiwen Li, Jihwan Jeong, David Quispe, Hyunwoo Kim, and ScottSanner. 2022. Online continual learning in image classification: An empiricalsurvey. Neurocomputing 469 (2022), 2851": "Marc Masana, Xialei Liu, Bartomiej Twardowski, Mikel Menta, Andrew D Bag-danov, and Joost Van De Weijer. 2022. Class-incremental learning: survey andperformance evaluation on image classification. IEEE Transactions on PatternAnalysis and Machine Intelligence 45, 5 (2022), 55135533. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, andBlaise Aguera y Arcas. 2017. Communication-efficient learning of deep net-works from decentralized data. In Artificial Intelligence and Statistics. PMLR,12731282. Zijie Pan, Li Hu, Weixuan Tang, Jin Li, Yi He, and Zheli Liu. 2021. Privacy-preserving multi-granular federated neural architecture search a general frame-work. IEEE Transactions on Knowledge and Data Engineering (2021). Krishna Pillutla, Kshitiz Malik, Abdel-Rahman Mohamed, Mike Rabbat, MaziarSanjabi, and Lin Xiao. 2022. Federated learning with partial model personalization.In International Conference on Machine Learning. PMLR, 1771617758. James Seale Smith, Leonid Karlinsky, Vyshnavi Gutta, Paola Cascante-Bonilla,Donghyun Kim, Assaf Arbelle, Rameswar Panda, Rogerio Feris, and Zsolt Kira.2023. CODA-Prompt: COntinual Decomposed Attention-based Prompting forRehearsal-Free Continual Learning. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition. 1190911919. Benyuan Sun, Hongxing Huo, Yi Yang, and Bo Bai. 2021. Partialfed: Cross-domainpersonalized federated learning via partial initialization. Advances in NeuralInformation Processing Systems 34 (2021), 2330923320.",
  "Alysa Ziying Tan, Han Yu, Lizhen Cui, and Qiang Yang. 2022. Towards person-alized federated learning. IEEE Transactions on Neural Networks and LearningSystems (2022)": "Saeed Vahidian, Mahdi Morafah, Weijia Wang, Vyacheslav Kungurtsev, ChenChen, Mubarak Shah, and Bill Lin. 2023. Efficient distribution similarity identifi-cation in clustered federated learning via principal angles between client datasubspaces. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 37.1004310052. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin. 2017. Attention is allyou need. Advances in Neural Information Processing Systems 30 (2017). Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, Han Zhang, Chen-YuLee, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, et al. 2022. Dualprompt:Complementary prompting for rehearsal-free continual learning. In EuropeanConference on Computer Vision. Springer, 631648. Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren,Guolong Su, Vincent Perot, Jennifer Dy, and Tomas Pfister. 2022. Learning toprompt for continual learning. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition. 139149. Hao Xiao, Weiyao Lin, Bin Sheng, Ke Lu, Junchi Yan, Jingdong Wang, Errui Ding,Yihao Zhang, and Hongkai Xiong. 2018. Group re-identification: Leveraging andintegrating multi-grain information. In Proceedings of the 26th ACM InternationalConference on Multimedia. 192200. Fu-En Yang, Chien-Yi Wang, and Yu-Chiang Frank Wang. 2023. Efficient modelpersonalization in federated learning via client-specific prompt generation. InProceedings of the IEEE/CVF International Conference on Computer Vision. 1915919168. Hongwei Yang, Hui He, Weizhe Zhang, and Xiaochun Cao. 2020. FedSteg: A feder-ated transfer learning framework for secure image steganalysis. IEEE Transactionson Network Science and Engineering 8, 2 (2020), 10841094.",
  "SENSITIVITY ANALYSIS": "As illustrated in , the aggregation of global prompts has im-proved the performance of both global prompts and local prompts.(a) shows the performance improvement of coarse-grainedglobal prompts evaluated with test accuracy (%) after the aggre-gation of global prompts. (b) illustrates the performance im-provement of fine-grained local prompts after the aggregation ofglobal prompts. We can find that both improvements are robust todifferent values of prompt pool size and prompt length.",
  "(b) Gain Af L": ": Sensitivity analyses of prompt length and promptpool size. Left: The performance improvement (%) of coarse-grained global prompts after the aggregation of globalprompts w.r.t. prompt length and prompt pool size .Right: The performance improvement (%) of fine-grainedlocal prompts after the aggregation of global prompts w.r.t.prompt length and prompt pool size . illustrates the temporal knowledge retention of the globalmodel and local models. As illustrated in the left sub-figure (a),the performance of global spatial knowledge retention exhibitsrobustness to different values of prompt length and prompt pool size,which means FedMGP achieves spatial-temporal transfer effectively.From (b), we can conclude that different values of prompt poolsize and prompt length have little effect on local spatial knowledgeretention, indicating that FedMGP mitigates temporal catastrophic",
  "BASELINES": "FedAvg : FedAvg is a fundamental algorithm in federated learn-ing. It works by first distributing a global model to multiple clients.Each client trains the model locally using its own data for a fewepochs. Then, the clients send their locally updated models backto a central server. The server aggregates these local models bycomputing their weighted average to update the global model. Thisprocess is repeated for several rounds until the global model con-verges.FedEWC : a combination of FedAvg and EWC, which is a com-monly used baseline in PFL and FCL. EWC is a regularization-basedCL method, mitigating forgetting by penalizing the changes of im-portant parameters of the previous tasks.FedProx : a heterogeneous and static FL method. It smoothsdata heterogeneity by adding a proximal term in the local objective.GLFC (Glocal Local Forgetting Compensation) : a synchronousFCIL method. GLFC designs a class-aware gradient compensationloss and a class-semantic relation distillation loss to mitigate forget-ting and distill consistent inter-class relations across tasks. A proxyserver is implemented to select the optimal previous global modelto assist the class-semantic relation distillation and a prototypegradient-based communication mechanism is developed to protectdata privacy.FedViT : a hybrid method of ViT and FedAvg. The global ag-gregation is performed by computing the average weights of theclassification heads.FedL2P : a hybrid method of L2P and FedAvg. L2P is a prompt-based CL method, which applies learnable task-specific prompts tomitigate forgetting.FedDualP : a hybrid method of DualPrompt and FedAvg. Du-alPrompt, a prompt-based CL method derived from L2P, decouplesthe learnable prompts into general and expert prompts, encodingtask-invariant and task-specific knowledge, respectively."
}