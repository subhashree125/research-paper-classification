{
  "ABSTRACT": "Recent studies have highlighted fairness issues in Graph NeuralNetworks (GNNs), where they produce discriminatory predictionsagainst specific protected groups categorized by sensitive attributessuch as race and age. While various efforts to enhance GNN fair-ness have made significant progress, these approaches are oftentailored to specific sensitive attributes. Consequently, they neces-sitate retraining the model from scratch to accommodate changesin the sensitive attribute requirement, resulting in high computa-tional costs. To gain deeper insights into this issue, we approachthe graph fairness problem from a causal modeling perspective,where we identify the confounding effect induced by the sensitiveattribute as the underlying reason. Motivated by this observation,we formulate the fairness problem in graphs from an invariantlearning perspective, which aims to learn invariant representationsacross environments. Accordingly, we propose a graph fairnessframework based on invariant learning, namely FairINV, whichenables the training of fair GNNs to accommodate various sensitiveattributes within a single training session. Specifically, FairINVincorporates sensitive attribute partition and trains fair GNNs byeliminating spurious correlations between the label and various sen-sitive attributes. Experimental results on several real-world datasetsdemonstrate that FairINV significantly outperforms state-of-the-artfairness approaches, underscoring its effectiveness. 1",
  "Corresponding author.1Our code is available via:": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08 ACM Reference Format:Yuchang Zhu, Jintang Li, Yatao Bian, Zibin Zheng, and Liang Chen. 2024.One Fits All: Learning Fair Graph Neural Networks for Various SensitiveAttributes. In Proceedings of the 30th ACM SIGKDD Conference on KnowledgeDiscovery and Data Mining (KDD 24), August 2529, 2024, Barcelona, Spain.ACM, New York, NY, USA, 12 pages.",
  "INTRODUCTION": "Graph neural networks (GNNs) have achieved tremendous successin processing graph-structured data , such as citationnetworks and social networks . Consequently, thisadvancement has led to their application across diverse domains,including fraud detection and recommender systems . How-ever, recent studies have unveiled a concerning trend thatGNNs make discriminatory decisions toward the specific protectedgroups defined by sensitive attributes, e.g., race, and age. This phe-nomenon, termed the group fairness problem of GNNs, hinders theapplication of GNNs in high-stake scenarios.To improve the fairness of GNNs, considerable efforts have beendevoted to debiasing the training data or learning fair GNNsdirectly , referred to as the pre-process and in-process ap-proaches, respectively. Within these two methodological categories,common implementations encompass adversarial learning ,distribution alignment among various protected groups ,graph-structured data modification , and edge reweight-ing . Despite significant progress, these approaches are oftentailored to specific sensitive attributes, as shown in (a).Consequently, training GNN models from scratch becomes impera-tive when faced with fairness requirement alterations in sensitiveattributes, such as transitioning from age-based considerations togender-related factors. Take loan approvals in a credit card networkas an example, according to fairness policies, initially trained GNNsare designed to make fair decisions toward the protected groupsdivided by age, e.g., age 25 and age > 25. However, when policieschange to focus on gender, necessitating fair treatment betweenmale and female groups, the previously tailored model optimizedfor age fairness becomes inadequate. Hence, this mandates retrain-ing the GNN model to ensure fairness regarding gender, which is alaborious and computationally intensive process.In summary, there is a significant demand for a universal graphfairness approach that trains fair GNNs across various sensitiveattributes in a single training session. Achieving such an approachentails addressing the following challenges: (1) Generalization",
  "Clean Graph": "Fairness-awareFramework DebiasingFramework GenderGNNs GNNs : An illustration of comparison between prior works and our work (FairINV). (a) Prior works (pre-process and in-process)achieve fairness toward the specific sensitive attribute; (b) Our work trains fair GNNs toward various sensitive attributes in asingle training session without accessing the sensitive attribute. to various sensitive attributes. Previous studies aim to achievefairness tailored for the specific sensitive attribute. Additionally,these approaches always require accessing the sensitive attributesin the training process, which is impractical in real-world scenariosdue to legal limitations . Correspondingly, our first chal-lenge is to design a fairness framework that achieves fairness w.r.t.various sensitive attributes in a single training session withoutaccessing the sensitive attribute, as shown in (b). (2) Fullfairness. According to .2, two causal pathways ( and G ) demonstrate how the sensitive attribute in-fluences the labels , misleading the trained GNNs to capture thesensitive attribute information for predictions. In this regard, isa confounder. To achieve full fairness, blocking these two causaleffects appears to be a straightforward solution. However, it is chal-lenging due to the presence of underlying spurious correlationsbetween unobservable variables and. As discussed in .2,prior works failed to eliminate both causal effects concurrently. In-spired by INV-REG , backdoor adjustment implemented by datapartition presents a promising approach to tackle this challenge.In this work, we first formulate the graph fairness issue from aninvariant learning perspective, where sensitive attributesas environments. Building upon this formulation, we propose a uni-versal graph fairness framework named FairINV. To overcome thefirst challenge, FairINV jointly optimizes a fair GNN for multiplesensitive attributes inferred automatically via sensitive attributepartition. To overcome the second challenge, FairINV incorporatesinvariant learning optimization objectives building upon sensitiveattribute partition to remove confounding effects induced by .Specifically, the optimization objective of FairINV gives rise toequal predictions of trained GNNs across environments (sensitiveattributes). In summary, FairINV mitigates spurious correlations be-tween various sensitive attributes and the label. Our contributionscan be summarized as follows:",
  "We study the fairness issue on graphs from an invariantlearning perspective. To the best of our knowledge, this isthe first attempt to explore graph fairness from this particularperspective": "We introduce FairINV, a universal graph fairness frameworkthat inherits the spirit of graph invariant learning. An unsu-pervised sensitive attributes partition of FairINV facilitatesfairness improvement in terms of various sensitive attributes. We conduct experiments on several real-world datasets tovalidate the effectiveness of FairINV. Experimental resultsshow that FairINV can train a fair GNN toward varioussensitive attributes in a single training session.",
  "RELATED WORK2.1Fairness in Graph Neural Networks": "The fairness of GNNs includes group fairness and in-dividual fairness . Our study focuses on the group fair-ness aspect, emphasizing equitable model decisions for each pro-tected group partitioned by the sensitive attribute. Recent stud-ies improving group fairness in GNNs typically segregate intopre-process and in-process approaches. Pre-process approaches aim to mitigate biases in training data beforetraining downstream tasks. To mitigate biases, techniques like ad-versarial learning , and distribution alignment serveas optimization objectives for debiasing training data. Additionally,some heuristic approaches modify the training graph orreweight edge by either enhancing connections betweendiverse groups or reducing connections within the same groups.In-process approaches aim to train fair GNNs through the fairness-aware framework. Similar to pre-process approaches, in-processapproaches also incorporate adversarial learning and dis-tribution alignment to learn GNNs. Despite significantprogress, these approaches are tailored to the specific sensitiveattribute, lacking considerations for various sensitive attributes.Despite Bose et al.s work of a compositional adversarialframework using a set of sensitive-invariant filters, it necessitatesprior knowledge of considered sensitive attributes and their specificvalues for each individual. In contrast, our work learns fair GNNstoward various sensitive attributes in a single training session with-out accessing sensitive attributes, which remains under-exploredfor prior works.",
  "One Fits All: Learning Fair Graph Neural Networksfor Various Sensitive AttributesKDD 24, August 2529, 2024, Barcelona, Spain": "across different environments while disregarding spurious correla-tions that exhibit variability . However, there is limited researchexploring the application of invariance learning in fairness. Adragnaet al. empirically illustrate how invariant risk minimization ininvariant learning can contribute to building fair machine learningmodels. Ma et al. point out the fairness-related bias in facerecognition stemming from confounding demographic attributes.Then, they iteratively partition data to annotate confounders andlearn invariant features to remove the confounding effect. Yet, theseexplorations of invariant learning in fairness predominantly focuson Euclidean data. Conversely, significant efforts haveaddressed the out-of-distribution problem within graph structuresfrom an invariant learning perspective. However, the effectivenessof invariant learning in ensuring graph fairness remains an under-explored area. To the best of our knowledge, our work is the firstto explore the graph fairness problem utilizing graph invariantlearning.",
  "Notations": "Let G = (V, E, X) denote an undirected and unweighted attributedgraph, where V is a set of nodes and E is a set of edges. Mean-while, |V| = and |E| = represent the number of nodes andedges, respectively. X R represents the node attribute ma-trix without the sensitive attribute where is the node attributedimension. A {0, 1} is the adjacency matrix where A = 1indicates the edge connection E between the node andthe node , and A = 0 otherwise. Nodes with the same sensitiveattribute value belong to the same protected group. Most GNNs fol-low the message-passing mechanism, which aggregates messagesfrom their neighbors, and can be summarized as follows:",
  "h()= UPD() ({h(1), AGG() ({h(1): N()})}),(1)": "where is the layer number, AGG() () and UPD() () denote ag-gregation function and update function in -th layer, respectively.N () denote the set of nodes adjacent to node .While our approach is applicable to various downstream tasks,in this paper, we exemplify its application using the node classifi-cation downstream task to illustrate the proposed methodology. AGNN model , consisting of an encoder and a linear classifier, takes a graph G as input and outputs the node predicted label = ((G)). The goal of is to predict such that it is as closeas possible to the ground truth labels .",
  "Causal Analysis": "To broaden insights, we construct a structural causal model (SCM) to analyze the group fairness issue in graph-structured data.(a) illustrates the causal relationship among the graph G,the sensitive attribute , and the node label . In the SCM, there aretwo causal pathways by which the sensitive attribute affects thenode label , leading to issues of group fairness in GNNs. A detaileddescription of these two causal pathways is provided below. S G Y Pre-process In-process SSensitive AttributeGObserved GraphYNode Label S G Y (a) Causal Graph(b) Causal Graph of prior works(c) Causal Graph of FairINV S G Y S-related information : Structural causal model for GNNs prediction. (a)The fairness issue on graphs can be caused by two causalpathways, i.e., and G ; (b) Prior works ei-ther exclusively eliminate the causal pathway G orexclusively eradicate the causal pathway ; (c) FairINVtackles the fairness issue through blocking both two causalpathways. G . This causal pathway describes the influenceof on the formation of graph-structured data G, whichsubsequently impacts the predictions of node labels by thetrained GNN. Specifically, the path G represents the im-pact of on the generation process of graph-structured data.In this context, the data exhibit two primary phenomena: (1)the graph topology exhibits sensitive homophily , whereconnected nodes are more likely to share the same sensitiveattribute . (2) Non-sensitive node attributes may implicitlyconvey information about . For instance, if representsgender, certain attributes like height, while not directly sen-sitive, become relevant in inferring an individuals gender.The path G encapsulates the training process of GNNs,wherein the network may inherit and subsequently propa-gate biases (information related to ) present in the trainingdata. . This causal pathway illustrates the underlying cor-relation between the node label and the sensitive attribute.This correlation often originates from societal discrimina-tion against protected groups, leading to biased predictionsin the trained GNN. For instance, given a social networkdataset, the task is to predict the users occupational field.The dataset predominantly comprises occupations of femalesas nurses and males as engineers. Consequently, the GNNtrained on this dataset tends to predict engineering as theoccupational field for males and nursing for females, thusrevealing a gender bias in its predictions. This phenomenoncan be attributed to the causal pathway inherent inthe dataset.In summary, discriminatory decisions in GNNs stem from thetwo causal pathways discussed above. In this regard, the pathway G is a backdoor path, with acting as a confounder.This pathway may mislead the trained GNN to utilize the sensitiveattribute for predictions, known as the spurious confounding effect.To remove this effect, a straightforward yet challenging solution in-volves eliminating G and . However, prior workshave not successfully removed both pathways simultaneously. As",
  "KDD 24, August 2529, 2024, Barcelona, SpainYuchang Zhu, Jintang Li, Yatao Bian, Zibin Zheng, & Liang Chen": "shown in (b), pre-process methods primarily focus on reduc-ing the information related to the sensitive attribute in the trainingdata, effectively removing the path G. Conversely, in-processmethods strive to develop a fair GNN that makes decisions indepen-dently of , akin to removing the path . Another approach tomitigate confounding effects is the backdoor adjustment, achievedby partitioning the training data into different splits. In our scenar-ios, we partition nodes into distinct demographic groups and learnGNNs invariant across these groups. Drawing inspiration from afairness study in face recognition , we attempt to formulate thegraph fairness issue from an invariant learning perspective. Lever-aging the environment inference capabilities of invariant learning,we can unsupervisedly infer the sensitive attribute of nodes, facili-tating group partitioning.",
  "Problem Formulation": "In this subsection, we formulate the graph fairness problem from aninvariant learning perspective. Our work focuses on the node-leveltask. Following the setting of EERM , we investigate the impactof the nodes ego-graph on the centered node. Specifically, given asingle graph G = (V, E, X), we have a set of ego-graph D = D from various environment E, where D = {G,} is graphsfrom environment. G and are the ego-graph and the node labelof node . The invariant learning aims to learn GNNs to generalizeto all unseen environments. Denote as a GNN model consistingof an encoder and a classifier, = (G) as the predicted labelof node , and R () as the empirical risk under environment .Formally, the invariant learning on the node level is to minimize:",
  "minmaxER ( )(2)": "where R ( ) = EG, [( (G),)], (, ) is the loss function.Based on the above minimization objective, the trained GNNperforms equally across all environments. Similarly, the goal offairness on the graph is to have the model equally treat different de-mographic groups divided by the sensitive attribute . In this regard,the centered node (the ego-graph) with different sensitive attributevalues or under different sensitive attributes can be regarded as agraph under different environments. Naturally, a fairness problemon graphs can be formulated as a form of invariant learning.In this work, we aim to learn fair GNNs toward various sensitiveattributes in a single training session. With the formulation ofinvariant learning, our goal is transformed into learning GNNsinvariant across different sensitive attributes and sensitive attributevalues. Formally, our goal is to minimize:",
  "Overview": "FairINV focuses on the node-level task, aiming to learn GNNs in-variant across various sensitive attributes within a single trainingsession, thereby achieving fairness on graph-structured data. Asshown in , our proposed method FairINV comprises twomodules, i.e., sensitive attribute partition (SAP) and sensitive invari-ant learning (SIL). The SAP module partitions nodes into differentsubsets by inferring variant ego-subgraphs for each centered node.It should be noted that the sampling of ego-subgraphs can be disre-garded due to the message-passing mechanism, which effectivelyaggregates the representations of neighboring nodes to update itsown representation. To optimize this module, we employ the In-variant Risk Minimization (IRM) objective , maximizing it toguide the SAP module in capturing the worst-case environment.This process can be seen as inferring the sensitive attribute valueof nodes. Since the maximization of the IRM objective is executedin an unsupervised manner, we can iteratively predict sensitiveattributes multiple times. This iterative process enables FairINV toachieve fairness with respect to various sensitive attributes in a sin-gle training session. Due to the formulation of the fairness problemfrom an invariant learning perspective in .3, we can natu-rally tackle this problem through invariant learning. Specifically,based on the partition results of SAP, the SIL module learns a GNNinvariant across different sensitive attribute partitions through avariance-based loss. The objective of being invariant across differ-ent sensitive attribute partitions implies the equitable treatmentof different demographic groups, thereby achieving fair decision-making. Overall, the SAP module is akin to data augmentation,facilitating the process of the SIL module. Due to such a trainingparadigm, FairINV follows the same inference process as vanillaGNNs.",
  "Sensitive Attributes Partition": "Existing methods are designed for the specific sensitive attributewhile assuming accessible sensitive attributes. However, these meth-ods are impractical in real-world scenarios due to legal restrictions.To overcome this challenge, there is a need to infer the sensitiveattribute value for each node in an unsupervised manner. Infer-ring the sensitive attribute value multiple times can be regardedas obtaining multiple sensitive attribute values, e.g., gender, andrace, facilitating the achievement of fairness w.r.t. various sensitiveattributes. Unfortunately, learning a sensitive attribute inferencemodel without access to the sensitive attribute ground truth is anon-trivial task.Inspired by the unsupervised environment inference in invariantlearning , we aim to maximize variability across environmentsto achieve the sensitive attribute partition. Based on our formula-tion of the fairness problem from an invariant learning perspective,the sensitive attributes can be seen as environments in invariantlearning. Nodes with different sensitive attribute values can beconsidered as being in different environments. Thus, maximizingvariability across environments indicates inferring a worst-case",
  ": The overview of FairINV. FairINV includes two stages: Sensitive Attributes Partition (SAP) and Sensitive InvariantLearning (SIL)": "sensitive attribute partition, where GNNs exhibit the worst fair-ness performance towards the demographic group divided by thesensitive attribute. However, directly inferring sensitive attributespartition through the aforementioned maximization objective isimpractical due to the interactive nature of graph-structured data.Following the inspiration from GIL , identifying variant sub-graphs as auxiliary information for sensitive attribute partition mayprovide a desirable solution.Following the above idea, we construct the SAP module to inferthe sensitive attribute value of each node. Specifically, the SAPmodule consists of a pre-trained GNN backbone , a variant infer-ence model , and a sensitive attribute inference model . Withan expected structure identical to the GNN model to be trained, serves as an Empirical Risk Minimization-trained (ERM-trained)reference model. In other words, it is trained on the node clas-sification task in a semi-supervised manner to capture spuriouscorrelations between variant patterns and node labels. Given anattributed graph G = (V, E, X) with unknown sensitive attributevalues, we sample an ego-graph set {G}V, where G is theego-graph of the centered node . takes G as input and outputsthe node representation h = (G). Due to the similar processbetween ego-graphs sampling and the message-passing mechanismof GNN, the sampling of ego-graphs can be disregarded. For twoconnected nodes and in G, takes the concatenation of noderepresentations h and h as input to measure the variant scoreof edge . Assuming inferring the sensitive attribute times, thevariant score in the -th inferring can be formulated as:",
  "= ( ([h, h])),(4)": "where denotes the concatenation operation, and () is a sig-moid function. In this context, can be implemented as a linearlayer, measuring the probability that edge belongs to the variantpattern.According to Eq. (4), we can obtain a variant score vector w R|E|, which includes variant scores for all edges. w representsvariant patterns, i.e., variant subgraphs, capturing the variant corre-lation between the graph structure and node labels under differentsensitive attribute groups. After inferring times, we have a variantscore vector set {w}=1, representing the variant correlation forvarious sensitive attributes. Accordingly, we can use these variant patterns to infer sensitive attributes. Specifically, we employ a GNNclassifier as the sensitive attribute inference model to generatethe sensitive attribute partition. Given the -th variant score vectorw, the -th sensitive attribute partition P can be formulated as:",
  "P = (G, w,),(5)": "where P R|V|, and is the number of sensitive attributegroups. For instance, in the case of a sensitive attribute like gender, = 2.To achieve accurate partitioning of sensitive attributes, optimiz-ing and with well-defined objectives is crucial. Our goal is tocapture variant patterns that result in significant performance dif-ferences across different sensitive attribute groups. Consequently,aligning with the approach of EIIL , we employ the IRM objec-tive as the optimization objective of and . Formally, the opti-mization objective of SAP can be formulated as follows:",
  "Vq()L((G),),(7)": "where q() : (|G, w,) denotes a soft per-partition risk andis a node-level implementation of Eq. (5).Notably, the application of the IRM objective enables inferringsensitive attributes in an unsupervised manner. The inferred sen-sitive attributes correspond to the demographic group partitionwith the worst-case fairness performance. In such an unsupervisedmanner, we can partition sensitive attributes times to identify thetop worst-case partitions, denoted by {P}=1.",
  "Towards Fairness via Invariant Learning": "In the SIL module, we aim to learn a fair GNN model including aGNN backbone and a classifier from an invariant learning per-spective. Prior works have revealed that training model in an ERMparadigm inevitably results in the capturing of spurious correlations.In our scenarios, such spurious correlations are the correlation be-tween the sensitive attribute and node labels, being uncovered as",
  "#Nodes1,00018,87667,79666,569403#Edges22,242321,308617,958583,61621,242#Attr.271827726695Sens.GenderRaceRegionRegionNationality": "variant patterns through the SAP module. Naturally, based on P,we guarantee the variance across the sensitive attribute groupsto optimize , which is motivated by the objective of EERM .In other words, this objective guides the model to leverage theinvariant patterns to yield equal performance on different sensitiveattribute groups . Given an attributed graph G = (V, E, X), wecan obtain variant score vector set {w}=1 and sensitive attributespartition set {P}=1. In the forward of the training pipeline, takesG, w as input to predict node labels = (G, w), = 1, 2, ...,.Thus, optimization objectives of can be formulated as follows:",
  "min ({L ( ,)}S ) + ({L ( ,)}S ), (8)": "where () and () are variance and mean functions, respec-tively. The sensitive attribute group is derived from P. L (, )is the classification loss function under and we employ a binarycross-entropy function as L in all experiments. is a hyperpa-rameter to balance two loss terms.In Eq.(8), the variance loss term aims to minimize the perfor-mance difference between various sensitive attribute groups whilethe mean loss term ensures the predicted accuracy across all sensi-tive attribute groups.",
  "EXPERIMENTS": "In this section, we conduct node classification experiments onseveral commonly used fairness datasets, including German, Bail,Pokec-z, Pokec-n, and NBA. presents the statistical informa-tion of these datasets. In our experiments, we aim to answer the fol-lowing three questions: RQ1: Can FairINV improve fairness whilemaintaining utility performance? RQ2: How does FairINV achievefairness across various sensitive attributes in a single training ses-sion? RQ3: How do relevant hyperparameters and componentsimpact FairINV?",
  "attribute, the goal of German is to classify clients into twocredit risks (high or low)": "Bail is a defendants dataset, where defendants in thisdataset are released on bail during 1990-2009 in U.S states .We regard nodes as defendants and edges are decided by thesimilarity of past criminal records and demographics. Con-sidering race as the sensitive attribute, the task is to predictwhether defendants will commit a crime after release (bailvs. no bail). Pokec-z/n is derived from a popular social networkapplication in Slovakia, where Pokec-z and Pokec-n are so-cial network data in two different provinces. Nodes denoteusers with features such as gender, age, interest, etc. Edgerepresents the friendship between users. Considering re-gion as the sensitive attribute, the task is to predict theworking field of the users. NBA is derived from a Kaggle dataset comprising ap-proximately 400 NBA basketball players from the 2016-2017season. Nodes denote NBA basketball players with featuressuch as performance statistics, age, etc. Edge represents therelationship between these players on Twitter. Consideringnationality (U.S. and overseas players) as the sensitive at-tribute, the goal is to predict whether the salary of the playeris over the median.",
  "FairVGNN learns a fair GNN by mitigating the sensi-tive attribute leakage using adversarial learning and weightclamping technologies": "5.1.3Evaluation Metrics. To evaluate the utility performance, weuse AUC and F1 scores. Additionally, we employ two commonlyused fairness metrics, i.e., = |( = 1| = 0) ( = 1| =1)| and = |( = 1| = 1, = 0) ( = 1| = 1, =1)| , to evaluate the fairness performance. 5.1.4Implementation Details. For all methods, including FairINV,we use a multi-layer GNN model consisting of a GNN backbone and a 1-layer linear classifier . To validate the generalizabilityof FairINV on various backbones, we employ the following GNNbackbones: a 1-layer GCN , a 1-layer GIN , and a 2-layerGraphSAGE . Here, the hidden dimension of all GNN backbonesis set to 16 for all datasets. Hyperparameter settings for all baselinemethods adhere to the guidelines provided by the respective authors.We conduct all experiments 5 times and reported average results.For FairINV, we utilize the Adam optimizer with the learningrate = 1 102, epochs=1000, and the weight decay = 1 105.Using the same optimizer, the learning rate for training the SAPmodules are set to {0.1, 0.1, 0.01, 0.5, 0.1} for German, Bail, Pokec-z,Pokec-n, and NBA datasets, respectively. Meanwhile, we set thebalanced parameter to {10, 10, 10, 1, 1} for German, Bail, Pokec-z,Pokec-n, and NBA datasets, respectively. The partition times andthe number of sensitive attribute groups are fixed at 3 and 2 forall datasets. In the SAP module, a 1-layer linear layer is used asthe variant inference model . We employ a model with the samestructure as the GNN model ( and ) as a sensitive attribute in-ference model . Meanwhile, has the same structure as the GNNmodel ( and ) and is trained by minimizing the cross-entropyloss function. For and the SAP module, we set the training epochto 500. Due to all baselines using the sensitive attribute, FairINVincorporates the sensitive attribute into the original node featuresfor fair comparison. Moreover, all evaluations of FairINV are con-ducted on a single NVIDIA RTX 4090 GPU with 24GB memory. Allmodels are implemented with PyTorch and PyTorch-Geometric.",
  "To answer RQ1, we conduct a comparison study between FairINVand four baseline methods for the node classification task across": "five datasets. Specifically, we verify the effectiveness of FairINVon three GNN backbones, i.e., GIN, and GraphSAGE. Limited bythe space, we only present the comparison results on the GCNbackbone and provide more results in Appendix A. As shown in, the following observations can be seen: (1) FairINV out-performs all baseline methods in terms of both utility and fairnessin most cases. (2) In instances where FairINV exhibits relativelylower performance, the best-performing baseline method surpassesFairINV by a slight margin. (3) FairINV improves fairness whilemaintaining utility performance, as evidenced by the performanceimprovement compared with vanilla GCN.The first two observations verify the effectiveness of FairINV onfairness performance, simultaneously showcasing the state-of-the-art performance achieved by FairINV. As for the last observation, thepotential explanation lies in FairINVs adherence to the invarianceprinciple , i.e., (1) sufficiency property and (2) invarianceproperty. The sufficiency property emphasizes the necessity of ade-quate predictive abilities for the downstream task, which explainsthe preservation of the utility performance of FairINV. Meanwhile,the invariance property assumes consistency across different envi-ronments, signifying the invariance across the sensitive attributegroups in FairINV. Consequently, this property serves as the under-lying reason for FairINVs superior fairness performance. Overall,leveraging invariant learning, FariINV captures invariant subgraphswith sufficient information for the downstream task while learningto be invariant across different sensitive attribute groups. Thus,FairINV improves fairness while preserving utility performance. Inaddition, as shown in Appendix A, similar results can be observedfrom the experiments on GIN and GraphSAGE backbones.",
  "Generalizing to Various Sensitive Attributes": "To answer RQ2, we generalize FairINV to various sensitive attributescenarios. Specifically, we employ FairINV once to train a GNNmodel and then evaluate the fairness performance of this GNNmodel toward various sensitive attributes. presents theresults of various sensitive attributes and inferior results comparedto vanilla GCN are marked with a gray background. We only presentresults on four datasets except for the NBA dataset due to the lack ofsuitable node features as the sensitive attribute. When the sensitiveattribute is Age, we set the median of age as the threshold toobtain binary values for the sensitive attribute. Furthermore, weprovide the comparison results of FairINV and baseline methods inmulti-sensitive attribute scenarios, as detailed in the Appendix B.We make the following observations from this table: (1) Fair-INV achieves superior performance compared with vanilla GCN interms of both fairness and utility. This observation demonstratesthat FairINV improves the fairness of GNNs towards various sen-sitive attributes in a single training session. (2) In some instances,FairINV exhibits slightly inferior fairness performance compared tovanilla GCN. We attribute this to the fact that the sensitive attributegroups partitioned by the SAP module are unrelated to the sensitiveattributes we have selected. This is primarily due to the model itselfmaking fairly equitable decisions concerning the sensitive attributeswe have chosen. In other words, when grouped according to theselected sensitive attributes, the variability values are relativelysmall. Consequently, the sensitive attribute groups partitioned by",
  ": The results of ablation study on all datasets": "SAP when maximizing variability are unrelated to the groups cor-responding to such low variability. For the results with Age as thesensitive attribute on the Pokec-n dataset, despite decisions of themodel being extremely unfair with respect to the sensitive attribute,FairINV still does not improve fairness. We attribute this to theaggressive partitioning of age into binary-sensitive attributes.",
  "Ablation Study": "To answer RQ3, we conduct an ablation study to investigate theimpact of each component of FairINV on improving fairness andmaintaining utility. Specifically, we investigate the effect of threecomponents including the variant inference model , the SAP mod-ule, and the SIL module, denoted by , ,and . removes , replacing w predictedby with random numbers. removes the SAP mod-ule, replacing P predicted by with the sensitive attribute groundtruth. removes the SIL module, replacing the objec-tive shown in Eq. (8) with minimizing the IRM objective shown inEq. (6). presents the ablation results on five datasets. From this figure, we observe that the removal of the SIL moduleleads to a decline in both utility and fairness, implying the sig-nificant impact of SIL on FairINV. Furthermore, from the resultsof , even when using the ground truth of sensitiveattributes to replace the predicted sensitive attribute partition Pby SAP, the performance of FairINV is still affected. This experi-mental phenomenon is consistent with previous research resultson invariant learning without environmental labels. Finally, wefind that removing the variant inference model affects the fairnessperformance of FairINV, indicating the importance of the variantinference model in capturing variant patterns.",
  "Hyperparameters Sensitivity": "To further answer RQ3, we investigate the parameter sensitivity ofFairINV w.r.t. two hyperparameters, i.e., the balanced parameter and the learning rate of SAP. Notably, the setting of benefitsfrom the independent training of SAP. We vary and withinthe range of {0.001, 0.01, 0.1, 0.5, 1, 10, 100}. We only illustrateresults on the Bail and Pokec-z datasets due to similar observationson other datasets. We observe that, with a wide range of variationsin two parameters, the performance of FairINV remains stable.However, a sharp decline in both utility and fairness performanceis noted when the value of is less than 0.01.",
  "Training Time Comparison": "To further investigate the computational cost of FairINV comparedto baseline methods, we conduct a training time comparison ex-periment across all datasets. Specifically, we repeat each methodfive times and record the total training time. We set to 1 and3 to implement two variants of FairINV, namely FairINV-1, andFairINV-3, representing FairINV trained for the single sensitiveattribute and three-sensitive attribute scenarios, respectively. Asshown in , FairINV exhibits lower computational costs on",
  ": Parameters sensitivity analysis on Pokec-z": "larger datasets compared to baseline methods. Although FairINV-3requires more training time, it trains fair GNN models toward threesensitive attributes, which are unreachable for baseline methods.Furthermore, we also observe that the training time of FairINV-3 issignificantly longer than that of FairINV-1. A possible explanationfor this phenomenon is the high computational cost associated with",
  "CONCLUSION": "In this work, we investigate the universal fairness problem, i.e.,training a fair GNN toward various sensitive attributes in a singletraining session. To address this problem, we first formulate sucha problem from a graph invariant learning point of view. Then,we propose a universal graph fairness approach, namely, FairINV.The core idea behind FairINV is to eliminate spurious correlationsbetween the sensitive attributes and labels in a graph variant learn-ing way. Experiments on several real-world datasets validate theeffectiveness of FairINV in both fairness and utility performance.We leave validation on other downstream tasks, e.g., edge-level, asfuture works. In addition, due to FairINV only focusing on groupfairness, future works will focus on considering fine-grained fair-ness, e.g., individual fairness.",
  "Sean Current, Yuntian He, Saket Gurukar, and Srinivasan Parthasarathy. 2022.Fairmod: Fair link prediction and recommendation via graph modification. arXivpreprint arXiv:2201.11596 (2022)": "Enyan Dai and Suhang Wang. 2021. Say no to the discrimination: Learning fairgraph neural networks with limited sensitive attribute information. In Proceedingsof the 14th ACM International Conference on Web Search and Data Mining. 680688. Yushun Dong, Jian Kang, Hanghang Tong, and Jundong Li. 2021. Individualfairness for graph neural networks: A ranking based approach. In Proceedingsof the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining.300310.",
  "Yushun Dong, Ninghao Liu, Brian Jalaian, and Jundong Li. 2022. Edits: Modelingand mitigating data bias for graph neural networks. In Proceedings of the ACMWeb Conference 2022. 12591269": "Yingtong Dou, Zhiwei Liu, Li Sun, Yutong Deng, Hao Peng, and Philip S Yu. 2020.Enhancing graph neural network-based fraud detectors against camouflagedfraudsters. In Proceedings of the 29th ACM international conference on information& knowledge management. 315324. Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and RichardZemel. 2012. Fairness through awareness. In Proceedings of the 3rd innovations intheoretical computer science conference. 214226. Wei Fan, Kunpeng Liu, Rui Xie, Hao Liu, Hui Xiong, and Yanjie Fu. 2021. Fairgraph auto-encoder for unbiased graph representations with wasserstein distance.In 2021 IEEE International Conference on Data Mining (ICDM). IEEE, 10541059.",
  "Zhimeng Jiang, Xiaotian Han, Chao Fan, Zirui Liu, Na Zou, Ali Mostafavi, andXia Hu. 2023. Chasing Fairness in Graphs: A GNN Architecture Perspective.arXiv preprint arXiv:2312.12369 (2023)": "Kareem L Jordan and Tina L Freiburger. 2015. The effect of race/ethnicity onsentencing: Examining sentence type, jail length, and prison length. Journal ofEthnicity in Criminal Justice 13, 3 (2015), 179196. Jian Kang, Jingrui He, Ross Maciejewski, and Hanghang Tong. 2020. Inform:Individual fairness on graph mining. In Proceedings of the 26th ACM SIGKDDinternational conference on knowledge discovery & data mining. 379389. Ahmad Khajehnejad, Moein Khajehnejad, Mahmoudreza Babaei, Krishna P Gum-madi, Adrian Weller, and Baharan Mirzasoleiman. 2022. Crosswalk: Fairness-enhanced node representation learning. In Proceedings of the AAAI Conferenceon Artificial Intelligence, Vol. 36. 1196311970.",
  "Jure Leskovec and Julian Mcauley. 2012. Learning to discover social circles inego networks. Advances in neural information processing systems 25 (2012)": "Haoyang Li, Ziwei Zhang, Xin Wang, and Wenwu Zhu. 2022. Learning invariantgraph representations for out-of-distribution generalization. Advances in NeuralInformation Processing Systems 35 (2022), 1182811841. Jintang Li, Ruofan Wu, Wangbin Sun, Liang Chen, Sheng Tian, Liang Zhu,Changhua Meng, Zibin Zheng, and Weiqiang Wang. 2023. Whats Behind theMask: Understanding Masked Graph Modeling for Graph Autoencoders. In KDD.ACM, 12681279. Jintang Li, Huizhe Zhang, Ruofan Wu, Zulun Zhu, Baokun Wang, ChanghuaMeng, Zibin Zheng, and Liang Chen. 2024. A Graph is Worth 1-bit Spikes: WhenGraph Contrastive Learning Meets Spiking Neural Networks. In ICLR.",
  "Peizhao Li, Yifei Wang, Han Zhao, Pengyu Hong, and Hongfu Liu. 2021. On dyadicfairness: Exploring and mitigating bias in graph connections. In InternationalConference on Learning Representations": "Yanying Li, Xiuling Wang, Yue Ning, and Hui Wang. 2022. Fairlp: Towards fairlink prediction on social network graphs. In Proceedings of the International AAAIConference on Web and Social Media, Vol. 16. 628639. Hongyi Ling, Zhimeng Jiang, Youzhi Luo, Shuiwang Ji, and Na Zou. 2022. Learn-ing fair graph representations via automated data augmentations. In The EleventhInternational Conference on Learning Representations. Yang Liu, Xiang Ao, Fuli Feng, Yunshan Ma, Kuan Li, Tat-Seng Chua, and QingHe. 2023. FLOOD: A flexible invariant learning framework for out-of-distributiongeneralization on graphs. In Proceedings of the 29th ACM SIGKDD Conference onKnowledge Discovery and Data Mining. 15481558. Jiali Ma, Zhongqi Yue, Kagaya Tomoyuki, Suzuki Tomoki, Karlekar Jayashree,Sugiri Pranata, and Hanwang Zhang. 2023. Invariant Feature Regularization forFair Face Recognition. In Proceedings of the IEEE/CVF International Conference onComputer Vision. 2086120870. Jonas Peters, Peter Bhlmann, and Nicolai Meinshausen. 2016. Causal inferenceby using invariant prediction: identification and confidence intervals. Journal ofthe Royal Statistical Society Series B: Statistical Methodology 78, 5 (2016), 9471012.",
  "Lubos Takac and Michal Zabovsky. 2012. Data analysis in public social networks.In International scientific conference and international workshop present day trendsof innovations, Vol. 1": "Xiaoyu Tan, LIN Yong, Shengyu Zhu, Chao Qu, Xihe Qiu, Xu Yinghui, Peng Cui,and Yuan Qi. 2023. Provably invariant learning without domain information. InInternational Conference on Machine Learning. PMLR, 3356333580. Yu Wang, Yuying Zhao, Yushun Dong, Huiyuan Chen, Jundong Li, and TylerDerr. 2022. Improving fairness in graph neural networks via mitigating sensi-tive attribute leakage. In Proceedings of the 28th ACM SIGKDD Conference onKnowledge Discovery and Data Mining. 19381948.",
  "Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How powerfulare graph neural networks? arXiv preprint arXiv:1810.00826 (2018)": "Moyi Yang, Junjie Sheng, Wenyan Liu, Bo Jin, Xiaoling Wang, and XiangfengWang. 2022. Obtaining Dyadic Fairness by Optimal Transport. In 2022 IEEEInternational Conference on Big Data (Big Data). IEEE, 47264732. Yuchang Zhu, Jintang Li, Liang Chen, and Zibin Zheng. 2024. The Devil is in theData: Learning Fair Graph Neural Networks via Partial Knowledge Distillation.In Proceedings of the 17th ACM International Conference on Web Search and DataMining. 10121021.",
  "ACOMPARISON FOR VARIOUS GNNBACKBONES": "To further investigate the generalizability of FairINV across vari-ous GNN backbones, we conduct comparative experiments usingGIN and GraphSAGE backbones. As shown in Tables 5and 6, we compare FairINV with three fairness baseline methods,including NIFTY , FairGNN , and FairVGNN . From thesetwo tables, we can observe that FairINV consistently outperformsthe three fairness baseline methods in most cases. Furthermore,upon summarizing the comparison results across the three back-bones, we find that most fairness methods, including FairINV, con-sistently enhance both utility and fairness performance on the Baildataset. This observation suggests an underlying relationship be-tween fairness and utility in the Bail dataset, providing a promisingavenue for future research.",
  "BCOMPARISON FOR MULTI-SENSITIVEATTRIBUTES": "We further present a comparison of FairINV and two baseline meth-ods in various sensitive attribute scenarios, as shown in .Due to the single sensitive attribute setting of these two methods,it is necessary to extend them by modifying the optimization ob-jectives. For NIFTY , we simultaneously flap various sensitiveattributes to construct the counterfactual graph. For FairGNN ,we train multiple sensitive attribute estimators and discriminatorssimultaneously. Although existing methods can be extended tomulti-sensitive attribute scenarios, their performance might be neg-atively affected since they are not explicitly designed for multiplesensitive attributes. From , we can observe that FairINV isthe only method that can achieve fairness and maintain utility. Inmost cases, FairINVs fairness performance is better than baselinemethods."
}