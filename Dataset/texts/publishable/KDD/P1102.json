{
  "Abstract": "To detect anomalies in real-world graphs, such as social, email,and financial networks, various approaches have been developed.While they typically assume static input graphs, most real-worldgraphs grow over time, naturally represented as edge streams. Inthis context, we aim to achieve three goals: (a) instantly detectinganomalies as they occur, (b) adapting to dynamically changingstates, and (c) handling the scarcity of dynamic anomaly labels.In this paper, we propose SLADE (Self-supervised Learning forAnomaly Detection in Edge Streams) for rapid detection of dynamicanomalies in edge streams, without relying on labels. SLADE detectsthe shifts of nodes into abnormal states by observing deviations intheir interaction patterns over time. To this end, it trains a deep neu-ral network to perform two self-supervised tasks: (a) minimizingdrift in node representations and (b) generating long-term interac-tion patterns from short-term ones. Failure in these tasks for a nodesignals its deviation from the norm. Notably, the neural networkand tasks are carefully designed so that all required operations canbe performed in constant time (w.r.t. the graph size) in response toeach new edge in the input stream. In dynamic anomaly detectionacross four real-world datasets, SLADE outperforms nine compet-ing methods, even those leveraging label supervision. Our code anddatasets are available at",
  "Edge Stream; Anomaly Detection; Self-supervised Learning": "ACM Reference Format:Jongha Lee, Sunwoo Kim, and Kijung Shin. 2024. SLADE: Detecting DynamicAnomalies in Edge Streams without Labels via Self-Supervised Learning. InProceedings of the 30th ACM SIGKDD Conference on Knowledge Discoveryand Data Mining (KDD 24), August 2529, 2024, Barcelona, Spain. ACM, NewYork, NY, USA, 12 pages. Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08",
  "Introduction": "The evolution of web technologies has dramatically enhanced hu-man life. Platforms such as email and social networks have madepeople communicating with diverse individuals and accessing help-ful information easier. Additionally, e-commerce has enabled peopleto engage in economic activities easily. However, as conveniencehas increased, many problems have emerged, such as financialcrimes, social media account theft, and spammer that exploited it.Many graph anomaly detection techniques have beendeveloped for tackling these problems. These techniques involverepresenting the interactions between users as a graph, therebyharnessing the connectivity between users to effectively identifyanomalies. However, graph anomaly detection in real-world sce-narios poses several challenges, as discussed below.C1) Time Delay in Detection:C1) Time Delay in Detection:C1) Time Delay in Detection:C1) Time Delay in Detection:C1) Time Delay in Detection:C1) Time Delay in Detection:C1) Time Delay in Detection:C1) Time Delay in Detection:C1) Time Delay in Detection:C1) Time Delay in Detection:C1) Time Delay in Detection:C1) Time Delay in Detection:C1) Time Delay in Detection:C1) Time Delay in Detection:C1) Time Delay in Detection:C1) Time Delay in Detection:C1) Time Delay in Detection: While most graph anomaly detec-tion methods assume static input graphs, real-world graphs evolveover time with continuous interaction events. In response to con-tinuous interaction events, it is important to quickly identify anom-alies. Delaying the detection of such anomalies can lead to increas-ing harm to benign nodes as time passes. However, employing staticgraph-based methods repeatedly on the entire graph, whenever aninteraction event occurs, inevitably leads to significant delays dueto the substantial computational expenses involved. To mitigatedelays, it is necessary to model continuous interaction events asedge streams and employ incremental computation to assess the ab-normality of each newly arriving edge with detection time constantregardless of the accumulated data size.Many studies have developed anomaly detection methodsfor edge streams, leveraging incremental computation. However, asthese methods are designed to target specific anomaly types (e.g.,burstiness), lacking learning-based components, they are oftenlimited in capturing complex ones deviating from targeted types.C2) Dynamically Changing States:C2) Dynamically Changing States:C2) Dynamically Changing States:C2) Dynamically Changing States:C2) Dynamically Changing States:C2) Dynamically Changing States:C2) Dynamically Changing States:C2) Dynamically Changing States:C2) Dynamically Changing States:C2) Dynamically Changing States:C2) Dynamically Changing States:C2) Dynamically Changing States:C2) Dynamically Changing States:C2) Dynamically Changing States:C2) Dynamically Changing States:C2) Dynamically Changing States:C2) Dynamically Changing States: In web services, users canexhibit dynamic states varying over time. That is, a users behaviorcan be normal during one time period but abnormal during another.For example, a normal users account can be compromised andthen manipulated to disseminate promotional messages. As a result,the users state transitions from normal to abnormal. Such a usercan be referred to as a dynamic anomaly, and detecting dynamicanomalies presents a greater challenge compared to the relativelyeasier task of identifying static anomalies.Addressing this challenge can be facilitated by tracking the evo-lution of node characteristics over time, and to this end, dynamicnode representation learning methods can be employed.However, existing dynamic node representation learning meth-ods require label information for the purpose of anomaly detection,which is typically scarce, as elaborated in the following paragraph.",
  "KDD 24, August 2529, 2024, Barcelona, SpainJongha Lee, Sunwoo Kim, and Kijung Shin": ": The left figure shows the linear increase of therunning time of SLADE with respect to the number of edgesin the Reddit dataset. The right figure shows the trade-offbetween detection speed and accuracy (with standard devia-tions) in the Wikipedia dataset provided by the competingmethods. The baseline methods with AUC scores below 60%are excluded from consideration to enhance the clarity of per-formance differences between the methods. SLADE exhibitsconstant processing time per edge (as proven in .2),offering the best trade-off between speed and accuracy. For atraining-time comparison, refer to Online Appendix D.2. : Comparison of AUC (in %) of SLADE and its variantsthat use a subset of the proposed components (i.e., tempo-ral contrast loss L, memory generation loss L, temporalcontrast score , and memory generation score ). SLADEwith all components performs the best overall, showing theeffectiveness of each.",
  "Anomaly Detection in CTDGs": "A continuous-time dynamic graph (CTDG) is a stream of edgesaccompanied by timestamps, which we also term an edge stream.Unsupervised Anomaly Detection in CTDGs:Unsupervised Anomaly Detection in CTDGs:Unsupervised Anomaly Detection in CTDGs:Unsupervised Anomaly Detection in CTDGs:Unsupervised Anomaly Detection in CTDGs:Unsupervised Anomaly Detection in CTDGs:Unsupervised Anomaly Detection in CTDGs:Unsupervised Anomaly Detection in CTDGs:Unsupervised Anomaly Detection in CTDGs:Unsupervised Anomaly Detection in CTDGs:Unsupervised Anomaly Detection in CTDGs:Unsupervised Anomaly Detection in CTDGs:Unsupervised Anomaly Detection in CTDGs:Unsupervised Anomaly Detection in CTDGs:Unsupervised Anomaly Detection in CTDGs:Unsupervised Anomaly Detection in CTDGs:Unsupervised Anomaly Detection in CTDGs: The majority ofunsupervised anomaly detection techniques applied to CTDGs aimto identify specific anomaly types. Sedanspot focuses on de-tecting (a) bursts of activities and (b) bridge edges between sparselyconnected parts of the input graph. MIDAS aims to spot bursts ofinteractions within specific groups, and F-FADE is effective foridentifying sudden surges in interactions between specific pairs ofnodes and swift changes in the community memberships of nodes.Lastly, AnoGraph spots dense subgraph structures along withthe anomalous edges contained within them. These methods aregenerally efficient, leveraging incremental computation techniques.Nonetheless, as previously mentioned, many of these approaches lack learnable components and thus may encounter challenges inidentifying complex anomaly patterns, i.e., deviations from normalpatterns in various aspects that may not be predefined.Representation Learning in CTDGs:Representation Learning in CTDGs:Representation Learning in CTDGs:Representation Learning in CTDGs:Representation Learning in CTDGs:Representation Learning in CTDGs:Representation Learning in CTDGs:Representation Learning in CTDGs:Representation Learning in CTDGs:Representation Learning in CTDGs:Representation Learning in CTDGs:Representation Learning in CTDGs:Representation Learning in CTDGs:Representation Learning in CTDGs:Representation Learning in CTDGs:Representation Learning in CTDGs:Representation Learning in CTDGs: Representation learning inCTDGs involves maintaining and updating representations of nodesin response to each newly arriving edge, capturing evolving pat-terns of nodes over time. To this end, several neural-network archi-tectures have been proposed. JODIE utilizes recurrent-neural-network (RNN) modules to obtain dynamic node representations.Dyrep combines a deep temporal point process with RNNs.TGAT leverages temporal encoding and graph attention to incorporate temporal information during neighborhood aggrega-tion. Based on temporal smoothness, DDGCL learns dynamicnode representations by contrasting those of the same nodes intwo nearby temporal views. TGN utilizes a memory moduleto capture and store long-term patterns. This module is updatedusing an RNN for each node, providing representations that encom-pass both temporal and spatial characteristics. Many subsequentstudies have also adopted memory modules. Assuming agradual process where memories for nodes do not show substan-tial disparities before and after updates, DGTCC contrastsmemories before and after updates for training.For the purpose of anomaly detection, dynamic node represen-tations can naturally be used as inputs for a classifier, which istrained in a supervised manner using anomaly labels. Recently,more advanced anomaly detection methods based on representa-tion learning in CTDGs have been proposed. SAD combines amemory bank with pseudo-label contrastive learning, which, how-ever, requires anomaly labels for training.",
  "Anomaly Detection in Other Graph Models": "In this subsection, we introduce anomaly detection approachesapplied to other graph models.Anomaly Detection in Static Graphs:Anomaly Detection in Static Graphs:Anomaly Detection in Static Graphs:Anomaly Detection in Static Graphs:Anomaly Detection in Static Graphs:Anomaly Detection in Static Graphs:Anomaly Detection in Static Graphs:Anomaly Detection in Static Graphs:Anomaly Detection in Static Graphs:Anomaly Detection in Static Graphs:Anomaly Detection in Static Graphs:Anomaly Detection in Static Graphs:Anomaly Detection in Static Graphs:Anomaly Detection in Static Graphs:Anomaly Detection in Static Graphs:Anomaly Detection in Static Graphs:Anomaly Detection in Static Graphs: Many approaches have been developed for anomaly detection in a static graph,which does not contain any temporal information. Among them, wefocus on those leveraging graph self-supervised learning, which ef-fectively deals with the absence of anomaly labels. ANEMONE contrasts node representations obtained from (a) features aloneand (b) both graph topology and features, identifying nodes withsubstantial differences as anomalies. DOMINANT aims to re-construct graph topology and attributes, using a graph-autoencodermodule, and anomalies are identified based on reconstruction error.These approaches assume a static graph, and their extensions todynamic graphs are not trivial, as node representations need toevolve over time to accommodate temporal changes.Anomaly Detection in DTDGs:Anomaly Detection in DTDGs:Anomaly Detection in DTDGs:Anomaly Detection in DTDGs:Anomaly Detection in DTDGs:Anomaly Detection in DTDGs:Anomaly Detection in DTDGs:Anomaly Detection in DTDGs:Anomaly Detection in DTDGs:Anomaly Detection in DTDGs:Anomaly Detection in DTDGs:Anomaly Detection in DTDGs:Anomaly Detection in DTDGs:Anomaly Detection in DTDGs:Anomaly Detection in DTDGs:Anomaly Detection in DTDGs:Anomaly Detection in DTDGs: A discrete-time dynamic graph(DTDG) corresponds to a sequence of graphs occurring at each timeinstance, also referred to as a graph stream. Formally, a DTDG is asetG1, G2, , G where G =V, E is the graph snapshotat time , and V and E are node- and edge-set in G, respectively. Several methods have been developed for detecting anom-alies, with an emphasis on anomalous edges, in a DTDG, which isa sequence of graphs at each time instance. Netwalk employsrandom walks and autoencoders to create similar representationsfor nodes that frequently interact with each other. Then, it identifies",
  "SLADE: Detecting Dynamic Anomalies in Edge Streams without Labels via Self-Supervised LearningKDD 24, August 2529, 2024, Barcelona, Spain": ": (a) and (b) show the distribution of anomaly scoresassigned by SLADE to instances of each node type in the twosynthetic datasets (visualization is based on Gaussian kerneldensity estimation). (c) and (d) show the anomaly scores ateach time period. Note that in all figures, SLADE clearlydistinguishes anomalies from normal nodes. For results fromseveral baseline methods, refer to Online Appendix D.3.",
  "Problem Description": "In this section, we introduce notations and, based on them, definethe problem of interest, dynamic node anomaly detection in CTDGs.Notations:Notations:Notations:Notations:Notations:Notations:Notations:Notations:Notations:Notations:Notations:Notations:Notations:Notations:Notations:Notations:Notations: A continuous-time dynamic graph (CTDG) G = (1,2, ) is a stream (i.e., continuous sequence) of temporal edges withtimestamps. Each temporal edge = (, ,) arriving at time is directional from the source node to the destination node . Thetemporal edges are ordered chronologically, i.e., +1 holds foreach {1, 2, }. We denote by V() = (,,)G {, }the temporal set of nodes arriving at time or earlier.Problem Description:Problem Description:Problem Description:Problem Description:Problem Description:Problem Description:Problem Description:Problem Description:Problem Description:Problem Description:Problem Description:Problem Description:Problem Description:Problem Description:Problem Description:Problem Description:Problem Description: We consider a CTDG G = (1,2, ),where each temporal edge = (, ,) indicates a behaviorof the source node towards the destination node at time .That is, the source node represents the actor node. We aim to ac-curately classify the current dynamic status of each node, which iseither normal or abnormal. We address this problem in an unsuper-vised setting. That is, we do not have access to the dynamic statesof any nodes at any time as input. In our experimental setups, theground-truth dynamic states are used only for evaluation purposes.Real-world Scenarios:Real-world Scenarios:Real-world Scenarios:Real-world Scenarios:Real-world Scenarios:Real-world Scenarios:Real-world Scenarios:Real-world Scenarios:Real-world Scenarios:Real-world Scenarios:Real-world Scenarios:Real-world Scenarios:Real-world Scenarios:Real-world Scenarios:Real-world Scenarios:Real-world Scenarios:Real-world Scenarios: Due to its substantial impact, this prob-lem has been explored in many previous studies ,but in supervised settings. For instance, in web services, a normalusers account can be compromised and then exploited to circulatepromotional messages. In such a case, the users state transitionsfrom normal to abnormal. Detecting such transitions promptly iscrucial to minimize the inconvenience caused by the disseminationof promotional messages.Why CTDGs?:Why CTDGs?:Why CTDGs?:Why CTDGs?:Why CTDGs?:Why CTDGs?:Why CTDGs?:Why CTDGs?:Why CTDGs?:Why CTDGs?:Why CTDGs?:Why CTDGs?:Why CTDGs?:Why CTDGs?:Why CTDGs?:Why CTDGs?:Why CTDGs?: Anomaly detection methods designed for DTDGs(refer to .2) process input on a per-graph basis, whereedges need to be aggregated over time to form a graph. Thus, theyare susceptible to delays in predicting the current state of nodesupon the arrival of an edge. In theory, methods designed for staticgraphs can be applied to our problem by re-running them when-ever a new edge arrives. However, this straightforward applicationmakes their time complexity per edge (super-)linear in the graphsize, causing notable delays. However, CTDG-based methods, in-cluding our proposed one, process the arriving edge, whenever itarrives, typically with constant time complexity regardless of the ac-cumulated graph size. As discussed in , this highlights the advantages of CTDG-based methods in time-critical applications,including (dynamic) anomaly detection.Comparison with Edge Anomaly Detection:Comparison with Edge Anomaly Detection:Comparison with Edge Anomaly Detection:Comparison with Edge Anomaly Detection:Comparison with Edge Anomaly Detection:Comparison with Edge Anomaly Detection:Comparison with Edge Anomaly Detection:Comparison with Edge Anomaly Detection:Comparison with Edge Anomaly Detection:Comparison with Edge Anomaly Detection:Comparison with Edge Anomaly Detection:Comparison with Edge Anomaly Detection:Comparison with Edge Anomaly Detection:Comparison with Edge Anomaly Detection:Comparison with Edge Anomaly Detection:Comparison with Edge Anomaly Detection:Comparison with Edge Anomaly Detection: In many scenar-ios, the dynamic state of an actor node can be equated with theanomalousness (or maliciousness) of its behavior. In other words,an actor node is assumed to be in the abnormal state if and onlyif it performs anomaly (or malicious) behavior. In such cases, thistask shares some similarities with detecting anomalous edges. How-ever, it should be noticed that the predicted current node states areused for predicting the anomalousness of future interactions (see.3) rather than assessing the anomalousness of interac-tions that have already occurred. Hence, for effective dynamic nodeanomaly detection, it is important to consider node-wise behavioraldynamics over time.",
  "Proposed Method: SLADE": "In this section, we present SLADE (Self-supervised Learning forAnomaly Detection in Edge Streams), our proposed method forunsupervised dynamic anomaly detection in CTDGs.The underlying intuition is that nodes in the normal state tendto exhibit structurally and temporally similar interaction patternsover time , while those in the abnormal state do not becauserepeating similar abnormal actions increases the risk of detection.Motivated by this idea, we consider two key assumptions: A1. Stable Long-Term Interaction Patterns: Nodes in thenormal state tend to repetitively engage in similar interactionsover a long-term period. This stable long-term interaction patternexhibits minimal variation within short-time intervals.",
  "A2. Potential for Restoration of Patterns: It would be feasibleto accurately regenerate the long-term interaction patterns of thenodes in the normal state using recent interaction information": "They account for both structural and temporal aspects of normalnodes. While A1 focuses on temporal aspects, A2 takes a furtherstep by specifying the extent of structural similarities over time.Upon these assumptions, SLADE employs two self-supervisedtasks for training its model (i.e., deep neural network) for maintain-ing and updating a dynamic representation of each node, which weexpect to capture its long-term interaction pattern.",
  "S2. Memory Generation: This aims to accurately generate dy-namic node representations based only on recent interactions(related to A2)": "By being trained for S1 and S2, the model is expected to learnnormal interaction patterns satisfying A1 and A2. Once the modelis trained, SLADE identifies nodes for which the model performspoorly on S1 and S2, as these nodes potentially deviate from pre-sumed normal interaction patterns.Specifically, to obtain dynamic representations, SLADE employsneural networks in combination with memory modules (see Sec-tion 4.1). The memories (i.e, stored information), which reflect thenormal patterns of nodes, are updated and regenerated to minimizeour self-supervised losses related to S1 and S2 (see .2).Lastly, these dynamic representations are compared with their pastvalues and momentary representations to compute anomaly scores",
  "(b) Query at time t": ": Overview of SLADE, whose objective is to measure the anomaly score of a query node at any time. For each newlyarriving edge, SLADE updates the memory vector of each endpoint using GRU. Given a query node, SLADE masks the memoryvector of the node and approximately regenerates it based on its recent interactions using TGAT. Then, it measures the anomalyscore of the query node based on the similarities (1) between previous and current memory vectors (related to S1) and (2)between current and generated memory vectors (related to S2). SLADE aims to maximize these similarities for model training.",
  "Memory Updater: This neural network captures evolving char-acteristics of nodes interaction patterns. It is employed to updatethe memory (i.e., stored information)": "Memory Generator: This neural network is used to generatethe memory of a target node from its recent interactions.Below, we examine the details of each module in order.Memory Module:Memory Module:Memory Module:Memory Module:Memory Module:Memory Module:Memory Module:Memory Module:Memory Module:Memory Module:Memory Module:Memory Module:Memory Module:Memory Module:Memory Module:Memory Module:Memory Module: In SLADE, the dynamic representation of eachnode, which represents its long-term interaction patterns, is storedand updated in a memory module introduced by Rossi et al. .Specifically, the memory module consists of a memory vector for each node , and each captures the interactions of the node up to the current time. When each node first emerges in theinput CTDG, is initialized to a zero vector. As participates ininteractions, is continuously updated by the memory updater,described in the following subsection.Memory Updater:Memory Updater:Memory Updater:Memory Updater:Memory Updater:Memory Updater:Memory Updater:Memory Updater:Memory Updater:Memory Updater:Memory Updater:Memory Updater:Memory Updater:Memory Updater:Memory Updater:Memory Updater:Memory Updater: Whenever a node participates in a new inter-action, the memory updater gradually updates its memory vector,aiming to represent its stable long-term interaction pattern. First,each interaction is transformed into raw messages . Each rawmessage consists of the encoded time difference between the mostrecent appearance of one endpoint and the present time, along withthe memory vector of the other endpoint. For instance, upon thearrival of a temporal edge (, ,) at time , with and",
  "= GRU(, ) where = MLP().(3)": "Here is the memory vector for node after time , persistinguntil a new interaction involving node occurs (see .2(RQ5) for exploration of alternatives to GRU ). We also maintain (i.e., the previous value of the memory vector) for its future usage.Memory Generator:Memory Generator:Memory Generator:Memory Generator:Memory Generator:Memory Generator:Memory Generator:Memory Generator:Memory Generator:Memory Generator:Memory Generator:Memory Generator:Memory Generator:Memory Generator:Memory Generator:Memory Generator:Memory Generator: The memory generator aims to restore thememory vectors, which represent existing long-term interactionpatterns, based on short-term interactions. The generated vectorsare used for training and anomaly scoring, as described later.The first step of generation is to (temporarily) mask the memoryvector of a target node at current time to a zero vector. Then, thememory vectors of its at most latest (1-hop) neighbors and thecorresponding time information of their latest interactions are usedas inputs of an encoder. As the encoder, we employ TGAT ,which attends more to recent interactions (see .2 (RQ5)for exploration of alternatives). That is, for a target node and the",
  "and K = V =s1 ||( 1), , s ||( )": "Here {1, ...,} denote the indices of the neighbors of the targetnode , {1, ..., } denote the timestamps of the most recent in-teractions with them before , and s denotes the generated memoryvector of at . As the memory vector has been masked, only timeinformation is used for query component .",
  "Memory Generation Loss: It encourages the similarity betweenretained and generated memory vectors": "Below, we describe each self-supervised loss component and thenthe entire training process in greater detail.Temporal Contrast Loss:Temporal Contrast Loss:Temporal Contrast Loss:Temporal Contrast Loss:Temporal Contrast Loss:Temporal Contrast Loss:Temporal Contrast Loss:Temporal Contrast Loss:Temporal Contrast Loss:Temporal Contrast Loss:Temporal Contrast Loss:Temporal Contrast Loss:Temporal Contrast Loss:Temporal Contrast Loss:Temporal Contrast Loss:Temporal Contrast Loss:Temporal Contrast Loss: For S1, we aim to minimize drift indynamic node representations within short time intervals. For thememory vector of each node , we use the previous memoryvector as the positive sample and the memory vectors of theother nodes as negative samples. Specifically, for each interactionat time of , we aim to minimize the following loss:",
  "|V(+)|=1exp(( (+), (+))),(5)": "where (+) is the current memory vector after processing theinteraction at , and (+) is the generated memory vector . Wepropose a novel self-supervised learning task that encourages amodel to learn the normal temporal pattern of data.Batch Processing for Efficient Training:Batch Processing for Efficient Training:Batch Processing for Efficient Training:Batch Processing for Efficient Training:Batch Processing for Efficient Training:Batch Processing for Efficient Training:Batch Processing for Efficient Training:Batch Processing for Efficient Training:Batch Processing for Efficient Training:Batch Processing for Efficient Training:Batch Processing for Efficient Training:Batch Processing for Efficient Training:Batch Processing for Efficient Training:Batch Processing for Efficient Training:Batch Processing for Efficient Training:Batch Processing for Efficient Training:Batch Processing for Efficient Training: For computational ef-ficiency, we employ batch processing for training SLADE, whichhas been commonly used . That is, instead of process-ing a single edge at a time, multiple edges are fed into the modelsimultaneously. To this end, a stream of temporal edges is dividedinto batches of a fixed size in chronological order. Consequently,memory updates take place at the batch level, and for these updates,the model uses only the interaction information that precedes thecurrent batch. Note that a single node can be engaged in multiple interactions within a single batch, leading to multiple raw mes-sages for the node. To address this, SLADE aggregates the rawmessages into one raw message using mean pooling and continueswith the remaining steps of memory update. In order for the tem-poral contrast in Eq. (4) reflects A1, which emphasizes minimizingthe difference between memory vectors before and after an updatewithin a short time span, the batch size should not be excessivelylarge. Therefore, it is crucial to establish an appropriate batch size,taking both this and efficiency into consideration.Final Training Objective:Final Training Objective:Final Training Objective:Final Training Objective:Final Training Objective:Final Training Objective:Final Training Objective:Final Training Objective:Final Training Objective:Final Training Objective:Final Training Objective:Final Training Objective:Final Training Objective:Final Training Objective:Final Training Objective:Final Training Objective:Final Training Objective: For each batch with temporal edgesE, the overall temporal contrast loss is defined as follows:",
  "(,, )E (,) + (,),(7)": "where and hyperparameters for weighting each sourcenode and each destination node, respectively.The final loss L for is the sum of both losses, i.e., L = L + L.Note that, since anomaly labels are assumed to be unavailable,SLADE is trained with the assumption that the state of all nodesappearing in the training set is normal, irrespective of their actualstates. However, SLADE still can learn normal patterns effectively,given that they constitute the majority of the training data, asdiscussed for various types of anomalies in .1.",
  "Anomaly Scoring": "After being trained, SLADE is able to measure the anomaly scoreof any node at any given time point. SLADE measures how mucheach node deviates from A1 and A2 by computing the temporalcontrast score and the memory generation score, which arebased on A1 and A2, respectively. Below, we describe each of them.Temporal Contrast Score:Temporal Contrast Score:Temporal Contrast Score:Temporal Contrast Score:Temporal Contrast Score:Temporal Contrast Score:Temporal Contrast Score:Temporal Contrast Score:Temporal Contrast Score:Temporal Contrast Score:Temporal Contrast Score:Temporal Contrast Score:Temporal Contrast Score:Temporal Contrast Score:Temporal Contrast Score:Temporal Contrast Score:Temporal Contrast Score: This score is designed to detect anoma-lous nodes that deviate from A1, and to this end, it measures the ex-tent of abrupt changes in the long-term interaction pattern. Specif-ically, the temporal contrast score (,) of a node at time (spec., before processing any interaction at ) is defined as the cosinedistance between its current and previous memory vectors:",
  "(,) = 1 ( (), ()).(8)": "Memory Generation Score:Memory Generation Score:Memory Generation Score:Memory Generation Score:Memory Generation Score:Memory Generation Score:Memory Generation Score:Memory Generation Score:Memory Generation Score:Memory Generation Score:Memory Generation Score:Memory Generation Score:Memory Generation Score:Memory Generation Score:Memory Generation Score:Memory Generation Score:Memory Generation Score: In order to identify anomalous nodesdeviating from A2, this score measures the degree of deviation ofshort-term interaction patterns from long-term interaction patterns.Specifically, the memory generation score (,) of a node attime is defined as the cosine distance between its current andgenerated memory vectors:",
  "Discussion on Anomaly Types": "Below, we discuss how SLADE can detect anomalies of varioustypes, without any prior information about the types. In .2(RQ4), we empirically confirm its effectiveness for all these types.T1) Hijacked Anomalies:T1) Hijacked Anomalies:T1) Hijacked Anomalies:T1) Hijacked Anomalies:T1) Hijacked Anomalies:T1) Hijacked Anomalies:T1) Hijacked Anomalies:T1) Hijacked Anomalies:T1) Hijacked Anomalies:T1) Hijacked Anomalies:T1) Hijacked Anomalies:T1) Hijacked Anomalies:T1) Hijacked Anomalies:T1) Hijacked Anomalies:T1) Hijacked Anomalies:T1) Hijacked Anomalies:T1) Hijacked Anomalies: This type involves a previously nor-mal users account being compromised at some point, exhibitingmalicious behaviors that deviate from the users normal pattern.SLADE, motivated by such anomalies, contrasts short-term andlong-term interaction patterns to spot them, as discussed above.T2) New or Rarely-interacting Anomalies:T2) New or Rarely-interacting Anomalies:T2) New or Rarely-interacting Anomalies:T2) New or Rarely-interacting Anomalies:T2) New or Rarely-interacting Anomalies:T2) New or Rarely-interacting Anomalies:T2) New or Rarely-interacting Anomalies:T2) New or Rarely-interacting Anomalies:T2) New or Rarely-interacting Anomalies:T2) New or Rarely-interacting Anomalies:T2) New or Rarely-interacting Anomalies:T2) New or Rarely-interacting Anomalies:T2) New or Rarely-interacting Anomalies:T2) New or Rarely-interacting Anomalies:T2) New or Rarely-interacting Anomalies:T2) New or Rarely-interacting Anomalies:T2) New or Rarely-interacting Anomalies: Many deep learning-based detection models struggle with anomalies involving (1) newlyintroduced or (2) rarely interacting nodes due to limited data forlearning their normal behaviors. SLADE assigns a higher anomalyscore (both contrast and generation scores) to such nodes becausetheir memory vectors undergo substantial changes until their long-term patterns are established. We find it advantageous to pay atten-tion to such nodes because, in some of the real-world datasets weused, including Wikipedia, Bitcoin-alpha, and Bitcoin-OTC, newand rarely interacting nodes are more likely to engage in anomalousactions than those with consistent interactions.1 T3) Consistent Anomalies:T3) Consistent Anomalies:T3) Consistent Anomalies:T3) Consistent Anomalies:T3) Consistent Anomalies:T3) Consistent Anomalies:T3) Consistent Anomalies:T3) Consistent Anomalies:T3) Consistent Anomalies:T3) Consistent Anomalies:T3) Consistent Anomalies:T3) Consistent Anomalies:T3) Consistent Anomalies:T3) Consistent Anomalies:T3) Consistent Anomalies:T3) Consistent Anomalies:T3) Consistent Anomalies: SLADE can also effectively identifyanomalies exhibiting interaction patterns that are consistent overtime but deviating from those of normal nodes. Due to the limitedcapacity (i.e., expressiveness) of the neural networks used in it,SLADE prioritizes learning prevalent patterns (i.e., those of normalnodes) over less common abnormal ones. As a result, this can leadto a violation of A2, causing SLADE to assign high anomaly scores,especially memory generation scores, to such nodes.",
  "Complexity Analysis": "We analyze the time complexity of SLADE in action after be-ing trained. Specifically, we examine the cost of (a) updating thememory in response to a newly arrived edge, and (b) calculatinganomaly scores for a query node.Memory Update:Memory Update:Memory Update:Memory Update:Memory Update:Memory Update:Memory Update:Memory Update:Memory Update:Memory Update:Memory Update:Memory Update:Memory Update:Memory Update:Memory Update:Memory Update:Memory Update: Given a newly arriving edge, SLADE updatesthe memory vector of each endpoint using GRU. The total time com-plexity is dominated by that of GRU, which is O(2 + ) ,where and indicate the dimensions of memory vectors andmessages respectively.Anomaly Scoring:Anomaly Scoring:Anomaly Scoring:Anomaly Scoring:Anomaly Scoring:Anomaly Scoring:Anomaly Scoring:Anomaly Scoring:Anomaly Scoring:Anomaly Scoring:Anomaly Scoring:Anomaly Scoring:Anomaly Scoring:Anomaly Scoring:Anomaly Scoring:Anomaly Scoring:Anomaly Scoring: Given a query node, SLADE first aims to gener-ate its memory vector using TGAT with input being a zero-masked",
  "We suspect that new or inactive accounts are typically used to perform anomalousactions since the cost of being suspended is low for such accounts": "memory of the query node and the memory vectors of its at most most recent neighbors. Thus, the time complexity of obtaininggenerated memory is O(2) if we assume, for simplicity,that the embedding dimension of the attention layers is the sameas the dimension of the memory vectors. Then, SLADE com-putes the similarities (a) between the current memory and previousmemory vectors and (b) between current and generated memoryvectors, taking O(2) time. As a result, the overall time complexityof SLADE for anomaly scoring of a node is O(2).The time complexity for both tasks is O(2 + ), whichis constant with respect to the graph size (i.e., the numbers ofaccumulated nodes and edges). If we assume that anomaly scoring(for each endpoint) is performed whenever each edge arrives, thetotal complexity becomes linear in the number of accumulatededges, as confirmed empirically in .2 (RQ2).",
  "Experiment Details": "In this subsection, we describe datasets, baseline methods, and eval-uation metrics that are used throughout our experiments. Then, weclarify the implementation details of the proposed method SLADE.Datasets:Datasets:Datasets:Datasets:Datasets:Datasets:Datasets:Datasets:Datasets:Datasets:Datasets:Datasets:Datasets:Datasets:Datasets:Datasets:Datasets: We assess the performance of SLADE on four real-worlddatasets: two social networks (Wikipedia and Reddit ) and twoonline financial networks (Bitcoin-alpha and Bitcoin-OTC ).The Wikipedia dataset records edits made by users on Wikipediapages. In this context, when a user is banned after a specific edit, theusers dynamic label is marked as abnormal. The Reddit dataset con-sists of posts made by users on subreddits. The users dynamic labelindicates whether the user is banned after the specific post. TheBitcoin-alpha and Bitcoin-OTC datasets are fundamentally struc-tured as trust-weighted signed networks. Within these datasets,Bitcoin alpha or OTC members assign ratings to other members,ranging from -10 (total distrust) to +10 (complete trust). We uti-lize these ratings with temporal information to identify anomalousnodes and assign dynamic labels to users. Further details of thedataset processing and the statistics are provided in Appendix A.Across all datasets, we assess the performance of our method andthe baseline models by utilizing the final 15% of the dataset inchronological order as the test set.Baselines Methods and Evaluation Metric.:Baselines Methods and Evaluation Metric.:Baselines Methods and Evaluation Metric.:Baselines Methods and Evaluation Metric.:Baselines Methods and Evaluation Metric.:Baselines Methods and Evaluation Metric.:Baselines Methods and Evaluation Metric.:Baselines Methods and Evaluation Metric.:Baselines Methods and Evaluation Metric.:Baselines Methods and Evaluation Metric.:Baselines Methods and Evaluation Metric.:Baselines Methods and Evaluation Metric.:Baselines Methods and Evaluation Metric.:Baselines Methods and Evaluation Metric.:Baselines Methods and Evaluation Metric.:Baselines Methods and Evaluation Metric.:Baselines Methods and Evaluation Metric.: We extensively com-pare SLADE with several baseline methods capable of anomaly de-tection in CTDGs under inductive settings (i.e., without further opti-mization for test sets). For four rule-based methods (SedanSpot ,",
  "Experimental Results": "RQ1) Accuracy:RQ1) Accuracy:RQ1) Accuracy:RQ1) Accuracy:RQ1) Accuracy:RQ1) Accuracy:RQ1) Accuracy:RQ1) Accuracy:RQ1) Accuracy:RQ1) Accuracy:RQ1) Accuracy:RQ1) Accuracy:RQ1) Accuracy:RQ1) Accuracy:RQ1) Accuracy:RQ1) Accuracy:RQ1) Accuracy: As shown in , SLADE and SLADE-HPsignificantly outperform other baseline methods in most of thedatasets. There are two notable observations in this analysis.First, regarding the unsupervised learning competitors (i.e., SedanS-pot, MIDAS, F-FADE, and Anoedge-l), SLADE consistently outper-forms them across all datasets, achieving performance gains ofup to 20.44% (in the Reddit dataset) compared to the second-bestperforming unsupervised model. This result demonstrates that real-world anomalies exhibit complex patterns hard to be fully capturedby fixed rule-based approaches. As a result, the necessity for moreintricate representation models becomes evident. : AUC (in %) in the detection of dynamic anomalynodes. The first four methods are rule-based models, and theothers are based on representation learning. For each dataset,the best and the second-best performances are highlighted inboldface and underlined, respectively. In most cases, SLADEand SLADE-HP perform best, even when compared to modelsthat rely on label information. For results in terms of AveragePrecision (AP), refer to Online Appendix D.1.",
  "SLADE-HP88.68 0.39 75.08 0.50 76.92 0.36 77.18 0.27": ": AUC (in %) when varying the test start ratio. Forlearning-based methods, temporal edges preceding the teststart ratio in the dataset are employed for training. If vali-dation is needed, the last 10% of the training set is used forvalidation. Note that SLADE performs best in most cases. Second, interestingly, while SLADE does not utilize any labelinformation, even for hyperparameter tuning, it still outperformsall supervised baseline models, on all datasets except for the Bit-coinOTC dataset. This result implies that the patterns of nodes inthe normal state in real-world graphs closely adhere to A1 and A2,and SLADE effectively captures such patterns. Furthermore, it isevident that measuring the extent to which nodes deviate from thepatterns provides crucial information for anomaly detection.In addition, we measure the performances of the consideredmethods while varying the proportion of the training split (or equiv-alently the test split). Specifically, we utilize the first T% of theedges as a train set and assess each model by using the remaining(100 T)% of edges. We refer to T as a test start ratio. As shownin , SLADE outperforms all baseline methods in most ofthe settings, across various test start ratios. Moreover, as seen inthe performances of SLADE between the 40% and 80% ratios in",
  "0.6872.19 0.6076.32 0.2875.80 0.19": "the Wikipedia dataset, using just half of the dataset leads to only amarginal performance degradation (spec., 3.2%).RQ2) Speed in Action:RQ2) Speed in Action:RQ2) Speed in Action:RQ2) Speed in Action:RQ2) Speed in Action:RQ2) Speed in Action:RQ2) Speed in Action:RQ2) Speed in Action:RQ2) Speed in Action:RQ2) Speed in Action:RQ2) Speed in Action:RQ2) Speed in Action:RQ2) Speed in Action:RQ2) Speed in Action:RQ2) Speed in Action:RQ2) Speed in Action:RQ2) Speed in Action: To empirically demonstrate the theoreticalcomplexity analysis in .2, we measure the running time ofSLADE in action after being trained on the Reddit dataset whilevarying the number of edges. Anomaly scoring is performed (foreach endpoint) only when each edge arrives. As depicted in theleft subplot of , the running time of SLADE is linear in thenumber of edges, being aligned with our analysis.Additionally, we compare the running time and AUC scores ofSLADE with those of all considered methods. As shown in theright plot of , while SLADE is about 1.99 slower thanSedanSpot, which is the most accurate rule-based approach, SLADEdemonstrates a significant improvement of 5.88% in AUC scores,compared to SedanSpot. Furthermore, SLADE is about 4.57 fasterthan TGN, which is the second most accurate following SLADE.RQ3) Ablation Study:RQ3) Ablation Study:RQ3) Ablation Study:RQ3) Ablation Study:RQ3) Ablation Study:RQ3) Ablation Study:RQ3) Ablation Study:RQ3) Ablation Study:RQ3) Ablation Study:RQ3) Ablation Study:RQ3) Ablation Study:RQ3) Ablation Study:RQ3) Ablation Study:RQ3) Ablation Study:RQ3) Ablation Study:RQ3) Ablation Study:RQ3) Ablation Study: The ablation study is conducted across thefour datasets to analyze the necessity of the used self-supervisedlosses (Eq (4) and Eq (5)) and two anomaly detection scores (Eq (8)and Eq (9)). To this end, we utilize several variants of SLADE, wherecertain scores or self-supervised losses are removed from SLADE.As evident from , SLADE, which uses all the proposed self-supervised losses and scores, outperforms its variants in most ofthe datasets (Wikipedia, Reddit, and Bitcoin-OTC). Moreover, evenin the Bitcoin-alpha dataset, the performance gap between SLADE : AUC (in %) in the detection of dynamic anomalynodes in the two synthetic datasets. Since anomalies are in-jected only into the test sets, the comparison is limited tounsupervised methods. F-FADE, which consistently achievesan AUC value below 0.5, is omitted from the table. For resultsof Average Precision (AP), refer to Online Appendix D.1.",
  "Synthetic-New78.05 1.6882.63 0.0761.86 2.4198.38 1.09": "and the best-performing variant is within the standard deviationrange. Notably, the generation score greatly contributes to accurateanomaly detection in most of the dataset, providing an empiricalperformance gain of up to 48.81% (on the Reddit dataset).RQ4) Type Analysis:RQ4) Type Analysis:RQ4) Type Analysis:RQ4) Type Analysis:RQ4) Type Analysis:RQ4) Type Analysis:RQ4) Type Analysis:RQ4) Type Analysis:RQ4) Type Analysis:RQ4) Type Analysis:RQ4) Type Analysis:RQ4) Type Analysis:RQ4) Type Analysis:RQ4) Type Analysis:RQ4) Type Analysis:RQ4) Type Analysis:RQ4) Type Analysis: We demonstrate the effectiveness of SLADEin capturing various types of anomalies discussed in .1.To this end, we create synthetic datasets by injecting anomalies tothe Email-EU dataset, which consists of emails between usersin a large European research institution. Specifically, we createanomalies that repetitively send spam emails to random recipientswithin a short time interval, mimicking spammers, and based onthe timing of spamming, we have two different datasets: Synthetic-Hijack: Accounts of previously normal users startdisseminating spam emails after being hijacked at a certain timepoint and continue their anomalous actions. We further catego-rize each anomaly as Type T1 during its initial 20 interactionsafter being hijacked, and as Type T3 after that. Synthetic-New: New anomalous users appear and initiate spread-ing spam emails from the beginning and continue their anoma-lous actions. We further categorize each anomaly as Type T2during its first 20 interactions, and as Type T3 afterward. In them, all anomalies are injected only into the test set (i.e., final10% of the dataset in chronological order), ensuring that they remainunknown to the model during training. Consequently, the taskinvolves detecting these anomalies in an unsupervised manner, andSLADE is compared only with the unsupervised baseline methods.As shown in , SLADE performs best, achieving perfor-mance gains up to 19.9% in the Synthetic-Hijack dataset and 19.06%in the Synthetic-New dataset. These results reaffirm the limitationsof traditional unsupervised methods in capturing anomalies beyondtheir targeted types. In contrast, as it can learn normal patterns fromdata, SLADE successfully identifies various types of anomalies.Furthermore, we provide qualitative analysis of how SLADE as-signs scores to the normal and anomalies of each type. Figures 4(a)and (b) show that SLADE clearly separates anomaly score distribu-tions of all anomaly types (T1, T2, and T3) from the distributionof the normal ones. (c) shows that the anomaly scores ofhijacked anomalies (T1) increase shortly after being hijacked. Fig-ure 4(d) shows that SLADE successfully assigns high anomaly scoresto new or rarely interacting anomalies (T2). Consistent anomalies(T3) receive high anomaly scores in both cases.RQ5) Structural Variants:RQ5) Structural Variants:RQ5) Structural Variants:RQ5) Structural Variants:RQ5) Structural Variants:RQ5) Structural Variants:RQ5) Structural Variants:RQ5) Structural Variants:RQ5) Structural Variants:RQ5) Structural Variants:RQ5) Structural Variants:RQ5) Structural Variants:RQ5) Structural Variants:RQ5) Structural Variants:RQ5) Structural Variants:RQ5) Structural Variants:RQ5) Structural Variants: There are two major neural networkcomponents in SLADE: (1) GRU , which updates the memoryof each node, and (2) TGAT , which generates the memoryof a target node. To demonstrate the effectiveness of each modulein dynamic anomaly detection in edge stream, we compare the",
  "performances of SLADE and its several variants where the memoryupdater and the memory generator are replaced by other neuralnetwork architectures": "SLADE-MLP (Instead of GRU): In this variant, we use an MLPinstead of the GRU module to update the memory of each node.Specifically, the memory update procedure (Eq (3) in the mainpaper) is replaced by = MLP([ ||]). Note that in this vari-ant, and are treated as if they are independent, and thus itcannot capture the temporal dependency between them.",
  "=1W1( ||( ))),": "where W1, W2 R 2 , {1, ...,} denote the indices of theneighbors of the target node , {1, ..., } denote the timesof the most recent interactions with them, and the weights ofeach linear layer are denoted as W1 and W2, respectively. Note : Comparison of AUC (in %) of SLADE and its struc-tural variants. For each dataset, the best and the second-bestperformances are highlighted in boldface and underlined,respectively. Across all datasets, SLADE consistently demon-strated the best or second-best performance compared to theother variants.",
  "that this variant does not use any attention mechanism i.e., allneighbors are treated with equal importance": "We compare the performances of SLADE and the above threevariants, utilizing the same hyperparameter settings as the originalSLADE. As shown in , SLADE achieves the best performancein three out of four datasets. This result demonstrates the effective-ness of GRU and TGAT, i.e., the importance of modeling temporaldependency in memory update and temporal attention in memorygeneration. Specifically, SLADE-MLP and SLADE-GAT consistentlyunderperform SLADE and SLADE-SUM, demonstrating the impor-tance of utilizing temporal information and temporal dependencyin detecting dynamic anomalies. While SLADE-SUM outperformsSLADE in the Wikipedia dataset, its performance gain is marginal,falling within the standard deviation.Extra Experimental Results in Online Appendix :Extra Experimental Results in Online Appendix :Extra Experimental Results in Online Appendix :Extra Experimental Results in Online Appendix :Extra Experimental Results in Online Appendix :Extra Experimental Results in Online Appendix :Extra Experimental Results in Online Appendix :Extra Experimental Results in Online Appendix :Extra Experimental Results in Online Appendix :Extra Experimental Results in Online Appendix :Extra Experimental Results in Online Appendix :Extra Experimental Results in Online Appendix :Extra Experimental Results in Online Appendix :Extra Experimental Results in Online Appendix :Extra Experimental Results in Online Appendix :Extra Experimental Results in Online Appendix :Extra Experimental Results in Online Appendix : We ex-plore more baselines, including (a) neural networks trained throughlink-prediction-based self-supervised learning and (b) anomaly-detection methods based on DTDGs and static graphs. We alsouse a transportation network dataset for evaluation. We furtherinvestigate (a) robustness to anomalies in training, (b) training withlabel supervision, (c) learnable time encoding, (d) anomalies withcamouflage, and (e) dynamic heterogeneous graphs.",
  "Conclusion": "We proposed SLADE, a novel self-supervised method for dynamicanomaly detection in edge streams, with the following strengths: SLADE does not rely on any label supervision while being ableto capture complex anomalies (). SLADE outperforms state-of-the-art anomaly detection methodsin the task of dynamic anomaly detection in edge streams. SLADEachieves a performance improvement of up to 12.80% and 4.23%compared to the best-performing unsupervised and supervisedbaseline methods, respectively (.2).",
  "Yen-Yu Chang, Pan Li, Rok Sosic, MH Afifi, Marco Schweighauser, and JureLeskovec. 2021. F-fade: Frequency factorization for anomaly detection in edgestreams. In WSDM": "Kyunghyun Cho, Bart Van Merrinboer, Caglar Gulcehre, Dzmitry Bahdanau,Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phraserepresentations using RNN encoder-decoder for statistical machine translation.In EMNLP. Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014.Empirical evaluation of gated recurrent neural networks on sequence modeling.In NeurIPS, Deep Learning and Representation Learning Workshop.",
  "Jundong Li, Harsh Dani, Xia Hu, and Huan Liu. 2017. Radar: Residual analysisfor anomaly detection in attributed networks.. In IJCAI": "Yixin Liu, Zhao Li, Shirui Pan, Chen Gong, Chuan Zhou, and George Karypis.2021. Anomaly detection on attributed networks via contrastive self-supervisedlearning. IEEE Transactions on Neural Networks and Learning Systems 33, 6 (2021),23782392. Yixin Liu, Shirui Pan, Yu Guang Wang, Fei Xiong, Liang Wang, Qingfeng Chen,and Vincent CS Lee. 2021. Anomaly detection in dynamic graphs via transformer.IEEE Transactions on Knowledge and Data Engineering (2021). Xiaoxiao Ma, Jia Wu, Shan Xue, Jian Yang, Chuan Zhou, Quan Z Sheng, Hui Xiong,and Leman Akoglu. 2021. A comprehensive survey on graph anomaly detectionwith deep learning. IEEE Transactions on Knowledge and Data Engineering (2021).",
  "Ashwin Paranjape, Austin R Benson, and Jure Leskovec. 2017. Motifs in temporalnetworks. In WSDM": "Tahereh Pourhabibi, Kok-Leong Ong, Booi H Kam, and Yee Ling Boo. 2020. Frauddetection: A systematic literature review of graph-based anomaly detectionapproaches. Decision Support Systems 133 (2020), 113303. Emanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, FedericoMonti, and Michael Bronstein. 2020. Temporal Graph Networks for Deep Learningon Dynamic Graphs. In ICML 2020 Workshop on Graph Representation Learning.",
  "AAPPENDIX: Dataset DetailsA.1Real-world Datasets": "We use 4 real-world datasets for a dynamic anomaly detection task;two social network datasets (Wikipedia and Reddit ) 2 and twofinancial network datasets (Bitcoin-alpha and Bitcoin-OTC ) 3.Basic descriptive statistics of each dataset are provided in .Below, we provide a detailed description of each dataset.In Wikipedia and Reddit datasets, which are social networksbetween users, a users dynamic label at time indicates the usersstate at time . Specifically, if a user is banned by administratorsat time , the label of the user at time is marked as abnormal.Otherwise, the user has a label of normal. Note that these labels areinherently given in the original datasets.Bitcoin-alpha and Bitcoin-OTC datasets are temporal weighted-signed networks between users. Here, the weight of each directionaledge indicates how much the source user trusts the destination user.Specifically, for each edge, its weight, which lies between -10 (totaldistrust) to +10 (total trust) is given, together with the time of theinteraction. Note that, in our experiments, the edge weights areutilized to create ground-truth dynamic labels, and they are notincluded in the inputs for the anomaly detection methods. Whilethe used social networks inherently contain dynamic node states(i.e., banned), such information of an individual user is not pro-vided in this case. Therefore, we assign a dynamic label to eachnode according to the weights of the interactions the node is in-volved in. Note that since the overall Bitcoin transaction systemsare anonymized, it is not reliable to determine the state of a useraccording to a single interaction.Hence, to obtain reliable labels, we adopt a two-stage labelingprocess: for each user, first, we define the (1) overall state of the",
  "WikipediaRedditBitcoin-alphaBitcoin-OTCSynthetic-HijackSynthetic-New": "# Nodes9,22710,9843,7835,881986996# Edges157,474672,44724,18635,592333,200333,200# Features1721720000# anomalies2173668742,5683,2903,290anomalies ratio0.14%0.05%3.61%7.22%0.99%0.99%anomaly typepost banedit banunreliable userunreliable userspammerspammer user, then define (2) the dynamic state of the user based on theoverall state. We further describe this process.(1): Defining overall state: For each user, if the sum of the scoresa user has received throughout the entire edge stream is smallerthan zero, the overall state of the user is decided to be abnormal.Otherwise, the overall state of the user is labeled as normal. (2): Defining dynamic state: For each user that has the normaloverall state, the dynamic label of the user is assumed to bealways normal. Conversely, for each user that has the abnormaloverall state, when the user becomes the destination node of anegative-weighted interaction (i.e., receiving a negative trustscore from other users), the dynamic label of the user at thecorresponding interaction time is assumed to be abnormal.Otherwise, if a user receives a positive trust score from anotheruser, we assume the dynamic label of the user at that time isassumed to be normal. Note that these created dynamic nodelabels are utilized as ground-truth dynamic node labels.As mentioned above, the weight of each edge determines thelabel of a destination node, not a source node4. Regarding nodeand edge features, all of the used baseline neural network-basedanomaly detection models require feature information of nodes andedges. On the other hand, in Bitcoin-alpha and -OTC datasets, suchinformation is not given. Following prior works that usedsuch models in the corresponding datasets, we utilize zero vectorsfor node and edge features of such datasets.Regarding data splits, for all the datasets, we fix the chronologicalsplit with 70 % for training, 15% for validation, and the last 15 %for testing, which is a common setting that is widely used in manyexisting works .",
  "A.2Semi-Synthetic Datasets": "Next, we describe the details of the Synthetic-Hijack and Synthetic-New datasets, which we utilize in .2. These datasets arevariants of the real-world email communication dataset: Email-EU . Nodes indicate users, and an edge (, ,) indicates anemail between users and at time . Since the dataset does nothave ground-truth anomalous interactions, we inject anomaliesdiscussed in .1 (T1, T2, and T3). To this end, we firstchoose the time interval where interactions have actively occurred(spec., interactions occur within (4.06 107, 4.54 107) timestamp)since our method and baselines can easily detect anomalies thatare injected in the later timestamp than 4.54 107. Then, we setthe timestamp region where the last 10% (in chronicle order) inter-actions occur as an evaluation region. After, we select candidateaccounts to perform anomalous actions:",
  "Synthetic-Hijack: Randomly select 10 nodes ( 1% of the nor-mal nodes) from nodes that are normal before the final 10% ofthe dataset and do not appear in the evaluation region": "Synthetic-New: Make 10 new nodes ( 1% of the normal nodes)that have never appeared before the evaluation region.Subsequently, we inject anomalies equivalent to approximately 1%of the total normal interactions into the evaluation region (see thenext paragraph for details). Each of these anomalous actions isrepresented as temporal edge (, ,), where at time , theanomalous source node from the selected candidates sends aspam email to some normal node . In this case, the dynamic labelof the node at time is anomalous.We further elaborate on how we make each anomalous edge. Weconsider the characteristics of spammers. We assume that spam-mers tend to target an unspecified majority with multiple spamemails in a short time interval. Based on this assumption, (1) wesample a timestamp in the evaluation region uniformly at random,(2) pick one anomalous node from the candidate pool, regardingit as the source node, (3) randomly choose 10 normal nodes asdestinations, and (4) make 10 edges by joining the selected sourcenode with each selected destination nodes. (5) Lastly, we assigneach edge a timestamp , where is sampled uniformly atrandom from . This process ((1) - (5)) is repeated until wehave approximately 1% of anomalies relative to the total normalinteractions for the Synthetic-Hijack and Synthetic-New.",
  "B.2Details of Baselines in Main Experiments": "As mentioned in .1, we tune most hyperparameters ofeach baseline method by conducting a full grid search on the vali-dation set of each dataset. For other hyperparameters, we strictlyfollow the setting provided in their official code, because it leadsto better results than grid searches. The selected hyperparametercombination of each model is reported in . Rule-based Methods. Hyperparameter search space of each rule-based method is as below: Sedanspot: sample size among (5000, 10000, 20000), number ofrandom walkers among (100, 200, 300), and restart probabilityamong (0.1, 0.5, 0.9)",
  "B.4Detailed Hyperparameters of SLADE andSLADE-HP": "We train both SLADE and SLADE-HP using the Adam optimizer,with a learning rate of 3 106 and a weight decay of 104. We fixthe dropout probability and the number of attention heads in TGATto 0.1 and 2, respectively. In addition, we fix the scaling scalar oftemporal encoding (Eq (2) in the main paper) to = 10, = 25.6,the weight of each loss in L (Eq (7) in the main paper) to = = 1, and the dimension of a time encoding to 256, which isequivalent to the memory dimension. As mentioned in .1 ofthe main paper, we tune several hyperparameters of SLADE-HP onthe validation set of each dataset, as we tune the hyperparametersof all baselines. The search space is as follows: SLADE-HP: batch size among (100, 200, 300), the weight in mem-ory generation loss L for a source node () among (0.1, 1, 10),and for a destination node () among (0.1, 1, 10).Our final hyperparameter settings for SLADE and SLADE-HP arealso reported in ."
}