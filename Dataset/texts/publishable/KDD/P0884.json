{
  "Abstract": "Multivariate time series data suffer from the problem of missingvalues, which hinders the application of many analytical methods.To achieve the accurate imputation of these missing values, ex-ploiting inter-correlation by employing the relationships betweensequences (i.e., a network) is as important as the use of temporaldependency, since a sequence normally correlates with other se-quences. Moreover, exploiting an adequate network depending ontime is also necessary since the network varies over time. However,in real-world scenarios, we normally know neither the networkstructure nor when the network changes beforehand. Here, we pro-pose a missing value imputation method for multivariate time series,namely MissNet, that is designed to exploit temporal dependencywith a state-space model and inter-correlation by switching sparsenetworks. The network encodes conditional independence betweenfeatures, which helps us understand the important relationshipsfor imputation visually. Our algorithm, which scales linearly withreference to the length of the data, alternatively infers networksand fills in missing values using the networks while discoveringthe switching of the networks. Extensive experiments demonstratethat MissNet outperforms the state-of-the-art algorithms for mul-tivariate time series imputation and provides interpretable results.",
  "Multivariate time series, Missing value imputation, Network infer-ence, State-space model, Graphical lasso": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08 ACM Reference Format:Kohei Obata, Koki Kawabata, Yasuko Matsubara, and Yasushi Sakurai. 2024.Mining of Switching Sparse Networks for Missing Value Imputation inMultivariate Time Series . In Proceedings of the 30th ACM SIGKDD Conferenceon Knowledge Discovery and Data Mining (KDD 24), August 2529, 2024,Barcelona, Spain. ACM, New York, NY, USA, 11 pages.",
  "Introduction": "With the development of the Internet of Things (IoT), multivariatetime series are generated in many real-world applications, such asmotion capture , and health monitoring . However, thereare inevitably many values missing from these data, and this hasmany possible causes (e.g., sensor malfunction). As most algorithmsassume an intact input when building models, missing value impu-tation is indispensable for real-world applications .In time series data, missing values often occur consecutively,leading to a missing block in a sequence, and can happen simultane-ously to multiple sequences . To effectively reconstruct missingvalues from such partially observed data, we must exploit both tem-poral dependency, by taking account of past and future values in thesequence, and inter-correlation, by using the relationship betweendifferent sequences (i.e., a network) . Here, a network doesnot necessarily mean the spatial proximity of sensors but ratherunderlying connectivity (e.g., Pearson correlation or partial correla-tion). Moreover, as time series data are normally non-stationary, sois the network; an adequate network must be exploited dependingon time . We collectively refer to a group of time pointswith the same network as a regime. shows an illustrativeexample where missing blocks randomly exist in a multivariatetime series consisting of three features (i.e., A, B, and C). Each timepoint belongs to either of two regimes with different networks (i.e.,#1 and #2), where the thickness of the edge indicates the strengthof the interplay between features. It is appropriate to use the valuesof feature C to impute the block missing from feature B in regime#1 since the network has an edge between B and C. On the otherhand, in regime #2, it is preferable to use feature A, as the networksuggests. Nevertheless, in real-world scenarios, we often have noinformation about the data; that is, we do not know the structureof the network, let alone when the network changes. Thus, givena partially observed multivariate time series, how can we infernetworks and allocate each time point to the correct regime? How",
  "KDD 24, August 2529, 2024, Barcelona, SpainKohei Obata, Koki Kawabata, Yasuko Matsubara, and Yasushi Sakurai": "Tolou Shadbahr, Michael Roberts, Jan Stanczuk, Julian Gilbey, Philip Teare, SrenDittmer, Matthew Thorpe, Ramon Vias Torn, Evis Sala, Pietro Li, Mishal Patel,Jacobus Preller, Ian Selby, Anna Breger, Jonathan R. Weir-McCall, EffrossyniGkrania-Klotsas, Anna Korhonen, Emily Jefferson, Georg Langs, Guang Yang,Helmut Prosch, Judith Babar, Lorena Escudero Snchez, Marcel Wassin, MarkusHolzer, Nicholas Walton, Pietro Li, James H. F. Rudd, Tuomas Mirtti, Antti SakariRannikko, John A. D. Aston, Jing Tang, and Carola-Bibiane Schnlieb. 2023. Theimpact of imputation quality on machine learning classifiers for datasets withmissing values. Communications Medicine 3, 1 (2023).",
  "Related work": "We review previous studies closely related to our work. shows the relative advantages of MissNet. Current approaches fallshort with respect to at least one of these desired characteristics.Time series missing value imputation. Missing value imputa-tion for time series is a very rich topic . We roughly classifymissing value imputation methods as Matrix Factorization (MF)-based, SSM-based, and Deep Learning (DL)-based approaches.MF-based methods, such as SoftImpute based on Singu-lar Value Decomposition (SVD), recover missing values from low-dimensional embedding matrices of partially observed data . For example, SoRec , proposed as a recommenda-tion system, constrains MF with a predefined network to exploitinter-correlation. Since MF is limited in capturing temporal depen-dency, TRMF uses an Auto-Regressive (AR) model and imposestemporal smoothness on MF.SSMs, such as Linear Dynamical Systems (LDS) , use latentspace to capture temporal dependency, where the data point de-pends on all past data points . To fit more complex timeseries, Switching LDS (SLDS) switches multiple LDS models.SSM-based methods, such as DynaMMo , focus on capturingthe dynamic patterns in time series rather than inter-correlationimplicitly captured through the latent space. To use the underlyingconnectivity in multivariate time series, DCMF , and its tensorextension Facets use SSM constrained with a predefined net-work, which is effective, especially when the missing rate is high.However, they assume that the network is accurately known andfixed, while it is usually unknown and may change over time inreal-world scenarios.Recently, extensive research has focused on DL-based methods,employing techniques including graph neural networks , self-attention , and, most recently, diffusion mod-els , to harness their high model capacity . Forexample, BRITS and M-RNN impute missing values accord-ing to hidden states from bidirectional RNN. To utilize dynamicinter-correlation in time series, POGEVON requires a sequenceof networks and imputes missing values in time series and missingedges in the networks, assuming that the network varies over time.Although DL-based methods can handle complex data, the impu-tation quality depends heavily on the size and the selection of thetraining dataset.",
  "Mining of Switching Sparse Networks for Missing Value Imputation in Multivariate Time SeriesKDD 24, August 2529, 2024, Barcelona, Spain": "Sparse network inference. From another perspective, our methodinfers sparse networks from time series containing missing valuesand discovers regimes (i.e., clusters) based on networks. Inferring asparse inverse covariance matrix (i.e., network) from data helps usto understand feature dependency in a statistical way. Graphicallasso , which maximizes the Gaussian log-likelihood impos-ing an 1-norm penalty, is one of the most commonly used tech-niques for estimating a sparse network from static data. Stdler andBhlmann have tackled inferring a sparse network from par-tially observed data according to conditional probability. However,the network varies over time ; thus, TVGL inferstime-varying networks by considering the time similarity with anetwork belonging to neighboring segments. To infer time-varyingnetworks in the presence of missing values, MMGL , whichemploys TVGL, uses the expectation-maximization (EM) algorithmto repeat the inference of time-varying networks and missing valueimputation based on conditional probability under the conditionthat each segment has the same observed features. Discoveringclusters based on networks , such as TICC and TAGM ,provides interpretable results that other traditional clustering meth-ods cannot find. However, they cannot handle missing values.As a consequence, none of the previous studies have addressedmissing value imputation for multivariate time series by employingsparse network inference and segmentation based on the network.",
  "Preliminaries3.1Problem definition": "In this paper, we focus on the task of multivariate time series miss-ing value imputation. We use a multivariate time series with features and timesteps = {1, 2, . . . , } R . To repre-sent the missing values in , we introduce the indicator matrix R , where , indicates the availability of feature attimestep : , being 0 or 1 indicates whether , is missing orobserved. Thus, the observed entries can be described as = ,where is a Hadamard product. Our problem is formally writtenas follows:",
  "Graphical lasso": "We use graphical lasso to infer the network for each regime.Given , graphical lasso estimates the sparse Gaussian inversecovariance matrix (i.e., network) R , also known as theprecision matrix. The network encodes pairwise conditional inde-pendencies among features, e.g., if , = 0, then features and are conditionally independent given the values of all the otherfeatures. The optimization problem is expressed as follows:",
  "log(2),(2)": "where must be symmetric positive definite (++). (, ) isthe log-likelihood and R is the empirical mean of . 0 is a hyperparameter for determining the sparsity level of thenetwork, and ,1 indicates the off-diagonal 1-norm. This is aconvex optimization problem that can be solved via the alternatingdirection method of multipliers (ADMM) .",
  "Optimization model": "MissNet infers sparse networks and fills in missing values usingthe networks while discovering regimes. We first introduce threeinteracting components of our model: a regime-switching model,an imputation model, and a network inference model. Then, wedefine the optimization formulation.Regime-switching model. MissNet describes the change of net-works by regime-switching. Let be the number of regimes, ={1, . . . } R be regime assignments, and be a one-hotvector 2 that indicates R belongs to -regime. We assumeregime-switching to be a discrete first-order Markov process:",
  "(1) = 0,(+1|) = +1,,(3)": "where R is the Markov transition matrix and 0 R isthe initial state distribution.Imputation model. MissNet imputes missing values exploitingtemporal dependency and inter-correlation indicated by the net-works. We assume that the temporal dependency is consistentthroughout all regimes and captured in the latent space of an SSM,which allows us to consider long-term dependency. We define the la-tent states = {1, 2, . . . , } R corresponding to , where is the number of latent dimensions, and +1 R is linear to through the transition matrix R with the covariance ,shown in Eq. (5). As defined in Eq. (4), the first timestep of latentstate 1 is defined by the initial state 0 and the covariance 0.",
  "|, ( ), N ( ( ), ( ) ).(6)": "MissNet captures inter-correlation by adding a constraint on (). Let it be assumed that the contextual matrix of the -regime () R encodes the inter-correlation of belonging to the-regime (1 ). We define the contextual latent factor ofthe -regime () R , and the -th column (1 ) ofthe contextual matrix ()as linear to ()through the observation",
  ",(10)": "where is any constant value for convenience, and is a hyper-parameter that controls the sparsity of the network (i.e., inversecovariance matrix) ( ) with 1-norm, which helps avoid capturingspurious correlations.Optimazation formulation. Our goal is to estimate the modelparameters = {, 0, 0, , , , , , , 0, } and find thelatent factors , , , , , where the letters with a dot indicate a setof vectors/matrices/scalers (e.g., = { ()}=1), that maximizesthe following joint probability distribution:",
  "Algorithm": "It is difficult to find the global optimal solution of Eq. (11), for thefollowing reasons: ( i ) As a constraint for , has to be fixed andencode inter-correlation; (ii) , and jointly determine , and and jointly determine ; (iii) Calculating the correct is NP-hard.Hence, we aim to find its local optimum instead, following the EMalgorithm, where the graphical model for each iteration is shownin . Specifically, to address the aforementioned difficulties, we",
  ": Graphical model of MissNet at each iteration": "employ the following steps: ( i ) we consider is observed in eachiteration, and we update it at the end of the iteration using ; (ii) weregard as a model parameter, thus { , , } are independent with{, }. We alternate the inference of {, } and and the updateof the model parameters; (iii) we employ a Viterbi approximationand infer the most likely . 4.2.1E-step. Given , we can infer {, } and independently.Let denote the indices of the observed entries of . Theobserved-only data and the corresponding observed-only obser-vation matrix ()are defined as follows:",
  "= (), ()= () (, :).(12)": "Inferring and . and are coupled, and so must be jointlydetermined. We first use a Viterbi approximation to find the mostlikely regime assignments that maximize the log-likelihood Eq. (11).The likelihood term obtained during the calculation of also actsas a Kalman Filter (forward algorithm). Then, we infer with aRauch-Tung-Streibel (RTS) smoother (backward algorithm).In a Viterbi approximation, finding requires the partial cost,1,, when the switch is to regime at time from regime attime 1. To calculate the partial cost, we define the following LDSstate and variance terms:",
  "=1, ,E[],": "where 0 1 is a hyperparameter employed as a trade-off forthe contributions of inter-correlation and temporal dependency.The details of updating {, 0, 0, , , , } are presented inAppendix A.3.For the network inference model, we calculate with Eq. (22)and then update () by calculating the empirical mean of be-longing to the -regime and () by solving the graphical lassoproblem shown in Eq. (23) via ADMM.",
  "(),)( ) .(25)": "4.2.4Overall algorithm. We have the overall algorithm shown asAlg. 1 to obtain a local optimal solution of Eq. (11). Given a partiallyobserved multivariate time series , an indicator matrix , thedimension of latent state , the number of regimes , the networkparameter , and the sparse parameter , our algorithm aims tofind the latent factors , , , , , other model parameters in ,and imputed time series .The MissNet algorithm starts by initializing with a linear in-terpolation, and by randomly initializing , , , , , and (Line 3).Then, it alternately updates the latent factors and parameters untilthey converge. In each iteration, we consider to be given and to be a model parameter. In an iteration, we first conduct a Viterbiapproximation to calculate the most likely regime assignments (Line 5-12). Then, we infer the expectations of and (Line 13-15and Line 16-18), and we update and model parameters (Line 20),and at the end of the iteration, we update (Line 21).",
  "Proof. See Appendix A.2": "represents the number of observed features of . Note that2 =1(3 + 2 + 2 + 3 ) is upper bounded by 2 3. Inpractice, the length of the time series () is often orders of mag-nitude greater than the number of features (). Hence, the actualrunning time of MissNet is dominated by the term related to ,which is linear in .",
  "Datasets. We use the following datasets.Synthetic. We generate two types of synthetic data, PatternA andPatternB, five times each, by defining and . We set = 1000, =": "50 and = 10 (Appendix B.1). PatternA has one regime ( = 1),and in PatternB ( = 2), two regimes switch at every 200 timesteps.MotionCapture. This dataset contains nine types of full bodymotions of MotionCapture database 3. Each motion measures thepositions of 41 bones in the human body, resulting in a total of 123features (X, Y, and Z coordinates).Motes. This dataset consists of temperature measurements fromthe 54 sensors deployed in the Intel Berkeley Research Lab 4. Weuse hourly data for the first two weeks (03-01 03-14). Originally,9.6% of the data is missing, including a blackout from 03-10 to 03-11where all the values are missing. 5.1.2Data preprocessing. We generate a synthetic missing block ata length of 0 5% of the data length and place it randomly until thetotal missing rate reaches {10, 20, . . . 80%}. Thus, a missing blockcan be longer than 0.05 when it overlaps. An additional 10% ofmissing values are added for hyperparameter tuning. Each datasetfeature is normalized independently using a z-score so that eachdataset has a zero mean and a unit variance. 5.1.3Comparison methods. We compare our method with state-of-the-art imputation methods ranging from classical baselines (Linearand Quadratic), MF-based methods (SoftImpute, CDRec and TRMF),SSM-based methods (DynaMMo and DCMF), to DL-based methods(BRITS, SAITS and TIDER).",
  "DynaMMo first fills in missing values using linear in-terpolation and then uses the EM algorithm to iterativelyrecover missing values and update the LDS model": "DCMF adds a contextual constraint to SSM and capturesinter-correlation by a predefined network. As suggested inthe original paper, we give the cosine similarity betweeneach pair of time series calculated after linear interpolationas a predefined network. This method is similar to MissNetif we set = 1, employ a predefined network that is fixedthroughout the algorithm, and eliminate the effect of regime-switching and network inference models from MissNet.",
  ": Critical difference diagram of real-world datasets": "5.1.4Hyperparameter setting. For MissNet, we use the latent di-mensions of 10, 30 and 15 for Synthetic, MotionCapture and Motes,respectively, and we set = 1.0, = 0.5 for all datasets. We setthe correct number of regimes on Synthetic datasets; we vary = {1, 2, 3} for other datasets. We list the detailed hyperparametersettings for the baselines in Appendix B.2.",
  "Results": "5.2.1Q1. Effectiveness. We show the effectiveness of MissNetover baselines in missing value imputation.Synthetic. (a) and (b) show the results obtained with Syn-thetic datasets. SSM and MF-based methods perform worse withPatternB than with PatternA due to the increased complexity ofdata. DL-based methods, especially BRITS, are less affected thanksto their high modeling power. MissNet significantly outperformsDCMF for PatternB although it produces similar results for Pat-ternA. This is because DCMF fails to capture inter-correlation withPatternB since it can only use one predefined network and cannotafford a change of network. Meanwhile, MissNet can capture theinter-correlation for two different regimes thanks to our regime-switching model. However, MissNet fails to discover the correcttransition when the missing rate exceeds 70%, and RMSE becomessimilar to DCMF.MotionCapture and Motes. The results for MotionCapture andMotes datasets are shown in (c) (l). We can see that MissNet and DCMF constantly outperform other baselines thanks to theirability to exploit inter-correlation explicitly. shows the corresponding critical difference diagram forall missing rates based on the Wilcoxon-Holm method , wheremethods that are not connected by a bold line are significantlydifferent in average rank. This confirms that MissNet significantlyoutperforms other methods, including DCMF, in average rank. Ouralgorithm for repeatedly inferring networks and the use of 1-normenables the inference of adequate networks for imputation, con-tributing to better results than DCMF, which uses cosine similarityas a predefined network that may contain spurious correlationsin the presence of missing values. Note that MissNet and DCMFexhibit only minor differences when the missing rate is low (10%)because a plausible predefined network can be calculated fromobserved data.Classical Linear and Quadratic baselines are unsuitable for im-puting missing blocks since they impute missing values mostlyfrom neighboring observed points and cannot capture temporalpatterns when there are large gaps. DL-based methods lack suffi-cient training data and are not suitable for the data we use here,making them perform particularly poorly at a high missing rate, asalso noted in . MF-based methods, SoftImpute and CDRec, havea higher RMSE than SSM-based methods since they do not modelthe temporal dynamics of the data. TRMF utilizes temporal depen-dency with the AR model, however, it can only capture certainlags specified on the hyperparameter of the AR model. SSM-basedmethods are superior in imputation to other groups owing to theirability to capture temporal dependency in latent space. DynaMMoimplicitly captures inter-correlation in latent space. Therefore, it isno match for MissNet or DCMF. demonstrates the results for the MotionCapture Run dataset(missing rate = 60%). We compare the imputation result for the sen-sor at RKNE provided by the top five methods in terms of averagerank, including MissNet, in (a). BRITS and SoftImpute failto capture the dynamics of time series while providing a good fitto observed values. The imputation of DynaMMo is smooth, but",
  ": Hyperparameter sensitivity results": "some parts are imprecise since it cannot explicitly exploit inter-correlation. MissNet and DCMF can effectively exploit other ob-served features associated with RKNE, thereby accurately imputingmissing values where other methods fail (e.g., = 20 40, 60 80). (b) shows the sensor network of Y-coordinate values obtainedby MissNet plotted on the human body, where a green/yellow dot(node) indicates a sensor placed on the front/back of the body andthe thickness and color (blue/red) of the edges are the value andsign (positive/negative) of partial correlations, respectively. We cansee that the sensors located close together have edges, meaning theyare conditionally dependent given all other sensor values. For ex-ample, the sensor at RKNE has edges between RTHI, RSHN, LKNE,and LTHI. They are located to RKNE nearby and show similar dy-namics, thus it is reasonable to consider that they are connected.Since MissNet can infer such a meaningful network from partiallyobserved data, the imputation of MissNet is more accurate thanthat of DCMF. 5.2.2Hyperparameter sensitivity. We take the Motes dataset andshow the impact of hyperparameters: the latent dimension , thenetwork parameter , and the sparse parameter . We show themean RMSE of all missing rates.Latent dimension. (a) shows the impact of . As becomeslarger, the models fitting against the observed data increases. Aswe can see, the RMSE is constantly decreasing as increases andstabilizes after 15. This shows that MissNet does not overfit theobserved data even for a large .Network parameter. determines the contributions of inter-correlation and temporal dependency to learning . If = 0, the",
  ": Case study on Motes dataset": "contextual matrix is ignored. If = 1, only is considered forlearning . (b) shows the results of varying and they arerobust except when = 1 (RMSE = 0.76). We can see that = 0.4shows the best result, indicating that both temporal dependencyand inter-correlation are important for precise imputation.Sparse parameter. controls the sparsity of the networks through 1-norm. The bigger becomes, the more sparse the net-works become, resulting in MissNet considering only strong inter-play. By contrast, when is small, MissNet considers insignificantinterplays. (c) shows the impact of . We can see that thesparsity of the networks affects the accuracy, and the best existsbetween 0.1 and 10. Thus, the 1-norm constraint helps MissNetto exploit important relationships. 5.2.3Q2. Scalability. We test the scalability of the MissNet algo-rithm by changing the number of the data length () in PatternA. shows the computation time for one iteration plotted with thedata length. As it shows, our proposed MissNet algorithm scaleslinearly with regard to the data length . 5.2.4Q3. Interpretability. We demonstrate how MissNet helps usunderstand data. We have shown an example with the MotionCap-ture Run dataset in (b) where MissNet provides an inter-pretable network. Here, we demonstrate the results on the Motesdataset (missing rate = 30%) of MissNet ( = 2). (a) showsthe regime assignments , and MissNet mostly assigns night hoursto regime #1 and working hours (about 9 am. to 10 pm.) to regime#2, suggesting that they have different networks. (b) and (c)show the networks for regimes #1 and #2 obtained by MissNetplotted on the building layout. The sensor numbers in the figureare plotted on the actual sensor deployments. As we can see, thetwo regimes have different networks, and a common feature is thatthe neighboring sensors tend to form edges, which aligns with ourexpectations, considering that the sensors measure temperatureand, thus, neighboring sensors correlate. The network of regime #2has more edges than that of #1, and the edges are 1.2 times longer",
  "Conclusion": "In this paper, we proposed an effective missing value imputationmethod for multivariate time series, namely MissNet, which cap-tures temporal dependency based on latent space and inter-correlationby the inferred networks while discovering regimes. Our proposedmethod has the following properties: (a) Effective: it outperformsthe state-of-the-art algorithms for multivariate time series imputa-tion. (b) Scalable: the computation time of MissNet scales linearlywith regard to the length of the data. (c) Interpretable: it providessparse networks and regime assignments, which help us understandthe important relationships for imputation visually. Our extensiveexperiments demonstrated the above properties of MissNet. The authors would like to thank the anonymous referees for theirvaluable comments and helpful suggestions. This work was sup-ported by JSPS KAKENHI Grant-in-Aid for Scientific Research Num-ber JP21H03446, JP22K17896, NICT JPJ012368C03501, JST-AIP JP-MJCR21U4, JST CREST JPMJCR23M3.",
  "Parikshit Bansal, Prathamesh Deshpande, and Sunita Sarawagi. 2021. MissingValue Imputation on Multidimensional Time Series. Proc. VLDB Endow. 14, 11(jul 2021), 25332545": "Jeroen Berrevoets, Fergus Imrie, Trent Kyono, James Jordon, and Mihaela van derSchaar. 2023. To impute or not to impute? missing data in treatment effectestimation. In International Conference on Artificial Intelligence and Statistics.PMLR, 35683590. Stephen P. Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein.2011. Distributed Optimization and Statistical Learning via the AlternatingDirection Method of Multipliers. Found. Trends Mach. Learn. 3, 1 (2011), 1122.",
  "Lei Li, James McCann, Nancy S Pollard, and Christos Faloutsos. 2009. Dynammo:Mining and summarization of coevolving sequences with missing values. In KDD.507516": "SHUAI LIU, Xiucheng Li, Gao Cong, Yile Chen, and YUE JIANG. 2022. Multivari-ate Time-series Imputation with Disentangled Temporal Representations. In TheEleventh International Conference on Learning Representations. Yuehua Liu, Tharam Dillon, Wenjin Yu, Wenny Rahayu, and Fahed Mostafa. 2020.Missing value imputation for industrial IoT sensor data with large gaps. IEEEInternet of Things Journal 7, 8 (2020), 68556867. Hao Ma, Haixuan Yang, Michael R Lyu, and Irwin King. 2008. Sorec: socialrecommendation using probabilistic matrix factorization. In Proceedings of the17th ACM conference on Information and knowledge management. 931940. Solt Kovcs Malte Londschien and Peter Bhlmann. 2021. Change-Point Detectionfor Graphical Models in the Presence of Missing Values. Journal of Computationaland Graphical Statistics 30, 3 (2021), 768779.",
  "Karthik Mohan, Palma London, Maryam Fazel, Daniela Witten, and Su-In Lee.2014. Node-Based Learning of Multiple Gaussian Graphical Models. J. Mach.Learn. Res. 15, 1 (jan 2014), 445488": "Ricardo Pio Monti, Peter Hellyer, David Sharp, Robert Leech, Christoforos Anag-nostopoulos, and Giovanni Montana. 2014. Estimating time-varying brain con-nectivity networks from functional MRI time series. NeuroImage 103 (2014),427443. A. Namaki, A.H. Shirazi, R. Raei, and G.R. Jafari. 2011. Network analysis of afinancial market based on genuine correlation and threshold method. Physica A:Statistical Mechanics and its Applications 390, 21 (2011), 38353841.",
  "Spiros Papadimitriou, Jimeng Sun, and Christos Faloutsos. 2005. Streamingpattern discovery in multiple time-series. (2005)": "Vladimir Pavlovic, James M Rehg, Tat-Jen Cham, and Kevin P Murphy. 1999. Adynamic Bayesian network approach to figure tracking using learned dynamicmodels. In Proceedings of the seventh IEEE international conference on computervision, Vol. 1. IEEE, 94101. Zhen Qin, Yibo Zhang, Shuyu Meng, Zhiguang Qin, and Kim-Kwang RaymondChoo. 2020. Imaging and fusing time series for wearable sensor-based humanactivity recognition. Information Fusion 53 (2020), 8087. Xiaobin Ren, Kaiqi Zhao, Patricia Riddle, Katerina Takova, Lianyan Li, andQingyi Pan. 2023. DAMR: Dynamic Adjacency Matrix Representation Learningfor Multivariate Time Series Imputation. SIGMOD (2023).",
  "Andrew Viterbi. 1967. Error bounds for convolutional codes and an asymptoticallyoptimum decoding algorithm. IEEE transactions on Information Theory 13, 2 (1967),260269": "Dingsu Wang, Yuchen Yan, Ruizhong Qiu, Yada Zhu, Kaiyu Guan, AndrewMargenot, and Hanghang Tong. 2023. Networked time series imputation viaposition-aware graph enhanced variational autoencoders. In KDD. 22562268. Xu Wang, Hongbo Zhang, Pengkun Wang, Yudong Zhang, Binwu Wang,Zhengyang Zhou, and Yang Wang. 2023. An Observed Value Consistent Diffu-sion Model for Imputing Missing Values in Multivariate Time Series. In KDD.24092418. Xiuwen Yi, Yu Zheng, Junbo Zhang, and Tianrui Li. 2016. ST-MVL: filling missingvalues in geo-sensory time series data. In Proceedings of the 25th InternationalJoint Conference on Artificial Intelligence.",
  "Proof of Lemma 1": "Proof. The overall time complexity is composed of four partsby taking the most time-consuming part of equations for eachiteration considering > , : the complexity for the inferenceof and is (2 =1(3 +2 + 2 + 3 )) related to Eq. (13)and Eq. (15); the inference of is (2 2) (Eq. (19)); M step is(2) related to the calculation of (Eq. (21)); and the updateof is ( 3) (Eq. (23)). Thus, the overall time complexity is(# (2 =1(3 + 2 + 2 + 3 ) + 2 2 + 2 + 3)).",
  "B.2Hyperparameters": "We describe the hyperparameters of the baselines. For Syntheticdatasets, we give a latent dimension of 10 for all baselines. Fora fair comparison, we set the latent dimension of the SSM-basedmethods at the same value as MissNet. For the MF-based methods,we vary the latent dimension {3, 5, 10, 15, 20, 30, 40}. We vary the ARparameter for TRMF {, }. To learn the DL-basedmethods, we add 10% of the data as missing values for training themodel. We vary the window size {16, 32, }. Other hyperparametersare the same as the original codes.",
  "CDiscussion": "While MissNet achieved superior performance against state-of-the-art baselines in missing value imputation, here, we mentiontwo limitations of MissNet in terms of sparse network inferenceand data size.As mentioned in .2.1, MissNet fails to discover thecorrect transition when the missing rate exceeds 70%. However,we claim that MissNet failing to discover the correct transitionwhen the missing rate exceeds 70% is reasonable; rather, correctlydiscovering transition up to 60% is valuable. Several studies tackled the sparse network inference under the existenceof missing values. They aim to infer the correct network and, thus,only utilize the observed value for the network inference. Sinceobserving a complete pair at a high missing rate is rare, it is difficultto infer the correct network. Therefore, the maximum missing ratein their experiments is 30%. Although the experimental settingsare different from ours, we can say that the task of sparse networkinference in the presence of missing values itself is challenging.As shown in the experiments, MissNet performs well even whena relatively small number of samples () and a large number offeatures () since MissNet is a parametric model and we assumethe sparse networks to capture inter-correlation. This cannot beachieved by DL models, which contain a massive number of param-eters that require a large amount of , especially when is largesince all the relationships between features need to be learned. How-ever, unlike DL models, the increased number of samples may notgreatly improve MissNets performance as it has a much smallernumber of parameters than DL models, even though switchingsparse networks increases the models flexibility."
}