{
  "ABSTRACT": "Graph machine learning (GML) is effective in many business ap-plications. However, making GML easy to use and applicable toindustry applications with massive datasets remain challenging.We developed GraphStorm, which provides an end-to-end solu-tion for scalable graph construction, graph model training andinference. GraphStorm has the following desirable properties: (a)Easy to use: it can perform graph construction and model trainingand inference with just a single command; (b) Expert-friendly:GraphStorm contains many advanced GML modeling techniquesto handle complex graph data and improve model performance; (c)Scalable: every component in GraphStorm can operate on graphswith billions of nodes and can scale model training and inferenceto different hardware without changing any code. GraphStorm hasbeen used and deployed for over a dozen billion-scale industry ap-plications after its release in May 2023. It is open-sourced in Github: ACM Reference Format:Da Zheng, Xiang Song, Qi Zhu, Jian Zhang, Theodore Vasiloudis, Runjie Ma,Houyu Zhang, Zichen Wang, Soji Adeshina, Israt Nisa, Alejandro Mottini,Qingjun Cui, Huzefa Rangwala, Belinda Zeng, Christos Faloutsos, GeorgeKarypis. 2024. GraphStorm: all-in-one graph machine learning frameworkfor industry applications. In Proceedings of the 30th ACM SIGKDD Conferenceon Knowledge Discovery and Data Mining (KDD 24), August 2529, 2024,Barcelona, Spain. ACM, New York, NY, USA, 13 pages.",
  "INTRODUCTION": "Recent research has demonstrated the value of GML across a rangeof applications and domains, such as social networks and e-commerce.However, deploying such GML solutions to solve real business prob-lems remains challenging for three reasons. First, industry graphsare massive, usually in the order of many millions or even billionsof nodes and edges. Second, industry graphs are complex. They areusually heterogeneous with multiple node types and edge types. Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08",
  ": Easy and Scalable GML with GraphStorm": "Some nodes and edges are associated with diverse features, such asnumerical, categorical and text/image features, while some othernodes or edges have no features. Third, many applications do notstore data in a graph format. To apply GML to these data, we needto first construct a graph. Defining a graph schema is part of graphmodeling and often requires multiple rounds of trials and errors.Today users need to manually develop complex code with li-braries such as DGL or Pytorch-Geometric . This usuallyrequires users to have significant modeling expertise in GML. Fur-thermore, even if a user has the required expertise, it requiressignificant efforts to develop techniques to achieve good perfor-mance and scale model training and inference to large graphs ona cluster of CPU/GPU machines. These challenges prevent GMLfrom becoming a commonly used technique in industry.To address these challenges, we developed GraphStorm, whichis a no-code/low-code framework for scientists in the industry todevelop, train, and deploy GML models for solving real businessproblems. It provides end-to-end pipelines for graph construction,model training and deployment to simplify every step of devel-oping GML models. It balances the usability, flexibility, modelingcapability, training efficiency and scalability so that users who arenot familiar with GML can easily apply it to their applications. Asillustrated in , GraphStorm allows users to develop GML",
  "KDD 24, August 2529, 2024, Barcelona, SpainDa Zheng et al": "shows an example of the graph schema configurationin a JSON file that contains the descriptions of the information ofnodes and edges.GraphStorm provides different modules for different graph tasks.For example, GraphStorm provides graphstorm.run.gs_link_predictionfor link prediction and provides graphstorm.run.gs_node_classificationfor node classification. The same module can be used for both train-ing and inference.",
  "Easy to use: GraphStorm provides users an easy solutionfrom graph construction to GML model training and infer-ence with a single command line for real-world applications": "Expert-friendly: GraphStorm provides many built-in mod-eling techniques to help scientists improve model perfor-mance in complex industry graphs without writing any code.It also provides a custom model API for users to develop andtrain their own models. Scalable: GraphStorm ensures that every step for GMLmodel development and deployment can scale to graphswith billions of nodes and hundreds of billions of edges andcan scale model training and inference in different hardware(e.g., from single GPU to multi-machine multi-GPU) withoutany code modification.",
  "There are many existing GML frameworks. DGL and Pytorch-Geometric are two popular frameworks that provide low-level": "API for writing graph neural network models (GNNs). Users needto write complex code to address many practical problems in graphapplications. On top of these low-level GML frameworks, peopledeveloped higher-level frameworks specialized for one type of GMLproblems. For example, DGL-KE and Pytorch-BigGraph contain a set of knowledge graph embedding models and providescalable training solutions. TGL provides a set of models forcontinuous-time temporal graphs and Pytorch-Geometric Temporal contains a set of models for spatiotemporal models. These high-level frameworks provide command-line interfaces to train modelswithout writing code, which simplifies model development.AliGraph , Euler , AGL , PGL and TensorFlow-GNN are industry GNN frameworks. They are built for distributedtraining and inference on large heterogeneous graphs. AliGraph,Euler and TensorFlow-GNN adds GNN-related functionalities toTensorFlow, such as graph data structure, message passing opera-tions, mini-batch sampling on graph data. AGL is built on top ofMapReduce for distributed training and inference of GNN. PGL is aGNN framework built on top of Paddle-Paddle . One of the keydesign choices is how to perform mini-batch sampling for GNNtraining. TensorFlow-GNN and AGL choose the option of prepro-cessing the input graph to generate mini-batch graphs and savethem to disks, while AliGraph, Euler and PGL choose on-the-flysampling. The main benefit of on-the-fly sampling is that it allowsusers to easily change some hyperparameters of GNN, such as thenumber of GNN layers and fanout, to tune model performance.GraphStorm is an industry GML framework built on top of DGLand provides high-level GML functionalities, such as end-to-endtraining/inference pipelines and advanced GML techniques. Tosupport fast prototyping, GraphStorm adopts on-the-fly sampling.CogDL is a high-level GML library that provides many built-in GML methods and benchmark datasets. CogDL is built for re-searchers to reproduce model performance and set up standardbenchmarks for comparing the performance of different models.GraphStorm is built for scientists to apply GML to industry applica-tions. Therefore, GraphStorm does not contain built-in benchmarkdatasets and instead, provides users an easy way to load tabular datainto GraphStorm for model training and inference. GraphStorm ismuch more scalable than CogDL because it intends to address thescalability challenges in the industry.",
  "DESIGN": "GraphStorm is designed as a general framework to accelerate GMLadoption in the industry. It provides functionalities to help everystep in the journey of developing and deploying GML models in theindustry, including graph construction, model prototyping, modeltuning and model deployment. Typically, graph modeling startsby defining the right graph schema. GraphStorm accepts data intabular format and provides a tool to construct a graph based onthe graph schema defined by users. After the graph construction,a user applies GML methods to the data to train an initial model.For model development, GraphStorm provides end-to-end pipelinesto train a model with a single command line. To improve modelperformance, GraphStorm provides many built-in techniques totackle common problems in industry graph applications; for moreadvanced users, GraphStorm provides custom model APIs for trying",
  ": The functionalities of GraphStorm. The colored lines show examples of constructing a complete model solution inGraphStorm": "brand new modeling ideas. Once the developed model meets thedeployment criteria, GraphStorm allows users to deploy the modelwithout any code modification on the ML service platforms, suchas Amazon SageMaker.To cover a large number of industry applications, GraphStormprovides modularized implementations to support diverse graphdata and applications. shows the abstractions requiredfor model training, including datasets, models, data loaders, tasks,training algorithms and evaluations. GraphStorm currently sup-ports four types of graph data: homogeneous graphs, heterogeneousgraphs, temporal graphs and textual graphs. For different types ofgraph data, GraphStorm provides different modeling techniques,such as RGCN and HGT for heterogeneous graphs andTGAT for temporal graphs. GraphStorm provides task-specificdata loaders: node data loaders for node-level tasks, edge data load-ers for edge-level tasks and graph data loaders for graph-level tasks.GraphStorm explicitly provides separate data loaders for predict-ing an attribute of an edge (EdgeDataLoader) and for predictingthe existence of an edge (LinkPredictionDataLoader) for the sakeof efficiency because LinkPredictionDataLoader not only samplespositive edges from a graph but also adopts many different methodsto construct negative edges for efficient training (see .3.4).GraphStorm supports different training strategies, such as single-task training, multi-task training, LM-GNN co-training and Expec-tationmaximization (EM) training method. GraphStorm currentlysupports seven graph tasks and provides corresponding evaluationmetrics for model evaluation. By combining different components,we can have a complete solution for a graph application. For exam-ple, as illustrated in , if we need to perform a link predictiontask on a heterogeneous graph with rich text features, we can takeRGAT as a graph model, use LinkPredictionDataLoader as the data",
  "User interface": "GraphStorm is designed to lower the bar of developing GML modelsfor beginner users as well as supporting advanced GML scientiststo experiment with new methods on industry-scale graph data. Tosupport these diverse requirements, GraphStorm provides two typesof interfaces. The command line interface allows users to quicklyprototype a GML model on their application data and allows themto improve model performance with builtin GML techniques inGraphStorm. The programming interface allows advanced users todevelop their custom models to further improve model performancefor their particular applications. 3.2.1Command line interface. GraphStorm provides the command-line interface for graph construction, model training, performancetuning and model inference. For graph construction, GraphStormprovides the ._ module that takes the ap-plication data stored in tabular format (e.g., CSV and Parquet) andconstructs a graph with the GraphStorm format for training andinference based on the graph schema defined by users. For model de-velopment, GraphStorm provides a module for each graph task. Forexample, GraphStorm provides .__ to trainand run inference of node classification models. GraphStorm allowsusers to configure model training and inference to enable manybuilt-in techniques (shown in .3) by using the commandarguments. Appendix B shows examples of GraphStorm commandsused for graph construction, model training and inference. 3.2.2Programming interface. GraphStorm provides the program-ming API to help advanced users implement their custom modelsto further improve model performance. GraphStorm provides task-specific trainers, predictors, evaluators as well as model templates. shows a GraphStorm training script to train an RGCNnode classification model for illustration. At a minimum, a useronly needs 8 lines of code in the training script to train and evaluate",
  "GraphStorm: all-in-one graph machine learning framework for industry applicationsKDD 24, August 2529, 2024, Barcelona, Spain": "the public and private benchmarks. By using GraphStorm, ML scien-tists can quickly prototype GML models and use built-in techniquesto improve model performance and outperform production mod-els. In addition to using the built-in techniques to improve modelperformance, users can also develop their own custom models inGraphStorm and rely on GraphStorm to scale their model train-ing to industry-scale graphs in a cluster of machines. Currently,GraphStorm has been used to develop and deploy GML models forover a dozen industry applications. We believe that the capabilityprovided by GraphStorm can also help GML researchers in theresearch community to experiment new modeling techniques onlarge complex graph data.",
  "(billions of edges). How to effectively and efficiently train aGML model with link prediction on a billion-scale graph?": "GraphStorm provides built-in techniques to address these model-ing issues to improve model performance. GraphStorm only adoptsmethods that can scale to billion-scale graphs and also ensures thattheir implementations are scalable. 3.3.1Joint modeling text and graph data. Many industry graphdata have rich text features on nodes and edges. For example, theAmazon search graph has rich text features on queries (keywords)and products (product descriptions) . This requires jointly mod-eling graph and text data together. By default, GraphStorm runslanguage models (LM), such as BERT, on the text features and runsa GNN model on the graph structure in a cascading manner.GraphStorm implements some efficient methods to train LMmodels and GNN models for downstream tasks . For example,Ioannidis et al. devises a three-stage training method that firstperforms graph-aware fine-tuning of the language model, trainthe GNN model with the fixed language model and follow withthe end-to-end fine-tuning; GLEM trains the LM model andthe GNN model iteratively on the downstream application. Theoriginal GELM paper was designed for homogeneous graphs andGraphStorm extends it to heterogeneous graphs. 3.3.2Efficient training large embedding tables for featureless nodes.In an industry graph, some node types do not have any node fea-tures. For example, when constructing the Microsoft academicgraph (MAG), it is hard to construct node features on author nodes;as a result, we typically do not have any features on the authornodes when constructing the graph. When training a GNN modelon the graph, GraphStorm by default adds learnable embeddingson author nodes, which results in a massive learnable embeddingtable. There are two problems of using learnable embeddings forfeatureless nodes. First, the learnable embedding table is usuallymassive because a graph can have many featureless nodes (e.g.,there are 200 million authors in the MAG graph). Secondly, addinglearnable embeddings significantly increases the model size, whicheasily causes overfitting. GraphStorm provides additional optionsto handle featureless nodes.GraphStorm provides methods that construct node features ofthe featureless nodes with the neighbors that have features.",
  "= (, )(1)": "With the constructed node features, a user can use a normal GNNmodel to generate GNN embeddings. We can have different choicesfor . We can use non-learnable function (e.g., average) or learnablefunctions (e.g., Transformer) to construct node features.GraphStorm also provides a two-stage training method to trainlearnable embeddings and GNN models, similar to the multi-stagetraining of LM+GNN. In the first stage, we use link prediction totrain the learnable embeddings. In the second stage, we fix thelearnable embeddings as node features and train a GNN model fordownstream tasks.",
  "Datasets": "We evaluate GraphStorm with two large heterogeneous graphs:Microsoft academic graph and Amazon review graph (shown in ). We construct two tasks on the benchmarkdatasets. For node classification, we predict the paper venue on theMAG data and predict the brand of a product on the Amazon reviewdataset; for link prediction, we predict what papers a paper citeson the MAG dataset and predict which products are co-purchasedwith a given product on the Amazon review dataset. More detailsof the benchmark datasets can be found in the Appendix.",
  "GML on Industrial-Scale Graphs": "We benchmark two main methods (pre-trained BERT+GNN andfine-tuned BERT+GNN) in GraphStorm on the two large datasets.For pre-trained BERT+GNN, we first use the pre-trained BERTmodel to compute BERT embeddings from node text features andtrain a GNN model for prediction. The alternative solution is tofine-tune the BERT models on the graph data before using the fine-tuned BERT model to compute BERT embeddings and train GNNmodels for prediction. We adopt different ways to fine-tune theBERT models for different tasks. For node classification, we fine-tune the BERT model on the training set with the node classificationtasks; for link prediction, we fine-tune the BERT model with thelink prediction tasks. shows the overall model performance of the two methodsand the overall computation time of the whole pipeline startingfrom data processing and graph construction. In this experiment, weuse 8 r5.24xlarge instances for data processing and use 4 g5.48xlargeinstances for model training and inference. Overall, GraphStormenables efficient graph construction and model training on largetext-rich graphs with hundreds of millions of nodes. We can see thatfor most cases, GraphStorm can finish all steps, even the expensiveBERT computations, within a few hours. The only exception isfine-tuning BERT model with link prediction and training a GNNlink prediction model on the MAG dataset, which take 2-3 days.The reason is that the training set of the dataset for link predictionhas billions of edges and we fine-tune BERT and train GNN modelon the entire training set. For pre-trained BERT+GNN, it takes 3.5hours to compute BERT embeddings on 240 million paper nodesin MAG and takes 8 hours on 240 million nodes in the Amazonreview data. To improve model performance, we fine-tune the BERTmodel on the training set of the downstream tasks, which leadsto a significant performance boost. Its performance improvementis also significant. For example, fine-tuning BERT on MAG canimprove BERT+GNN by 11% for node classification and by 40% forlink prediction.",
  "B419 min88 min81.5 min10B831 min1641 min168 min100B1661 min32416 min3250 min": "We also benchmark GraphStorm on large synthetic graphs tofurther illustrate its scalability. We generate three synthetic graphswith one billion, 10 billion and 100 billion edges respectively. Theaverage degree of each graph is 100 and the node feature dimen-sion is 64. We train a GCN model for node classification on eachgraph. In this experiment, we use r5.24xlarges instances for datapre-processing, graph partition and model training. For graph par-tition, we use a random partition instead of METIS partition. Formodel training, we take 80% of nodes as training nodes and runthe training for 10 epochs. shows the computation time ofgraph pre-processing, graph partition and model training. Overall,GraphStorm enables graph construction and model training on 100billion scale graphs within hours and shows good scalability. Fordata pre-processing, when increasing the graph size from 1 billionto 100 billion edges, the overall computation cost (measured byinstance-minutes) only increases by 13. For model training, whenincreasing both the graph size and training set by 100X, the overalltraining cost only increases by 133. For graph partition, whenincreasing the graph size by 100, the overall computation costonly increases by 208.",
  "Modeling graph data from datapreprocessing": "GML leverages the inductive bias inherent in graph structures,making the graph schema crucial for the successful application ofGML. GraphStorm offers an efficient graph construction pipelineto power scientists to experiment GML modeling starting from thegraph construction stage. We use the Amazon Review graph to il-lustrate the importance of graph schema. We gradually increase the",
  "HomogenousitemNoMRR:0.937Acc:0.640Heterogenous-v1+reviewNoMRR:0.944Acc:0.742Heterogenous-v2+customercustomerMRR:0.960Acc:0.725": "heterogeneity of the graph schema and observe the correspondingvariations in model performance. As illustrated in , addingreview nodes and the (item, receives, review) edge enhances per-formance in both co-purchase prediction and brand classificationtasks. Moreover, incorporating featureless customer nodes and the(customer, writes, review) edge further improves co-purchase pre-diction performance, but not node classification. This improvementis likely because items reviewed by the same customer have a higherlikelihood of being purchased together, although this factor doesnot significantly influence the determination of an items brand.This indicates the importance of defining the right graph schemato improve performance and that users need to try out differentgraph schemas to determine the best schema for a given task.",
  "Performance of GraphStorm techniques": "Besides the aforementioned GNN and LM+GNN models, Graph-Storm supports many techniques to improve model performanceon industry graph data. In this section, we evaluate some of thetechniques described in .3 on our benchmark datasets. 4.4.1Joint text and graph modeling. We benchmark GraphStormscapability of jointly modeling text and graph data. We use differentmethods to train BERT and GNN on the full MAG data to illustratethe effectiveness of different methods (). Here we com-pare four different methods: fine-tune BERT to predict the venue,train GNN with BERT embeddings from pre-trained BERT model(pre-trained BERT+GNN), train GNN with BERT embeddings gen-erated from a BERT model fine-tuned with link prediction (FTLPBERT+GNN), train GNN with BERT embeddings generated from aBERT model fine-tuned with venue prediction (FTNC BERT+GNN).First, BERT+GNN is much more effective in predicting paper venues",
  "DistilBERT with GNN distillation (128 dim)44.53%": "than BERT alone by up to 54%. It also shows the importance ofBERT fine-tuning in the prediction task. Even though fine-tuningBERT with link prediction can improve the prediction accuracy(by up to 7.6% over pre-trained BERT+GNN), the best accuracy isachieved by first fine-tuning the BERT model with venue predic-tion and training GNN with the fine-tuned BERT model with venueprediction (by 17.6%).",
  ": Jointly modeling text and graph data on MicrosoftAcademic Graphs": "4.4.2GNN distillation. We evaluate the GNN distillation capabilityin GraphStorm to improve the performance of the BERT-like lan-guage models. In this experiment, we train a GNN model to predictvenues of papers in MAG and distill it to a language model; wecompare this distilled model with a language model as the baselinethat is directly fine-tuned to predict venues. We choose pre-trained70MM HuggingFace DistilBERT for the BERT model architec-ture and use its initial weights for both the baseline and the GNNdistilled model. For the baseline, we fine-tune the DistilBERT usingthe venue labels. For GNN distilled model, we conduct the distil-lation to minimize the distance between the embeddings from aGNN teacher model and the embeddings from a DistilBERT studentmodel. MSE loss is used to supervise the training. After the baselineDistilBERT and GNN-distilled DistilBERT are trained, we use themto generate embeddings on paper nodes separately. We evaluatethe performance of the generated embeddings by training MLP de-coders for embeddings from baseline DistlBERT and GNN-distilledDistilBERT separately. As shown in , the GNN-distilled Distil-BERT embeddings outperform the baseline DistilBERT embeddingsby 8.2%, demonstrating the structured knowledge from the GNNteacher model are transferred to the GNN-distilled DistilBERT.",
  "Loss funcNeg-Sampleepoch time#epochsMetric": "contrastivein-batch1340.90s5MRR:0.951contrastivejoint-10241344.65s8MRR:0.956contrastivejoint-321286.64s8MRR:0.958contrastivejoint-41289.9s10MRR:0.956contrastiveuniform-321726.19s8MRR: 0.957contrastiveuniform-1024OOM cross-entropyin-batch1343.94s20MRR: 0.250cross-entropyjoint-10241330.50s18MRR:0.334cross-entropyjoint-321290.72s15MRR:0.380cross-entropyjoint-41288.53s16MRR:0.645cross-entropyuniform-321746.68s20MRR:0.377cross-entropyuniform-1024OOM conduct link prediction on the (item, also-buy, item) edges. Wecompare the model performance and training time with differentloss functions and negative sampling settings. The loss functionsinclude contrastive loss and cross entropy loss. The negative sam-pling methods include in-batch negative sampling, joint negativesampling with the number of negatives of 1024, 32 and 4 (denoted asjoint-1024, joint-32 and joint-4, respectively), and uniform negativesampling with the number of negatives of 1024 and 32 (denoted asuniform-1024 and uniform-32, respectively). We set the local batchsize to 1024 and the maximum training epochs to 20. shows the result. Overall, the performance of contrastiveloss is much better than cross entropy loss. Contrastive loss is morerobust to the variance of the number of negative edges. Cross en-tropy loss works much better when the number of negatives issmall, e.g., 4, but its performance becomes lower when the num-ber of negatives is larger. Furthermore, contrastive loss convergesmuch faster than cross entropy loss. In general, uniform negativesampling is more computationally expensive and consumes moreGPU memory than the other two sampling methods, as it samplesmany more negative nodes. For example, with batch size of 1024,both in-batch and joint-32 samples 1024 nodes to construct negativeedges, while uniform-32 has to sample 32,768 nodes. So the epochtime of uniform sampling is larger than the other two.",
  "DISCUSSION AND CONCLUSIONS": "GraphStorm is a general no-code/low-code GML framework de-signed for industry applications. It provides end-to-end pipelinesfor graph construction, model training and inference for many dif-ferent graph tasks and scales to industry graphs with billions ofnodes efficiently. This helps scientists develop GML models startingfrom graph schema definition and GML model prototyping on largeindustry-scale graphs without writing code. Based on our expe-rience working with scientists in the industry, this significantlyreduces the overhead of developing a new GML model and tuningits model performance for an industry application. GraphStormprovides many advanced GML modeling techniques to handle prob-lems commonly encountered in industry applications and we havefully verified the effectiveness and scalability of the techniques on",
  "Euler github. September 2020. Pgl github. September 2020. D. Busbridge, D. Sherburn, P. Cavallo, and N. Y. Hammerla. Relational graphattention networks. 2019": "Y. Cen, Z. Hou, Y. Wang, Q. Chen, Y. Luo, Z. Yu, H. Zhang, X. Yao, A. Zeng, S. Guo,Y. Dong, Y. Yang, P. Zhang, G. Dai, Y. Wang, C. Zhou, H. Yang, and J. Tang. Cogdl:A comprehensive library for graph deep learning. In Proceedings of the ACMWeb Conference 2023 (WWW23), 2023.",
  "da Xu, chuanwei ruan, evren korpeoglu, sushant kumar, and kannan achan. In-ductive representation learning on temporal graphs. In International Conferenceon Learning Representations (ICLR), 2020": "O. Ferludin, A. Eigenwillig, M. Blais, D. Zelle, J. Pfeifer, A. Sanchez-Gonzalez,W. L. S. Li, S. Abu-El-Haija, P. Battaglia, N. Bulut, J. Halcrow, F. M. G. de Almeida,P. Gonnet, L. Jiang, P. Kothari, S. Lattanzi, A. Linhares, B. Mayer, V. Mirrokni,J. Palowitch, M. Paradkar, J. She, A. Tsitsulin, K. Villela, L. Wang, D. Wong, andB. Perozzi. TF-GNN: graph neural networks in tensorflow. CoRR, abs/2207.03522,2023.",
  "T. N. Kipf and M. Welling. Semi-supervised classification with graph convolu-tional networks. arXiv preprint arXiv:1609.02907, 2016": "A. Lerer, L. Wu, J. Shen, T. Lacroix, L. Wehrstedt, A. Bose, and A. Peysakhovich.PyTorch-BigGraph: A Large-scale Graph Embedding System. In Proceedings ofthe 2nd SysML Conference, Palo Alto, CA, USA, 2019. J. McAuley, C. Targett, Q. Shi, and A. Van Den Hengel. Image-based recommen-dations on styles and substitutes. In Proceedings of the 38th international ACMSIGIR conference on research and development in information retrieval, pages4352, 2015. B. Rozemberczki, P. Scherer, Y. He, G. Panagopoulos, A. Riedel, M. Astefanoaei,O. Kiss, F. Beres, G. Lopez, N. Collignon, and R. Sarkar. PyTorch Geometric Tem-poral: Spatiotemporal Signal Processing with Neural Machine Learning Models.In Proceedings of the 30th ACM International Conference on Information andKnowledge Management, page 45644573, 2021.",
  "P. Velikovi, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Bengio. Graphattention networks. arXiv preprint arXiv:1710.10903, 2017": "M. Wang, D. Zheng, Z. Ye, Q. Gan, M. Li, X. Song, J. Zhou, C. Ma, L. Yu, Y. Gai,T. Xiao, T. He, G. Karypis, J. Li, and Z. Zhang. Deep graph library: A graph-centric, highly-performant package for graph neural networks. arXiv preprintarXiv:1909.01315, 2019. H. Xie, D. Zheng, J. Ma, H. Zhang, V. N. Ioannidis, X. Song, Q. Ping, S. Wang,C. Yang, Y. Xu, B. Zeng, and T. Chilimbi. Graph-aware language model pre-training on a large graph corpus can help multiple graph applications.In",
  "Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery andData Mining, KDD 23, page 52705281, New York, NY, USA, 2023. Associationfor Computing Machinery": "B. Yang, S. W.-t. Yih, X. He, J. Gao, and L. Deng. Embedding entities and re-lations for learning and inference in knowledge bases. In Proceedings of theInternational Conference on Learning Representations (ICLR) 2015, 2015. M. Zaharia, M. Chowdhury, M. J. Franklin, S. Shenker, and I. Stoica. Spark: Clustercomputing with working sets. In 2nd USENIX Workshop on Hot Topics in CloudComputing (HotCloud 10), Boston, MA, June 2010. USENIX Association. D. Zhang, X. Huang, Z. Liu, J. Zhou, Z. Hu, X. Song, Z. Ge, L. Wang, Z. Zhang,and Y. Qi. Agl: A scalable system for industrial-purpose graph machine learning.Proceedings of the VLDB Endowment, 13(12).",
  "where is the training graph": "Contrastive loss: The contrastive loss compels the repre-sentations of connected nodes to be similar, while forcingthe representations of disconnected nodes remain dissimi-lar. In the implementation, we use the score computed bythe score function to represent the distance between nodes.When computing the loss, we group one positive edge withthe negative edges corresponding to it. The loss functionis as following:",
  "A.2.1Negative sampling methods. GraphStorm provides four neg-ative sampling methods:": "Uniform negative sampling: Given training edges un-der edge type (_,_,_) and the number of nega-tives set to , uniform negative sampling randomly samples nodes from _ for each training edge. It corrupts thetraining edge to form negative edges by replacing its des-tination node with sampled negative nodes. In total, it willsample negative nodes. Joint negative sampling: Given training edges underedge type (_,_,_) and the number of negativesset to , joint negative sampling randomly samples nodesfrom _ for every training edges. For these trainingedges, it corrupts each edge to form negative edges byreplacing its destination node with the same set of negativenodes. In total, it only needs to sample negative nodes.(We suppose is dividable by for simplicity.) Local joint negative sampling: Local joint negative sam-pling samples negative edges in the same way as joint nega-tive sampling except that all the negative nodes are sampledfrom the local graph partition. In-batch negative sampling: In-batch negative samplingcreates negative edges by exchanging destination nodesbetween training edges. For example, suppose there arethree training edges (1, 1), (2, 2), (3, 3), In-batch neg-ative sampling will create two negative edges (1, 2) and(1, 3) for (1, 1), two negative edges (2, 1) and (2, 3)for (2, 2) and two negative edges (3, 1) and (3, 2) for(3, 3). If the batch size is smaller than the number of nega-tives, either of the above three negative sampling methodscan be used to sample extra negative edges."
}