{
  "ABSTRACT": "We address the task of probabilistic anomaly attribution in the black-box regression setting, where the goal is to compute the probabilitydistribution of the attribution score of each input variable, given anobserved anomaly. The training dataset is assumed to be unavailable.This task differs from the standard XAI (explainable AI) scenario,since we wish to explain the anomalous deviation from a black-boxprediction rather than the black-box model itself.We begin by showing that mainstream model-agnostic expla-nation methods, such as the Shapley values, are not suitable forthis task because of their deviation-agnostic property. We thenpropose a novel framework for probabilistic anomaly attributionthat allows us to not only compute attribution scores as the predic-tive mean but also quantify the uncertainty of those scores. Thisis done by considering a generative process for perturbations thatcounter-factually bring the observed anomalous observation backto normalcy. We introduce a variational Bayes algorithm for deriv-ing the distributions of per variable attribution scores. To the bestof our knowledge, this is the first probabilistic anomaly attributionframework that is free from being deviation-agnostic.",
  "(, )": "increment local gradientdeviation (b) deviation vs. increment : Problem setting and motivation. (a) Given a black-box deterministic regression model and anomalous sample(s),our goal is to find the probability distribution of input vari-ables responsibility scores without access to the trainingdata. (b) Existing attribution methods attempt to explain ei-ther the local gradient or the increment from a referencepoint 0, rather than the deviation of the sample in question. transparency in advanced machine learning (ML) algorithms, mak-ing explainable artificial intelligence (XAI) an active research areain the data mining community. While early XAI studies tendedto focus on the psychological aspects of how AI should be madeexplainable, the bulk of research interest is now shifting towardsactionability in business and industrial applications, as the adoptionof AI is becoming more widespread .One important problem in this context is how to explain anunusual event, observed as a significant discrepancy from the pre-diction of an ML model. Although this problem encompasses vari-ous different scenarios, we are particularly interested in the taskof anomaly attribution in the doubly black-box regressionsetting (see (a)): We are given a black-box regression model = (), where is the real-valued noisy output (such as milesper gallon) and is a vector of noisy real-valued input variables(such as drivers weight and average speed). We have access to theAPI (application programming interface) of () but do not haveaccess to either its parametric form or training data (hence, dou-bly). Given a limited amount of test samples, we ask: how can wequantify the contribution of each input variable in the face of anunexpected deviation between observation and prediction?This question has typically been addressed with one of the fol-lowing three model-agnostic post-hoc XAI methods in the literature:1) Local linear surrogate modeling, which is best known under thename LIME (Local Interpretable Model-agnostic Explanations) ;2) Shapley value (SV), which was first introduced to the ML com-munity by ; and 3) integrated gradient (IG) .",
  "KDD 23, August 610, 2023, Long Beach, CA, USATsuyoshi Id and Naoki Abe": "Despite their popularity, however, there are two major limi-tations with those methods. One is that all of them are, in fact,deviation-agnostic, meaning that they explain the black-boxfunction () itself in the form of the local gradient or an incre-ment, not the observed deviation, as illustrated in (b). Here,note that, unlike the standard XAI scenarios, we seek explanationsrelative to the deviation from a black-box prediction, as we willdiscuss in detail later. The other limitation is that they have lim-ited capabilities of quantifying the uncertainty of the attributionscores. Motivated by the requirements from industrial applications,e.g., , uncertainty quantification (UQ) of attribution scores isbecoming a major topic in XAI research. In the black-box settingwithout access to the training data, however, this problem is consid-ered extremely challenging and limited work has been done to date.Existing works include empirical comparative studies, e.g., ,and semi-theoretical analysis based on known results of probabilis-tic linear regression .In this paper, we propose a novel probabilistic framework calledthe generative perturbation analysis (GPA) for anomaly attri-bution in the black-box regression setting, which we believe is thefirst fully probabilistic black-box attribution algorithm. The keyidea is to consider a counterfactual data generative process includ-ing perturbation as a model parameter, and reduce the task ofattribution to that of statistical parameter estimation. In this way,the uncertainty in attribution is naturally evaluated by finding itsposterior distribution. Here we additionally introduce a novel ideaof using variational Bayes inference to decompose the contributionof each of the input variables.To summarize, our contributions are: 1) to mathematically showthat the existing attribution methods have the deviation-agnosticproperty; 2) to uncover their interrelationship that has been hithertounnoticed; and 3) to propose the first generative framework foranomaly attribution.",
  "RELATED WORK": "Anomaly attribution has been studied as a sub-task of anomalydetection in the ML community, typically in the white-box unsu-pervised setting. In the supervised setting, the majority of priorworks are about either model- or classification-specific algorithms.For example, saliency maps and layer-wise relevance prop-agation are well-known model-specific attribution methods.Sainyam et al. leveraged a counterfactual framework forprobabilistic black-box explanations in the classification settingwith binary variables. Similar approaches have been discussed un-der the terms like perturbation-based or mask-based (e.g. ),but most of them are for classification without the capability ofcomputing the distribution of attribution score and are not directlyapplicable to the present setting.In the model-agnostic regression setting, 1) local linear modeling,2) SV, and 3) IG have been widely used for black-box attribution, assummarized in , along with three additional methods: Theexpected integrated gradient (EIG) , which is a generalized ver-sion of IG, the -score, which is a standard outlier detection metricin the unsupervised setting, and likelihood compensation (LC) ,which conducts a semi-probabilistic analysis for attribution. In thecontext of anomaly attribution, LIME and its variant have been applied to anomaly explanation . SV is used in sensor faultdiagnosis and for explaining unexpected observations in cropyield analysis and unusual warranty claims . Also, the useof IG for anomaly explanation is discussed by Sipple .Interestingly, it has been suggested that these attribution meth-ods may have some mutual connection. Prior work along this lineincludes Deng et al. , which attempted to characterize IG usingTaylor expansion and gave the first definition of EIG. Also, Sun-dararajan and Najmi proposed a unified attribution framework,where they pointed out that there can be a few different definitionsfor SV and discussed the relationship with IG in a qualitative man-ner. Lundberg and Lee reintroduced the SV-based attributionmethod to propose a hybrid method between SV and LIME.Inspired by these works, we go one step further in this paper: Weexplicitly show a mathematical relationship between those existingattribution methods, and show that the deviation-agnostic property(see the -sensitive column in ) is an inherent consequenceof the common mathematical structure.Another important contribution of this paper is the proposal ofa principled framework for probabilistic prediction of attributionscores. Most of the existing works tackling this problem under settings similar to ours use the standard result of probabilisticlinear regression (see, e.g., Chap. 3 of ) to evaluate uncertaintyin the regression coefficients as the LIME attribution score (thebuilt-in UQ column in ). However, the black-box model () is generally highly nonlinear; It is not clear to what extentthe theoretical results of the linear model apply. Also, it is not clearhow the distribution of the attribution score is computed for eachinput variable (hence yes/no in the table). In fact, BayLIME sposterior covariance is a constant that depends only on the hyper-parameters independently of () (See Sec. 6.3). LC shares asimilar starting point with ours but differs fundamentally in that itis not able to compute the probability distribution of the attributionscore. Guo et al. used a Dirichlet-enhanced probabilistic linearregression mixture but it is intended for global model explanationsrather than local anomaly attribution.",
  "PROBLEM SETTING": "As mentioned earlier, we focus on the task of anomaly attribution inthe regression setting rather than classification or unsupervised set-tings. (a) summarizes the overall problem setting. Supposewe have a (deterministic) regression model = () in the doublyblack-box setting: Neither the training data set Dtrain nor the (true)distribution of is available (see the training-data-free column in). Throughout the paper, the input variable R and theoutput variable R are assumed to be noisy real-valued, where is the dimensionality of the input vector. We also assume thatqueries to get the response () can be performed cheaply at any .In practice, anomaly attribution is typically coupled with anom-aly detection: When we observe a test sample (,) = (,),we first compute an anomaly score = (,) to quantify howanomalous it is. Then, if R is high enough, we go to the nextstep of anomaly attribution. In this scenario, the task of anomalyattribution is defined as follows.",
  "compute the distribution of the score for each input variable indicativeof the extent to which that variable is responsible for the sample beinganomalous": "We can readily generalize the problem to that of collective proba-bilistic anomaly detection and attribution. Specifically, given a testdata set Dtest = {(,) | = 1, . . . , test}, where is the index forthe -th test sample and test is the number of test samples, we canconsider anomaly score as well as attribution score distributionsfor the whole test set Dtest.The standard approach to anomaly detection is to use thenegative log-likelihood of the test sample(s) as the anomaly score(See, e.g., ). Assume that, from the deterministicregression model, we can somehow obtain ( | ), a probabilitydensity over given the input signal . Under the i.i.d. assumption,the anomaly score can be written as",
  "Notation. We use boldface to denote vectors. The -th dimensionof a vector is denoted as . The 1 and 2 norms of a vectorare denoted by 1 and 2, respectively, and are defined as": "1 | | and 2 2 . The sign function sign() isdefined as being 1 for > 0, and 1 for < 0. For = 0, thefunction takes an indeterminate value in . For a vector input,the definition applies element-wise, yielding a vector of the samesize as the input vector. We distinguish between a random variableand its realizations via the absence or presence of a superscript. Fornotational simplicity, we use () or () as a proxy to representdifferent probability distributions, whenever there is no confusion.For instance, () is used to represent the probability density ofa random variable while ( | ) is a different distribution ofanother random variable conditioned on .",
  "EXISTING ATTRIBUTION METHODS AREDEVIATION-AGNOSTIC": "This section summarizes our remarkable new results on the existingattribution methods: 1) IG, SV, and LIME are inherently deviation-agnostic and are not appropriate for anomaly attribution, 2) SV isequivalent to EIG up to the second order in the power expansion,and 3) LIME can be derived as the derivative of IG or EIG in a certainlimit. Throughout this subsection, we assume that the derivative of",
  "Deviation-agnostic properties": "4.1.1LIME. In general, the local linear surrogate modeling ap-proach fits a linear regression model locally to explain a black-box function in the vicinity of a given test sample (,). Foranomaly attribution, we need to consider the deviation function (,) () instead of (). Algorithm 1 summarizes thelocal anomaly attribution procedure. Let denote the -th outputby LIME (,). Rather unexpectedly, despite the modification tofit (,) rather than (), the following property holds:",
  "=1( ( []) ( + 0) [])2 + 1,": "which is equivalent to the lasso objective for LIME with the intercept + 0. Since the lasso objective is convex, the solution is unique.With an arbitrary adjusted intercept, the attribution score remainsunchanged. Hence, , LIME (,) = LIME (). In the local linear surrogate modeling approach, the final attribu-tion score can vary depending on the nature of the regularizationterm. For the theoretical analysis below, we use a generic algorithmby setting 0+ in Algorithm 1, and call the resulting attributionscore LIME0 for = 1, . . . , . As is well-known, LIME0 is a localestimator of / at = if is locally differentiable.",
  "S:|S |= (S),(6)": "where S denotes any subset of the variable indices {1, . . . , }excluding . |S | is the size of S. The second summation runsover all possible choices of S under the constraint |S | = fromthe first summation. We also define the complement S, which isthe subset of {1, . . . , } excluding and S. For example, if =12, = 3 and S = {1, 2}, the complement S will be {4, 5, . . . , 12}.Corresponding to this division, we rearrange the variables as =(, S, S ). Finally, the (S) term is defined as the differencebetween the expected values of under two different conditions:One is (, S,) = ( , S,) with S to be integrated out. The",
  "Relationship between IG, SV, and LIME": "The fact that (E)IG, SV, and LIME share the same deviation-agnosticproperty suggests that they may share a common mathematicalstructure. In what follows, we show two results showing the inter-relationship between them. 4.2.1SV and EIG. First, let us consider the relationship betweenSV and EIG. The integral in IG and the combinatorial definitionSV are major obstacles in getting deeper insights into what theyreally represent. This issue can be partially resolved by resortingto power expansion. The following remarkable property holds:",
  "where d () ()": "We leave the proof to our companion paper due to spacelimitations. While the sum rule b) is known, Theorem 4 a) is thefirst result directly establishing the fact that , SV EIG, to thebest of our knowledge. In Sec. 6, we empirically show that indeedSV and EIG systematically give similar attribution scores. 4.2.2LIME and EIG. Second, let us now consider the relationshipbetween LIME and EIG. LIME, as a local linear surrogate modelingapproach, differs from EIG and SV in two regards. First, LIMEdoes not need the true distribution (). Instead, it uses a localdistribution to populate local samples. Second, LIME is definedas the gradient, not a differential increment. These observationslead us to an interesting question: Is the derivative of EIG in thelocal limit the same as the LIME attribution score? The followingtheorem answers this question affirmatively:",
  "where the localized Gaussian (0) = N ( | ,I) is used in thedefinition of EIG": "We leave the proof to our companion paper . Since EIG, SV,and LIME can be derived from or associated with IG, it is legitimateto say that they are in the integrated gradient (IG) family. SinceIG is deviation-agnostic, we conclude that the deviation-agnosticproperty is a common characteristic of the IG family. 4.2.3Increment vs. deviation and local vs. global. Now let us con-sider the implications of these results in anomaly attribution. Thedefinition of IG in Eq. (3) indicates that IG explains the increment of () from the baseline point rather than the deviation, as illustratedin . The baseline is arbitrary. Hence, the increment is not di-rectly relevant to the observed anomaly in general. EIG (and thus SVby Theorem 4) neutralizes this limitation by taking the expectation.However, it results in losing the locality of explanation because itattempts to explain the increment from any point in the domain,as suggested in regarding SV. They are unsuitable for anomaly",
  "Generative Perturbation Analysis for Probabilistic Black-Box Anomaly AttributionKDD 23, August 610, 2023, Long Beach, CA, USA": "attribution due to both their deviation-agnostic property and thelack of locality. LIME, on the other hand, maintains the localityby choosing the baseline input in the infinitesimal neighborhood,i.e., 0 , but it is still deviation-agnostic.In general, we need a certain reference point to define anoma-lousness (cf. the reference point column in ). The aboveobservations motivate us to explore a new idea in choosing a ref-erence point. In this regard, the likelihood-based approach firstproposed by the present authors is quite suggestive. Inspiredby , we propose a novel generative framework for anomalyattribution, where the notion of normalcy is equated to maximuma posteriori (MAP) estimation, as presented in the next section.",
  "Generative model description": "In a typical anomaly detection scenario, samples in the trainingdataset are assumed to have been collected under normal conditions,and hence, the learned function = () represents normalcy aswell. As discussed in Sec. 3, the canonical measure of anomalousnessis the negative log likelihood ln( | ). A low likelihood valuesignifies anomaly, and vice versa. From a geometric perspective,on the other hand, being an anomaly implies deviating from acertain normal value. We are interested in integrating these twoperspectives. 5.1.1Perturbation as explanation. Suppose we just observed a testsample (,) being anomalous because of a low likelihood value.Given the regression function = (), there are two possiblegeometric interpretations on the anomalousness (see Figs. 1 (b)and 2 (a)). One is to start with the input = , and observe thedeviation () . In some sense, (,) = (, ()) is a refer-ence point against which the observed sample (,) is judged.The other is to start with the output = , and move horizontally,looking for a perturbation such that = + gives the maxi-mum possible fit to the normal model. In this case, the referencepoint is ( + ,) and is the deviation measured horizontally.Since is supposed to be zero if the sample is perfectly normal,each component 1, . . . , can be viewed as a value indicative ofthe responsibility of each input variable. 5.1.2Generative model. Based on the intuition above, we define anovel probabilistic attribution approach through a data-generatingprocess of observed data. The idea is that we write down a gen-erative process for the observable variables (,) as a parametricmodel of . Then, the whole task of anomaly attribution is reducedto a parameter estimation problem, given an observed test point(,) = (,). Specifically, the probabilistic regression model( | ) is now viewed as a parametric model ( | , ) by setting to + . With an extra parameter representing the precision ofthe regression function and also prior distributions for and , we",
  "(0) 01 exp(0),(11)": "where N ( | , ) and Gam( | , ) denote the Gaussian and gammadistributions, respectively, and ,0,0 are hyperparameters. Asmentioned above, plays the role of a model parameter here. No-tice that Eq. (9) naturally represents the horizontal point-seekingmentioned above. If we point-estimated with Eq. (9) alone, wewould have the one that achieves ( + ) . The challengehere is how to find the distribution of . The prior distribution ()in Eq. (10) introduces potential variability of to the model. Since = 0 represents the normal state, the use of zero-mean Gaussianmakes sense. Other zero-mean distributions may work. In fact, wemodify this prior a bit later, as discussed in Sec. 5.3.The precision parameter in Eq. (9) describes potential noisethat may have contaminated the data, as illustrated as the greyband in . As the preciseness of the measurements may varyfrom sample to sample, the use of a single value can be risky.The prior () takes care of this aspect. As will be seen later, ourmodel uses a mixture of Gaussians with different values of insome sense, which leads to the -distribution instead of Gaussian forthe observation model, adding extra capability of handling heavynoise.Finally, we make two remarks about the proposed generativemodel. First, Bayesian (linear) regression models similar to theabove have been considered in the literature, e.g., . Our modelis fundamentally different from them in that (1) as an explanationis not linear regression coefficients, and (2) we do not approximate () as a liner function. As for other Bayesian regression approaches,Moreira et al. used the Gaussian process in the active learningsetting, but not for attribution. Second, one might wonder whetherthe particular choice of a parametric form might lead to the loss ofgenerality. Regarding this question, it is critical to understand thatEq. (9) is about the deviation or the error () . Although thevariability of over the entire domain obviously does not followGaussian in general, the error is often well-represented by Gaussianor -distribution. This is exactly the same situation Carl FriedrichGauss faced when he invented Gaussian-based fitting : Planetarymotions do not follow Gaussian, but the error does.",
  "Inference approach": "Given the generative model above, the task of probabilistic attri-bution is now turned to that of finding the posterior distributionof . However, there are two major differences from the standardBayesian inference: 1) () is a black-box function. Exact infer-ence is not possible. Approximating () with a specific functionalform, such as the linear function, may not always be possible, ei-ther. 2) Posterior inference generally yields a joint distribution for, denoted by (). However, this is not what we want since itdoes not directly explain the contribution of the individual inputvariables. This section explains how we addressed these challenges.",
  "() = (1, . . . ,) =1 (),(12)": "so that end-users can directly use () to get insights on thecontribution of the -th input variable (see ). The factorizedform (12) is reminiscent of what is assumed in the variational Bayes(VB) algorithm , and we can be guided by VBs general solutionapproach. Specifically, we find the unknown distributions {} byminimizing the KL (KullbackLeibler) divergence between and . The key fact here is that is proportional to the completelikelihood by Bayes rule. Since is an unobserved intermediateparameter, it can be marginalized. The integration can be performedanalytically, yielding the following form of the likelihood:",
  "d (),(15)": "where the first term is the KL divergence and the second term isto include the normalization condition with being Lagrangesmultiplier. Note that the proportional coefficient in Eq. (14) has noeffect here so we do not have to determine it.By the calculus of variations w.r.t. , it is straightforward to getthe minimizer as",
  ": return { () | = 1, . . . , } and": "where c. is a symbol representing an unimportant constant in gen-eral. Since both and ( ) are unknown, this procedure isiterative in nature. Also, since s are a functional of the black-boxfunction (), analytically performing the integration is not possi-ble. Although Monte Carlo techniques can be used in theory, theyare not a preferable choice in realistic usage scenarios, where theend-users actively interact with the attribution tool with differenttest points.",
  "Computing attribution score distribution": "Here, we propose a practical solution to address these challenges.For attribution purposes, we do not necessarily need the posteriordistribution over the entire domain. What we are interested in ishow attribution score is distributed around the most probable value.Hence, we evaluate the expectation in Eq. (16) through the empiricaldistribution of ( ) with a sample at the maximum posteriori(MAP) point. In this approach, the variable-wise posterior is givensimply by",
  "() (1, . . . ,1,,+1, . . . ,),(17)": "where is the MAP solution arg max ln(). Since thisis a one-dimensional (1D) distribution and we know distributesaround zero, the normalization constant can be determined easily.Numerical integration is one approach. Otherwise, one may treat () as a discrete distribution on a 1D grid. Specifically, we define1D grid points , . . . , [g] over [max,max], where g is anarbitrary number of grid points, such as 100, and max can be, forexample, max 1.1 max | |. The distribution () on the gridis obtained from its unnormalized version () by",
  ".(22)": "We call the proposed probabilistic attribution framework the gen-erative perturbation analysis (GPA) hereafter. As illustrated in, the GPA distribution {} is useful to have in order to evalu-ate the general informativeness of the input variables. 5.3.1Solving MAP problem. One of the standard solution approachesto the optimization problem of the type Eq. (19) is proximal gradientdescent , although the unavailability of closed-form expressionof the gradient of () makes the procedure a bit complicated. If anumerical estimation method for ( + ) is available, Eq. (19)can be reduced to an iterative lasso regression problem:",
  "where we defined (). Upon convergence, we set = . See lines 2-11 in Algorithm 2": "5.3.2Algorithm summary. Algorithm 2 summarizes the entire al-gorithm of GPA. Whenever possible, it is recommended to stan-dardize somehow so that it distributes around zero with unitvariance for each variable. For standardized data, the 2 strength can be a value of O(1), such as 0.1, which can also be a reason-able starting point for . The 1 strength should be in the range0 < 1. We fixed = 0.5 in our experiment. As 20 has theinterpretation of degrees of freedom, one reasonable starting pointis 20 test + 1. As described in Appendix D, () can be chosenas a constant 0 02 /, where 2 is an estimate of the vari-ance of (), or the maximizer of the marginalized likelihood,and is the number of virtual samples, which can be O(10). Assummarized in , we used = 1 or 10, and also 20 = 11 inour experiments to simulate the variability of realistic cases.It is easy to see that the complexity of the algorithm is(test)per iteration round. Note that test = 1 is the most common choice(i.e., local explanation) and the algorithm does not use any trainingdata. Hence, typical scalability analysis about the data set size is",
  "EXPERIMENTS": "This section presents empirical evaluation of the proposed anom-aly attribution framework1. The goals of this evaluation are to 1)provide a clear picture of what deviation-sensitivity of an attri-bution method buys us; 2) demonstrate GPAs unique capabilityof providing the probability distribution of attribution scores; 3)quantitatively analyze the consistency and inconsistency amongdifferent attribution methods.",
  "Datasets and baselines": "Based on the datasets summarized in , we compared GPAwith seven baselines: Six non-distributional attribution methods,LIME (Sec. 4.1.1), (E)IG (Sec. 4.1.2), SV (Sec. 4.1.3), LC , and the -score (e.g., ), as well as one distributional method, BayLIME .For anomaly attribution, LIME, SV, IG, and EIG are applied to thedeviation () rather than (). The -score is a standardunivariate outlier detection metric in the unsupervised setting, andis defined as ( )/ for the -th variable, where , are the mean and the standard deviation of , respectively. In SV,we used the same sampling scheme as that proposed in withthe number of configurations limited to 100. In IG and EIG, we usedthe trapezoidal rule with 100 equally-spaced intervals to performthe integration w.r.t. . For IG, EIG, LC, and GPA, we used the samegradient estimation algorithm described in Appendix C.To compute SV, EIG, and the -score, we used the empiricaldistribution of the training data to approximate (). Note that thisis actually not possible to do in our doubly black-box setting. We areincluding SV, EIG, and the -score here for comparison purposesnonetheless.",
  "() = 2 cos(1) cos(2).(25)": "One remarkable feature of this model is that it is possible to calculateclosed-form attribution scores. See Appendix A for the details.Suppose we have a test point at = (1/2, 0) with three different values, A ( = 1), B ( = 1), and C ( = 0), as illustrated in. In this case, SV and LIME attribution scores are (0, 0) and(2, 0), respectively. IGs scores are (2, 0) and (2/3, 8/3) if wechoose 0 = (0, 0) and (0, 1), respectively. These do not dependon due to the deviation-agnostic property, in contrast to GPA,which gives 1 = (1/) arccos /2 1 and 2 = 0. visualizes the attribution scores with what we call thelitmus plot, where larger values get darker colors (0 gets white)and negative/positive values get blue/red colors. Due to space limi-tations, we omitted LC, which results in the same solution as thatof GPA, and the -score. For GPA, the scores are normalized bydividing by max | | for each test point. Similar normalizationwas done for the baselines with the convention 0 0 = 0.In this example, A and B are outliers due to a shift in the 1direction, while C is normal in terms of deviation. Hence, an idealattribution would be that 1 alone gets a strong signal only for Aand B. GPA precisely reproduces this, but all the baselines do not:They gave the same score for A, B, and C, as a consequence of thedeviation-agnostic property. The figure also shows that IGs scoressensitively depend on the choice of 0, making IG trickier to usefor the end-users. SV always satisfies SV1 = SV2 regardless of inthis case, and does not provide any clue for input attribution. Thisis a manifestation of the loss of locality in SV discussed in Sec. 4.2.3.",
  ": Diabetes: Comparison of normalized attributionscores in the litmus plot for the top outlier detected": "variable (progression) and = 10 predictors including the body-mass index (bmi). For this dataset, we held out 20% of the samplesand trained a deep neural network (DNN) on the rest as the black-box model (). We identified the top outlier using Eq. (1), which ishighlighted in Fig 5. compares attribution scores for the top outlier. We set0 = 0 for IG. All the attribution methods identify bmi and s5as the top contributors. For both, GPA and LC get a large negativescore since a smaller value is more typical for such a low value,as shown in the scatter plot in . GPA gave bmi = 0.81 ands5 = 0.55. Note that these values have actual meaning ratherthan just the magnitude of responsibility: A big negative in bmimeans that the BMI is too high for such a low level. In otherwords, they would have looked normal if they were a little skinnier.Explainability of this kind is particularly useful in practice as thescore provides actionable insights about how the status quo couldbe changed for the better. The alternative methods do not havesuch ability. LIME is positive for bmi because the slope is positiveat the , regardless of the value. Similarly, IG, EIG, and SV gavepositive values for bmi because () is higher than the mean of ,regardless of the specific value of .",
  "Distribution analysis": "6.3.1Comparison with BayLIME. Now let us discuss how GPAprovides useful insights into the uncertainty of the attributionscore. To the best of our knowledge, BayLIME is the onlymethod in the literature applicable to our setting.We used the BostonHousing data , where the task is to predict, the median home price (MEDV) of the districts in Boston, with, the input vector of size = 13 including such features as thepercentage of the lower status of the population (LSTAT) and theaverage number of rooms (RM)2. For this dataset, we held out 20%of the samples as Dtest and trained the random forest (RF) onthe rest as the black-box model (). We identified the top outlierusing Eq. (1) and computed the score distribution for the top outlier.The estimated distributions are shown in . As clearly seenfrom the figure, BayLIME gives the same curve for all the variablesapart from the mean locations. In fact, the variance is given as a",
  ": Estimated score distribution. Left: BostonHousing.Right: CaliforniaHousing for collective attribution": "constant 1/( + s), where s is the number of virtual samplesgenerated for estimating the regression coefficients and is 10 in ourcase (See Appendix B). In contrast, GPA provides variable-specificdistributions. As illustrated in , less informative variables tendto produce a flatter distribution in GPA. In this case, we immediatelysee that RM and LSTAT are two dominating variables. Such aninsight is not obtainable from BayLIME. It is interesting to see thatthe score distributions given by GPA tend to be piece-wise constant,reflecting the fact that RF is a collection of decision stumps. 6.3.2Collective attribution. To show the unique capability of GPAfor collective attribution, we used the CaliforniaHousing dataset. The task is to predict the median house value of smallgeographical segments using predictor variables such the longitudeand latitude. We held out 20% of the samples and trained gradientboosted trees (GBT) on the rest. In this case, we identifiedthree top outliers as shown in . The question is whether thoseoutliers have common characteristics in their outlier-ness. shows the computed distributions, where we omittedBayLIME due to its triviality. Very interestingly, Latitude has avery sharp peak at a negative value. This indicates that the veryhigh MedHouseVal in stands out in that latitude and theywould look more common if they existed in a southern location.",
  "Consistency analysis": "We have compared GPA with seven alternative methods in a ratherqualitative fashion so far. One important question in practice ishow those methods are consistent or inconsistent among them over-all. To answer this question, we identified five top outliers in thethree real-world datasets (BostonHousing, CaliforniaHousing,Diabetes), and computed how their attribution scores are con-sistent with those of GPA in terms of four metrics: Kendalls ,Spearmans , the sign match ratio (SMR), and the hit ratio at 25%(h25). See Appendix E for the detail.The result is summarized in . We omitted EIG because ofTheorem 4. LC achieves very high consistency with GPA, althoughit lacks a built-in mechanism for UQ. This is understandable since itcan be viewed as a point-estimation version of GPA in some sense.",
  "0.060.31 0.190.72 0.080.58 0.100.15 0.150.98 0.030.41 0.210.88 0.040.75 0.100.22 0.20SMR1.00 0.000.62 0.110.38 0.080.62 0.220.60 0.10h251.00 0.000.60 0.220.90 0.220.80 0.270.30 0.45": "As expected, h25 generally has high scores, apart from the -score.This suggests that those attribution methods are a useful tool forselecting important features. Even in the other metrics, includingthe SMR, they produce reasonably consistent attributions in somecases. However, in some 20-30% of cases they are not necessarilyconsistent, which is a natural consequence of the fact that GPA isdeviation-sensitive but the others are not.",
  "Practical utility of the GPA framework": "We remark further on the practical utility of the proposed frame-work, using the BostonHousing data as an example. Recall that thetop detected outlier has two variables with dominating attributionscores. Depending on ones role, different insights may be obtainedfrom this analysis: From the end users perspective, the outlier in may point to a bargain since this house (district) has unusuallymore rooms and much fewer low-income neighbors than expectedfor the price range; For a modeler who is interested in debuggingthe model, the two dominating attribution scores may hint thatthe model may be failing to capture the relationship between thehousing price and the variables RM and LSTAT, prompting the mod-eler to revise (e.g. contextualize) how these variables are defined.While the attribution scores may not decisively pinpoint the exactinterpretation, the rich and accurate information given by GPAprovides valuable clues in either usage scenario.",
  "CONCLUSIONS": "We have proposed GPA, a novel generative approach to probabilisticattribution of black-box regression models. The key idea is to reducethe attribution task to a statistical parameter estimation problem.This can be done by viewing the perturbation as a model pa-rameter of the generative process for the observed variables (,),where the posterior distribution gives the distribution of the at-tribution score. We proposed a variational inference algorithm toobtain variable-wise distributions.We have also shown that the existing input attribution meth-ods, namely integrated gradient IG), local linear surrogate mod-eling (LIME), and Shapley values (SV), are inherently deviation-agnostic and, thus, are not designed to be a viable solution foranomaly attribution. Unlike these methods, GPA is capable of pro-viding directly interpretable insights in a deviation-sensitive anduncertainty-aware manner.",
  "Jerome H. Friedman. 2002. Stochastic gradient boosting. Computational statistics& data analysis 38, 4 (2002), 367378": "Sainyam Galhotra, Romila Pradhan, and Babak Salimi. 2021. Explaining black-boxalgorithms using probabilistic contrastive counterfactuals. In Proceedings of the2021 International Conference on Management of Data (SIGMOD 21). 577590. Damien Garreau and Ulrike von Luxburg. 2020. Explaining the Explainer: A FirstTheoretical Analysis of LIME. In The 23rd International Conference on ArtificialIntelligence and Statistics (AISTATS 20) (Proceedings of Machine Learning Research,Vol. 108). PMLR, 12871296. Ioana Giurgiu and Anika Schumann. 2019. Additive Explanations for AnomaliesDetected from Multivariate Temporal Data. In Proceedings of the 28th ACM Inter-national Conference on Information and Knowledge Management (CIKM 19). ACM,22452248.",
  "Tsuyoshi Id and Naoki Abe. 2023. Black-Box Anomaly Attribution. arXivpreprint arXiv:2305.18440 (2023)": "Tsuyoshi Id, Amit Dhurandhar, JiNavrtil, Moninder Singh, and Naoki Abe.2021. Anomaly Attribution with Likelihood Compensation. In Proceedings of theAAAI Conference on Artificial Intelligence (AAAI 21), Vol. 35. 41314138. I. Elizabeth Kumar, Suresh Venkatasubramanian, Carlos Scheidegger, and SorelleFriedler. 2020. Problems with Shapley-value-based explanations as feature im-portance measures. In International Conference on Machine Learning (ICML 20).PMLR, 54915500. Markus Langer, Daniel Oster, Timo Speith, Holger Hermanns, Lena Kstner,Eva Schmidt, Andreas Sesing, and Kevin Baum. 2021. What do we want fromExplainable Artificial Intelligence (XAI)? A stakeholder perspective on XAI anda conceptual model guiding interdisciplinary XAI research. Artificial Intelligence296 (2021), 103473.",
  "Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2013. Deep insideconvolutional networks: Visualising image classification models and saliencymaps. arXiv preprint arXiv:1312.6034 (2013)": "John Sipple. 2020. Interpretable, Multidimensional, Multimodal Anomaly Detec-tion with Negative Sampling for Detection of Device Failure. In Proceedings ofthe 37th International Conference on Machine Learning (ICML 20). 90169025. John Sipple and Abdou Youssef. 2022. A general-purpose method for applying Ex-plainable AI for Anomaly Detection. In Proceeding of the International Symposiumon Methodologies for Intelligent Systems (ISMIS 22). Springer, 162174. Dylan Slack, Anna Hilgard, Sameer Singh, and Himabindu Lakkaraju. 2021.Reliable post hoc explanations: Modeling uncertainty in explainability. Advancesin Neural Information Processing Systems (NeurIPS 21) 34 (2021), 93919404.",
  "Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic attributionfor deep networks. In Proceedings of the 34th International Conference on MachineLearning (ICML 17). 33193328": "Kenji Yamanishi, Jun ichi Takeuchi, Graham Williams, and Peter Milne. 2000.On-line Unsupervised Outlier Detection Using Finite Mixtures with DiscountingLearning Algorithms. In Proceedings of the Sixth ACM SIGKDD InternationalConference on Knowledge Discovery and Data Mining (KDD 00). 320324. Kenji Yamanishi, Jun-Ichi Takeuchi, Graham Williams, and Peter Milne. 2004.On-line unsupervised outlier detection using finite mixtures with discountinglearning algorithms. Data Mining and Knowledge Discovery 8, 3 (2004), 275300. Xiao Zhang, Manish Marwah, I-ta Lee, Martin Arlitt, Dan Goldwasser, et al. 2019.ACEAn Anomaly Contribution Explainer for Cyber-Security Applications. InProceedings of the 2019 IEEE International Conference on Big Data (Big Data 19).IEEE, 19912000. Yujia Zhang, Kuangyan Song, Yiming Sun, Sarah Tan, and Madeleine Udell. 2019.Why Should You Trust My Explanation? Understanding Uncertainty in LIMEExplanations. arXiv preprint arXiv:1904.12991 (2019). Xingyu Zhao, Wei Huang, Xiaowei Huang, Valentin Robu, and David Flynn.2021. BayLIME: Bayesian local interpretable model-agnostic explanations. InProceeding of the 37th Conference on Uncertainty in Artificial Intelligence (UAI 21).PMLR, 887896. Yilun Zhou, Serena Booth, Marco Tulio Ribeiro, and Julie Shah. 2022. Do featureattribution methods correctly attribute features?. In Proceedings of the 36th AAAIConference on Artificial Intelligence (AAAI 22). 96239633.",
  "CESTIMATING THE GRADIENT OFBLACK-BOX FUNCTION": "To find the MAP solution for GPA, we need to numerically estimatethe gradient of the black-box function (). To handle the potentialnon-differentiability of , we define the gradient as the local meanof the slope function [ ( +) ()]/, where +, is a small random perturbation, and is a unit vector which takes 1in the -th entry and 0 otherwise. The local mean can be estimatedby numerically evaluating",
  ",(C.11)": "where () is a local distribution for around . One reasonablechoice is () = N ( | 0,21) with 1 being the standard deviationof the perturbations. For numerical stability, we used 1 = 1 in ourexperiments, where the input variables have been standardized. Thenumber of Monte Carlo samples was set to 10, which was confirmedto provide sufficient convergence in our experiments.",
  "DPARAMETER TUNING APPROACH": "GPAs distribution can be used for verifying whether the computedMAP value has reached a satisfactory local maximum. In our exper-iments, we started with a default set of parameters: = 0.1/test, = 0.1test, and = 10. If any of the GPA distributions lookedinconsistent with the MAP value, we gradually decreased downto 1 and increased up to 1. We kept fixed at 0.5, which turnedout to achieve a sparsity level comparable to that of LIME.We discuss how to initialize 0,0 in the gamma prior below.",
  "= ( + 1)/2.(D.12)": "Here, denotes the sample size and can be equated to test. Wehave added 1 so 0 = 1 when test = 1. Otherwise, it can beinterpreted as the virtual sample size, which can be a measure of theconfidence level. Since UQ generally boils down to an ill-posed taskthat estimates uncertainty somehow when many samples are notavailable, can be viewed as a controllable parameter to simulatewhat would be seen when there were abundant samples. In such acase, could be a value like 1 10.",
  "Recall that the derived-distribution has the scale parameter": "0/0.As the scale parameter corresponds to the standard deviation, wesee that the above relationship 0/0 2 is consistent with it.Equation (D.15) can be also used as a constant approximation for(). However, for evaluating the probability density function of ,it tends to give a bit too large value. This is understandable becauseif, e.g., test = 1, a majority of the probability mass is from the prior,giving a dull peak around zero. To reproduce a realistic distribution,we need to simulate the situation where there are a reasonablenumber of test samples. This can be done by choosing a smaller0/0 because the precision (the reciprocal of the variance) linearlyincreases as a function of the sample size in Bayesian estimation.Hence, when estimating the distribution in GPA, we can include acorrection factor as",
  "ECOMPARING ATTRIBUTION SCORES": "We computed the following four metrics to evaluate the consis-tency among different attribution methods. The first and secondmetrics are Kendalls and Spearmans , calculated for two abso-lute attribution score vectors. They take a value of 1 if the ordersare the same regardless of their values. The third metric is whatwe call the sign match ratio (SMR), which takes on 1 when all thesigns are consistent between corresponding vector elements. Whencomparing an attribution score vector against a reference scorevector , SMR is defined as",
  "=1I (sign() sign() = 1) ,(E.17)": "where I() is the indicator function that takes on 1 when the argu-ment is true, 0 otherwise. We define sign(0) = 0 in this case. Notethat this favors sparse attribution scores: If = 0, then the score isalways 1 regardless of . Finally, the fourth metric is what we callhit25, which gives 1 when the top 25% of the absolute entries per-fectly match between and , and 0 if none of the top 25% membersof is included in that of . As hit25 depends on neither the signnor the rank, it quantifies simply the match of top contributors."
}