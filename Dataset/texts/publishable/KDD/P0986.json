{
  "ABSTRACT": "Expander decompositions of graphs have significantly advancedthe understanding of many classical graph problems and led tonumerous fundamental theoretical results. However, their adoptionin practice has been hindered due to their inherent intricacies andlarge hidden factors in their asymptotic running times. Here, weintroduce the first practically efficient algorithm for computingexpander decompositions and their hierarchies and demonstrate itseffectiveness and utility by incorporating it as the core componentin a novel solver for the normalized cut graph clustering objective.Our extensive experiments on a variety of large graphs showthat our expander-based algorithm outperforms state-of-the-artsolvers for normalized cut with respect to solution quality by alarge margin on a variety of graph classes such as citation, e-mail,and social networks or web graphs while remaining competitive inrunning time.",
  "This work is licensed under a Creative Commons Attribution 4.0 International License": "algorithms for such diverse problems as, e.g., routing , connec-tivity , (approximate) maximum flow , or triangle enu-meration . Informally, an expander is a well-connected graph.Its expansion or conductance = minborder() min(vol(),vol( )) is theminimum ratio between the number of edges leaving any subset ofvertices and the number of edges incident to vertices of the subset.A large value of (close to 1) indicates a graph in which no subsetof vertices can be easily disconnected from the rest of the graph.An expander decomposition of a graph partitions the verticessuch that the subgraph induced by each part is an expander andthe number of edges between different components of the parti-tion is low.1 Many results over the years havedemonstrated that such a decomposition not only exists for everygraph, but can be computed in near-linear time. At a high level,the success of expander decomposition-based algorithms is dueto the fact that many problems are easy on expanders: One firstidentifies regions of a graph where a problem is easy to solve (theexpanders), solves the problem (or a suitable sub-problem) withineach region, and then combines the individual solutions to obtain asolution for the overall problem.Expander hierarchies apply expander decompositions in ahierarchical manner. This is done by computing an expander decom-position of the graph and then contracting the individual expandersinto single vertices repeatedly. This process is repeated until theentire graph has been contracted to a single vertex. This recursivedecomposition has a corresponding decomposition tree (the so-called (flow) sparsifier) where the root corresponds to the entiregraph, the leaves correspond to vertices of the original graph, andinner vertices correspond to the expanders found during the de-composition procedure. The sparsifier approximately preserves thecut-flow structure of the original graph in a rigorous sense. Thisrelationship has been exploited in numerous applications in staticand dynamic graph problems to obtain fast algorithmswith provable guarantees.",
  "Kathrin Hanauer, Monika Henzinger, Robin Mnk, Harald Rcke, and Maximilian Vtsch": "While many algorithms for expander decomposition that offerstrong asymptotic bounds on the running time have been sug-gested , 2 practical algorithms based on expanderdecomposition have not seen success thus far. This is due to thesealgorithms requiring one to solve many maximum flow problems,which means these algorithms are prohibitively slow in practice forgraphs with many edges. Arvestad , e.g., reports that decompos-ing a graph with approximately 105 edges can take almost 5 minfor various values of in their implementation of .In practice, the multilevel graph partitioning framework has beenthe de-facto approach for computing high quality solutions forcut problems on graphs. This framework consists of a coarseningstep, in which a smaller representation of the graph is computed, asolving step, where a solution is computed on the coarse graph anda refinement step, during which the solution is improved using localmethods during the graph uncoarsening process. Multilevel graphpartitioning has been successfully applied for computing balancedcuts and normalized cuts in graphs .We note that algorithms based on the expander hierarchy ap-proach may also be regarded as a variant of multilevel graph par-titioning. In this approach, a graph is first coarsened by recursivecontractions to obtain a smaller graph that reflects the same basiccluster structure as the input graph. Then an initial partition is com-puted on the sparsifier and, afterwards, in a series of refinementsteps the solution is mapped to the input graph while improving itlocally as we undo the coarsening. Despite the theoretical guaran-tees brought forward by expander decompositions and expanderhierarchies, the latter have not yet led to similar breakthroughs inthe field of fast, high-quality experimental algorithms. The reason isthat the expander decomposition required at each coarsening stepbecomes a significant performance bottleneck in practice. In con-trast, multilevel graph partitioners like METIS and KaHiP use a fast matching approach here.In this work we mitigate the computational bottleneck by intro-ducing a novel, practically efficient random-walk-based algorithmfor expander decomposition. Based on this, we give the first im-plementation of the expander hierarchy and, thus, an algorithmto compute tree flow sparsifiers, allowing us to solve various cutproblems on graphs effectively. Exemplarily, we show that ourapproach is eminently suitable to compute normalized -cuts ongraphs, where the goal is to partition the vertex set into clusterssuch that the sum over the number of edges leaving each cluster,normalized by the clusters volume, is minimized. Normalized cutis a popular graph clustering objective and particularly able to cap-ture imbalanced clusterings. Its manifold applications include, e.g.,community detection and mining , topic reconstruction ,story segmentation , bioinformatics , tumor localiza-tion , and image segmentation . It is closely related tospectral clustering, which uses spectral properties of eigenvaluesand eigenvectors of the graphs Laplacian. However, the spectralapproach suffers both from very large running times and memoryrequirements to compute and store the eigenvectors, and thus doesnot scale well . This problem was addressed by Dhillon et al. as well as Zhao et al. , who presented algorithms for normalized",
  "cut that either use spectral methods only after coarsening orapply them on a spectrally sparsified graph": "Contributions. We present the first practically efficient algorithmfor expander decomposition and the first implementation to com-pute an expander hierarchy. Our approach is based on randomwalks and is justified by rigorous theoretic and empirical analysis.We report on a comprehensive experimental study on normalizedcut solvers, comprising 50 medium-sized to very large graphs ofvarious types, where we compare our expander-based algorithm,XCut, to Graclus by Dhillon et al. , the approach by Zhao etal. , as well as the state-of-the-art graph partitioners METIS andKaHiP, which do not specifically optimize towards the normalizedcut objective, but are fast and in practice often used for this task.The experiments show that our algorithm produces superior nor-malized cuts on graph classes such as citation, e-mail, and socialnetworks, web graphs, and generally scale-free graphs, and is onlyslightly worse on others. On average, it is still distinctly the bestacross all graphs and values of .If only a single value of is desired, its running time is on av-erage only 3 times slower than the runner-up, Graclus, and neverexceeded 18 min. A notable advantage of XCut is that it can quicklycompute solutions for multiple values of once a sparsifier is com-puted, which can be faster than running Graclus multiple times.",
  "RELATED WORK": "Our work is motivated by recent theoretical results building on expander decompositions and the ex-pander hierarchy and inspired by non-spectral approaches to tackle the normalized cut problem.Mohar showed that the computation of the so-called isoperi-metric number or conductance of a graph (see section 3) is NP-hard,which implies the hardness of the normalized -cut problem for 2. Normalized cut remains NP-hard on weighted trees .A number of tools that have been used to solve normalizedcut use spectral methods . This usually requiresto compute eigenvectors of the Laplacian matrix, which wasshown to scale badly in practice . Afterwards, an additionaldiscretization step is necessary to obtain the clustering.Dhillon et al. therefore suggest an algorithm called Graclus,which is based on the multilevel graph partitioning framework. Itapplies the same coarsening steps as METIS, but with a modifiedmatching procedure. The coarsened graph can then be partitionedusing different approaches, including a spectral one. In the refine-ment step, Graclus uses a kernel -means-based local search algo-rithm for improving the normalized cut objective value. The authorsevaluate their algorithm experimentally against METIS as well as aspectral clustering algorithm . They show that it outperformsthe spectral method w.r.t. normalized cut value, running time, andmemory usage. It also produces better results than METIS and iscomparable w.r.t. running time.Zhao et al. employ a joint spectral sparsification and coars-ening scheme to produce a smaller representation of the graphthat preserves the eigenvectors of the Laplacian in near-linear time(). Afterwards, a normalized cut is computed on the sparsifierusing spectral clustering. Their sparsification scheme obtains a sig-nificant reduction in the number of edges and nodes, which makes",
  "Expander Hierarchies for Normalized Cuts on Graphs": "it feasible to apply the spectral method on the reduced graphs, with-out the quadratic term in the running time growing too large. Theauthors again perform an experimental comparison with METISand observe that their algorithm overall outperforms METIS w.r.t.the normalized cut value, while being slightly slower.METIS , KaHiP , and many other graph partitioningtools do not solve the normalized cut problem and insteadaim to find good solutions to a balanced graph partitioning problem,where the vertex set is to be partitioned into sets of (roughly)equal size, while minimizing the number of edges cut. CHACO implements a spectral graph partitioning approach, but the numberof clusters is limited to at most = 8.Nie et al. recently presented a spectral normalized cut solverbased on the coordinate descent method along with several speedupstrategies. They evaluate their algorithm against two other spectralmethods on a number of medium-sized data sets and showthat it consistently computes the best solution and is the fastest. Anotable difference is that is not an input parameter.",
  "PRELIMINARIES": "For an undirected graph = (, ) we use to denote the degreeof vertex , to denote the degree vector, i.e., the vector ofvertex degrees, and = to denote the corresponding degreematrix. is used to denote the maximum degree of a vertex in .For a subset of vertices we define the volume vol() as the sumof vertex degrees of vertices in . Its border border() (or capacity)is defined as |(, )|, where (,) = {{,} | , }is the set of edges between subsets , , and = \\ .A -cut is a partition P of the vertex set into non-empty parts.Given such a partition P = (1, . . . ,), its normalized cut value is defined as",
  "vol()": "Here the third step follows because /vol() as is thefraction of the flow of commodity that stays in (normalized by1/vol()). This means that at least 1/4 of the flow that starts in has to leave . At the same time an equivalent amount of flow has toenter , which means that the traffic across the edges in (, \\ )is at least vol()/2. As the congestion is only , there must be atleast 1",
  "EXPANDER DECOMPOSITION USINGRANDOM WALKS": "In this section we describe our random-walk-based approach for ob-taining expander decompositions and present its theoretical guaran-tees. The complete proofs The complete proofs for these guaranteescan be found in the appendix.A natural approach for computing expander decompositions isto find a low conductance cut to split the graph into two parts andthen to recurse on both sides. If no such cut exists, we have certifiedthe (sub-)graph to be an expander. In the end, the whole procedureterminates if each subgraph is an expander, i.e., it terminates withan expander decomposition. Saranurak and Wang used thisgeneral approach to obtain an expander decomposition that runsin time O( log4 /) and only cuts O( log3 ) edges. Whiletheir flow-based techniques give very good theoretical guarantees,the hidden constants do not seem to allow for good practical per-formance (see e.g. ).In this work we base the cut procedure of the expander decompo-sition on random walks. As a consequence, we can only guaranteethat our decomposition cuts at most O( ) edges4, since weare limited by the intrinsic Cheeger barrier of spectral methods.However, random walks have a very simple structure, which leadsto a simple algorithm with good practical performance. The weakerdependency on (",
  "(b) or vol( ) O(/log2 ) and A is a near 6-expander": "Given the cut procedure, an expander decomposition is computedas follows. On the current subgraph, execute the cut procedure, toeither find a low conductance cut or certify that none exists. If nosuch cut exists, then is a certified -expander and we terminate.Otherwise we check whether is sufficiently balanced, i.e., thevolume of the smaller side is at least (/log2 ). In that case we",
  "Definition 1 (Near -expander). Given a graph = (, ).A subset is a near -expander in if for all sets withvol() vol()/2 : |(, \\ )| vol()": "Note that if the LHS in the above equation were |(, \\ )|then {} would be a -expander.If { } (for returned by the cut-procedure) is indeed a -expander we can just recurse on the smaller side and wouldobtain a low recursion depth. Saranurak and Wang introduced atrimming procedure that, given a subset that is a near 6-expanderwith volume vol() 9 vol( )/10 and |(, )| vol()/10,computes a subset that is a proper -expander and hasvolume vol() 1 2 vol(). By applying this trimming step to we can return { } as a proper -expander and recurse on theremaining graph still with a small recursion depth. Overall, usingour cut procedure within this framework gives Theorem 1.",
  "Finding Low Conductance Cuts": "We now give a detailed description of the cut procedure that formsthe basis of Theorem 2. The goal is to either certify that is a-expander or to find a low conductance cut that is as balanced aspossible. The idea is to exploit that random walks converge quicklyon expanders and hence when they dont, we know there must bea low conductance cut. See Algorithm 1 for an outline.We employ a concurrent random walk, where each node dis-tributes its unique commodity in the graph. We are interested inthe probability that after t steps a particle that started say at node iis at some other node j. The walk has converged if this distributionis essentially identical for every starting vertex. If we quickly reachthis stationary distribution, there cannot be a low conductance cutand hence the graph must be an expander. Otherwise we can useinformation gathered from the walk to find a low conductance cut.Such a cut may however be very unbalanced. The proceduretherefore accumulates low conductance cuts until the combinedcut is a balanced low conductance cut or the graph that remainsdoes not have a low conductance cut anymore. Here we call a cut balanced if its balance () := 2/log2 .More precisely, the algorithm maintains a partition of intotwo sets , for each iteration with initial values := , := .We repeat the following for = 1/(12) steps: In iteration , wegenerate a new random unit vector and execute 1 steps of therandom walk, initialized according to . This walk yields a vector ,on which we analyze the conductance of all sweep cuts, i.e., cuts ofthe form := { : } for some value . Note that theseconductance values can be calculated in linear time after sortingthe entries of .We consider a cut to have low conductance if the value is belowthe threshold = O( log3/2 ). A lower threshold value wouldgive better guarantees in case (2) of Theorem 2 but at the same timeit would increase the number of rounds required to converge andhence worsen the guarantee of case (1) in Theorem 2. The valueis chosen to ensure we can guarantee () in case (1) of theTheorem.",
  "(2) If there exists a sweep cut with conductance < and balance we return this low conductance cut": "(3) Otherwise we consider the two-ended sweep cuts of theform , := { : } for values and find the one with largest volume among those with(,) < . If this cut has balance we return it, oth-erwise we move it from to and check whether hasbecome balanced. The random walk can be interpreted as the projection of a muchhigher dimensional random walk onto the randomly chosen direc-tion . This projection step is crucial for the implementation tobecome computationally feasible and we show that the projectionsapproximate the original structure sufficiently well.In order to argue the correctness of the cut procedure we haveto show that it is highly unlikely that the procedure does not find acut on a graph that has expansion less than . For this we arguethat after random walk steps (without finding a cut) the walk willhave converged to its stationary distribution w.h.p. Because of thechoice of parameters such a quick convergence is only possible if is a near 6-expander. Whenever the cut procedure returns a cut,it is guaranteed to have conductance at most . From this it followsthat the expander decomposition cuts at most O( log) edges.The entire argument can be found in Section A in the appendix.",
  "XCut A NEW NORMALIZED CUTALGORITHM": "In this section, we introduce the algorithm XCut, which is basedon the previous sections novel random walk-based expander de-composition. We note the apparent similarity between multilevelgraph partitioning and the expander hierarchy and use this as thebasis of XCut. As an outline, we use the novel random walks toconstruct the expander hierarchy to obtain a coarse representationof the graph, the tree flow sparsifier. We then compute an initialsolution on the tree. Finally, we use an iterative refinement stepwhile descending the hierarchy to improve the solution we found.Compared to other contraction schemes, which lead to each vertexin the coarsest graph representing roughly the same number ofnodes in the base graph, the subtrees on each level in the tree flowsparsifier can represent a vastly different number of vertices.Expander Decomposition. While the expander decomposition out-lined in section 4 is much simpler to implement than that of ,as we do not rely on maximum flow computations at all, we madeseveral choices in the implementation to speed up computation.We iterate a single random walk, and after each iteration, we checkwhether we can find a sparse cut. If we find a suitable cut, we dis-connect the edges going across it, but contrary to the algorithm insection 4, we do not restart the random walk, as we find we canextract further information about the cut structure of the graphfrom the state of the random walk. For example, if the random walkhas mixed very well on one of the new components, it is likely tobe an expander, while if there is another sparse cut in the compo-nent, then the random walk will likely not have mixed well on thecomponent. One may think that reducing the number of randomwalks might lead to a loss of guarantees and higher variance ofthe algorithm, but in experiments conducted while designing thealgorithm, we found that on real graph instances running multipleconcurrent random walks is not necessary. In fact, only a singlegraph in our 50 graph benchmark had noticeable variance. See alsothe discussion in subsection 6.3.The main parameter of the expander decomposition is the cutvalue , which is the minimal sparsity of the cuts our algorithmmakes. Additionally, we introduce a parameter , to be used asa threshold for certifying\" that a component is an expander, aswe found that choosing the threshold to be 1/(4 vol( )2) does notoffer any benefits over a much larger value. See also subsection 6.2for details on choosing the value for this parameter. We make afinal modification to the theoretical algorithm in that we omit thetrimming step on unbalanced cuts, since it does not provide anyfurther speedup of the expander decomposition routine in practice.Automatically Choosing . The theoretical analysis in sug-gests a choice for and that is sufficient to prove the theoreticalresults. In preliminary experiments we found this choice to be toopessimistic and, in fact, by adapting and to the graph we canobtain better results. However, there is a trade-off to be made: If is too small, the expander decomposition will not find many sparsecuts, and we obtain sparsifiers of low quality. On the other hand, if is too large, most cuts will be sparser than , which leads to manycuts being made and increases the running time. In the worst case,this can even prevent the algorithm from terminating. Thus, our goal is to choose such that it offers a good quality vs.time trade-off. When choosing , we can only observe whether thiswas a good choice in a post hoc fashion. A naive strategy wouldbe to start with a large and decrease it until the expander hier-archy terminates within a reasonable amount of time. However,this approach is wasteful, as we discard previously computed de-compositions, even if they were good. Instead, we decrease bymultiplying it with constant factor < 1 whenever the expander de-composition on a specific level cuts too many edges in . We thenuse the new = for the remaining expander decompositions,decreasing it further as required.Solving on the Sparsifier. To solve normalized -cut on the treesparsifier obtained from the hierarchy, we want to remove 1edges to decompose it into a forest of trees. Given a solution, i.e.,a tuple of edges (1, . . . ,1), we assign vertex to the cluster associated with if is the first edge we encounter on thepath from to the root. If no edge in the solution lies on the pathto the root, we assign to cluster . As normalized cut is NP-hardalso on trees (see section 2), we introduce two heuristic approachesthat take time O() each:Greedy: The simple greedy heuristic picks the edge in the sparsi-fier which minimizes the increase in the normalized cut objective inevery step. By simply computing the cost of cutting each remainingedge, it takes () time to find this edge, assuming the number ofvertices in the sparsifier is O(). To partition a graph into clus-ters, we repeat this process 1 times. For = 2, this algorithmproduces the optimal solution on the tree as we pick the edge thatminimizes the cut objective.Dynamic Programming: Each row of the dynamic program cor-responds to a level, and we make one cell for each pair (,) ofthe row, where is a vertex in the sparsifier and . Thevalue of each cell is the normalized cut value of decomposingthe subtree rooted at into parts. We write (,) for this value.Additionally, we write (,) for the weight of cut edges in thesolution incident to the subtree rooted at , as well as (,) forthe volume remaining in the subtree.Without loss of generality, assume the tree is binary, as otherwisewe can binarize the tree by inserting edges of infinite cost (seeHenzinger et al. for details). The value of a cell is computedaccording to the following rule:",
  "where cutParent(DP(, 1)) = DP(, 1) + (,)+cut(,1)": "vol(,1)is the best solution where the edge going to the parent is cut. Foreach vertex of 0, we initialize the bottom row of the programwith DP(, 0) = 0, DP(, 1) = 1 and DP(, ) = for . Inthe root vertex we use the special rule DP(, ) = min DP(,) +DP(, ) + cut(,)+cut(,) vol(,)+vol(,) , where the last term ensures wedo not produce a solution with vol(,) = 0, leading to cluster being empty.The Refinement Step. Finally, we perform an iterative refinementas we descend the hierarchy. While descending, we introduce newclusters according to the edges in the solution. On each level we thenperform vertex swaps that improve the normalized cut objective,",
  "CSNSCNEMFEOPBPRNIFUSDMCFSNCLWB": ": Relative improvement over Graclus (y-axis) vs. theaverage degree divided by the median degree (x-axis). Largerx-values signify that the graph exhibits a skewed, power-law-like degree distribution. Note the negative trend, except forthe outlier towards the right corresponding to instance SN7. the other solvers in the comparison due to it producing 530 timesgreater on the citation network (CN) instances.While it is difficult to draw good conclusions for the runningtimes from the values reported in as no source code is available,we note that on some instances XCut takes less than 10 % of thereported time while producing higher-quality solutions, whichmight be indicative of a running time advantage of our solver.",
  "EXPERIMENTAL EVALUATION": "In the previous sections, we have shown that our approach pro-vides provable guarantees on the approximation ratio for the value of the normalized cut5 (and its relatives, sparse cut, and low-conductance cut) if = 2. We now turn to evaluate XCut exper-imentally in different configurations. We compare the objectivevalue of normalized cuts produced by XCut and its running timeagainst the normalized cut solver Graclus by Dhillon et al. andthe state-of-the-art graph partitioning packages METIS andKaHiP (kaffpa) , all of which are available publicly. These algo-rithms are based on the multilevel graph partitioning frameworkand produce disjoint partitions of the vertices into clusters, where is a freely choosable parameter. We note that METIS and KaHiPsolve the balanced -partitioning problem rather than normalizedcut. Nevertheless, we include these solvers in our comparison, asthey are used for this task in practice and we found that they can out-perform Graclus on some of the graphs in our benchmark dataset.By this, we follow the methodology of and .In addition, we compare our results to the values reported for thenormalized cut algorithm by Zhao et al. 6. We omit comparisonsto solvers employing spectral methods such as the recent works byChen et al. and Nie et al. , as this approach does not scale",
  "See for the precise definition.6Unfortunately, the code is not available publicly and also could not be provided bythe authors upon request before the submission deadline": "well to large datasets of millions of nodes . In preliminary exper-iments we found that the solver of Nie et al. uses over 330GB ofmemory on instance CN3, whereas our solver used less than 400MBof memory. Furthermore, the algorithm presented in does notnecessarily produce clusters, thus making direct comparisonsdifficult. Lastly, the space complexity of spectral methods becomesprohibitive for larger values of , as noted by .",
  "Experimental Setup": "Instances. Our setup includes 50 graphs from various applica-tions. See for an overview and the full version of the paperfor details. To facilitate comparability, our collection contains theeight instances used by Dhillon et al. (BP1, CF1, CS13, DM1,OP1, and OP2) as well as the 21 instances used by Zhao et al. .In addition, we selected 21 real-world networks that cover variousapplication areas, including some of larger sizes. All instances areavailable publicly in the Network Repository or the SuiteSparseMatrix collection .Note that a graph with or more connected components alwayshas a (normalized) -cut of size 0. Surprisingly, we found that XCutis the only solver tested here that finds the trivial optimal solutionif is less than the number of connected components. However,testing connectivity before starting a solver remedies this problem,which is why we decided to exclude graphs with more than 128connected components except for one (graph ID 0), which we keepfor consistency reasons as it was used in previous comparisons . Methodology. As XCut, KaHiP, and METIS are randomized, weran each of them = 10 times per instance with different seeds.Graclus is deterministic, and we ran it three times to obtain astable value for the running time. We use the arithmetic meanover the runs for each instance to approximate the expectedvalue of and the running time. When reporting values, we writeXCutmean for the mean value across these 10 runs, and XCutminfor the minimum. As KaHiP and METIS behaved almost identicallyover all runs, we only report mean values for them. For eachalgorithm and each graph, we compute a partitioning consistingof {2, 4, 8, 16, 32, 64, 128} clusters as well as (smaller is better).As a second criterion, we compare the algorithms running times.All experiments were conducted on a server with an Intel Xeon16 Core Processor and 1.5 TB of RAM running Ubuntu 22.04 withLinux kernel 5.15. XCut is implemented in C++ and compiled usinggcc 11.4 with full optimization7. For all other solvers, we followedthe build instructions shipped with their code. As Graclus is single-threaded, we ran the single-threaded version of every algorithm.METIS was run in its default configuration. We used KaHiP withthe fsocial flag, which is tailored to quickly partitioning socialnetwork-like graphs8 and Graclus with options -l 20 -b to enablethe local search step and only consider boundary vertices duringlocal search, as suggested by the authors for larger graphs .",
  "Type (Abbreviation)#| |||": "Bipartite (BP)11.4M4.3M1.7kComputational Fluids (CF)117k1.4M269Clustering (CL)2 4.8k-100k 6.8k-500k3-17Citation Network (CN)9 226k-1.1M 814k-56M 238-1.1kCircuit Simulation (CS)35k-30k9.4k-54k31-573Duplicate Materials (DM)114k477k80Email Network (EM)233k-34k54k-181k 623-1383Finite Elements (FE)2 78k-100k 453k-662k39-125Infrastructure Network (IF)2 2.9k-49k6.5k-16k19-242Numerical Simulation (NS)2 11k-449k75k-3.3M28-37Optimization (OP)237k-62k131k-2.1M 54-8.4kRandom Graph (RD)114k919k293Road Network (RN)4 114k-6.7M 120k-7M6-12Social Network (SN)7 404k-4M 713k-28M 626-106kTriangle Mixture (TM)710k-77k54k-2M22-18kUS Census Redistricting (US) 1330k789k58Web Graph (WB)3 1.3k-1.9M 2.8k-4.5M59-2.6k Running Time [s] 1e-091e-081e-071e-061e-050.00010.0010.010.11.010.0100.0Graph IDTM6CN7",
  ": Running time vs. normalized cut for differentchoices of on two different graphs for = 16. Colors denotedifferent levels of , while shapes indicate the graph": "compare for both heuristics the running time and across tenprecomputed sparsifiers each for 19 representative graphs. Thequality returned by DP was never better than that of Greedy. Whilethe running times for both heuristics are linear in the size of thesparsifier, the DP approach scaled worse in . For example, for = 32, DP was three times slower than Greedy, and seven timesslower for = 128. See the full version of the paper for a plot withthe results for = 104. Greedy was faster and produced no worsequality than DP for all values of . Thus, we only report results forGreedy for all further experiments. Parameter Choice for the Expander Decomposition (, ). In Fig-ure 2, we examine the effect of the threshold parameter on thequality of solutions and running time. If the parameter is chosentoo large, especially greater than one, the entire graph will likelybe certified as an expander before any cuts are made, leading tovery large . Furthermore, we no longer obtain any significant k",
  ": Geometric mean of the cut value across all graphsfor each for XCutmean, Graclus, METIS, and KaHiP": "improvement in for values of below 104. At the same time,the running time increases inversely with as extra iterations areneeded to converge, which is shown by the vertical arrangementof the dots in . For graph instance TM6, we also find thatnon-Pareto-optimal choices of exist, namely 0.1 and 0.01, whereboth the running time and the returned value are worse than 103 and 104. This suggests that the choice of is an important designdecision. On all our instances, = 104 produced Pareto-optimalresults, so we conclude that we can configure this parameter to bea constant independently of the graphs structure.For the automatic tuning of , we chose a starting thresholdof 0.3, as this is around the largest value for which the expanderdecomposition finds structurally interesting cuts. Whenever istoo large, i.e., the node reduction |+1|/| | > 0.95, we multiply by a factor of = 0.8 and restart the expander decomposition.",
  "Comparison to Graclus, METIS, and KaHiP": "depicts the solution quality of every solver relative to thatproduced by Graclus on all instances for = 32, showing both themean and the minimum of the ten runs for XCut. For other values of, the overall picture remains the same, see and Appendix D.We group the graphs by type, with containing two plots, theupper showing the disconnected IMDB graph and email, citation,and social network graphs as well as infrastructure networks, asthese are the graphs on which XCut is particularly strong.Looking at absolute values of , we find that across all instances,the geometric mean is at least 70 % lower than our competitors,see . Interestingly, when we only consider the seven graphsfrom , the geometric means become 0.84 for XCutmin and 0.90for XCutmean, while they are 1.11, 1.19 and 1.03 for Graclus, METIS,and KaHiP respectively, which implies that KaHiP slightly outper-forms Graclus in terms of on their benchmark (but not XCut).One point of note is that XCut does not always find small nor-malized cuts on the social network SN7 representing user-userinteractions in the Foursquare social network. This appears to bedue to a very high-degree node that connects to approximately15 % of all vertices, leading to fast convergence of the random walk.Due to the averaging effect of such a vertex, many nodes havealmost identical values that vary only slightly between runs.Thus, when sorting by -value, their order can vary greatly depend-ing on the initial values, leading to very different candidate cuts.This is the only graph where the minimum and mean of the ten",
  "All0.250.220.891.00.94": "runs of XCut differ significantly (33 % for XCutmin and +39 % forXCutmean relative to Graclus). This indicates that the theoreticalalgorithm sketched in section 4, which uses multiple concurrentrandom walks (corresponding to more attempts to find a good cut),would likely have reduced the variance here. This is also the onlygraph where the fact that we only use a single random walk impairsthe quality of the result.The graph classes on which XCut does not perform as well havefairly homogeneous degree distributions and often appear grid-likewhen drawn. We conjecture that in these grid-like graphs, there areno good expanders (which XCut is trying to find). Instead, sparsecuts arise mainly from the fact that the cut is balanced, i.e., thecomponents we disconnect are all large enough, rather than beingsparsely interconnected, which is exploited by the other solvers.",
  "Comparison to Zhao et al": "In Table D.5 we compare for XCut and Graclus to the valuesreported by Zhao et al. for = 30. We observe a similar butweaker pattern as in the previous section. On graphs arising fromnumerical simulation and finite element problems, XCut performsworse than Zhao et al., but never by more than 36 %. On the tri-angle mixture instances, XCut achieves better on four instances,while their solver outperforms XCut on two instances. On citationnetworks, clustering instances, and those based on maps (RN1 andUS1), XCut outperforms the algorithm by Zhao et al., with beingup to 2.5 times lower on US1 and CN7. Altogether, XCut is betterthan Zhao et al. on roughly 2/3 of the instances, while Graclus doesbest on one instance. The geometric mean across all instances is1.46 for XCutmean, 1.39 for XCutmin, 1.64 for Zhao et al., and 3.06for Graclus. We note that ours is 11 % lower for XCutmean and 15 %lower for XCutmin, while Gracluss value is almost twice that of Average Degree / Median Degree 0.0 0.2 0.4 0.6 0.8 1.0 1.2 (% change vs Graclus)",
  "Running Time": "In our experiments, we found that on many graphs, the runningtime is spent mainly on computing the expander hierarchy, whereon some instances, this step accounts for over 80% of the runningtime, even for = 128. See Figure D.9 for some examples. How-ever, even on the largest instances in our benchmark, the absoluterunning time never exceeded 18 minutes.Overall, XCut is on average three times slower than Graclus,20.8 times slower than METIS and 6.7 times slower than KaHiP forthe mean execution time across all choices of and all instances.Interestingly, we find that XCuts running time is lower than Gra-cluss on several social network graphs and the triangle mixtureinstances while it produces a better solution. See the full version ofthe paper for detailed running times on some exemplary instances.Finally, recall that once our algorithm has computed a sparsifier,we can obtain a solution on the sparsifier for different values of without recomputing the sparsifier, which is a unique featureamong the solvers tested here. If we are interested in all sevenvalues of , e.g., and only count the time to compute the sparsifieronce for XCut, our experiments take 0.56, 3.82, and 1.24 times therunning time to compute partitions across all graphs and valuesof using Graclus, METIS, and KaHiP, respectively. In particular,XCut then is 44 % faster than Graclus. This demonstrates the utilityof XCut as a tool for exploratory data analysis. We could achievefurther speedups by only computing a solution for = 128 andthen choosing the initial subsets of edges for the other values of ,only performing the refinement.",
  "Algorithm Name": "XCutXCut (min)KaHiPMETIS Figure D.8: Percentage deviation of the returned normalized cut value relative to Graclus for = 128. This means that a valueof -75% means that the normalized cut value is 75% lower (i.e., better). The thin black bars indicate the standard error acrossour runs. The top graph shows the disconnected IMDB graph (BP1), citation network instances (CN), email networks (EM),infrastructure graphs (IF), social networks (SN) and web graphs (WB), while the bottom shows the remaining instances. SeeTable C.3 for details. k 0.0 0.2 0.4 0.6 0.8 Time [s] Graph ID = EM2 k 0.0 0.5 1.0 1.5 2.0 Graph ID = FE1 k Graph ID = CN8 k Graph ID = WB3 k Graph ID = SN5",
  "Discussion": "XCut outperforms other software when computing normalizedcuts on social, citation, email, and infrastructure networks and webgraphs. It performs slightly worse on graphs arising from specificcomputational tasks, such as finite elements, circuit, or numericalsimulations. The graphs on which this behavior occurs tend tohave degree distributions concentrated around the average degree,suggesting that they are not graphs with a scale-free structure.In we plot the relationship between our improvement",
  "CONCLUSION": "In this work we introduced XCut, a new algorithm for solving thenormalized cut problem. It is based on a novel expander decom-position algorithm and to the best of our knowledge, it is the firstpractical application of the expander hierarchy. XCut clearly out-performs other solvers in the experimental study on social, citation,email, and infrastructure networks and web graphs, and also inthe geometric mean over all instances. It scales to instances with tens of millions of edges and can produce solutions for multiplenumbers of clusters with little overhead by comparison. We areconfident that with further optimization and the use of parallelismit will be possible to scale our algorithm to even larger graphs, whilefurther improving the solution quality, especially since computingthe expander decomposition appears to be highly parallelizable.We also believe that the expander hierarchy and our expanderdecomposition can be applied to other graph cut problems in thefuture, as the tree flow sparsifiers approximate all cuts in the graph,and there are theoretical results that suggest this might be the case.XCut is open source software and its code is freely available onGitLab . Monika Henzinger: This project has re-ceived funding from the European ResearchCouncil (ERC) under the European UnionsHorizon 2020 research and innovation programme (Grant agree-ment No. 101019564) and the Austrian Science Fund (FWF) grantDOI 10.55776/Z422, grant DOI 10.55776/I5982, and grant DOI10.55776/P33775 with additional funding from the netidee SCIENCEStiftung, 20202024.Harald Rcke, Robin Mnk: This project has received fundingfrom the Deutsche Forschungsgemeinschaft (DFG, German Re-search Foundation) 498605858 and 470029389.",
  "Yi-Jun Chang, Seth Pettie, Thatchaphol Saranurak, and Hengjie Zhang. 2021.Near-optimal Distributed Triangle Enumeration via Expander Decompositions.J. ACM 68, 3 (2021), 21:121:36": "Li Chen, Rasmus Kyng, Yang P. Liu, Richard Peng, Maximilian Probst Gutenberg,and Sushant Sachdeva. 2022. Maximum Flow and Minimum-Cost Flow in Almost-Linear Time. In 63rd IEEE Annual Symposium on Foundations of Computer Science,FOCS 2022, Denver, CO, USA, October 31 - November 3, 2022. IEEE, 612623. Xiaojun Chen, Feiping Nie, Joshua Zhexue Huang, and Min Yang. 2017. Scal-able Normalized Cut with Improved Spectral Rotation. In Proceedings of theTwenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI 2017,Melbourne, Australia, August 19-25, 2017, Carles Sierra (Ed.). ijcai.org, 15181524.",
  "Timothy A Davis and Yifan Hu. 2011. The University of Florida sparse matrixcollection. ACM Transactions on Mathematical Software (TOMS) 38, 1 (2011),125": "Inderjit S Dhillon, Yuqiang Guan, and Brian Kulis. 2007. Weighted graph cutswithout eigenvectors a multilevel approach. IEEE transactions on pattern analysisand machine intelligence 29, 11 (2007), 19441957. Gramoz Goranci, Harald Rcke, Thatchaphol Saranurak, and Zihan Tan. 2021.The expander hierarchy and its applications to dynamic graph algorithms. InProceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms (SODA).SIAM, 22122228. Bernhard Haeupler, Harald Rcke, and Mohsen Ghaffari. 2022. Hop-constrainedexpander decompositions, oblivious routing, and distributed universal optimal-ity. In Proceedings of the 54th Annual ACM SIGACT Symposium on Theory ofComputing. 13251338. Kathrin Hanauer, Monika Henzinger, Robin Mnk, Harald Rcke, and Maxi-milian Vtsch. 2024. Expander Hierarchies for Normalized Cuts on Graphs.In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discov-ery and Data Mining, KDD 2023, August 2529, 2024, Barcelona, Spain. ACM. To appear.",
  "Bruce Hendrickson and Robert W. Leland. 1995. A Multi-Level Algorithm ForPartitioning Graphs. In Proceedings Supercomputing 95, San Diego, CA, USA,December 4-8, 1995, Sidney Karin (Ed.). ACM, 28": "Monika Henzinger, Stefan Neumann, Harald Rcke, and Stefan Schmid. 2023.Dynamic Maintenance of Monotone Dynamic Programs and Applications. In40th International Symposium on Theoretical Aspects of Computer Science, STACS2023, March 7-9, 2023, Hamburg, Germany (LIPIcs, Vol. 254), Petra Berenbrink,Patricia Bouyer, Anuj Dawar, and Mamadou Moustapha Kant (Eds.). SchlossDagstuhl - Leibniz-Zentrum fr Informatik, 36:136:16. Yiding Hua, Rasmus Kyng, Maximilian Probst Gutenberg, and Zihang Wu. 2023.Maintaining expander decompositions via sparse cuts. In Proceedings of the 2023Annual ACM-SIAM Symposium on Discrete Algorithms (SODA). SIAM, 4869.",
  "Bojan Mohar. 1989. Isoperimetric numbers of graphs. J. Comb. Theory, Ser. B 47,3 (1989), 274291": "Danupon Nanongkai and Thatchaphol Saranurak. 2017. Dynamic spanningforest with worst-case update time: adaptive, las vegas, and o (n1/2-)-time. InProceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing.11221129. Danupon Nanongkai, Thatchaphol Saranurak, and Christian Wulff-Nilsen. 2017.Dynamic minimum spanning forest with subpolynomial worst-case update time.In 2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS).IEEE, 950961.",
  "Maxim Naumov and Timothy Moon. 2016. Parallel spectral graph partitioning.NVIDIA, Santa Clara, CA, USA, Tech. Rep., NVR-2016-001 (2016)": "Andrew Y. Ng, Michael I. Jordan, and Yair Weiss. 2001.On SpectralClustering: Analysis and an algorithm. In Advances in Neural Informa-tion Processing Systems 14 [Neural Information Processing Systems: Naturaland Synthetic, NIPS 2001, December 3-8, 2001, Vancouver, British Columbia,Canada], Thomas G. Dietterich, Suzanna Becker, and Zoubin Ghahramani(Eds.). MIT Press, 849856. Feiping Nie, Jitao Lu, Danyang Wu, Rong Wang, and Xuelong Li. 2024. A NovelNormalized-Cut Solver With Nearest Neighbor Hierarchical Initialization. IEEETrans. Pattern Anal. Mach. Intell. 46, 1 (2024), 659666.",
  "Stella X. Yu and Jianbo Shi. 2003. Multiclass Spectral Clustering. In 9th IEEEInternational Conference on Computer Vision (ICCV 2003), 14-17 October 2003, Nice,France. IEEE Computer Society, 313319": "Jin Zhang, Lei Xie, Wei Feng, and Yanning Zhang. 2009. A Subword NormalizedCut Approach to Automatic Story Segmentation of Chinese Broadcast News.In Information Retrieval Technology, 5th Asia Information Retrieval Symposium,AIRS 2009, Sapporo, Japan, October 21-23, 2009. Proceedings (Lecture Notes inComputer Science, Vol. 5839), Gary Geunbae Lee, Dawei Song, Chin-Yew Lin,Akiko N. Aizawa, Kazuko Kuriyama, Masaharu Yoshioka, and Tetsuya Sakai",
  "ATHEORETICAL ANALYSIS OF THEEXPANDER DECOMPOSITION": "The key ingredient for the analysis is to show that it is very unlikelythat the cut procedure does not return a cut within iterationswhen started on a graph with conductance less than . Conse-quently, if the algorithm does not return a cut for iterations wecan declare to be a -expander (or actually {} to be a near6-expander), with a small probability of error.For this we analyze the random walks in terms of flows. Eachnode injects units of flow of a unique commodity. This flowis distributed according to the random walk. Let () denote theamount of flow from node that has reached node after steps.We define () := ( )",
  "vol( )2 in any step , then () is anear 6-expander in": "Proof. We omit the time step to avoid notational clutter. Therandom walk can be viewed as establishing a multicommodity flowwithin the network. is the flow of commodity that reachednode . The flow has congestion at most the number of steps ofthe random walk. We take at most = 1/(12) steps.Now consider a cut , with vol() vol()/2. We haveto show that |(, \\ )| vol()/(2). For the potential to beless than 1/(4 vol( )2), we must have +1",
  "vol() 6 vol() edges across the cut": "In the following we show that with constant probability duringone iteration we either return a balanced low conductance cut or thepotential decreases significantly. For this we first have to analyzeby how much the potential decreases during a random walk step. Potential Decrease by Random Walk. Fix a round . Note that therandom walk for round only takes the 1 random walk steps forthe graphs {(1)}, . . . ,{( 1)}, the step for {()} followsin the next round. In the following we develop an expression forhow much the potential will decrease due to the random walk stepin the current iteration . For this we need a technical claim, whichis proven in Section A.1.",
  "{, } {( )} () ()22( ) () ()22": "Proof. To simplify the analysis of a random walk step we vieweach vertex of {()} as consisting of sub-vertices as follows.For each edge {,} we introduce two sub-vertices one at andone at , that are connected by an edge. In this way, every super-vertex from () receives sub-vertices and every sub-vertexhas a unique neighbour.During a random walk step, each sub-vertex first takes a copyof the flow vector of its super-vertex. Then the vectors are averagedalong every edge, and finally a super-vertex computes the averageof the vector of its sub-vertices.We can express the potential by either summing over super-vertices or sub-vertices:",
  "{, } 22and averaging among sub-nodes does not increase it again. Hence,the lemma follows": "Projected Potential Decrease. The algorithm does not maintain the vectors or the potential explicitly, as that would be computation-ally infeasible Rather it operates on their projections onto somerandom vector . The entries in the vector of the algorithm areactually = ( ). This follows since performing the randomwalk and then projecting the resulting vectors onto is equiva-lent to first projecting onto and then running the random walkon the vector of projections. Ultimately, subtracting the weightedaverage of the values corresponds to subtracting from the vectors before the projection.Consider the quotient",
  "( ) 2,": "and compare it to the bound () for the relative potential decreasein Lemma 2. Up to a factor of 1/2, () is obtained by individuallyperforming a random projection on each vector in the expressionfor (). Using properties of random projections, we will show that() is a good indicator for the potential decrease (). Lemma 3 (Projection Lemma). Let 1, . . . , be a collection of -dimensional vectors. Let denote a random -dimensional vector witheach coordinate sampled independently from a Gaussian distributionN (0, 1/). Then",
  "For the further analysis we will always assume that we arein a good round": "Finding a Cut. With the above lemma we have established, that alarge () value ensures the next step of the random walk reducesthe potential a lot. Now we show that a small () value ensuresthat the algorithm finds a low conductance cut.Indeed the quotient () (the Rayleigh quotient of using thegraph Laplacian), and its analysis lies at the heart of the proof forthe celebrated Cheeger inequality. It states that a small () valueimplies the existence of a low conductance sweep cut.",
  "Note that holds by design of the algorithm due to Line 10 ofthe algorithm. With this Cheeger Lemma we thus get the guaranteethat there has to be a sweep cut through whose conductance satisfies": "2(). Together with Lemma 4 this now impliesthe following: If we will only make little progress in the potentialwith the next random walk step, i.e., () for some small ,then we find a sweep cut through the vector of projections withconductance O( log). By contraposition, if all sweep cutshave conductance at least , then () (2/log2 ).This motivates our approach of analyzing the sweep cuts on and differentiating three cases: 1) No low conductance sweep cut ex-ists, 2) A balanced low conductance cut exists, or 3) An unbalancedlow conductance cut exists.If we find a balanced sweep cut, we return immediately as thisensures the recursion depth of the surrounding expander decom-position remains small. If no sweep cut has conductance below, the above reasoning ensures we make sufficient progress withthe next random walk step. Next we prove that even if we find anunbalanced cut, the potential still decreases sufficiently. Handling Unbalanced Cuts. Let (, ) be an unbalanced cut, wherevol() 2 log2(vol())/vol() and there is no -low conductancesweep cut through in . We show that if () is small and wethus cannot ensure sufficient progress from the random walk, thenthe set must contain a large fraction of the potential. Hence weinstead make progress by removing when we set ( + 1) = .Consider the following lemma, which gives an upper bound onhow much the () value can at most increase if we remove a cutand recenter the remaining projected values. Recentering the vectorafter removing the cut allows us to use Lemma 5 to argue about theexistence of low conductance cuts in the remaining subgraph. Let R|| be the shortened and recentered vector of the projections, i.e., := for all 9. By design, it then holds .",
  "(3/41/4) vol() 2.(3)": "Summing over gives = vol() = 0. Thisallows us to use Lemma 5 in the subgraph {}. By design ofthe algorithm, since we did not find a balanced low conductancecut, (, ) is a two-ended sweep cut where has maximal volumeamong low conductance cuts. Hence there is no low conductancesweep cut through that crosses the set . We get",
  "gives that after1": "96 good rounds, the potential is below the thresh-old.By Claim 2, a round is good with probability 1/4. Let bethe indicator random variable representing whether round wasgood and = their sum. Clearly, is the number of goodrounds and its expectation is E[] = /4. Since 1/log2 weget log2()/12 and thus E[] log2()/48. Using a Chernoffbound then gives",
  ". Applying a union bound over the at most 2 vectors gives thestatement. Finally, note that 15 ln < 11 log2 for all > 1": "4. Let denote the vector where the -th entry is . Then := ( )2 = ()2 = ()2 = 22 ()2. Letmax denote the largest length of an -vector. We classifiy the -vectors whose length fall in the range (max/, max] into log2 classes so that the length of vectors in the same class differs byat most a factor of 2. Formally, the -th class for 1, . . . , log2 contains vectors with length in the range (max/2, max/21]. Scalethe length for each classified vector down to the lowest lengthin its class and scale the unclassified vectors down to length 0. Let denote scaled length of a vector . We have 2 classified 2/4 2/4 (/)2",
  "2": "for 4. The second inequality holds because an unclassifiedvector has at most length max/ and the third inequality becausethere is one vector with length max. Now, we choose the classwhere the total scaled length of vectors is largest. Let R denote theindex set of vectors in this class, let R denote their scaled lengthand let R denote their number.Then R2R 1 8 log2 2. Now, we consider the sum ofsquared projections of the scaled vectors in R. This is distributedaccording to a 2-distribution with R degrees of freedom scaled by2R/. The median for this distribution is R (1 2/(9)) 0.4.Hence, with probability 1/2",
  "BADDITIONAL RELATED WORK": "Multilevel Graph Partitioning. This framework has been em-ployed by many successful graph partitioning tools, e.g., METIS ,Graclus , or KaHiP . The general idea of the paradigm is tocompute a small summary (coarsening) of the graph, on which itis easier to solve the problem we are interested in, and then map-ping this coarse solution onto the original graph. In more technicalterms, a solver following this paradigm has three phases: (i) Coarsen-ing: Compute a series of successively smaller graphs 0,1, . . . ,such that |0| > |1| > > | | with the aim to create a coarsesummary of the original graph. (ii) Solving: Obtain a solution to theinitial problem on the graph . Although is small, heuristicsare usually employed here. (iii) Refinement: Successively map thesolution from graph to graph 1, while applying heuristicsthat increase the solution quality.",
  "#NameType| |||mean 25th 50th 75th 90th": "BP1imdbBipartite1 403 2784 303 38316526.1312615CF1ramage02Computational Fluids16 8301 424 761269 169.31131170170269CL1ukClustering4824683732.833333CL2smallworldClustering100 000499 9981710.009101112CN1citationCiteseerCitation Network268 4951 156 64713188.62251018CN2ca-hollywood-2009Citation Network1 069 126 56 306 65311 467 105.33133175212CN3coAuthorsDBLPCitation Network299 067977 6763366.5424714CN4ca-MathSciNetCitation Network332 689820 6444964.9313511CN5ca-coauthors-dblpCitation Network540 486 15 245 729329956.41133474135CN6ca-dblp-2012Citation Network317 0801 049 8663436.6224714CN7ca-citeseerCitation Network227 320814 13413727.1624815CN8coPapersCiteseerCitation Network434 102 16 036 720118873.88153992177CN9ca-dblp-2010Citation Network226 413716 4602386.3324713CS1add32Circuit Simulation49609462313.822349CS2rajat10Circuit Simulation30 20250 1011013.322444CS3memplusCircuit Simulation17 75854 1965736.1023411DM1 pcrystk02Duplicate Materials13 965477 3098068.3653808080EM1email-enron-largeEmail Network33 696180 811138310.7313719EM2email-EUEmail Network32 43054 3976233.351113FE1fe_toothFinite Elements78 136452 5913911.586121518FE2fe_rotorFinite Elements99 617662 43112513.3011131417IF1inf-openflightsInfrastructure Network293915 67724210.6723828IF2inf-powerInfrastructure Network49416594192.672235NS1wing_nodalNumerical Simulation10 93775 4882813.8012141617NS2autoNumerical Simulation448 6953 314 6113714.7713151618OP1gupta2Optimization62 0642 093 111841267.4525384749OP2finance256Optimization37 376130 560546.9946812RD1appuRandom Graph14 000919 552293 131.36109131154176RN1inf-roadNet-CARoad Network1 957 0272 760 388122.822334RN2inf-roadNet-PARoad Network1 087 5621 541 51492.832344RN3inf-italy-osmRoad Network6 686 4937 013 97892.102223RN4luxembourg_osmRoad Network114 599119 66662.092223SN1soc-youtube-snapSocial Network1 134 8902 987 62428 7545.271138SN2soc-flickrSocial Network513 9693 190 452436912.4111517SN3soc-lastfmSocial Network1 191 8054 519 33051507.5812411SN4soc-twitter-followsSocial Network404 719713 3196263.531113SN5soc-pokecSocial Network1 632 803 22 301 96414 85427.324133570SN6soc-livejournalSocial Network4 033 137 27 933 062265113.85251535SN7soc-FourSquareSocial Network639 0143 214 986 106 21810.0611419TM1 vsp_vibrobox_scagr7-2c_rlfdddTriangle Mixture77 328435 58666911.2735826TM2 vsp_bump2_e18_aa01_model1_crew1 Triangle Mixture56 438300 80160410.66571118TM3 vsp_barth5_1Ksep_50in_5KoutTriangle Mixture32 212101 805226.326677TM4 vsp_model1_crew1_cr42_south31Triangle Mixture45 101189 97617 6638.423579TM5 vsp_p0291_seymourl_iiasaTriangle Mixture10 49853 86822910.26371116TM6 vsp_bcsstk30_500sep_10in_1KoutTriangle Mixture58 3482 016 57821969.12456492106TM7 vsp_befref_fxm_2_4_air02Triangle Mixture14 10998 224153113.92691115US1mi2010US Census Redistricting329 885789 045584.783458WB1 web-it-2004Web Graph509 3387 178 41346928.1914161939WB2 web-googleWeb Graph12992773594.2712512WB3 web-wikipedia2009Web Graph1 864 4334 507 31526244.8412510",
  "Algorithm": "XCutKaHiPGraclusMETIS Figure D.10: Plots of running time versus . The first row shows the usual behavior of the solvers in our comparison, with oursolver starting at a much higher level than the competition. This is due to the fact that we need to first compute the expanderhierarchy, which gives a hard lower bound on the running time of our algorithm for any choice of . The running time thengrows linearly with , due to Greedy taking time (). The bottom row contains some instances on which the running timeresults are unusual. Often Graclus running time grows very quickly, but we are unsure what causes this on these instances inparticular. Table D.5: Cut values and running time of our Algorithm vs the values reported by Zhao et al . All values are for = 30.The Cut-columns contain the normalized cut value (lower is better). The minimum value in each row is marked bold. Thebottom row contains the geometric mean of all values."
}