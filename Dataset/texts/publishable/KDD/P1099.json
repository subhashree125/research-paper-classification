{
  "Abstract": "Complex Query Answering (CQA) over Knowledge Graphs (KGs)is a challenging task. Given that KGs are usually incomplete, neu-ral models are proposed to solve CQA by performing multi-hoplogical reasoning. However, most of them cannot perform wellon both one-hop and multi-hop queries simultaneously. Recentwork proposes a logical message passing mechanism based on thepre-trained neural link predictors. While effective on both one-hop and multi-hop queries, it ignores the difference between theconstant and variable nodes in a query graph. In addition, duringthe node embedding update stage, this mechanism cannot dynami-cally measure the importance of different messages, and whetherit can capture the implicit logical dependencies related to a nodeand received messages remains unclear. In this paper, we proposeConditional Logical Message Passing Transformer (CLMPT), whichconsiders the difference between constants and variables in the caseof using pre-trained neural link predictors and performs messagepassing conditionally on the node type. We empirically verifiedthat this approach can reduce computational costs without affect-ing performance. Furthermore, CLMPT uses the transformer toaggregate received messages and update the corresponding nodeembedding. Through the self-attention mechanism, CLMPT canassign adaptive weights to elements in an input set consisting ofreceived messages and the corresponding node and explicitly modellogical dependencies between various elements. Experimental re-sults show that CLMPT is a new state-of-the-art neural CQA model.",
  "Corresponding author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain. 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08",
  "Introduction": "Knowledge graphs (KGs) store factual knowledge in the form oftriples that can be utilized to support a variety of downstream tasks. However, given that modern KGs are usually auto-generated or built through crowd-sourcing , so real-worldKGs are often considered noisy and incomplete, whichis also known as the Open World Assumption . Answeringqueries on such incomplete KGs is a fundamental yet challengingtask. To alleviate the incompleteness of KGs, knowledge graph rep-resentation methods , which can be viewed as neurallink predictors , have been developed. They learn representa-tions based on the available triples and generalize them to unseentriples. Such neural link predictors can answer one-hop atomicqueries on incomplete KGs. But for multi-hop complex queries,the query-answering models need to perform multi-hop logicalreasoning over incomplete KGs. That is, not only to utilize avail-able knowledge to predict the unseen one but also execute logicaloperators, such as conjunction (), disjunction (), and negation() .Recently, neural models have been proposed tosolve Complex Query Answering (CQA) over incomplete KGs. Thecomplex queries that these models aim to address belong to animportant subset of the first-order queries, specifically Existentiallyquantified First Order queries with a single free variable (EFO-1). Such EFO-1 queries contain logical operators, including",
  ": EFO-1 query and its query graph for the questionWho starred the films that were directed by Mizoguchi butnever won a Venice Film Festival Award?": "conjunction, disjunction, and negation, as well as the existentialquantifier () . As shown in , one can get the corre-sponding EFO-1 formula and query graph given a question. Thequery graph is a graphical representation of the EFO-1 query, whereeach edge is an atomic formula that contains a predicate with a(possible) negation operator, and each node represents an inputconstant entity or a variable. Most neural CQA models convert thequery graph into the computation graph, where logical operatorsare replaced with corresponding set operators. These models embedthe entity set into specific vector spaces and executeset operations parameterized by the neural networks according tothe computational graph. However, these models cannot performwell on both one-hop and multi-hop queries simultaneously. Theytend to be less effective than classical neural link predictors on one-hop atomic queries .To this end, Logical Message Passing Neural Network (LMPNN) proposes a message passing framework based on pre-trainedneural link predictors. Specifically, for each edge in the query graph,LMPNN uses the pre-trained neural link predictor to infer an in-termediate embedding for a node given neighborhood information.The intermediate embedding can be interpreted as a logical mes-sage passed by the neighbor on the edge. demonstratesthe logical message passing with red/blue arrows. Then, LMPNNaggregates messages and updates node embeddings in a mannersimilar to Graph Isomorphism Network (GIN) . While effec-tive on both one-hop and multi-hop queries, LMPNN ignores thedifference between constant and variable nodes. Logical messagepassing can be viewed as learning an embedding for the variable byutilizing information of constant entities. It makes this embeddingsimilar or close to the embedding of the solution entity in the em-bedding space of the pre-trained neural link predictor. However, asshown by the red arrows in , LMPNN also passes messagesto constant entitiy nodes with pre-trained embeddings and updatestheir embeddings, which we believe is unnecessary and experi-mentally demonstrated. In addition, for node embedding updating,despite the expressive power of GIN in graph structure learning, it cannot dynamically measure the importance of differentlogical messages. Furthermore, it is unclear whether the GIN-likeapproach can capture the implicit complex logical dependencies be-tween messages received by a node and between messages and thatnode. Explicitly modeling the above implicit logical dependencies while being able to measure the importance of different messagesdynamically is an open challenge we take on in this paper.We propose Conditional Logical Message Passing Transformer(CLMPT), a special Graph Neural Network (GNN) for CQA. EachCLMPT layer has two stages: (1) passing the logical message ofeach node to all of its neighbors that are not constant entity nodes;(2) updating the embedding of each variable node based on thelogical messages received by the variable node. This means thatduring forward message passing, CLMPT does not use the pre-trained neural link predictor to infer the intermediate embeddings(i.e. logical messages) for constant entity nodes with pre-trainedembeddings and does not update constant node embeddings. Wecall such a mechanism Conditional Logical Message Passing, wherethe condition depends on whether the node type is constant orvariable. We empirically verified that it can reduce computationalcosts without affecting performance. Furthermore, CLMPT usesthe transformer to aggregate messages and update the corre-sponding node embedding. The self-attention mechanism in thetransformer can dynamically measure the importance of differentelements in the input and capture the interactions between any twoelements. It allows us to explicitly model the logical dependenciesbetween messages received by a node and between messages andthat node. We conducted experiments on three popular KGs: FB15k, FB15k-237 , and NELL995 . The experimental resultsshow that CLMPT can achieve a strong performance.The main contributions of this paper are summarized as follows:",
  "Related Work": "Neural Link Predictors. Reasoning over KGs with missing knowl-edge is one of the fundamental problems in Artificial Intelligenceand has been widely studied. Traditional KG reasoning tasks suchas link prediction are essentially one-hop atomic queryproblems. Representative methods for link prediction are knowl-edge graph representation methods , whichcan be regarded as neural link predictors . Specifically, they em-bed entities and relations into continuous vector spaces and predictunseen triples by scoring triples with a well-defined scoring func-tion. Such latent feature models can effectively answer one-hopatomic queries over incomplete KGs. Other methods for link pre-diction include rule learning , text representation learning, and GNNs . Neural Complex Query Answering. Complex queries over KGscan be regarded as one-hop atomic queries combined with existen-tial first-order logic operators. The scope of complex queries thatexisting works can answer is expanded from conjunctive queries, to Existential Positive First-Order (EPFO) queries ,",
  "Conditional Logical Message Passing Transformer for Complex Query AnsweringKDD 24, August 2529, 2024, Barcelona, Spain": ": The MRR results of CLMPT-small using frozen neural link predictor and using trainable neural link predictor. ParamTand ParamF represent trainable and frozen parameters of the model, respectively. The F in parentheses indicates that the modeluses a frozen neural link predictor, and T indicates that it uses a trainable one.",
  "Background": "Model-theoretic Concepts for Knowledge Graphs. A KnowledgeGraph KG consists of a set of entities V and a set of relationsR. It can be defined as a set of triples E = (,, ) V R V, namely KG = (V, E, R). A first-order language L canbe defined as (F, R, C), where F , R, and C are sets of symbolsfor functions, relations, and constants, respectively . Underlanguage LK G, the KG can be represented as a first-order logicknowledge base . In this case, the relation symbols in R denotebinary relations, the constant symbols in C represent the entities,and the function symbol set satisfies F = . In other words, KGis an LK G-structure, where each entity V is also a constant C = V and each relation R is a set V V. Wehave (1,2) = when (1,2) . An atomic formula is either (1,2) or (1,2), where is a term that can be a constant or avariable, and is a relation that can be viewed as a binary predicate. A variable is a bound variable when associated with a quantifier.Otherwise, it is a free variable. By adding connectives (conjunction, disjunction , and negation ) to such atomic formulas andadding quantifiers (existential and universal ) to variables, wecan inductively define the first order formula . EFO-1 Queries. In this paper, we consider Existential First Orderqueries with a single free variable (EFO-1) . Such EFO-1queries are an important subset of first-order queries, using exis-tential quantifier, conjunction, disjunction, and atomic negation.Following the previous studies , we define the EFO-1query as the first order formula in the following Disjunctive NormalForm (DNF) :",
  "= (, ) (, ) (, ) (, ) ,(2)": "where V is an input constant entity, , {,1, ...,} arevariables, . To address the EFO-1 query , the correspondinganswer entity set V needs to be determined, where isa set of entities such that iff [ = ,1, ...,] = . Inparticular, since we use DNF to represent the first order formula, theEquation 1 can also be expressed as the disjunction of conjunctive",
  "[,1, ...,] = 1(,1, ...,) ... (,1, ...,),(3)": "where = 1, ..., 1 ... is a conjunctive query.According to , we can get the answer set by takingthe union of the answer sets of each conjunctive query, namely = =1. This means that solving all conjunctive queries, 1 yields the answer set for the EFO-1 query .Such DNF-based processing can solve complex queries involvingdisjunction operators in a scalable way . Query Graph. Since disjunctive queries can be solved in a scalablemanner by transforming queries into the disjunctive normal form,it is only necessary to define query graphs for conjunctive queries.We follow previous works and represent the conjunctivequery as the query graph where the terms are represented as nodesconnected by the atomic formulas. That is, each edge in the querygraph is an atomic formula. According to the definition in Equation2, an atomic formula contains relations, negation information, andterms. Therefore, each node in the query graph is either a constantentity or a free or existential variable, as illustrated in . Inthis paper, the term \"constants\" has specific meanings dependingon the context. When discussing an EFO-1 query, \"constants\" refersto the input constant entities within that query. In the contextof the query graph, \"constants\" denotes the constant nodes thatcorrespond to these input constant entities. Neural Link Predictors. A neural link predictor has a correspond-ing scoring function that is used to learn the embeddings of entitiesand relations in KG. For a triple, the embeddings of the head entity,relation, and tail entity are , , and , respectively. The scoringfunction (,,) can calculate the likelihood score of whether thetriple exists. By using the scoring function (,,) and the sig-moid function , neural link predictors can produce a continuoustruth value (,,) for a triple. For example, the scoringfunction of ComplEx is as follows:",
  "(,,) = ((,,)).(5)": "In this work, the neural link predictor handles not only specificentity embeddings but also variable embeddings. That is, the em-beddings , correspond to embedding of terms, which can be theembeddings of constant entities or the embeddings of variables.Following previous works , in our work, we use ComplEx-N3 as the neural link predictor of CLMPT. One-hop Inference on Atomic Formulas. As shown in ,each edge in a query graph is an atomic formula containing theinformation of relation, logical negation, and terms. Passing a logi-cal message on an edge is essentially an operation of utilizing thisinformation to perform one-hop inference on an atomic formula.On each edge, when a node is at the head position, its neighbor isat the tail position, and vice versa. A logical message passed from aneighbor to a node is essentially an intermediate embedding of that node. Prior work proposes to obtain the intermediate embed-dings of nodes by one-hop inference that maximizes the continuoustruth value of the (negated) atomic formulas. Specifically, a logicalmessage encoding function is proposed to perform one-hop infer-ence. Such a function has four input parameters, including neighborembedding, relation embedding, direction information ( or ), and logical negation information (0 for no negation and 1for with negation), and it has four cases depending on the inputparameters. Given the tail embedding and relation embedding on a non-negated atomic formula, is formulated in the form ofcontinuous truth value maximization to infer the head embedding: = (,, , 0) := D (,,),(6)",
  "Proposed Method": "In this section, we propose Conditional Logical Message PassingTransformer (CLMPT) to answer complex queries. As a specialmessage passing neural network , each CLMPT layer has twostages: (1) passing the logical message of each node to all of itsneighbors that are not constant entity nodes; (2) updating the em-bedding of each variable node based on the messages received by",
  "Conditional Logical Message Passing": "A query graph contains two types of nodes: constant entity nodesand variable nodes. We decide whether to pass logical messages toa node based on its type. Specifically, for any neighbor of a node,when the neighbor is a variable node, we use the logical messageencoding function to calculate the corresponding message andpass it to the neighbor; when the neighbor is a constant entity node,we do not pass the logical message to it. (a) demonstratesthe conditional logical message passing with blue arrows. Everynode in the query graph except the constant entity nodes receivesthe logical message from all of its neighbors.",
  "Node Embedding Conditional UpdateScheme": "For a constant entity node, we denote its embedding at the -thlayer as ()and let ()be the embedding of a variable node atthe -th layer. Next, we discuss how to calculate the ()and ()from the input layer = 0 to latent layers > 0. For a constantentity node, (0)is the corresponding entity embedding in the pre-trained neural link predictor, which can be frozen or optimizedcontinuously. For (0), we follow previous works and assigntwo learnable embeddings , for the existential variable node and the free variable node respectively, and set that all existentialvariable nodes share one , namely (0)= and (0)= .Similar to conditional logical message passing, for updating thenode embeddings, we also do not consider constant entity nodes.That is, we only update node embeddings for variable nodes in thequery graph. Thus, the embeddings of the constant entity nodes arethe same at each layer, namely ()= (0). While for () , we usethe corresponding information from the ( 1)-th layer to update it.Specifically, for a variable node , we represent its neighbor set inthe query graph as N (). For each neighbor node N (), one canobtain information about the edge (i.e., the atomic formula) between and , which contains the neighbor embedding (1) D, therelation R, the direction { , }, and thenegation indicator {0, 1}. Based on the edge information,we can use the logical message encoding function to compute thelogical message () that passes to :",
  "() = ((1),, , ).(14)": "Let be the number of neighbor nodes in N (), where 1.For the variable node , it receives logical messages from neighbors: ()1 , ...,() . We use the transformer to encode theinput set consisting of these logical messages and the variable nodeembedding (1)to obtain the updated embedding of node :",
  "()= (() (()1 , ...,() ,(1))),(15)": "where is a mean pooling layer. is a standard transformerencoder stacked with multiple layers of transformer encoder blocks,where the attention computation approach is consistent with thebidirectional multi-head self-attention of the vanilla transformer. Specifically, given an input embedding set , it is first pro-jected to query (), key (), and value ( ) matrices through a linearprojection such that = , = and = respec-tively. Then, the self-attention can be compute via",
  "),(16)": "where represents the dimension of , and , , areparameter matrices. We use multi-head attention, which concate-nates multiple instances of Equation 16 and then obtains the outputthrough a linear projection. It is worth noting that since the inputto Equation 15 is a set consisting of the node embedding and themessages rather than a sequence, we do not use any positional en-coding. This means that our architecture is permutation invariant.Through the self-attention mechanism, we can explicitly modelthe complex logical dependencies between various elements in theinput set and dynamically measure the importance of different ele-ments. (b) shows how to obtain the updated embeddingof the existential variable node through the encoding processdefined by Equation 15.",
  "(,,) = [(,)/],(18)": "where (, ) is the cosine similarity, is a hyperparameter, is the embedding of positive answer , is the embedding ofthe -th noisy samples, and () is the embedding of the predictedanswer to the query , corresponding to the embedding of the freevariable of the query at the final layer of CLMPT.",
  "Answering Complex Queries with CLMPT": "For a complex query defined in Equation 3, i.e., a DNF query, wefollow the previous works and use DNF-based processingto get answer entities. Specifically, we estimate the predicted an-swer embeddings for each sub-conjunctive query of the complexquery. Then, the answer entities are ranked by the maximal cosinesimilarity against these predicted answer embeddings. Therefore,CLMPT only needs to consider the query graph of the conjunctivequery.For a given conjunctive query, let the depth of CLMPT be , andwe apply the CLMPT layers times to the query graph of . Then,we can obtain the free variable embedding ()at the final layer. We",
  "use ()as the embedding of the predicted answer entity, namely": "() = (). Based on the cosine similarity between () and theentity embeddings in the pre-trained neural link predictor, we canrank the entities to retrieve answers. For the depth , accordingto the analyses in , should be the largest distance betweenthe constant entity nodes and the free variable node to ensure thefree variable node successfully receives all logical messages fromthe constant entity nodes. This means is not determined, and should change dynamically with different conjunctive query types.Thus, we follow prior work and assume all CLMPT layersshare the same transformer encoder.In this case, the parameters in CLMPT include two embeddingsfor existential and free variables, a transformer encoder, and theparameters in the pre-trained neural link predictor. For the pre-trained neural link predictor, it can be frozen or not. In our work, wecontinue to optimize pre-trained neural link predictors by default,that is, we do not freeze pre-trained embeddings of entities andrelations.",
  "Experiments5.1Experimental Settings": "5.1.1Datasets and Queries. We evaluate our method on threecommonly-used standard knowledge graphs: FB15k , FB15k-237, and NELL995 . For a fair comparison with previous works,we use the datasets of complex queries proposed by BetaE ,which consist of five conjunctive query types (1/2/3/2/3),five query types with atomic negations (2/3///), andfour zero-shot query types (//2/), as illustrated in Figure : All the query types considered in our experiments,where , , , and represent projection, intersection, union,and negation, respectively. The naming of each query typereflects how they were generated in the BetaE paper .",
  "Test1p66,99022,80417,021Others8,0005,0004,000": "3. The datasets provided by BetaE introduce complex queries withhard answers that cannot be obtained by traversing the KG directly.That is, the evaluation focuses on discovering such hard answers tocomplex queries. We follow previous works and trainour model with five conjunctive query types and five query typeswith atomic negations. When evaluating the model, we consider allquery types including the four zero-shot query types. The statisticsfor each dataset are presented in . 5.1.2Evaluation Protocol. The evaluation scheme follows the pre-vious works , which divides the answers to each complexquery into easy and hard sets. For test and validation splits, we de-fine hard answers as those that cannot be obtained by direct graphtraversal on KG. In order to get these hard answers, the model is re-quired to impute at least one missing edge, which means the modelneeds to complete non-trivial reasoning . Specifically, foreach hard answer of a query, we rank it against non-answer entitiesbased on their cosine similarity with the free variable embeddingof the query and calculate the Mean Reciprocal Rank (MRR). 5.1.3Baselines. We consider the state-of-the-art neural complexquery answering models for EFO-1 queries in recent years as ourbaselines to compare: BetaE , ConE , Q2P , MLP ,GammaE , CylE , CQD-CO , and LMPNN , whereCQD-CO and LMPNN use pre-trained neural link predictor, while",
  "CylE56.517.515.641.451.227.219.615.712.35.67.511.23.43.728.56.3": "(Using pre-trained neural link predictor)CQD-CO60.818.313.236.543.030.022.517.613.70.10.14.00.05.228.41.9LMPNN60.622.117.540.150.328.424.917.215.78.510.812.23.94.830.78.0CLMPT58.922.118.441.851.928.824.418.616.26.68.111.83.84.531.37.0 other models do not. In addition, some other neural CQA modelscannot handle logical negation operators, such as BIQE , PREM, kgTransformer , etc., which use the EPFO queries gener-ated by Q2B for training and evaluation. We compare theseworks on the Q2B datasets in Appendix B. We also compare ourwork with symbolic integration CQA models in Appendix C. 5.1.4Model Details. Following LMPNN and CQD-CO , wechoose ComplEx-N3 as the neural link predictor in ourwork and use the ComplEx-N3 checkpoints released by CQD-COto conduct experiments. The rank of ComplEx-N3 is 1,000, and theepoch for the checkpoints is 100. For all datasets, we use a two-layer transformer encoder with eight self-attention heads, wherethe dimension of the hidden layer of the feed-forward network(FFN) in the transformer is 8,192. For the training objective, thenegative sample size is 128, and is chosen as 0.05 for FB15k-237 and FB15k and 0.1 for NELL995. For more details about theimplementation and experiments, please refer to Appendix A.",
  "Default Setting45.713.711.337.452.028.219.014.311.17.713.78.05.05.125.97.9": "FB15K-237, and NELL995, respectively. Since conditional logicalmessage passing has little impact on the model performance (itwill be later discussed in .3.1), these performance improve-ments are mainly due to the transformer-based node embeddingupdate scheme. This shows that this approach, which explicitlymodels dependencies between input elements by assigning adap-tive weights to the messages and the corresponding node throughthe self-attention mechanism, is more suitable for the logical querygraphs of EPFO queries than the GIN-like approach. For the neg-ative queries, CLMPT achieves the best average performance onFB15k-237 and sub-optimal performance on the remaining two KGs.One potential explanation for these sub-optimal results is that thetransformer architecture lacks inductive biases and relies more onthe training data. As shown in the statistics in , the numberof negative queries in the training set is an order of magnitude lessthan the EPFO queries, which may lead to the tendency of the trans-former encoder to learn the patterns relevant to answering EPFOqueries, resulting in insufficient modeling of relevant patterns thatanswer negative queries.",
  "Ablation Study": "5.3.1Conditional Logical Meassage Passing. The logical messagepassing mechanism can be viewed as learning an embedding forthe variable by utilizing information of constant entities. It makesthis embedding similar to the embedding of the solution entityin the embedding space of the pre-trained neural link predictor.As in the previous analyses, we argue that for a constant nodewith the corresponding entity embedding, it is unnecessary toupdate its embedding in the forward passing of the model. Toverify this, we conduct experiments on whether to pass messagesto the constant node and update its embedding, and the results areshown in , where \"w/ C\" indicates \"with conditional logicalmessage passing\" and \"w/o C\" indicates \"without conditional logicalmessage passing\". From the experimental results, the performanceof LMPNN with conditional logical message passing on the twoKGs is not reduced but improved. CLMPT achieves better resultson FB15k-237 than the variant that does not use conditional logicalmessage passing, only slightly underperforming the variant onthe EPFO queries of NELL995. In addition, we evaluate how muchunnecessary computational cost the conditional logical messagepassing mechanism can avoid on an RTX 3090 GPU. Specifically, wecalculate the relative percentage of the reduction in GPU memory",
  "Time = ( )/,(20)": "where and respectively refer to the GPU memory usageand the required training time when the model uses conditional log-ical message passing, while and correspond to thesituation without conditional logical message passing. As shownin , conditional logical message passing reduces compu-tational costs without significantly negatively impacting modelperformance. In particular, the computational cost reduction isparticularly significant for CLMPT, which uses the more complextransformer architecture. The above results confirm our view thatthe model does not need to consider the constant entity nodes inthe query graph during the forward message passing.",
  "NELL995LMPNN30.78.030.17.9CLMPT32.06.831.37.0": "5.3.2Hyperparamaters and Pooling Approaches. We evaluate theperformance of CLMPT with different hyperparameter settings andpooling approaches on FB15k-237. The default setting we adoptedis described in .1.4. As is shown in , regardless ofreducing the number of transformer encoder layers or halving thedimensions of the hidden layer of the FFN, the model still achievesthe best performance than the baselines on FB15k-237, which re-flects the effectiveness of our proposed method. When the numberof transformer encoder layers increases, the performance changeslittle, meaning the two-layer transformer encoder is sufficient. Inaddition, we can find that in NCE loss is very important to the",
  "NELL995": "LMPNN60.622.117.540.150.328.424.917.215.78.510.812.23.94.830.78.033M128MCLMPT-small(F)60.621.717.742.251.730.724.519.415.66.47.911.03.74.531.66.732M128MCLMPT-small(T)59.121.317.541.851.829.023.519.015.66.47.911.23.84.531.06.8160M0M performance. When is 0.01 or 0.1, the performance of the modelis still competitive on EPFO queries, but there is a large gap withthe average performance under the default settings. For the pool-ing approaches, we evaluate max pooling and sum pooling. Theexperimental results show max pooling does not perform as wellas sum pooling and mean pooling. 5.3.3Frozen or Trainable Neural Link Predictors. For the pre-trained neural link predictor, it can be frozen or trainable. As men-tioned above (see .4), in our work, we optimize all param-eters in the pre-trained neural link predictor by default. It is worthnoting that the pre-trained neural link predictor is frozen in LMPNN.In order to explore the impact of freezing the pre-trained neurallink predictor on model performance, we conduct experiments onthree KGs, and the results are shown in .For LMPNN, using frozen neural link predictors performs betterthan using trainable ones, except for EPFO queries on FB15k andFB15k-237. Therefore, in the main results, we only compare theMRR results of LMPNN using frozen neural link predictors, i.e. theresults reported in the original paper . For CLMPT, with theexception of EPFO queries on NELL995, CLMPT using the trainableneural link predictor performs relatively better. Thus, we continueto optimize the pre-trained neural link predictor on complex queriesby default. For these results of CLMPT, according to the progressof pre-training and fine-tuning paradigms , one possiblereason is that the neural link predictor is pre-trained on one-hopqueries, and tuning its parameters on complex queries can make theembeddings of entities and relations more suitable for the patternsand characteristics of CQA. The better performance of LMPNNusing trainable neural link predictors on EPFO queries of FB15k andFB15K-237 can also support our argument to some extent becausethe number of EPFO queries in the datasets is much more thannegative ones; in this case, the model is more inclined to learnpatterns that answer EPFO queries.Although CLMPT, which uses the trainable neural link predictor,has more training parameters, it still significantly outperformsLMPNN on EPFO queries across all datasets when the neural linkpredictor is frozen. This reflects the effectiveness of CLMPT.",
  "Analyses on Model Parameters": "Since CLMPT introduces the transformer, it has more parametersthan LMPNN. To further evaluate the effectiveness of CLMPT, wecompare LMPNN using a smaller model whose parameters areclose to those of LMPNN. Specifically, we set layer = 1 and FFN = 4096, and we represent such a smaller model as CLMPT-small. Since the parameters of the neural link predictor in LMPNNare frozen, we consider using both frozen and trainable neural linkpredictors for fair comparison. We represent the models in thesetwo cases as CLMPT-small(F) and CLMPT-small(T), respectively.The experimental results are shown in . When their modelparameters are close, CLMPT still achieves the best average per-formance on EPFO queries. In the case of freezing the neural linkpredictor, CLMPT-small(F) has fewer trainable parameters thanLMPNN on NELL995, but it still significantly outperforms LMPNNon average in anwsering EPFO queries. These experimental resultsreflect the effectiveness of CLMPT.",
  "Conclusion": "In this paper, we propose CLMPT, a special message passing neuralnetwork, to answer complex queries over KGs. Based on one-hopinference by pre-trained neural link predictor on atomic formu-las, CLMPT performs logical message passing conditionally on thenode type. In the ablation study, we verify that this conditional mes-sage passing mechanism can effectively reduce the computationalcosts and even improve the performance of the model to someextent. Furthermore, CLMPT uses the transformer to aggregatemessages and update the corresponding node embedding. Throughthe self-attention mechanism, CLMPT can explicitly model logicaldependencies between messages received by a node and betweenmessages and that node by assigning adaptive weights to the mes-sages and the node. The experimental results show that CLMPTachieves a strong performance through this transformer-based nodeembedding update scheme. Future work may integrate symbolicinformation into CLMPT to improve the performance in answeringnegative queries. For limitations, please refer to Appendix D. We thank the anonymous reviewers for their helpful feedbacks.The work described in this paper was partially funded by the Na-tional Natural Science Foundation of China (Grant Nos. 62272173,62273109), the Natural Science Foundation of Guangdong Province(Grant Nos. 2024A1515010089, 2022A1515010179), the Science andTechnology Planning Project of Guangdong Province (Grant No.2023A0505050106), and the National Key R&D Program of China(Grant No. 2023YFA1011601).",
  "Erik Arakelyan, Daniel Daza, Pasquale Minervini, and Michael Cochez. 2020.Complex Query Answering with Neural Link Predictors. In International Confer-ence on Learning Representations": "Erik Arakelyan, Pasquale Minervini, Daniel Daza, Michael Cochez, and IsabelleAugenstein. 2023. Adapting Neural Link Predictors for Data-Efficient ComplexQuery Answering. In Thirty-seventh Conference on Neural Information ProcessingSystems. Jiaxin Bai, Chen Luo, Zheng Li, Qingyu Yin, Bing Yin, and Yangqiu Song. 2023.Knowledge graph reasoning over entities and numerical values. In Proceedingsof the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.5768. Jiaxin Bai, Zihao Wang, Hongming Zhang, and Yangqiu Song. 2022.Query2Particles: Knowledge Graph Reasoning with Particle Embeddings. InFindings of the Association for Computational Linguistics: NAACL 2022. 27032714.",
  "Jiaxin Bai, Tianshi Zheng, and Yangqiu Song. 2023. Sequential query encoding forcomplex query answering on knowledge graphs. arXiv preprint arXiv:2302.13114(2023)": "Yushi Bai, Xin Lv, Juanzi Li, and Lei Hou. 2023. Answering complex logical querieson knowledge graphs via query computation tree optimization. In InternationalConference on Machine Learning. PMLR, 14721491. Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor.2008. Freebase: a collaboratively created graph database for structuring humanknowledge. In Proceedings of the 2008 ACM SIGMOD international conference onManagement of data. 12471250. Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Ok-sana Yakhnenko. 2013. Translating embeddings for modeling multi-relationaldata. Advances in neural information processing systems 26 (2013). Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam Hruschka,and Tom Mitchell. 2010. Toward an architecture for never-ending languagelearning. In Proceedings of the AAAI conference on artificial intelligence, Vol. 24.13061313.",
  "Xuelu Chen, Ziniu Hu, and Yizhou Sun. 2022. Fuzzy logic based logical query an-swering on knowledge graphs. In Proceedings of the AAAI Conference on ArtificialIntelligence, Vol. 36. 39393948": "Nurendra Choudhary, Nikhil Rao, Sumeet Katariya, Karthik Subbian, and Chan-dan Reddy. 2021. Probabilistic entity representation model for reasoning overknowledge graphs. Advances in Neural Information Processing Systems 34 (2021),2344023451. Nurendra Choudhary, Nikhil Rao, Sumeet Katariya, Karthik Subbian, and Chan-dan K Reddy. 2021. Self-supervised hyperboloid representations from logicalqueries over knowledge graphs. In Proceedings of the Web Conference 2021. 13731384.",
  "Leonid Libkin and Cristina Sirangelo. 2009. Open and Closed World Assumptionsin Data Exchange. Description Logics 477 (2009)": "Lihui Liu, Boxin Du, Heng Ji, ChengXiang Zhai, and Hanghang Tong. 2021.Neural-answering logical queries on knowledge graphs. In Proceedings of the27th ACM SIGKDD conference on knowledge discovery & data mining. 10871097. Xiao Liu, Shiyu Zhao, Kai Su, Yukuo Cen, Jiezhong Qiu, Mengdi Zhang, WeiWu, Yuxiao Dong, and Jie Tang. 2022. Mask and reason: Pre-training knowledgegraph transformers for complex logical queries. In Proceedings of the 28th ACMSIGKDD Conference on Knowledge Discovery and Data Mining. 11201130. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, OmerLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: Arobustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692(2019).",
  "Ilya Loshchilov and Frank Hutter. 2019. Decoupled Weight Decay Regularization.In International Conference on Learning Representations": "Liheng Ma, Chen Lin, Derek Lim, Adriana Romero-Soriano, Puneet K. Dokania,Mark Coates, Philip H. S. Torr, and Ser-Nam Lim. 2023. Graph Inductive Biases inTransformers without Message Passing. In International Conference on MachineLearning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA (Proceedings ofMachine Learning Research, Vol. 202). PMLR, 2332123337. Zhuang Ma and Michael Collins. 2018. Noise Contrastive Estimation and Neg-ative Sampling for Conditional Models: Consistency and Statistical Efficiency.In Proceedings of the 2018 Conference on Empirical Methods in Natural LanguageProcessing. 36983707.",
  "Maximilian Nickel, Volker Tresp, Hans-Peter Kriegel, et al. 2011. A three-waymodel for collective learning on multi-relational data.. In Icml, Vol. 11. 31044823104584": "Hongyu Ren, Hanjun Dai, Bo Dai, Xinyun Chen, Denny Zhou, Jure Leskovec,and Dale Schuurmans. 2022. Smore: Knowledge graph completion and multi-hopreasoning in massive knowledge graphs. In Proceedings of the 28th ACM SIGKDDConference on Knowledge Discovery and Data Mining. 14721482. Hongyu Ren, Mikhail Galkin, Michael Cochez, Zhaocheng Zhu, and Jure Leskovec.2023. Neural graph reasoning: Complex logical query answering meets graphdatabases. arXiv preprint arXiv:2303.14617 (2023).",
  "Hongyu Ren and Jure Leskovec. 2020. Beta embeddings for multi-hop logicalreasoning in knowledge graphs. Advances in Neural Information ProcessingSystems 33 (2020), 1971619726": "Ali Sadeghian, Mohammadreza Armandpour, Patrick Ding, and Daisy Zhe Wang.2019. Drum: End-to-end differentiable rule mining on knowledge graphs. Ad-vances in Neural Information Processing Systems 32 (2019). Apoorv Saxena, Adrian Kochsiek, and Rainer Gemulla. 2022.Sequence-to-Sequence Knowledge Graph Completion and Question Answering. In Proceedingsof the 60th Annual Meeting of the Association for Computational Linguistics (Volume1: Long Papers). 28142828. Apoorv Saxena, Aditay Tripathi, and Partha Talukdar. 2020. Improving multi-hopquestion answering over knowledge graphs using knowledge base embeddings.In Proceedings of the 58th annual meeting of the association for computationallinguistics. 44984507.",
  "neural information processing systems 34 (2021), 2426124272": "Kristina Toutanova and Danqi Chen. 2015. Observed versus latent featuresfor knowledge base and text inference. In Proceedings of the 3rd workshop oncontinuous vector space models and their compositionality. 5766. Tho Trouillon, Johannes Welbl, Sebastian Riedel, ric Gaussier, and GuillaumeBouchard. 2016. Complex embeddings for simple link prediction. In Internationalconference on machine learning. PMLR, 20712080.",
  "B Wang, T Shen, G Long, T Zhou, Y Wang, and Y Chang. 2021. Structure-augmented text representation learning for efficient knowledge graph completion.In Proceedings of the Web Conference 2021. ACM": "Siyuan Wang, Zhongyu Wei, Meng Han, Zhihao Fan, Haijun Shan, Qi Zhang,and Xuanjing Huang. 2023. Query Structure Modeling for Inductive LogicalReasoning Over Knowledge Graphs. arXiv preprint arXiv:2305.13585 (2023). Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan Zhang, Zhiyuan Liu,Juanzi Li, and Jian Tang. 2021. KEPLER: A unified model for knowledge embed-ding and pre-trained language representation. Transactions of the Association forComputational Linguistics 9 (2021), 176194. Xiang Wang, Tinglin Huang, Dingxian Wang, Yancheng Yuan, Zhenguang Liu,Xiangnan He, and Tat-Seng Chua. 2021. Learning intents behind interactionswith knowledge graph for recommendation. In Proceedings of the web conference2021. 878887. Zihao Wang, Weizhi Fei, Hang Yin, Yangqiu Song, Ginny Y Wong, and SimonSee. 2023. Wasserstein-Fisher-Rao Embedding: Logical Query Embeddings withLocal Comparison and Global Transport. arXiv preprint arXiv:2305.04034 (2023). Zihao Wang, Yangqiu Song, Ginny Wong, and Simon See. 2023. Logical MessagePassing Networks with One-hop Inference on Atomic Formulas. In The EleventhInternational Conference on Learning Representations. Zihao Wang, Hang Yin, and Yangqiu Song. 2022. Benchmarking the CombinatorialGeneralizability of Complex Query Answering on Knowledge Graphs. Proceedingsof the Neural Information Processing Systems Track on Datasets and Benchmarks 1(NeurIPS Datasets and Benchmarks 2021) (2022). Chenyan Xiong, Russell Power, and Jamie Callan. 2017. Explicit semantic rankingfor academic search via knowledge graph embedding. In Proceedings of the 26thinternational conference on world wide web. 12711279. Wenhan Xiong, Thien Hoang, and William Yang Wang. 2017. DeepPath: AReinforcement Learning Method for Knowledge Graph Reasoning. In Proceedingsof the 2017 Conference on Empirical Methods in Natural Language Processing.564573.",
  "Zezhong Xu, Wen Zhang, Peng Ye, Hui Chen, and Huajun Chen. 2022. Neural-symbolic entangled framework for complex query answering. Advances in NeuralInformation Processing Systems 35 (2022), 18061819": "Bishan Yang, Scott Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. 2015.Embedding Entities and Relations for Learning and Inference in Knowledge Bases.In Proceedings of the International Conference on Learning Representations (ICLR)2015. Dong Yang, Peijun Qing, Yang Li, Haonan Lu, and Xiaodong Lin. 2022. GammaE:Gamma Embeddings for Logical Queries on Knowledge Graphs. In Proceedingsof the 2022 Conference on Empirical Methods in Natural Language Processing.745760.",
  "Hang Yin, Zihao Wang, Fei Weizhi, and Yangqiu Song. 2023. EFO-CQA: TowardsKnowledge Graph Complex Query Answering beyond Set Operation. (2023)": "Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He,Yanming Shen, and Tie-Yan Liu. 2021. Do transformers really perform badlyfor graph representation? Advances in Neural Information Processing Systems 34(2021), 2887728888. Wen Zhang, Bibek Paudel, Liang Wang, Jiaoyan Chen, Hai Zhu, Wei Zhang,Abraham Bernstein, and Huajun Chen. 2019. Iteratively learning embeddingsand rules for knowledge graph reasoning. In The world wide web conference.23662377.",
  "AMore Experimental Details": "Our code is implemented using PyTorch. We use NVIDIA TeslaA10 GPU (24GB) and NVIDIA GeForce RTX 3090 GPU (24GB) toconduct all of our experiments. We use the AdamW optimizer,whose weight decay is 1e-4, to tune the parameters. The learningrate is 5e-5 for NELL995 and FB15k-237 and 1e-4 for FB15k. Thebatch size is 512 for NELL995 and FB15k-237 and 1,024 for FB15k.The training epoch for all datasets is 100. Following CQD-CO and LMPNN , we choose ComplEx-N3 as the neurallink predictor for fair comparison. In this case, we consider thelogical message encoding function corresponding to Equations1013. For these equations, is a hyperparameter that needs tobe determined. In LMPNN application, for simplicity, LMPNN justlets 3 = 1 and then all denominators in these closed-formexpressions are 1, namely:",
  "(,, , 1) := .(24)": "To make a fair comparison with LMPNN, in CLMPT application,we also use the logical message encoding function defined inEquations 2124 to compute messages passed to the variable nodes.For the experiments (see .3.1) to evaluate how much unnec-essary computational costs can be avoided by conditional logicalmessage passing mechanism, we repeat the experiments three timeson an RTX 3090 GPU and then average the calculated Memory(see Equation 19) and Time (see Equation 20) metrics.",
  "BComparison with More Neural CQA Modelson Q2B Datasets": "To further evaluate the performance of CLMPT, we also considercomparing neural CQA models that cannot handle logical negationand are trained and evaluated on Q2B datasets , including GQE, Q2B , BIQE , PERM , kgTransformer , andSILR . Among these models, BIQE, kgTransformer, and SILRall use the transformer to encode complex queries directly.BIQE and SILR serialize the query graph and treat the CQA task asthe sequence learning task to solve. kgTransformer uses the pre-training and fine-tuning strategies based on Heterogeneous GraphTransformer (HGT) to answer complex queries. In essence,kgTransformer is also a transformer-based graph neural network.",
  "CLMPT-small(F)60.621.717.742.251.730.724.519.415.66.47.911.03.74.531.66.7CLMPT58.922.118.441.851.928.824.418.616.26.68.111.83.84.531.37.0": "We use the results reported in these papers for comparison.Since they report the Hits@3 results, for a fair comparison, wealso use Hits@3 as the evaluation metric, which calculates theproportion of correct answer entities ranked among the top 3.As shown in the results in , CLMPT reaches the best per-formance on average across all datasets. For those models that usethe transformer to encode the entire query graph directly, namelyBIQE, SILR, and kgTransformer, we speculate that their insufficientgraph inductive biases may negatively affect their performance. Toencode the entire query graph at once using the transformer, BIQEdesigns a special positional encoding scheme, and SILR designs aparticular graph serialization method and structural prompt. Thesedesigns can be viewed as introducing specific graph inductive biasesinto the transformer to encode the query graph. For kgTransformer,it relies primarily on the graph inductive biases of the HGT ar-chitecture. According to the progress of the graph transformers, graph inductive biases are critical to using the transformerto encode graph data. In particular, structural encoding is importantfor the graph transformers without message passing. Therefore, wespeculate that the graph inductive biases introduced by these CQAmodels may not be sufficient to encode the query graph that definescomplex logical dependencies. On the other hand, it is not clear howto introduce appropriate graph inductive biases for query graphsthat define first-order logical dependencies. By contrast, CLMPT,based on logical message passing, uses the transformer to aggregatemessages and update node embeddings rather than to encode theentire query graph. This difference makes CLMPT less dependenton graph inductive biases than the transformer-based CQA modelsmentioned above. However, it also suggests that appropriate graphinductive biases may enhance the performance of CLMPT. Thisextension can be explored in the future.",
  "CComparison with Symbolic IntegrationModels": "We discuss the differences between symbolic integration CQA mod-els and neural CQA models in . The model proposed inthis paper is a neural CQA model, so our comparisons and relateddiscussions are mainly carried out within the scope of neural CQAmodels. Since symbolic information can enhance the performanceof neural CQA models , in this section, we consider two repre-sentative symbolic integration models, GNN-QE and ENeSy, for comparison with CLMPT to show the potential. We listthe MRR results reported in their original papers. The results areshown in . It is found that CLMPT is also competitive evenwith the symbolic integration models on answering EPFO queries.Specifically, CLMPT achieves a better average performance thanthese symbolic integration models on EPFO queries for NELL995.For FB15k-237, CLMPT still has a gap in performance with GNN-QE, but it can outperform ENeSy on EPFO queries. For the negativequeries, there are still gaps between the neural CQA models andthese symbolic integration models because the fuzzy sets equippedwith symbolic information used in the reasoning process of thesemodels can effectively deal with logical negation through proba-bilistic values. These results suggest that neural models can be po-tentially improved with symbolic integration. However, it is worthnoting that introducing additional symbolic information requireslarger computational costs.According to , for larger knowledge graphs, neural CQAmodels and symbolic integration models have different scalabil-ities. As we discussed in , symbolic integration modelsrequire more computing resources and are more likely to sufferfrom scalability issues. For example, GNN-QE employs NBFNet to compute message passing on the whole KG, resulting in complex-ity that is linear to (|E| + |V|), where |E| is the number of edgesin KG, |V| is the number of nodes in KG, and is the embeddingdimension. For CLMPT, which only performs message passing onthe query graph, the complexity is just (). Besides, GNN-QE es-timates the probability of whether each entity is the answer at eachintermediate step, making the size of its fuzzy sets scale linearlywith |V|. As a result, GNN-QE requires much more computationalcosts, requiring 128GB GPU memory to run a batch size of 32. ForCLMPT-small(F), which has only 32M trainable parameters, it onlyrequires less than 12GB GPU memory to run a batch size of 1024.Even in this case, CLMPT-small(F) can still achieve performancecompetitive with GNN-QE on NELL995. We suspect that integratingsymbolic information into CLMPT can improve the performance,especially on negative queries. However, exploring how to integratesymbolic information into CLMPT is beyond the scope of this paper.This extension is left for future work.",
  "DLimitations": "Due to the use of the transformer architecture in the node em-bedding update scheme in CLMPT, more computational costs arerequired compared to some previous works based on multi-layerperceptron. In addition, CLMPT only achieves sub-optimal perfor-mance on negative queries. We suspect that integrating symbolicinformation into CLMPT may improve the performance on negativequeries."
}