{
  "ABSTRACT": "Discriminatory language and biases are often present in hate speechduring conversations, which usually lead to negative impacts ontargeted groups such as those based on race, gender, and religion.To tackle this issue, we propose an approach that involves a two-step process: first, detecting hate speech using a classifier, andthen utilizing a debiasing component that generates less biased orunbiased alternatives through prompts. We evaluated our approachon a benchmark dataset and observed reduction in negativity dueto hate speech comments. The proposed method contributes to theongoing efforts to reduce biases in online discourse and promote amore inclusive and fair environment for communication.",
  "INTRODUCTION": "In the era of social media and online platforms, communicationand idea exchange have reached at its peak. Despite many bene-fits, these platforms also facilitate the spread of hate speech andoffensive language. Hate speech often contains biases, perpetuat-ing stereotypes and discriminatory language, which exacerbatesthe negative impact of such content on different targeted groups(based on race, gender, religion) . Addressing these biases is acrucial step towards developing unbiased text processing systemsand fostering healthy online interactions.In this paper, we propose a debiasing technique that leverageslanguage generation and in-context prompting to minimizethe influence of lexical biases. A prompt is an instruction usuallyconsisting of a few words or sentences that provide context orconstraints for the model to follow . The method works by firstdetecting hate speech using a classifier, then employing a debiasing",
  "KDD 23, August 6-10, 2023, Long Beach, CA2023. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00": "component that generates less biased alternatives through incorpo-rating context-aware prompts designed to reduce the presence ofbiased language patterns.We evaluate our approach on benchmark dataset and demon-strate its effectiveness in debiasing hate speech texts. The resultsshow a classifier accuracy of 95% and debiasing accuracy of 89%,along with a notable reduction in negative sentiment within hatespeech comments. This method contributes to the ongoing effortsto reduce biases in online discourses.",
  "RELATED WORK": "Bias in language models and embeddings is a broad and subjectivetopic . Research has identified gender bias in popular embed-dings such as GloVe and Word2Vec and quantified biases usingthe word embedding association test (WEAT) . Efforts havebeen made to reduce biases in Transformer-based language modelslike BERT and GPT-3 and in conversational AI systems .Hate speech refers to the use of derogatory, abusive or threat-ening language towards individuals or groups based on their race,ethnicity, gender, religion, sexual orientation or any other character-istic. Several studies have focused on developing machine learningmodels for detecting hate speech in text . One work presentsa dataset, HOLISTIC BIAS, which consists of nearly 600 descriptorterms across 13 different demographic axes. The work demonstratesthat this dataset is highly effective for measuring previously unmea-surable biases in token likelihoods and generations from languagemodels, as well as in an offensiveness classifier.Another work quantifies sentiment bias through individualand group fairness metrics and proposes embedding and sen-timent prediction-derived regularization on the language modelslatent representations. The role of individual neurons and attentionheads in mediating gender bias across three datasets designed togauge a models sensitivity to gender bias are also studied .A related paper describes metrics for measuring politicalbias in language generation and proposes a reinforcement learn-ing framework for mitigating political biases in generated text.StereoSet , a large-scale natural English dataset to measurestereotypical biases in four domains: gender, profession, race, andreligion, is presented and it is shown that popular models like BERT,GPT-2, RoBERTa, and XLnet exhibit strong stereotypical biases.A novel approach to mitigate gender disparity in text generationby learning a fair model during knowledge distillation is presentedin a study and two modifications based on counterfactualrole reversal are proposedmodifying teacher probabilities andaugmenting the training set. A related work shows that GPT-3,a state-of-the-art contextual language model, captures persistent",
  "KDD 23, August 6-10, 2023, Long Beach, CASRaza, et al": "Muslim-violence bias, demonstrating that it appears consistentlyand creatively in different uses of the model. These biases becomeeven severe compared to biases about other religious groups.Prompt engineering has recently emerged as a promising ap-proach to mitigate biases in language models . In the contextof hate speech detection, few-shot learning can be useful inscenarios where there is limited labeled data available for a particu-lar language or dialect. Recent studies have shown promisingresults in using few-shot learning for hate speech detection, whichis also a motivation. In this work, we use the OPT based model fordebiasing but we introduce fairness aware prompts to achieve thegoal of debiasing the texts.",
  "METHODOLOGY3.1Hate speech classifier": "We utilize the BERT for building an efficient hate speech de-tection. The input to the BERT encoder consists of tokenized textsequences with special tokens [CLS] and [SEP] for classification andseparation. The output from the BERT encoder is passed througha SoftMax layer to provide a probability distribution for the textbeing classified as hate speech or non-hate speech. The hate speechclassifier is trained on a labeled dataset using binary cross-entropyloss as the objective function, which aims to minimize the differencebetween the predicted probability distribution and the true binarylabels, encouraging the model to accurately classify hate speechand non-hate speech comments.",
  "Debiasing model": "We use the pre-trained OPT (OpenAIs Pre-trained Transformer) model for debiasing hate speech classification. We employthe few-shot learning with prompts to further refine the debiasingprocess. Our approach to debiasing through prompts is inspired bysimilar works such as , however, we make our own changesthrough task-specific examples.To incorporate the debiasing prompts into the OPT model, weprovide the prompt, the input text, resulting in a new input se-quence. The OPT model processes this combined sequence andgenerates the debiased output based on the contextualized em-beddings of both the prompt and the input text. We adjust thetemperature of the OPT model during inference to control the levelof randomness and diversity in the generated text samples. Thishelps to mitigate biases while preserving the overall meaning ofthe input text.",
  "Pipeline": "Our pipeline stacks both the hate speech classification and debias-ing models as follows: (i) pass the input text through the hate speechclassifier to obtain the predicted probability of it being hateful ornon-hateful, and (ii) pass the hateful text through the debiasingmodel using few-shot learning with prompts to generate the de-biased text during the language generation task. This two-stageapproach first classifies the hate content in the input text, and if itis deemed hateful by the model, then it debiases it. We show ourpipeline approach in .",
  "EXPERIMENT AND RESULTS4.1Dataset": "In this study, we utilize the Hate Speech Dataset from a whitesupremacy online community . The dataset consists of a di-verse range of text samples, including both overt and subtle in-stances of hate speech, posing significant challenges for automatedclassification and debiasing models. The dataset contains a total of10,568 sentences, classified as conveying hate speech or not. Thesize of dataset, along with the average sentence length, word count,and vocabulary size for each class.",
  "MethodF1 Score SD": "Classifier performanceRule-based0.60 0.03SVM0.70 0.02BERT-d0.90 0.04RoBERTa-HS0.92 0.01Proposed0.95 0.05Debiasing performanceDebiased with zero-shot learning0.86 0.02Debiased with few-shot learning (5)0.88 0.03Debiased with few-shot learning (10)0.89 0.02 and over-sampling. We start by randomly removing instances fromthe NOHATE class to balance the representation. After this, weaugment the HATE class byduplicating instances using a methodlike SMOTE. This ensures both classes are equally represented,improving our models ability to learn from both hate speech andnon-hate speech instances.",
  "Hyperparameters and Evaluation": "We fine-tuned the BERT model for hate speech classification todevelop an efficient and accurate classifier. For the debiaser model,we use the GPT-2 -small model with 117M parameters. We tunedthe model temperature (0.1 - 1.0) in our debiaser model to balancediversity in the generated text samples . Additionally, we uti-lized few-shot learning with prompts to further refine the debiasingprocess. We used 5 and 10 examples per category in our few-shotlearning experiments. For the classification task and to train thewhole pipeline, we used 3 epochs, optimizing the parameters usingthe Adam optimizer with a learning rate of 5e-5, weight decay of0.5 and an epsilon value of 1e-8. We searched a grid of hyperpa-rameters, including batch sizes of 4, 8, and 16, 32, 64 . We limitedthe input sequences to 128 (subword) tokens and trained the modelin batches of 16 (to avoid out-of-memory issues). We employ F1score (by calculating the harmonic mean precision and recall) andaccuracy. For training, we used Google Colab Pro, which providedaccess to an NVIDIA Tesla T4 GPU with 16 GB of memory.",
  "Performance Evaluation": "presents the results of our proposed method for mitigatingbiases in hate speech classification. The classifiers performanceis evaluated using F1-score metrics. Further, we introduce a novelmeasure - the bias score - to assess the effectiveness of our debiasingmodel.In this study, our classifier generates a quantifiable bias scorefor each text, reflecting the degree of hate speech bias. Each textis scored both before and after the debiasing process. A decreasein this bias score post-debiasing is indicative of successful biasmitigation. This metric provides us a way to numerically gauge theeffectiveness of our proposed debiasing model.The results shows that the proposed method achieves the high-est F1 score of about 95%, outperforming all other classification methods. Moreover, the debiasing performance of the proposedmethod with few-shot learning and prompts achieves an impres-sive F1 score of 89%, with a low standard deviation. This indicatesthat the proposed method can effectively mitigate biases in hatespeech classification.To evaluate the performance of the debiaser model, we conductedexperiments with zero-shot and then 5 and 10 prompts with exam-ples, respectively. The results show that using few-shot learningwith prompts improves the F1 score compared to zero-shot learning.This indicates that the model is better able to mitigate biases in theinput text with the guidance of a few examples. This technique hasthe potential to enhance the performance of the debiasing modelfurther with more diverse and relevant prompt examples, whichcould be explored in future work.Classification performance: Next, we show the performanceof the classifier model on a sample of 100 examples and report theresults in . We observe in the , the model has made",
  ": Confusion matrix for hate speech detection": "45 true positive predictions, 5 false positive predictions, and 35true negative predictions. The false negative count is 15. The hightrue positive rate indicates that the model is correctly identifying amajority of the hate speech sentences, while the low false positiverate indicates that the model is not incorrectly labeling non-hatespeech sentences as hate speech. However, the false negative countis 15, indicating that there is still room for improvement in correctlyidentifying all instances of hate speech. Since, these results arebased on a sample of 100 instances and may not be representativeof the models performance on a larger dataset. Further evaluationand tuning of the model may be necessary to improve its overallperformance.Debiasing Performance: To further measure the effectivenessof our debiasing method, we measure and compare a range ofperformance metrics before and after debiasing. We first train ourclassifier on the original, biased dataset, and calculate the bias score.We then apply our debiasing process to the data, train the classifieron the debiased dataset, and again calculate the bias score. The",
  "False Positives Rate20%12%False Negatives Rate15%17%Debiasing performanceBias Score*65%35%Reduction in Bias Score-30%": "*Bias score represents the degree of bias in the model, with a higherpercentage indicating more bias. In this case, its a measure of how muchthe model is biased towards incorrectly classifying certain non-hatefulspeech as hate speech. change in the bias score serves as an indication of the effectivenessof our debiasing method.As shown in , our debiasing method successfully reducedthe bias score by 30%, from 65% to 35%. This indicates a signif-icant reduction in the models inclination to misclassify certainnon-hateful speech as hateful. However, these improvements wereaccompanied by a slight decrease in hate speech detection accuracy,with the accuracy dropping from 85% to 83%. Additionally, therewas a slight increase in the false negatives rate, which rose from 15%to 17%. These findings suggest that the debiasing caused slightlyless accuracy in identifying the hateful speech. On the other hand,the false positives rate decreased from 20% to 12%, indicating animprovement in correctly identifying non-hateful speech .",
  "CONCLUSION": "We propose a hate speech classifier and debiaser that employsprompts to generate less biased alternatives. Our approach firstdetects hate speech using a classifier and then utilizes a debiasingcomponent that generates less or no biased alternatives. Our pro-posed approach has some limitations. One limitation is the size andquality of the available training data, which may not be sufficientto cover all possible scenarios. Another limitation is the need forfine-tuning of the language model, which may require additionalresources and expertise. Furthermore, the effectiveness of our ap-proach may be affected by the quality of the underlying languagemodel used for generating alternative text. Nonetheless, further re-search is needed to address the complex issue of online hate speechand biased language comprehensively.",
  "Abubakar Abid, Maheen Farooqi, and James Zou. 2021. Persistent anti-muslimbias in large language models. In Proceedings of the 2021 AAAI/ACM Conferenceon AI, Ethics, and Society. 298306": "Aishwarya Anand, Murthy Devarakonda, Manik Gambhir, Arnav Wadhwa, andMark J Carman. 2021. Few-Shot Learning for Hate Speech Detection on SocialMedia. In Proceedings of the 1st Workshop on Online Abuse and Harms (WOAH2021). 3035. Soumya Barikeri, Anne Lauscher, Ivan Vuli, and Goran Glava. 2021. Reddit-Bias: A real-world resource for bias evaluation and debiasing of conversationallanguage models. arXiv preprint arXiv:2106.03521 (2021).",
  "arXiv:2010.14534 (2020). arXiv:2010.14534": "Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam TKalai. 2016. Man is to computer programmer as woman is to homemaker?debiasing word embeddings. Advances in neural information processing systems29 (2016). Aylin Caliskan, Joanna J Bryson, and Arvind Narayanan. 2017. Semantics derivedautomatically from language corpora contain human-like biases. In Proceedingsof the 2017 Conference on Empirical Methods in Natural Language Processing.20752086. Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017.Automated hate speech detection and the problem of offensive language. InEleventh International AAAI Conference on Web and Social Media.",
  "Tanmay Garg, Sarah Masud, Tharun Suresh, and Tanmoy Chakraborty. 2023.Handling Bias in Toxic Speech Detection: A Survey. Comput. Surveys (2023). arXiv:2202.00126": "Lara Grimminger and Roman Klinger. 2020. Hate Towards the Political Opponent:A Twitter Corpus Study of the 2020 {US} Elections on the Basis of Offensive Speechand Stance Detection. Proceedings of the Eleventh Workshop on ComputationalApproaches to Subjectivity, Sentiment and Social Media Analysis, 187197. Umang Gupta, Jwala Dhamala, Varun Kumar, Apurv Verma, Yada Pruksachatkun,Satyapriya Krishna, Rahul Gupta, Kai-Wei Chang, Greg Ver Steeg, and Aram Gal-styan. 2022. Mitigating gender bias in distilled language models via counterfactualrole reversal. arXiv preprint arXiv:2203.12574 (2022). Po-Sen Huang, Huan Zhang, Ray Jiang, Robert Stanforth, Johannes Welbl, JackRae, Vishal Maini, Dani Yogatama, and Pushmeet Kohli. 2019. Reducing sen-timent bias in language models via counterfactual evaluation. arXiv preprintarXiv:1911.03064 (2019). Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, andGraham Neubig. 2023. Pre-train, Prompt, and Predict: A Systematic Survey ofPrompting Methods in Natural Language Processing. Comput. Surveys 55, 9(2023). arXiv:2107.13586 Ruibo Liu, Chenyan Jia, Jason Wei, Guangxuan Xu, Lili Wang, and SoroushVosoughi. 2021. Mitigating political bias in language models through reinforcedcalibration. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35.1485714866. Emily May, Yao Wang, Shikha Bordia, Hannah Gao, Jilin Li, and Percy Liang. 2019.Measuring and mitigating unintended bias in text classification. In Proceedings ofthe AAAI Conference on Artificial Intelligence, Vol. 33. 74797486.",
  "Shaina Raza, Deepak John Reji, and Chen Ding. 2022. Dbias: detecting biasesand ensuring fairness in news articles. International Journal of Data Science andAnalytics (2022), 121": "Eric Michael Smith, Melissa Hall, Melanie Kambadur, Eleonora Presani, andAdina Williams. 2022. Im sorry to hear that: Finding New Biases in LanguageModels with a Holistic Descriptor Dataset. In Proceedings of the 2022 Conference onEmpirical Methods in Natural Language Processing. Association for ComputationalLinguistics, Abu Dhabi, United Arab Emirates, 91809211. Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo,Yaron Singer, and Stuart Shieber. 2020. Investigating gender bias in languagemodels using causal mediation analysis. Advances in neural information processingsystems 33 (2020), 1238812401. Heting Wu, Zhenxin Sun, Sheng Li, and Xiang Ren. 2022. Leveraging PromptEngineering and Counterfactual Data Augmentation for Fairer Language Mod-els. In Proceedings of the 2022 Conference of the North American Chapter of theAssociation for Computational Linguistics (NAACL-HLT)."
}