{
  "Microsoft, 1045 La Avenida, Mountain View, CA, 94043, USA": "AbstractThis paper presents AutoHint, a novel framework for automatic prompt engineering and optimization for Large LanguageModels (LLM). While LLMs have demonstrated remarkable ability in achieving high-quality annotation in various tasks,the key to applying this ability to specific tasks lies in developing high-quality prompts. Thus we propose a framework toinherit the merits of both zero-shot learning and few-shot learning by incorporating enriched instructions derived frominput-output demonstrations to optimize original prompt. We refer to the enrichment as the Hint and propose a framework toautomatically generate the hint from labeled data. More concretely, starting from an initial prompt, our method first instructsa LLM to deduce new hints for selected samples from incorrect predictions, and then summarizes from per-sample hints andadds the results back to the initial prompt to form a new, enriched instruction. The proposed method is evaluated on theBIG-Bench Instruction Induction dataset for both zero-shot and few-shot prompts, where experiments demonstrate that ourmethod is able to significantly boost accuracy for multiple tasks.",
  ". Introduction": "Large Language Models (LLM) have shown remarkableability to achieve comparable or even surpass human an-notation quality in various tasks , where high-qualityprompts are the key to activate such abilities. This hasentailed explorations from many directions with a largebody of studies on prompt engineering, including meth-ods built upon human instinct or domain knowledge ,data-driven approaches , or prompt optimization viain-context learning , etc.LLMs could be prompted by zero-shot or few-shotlearning. In the former, the LLM will be provided generalinstructions for the task at hand, as shown in the exam-ple in the left of . By contrast, under a few-shotsetting, LLM is provided with a number of input-outputpairs as demonstrations followed by an unseen input,and then is instructed to generate a corresponding out-put. Both of these two approaches come with their ownadvantages and disadvantages: the former exhibits bettergeneralization but may lack the necessary clarity andspecificity, since the instructions can be vague or provideonly general descriptions which is not easy for LLM tointerpret, whereas the latter one offers more detailed in-formation by providing demonstrations but is sensitive LLM4AI23: Workshop on Foundations and Applications in Large-scaleAI Models -Pre-training, Fine-tuning, and Prompt-based Learning, co-located with the 29TH ACM SIGKDD CONFERENCE ON KNOWLEDGEDISCOVERY AND DATA MINING (KDD), August 6-10, 2023, LongBeach, CA, USA (H. Sun); (X. Li)",
  "ISSN 1613-0073CEUR Workshop Proceedings (CEUR-WS.org)": "to sample selection or even sample ordering .The above analysis motivates us to seek an alternativeapproach by combining merits from both sides, via in-ducting enriched instructions from input-output demon-strations and then employing them to refine the originalinstruction. Throughout this paper, we will refer to theseenriched instructions as Hints. (right) showsan example of the hints generated for the Epistemic Rea-soning task in BIG-Bench Instruction Induction (BBII) dataset. The original description for this task is to Deter-mine whether one sentence entails the next, as highlightedin blue. In contrast, the hints generated by our method(highlighted in green) showcase a more elaborate instruc-tion, providing a breakdown of explanations for bothentailment and non-entailment cases. As a result, it pro-vides more clear information for LLM to interpret thetask and proceed with expected actions.Having the enriched hints is good, but having au-tomatically generated hints is better. That is why wepropose a framework called AutoHint which aims togenerate hints automatically. Our framework samplesfrom input-output pairs and leverages LLM for dueucinghints accordingly. The sampling is based on wronglyanswered samples, as it is fair to assume that the originalprompt has already conveyed sufficient information forthe correctly answered ones. Following that, we select asmall subset and prompt the LLM to summarize the hintsthat align best with that subset without including case-specific information. These hints are incorporated intothe original prompt to generate a refined version. As ourmethod is orthogonal to zero-shot and few-shot settings,we evaluate it under both settings on BBII dataset, where",
  ". Related Work": "Since the explosive arrival of LLMs , many works havestudied how to boost their capabilities to solve emergent,complex tasks, among which in-context learning andChain-of-Thought have proved to be effective two ofthe most effective methods. Along this line of research,different directions have been explored . Nevertheless, in most tasks the most effectiveprompts are still human-crafted, which greatly limits theapplication of LLM prompting.To fill this gap, there has been a growing interest inautomatic prompt engineering, with most prior attemptsrequiring access to the internal variables of LLMs ondifferent levels, either through the differentiable tuningof soft prompts or training auxiliary componentsor models for prompt optimization.However, with the growing trend of limited accessibilityto LLMs, such methods are becoming less feasible forgeneral practitioners.For that reason, recent works are witnessing a notice-able shift towards approaches that solely rely on feed-back from LLMs . More specifically, in-structs LLMs to deduce the task based on input-outputdemonstrations, and this idea is further extended into a generation-scoring-selection workflow in . In con-trast to existing works, our method enriches the generalinstructions obtained from , via exploiting com-plementary hints with enhanced clarity and specificityfrom input-output demonstrations. Therefore, our pro-posed method is orthogonal to existing works and can becombined with them to obtain potentially better results.",
  ". Overview": "Given a task with a training dataset containing i.i.d.samples as input-output demonstrations: ={(1, 1), , (, )}, our method first inferencesthe predictions with an initial prompt , and thenuses another prompt to deduce a reason or hint for each sample having incorrect predictions. We de-note the wrongly predicted samples as ={(1, 1, 1), , (, , )}, and then sample samples from to form a subset ={(1, 1, 1), , (, , )}. After that, we lever-age a prompt to summarize all the individual in into a final hint . Finally, a new prompt is generatedby merging learned into to form prompt +1. Theoverall procedure is summarized in Algorithm 1.",
  "(|(), )(1)": "where is the initial prompt constructed by concate-nating a very general task description with the sampledinput.Upon obtaining the inference results, we will onlyretain incorrectly predicted samples i.e., , forfurther processing. The reasons for this are twofold.First, proceeding with all training data would result insignificant costs. Second, we believe that the current",
  "= () + () + (),(3)": "where () denotes the encoder that transforms the tex-tual input , and into dense vectors, with , and denoting the combination weights. We use theBERT-base model as our encoder (), but other encoderscould be easily integrated into our framework.As will be discussed in , we use a validationset to examine different sampling strategies and selectthe best performing one, and then report its evaluationresults on the holdout test set.",
  ". Hints Summarization": "Given the hints generated from .3, we leverageanother prompt shown in to generate a sum-marized hint to add to the initial prompt , resulting inan enriched prompt +1. An example of the enrichedprompt with the summarized hints is shown in (right).The idea of summarizing individual hints here is con-ceptually analogous to the calculation involved in de-riving a mini-batch stochastic gradient descent (SGD),",
  ": Prompt template for summarizing hints": "where individual gradients within the mini-batch are ag-gregated into a single gradient. However, compared toits numerical counterpart, summarizing hints is muchmore difficult, since the former operates in a continuousspace with well-established numerical operations, whichis clearly not the case for hints summarization. In thispaper, we propose to transform this non-trivial task intoa tractable problem by leveraging the ability of LLMs tosolve emergent tasks. To our knowledge, this is the firstattempt to leverage LLMs to summarize hints or promptsin automatic prompt generation to make the result morestable and comprehensive.",
  ". Experimental Setup": "Dataset We examine our method on 6 tasks from the BIG-Bench Instruction Induction (BBII)1 dataset introducedin , including Epistemic Reasoning, Logical Fallacy De-tection, Implicatures, Hyperbaton, Causal Judgment andWinowhy. We specifically select these tasks based ontheir relatively large data volume and easy-to-parse an-swer format, which allows us to obtain higher accuracy.For each task, we randomly split the dataset into train-ing (60%), validation (20%) and test sets (20%), where thetraining set is used for inducing the hints as mentionedin , while the validation set is used for hyper-parameter tuning and prompt selection. After choosingthe best enriched prompt based on the validation setperformance, we report the final evaluations on the testset.Baselines and Metrics Since our proposed frame-work aims to exploit useful information on top of givenprompts, we employ the initial prompt as our base-line, which is the prompt retrieved from BBII datasetin our implementation. Given all the tasks are binary-classification tasks, we report Overall Accuracy (Acc.)and Balanced Accuracy (Bal Acc.) as our main evaluation metrics. Balanced Accuracy is averaged per-categoryaccuracy.Implementation details We use the Azure OpenAIAPI service (GPT-4) to evaluate both the baseline and ourmethod. For the inference and summarization prompts,we set temperature as 0 and topP as 1. For hint genera-tion, we set temperature as 0.1 and topP as 0.95 to allowsome exploration. We conduct hyper-parameter tuningto examine different sampling strategies. For each task,we test 20 sets of hyper-parameters on the validation setand select the best one to conduct evaluation on the testset. To save costs, we randomly sample a subset fromthe validation set for comparison if it is too large, thenkeep the top three settings to run a full validation setevaluation and select the final prompt for the test setevaluation.",
  ". Experiment Results and Analysis": "Our main results are summarized in , where weobserve significant improvement on both accuracy andbalanced accuracy on 5 out of the 6 tasks after adding thehints mined from the proposed method, demonstratingthe effectiveness of our proposed framework under zero-shot settings.We also look into the generated hints and observe thatfor most tasks, our method is able to mine useful defini-tions and explanations with more detailed informationafter the very first iteration. For the example in ,practical instructions are added for both the entailmentand non-entailment segments, assisting annotators in-cluding LLMs to better understand the task at hand.Regarding sampling strategies, Clustering achieves thebest result in 3 out of 6 tasks, Random-balanced leadsto the best result on the remaining tasks, while plainRandom sampling shows the worst results. This high-lights the importance of adopting proper sampling strate-gies for generating a good summary. Meanwhile, theremarkable performance of Random-balanced samplingalso echos our finding that our method is able to induceexplicit per-category explanations as shown in .In addition, the best results are achieved when no morethan 3 samples are used per label. This may be becausemore samples confuse the model when generating thesummary.Meanwhile, as our method is orthogonal to few-shortlearning, we also evaluate AutoHint in this setting, andthe results are summarized in , where our methodboosts accuracy significantly in 5 out of 6 tasks. Afteradding demonstrations randomly sampled from the train-ing set, we observe a decrease in accuracy on 4 out of 6tasks, along with notable performance fluctuation acrossdifferent sampling seeds. This observation aligns withprevious studies highlighting the importance ofcarefully selecting effective demonstrations to mitigate",
  ". More Iterations": "In addition, we also conduct experiments by running ad-ditional iterations to generate more informative prompts.New hints are appended to existing ones by a prefix Andadditional hint might be useful is. Overall, we observe asignificant boost on the average accuracy over promptcandidates on the validation set, this indicates our im-provement on the original prompt is solid. However, afterselecting the best prompt, only 3 tasks (Causal Judge-ment, Logical Fallacy Detection and Hyperbaton) showfurther improvement while the rest show accuracy de-cay. When manually checking the new hints, we findthat they indeed present complementary or more specificinformation to existing ones. For instance, new hintsfor the Epistemic Reasoning task focus on if the premise and hypothesis are identical or include additional details.While it is beneficial to infuse new, correct information,it places higher demands on the ability of LLM to com-prehend these diverse hints. Our current approach toincorporating hints from later iterations is simplistic,and as a result, it hinders the language model from effec-tively assimilating the more informative instructions. Asa result, it requires a more effective strategy for merginginformation from different iterations. Therefore, we planto address this as part of our future work.",
  "TaskFirst iterationSecond iterationAcc.Bal Acc.Acc.Bal Acc": "Epistemic Reasoning90.590.1588.288.2Logical Fallacy Detection84.0383.6687.6887Implicatures91.9391.9292.9291.91Hyperbaton82.4182.4190.5690.56Causal Judgment65.7965.7971.0571.05Winowhy73.1271.7873.171.8 optimize costs for steps 2 and 4. For step 2, we can samplea subset first as the final data points needed to obtain thesummary is small. For the last step, we can evaluate on asubset if the validation set is large, or similar to APE ,iteratively prune the prompts.",
  ". Conclusion": "We propose a novel framework for automatic promptoptimization by combining merits from zero-shot andfew-shot learning. We evaluate our method on the BBIIdataset under both settings, and demonstrate the effec-tiveness of the proposed method. Our proposed methodcould also be combined with existing methods to achievepotentially better results. While we didnt deploy thismethod in our production system yet, we are able toleverage this method for reducing some manual analysiseffort in prompt optimization. There are more discus-sions along this work like more-representative sampleselection, merging information from different iterationsto generate better prompt and cost saving, which weleave them for future work.",
  "F. Gilardi, M. Alizadeh, M. Kubli,Chatgpt out-performs crowd-workers for text-annotation tasks,arXiv preprint arXiv:2303.15056 (2023)": "T. Wu, M. Terry, C. J. Cai, Ai chains: Transparentand controllable human-ai interaction by chaininglarge language model prompts, in: Proceedingsof the 2022 CHI Conference on Human Factors inComputing Systems, 2022, pp. 122. Y. Zhou, A. I. Muresanu, Z. Han, K. Paster, S. Pitis,H. Chan, J. Ba, Large language models are human-level prompt engineers, in: The Eleventh Inter-national Conference on Learning Representations,2023.",
  "W. Xu, A. Banburski-Fahey, N. Jojic, Reprompt-ing: Automated chain-of-thought prompt infer-ence through gibbs sampling,arXiv preprintarXiv:2305.09993 (2023)": "L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wain-wright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama,A. Ray, et al., Training language models to followinstructions with human feedback, Advances inNeural Information Processing Systems 35 (2022)2773027744. J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia,E. H. Chi, Q. V. Le, D. Zhou, et al., Chain-of-thoughtprompting elicits reasoning in large language mod-els, in: Advances in Neural Information ProcessingSystems, 2022. K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen,H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton,R. Nakano, et al., Training verifiers to solve mathword problems, arXiv preprint arXiv:2110.14168(2021).",
  "E. Zelikman, Y. Wu, J. Mu, N. Goodman, Star: Boot-strapping reasoning with reasoning, in: Advancesin Neural Information Processing Systems, ????": "D. Zhou, N. Schrli, L. Hou, J. Wei, N. Scales,X. Wang, D. Schuurmans, O. Bousquet, Q. Le, E. Chi,Least-to-most prompting enables complex reason-ing in large language models,arXiv preprintarXiv:2205.10625 (2022). P. Lu, L. Qiu, K.-W. Chang, Y. N. Wu, S.-C. Zhu,T. Rajpurohit, P. Clark, A. Kalyan,Dynamicprompt learning via policy gradient for semi-structured mathematical reasoning, arXiv preprintarXiv:2209.14610 (2022). G. Qin, J. Eisner, Learning how to ask: Queryinglms with mixtures of soft prompts, in: Proceed-ings of the 2021 Conference of the North AmericanChapter of the Association for Computational Lin-guistics: Human Language Technologies, 2021, pp.52035212. B. Lester, R. Al-Rfou, N. Constant, The power ofscale for parameter-efficient prompt tuning, in:Proceedings of the 2021 Conference on EmpiricalMethods in Natural Language Processing, 2021, pp.30453059."
}