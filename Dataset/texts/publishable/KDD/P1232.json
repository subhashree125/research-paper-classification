{
  "ABSTRACT": "Graph structure learning is a well-established problem that aims atoptimizing graph structures adaptive to specific graph datasets tohelp message passing neural networks (i.e., GNNs) to yield effectiveand robust node embeddings. However, the common limitationof existing models lies in the underlying closed-world assumption:the testing graph is the same as the training graph. This premiserequires independently training the structure learning model fromscratch for each graph dataset, which leads to prohibitive computa-tion costs and potential risks for serious over-fitting. To mitigatethese issues, this paper explores a new direction that moves forwardto learn a universal structure learning model that can generalizeacross graph datasets in an open world. We first introduce themathematical definition of this novel problem setting, and describethe model formulation from a probabilistic data-generative aspect.Then we devise a general framework that coordinates a singlegraph-shared structure learner and multiple graph-specific GNNsto capture the generalizable patterns of optimal message-passingtopology across datasets. The well-trained structure learner candirectly produce adaptive structures for unseen target graphs with-out any fine-tuning. Across diverse datasets and various challeng-ing cross-graph generalization protocols, our experiments showthat even without training on target graphs, the proposed model i)significantly outperforms expressive GNNs trained on input (non-optimized) topology, and ii) surprisingly performs on par withstate-of-the-art models that independently optimize adaptive struc-tures for specific target graphs, with notably orders-of-magnitudeacceleration for training on the target graph.",
  "Corresponding author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 23, August 610, 2023, Long Beach, CA, USA 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0103-0/23/08...$15.00",
  "INTRODUCTION": "Graph neural networks (GNNs) , as a de facto modelclass based on the message passing principle, have shown promisingefficacy for learning node representations for graph-structured data,with extensive applications to, e.g., physics simulation , trafficprediction , drug recommendation . However, due to theinevitable error-prone data collection , the input graph maycontain spurious and unobserved edges that lead to sub-optimalresults of GNNs and degrade the downstream performance.Graph structure learning serves as a plausible remedy forsuch an issue via optimizing graph structures and GNN classifiersat the same time. To this end, recent endeavors explore differenttechnical aspects, e.g., parameterizing each potential edge betweenany pair of nodes or estimating potential links through aparameterized network , etc. However, existing modelslimit their applicability within a closed-world hypothesis: the train-ing and testing of structure learning models, which optimize thegraph structures, are performed on the same graph. The issue, how-ever, is that since structure learning is often heavy-weighted andrequires sophisticated optimization, it can be prohibitively resource-consuming to train structure learning models from scratch for eachgraph dataset. Moreover, due to limited labels in common graph-based predictive tasks, structure learning models are prone to over-fitting given that they cannot utilize the common knowledge sharedacross different graph datasets.To resolve the above dilemma, this paper attempts to explore anovel problem setting termed Open-World Graph Structure Learn-ing. Specifically, we target learning a generalizable graph structurelearning model which is trained with multiple source graphs andcan be directly adapted for inference (without re-training or fine-tuning) on new unseen target graphs. We formulate the problem",
  "Predictions": "......... GNN : predict on test set: update on target graphs: update on source graphs sharedparameters : Illustration of Open-World Graph Structure Learn-ing. In a diverse set of source graphs, we train multipledataset-specific GNNs and a shared structure learner. In thetarget graph, we directly utilize the learned structure learnerand only need to train a new GNN. as a bi-level optimization target that jointly learns a single dataset-shared structure learner and multiple dataset-specific GNNs tai-lored for particular graph datasets, as shown in . Under sucha framework, the well-trained structure learner can leverage thecommon transferrable knowledge across datasets for enhancinggeneralization and more critically, be readily utilized to yield adap-tive message-passing topology for arbitrarily given target graphs.With the guidance of the aforementioned general goal, we pro-pose GraphGLOW (short for A Graph Structure Learning Model forOpen-World Generalization) that aims at learning the generalizablepatterns of optimal message-passing topology across source graphs.Specifically, we first take a bottom-up perspective and formulatethe generative process for observed data in a probabilistic manner.On top of this, we derive a tractable and feasible learning objectivethrough the lens of variational inference. The structure learner isspecified as a multi-head weighted similarity function so as to guar-antee enough expressivity for accommodating diverse structuralinformation, and we further harness an approximation scheme toreduce the quadratic complexity overhead of learning potentialedges from arbitrary node pairs.To reasonably and comprehensively evaluate the model, we de-vise experiments with a diverse set of protocols that can measurethe generalization ability under different difficulty levels (accordingto the intensity of distribution shifts between source graphs andtarget graphs). Concretely, we consider: 1) In-domain generaliza-tion, in which we generalize from some citation (social) networks toother citation (social) networks. 2) Cross-domain networks gener-alization between citation and social networks. The results, whichare consistent across various combinations of source and targetgraph datasets, demonstrate that when evaluated on the targetgraphs, our approach i) consistently outperforms directly trainingthe GNN counterpart on original non-optimized graph structuresof the target datasets and ii) performs on par with state-of-the-artstructure learning methods trained on target graphs fromscratch with up to 25 less training time consumed. Our code isavailable at",
  "PRELIMINARY AND PROBLEM DEFINITION": "Node-Level Predictive Tasks. Denote a graph with nodes asG = (A, X, Y) where A = {} is an adjacency matrix ( =1 means the edge between node and exists and 0 otherwise),X = {x} is a feature matrix with x a -dimensional nodefeature vector of node , and Y = {} with the label vectorof node and class number. The node labels are partially observedas training data, based on which the node-level prediction aims topredict the unobserved labels for testing nodes in the graph usingnode features and graph structures. The latter is often achievedvia a GNN model, denoted as , that yields predicted node labelsY = (A, X) and is optimized with the classification loss =arg min = L( Y, Y) using observed labels from training nodes.Closed-World Graph Structure Learning (GLCW). The stan-dard graph structure learning for node-level predictive tasks trainsa graph structure learner to refine the given structure, i.e.,A = (A, X), over which the GNN classifier conducts mes-sage passing for producing node representations and predictions.The is expected to produce optimal graph structures that cangive rise to satisfactory downstream classification performance ofthe GNN classifier. Formally speaking, the goal for training alongwith can be expressed as a nested optimization problem:",
  "= arg minminL (( (A, X), X), Y) .(1)": "The above formulation of graph structure learning under closed-world assumptions constrains the training and testing nodes in thesame graph, which requires to be trained from scratch on eachgraph dataset. Since is often much more complicated (e.g., withorders-of-magnitude more trainable parameters) and difficult foroptimization (due to the bi-level optimization (1)) than the GNN ,the GLCW would lead to undesired inefficiency and vulnerabilityfor serious over-fitting (due to limited labeled information).Open-World Graph Structure Learning (GLOW). In thiswork, we turn to a new learning paradigm that generalizes graphstructure learning to open-world assumptions, borrowing the con-cepts of domain generalization and out-of-distribution gen-eralization , more broadly. Specifically, assume that we are givenmultiple source graphs, denoted as {G}=1 = {(A, X, Y)}=1,and a target graph G = (A, X, Y), whose distribution is oftendifferent from any source graph. The goal is to train a universalstructure learner on source graphs which can be directly usedfor inference on the target graph without any re-training or fine-tuning. The trained structure learner is expected to produce desiredgraph structures that can bring up better downstream classificationof a GNN classifier optimized for the target graph.More specifically, we consider a one-to-many framework that co-ordinates a shared graph structure learner and multiple dataset-specific GNNs { }=1, where with independent parame-terization is optimized for a given source graph G. With theaim of learning a universal that can generalize to new unseentarget graphs, our training goal can be formulated as the followingbi-level optimization problem:",
  "(e) Transfer on target graphs": ": Illustration of the proposed framework GraphGLOW targeting open-world graph structure learning. The middlepart of the figure presents the training process for the structure learner together with multiple dataset-specific GNNs onsource graphs. In (a)-(e) we illustrate the details of graph structure learner, backbone GNN, iterative training process, trainingprocedure and transferring procedure. When the training is finished, the structure learner is fixed and we only need to train adataset-specific GNN network on new target graph with latent structures inferred by the well-trained structure learner. where the inner optimization is a multi-task learning objective. Gen-erally, (2) aims at finding an optimal that can jointly minimizethe classification loss induced by GNN models, each trained fora particular source graph. After training, we can directly adapt to the target graph for testing purpose, and only need to train aGNN on the target graph:",
  "PROPOSED MODEL": "To handle the above problem, we present an end-to-end learningframework GraphGLOW that guides the central graph structurelearner to learn adaptive message-passing structures exploited bymultiple GNNs. The overview of GraphGLOW is shown in .The fundamental challenge of GLOW lies in how to model andcapture the generalizable patterns among adaptive structures ofdifferent graphs. To this end, we first take a data-generative per-spective that treats the inputs and inter-mediate results as randomvariables and investigate into their dependency, based on whichwe present the high-level model formulation in a probabilistic form(Sec. 3.1). Then we proceed to instantiate the model components(Sec. 3.2). Finally, we discuss differentiable training approaches foroptimization (Sec. 3.3).",
  "Model Formulation": "To commence, we characterize the data generation process by alatent variable model, based on which we derive the formulation ofour method. We treat the latent graph A (given by ) as a latentvariable whose prior distribution is given by ( A|A, X). The priordistribution reflects how one presumed on the latent structuresbefore observed labels arrive. Then, the prediction is given by a predictive distribution (Y| A, A, X). The learning objective aimsat maximizing the log-likelihood of observed labels, which canbe written as: log(Y|A, X) = logA (Y|A, X, A)( A|A, X) A. Toestimate latent graphs that could enhance message passing fordownstream tasks, one plausible way is to sample from the posterior,i.e., ( A|Y, A, X), conditioned on the labels from downstream tasks.Using the Bayes rule, we have",
  "( A|Y, A, X) =(Y|A, X, A)( A|A, X)A (Y|A, X, A)( A|A, X) A.(4)": "However, the integration over A in the denominator is intractablefor computation due to the exponentially large space of A.To circumvent the difficulty, we can introduce a variational dis-tribution( A|A, X) over A as an approximation to ( A|Y, A, X). Wecan sample latent graphs from ( A|A, X), i.e., instantiate it as thestructure learner , and once ( A|A, X) = ( A|Y, A, X), we couldhave samples from the posterior that ideally generates the optimalgraph structures for downstream prediction. By this principle, wecan start with minimizing the Kullback-Leibler divergence between and and derive the learning objective as follows:",
  ".(6)": "The equality holds if and only if D(( A|A, X)( A|Y, A, X)) = 0.The above fact suggests that we can optimize the ELBO as a surro-gate for log(Y|A, X) which involves the intractable integration.More importantly, when the ELBO is optimized w.r.t. distribution,the variational bound is lifted to the original log-likelihood andone has ( A|A, X) = ( A|Y, A, X), i.e., the variational distributionequals to the true posterior, which is what we expect.Pushing further and incorporating source graphs G (we omitthe superscript for simplicity), we arrive at the following objective:",
  "+ log0( A|A = A, X = X) log ( A|A = A, X = X).(7)": "Here we instantiate ( A|A, X) as the shared structure learner ,( A|A, X) as a (shared) non-parametric prior distribution 0 for la-tent structures, and (Y|A, X, A) as the dataset-specific GNN model, to suit the framework for our formulated problem in Sec-tion 2. The formulation of (7) shares the spirits with Bayesian metalearning . We can treat the GNN training as a dataset-specificlearning task and latent graph as a certain learning algorithm orhyper-parameter, so (7) essentially aims at learning a structurelearner that can yield desirable learning algorithm for each specificlearning task on graphs. Furthermore, the three terms in (7) havedistinct effects: i) the predictive term log acts as a supervisedclassification loss; ii) the prior term log0 serves for regularizationon the generated structures; iii) the third term, which is essentiallythe entropy of , penalizes high confidence on certain structures.To sum up, we can optimize (7) with joint learning of the struc-ture learner and GNN models { }=1 on source graphs{G}=1 for training the structure learner. After that, we cangeneralize the well-trained to estimate latent graph structuresfor a new target graph G = (A, X) and only need to train theGNN model w.r.t. the predictive objective with fixed :",
  "Model Instantiations": "3.2.1Instantiation for ( A|A, X). The variational distributionaims at learning the conditional distribution that generates suitablelatent structures for message passing based on input observations.A natural means is to assume each edge of the latent graph as aBernoulli random variable and the distribution is a product of independent Bernoulli random variables .The graph structure learner can be used for predicting theBernoulli parameter matrix. To accommodate the information fromnode features and graph structure, we can use the node representa-tion, denoted as z R, where is the embedding dimension, to",
  ",(9)": "where (, ) is a similarity function for two vectors, denotesHadamard product, is a function that converts the input into val-ues within and w1, w2 R are two weight vectors of the-thhead. Common choices for (, ) include simple dot-product, cosinedistance , RBF kernel , etc. Here we introduce heads andaggregate their results to enhance models expressiveness for cap-turing the underlying influence between nodes from multifacetedcauses, following the spirit of multi-head attention . Besides,the weight vectors in (9) could learn to element-wisely scale theinput vectors, i.e., node representations, and adaptively attend todominant features. Apart from these, two weight vectors w1, w2with independent parameterization could potentially have the sameor distinct directions, which makes the model capable of connectingsimilar or dissimilar nodes and expressive enough to handle bothhomophilous and non-homophilous graphs.To obtain discrete latent graph A = { } , one can samplefrom () to obtain each latent edge. However,such an approach induces the quadratic algorithmic complexity( 2) for computing and storing an estimated structure that entailspotential links between any node pair, which could be prohibitivefor large graphs. To reduce space and time complexity, we adopta pivot-based structure learning method, as shown in . Con-cretely, we randomly choose nodes in the graph as pivots, where is a hyperparameter much smaller than (e.g., 110). Wethen leverage pivot nodes as intermediates and convert the graph A, which can be prohibitively large with dense edges, into acascade of one node-pivot bipartite graph B1 and one pivot-node bipartite graph B2 = B1 , which can effectively controlthe computational cost with proper . In this way, we can computea node-pivot similarity matrix = {} based on (9), to pa-rameterize the distribution of latent graph structures. This onlyrequires () time and space complexity, and one can samplefrom each () to obtain B1 and B2. In the meanwhile,the original adjacency matrix could be retrieved by A = B1 B2,which suggests that one can execute message passing on B1 andB2 to approximate that on A (see more details in .2.2).In terms of the acceleration of structure learning, other strategieslike the all-pair message passing schemes with linear complexityexplored by can also be utilized to achieve the purpose. 3.2.2Instantiation for (Y|A, X, A). The predictive distribution,parameterized by the GNN network , aims at recursively prop-agating features along the latent graph to update node representa-tions and producing the prediction result for each node. We thenpresent the details of GNNs message passing on the latent graphin order for enough expressiveness, stability and efficiency.To begin with, we review the message-passing rule in commonGNN models, like GCN , that operates on the original graph A:",
  "Pivots": ": Illustration for scalable structure learning messagepassing, which reduces algorithmic complexity from ( 2)to (). We choose nodes as pivots and convert the matrix to the product of two node-pivot matrices (wherethe message passing is executed with two steps, i.e., node-to-pivot and pivot-to-node.). where W() R is a weight matrix, is non-linear activation,and D denotes a diagonal degree matrix from input graph A andZ() = {z() } is a stack of node representations at the -th layer.With the estimated latent graph A = B1 B2, we perform messagepassing MP2() in a two-step fashion to update node representa-tions:",
  "where C(+ 1": "2 ) is an intermediate node representation and ={} is the node-pivot similarity matrix calculated by (9).Such a two-step procedure can be efficiently conducted within() time and space complexity.Despite that the feature propagation on the estimated latentstructure could presumably yield better node representations, theoriginal input graph structures also contain useful information,such as effective inductive bias . Therefore, we integrate twomessage-passing functions to compute layer-wise updating for noderepresentations: Z(+1) = MP1(Z(), A)W() + (1 )MP2(Z(), A)W(),(12)where is a trading hyper-parameter that controls the concentra-tion weight on input structures. Such design could also improvethe training stability by reducing the impact from large variationof latent structures through training procedure.With GNN layers, one can obtain the prediction Y by settingY = Z() and W(1) R where is the number of classes.Alg. 1 shows the feed-forward computation of message passing. 3.2.3Instantiation for 0( A|A, X). The prior distribution reflectshow we presume on the latent graph structures without the in-formation of observed labels. In other words, it characterizes howlikely a given graph structure could provide enough potential forfeature propagation by GNNs. The prior could be leveraged forregularization on the estimated latent graph A. In this considera-tion, we choose the prior as an energy function that quantifies thesmoothness of the graph:",
  ",(13)": "where is the Frobenius norm. The first term in (13) mea-sures the smoothness of the latent graph , with the hypothesisthat graphs with smoother feature has lower energy (i.e., higherprobability). The second term helps avoiding too large node de-grees . The hyperparameters and control the strength forregularization effects.While we can retrieve the latent graph via A = B1 B2, the compu-tation of (13) still requires ( 2) cost. To reduce the overhead, weapply the regularization on the pivot-pivot adjacency matrixE = B2 B1 as a proxy regularization:",
  "For optimization with (7), we proceed to derive the loss functionsand updating gradients for and based on the three termsE [log], E [log0] and E [log]": "3.3.1Optimization for E [log]. The optimization difficultystems from the expectation over , where the sampling process isnon-differentiable and hinders back-propagation. Common strate-gies for approximating the sampling for discrete random variablesinclude Gumbel-Softmax trick and REINFORCE trick .However, both strategies yield a sparse graph structure each time ofsampling, which could lead to high variance for the prediction resultlog (Y|A, X, A) produced by message passing over a sampledgraph. To mitigate the issue, we alternatively adopt the NormalizedWeighted Geometric Mean (NWGM) to move the outer expec-tation into the feature-level. Specifically, we have (see Appendix Afor detailed derivations)",
  "log (Y|A, X, A = E ( A|A,X) [ A]).(15)": "We denote the opposite of the above term as L (). The gradientw.r.t. can be similarly derived. The above form is a biased esti-mation for the original objective, yet it can reduce the variance fromsampling and also improve training efficiency (without the needof message passing over multiple sampled graphs).(15) induces thesupervised cross-entropy loss. 3.3.2Optimization for E [log0]. As for the second term in (7),we adopt the REINFORCE trick, i.e., policy gradient, to tackle thenon-differentiability of sampling from. Specifically, for each feed-forward computation, we sample from the Bernoulli distributionfor each edge given by the estimated node-pivot similarity matrix,i.e., (), and obtain the sampled latent bipartite graphB1 and subsequently have E = B1 B2 = B1 B1 . The probability forthe latent structure could be computed by",
  "where we again adopt the node-pivot similarity matrix as a proxyfor the estimated latent graph": "3.3.4Iterative Structure Learning for Acceleration. A straightfor-ward way is to consider once structure inference and once GNNsmessage passing for prediction in each feed-forward computation.To enable structure learning and GNN learning mutually reinforceeach other , we consider multiple iterative updates of graphstructures and node representations before once back-propagation.More specifically, in each epoch, we repeatedly update node repre-sentations Z (where the superscript denotes the -th iteration)and latent graph A until a given maximum budget is achieved. Toaccelerate the training, we aggregate the losses L in each iterationstep for parameter updating. As different graphs have differentfeature space, we utilize the first layer of GNN as an encoder atthe very beginning and then feed the encoded representations tostructure learner. The training algorithm for structure learner on source graphs is described in Alg. 2 (in the appendix) where wetrain structure learner for multiple episodes and in each episode,we train on each source graph for several epochs. In testing, thewell-trained is fixed and we train a GNN on the target graphwith latent structures inferred by , as described in Alg. 3.",
  "RELATED WORKS": "Graph Neural Networks. Graph neural networks (GNNs) have achieved impressive performances in modelinggraph-structured data. Nonetheless, there is increasing evidencesuggesting GNNs deficiency for graph structures that are inconsis-tent with the principle of message passing. One typical situation liesin non-homophilous graphs , where adjacent nodes tend to havedissimilar features/labels. Recent studies devise adaptive featurepropagation/aggregation to tackle the heterophily .Another situation stems from graphs with noisy or spurious links,for which several works propose to purify the observed structuresfor more robust node representations . Our work is relatedto these works by searching adaptive graph structures that is suit-able for GNNs message passing. Yet, the key difference is that ourmethod targets learning a new graph out of the scope of input one,",
  "while the above works focus on message passing within the inputgraph": "Graph Structure learning. To effectively address the limitationsof GNNs feature propagation within observed structures, manyrecent works attempt to jointly learn graph structures and theGNN model. For instance, models each edge as a Bernoulli ran-dom variable and optimizes graph structures along with the GCN.To exploit enough information from observed structure for struc-ture learning, proposes a metric learning approach based onRBF kernel to compute edge probability with node representations,while adopts attention mechanism to achieve the similar goal.Furthermore, considers an iterative method that enables mutualreinforcement between learning graph structures and node em-beddings. Also, presents a probabilistic framework that viewsthe input graph as a random sample from a collection modeled bya parametric random graph model. harnesses variationalinference to estimate a posterior of graph structures and GNN pa-rameters. While learning graph structures often requires ( 2)complexity, a recent work proposes an efficient Transformerthat achieves latent structure learning in each layer with() com-plexity. However, though these methods have shown promisingresults, they assume training nodes and testing nodes are from thesame graph and consider only one graph. By contrast, we considergraph structure learning under the cross-graph setting and proposea general framework to learn a shared structure learner which cangeneralize to target graphs without any re-training. Out-of-Distribution Generalization on Graphs. Due to the demandfor handling testing data in the wild, improving the capabilityof the neural networks for performing satisfactorily on out-of-distribution data has received increasing attention . Recent studies, e.g., explore effective treatmentsfor tackling general distribution shifts on graphs, and there arealso works focusing on particular categories of distribution shiftslike size generalization , molecular scaffold generalization ,feature/attribute shifts , topological shifts , etc. To thebest of our knowledge, there is no prior works considering OODgeneralization in the context of graph structure learning. In ourcase, the target graph, where the structure learner is expected toyield adaptive structures, can have disparate distributions thanthe source graphs. The distribution shifts could potentially stemfrom feature/label space, graph sizes or domains (e.g., from socialnetworks to citation networks). As the first attempt along this path,our work can fill the research gap and enable the graph structurelearning model to deal with new unseen graphs in an open world.",
  "EXPERIMENTS": "We apply GraphGLOW to real-world datasets for node classificationto test the efficacy of proposed structure learner for boosting per-formance of GNN learning on target graphs with distribution shiftsfrom source graphs. We specify the backbone GNN network forGraphGLOW as a two-layer GCN . We focus on the followingresearch questions: 1) How does GraphGLOW perform compared with directly train-ing GNN models on input structure of target graphs?",
  "GraphGLOW: Universal and Generalizable Structure Learning for Graph Neural NetworksKDD 23, August 610, 2023, Long Beach, CA, USA": ": Test accuracy (%) on target graphs for in-domain generalizations. For each social network (resp. citation network) astarget dataset, we consider the other social networks (resp. citation networks) as source graphs. GraphGLOW* is an oraclemodel that shares the same architecture as our model GraphGLOW and is directly trained on target graphs.",
  "GraphGLOW*71.1 0.372.2 0.570.3 0.966.8 1.483.5 0.673.9 0.779.9 0.5": "2) How does GraphGLOW perform compared to state-of-the-artstructure learning models that are directly trained on target datasetsin terms of both accuracy and training time? 3) Are the proposed components of GraphGLOW effective andnecessary for the achieved performance? 4) What is the impact of hyper-parameter on performance andwhat is the impact of attack on observed edges? 5) What is the property of inferred latent graphs and what gener-alizable pattern does the structure learner capture?",
  "Experimental Protocols": "Datasets. Our experiments are conducted on several public graphdatasets. First we consider three commonly used citation networksCora, CiteSeer and PubMed. We use the same splits as in .These three datasets have high homophily ratios (i.e., adjacentnodes tend to have similar labels) . Apart from this, we alsoconsider four social networks from Facebook-100 , which havelow homophily ratios. Readers may refer to Appendix B for moredataset information like splitting ratios.Competitors. We mainly compare with GCN , the GNNcounterpart trained on input structure, for testing the efficacy ofproduced latent graphs by GraphGLOW. As further investigation,we also compare with other advanced GNN models: GraphSAGE, GAT , APPNP , H2GCN and GPRGNN . HereAPPNP, H2GCN and GPRGNN are all strong GNN models equippedwith adaptive feature propagation and high-order aggregation. Forthese pure GNN models, the training and testing are considered on(the same) target graphs. Furthermore, we compete GraphGLOWwith state-of-the-art graph structure learning models, IDS ,IDGL and VGCN . Since these models are all designed fortraining on one dataset from scratch, we directly train them on thetarget graph and they in principle could yield better performancethan GraphGLOW.We also consider variants of GraphGLOW as baselines. Wereplace the similarity function with attention-based structurelearner, denoted as GraphGLOWat, which follows the same trainingscheme as GraphGLOW. Besides, we consider some non-parametric similarity functions like dot-product, KNN and cosine distance (de-noted as GraphGLOWdp, GraphGLOWknn and GraphGLOWcos,respectively). For these models, we only need to train the GNNnetwork on target graphs with the non-parametric structure learn-ers yielding latent structures. In addition, we introduce a variantGraphGLOW* that shares the same architecture as GraphGLOWand is directly trained on target graphs. Also, GraphGLOW* inprinciple could produce superior results than GraphGLOW. We re-port the test accuracy given by the model that produces the highestvalidation accuracy within 500 training epochs.",
  "In-domain Generalization": "We first consider transferring within social networks or citationnetworks. The results are reported in where for each socialnetwork (resp. citation network) as the target, we use the othersocial networks (resp. citation networks) as the source datasets.GraphGLOW performs consistently better than GCN, i.e., the coun-terpart using observed graph for message passing, which provesthat GraphGLOW can capture generalizable patterns for desirablemessage-passing structure for unseen datasets that can indeed boostthe GCN backbones performance on downstream tasks. In particu-lar, the improvement over GCN is over 5% on Cornell5 and Reed98,two datasets with low homophily ratios (as shown in ). Thereason is that for non-homophilous graphs where the message pass-ing may propagate inconsistent signals (as mentioned in ),the GNN learning could better benefits from structure learning thanhomophilous graphs. Furthermore, compared to other strong GNNmodels, GraphGLOW still achieves slight improvement than thebest competitors though the backbone GCN network is less expres-sive. One could expect further performance gain by GraphGLOWif we specify the GNN backbone as other advanced architectures.In contrast with non-parametric structure learning models andGraphGLOWat, GraphGLOW outperforms them by a large marginthroughout all cases, which verifies the superiority of our designof multi-head weighted similarity function that can accommodatemulti-faceted diverse structural information. Compared with Graph-GLOW*, GraphGLOW performs on par with and even exceeds it on",
  "(e) Reed98": ": Comparison of test accuracy and training time with SOTA structure learning models (LDS , IDGL and VGCN ).The radius of circle is proportional to standard deviation. The experiments are run on one Tesla V4 with 16 GPU memory. Weadopt the same setting as and report the results on target datasets. For Cornell5 and PubMed, the competitor modelssuffer out-of-memory. Cornell5 and Amherst41. The possible reasons are two-fold. First,there exist sufficient shared patterns among citation networks (resp.social networks), which paves the way for successful generalizationof GraphGLOW. Second, GraphGLOW* could sometimes overfitspecific datasets, since the amount of free parameters are regularlyorders-of-magnitude more than the number of labeled nodes in thedataset. The results also imply that our transfer learning approachcan help to mitigate over-fitting on one dataset. Moreover, Graph-GLOW can generalize structure learner to unseen graphs that isnearly three times larger than training graphs, i.e., Cornell5.",
  "Cross-domain Generalization": "We next consider a more difficult task, transferring between socialnetworks and citation networks. The difficulty stems from twoaspects: 1) social networks and citations graphs are from distinctcategories thus have larger underlying data-generating distributiongaps; 2) they have varied homophily ratios, which indicates thatthe observed edges play different roles in original graphs. In we report the results. Despite the task difficulty, GraphGLOW man-ages to achieve superior results than GCN and also outperformsother non-parametric graph structure learning methods throughoutall cases. This suggests GraphGLOWs ability for handling targetgraphs with distinct properties. In we further compare GraphGLOW with three state-of-the-art graph structure learning models that are directly trained ontarget graphs. Here we follow the setting in . The resultsshow that even trained on source graphs that are different from thetarget one, GraphGLOW still performs on par with the competitorsthat are trained and tested on (the same) target graphs. Notably,GraphGLOW significantly reduces training time. For instance, inJohn Hopkins55, GraphGLOW is 6x, 9x and 40x faster than IDGL,LDS and VGCN, respectively. This shows one clear advantage ofGraphGLOW in terms of training efficiency and also verifies thatour model indeed helps to reduce the significant cost of trainingtime for structure learning on target graphs.",
  "Ablation Studies": "We conduct ablation studies to test the effectiveness of iterativelearning scheme and regularization on graphs.Effect of Iterative Learning. We replace the iterative learningprocess as a one-step prediction (i.e., once structure estimation andupdating node representations in once feed-forward computation)and compare its test accuracy with GraphGLOW. The results areshown in (a) where we follow the setting of . Thenon-iterative version exhibits a considerable drop in accuracy (aslarge as 5.4% and 8.8% when tested on target graphs Cornell5",
  "(b)": ": (a) The curves of homophily ratios for latent struc-tures during the learning process. (b) The variance of neigh-borhood distribution of nodes with the same label in originalgraphs and learnt structure. and Amherst41, respectively). Therefore, the iterative updates in-deed help to learn better graph structures and node embeddings,contributing to higher accuracy for downstream prediction.Effect of Regularization on structures. We remove the reg-ularization on structures (i.e., setting = = 0) and comparewith GraphGLOW. As shown in (a), there is more or lossperformance degradation. In fact, the regularization loss derivedfrom the prior distribution for latent structures could help to pro-vide some guidance for structure learning, especially when labeledinformation is limited.",
  "Hyper-parameter Sensitivity": "In (in the appendix), we study the variation of models perfor-mance w.r.t. (the weight on input graphs) and (the number ofpivots) on target datasets Cora and CiteSeer. Overall, the model isnot sensitive to s. For Cora, larger contributes to higher accu-racy, while for CiteSeer, smaller yields better performance. Thepossible reason is that the initial graph of Cora is more suitable formessage passing (due to higher homophily ratio). For the impact ofpivot number, as shown in (b), a moderate value of couldprovide decent downstream performance.",
  "Robustness Analysis": "In addition, we find that GraphGLOW is more immune to edgedeletion attack than GCN. We randomly remove 10-50% edges oftarget graphs respectively, and then apply GraphGLOW and GCN.We present the results in Johns Hopkins55 in (b) and leavemore results in Appendix D. When the drop ratio increases, the performance gap between two models becomes more significant.This is due to our structure learners ability for learning new graphstructures from node embeddings, making it less reliant on initialgraph structures and more robust to attack on input edges.",
  "Case Study": "We further probe into why our approach is effective for node clas-sification by dissecting the learnt graph structures. Specifically, wemeasure the homophily ratios of learnt structures and their vari-ance of neighborhood distributions of nodes with same labels. Asnodes receive messages from neighbors in message passing, themore similar the neighborhood patterns of nodes within one classare, the easier it is for GNNs to correctly classify them . We usehomophily metric proposed in to measure homophily ratios.For calculation of variance of neighborhood distribution, we firstcalculate variance for each class, and then take weighted sum to getthe final variance, where the weight is proportional to the numberof nodes within corresponding class.Homophily Ratio. We choose Amherst41, Johns Hopkins55and Reed98 as target graphs, and record the homophily ratios ofinferred latent structures every five epochs during training. Asshown in (a). the homophily ratios of inferred latent graphsexhibit a clear increase as the training epochs become more andthe final ratio is considerably larger than that of input graph. Theresults indicate that the trained structure learner incline to outputmore homophilous latent structures that are reckoned to be moresuitable for message passing.Neighborhood Distribution Variance. As shown in (b),the variance of neighborhood distribution of nodes with the samelabel is significantly smaller in our learnt structure, making it easierto classify nodes through message passing. The results also implythat high homophily ratio and similar intra-class neighborhoodpatterns could be two of the underlying transferrable patterns ofoptimal message-passing structure, identified by GraphGLOW.",
  "CONCLUSION": "This paper proposes Graph Structure Learning Under Cross-GraphDistribution Shift, a new problem that requires structure learnerto transfer to new target graphs without re-training and handlesdistribution shift. We develop a transfer learning framework thatguides the structure learner to discover shared knowledge acrosssource datasets with respect to optimal message-passing structurefor boosting downstream performance. We also carefully design themodel components and training approach in terms of expressive-ness, scalability and stability. We devise experiments with variousdifficulties and demonstrate the efficacy and robustness of our ap-proach. Although our framework is pretty general, we believe theirare other potential methods that can lead to equally competitiveresults, which we leave as future work. The work was supported in part by National Key Research and De-velopment Program of China (2020AAA0107600), NSFC (62222607),Science and Technology Commission of Shanghai Municipality(22511105100), and Shanghai Municipal Science and TechnologyMajor Project (2021SHZDZX0102).",
  "Luca Franceschi, Mathias Niepert, Massimiliano Pontil, and Xiao He. 2019. Learn-ing discrete structures for graph neural networks. In ICML": "Zhan Gao, Subhrajit Bhattacharya, Leiming Zhang, Rick S. Blum, AlejandroRibeiro, and Brian M. Sadler. 2021. Training Robust Graph Neural Networks withTopology Adaptive Edge Dropping. arXiv preprint arXiv:2106.02892 (2021). Haoyu Geng, Chao Chen, Yixuan He, Gang Zeng, Zhaobing Han, Hua Chai,and Junchi Yan. 2023. Pyramid Graph Neural Network: a Graph Sampling andFiltering Approach for Multi-scale Disentangled Representations. In SIGKDD.",
  "DatasetType# Node# EdgeHomo": "Coracitation2,7085,4290.77CiteSeercitation3,3274,7320.63PubMedcitation19,71744,3380.66Amherst41social2,23590,9640.06Cornell5social18,660790,7770.09Johns Hopkins55social5,180186,5860.10Reed98social96218,8120.04 The backbone GNN network is specified as a two-layer GCNmodel. We set the similarity function in (9) as cosine similarity and as a threshold-based truncation. Besides, since the dimensions ofinput node features are different across datasets, we adopt a transfor-mation network that converts input features into a -dimensionalnode representations before the structure learning module as shownin Alg. 2 (Z0 = MLP(X;) or Z0 = GCN(A, X;)). We canspecify the transformation as a one-layer MLP or a one-layer GCNnetwork (what we adopt). Most of the experiments were conductedon an NVIDIA GeForce RTX 2080 Ti with 11GB memory. For ex-periments involving two larger datasets, PubMed and Cornell5, weutilized an NVIDIA GeForce RTX 3090 with 24 GB memory.",
  "CHYPERPARAMETERS": "We use grid search on validation set to tune the hyperparameters.The learning rate is searched in {0.001, 0.005, 0.01, 0.05}; Dropoutis searched in {0, 0.2, 0.3, 0.5, 0.6}; Hidden channels is searchedin {16, 32, 64, 96}. Other hyperparameters for specific models arestated below.For GCN, GraphSAGE and H2GCN, we use 2 layers. For GAT,we search gat head number in {2, 4} and use 2 layers. For APPNPand GPR, we search in {0.1, 0.2, 0.5} and set to 10. We list thesearching space of structure learning methods below. GraphGLOW and its variants: pivot number {800, 1000,1200, 1400}, embedding size {16, 32, 64, 96}, [0.1, 0.9], {0, 0.1, 0.15, 0.2, 0.25, 0.3}, {0, 0.1, 0.15, 0.2, 0.25, 0.3},threshold {4e-5, 8.5e-5}, {4, 6}, = 10, {1, 2, 3}. LDS: the sampling time = 16, the patience window size {10, 20}, the hidden size {8, 16, 32, 64}, the inner learningrate {1e-4, 1e-3, 1e-2, 1e-1}, and the number of updatesused to compute the truncated hypergradient {5, 10, 15}.",
  "Cora83.2 0.482.5 0.5CiteSeer73.8 0.973.3 0.7PubMed79.6 0.779.6 0.7Amherst4168.1 1.368.3 1.5Johns Hopkins5571.8 0.772.1 0.9Cornell570.5 1.069.5 1.0Reed9867.3 1.265.9 1.4": "competence, which suggests that GraphGLOW is not sensitive tothe transformation network used for converting node features withvarious dimensions into embeddings with a shared dimension. Thisalso implies that simple neural architectures, e.g. MLP and GCN,could provide enough capacity for extracting the information ininput observation, which is leveraged by the shared graph struc-ture learner to discover generalizable patterns in optimal message-passing structure.We also provide more results of edge deletion experiments in . We randomly remove 10-50% edges of target graphs respectively,and then apply GraphGLOW and GCN. The results demonstratethat GraphGLOW is more immune to edge deletion. This is due toour structure learners ability for learning new structures, makingit less reliant on initial graph structures and more robust to attackon input edges."
}