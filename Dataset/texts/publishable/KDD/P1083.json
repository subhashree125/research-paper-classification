{
  "ABSTRACT": "Deep Recommender Systems (DRS) are increasingly dependent ona large number of feature fields for more precise recommendations.Effective feature selection methods are consequently becomingcritical for further enhancing the accuracy and optimizing storageefficiencies to align with the deployment demands. This researcharea, particularly in the context of DRS, is nascent and faces threecore challenges. Firstly, variant experimental setups across researchpapers often yield unfair comparisons, obscuring practical insights.Secondly, the existing literatures lack of detailed analysis on selec-tion attributes, based on large-scale datasets and a thorough com-parison among selection techniques and DRS backbones, restrictsthe generalizability of findings and impedes deployment on DRS.Lastly, research often focuses on comparing the peak performanceachievable by feature selection methods. This approach is typicallycomputationally infeasible for identifying the optimal hyperpa-rameters and overlooks evaluating the robustness and stability ofthese methods. To bridge these gaps, this paper presents ERASE, acomprehensive bEnchmaRk for feAture SElection for DRS. ERASEcomprises a thorough evaluation of eleven feature selection meth-ods, covering both traditional and deep learning approaches, acrossfour public datasets, private industrial datasets, and a real-worldcommercial platform, achieving significant enhancement. Our codeis available online1 for ease of reproduction.",
  "Authors contributed equally to this research.Corresponding author.1": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08.",
  "Benchmark, Feature Selection, Deep Recommender System": "ACM Reference Format:Pengyue Jia, Yejing Wang, Zhaocheng Du, Xiangyu Zhao, Yichao Wang,Bo Chen, Wanyu Wang, Huifeng Guo, and Ruiming Tang. 2024. ERASE:Benchmarking Feature Selection Methods for Deep Recommender Systems.In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discoveryand Data Mining (KDD 24), August 2529, 2024, Barcelona, Spain. ACM, NewYork, NY, USA, 12 pages.",
  "INTRODUCTION": "Recommender systems have become indispensable in many sectorsranging from e-commence to content streaming in the realm of in-formation explosion . With the application of deep learningtechniques, Deep Recommender System (DRS) exhibits amplifiedprediction ability of user preference, providing personalized expe-rience and dominating the deployment landscape .To improve the accuracy of recommendations, DRS is progres-sively integrating an expanding array of feature fields into theirpredictive models, which can number in the hundreds or even thou-sands . The significance of each feature varies, resulting in theaccumulation of superfluous or extraneous features. Consequently,feature selection, which concentrates on pinpointing and leverag-ing the most critical features, is becoming increasingly crucial inmodern DRS . One immediate benefit of feature selection isthe enhancement of prediction performance, achieved by eliminat-ing non-contributory features that could otherwise adversely affectpredictions. From an industrial viewpoint, selecting predictive fea-tures is also essential for meeting deployment criteria regardingmemory usage since unnecessary storage demands can often beinflated by the presence of redundant features.Hand-crafted feature selection usually requires lots of expertknowledge and labor efforts, which is usually infeasible to achieveoptimal results when the candidate set contains thousands of fea-tures. Researchers have designed various methods to automaticallyselect predictive features, including statistical methods ,learning methods , and agent-based methods .Despite the satisfactory results these methods achieved, they alsoreveal the following issues hindering the development of this field:",
  "KDD 24, August 2529, 2024, Barcelona, SpainPengyue Jia, et al": "Experimental Differences. Recent years have witnessed a va-riety of methods designed for DRS . How-ever, these works conduct experiments with varied settings. Forexample, MultiSFS is designed to select features for multi-task DRS. SHARK suggests the feature selection methodF-permutation and a quantization method. Optfs selects thefeature from the value-level while others mainly select from thefield-level. These experimental differences usually lead to unfairor unavailable comparisons, making the subsequent researchersstruggle to generate practical insights. Insufficiency. Existing feature selection benchmarks are pre-dominantly tailored for conventional downstream tasks, such asclassification . They are primarily built upon synthetic ordomain-specific datasets , which diverges significantlyfrom the complex, large-scale datasets encountered in DRS. Thisdivergence results in a notable deficiency in guiding feature se-lection specifically for DRS, due to the benchmarks disparatescope and focus. DeepLasso , while being among the bench-marks most closely related to DRS, primarily addresses tabularlearninga context that, despite similarities, does not fully alignwith the intricacies of recommendation tasks. Furthermore, suchbenchmarks often limit their exploration to traditional selectionmethods and rely on datasets of a much smaller scale, therebyomitting crucial, in-depth analysis from an industrial perspectivefor DRS. Moreover, while related literature surveys pro-vide comprehensive reviews of the field, they fall short in offeringempirical evidence or experimental results that could motivatepractical deployments in DRS, lacking the necessary data-drivensupport to inform and inspire future research directions. Assessment Deficit. A pivotal hyperparameter for feature se-lection methods is the number of features to be selected, denotedas in this study. Feature selection methods often exhibit signifi-cant variability in performance across different values of , andthe optimal is not the same for all methods . Thisvariability on introduces two significant challenges in evalu-ating feature selection methods. First, the direct comparison ofmethods at their respective optimal may not constitute a fairassessment. Such comparisons fail to account for the differentmemory requirements associated with varying optimal val-ues for different selection methods. Furthermore, this approachneglects the performance variability under sub-optimal hyperpa-rameter settings, thereby obscuring insights into the methodsrobustness and stability across a range of values. Second, theexhaustive search for the optimal across the entire spectrumof possible values is time-consuming and computationally in-tensive. This necessitates an evaluation methodology capable ofeffectively assessing a methods performance based on partial values, offering a more efficient means to gauge feature selectioneffectiveness without finishing complete iterations. To address these issues, we propose ERASE, a comprehensivebEnchmaRk for feAture Selection for DRS. ERASE initiates a uni-fied and fair experimental framework, minimizing experimentaldiscrepancies across various selection methods. Remarkably, ERASEpioneers as the first feature selection benchmark with a focus onDRS tasks, incorporating both prevalent DRS feature selection tech-niques and conventional methods. It introduces a novel taxonomy to classify these methods and unearth intrinsic patterns among groupsof methods. By evaluating the performance on widely used publicdatasets and authentic industrial production datasetsthrough bothoffline comparison and online testingERASE furnishes strong em-pirical support, facilitating the generation of actionable insights.In its endeavor to provide a comprehensive assessment of featureselection methods, ERASE contrasts the optimal performance ofcompared selection methods alongside their outcomes under spe-cific deployment prerequisites. Additionally, we introduce a novelmetric, AUKC, specifically crafted for assessing the robustness andstability of feature selection methods across a range of featurequantities , thereby addressing the critical lack of such evaluativemetrics. We summarize our major contributions as follows: We present ERASE, a comprehensive benchmark for DRS featureselection methods, providing a fair comparison for emergingselection techniques with various datasets and DRS backbones.To the best of our knowledge, we are the first to focus on bench-marking feature selection methods for recommendation tasks. We recognize the assessment shortfall linked to the substantialdependency of selection efficacy on the hyperparameter , andin response, we introduce a novel evaluation metric, AUKC. Thismetric is designed to evaluate the robustness and stability of fea-ture selection methods, bridging the existing gap in assessment. We carry out thorough experiments across four widely usedpublic datasets and real-world industrial production datasets,yielding insights from various angles. Notably, our experimen-tal findings have guided optimizations in our online platform,achieving a 20% reduction in latency without compromising ef-fectiveness, validating the practical utility of our benchmark.",
  "Datasets": "To comprehensively evaluate the effectiveness of different featureselection methods, we select four public datasets for our exper-iments. Avazu2 and Criteo3 are selected because they are fre-quently used dataset for studying feature selection in DRS . To compare the performance of different methods with asmall feature set, we choose the popular Movielens-1M4 datasetin the recommendation field. Additionally, to compare the per-formance of different methods in scenarios closer to real-worldrecommender systems, we supplement with the AliCCP5 dataset,which possesses user and item features and includes a rich set of85,316,519 interaction samples. The statistics of datasets and thedetailed introduction are illustrated in Appendix A.",
  ": Benchmark Overview": "methods. 1) Training strategy. Based on the training strategy,the methods can be single-stage or two-stage. Single-stage usuallydirectly integrates the feature selection module into the originalmodel without changing the training logic. In contrast, two-stagemethods contain searching and retraining phases. Informative fea-ture fields or feature values are selected in the searching phase,and the backbone model is trained with these selected featuresin the retraining phase. 2) Selection type. There are two typesof selection in feature selection methods: soft selection and hardselection. The soft selection offers a mask to affect inputs. Featuresmultiplied with 0 are considered filtered out. For the hard selection,features are removed directly from the inputs. 3) Selection tech-nique. Depending on the techniques used for selection, as shownin , we divide methods contained in our benchmark intothree categories: shallow feature selection, gate-based selection,and sensitivity-based selection.Due to the unbalanced distribution of feature selection methodsclassifying on training strategy and selection type, we elaborateour work based on selection technique classification. Specifically,our benchmark contains the following methods:1. Shallow Feature Selection : Methods Overview. For the type column, \"Shallow\"represents shallow feature selection methods, \"Gate\" repre-sents gate-based feature selection methods, and \"Sensitivity\"represents the sensitivity-based feature selection methods.For the single-stage, two-stage, soft selection, and hard se-lection columns, represents applicable, represents notapplicable, and represents that this method can be appli-cable after appropriate modifications.",
  "LassoShallowGBDTShallowRFShallowXGBoostShallowAutoFieldGateAdaFSGateOptFSGateLPFSGatePermutationSensitivitySHARKSensitivitySFSSensitivity": "Lasso . The least absolute shrinkage and selection operator(Lasso) algorithm is a traditional and useful method in machinelearning. It performs both variable selection and regularizationto improve the model performance. GBDT . Gradient-boosted decision tree (GBDT) achievessuperior performance by continually adding trees to fit residuals.By aggregating the feature importance scores from each tree, italso serves as an effective method for feature selection.",
  "Shallow Feature Selection": "Importance ScoreFeatureFeature EmbeddingForwardBackwardSensitivity Score : Overview of three categories of feature selection methods in deep recommender systems. Shallow methods typicallyuse statistical algorithms to assign feature importance to each field. Gate-based methods, on the other hand, assign gates tofeature embeddings and consider the gate values as the importance of the corresponding features. Sensitivity-based methodsderive parameter sensitivity from backward propagation steps and calculate feature importance accordingly. SHARK . Shark takes the novel first-order component ofTaylor expansion as the feature importance score for model pre-diction. It then prunes those features with lower scores from theembedding table to improve model performance and efficiency.",
  "Backbone Models": "To comprehensively evaluate the effectiveness of feature selectionmethods, we choose four popular DRS models as backbones: 1)Wide&Deep: A classical model contains shallow and deep net-works to capture feature interactions. 2) DeepFM: A model withFM module to automatically learn feature interactions. 3) DCN: Amodel with cross layers to study feature interactions. 4) FibiNet: Amodel equipped with SENet and bilinear layer to adaptively learnfeature importance and capture high-order feature interactions.The detailed introduction of these models is in Appendix B.",
  "Metrics": "In this paper, we focus on the Click-Through Rate (CTR) predictiontask, so we take AUC and Logloss as the two main metrics in ourbenchmark. The detailed introduction is in Appendix C. Addition-ally, since there is currently no metric that comprehensively evalu-ates the performance of feature selection methods across varyingnumbers of selected features, we propose a new evaluation metricnamed AUKC to address this challenge.AUKC. Current metrics cannot assess the robustness and stabilityof a specific feature selection method across different numbers ofselections. Therefore, we introduce a new metric Area Under the-performance Curve (AUKC) to fill this gap. AUKC measures thearea under the performance curve of feature selection methods",
  "=1( + 1 1)(1)": "where || denotes the number of all feature fields, is theAUC score when selecting feature fields follow a specific featureselecting method. If there are no input features, the model wouldmake random predictions so that 0 = 0.5.In practice, due to resource constraints, it is generally not feasibleto conduct experiments with every possible number of selections.Therefore, we further propose a more general form of AUKC toaccommodate this change:",
  "=1(, + , 1) (2)": "where || denotes the number of all feature fields and | | is thenumber of segments across the entire length of the feature fields set., is the AUC corresponding to the number of features selectedat the left endpoint of the -th segment, and , denotes theAUC with the number of selections at the right endpoint of theth segment. represents the length of the -th segment. Thedetailed process of the formula derivation is in Appendix D.AUKC and AUC share a similar conceptual basis, representingthe area under a curve, and both have a value range from 0 to 1.However, unlike AUC, the endpoint of AUKC does not necessar-ily equal 1, and the values in the middle of the curve can exceedthe endpoint value. This occurs because eliminating features withless information can result in a model that performs better thanone using all available features. The AUKC metric considers theeffectiveness of feature selection at different numbers of selectedfeatures, , thereby providing a more comprehensive reflection ofthe efficacy of feature selection methods.",
  "Experimental Details": "We implement the benchmark framework based on Pytorch 1.11.In the training phase, we utilize the Adam optimizer with 1 = 0.9,2 = 0.999, and = 1 108. We set the learning rate as 0.001, thebatch size as 4,096, and the embedding size as 8. For the activationfunction, if the original papers do not emphasize a specific one,we use ReLU as the activation function. We release the repositoryof our benchmark online6. We conduct each experimental settingthree times and record its average metrics to mitigate the impactof experimental fluctuations.",
  "Overall Performance (RQ1)": "In this subsection, we compare the performance of feature selectionmethods with different backbone models. The complete experimen-tal results for all four backbone models are illustrated in Appen-dix E. For the two-stage approaches (Lasso, GBDT, RF, XGBoost,AutoField, Permutation, SHARK, and SFS), we experiment withdifferent values of selected features during the retraining stage andadopt the best results. In the case of the single-stage approach, if itcan also be modified to a two-stage method (OptFS and LPFS), weconduct experiments on both ways and record the optimal results;if it only supports single-stage (i.e., AdaFS), we directly documentits results. From , we can observe the following points: In general, gate-based methods show better performance com-pared to shallow feature selection and sensitivity-based featureselection. The likely reason is that compared to using traditionalmachine learning methods or solely relying on gradient informa-tion, leveraging the powerful expression and learning capabilitiesof deep neural networks allows for a more thorough explorationof feature importance. The effectiveness of feature selection methods remains relativelyconsistent across different backbone models. This is because theinformation contained in feature combinations is objective andfactual and is not affected by the backbone model. The validity of feature selection methods is distinct across dif-ferent datasets. The shallow feature selection methods performbetter in Criteo and Aliccp. The possible reason is the featurevalue distribution in the other two datasets is very imbalanced,making shallow models prone to overfitting. Gate-based feature selection methods are good at dealing with limited data, whilesensitivity-based feature selection methods achieve the best per-formance among all methods in datasets with rich samples. Thisfinding can be attributed to the fact that in the data-scarce sce-nario, gradients are too sensitive to judge feature importance.However, with sufficient data support, the stability of gradientinformation will be greatly improved, allowing for more accuratefeature importance rankings. The two-stage approach yields more stable results comparedto the single-stage method across different backbone models.This is because the feature combinations filtered out by the two-stage method can exist independently after the search phase,whereas the single-stage method requires retraining the featureimportance scores with each training session.",
  "Stability Evaluation (RQ2)": "In this section, we conduct experiments on each backbone modelwith different number of selected features to investigate theirrobustness and stability. Specifically, we iterate a specific list for (e.g., for Avazu) and record the correspondingperformance of feature selection methods. Then, the stability offeature selection methods can be revealed from two perspectives,visual patterns of the performance curve () and AUKCvalues calculated as Equation (1) ().In Figrue 3, the x-axis represents the number of selected features and the y-axis represents the evaluation metrics AUC or Logloss.We only visualize the results of DeepFM, for we find that the trendsfor different backbone models remain consistent. The detailed ex-perimental results for the other backbone models are listed in ourreleased repository. In addition, it is worth noting that to makeOptFS applicable in this experiment, we assign importance scoresto features based on the drop rates of feature values in each featurefield. From , we can find: The effectiveness of features selected by different methods variesin ranking when the number of selections changes. Specifically,when the number of features selected is smaller (i.e., 5 or 10),Autofield, SHARK, and SFS perform better. They obtain infor-mation through the values or gradients of gate vectors, guidingthe sorting of feature importance. They are good at selecting themost informative features from the candidate feature sets. OptFSdoesnt initially perform well, as it is inherently a single-stagemethod. Modifying it to a two-stage method based on differentfeature value drop rates may not align with the goal of enhanc-ing effectiveness through feature selection. When the numberof features approaches the full set of features, the Permutationmethod performs well. This is because the permutation methodcalculates feature importance scores based on the loss of effectcaused by dropping a feature from the full set. It tends to identifyredundant features with less information. The XGBoost and GBDT methods show different trends on theAvazu and Criteo datasets. Specifically, they perform poorly onthe Avazu dataset but well on the Criteo dataset. This is be-cause the Avazu datasets feature values are concentrated in afew feature fields, leading to an imbalance that makes XGBoostand GBDT prone to overfitting, which affects the assessment offeature importance .",
  "To offer a more direct comparison of the stability, we evaluatethe selection results using the AUKC metric. shows AUKCresults on Avazu and Criteo. We can conclude:": "Overall, sensitivity-based feature selection methods outperformgate-based feature selection methods, both of which are superiorto shallow feature selection methods. The possible reason is thatgradient-based methods calculate feature importance based onpartial batches, which helps to prevent overfitting. The inferiorperformance of shallow methods may be attributed to their ex-cessive simplicity, which cannot effectively capture the complexrelationships between features to determine feature importance. Regarding specific methods, AutoField, SHARK, and SFS exhibitthe best performance, demonstrating leading effects across dif-ferent datasets. It is noteworthy that AutoField significantly sur-passes other gate-based feature selection methods. This may beattributed to AutoField being trained in a bi-level optimizationmanner, which reduces the risk of overfitting, making the selec-tion results more stable and reliable.",
  "FS MethodsAvazuCriteomemory remainmemory remain": "Lasso99.99963%1768.73103%25GBDT100.00000%2323.22062%30RF99.99161%1099.56109%20XGBoost100.00000%2375.20371%33AutoField99.91556%108.17985%20AdaFS100.00000%23100.00000%39OptFS99.73494%1779.05157%25LPFS99.96204%150.00100%15Permutation71.56409%1548.77971%25SHARK99.98986%107.31463%15SFS99.98345%108.16199%15 3.4.1Experiments with performance limitations. In real-world sce-narios, an important consideration is reducing the memory usage ofa model as much as possible while ensuring its effectiveness. To testthe memory-saving capabilities of different methods under the con-straint of maintaining effectiveness, experiments were conductedon two classic DRS datasets, Avazu and Criteo, using DCN as thebackbone model. Firstly, we calculate an acceptable threshold foreffectiveness based on the baseline performance of the backbonemodel (without feature selection, using the full set of features),considering a 1% loss in AUC as acceptable. The threshold is AUC0.77873 for Avazu and AUC 0.79245 for Criteo. Based on the featureimportance ranking list obtained in the search phase by differentfeature selection methods, we select features from highest to lowestimportance until the models performance exceeds the threshold.We record the number of features and feature values required toreach this lower threshold of effectiveness for each feature selectionmethod. Since most parameters in DRS models are concentrated inthe embedding table, we only considered the memory usage of theembedding table when calculating the memory models required.From , we can conclude that: From the perspective of memory usage, methods like AutoField,LPFS, Permutation, SHARK, and SFS perform better. This indi-cates that even in DRS scenarios rich in high-dimensional sparsefeatures, these methods do not rely solely on ID-type features(e.g., userid). Instead, they effectively select informative features.Its noteworthy that AdaFS shows 100% memory usage on bothdatasets. This is because AdaFS is an instance-level selection",
  "method (where gate weights vary with each input sample), andtherefore cannot save memory usage": "In terms of the number of features, RF, AutoField, LPFS, SHARK,and SFS only require half or fewer of the original feature setto achieve 99% of the backbone models performance. On theother hand, GBDT and XGBoost need more features. A possiblereason for this is that GBDT and XGBoost are prone to overfittingin high-dimensional sparse DRS datasets, which can affect thejudgment of feature importance. From the perspective of the datasets, the memory-saving effec-tiveness of various methods is less pronounced on the Avazudataset compared to the Criteo dataset. This is because mostfeature values in the Avazu dataset are concentrated in just afew feature fields, whereas in Criteo, the feature values are moreevenly distributed. Therefore, when these particular feature fieldsare indispensable sources of information for CTR prediction, thememory-saving effect becomes less noticeable. 3.4.2Experiments with memory limitations. Maximizing effective-ness within limited memory is also an important application sce-nario for feature selection. To test the capability of different methodsin feature selection under memory constraints, experiments wereconducted on the Criteo dataset using DCN as the backbone model.The memory limits were categorized into three levels: 25% memory,50% memory, and 75% memory. Specifically, since the parametersin DRS are mostly concentrated in the embedding table, we usedthe memory usage of the full feature sets embedding table as areference. We set thresholds at 25%, 50%, and 75% of the total mem-ory to record the optimal performance achievable under memoryconstraints by different feature selection methods. Since AdaFS isincapable of performing a hard selection of partial feature fields, it",
  "is not included in this experiment. Based on the results presentedin , we can draw the following conclusion:": "Overall, gate-based feature selection and sensitivity-based featureselection methods perform better. Even when restricted to usingonly 25% of the available memory, they can achieve results closeto those obtained using the full set of features. This is becausethese methods rely on more powerful deep learning networksthat can better model feature importance and are not dependenton high-dimensional sparse ID-type features in DRS. Specifically, AutoField achieves the best results because its de-signed controller effectively learns the importance of each fieldfor the prediction outcome. The DARTS (Differentiable Architec-ture Search) parameter updating manner made the learningof the gate vector more robust and capable of generalization. Incontrast, Random Forest (RF) performs the worst. This is because,in the context of high-dimensional sparse DRS, RF tends to assignhigh weights to ID-type features with numerous feature values.Such feature fields offer more opportunities for splitting, whichfacilitates the training of RF. However, this approach increasesthe risk of model overfitting. This tendency of RF to favor featureswith many splitting points can lead to a bias towards complex,less generalizable models that dont necessarily capture the mostpredictive or relevant features for the task at hand.",
  "offline dataset is aligned with the online service model, which con-tains over 157 features. All of these features have been roughlyproved useful in offline leave-one-out experiments": "3.5.2Experiment. Considering the industry dataset is relativelyhuge, we only select the top 5 elite methods for comparison ac-cording to their performance on public datasets, including twogate-based methods (AutoField and LPFS) and three sensitivity-based methods (SHARK, Permutation, and SFS).The feature preprocessing is slightly different from the publicdataset. Specifically, we utilize AutoDis technique to directlyconvert dense features into embeddings with the same size as sparsefeatures. As for multi-hot features, we adopt the reduce-mean oper-ator to aggregate multiple embeddings into one feature embedding.Finally, the preprocessed features are fed into the FibiNet backbone.After training, we grab each methods feature importance andperform top-k experiments. The experiment result is demonstratedin and . From the redundant feature eliminationperspective, SHARK, AutoField, and Permutation perform the best.They eliminate about half of the feature sets (80 features) withoutdecreasing model accuracy. However, from an effective featuremining perspective, AutoField performs best for small feature sets,aligning with our observations from the public datasets. We canalso find that AutoField achieves the highest AUKC in ,demonstrating its robustness and stability across different numbersof selections. LPFSs performance is slightly misaligned with theone on the public dataset. We attribute this misalignment to thesensitive learnable polarization module.",
  "Online Experiments (RQ5)": "To validate the effectiveness of our benchmark in online environ-ments, we reconfigure partial benchmarks to adapt to our featurefactory, package them into a new toolkit, and conduct online taskswith this toolkit for redundant feature elimination tasks.Specifically, we create an experiment group that reduces 30% fea-tures with our toolkit in a latency-sensitive online recommendationplatform. Then, we deploy the experimental group for the A/B testwith 5% traffic (approximately 1 million users). After one weeksobservation, the inference latency was reduced by approximately20% while the number of video views (vv) and average play durationremained the same. This experiment proves the effectiveness of ourbenchmark toolkits in the online environment.",
  "Similarity Visualization (RQ6)": "In this section, we visualize the similarity between feature rank-ings of different feature selection methods in DRS across 4 publicdatasets. shows the heatmap of pair-wise Spearman corre-lation similarities. In the heatmap, the x-axis and y-axis representdifferent feature selection methods. The color of each square indi-cates the level of similarity. The more the color leans towards darkgreen, the more similar between the feature rankings derived fromthe two feature selection methods. Conversely, the more the colorleans towards light green, the greater the difference in the featurerankings selected by the two methods.We can discern the following information from the heatmap:1) The heatmap displays three darker clusters in the upper left,middle, and lower right. This is because the features selected by",
  ": Similarity between feature selection methods": "the three different category methods are more similar within eachcategory. 2) The rankings of LPFS and OptFS are highly similar. Thisis because they are both gate-based methods and use the L0 regular-ization term to control the sparsity of the gates. 3) The rankings ofAutoField and Permutation are the most dissimilar. This aligns withour conclusion in .3. AutoField, as a gate-based method,determines feature importance by learning the gate weights of dif-ferent feature fields during the models training process, aiding inuncovering features rich in information. On the other hand, Per-mutation, as a sensitivity-based feature selection method, judgesfeature importance based on the loss of effect caused by shuffling aparticular feature. This approach is more conducive to identifyingfeatures with less information. Therefore, their rankings show sig-nificant differences. 4) The rankings of SHARK and SFS are quitesimilar because they both determine feature importance based onthe gradient of the features, sharing the same source of information.",
  "RELATED WORKS": "Feature Selection Methods. Machine learning has shown supe-rior results in data mining . The modeling perfor-mance relies heavily on input features, making feature selectiona crucial part of feature engineering. Traditional feature selectionmethods fall into three categories: filter, wrapper, and embeddedmethods . Filter methods utilize criteria to identify predictivefeature fields, exemplified by the Chi2 score and mutual infor-mation . Wrapper methods employ black-box models to selectpredictive features, assessing the utility of feature subsets through,often, generic algorithms . Embedded methods, on the otherhand, integrate the feature selection process within the predictionmodel, thereby evaluating the effectiveness of feature subsets inconjunction. Notable embedded methods include LASSO andGradient Boosting Machine . The advent of deep learning hasspurred novel approaches , such as gate-based methods, where learnable gates determine the significanceof feature fields , and sensitivity-based methods, leveraging gradient techniques to identify critical features by theirsensitivity . Furthermore, some studies explored rein-forcement learning to automate feature selection, using agents topinpoint predictive features .Considering their appliability to DRS, our benchmark focus ontraditional embedded methods (termed shallow methods), along-side gate-based and sensitivity-based methods. Other techniquesoften fall short in terms of effectiveness or efficiency within theDRS context. Specifically, filter methods might overlook the DRSmodels nuances by relying solely on data-intrinsic relationships,leading to suboptimal outcomes. Besides, wrapper and reinforce-ment learning methods, typically designed for smaller datasets,become impractically time-consuming for the expansive datasetscharacteristic of DRS, which can encompass millions of samples.Benchmarks for Feature Selection. Most existing benchmarksare tailored for classical downstream models and rely on syn-thetic or domain-specific datasets . Due to their constrainedrelevance, these benchmarks frequently fall short in providing thenecessary guidance to propel research forward in the domain ofDRS feature selection. The most related work, DeepLasso , con-fines its evaluation to shallow methods and a narrow set of back-bone models, primarily targeting tabular learning tasks. Despiteutilizing real-world datasets, the scale of these datasets pales in com-parison to those encountered in DRS scenarios, thus limiting theprovision of actionable insights for DRS feature selection strategies.ERASE undertakes a thorough evaluation of shallow, gate-based,and sensitivity-based methods across an array of representative DRSbackbones, leveraging both large-scale public datasets and privateindustrial datasets. Furthermore, ERASE proposes a novel metricdesigned to assess feature selection efficacy comprehensively.",
  "CONCLUSION": "In this study, we introduce ERASE, a comprehensive bEnchmaRkfor feAture SElection for deep recommender systems (DRS). ERASEintegrates a broad spectrum of feature selection methods pertinentto DRS and ensures fair and comprehensive experimentation acrossfour public datasets as well as real-world industrial datasets. Fur-thermore, ERASE pioneers the adoption of the AUKC metric, de-vised to address the shortcomings of existing metrics by offeringa thorough evaluation of the robustness and stability of featureselection methods over various numbers of selected features. Theempirical validation provided by both offline and online analyses onindustrial datasets underscores the applicability of our findings inpractical settings, offering valuable perspectives on the deploymentof feature selection methods within DRS environments.",
  "Vernica Boln-Canedo, Noelia Snchez-Maroo, and Amparo Alonso-Betanzos.2013. A review of feature selection methods on synthetic data. Knowledge andinformation systems 34 (2013), 483519": "Vernica Boln-Canedo, Noelia Snchez-Marono, Amparo Alonso-Betanzos,Jos Manuel Bentez, and Francisco Herrera. 2014. A review of microarraydatasets and applied feature selection methods. Information sciences 282 (2014). Andrea Bommert, Xudong Sun, Bernd Bischl, Jrg Rahnenfhrer, and Michel Lang.2020. Benchmark for filter methods for feature selection in high-dimensionalclassification data. Computational Statistics & Data Analysis 143 (2020), 106839. Leo Breiman. 2001. Random forests. Machine learning 45 (2001), 532. Bo Chen, Xiangyu Zhao, Yejing Wang, Wenqi Fan, Huifeng Guo, and Ruim-ing Tang. 2024. A comprehensive survey on automated machine learning forrecommendations. ACM Transactions on Recommender Systems 2, 2 (2024), 138.",
  "Tianqi Chen and Carlos Guestrin. 2016. Xgboost: A scalable tree boosting system.In Proceedings of the 22nd acm sigkdd international conference on knowledgediscovery and data mining. 785794": "Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al.2016. Wide & deep learning for recommender systems. In Proceedings of the 1stworkshop on deep learning for recommender systems. 710. Valeriia Cherepanova, Roman Levin, Gowthami Somepalli, Jonas Geiping,C. Bruss, Andrew Wilson, Tom Goldstein, and Micah Goldblum. 2023.APerformance-Driven Benchmark for Feature Selection in Tabular Deep Learning.In NeurIPS 2023 Second Table Representation Learning Workshop.",
  "SL Shiva Darshan and CD Jaidhar. 2018. Performance evaluation of filter-basedfeature selection techniques in classifying portable executable files. ProcediaComputer Science 125 (2018), 346356": "Wei Fan, Kunpeng Liu, Hao Liu, Pengyang Wang, Yong Ge, and Yanjie Fu. 2020.AutoFS: Automated Feature selection via diversity-aware interactive reinforce-ment learning. In 2020 IEEE International Conference on Data Mining (ICDM).IEEE, 10081013. Aaron Fisher, Cynthia Rudin, and Francesca Dominici. 2019. All Models areWrong, but Many are Useful: Learning a Variables Importance by Studying anEntire Class of Prediction Models Simultaneously. Journal of machine learningresearch: JMLR 20 (2019).",
  "Jerome H Friedman. 2001. Greedy function approximation: a gradient boostingmachine. Annals of statistics (2001), 11891232": "Pablo M Granitto, Cesare Furlanello, Franco Biasioli, and Flavia Gasperi. 2006.Recursive feature elimination with random forest for PTR-MS analysis of agroin-dustrial products. Chemometrics and intelligent laboratory systems (2006). Huifeng Guo, Bo Chen, Ruiming Tang, Weinan Zhang, Zhenguo Li, and XiuqiangHe. 2021. An embedding learning framework for numerical features in ctrprediction. In Proceedings of the 27th ACM SIGKDD Conference on KnowledgeDiscovery & Data Mining. 29102918.",
  "Pengyue Jia, Ling Chen, and Dandan Lyu. 2024. Fine-Grained Population MobilityData-Based Community-Level COVID-19 Prediction Model. Cybernetics andSystems 55, 1 (2024), 184202": "Pengyue Jia, Yiding Liu, Xiaopeng Li, Xiangyu Zhao, Yuhao Wang, YantongDu, Xiao Han, Xuetao Wei, Shuaiqiang Wang, and Dawei Yin. 2024. G3: AnEffective and Adaptive Framework for Worldwide Geolocalization Using LargeMulti-Modality Models. arXiv preprint arXiv:2405.14702 (2024). Pengyue Jia, Yiding Liu, Xiangyu Zhao, Xiaopeng Li, Changying Hao, ShuaiqiangWang, and Dawei Yin. 2023. Mill: Mutual verification with large language modelsfor zero-shot query expansion. arXiv preprint arXiv:2310.19056 (2023). Pengyue Jia, Yichao Wang, Shanru Lin, Xiaopeng Li, Xiangyu Zhao, Huifeng Guo,and Ruiming Tang. 2024. D3: A Methodological Exploration of Domain Division,Modeling, and Balance in Multi-Domain Recommendations. In Proceedings of theAAAI Conference on Artificial Intelligence, Vol. 38. 85538561.",
  "Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-semantic alignments forgenerating image descriptions. In Proceedings of the IEEE conference on computervision and pattern recognition. 31283137": "Youngjune Lee, Yeongjong Jeong, Keunchan Park, and SeongKu Kang. 2023. MvFS:Multi-view Feature Selection for Recommender System. In Proceedings of the32nd ACM International Conference on Information and Knowledge Management. Muyang Li, Zijian Zhang, Xiangyu Zhao, Wanyu Wang, Minghao Zhao, RunzeWu, and Ruocheng Guo. 2023. Automlp: Automated mlp for sequential recom-mendations. In Proceedings of the ACM Web Conference 2023. 11901198. Xiaopeng Li, Lixin Su, Pengyue Jia, Xiangyu Zhao, Suqi Cheng, Junfeng Wang,and Dawei Yin. 2023. Agent4Ranking: Semantic Robust Ranking via PersonalizedQuery Rewriting Using Multi-agent LLM. arXiv preprint arXiv:2312.15450 (2023). Weilin Lin, Xiangyu Zhao, Yejing Wang, Tong Xu, and Xian Wu. 2022. AdaFS:Adaptive Feature Selection in Deep Recommender System. In Proceedings of the28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD). Dugang Liu, Chaohua Yang, Xing Tang, Yejing Wang, Fuyuan Lyu, Weihong Luo,Xiuqiang He, Zhong Ming, and Xiangyu Zhao. 2024. MultiFS: Automated Multi-Scenario Feature Selection in Deep Recommender Systems. In Proceedings of the17th ACM International Conference on Web Search and Data Mining. 434442. Huan Liu and Rudy Setiono. 1995. Chi2: Feature selection and discretization ofnumeric attributes. In Proceedings of 7th IEEE international conference on toolswith artificial intelligence. IEEE, 388391.",
  "Hanxiao Liu, Karen Simonyan, and Yiming Yang. 2018. DARTS: DifferentiableArchitecture Search. In International Conference on Learning Representations": "Haochen Liu, Xiangyu Zhao, Chong Wang, Xiaobing Liu, and Jiliang Tang. 2020.Automated embedding size search in deep recommender systems. In Proceedingsof the 43rd International ACM SIGIR Conference on Research and Development inInformation Retrieval. 23072316. Kunpeng Liu, Yanjie Fu, Pengfei Wang, Le Wu, Rui Bo, and Xiaolin Li. 2019.Automating feature subspace exploration via multi-agent reinforcement learning.In Proceedings of the 25th ACM SIGKDD International Conference on KnowledgeDiscovery & Data Mining. 207215.",
  "Yejing Wang, Xiangyu Zhao, Tong Xu, and Xian Wu. 2022. AutoField: AutomatingFeature Selection in Deep Recommender Systems. In Proceedings of the ACM WebConference": "Beichuan Zhang, Chenggen Sun, Jianchao Tan, Xinjun Cai, Jun Zhao, MengqiMiao, Kang Yin, Chengru Song, Na Mou, and Yang Song. 2023. SHARK: A Light-weight Model Compression Approach for Large-Scale Recommender Systems.In Proceedings of the 32nd ACM International Conference on Information andKnowledge Management (CIKM 23). Zijian Zhang, Shuchang Liu, Jiaao Yu, Qingpeng Cai, Xiangyu Zhao, ChunxuZhang, Ziru Liu, Qidong Liu, Hongwei Zhao, Lantao Hu, et al. 2024. M3oE: Multi-Domain Multi-Task Mixture-of Experts Recommendation Framework. arXivpreprint arXiv:2404.18465 (2024). Xiangyu Zhao, Haochen Liu, Wenqi Fan, Hui Liu, Jiliang Tang, and Chong Wang.2021. Autoloss: Automated loss function search in recommendations. In Pro-ceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & DataMining. Xiangyu Zhao, Haochen Liu, Hui Liu, Jiliang Tang, Weiwei Guo, Jun Shi, SidaWang, Huiji Gao, and Bo Long. 2021. Autodim: Field-aware embedding dimensionsearchin recommender systems. In Proceedings of the Web Conference 2021. Xiaosa Zhao, Kunpeng Liu, Wei Fan, Lu Jiang, Xiaowei Zhao, Minghao Yin, andYanjie Fu. 2020. Simplifying reinforced feature selection via restructured choicestrategy of single agent. In 2020 IEEE International Conference on Data Mining",
  "ADATASETS": "In this subsection, we introduce the datasets utilized in our bench-mark. The statistics of these datasets are listed in . We selectthe following four datasets for use: Avazu. Avazu is a commonly used dataset in the field of featureselection for deep recommendation systems. It consists of 23features and 40,428,967 interaction records. We divide the datasetinto training, validation, and test sets in a 7:2:1 ratio. Criteo. Criteo is another frequently used dataset for studyingfeature selection in Deep Recommendation Systems. The Criteodataset contains 45,850,617 samples and 39 features, offering alarger number of features. We also adopt a 7:2:1 ratio for dividingthe data into training, validation, and test sets to train the model. Movielens-1m. The Movielens dataset is a well-known publicmovie dataset in the field of recommendation systems, featuringmultiple variants with different data volumes to meet diverseresearch needs. We choose the Movielens-1M dataset, whichcomprises 9 features. The limited number of features presents agreater challenge for feature selection methods. In our experi-ments with Movielens, we also divide the dataset into training,validation, and test sets according to a 7:2:1 ratio. The interactionswith a rating bigger than 3 are considered as positive samples. AliCCP. Alibaba Click and Conversion Prediction (AliCCP) datasetis extracted from the real-world e-commerce platform Taobao. Itis a popular dataset used for Click-Through Rate (CTR) estima-tion. It consists of 23 features and 85,316,519 samples, makingit an effective tool for evaluating feature selection methods inscenarios closely resembling real advertising contexts. We followthe original AliCCP splitting approach , allocating 50% ofthe data for training, with the remaining data split equally intovalidation and test sets in a 1:1 ratio.",
  "BBACKBONE MODELS": "we apply a variety of feature selection methods to the followingfour popular deep recommendation models in this work. Wide&Deep . Wide and Deep (Wide&Deep) model is devel-oped by Google to improve the recommender systems. It com-bines wide and deep models to fit specific feature combinations inlarge-scale datasets and previously unseen feature combinationsthrough low-dimensional dense embeddings. DeepFM . DeepFM is an effective model in recommendersystems. It combines the strengths of FM models and deep neuralnetworks to extract both low-order feature interactions and high-order feature interactions. DCN . The Deep & Cross Network (DCN) model is proposedby Google to capture both explicit and implicit feature interac-tions for prediction tasks. The cross layers have cross operationsthat learn complex feature interactions to improve the predictionperformance. FibiNet . The Feature Importance and Bilinear feature Inter-action NETwork (FibiNet) is an innovative model in CTR predic-tion. It introduces the SENET and bilinear layer. SENET learnsfeature importance adaptively and the bilinear layer captureshigh-order feature interactions.",
  "CMETRICS": "In this section, we detail the two frequently used metrics in recom-mender systems: AUC and Logloss.1) AUC. Area Under the ROC Curve (AUC) measures the areaunder the ROC curve. The ROC curve is one graph shows that theclassification performance of the model under different thresholds.In addition, in random positive-negative pairs, AUC also representsthe probability that the model ranks the positive sample with ahigher score than the negative one. It can be formalized as follows:",
  "DFORMULA DERIVATION OF AUKC": "In this section, we detail the formula derivation process of AUKC.AUKC is a comprehensive metric for assessing the AUC perfor-mance of a given feature selection method across different numbersof feature selections. Under ideal circumstances, experiments canbe conducted for each number of selections to obtain the corre-sponding AUC metrics, and then derive AUKC:",
  "where | | represents the number of segments for the number offeatures selected across the entire length of the feature set. For": "example, with 6 features, if experiments are conducted at featurecounts of 2,4,5, and 6, then | | = 4. , is the AUC correspond-ing to the number of features selected at the left endpoint of theth segment, and , denotes the AUC with the number ofselections at the right endpoint of the th segment. representsthe length of the th segment."
}