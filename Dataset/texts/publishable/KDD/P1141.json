{
  "ABSTRACT": "Recently, the emergence of large language models (LLMs) has revo-lutionized the paradigm of information retrieval (IR) applications,especially in web search, by generating vast amounts of human-like texts on the Internet. As a result, IR systems in the LLM eraare facing a new challenge: the indexed documents are now notonly written by human beings but also automatically generated bythe LLMs. How these LLM-generated documents influence the IRsystems is a pressing and still unexplored question. In this work,we conduct a quantitative evaluation of IR models in scenarioswhere both human-written and LLM-generated texts are involved.Surprisingly, our findings indicate that neural retrieval models tendto rank LLM-generated documents higher. We refer to this cat-egory of biases in neural retrievers towards the LLM-generatedcontent as the source bias. Moreover, we discover that this bias isnot confined to the first-stage neural retrievers, but extends to thesecond-stage neural re-rankers. Then, in-depth analyses from theperspective of text compression indicate that LLM-generated textsexhibit more focused semantics with less noise, making it easier forneural retrieval models to semantic match. To mitigate the sourcebias, we also propose a plug-and-play debiased constraint for the op-timization objective, and experimental results show its effectiveness.Finally, we discuss the potential severe concerns stemming from theobserved source bias and hope our findings can serve as a criticalwake-up call to the IR community and beyond. To facilitate futureexplorations of IR in the LLM era, the constructed two new bench-marks are available at",
  "Jun Xu is the corresponding author. Work partially done at Engineering Research Cen-ter of Next-Generation Intelligent Search and Recommendation, Ministry of Education": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08",
  "Source Bias, Information Retrieval, LLM-Generated Texts, ArtificialIntelligence Generated Content": "ACM Reference Format:Sunhao Dai, Yuqi Zhou, Liang Pang, Weihao Liu, Xiaolin Hu, Yong Liu,Xiao Zhang, Gang Wang and Jun Xu. 2024. Neural Retrievers are BiasedTowards LLM-Generated Content. In Proceedings of the 30th ACM SIGKDDConference on Knowledge Discovery and Data Mining (KDD 24), August2529, 2024, Barcelona, Spain. ACM, New York, NY, USA, 13 pages.",
  "INTRODUCTION": "With the advent of large language models (LLMs), exemplified byChatGPT, the field of artificial intelligence generated content (AIGC)has surged to new heights of prosperity . LLMs have demon-strated their remarkable capabilities in automatically generatinghuman-like text at scale, resulting in the Internet being inundatedwith an unprecedented volume of AIGC content . This influxof LLM-generated content has fundamentally reshaped the digitalecosystem, challenging conventional paradigms of content creation,dissemination, and information access on the Internet .Meanwhile, information retrieval (IR) systems have become in-dispensable for navigating and accessing the Internets vast infor-mation landscape . As illustrated in , in the erapreceding the widespread emergence of LLMs, IR systems focusedon retrieving documents solely from the human-written corpus inresponse to users queries . However, the proliferation ofAIGC driven by LLMs has expanded the corpus of IR systems to in-clude both human-written and LLM-generated texts. Consequently,this paradigm shift raises a fundamental research question: Whatis the impact of the proliferation of generated content on IRsystems? We aim to explore whether existing retrieval models tendto prioritize LLM-generated text over human-written text, evenwhen both convey similar semantic information. If this holds, LLMsmay dominate information access, particularly as their generatedcontent is rapidly growing on the Internet .",
  ": The overview evolution of IR paradigm from thePre-LLM era to the LLM era": "To approach the fundamental research question, we decomposeit into four specific research questions. The first question is RQ1:How to construct an environment to evaluate IR models inthe LLM era? Given the lack of public retrieval benchmarks encom-passing both human-written and LLM-generated texts, we proposean innovative and practical method to create such a realistic evalu-ation environment without the need of costly human annotation.Specifically, we leverage the original human-written texts as theinstruction conditions to prompt LLMs to generate rewritten textcopies while preserving the same semantic meaning. In this way, wecan confidently assign the same relevancy labels to LLM-generateddata according to the original labels. Extensive empirical analysisvalidates the quality of our constructed environment, demonstrat-ing its effectiveness in mirroring real-world IR scenarios in the LLMera. As a result, we introduce two new benchmarks, SciFact+AIGCand NQ320K+AIGC, tailored for IR research in the LLM era.With the constructed environment, we further explore RQ2: Areretrieval models biased towards LLM-generated content? Weconduct comprehensive experiments with various representativeretrieval models, ranging from traditional lexical models to modernneural models based on pretrained language models (PLMs) . Surprisingly, we uncover that neural retrievers are biasedtowards LLM-generated texts, i.e., tend to rank LLM-generatedtexts in higher positions. We refer to this as source bias, as theneural retrievers favor content from specific sources (i.e., LLM-generated content). Further experiments indicate that the sourcebias not only extends to the second-stage neural re-rankers from thefirst-stage retrieval but also manifests more severely. These findingscorroborate the prevalence of source bias in neural retrieval models.Then, what we are curious about is RQ3: Why are neural re-trieval models biased towards LLM-generated texts? Inspiredby the recent studies positing LLMs as lossless compressors ,we analyze the cause of source bias from the viewpoint of textcompression. Our analysis of singular values in different cor-pora reveals that LLM-generated texts exhibit more focused seman-tics with minimal noise, enhancing their suitability for semantic matching. Furthermore, our in-depth perplexity analysis shows thatLLM-generated texts consistently achieve lower perplexity scores,which indicates a higher degree of comprehensibility and confi-dence from the PLMs perspective. These observations collectivelysuggest that LLM-generated texts are more readily understandableto PLM-based neural retrievers, thereby resulting in source bias.Finally, we try to answer RQ4: How to mitigate source bias inneural retrieval models? To tackle this, we propose an intuitiveyet effective debiased constraint. This constraint is designed topenalize biased samples during the optimization process, therebyshifting the focus of retrieval models from exploiting inherent short-cuts to emphasizing semantic relevance. Besides, our debiased con-straint is model-agnostic and can be plugged and played to theranking optimization objectives of various neural retrieval models.Furthermore, it offers the capability to control the degree of biasremoval, offering the flexibility to balance the treatment betweenthe two sources of content based on specific requirements andenvironmental considerations.Last but not least, we discuss the potential emerging concernsstemming from source bias, highlighting the risk of human-writtencontent being gradually inaccessible, especially due to the rapidlyincreasing LLM-generated content on the Internet . Further-more, source bias could be maliciously exploited to manipulatealgorithms and potentially amplify the spread of misinformation,posing a threat to online security. In light of these pressing issues,we hope that our findings serve as a resounding wake-up call to allstakeholders involved in IR systems and beyond.In summary, the contributions of this paper are as follows:(1) We introduce a more realistic paradigm of IR systems consid-ering the growing prosperity of AIGC, where the retrieval corpusconsists of both human-written and LLM-generated texts. We thenuncover a new inherent bias in both neural retrieves and re-rankerspreferring LLM-generated content, termed as source bias.(2) We provide an in-depth analysis and insights of source biasfrom a text compression perspective, which indicates that LLM-generated texts maintain more focused semantics with minimalnoise and are more readily comprehensible for neural retrievers.(3) We propose a debiased constraint to penalize the biased sam-ples during optimization, and experimental results demonstrate itseffectiveness in mitigating source bias in different degrees.(4) We also provide two new benchmarks, SciFact+AIGC andNQ320K+AIGC, which contain both high-quality human-writtenand various LLM-generated corpus and corresponding relevancylabels. We believe these two benchmarks can serve as valuableresources for facilitating future research of IR in the LLM era.",
  "RQ1: ENVIRONMENT CONSTRUCTION": "With the increasing usage of LLMs in generating texts (e.g., para-phrasing or rewriting), the corpus of IR systems includes bothhuman-written and LLM-generated texts nowadays. Constructingan IR dataset in the LLM era typically involves two steps: collectingboth human-written and LLM-generated corpora and then employ-ing human evaluators to annotate relevancy labels for each query-document pair. Given that LLM-generated content is currentlyunidentifiable and the significant cost of human annotation,",
  "Human-Written Text": "Allele, also called allelomorph, any one of two or more genes that may occur alternatively at a given site (locus) on a chromosome. Alleles may occur in pairs, or there may be multiple alleles affecting the expression (phenotype) of a particular trait. Allele, also known as an allelomorph, refers to any of the two or more genes that can exist alternatively at a specific location (locus) on a chromosome. These alleles can exist in pairs, or there can be multiple alleles that influence the expression (phenotype) of a specific trait.",
  ": The overall paradigm of the proposed evaluation framework for IR in the LLM era": "we introduce a natural and practical framework for quantitativelyevaluating retrieval models in the LLM era, as shown in .To better align with real-world scenarios, the evaluation environ-ments should meet the following three essential criteria. Firstly,it is imperative to distinguish between human-written and LLM-generated texts within the corpus. Secondly, we need access torelevancy labels for LLM-generated data in response to queries.Thirdly, each human-written text should better have a correspond-ing LLM-generated counterpart with the same semantics, ensuringthe most effective and fair evaluation.",
  "Notation": "Formally, in the Pre-LLM era, given a query Q where Q is theset of all queries, the traditional IR system aims to retrieve a listof top- relevant documents { (1), (2), . . . , ()} from a corpusC = {1 ,2 , . . . } which consists of human-written docu-ments. However, in the era of LLMs, there is also LLM-generatedtext in the corpus. To evaluate the IR models in the LLM era, we alsocreate an additional LLM-generated corpus C = {1 ,2 , . . . , }where each document is generated by a LLM, e.g., 1 can be createdby prompting ChatGPT to rewrite 1 while preserving its originalsemantics information. Consequently, given a query , the objec-tive of a retriever in the LLM era is to return the top- relevantdocuments from the mixed corpus C = C C.",
  "Constructing IR Datasets in the LLM Era": "In this section, we prompt LLMs to rewrite human-written corpusto build two new standard retrieval datasets: SciFact+AIGC andNQ320K+AIGC. These two new datasets can serve as valuableresources to facilitate future research of IR in the LLM era. 2.2.1Human-Written Corpus. We first choose two widely usedretrieval datasets written by humans in the Pre-LLM era as theseed data: SciFact and NQ320K. SciFact1 dataset aims to re-trieve evidence from the research literature containing scientificpaper abstracts for fact-checking. NQ320K2 is based on theNatural Questions (NQ) dataset from Google, where the documentsare gathered from Wikipedia pages, and the queries are naturallanguage questions. Following the practice in BEIR benchmark ,",
  "we process these two datasets in a standard format: corpus C,queries Q, and relevancy labels R = {(,,)}=1, where is the number of labeled query-document pairs in the dataset": "2.2.2LLM-Generated Corpus. For the LLM-generated corpus, werepurpose the original human-written corpus as our seed data andinstruct LLMs to rewrite each given text from the human-writtencorpus. As the written text generated by LLM carries almost thesame semantic information as the original human-written text, wecan assign the same relevancy labels to new <query, LLM-generateddocument> pairs as those assigned to the original labeled <query,human-written document> pairs.Our instruction is straightforward: Please rewrite the followingtext: {{human-written text}}, as illustrated in the left part of .This straightforward instruction enables LLMs to generate textwithout too many constraints while maintaining semantic equiva-lence to the original human-written text. Specifically, we chooseLlama2 and ChatGPT to rewrite each seed human-written cor-pus, as Llama2 and ChatGPT are both the most widely-used andnearly the state-of-the-art open-sourced and closed-source LLM,respectively. We only generate texts with ChatGPT correspondingto the texts in SciFact dataset, mainly due to the significant costinvolved in processing the larger NQ320K dataset.For the LLM-generated corpus, we conduct post-processing to re-move unrelated parts of the original response from LLM like Sure,heres a possible rewrite of the text:. As a result, we can obtain twocorresponding LLM-generated corpora with SciFact and NQ320Kas seed data. After that, we extend the original labels of queryand human-written text R = {(,,)}=1 to get the cor-responding label of LLM-generated text R = {(,,)}=1.We will validate the quality of the datasets in the following sec-tion. Combining each original human-written corpus C with itscorresponding LLM-generated corpus C, original queries Q, andlabels R R, we can create two new datasets, denoted as Sci-Fact+AIGC and NQ320K+AIGC. summarizes the statisticsof the proposed two datasets.",
  "| | ) and the overlap ( | |": "| |) between each LLM-generateddocument and orginal human-written document. As shown in Fig-ure 3, both the Jaccard similarity and overlap distributions ex-hibit normal distribution, with peaks at about 0.6 and 0.8 for Sci-Fact+AIGC, and about 0.4 and 0.6 for NQ320K+AIGC, respectively.These observations suggest that while there is a considerable over-lap of terms between the LLM-generated text and the originalhuman-written text, there are also distinct differences, especiallynoticeable in the NQ320K+AIGC dataset. 2.3.2Semantic-based Statistics and Analysis. For the LLM-generatedtexts, a pivotal consideration is whether they faithfully preservethe underlying semantics of the corresponding human-written cor-pus. If they indeed do so, we then can confidently assign them thesame relevancy labels as the labels of their corresponding originalhuman-written texts given each query.To assess this, we first leverage the OpenAI embedding model3",
  ": Distribution of cosine similarity of semantic embed-ding between Llama2-generated and human-written corpora": "human-written texts. Subsequently, we visualize these embeddingsthrough T-SNE in . We observe a strikingly close over-lap between the Llama2-generated corpus and the human-writtencorpus in the latent space. This observation strongly suggests thatthese LLM-generated corpora adeptly preserve the original seman-tics. Moreover, we delve into the cosine similarity of semantic em-beddings between the LLM-generated text and their correspondinghuman-written counterparts. The results, as shown in , alsoindicate a high degree of similarity, with most values exceeding 0.95,affirming the faithful preservation of semantics in LLM-generatedtext. Hence, for each query-document pair (,), we can confi-dently assign the relevancy label to be the same as that of (, ).",
  "Which document is more relevant to the given query?HumanLLMEqualHumanLLMEqual0.0%(0.0%)0.0%(0.0%)100.0%(82.0%)2.0%(0.0%)0.0%(0.0%)98.0%(81.6%)": "Which document exhibits higher quality by considering the following aspects:linguistic fluency, logical coherence, and information density?HumanLLMEqualHumanLLMEqual8.0%(0.0%)6.0%(0.0%)86.0%(46.5%)4.0%(0.0%)6.0%(0.0%)90.0%(60.%) 2.3.3Retrieval Performance Evaluation. To further validate theaccuracy of the relevancy label assignments, we conduct an eval-uation of retrieval models on the human-written corpus and theLLM-generated corpus, respectively. The following representativeretrieval models are adopted in the experiments: (1) Lexical Re-trieval Models: TF-IDF and BM25 and (2) Neural RetrievalModels: ANCE , BERM , TAS-B , Contriever .The results on each sole source corpus on the proposed two newbenchmarks are presented in . It is evident that all retrievalmodels exhibit no significant performance discrepancies in termsof various ranking metrics between the human-written and LLM-generated corpora across all datasets. This observation reinforcesthe confidence in the quality of our newly constructed datasets. 2.3.4Human Evaluation. Note that in our constructed datasets,LLMs were instructed to rewrite human-written texts based solelyon the original human-written text, without any query-related in-put, thereby preventing the additional query-specific informationduring rewriting. Moreover, to further verify this, we conduct a hu-man evaluation. Specifically, we randomly select 50 <query, human-written document, LLM-generated document> triples from eachdataset. The human annotators, comprising the authors and theirhighly educated colleagues, are asked to determine which docu-ment is more semantically relevant to the given query. The optionsare Human, LLM, or Equal. During the evaluation, annota-tors are unaware of the source of each document. Each triple islabeled at least by three different annotators, with the majorityvote determining the final label. The results in , confirm thatboth sources of texts have almost the same semantic relevance tothe given queries, which guarantees the fairness of our followingexploration of source bias.Additionally, we also conduct further human evaluations specifi-cally focused on text quality. The human annotators are asked todetermine Which document exhibits higher quality by consider-ing the following aspects: linguistic fluency, logical coherence, andinformation density? The notation process is the same as above,and the results are summarized in . The results indicate nosignificant distinction between LLM-generated and human-writtencontent on text quality, demonstrating consistency across bothsources. In fact, we also analyze the data cases and find that LLMstypically alter only parts of the vocabulary, leading to minor stylis-tic differences without impacting the core content, which can befurther verified with these human evaluations.",
  "RQ2: UNCOVERING SOURCE BIAS": "In this section, we conduct extensive experiments on the con-structed datasets to explore the source bias from various aspects.With the constructed simulated environment, we first introducethe evaluation metrics to quantify the severity of source bias. Wethen conduct experiments with different retrieval models on boththe first-stage retrieval and the second-stage re-ranking.",
  "Evaluation Metrics for Source Bias": "To quantitatively explore source bias, we calculate ranking metrics,targeting separately either human-written or LLM-generated cor-pus. Specifically, for each query, an IR model produces a rankinglist that comprises documents from mixed corpora. We then calcu-late top- Normalized Discounted Cumulative Gain (NDCG@)and Mean Average Precision (MAP@), for {1, 3, 5}, indepen-dently for each corpus source. When assessing one corpus (e.g.,human-written), documents from the other (e.g., LLM-generated)are treated as non-relevant, though the original mixed-source rank-ing order is maintained. This approach allows us to independentlyassess the performance of IR models on each corpus source.To better normalize the difference among different benchmarks,we also introduce the relative percentage difference as follows:",
  "Bias in Neural Retrieval Models": "In our assessment of various retrieval models on SciFact+AIGCand NQ320K+AIGC datasets, we observe distinct phenomena whenevaluating against human-written and LLM-generated corpora, asreported in . Our key findings are as follows:Lexical models prefer human-written texts. Lexical modelslike TF-IDF and BM25 show a tendency to favor human-writtentexts over LLM-generated texts across most ranking metrics inboth datasets. A plausible explanation for this phenomenon lies inthe term-based distinctions between text generated by LLMs andhuman-written content, as evident in . Additionally, thequeries are crafted by humans and thus exhibit a style more closelyaligned with human-written text.Neural retrievers are biased towards LLM-generated texts.Neural models, which rely on semantic matching with PLMs, demon-strate a pronounced preference for LLM-generated texts, often per-forming over 30% better on these compared to human-written texts.These findings suggest an inherent bias in neural retrievers towardsLLM-generated text, which we named the source bias. This sourcebias may stem from PLMs-based neural retrievers and LLMs sharingsimilar Transformer-based architectures and pretraining ap-proaches, leading to potential exploitation of semantic shortcuts inLLM-generated text during semantic matching. Additionally, LLMsseem to semantically compress information in a manner that makes",
  "KDD 24, August 2529, 2024, Barcelona, SpainSunhao Dai et al": "Jorge Agnese, Jonathan Herrera, Haicheng Tao, and Xingquan Zhu. 2020. Asurvey and taxonomy of adversarial neural networks for text-to-image synthesis.Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 10, 4 (2020),e1345. Qingyao Ai, Ting Bai, Zhao Cao, Yi Chang, Jiawei Chen, Zhumin Chen, ZhiyongCheng, Shoubin Dong, Zhicheng Dou, Fuli Feng, et al. 2023. Information RetrievalMeets Large Language Models: A Strategic Report from Chinese IR Community.AI Open 4 (2023), 8090. Ian L Alberts, Lorenzo Mercolli, Thomas Pyka, George Prenosil, Kuangyu Shi,Axel Rominger, and Ali Afshar-Oromieh. 2023. Large language models (LLM)and ChatGPT: what will the impact on nuclear medicine be? European journal ofnuclear medicine and molecular imaging 50, 6 (2023), 15491552. Sina Alemohammad, Josue Casco-Rodriguez, Lorenzo Luzi, Ahmed Imtiaz Hu-mayun, Hossein Babaei, Daniel LeJeune, Ali Siahkoohi, and Richard G Baraniuk.2023. Self-consuming generative models go mad. arXiv preprint arXiv:2307.01850(2023). Kevin Aslett, Zeve Sanderson, William Godel, Nathaniel Persily, Jonathan Nagler,and Joshua A Tucker. 2023. Online searches to evaluate misinformation canincrease its perceived veracity. Nature (2023), 19. Leif Azzopardi, Mark Girolami, and Keith Van Risjbergen. 2003. Investigating therelationship between language model perplexity and IR precision-recall measures.In Proceedings of the 26th annual international ACM SIGIR conference on Researchand development in informaion retrieval. 369370. Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, BryanWilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. 2023. A multitask,multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, andinteractivity. arXiv preprint arXiv:2302.04023 (2023). Yoshua Bengio, Geoffrey Hinton, Andrew Yao, Dawn Song, Pieter Abbeel, Yu-val Noah Harari, Ya-Qin Zhang, Lan Xue, Shai Shalev-Shwartz, Gillian Had-field, et al. 2023. Managing ai risks in an era of rapid progress. arXiv preprintarXiv:2310.17688 (2023). James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, LongOuyang, Juntang Zhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhari-wal, Casey Chu, and Yunxin Jiao. 2023. Improving Image Generation with BetterCaptions. (2023).",
  "ContrieverHuman-Written24.044.047.223.339.141.0LLM-Generated33.048.350.631.344.045.4Relative -31.6-9.3-7.0-29.3-11.8-10.2": "it more comprehensible to neural models. A deeper explorationinto the causes of source bias is presented in the following section.To strengthen our conclusion that source bias is not limited toany specific LLM, we extend our investigation to include ChatGPT,another widely adopted and nearly state-of-the-art LLM. We em-ploy ChatGPT to generate a corpus using the same prompts as thoseutilized with Llama2 in the above experiments. Subsequently, in Ta-ble 5, we report the evaluation results on the SciFact+AIGC dataset,which contains both human-written and ChatGPT-generated texts.Once again, the results clearly indicate a bias within neural retrievalmodels, favoring the corpus generated by ChatGPT across all rank-ing metrics. This observation provides additional substantiation ofthe presence of source bias within these neural retrieval models.Furthermore, we also explore the popular InstructGPT-promptsGitHub Repository, which includes several common prompts for",
  "Bias in Re-Ranking Stage": "In a typical IR system, there are two primary stages of documentfiltering. The first stage involves a retriever, responsible for doc-ument recall, while the second stage employs a re-ranker, whichfine-tunes the ordering of documents within the initially retrievedset. While we have revealed the presence of the source bias in thefirst stage, a natural pivotal research question remains: does this",
  ": Comparision of the relative singular value (SV) ofthe different corpus after SVD. The singular values are sortedin descending order from left to right": "bias also manifest in the re-ranking stage? To delve into this, weselect two representative and state-or-the-art re-ranking models:MiniLM and monoT5 to rerank the top-100 documentlist retrieved by a first-stage BM25 model.The results on the SciFact+AIGC dataset with Llama-generatedcorpus and ChatGPT-generated corpus are presented in .From the results, while even the first-stage retrievers (BM25) mayexhibit a preference for human-written content, the second-stagere-rankers once again demonstrate a bias in favor of LLM-generatedcontent. Remarkably, the bias in re-ranking models appears to bemore severe, as evidenced by the relative percentage difference of67.3% and 59.4% in NDCG@1 for monoT5, respectively. Thesefindings further confirm the pervasiveness of source bias in neuralranking models that rely on PLMs, regardless of the first retrievalstage or second re-ranking stage.",
  "RQ3: THE CAUSE OF SOURCE BIAS": "In this section, we delve deeper into why neural retrieval mod-els exhibit source bias. Our objective is to determine whether theLLM-generated texts, characterized by reduced noise and more con-centrated semantic topics, are inherently easier for neural retrievalmodels to semantically match. We conduct a series of analyses fromthe perspective of text compression and provide valuable insights.",
  "Viewpoint from Text Compression": "We first explore the cause of source bias from a compression per-spective, drawing inspiration from recent studies that suggest LLMsare lossless compressors . We hypothesize that LLMs efficientlyfocus on essential information, minimizing noise during generation,in contrast to human-written texts, which may include more di-verse topics and incidental noise. To verify this, we employ SingularValue Decomposition (SVD) to compare topic concentrationand noise in human-written and LLM-generated texts. The dimen-sion of the SVD corresponds to the maximum number of topics, andthe singular value associated with each topic represents its strength.High singular values predominantly capture primary topic infor-mation, whereas low singular values indicate noise.Specifically, we utilize the OpenAI embedding model to obtainembedding matrices for each corpus in the SciFact+AIGC datasetand then conduct SVD. The resulting singular values are arranged",
  ": Comparision of the PPL of the different corpus": "in descending order, and their comparison to the human-writtencorpus is visualized in . As we can see, LLM-generatedtexts exhibit larger singular values at the top large singular values,while smaller singular values at the tail small singular values. Thisobservation suggests that LLM-generated texts tend to have morefocused semantics with less noise, rendering them more suitablefor precise semantic matching. In contrast, human-written textsoften contain a wider range of latent topics and higher levels ofnoise, making them harder for neural retrievers to understand. Asa result, this difference in semantic concentration may contributeto the observed source bias in neural retrievers.",
  "Further Analysis from Perplexity": "Considering that most modern neural retrievers are grounded onPLMs , such as BERT , Roberta , and T5 , weanalyze the perplexity of PLMs to further support the conclusionabove from the viewpoint of compression that LLM-generated textscan be better understood by PLMs. Perplexity is an important metricfor evaluating how well a language model can understand a giventext . For a specific language model (LM) and a document =(0,1, ,), the log perplexity is defined as the exponentiatedaverage negative log-likelihood of each token in the tokenizedsequence of 5:",
  ",": "where is the token length of text and LM() is the predictedlikelihood of the -th token conditioned on the context. Lowerperplexity suggests more confidence and understanding of LM fortext patterns, while higher perplexity implies greater uncertaintyin predictions, often arising from complex or unpredictable textpatterns.Using the most widely-used LM, BERT , as an example, weemploy it to calculate the PPL for different corpus. As BERT isnot an autoregressive LM, we follow standard practices to calculate the likelihood of each token conditioned on the othertokens, i.e.,",
  "LM( |context) := BERT( |\\{})": "The distribution of perplexity for different corpus in the SciFact+AIGCdataset is shown in . Notably, LLM-generated texts consis-tently exhibit significantly lower perplexity, indicating enhancedcomprehensibility and higher confidence from BERTs perspective.Consequently, PLMs-based neural retrievers can more effectivelymodel the semantics of LLM-generated texts, leading to the ob-served source bias in favor of LLM-generated texts.",
  "Our Method: A Debiased Constraint": "Our earlier findings of source bias indicate that neural retrieverstend to rank LLM-generated documents in higher positions. Thus,the motivation of our debiased method is straightforward, which isto force the retrieval models to focus on modeling the semantic rele-vance and not assign higher predicted relevance scores to the LLM-generated documents. Specifically, following the practice in Sec-tion 2.2, we first generate the corresponding LLM-generated corpusC for the original human-written training corpus C . In this way,we can get the new paired training data D = {(,,)}=1,where each element (,,) is a <query, human-written docu-ment, LLM-generated document> triplet. and are the corre-sponding human-written and LLM-generated relevant documentsfor the query , respectively. Then we introduce the debiased con-straint, which can be defined as",
  ": Performance comparison of neural retrieverson only human-written SciFact dataset with different co-efficient in our proposed debiased method": "where (,; ) and (, ; ) are the predicted relevance scoresof (,) and (, ) by the retrieval models with parameters ,respectively. This constraint can penalize biased samples when thepredicted relevance score of (,) is greater than that of (, ).Based on the debiased constraint defined in (1), we can definethe final loss for training an unbiased neural retriever:",
  "L = Lrank + Ldebias(2)": "where the Lrank can be any common-used loss for the ranking task,e.g., contrastive loss or regression loss . And is thedebiased co-efficient that can balance the ranking performance andthe degree of the source bias. The larger indicates the greaterpenalty on the biased samples, leading to the retriever being morelikely to rank the human-written texts in higher positions.",
  "Results and Analysis": "To evaluate the effectiveness of our proposed debiased method,we equip the debiased constraint defined in Eq. (1) to two rep-resentative neural retrievers: ANCE and BERM . In theexperiments, we vary the debiased co-efficient within the range of{1-4, 5-4, 1-3, 5-3, 1-2}. The original retrieval models learnedwithout the debiased constraint are denoted as w/o debias. Theresults on the SciFact+AIGC dataset are presented in .As we can see, as the debiased co-efficient increases, the Rela-tive gradually shifts from negative to positive across almost allmetrics and mixed datasets. This trend indicates that the neuralretrieval models can rank human-written text higher than LLM-generated text with large . This can be attributed to the inclusionof our debiased constraint into the learning objective, which canpenalize the biased samples and compel the retrieval models not toassign higher predicted relevance scores to LLM-generated content.Moreover, as shown in , our method not only maintains theretrieval performance on the sole human-written corpus but alsoprovides improvements, especially with BERM as the backbone.This improvement is likely due to the inclusion of LLM-generatedsamples, which might enhance the models ability to discern rele-vance among similar documents.In summary, these empirical results have demonstrated the ef-ficacy of our proposed debiased method in mitigating source biasto different extents by adjusting the debiased coefficient . Thisflexibility allows for customizing debiasing mechanisms to meet di-verse perspectives and demands. Notably, the decision to maintain",
  "Neural Retrievers are Biased Towards LLM-Generated ContentKDD 24, August 2529, 2024, Barcelona, Spain": "equality between the two content sources or favor human-writtencontent can be tailored based on specific requirements and envi-ronmental considerations. For example, users may not mind thecontents source if it is of high quality and fulfills their informa-tional needs. However, bias becomes a significant issue when weaim to credit content providers and encourage more creation, im-pacting the sustainability of the content creation ecosystem. Theoptimal strategy for enhancing the sustainable development of theIR ecosystem remains an open question for further exploration.",
  "DISCUSSION: SOUNDING THE ALARM": "Through a rigorous series of experiments and thorough analysis,we have identified that neural retrieval models demonstrate clearpreferences for LLM-generated texts, referred to as source bias. Thisbias, with the burgeoning proliferation of LLMs and AIGC, mayraise significant concerns for a variety of aspects.First, the presence of source bias poses a significant risk ofgradually rendering human-written content less accessible, po-tentially causing a disruption in the content ecosystem. More se-verely, the concern is escalating with the growing prevalence ofLLM-generated content online. Second, there is the risk thatsource bias may amplify the spread of misinformation, especiallyconsidering the potential of LLMs to generate deceptive content,whether intentionally or not . Third, source bias maybe maliciously exploited to attack against neural retrieval modelswithin todays search engines, creating a precarious vulnerabilitythat could be weaponized by malicious actors, reminiscent of earlierweb spam link attacks against PageRank .As discussed above, since LLMs can be readily instructed togenerate texts at scale, source bias presents potential tangible andserious threats to the ecosystem of web content, public trust, and on-line safety. We hope this discussion will sound the alarm regardingthe risks posed by source bias in the LLM era.",
  "RELATED WORK": "Large Language Models for IR. The emergence of large languagemodels (LLMs) has ushered in a transformative eraacross various research domains, such as natural language process-ing (NLP) , education , recommender systems ,finance , and medicine . In the field of IR, much ef-fort has also been made to utilize the remarkable knowledge andcapabilities of LLMs to enhance IR systems . In the indus-try community, an exemplary successful application is New Bing6,which is an LLM-powered search assistant that adeptly extractsinformation from various web pages and delivers concise sum-marized responses to user queries, thereby improving the searchexperience. In the research community, there has been a proactiveexploration of integrating LLMs into the IR components, includingquery rewriters , retrievers , re-rankers , andreaders . For a more comprehensive overview of the recentadvancements in LLMs for IR, please refer to the recent survey .",
  "Artificial Intelligence Generated Content. Artificial IntelligenceGenerated Content (AIGC) is a rapidly advancing field that involvesthe creation of content using advanced Generative AI (GAI) [1, 12,": "65]. Unlike traditional content crafted by humans, AIGC can be gen-erated at scale and in considerably less time . Recently, thedevelopment of LLMs and other GAI models has greatly improvedthe quality of AIGC content than before. For instance, LLMs suchas ChatGPT have shown impressive abilities in generating human-like content . The DALL-E-3 , another state-of-the-arttext-to-image generation system, can follow user instructions toproduce high-quality images. Nevertheless, as AIGC becomes moreprevalent across myriad domains, ethical concerns, and potentialrisks come into sharper focus . In fact, inevitably, the GAImodels may generate content with bias and discrimination as thelarge training data always contain bias and toxicity . Fur-thermore, researchers have found that LLMs can be manipulatedinto generating increasingly deceptive misinformation, posing chal-lenges to online safety . In addition, some recent studiesindicate that training GAI models with synthetic data could result inthe collapse of the next-generation models . Thus, AIGCis a double-edged sword that requires cautious handling.",
  "CONCLUSION AND FUTURE WORK": "In this paper, we provide a preliminary analysis of the impact of theproliferation of generated content on IR systems, which is a pressingand emerging problem in the LLM era. We first introduce twonew benchmarks, SciFact+AIGC and NQ320K+AIGC, and build anenvironment for evaluating IR models in scenarios where the corpuscomprises both human-written and LLM-generated texts. Throughextensive experiments within this environment, we uncover anunexpected bias of neural retrieval models favoring LLM-generatedtext. Moreover, we provide an in-depth analysis of this bias from theperspective of text compression. We also introduce a plug-and-playdebiased strategy, which shows the potential to mitigate the sourcebias to different degrees. Finally, we discuss the crucial concernsand potential risks of this bias to the whole web ecosystem.Our study offers valuable insights into several promising direc-tions for future research, including exploring source bias in otherinformation systems (e.g., recommender systems and advertisingsystems) and examining source bias in neural models towards AIGCdata across multiple data modalities, not limited to text. Moreover,uncovering the root cause of the source bias and thus further miti-gating it are difficult but crucial research directions. This work was funded by the National Key R&D Program of China(2023YFA1008704), the National Natural Science Foundation ofChina (No. 62377044, 62276248, 62376275, 62076234), Beijing Nat-ural Science Foundation (No. 4222029), Beijing Key Laboratory ofBig Data Management and Analysis Methods, Major Innovation& Planning Interdisciplinary Platform for the Double-First ClassInitiative, PCC@RUC, funds for building world-class universities(disciplines) of Renmin University of China, and the Youth Innova-tion Promotion Association CAS under Grants No.2023111. Thiswork was supported by the Fundamental Research Funds for theCentral Universities, and the Research Funds of Renmin Universityof China (RUC24QSDL013). We thank all the anonymous reviewersfor their positive and insightful comments.",
  "Martin Briesch, Dominik Sobania, and Franz Rothlauf. 2023. Large LanguageModels Suffer From Their Own Output: An Analysis of the Self-ConsumingTraining Loop. arXiv preprint arXiv:2311.16822 (2023)": "Sbastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, EricHorvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023.Sparks of artificial general intelligence: Early experiments with gpt-4. arXivpreprint arXiv:2303.12712 (2023). Yihan Cao, Siyu Li, Yixin Liu, Zhiling Yan, Yutong Dai, Philip S Yu, and LichaoSun. 2023. A comprehensive survey of ai-generated content (aigc): A history ofgenerative ai from gan to chatgpt. arXiv preprint arXiv:2303.04226 (2023).",
  "Sukmin Cho, Soyeong Jeong, Jeongyeon Seo, and Jong C Park. 2023. DiscretePrompt Optimization via Constrained Generation for Zero-shot Re-ranker. arXivpreprint arXiv:2305.13729 (2023)": "Sunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu, Zihua Si, Chen Xu, Zhongx-iang Sun, Xiao Zhang, and Jun Xu. 2023. Uncovering ChatGPTs Capabilitiesin Recommender Systems. In Proceedings of the 17th ACM Conference on Recom-mender Systems. Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov,Kelvin Guu, Keith B Hall, and Ming-Wei Chang. 2022. Promptagator: Few-shotdense retrieval from 8 examples. arXiv preprint arXiv:2209.11755 (2022). Grgoire Deltang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, TimGenewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang, MatthewAitchison, Laurent Orseau, et al. 2023. Language modeling is compression. arXivpreprint arXiv:2309.10668 (2023). Ameet Deshpande, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, andKarthik Narasimhan. 2023. Toxicity in chatgpt: Analyzing persona-assignedlanguage models. arXiv preprint arXiv:2304.05335 (2023). Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding. InProceedings of the 2019 Conference of the North American Chapter of the Associationfor Computational Linguistics: Human Language Technologies. 41714186. Wenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, JiliangTang, and Qing Li. 2023. Recommender systems in the era of large languagemodels (llms). arXiv preprint arXiv:2307.02046 (2023). Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding,Jianwei Yue, and Yupeng Wu. 2023. How Close is ChatGPT to Human Experts?Comparison Corpus, Evaluation, and Detection. arXiv preprint arXiv:2301.07597(2023).",
  "Zoltn Gyngyi, Hector Garcia-Molina, and Jan Pedersen. 2004. Combating webspam with trustrank. In Proceedings of the Thirtieth international conference onVery large data bases-Volume 30. 576587": "Hans WA Hanley and Zakir Durumeric. 2023. Machine-Made Media: Moni-toring the Mobilization of Machine-Generated Articles on Misinformation andMainstream News Websites. arXiv preprint arXiv:2305.09820 (2023). Sebastian Hofsttter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin, and AllanHanbury. 2021. Efficiently teaching an effective dense retriever with balancedtopic aware sampling. In Proceedings of the 44th International ACM SIGIR Confer-ence on Research and Development in Information Retrieval. 113122.",
  "Allen H Huang, Hui Wang, and Yi Yang. 2023. FinBERT: A large language modelfor extracting information from financial text. Contemporary Accounting Research40, 2 (2023), 806841": "Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bo-janowski, Armand Joulin, and Edouard Grave. 2021. Unsupervised dense in-formation retrieval with contrastive learning. arXiv preprint arXiv:2112.09118(2021). Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni,Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and EdouardGrave. 2022. Few-shot learning with retrieval augmented language models. arXivpreprint arXiv:2208.03299 (2022).",
  "Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. 2020. Document ranking with apretrained sequence-to-sequence model. arXiv preprint arXiv:2003.06713 (2020)": "Desnes Nunes, Ricardo Primi, Ramon Pires, Roberto de Alencar Lotufo, andRodrigo Nogueira. 2023. Evaluating GPT-3.5 and GPT-4 Models on BrazilianUniversity Admission Exams. ArXiv abs/2303.17003 (2023). Yikang Pan, Liangming Pan, Wenhu Chen, Preslav Nakov, Min-Yen Kan, andWilliam Yang Wang. 2023. On the Risk of Misinformation Pollution with LargeLanguage Models. arXiv preprint arXiv:2305.13661 (2023). Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits oftransfer learning with a unified text-to-text transformer. The Journal of MachineLearning Research 21, 1 (2020), 54855551.",
  "Jinyan Su, Terry Yue Zhuo, Jonibek Mansurov, Di Wang, and Preslav Nakov. 2023.Fake News Detectors are Biased against Texts Generated by Large LanguageModels. arXiv preprint arxiv:2309.08674 (2023)": "Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin, and ZhaochunRen. 2023. Is ChatGPT Good at Search? Investigating Large Language Models asRe-Ranking Agent. arXiv preprint arXiv:2304.09542 (2023). Himanshu Thakur, Atishay Jain, Praneetha Vaddamanu, Paul Pu Liang, and Louis-Philippe Morency. 2023. Language Models Get a Gender Makeover: MitigatingGender Bias with Few-Shot Data Interventions. In Proceedings of the 61st AnnualMeeting of the Association for Computational Linguistics. 340351. Nandan Thakur, Nils Reimers, Andreas Rckl, Abhishek Srivastava, and IrynaGurevych. 2021. BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation ofInformation Retrieval Models. In Thirty-fifth Conference on Neural InformationProcessing Systems Datasets and Benchmarks Track. Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, LauraGutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. 2023. Large language modelsin medicine. Nature medicine 29, 8 (2023), 19301940. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-mine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-ale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXivpreprint arXiv:2307.09288 (2023).",
  "Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.Journal of machine learning research 9, 11 (2008)": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin. 2017. Attention is allyou need. Advances in neural information processing systems 30 (2017). David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen,Arman Cohan, and Hannaneh Hajishirzi. 2020. Fact or fiction: Verifying scientificclaims. arXiv preprint arXiv:2004.14974 (2020).",
  "Tao Wang, Yushu Zhang, Shuren Qi, Ruoyu Zhao, Zhihua Xia, and Jian Weng.2023. Security and privacy on generative data in aigc: A survey. arXiv preprintarXiv:2309.09435 (2023)": "Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou.2020. Minilm: Deep self-attention distillation for task-agnostic compression ofpre-trained transformers. Advances in Neural Information Processing Systems 33(2020), 57765788. Xiting Wang, Xinwei Gu, Jie Cao, Zihua Zhao, Yulan Yan, Bhuvan Middha, andXing Xie. 2021. Reinforcing pretrained models for generating attractive textadvertisements. In Proceedings of the 27th ACM SIGKDD Conference on KnowledgeDiscovery & Data Mining. 36973707. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, SebastianBorgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al.2022. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682(2022).",
  "Jiayang Wu, Wensheng Gan, Zefeng Chen, Shicheng Wan, and Hong Lin. 2023.Ai-generated content (aigc): A survey. arXiv preprint arXiv:2304.06632 (2023)": "Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebas-tian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann.2023.Bloomberggpt: A large language model for finance.arXiv preprintarXiv:2303.17564 (2023). Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett,Junaid Ahmed, and Arnold Overwijk. 2020. Approximate nearest neighbor nega-tive contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808(2020). Jun Xu and Hang Li. 2007. Adarank: a boosting algorithm for informationretrieval. In Proceedings of the 30th annual international ACM SIGIR conference onResearch and development in information retrieval. 391398. Shicheng Xu, Liang Pang, Huawei Shen, and Xueqi Cheng. 2022. Match-Prompt:Improving Multi-task Generalization Ability for Neural Text Matching via PromptLearning. In Proceedings of the 31st ACM International Conference on Information& Knowledge Management. 22902300. Shicheng Xu, Liang Pang, Huawei Shen, and Xueqi Cheng. 2023. BERM: Trainingthe Balanced and Extractable Representation for Matching to Improve General-ization Ability of Dense Retrieval. In Proceedings of the 61st Annual Meeting of",
  "the Association for Computational Linguistics. 66206635": "Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, HaomingJiang, Bing Yin, and Xia Hu. 2023. Harnessing the power of llms in practice: Asurvey on chatgpt and beyond. arXiv preprint arXiv:2304.13712 (2023). Andrew Yates, Rodrigo Nogueira, and Jimmy Lin. 2021. Pretrained transformersfor text ranking: BERT and beyond. In Proceedings of the 14th ACM InternationalConference on web search and data mining. 11541156.",
  "Wayne Xin Zhao, Jing Liu, Ruiyang Ren, and Ji-Rong Wen. 2023. Dense TextRetrieval based on Pretrained Language Models: A Survey. ACM Trans. Inf. Syst.(dec 2023)": "Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A surveyof large language models. arXiv preprint arXiv:2303.18223 (2023). Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chen-long Deng, Zhicheng Dou, and Ji-Rong Wen. 2023. Large language models forinformation retrieval: A survey. arXiv preprint arXiv:2308.07107 (2023).",
  "= 1": ": A toy example to illustrate how ranking metricsare calculated for each target corpus. In this toy example,given a query, the top 6 documents are retrieved and therank is from left to right in descending order. Two relevantdocuments (i.e., 1 and 1 ) are highlighted with red boxes. In this section, we provide a toy example for illustrating thecalculation of the evaluation metrics for exploring source bias,as depicted in . Specifically, for each query, an IR modelproduces a ranking list that comprises documents from both human-written and LLM-generated corpora. We then calculate rankingmetrics separately for human-written and LLM-generated texts,depending on the target data source. When computing metrics forone target (e.g., human-written corpus C ), the data correspondingto the other side (e.g., LLM-generated corpus C) is disregarded (i.e.,mask all the positive label R as negative), but the rank of eachdocument is still based on the original ranking list that incorporatesa mixture of both types of text. For instance, in this toy example,when targeting the human-written corpus, the relevant document1 generated by LLM is treated as a negative sample. And whencalculating the ranking metrics, we only consider the rank of thepositive human-written documents. When the target corpus is LLM-generated, we adopt the same principle, i.e., only take the rank ofthe positive LLM-generated documents into account for calculatingthe ranking metrics.",
  "PromptANCEBERMTAS-BContriever": "Rewrite the text below in your own words:-7.0-22.8-11.2-27.6Paraphrase the provided text while maintaining its meaning:-26.0-55.3-24.1-13.6Summarize the following passage in a concise manner:-1.4-43.3-34.0-32.4Simplify the given passage while keeping the main ideas:-29.0-21.7-22.5-40.9Rephrase the given text using alternative expressions:-25.3-34.7-61.9-18.4Condense the following passage to focus on key points:-19.0-24.8-22.8-16.4Briefly restate the provided text without losing its essence:-29.6-50.7-41.3-29.8Reword the passage below to make it more succinct:-40.6-71.1-54.7-34.0Express the following text in a different way while keeping its intent:-50.7-39.8-34.90.0",
  "CTHEORETICAL ANALYSIS AND INSIGHTS": "In , we have compared the PPL for different corpus usingthe BERT model. In this section, we aim to further provide sometheoretical insights into the above observations that LLM-generatedtexts have a smaller perplexity than human-written texts.Without loss of generality, we define the PPL in an autoregressivemanner. Let denote a document written by humans, and adocument generated by an LLM conditioned on . For a givendocument and BERT model B, PPL is calculated as",
  "=1log BERT( |<, )": "When evaluated by humans, we use PPL( | , H) to representthe PPL of conditioned on .In the theorem below, we introduce three assumptions: Seman-tic Superiority, Conditional Redundancy, and Bounded Perplexity,to theoretically establish the sufficient conditions under whichPPL(, B) PPL(, B) holds. Semantic Superiority suggeststhat the perplexity of human-written texts, when evaluated byhumans, is lower than when evaluated by BERT. Conditional Re-dundancy implies that the perplexity of , given , is lower thanthe perplexity of when evaluated directly. This is intuitivelytrue when the information added in generating from to",
  "ELLM( | )PPL(, B) PPL(, B) 0": "In Theorem C.1, the KL divergence is used to compare the distri-butions of the document conditioned on according to theLLM, BERT model, and humans. It is worth emphasizing that in-equation (3) is not the assumption on the understanding capabilitiesof BERT, LLM, and humans. Instead, this inequation assumes thatwhen predicting given , the predictions by LLM are moreclosely aligned with those of BERT.We demonstrate that, when inequation (3) is satisfied, the per-plexity (evaluated by PLMs such as BERT) of is lower thanthat of . Wed like to emphasize that it is reasonable to expectthat inequation (3) holds true because both LLM and BERT areTransformer-based models that use similar pretraining paradigms.The commonality in model structure and learning paradigms maylead to similar inherent biases in text prediction, making their pre-dictions more aligned with each other.The proof for Theorem C.1 is provided as follows:"
}