{
  "Abstract": "Large language models (LLMs) have attracted considerable atten-tion in various fields for their cost-effective solutions to diversechallenges, especially with advancements in instruction tuning andquantization. E-commerce, with its complex tasks and extensiveproduct-user interactions, presents a promising application area forLLMs. However, the domain-specific concepts and knowledge inher-ent in e-commerce pose significant challenges for adapting generalLLMs. To address this issue, we developed EC-Guide1, a compre-hensive e-commerce guide for instruction tuning and quantizationof LLMs. We also heuristically integrated Chain-of-Thought (CoT)during inference to enhance arithmetic performance. Our approachachieved the 2nd place in Track 2 and 5th place in Track 5 at theAmazon KDD Cup242. Additionally, our solution is model-agnostic,enabling effective scalability across larger systems.",
  "Equal contribution.Corresponding author.1": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from , Aug 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-1-4503-XXXX-X/18/06 array of products and languages . With the advent of large lan-guage models (LLMs), there is a growing belief in their capabilityto tackle these challenges. To this end, the organizers of AmazonKDD Cup24 introduced ShopBench, a benchmark designed to sim-ulate the complexities of online shopping. It includes 57 tasks andapproximately 20,000 questions sourced from real-world Amazonshopping data. The competition includes 5 Tracks:",
  "Related Works": "As a data-driven technology, LLMs exhibit exceptional performance,promoting the development of numerous datasets. GSM8K fo-cuses on grade school mathematics requiring multi-step reasoning.ECInstruct introduces diverse e-commerce subtasks to guidethe instruction tuning of general LLMs. Amazon-M2 is a multilin-gual product recommendation dataset enhancing the understandingof user preferences. Additionally, other datasets provide varied formats for evaluating LLMs. To further explore e-commerce applications, we developed EC-Guide, a comprehensiveguide for instruction tuning and quantization.Apart from the rapid development of datasets, researchers are in-creasingly focusing on training and inference techniques. LoRA significantly reduces fine-tuning costs by freezing LLMs weightsand injecting a learnable matrix. QLoRA further reduces costsby introducing quantization techniques. GPTQ is a one-shotweight quantization for LLMs by leveraging approximate second-order information to compress models down to lower bits (like4-bit) per weight with minimal impact on performance. CoT en-hances the reasoning ability of LLMs through appropriate promptswithout additional training. Based on these methods, we adaptedgeneral LLM to specific domains through instruction tuning anddeployed by quantization and CoT within limited resources.",
  "Methodology": "We first constructed our instruction tuning dataset, EC-Guide, bymining existing datasets , and utilizing ChatGPT3 for datageneration. We also expanded the official development dataset toenhance evaluation. Through instruction tuning, we embeddedknowledge specifically relevant to e-commerce tasks into existingLLMs. However, in Round 2, solutions had access to 4NVIDIA T4(16GB) GPUs4, making it impractical to directly deploy powerfulLLMs like Yi-1.5-34B or Qwen2-72B without quantization.During inference, we applied CoT to further boost LLM calculationperformance. Details are as follows.",
  "Dataset Construction": "Due to the lack of large-scale training datasets for the AmazonKDD Cup24, we employed paraphrasing and extending existingdatasets to develop EC-Guide, encompassing 74k examplesacross five task types: Generation, Multiple Choice Question, Re-trieval, Ranking, and Named Entity Recognition. Statistic detailsare shown in . Furthermore, we expanded the official de-velopment dataset from 96 to 506 examples by strategies such asoption reordering and segment sampling from our EC-Guide, whichsignificantly enhances the robustness of the development datasetto effectively evaluate our model. 3.1.1Generation. In the generation task, each question is pairedwith a specific instruction, and the model is to generate text thatprecisely follows these instructions. There are multiple types of gen-eration questions, including elaboration, extraction, summarizationand translation.For the elaboration set, we introduced the following subtasks:1. Product Elaboration (PE): We utilized the Attribute Value Ex-traction from ECInstruct to collect type-related attributes,then filtered to obtain 479 entities, and used ChatGPT to gener-ate detailed descriptions for each entity. 2. Product Question andAnswer (PQA): We selected samples from McAuley and Yang with only one answer, filtered them based on answer length, and",
  "ALL-74,704-": "finally obtained 6,834 QA pairs. 3. Category Recognition (CR): Weextracted product-category relationships from McAuley et al. ,and sampled 1,000 entries to prompt the model to recognize cate-gories from product titles. 4. Explaining Pairwise Fashion (EPF): Wemodified prompts from the PFE dataset and filtered based onresponse length, yielding 3,000 instances to evaluate whether thementioned clothes matched. 5. Explaining Bought Together (EBT):We directly integrated 2,315 entries from IntentionQA to assessthe reasons for buying two products together.For the extraction set, we introduced two subtasks: extractingreview keyphrases (ERK) and extracting product keyphrases (EPK),which focus on extracting keyphrases related to aspects or featuresfrom reviews and products, respectively. ERK is based on the Sen-timent Analysis of ECInstruct , where we used ChatGPT toextract aspect-keyphrase pairs from reviews and obtained 1,000",
  "EC-Guide: A Comprehensive E-Commerce Guide for Instruction Tuning and QuantizationKDDCup24, Aug 2529, 2024, Barcelona, Spain": "samples. EPK is sourced from the PFE dataset , where we fil-tered out data items with overly short descriptions and extractedfeature-keyphrase pairs, yielding 3,000 cases.The summarization set includes two subtasks: 1. Product Key-word Summarization (PKS) involves summarizing keywords listto encapsulate product information. We sampled product infor-mation from existing datasets , and used ChatGPT forannotation to produce 1,296 entries. 2. Review Title Summarization(RTS) aims to create concise titles for reviews. We filtered previousdataset to obtain 1,455 cases.For our translation set, we employed ChatGPT to translate prod-uct titles between English and several other languages, includingSpanish, German, Italian, Japanese, and French. We also translatedfrom other languages into English, resulting in a total of 2,000 trans-lation pairs. Additionally, we utilized samples from Flores toenhance our translation tasks, resulting in 997 additional pairs. 3.1.2Multiple Choice Question. Multiple choice question, widelyused for their objective assessment, require selecting the correctanswer from the choices list identified by Arabic numerals. Specifi-cally, we include the following subtasks: 1. Select Product based onAttribute (SPA): We sampled attributes from the Attribute ValueExtraction of ECInstruct , and generated product options, re-sulting in 520 data entries. 2. Select Attribute based on Product(SAP): Using product titles extracted from Amazon Reviews , wegenerated multiple choice questions about attributes with ChatGPT,yielding 1,385 data entries. 3. Product Relation Prediction (PRP): Weextracted 1,499 cases from Product Relation Prediction and Prod-uct Matching in ECInstruct . The former predicts potentialpurchase or browsing intentions between two products, while thelatter judges whether two products are the same. 4. Query ProductRelation Prediction (QPRP): Based on relationships between queriesor products in the Shopping Queries Dataset , we constructed2,150 cases. 5. Query Product Relation Judgement (QPRJ): Sampledfrom Product Substitute Indentification of ECInstruct , QPRJincludes 501 examples. 6. Sentiment Analysis (SA): We directlyextracted 3,500 data entries from Sentiment Analysis of ECIn-struct and Womens Clothing E-Commerce Reviews . 7.Product Keyword Summarization (PKS): Using a method similar toPKS in .1.1, we obtained 271 cases. 8. Multilingual Descrip-tion Matching (MDM): MDM comprises 300 cases, aiming to matchproduct titles with the correct features in multiple languages. Wefiltered products from Amazon Reviews and translated theirfeatures into various languages (English, Spanish, German, Ital-ian, Japanese, and French) with ChatGPT. And then we sampledthree features from other products to form the options for each finalquestion. 9. Arithmetic and Commonsense Reasoning (ACR): We ob-tained 7,184 items from GSM8K and CommonsenseQA . Andwe annotated the rationale with ChatGPT for CommonsenseQA. 3.1.3Retrieval. In this task, the models objective is to retrieveanswers from a list of candidate items to meet specific requirements.The subtasks are as follows: 1. Inferring Potential Purchases (IPP):There are two main categories multi-to-one and one-to-multi inIPP. For the multi-to-one, which predicts the next purchase itembased on multiple items in the purchase history, we filtered casesfrom the Sequential Recommendation in ECInstruct to obtain3,950 cases. For the one-to-multi, which predicts multiple potential purchase items based on a single purchased item, we mined thedata from Amazon-M2 , resulting in a total of 6,824 cases. 2. Re-trieving Review Snippets (RRS): We sampled Amazon Reviews to obtain 3,000 products and their corresponding reviews. Thenwe generated multiple aspect-snippet pairs from the reviews withChatGPT. Finally, considering the proportion of positive and neg-ative reviews, we created a total of 810 data entries. 3. RetrievingReview Aspects (RRA): Using the same data source as RRS, we ran-domly combined reviews and aspects to obtain 1,000 data entries.4. Category Recognition (CR): Similar to the CR in .1.1,we constructed retrieval lists by randomly sampling categories,resulting in 7,500 data items. 5. Product Recognition (PR): Usingthe data from CR, we reversed the product-category table to createa category-product dictionary, resulting in 2,297 data entries. 3.1.4Ranking. In this task, the models goal is to reorganize itemsin the candidate list based on how well they meet the requirements.We utilized the Shopping Queries Dataset , which assesses therelevance between the queries and products using ESCI judgments(Exact, Substitute, Complement, Irrelevant). Furthermore, leverag-ing the multilingual source dataset, our ranking instances comprises2,064 queries in English, 790 in Japanese, and 1,184 in Spanish. 3.1.5Named Entity Recognition. Named Entity Recognition (NER)is an important benchmark for evaluating LLMs and is prevalentacross various domains. In this task, we extracted 1,446 entries fromAttribute Value Extraction of ECInstruct , 1,099 entries fromAmazon Reviews , and 4,884 entries from Rifat et al. .",
  "Instruction Tuning": "Instruction tuning enhances the models ability to generate contex-tually appropriate answers based on the given instruction, whichallows the model to understand and execute tasks specified in theinstructions. During training, the objective is to optimize the model using a curated dataset D = { (),()}=1 by minimizing thenegative log-likelihood of the output :",
  "Post Training Quantization": "It is widely accepted that larger scale generally leads to greatermodel capabilities . However, LLMs typically store parame-ters in high-precision floating-point formats, demanding significantcomputational resources for inference. Deploying larger modelsnecessitates effective compression methods, such as quantization,to accelerate inference and save memory. Therefore, we utilizeGPTQ , which is a training-free quantization for LLMs to achievehigh accuracy and efficiency. We detailed the memory of weightsloaded in . In particular, we equally sample training exam-ples based on task types from our training set, ultimately allocate",
  "Chain-of-Thought (CoT) Reasoning": "After instruction tuning and quantization on the elaborately de-signed EC-Guide, we introduced CoT in the inference only formultiple choice questions involving calculations. Specifically, weemployed a heuristic strategy to determine if a question belongto arithmetic-based by counting the number of digits. We thenprefixed the prompt with Lets think step by step., stimulatingthe model to generate a rationale that leads to the correct answer.Notably, we observed that applying CoT to Track 2 in Round 1increased the score from 0.7417 to 0.7908 with the same model.",
  "Experiments": "We finetuned all models on 4A40 GPUs or 8RTX3090 GPUs withQLoRA. We deployed LLMs by vllm6, which utilizes PagedAttentionto manage attention keys and values, to accelerate inference. demonstrates the performance of different models withthe same training setting. Notably, Yi-1.5-34B achieved the high-est scores across both Track 2 and 5 in both development andofficial test set. We also observed that models quantized using out-of-domain datasets C4 exhibited significant performance dropscompared to those using in-domain sampled data. presentsour ablation study, which highlights the influence of different ratiosof task types in training set, and suggests that smaller training setssometimes outperform larger ones in specific scenarios. This obser-vation leads us to hypothesize about the existence of a performancetrade-off among different tasks.",
  "The Amazon KDD Cup24 competition presents a unique challengeby focusing on the application of LLMs in E-commerce across mul-tiple tasks. Our solution for addressing Tracks 2 and 5 involves": "a comprehensive pipeline encompassing dataset construction, in-struction tuning and post-training quantization. The core of ourstrategy is EC-Guide specifically tailored for E-commerce scenarios.Notably, we heuristically integrated CoT reasoning to enhance thearithmetic capabilities of LLMs, resulting in improved performancein both Tracks. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun,Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,Christopher Hesse, and John Schulman. 2021. Training Verifiers to Solve MathWord Problems. arXiv preprint arXiv:2110.14168 (2021).",
  "Yupeng Hou, Jiacheng Li, Zhankui He, An Yan, Xiusi Chen, and Julian McAuley.2024. Bridging language and items for retrieval and recommendation. arXivpreprint arXiv:2403.03952 (2024)": "Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, SheanWang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of largelanguage models. arXiv preprint arXiv:2106.09685 (2021). Wei Jin, Haitao Mao, Zheng Li, Haoming Jiang, Chen Luo, Hongzhi Wen, HaoyuHan, Hanqing Lu, Zhengyang Wang, Ruirui Li, et al. 2024. Amazon-m2: Amultilingual multi-locale shopping session dataset for recommendation and textgeneration. Advances in Neural Information Processing Systems 36 (2024). Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and YusukeIwasawa. 2022. Large language models are zero-shot reasoners. Advances inneural information processing systems 35 (2022), 2219922213. Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel.2015. Image-based recommendations on styles and substitutes. In Proceedingsof the 38th international ACM SIGIR conference on research and development ininformation retrieval. 4352.",
  "nicapotato. 2018. Womens E-Commerce Clothing Reviews": "Bo Peng, Xinyi Ling, Ziru Chen, Huan Sun, and Xia Ning. 2024. eCeLLM: Gener-alizing Large Language Models for E-commerce from Large-scale, High-qualityInstruction Data. arXiv preprint arXiv:2402.08831 (2024). Chandan K Reddy, Llus Mrquez, Fran Valero, Nikhil Rao, Hugo Zaragoza,Sambaran Bandyopadhyay, Arnab Biswas, Anlu Xing, and Karthik Subbian. 2022.Shopping queries dataset: A large-scale ESCI benchmark for improving productsearch. arXiv preprint arXiv:2206.06588 (2022)."
}