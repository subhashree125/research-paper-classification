{
  "Abstract": "Recommender systems require the simultaneous optimization ofmultiple objectives to accurately model user interests, necessitatingthe application of multi-task learning methods. However, existingmulti-task learning methods in recommendations overlook the spe-cific characteristics of recommendation scenarios, falling short inachieving proper gradient balance. To address this challenge, weset the target of multi-task learning as attaining the appropriatemagnitude balance and the global direction balance, and proposean innovative methodology named GradCraft in response. Grad-Craft dynamically adjusts gradient magnitudes to align with themaximum gradient norm, mitigating interference from gradientmagnitudes for subsequent manipulation. It then employs projec-tions to eliminate gradient conflicts in directions while consideringall conflicting tasks simultaneously, theoretically guaranteeing theglobal resolution of direction conflicts. GradCraft ensures the con-current achievement of appropriate magnitude balance and globaldirection balance, aligning with the inherent characteristics of rec-ommendation scenarios. Both offline and online experiments attestto the efficacy of GradCraft in enhancing multi-task performance inrecommendations. The source code for GradCraft can be accessedat",
  "*Work done at Kuaishou.Corresponding author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain. 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08",
  "Multi-task Learning; Recommender System; Gradient Crafting": "ACM Reference Format:Yimeng Bai, Yang Zhang, Fuli Feng, Jing Lu, Xiaoxue Zang, Chenyi Lei, YangSong. 2024. GradCraft: Elevating Multi-task Recommendations throughHolistic Gradient Crafting. In Proceedings of the 30th ACM SIGKDD Con-ference on Knowledge Discovery and Data Mining (KDD 24), August 2529, 2024, Barcelona, Spain. ACM, New York, NY, USA, 10 pages.",
  "Introduction": "Recommender systems assume a pivotal role in personalized in-formation filtering, significantly shaping individual online expe-riences . The effectiveness of the systems often hingeson the ability to thoroughly model user interests, which typicallyentails simultaneously optimizing multiple user feedback that re-flects different facets of user satisfaction . For instance, ashort video recommender system needs to optimize both the time ofwatching a video and the likelihood of liking it . Consequently,there has been an increasing trend towards applying multi-tasklearning in recommender systems to model the various facets ofuser satisfaction simultaneously , forming the mainstream ap-proach in major industry applications .Multi-task learning aims to optimize multiple objectives simul-taneously. Current approaches in recommendation predominantlyinvolve the direct application of general multi-task optimizationmethods from machine learning. These methods typically focus onachieving a proper balance among tasks to prevent negative transfereffects from two gradient perspectives. The first line of work in-volves reweighting loss, adjusting the gradient magnitudes based onspecific criteria such as uncertainty and update speed",
  "KDD 24, August 2529, 2024, Barcelona, Spain.Yimeng Bai et al": ": Performance comparison between the baselines and our GradCraft on Wechat and Kuaishou, where the best results arehighlighted in bold and sub-optimal results are underlined. The labels Follow and Forward are respectively abbreviated asFol and For for simplicity. AV-A and AV-G denote the average value of AUC and GAUC across different tasks, respectively.Similarly, RI-A and RI-G signify the relative improvements of AUC and GAUC.",
  "Preliminary2.1Multi-task Recommendation": "Multi-task recommendation aims to optimize multiple recommen-dation objectives simultaneously. Let D represent the historicaldata. Each sample in D is denoted as (,), where represents thefeatures of a user-item pair, and = [1, . . . , ] denotes distincttask labels of user behaviors, such as Effective View andLike. The target is to learn a multi-task recommender model thatuses to predict the labels by fitting D. Each task involves theprediction of a specific label , and corresponds to a specific lossobjective , which can be expressed as",
  "= ( (),; D)( = 1, . . . ,),(1)": "where denotes the common recommendation loss function, suchas Binary Cross Entropy (BCE) loss and Mean Squared Error(MSE) loss . Here, for briefness, we omit the regularization termwhich is widely adopted to prevent overfitting.Multi-task Optimization. To optimize the multiple objectives, ex-isting methodologies adhere to a unified paradigm: initially, thegradients of different tasks are manipulated and then combinedinto a single gradient using specialized methods; subsequently, themodel parameters are updated according to the combined result.Each task gradient can be obtained through backpropagation. For-mally, the gradient of the -th task can be represented as",
  "Gradient Balance": "Recommendation tasks often exhibit the significant heterogeneityacross various aspects, such as data sparsity . This hetero-geneity can lead to differences in gradient magnitudes and incon-sistencies in update directions among tasks, leading to potentialnegative transfer effects . To mitigate such effects, it is essentialto achieve magnitude and direction balance. 2.2.1Magnitude Balance. The assessment of the magnitude of atask gradient typically relies on its norm , denoted as .Magnitude balance concerns the consistency in the magnitudes ofdifferent task gradients, aiming to prevent situations where tasks and exhibit a significant difference in magnitudes, expressed as",
  "GradCraft: Elevating Multi-task Recommendations through Holistic Gradient CraftingKDD 24, August 2529, 2024, Barcelona, Spain": "video, along with diverse user feedback. We randomly split theminto training, validation, and test sets, following an 8:1:1 ratio.In short-video recommendation, there are two types of tasks:those related to viewing behaviors and those related to interactivebehaviors. Therefore, we set user usage time and engagement asour optimization objectives, which are assessed using viewing la-bels and engagement labels. Specifically, we select EffectiveView(EV) , LongView (LV) , and CompleteView (CV) as view-ing labels. EV indicates whether the watch time of an example hasexceeded 50% of the overall watch time in the dataset, while LVindicates whether the watch time has exceeded 75%. CV reflectswhether the watch time of an example has surpassed the videoduration. For engagement labels, we directly use Like, Follow, andForward. All labels above are binary and fitted with BCE loss.",
  "Overview": "We aim to achieve a simultaneous balance in both the gradientmagnitude and direction. To accomplish this, we propose a sequen-tial paradigm that involves aligning gradient norms followed byprojection operations, as illustrated in . Firstly, we dynami-cally align gradient norms across all tasks based on the maximumnorm, establishing an appropriate balance in magnitudes. Secondly,using this balanced outcome, we apply projections to eliminategradient conflicts while considering all conflicting tasks concur-rently, ensuring a global balance in directions. Finally, we mergethe gradients and update the recommender model. Given that ourmethod operates at the gradient level, we name it GradCraft.",
  "Magnitude Adjustment": "In order to mitigate interference arising from differences in gradientmagnitudes across tasks, our primary focus lies in the adjustmentof gradients to ensure an appropriate level of magnitude balance.Rather than pursuing absolute uniformity of gradient norms across different tasks, we aim to prevent excessive differences in norms,such as those spanning multiple orders of magnitude. This helpsavert dominance by certain tasks while preserving task specificity.To achieve this, for each task, we adjust its gradient norm by com-bining its original norm with the maximum norm among tasks.Formally, the adjustment is performed as",
  "Global Direction Deconfliction": "After adjusting the magnitudes, we aim to achieve the global gra-dient balance. For each task, we utilize projections to ensure itsgradient does not conflict with the gradients of all other tasksconcurrently. Subsequently, we linearly combine the deconflictedgradients from all tasks for the final model updating.Gradient projection. For a given task gradient , we denote thegradients conflicting with it as = [ 1, . . . , ] R, where represents the -th conflicting gradient. We define a projectiontarget to achieve non-negative similarities between the deconflictedgradient and all conflicting gradients as",
  "= [ 1 , . . . , ],(6)": "where represents the deconflicted task gradient, and 0 servesas a factor for adjusting the desired similarity, with a higher valueindicating higher positive similarity. Notably, instead of solely pur-suing gradient orthogonality ( = 0) between tasks, we require acertain level of positive similarity to emphasize the positive transferof knowledge across tasks, thereby enhancing conflict resolution.Theoretically, the desired gradient could be obtained as thesum of the original gradient and the projection onto the linear spaceof all conflicting gradients, which can be formulated as",
  "=1,(10)": "where denotes the learning rate.Algorithm summarization. The intricacies of GradCraft are eluci-dated in Algorithm 1. During the implementation phase, updates areperformed on a batch of data. In each iteration, the algorithm com-mences by computing all task gradients (lines 4-7). Subsequently, itapplies magnitude adjustments to ensure an appropriate magnitudebalance, avoiding interference from the gradient magnitude (lines9-11). Following this, if conflicts arise among task gradients, a gra-dient projection method is employed to ensure a global directionbalance for each task (lines 13-19). Ultimately, the gradients fordifferent tasks are combined to update the model parameters (line21). It is important to highlight that the update process is adaptableenough to accommodate various optimizers such as Adam and Adagrad . Besides, the update process exclusively involvesupdating the shared model parameters, which aligns with the ap-proach established in previous research .",
  "1.(11)": "Disregarding the magnitude adjustment to gradients here, this com-putation aligns with the normal conflict projection method.In comparison, our method simultaneously addresses all conflict-ing tasks for each task while requiring a certain level of positivesimilarity, resulting in global and thorough conflict resolution. No-tably, our method does not significantly introduce extra computa-tion complexity. Considering R, where is the numberof conflicting task gradients for , we can efficiently compute itsinverse in Equation (9) and obtain deconflicted gradients.",
  "Experiment": "In this section, we conduct a series of experiments to answer thefollowing research questions:RQ1: How does GradCraft perform on recommendation data com-pared to existing multi-task learning methods?RQ2: What is the impact of the individual components of GradCrafton its effectiveness?RQ3: How do the specific hyper-parameters of GradCraft influenceits recommendation performance?RQ4: How is the scalability of GradCraft across different levels ofthe gradient imbalance?RQ5: How effective is GradCraft when applied to real industryrecommender systems?",
  "Datasets. We conduct extensive experiments on an open-world dataset and our product dataset: Wechat and Kuaishou": "Wechat. This public dataset is released as part of the WeChatBig Data Challenge1, capturing user behaviors on short videosover a two-week period. To ensure dataset quality, we applieda 10-core filtering process, ensuring that each user/video has aminimum of 10 samples. Kuaishou. This dataset is sourced from our Kuaishou2 platform,reflecting a real-world scenario for short video recommendations.It comprises short video recommendation records for 10,000 usersover a five-day period. Due to the sparser nature of the dataset,we applied a 20-core filtering process during preprocessing.",
  "PCGrad+. This variant takes into account magnitude balanceand adjusts gradient magnitudes based on Equation (5), buildingupon the foundation of PCGrad": "4.1.3Evaluation Metrics. In order to conduct a comprehensiveevaluation of performance with respect to optimizing multiplerecommendation objectives, we employ two widely recognizedaccuracy metrics: AUC and GAUC . Following previous work , we mainly focus on the average performance across all tasks.Specifically, we utilize both the average metric across all tasks andthe relative metric improvement compared with the Single baselineacross all tasks, which can be expressed as",
  "GAUC ().(15)": "Here, M represents the specific multi-task learning method, withAV-A and AV-G denoting the average value of AUC and GAUC,respectively. Similarly, RI-A and RI-G signify the relative improve-ment in AUC and GAUC, respectively. Across all metrics, highervalues indicate better recommendation results. 4.1.4Implementation Details. To ensure fair comparisons, we em-ploy the PLE model as the backbone recommender model forall the methods under consideration. Each task is composed of ashared expert, a task-specific expert, a gate network, and a towernetwork. The experts are instantiated as DeepFM , combininga Factorization Machine (FM) component with a Multi-LayerPerceptron (MLP) module. The hidden layer configuration forthe MLP is set to 25612864. The tower network is implementedas an MLP with a hidden layer configuration of 32 16. The gatenetwork structure is based on a linear layer with Softmax serv-ing as the activation function. The embedding size is consistentlyset to 16 for all user and video features.In terms of model optimization, we employ the Adam opti-mizer , setting the maximum number of optimization epochsto 1000. Optimal models are identified based on validation results,utilizing an early stopping strategy with a patience setting of 10.Parameters for the backbone recommender model are initializedusing a Gaussian distribution, where the mean is fixed at 0, andthe standard deviation is set to 0.01. The dropout ratio is set to0.2. We leverage the grid search to find the best hyper-parameters.For our method and all baselines, we search the learning rate inthe range of {1-4, 5-4, 1-3}, the size of mini-batch in the rangeof {2048, 4096}, and the 2 regularization coefficient in {0, 1-6,1-5, 1-4, 1-3}. For the special hyper-parameters of baselines, wesearch most of them in the ranges provided by their papers. Re-garding our methodology, the hyper-parameter in Equation (5)to regulate the closeness to the maximum norm is searched withinthe interval using a step size of 0.1, and the hyper-parameter in Equation (6) to achieve the desired similarity is searched in therange of {0, 1-12, 1-11, 1-10, 1-9, 1-8, 1-7}.",
  "This suggests that GradCrafts global gradient projection methodsurpasses PCGrads pair-wise projection method, leading to theglobal and thorough direction deconfliction": "In contrast, loss reweighting methods such as EW, UC, and DWAexhibit poor performance. Their reliance on overall loss values,without granular gradient analysis, limits their effectiveness in en-hancing multi-task optimization. This highlights the importanceof taking into account more fine-grained gradient magnitude anddirection for enhanced performance. Methods that exclusively prioritize either magnitude balance ordirection balance struggle to achieve optimal recommendationperformance and may even lead to degradation (MGDA). Thisemphasizes the need of holistically addressing both magnitudeand direction balance in multi-task recommendations.",
  "Ablation Study (RQ2)": "To enhance the multi-task recommendation performance in Grad-Craft, we propose the incorporation of a magnitude adjustment ap-proach and a gradient projection method, with two hyper-parameters and . To substantiate the rationale behind these design decisions,we conduct an exhaustive evaluation by systematically disablingone critical design element at a time to obtain various variants.Specifically, the following variants are introduced:",
  ": Results of the performance of GradCraft acrossdifferent values of on Wechat": "GradCraft-local, which removes the global gradient projectionand replaces it with the normal projection in PCGrad. illustrates the comparison results on Wechat, from whichwe draw the following observations: When GradCraft disables the factors and the , there are de-creases in performance across all metrics. These results confirmthe pivotal role of in maintaining a certain level of positivesimilarity to facilitate the transfer of knowledge across tasks, and in controlling magnitude proximity levels. Comparatively, GradCraft-ori outperforms GradCraft-fix. Thesevariants correspond to aligning the magnitude with the maxi-mum norm and retaining the original magnitude, with set to 1and 0, respectively. This observation suggests that indiscriminateadjustment of magnitude to match the maximum norm may detri-mentally impact recommendation performance, underscoring thesignificance of appropriate proximity. The performance of GradCraft-ori and GradCraft-local is similar,indicating no advantage of global gradient projection over thenormal projection strategy when magnitude adjustment is absent.However, the performance gap between GradCraft and PCGrad+in underscores the superiority of the gradient projectionmethod. This outcome can be attributed to disruption caused bymagnitudes, underscoring the critical role of initially adjustingmagnitudes to achieve magnitude balance.",
  "In-depth Analysis (RQ3 & RQ4)": "4.4.1The Effect of Hyper-parameter & . In our investigation,the two factors and assume pivotal roles in influencing theeffectiveness of GradCraft. We undertake a systematic examinationto scrutinize the impact of varying them on the performance. Wereport the AV-G and RI-G for simplicity, as shown in and. It becomes evident that GradCraft achieves optimal AV-G",
  ": Results of the performance of GradCraft in com-parison with the best baseline across different task number on Wechat": "and RI-G when is set to 0.1 and is set to 1-10. However, theperformance tends to deteriorate when they become excessivelylarge. This underscores the significance of selecting an appropriatevalue for and . Further analysis reveals that when [0, 0.3] and , the performance remains consistently stable,indicating the robustness within the range. This stability is crucialfor ensuring reliable performance of the magnitude adjustmentapproach and the gradient projection method. 4.4.2The Effect of Task Number . In multi-task recommenda-tions, the degree of gradient imbalance is intricately linked to thetask number, with higher task numbers leading to an increase inthe number of potential conflicting task pairs. Consequently, weconduct a comprehensive study to evaluate the impact of varyingtask numbers on GradCrafts performance. We also present theperformance of the best baseline for comparative analysis. Specif-ically, we adjust the task number in the range of {2, 4, 6} whileensuring an equal number of viewing labels and engagement labels.For = 2, we designate EV and Like as the tasks, and for = 4,we incorporate EV, LV, Like, and Follow. For = 6, we use all thelabels mentioned. We depict the relative improvement metrics RI-Aand RI-G in , and our observations are as follows: Both the metrics of GradCraft exhibit a consistent increase withthe task number. In contrast, the best baseline method does notdisplay a similar trend. This stark contrast suggests that Grad-Craft possesses a unique capability to achieve gradient balance,",
  "Base---GradCraft+0.505%+0.950%+1.746%": "which scales up effectively with the increasing complexity intro-duced by a growing number of tasks. Consequently, GradCraftshowcases its potential for practical application in complex rec-ommendation scenarios. This enhanced performance can be at-tributed to the implementation of flexible magnitude adjustmentand thorough direction conflict elimination in GradCraft. Moreover, as the number of tasks increases, the performancegap between GradCraft and the best baseline method widens.This expanding gap provides further evidence supporting theadvantages of GradCraft in achieving both appropriate magni-tude balance and global direction balance at the gradient level. Itis worth mentioning that for = 2, both methods yield similarresults in terms of the RI-A and RI-G metrics. This similarity canbe attributed to the fact that when there is only one pair of tasks,the global projection method employed by GradCraft closely re-sembles the normal conflict projection method. This consistencyaligns with the earlier discussion presented in Equation (11).",
  "Online Experiment (RQ5)": "We conduct an online A/B experiment on our production plat-form, leveraging traffic from over 15 million users. We assessethree key business and engagement metrics: the average time usersspend watching videos (WT), the number of effective video view-ing records (VV), and the instances of video sharing (Share). Ourfindings, presented in , demonstrate notable performanceenhancements achieved by our method compared to the state-of-the-art multi-task learning baseline implemented in Kuaishou.",
  "Multi-task Optimization": "Multi-task learning necessitates the simultaneous optimization ofmultiple tasks. Prior research has proposed various optimizationmethods to mitigate the imbalance among different tasks, broadlycategorized into two lines. The first category involves reweightingloss, adjusting the gradient magnitudes based on different aspectsof the specific criteria . For example, UC adjusts the loss weights according to the uncertainty associatedwith each task, while DWA adapts the loss weights by tak-ing into account the rate of change of the loss value. The secondcategory focuses on manipulating gradient directions to diminishthe direction conflict . For instance, MGDA manipulates gradients to achieve a local Pareto optimal solution. PCGrad addresses gradient interference by pair-wise projec-tions. CAGrad identifies the optimal update vector within asphere around the average gradient and maximizes the worst localimprovement between tasks. IMTL learns weights to ensurethat the aggregated gradient has equal projections onto each taskgradient. Among the mentioned works, CAGrad implicitly consid-ers the gradient magnitude. However, it only imposes restrictionson the magnitude of the update vector, rather than finely modifyingthe magnitudes of each individual task like our proposed GradCraft.In recent times, there has been a growing focus on developing tai-lored strategies specifically for the recommender system , with a particular emphasis on diverse optimization objectives.PE-LTR introduces a Pareto-efficient algorithmic frameworkfor e-commerce recommendations. LabelCraft proposes a label-ing model that aligns with the objectives of short video platforms.MetaBalance aims to achieve equilibrium among auxiliarylosses by manipulating their gradients to enhance knowledge trans-fer for the target task. SoFA optimizes item-side group fairnesswhile maintaining recommendation accuracy constraints. Amongthese works, MetaBalance bears resemblance to our GradCraft as itincorporates adjustments to gradient magnitudes. However, Meta-Balance primarily focuses on multi-behavior learning and solelyoptimizes performance on the target task. Additionally, it rigidlyemploys the gradients of the target task as adjustment criteria. Incontrast, GradCraft focuses on the optimization of multiple ob-jectives and dynamically utilizes the maximum norm of gradientsacross all tasks, resulting in greater adaptability and versatility.",
  "Multi-task Model": "Multi-task models aim to excel in multiple interrelated tasks simul-taneously, extracting shared information to enhance proficiencyin each task. While hard parameter sharing models are commonlyused, they may suffer from detrimental transfer effects due to taskdisparities. To address this, soft parameter sharing models havebeen introduced, such as the cross-stitch network and sluicenetwork , which combine task-specific hidden layers usinglinear combinations. Gating and attention mechanisms have alsobeen utilized for effective information fusion. Examples includeMoE , which uses a gate structure to combine various experts,and MTAN , which incorporates task-specific attention moduleswithin a shared network.In recommendations, hard parameter sharing at the bottom(SharedBottom) remains pervasive owing to its simplicity andefficiency, effectively addressing the oversight of task correlationsin traditional models rooted in collaborative filtering and matrixfactorization . MMoE goes a step further by shar-ing all experts across diverse tasks, utilizing distinct gates for eachtask to augment the capabilities of the MoE framework. Conversely,ESMM adopts a soft parameter sharing structure, simultane-ously optimizing two correlated tasks through sequential modesto mitigate the sparsity inherent in the prediction target. Expand-ing upon the shared experts paradigm in MMoE, PLE estab-lishes independent experts for each task, and adopts multi-levelextraction networks with progressive separation routing. Further-more, AdaTT enhances its capability by utilizing an adaptivefusion mechanism, enabling the model to more effectively select",
  "Conclusion": "This study investigated the application of multi-task learning meth-ods in the recommender system. Recognizing the distinct charac-teristics of recommendations, we proposed GradCraft to simulta-neously achieve an appropriate magnitude balance and a globaldirection balance to enhance the multi-task optimization. Grad-Craft dynamically adjusted the gradient magnitudes to align withthe maximum gradient norm to establish the appropriate magni-tude balance, mitigating interference from gradient magnitudesfor subsequent manipulation. Subsequently, it employed projec-tions to eliminate gradient conflicts in directions while consideringall conflicting tasks concurrently, thereby ensuring global direc-tion balance. Extensive experiments conducted on both real-worlddatasets and our production platform provided empirical evidenceof its effectiveness in enhancing multi-task recommendations.In our future work, we will enhance the comprehensivenessof our method by integrating the resolution of conflicting gradi-ents with the improvement of consistency among other gradients.Additionally, we plan to apply our method to other domains, includ-ing Computer Vision (CV) and Natural Language Processing(NLP) , in order to evaluate its general applicability. Moreover,we recognize the complexity of industrial recommendation scenar-ios and will focus on developing more effective multi-task learningmethods tailored for large-scale industrial settings. This work is supported by the National Key Research and Devel-opment Program of China (2022YFB3104701), the National NaturalScience Foundation of China (62272437), and the CCCD Key Lab ofMinistry of Culture and Tourism. Yimeng Bai, Yang Zhang, Jing Lu, Jianxin Chang, Xiaoxue Zang, Yanan Niu, YangSong, and Fuli Feng. 2024. LabelCraft: Empowering Short Video Recommenda-tions with Automated Label Crafting. In Proceedings of the 17th ACM Interna-tional Conference on Web Search and Data Mining (Merida, Mexico) (WSDM24). Association for Computing Machinery, New York, NY, USA, 10 pages. Qingpeng Cai, Zhenghai Xue, Chi Zhang, Wanqi Xue, Shuchang Liu, RuohanZhan, Xueliang Wang, Tianyou Zuo, Wentao Xie, Dong Zheng, Peng Jiang, andKun Gai. 2023. Two-Stage Constrained Actor-Critic for Short Video Recom-mendation. In Proceedings of the ACM Web Conference 2023 (Austin, TX, USA)(WWW 23). Association for Computing Machinery, New York, NY, USA, 865875. Jianxin Chang, Chenbin Zhang, Yiqun Hui, Dewei Leng, Yanan Niu, Yang Song,and Kun Gai. 2023. PEPNet: Parameter and Embedding Personalized Networkfor Infusing with Personalized Prior Information. In Proceedings of the 29th ACMSIGKDD Conference on Knowledge Discovery and Data Mining (Long Beach, CA,USA) (KDD 23). Association for Computing Machinery, New York, NY, USA,37953804. Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. 2018.GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Mul-titask Networks. In Proceedings of the 35th International Conference on MachineLearning (Proceedings of Machine Learning Research, Vol. 80). PMLR, 794803. Roberto Cipolla, Yarin Gal, and Alex Kendall. 2018. Multi-task Learning UsingUncertainty to Weigh Losses for Scene Geometry and Semantics. In Proceedingsof the IEEE Conference on Computer Vision and Pattern Recognition. 74827491. Jingtong Gao, Xiangyu Zhao, Muyang Li, Minghao Zhao, Runze Wu, RuochengGuo, Yiding Liu, and Dawei Yin. 2024. SMLP4Rec: An Efficient All-MLP Archi-tecture for Sequential Recommendations. ACM Trans. Inf. Syst. 42, 3, Article 86(jan 2024), 23 pages. Xudong Gong, Qinlin Feng, Yuan Zhang, Jiangling Qin, Weijie Ding, Biao Li,Peng Jiang, and Kun Gai. 2022. Real-Time Short Video Recommendation onMobile Devices. In Proceedings of the 31st ACM International Conference onInformation & Knowledge Management (Atlanta, GA, USA) (CIKM 22). Asso-ciation for Computing Machinery, New York, NY, USA, 31033112. Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.DeepFM: a factorization-machine based neural network for CTR prediction. InProceedings of the 26th International Joint Conference on Artificial Intelligence(Melbourne, Australia) (IJCAI17). AAAI Press, 17251731. Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, YongDong Zhang, and MengWang. 2020. LightGCN: Simplifying and Powering Graph Convolution Networkfor Recommendation. In Proceedings of the 43rd International ACM SIGIR Confer-ence on Research and Development in Information Retrieval (Virtual Event, China)(SIGIR 20). Association for Computing Machinery, New York, NY, USA, 639648. Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-SengChua. 2017. Neural Collaborative Filtering. In Proceedings of the 26th InternationalConference on World Wide Web (Perth, Australia) (WWW 17). International WorldWide Web Conferences Steering Committee, Republic and Canton of Geneva,CHE, 173182. Yun He, Xue Feng, Cheng Cheng, Geng Ji, Yunsong Guo, and James Caverlee. 2022.MetaBalance: Improving Multi-Task Recommendations via Adapting GradientMagnitudes of Auxiliary Tasks. In Proceedings of the ACM Web Conference 2022(Virtual Event, Lyon, France) (WWW 22). Association for Computing Machinery,New York, NY, USA, 22052215.",
  "Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-mization. arXiv preprint arXiv:1412.6980 (2014)": "Pang Wei Koh and Percy Liang. 2017. Understanding black-box predictions viainfluence functions. In Proceedings of the 34th International Conference on MachineLearning - Volume 70 (Sydney, NSW, Australia) (ICML17). JMLR.org, 18851894. Danwei Li, Zhengyu Zhang, Siyang Yuan, Mingze Gao, Weilin Zhang, ChaofeiYang, Xi Liu, and Jiyan Yang. 2023. AdaTT: Adaptive Task-to-Task Fusion Networkfor Multitask Learning in Recommendations. In Proceedings of the 29th ACMSIGKDD Conference on Knowledge Discovery and Data Mining (Long Beach, CA,USA) (KDD 23). Association for Computing Machinery, New York, NY, USA,43704379.",
  "Baijiong Lin, Feiyang Ye, Yu Zhang, and Ivor W Tsang. 2021. Reasonable ef-fectiveness of random weighting: A litmus test for multi-task learning. arXivpreprint arXiv:2111.10603 (2021)": "Xiao Lin, Hongjie Chen, Changhua Pei, Fei Sun, Xuanji Xiao, Hanxiao Sun,Yongfeng Zhang, Wenwu Ou, and Peng Jiang. 2019. A pareto-efficient algorithmfor multiple objective optimization in e-commerce recommendation (RecSys19). Association for Computing Machinery, New York, NY, USA, 2028. Bo Liu, Xingchao Liu, Xiaojie Jin, Peter Stone, and Qiang Liu. 2021.Conflict-Averse Gradient Descent for Multi-task learning. In Advances inNeural Information Processing Systems, Vol. 34. Curran Associates, Inc.,1887818890. Liyang Liu, Yi Li, Zhanghui Kuang, Jing-Hao Xue, Yimin Chen, Wenming Yang,Qingmin Liao, and Wayne Zhang. 2021. Towards Impartial Multi-task Learning. In9th International Conference on Learning Representations (Virtual Event, Austria).OpenReview.net, 12 pages. Qi Liu, Zhilong Zhou, Gangwei Jiang, Tiezheng Ge, and Defu Lian. 2023. DeepTask-specific Bottom Representation Network for Multi-Task Recommendation.In Proceedings of the 32nd ACM International Conference on Information andKnowledge Management (Birmingham, United Kingdom) (CIKM 23). Associationfor Computing Machinery, New York, NY, USA, 16371646.",
  "Agnes Lydia and Sagayaraj Francis. 2019. Adagradan optimizer for stochasticgradient descent. Int. J. Inf. Comput. Sci 6, 5 (2019), 566568": "Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H. Chi. 2018.Modeling Task Relationships in Multi-Task Learning with Multi-Gate Mixture-of-Experts. In Proceedings of the 24th ACM SIGKDD International Conferenceon Knowledge Discovery & Data Mining (London, United Kingdom) (KDD 18).Association for Computing Machinery, New York, NY, USA, 19301939. Xiao Ma, Liqin Zhao, Guan Huang, Zhi Wang, Zelin Hu, Xiaoqiang Zhu, and KunGai. 2018. Entire space multi-task model: An effective approach for estimatingpost-click conversion rate. In The 41st International ACM SIGIR Conference onResearch & Development in Information Retrieval. 11371140. Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Martial Hebert. 2016.Cross-stitch networks for multi-task learning. In Proceedings of the IEEE conferenceon computer vision and pattern recognition. 39944003. Yunzhu Pan, Chen Gao, Jianxin Chang, Yanan Niu, Yang Song, Kun Gai, DepengJin, and Yong Li. 2023. Understanding and Modeling Passive-Negative Feed-back for Short-Video Sequential Recommendation (RecSys 23). Association forComputing Machinery, New York, NY, USA, 540550. Yunzhu Pan, Nian Li, Chen Gao, Jianxin Chang, Yanan Niu, Yang Song, DepengJin, and Yong Li. 2023. Learning and Optimization of Implicit Negative Feedbackfor Industrial Short-Video Recommender System. In Proceedings of the 32nd ACMInternational Conference on Information and Knowledge Management (London,United Kingdom) (CIKM 23). Association for Computing Machinery, New York,NY, USA, 47874793.",
  "Sebastian Ruder, Joachim Bingel, Isabelle Augenstein, and Anders Sgaard. 2017.Sluice networks: Learning what to share between loosely related tasks. arXivpreprint arXiv:1705.08142 2 (2017)": "Indu S., Srinivas N.K., Harish P.J., GangaPrasad R., Nobby Varghese, N.S.Sreekanth, and Supriya N. Pal. 2013. NLP@Desktop: a service oriented architec-ture for integrating NLP services in desktop clients. SIGSOFT Softw. Eng. Notes38, 4 (jul 2013), 14. Ozan Sener and Vladlen Koltun. 2018. Multi-Task Learning as Multi-ObjectiveOptimization. In Proceedings of the 32nd International Conference on Neural Infor-mation Processing Systems (Montreal, Canada) (NIPS18). Curran Associates Inc.,Red Hook, NY, USA, 525536. Hongyan Tang, Junning Liu, Ming Zhao, and Xudong Gong. 2020. Progressive Lay-ered Extraction (PLE): A Novel Multi-Task Learning (MTL) Model for PersonalizedRecommendations. In Proceedings of the 14th ACM Conference on RecommenderSystems (Virtual Event, Brazil) (RecSys 20). Association for Computing Machinery,New York, NY, USA, 269278. Simon Vandenhende, Stamatios Georgoulis, Wouter Van Gansbeke, Marc Proes-mans, Dengxin Dai, and Luc Van Gool. 2021. Multi-task learning for denseprediction tasks: A survey. IEEE transactions on pattern analysis and machineintelligence 44, 7 (2021), 36143633.",
  "Deborah Walters. 2003. Computer vision. John Wiley and Sons Ltd., GBR, 431435": "Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019.Neural Graph Collaborative Filtering. In Proceedings of the 42nd InternationalACM SIGIR Conference on Research and Development in Information Retrieval(Paris, France) (SIGIR19). Association for Computing Machinery, New York, NY,USA, 165174. Yuhao Wang, Ha Tsz Lam, Yi Wong, Ziru Liu, Xiangyu Zhao, Yichao Wang, BoChen, Huifeng Guo, and Ruiming Tang. 2023. Multi-Task Deep RecommenderSystems: A Survey. arXiv preprint arXiv:2302.03525 (2023). Zirui Wang, Yulia Tsvetkov, Orhan Firat, and Yuan Cao. 2021. Gradient Vaccine:Investigating and Improving Multi-task Optimization in Massively MultilingualModels. In 9th International Conference on Learning Representations (Virtual Event,Austria). OpenReview.net, 12 pages. Jiancan Wu, Xiang Wang, Xingyu Gao, Jiawei Chen, Hongcheng Fu, Tianyu Qiu,and Xiangnan He. 2023. On the Effectiveness of Sampled Softmax Loss for ItemRecommendation. ACM Trans. Inf. Syst. (dec 2023). Just Accepted. Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, andChelsea Finn. 2020. Gradient Surgery for Multi-Task Learning. In Proceedingsof the 34th International Conference on Neural Information Processing Systems(Vancouver, BC, Canada) (NIPS 20). Curran Associates Inc., Red Hook, NY, USA,Article 489, 13 pages. Ruohan Zhan, Changhua Pei, Qiang Su, Jianfeng Wen, Xueliang Wang, GuanyuMu, Dong Zheng, Peng Jiang, and Kun Gai. 2022. Deconfounding Duration Biasin Watch-Time Prediction for Video Recommendation. In Proceedings of the 28thACM SIGKDD Conference on Knowledge Discovery and Data Mining (WashingtonDC, USA) (KDD 22). Association for Computing Machinery, New York, NY, USA,44724481. Yang Zhang, Yimeng Bai, Jianxin Chang, Xiaoxue Zang, Song Lu, Jing Lu, FuliFeng, Yanan Niu, and Yang Song. 2023. Leveraging Watch-Time Feedback forShort-Video Recommendations: A Causal Labeling Framework. In Proceedingsof the 32nd ACM International Conference on Information and Knowledge Man-agement (Birmingham, United Kingdom) (CIKM 23). Association for ComputingMachinery, New York, NY, USA, 49524959.",
  "Yang Zhang, Zhiyu Hu, Yimeng Bai, Fuli Feng, Jiancan Wu, Qifan Wang, andXiangnan He. 2023. Recommendation unlearning via influence function. arXivpreprint arXiv:2307.02147 (2023)": "Yang Zhang, Tianhao Shi, Fuli Feng, Wenjie Wang, Dingxian Wang, Xiangnan He,and Yongdong Zhang. 2023. Reformulating CTR Prediction: Learning InvariantFeature Interactions for Recommendation. In Proceedings of the 46th InternationalACM SIGIR Conference on Research and Development in Information Retrieval(Taipei, China) (SIGIR 23). Association for Computing Machinery, New York, NY,USA, 13861395. Haiyuan Zhao, Lei Zhang, Jun Xu, Guohao Cai, Zhenhua Dong, and Ji-Rong Wen.2023. Uncovering User Interest from Biased and Noised Watch Time in VideoRecommendation. In Proceedings of the 17th ACM Conference on Recommender Sys-tems (Singapore, Singapore) (RecSys 23). Association for Computing Machinery,New York, NY, USA, 528539."
}