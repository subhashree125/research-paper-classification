{
  "ABSTRACT": "Recently, pre-training and fine-tuning has been adopted as a stan-dard workflow for many graph tasks since it can take general graphknowledge to relieve the lack of graph annotations from each ap-plication. However, graph tasks with node level, edge level, andgraph level are far diversified, making the pre-training pretext oftenincompatible with these multiple tasks. This gap may even cause anegative transfer to the specific application, leading to poor results.Inspired by the prompt learning in natural language processing(NLP), which has presented significant effectiveness in leveragingprior knowledge for various NLP tasks, we study the promptingtopic for graphs with the motivation of filling the gap between pre-trained models and various graph tasks. In this paper, we propose anovel multi-task prompting method for graph models. Specifically,we first unify the format of graph prompts and language promptswith the prompt token, token structure, and inserting pattern. Inthis way, the prompting idea from NLP can be seamlessly intro-duced to the graph area. Then, to further narrow the gap betweenvarious graph tasks and state-of-the-art pre-training strategies, wefurther study the task space of various graph applications and re-formulate downstream problems to the graph-level task. Afterward,we introduce meta-learning to efficiently learn a better initializa-tion for the multi-task prompt of graphs so that our promptingframework can be more reliable and general for different tasks. Weconduct extensive experiments, results from which demonstratethe superiority of our method.",
  "Networks Online social networks; Computing method-ologies Knowledge representation and reasoning": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 23, August 610, 2023, Long Beach, CA, USA 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0103-0/23/08...$15.00",
  "INTRODUCTION": "Graph neural networks (GNNs) have been widely applied to variousapplications such as social computing , anomaly detection , and network analysis . Beyond exploring various ex-quisite GNN structures, recent years have witnessed a new researchtrend on how to train a graph model for dedicated problems.Traditional supervised learning methods on graphs heavily relyon graph labels, which are not always sufficient in the real world.Another shortcoming is the over-fitting problem when the testingdata is out-of-distribution . To solve these challenges, manystudies turn to pre-training and fine-tuning , which meanspre-training a graph model with easily accessible data, and thentransferring the graph knowledge to a new domain or task viatuning the last layer of the pre-trained model. Although muchprogress has been achieved on pre-training strategies , there stillexists a huge gap between these pretexts and multiple downstreamtasks. For example, a typical pretext for the pre-training graph isbinary edge prediction. Usually, this pre-training strategy makesconnected nodes closer in a latent representation space. However,many downstream tasks are not limited to edge-level tasks but alsoinclude node-level tasks (e.g., node multi-class classification) orgraph-level tasks (e.g., graph classification). If we transfer the abovepre-trained model to multi-class node classification, it may requireus to carefully search the results in higher dimensional parameterspace for the additional classes of node labels. This tuning mayeven break down (a.k.a negative transfer ) when connectednodes have different labels. Tuning this pre-trained model to graph-level tasks is neither efficient because we have to pay huge effortsto learn an appropriate function mapping node embedding to thewhole graph representation.",
  ": Fine-tuning, Pre-training, and Prompting": "A promising solution to the above problems is to extend pre-training and fine-tuning to pre-training, prompting, and fine-tuning. Prompt learning is a very attractive idea derived fromnatural language processing (NLP) and has shown notable effective-ness in generalizing pre-trained language models to a wide range oflanguage applications . Specifically, a language prompt refers toa piece of text appended to the rear of an input text. For example, asentiment task like KDD2023 will witness many high-quality papers.I feel so [MASK] can be easily transferred to a word predictiontask via a preset prompt (I feel so [MASK]). It is highly expectedthat the language model may predict [MASK] as excited ratherthan upset without further optimizing parameters for the newsentiment task because this model has already been pre-trained viathe pretext of masked words prediction and contains some usefulknowledge to answer this question. By this means, some down-stream objectives can be naturally aligned with the pre-trainingtarget. Inspired by the success of the language prompt, we hope tointroduce the same idea to graphs. As shown in , prompttuning in the graph domain is to seek some light-weighted prompt,keep the pre-training model frozen, and use the prompt to refor-mulate downstream tasks in line with the pre-training task. In thisway, the pre-trained model can be easily applied to downstreamapplications with highly efficient fine-tuning or even without anyfine-tuning. This is particularly useful when the downstream taskis a few-shot setting.However, designing the graph prompt is more intractable thanlanguage prompts. First, classic language prompts are usually somepreset phrases or learnable vectors attached at the end of input texts.As shown in , we only need to consider the content for thelanguage prompt, whereas the graph prompt not only requires theprompt content but also needs to know how to organize theseprompt tokens and how to insert the prompt into the original graph,both of which are undefined problems.Second, there is a huge difficulty in reconciling downstreamproblems to the pre-training task. In the NLP area, we usually pre-train a language model via masked prediction and then transfer itto various applications like question answering , sentiment clas-sification . The underlying support is that these languagetasks usually share a large overlapping task sub-space, making amasked language task easily transferred to other applications. How-ever, how much does the same observation exist (if truly exists) ingraph learning? It is crucial but difficult to decide on an appropriatepre-training task and reformulate downstream tasks to improve",
  ": Our graph prompt inspired by the language prompt": "the capability of model generalization. Currently, we only find veryfew works studying the graph prompt issue. However, it canonly deal with a single-type task (e.g., node classification) using aspecific pretext (e.g., edge prediction), which is far from addressingthe multi-task setting with different-level tasks.Last but not least, learning a reliable prompt usually needs hugemanpower and is more sensitive to prompt initialization in themulti-task setting . Although there are some works in the NLP area trying to initialize the prompt via hand-craftedcontent or some discrete features, these methods are task-bounded,which is not sufficient when we confront a new task. This problemmay be even worse in our multi-task graph area since graph featuresvary a lot in different domains and tasks.Presented work. To further fill the gap between graph pre-training and downstream tasks, we introduce the prompt methodfrom NLP to graphs under the multi-task background. Specifically,to address the first challenge, we propose to unify the format ofthe language prompt and graph prompt in one way so that we cansmoothly transfer the prompt idea from NLP to graphs, then wedesign the graph prompt from prompt tokens, token structures,and prompt inserting patterns. To address the second challenge,we first study the task subspace in graphs and then propose toreformulate node-level and edge-level tasks to graph-level tasks byinduced graphs from original graphs. To address the third challenge,we introduce the meta-learning technique over multiple tasks tolearn better prompts. We carefully evaluate our method with otherapproaches and the experimental results extensively demonstrateour advantages.Contributions:",
  "BACKGROUND": "Graph Neural Networks. Graph neural networks (GNNs) havepresented powerful expressiveness in many graph-based applica-tions . The nature of most GNNs is to capture the un-derlying message-passing patterns for graph representation. To thisend, there are many effective neural network structures proposedsuch as graph attention network (GAT) , graph convolutionnetwork (GCN) , Graph Transformer . Recent works alsoconsider how to make graph learning more adaptive when dataannotation is insufficient or how to transfer the model to a newdomain, which triggered many graph pre-training studies insteadof traditional supervised learning.Graph Pre-training. Graph pre-training aims to learn somegeneral knowledge for the graph model with easily accessible infor-mation to reduce the annotation costs of new tasks. Some effectivepre-training strategies include node-level comparison like GCA, edge-level pretext like edge prediction , and graph-levelcontrastive learning such as GraphCL and SimGRACE .In particular, GraphCL minimizes the distance between a pair ofgraph-level representations for the same graph with different aug-mentations whereas SimGRACE tries to perturb the graph modelparameter spaces and narrow down the gap between different per-turbations for the same graph. These graph-level strategies performmore effectively in graph knowledge learning and are the de-fault strategies of this paper.Prompt Learning & Motivations. Intuitively, the above graph-level pre-training strategies have some intrinsic similarities withthe language-masked prediction task: aligning two graph viewsgenerated by node/edge/feature mask or other perturbations is verysimilar to predicting some vacant blanks on graphs. That inspiresus to further consider: why cant we use a similar format promptfor graphs to improve the generalization of graph neural networks?Instead of fine-tuning a pre-trained model with an adaptive taskhead, prompt learning aims to reformulate input data to fit thepretext . Many effective prompt methods are firstly proposedin the NLP area, including some hand-crafted prompts like GPT-3, discrete prompts like , and trainable prompts in the con-tinuous spaces like . Despite significant results achieved,prompt-based methods have been rarely introduced in graph do-mains yet. We only find very few works like GPPT , trying todesign prompts for graphs. Unfortunately, most of them are verylimited and are far from sufficient to meet the multi-task demands.",
  "MULTI-TASK PROMPTING ON GRAPHS3.1Overview of Our Framework": "Objective: In this paper, we aim to learn a prompt graph that can beinserted into the original graph, through which we wish to furtherbridge the gap between a graph pre-training strategy and multipledownstream tasks, and further relieve the difficulties of transferringprior knowledge to different domains.Overview: To achieve our goal, we propose a novel multi-taskprompting framework for graph models. First, we unify variousgraph tasks in the same format and reformulate these downstreamtasks as graph-level tasks. Second, with the unified graph-levelinstances, we further narrow down the gap among multiple tasksby a novel prompt graph with learnable tokens, inner structures, and",
  "Reformulating Downstream Tasks": "3.2.1Why Reformulate Downstream Tasks. The success of the tra-ditional pre-training and fine-tuning framework in the NLP arealargely lies in the fact that the pre-training task and downstreamtasks share some common intrinsic task subspace, making the pre-training knowledge transferable to other downstream tasks (a). However, things are a little complicated in the graph domainsince graph-related tasks are far from similar. As shown in b,it is far-fetched to treat the edge-level task and the node-level taskas the same one because node-level operations and edge-level oper-ations are far more different . This gap limits the performanceof pre-training models and might even cause negative transfer .The same problem also happens in our pre-training, prompting,and fine-tuning framework since we aim to learn a graph promptfor multiple tasks, which means we need to further narrow downthe gap between these tasks by reformulating different graph tasksin a more general form. 3.2.2Why Reformulate to the Graph Level. With the above moti-vation, we revisit the potential task space on the graph and findtheir hierarchical relation as shown in b. Intuitively, manynode-level operations such as changing node features, delete/adda node, or edge-level operations such as add/delete an edge, canbe treated as some basic operations at the graph level. For example,delete a subgraph can be treated as delete nodes and edges. Com-pared with node-level and edge-level tasks, graph-level tasks aremore general and contain the largest overlapping task sub-spacesfor knowledge transfer, which has been adopted as the mainstreamtask in many graph pre-training models . This observa-tion further inspires us to reformulate downstream tasks to looklike the graph-level task and then leverage our prompting model tomatch graph-level pre-training strategies. 3.2.3How to Reformulate Downstream Tasks. Specifically, we refor-mulate node-level and edge-level tasks to graph-level tasks by build-ing induced graphs for nodes and edges, respectively. As shown ina, an induced graph for a target node means its local areain the network within distance, which is also known as its -egonetwork. This subgraph preserves the nodes local structure byneighboring node connections and its semantic context by neigh-boring node features, which is the main scope of most graph neural",
  "KDD 23, August 610, 2023, Long Beach, CA, USAXiangguo Sun, Hong Cheng, Jia Li, Bo Liu, and Jihong Guan": "Takuya Akiba, Takanori Hayashi, Nozomi Nori, Yoichi Iwata, and Yuichi Yoshida.2015. Efficient top-k shortest-path distance queries on large networks by prunedlandmark labeling. In Proceedings of the AAAI Conference on Artificial Intelligence,Vol. 29. Yunsheng Bai, Hao Ding, Yang Qiao, Agustin Marinovic, Ken Gu, Ting Chen,Yizhou Sun, and Wei Wang. 2019. Unsupervised inductive graph-level represen-tation learning via graph-graph proximity. In Proceedings of the 28th InternationalJoint Conference on Artificial Intelligence. 19881994. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shot learners. Advances in neuralinformation processing systems 33 (2020), 18771901. Hongxu Chen, Hongzhi Yin, Xiangguo Sun, Tong Chen, Bogdan Gabrys, andKatarzyna Musial. 2020. Multi-level graph convolutional networks for cross-platform anchor link prediction. In Proceedings of the 26th ACM SIGKDD interna-tional conference on knowledge discovery & data mining. 15031511. Junru Chen, Yang Yang, Tao Yu, Yingying Fan, Xiaolong Mo, and Carl Yang. 2022.BrainNet: Epileptic Wave Detection from SEEG with Hierarchical Graph DiffusionLearning. In Proceedings of the 28th ACM SIGKDD Conference on KnowledgeDiscovery and Data Mining. 27412751.",
  "Prompt Graph Design": "3.3.1Prompting NLP and Graph in One Way. To seamlessly trans-fer the prompting idea from NLP to the graph domain, we proposeto unify NLP Prompt and Graph Prompt in one perspective. Havingcompared the demand of NLP and graph area as shown in , we found that the prompt in NLP and graph areas should con-tain at least three components: (1) prompt token, which containsthe vectorized prompting information with the same size as theinput word/node vector; (2) token structure, which indicates the con-nection of different tokens. In the NLP area, prompt tokens (a.k.aprompt words) are preset as a linear relation like a sub-sentence ora phrase; whereas in the graph domain, the connections of differenttokens are non-linear and far more complicated than NLP prompts;(3) inserting pattern, which presents how to add the prompt to theinput data. In the NLP area, the prompt is usually added in the frontor the back end of the input sentences by default. However, in thegraph area, there are no explicit positions like a sentence to jointgraph prompt, making the graph prompting more difficult. 3.3.2Prompt Tokens. Let a graph instance be G = (V, E) whereV = {1, 2, , } is the node set containing nodes; eachnode has a feature vector denoted by x R1 for node ; E ={(, )|, V} is the edge set where each edge connects apair of nodes in V. With the previous discussion, we here presentour prompt graph as G = (P, S) where P = {1, 2, , | P|}",
  "denotes the set of prompt tokens and |P| is the number of tokens.Each token P can be represented by a token vector p R1": "with the same size of node features in the input graph; Note that inpractice, we usually have |P| and |P| where is thesize of the hidden layer in the pre-trained graph model. With thesetoken vectors, the input graph can be reformulated by adding the-th token to graph node (e.g., x = x + p). Then, we replacethe input features with the prompted features and send them to thepre-trained model for further processing. 3.3.3Token Structures. S = {(, )|, P} is the tokenstructure denoted by pair-wise relations among tokens. Unlike theNLP prompt, the token structure in the prompt graph is usuallyimplicit. To solve this problem, we propose three methods to designthe prompt token structures: (1) the first way is to learn tunableparameters:",
  "A =| P|1=1=+1{ }": "where is a tunable parameter indicating how possible the token and the token should be connected; (2) the second way isto use the dot product of each prompt token pair and prune themaccording to the dot value. In this case, (, ) S iff (p p) < where () is a sigmoid function and is a pre-defined threshold;(3) the third way is to treat the tokens as independent and then wehave S = . 3.3.4Inserting Patterns. Let be the inserting function that indi-cates how to add the prompt graph G to the input graph G, thenthe manipulated graph can be denoted as G = (G, G). We candefine the inserting pattern as the dot product between prompttokens and input graph nodes, and then use a tailored connectionlike x = x + | P|=1 p where is a weighted value to pruneunnecessary connections:",
  "Multi-task Prompting via Meta Learning": "3.4.1Constructing Meta Prompting Tasks. Let be the -th taskwith supporting data D and query data D ; Specifically, for thegraph classification task, D and D contain labeled graphs; forthe node classification task, we generate an induced graph for eachnode as mentioned in section 3.2.3, align the graph label with thetarget node label, and treat this graph as a member in D or D ;for the edge classification task, we first generate edge inducedgraphs for training and testing and the edge label is up to its twoendpoints. 3.4.2Applying Meta-learning to Graph Prompting. Let be promptparameters, be the fixed parameters of the pre-trained graphbackbone, and be the taskers parameters. We use , | to denotethe pipeline with prompt graph (), pre-trained model (, fixed),and downstream tasker (). Let LD ( ) be the task loss with pipline on data D. Then for each task , the corresponding parameters",
  "(4)": "where H (L) is the Hessian matrix with (H (L)) = 2L/ ;and can be updated in the same way.Kindly note that in the prompt learning area, the task head is alsoknown as the answering function, which connects the prompt to theanswers for downstream tasks to be reformulated. The answeringfunction can be both tunable or hand-craft templates. In section 3.5,we also propose a very simple but effective hand-crafted promptanswering template without any tunable task head. 3.4.3Overall Learning Process. To improve the learning stability,we organize these tasks as multi-task episodes where each episodecontains batch tasks including node classification ( for short),edge classification ( for short), and graph classification ( forshort). Let E = (TE, LE, SE, QE ) be a multi-task episode. We de-",
  "{D1, , D }, and query data QE = {Q()E , Q()E , Q()E } where": "S()E= {D1, , D }. Then the multi-task prompting is pre-sented in Algorithm 1. We treat each node/edge/graph class asa binary classification task so that they can share the same taskhead. Note that our method can also deal with other tasks beyondclassification with only a few adaptations (see Appendix A).",
  "Why It Works?": "3.5.1Connection to Existing Work. A prior study on graph promptis proposed by , namely GPPT. They use edge prediction asa pre-training pretext and reformulate node classification to thepretext by designing labeled tokens added to the original graph.The compound graph will be sent to the pre-trained model againto predict the link connecting each node to the label tokens. Theirwork somehow is a special case of our method when our prompt",
  "return ,|": "graph only contains isolated tokens, each of which corresponds to anode category. However, there are at least three notable differences:(1) GPPT is not flexible to manipulate original graphs; (2) GPPT isonly applicable for node classification; and (3) GPPT only supportsedge prediction task as the pretext but is not compatible with moreadvanced graph-level pre-training strategies such as GraphCL ,UGRAPHEMB , SimGRACE etc. We further discuss theseissues w.r.t. flexibility, efficiency, and compatibility as below. 3.5.2Flexibility. The nature of prompting is to manipulate theinput data to match the pretext. Therefore, the flexibility of dataoperations is the bottleneck of prompting performance. Let beany graph-level transformation such as changing node features,adding or removing edges/subgraphs etc., and be the frozenpre-trained graph model. For any graph G with adjacency matrix Aand node feature matrix X, Fang et al. have proved that we canalways learn an appropriate prompt token making the followingequation stand:",
  "A, X + = ((A, X)) + (5)": "This means we can learn an appropriate token applied to the orig-inal graph to imitate any graph manipulation. Here denotesthe error bound between the manipulated graph and the promptinggraph w.r.t. their representations from the pre-trained graph model.This error bound is related to some non-linear layers of the model(unchangeable) and the quality of the learned prompt (changeable),which is promising to be further narrowed down by a more ad-vanced prompt scheme. In this paper, we extend the standalonetoken to a prompt graph that has multiple prompt tokens withlearnable inner structures. Unlike the indiscriminate inserting inEquation (5) (X + means the prompt token should be addedto every node of the original graph), the inserting pattern of ourproposed prompt graph is highly customized. Let (G, G) denote",
  "(G, G)= (g(A, X)) + (6)": "By efficient tuning, the new error bound can be further re-duced. In section 4.6, we empirically demonstrate that can besignificantly smaller than via efficient training. That meansour method supports more flexible transformations on graphs tomatch various pre-training strategies. 3.5.3Efficiency. Assume an input graph has nodes and edgesand the prompt graph has tokens with edges. Let the graphmodel contain layers and the maximum dimension of one layer be. The parameter complexity of the prompt graph is only (). Incontrast, some typical graph models (e.g., GAT ) usually contain(2 + ) parameters to generate node embedding and addi-tional () parameters to obtain the whole graph representation( is the multi-head number). The parameters may be even larger inother graph neural networks (e.g., graph transformer ). In ourprompt learning framework, we only need to tune the prompt withthe pre-trained graph model frozen, making the training processconverge faster than traditional transfer tuning.For the time complexity, a typical graph model (e.g., GCN )usually needs(2++) time to generate node embeddingvia message passing and then obtain the whole graph representation(e.g., () for summation pooling). By inserting the prompt intothe original graph, the total time is((+)2+(+)+(+)).Compared with the original time, the additional time cost is only(2++) where , , .Besides the efficient parameter and time cost, our work is alsomemory friendly. Taking node classification as an example, thememory cost of a graph model largely includes parameters, graphfeatures, and graph structure information. As previously discussed,our method only needs to cache the prompt parameters, which arefar smaller than the original graph model. For the graph featuresand structures, traditional methods usually need to feed the wholegraph into a graph model, which needs huge memory to cache thesecontents. However, we only need to feed an induced graph to themodel for each node, the size of which is usually far smaller thanthe original graph. Notice that in many real-world applications, weare often interested in only a few parts of the total nodes, whichmeans our method can stop timely if there is no more node to bepredicted and we do not need to propagate messages on the wholegraph either. This is particularly helpful for large-scale data. 3.5.4Compatibility. Unlike GPPT, which can only use binary edgeprediction as a pretext, and is only applicable for node classifica-tion as downstream tasks, our framework can support node-level,edge-level, and graph-level downstream tasks, and adopt variousgraph-level pretexts with only a few steps of tuning. Besides, whentransferring the model to different tasks, traditional approaches usu-ally need to additionally tune a task head. In contrast, our methodfocuses on the input data manipulation and it relies less on thedownstream tasks. This means we have a larger tolerance for thetask head. For example, in section 4.3, we study the transferabilityfrom other domains or tasks but we only adapt our prompt, leaving the source task head unchanged. We can even select some specificpretext and customize the details of our prompt without any tunedtask head. Here we present a case that does not need to tune a taskhead and we evaluate its feasibility in section 4.4. Prompt without Task Head Tuning:Pretext: GraphCL , a graph contrastive learning taskthat tries to maximize the agreement between a pair ofviews from the same graph.Downstream Tasks: node/edge/graph classification.Prompt Answer: node classification. Assume there are categories for the nodes. We design the prompt graph with sub-graphs (a.k.a sub-prompts) where each sub-graph has tokens. Each sub-graph corresponds to one node category.Then we can generate graph views for all input graphs.We classify the target node with label ( = 1, 2, ,) ifthe -th graph view is closest to the induced graph. It issimilar to edge/graph classification. Interestingly, by shrinking the prompt graph as isolate tokensaligned with node classes and replacing the induced graphs withthe whole graph, our prompt format can degenerate to GPPT, whichmeans we can also leverage edge-level pretext for node classifica-tion. Since this format is exactly the same as GPPT, we will notdiscuss it anymore. Instead, we directly compare GPPT on nodeclassification with our method.",
  "EVALUATION": "In this section, we extensively evaluate our method with other ap-proaches on node-level, edge-level, and graph-level tasks of graphs.In particular, we wish to answer the following research questions:Q1: How effective is our method under the few-shot learning back-ground for multiple graph tasks? Q2: How adaptable is our methodwhen transferred to other domains or tasks? Q3: How do the maincomponents of our method impact the performance? Q4: How effi-cient is our model compared with traditional approaches? Q5: Howpowerful is our method when we manipulate graphs?",
  "Experimental Settings": "4.1.1Datasets. : We compare our methods with other approacheson five public datasets including Cora , CiteSeer , Reddit ,Amazon , and Pubmed . Detailed statistics are presentedin where the last column refers to the number of nodeclasses. To conduct edge-level and graph-level tasks, we sampleedges and subgraphs from the original data where the label of anedge is decided by its two endpoints and the subgraph label followsthe majority of the subgraph nodes. For example, if nodes have 3different classes, say 1,2,3, then edge-level tasks contain at least6 categories (1,2,3,12,13,23). We also evaluate additionalgraph classification and link prediction on more specialized datasetswhere the graph label and the link label are inborn and not relatedto any node (see Appendix A). 4.1.2Approaches. Compared approaches are from three categories:(1) Supervised methods: these methods directly train a GNNmodel on a specific task and then directly infer the result. We heretake three famous GNN models including GAT , GCN ,and Graph Transformer (short as GT). These GNN models",
  "Cora2,7085,4291,4337CiteSeer3,3279,1043,7036Reddit232,96523,213,83860241Amazon13,752491,72276710Pubmed19,71788,6485003": "are also included as the backbones of our prompt methods. (2)Pre-training with fine-tuning: These methods first pre-train aGNN model in a self-supervised way such as GraphCL andSimGRACE , then the pre-trained model will be fine-tuned fora new downstream task. (3) Prompt methods: With a pre-trainedmodel frozen and a learnable prompt graph, our prompt methodaims to change the input graph and reformulate the downstreamtask to fit the pre-training strategies. 4.1.3Implementations. We set the number of graph neural layersas 2 with a hidden dimension of 100. To study the transferabilityacross different graph data, we use SVD (Singular Value Decompo-sition) to reduce the initial features to 100 dimensions. The tokennumber of our prompt graph is set as 10. We also discuss the impactof token numbers in section 4.4 where we change the token numberfrom 1 to 20. We use the Adam optimizer for all approaches. Thelearning rate is set as 0.001 for most datasets. In the meta-learningstage, we split all the node-level, edge-level, and graph-level tasksrandomly in 1:1 for meta-training and meta-testing. Reported re-sults are averaged on all tested tasks. More implementation detailsare shown in Appendix A, in which we also analyze the perfor-mance on more datasets and more kinds of tasks such as regression,link prediction, and so on.",
  "Multi-Task Performance with Few-shotLearning Settings (RQ1)": "We compared our prompt-based methods with other mainstreamtraining schemes on node-level, edge-level, and graph-level tasksunder the few-shot setting. We repeat the evaluation 5 times andreport the average results in , (Appendix A), and (Appendix A). From the results, we can observe that mostsupervised methods are very hard to achieve better performancecompared with pre-train methods and prompt methods. This isbecause the empirical annotations required by supervised frame-works in the few-shot setting are very limited, leading to poorperformance. In contrast, pre-training approaches contain moreprior knowledge, making the graph model rely less on data labels.However, to achieve better results on a specific task, we usuallyneed to carefully select an appropriate pre-training approach andcarefully tune the model to match the target task, but this hugeeffort is not ensured to be applicable to other tasks. The gap be-tween pre-training strategies and downstream tasks is still verylarge, making the graph model very hard to transfer knowledgeon multi-task settings (we further discuss the transferability in sec-tion 4.3.) Compared with pre-training approaches, our solutionsfurther improve the compatibility of graph models. The reportedimprovements range from 1.10% to 8.81% on node-level tasks, 1.28% to 12.26% on edge-level tasks, and 0.14% to 10.77% on graph-leveltasks. In particular, we also compared our node-level performancewith the previously mentioned node-level prompt model GPPT in. Kindly note that our experiment settings are totally dif-ferent from GPPT. In GPPT, they study the few-shot problem bymasking 30% or 50% data labels. However, in our paper, we proposea more challenging problem: how does the model perform if wefurther reduce the label data? So in our experiment, each class onlyhas 100 labeled samples. This different setting makes our labeledratio approximately only 25% on Cora, 18% on CiteSeer, 1.7% onReddit, 7.3% on Amazon, and 1.5% on Pubmed, which are far lessthan the reported GPPT (50% labeled).",
  "Transferability Analysis (RQ2)": "To evaluate the transferability, we compared our method with thehard transfer method and the fine-tuning method. Here the hardtransfer method means we seek the source task model which hasthe same task head as the target task and then we directly conductthe model inference on the new task. The fine-tune method meanswe load the source task model and then tune the task head for thenew task. We evaluate the transferability from two perspectives: (1)how effectively is the model transferred to different tasks withinthe same domain? and (2) how effectively is the model transferredto different domains? 4.3.1Transferability to Different Level Tasks. Here we pre-trainthe graph neural network on Amazon, then conduct the model ontwo source tasks (graph level and node level), and further evaluatethe performance on the target task (edge level). For simplicity, bothsource tasks and the target task are built as binary classificationswith 1 : 1 positive and negative samples (we randomly select a classas the positive label and sample negatives from the rest). We reportthe results in , from which we have two observations: First,our prompt method significantly outperforms the other approachesand the prediction results make sense. In contrast, the problem ofthe hard transfer method is that the source model sometimes cannot well decide on the target tasks because the target classes maybe far away from the source classes. This may even cause negativetransfer results (results that are lower than random guess). In mostcases, the fine-tuning method can output meaningful results witha few steps of tuning but it can still encounter a negative transferproblem. Second, the graph-level task has better adaptability thanthe node-level task for the edge-level target, which is in line withour previous intuition presented in (section 3.2). 4.3.2Transferability to Different Domains. We also conduct themodel on Amazon and PubMed as source domains, then load themodel states from these source domains and report the performanceon the target domain (Cora). Since different datasets have variousinput feature dimensions, we here use SVD to unify input featuresfrom all domains as 100 dimensions. Results are shown in ,from which we can find that the good transferability of our promptalso exists when we deal with different domains.",
  "pre-train+fine-tune": "GraphCL+GAT85.50 85.54 89.3183.00 85.47 92.1372.03 72.82 83.2392.15 92.18 94.7885.50 85.50 86.33GraphCL+GCN85.50 85.59 87.9486.50 84.57 94.5671.00 71.90 80.3393.58 93.55 94.9378.75 77.29 89.40GraphCL+GT85.95 85.05 87.9284.50 81.87 88.3669.63 70.06 81.3591.68 91.55 94.7886.85 86.93 88.91SimGRACE+GAT86.04 86.33 88.5583.50 85.84 90.0981.32 81.64 88.6193.58 93.57 93.9187.33 86.70 88.02SimGRACE+GCN85.95 86.05 89.3384.50 86.46 91.6080.50 81.52 89.1190.73 90.52 94.8585.26 84.64 86.99SimGRACE+GT86.40 86.47 89.6481.00 81.54 89.8169.50 70.97 77.1192.63 92.56 94.0485.95 86.05 89.37",
  "prompt": "GraphCL+GAT86.40 86.47 89.4686.50 89.93 92.2473.36 73.32 84.7794.08 94.02 94.2085.95 85.97 87.17GraphCL+GCN85.95 86.01 88.9587.00 85.87 95.3572.50 72.91 81.3794.05 94.05 94.9884.60 84.43 88.96GraphCL+GT86.05 85.17 88.9385.50 85.28 88.6072.63 70.97 82.3992.63 92.64 94.8287.03 86.96 89.10SimGRACE+GAT86.67 86.36 89.5187.50 88.37 91.4782.62 83.33 89.4193.35 94.66 94.6187.75 87.69 88.88SimGRACE+GCN86.85 86.90 89.9585.00 85.85 91.9581.00 82.24 89.4393.95 92.06 93.8985.50 85.54 87.30SimGRACE+GT86.85 86.87 89.7587.50 86.63 90.8576.50 80.82 86.8494.05 94.06 94.9686.40 86.50 89.74",
  ": Effectiveness of main components": "any across links between prompt tokens and the input graphs. Wereport the performance in , from which we can find themeta-learning and token structure all contribute significantly to thefinal results. In particular, the inserting pattern between a promptgraph and the input graph plays a very crucial role in the finalperformance. As previously discussed, the purpose of the prompt-based method is to relieve the difficulty of traditional pre-train,fine-tuning by filling the gap between the pre-training model andthe task head. This means the prompt graph is proposed to furtherimprove the fine-tuning performance. This is particularly importantwhen we transfer the model across different tasks/domains, whichproposes harder demand for the task head. As suggested in , even when we totally remove the tunable task head, the w/o hvariant can still perform very competitively, which suggests thepowerful capability of bridging upstream and downstream tasks.",
  "All in One: Multi-Task Prompting for Graph Neural NetworksKDD 23, August 610, 2023, Long Beach, CA, USA": "the complexity of the prompt graph very small. The limited to-ken numbers make our tunable parameter space far smaller thantraditional methods, which can be seen in . This meansour method can be efficiently trained with a few steps of tuning.As shown in , the prompt-based method converges fasterthan traditional pre-train and supervised methods, which furthersuggests the efficiency advantages of our method.",
  "Flexibility on Graph Transformation (RQ5)": "As discussed in section 3.5.2, the flexibility of data transformationis the bottleneck of prompt-based methods. Here we manipulateseveral graphs by dropping nodes, dropping edges, and maskingfeatures, then we calculate the error bound mentioned in Equa-tion 5 and 6. We compare the original error with the naive promptmentioned in Equation 5, and our prompt graph with 3, 5, and10 tokens. As shown in , our designed prompt graph sig-nificantly reduces the error between the original graph and themanipulated graph. This means our method is more powerful tostimulate various graph transformations and can further supportsignificant improvement for downstream tasks. This capability canalso be observed in the graph visualization from two approaches.As shown in , the graph representations from a pre-trainedmodel present lower resolution to node classes compared with ourprompted graph.",
  ": Visualization of graph representations": "This research is supported in part by project #MMT-p2-23 of theShun Hing Institute of Advanced Engineering, The Chinese Univer-sity of Hong Kong, by grants from the Research Grant Council ofthe Hong Kong Special Administrative Region, China (No. CUHK14217622), NSFC (No. 61972087, No. 62206067, No. U1936205, No.62172300, No. 62202336), Guangzhou-HKUST(GZ) Joint FundingScheme (No. 2023A03J0673), National Key R&D Program of China(No. 2022YFB3104300, No. 2021YFC3300300), the Fundamental Re-search Funds for the Central Universities (No. ZD-21-202101), andOpen Research Projects of Zhejiang Lab (No. 2021KH0AB04). Thefirst author, Dr. Xiangguo Sun, in particular, wants to thankhis parents for their kind support during his tough period.",
  "Wei Jin, Tyler Derr, Haochen Liu, Yiqi Wang, Suhang Wang, Zitao Liu, and JiliangTang. 2020. Self-supervised learning on graphs: Deep insights and new direction.arXiv preprint arXiv:2006.10141 (2020)": "Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The Power of Scale forParameter-Efficient Prompt Tuning. In Proceedings of the 2021 Conference onEmpirical Methods in Natural Language Processing. 30453059. Jia Li, Zhichao Han, Hong Cheng, Jiao Su, Pengyun Wang, Jianfeng Zhang, andLujia Pan. 2019. Predicting path failure in time-evolving graphs. In Proceedings ofthe 25th ACM SIGKDD International Conference on Knowledge Discovery & DataMining. 12791289.",
  "Xiang Lisa Li and Percy Liang. 2021. Prefix-Tuning: Optimizing ContinuousPrompts for Generation. In Proceedings of the 59th Annual Meeting of the Associa-tion for Computational Linguistics. 45824597": "Yan Ling, Jianfei Yu, and Rui Xia. 2022. Vision-Language Pre-Training for Mul-timodal Aspect-Based Sentiment Analysis. In Proceedings of the 60th AnnualMeeting of the Association for Computational Linguistics. 21492159. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Gra-ham Neubig. 2021. Pre-train, prompt, and predict: A systematic survey of prompt-ing methods in natural language processing. arXiv preprint arXiv:2107.13586(2021). Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and JieTang. 2022. P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning AcrossScales and Tasks. In Proceedings of the 60th Annual Meeting of the Association forComputational Linguistics (Volume 2: Short Papers). 6168.",
  "advances in natural language processing via large pre-trained language models:A survey. arXiv preprint arXiv:2111.01243 (2021)": "Yujia Qin, Xiaozhi Wang, Yusheng Su, Yankai Lin, Ning Ding, Zhiyuan Liu,Juanzi Li, Lei Hou, Peng Li, Maosong Sun, et al. 2021. Exploring low-dimensionalintrinsic task subspace via prompt tuning. arXiv preprint arXiv:2110.07867 (2021). Anna Rogers, Olga Kovaleva, Matthew Downey, and Anna Rumshisky. 2020.Getting closer to AI complete question answering: A set of prerequisite real tasks.In Proceedings of the AAAI conference on artificial intelligence, Vol. 34. 87228731.",
  "Zheyan Shen, Jiashuo Liu, Yue He, Xingxuan Zhang, Renzhe Xu, Han Yu, andPeng Cui. 2021. Towards out-of-distribution generalization: A survey. arXivpreprint arXiv:2108.13624 (2021)": "Yunsheng Shi, Zhengjie Huang, Shikun Feng, Hui Zhong, Wenjin Wang, and YuSun. 2020. Masked label prediction: Unified message passing model for semi-supervised classification. arXiv preprint arXiv:2009.03509 (2020). Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and SameerSingh. 2020. AutoPrompt: Eliciting Knowledge from Language Models withAutomatically Generated Prompts. In Empirical Methods in Natural LanguageProcessing (EMNLP). Mingchen Sun, Kaixiong Zhou, Xin He, Ying Wang, and Xin Wang. 2022. GPPT:Graph pre-training and prompt tuning to generalize graph neural networks. InProceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery andData Mining. 17171727. Xiangguo Sun, Hong Cheng, Bo Liu, Jia Li, Hongyang Chen, Guandong Xu, andHongzhi Yin. 2023. Self-supervised Hypergraph Representation Learning forSociological Analysis. IEEE Transactions on Knowledge and Data Engineering(2023). Xiangguo Sun, Hongzhi Yin, Bo Liu, Hongxu Chen, Qing Meng, Wang Han, andJiuxin Cao. 2021. Multi-level hyperedge distillation for social linking predictionon sparsely observed networks. In Proceedings of the Web Conference 2021. 29342945. Xiangguo Sun, Hongzhi Yin, Bo Liu, Qing Meng, Jiuxin Cao, Alexander Zhou,and Hongxu Chen. 2022. Structure Learning Via Meta-Hyperedge for DynamicRumor Detection. IEEE Transactions on Knowledge and Data Engineering (2022).",
  "Max Welling and Thomas N Kipf. 2016. Semi-supervised classification with graphconvolutional networks. In J. International Conference on Learning Representations(ICLR 2017)": "Jun Xia, Lirong Wu, Jintao Chen, Bozhen Hu, and Stan Z Li. 2022. SimGRACE: ASimple Framework for Graph Contrastive Learning without Data Augmentation.In Proceedings of the ACM Web Conference 2022. 10701079. Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, andYang Shen. 2020. Graph contrastive learning with augmentations. Advances inNeural Information Processing Systems 33 (2020), 58125823.",
  "In this section, we supplement more experiments to evaluate theeffectiveness of our framework further. The source code is publiclyavailable at": "Additional Datasets Besides the datasets mentioned in themain experiments of our paper, we here supplement more datasetsin to further evaluate the effectiveness of our framework.Specifically, ENZYMES and ProteinsFull are two molecule/proteindatasets that are used in our additional graph-level classificationtasks. Movielens and QM9 are used to evaluate the performance ofour method on edge-level and graph-level regression, respectively. In particular, Movielens contains users rating scores to the movies,each edge in which has a score value ranging from 0 to 5. QM9 is amolecule graph dataset where each graph has 19 regression targets,which are treated as graph-level multi-output regression. Person-alityCafe and Facebook datasets are used to test the performanceof link prediction, both of which are social networks where edgesdenote the following/quoting relations.Multi-label v.s. Multi-class Classification In the main experi-ments, we treat the classification task as a multi-label problem. Herewe present the experimental results under a multi-class setting. Asreported in , our prompt-based method still outperforms therest methods.Additional Graph-level Classification Here, we evaluate thegraph-level classification performance where the graph label is notimpacted by nodes attributes. As shown in , our method ismore effective in the multi-class graph classification, especially inthe few-shot setting.Edge/Graph-level Regression Beyond classification tasks, ourmethod can also support to improve graph models on regressiontasks. Here, we evaluate the regression performance of both graph-level (QM9) and edge-level (MovieLens) datasets by MAE (meanabsolute error) and MSE (mean squared error). We only feed 100-shot edge induced graphs for the model and the results are shownin , from which we can observe that our prompt-basedmethods outperform traditional approaches.Link Prediction Beyond edge classification, link prediction isalso a widely studied problem in the graph learning area. Here, theedges are split into three parts: (1) 80% of the edges are for messagepassing only. (2) 10% of the rest edges as the supervision trainingset. and (3) the rest edges as the testing set. For each edge in thetraining set and the testing set, we treat these edges as positivesamples and sample non-adjacent nodes as negative samples. Wegenerate the edge-induced graph for these node pairs according tothe first part edges. The graph label is assigned as positive if thenode pairs have a positive edge and vice versa. To further evaluateour methods potential in the extremely limited setting, we onlysample 100 positive edges from the training set to train our model.In the testing stage, each positive edge has 100 negative edges.We evaluate the performance by MRR (mean reciprocal rank), andHit Ratio@ 1, 5, 10. Results from demonstrate that theperformance of our prompt-based method still keeps the best inmost cases."
}