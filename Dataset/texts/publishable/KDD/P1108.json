{
  "ABSTRACT": "We introduce Lumos, the first end-to-end multimodal question-answering system with text understanding capabilities. At the coreof Lumos is a Scene Text Recognition (STR) component that ex-tracts text from first person point-of-view images, the output ofwhich is used to augment input to a Multimodal Large LanguageModel (MM-LLM). While building Lumos, we encountered numer-ous challenges related to STR quality, overall latency, and modelinference. In this paper, we delve into those challenges, and discussthe system architecture, design choices, and modeling techniquesemployed to overcome these obstacles. We also provide a compre-hensive evaluation for each component, showcasing high qualityand efficiency.",
  "Joint First Authors": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-1-4503-XXXX-X/18/06 ACM Reference Format:Ashish Shenoy, Yichao Lu, Srihari Jayakumar, Debojeet Chatterjee, MohsenMoslehpour, Pierce Chuang, Abhay Harpale, Vikas Bhardwaj, Di Xu, ShicongZhao, Longfang Zhao, Ankit Ramchandani, Xin Luna Dong, and Anuj Kumar.2024. Lumos : Empowering Multimodal LLMs with Scene Text Recognition.In Proceedings of Knowledge Discovery and Data Mining (KDD 24). ACM,New York, NY, USA, 11 pages.",
  "INTRODUCTION": "Visual question answering has been a research area for nearly adecade and has gained increased attention with recent progresson LLMs and vision language pre-training (Multi-Modal LLMs).Industry anticipates that very soon, we will have smart assistantsthat understand scenes/images just as well as humans . In thispaper, we focus on one key abilities needed for scene understanding,visual understanding and question-answering related to text in thescene. We describe Lumos1, an end-to-end system for multimodaltext-understanding. Lumos is well-suited for real-world applicationand readily leverages on-device processing to enable smooth userexperiences. shows example user interactions for some of Lumossuse-cases. At the first glance, one may think this problem is al-ready solved by Multimodal Large Language Models (MM-LLMs).In , MM-LLMs demonstrated capabilities understanding textsfrom images without a standalone STR component. It would seemstraight-forward to enable this feature for edge devices by taking apicture using the on-device camera, transfering to a cloud-based ser-vice, along with any voice recognition of user questions, and thenhaving an MM-LLM answer the question using the image. If textrecognition is sub-optimal when relying purely on the MM-LLM,",
  "KDD 24, August 2529, 2024, Barcelona, SpainShenoy, Lu, Jayakumar, Chatterjee, Moslehpour, Chuang, Harpale, Bhardwaj, et al": "STR I nf er ence Phot o Capt ur e Compr ess & Tr ansf er ASR TTS User Quer y Recogni zed Text and Locat i ons Pr ompt Desi gner On Devi ce Ser ver Si de Low r esol ut i on i mage Phot o capt ur e i ni t i at ed I mage avai l abl e I mage t r ansf er compl et e",
  ": Lumos Quality metrics": "one might choose to run a separate Scene Text Recognition (STR),another mature technique, on the image and send the recognizedtexts to the MM-LLM as prompt input to facilitate text understand-ing. We will now discuss in detail why such implementations areinadequate and the challenges we solve within Lumos. The first and key challenge we encounter is latency: just transferringa high-resolution image from device to cloud cost significant timeresulting in a poor user experience. For instance, transmitting animage of size 3 4 (standard resolution for todays devices) froma device to the cloud may take several seconds before even runningany AI models. And the end-to-end time to get a response wouldbe even longer making for a poor experience. Alternatively, if we transfer only a low-resolution thumbnail, thetransfer time can be significantly reduced (e.g., transferring a thumb-nail of size 450 600 pixels takes only a few hundred ms). However,this results in significantly degraded quality on text recognition.As shown in , the accuracy of question answering relyingsolely on running MM-LLM over thumbnails is only 52%. A sepa-rate cloud-based STR can barely recognize texts on the thumbnailseither, since the size is too small, illegible even for humans. Now assuming we choose an on-device STR solution, the secondchallenge is the constrained compute and memory resources ondevices. Although running STR models on-device may seem likea viable solution to address latency concerns, current state-of-the-art STR models are not readily suitable for on-device usage; forexample, Googles recent work features a text detection modelthat alone has a size of 240MB, impractical for on-device use whereseveral other processes might be running and sharing memory. The final set of challenges arise with doing STR on in-the-wild textimages, which are different from common web images, scanned doc-uments or zoomed-in images. Images taken on-the-go and outdoorscan amplify the challenges of STR. 1) The cameras are typicallywide angle, and thus the text of interest occupies only a small por-tion of the image; furthermore, there is often additional backgroundtext which can be irrelevant to the user query (see (c)). 2)The text in the scene may not have any uniformity: rotated, diverseorientations and font sizes. 3) The image quality might be poorowing to sub-optimal lighting condition, user movement, and thecamera angle. For all of these reasons, traditional OCR (OpticalCharacter Recognition) systems, despite their strong performanceon scanned documents and screenshots, can fall short on a STRtask in an in-the-wild text setting. As an example, the cloud-basedOCR solution Rosetta exhibits a surprising 53% Word Error Rate(WER) on our in-the-wild text STR benchmark (see fordetails). In this paper, we discuss our results overcoming these three chal-lenges. (1) In our tests, our proposed system has an average end-to-end latency of 5 seconds, including photo capture, image transfer,on-device STR execution, and on-cloud MM-LLM execution. (2) Ouron-device STR models have a total size of 8Mb, a peak memoryfootprint of 200Mb, an average latency of 1sec, and 0.4 mWhpower usage. (3) Despite the low cost, our STR solution achievescompetitive quality on public STR benchmarks when compared tostate-of-the-art STR solutions from other cloud service providers(b). On our own in-the-wild text benchmarks, it achievesa 14.6% WER and enables an average accuracy of 80% on complex",
  "Lumos : Empowering Multimodal LLMs withScene Text RecognitionKDD 24, August 2529, 2024, Barcelona, Spain": "[n. d.]. AWS Rekognition. [n. d.]. Google Cloud OCR. OpenAI (2023). 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, YanaHasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, RomanRing, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei,Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, AidaNematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, OriolVinyals, Andrew Zisserman, and Karen Simonyan. 2022. Flamingo: a VisualLanguage Model for Few-Shot Learning. arXiv:2204.14198 [cs.CV]",
  "PREVIOUS WORK": "OCR and STR. The field of OCR has been a focal point of researchfor many years. However, the spectrum of difficulty in recognizingtext in natural environments is notably broad. At one end, OCRs ap-plication to scanned documents containing well-structured printedtext is widely recognized as one of the most successful implementa-tions of computer vision . Conversely, STR focuses on recog-nizing text in the wild, which still represent a significant challengedue to the larger variance of wild text objects .The STR problem we are solving in this paper considers in-the-wildtext images (so the area of interest is considerably smaller), andneeds to be tackled on device, thus is much harder and requiresbetter model designs and tuning.On-device STR. When it comes to STR on-device, in an ex-tremely lightweight OCR system with a size of only 3.5Mb is pro-posed; the model achieves impressive latency on GPUs but still falls short when it comes to CPUs. Munjal et al. describes anefficient lightweight STR system, which has only 0.88M parametersand performs real-time text recognition at a speed of 2.44 ms perword crop of size 1664. In comparison, the STR solution describedin this paper takes 0.29 ms per word crop of size 48 320. Multimodal LLMs and Text Recognition Ability More recently,MM-LLMs have demonstrated potential in addressing a varietyof tasks, including text recognition . Whilethe current trend leans towards the use of all-modality LLMs, theyhave limitations particularly in handling text-in-the-wild scenar-ios. Furthermore, the challenges associated with high transfer la-tency as described in makes these models impractical forimmediate use . A different approach, the Flamingo mod-els , have shown impressive performance on tasks such asgeneric VQA and captioning, but fall short when compared to on text rich VQA. Both sets of models are sub-optimal comparedto OCR-assisted VQA as we discussed in this paper and are notoptimized for memory and compute at inference time.",
  "OVERALL ARCHITECTURE": "We now describe the overall architecture of Lumos (see ).To simplify, we focus on multimodal use cases, assuming a picturewill be taken once the user triggers the flow, and the device willprovide the image at two resolutions 3 4 (full resolution), and450 600 (thumbnail). Device-side: At the device side, when a user gives a voice query,three components will start in parallel. First, Automatic SpeechRecognition (ASR) starts processing the query after a wake word.Second, the photo capture, compression (e.g., from a 3 4 full-resolution image to a 450 600 thumbnail) and transfer to cloudwill begin in parallel to the voice query completion (to reduceoverall system latency). Third, the STR component will start as soonas the full-resolution image is ready. As indicated by in ,we carefully design the system to parallelize the time consumingcomponents, STR inference and image transfer, to reduce latency. Cloud-side: The cloud side hosts a MM-LLM model, which takesas input the low-resolution thumbnail, a prompt composed of therecognized texts and their coordinates from STR, and the userquery from ASR, and generates the answer response. An illustra-tive prompt to MM-LLM can be found in Appendix . Sub-sequently, the TTS (Text-to-Speech) component translates theresponse to voice signal and sends back to the user.",
  "This architecture incorporates three design choices we havecarefully made": "Where to do STR? As discussed in detail in , to reducelatency, we transfer only a low-resolution image to the cloud.However, neither an MM-LLM nor an STR model can achievedesired quality on such a low-resolution image, especiallygiven that the text area is typically quite small in the in-the-wild text image. We thus apply STR on device with thefull-resolution image, and only on the region of interest (seesection 4.1 for details).",
  "and image transfer in parallel (see ). With this design,for the majority of the cases STR does not add extra latency": "How to extend to MM-LLM use cases where STR is not necessaryto answer the user question? Ideally, we wish to build a singlemultimodal assistant that can answer text-heavy questionsas well as generic questions where text comprehension is notnecessary. Determining whether a user question is based onthe text in the scene requires an NLU (Natural Language Un-derstanding) component, which can take extra time and mayhave limited quality with the limited computation poweron device. We instead conduct STR in all cases and deferthe decision to the MM-LLM on the cloud. This approach isfeasible only because of our significant reduction of latency(particularly through parallelization) and optimization ofhardware efficiency for STR. It is worth mentioning that placing STR on-device poses signifi-cant constraints on the models architecture, latency, memory, andbattery consumption, in addition to the quality challenges for in-the-wild text STR discussed in . Despite these limitations,our on-device STR model achieves strong performance comparedto three state-of-the-art cloud STR solutions that do not have suchconstraints (see for details). In the next section, we describehow we achieve this.",
  "We now describe our core techniquethe on-device STR. Thispipeline contains four sub-components as depicted in": "Region of Interest (ROI) detection takes an image asinput (at both 3 4 resolution and a thumbnail resolution),outputs a cropped image (about 1 1.3 size) that containsall the text likely needed to answer the user query. Thiscomponent plays a key role to ensure that we run the restof the STR pipeline only on the relevant portion of the inputimage, reducing both computational cost and backgroundnoise.",
  "Reading-order reconstruction organizes recognized wordsinto paragraphs and in reading order within each paragraphbased on the layout. It outputs text paragraphs as well astheir location coordinates": "We note that in most previous works STR refers to only the Textdetection and Text recognition parts. We included two additionalcomponentsROI detection and Reading order reconstructioninour STR system to address Lumos specific challenges. The primarychallenges we face include the limited hardware for inference andthe large variation of texts in the wild. We address these challengeesthrough careful model architecture selection and training data cura-tion and augmentation, as we discuss in detail next.",
  "ROI Detection": "Motivation ROI detection plays a key role for on-device STR andthere are three motivations behind it. First, as shown in (b),because of the nature of in-the-wild text images, the text area ofinterest often occupies only a small fraction of the image, even ifthe object is only an arm length away from the device. Running STRdirectly on the full-resolution image can be prohibitively expen-sive with the limited computational power of the device, whereasdownsizing the image can make the texts too small to be legibleeven to humans. Second, as shown in (c), the image maycontain a lot of background text that are irrelevant to the user query,such as text from products on the shelves. Recognizing these textsconsumes the limited hardware resources, increases the latency,and confuses the MM-LLM at the downstream. Third, users oftenhold the paper or the object of interest like in (c), or pointto the particular words or phrases like in (a), where thosegestures provide critical clues for ROI detection. These motivationsunderscore the importance of identifying the ROI before proceedingwith other steps in STR.",
  "ROI. A major challenge for ROI is the non-holding or non-pointinghands in the picture, which can lead to wrong detection results (seeexample in in the Appendix)": "Solution and modeling We treat ROI detection as an object(salient area) detection problem, facilitated with keypoint detec-tion in presence of a pointing finger. For finger pointing, we detecttwo key pointsthe last joint and the tip of index finger; the twopoints formulate a pointing vector, as shown in (a). We traina model that jointly detects both the ROI and the two keypoints(when present). If the keypoints are detected, we include an addi-tional prompt to the downstream MM-LLM, describing the pointingevent as well as the words and the paragraphs closest to the tipof the index finger in the direction of the pointing vector. We usethe Mask-rcnn model since it can provide a unified frameworkfor both object and keypoint detection. We apply inference on the450 600 thumbnail. Training data We trained the model using 80K in-the-wild textimages annotated with salient regions, and 20K images with handholding or finger pointing. To reduce false positives caused byaccidental hands, we included 10K images with a hand that is neitherholding nor pointing as hard negatives in our training data.",
  "Text Detection": "Problem definition and challenges Text detection takes thecropped image in full-resolution as input, predicts location of eachword as bounding boxes. There are three challenges for detectingtext in the wild: C1. the text size can be very small (e.g., ingredientson a coke can at arm length) or very big (e.g., storefront); C2. text canoften be tilted with the nature of the image; C3. we are not able touse state-of-the-art text detection model architectures like with the on-device constraint. Solution and modeling To account for the tilted text (C2), ourdetector predicts rotated bounding box as mentioned in . To becomputationally efficient (C3), we use an anchor-free single-stagedetector as described in (instead of a two-stage detector). Weuse FBNetv2 (with 1.1 million parameters) with PAN neck for the backbone of the detector. FBNetv2 is a CNN model designedfor transforming input images into feature maps; this backbonenot only is computationally efficient (C3) but also provides strong image features at different scales (C1). For the loss, we use a variantof the well-known focal loss as classification loss, and theKLD loss as our box regression loss for its state-of-the-artperformance on rotated box (C2). Training data Our training data consist of 140K images with 6million annotated bounding boxes, combining public STR datasetslike text OCR and in-house annotations on in-the-wild textimages. To address the challenge of text scale variation (C1), weapplied aggressive scale jittering, data augmentation that increasesor reduces input image sizes, to create variational sizes of boundingboxes in training data.",
  "Text Recognition": "Problem definition and challenges Text recognition takes theimage crop from ROI detection and the word bounding box coordi-nates, and outputs the recognized words for each box. There arethree key challenges we need to address: C1. huge diversity in thewidths of bounding boxes (e.g., URLs tend to be longer, price tagstend to be extremely small); C2. diversity of text appearances interms of font, size, orientation, and background; C3. existence of(quite some) text detection errors; C4. hardware constraints. Solution and modeling We transform the problem of recognizinga word into the problem of recognizing a sequence of characters. Be-cause of hardware acceleration constraints (C4) as we will describein , we are limited to using fixed width and height for eachbounding box. Therefore, we scale each bounding box to a fixedheight of 48 pixels and a fixed width of 320 pixels to ensure that theinput to the model is consistent and can be processed efficiently.Based on statistics we assume that each individual character has awidth of 8 pixels. Thus, we recognize a maximum of 40 characters(320/8) per bounding box; a word rarely exceeds this limit. Thefinal recognizer output is a posterior of shape 40 x || andthe size of the alphabets in our model is top-150 most frequentlyused Latin characters obtained from the training data.We again use the FBNetv2 backbone and train the model usingCTC (Connectionist Temporal Classification) loss, as it can handlevariable-length input sequences (C1) and has lower latency andcomputational complexity (C4), critical in dense text scenarios.",
  ": Left: Word bounding boxes. Right: Paragraphs fromout Reading Order Reconstruction component": "Training data During training, to handle the extreme variationsin bounding box lengths (C1), we employ curriculum learning; thatis, we gradually increase the complexity of the input images. Webegin with words containing a maximum of 16 characters andprogressively increase the character limit up to a maximum of 40characters. This helps the model learn the necessary features andpatterns more effectively.Overall, the recognizer model is trained on 3M word bound-ing boxes, with 15% being synthetically generated to increase therobustness of the model. To be more robust against detector er-rors (C3), we introduce random cropping around the boundariesof the bounding boxes based on error patterns we have observedin detector evaluation, combined with jittering. We incorporatedRandAug , which applies random combinations of image trans-formations such as rotation, shearing, brightness adjustment, andcontrast adjustment to input images. By exposing the model to awide range of transformed images, it learns to be more robust tothese transformations and generalizes better to new, unseen data(C2).",
  "Reading Order Reconstruction": "Problem definition The Reading Order Reconstruction moduleconnects the words to paragraphs, returns the words in the para-graph in reading order, together with the coordinates of each para-graph. shows sample paragraphs. Solutions We identify paragraphs in three steps. First, we connectthe words to paragraphs. We expand the word bounding boxesboth vertically and horizontally by predefined ratios, as shown in. The expansion ratios are selected to fill the gaps betweenwords within a line and lines within a paragraph and are the samefor all bounding boxes. We then group bounding boxes that havesignificant overlap after expansion as a paragraph. For each para-graph, we then apply raster scan (i.e., sorting by Y coordinate thenX) to the words to generate the paragraph in reading order. Finally,we compute the location of the paragraph by finding the minimumarea rectangle enclosing all words in the paragraph. See Algorithm1 in the Appendix for detailed description of the Reading orderreconstruction module.We found this simple heuristic approach achieves a good qualitymost of the time with low computation cost. The accuracy for thismodule is 92% using metrics defined in [? ]. Pyt or ch",
  "Eager Model": "( f p32) Quant i zed Model ( i nt 8) Tor chscr i pt ed Model ( i nt 8) Har dwar e Accel er at or model Cal i br at i on Dat a Cal i br at i on and Dynami c PTQ t or ch. j i t . t r acet or ch. j i t . scr i pt opt i mi ze f or har dwar e accel er at or",
  "ON-DEVICE EXPORT": "As mentioned in the introduction, Lumos need to be compatiblewith devices to make our smart assistant more accessible. We eval-uated our on-device systems performance with on our testingdevices, which is equipped with hardware accelerators for deeplearning models. We describe the process of exporting our modelsto the testing device as well as the memory/latency in this setting. (1) Quantization to int8 We first quantize the float32 models toint8 models to save inference latency and runtime memory.We use Post Training Quantization (PTQ) to do this,because the sizes of our models were relatively small andPTQ requires a calibration step only after models are fulltrained.",
  "(2) On-device CPU models We next transfer the models toTorchScript models using packages provided by PyTorch.This provides a model that is executable on CPU of the de-vice": "(3) On-device hardware accelerated models Modern devicesoften comes with a hardware accelerator for deep learningmodels. To utilize this, we take a further step making ourmodel hardware accelerator compatible, and evaluate thelatency of our system on hardware accelerator. We emphasize that the model execution efficiency is achievedwith cost. First, we are constrained to use quantization and hard-ware accelerator friendly models, limited our modeling choices asstated in . Second, quantization and model export wouldcause accuracy drops in the ML models. Regardless, our system stillachieves competitive performance compared to other STR servicesas we show soon in .",
  "Experiment Setup": "Datasets summarizes the datasets we used for evaluation.We have two benchmarks: In-house wild text benchmark and Publicwild text benchmark . In-house wild text benchmark contains968 in-the-wild text images taken from an edge device and contains47K word boxes. The benchmark contains annotations for the wordboxes and transcriptions, and in addition annotations for salientareas for ROI evaluation. Public wild text benchmark is a broadly-used STR benchmark, containing 1.7K images and 146K word boxes. We then created task-specific datasets to evaluate end-to-end qualityof summarization, word lookup and a few other tasks on the In-house wild text benchmark. We first sampled text-heavy images fromthe benchmark, and then our annotators created 3 task-relatedquestions for each image. Metrics definition We have two major metrics. To understand theend-to-end question answering quality, we measure QA accuracyas the percentage of successful responses among all answers. Agroup of raters manually decided the correctness of each responsejudging from the image, user query and generated response, basedon the relevancy, fluency and factual accuracy of the response. To understand the quality of STR solutions, we measured the WordError Rate (WER), a standard metric extensively used in the domainof speech and text recognition . WER considers 3types of errors: 1) Deletion: a ground truth word that is not detected;2) Insertion: a prediction that is not matched to any ground truthword box; 3) Substitution: a prediction that matches a ground truthbox, but the word recognized is different from the ground truth.WER is the sum of Deletion, Insertion, Substitution errors divided bythe total number of words in the ground truth. With the existenceof insertion errors, WER can be higher than 1. A lower WER isindicative of higher quality of the models.",
  "End-to-End Quality": "We evaluated the overall quality of three variants of Lumos: 1)MMLLM only: we provide only the 450 600 thumbnail and userquery to the MM-LLM; 2) MM-LLM+STR: we in addition pro-vide the text output from the on-device STR to MM-LLM; 3) MM-LLM+STR+Positions: we in addition provide the paragraph location(from reading order reconstruction module). See for detailedinput formats of these variants. compares the QA accuracy of the three variants on thetask-specific E2E datasets. We have four observations. First, Lumosobtains a high average QA accuracy, 80%, in question answering.Second, the on-device STR significantly improves QA accuracy onall three tasks over MM-LLM only (80% vs. 52%) . The improvementis particularly large for the summarization task (+35%), where Lu-mos needs to comprehend dense texts. Third, sending positions to",
  "Rosetta OCR53%46.0%1.1%5.9%15MbLumos STR Server13%4.7%1.4%6.9%30MbLumos STR Device14.6%5.1%1.8%7.7%8Mb": "MM-LLM further improves the performance on all tasks (+1.6%),as it allows the model to better handle the spatial relationshipsbetween words in the scene. Finally, among different tasks, we ob-serve the best quality on summarization (88%), which has highertolerance on small recognition errors; the quality on word lookup islowest (67%), as we observe a large variety of hand-word positions,making the problem much more difficult.",
  "STR quality": "Lumos STR quality We next compare quality of 5 STR Systems: 1)Rosetta , a well known STR system from the research community;2) Google Cloud OCR ; 3) AWS Rekognition ; 4) Lumos STRCloud: Lumos STR running on cloud; 5) Lumos STR Device: LumosSTR running on our device hardware. For a fair comparison, weremoved punctuations from the benchmarks since different baselineSTR systems treat them differently, as a separate word or part of aword. We also removed words smaller than 8 pixels high since it ishard for humans to read. shows the WER of each solution, together with error break-downs in terms of deletion, insertion, substitution errors. We havefour observations. 1) Lumos STR has a reasonably low WER, 30%on the public benchmark and 13% on the in-house benchmark. 2)Lumos STR outperforms Rosetta, AWS, and Google, despite nevertrained on the public wild text benchmark (we do not know ifGoogle and AWS were trained on the public wild text benchmark).Rosetta made a lot of deletion errors as it missed small texts and hasa low word limit per image. Similarly, AWS has a low word limitper image, leading to high deletion errors. 3) Lumos STR Device issmallest in model size with only 8Mb parameters; nevertheless, itsacrifices WER by only 1-2% comparing with the on-server modeland still has a competitive performance. 4) Finally, among differenttypes of errors, Substitution errors is only a small portion (<10%),showing that word detection is a much bigger challenge than wordrecognition for STR tasks.",
  "wild text benchmark. There are three contributors for quality im-provements as shown in": "ROI detection allows us to run our detection and recognitionon a text-dense cropped region in original size, instead of onan aggressively downsized (3x-4x) full image, thus reducingWER by 11%, and especially reducing WER on small-fonttexts. Our detection model uses additional in-domain data anddata augmentation for training to improve robustness, andincreases word limit per image, thus reducing WER by 16%.In particular, we increased recall of detecting word boxes,thus reducing deletion errors, in detection of small text (<15pixels tall) by 14% and of large text (>120 pixels tall) by 20%.",
  "Finally, these improvements are well preserved in model quanti-zation and export, which increased WER by only 1.6% but achievedhuge efficiency gains as we discuss soon in .4": "ROI detection recall To illustrate the effectiveness of the ROIdetection component, we compared the performance of 3 imagecropping methods: 1) Center Crop: heuristic-rule baseline that cropsthe 1500*2000 center region (similar as the ROI output size); 2) ROIdetection: use an object detection model to detect the region; 3)ROI detection with hand cues: use object detection together withthe holding and pointing gestures to detect the region.We measured ROI quality by word-level recallhow many wordsof interest are included in the ROI output region. shows theresults on the in house wild text benchmark. We are able to reach99% recall with our ROI detection component while reducing imagesize by 25% on average. Our model achieves much higher recall(+32%) than the Center Crop baseline, and including hand cuesfurther improves the recall (+1.3%).",
  "CONCLUSION": "This paper presented Lumos, one of the first smart multimodalassistant with strong text understanding capabilities which is alsodevice compatible. Our comprehensive evaluation demonstratesthe effectiveness of our proposed method, outperforming existingapproaches in terms of accuracy. Additionally, we have shownthat our system meets the stringent latency, size, memory, power,and compute requirements for on-device deployment. Overall, ourwork represents a significant step towards enabling MM-LLMs toread in real-world scenarios, paving the way for more advancedapplications in the fields of computer vision and natural languageprocessing. Future work includes further optimizations to our on-device models, and research on end-to-end text recognition andvisual translation with multimodal large language models. The authors would like to thank Mei-Yuh Hwang, Praveen Krishnan,Guan Pang, Becka Silvert, Renato Sanchez, Crystal Nakatsu, LucasKabela, Frank Seide, Samyak Datta, Peyman Heidari, Shashank Jain,Nish Gupta, Kate Ovchinnikova, Rongzhou Shen, Saumya Mukul,Shane Moon, David Strauss, Lintao Cui, Sofiane Djeffal, MeghaTiwari, Vitaly Berov, Shanying Luo for their valuable inputs andcontributions.",
  "Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,C. Lawrence Zitnick, and Devi Parikh. 2015. VQA: Visual Question Answering.CoRR abs/1505.00468 (2015). arXiv:1505.00468": "Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, WanrongZhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev,Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and LudwigSchmidt. 2023. OpenFlamingo: An Open-Source Framework for Training LargeAutoregressive Vision-Language Models. arXiv:2308.01390 [cs.CV] Dhanush Bekal, Ashish Shenoy, Monica Sunkara, Sravan Bodapati, and KatrinKirchhoff. 2021. Remember the Context! ASR Slot Error Correction ThroughMemorization. In 2021 IEEE Automatic Speech Recognition and UnderstandingWorkshop (ASRU). 236243. Fedor Borisyuk, Albert Gordo, and Viswanath Sivakumar. 2018. Rosetta: LargeScale System for Text Detection and Recognition in Images. In Proceedings of the24th ACM SIGKDD International Conference on Knowledge Discovery & Data Min-ing (London, United Kingdom) (KDD 18). Association for Computing Machinery,New York, NY, USA, 7179.",
  "Ekin D. Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V. Le. 2019. RandAug-ment: Practical data augmentation with no separate search. CoRR abs/1909.13719(2019). arXiv:1909.13719": "Saket Dingliwal, Ashish Shenoy, Sravan Bodapati, Ankur Gandhe, Ravi TejaGadde, and Katrin Kirchhoff. 2021. Efficient domain adaptation of languagemodels in ASR systems using Prompt-tuning.CoRR abs/2110.06502 (2021).arXiv:2110.06502 Saket Dingliwal, Ashish Shenoy, Sravan Bodapati, Ankur Gandhe, Ravi TejaGadde, and Katrin Kirchhoff. 2022. Domain Prompts: Towards memory andcompute efficient domain adaptation of ASR systems. In Proc. Interspeech 2022.684688. Yuning Du, Chenxia Li, Ruoyu Guo, Xiaoting Yin, Weiwei Liu, Jun Zhou, YifanBai, Zilin Yu, Yehua Yang, Qingqing Dang, and Haoshuang Wang. 2020. PP-OCR:A Practical Ultra Lightweight OCR System. arXiv:2009.09941 [cs.CV] Hao Feng, Zijian Wang, Jingqun Tang, Jinghui Lu, Wengang Zhou, HouqiangLi, and Can Huang. 2023.UniDoc: A Universal Large Multimodal Modelfor Simultaneous Text Detection, Recognition, Spotting and Understanding.arXiv:2308.11592 [cs.AI]",
  "Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia. 2018. Path AggregationNetwork for Instance Segmentation. arXiv:1803.01534 [cs.CV]": "Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Yang Liu, Biao Yang, MingxinHuang, Dezhi Peng, Mingyu Liu, Mingrui Chen, Chunyuan Li, Xucheng Yin,Cheng lin Liu, Lianwen Jin, and Xiang Bai. 2023. On the Hidden Mystery of OCRin Large Multimodal Models. arXiv:2305.07895 [cs.CV] Shangbang Long, Siyang Qin, Dmitry Panteleev, Alessandro Bissacco, YasuhisaFujii, and Michalis Raptis. 2022. Towards End-to-End Unified Scene Text Detec-tion and Layout Analysis. arXiv:2203.15143 [cs.CV] Shangbang Long, Siyang Qin, Dmitry Panteleev, Alessandro Bissacco, YasuhisaFujii, and Michalis Raptis. 2022. Towards End-to-End Unified Scene Text Detec-tion and Layout Analysis. In 2022 IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR). 10391049. Jianqi Ma, Weiyuan Shao, Hao Ye, Li Wang, Hong Wang, Yingbin Zheng, andXiangyang Xue. 2018. Arbitrary-Oriented Scene Text Detection via RotationProposals. IEEE Transactions on Multimedia 20, 11 (Nov. 2018), 31113122.",
  "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2016.Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks.arXiv:1506.01497 [cs.CV]": "Ashish Shenoy, Sravan Bodapati, and Katrin Kirchhoff. 2021. ASR Adaptation forE-commerce Chatbots using Cross-Utterance Context and Multi-Task LanguageModeling. In Proceedings of the 4th Workshop on e-Commerce and NLP, ShervinMalmasi, Surya Kallumadi, Nicola Ueffing, Oleg Rokhlenko, Eugene Agichtein,and Ido Guy (Eds.). Association for Computational Linguistics, Online, 1825. Ashish Shenoy, Sravan Bodapati, Monica Sunkara, Srikanth Ronanki, and KatrinKirchhoff. 2021. Adapting Long Context NLM for ASR Rescoring in Conversa-tional Agents. In Proc. Interspeech 2021. 32463250. Baoguang Shi, Xiang Bai, and Cong Yao. 2017. An End-to-End Trainable NeuralNetwork for Image-Based Sequence Recognition and Its Application to Scene TextRecognition. IEEE Trans. Pattern Anal. Mach. Intell. 39, 11 (nov 2017), 22982304. Baoguang Shi, Xinggang Wang, Pengyuan Lyu, Cong Yao, and Xiang Bai.2016. Robust Scene Text Recognition with Automatic Rectification. In 2016IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 41684176. Yongxin Shi, Dezhi Peng, Wenhui Liao, Zening Lin, Xinhong Chen, Chongyu Liu,Yuyi Zhang, and Lianwen Jin. 2023. Exploring OCR Capabilities of GPT-4V(ision): A Quantitative and In-depth Evaluation. arXiv:2310.16809 [cs.CV]",
  "Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen.2021. MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transform-ers. arXiv:2012.00759 [cs.CV]": "Kai Wang and Serge Belongie. 2010. Word Spotting in the Wild. In Proceedingsof the 11th European Conference on Computer Vision: Part I (Heraklion, Crete,Greece) (ECCV10). Springer-Verlag, Berlin, Heidelberg, 591604. Xue Yang, Xiaojiang Yang, Jirui Yang, Qi Ming, Wentao Wang, Qi Tian, andJunchi Yan. 2022. Learning High-Precision Bounding Box for Rotated ObjectDetection via Kullback-Leibler Divergence. arXiv:2106.01883 [cs.CV] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Guohai Xu, ChenliangLi, Junfeng Tian, Qi Qian, Ji Zhang, Qin Jin, Liang He, Xin Alex Lin, and Fei Huang.2023. UReader: Universal OCR-free Visually-situated Language Understandingwith Multimodal Large Language Model. arXiv:2310.05126 [cs.CV]",
  "MM-LLM onlyWhat does this sign sayThe sign says, \"Haz Matado\"irrelevantwhich translates to \"Youve killed it.\"": "MM-LLM+STRHere is a transcription of the recognizedThe sign says, \"HAZtext in the image, which may containVOLUNTARIADO en esta tienda\" whichrelevantmisspelled words. [HAZ VOLUNTARIADOtranslates to \"DO VOLUNTEER WORKen esta tienda R OKFAM intermon].this store\"My question is whatdoes this sign sayin MM-LLM+STR+PositionI have included the coordinates of theThe sign says, \"HAZtext within the image. The coordinatesVOLUNTARIADO en esta tienda\" whichrelevant(0,0) is indicative of the top-lefttranslates to \"DO VOLUNTEER WORKcorner of the image and (3024, 4032)in this store\"is indicative of the bottom right cornerof the image. For example, the tuple(zzzz, 10, 20) means the paragraphzzzz is centered at the point (10, 20)within the image. Here is a transcriptionof the recognized text in the imageaccording to this coordinate system.It may contain misspelled words.[(HAZ VOLUNTARIADO en esta tienda, 1260, 1532) (R OKFAM intermon,1282, 1939)]. My question is whatdoes this sign say."
}