{
  "ABSTRACT": "Open-vocabulary detection (OVD) is a new object detection para-digm, aiming to localize and recognize unseen objects defined byan unbounded vocabulary. This is challenging since traditional de-tectors can only learn from pre-defined categories and thus fail todetect and localize objects out of pre-defined vocabulary. To handlethe challenge, OVD leverages pre-trained cross-modal VLM, suchas CLIP, ALIGN, etc. Previous works mainly focus on the openvocabulary classification part, with less attention on the localizationpart. We argue that for a good OVD detector, both classificationand localization should be parallelly studied for the novel objectcategories. We show in this work that improving localization as wellas cross-modal classification complement each other, and composea good OVD detector jointly. We analyze three families of OVDmethods with different design emphases. We first propose a vanillamethod, i.e., cropping a bounding box obtained by a localizer andresizing it into the CLIP. This vanilla method totally decouples thelocalization and classification components, making it convenient toimprove the OVD performance by applying more advanced objectlocalization models and VLMs. However, resizing cropped regionsinevitably causes the deformation of the object and leads slow calcu-lation speed. To address these, we next introduce another approach,which combines a standard two-stage object detector with CLIP.A two-stage object detector includes a visual backbone, a regionproposal network (RPN), and a region of interest (RoI) head. Wedecouple RPN and ROI head (DRR) and use RoIAlign to extractmeaningful features. In this case, it avoids resizing objects. To fur-ther accelerate the training time and reduce the model parameters,we couple RPN and ROI head (CRR) as the third approach. Weconduct extensive experiments on these three types of approaches",
  "*Both authors contributed equally to this research.Corresponding Author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior specific permission and/or afee. Request permissions from KDD 23, August 07, 2023, Long Beach, CA 2023 Association for Computing Machinery.ACM ISBN 978-1-4503-XXXX-X/18/06...$15.00 in different settings. On the OVD-COCO benchmark, DRR obtainsthe best performance and achieves 35.8 Novel AP50, an absolute2.8 gain over the previous state-of-the-art (SOTA). For OVD-LVIS,DRR surpasses the previous SOTA by 1.9 AP50 in rare categories.We also provide an object detection dataset called PID and providea baseline on PID.",
  "INTRODUCTION": "Object detection is a prominent vision task, aiming at localizing andrecognizing objects in images. This task requires a variety of fine-grained annotations (e.g., the bounding boxes and classes) of eachobject during training, which, however, makes it hard to extend thesize of data since manual human annotations are costly and tedious.In this sense, traditional object detectors may fail to precisely detectand localize objects out of pre-defined vocabulary at inference. Open-vocabulary detection (OVD), a task to detect unseen objectsdefined by an unbounded vocabulary, has attracted much attention inthe most recent period. The core challenge of the OVD task is how tolocalize and classify unseen (novel) categories at the inference stagesince they can only learn the knowledge from pre-defined (base) cat-egories during training. We next analyze the corresponding existingsolutions to overcome the above challenge from two perspectives:classification and localization. To classify the novel categories, several works leveragethe excellent zero-shot generalization ability of large-scale vision-language models (VLMs) such as CLIP . To this end, they mod-ify a two-stage object detector, including using the visual encoderof CLIP as the backbone of the object detector, replacing the class-specific classification head with a class-agnostic classification head,",
  "Multimodal KDD 23, August 07, 2023, Long Beach, CAJincheng Li, Chunyu Xie, Xiaoyu Wu, Bin Wang, and Dawei Leng": "pre-trained RPN to obtain a set of regions of interest (i.e., proposals).We then use the visual encoder (e.g., ResNet-50) of CLIP toencode the whole image. Given proposal boxes of an image, weextract their features along the first 3 blocks based on the wholeimage encoding and pool them using RoIAlign. The pooled featuresare then encoded by the last block of the visual encoder. We alsoreplace the class head of Fast R-CNN with the text embeddingsencoded by the CLIP text encoder. Considering the setup of open-vocabulary detection , we use the base class and hand-craftedprompt as the input of the text encoder of CLIP during training.During inference, we replace the base class with a combination ofbase and novel classes to generate new text embeddings.",
  "RELATED WORK": "Vision-Language Pre-training. Vision-language pre-training aimsto learn the correspondence between vision and natural language. Itis attractive that pre-trained vision-language models (VLMs) trained on large-scale image-text pairs show excellent zero-shot/few-shot migration ability on classification, object detection,and instance segmentation tasks. In particular, F-VLM showsthat pre-trained VLMs have a strong generalization ability whentransferring to the OVD task. Meanwhile, F-VLM eliminates theneed for knowledge distillation or detection-tailored pre-training.This motivates us to investigate how to make better use of VLMson the open-vocabulary object task. In this paper, we apply differentVLMs in the fundamental approaches, including original VLMs,larger VLMs, and detection-tailored pre-training VLMs. Open-Vocabulary Detection. Traditional object detection mayfail to localize and recognize unseen objects in an image at inference,i.e., zero-shot object detection. Recently, OVR-RCNN proposesthe open-vocabulary detection (OVD) benchmark to detect and local-ize objects for which no bounding box annotation is provided duringtraining. While OVR-RCNN evaluates the models on tens of cate-gories, ViLD proposes to evaluate on more than 1,000 categories,i.e., LVIS . Following the OVD benchmark, most existing meth-ods are proposed with different forms of weak supervision, such asextra image-caption pairs , extra classification datasets, and vision-language pre-trained models like CLIP. For example, RegionCLIP introduces a region-level pre-training alignment method with extra image-text pairs e.g., CC3M, demonstrating its capability on zero-shot and OVD task transferlearning. Detic trains the classifiers of a detector on image clas-sification data, i.e., ImageNet-21K , yielding excellent detectorseven for classes without box annotations. BARON proposes toalign the embedding of the bag of regions beyond individual regions,relying on the generalization ability of large-scale vision-languagepre-trained models. These methods keep achieving better resultsthan previous state-of-the-art methods with various techniques, suchas more powerful offline proposal generators , knowledge distil-lation , and prompt learning . When facing a new dataset fornew scenarios, it leaves researchers and engineers confused aboutwhich approaches or techniques to use. In this paper, we summarizethree fundamental approaches for open-vocabulary detection and in-vestigate them with different techniques, showing surprising resultsof different combinations.",
  "Coupling RPN and RoI head (CRR)Region embeddings": ": An overview of three approaches of open-vocabulary detection: a vanilla method, decoupling RPN and ROI head (DRR), andcoupling RPN and ROI head (CRR). Note that Vanilla and DRR require two backbones while CRR only needs one visual backbone.For simplicity, we omit the logits of regions when training RPN on base categories. For all three fundamental approaches, we get theclassification scores via cosine similarity, calculated by the region embeddings and the text embeddings, where the text embeddings areobtained by the text encoder of CLIP.",
  "To solve the open-vocabulary detection problem, we attempt toisolate the open-vocabulary detection task into two independentsub-tasks, that is, object localization and object classification": "Object Localization. To localize objects, both the class-awareobject detector, such as YOLO , Faster R-CNN , and the class-agnostic localizer, such as RPN, OLN can be used. The firstchallenge for OVD task is to localize novel objects. To solve thisproblem, we adopt the detection backbone with RPN to localize allobjects and classify them as the foreground class. Furthermore, weimprove RPN using Faster R-CNN. Here, the multi-classes headwithin Faster R-CNN is changed to a class-agnostic head so that itcan resolve a binary task like RPN. The class-agnostic module withinRPN or modified Faster R-CNN can generalize to novel objects .Besides, we can use a classification-free network such as OLN for the purpose of localizing objects. In this case, we can modify thetraining loss to estimate the objectness of each region purely sincelocalization-related metric tends to be robust to novel objects in theopen world . Object Classification. After localizing object candidates, weleverage a pre-trained large-scale vision-language model (VLM)such as CLIP to classify them. Specifically, We crop and resize theobject candidates, and feed them into the visual encoder of VLM toachieve the corresponding region embeddings. In order to providemore context cues, these region embeddings are ensemble from 1crop (crop the image according to the bounding box) and 1.5 crop (extend the crop size to 1.5 times). We feed the category texts with aset of prompt templates and then feed them into the text encoder ofVLM to obtain the text embeddings. Finally, The cosine similarity iscalculated by the averaged text embeddings and region embeddings,and the per-class NMS is adopted to obtain the final detection results. As this solution decomposes the OVD task into the object lo-calization and classification sub-tasks completely, it can be easilyassembled by different detectors and VLMs, leading to a power-ful performance. In the vanilla method, each cropped object regionneeds to be fixed to the same scale for the VLMs. On the one hand,these operations reduce the efficiency of the model. On the otherhand, the deformation of the regions brings more difficulties to objectdetection, especially for small targets.",
  "Decoupling RPN and ROI head (DRR)": "A two-stage object detector consists of a visual encoder backbone,a region proposal network (RPN), and a region of interest (RoI)head. Here, the RPN is trained end-to-end to generate high-qualityregion proposals and then provided for detection in the ROI head.While the traditional two-stage object detector jointly trains RPNand the ROI head, several works propose to decouple themto avoid the conflict that the sensitivity of the classification head tonovel categories hampers the universality ability of the RPN head.So far, there are no detailed discussions about the effect of whetherto decouple RPN and ROI head or not in existing works. In thispaper, we try to analyze them and hope to provide a view of thebalance of model performance and cost for researchers. As aforementioned, we first introduce a training approach thatdecouples the RPN and ROI head in a two-stage object detector.Following existing works , we use different backbones forlocalization and classification. That is, one backbone is designed forRPN and the other is for ROI head. Given an image, we first use a",
  "Coupling RPN and ROI head (CRR)": "Decoupling proposal generation and ROI head is an efficaciousmethod to keep the universality ability of the proposal generationstage since the proposal generation stage has a class-agnostic classifi-cation, which can be easily extended to novel classes. However, thisdecoupling scheme means that RPN and ROI modules use differentbackbone encoders, which increases the training cost. In the stage ofmodel deployment and inference, this solution will also bring a lotof extra computing time. Considering the computational efficiency,we practice the scheme of sharing the visual backbone and conductdetailed experiments. These experiments can provide researcherswith speed-accuracy trade-offs.",
  "EXPERIMENTS": "Dataset and Metrics. We comprehensively evaluate three funda-mental approaches of OVD task on COCO , LVIS , and ourproposed Product Image Dataset (PID) benchmarks. COCO is astandard dataset comprising 80 categories of common objects ina natural context. It contains 118k images with bounding boxesand instance segmentation annotations. We follow OVR-CNN to split the object categories into 48 base categories and 17 novelcategories. We also follow ViLD for the LVIS dataset to splitthe 337 rare categories into novel categories and the rest commonand frequent categories into base categories. For simplicity, we de-note the open-vocabulary benchmarks based on COCO and LVISas OVD-COCO and OVD-LVIS, respectively. Following ViLD ,the Novel AP50 and AP are the main metric on OVD-COCO andOVD-LVIS, respectively. Our open-vocabulary object detectors aretrained on base classes. Besides, we split PID into the base and novelcategories. The detailed introductions can refer to Sec 5.1.The dataset used for detector pre-training is established from theBigDetection dataset . As the BigDetection dataset has nearlycovered all categories in COCO, we remove the COCO images anddelete both novel and base categories of the COCO dataset from theannotations. Some categories with less than 100 train samples areremoved as well. We also exclude LVIS data from the BigDetectiondataset. Finally, we establish this new BigDetection dataset namedBigDetection* (BD*) which has 489 categories for detector pre-training.Implementation Details of the vanilla method. In the OVD-COCO and OVD-LVIS experiments, We use Faster R-CNN as theobject detector and frozen CLIP as the object classifier. SGD opti-mizer is adopted for the 8 training schedule. We train the detector for 720k iterations and divide the learning rate by 10 at 660k and700k iterations. We also adopt linear warmup for the first 1,000 itera-tions starting from a learning rate of 0 to 0.02. Then, we fine-tune themodel on base categories of OVD-COCO and OVD-LVIS separately.For OVD-COCO, we train 90k iterations and scale down the learningrate at 60k and 80k. For OVD-LVIS, we train 180k iterations andscale down at 120k and 160k. For these two datasets, the learningrate increases to 0.0002 for the first 5k iterations for the warmup. In the PID experiments, we use Faster R-CNN pre-trained onthe BigDetection dataset as the object detector. ProductCLIPwith prompt tuning, which is introduced in Sec 5.3, is applied as theobject classifier. The detectors are fine-tuned with the 1x trainingschedule. A warm-up step with a learning rate of 0.001 is performedfor the first 400 iterations. On both the pre-training and fine-tuningstages, we select 16 samples per GPU with the class-aware sampler.Multi-scale training is adopted with the short edge in the range and the long edge up to 1333. We use 8 A100 GPUS to performthe experiments. Implementation Details of DRR and CRR. In , the de-tection backbone and the CLIP visual backbone are both ResNet50.We adopt ImageNet and RegionCLIP pre-trained param-eters for the detection backbone and the CLIP visual backbone,respectively. During training, we first train the RPN to obtain theproposal bounding boxes on the base categories. We then use aFaster R-CNN with ResNet50-C4 architecture as the detector and1x training schedule (90k iterations). We train the detector using theoffline pre-trained RPN. That is, we decouple the RPN and the ROIhead. We use an SGD optimizer with a learning rate of 0.002 and amin-batch of 16. We set the weight of the background category to0.2 and 0.8 for OVD-COCO and OVD-LVIS, respectively. We usethe same learning rate and input-scale strategy as the vanilla method.We replace the base categories in the classification head with base +novel categories during inference. We use the top-ranked 100 pro-posals at test time for all detectors. For CRR, we keep the samesettings as DRR, except that CRR removes the detection backbone,as shown in . We use the same settings as OVD-COCO when conducting ex-periments on PID except that the pre-trained weight of CLIP visualbackbone. As aforementioned in the vanilla method, we use Product-CLIP as the pre-trained weight on PID.",
  "Comparison with State-of-the-arts": "4.1.1Experiment on OVD-COCO. We compare all three kindsof basic methods with most existing state-of-the-art methods onOVD-COCO. summarizes the results. Although the vanillamethod is flexible to replace the detector and classification model,it achieves comparable results on novel categories but obtains badresults on base categories. One possible reason is that CLIP hasgaps between images and resized images. Moreover, CRR obtainsa higher Novel AP50 than RegionCLIP, but lower than BARON.However, both RegionCLIP and BARON are with extra backbone,which brings more computational complexity. DRR achieves the bestresults and outperforms BARON by 2.7 Novel AP50. These resultsdemonstrate that DRR has the potential to be the best baseline whenselecting approaches for open-vocabulary detection.",
  "Faster R-CNN*29.632.131.1": "4.1.2Experiment on OVD-LVIS. We further conduct experi-ments and compare them with state-of-the-art methods on a largeropen-vocabulary dataset, i.e., OVD-LVIS. We follow the setup ofRegionCLIP and provide the results in . For a fair com-parison with the previous SOTA (i.e., BARON ), we also reportthe ensemble results which require novel class following BARON.DRR achieves 20.1 AP, which is significantly better than Region-CLIP by 3.0 AP. Meanwhile, DRR becomes the new SOTA onOVD-LVIS with the setting of requiring novel class during inference.Similar to OVD-COCO, CRR still leads a competitive result. Instead,the vanilla method obtains bad results compared to other methods,indicating that the operation of crop and resize is non-trivial torecognizing small objects in OVD-LVIS.",
  "Vanilla29.632.131.1": "4.2.1Vanilla. As this two-stage framework isolates the objectlocalization and classification completely, it can be easily assembledby different models. With the help of advanced object detectionmodels and vision-language models, OVD can be improved flexiblyby applying different object localization and object classificationmodules. The influence of object localization. shows the influ-ence of different object localizers. All the experiments use CLIP-ViT-B/32 as the classifier. We attempt several different objectlocalization networks including RPN, Faster R-CNN, and OLN with different training schedules. We evaluate the models on theOVD-COCO dataset. As Faster R-CNN can achieve more accuratebounding boxes than RPN, it leads to a higher AP50 on both noveland base categories. Besides, in order to improve the performance",
  "DRR (Ours)ResNet50x441.957.853.7": "for the novel class, we also evaluate the class-agnostic object local-ization network like OLN. OLN learns generalizable objectness andtends to propose any objects in the image. Compared with the FasterR-CNN, it outperforms 6.7% AP50 on novel categories. To obtainbetter generalization ability, we pre-trained the Faster R-CNN on theBigDetection* dataset. From the fourth row of , we observethat the additional pre-training brings a 3.4% AP50 improvement. The influence of object classification. shows the influ-ence of different classifiers. According to the above experiments, weuse the Faster R-CNN pre-trained on the BigDetection* dataset forobject localization. CLIP models are used to classify croppedregion proposals. Compared with CLIP-ResNet50 and CLIP-ViT-B/32, CLIP-ViT-L/14 shows better performance. This experimentproves that a strong open-vocabulary object classification model canprovide the powerful capability for detecting novel objects. The influence of image embedding ensemble. After localizingobjects, we crop and resize the object regions for the object classifierto compute image embedding. There are limited contextual cues ifcropping directly according to the bounding box localized by theobject detector. We attempt to expand the bounding boxes by 1.5times and ensemble the image embeddings from 1 crop and 1.5crop. We use the Faster R-CNN pre-trained on the BigDetection*dataset for object localization and CLIP-ViT-B/32 for object clas-sification. shows that the image embedding ensemble canimprove the OVD performance efficiently. 4.2.2DRR. The effect of RPN. As illustrated in , wefirst use an offline RPN to obtain proposal bounding boxes and thenextract the corresponding features with the help of the visual encoderof CLIP. In this section, we investigate the effect of different RPN un-der the settings of DRR. We train several RPNs on different datasetsto determine whether a proposal is a foreground. We also replaceRPN with Faster R-CNN with a class-agnostic head to improve the detection performance. Compared to RPN, Faster R-CNN preferto output more accurate bounding boxes with more high-qualityobjectness logits, thus we attempt to multiply the logits with thefinal CLIP scores, called Multiplying RPN score. That is, it can beformulated as 1 2, where 1 and 2 denote the objectness logitsand the CLIP score, respectively. summarizes the results. From , we get the following observations. 1) ReplacingRPN with Faster R-CNN cannot achieve the expected results. Gen-erally, we can improve the overall model performance by improvingthe offline RPN. However, COCO-48-RPN and BD*-489-COCO-48-RPN obtain similar Novel AP50 when without multiplying RPNscore (31.0 vs. 30.5), indicating a better offline RPN does not work aswell. This is because the offline RPN only provides bounding boxesfor the visual backbone of CLIP and does not directly participate inthe loss of classification. 2) The significant objectness logits withina better offline RPN are indeed important for model performance.We find that DRR with multiplying RPN score leads to a NovelAP50 of 35.8 compared to the one without multiplying RPN score,which becomes a new SOTA over ResNet50 on OVD-COCO. The effect of CLIP visual backbones. From , we arguethat the detection-tailored pre-training like RegionCLIP is re-quired when needing the best performance. We then use a largervisual encoder to extract the features of regions of interest. We useResNet50x4 as the backbone and conduct experiments on OVD-COCO. Relying on the observation from , we use BD*-489-COCO-48-RPN as the RPN and multiply the CLIP scores with RPNscores during inference for DRR. In , DRR surpasses the pre-vious state-of-the-art (i.e., RegionCLIP ) by 2.6 AP50 in novelcategories, which benefits from more accurate bounding boxes andmore significant objectness logits. Meanwhile, ResNet50x4 showsbetter results than ResNet50 from Tables 7 & 1. This implies that amodel with a larger capacity is able to obtain better representationsand improve overall performance.",
  "CRR111.6 M13": "4.2.3CRR. Analysis of Computational efficiency. showsthe performance and computational efficiency of the three frame-works. For a fair comparison, we obtain the results of differentmodels under the same environment. We use the Detectron2 1 tool-box based on PyTorch 2 and an A100 GPU to calculate the FPS. Thebatch size (denoted as BS in Table) is set to 1. From , we cansee that the scheme of sharing the visual backbone has the highestcomputational efficiency compared to the other two models, and itis far more accurate than the scheme of CLIP on cropped regions.Sharing the visual backbone is indeed more effective in specificreal-world scenarios.",
  "CRRResNet5027.634.331.0": "bounding boxes in Chinese. PID contains 37,540 images and 52,796annotations. The image size of PID is 800 800. To adapt to thesetting of open vocabulary object detection like OVR-RCNN ,we split PID into a training dataset and a test dataset, and put moreclasses in the test dataset. Here, the training dataset consists of 233classes, 14,802 images, and 20,690 annotations while the test datasetincludes 466 classes, 22,738 images, and 32,106 annotations. Thatis, we use 233 classes as base categories and 233 classes as novelcategories. We provide several examples with annotations in . The product image dataset will be released.",
  "Finetuned results on PID": "We train three approaches on the base categories of PID and thenevaluate them on the base and novel categories. We adopt ResNet50as the visual backbone of CLIP. The results are shown in .Somewhat surprisingly, the vanilla method achieves the best per-formance compared to DRR and CRR, which is inconsistent withthe results on OVD-COCO and OVD-LVIS. One possible reason isthat the objects in PID are large such that cropping and resizing theproposal is an effective way to improve the overall performance.",
  "Ablation study on PID": "Summary of ablation study. Several related works inopen-vocabulary detection have applied a variety of techniques toimprove the model performance. However, it is worth noting thatsome works use a part of these techniques while others do not, whichmakes this line of work confusing. That is, it is hard to use existingtechniques to improve the model performance when transferring toa new dataset, e.g., PID. Here, we investigate the effectiveness ofcommonly used techniques on PID, including different proposal gen-erators, multiplying RPN score, knowledge distillation, and promptlearning. We use DRR to conduct all experiments on PID. Why use DRR to conduct ablation study on PID? From , it seems that the vanilla method is a better choice comparedto CRR and DRR. However, it suffers from slow inference speed and achieves unsatisfactory results on the two popular publicOVD datasets, i.e., OVD-COCO and OVD-LVIS (see Tables 1 & 2& 9). Instead, DRR outperforms the vanilla method and CRR onOVD-COCO and OVD-LVIS and is with excellent computational",
  "efficiency (see ). Since the main goal is to investigate whichtechnique is more effective when transferring to new datasets, weadopt DRR to conduct the ablation study on PID": "The effect of RPNs and the ensemble score. First, we train aCLIP model using image-text product pairs and name it ProductCLIP.Note that the text encoder of ProductCLIP is designed for Chinese.We then train an offline RPN on the base categories of PID forDRR, called PID-233-RPN. Similar to , we investigate theeffectiveness of multiplying RPN score during inference. We alsotrain a BD*-489-PID-233-RPN to further improve the quality ofbounding boxes and objectness logits. From , the model withmultiplying RPN score can achieve better detection performanceover base and novel categories. When improving the performanceof the offline RPN, the Novel AP50 improves from 26.6 to 30.7. Wecan obtain similar observations as in . The effect of knowledge distillation. Knowledge distillation is acommonly used technique to bridge the gap between the object-levelrepresentations from detectors and the image-level representationsfrom CLIP. Thus, they can obtain significant classification scores",
  "What Makes Good Open-Vocabulary Detector: A Disassembling PerspectiveMultimodal KDD 23, August 07, 2023, Long Beach, CA": "from the object-level representations and the embeddings from thetext encoder of CLIP. Following , we use L1 knowledge distilla-tion to check whether this technique is valid for PID. We also adjustdifferent values of temperature in knowledge distillation to find agood hyperparameter. summarizes the results. We observethat DRR with knowledge distillation (KD) achieves slightly betterresults than the one without KD, i.e., 22.6 vs. 22.0. One possiblereason is that we pre-train the visual backbone of CLIP using image-text product pairs, that is, there is no gap between the object-levelrepresentations from the visual backbone with the text encoder. The effect of prompt numbers. In the setup of open-vocabularydetection, we usually obtain the text embeddings by directly feedingthe concepts with human-crafted prompts into the text encoder ofCLIP. Here, We evaluate different number of prompts of the textencoder in . We observe that the effectiveness of the numberof prompts can be negligible on PID. The effect of prompt tuning. Next, we introduce another com-mon technique in prompt modification. We replace the human-crafted prompt with a learned prompt by prompt tuning . summarizes the results. The models with prompt tuning canimprove by 1.0 Novel AP50, indicating that prompt tuning is aneffective technique to improve the model performance.",
  "CONCLUSION AND SOCIAL IMPACTS": "In this paper, we have conducted a comprehensive study of threefundamental commonly used approaches for open-vocabulary de-tection, a vanilla method, decoupling RPN and ROI head (DRR),and coupling RPN and ROI head (CRR). We analyze the advantagesand the disadvantages of the three approaches and give a discussionabout how and when to use them. Extensive experiments demon-strate the effectiveness of different approaches under different setupson COCO and LVIS benchmarks. Besides, we propose a productdataset for object detection called PID and provide a strong baselineon PID. We hope our analyses and datasets promote the developmentof open-vocabulary detection.Acknowledgement. This work was supported in part by the Na-tional Key Research and Development Program of China underGrant 2018AAA0100405. We thank Bang Yang for the help in col-lecting CC3M. We also thank DeXin Wang and Dan Zhou for thehelp in collecting PID. Hanoona Bangalath, Muhammad Maaz, Muhammad Uzair Khattak, Salman HKhan, and Fahad Shahbaz Khan. 2022. Bridging the gap between object andimage-level representations for open-vocabulary detection. Advances in NeuralInformation Processing Systems 35 (2022). Likun Cai, Zhi Zhang, Yi Zhu, Li Zhang, Mu Li, and Xiangyang Xue. 2022. BigDe-tection: A Large-scale Benchmark for Improved Object Detector Pre-training. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-nition. 47774787. Peixian Chen, Kekai Sheng, Mengdan Zhang, Yunhang Shen, Ke Li, and Chun-hua Shen. 2022. Open Vocabulary Object Detection with Proposal Mining andPrediction Equalization. arXiv preprint arXiv:2206.11134 (2022). Yu Du, Fangyun Wei, Zihe Zhang, Miaojing Shi, Yue Gao, and Guoqi Li. 2022.Learning to prompt for open-vocabulary object detection with vision-languagemodel. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition. 1408414093. Chengjian Feng, Yujie Zhong, Zequn Jie, Xiangxiang Chu, Haibing Ren, XiaolinWei, Weidi Xie, and Lin Ma. 2022. Promptdet: Expand your detector vocabularywith uncurated images. In European Conference on Computer Vision.",
  "Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui. 2022. Open-vocabularyobject detection via vision and language knowledge distillation. In InternationalConference on Learning Representations": "Agrim Gupta, Piotr Dollar, and Ross Girshick. 2019. Lvis: A dataset for largevocabulary instance segmentation. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition. 53565364. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 2021. Scaling up visual and vision-language representation learning with noisy text supervision. In InternationalConference on Machine Learning. 49044916. Dahun Kim, Tsung-Yi Lin, Anelia Angelova, In So Kweon, and Weicheng Kuo.2022. Learning open-world object proposals without learning to classify. IEEERobotics and Automation Letters 7, 2 (2022), 54535460. Weicheng Kuo, Yin Cui, Xiuye Gu, AJ Piergiovanni, and Anelia Angelova. 2023.F-VLM: Open-Vocabulary Object Detection upon Frozen Vision and LanguageModels. In International Conference on Learning Representations. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, DevaRamanan, Piotr Dollr, and C Lawrence Zitnick. 2014. Microsoft coco: Commonobjects in context. In European Conference on Computer Vision. 740755. Zongyang Ma, Guan Luo, Jin Gao, Liang Li, Yuxin Chen, Shaoru Wang, CongxuanZhang, and Weiming Hu. 2022. Open-vocabulary one-stage detection with hierar-chical visual-language knowledge distillation. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition. 1407414083. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,et al. 2021. Learning transferable visual models from natural language supervision.In International Conference on Machine Learning. 87488763. Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. 2016. You onlylook once: Unified, real-time object detection. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition. 779788.",
  "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015. Faster r-cnn:Towards real-time object detection with region proposal networks. Advances inNeural Information Processing Systems 28 (2015)": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, SeanMa, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al.2015. Imagenet large scale visual recognition challenge. International Journal ofComputer Vision 115 (2015), 211252. Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. 2018. Con-ceptual captions: A cleaned, hypernymed, image alt-text dataset for automaticimage captioning. In Proceedings of the 56th Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Papers). 25562565. Zhenyu Wang, Yali Li, Xi Chen, Ser-Nam Lim, Antonio Torralba, HengshuangZhao, and Shengjin Wang. 2023. Detecting Everything in the Open World: To-wards Universal Object Detection. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition. Size Wu, Wenwei Zhang, Sheng Jin, Wentao Liu, and Chen Change Loy. 2023.Aligning bag of regions for open-vocabulary object detection. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern Recognition. Xiaoshi Wu, Feng Zhu, Rui Zhao, and Hongsheng Li. 2023. CORA: AdaptingCLIP for Open-Vocabulary Detection with Region Prompting and Anchor Pre-Matching. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition. Chunyu Xie, Heng Cai, Jianfei Song, Jincheng Li, Fanjing Kong, Xiaoyu Wu,Henrique Morimitsu, Lin Yao, Dexin Wang, Dawei Leng, et al. 2022. Zero andR2D2: A Large-scale Chinese Cross-modal Benchmark and A Vision-LanguageFramework. arXiv preprint arXiv:2205.03860 (2022).",
  "An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, andChang Zhou. 2022. Chinese CLIP: Contrastive Vision-Language Pretraining inChinese. arXiv preprint arXiv:2211.01335 (2022)": "Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, and Shih-Fu Chang. 2021.Open-vocabulary object detection using captions. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition. 1439314402. Shiyu Zhao, Zhixing Zhang, Samuel Schulter, Long Zhao, BG Vijay Kumar,Anastasis Stathopoulos, Manmohan Chandraker, and Dimitris N Metaxas. 2022.Exploiting unlabeled data with vision and language models for object detection.In European Conference on Computer Vision. 159175. Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liu-nian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, et al. 2022. Region-clip: Region-based language-image pretraining. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition. 1679316803."
}