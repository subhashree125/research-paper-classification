{
  "ABSTRACT": "Graphs are an essential data structure utilized to represent rela-tionships in real-world scenarios. Prior research has establishedthat Graph Neural Networks (GNNs) deliver impressive outcomesin graph-centric tasks, such as link prediction and node classifica-tion. Despite these advancements, challenges like data sparsity andlimited generalization capabilities continue to persist. Recently,Large Language Models (LLMs) have gained attention in natu-ral language processing. They excel in language comprehensionand summarization. Integrating LLMs with graph learning tech-niques has attracted interest as a way to enhance performance ingraph learning tasks. In this survey, we conduct an in-depth re-view of the latest state-of-the-art LLMs applied in graph learningand introduce a novel taxonomy to categorize existing methodsbased on their framework design. We detail four unique designs:i) GNNs as Prefix, ii) LLMs as Prefix, iii) LLMs-Graphs Integra-tion, and iv) LLMs-Only, highlighting key methodologies withineach category. We explore the strengths and limitations of eachframework, and emphasize potential avenues for future research, in-cluding overcoming current integration challenges between LLMsand graph learning techniques, and venturing into new applica-tion areas. This survey aims to serve as a valuable resource for re-searchers and practitioners eager to leverage large language modelsin graph learning, and to inspire continued progress in this dynamicfield. We consistently maintain the related open-source materialsat",
  "Corresponding author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08.",
  "INTRODUCTION": "Graphs, comprising nodes and edges that signify relationships, areessential for illustrating real-world connections across various do-mains. These include social networks , molecular graphs ,recommender systems , and academic networks . Thisstructured data form is integral in mapping complex interconnec-tions relevant to a wide range of applications.In recent years, Graph Neural Networks (GNNs) have emergedas a powerful tool for a variety of tasks, including node classifica-tion and link prediction . By passing and aggregating infor-mation across nodes and iteratively refining node features throughsupervised learning, GNNs have achieved remarkable results incapturing structural nuances and enhancing model accuracy. Toaccomplish this, GNNs leverage graph labels to guide the learningprocess. Several notable models have been proposed in the litera-ture, each with its own strengths and contributions. For instance,Graph Convolutional Networks (GCNs) have been shown tobe effective in propagating embeddings across nodes, while GraphAttention Networks (GATs) leverage attention mechanisms toperform precise aggregation of node features. Additionally, GraphTransformers employ self-attention and positional encod-ing to capture global signals among the graph, further improvingthe expressiveness of GNNs. To address scalability challenges inlarge graphs, methods such as Nodeformer and DIFFormer have been proposed. These approaches employ efficient attentionmechanisms and differentiable pooling techniques to reduce com-putational complexity while maintaining high levels of accuracy.Despite these advancements, current GNN methodologies still faceseveral challenges. For example, data sparsity remains a signifi-cant issue, particularly in scenarios where the graph structure isincomplete or noisy . Moreover, the generalization ability ofGNNs to new graphs or unseen nodes remains an open researchquestion, with recent works highlighting the need for more robustand adaptive models .",
  "KDD 24, August 2529, 2024, Barcelona, SpainXubin Ren, Jiabin Tang, Dawei Yin, Nitesh Chawla, & Chao Huang": "Seth A Myers, Aneesh Sharma, Pankaj Gupta, and Jimmy Lin. 2014. Informa-tion network or social network? The structure of the Twitter follow graph. InProceedings of the 23rd international conference on world wide web. 493498. Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu.2024. Unifying large language models and knowledge graphs: A roadmap. IEEETransactions on Knowledge and Data Engineering (2024).",
  "Definitions": "Graph-Structured Data. In computer science, a graph G = (V, E)is a non-linear data structure that consists of a set of nodes V, and aset of edges E connecting these nodes. Each edge E is associatedwith a pair of nodes (, ), where and are the endpoints of theedge. The edge may be directed, meaning it has a orientation from to , or undirected, meaning it has no orientation. Furthermore,A Text-Attributed Graph (TAG) is a graph that assigns a sequentialtext feature (i.e., sentence) to each node, denoted as t, which iswidely used in the era of large language models. The text-attributedgraph can be formally represented as G = (V, E, T), where T isthe set of text features.Graph Neural Networks (GNNs) are deep learning architecturesfor graph-structured data that aggregate information from neigh-boring nodes to update node embeddings. Formally, the update of anode embedding h R in each GNN layer can be represented as:",
  "h(+1)= (({h() : N ()}), h() ),(1)": "where N () denotes a neighbor node of , and () and ()are aggregation and update functions, respectively. By stacking GNN layers, the final node embeddings can be used for downstreamgraph-related tasks such as node classification and link prediction.Large Language Models (LLMs). Language Models (LMs) is astatistical model that estimate the probability distribution of wordsfor a given sentence. Recent research has shown that LMs with bil-lions of parameters exhibit superior performance in solving a widerange of natural language tasks (e.g., translation, summarizationand instruction following), making them Large Language Models(LLMs). In general, most recent LLMs are built with transformerblocks that use a query-key-value (QKV)-based attention mecha-nism to aggregate information in the sequence of tokens. Based onthe direction of attention, LLMs can be categorized into two types(given a sequence of tokens x = [0,1, ...,]): Masked Language Modeling (MLM). Masked Language Mod-eling is a popular pre-training objective for LLMs that involvesmasking out certain tokens in a sequence and training the modelto predict the masked tokens based on the surrounding context.Specifically, the model takes into account both the left and rightcontext of the masked token to make accurate predictions:",
  "LARGE LANGUAGE MODELS FOR GRAPHS3.1GNNs as Prefix": "In this section, we discuss the application of graph neural networks(GNNs) as structural encoders to enhance the understanding ofgraph structures by LLMs, thereby benefiting various downstreamtasks, i.e., GNNs as Prefix. In these methods, GNNs generally playthe role of a tokenizer, encoding graph data into a graph tokensequence rich in structural information, which is then input intoLLMs to align with natural language. These methods can gener-ally be divided into two categories: i) Node-level Tokenization: eachnode of the graph structure is input into the LLM, aiming to makethe LLM understand fine-grained node-level structural informa-tion and distinguish relationships. ii) Graph-level Tokenization: thegraph is compressed into a fixed-length token sequence using a spe-cific pooling method, aiming to capture high-level global semanticinformation of the graph structure. 3.1.1Node-level Tokenization. For some downstream tasks ingraph learning, such as node classification and link prediction,the model needs to model the fine-grained structural informationat node level, and distinguish the semantic differences betweendifferent nodes. Traditional GNNs usually encode a unique repre-sentation for each node based on the information of neighboringnodes, and directly perform downstream node classification or link prediction. In this line, the node-level tokenization method is uti-lized, which can retain the unique structural representation of eachnode as much as possible, thereby benefiting downstream tasks.Within this line, GraphGPT proposes to initially alignthe graph encoder with natural language semantics through text-graph grounding, and then combine the trained graph encoderwith the LLM using a projector. Through the two-stage instruc-tion tuning paradigm, the model can directly complete variousgraph learning downstream tasks with natural language, thus per-form strong zero-shot transferability and multi-task compatibil-ity. The proposed Chain-of-Thought distillation method empowersGraphGPT to migrate to complex tasks with small parameter sizes.Then, HiGPT proposes to combine the language-enhancedin-context heterogeneous graph tokenizer with LLMs, solving thechallenge of relation type heterogeneity shift between differentheterogeneous graphs. Meanwhile, the two-stage heterogeneousgraph instruction-tuning injects both homogeneity and heterogene-ity awareness into the LLM. And the Mixture-of-Thought (MoT)method combined with various prompt engineering further solvesthe common data scarcity problem in heterogeneous graph learn-ing. GIMLET , as a unified graph-text model, leverages naturallanguage instructions to address the label insufficiency challengein molecule-related tasks, effectively alleviating the reliance onexpensive lab experiments for data annotation. It employs a gen-eralized position embedding and attention mechanism to encodeboth graph structures and textual instructions as a unified tokencombination that is fed into a transformer decoder. GraphTransla-tor proposes the use of a translator with shared self-attentionto align both the target node and instruction, and employs crossattention to map the node representation encoded by the graphmodel to fixed-length semantic tokens. The proposed daul-phasetraining paradigm empowers the LLM to make predictions basedon language instructions, providing a unified solution for bothpre-defined and open-ended graph-based tasks. Instead of usingpre-computed node features of varying dimensions, UniGraph leverages Text-Attributed Graphs for unifying node representations,featuring a cascaded architecture of language models and graph",
  ": GNNs as Prefix": "neural networks as backbone networks. In recent research on rec-ommendation systems, XRec has been proposed as a methodthat utilizes the encoded user/item embeddings from graph neuralnetworks as collaborative signals. These signals are then integratedinto each layer of large language models, enabling the generationof explanations for recommendations, even in zero-shot scenarios. 3.1.2Graph-level Tokenization. On the other hand, to adaptto other graph-level tasks, models need to be able to extract globalinformation from node representations, to obtain high-level graphsemantic tokens. In the method of GNN as Prefix, Graph-level tok-enization abstracts node representations into unified graph repre-sentations through various \"pooling\" operations, further enhancingvarious downstream tasks.Within this domain, GraphLLM utilizes a graph transformerthat incorporates the learnable query and positional encoding to en-code the graph structure and obtain graph representations throughpooling. These representations are directly used as graph-enhancedprefix for prefix tuning in the LLM, demonstrating remarkable effec-tiveness in fundamental graph reasoning tasks. MolCA withCross-Modal Projector and Uni-Modal Adapter is a method thatenables a language model to understand both text- and graph-basedmolecular contents through the proposed dual-stage pre-trainingand fine-tuning stage. It employs a cross-modal projector imple-mented as a Q-Former to connect a graph encoders representationspace and a language models text space, and a uni-modal adapterfor efficient adaptation to downstream tasks. InstructMol in-troduces a projector that aligns the molecular graph encoded bythe graph encoder with the molecules Sequential information andnatural language instructions, with the first stage of AlignmentPretraining and the second stage of Task-specific Instruction Tun-ing enabling the model to achieve excellent performance in vari-ous drug discovery-related molecular tasks. GIT-Mol furtherunifies the graph, text, and image modalities through interactioncross-attention between different modality encoders, and alignsthese three modalities, enabling the model to simultaneously per-form four downstream tasks: captioning, generation, recognition,and prediction. GNP employs cross-modality pooling to in-tegrate the node representations encoded by the graph encoderwith the natural language tokens, resulting in a unified graph rep-resentation. This representation is aligned with the instructionthrough the LLM to demonstrate superiority in commonsense andbiomedical reasoning tasks. Recently, G-Retriever utilizesretrieval-augmented techniques to obtain subgraph structures. Itcompletes various downstream tasks in GraphQA (Graph QuestionAnswering) through the collaboration of graph encoder and LLMs. 3.1.3Discussion. The GNN as Prefix approach aligns the mod-eling capability of GNNs with the semantic modeling capabilityof LLMs, demonstrating unprecedented generalization, i.e., zero-shot capability, in various graph learning downstream tasks andreal-world applications. However, despite the effectiveness of theaforementioned approach, the challenge lies in whether the GNNas Prefix method remains effective for non-text-attributed graphs.Additionally, the optimal coordination between the architectureand training of GNNs and LLMs remains an unresolved question.",
  "LLMs as Prefix": "G-Prompt Embs. from LLMs for GNNsGeneral GrapharXiv2023SimTeG Embs. from LLMs for GNNsGeneral GrapharXiv2023GALM Embs. from LLMs for GNNsGeneral GraphKDD2023OFA Embs. from LLMs for GNNsGeneral GraphICLR2024TAPE Embs. from LLMs for GNNsGeneral GraphICLR2024LLMRec Embs. from LLMs for GNNsRecommendationWSDM2024OpenGraph Labels from LLMs for GNNsGeneral GrapharXiv2024LLM-GNN Labels from LLMs for GNNsGeneral GraphICLR2024GraphEdit Labels from LLMs for GNNsGeneral GrapharXiv2023RLMRec Labels from LLMs for GNNsRecommendationWWW2024",
  ": LLMs as Prefix": "In the field of recommendation, LLMRec achieves graph aug-mentation on user-item interaction data using GPT-3.5, which notonly filters out noise interactions and adds meaningful trainingdata, but also enriches the initial node embeddings for users anditems with generated rich textual profiles, ultimately improving theperformance of recommenders. 3.2.2Labels from LLMs for GNNs. Another approach leveragesthe generated labels from large language models as supervisionto improve the training of graph neural networks. Notably, thesupervised labels in this context are not limited to categorizedlabels in classification tasks, but can take various forms such asembeddings, graphs, and more. The generated information fromthe LLMs is not used as input to the GNNs, but rather forms thesupervision signals for better optimization, which enables GNNsto achieve higher performance on various graph-related tasks.Follow this line, OpenGraph employs LLMs to generatenodes and edges, mitigating the issue of sparse training data. Thegeneration process for nodes and edges is refined using the Gibbssampling algorithm and a tree-of-prompt strategy, which is thenutilized to train the graph foundation model. LLM-GNN lever-ages LLMs as annotators to generate node category predictionswith confidence scores, which serve as labels. Post-filtering is thenemployed to filter out low-quality annotations while maintaininglabel diversity. Finally, the generated labels are used to train GNNs.GraphEdit leverages the LLMs to build an edge predictor,which is used to evaluate and refine candidate edges against theoriginal graphs edges. In recommender systems, RLMRec leverages LLMs to generate text descriptions of user/item prefer-ences. These descriptions are then encoded as semantic embeddingsto guide the representation learning of ID-based recommendersusing contrastive and generative learning techniques . 3.2.3Discussion. Despite the progress made by the aforemen-tioned methods in enhancing graph learning performance, a lim-itation persists in their decoupled nature, where LLMs are notco-trained with GNNs, resulting in a two-stage learning process.This decoupling is often due to computational resource limitationsarising from the large size of the graph or the extensive parametersof LLMs. Consequently, the performance of the GNNs is heavilydependent on the pre-generated embeddings/labels of LLMs or eventhe design of task-specific prompts.",
  "LLMs-Graphs Integration": "The methods introduced in this section aim to further integratelarge language models with graph data, encompassing variousmethodologies that enhance not only the ability of LLMs to tacklegraph tasks but also the parameter learning of GNNs. These workscan be categorized into three types: i) Fusion Training of GNNsand LLMs, which aims to achieve fusion-co-training of the parame-ters of both models; ii) Alignment between GNNs and LLMs, whichfocuses on achieving representation or task alignment betweenthe two models; and iii) LLMs Agent for Graphs, which builds anautonomous agent based on LLMs to plan and solve graph tasks. 3.3.1Alignment between GNNs and LLMs. In general, GNNsand LLMs are designed to handle different modalities of data, withGNNs focusing on structural data and LLMs focusing on textualdata. This results in different feature spaces for the two models. Toaddress this issue and make both modalities of data more beneficialfor the learning of both GNNs and LLMs, several methods use tech-niques such as contrastive learning or Expectation-Maximization(EM) iterative training to align the feature spaces of the two models.This enables better modeling of both graph and text information,resulting in improved performance on various tasks.Within this topic, MoMu is a multimodal molecular foun-dation model that includes two separate encoders, one for handlingmolecular graphs (GIN) and another for handling text data (BERT).It uses contrastive learning to pre-train the model on a datasetof molecular graph-text pairs. This approach enables MoMu todirectly imagine new molecules from textual descriptions. Alsoin the bioinfo domain, MoleculeSTM combines the chemi-cal structure information of molecules (i.e., molecular graph) withtheir textual descriptions (i.e., SMILES strings), and uses a con-trastive learning to jointly learn the molecular structure and textualdescriptions. It show great performance on multiple benchmarktests, including structure-text retrieval, text-based editing tasks,and molecular property prediction. Similarly, in ConGraT , acontrastive graph-text pretraining technique is proposed to alignthe node embeddings encoded by LMs and GNNs simultaneously.The experiments are conducted on social networks, citation net-works, and link networks, and show great performance on nodeand text classification as well as link prediction tasks. Furthermore,G2P2 enhances graph-grounded contrastive pre-trainingby proposing three different types of alignment: text-node, text-summary, and node-summary alignment. This enables G2P2 toleverage the rich semantic relationships in the graph structure toimprove text classification performance in low-resource environ-ments. GRENADE is a graph-centric language model thatproposes graph-centric contrastive learning and knowledge align-ment to achieve both node-level and neighborhood-level alignmentbased on the node embeddings encoded from GNNs and LMs. Thisenables the model to capture text semantics and graph structureinformation through self-supervised learning, even in the absenceof human-annotated labels. In addition to contrastive learning,THLM leverages BERT and HGNNs to encode node embed-dings and uses a positive-negative classification task with negativesampling to improve the alignment of embeddings from two differ-ent modalities. Recently, GLEM adopts an efficient and effec-tive solution that integrates graph structure and language learning",
  "through a variational expectation-maximization (EM) framework.By iteratively using LMs and GNNs to provide labels for each otherin node classification, GLEM aligns their capabilities in graph tasks": "3.3.2Fusion Training of GNNs and LLMs. Although align-ment between the representations of GNNs and LLMs achievesco-optimization and embedding-level alignment of the two mod-els, they remain separate during inference. To achieve a higherlevel of integration between LLMs and GNNs, several works havefocused on designing a deeper fusion of the architecture of themodules, such as transformer layers in LLMs and graph neural lay-ers in GNNs. Co-training GNNs and LLMs can result in a win-winbi-directional benefit for both modules in graph tasks.Along this line, GreaseLM integrates transformer layers andGNN layers by designing a specific forward propagation layer thatenables bidirectional information passing between LM and GNNthrough special interaction markers and interaction nodes. Thisapproach allows language context representations to be groundedin structured world knowledge, while subtle linguistic differences(such as negation or modifiers) can affect the representation ofthe knowledge graph, which enables GreaseLM to achieve highperformance on Question-Answering tasks. DGTL proposesdisentangled graph learning to leverage GNNs to encode disentan-gled representations, which are then injected into each transformerlayer of the LLMs. This approach enables the LLMs to be aware ofthe graph structure and leverage the gradient from the LLMs to fine-tune the GNNs. By doing so, DGTL achieves high performance onboth citation network and e-commerce graph tasks. ENGINE adds a lightweight and tunable G-Ladder module to each layer ofthe LLM, which uses a message-passing mechanism to integratestructural information. This enables the output of each LLM layer(i.e., token-level representations) to be passed to the correspond-ing G-Ladder, where the node representations are enhanced andthen used for downstream tasks such as node classification. Moredirectly, GraphAdapter uses a fusion module (typically amulti-layer perceptrons) to combine the structural representationsobtained from GNNs with the contextual hidden states of LLMs(e.g., the encoded node text). This enables the structural informationfrom the GNN adapter to complement the textual information fromthe LLMs, resulting in a fused representation that can be used forsupervision training and prompting for downstream tasks.",
  "LLMs Agent for Graphs. With the powerful capabilities ofLLMs in understanding instructions and self-planning to solve tasks,": "an emerging research direction is to build autonomous agents basedon LLMs to tackle human-given or research-related tasks. Typically,an agent consists of a memory module, a perception module, and anaction module to enable a loop of observation, memory recall, andaction for solving given tasks. In the graph domain, LLMs-basedagents can interact directly with graph data to perform tasks suchas node classification and link prediction.In this field, Pangu pioneered the use of LMs to navigateKGs. In this approach, the agent is designed as a symbolic graphsearch algorithm, providing a set of potential search paths for thelanguage models to evaluate in response to a given query. The re-maining path is then utilized to retrieve the answer. Graph Agent(GA) converts graph data into textual descriptions and gen-erates embedding vectors, which are stored in long-term memory.During inference, GA retrieves similar samples from long-termmemory and integrates them into a structured prompt, which isused by LLMs to explain the potential reasons for node classificationor edge connection. FUXI framework integrates customizedtools and the ReAct algorithm to enable LLMs to act as agentsthat can proactively interact with KGs. By leveraging tool-basednavigation and exploration of data, these agents perform chainedreasoning to progressively build answers and ultimately solve com-plex queries efficiently and accurately. Readi is another ap-proach that first uses in-context learning and chain-of-thoughtprompts to generate reasoning paths with multiple constraints,which are then instantiated based on the graph data. The instanti-ated reasoning paths are merged and used as input to LLMs to gen-erate an answer. This method has achieved significant performanceimprovements on KGQA (knowledge graph question answering)and TableQA (table question answering) tasks. Recently, RoG is proposed to answer graph-retaled question in three steps: plan-ning, retrieval, and reasoning. In the planning step, it generatesa set of associated paths based on the structured information ofthe knowledge graph according to the problem. In the retrievalstep, it uses the associated paths generated in the planning stage toretrieve the corresponding reasoning paths from the KG. Finally,it uses the retrieved reasoning paths to generate the answer andexplanation for the problem using LLMs. 3.3.4Discussion. The integration of LLMs and graphs has shownpromising progress in minimizing the modality gap between struc-tured data and textual data for solving graph-related tasks. Bycombining the strengths of LLMs in language understanding andthe ability of graphs to capture complex relationships betweenentities, we can enable more accurate and flexible reasoning overgraph data. However, despite the promising progress, there is stillroom for improvement in this area. One of the main challengesin integrating LLMs and graphs is scalability. In alignment andfusion training, current methods often use small language modelsor fix the parameters of LLMs, which limits their ability to scale tolarger graph datasets. Therefore, it is crucial to explore methodsfor scaling model training with larger models on web-scale graphdata, which can enable more accurate and efficient reasoning overlarge-scale graphs. Another challenge in this area is the limitedinteraction between graph agents and graph data. Current methodsfor graph agents often plan and execute only once, which may notbe optimal for complex tasks requiring multiple runs. Therefore,",
  ": LLMs-Only": "it is necessary to investigate methods for agents to interact withgraph data multiple times, refining their plans and improving theirperformance based on feedback from the graph. This can enablemore sophisticated reasoning over graph data and improve the ac-curacy of downstream tasks. Overall, the integration of LLMs andgraphs is a promising research direction with significant potentialfor advancing the state-of-the-art in graph learning. By address-ing the aforementioned challenges and developing more advancedmethods for integrating LLMs and graphs, we can enable moreaccurate and flexible reasoning over graph data and unlock newapplications in areas such as knowledge graph reasoning, molecularmodeling, and social network analysis.",
  "LLMs-Only": "NLGraph Tuning-freeGraph ReasoningNeurIPS2024GPT4Graph Tuning-freeGraph Reasoning & QAarXiv2023Beyond Text Tuning-freeGeneral GrapharXiv2023Graph-LLM Tuning-freeGeneral GraphKDD Exp. News.2023GraphText Tuning-freeGeneral GrapharXiv2023Talk like a Graph Tuning-freeGraph ReasoningarXiv2023LLM4DyG Tuning-freeDynamic GrapharXiv2023GraphTMI Tuning-freeGeneral GrapharXiv2023Ai et al. Tuning-freeMulti-modal GrapharXiv2023InstructGLM Tuning-requiredGeneral GraphEACL2024WalkLM Tuning-requiredGeneral GraphNeurIPS2024LLaGA Tuning-requiredGeneral GraphICML2024InstructGraph Tuning-requiredGeneral Graph & QA & ReasoningarXiv2024ZeroG Tuning-requiredGeneral GrapharXiv2024GraphWiz Tuning-requiredGraph ReasoningarXiv2024GraphInstruct Tuning-requiredGraph Reasoning & GenerationarXiv2024MuseGraph Tuning-requiredGeneral GrapharXiv2024",
  "LLMs for Multi-modal Graphs": "Recent studies have demonstrated the remarkable ability of largelanguage models to process and understand multi-modal data ,such as images and videos . This capability has opened upnew avenues for integrating LLMs with multi-modal graph data,where nodes may contain features from multiple modalities .By developing multi-modal LLMs that can process such graph data,we can enable more accurate and comprehensive reasoning overgraph structures, taking into account not only textual informationbut also visual, auditory, and other types of data.4.2Efficiency and Less Computational Cost In the current landscape, the substantial computational expensesassociated with both the training and inference phases of LLMspose a significant limitation , impeding their capacity toprocess large-scale graphs that encompass millions of nodes. Thischallenge is further compounded when attempting to integrateLLMs with GNNs, as the fusion of these two powerful modelsbecomes increasingly arduous due to the aforementioned compu-tational constraints . Consequently, the necessity to discover and implement efficient strategies for training LLMs and GNNswith reduced computational costs becomes paramount. This is notonly to alleviate the current limitations but also to pave the way forthe enhanced application of LLMs in graph-related tasks, therebybroadening their utility and impact in the field of data science.4.3Tackling Different Graph Tasks The prevailing methodologies LLMs have primarily centered theirattention on conventional graph-related tasks, such as link predic-tion and node classification. However, considering the remarkablecapabilities of LLMs, it is both logical and promising to delve intotheir potential in tackling more complex and generative tasks, in-cluding but not limited to graph generation , graph understand-ing, and graph-based question answering . By expanding thehorizons of LLM-based approaches to encompass these intricatetasks, we can unlock a myriad of new opportunities for their appli-cation across diverse domains. For instance, in the realm of drugdiscovery, LLMs could facilitate the generation of novel molecularstructures; in social network analysis, they could provide deeper in-sights into intricate relationship patterns; and in knowledge graphconstruction, they could contribute to the creation of more com-prehensive and contextually accurate knowledge bases.",
  "User-Centric Agents on Graphs": "The majority of contemporary LLM-based agents, specifically de-signed to address graph-related tasks, are predominantly tailored forsingle graph tasks. These agents typically adhere to a one-time-runprocedure, aiming to resolve the provided question in a single at-tempt. Consequently, these agents are neither equipped to functionas multi-run interactive agents, capable of adjusting their generatedplans based on feedback or additional information, nor are theydesigned to be user-friendly agents that can effectively manage awide array of user-given questions. An LLM-based agent thatembodies the ideal qualities should not only be user-friendly butalso possess the capability to dynamically search for answers withingraph data in response to a diverse range of open-ended questionsposed by users. This would necessitate the development of an agentthat is both adaptable and robust, able to engage in iterative interac-tions with users and adept at navigating the complexities of graphdata to provide accurate and relevant answers.",
  "CONCLUSION": "In this comprehensive survey, we delve into the current state oflarge language models specifically tailored for graph data, propos-ing an innovative taxonomy grounded in the distinctive designsof their inference frameworks. We meticulously categorize thesemodels into four unique framework designs, each characterized byits own set of advantages and limitations. Additionally, we providea detailed discussion on these characteristics, enriching our analysiswith insights into potential challenges and opportunities withinthis field. Our survey not only serves as a critical resource for re-searchers keen on exploring and leveraging large language modelsfor graph-related tasks but also aims to inspire and guide futureresearch endeavors in this evolving domain. Through this work,we hope to foster a deeper understanding and stimulate furtherinnovation in the integration of LLMs with graphs.",
  "William Brannon et al. 2023. Congrat: Self-supervised contrastive pretrainingfor joint graph and text embeddings. arXiv preprint arXiv:2305.14321 (2023)": "He Cao, Zijing Liu, Xingyu Lu, Yuan Yao, and Yu Li. 2023. Instructmol: Multi-modal integration for building a versatile and reliable molecular assistant indrug discovery. arXiv preprint arXiv:2311.16208 (2023). Ziwei Chai, Tianjie Zhang, Liang Wu, Kaiqiao Han, Xiaohai Hu, XuanwenHuang, and Yang Yang. 2023. Graphllm: Boosting graph reasoning ability oflarge language model. arXiv preprint arXiv:2310.05845 (2023).",
  "Runjin Chen, Tong Zhao, Ajay Jaiswal, Neil Shah, and Zhangyang Wang. 2024.LLaGA: Large Language and Graph Assistant. arXiv preprint arXiv:2402.08170(2024)": "Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei,Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, et al. 2024. Exploring thepotential of large language models (llms) in learning on graphs. ACM SIGKDDExplorations Newsletter 25, 2 (2024), 4261. Zhikai Chen, Haitao Mao, Hongzhi Wen, Haoyu Han, Wei Jin, Haiyang Zhang,Hui Liu, and Jiliang Tang. 2023. Label-free node classification on graphs withlarge language models (llms). arXiv preprint arXiv:2310.04668 (2023). Sitao Cheng, Ziyuan Zhuang, Yong Xu, Fangkai Yang, Chaoyun Zhang, XiaotingQin, Xiang Huang, Ling Chen, Qingwei Lin, Dongmei Zhang, et al. 2024. Call MeWhen Necessary: LLMs can Efficiently and Faithfully Reason over StructuredEnvironments. arXiv preprint arXiv:2403.08593 (2024). Debarati Das, Ishaan Gupta, Jaideep Srivastava, and Dongyeop Kang. 2023.Which Modality should I useText, Motif, or Image?: Understanding Graphswith Large Language Models. arXiv preprint arXiv:2311.09862 (2023).",
  "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:Pre-training of deep bidirectional transformers for language understanding.arXiv preprint arXiv:1810.04805 (2018)": "Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su,Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et al. 2023. Parameter-efficient fine-tuning of large-scale pre-trained language models. Nature MachineIntelligence 5, 3 (2023), 220235. Keyu Duan, Qian Liu, Tat-Seng Chua, Shuicheng Yan, Wei Tsang Ooi, QizheXie, and Junxian He. 2023. Simteg: A frustratingly simple approach improvestextual graph learning. arXiv preprint arXiv:2308.02565 (2023).",
  "Jiayan Guo, Lun Du, and Hengyu Liu. 2023. Gpt4graph: Can large languagemodels understand graph structured data? an empirical evaluation and bench-marking. arXiv preprint arXiv:2305.15066 (2023)": "Zirui Guo, Lianghao Xia, Yanhua Yu, Yuling Wang, Zixuan Yang, Wei Wei, LiangPang, Tat-Seng Chua, and Chao Huang. 2024. GraphEdit: Large LanguageModels for Graph Structure Learning. arXiv preprint arXiv:2402.15183 (2024). Xiaoxin He, Xavier Bresson, Thomas Laurent, Adam Perold, Yann LeCun, andBryan Hooi. 2023. Harnessing explanations: Llm-to-lm interpreter for enhancedtext-attributed graph representation learning. In The Twelfth International Con-ference on Learning Representations. Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and MengWang. 2020. Lightgcn: Simplifying and powering graph convolution network forrecommendation. In Proceedings of the 43rd International ACM SIGIR conferenceon research and development in Information Retrieval. 639648. Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V Chawla, Thomas Laurent, Yann Le-Cun, Xavier Bresson, and Bryan Hooi. 2024. G-Retriever: Retrieval-AugmentedGeneration for Textual Graph Understanding and Question Answering. arXivpreprint arXiv:2402.07630 (2024).",
  "Yufei He and Bryan Hooi. 2024. UniGraph: Learning a Cross-Domain GraphFoundation Model From Natural Language. arXiv preprint arXiv:2402.13630(2024)": "Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley,and Wayne Xin Zhao. 2024. Large language models are zero-shot rankersfor recommender systems. In European Conference on Information Retrieval.Springer, 364381. Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, BowenLiu, Michele Catasta, and Jure Leskovec. 2020. Open graph benchmark: Datasetsfor machine learning on graphs. Advances in neural information processingsystems 33 (2020), 2211822133.",
  "Yuntong Hu, Zheng Zhang, and Liang Zhao. 2023. Beyond Text: A Deep Diveinto Large Language Models Ability on Understanding Graph Data. arXivpreprint arXiv:2310.04944 (2023)": "Chao Huang, Xubin Ren, Jiabin Tang, Dawei Yin, and Nitesh Chawla. 2024.Large Language Models for Graphs: Progresses and Directions. In CompanionProceedings of the ACM on Web Conference 2024. 12841287. Xuanwen Huang, Kaiqiao Han, Dezheng Bao, Quanjin Tao, Zhisheng Zhang,Yang Yang, and Qi Zhu. 2023. Prompt-based node feature extractor for few-shotlearning on text-attributed graphs. arXiv preprint arXiv:2309.02848 (2023).",
  "Xinyu Lin, Wenjie Wang, Yongqi Li, Shuo Yang, Fuli Feng, Yinwei Wei, and Tat-Seng Chua. 2024. Data-efficient Fine-tuning for LLM-based Recommendation.arXiv preprint arXiv:2401.17197 (2024)": "Hao Liu, Jiarui Feng, Lecheng Kong, Ningyue Liang, Dacheng Tao, Yixin Chen,and Muhan Zhang. 2023. One for all: Towards training one graph model for allclassification tasks. arXiv preprint arXiv:2310.00149. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2024. Improved base-lines with visual instruction tuning. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition. 2629626306.",
  "Yijian Qin, Xin Wang, Ziwei Zhang, and Wenwu Zhu. 2023. Disentangledrepresentation learning with large language models for text-attributed graphs.arXiv preprint arXiv:2310.18152 (2023)": "Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, StefanoErmon, and Chelsea Finn. 2024. Direct preference optimization: Your languagemodel is secretly a reward model. Advances in Neural Information ProcessingSystems 36 (2024). Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limitsof transfer learning with a unified text-to-text transformer. Journal of machinelearning research 21, 140 (2020), 167.",
  "Xubin Ren, Wei Wei, Lianghao Xia, and Chao Huang. 2024. A Comprehen-sive Survey on Self-Supervised Learning for Recommendation. arXiv preprintarXiv:2404.03354 (2024)": "Xubin Ren, Wei Wei, Lianghao Xia, Lixin Su, Suqi Cheng, Junfeng Wang, DaweiYin, and Chao Huang. 2024. Representation learning with large language modelsfor recommendation. In Proceedings of the ACM on Web Conference 2024. 34643475. Xubin Ren, Lianghao Xia, Jiashu Zhao, Dawei Yin, and Chao Huang. 2023. Disen-tangled contrastive collaborative filtering. In Proceedings of the 46th InternationalACM SIGIR Conference on Research and Development in Information Retrieval.11371146.",
  "Bing Su, Dazhao Du, Zhao Yang, et al. 2022. A molecular multimodal founda-tion model associating molecule graphs with natural language. arXiv preprintarXiv:2209.05481 (2022)": "Yanchao Tan, Hang Lv, Xinyi Huang, Jiawei Zhang, Shiping Wang, and CarlYang. 2024. MuseGraph: Graph-oriented Instruction Tuning of Large LanguageModels for Generic Graph Mining. arXiv preprint arXiv:2403.04780 (2024). Yanchao Tan, Zihao Zhou, Hang Lv, Weiming Liu, and Carl Yang. 2024. Walklm:A uniform language model fine-tuning framework for attributed graph embed-ding. Advances in Neural Information Processing Systems 36 (2024).",
  "Qinyong Wang, Zhenxiang Gao, and Rong Xu. 2023. Graph Agent: ExplicitReasoning Agent for Graphs. arXiv preprint arXiv:2310.16421 (2023)": "Yizhong Wang, Hamish Ivison, Pradeep Dasigi, et al. 2023. How far can camelsgo? exploring the state of instruction tuning on open resources. Advances inNeural Information Processing Systems 36 (2023), 7476474786. Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, JunfengWang, Dawei Yin, and Chao Huang. 2024. Llmrec: Large language modelswith graph augmentation for recommendation. In Proceedings of the 17th ACMInternational Conference on Web Search and Data Mining. 806815. Zhihao Wen and Yuan Fang. 2023. Augmenting low-resource text classificationwith graph-grounded pre-training and prompting. In Proceedings of the 46thInternational ACM SIGIR Conference on Research and Development in InformationRetrieval. 506516.",
  "Zhihao Wen and Yuan Fang. 2023. Prompt tuning on graph-augmented low-resource text classification. arXiv preprint arXiv:2307.10230 (2023)": "Qitian Wu, Chenxiao Yang, Wentao Zhao, Yixuan He, David Wipf, and JunchiYan. 2023. Difformer: Scalable (graph) transformers induced by energy con-strained diffusion. arXiv preprint arXiv:2301.09474 (2023). Qitian Wu, Wentao Zhao, Zenan Li, David P Wipf, and Junchi Yan. 2022. Node-former: A scalable graph structure learning transformer for node classification.Advances in Neural Information Processing Systems 35 (2022), 2738727401.",
  "Zeyang Zhang, Xin Wang, Ziwei Zhang, Haoyang Li, Yijian Qin, Simin Wu, andWenwu Zhu. 2023. LLM4DyG: Can Large Language Models Solve Problems onDynamic Graphs? arXiv preprint arXiv:2310.17110 (2023)": "Haiteng Zhao, Shengchao Liu, Ma Chang, Hannan Xu, Jie Fu, Zhihong Deng,Lingpeng Kong, and Qi Liu. 2024. Gimlet: A unified graph-text model forinstruction-based molecule zero-shot learning. Advances in Neural InformationProcessing Systems 36 (2024). Jianan Zhao, Hesham Mostafa, Michael Galkin, Michael Bronstein, ZhaochengZhu, and Jian Tang. 2024. GraphAny: A Foundation Model for Node Classifica-tion on Any Graph. arXiv preprint arXiv:2405.20445 (2024). Jianan Zhao, Meng Qu, Chaozhuo Li, Hao Yan, Qian Liu, Rui Li, Xing Xie, andJian Tang. 2022. Learning on large-scale text-attributed graphs via variationalinference. arXiv preprint arXiv:2210.14709 (2022).",
  "LLMs-Graphs Interaction": "MoMu Alignment between GNNs and LLMsBioinformaticsarXiv2022ConGraT Alignment between GNNs and LLMsGeneral GrapharXiv2023G2P2 Alignment between GNNs and LLMsGeneral GraphSIGIR2023GRENADE Alignment between GNNs and LLMsGeneral GraphEMNLP2023MoleculeSTM Alignment between GNNs and LLMsBioinformaticsNature MI2023THLM Alignment between GNNs and LLMsHeterogeneous GraphEMNLP2023GLEM Alignment between GNNs and LLMsGeneral GraphICLR2023GreaseLM Fusion Training of GNNs and LLMsGraph-based QAICLR2022DGTL Fusion Training of GNNs and LLMsGeneral GrapharXiv2023ENGINE Fusion Training of GNNs and LLMsGeneral GrapharXiv2024GraphAdapter Fusion Training of GNNs and LLMsGeneral GraphWWW2024Pangu LLMs Agent for GraphsGraph-based QAACL2023Graph Agent LLMs Agent for GraphsGeneral GrapharXiv2023FUXI LLMs Agent for GraphsGraph-based QAarXiv2024Readi LLMs Agent for GraphsGraph-based QAarXiv2024RoG LLMs Agent for GraphsGraph-based QAICLR2024"
}