{
  "ABSTRACT": "Graph clustering, a fundamental and challenging task in graph min-ing, aims to classify nodes in a graph into several disjoint clusters.In recent years, graph contrastive learning (GCL) has emerged asa dominant line of research in graph clustering and advances thenew state-of-the-art. However, GCL-based methods heavily rely ongraph augmentations and contrastive schemes, which may poten-tially introduce challenges such as semantic drift and scalabilityissues. Another promising line of research involves the adoptionof modularity maximization, a popular and effective measure forcommunity detection, as the guiding principle for clustering tasks.Despite the recent progress, the underlying mechanism of modu-larity maximization is still not well understood. In this work, wedig into the hidden success of modularity maximization for graphclustering. Our analysis reveals the strong connections betweenmodularity maximization and graph contrastive learning, wherepositive and negative examples are naturally defined by modular-ity. In light of our results, we propose a community-aware graphclustering framework, coined Magi, which leverages modularitymaximization as a contrastive pretext task to effectively uncover theunderlying information of communities in graphs, while avoidingthe problem of semantic drift. Extensive experiments on multiplegraph datasets verify the effectiveness of Magi in terms of scalabil-ity and clustering performance compared to state-of-the-art graphclustering methods. Notably, Magi easily scales a sufficiently largegraph with 100M nodes while outperforming strong baselines.",
  "Both authors contributed equally to this research.Corresponding author": "Permission to make digital or hard copies of part or all of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for third-party components of this work must be honored.For all other uses, contact the owner/author(s).KDD 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s).ACM ISBN 979-8-4007-0490-1/24/08.",
  "Graph clustering, graph contrastive learning, modularity maximiza-tion": "ACM Reference Format:Yunfei Liu, Jintang Li, Yuehe Chen, Ruofan Wu, Ericbk Wang, Jing Zhou,Sheng Tian, Shuheng Shen, Xing Fu, Changhua Meng, Weiqiang Wang,and Liang Chen. 2024. Revisiting Modularity Maximization for Graph Clus-tering: A Contrastive Learning Perspective. In Proceedings of the 30th ACMSIGKDD Conference on Knowledge Discovery and Data Mining (KDD 24),August 2529, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.",
  "INTRODUCTION": "Graph clustering is a fundamental problem in graph analysis, crucialfor uncovering structures and relationships between nodes in agraph. The primary objective of graph clustering is to group orpartition the nodes in a graph into clusters or communities basedon their structural properties or connectivity patterns. So far, graphclustering has been widely studied and extensively applied acrossvarious domains, including social network analysis , imagesegmentation and recommendation systems .As a longstanding field of research, graph clustering continues toevolve through the development of novel algorithms and techniques.Recently, graph neural networks (GNNs) have emerged as the defacto deep learning architecture for dealing with graph data .In light of the learning capability of GNNs on graph-structured data,researchers have shifted their attention to exploring GNN-based ap-proaches for graph clustering. Typically, GNNs are employed as en-coders to learn node representations, which are often accomplishedwith an auxiliary task to help uncover the underlying patterns forclustering. As the graph clustering task is commonly approached in",
  ": An illustrative overview of how positive and nega-tive examples of a query node are guided by the modularitymeasure": "an unsupervised manner due to the absence of labeled annotations,this has motivated the exploration of self-supervised graph learningmethods for graph clustering. Contrastive learning , whichaims to learn representations that bring similar instances closer inthe representation space and distances dissimilar ones, has beenproven to learn cluster-preserving representations . Therefore,recently, graph contrastive learning methods (GCLs) have made significant breakthroughs in graph clustering and grad-ually become the mainstream approach for graph clustering.Inspired by , early GCLs adopted instance discrimi-nation as a pretext task, aiming to learn representations that areinvariant to different augmented views of the graph. Recently, someworks have proposed augmentation-free GCLs , whichextract contrastive information from the graph data itself and con-struct corresponding pseudo-labels. Several works useK-means and K-Nearest Neighbors (KNN) to mine the potentialcluster within graph features, and utilize the clustering results asfoundation for positive/negative sample pairs. Although the GCLsmentioned above have achieved significant success in the graphclustering task, we note that these methods still suffer from at leastone of the following challenges:(C1) Scalability. Mainstream GCLs typically rely on data aug-mentation to create multiple views and employ multiple encoders toobtain corresponding representations. However, this poses compu-tational challenges and scalability issues when applied to large-scalegraphs. On the other hand, feature-based augmentation-free GCLsrequire the use of high-cost algorithms such as K-means and KNNto construct pseudo-labels, which also limits their capability toscale to large-scale datasets.(C2) Semantic drift. Pretext tasks play an important role inenabling GCL to better adapt to downstream tasks. A typical ap-proach is to define instance-wise augmentations and then posethe problem as that of learning to closely align these augmenta-tions with the original, while keeping them separate from others.However, this type of pretext task neglects the inherent structure ofgraph data, which can result in semantic drift during downstreamclustering tasks . Augmentation-free GCLs can mitigate this bymining the information inherently carried by graph data. However,as previously mentioned, methods based on K-means and KNNlack scalability, and using simple random walks to mine positivesamples within a nodes neighborhood can easily lead to a com-munity semantic drift, especially when a node is situated at theedge of a community. Recently, methods based on neural modularity maximiza-tion have made new progress in graph clustering tasks . Byusing a single GNN encoder to encode the relaxed community as-signment matrix, these methods effectively combine the modularitymaximization objective with GNNs and achieve state-of-the-art per-formance in graph clustering tasks. The modularity maximizationobjective can effectively perceive the potential community struc-ture within networks and provide guidance for representationlearning. However, the design of the modularity function is heuris-tic, and the underlying reasons for its success as an optimizationobjective remain largely unexplored.In this work, we provide an in-depth analysis of modularitymaximization and bridge the gap between modularity maximiza-tion and GCL. Our analysis shows that modularity maximizationis essentially graph contrastive learning, where the positive andnegative examples are naturally guided by the modularity matrix(see ). Based on our findings, we attempt to integrate thelatest developments in the fields of neural modularity maximizationand graph contrastive learning, and propose a community-awaregraph clustering framework, coined Magi. Magi can mitigate theeffects of semantic drift by perceiving the underlying communitystructures in the graph, and since it doesnt rely on data augmen-tation, it can easily scale to a sufficiently large graph with 100Mnodes. Our contributions can be summarized as follows: Modularity maximization = contrastive learning. Weestablish the connection between modularity maximizationand graph contrastive learning. Our findings reveal that mod-ularity maximization can be viewed as leveraging potentialcommunity information in graphs for contrastive learning. Community-aware pretext task and scalable frame-work. We propose Magi, a community-aware graph con-trastive learning framework that uses modularity maximiza-tion as its pretext task. Magi avoids semantic drift by lever-aging underlying community structures and eliminates theneed for graph augmentation. Magi incorporates a two-stagerandom walk approach to perform modularity maximizationpretext tasks in mini-batch form, thereby achieving goodscalability. Experimental results. We conduct extensive experimentson 8 real-world graph datasets with different scales. Magihas consistently outperformed several state-of-the-arts inthe task of graph clustering. Notably, Magi easily scales toan industrial-scale graph with 100M nodes, showcasing itsscalability and effectiveness in large-scale scenarios.",
  "RELATED WORK2.1Graph clustering": "Graph clustering is a widely studied problem in academia and indus-try. Classical clustering methods involve either solving an optimiza-tion problem or using some heuristic, non-parametric approaches.Prominent examples include K-means , spectral clustering ,and Louvain . However, the shallow architecture of these meth-ods limits their performance. With the rise of deep neural networksin graph representation learning, random walk-based methods suchas DeepWalk and Node2vec have also been introduced foraddressing clustering tasks.",
  "Revisiting Modularity Maximization for Graph Clustering: A Contrastive Learning PerspectiveKDD 24, August 2529, 2024, Barcelona, Spain": "Early works typically focus on single dimension form graphs,e.g., graph structure or node attributes. Thanks to the great abil-ity of graph neural networks (GNNs) in learning jointlystructure and attribute information, graph autoencoders stand outas an emerging approach for unsupervised graph learning tasks.For example, GAE and VGAE learn to reconstruct the graphstructure as the self-supervised learning task using GNNs. Follow-up works extend GAE by employing Laplacian sharpening ,Laplacian smoothing , generative adversarial learning , andmasked autoencoding . However, self-supervised learning tasksor pretext tasks in GAEs are not aligned with downstream taskssuch as graph clustering. As a result, the learned representationsmay not effectively capture the relevant information for clusteringtasks.",
  "Graph contrastive learning": "Over the past few years, graph contrastive learning (GCL) hasemerged as a powerful technique for learning representations ofgraph-structured data. Deep Graph Infomax (DGI) follows theapproach of mutual information-based learning, as proposed byadapted . GRACE maximizes the agreement of node repre-sentations between two corrupted views of a graph. GraphCL incorporates four graph augmentation techniques to learn unsuper-vised graph representations through a unified contrastive learningframework. One step further, MVGRL introduces node diffusionand contrasts node-level embeddings with representations of aug-mented graphs. Following the BYOL , BGRL eliminates theneed for negative sampling by minimizing an invariance-based lossfor augmented graphs within a batch. In light of the success of GCL,there have been attempts to apply GCL techniques to graph clus-tering and achieve promising results . However, mostmethods employ classical instance discrimination as the pretexttask, which requires sophisticated graph augmentation techniquesto obtain meaningful view representations. As a result, they maysuffer from challenges such as semantic drift and limitations inscaling up to handle large graphs.",
  "Neural modularity maximization": "As one of the most commonly used metrics for community detec-tion, modularity measures the quality of a partition in a networkby evaluating the density of connections within communities com-pared to random connections . Maximizing the modularitydirectly is proven to be NP-hard . As a result, heuristics suchas spectral relaxation and greedy algorithms have beendeveloped to approximate the modularity and find suboptimal butreasonable community partitions. However, previous works focussimply on the graph structure, while ignoring the abundant infor-mation associated with the nodes (e.g., attributes). With the adventof graph neural networks, combining them with modularity maxi-mization has become a promising research direction. utilizesan autoencoder to encode the community assignment matrix andcombines it with a reconstruction loss to train the autoencoder. and extend this idea by employing (variational)graph autoencoders. explored the benefits brought by high-order proximity and used high-order polynomials of the adjacency matrix to calculate the high-order modularity matrix. con-structs two loss functions based on modularity, corresponding tosingle and multi-attribute networks, while DMoN introducescollapse regularization to prevent the community allocation matrixfrom falling into spurious local minima. Despite the recent progress,the underlying mechanism of neural modularity maximization forgraph clustering is still not well understood.",
  "Problem statement and notations": "Given an attribute graph G = (V, E, X), where V = {1, 2, ..., }denotes a set of nodes and = |V|; E VV denotes correspond-ing edges between nodes, where each node V is associatedwith a -dimensional feature vector R. Let X R denote the node attribute matrix and A R the adjacencymatrix, respectively. Given the graph G and node attributes X, thegoal of graph clustering is to partition the nodes set V into parti-tions {V1, V2, V3, . . . , V} such that nodes in the same cluster aresimilar/close to each other in terms of the graph structure as wellas in terms of attributes.",
  ")(,),(1)": "where = A, is the degree of node , = |E| is the total num-ber of edges in the graph, and is the community to which node is assigned. (,) = 1 is an indicator function, i.e., (,) = 1 if = and 0 otherwise, suggesting whether nodes and belong",
  "to the same community. Essentially, A, and": "2 can be regardas the observed and expected number of edges between nodes and . Since a larger () leads to a better community partition,modularity maximization has become a popular approach in findinggood community division .Typically, modularity maximization can be viewed as a con-strained optimization objective in the following form:",
  "(3)": "where M+ = {(, )|B, > 0} and M = {(, )|B, 0} arepositive and negative pairs, respectively. In this regard, modular-ity maximization is similar in form to graph autoencoders, withthe goal of optimization is to reconstruct the modularity matrix Brather than adjacency matrix A. Motivated by a recent work that showed the equivalence between graph autoencoders and con-trastive learning, we provide our intuition to explicitly relate mod-ularity maximization to contrastive learning.Here we further unify Eq. (3) into a general form from the per-spective of optimization. Since directly maximizing Eq. (3) is in-tractable, we can now relax P {0, 1} to its continuous analogZ R , where Z is node representations learned from a graphencoder such that Z = (G).",
  "(4)": "where (, ) is the decoder network that decodes the pairwise noderepresentations (,) into the modularity score. Typically, thedecoder can be simply defined by cosine similarity function, i.e.,(,) = or a neural network. Community coefficient B, isabsorbed into and and should be adjusted by the neural networkduring training. According to Eq. (4), we obtain a general form ofgraph contrastive learning, where positive and negative pairs areguided by the modularity coefficient B,.",
  "Opportunity: community-aware pretext task": "As discussed in , current GCL methods for graph cluster-ing, whether augmentation-based or augmentation-free, may facechallenges related to scalability and/or semantic drift. Therefore,an alternative yet promising way is to utilize modularity maximiza-tion as a pretext task to generate community-aware pseudo-labelsthat guide contrastive learning. Specifically, the employment of themodularity matrix allows us to reveal the underlying communitystructure within the graph. In this regard, positive sample pairs arenaturally defined as connected node pairs within the same com-munity, while negative sample pairs are unconnected node pairs from different communities. This leverages the intrinsic communitystructure in the network to mitigate the impact of semantic driftwhile also eliminating the need for graph augmentations that mayhinder its scalability. Moreover, by establishing the relationshipbetween modularity maximization and contrastive learning, we areable to integrate the latest advancements in modularity maximiza-tion into research on graph contrastive learning. Recent advancesin the field of modularity maximization mainly include: (1) neu-ral modularity maximization ; (2) high-order proximity inmodularity ; (3) Better parallel optimization algorithms ,etc. The ones that are closely related to contrastive learning are (1)and (2). In this work, we propose to obtain cluster assignment viaGNN encoder , which allows (soft) cluster assignment to bedifferentiable for neural modularity maximization. In addition, wefully consider high-order proximity in modularity, and propose asampling method based on random walk to ensure scalability whilecapturing high-order proximity within the community.",
  "GNN encoder": "An encoder in GCL is a crucial component that maps the inputgraph data into latent representations. In Magi, we employ differ-ent GNNs as encoders to encode graphs of varying sizes. Here weconsider only two representative GNNs, i.e., GCN and Graph-SAGE . It is important to note that Magi is not specific to anyparticular GNN model and can be applied with other architectures(e.g., GAT ) as well.GCN. GCN is a popular GNN model that has been widely used forvarious graph-based tasks. GCN operates by aggregating informa-tion from neighboring nodes and propagating it through multiplelayers. It leverages the spectral graph convolution operation, whichis based on the graph Laplacian matrix, to capture local and globalgraph structure. Formally, the message propagation of -th layer inGCN is defined as:",
  "(|N| + 1) (|N| + 1),(5)": "where LeakyReLU() = (0, ) + (0, ) is activation func-tion. N is the set of adjacent neighbors of node. W is the learnableweight of layer and () is the latent representation with (0) = .GraphSAGE. GCN relies on a symmetric and fixed aggregationfunction based on the graph Laplacian, which can limit its ability tocapture diverse neighborhood structures and scale to large graphseffectively. GraphSAGE is a GNN alternative designed to betterhandle large-scale graphs in an inductive manner:",
  "Modularity maximization pretext task": "The core insight behind neural modularity maximization is thecomputation of the modularity matrix B. In order to adapt theminibatch training to obtain good scalability, an intuitive idea isto randomly sample nodes in the graph and obtain contrastivepairs from their corresponding sub-modularity matrix. However, itis important to note that this approach potentially introduces thefollowing issues: Structure bias. The sampled subgraph may not necessarilyshare a similar structure with the overall graph, which canhinder the performance of contrastive learning. Nevertheless,this issue becomes more serious as the graph becomes largerand sparser. Lack of high-order proximity. The vanilla modularitymatrix only focuses on the first-order proximity within thegraph, which contradicts our intuitive understanding of com-munity structure in networks, that is, two nodes often be-long to the same community due to high-order proximity,such as having many common neighbors. Although a re-cent work explored the benefits brought by high-orderproximity and used high-order polynomials of the adjacencymatrix A to calculate the high-order modularity matrix. How-ever, the cost of computing high-order polynomials of theadjacency matrix A for large-scale graphs is still prohibi-tive. We discuss the benefits of incorporating high-orderproximity through experimental investigation in .2. In this work, we propose a two-stage random walk-based sam-pling method to address the aforementioned challenges, as by ad-justing the depth of the random walks, we can effectively capturethe high-order proximity within the network. Specifically, we em-ploy a first-stage random walk to sample multiple sub-communitiesand merge them into a training batch, ensuring that the corre-sponding subgraph within the batch contains effective community structures to guide the model. Subsequently, we perform a second-stage random walk to generate a similarity matrix between nodeswithin the batch, and we draw on the concept of the configurationmodel to calculate the corresponding modularity matrix in themini-batch form.Next, we will provide a detailed description of these two stages,which we refer to as S1 and S2, respectively.(S1) Sampling multiple sub-communities. For any in setN consisting of randomly selected root nodes in the graph, basedon the principle of internal connectivity within communities, theremust be an overlap between the neighborhood of node and itspotential community, denoted as O. Accurately selecting O canbe an expensive task because it involves detecting potential com-munity structures in the graph. However, based on the definitionthat a community is a set of nodes is densely connected internally, weutilize random walks to approximate O. To be specific, we initiate random walks of depth rooted at node and denote the set ofvisited nodes as U1 , with the corresponding visit count recordedas 1 , which each element 1, as the number of visits from to .We filter O in the following way:",
  "|U1 |}(7)": "In brief, we consider nodes with visits greater than the mean asO. Then, for nodes in N, we perform the same operation and setB = NO as a training batch.(S2) Mini-batch modularity matrix. It is intuitive to treatpair nodes from the same sub-community as positives and thosefrom different sub-communities as negatives. However, we mustconsider the potential overlaps between different sub-community,necessitating further exploration of the relationships among nodesin B. Specifically, we perform again random walks of depth foreach node in B. For any two nodes and in B, let 2, be thenumber of visits from to , we can then construct a similaritymatrix S for the nodes in B based on the number of visits, whicheach element:",
  "log ( /)M+ ( /) + M ( /)(11)": "The softmax-based SimCLR contrastive loss is a hardness-awareloss function , where is temperature and plays a key rolein controlling the local separation and global uniformity of therepresentation distributions. Appropriate temperature selectioncan effectively alleviate the uniform-tolerance dilemma. Based onthe above analysis, we use the SimCLR loss function defined inEq.(11) to learn the encoder.",
  "Complexity analysis": "The space complexity of our algorithm is O( + +2), where and are the number of nodes and edges in the graph, respectively. is the attribute dimension, and is the batch size. For all datasets,we store the graph feature X, the sparse adjacency matrix A, and themini-batch modularity matrix B in memory, which require O(),O(), and O(2) storage space, respectively. For GPUs, since onlythe subgraph data corresponding to each batch needs to be stored,the space complexity is reduced to O(( + )). For a given batch,the forward and backward computation costs O(2). Hence, for nodes, and epochs, time complexity is O(2). We compare thetime and space complexity of all methods used in , and theresult (see in Appendix) shows that Magi performs betterin terms of both time and memory complexity as compared to othermethods that leverage both graph and feature information.",
  "Baselines": "We compare Magi with 11 baselines, which can be categorized intofive groups:(1) Methods utilize graph structure only. Node2vec is ascalable graph embedding technique that utilizes random walks onthe graph structure.(2) Methods utilize graph features only. K-means is atraditional clustering algorithms that utilize only graph features.(3) GCLs with augmentations. DGI , GRACE , MV-GRL and BGRL are GCLs based on data augmentation tolearn node representations.(4) Augmentation-free GCLs. CCGC , SCGDN , andS3GC are recent augmentation-free GCLs.(5) Methods based on neural modularity maximization.DGCLUSTER and DMoN are two recent neural modularitymaximization methods that can achieve good performance in graphclustering, in which DGCLUSTER is a semi-supervised method.",
  "Metrics": "Following the evaluation setup of , we measure 4 metricsrelated to evaluating the quality of cluster assignments: Accuracy(ACC), Normalized Mutual Information (NMI), Adjusted Rand Index(ARI) and Macro-F1 Score (F1). For all the aforementioned metrics,higher values indicate better clustering performance. In our ex-periments, we first generate representations for each method andthen perform spectral clustering on the embeddings of small-scaledatasets (Cora, Citeseer, Photo, Computers) and K-means clusteringon the other datasets to produce cluster assignments for evaluation.",
  "Experimental setup": "We use a single NVIDIA A100 GPU with 40GB memory for eachmethod. All experiments are repeated 5 times and the mean valuesare reported in . For all datasets except for ogbn-papers100M,we set the number of run epochs at 400 and limit the maximumruntime to 1 hour. For ogbn-papers100M we allow up to 6 hours oftraining in addition to 256GB memory limitation. We employ fullbatch training on small-scale graphs and set the number of rootednodes as 2048 for large and extra-large datasets. Due to spacelimitations, more details about the hyperparameters are mentioned",
  "F10.1240.2470.1920.2500.2100.276": "in in the Appendix. We have provided a mini-batch andhighly scalable implementation of Magi in PyTorch, making it easyfor experiments with all datasets to adapt to GPU processing. For alldatasets, it is only need to store the subgraphs and correspondingmodularity matrix in a sparse form to execute Magis forward andbackward pass on the GPU. We also provide the GPU memorycost and the time required to execute the training process for eachmethod in in Appendix.",
  "EXPERIMENTAL RESULTS": "In this section, we present the experimental results of Magi inthe graph clustering task. In addition to the main experiments, wehave conducted supplementary experiments aimed at answeringthe following research questions:(Q1): Can Magi scale to extra-large scale graphs containing upto 100 million nodes and outperform state-of-the-art baselines?(Q2): Can modularity maximization pretext tasks effectivelymitigate the problem of semantic drift?",
  ": The performance of Magi with varying the numberand depth of random walks on the Cora and Reddit datasetin terms of NMI": "small-scale datasets, namely Cora, Citeseer, Photo, and Computers,we observe that MVGRL, S3GC, and DGCLUSTER emerge as thethree strongest baseline methods. Nonetheless, we find that Magiconsistently ranks first or second in performance in most cases.We note that even when compared to semi-supervised methodssuch as DGCLUSTER, Magi is only slightly behind on the Photodataset and maintains a consistent lead on the other three datasets,demonstrating Magi s superior performance. Next, we observe theperformance on large-scale datasets and find that Magi notablyoutperforms baseline methods like DGI, DMoN and S3GC. Magiis 3.6% better on ogbn-arxiv, 29% better on Reddit and 2.6%better on ogbn-products in terms of clustering F1 as comparedto the second-best method. Significantly, Magi outperforms thesecond-best method on the Reddit dataset with an approximateincrease of 18% in Accuracy, 7% in NMI, 16% in ARI, and 29% in F1 score. One possible reason for the superior performanceis that the Reddit dataset exhibits a higher graph density, whichresults in a more distinct community structure.ogbn-papers100M: To answer Q1, we conduct a comparativeanalysis of Magi performance on an extra-large dataset contain-ing 111M nodes and 1.6B edges. Note that we compare K-means,Node2vec, DGI, and S3GC since others can not scale to this dataset.The results are presented in . Our experimental results re-veal that Magi adeptly scales to this dataset and demonstrates asignificant performance improvement over methods relying solelyon features (K-means) by approximately 14.4%, exclusively on 0.10.30.50.70.9 tau Value (%)",
  "Semantic drift mitigation": "In practice, it is difficult to measure the degree of semantic drift ina graph. To answer Q2, we alternatively measure the semantic driftbased on the quality of pseudo-labels generated by various methods.Specifically, considering the augmentation-free GCLs referenced in.2, we employ ground-truth labels to evaluate the accuracyof pseudo-labels produced by these methods. In this context, apositive sample pair is considered as 1 if both nodes belong to thesame class, while a negative sample pair is deemed 1 if the nodesbelong to different classes; otherwise, they are labeled as 0. Theresults (refer to ) demonstrate that, in comparison to otheraugmentation-free GCLs, Magi generates higher-quality pseudo-labels. This confirms the efficacy of modularity maximization as apretext task in mitigating semantic drift.",
  "Ablation study": "To answer Q3, we first conduct thorough ablation studies on hy-perparameters including the number of walks , depth of walks ,number of randomly rooted nodes and temperature to examinethe stability of Magi s clustering. Our findings are listed below:(1) The performance of Magi remains relatively stable as long asthe number and depth of random walks are not excessively extreme,as shown in . We also note that the walk depth 5 is anoptimal choice, as a larger walk depth tends to sample positive pairs",
  ": The performance of Magi with varying differ-ent number of randomly rooted nodes on the ogbn-arxivdataset in terms of NMI": "with differing community semantics. This finding is consistent withthe previous work .(2) Magi can achieve the best performance when the temperature is around 0.5 (see ). A temperature that is too smallor too large can lead to the model paying too much or too littleattention to hard negative samples , respectively.(3) Different number of randomly sampled root nodes havesimilar clustering performance, but a smaller can lead to fasterconvergence (see ). Combined with Magis stable perfor-mance at smaller walk depths, this demonstrates Magis good scal-ability, meaning that for each node, Magi only needs to sample afew positive samples.High-order proximity. We then showcase the effectiveness ofexploring high-order proximity in Magi. In a given batch B, theeffectiveness of employing random walks versus directly samplinga submatrix from the vanilla modularity matrix is compared acrossvarious datasets. demonstrates that, in the majority of cases,the use of random walks outperforms direct sampling. This resulthighlights the advantage of incorporating high-order proximity intothe analysis, enabling a better capture of the complex relationshipsand community structure within the graph.Effectiveness of different components. We conduct ablationstudy to manifest the efficacy of different components in Magi. Weset five variants of our model for comparison. Results are shownin in Appendix, where Magi (w/ SL) refers to the use ofsimple contrastive loss defined in Eq.( 10), Magi (w/ MS) refersto use S1 and the sign of modularity score to define positive andnegative sample sets, Magi (w/ EI) refers to use S1 and edgeindicators to define positive and negative sample sets, Magi (w/ HMS) refers to use sign of high-order modularity score todefine positive and negative sample sets and Magi refers to use S1and S2 to define positive and negative sample sets.In , it is observed that each improvement of our modelcontributes to the final performance. First, loss (11) performs betterthan loss (10). Secondly, the direct use of edge indicators to definethe positive and negative sample sets achieves the worst effect. Weinfer that this may be because some edges exist between differentcommunities, which leads to some positive sample pairs from dif-ferent communities, increasing the impact of semantic drift. Thiscan be mitigated by the use of modularity scores, and considerationof high-order proximity in the community can further eliminatethe impact of semantic drift. Finally, our sampling strategy achievessimilar performance compared to using the sign of high order mod-ularity matrix proposed in , but our sampling strategy has lowercomputational complexity and can be scaled to large-scale graphdatasets with 100M nodes.",
  "Visualization": "In this part, we measure the quality of the generated embeddingsby directly employing t-SNE . The generated embeddings ofeach method are projected into 2-dimensional vectors for visual-ization. Due to space limitations, we have selected seven strongbaseline methods for visual analysis. It should also be noted thatDGCLUSTER , being a semi-supervised method, was excludedfrom this comparison. The visualization clearly demonstrates thatMagi produces representations with significantly higher clusteringefficacy compared to baseline methods.",
  "CONCLUSION": "In this paper, we explore the problem of graph clustering via neuralmodularity maximization. Our work establishes the connection be-tween neural modularity maximization and graph contrastive learn-ing. This insight motivates us to propose Magi, a community-awaregraph clustering framework with modularity maximization as thepretext task for contrastive learning. Magi is an augmentation-freeGCL framework, which avoids potential semantic drift and scala-bility issues. To ensure better scalability, Magi adopts a two-stagerandom walk to approximate the modularity matrix in a mini-batchmanner, followed by a principled contrastive loss to optimize thegoal of modularity maximization. Extensive experiments on eightreal-world graph datasets demonstrate the effectiveness of ourmethod, which achieves state-of-the-art in most cases comparedwith strong baselines. We hope that the straightforward nature ofour approach serves as a reminder to the community to reevalu-ate simpler alternatives that may have been overlooked, therebyinspiring future research.",
  "Vandana Bhatia and Rinkle Rani. 2018. DFuzzy: a deep learning-based fuzzyclustering model for large graphs. Knowledge and Information Systems 57 (2018),159181": "Aritra Bhowmick, Mert Kosan, Zexi Huang, Ambuj K. Singh, and Sourav Medya.2024. DGCLUSTER: A Neural Framework for Attributed Graph Clustering viaModularity Maximization. In AAAI. AAAI Press, 1106911077. Vincent D Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefeb-vre. 2008. Fast unfolding of communities in large networks. Journal of statisticalmechanics: theory and experiment 2008, 10 (2008), P10008. Ulrik Brandes, Daniel Delling, Marco Gaertler, Rachelle Goerke, Martin Hoefer,Zoran Nikoloski, and Donald Wagner. 2006. Maximizing Modularity is hard.arXiv: Data Analysis, Statistics and Probability (2006).",
  "Namkyeong Lee, Junseok Lee, and Chanyoung Park. 2022. Augmentation-FreeSelf-Supervised Learning on Graphs. (2022)": "Jintang Li, Ruofan Wu, Wangbin Sun, Liang Chen, Sheng Tian, Liang Zhu,Changhua Meng, Zibin Zheng, and Weiqiang Wang. 2023. Whats Behind theMask: Understanding Masked Graph Modeling for Graph Autoencoders. In KDD.ACM, 12681279. Jintang Li, Huizhe Zhang, Ruofan Wu, Zulun Zhu, Baokun Wang, ChanghuaMeng, Zibin Zheng, and Liang Chen. 2024. A Graph is Worth 1-bit Spikes: WhenGraph Contrastive Learning Meets Spiking Neural Networks. In ICLR.",
  "MethodTime ComplexitySpace ComplexityMemory Cost(MB)Time Cost(s)": "Node2vecO()O()1,12262.1DGIO( + 2)O( + + 2)21836.8GRACEO( 2 + + 2)O( + )2772.9BGRLO( + 2)O( + + 2)1,20131.2MVGRLO( 2 + 2)O( 2 + + 2)2,92433.9SCGDNO( 2 + 2)O( 2 + + 2)1,08031.9CCGCO( + 2 + 2)O( 2 + ( + ) + 2)3,23338.0DGCLUSTERO( 2 + 2)O( 2 + + 2)2676.5DMoNO(( + ))O( + )3019.3S3GCO(2)O( + + 2)2,14827.2MagiO(2)O( + + 2)2092.7"
}