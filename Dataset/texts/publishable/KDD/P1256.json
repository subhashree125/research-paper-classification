{
  "ABSTRACT": "Adversarial training is a method for enhancing neural networksto improve the robustness against adversarial examples. Besidesthe security concerns of potential adversarial examples, adversarialtraining can also improve the performance of the neural networks,train robust neural networks, and provide interpretability for neuralnetworks. In this work, we take the first step to introduce adversar-ial training in time series analysis by taking the finance field as anexample. Rethinking existing researches of adversarial training, wepropose the adaptively scaled adversarial training (ASAT) in timeseries analysis, by treating data at different time slots with time-dependent importance weights. Experimental results show that theproposed ASAT can improve both the accuracy and the adversarialrobustness of neural networks. Besides enhancing neural networks,we also propose the dimension-wise adversarial sensitivity indica-tor to probe the sensitivities and importance of input dimensions.With the proposed indicator, we can explain the decision bases ofblack box neural networks.",
  "INTRODUCTION": "Neural networks are found vulnerable to adversarial examples despite their promising performance. Adversarial exam-ples are generated by adding small malicious perturbations to theinput data that can mislead models, which reveal the vulnerability Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior specific permission and/or afee. Request permissions from 21 workshop, August, 2021, Virtual Workshop 2021 Association for Computing Machinery.ACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00 of neural networks with respect to the input data. Adversarial train-ing is a method for enhancing neural networksthat can improve both the adversarial robustness and the gener-alization ability at the same time. Besides the security concernsthat neural network faces potential risks of adversarial exampleattack, researches on adversarial training focus on improving theaccuracy of the neural networks , training robust neu-ral networks , and providing interpretability for neuralnetworks .Traditional adversarial training methods adopt the -norm con-straint that treats every dimension of perturbations symmetrically.Rethinking the shift-invariant hypothesis of convolutionneural networks in the computer vision (CV) field and the equiva-lence of different dimensions of word embeddings in the naturallanguage processing (NLP) field , it is roughly reasonablethat every dimension of the input data is treated as of similar signif-icance by the -norm constraint in adversarial training methods.However, in time series, different dimensions of inputs are not sym-metrical. Therefore, we propose to adopt a constraint with adaptivescales according to the importance of different dimensions in timeseries.In our work, we propose the adaptively scaled adversarial train-ing (ASAT) in time series analysis in the finance field. The mainaim of our work is to improve the generalization ability of neuralnetworks. Visual analysis shows that ASAT can alleviate overfit-ting to false clues, as shown in Sec. 5.3. Existing researches onadversarial training mainly focus on the CV andNLP fields. To the best of our knowledge, we takethe first step towards adversarial training in time series analysis orthe finance field.To validate the effects of ASAT, we implement some representa-tive baselines on a representative time series task on the volumeprediction task in the finance field, including moving baselines andseveral neural network baselines. Among them, we introduce astrong baseline, the Transformer model , in volume predictionfor the first time. Experimental results show that ASAT can improvethe accuracy of neural networks significantly, and can improve theadversarial robustness of neural networks with a big gap.Besides enhancing neural networks, we also try to explain thedecision bases of black box neural networks towards more inter-pretability for neural network decisions via the proposed dimension-wise adversarial sensitivity indicator. The dimension-wise adver-sarial sensitivity indicator can reveal the importance of any inputdimension in the decision process and help us understand the deci-sion bases of neural networks. A traditional method for interpretingdecisions of linear models is factor analysis, where factor loading",
  "KDD 21 workshop, August, 2021, Virtual WorkshopZhang, et al": "is adopted to measure the importance of a factor. The weights ofthe linear model can be seen as the factor loadings roughly. Weprove that the proposed sensitivity is proportional to the absolutevalue of the weight or the factor loading in the linear model, whichindicates that the proposed indicator covers traditional measure-ments. Moreover, in the nonlinear neural networks, the proposedindicator can also probe the importance of input data dimensions.With the proposed indicator, we have some interesting findings:(1) ASAT can alleviate overfitting. As analyzed in Sec. 5.3, baselinemodels with ASAT are sensitive to different dimensions duringmultiple training, which indicates that baseline models tend tooverfit to some false clues, while ASAT can alleviate it. (2) ASATcan help models capture fluctuations in time series. As analyzed inSec. 5.4, we make detailed examinations on a single data instanceand find that ASAT can help models to be more sensitive to ab-normal fluctuations. The decision bases of models tend to be morereasonable with ASAT. (3) Models tend to pay close attention to thevolumes of recent time slots. Models tend to be sensitive to volumesof recent time slots. Since recent time slots are more important intime series regularization, it is reasonable and also accords withhuman intuition.Our contributions are summarized as follows: To the best of our knowledge, we are the first to proposeto apply adversarial training in time series analysis or thefinance field to improve both the adversarial robustness andaccuracy of neural networks. Rethinking existing adversarialtraining approaches, we propose to rescale dimensions ofthe perturbation according to their importance, and proposethe adaptively scaled adversarial training (ASAT) algorithm.",
  "Adversarial Examples": "Let D denote the dataset, x = (1,2, ,)T R and standfor a data input and its ground truth, denote the neural networkwith the parameter vector and L denote the loss function. Theadversarial example can be defined as the small pertur-bation = (1,2, ,)T R on the data input x that canmislead the model and cause the maximum increases in the lossfunction,",
  "+1 = arg max ( + u+1)(5)": "where 0 is usually set to the zero vector and u is the -th updatein PGD, which is usually solved in Eq.(4) with the constraint 1 ={ : } and the step size is set to . (v) is the projectionfunction that projects v into the set . In two common cases, 2-norm and +-norm , the projection functions are,",
  "Dimension-wise Adversarial Sensitivity": "Inspired by the adversarial sensitivity, we propose to probe thesensitivities of different input dimensions and analyze the decisionbases of neural networks via dimension-wise adversarial sensitiv-ity. Besides the adversarial sensitivity of the whole model, we canalso define the dimension-wise adversarial sensitivity, namely sen-sitivity of the -th input dimension. Suppose R () denotes thesensitivities of the -th dimension, namely the expectation of themaximum loss change when the-th dimension of the input changesno more than , which is defined as",
  "R () := Emax| |,=0 L( (x + , ),) L( (x, ),)(10)": "The sensitivities of input dimensions can reveal the decisionbasis of the model and the importance of different dimensions inthe process of the prediction process. In traditional factor analysismethods, the factor loading measures the importance of a factor,which can be seen as the weight of the factor in a linear modelroughly. The proposed sensitivity does not only cover traditionalmeasurements in the linear model, as illustrated in Proposition 1,but also evaluates the importance of different dimensions in thenonlinear model.",
  "namely R () | | holds approximately when is small": "Proposition 1 illustrates that the proposed sensitivity is propor-tional to the absolute value of the weight or the factor loading in alinear model. We put the proof in Appendix. In our work, we adoptthe mean squared error (MSE) loss for time series regression tasks,L( ,) = ( )2 and = 2.",
  "Rethinking the Constraint Set inAdversarial Training": "Adversarial training algorithms are widely adopted in the CV and NLP fields. The widely-adopted -normconstraint, = { : }, is invariant about different dimen-sions of the perturbation, which is reasonable in both the CV andNLP fields.In the CV field, an important hypothesis of convolution neuralnetworks is shift-invariant and different pixels in an imagecan be treated of equal significance. In the NLP field, adversarialattacks can be conducted on the word embeddings and differentdimensions of word embeddings are of similar significance .However, in time series, different dimensions of inputs are notsymmetrical. Therefore, directly adopting the -norm constraintin adversarial training in time series is not reasonable.",
  "Rescaling the Constraint in AdversarialTraining in Time Series": "Scales of all dimensions of the -norm bounded constraint arethe same. However, in time series, dimensions with different times-tamps may be of different significance. The time series is usuallydefined as X = (x1, x2, , x ), where x is the input vector atthe timestamp . In our work, we flatten a time series into a vectorx = (1,2, ,)T R and define timestamp as the timestampof the -th dimension.Suppose = (1, 2, , )T, we multiply the scale of dimen-sion of the perturbation with to rescale the radius of dimension from to . As illustrated in , the 2-norm bounded con-straint specifies a -dimensional hyperball in R (a circle in R2, asshown in a) and the +-norm bounded constraint specifies a-dimensional hypercube in R (a square in R2, as shown in b).After rescaling, the hyperball is transformed to a hyperellipsoid (aellipse in R2, as shown in c) and the hypercube is transformed",
  "Adaptively Scaled Adversarial Training": "To rescale different dimensions of perturbations in adversarial train-ing, we propose the adaptively scaled adversarial training (ASAT)algorithm, as illustrated in Algorithm 1. The target of the proposedalgorithm is minimizing both the clean loss and the adversarialsensitivity. Following Zhu et al. , we optimize the risk on abatch B by minimizing the average risks of multiple adversarial",
  "=0L( (x + , ),)(16)": "In the constraint set = { : 1 }, we rescale thedimension with adaptively according to timestamp. Suppose denotes that timestamp is the -th latest timestamp (we onlyconsider unique timestamps), we set = (), where : N+ is a function that calculates the scales of different timestamps.Since in time series analysis, data of closer timestamps may be moreimportant, We may assume () ( + 1) and (1) = 1. The scale() decays when increases.We consider three simple decay functions:",
  "Constant (Const). () = 1. Exponential Decay (Exp). () = 1. Linear Decay (Linear). () = 1 (1 ) 1": "1.where the hyperparameter (0, 1] controls the decaying speed.When = 1, decay functions degrade to constant. denotes themaximum value of , i.e., the number of different timestamps. Forother complex decay functions, more hyperparameters may be in-volved. It is costly for hyperparameter tuning for other complexdecay functions involving more hyperparameters. Therefore, weonly consider three simple decay functions, the constant, exponen-tial decay, and linear decay functions, in our work and empiricallyadopt the exponential decay function.",
  "Tasks and Datasets": "4.1.1Volume prediction task. We choose the volume predictiontask, the input data consists of log prices and log volumes of pre-vious 12 time slots and the same time slots in previous 20 tradingdays. The prices include open, close, high, and low prices. The inputdata consists of 32 4 log prices and 32 1 log volumes totally. Thetarget is to regress the log volume. In our experiments, the valuesof of -th nearest time slots and the -th nearest day in history are and , respectively. 4.1.2Datasets and data preprocessing. For our research, we adoptthe hourly inter-day volume prediction dataset and the five-minuteintra-day volume prediction dataset. The two datasets are extractedfrom the price and volume data of the Topix500 (price index ofthe 500 most liquid and highly market capitalized stocks in TokyoStock) between Jan. 2017 and Feb. 2018. We adopt the data of 2017 asthe training set and development set, and the data of Jan. 2018 andFeb. 2018 as the test set. The training set and the development set",
  "ModelMSERMSEMAEACCMSERMSEMAEACC": "yesterday0.2860.5350.4060.6891.2051.0980.7970.66620-day average0.2490.4990.3850.7100.7000.8370.6080.70920-day EMA0.2880.5360.4130.6960.8080.8990.6590.692last time slot0.3240.5690.4390.5001.1251.0600.7450.50012-slot average0.5240.7240.4130.6960.9880.9940.7140.62912-slot EMA0.3310.5750.4430.6421.3141.1460.8440.60220-day and 12-slot average0.2710.5210.3930.6620.7410.8610.6090.695 Linear0.2270.0190.4770.0190.3700.0230.7080.0120.8080.0720.8920.0410.7000.0290.6600.037+ASAT0.2060.0040.4540.0050.3440.0060.7220.0060.6750.0610.8200.0370.6220.0470.6930.027-values2.1632.3412.1882.0872.8192.6072.8252.751 LSTM0.2200.0010.4700.0010.3580.0030.7080.0040.7970.0190.8920.0110.6900.0130.6710.006+ASAT0.2170.0020.4670.0020.3540.0020.7140.0030.7560.0150.8700.0080.6750.0120.6730.005-values2.6832.6832.2192.4003.3873.2352.8260.512# Transformer0.2190.0120.4700.0130.3570.0120.7110.0120.6650.0440.8150.0270.6100.0320.6950.019+ASAT0.2050.0050.4530.0050.3430.0040.7240.0050.6130.0280.7830.0180.5710.0180.7170.007-values2.1543.3032.2142.0001.9941.9722.1242.713 are randomly split with the ratio of 3 : 1. The statistics of datasetsare in .For example, in the hourly dataset, if the ground truth is thelog volume of 10:00-11:00, Jan. 21, then the previous 12 time slotsare: 9:00-10:00, Jan. 21, 14:00-15:00, Jan. 20, etc, and the history 20days are: 10:00-11:00, Jan. 20, 10:00-11:00, Jan. 19, etc. It is similarin data preprocessing of the five-minute dataset. We deleted thedata instances consisting of missing volumes or prices. In the five-minute dataset, the previous 12 time slots are collected from thesame day to the ground truth. 4.1.3Evaluation metrics. We adopt four evaluation metrics: meansquared error (MSE), root mean squared error (RMSE), mean abso-lute error (MAE), and accuracy (ACC). Suppose = (x, ) and is the ground truth, then these metrics are defined as,",
  "Linear. The linear model is formulated as (x, ) = Tx + ,where x R160 is the flattened vector of the concatenated input ofboth 12-slot and 20-day history": "4.2.3LSTM. The long-short term memory (LSTM) networkscan capture features and long-term dependency in the entire se-quences of data. Following Libman et al. , we implement twoone-layer LSTM models with the global attention mechanism to generate the representation vectors of previous time slots and20-day history. Before feeding input into LSTM, we first adopt twolinear layers to project the inputs into two higher dimensionalspaces, respectively. Then, we concatenate the representation vec-tors and feed them into a linear layer to regress the log volume. Theinput size, hidden size, and output size of LSTM models are 200. 4.2.4Transformer. We also implement a six-layer Transformer encoder model as a baseline. We concatenate the 12-slot and 20-dayhistory input and add a special token [CLS] before it to get a seriesX R335. The data of [CLS], X[0, :], is treated as a trainable pa-rameter of Transformer. Before feeding input into Transformer, wefirst adopt one linear layer to project the inputs into a higher di-mensional space. Then, we feed the output representation of [CLS]of the last layer into a linear layer to regress the volume. The inputsize, hidden size, and output size are 200, where the hidden statesare split into 8 heads.",
  "Settings and Choices of Hyperparameters": "We train every model for 5 epochs and report the test performanceon the checkpoint with the lowest valid loss. We adopt the Adamoptimizer and initialize the learning rate with 0.001. The batch sizeis 32. We repeat every experiment with 4 runs. Experiments areconducted on a GeForce GTX TITAN X GPU.Following Zhu et al. , we set = 3 and = 1.5 / =/2 in ASAT, where the hyperparameter is the step numberin ASAT attacks, is the step size and controls the strength ofadversarial training in the constraint set . We try 2 and + in ,and grid-search the hyperparameters and in ASAT. The processof hyperparameter search shows that too large or small cannotimprove the model accuracy well, and an appropriate needs to beselected. For the same , an appropriate needs to be selected too.Detailed experimental results in the choice of hyperparameters arereported in Appendix.",
  "Experimental Results": "After selecting the best configurations of hyperparameters, theexperimental results are shown in . It can be concluded thatthe 20-day average baseline performs best among multiple movingaverage baselines and three neural network baselines perform betterthan the 20-day average baseline. We only report results of the20-day average baseline in the following analysis. Moreover, theproposed ASAT can improve the performance of baselines.We also conduct student- tests to verify that the proposed ASAToutperforms baselines statistically significantly ( < 0.05). Thedegree of freedom is df= 6 and the critical -value is 1.943. We cansee that all -values are larger than 1.943 except the -values ofLSTM ACC on the five-minute dataset.",
  "Comparisons of Decay Functions": "We conduct an ablation study to investigate the influence of differ-ent decay functions. Experimental results are shown in .Experimental results show that the exponential decay functionoutperforms others in most cases. Generally speaking, adversarialtraining can improve the generalization ability of baseline models,which illustrates the effectiveness of adversarial training. Linearand exponential decay functions outperform the constant decayfunction, which indicates that our proposed ASAT algorithm out-performs conventional adversarial training widely adopted in theCV and NLP fields. Among linear and exponential decay functions,the exponential decay function performs best.",
  "We first choose an 2-norm constraint attack with = 0.2 andreport the clean performance and performance with attack (w/attack) of models. Results are shown in . Experimental results": "show that ASAT can improve the adversarial robustness of modelswith a big gap. Moreover, baseline linear, LSTM and Transformermodels tend to be more sensitive to adversarial attacks than the20-day average, which indicates that the traditional 20-day averagebaseline is more robust than neural networks. However, ASAT canimprove the robustness of neural networks and achieve similarrobustness to the 20-day average.We also conduct multiple adversarial attacks on models on thefive-minute dataset. Experimental results are shown in .Similarly, we can conclude that ASAT can improve the adversarialrobustness of models with a big gap.",
  "Probing Sensitivities of Input Dimensions": "We probe the sensitivities of different input dimensions in linearmodels, trained with three random seeds, the visualization resultsare shown in and . Here we choose the five-minutedataset, = 1, and the sensitivities are defined on the whole dataset.As shown in a and b, the sensitivity of the linearmodel is proportional to the absolute value of the weight or thefactor loading in a linear model, which accords with the theoreticalresults in Proposition 1 that the adversarial sensitivity of a singledimension in linear models can reflect the absolute value of theweight or the factor loading.b, a and b show the sensitivities of baselinelinear models with three random seeds. The dimensions modelspay close attention to are different during multiple training, whichindicates that baseline linear models tend to overfit to some falseclues. However, as shown in c, c and d, modelswith ASAT often pay close attention to some recent time slotsand are sensitive to them, which accords with human intuitionand is reasonable since recent time slots are more important intime series regularization. It also indicates that ASAT can alleviatethe overfitting since models with ASAT often focus on similardimensions during multiple training.",
  "Analysing Decision Basis on IndividualInstance": "In Eq.(10), the sensitivity can be defined on a dataset D or a singledata instance (x,). When it is defined on a single instance, itcan be adopted to probe the decision basis on a single instance.This probing method can provide more interpretability for neuralnetwork decisions, and provide insights on explaining the decisionbasis of a black box model, both on a dataset or an input.We choose the five-minute dataset, = 1, and randomly choosea data instance in the dataset to plot the sensitivities on the datasetand on a single data instance. As shown in , a showsthe sensitivities of a Transformer model trained with ASAT on thedataset. We can see that the model mainly concerns the volumes ofnearest time slots and volumes in 20-day history. We also probe thedecision basis on the data instance, whose volumes are shown inb. The volumes of the 9-th nearest time slots, the 14-th nearestday in history, and the 4-th nearest day in history are relatively lowcompared to other time slots or days in history. Both the baselineTransformer (shown in c) and the ASAT Transformer (shownin d) are sensitive to these time slots or days. They pay closeattention to these abnormal volumes and the corresponding prices.Therefore, their decisions on this data instance can capture theseabnormal fluctuations. It can also be concluded that Transformer isa strong baseline and it outperforms other neural network models.Moreover, the model with ASAT tends to be more sensitive toabnormal fluctuations, which indicates that ASAT can help thedecision bases of models be more reasonable. A potential negative societal impact is that malicious attackersmay attack financial models or analyze the decision bias of financialmodels for plagiarizing trade secrets, with the proposed indicator.Therefore, we call on financial model providers to only provideblack box models to defend the potential attacks or plagiarism,since the calculation of the proposed indicator requires gradients.",
  "RELATED WORK6.1Adversarial Training": "Szegedy et al. first propose the concept of adversarial exam-ples that can mislead deep neural networks with small maliciousperturbations, even in the physical world scenarios . Besidesgenerating adversarial examples to attack models , existing studies also concern improving the adversarialrobustness and generalization ability of neural networks via ad-versarial training . Adversarial training is widelyadopted in both the computer vision field andthe natural language processing field . To the bestof our knowledge, we are the first to propose to apply adversarialtraining in time series analysis or the finance field.",
  "Time Series Analysis and VolumePrediction": "Trading volume prediction plays an important role in algorithmictrading strategies . Many efforts are paid tovolume prediction . Machine learn-ing or deep learning methods have many applications in volumeprediction. Liu and Lai propose a dynamic SVM-based ap-proach for volume forecasting and Chen et al. adopt a Kalmanfilter approach. Libman et al. first adopts LSTM models in vol-ume prediction. Huptas models the volume forecasting taskas Bayesian autoregressive conditional models. Antulov-Fantulinet al. proposes temporal mixture ensemble models for volumepredictions. To the best of our knowledge, we propose to adoptTransformer in volume prediction for the first time.",
  "CONCLUSION AND FUTURE WORK": "In this work, we first introduce the adaptively scaled adversarialtraining (ASAT) to enhance neural networks in time series analysis.The proposed approach adaptively rescales different dimensions ofperturbations according to their importance. ASAT can improveboth the accuracy and adversarial robustness of several baselinemodels on intra-day and inter-day volume prediction tasks. We alsopropose the dimension-wise adversarial sensitivity indicator andutilize it to explain the decision bases of neural networks. In thiswork, we still use pre-determined decay functions in the scales ofdimensions, and the characteristics of different instances have notyet been taken into consideration. In the future, we will investigatean instance-wise adaptively scaled adversarial training method.",
  "Ran Chen, Yiyong Feng, and Daniel Palomar. 2016. Forecasting intraday tradingvolume: a kalman filter approach. Available at SSRN 3101695 (2016)": "Yong Cheng, Lu Jiang, and Wolfgang Macherey. 2019. Robust Neural MachineTranslation with Doubly Adversarial Inputs. In Proceedings of the 57th AnnualMeeting of the Association for Computational Linguistics. Association for Compu-tational Linguistics, Florence, Italy, 43244333. Yong Cheng, Lu Jiang, Wolfgang Macherey, and Jacob Eisenstein. 2020. Ad-vAug: Robust Adversarial Augmentation for Neural Machine Translation. InProceedings of the 58th Annual Meeting of the Association for Computational Lin-guistics. Association for Computational Linguistics, Online, 59615970. Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. 2018. HotFlip: White-Box Adversarial Examples for Text Classification. In Proceedings of the 56th AnnualMeeting of the Association for Computational Linguistics, ACL 2018, Melbourne,Australia, July 15-20, 2018, Volume 2: Short Papers, Iryna Gurevych and YusukeMiyao (Eds.). Association for Computational Linguistics, 3136. Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining andHarnessing Adversarial Examples. In 3rd International Conference on LearningRepresentations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference TrackProceedings.",
  "Hamid Karimi, Tyler Derr, and Jiliang Tang. 2019.Characterizing the De-cision Boundary of Deep Neural Networks.CoRR abs/1912.11460 (2019).arXiv:1912.11460": "Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. 2017. Adversarial examplesin the physical world. In 5th International Conference on Learning Representations,ICLR 2017, Toulon, France, April 24-26, 2017, Workshop Track Proceedings. Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting Wang. 2019.TextBug-ger: Generating Adversarial Text Against Real-world Applications. In26th Annual Network and Distributed System Security Symposium, NDSS2019, San Diego, California, USA, February 24-27, 2019. The Internet Soci-ety. Yitong Li, Timothy Baldwin, and Trevor Cohn. 2018. Whats in a Domain?Learning Domain-Robust Text Representations using Adversarial Training. InProceedings of the 2018 Conference of the North American Chapter of the Associationfor Computational Linguistics: Human Language Technologies, NAACL-HLT, NewOrleans, Louisiana, USA, June 1-6, 2018, Volume 2 (Short Papers), Marilyn A. Walker,Heng Ji, and Amanda Stent (Eds.). Association for Computational Linguistics,474479.",
  "Xiaotao Liu and Kin Keung Lai. 2017. Intraday volume percentages forecastingusing a dynamic SVM-based approach. Journal of Systems Science and Complexity30, 2 (2017), 421433": "Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective Ap-proaches to Attention-based Neural Machine Translation. In Proceedings of the2015 Conference on Empirical Methods in Natural Language Processing, EMNLP2015, Lisbon, Portugal, September 17-21, 2015, Llus Mrquez, Chris Callison-Burch,Jian Su, Daniele Pighin, and Yuval Marton (Eds.). The Association for Computa-tional Linguistics, 14121421. Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, andAdrian Vladu. 2018. Towards Deep Learning Models Resistant to AdversarialAttacks. In 6th International Conference on Learning Representations, ICLR 2018,Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings.",
  "Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and PascalFrossard. 2016. Universal adversarial perturbations. CoRR abs/1610.08401 (2016).arXiv:1610.08401": "Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. 2016.DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks. In2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016,Las Vegas, NV, USA, June 27-30, 2016. IEEE Computer Society, 25742582. Ali Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John P. Dickerson, ChristophStuder, Larry S. Davis, Gavin Taylor, and Tom Goldstein. 2019.Adversar-ial training for free!. In Advances in Neural Information Processing Systems32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS2019, December 8-14, 2019, Vancouver, BC, Canada, Hanna M. Wallach, HugoLarochelle, Alina Beygelzimer, Florence dAlch-Buc, Emily B. Fox, and RomanGarnett (Eds.). 33533364. Uri Shaham, Yutaro Yamada, and Sahand Negahban. 2015. Understanding Ad-versarial Training: Increasing Local Stability of Neural Nets through RobustOptimization. CoRR abs/1511.05432 (2015). arXiv:1511.05432",
  "Xunyu Ye, Rui Yan, and Handong Li. 2014. Forecasting trading volume in theChinese stock market based on the dynamic VWAP. Studies in Nonlinear Dynamics& Econometrics 18, 2 (2014), 125144": "Dinghuai Zhang, Tianyuan Zhang, Yiping Lu, Zhanxing Zhu, and Bin Dong. 2019.You Only Propagate Once: Accelerating Adversarial Training via Maximal Princi-ple. In Advances in Neural Information Processing Systems 32: Annual Conferenceon Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019,Vancouver, BC, Canada, Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer,Florence dAlch-Buc, Emily B. Fox, and Roman Garnett (Eds.). 227238. Richard Zhang. 2019. Making Convolutional Networks Shift-Invariant Again. InProceedings of the 36th International Conference on Machine Learning, ICML 2019,9-15 June 2019, Long Beach, California, USA (Proceedings of Machine LearningResearch, Vol. 97), Kamalika Chaudhuri and Ruslan Salakhutdinov (Eds.). PMLR,73247334.",
  "BDETAILS OF HYPERPARAMETERS CHOICE": "We grid-search the hyperparameters in {0.001, 0.002, 0.005, 0.01,0.02, 0.05, 0.1, 0.2, 0.5, 1} and in {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,0.8, 0.9, 0.95} and try 2 and +. To investigate the influence ofhyperparameters and , we take the Transformer model on thefive-minute dataset as an example, and discuss the influence ofhyperparameters and according to the experimental results.",
  "We conduct hyperparameter search experiments to find the bestconfigurations of hyperparameters": "We first report the best configurations on the hourly dataset.On the linear model, we adopt the + constraint, = 0.01, and = 0.1. On the LSTM model, we adopt the + constraint, = 0.1,and = 0.7. On the Transformer model, we adopt the 2 constraint, = 0.05, and = 0.5.We then report the best configurations on the five-minute dataset.On the linear model, we adopt the + constraint, = 0.1, and =0.95. On the LSTM model, we adopt the 2 constraint, = 0.5, and = 0.9. On the Transformer model, we adopt the + constraint, = 0.02, and = 0.7."
}