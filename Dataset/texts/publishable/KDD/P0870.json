{
  "Abstract": "Online shopping is a complex multi-task, few-shot learning problem with a wideand evolving range of entities, relations, and tasks. However, existing models andbenchmarks are commonly tailored to specific tasks, falling short of capturingthe full complexity of online shopping. Large Language Models (LLMs), withtheir multi-task and few-shot learning abilities, have the potential to profoundlytransform online shopping by alleviating task-specific engineering efforts and byproviding users with interactive conversations. Despite the potential, LLMs faceunique challenges in online shopping, such as domain-specific concepts, implicitknowledge, and heterogeneous user behaviors. Motivated by the potential andchallenges, we propose Shopping MMLU, a diverse multi-task online shoppingbenchmark derived from real-world Amazon data. Shopping MMLU consists of 57tasks covering 4 major shopping skills: concept understanding, knowledge reason-ing, user behavior alignment, and multi-linguality, and can thus comprehensivelyevaluate the abilities of LLMs as general shop assistants. With Shopping MMLU,we benchmark over 20 existing LLMs and uncover valuable insights about practicesand prospects of building versatile LLM-based shop assistants. Shopping MMLUcan be publicly accessed at Inaddition, with Shopping MMLU, we host a competition in KDD Cup 2024 2 withover 500 participating teams. The winning solutions and the associated workshopcan be accessed at our website",
  "Introduction": "Machine learning (ML) has been applied to various user-oriented online services, such as onlinecommunities, streaming services, etc, with online shopping being among the most successful ones.In recent years, ML methods are applied to various online shopping tasks, such as user queries, sessions , reviews , product attributes , etc. To facilitate thedevelopment of ML methods, many benchmarks are designed to lower the barrier forresearchers and engineers to develop and evaluate novel solutions to real-world online shopping tasks.",
  ": Distinctive characteristics of online shopping with real-world examples": "Online shopping is complex with numerous entities, relations, and tasks. For example, products areassociated with attributes, attribute values, and product categories. Users interact with products withvarious behaviors such as queries, clicks, and purchases. Therefore, online shopping creates multi-task learning problems involving a joint understanding and modeling of these entities. Moreover,the entities and tasks are not fixed but expand over time with new users and services, such as theexpansion of Amazon from shopping to streaming services, creating few-shot learning problems inthe process. However, the multi-task, few-shot learning nature of online shopping is not sufficientlycaptured by existing works and benchmarks which mainly design task-specific models and datasets. Large language models (LLMs) emerge as promising solutions to the multi-task, few-shot learningproblem of online shopping. Recent works like GPT-3, T5, and FLAN have shown that asingle LLM can perform various text-related tasks with state-of-the-art performances, and can alsogeneralize to unseen tasks with few-shot examples or even task descriptions only. These resultsmotivate us to explore LLM-based solutions for online shopping with two advantages. First, we traina single LLM for all tasks instead of multiple task-specific models, which mitigates task-specificengineering efforts. Second, the trained LLM can seamlessly adapt to emerging tasks with onlyfew-shot examples, lowering the costs for collecting large-scale data for model re-training. Moreover,LLM-based shop assistants can also improve user experiences by giving real-time interactive feedbackto customer questions.",
  "Domain-specific Short Texts. Texts in online shopping contain domain-specific entities, such asbrands, models, etc., which may be challenging for general LLMs, especially without specific context": "Implicit Knowledge. Complex implicit knowledge and reasoning is required in online shopping tounderstand whether two products are compatible, or whether two brands produce similar items. Thus,it is challenging for LLMs to understand and adequately use the knowledge to perform reasoning. User Behaviors. Aside from texts, implicit user behaviors exist (e.g. purchase, view, query-then-click,etc.) in online shopping. While implicit user behaviors are vital in understanding user intentions,general LLMs may not understand them as they rarely exist in pre-training data.",
  "Shopping MMLUYes57YesYesYes6": "Motivated by the above potentials and challenges, we propose Shopping MMLU, a diverse multi-taskonline shopping benchmark for LLMs. Shopping MMLU consists of 57 tasks and 20,799 questionscurated with real-world Amazon data and covers an extensive range of shopping entities like products,categories, attributes, queries, reviews, sessions, etc. We re-formulate all tasks in Shopping MMLUas text-to-text generation to accommodate LLM-based solutions. Furthermore, to enable fine-grainedanalysis of model capabilities, we split Shopping MMLU into 4 shopping skills corresponding to thecharacteristics shown in : shopping concept understanding, shopping knowledge reasoning,user behavior alignment, and multi-lingual abilities. We benchmark over 20 LLMs on ShoppingMMLU to explore the potential of building LLM-based online shop assistants. Our experimentalresults uncover valuable insights for domain-specific LLMs in online shopping, such as task-wisecorrelations, transferability of general knowledge, effects of instruction fine-tuning, and in-contextlearning. We believe that Shopping MMLU can inspire and facilitate the transition from task-specific efforts toversatile LLM-based methods in online shopping. Moreover, as the characteristics of online shopping() exist in other user-oriented services as well, we also expect that the insights uncovered byShopping MMLU would benefit efforts to build domain-specific LLMs in a wider range of fields.",
  "Related Work": "Online Shopping DatasetsWe summarize related online shopping datasets in . Previously,online shopping datasets often focus on one or several closely related tasks, e.g. MAVE forattribute value extraction, Amazon-M2 for session-based recommendation, Amazon-ESCI for query-product matching, etc. Consequently, they fail to reflect the multi-task nature of onlineshopping as their coverage of tasks and skills is limited. More recently, multi-task online shopping datasets are curated to build versatile LLM-based shopassistants, such as EComInstruct for EcomGPT and ECInstruct for eCeLLM . Both EComIn-struct and ECInstruct reformulate online shopping tasks into text-to-text generation and fine-tune asingle LLM to perform all tasks. However, despite being multi-task datasets, EComInstruct solelyfocuses on shopping concept understanding (12 out of 12 tasks), while ECInstruct primarily tacklesuser behavior alignment (8 out of 10 tasks). Therefore, their coverage of skills in online shopping isstill limited compared to Shopping MMLU, especially in reasoning and multi-lingual abilities. LLMs for Online ShoppingShopping websites house various texts such as product titles, descrip-tions, reviews, ads, etc., motivating an extensive study of LLM solutions to online shopping tasks,such as recommendation , ranking , named entity recognition , etc. However, thesemethods are limited to specific tasks without fully exploring the multi-task nature of LLMs. More recent works leverage instruction fine-tuning (IFT) to adapt general domain LLMs to onlineshopping, such as EcomGPT and eCeLLM. However, as shown in , the capabilities ofEcomGPT and eCeLLM may be limited as their training data cover a limited range of shopping tasksand skills. Web Agent Benchmarks for Online ShoppingLLMs bring about exciting prospects in developingagents that can perform sequential decision making and task execution following text instructions.As online shopping websites are diverse, realistic, and interactive environments, many benchmarksare developed upon them, such as WebShop and WebArena where agents are required to perform shopping tasks such as purchasing products and summarizing reviews. We believe thatShopping MMLU is complementary to these agent benchmarks, as agents should first gain sufficientknowledge of online shopping before executing composite decision making tasks.",
  "Raw Data Sources": "Shopping MMLU is curated primarily with real-world, internal or public Amazondata, such as product catalogs, reviews, browse ses-sions, queries, etc. We remove all IDs (e.g. userID,sessionID, etc.) to ensure anonymity. We also useClaude 2 to synthesize data for some tasks thatdo not involve concrete product or user data. Detailsof raw data sources are given in Appendix A.2.",
  "Task Taxonomy": "Shopping MMLU consists of 57 tasks across 4 shopping skills corresponding to . Moreover,we divide each skill into sub-skills to enable more fine-grained evaluation. A simplified taxonomy isshown in , with the full taxonomy in in the Appendix. We introduce each skill andtheir sub-skills as follows, and leave more details in Appendix A.3. Shopping Concept Understanding(\"Concept\" for short). Online shopping concepts such asbrands and product models are domain-specific and not often seen in pre-training. Moreover, theyoften appear in short texts (e.g. queries, attribute-value pairs) and thus no sufficient contexts are givento help understand them. Hence, failing to understand these concepts compromises the performance ofLLMs on downstream tasks. We include the following sub-skills in this skill: concept normalization,elaboration, relational inference, sentiment analysis, information extraction, and summarization. Shopping Knowledge Reasoning(\"Reasoning\" for short). This skill focuses on understandingand applying various implicit knowledge to perform reasoning over products and their attributes. Forexample, calculations such as the total volume of a product pack require numeric reasoning, andfinding compatible products requires multi-hop reasoning among various products over a productknowledge graph. Based on the specific type of reasoning required, we split this skill into threesub-skills, numeric, commonsense, and multi-hop reasoning. User Behavior Alignment(\"Behavior\" for short). Accurately modeling user behaviors is a crucialskill in online shopping. A large variety of user behaviors exist in online shopping, including queries,clicks, add-to-carts, purchases, etc. Moreover, these behaviors are generally implicit and not expressedin text. Consequently, LLMs trained with general texts encounter challenges in aligning with theheterogeneous and implicit user behaviors as they rarely observe such inputs during pre-training. Wefurther design the following sub-skills to reflect such heterogeneous behaviors: query-query relation,query-product relation, sessions, purchases, and reviews & QAs. Multi-lingual Abilities(\"Multi-lingual\" for short). Multi-lingual models are desired in onlineshopping as they can be deployed in multiple marketplaces without re-training. Therefore, we designthe skill of multi-lingual online shopping, consisting of multi-lingual concept understanding andmulti-lingual user behavior alignment.",
  "Task Types": "We include 5 types of tasks in Shopping MMLU for a comprehensive evaluation of shopping skills,including multiple choice, retrieval, ranking, named entity recognition, and generation. Due todifferent format requirements, each type of task requires specific prompts such that the evaluatedLLMs follow the instructions and generate valid answers, which we show in Appendix A.5. Evaluation MetricsWe use accuracy for multiple choice tasks, hit rate@3 for retrieval tasks,normalized discounted cumulative gain (NDCG) for ranking tasks, and micro F1 for named entityrecognition tasks. For generation tasks, we apply ROUGE-L scores for extraction tasks (i.e. theanswer is a sub-string of the input), BLEU scores for translation tasks, and sentence transformersimilarity for other generation tasks. Details of the metrics are introduced in Appendix A.4. Wetake an average of all task-wise metrics (i.e. macro average) as the score of a skill.",
  "Data Quality Control": "Datasets of online shopping are either defined by human behaviors or are human-labeled, and thusmay contain noise or errors. To address the issue, we manually inspect all data samples to ensure thevalidity of the questions. We also remove potentially offensive contents and all links to images andvideos in product descriptions and reviews. Details of data filtering are described in Appendix A.6.",
  "Experimental Setup": "We apply zero-shot evaluation for Shopping MMLU for three main reasons. First, zero-shot evaluationresembles the real-world scenario where customers directly enter their questions without creatingfew-shot examples. Second, zero-shot evaluation rules out variances brought by different few-shotexamples. Finally, all evaluated models achieve non-trivial results under zero-shot evaluation onShopping MMLU. All models are tested with the same prompts.",
  "<5BQWen1.5-4B57.2152.5642.7449.78Phi-249.3442.8336.3832.91eCeLLM-S49.4039.0636.3332.79": "Proprietary ModelsWe evaluate ChatGPT , Claude-2 , and Claude-3 Sonnet , which arestate-of-the-art LLMs trained with general domain data and provide insights on how well LLMs cansolve domain-specific online shopping problems with general knowledge only. Open-Source General ModelsOpen-source LLMs can be categorized as base and chat models.Base models refer to LLMs that are only pre-trained with next-token prediction without any modera-tion techniques, while chat models often undergo IFT such that they follow the input instructions.We include both base and chat models to see how the instruction following abilities of chat modelstransfer from the general domain to the specific domain of online shopping. Specifically, we considerLLaMA2 (7/13/70B, base and chat) , LLaMA3 (8/70B, base and instruct) , Mistral (7/8x7B,base and instruct) , QWen1.5 (4/7/14/72B) , and Phi-2 models. Domain-specific ModelsWe evaluate eCeLLM-S, M, and L models that are fine-tuned withdomain-specific online shopping IFT data (ECInstruct ) over Phi-2, Mistral-7B, and LLaMA2-13B, respectively, to see how domain-specific IFT helps improve model performances on ShoppingMMLU.",
  "We show the scores of all evaluated models on each skill of Shopping MMLU in . Due tospace limitations, we omit detailed task-wise scores. We draw the following insights from": "First, proprietary LLMs remain the state-of-the-art, while open-source LLMs are catching up.Claude-3 Sonnet performs the best across all models, followed by Claude-2 and ChatGPT. Overall,these proprietary LLMs remain the strongest even in the specific domain of online shopping. We alsoobserve that LLaMA3-70B-Instruct and QWen1.5-72B perform on par with ChatGPT and Claude-2,demonstrating the potential of building powerful LLM shop assistants with public resources. Second, Shopping MMLU is a challenging benchmark. While eCeLLMs outperform GPT-4 ontheir dataset ECInstruct , they are still far behind ChatGPT on Shopping MMLU, showing thatShopping MMLU is a more complex and challenging benchmark for online shopping than ECInstruct. 0.00.20.40.60.81.0",
  ": Task and skill-wise score correlations of Shopping MMLU": "Finally, domain-specific models are not always strong. While eCeLLMs perform better on ShoppingMMLU than their base models (eCeLLM-M/Mistral-7B, eCeLLM-L/LLaMA2-13B), they are notalways strong compared to LLMs with similar numbers of parameters. For example, among 13BLLMs, eCeLLM-L generally performs worse than QWen1.5-14B; among 7B LLMs, eCeLLM-Mgenerally performs worse than LLaMA3-8B-Instruct. These facts indicate that LLMs with propertraining in the general domain already excel in online shopping without domain-specific tuning.",
  "How Multi-task is Online Shopping?": "According to , the key of multi-task learning is to leverage useful information in multiple tasks toimprove the performances of all tasks. Consequently, the more the shared knowledge, the more likelywe can jointly improve all tasks in Shopping MMLU and build versatile LLM-based shop assistants.Thus, in this section, we analyze the extent to which knowledge is shared among tasks in ShoppingMMLU by analyzing the score correlations between pairwise tasks and skills. We first analyze the task-wise score correlations. Let si be the scores achieved by all evaluated LLMson task i, the score correlation between tasks i and j is defined as cij = PearsonCorr(si, sj). Thedistribution of cij is shown in (a). As shown, the scores of most task pairs (1589 out of 1596)are positively correlated. Moreover, with an average of 0.557 and a standard deviation of 0.209, thescore correlations are significantly positive, indicating a notable amount of shared knowledge amongtasks in Shopping MMLU. We analyze task pairs with negative correlations in Appendix B.3. We similarly compute the score correlations between pairwise skills and plot them in (b).As shown, all skills are positively correlated with each other with correlations of at least 0.9. Theobservation further underscores the multi-task nature of Shopping MMLU and the potential of jointlyimproving online shopping skills as a whole with unified solutions.",
  "General Knowledge Transfers Well to Online Shopping": "The field of LLMs advances at a rapid pace, yielding models with increasingly powerful capabilities.Therefore, we analyze whether the specific domain of online shopping benefits from the advancingLLMs and their increasing general knowledge and abilities. We calculate the score correlationsbetween each skill in Shopping MMLU and the Open LLM Leaderboard , consisting of MMLU,GSM8K, Winogrande, HellaSwag, TruthfulQA, and ARC . The correlations areshown in (a), where all skills show strongly positive correlations with the Open LLM Leader-board scores. The high correlations indicate that general knowledge transfers well to the specificdomain of online shopping, and that powerful LLM-based shop assistants should be established uponstrong base models.",
  "Effects of Instruction Fine-tuning": "In this section, we analyze the effects of IFT onShopping MMLU. We analyze both general domainand domain-specific IFT to understand whether the in-struction following ability transfers from the generaldomain to online shopping, and how domain-specificIFT achieves further improvement. General domain IFTWe analyze LLaMA2 and LLaMA3 models to study the impact of generaldomain IFT on Shopping MMLU. We plot the relation between scores of base models and theimprovements brought by IFT (e.g. LLaMA3-8B-Instruct VS Base) in . We only plotthe average values across all 4 skills and leave details in Appendix B.4. We make the followingobservations. First, general domain IFT helps in most cases. Among the 5 models tested, IFT leads to performanceimprovements on 4 of them, indicating that the instruction following ability brought by general domainIFT often transfers to the specific domain of online shopping. Second, IFT data and recipe matters.Comparing LLaMA2 and LLaMA3, we find that LLaMA3 models generally benefit more from IFT,which can be attributed to the better instruction data with careful curation used to tune LLaMA3. Finally, general domain IFT is less helpful on stronger base models. Within each modelfamily, IFT leads to less improvements on stronger base models. Notably, IFT leads to performancedecline on LLaMA2-70B. We hypothesize that as base models gets stronger, they may overfit to therelatively small IFT dataset during IFT, resulting in the catastrophic forgetting of helpful knowledge. Domain-specific IFTAs shown in , while eCeLLMs perform better than their base modelswith domain-specific IFT, they do not compare favorably against strong general domain LLMs(LLaMA3 and QWen1.5). Therefore, we analyze the reasons underlying the limited improvementsand shed light on how domain-specific IFT data should be curated. We show the comparisons betweeneCeLLMs and their base models in . We also include Zephyr and Vicuna-13B, which aretuned with general domain IFT over Mistral-7B and LLaMA2-13B, respectively. We make thefollowing observations.",
  "L outperform their base models. The observation indicates that domain-specific IFT works onlyon sufficiently strong base models, which echoes the phenomenon in general domain IFT": "Domain-specific IFT only works on observed tasks and skills. As shown in (b) and6(c), eCeLLMs primarily improve over their counterparts on \"Behavior\" and \"Multi-lingual\"skills, which should be attributed to their IFT datasets. In ECInstruct used to tune eCeLLMs,8 out of 10 tasks belong to the \"Behavior\" skill. Therefore, it is not surprising that eCeLLMsperform well on the \"Behavior\" skill. We also hypothesize that the knowledge transfer fromEnglish to other languages leads to the improvement on the \"Multi-lingual\" skill, as this skillconsists heavily of multi-lingual user behavior alignment tasks. However, eCeLLMs achievelimited improvements on \"Concept\" and \"Reasoning\" skills, showing that domain-specific IFTonly works on skills included in the IFT data and does not generalize well to unseen skills.Therefore, domain-specific IFT data should be curated with sufficient diversity and coverage. We also test eCeLLM-M and L on 5 general LLM benchmarks, MMLU, HellaSwag, Winogrande,TruthfulQA, and GSM8K to analyze why domain-specific IFT fails to generalize to unseen skills.Results are shown in , where eCeLLMs perform worse than their base models in most generalLLM benchmarks. Thus, domain-specific IFT fails to improve or even compromises the modelsgeneral knowledge, which may explain their inability to generalize to unseen skills.",
  "Effects of In-context Learning": "LLMs are capable of learning from few-shot examples in prompts, known as in-context learning.As few-shot learning is common in online shopping, such as cold-start users, we analyze how wellLLMs adapt to unseen tasks with few-shot examples and thus solve the few-shot learning problem.We select representative subsets of models and tasks for the analysis (details in Appendix B.5). Foreach selected task, we split the dataset into a training set of 20 samples, and the rest as test sets. Weevaluate under 0-, 1-, and 5-shot settings and show results in . For each setting, we randomlysample few-shot examples from the training set and show the mean score of 5 random seeds. Weobserve the following phenomena despite the mixed results.",
  ": Results of in-context learning (0-, 1-, and 5-shot) on representative tasks in ShoppingMMLU": "and skills, in-context examples lead to worse scores (e.g. ChatGPT, Vicuna-13B and eCeLLM-L in (c)). The observation indicates that few-shot learning in online shopping remainschallenging even with strong LLMs. Second, in-context learning does not help reasoning tasks.We observe from (b) that in-context learning fails to improve the performance of any modelon shopping knowledge reasoning tasks. We further explore this observation with chain-of-thought(CoT) prompting , whose results are shown in Appendix B.5.",
  "Conclusion and Future Work": "This paper presents Shopping MMLU, a multi-task online shopping benchmark for LLMs aimingto facilitate LLMs-based solutions to a unified, multi-task modeling of online shopping. ShoppingMMLU features a wide range of online shopping skills, tasks, and entities, and thus is suitable forresearchers and practitioners to comprehensively evaluate their solutions of domain-specific LLMonline shop assistants. With Shopping MMLU, we perform extensive experiments on over 20 LLMs,whose results uncover valuable insights on building domain-specific LLMs for online shopping, suchas task- and skill-wise relations, general knowledge, instruction fine-tuning, and in-context learning. Shopping MMLU triggers a series of future work. In Appendix C we show that state-of-the-art proprietary LLMs still lags behind task-specific methods on some tasks of Shopping MMLU,motivating advanced training recipes and data for LLMs in online shopping. We also discuss broaderimpacts and limitations in Appendix C.",
  "A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand,G. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023": "H. Jiang, T. Cao, Z. Li, C. Luo, X. Tang, Q. Yin, D. Zhang, R. Goutam, and B. Yin. Short textpre-training with extended token classification for e-commerce query understanding. arXivpreprint arXiv:2210.03915, 2022. W. Jin, H. Mao, Z. Li, H. Jiang, C. Luo, H. Wen, H. Han, H. Lu, Z. Wang, R. Li, et al.Amazon-m2: A multilingual multi-locale shopping session dataset for recommendation and textgeneration. Advances in Neural Information Processing Systems, 36, 2023. E. Kasneci, K. Seler, S. Kchemann, M. Bannert, D. Dementieva, F. Fischer, U. Gasser,G. Groh, S. Gnnemann, E. Hllermeier, et al. Chatgpt for good? on opportunities andchallenges of large language models for education.Learning and individual differences,103:102274, 2023. J. Li, M. Wang, J. Li, J. Fu, X. Shen, J. Shang, and J. McAuley. Text is all you need: Learninglanguage representations for sequential recommendation. In Proceedings of the 29th ACMSIGKDD Conference on Knowledge Discovery and Data Mining, pages 12581267, 2023. S. Li, F. Lv, T. Jin, G. Li, Y. Zheng, T. Zhuang, Q. Liu, X. Zeng, J. Kwok, and Q. Ma. Queryrewriting in taobao search. In Proceedings of the 31st ACM International Conference onInformation & Knowledge Management, pages 32623271, 2022. Y. Li, S. Ma, X. Wang, S. Huang, C. Jiang, H.-T. Zheng, P. Xie, F. Huang, and Y. Jiang.Ecomgpt: Instruction-tuning large language models with chain-of-task tasks for e-commerce. InProceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1858218590,2024. P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga, Y. Zhang, D. Narayanan,Y. Wu, A. Kumar, et al. Holistic evaluation of language models. Transactions on MachineLearning Research, 2023.",
  "S. Lin, J. Hilton, and O. Evans. Truthfulqa: Measuring how models mimic human falsehoods,2022": "X. Liu, Z. Li, Y. Gao, J. Yang, T. Cao, Z. Wang, B. Yin, and Y. Song. Enhancing userintent capture in session-based recommendation with attribute patterns. Advances in NeuralInformation Processing Systems, 36, 2023. C. Luo, W. Headden, N. Avudaiappan, H. Jiang, T. Cao, Q. Yin, Y. Gao, Z. Li, R. Goutam,H. Zhang, et al. Query attribute recommendation at amazon search. In Proceedings of the 16thACM Conference on Recommender Systems, pages 506508, 2022.",
  "Meta-AI-Research. Introducing Meta Llama 3: The most capable openly available LLM to date,4 2024": "J. Ni, J. Li, and J. McAuley. Justifying recommendations using distantly-labeled reviews andfine-grained aspects. In Proceedings of the 2019 conference on empirical methods in naturallanguage processing and the 9th international joint conference on natural language processing(EMNLP-IJCNLP), pages 188197, 2019. J. Ni, Z. C. Lipton, S. Vikram, and J. McAuley. Estimating reactions and recommendingproducts with generative models of reviews. In Proceedings of the Eighth International JointConference on Natural Language Processing (Volume 1: Long Papers), pages 783791, 2017. K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: a method for automatic evaluationof machine translation. In Proceedings of the 40th annual meeting of the Association forComputational Linguistics, pages 311318, 2002.",
  "M. Post. A call for clarity in reporting bleu scores. arXiv preprint arXiv:1804.08771, 2018": "C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J.Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal ofmachine learning research, 21(140):167, 2020. C. K. Reddy, L. Mrquez, F. Valero, N. Rao, H. Zaragoza, S. Bandyopadhyay, A. Biswas,A. Xing, and K. Subbian. Shopping queries dataset: A large-scale esci benchmark for improvingproduct search. arXiv preprint arXiv:2206.06588, 2022. N. Reimers and I. Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks.In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processingand the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),pages 39823992, 2019.",
  "K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi. WINOGRANDE: an adversarialwinograd schema challenge at scale, 2019": "H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXivpreprint arXiv:2307.09288, 2023. Q. Wang, L. Yang, B. Kanagal, S. Sanghai, D. Sivakumar, B. Shu, Z. Yu, and J. Elsas. Learningto extract attribute value from product via question answering: A multi-task approach. InProceedings of the 26th ACM SIGKDD international conference on knowledge discovery &data mining, pages 4755, 2020.",
  "J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le.Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021": "J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural informationprocessing systems, 35:2482424837, 2022. W. Wei, X. Ren, J. Tang, Q. Wang, L. Su, S. Cheng, J. Wang, D. Yin, and C. Huang. Llmrec:Large language models with graph augmentation for recommendation. In Proceedings of the17th ACM International Conference on Web Search and Data Mining, pages 806815, 2024. P. West, C. Bhagavatula, J. Hessel, J. Hwang, L. Jiang, R. Le Bras, X. Lu, S. Welleck, andY. Choi. Symbolic knowledge distillation: from general language models to commonsense mod-els. In Proceedings of the 2022 Conference of the North American Chapter of the Associationfor Computational Linguistics: Human Language Technologies, pages 46024625, 2022.",
  "K. Xu, X. Wan, H. Wang, Z. Ren, X. Liao, D. Sun, C. Zeng, and K. Chen. Tacc: A full-stackcloud computing infrastructure for machine learning tasks. arXiv preprint arXiv:2110.01556,2021": "H. Yang and K. Li. Pyabsa: Open framework for aspect-based sentiment analysis, 2022. L. Yang, Q. Wang, Z. Yu, A. Kulkarni, S. Sanghai, B. Shu, J. Elsas, and B. Kanagal. Mave: Aproduct dataset for multi-source attribute value extraction. In Proceedings of the fifteenth ACMinternational conference on web search and data mining, pages 12561265, 2022.",
  "Y. Zhang and Q. Yang. A survey on multi-task learning. IEEE Transactions on Knowledge andData Engineering, 34(12):55865609, 2021": "G. Zheng, S. Mukherjee, X. L. Dong, and F. Li. Opentag: Open attribute value extractionfrom product profiles. In Proceedings of the 24th ACM SIGKDD international conference onknowledge discovery & data mining, pages 10491058, 2018. S. Zhou, F. F. Xu, H. Zhu, X. Zhou, R. Lo, A. Sridhar, X. Cheng, T. Ou, Y. Bisk, D. Fried,et al. Webarena: A realistic web environment for building autonomous agents. arXiv preprintarXiv:2307.13854, 2023.",
  "Amazon-ESCI , including user queries, related products, as well the relevance betweenthe query and the products. The data come in 3 languages, English, Japanese, and Spanish": "Amazon Product Keyphrases , including product metadata (title, descriptions) as wellas product keyphrases derived from users queries. The data come in 5 languages, English,German, Spanish, French, and Italian. Amazon Reviews , including product metadata, user reviews to products, as well asvarious tags such as the number of upvotes received by each review, product-productrelations (also-buy, also-view), etc. Amazon QA , including user-generated questions and answers about products, as well asproduct reviews that may be related to the question. The goal of the dataset is to automaticallygenerate answers to user questions based on contexts in the reviews.",
  ". Commonsense Reasoning, which tests the models ability to infer and reason over common-sense knowledge of daily products (e.g. intended usage and purpose)": "3. Multi-hop Reasoning, which requires the model to draw connections across multiple entitiesand relations in online shopping to get the answer (e.g. similarity, compatibility, complementar-ity, etc.). User Behavior Alignment(\"Behavior\" for short). Accurately modeling user behaviors is a crucialskill in online shopping. Various kinds of user behaviors exist in online shopping, including queries,clicks, add-to-carts, purchases, etc. Moreover, these behaviors are generally implicit and not expressedin languages. Consequently, LLMs trained with general texts encounter challenges in aligning withthe heterogeneous and implicit user behaviors as they rarely observe such inputs during pre-training.We further design the following sub-skills, each of which focuses on a specific type of user behavior. 1. Queries. Most online shopping experiences starts with a user query that reflect the users initialintentions. Afterwards, the user may either initiate other related queries, or browse productsthat meet his intentions. We thus include two sub-skills corresponding to both scenarios,query-query relation and query-product relation.",
  ". Purchases, which focuses the models ability to help users directly make purchase decisionswithout the arduous process of searching and browsing": "4. Reviews and QAs, which requires the model to provide helpful feedbacks to various user-generated contents on an online shopping platform, such as answering product-related questions,and casting votes to informative reviews. Multi-lingual Abilities(\"Multi-lingual\" for short). We include sub-skills of multi-lingual shop-ping concept understanding, and multi-lingual user behavior alignment in this skill, which aremulti-lingual versions of the corresponding skills, respectively.",
  "Multiple Choice: We follow the HELM style of evaluating multiple choice questions.Specifically, we let the model generate one token and compare it with the answer to calculateaccuracy": "Retrieval: We let the model generate three comma-separated numbers (e.g. \"1, 2, 3\"), splitthe generation with comma, and compare the retrieved list with the ground truth to calculatehit rate@3. We set the number of retrieved instances as 3 because all retrieval tasks inShopping MMLU has fewer than 3 positive examples. Let retr denote the retrieved set ofinstances, and truth denote the ground truth, hit rate@3 is calculated as",
  "|truth|.(1)": "Ranking: Each ranking question is provided with a query and 5 candidate samples, andeach candidate is assigned a relevance score to the query. The model is asked to generate apermutation from 1 to 5, separated with comma (e.g. 1, 2, 3, 4, 5), which we will split withcomma and obtain the re-ranked list. Let ranki denote the i-th sample in the re-ranked list,rel() denote the relevance of the sample, NDCG is calculated as",
  "Task: Related Keywords Retrieval": "A user on an e-commerce platform has just made a query.The user wants to make another query with a shopping intention(narrowing, substitute, or complement).You are given a list of 15 numbered queries.Choose three queries that the user is most likely to makeaccording to the previous query and the intention.You should output three numbers, separated by comma. Do not give explanations.Previous Query: white cardigan for womenIntention: narrowingQuery List:1. white camisoles for women2. white jean jacket women3. white button down shirt women4. orange throw blanket5. white shrug for women6. white cardigan for women summer7. white cardigan for women short sleeve8. mattress cover full9. black cardigan for women10. tide free and gentle laundry detergent11. green bag12. platform crocs13. cream cardigan for women14. frog hat15. white cardigan for women dressyOutput:",
  "A.5Sample Prompts": "In this section, we show sample prompts for multiple choice, retrieval, ranking, named entityrecognition, and generation tasks in , 8, 9, 10, and 11, respectively. By default, we use numberchoices for multiple choice questions. However, as the numbers may be confused with decimal points,we use letter choices for tasks involving numeric reasoning (example shown in 12). All questions areevaluated with a prepended system prompt:",
  "Task: Query Product Ranking": "You are an intelligent shopping assistant that can rank products based on their relevance to the query.The following numbered list contains 5 products.Please rank the products according to their relevance withthe query super radio 3 amfm high-performance super radio.Product List:1. Wireless Bluetooth Speaker 4.0 Speaker Stereo Strong Enhanced Bass FM Radio MP3 Player (Gray)2. Monster Rockin Roller Charge Bluetooth Speaker3. Amazon Basics 16-Gauge Speaker Wire Cable, 100 Feet4. RCA RP7887 Super Radio 35. Amazon Basics 12 Pack D Cell All-Purpose Alkaline Batteries, 5-Year Shelf Life, Easy to Open Value PackYou should output a permutation of 1 to 5. There should be a comma separating two numbers.Each product and its number should appear only once in the output.Only respond with the ranking results. Do not say any word or explanations.Output:",
  "Task: Query Named Entity Recognition": "You are a helpful online shop assistant and a linguist.A customer on an online shopping platform has made the following query.Please extract phrases from the query that correspond to the entity type brand.Please directly output the entity without repeating the entity type.If there are multiple such entities, separate them with comma. Do not give explanations.Query: sigma lens for canonOutput:",
  "Product Category Generation. We remove all products where its product category existsin its title and metadata": "Aspect-based Sentiment Classification. In many cases, the sentiment towards an aspectexpressed in a review is mixed, i.e. there are both positive and negative mentions. Toavoid ambiguity, we select positive and negative samples as reviews that have solelypositive/negative mentions on an aspect. Aspect-sentiment-based Review Retrieval. Many reviews snippets are vague and canbe associated with many aspects, such as works great, looks good, nice product. Wemanually check the questions to filter out these vague review snippets. Attribute Value Extraction. The raw data includes attribute, attribute values, as well asproduct metadata. However, in many cases the attributes cannot be derived from the givenmetadata. We perform manual inspection to make sure that all attributes can be found in thegiven product information. Aspect-based Review Keyphrase Extraction. Many reviews mention an aspect more thanonce. To avoid ambiguity (and to reduce the task difficulty), we select reviews and aspects,such that the aspect is only mentioned once in the review.",
  "Task: Product Title Generation": "Please generate an adequate title for the product with the following descriptions.Product Descriptions: Quest Salted Caramel protein shakes are simply made with 11 ingredients.The end result is a delicious, naturally flavored shake that provides your body with 30g of protein,3 grams of carbs, and 1 gram of sugar. Our non-GMO shakes are custom-made and mixed to perfectionto ensure every sip is as delicious as your cravings. Each shake has 30g of protein, 3-4g carbsand 1g of sugar - and is naturally flavored and non-GMOOutput:",
  "Task: Product Numeric Reasoning": "The product MADHAVA Organic Light Agave, 46 oz. Bottle (Pack of 2) |100% Pure Organic Blue Agave Nectar | Natural Sweetener, Sugar Alternative |Vegan | Organic | Non GMO | Liquid Sweetener appears on e-commerce website.What is the total volume of the two bottles of agave nectar?(A) 25.4 fl oz(B) 202.8 fl oz(C) 26 fl oz(D) 92 fl ozPlease answer the question with a single letter indicating your choice.Answer:",
  "themselves are not accurate in some cases. We check for both types of noises and filterquestions accordingly": "Implicit Attribute Selection. The raw data for this task is derived from customer behaviors(i.e. common attributes of clicked products after a query), and thus is very noisy. Wemanually check for the validity of the implicit attributes. Session-based Query/Product Recommendation. We manually inspect all sessions andremove sessions with abrupt changes in shopping intentions. We empirically observe thatall models perform better after the filtering. Review Helpfulness Selection. We remove reviews with images and videos as theseadditional information also contributes to helpfulness. We also remove reviews that havean abnormal number of helpfulness votes (e.g. the most helpful review has >2000 votes,while the remaining ones have about 100 votes). We empirically observe consistently betterperformances after removing such reviews. Product-based Question Answering. This task is adapted from the Amazon QA dataset. AmazonQA has an answerability classifier predicting whether a question is answerablegiven the context information. However, we empirically find out that the classifier is oflimited precision, i.e. it marks lots of unanswerable questions as answerable. Therefore, wemanually inspect all questions and contexts and only include answerable questions.",
  "B.3Tasks with Negative Correlations": "We list tasks pairs with negative correlations in . We observe that all tasks pairs with negativescore correlations involve one generation task (underlined). Thus, we hypothesize that the negativecorrelations can be partially attributed to the metrics for generation tasks (i.e. sentence transformersimilarity). Indeed, for generation tasks, the reference text may not be perfect, and a generationdissimilar with the reference is not necessarily bad.",
  "We verify the hypothesis with a case study on the task of \"Attribute Naming from Description\" in": "1. In the example of \"Inside Diameter\", it is clear that ChatGPT performs the worst because itdid not closely follow the instructions. However, sentence transformer ranks it favorablyagainst all other models, probably due to the common 2-gram inside diameter. 2. In the example of \"Power Plug\", human evaluators generally prefer power plug type as itresembles the name of an attribute more. However, the reference text is power plug, andthus ranks it over power plug type, which goes against human preference. 3. In the example of Number of Pieces, human evaluators generally prefer quantity per unitas unit is mentioned in the description. However, the reference text (Number of Pieces)does not include the information of unit, and thus, the answer quantity per unit is rankedworst among all answers.",
  "LLaMA3 models generally benefit more from general domain IFT, which should be at-tributed to the better quality of IFT data": "Across all 4 skills, we observe that stronger base models generally benefit less from gen-eral domain IFT. In addition, among LLaMA2 models, we observe 3 cases where IFThurts the performances (LLaMA2-70B/Reasoning, LLaMA2-70B/Multi-lingual, LLaMA2-13B/Multi-lingual), showing that the stronger the base model is, the more likely generaldomain IFT will have a negative impact.",
  ": Results of in-context learning (0-, 1-, and 5-shot, with and without CoT) on representativereasoning tasks in Shopping MMLU": "In addition to ordinary few-shot prompting which simply puts question-answer pairs in the prompt,we also explore the effects of few-shot chain-of-thought (CoT) prompting. Specifically, wegenerate reasoning processes with GPT-4 and manually check their correctness, and put questions,reasoning processes, and final answers in the prompts. We apply CoT prompting on two reasoningtasks, Product Numeric Reasoning and Product Compatibility, and show results in . Weobserve that CoT prompting significantly boosts the performances on numeric reasoning (ProductNumeric Reasoning, (a)). However, its effects on implicit multi-hop reasoning (ProductCompatibility, (b)) is mixed, especially between 1-shot and 5-shot learning. Nonetheless,with 5-shot CoT prompting, all models achieve some improvements, showing that CoT prompting isgenerally helpful in enhancing the reasoning ability of LLMs, while the naive few-shot promptingfails.",
  "CFuture Work and Limitations": "With a comprehensive evaluation of LLMs in online shopping, Shopping MMLU opens up a broadhorizon of future work. Specifically, we highlight the room for improvement by showing that existingLLMs are still lagging behind task-specific state-of-the-art methods with three examples. Detailedresults are shown in . Aspect-based Sentiment Classification, which is a typical task in fine-grained understand-ing of user reviews. We compare state-of-the-art LLM solutions with the pre-trained modelin PyABSA 3. We only compare on reviews with positive and negative sentimentsas the other two choices (mixed, and the aspect is not mentioned) are not covered inPyABSA. As shown, the pre-trained model in PyABSA outperforms all proprietary LLMs. Query-product Relation Selection, which is a typical task in understanding user queriesand shopping intentions. We compare LLM solutions with an open-source solution fromKDD Cup 2022 4, which ranks 2nd in this task. As shown, the task-specific method",
  "outperforms all proprietary LLMs by a significant margin. We note that Shopping MMLUdata for this task are sampled from the test set of KDD Cup 2022, and thus there is no riskof data leakage": "Query-product Ranking, which is a crucial task in improving the browsing experience ofusers. We also compare LLM solutions with the solution from KDD Cup 2022 , whichranks 6th in this task. Similarly, all proprietary LLMs perform worse than the task-specificmethod. Similarly, as Shopping MMLU data is sampled from the test set of KDD Cup 2022,there is no risk of data leakage. Therefore, significant efforts are still needed to advance the performance of LLM-based multi-tasksolutions beyond task-specific ones in online shopping, such as more diverse continue pre-trainingand IFT datasets with higher quality. Another interesting direction is to build an LLM-based onlineshopping agent that adaptively routes a question to its corresponding task-specific method. In addition, as the characteristics of online shopping in , i.e. domain-specific concepts,implicit knowledge, human behaviors, and multi-linguality are not unique but apply to a widerange of specific domains (e.g. code , education , psychology , etc.), we believethat Shopping MMLU provides a testbed for future research and development efforts that builddomain-specific LLMs in general, such as data mixing strategies, mitigating catastrophic forgetting,knowledge-selective training, retrieval-augmented generation (RAG), etc. We also believe that theinsights uncovered in this work effectively lower the technological barrier of developing LLM-basedapplications, making it more accessible and inclusive to the community. We finally discuss the limitations of our work. First, we acknowledge that even though we performmanual inspections, label errors may still exist in Shopping MMLU due to subjective human knowl-edge, preferences, and behaviors. Second, Shopping MMLU primarily focuses on the purpose ofevaluation, and thus we do not provide a diverse IFT dataset in online shopping in this work. Weidentify an equally diverse IFT dataset as Shopping MMLU for future work. Finally, despite ourefforts to include as many tasks and skills as possible, our efforts are mostly limited to Amazondata. Therefore, Shopping MMLU, as well as the insights revealed may not accurately reflect onlineshopping behaviors in other platforms.",
  "(a) If your work uses existing assets, did you cite the creators? [Yes] .1 andAppendix A.2": "(b) Did you mention the license of the assets? [Yes] Appendix A.1.(c) Did you include any new assets either in the supplemental material or as a URL? [Yes](d) Did you discuss whether and how consent was obtained from people whose data youreusing/curating? [Yes] We obtain permission from the Amazon legal team, who confirmsthat the usage and curation of data are ethical."
}