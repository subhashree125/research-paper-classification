{
  "ABSTRACT": "The burgeoning volume of graph data presents significant com-putational challenges in training graph neural networks (GNNs),critically impeding their efficiency in various applications. To tacklethis challenge, graph condensation (GC) has emerged as a promis-ing acceleration solution, focusing on the synthesis of a compactyet representative graph for efficiently training GNNs while re-taining performance. Despite the potential to promote scalable useof GNNs, existing GC methods are limited to aligning the con-densed graph with merely the observed static graph distribution.This limitation significantly restricts the generalization capacity ofcondensed graphs, particularly in adapting to dynamic distributionchanges. In real-world scenarios, however, graphs are dynamic andconstantly evolving, with new nodes and edges being continuallyintegrated. Consequently, due to the limited generalization capac-ity of condensed graphs, applications that employ GC for efficientGNN training end up with sub-optimal GNNs when confrontedwith evolving graph structures and distributions in dynamic real-world situations. To overcome this issue, we propose open-worldgraph condensation (OpenGC), a robust GC framework that inte-grates structure-aware distribution shift to simulate evolving graphpatterns and exploit the temporal environments for invariance con-densation. This approach is designed to extract temporal invariantpatterns from the original graph, thereby enhancing the gener-alization capabilities of the condensed graph and, subsequently,the GNNs trained on it. Furthermore, to support the periodic re-condensation and expedite condensed graph updating in life-longgraph learning, OpenGC reconstructs the sophisticated optimiza-tion scheme with kernel ridge regression and non-parametric graphconvolution, significantly accelerating the condensation processwhile ensuring the exact solutions. Extensive experiments on bothreal-world and synthetic evolving graphs demonstrate that OpenGC",
  "Corresponding author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08...$15.00",
  "INTRODUCTION": "Graph data is used to represent complex structural re-lationships among various entities and has enabled applicationsacross a diverse range of domains, such as chemical molecules, social networks , and recommender systems. However, the exponential growth of data volume inthese applications poses significant challenges in data storage, trans-mission, and particularly the training of graph neural networks(GNNs) . These challenges become more pronouncedin scenarios that require training multiple GNN models, such asneural architecture search , continual learning , and fed-erated learning . Consequently, there is a pressing need for moreefficient methodologies for processing large-scale graph data. In re-sponse, graph condensation (GC) has emerged, aimingto synthesize a compact (e.g., 1, 000 smaller) yet informative graphthat captures essential characteristics of the large original graph.The condensed graph enables fast training of numerous GNNs bear-ing different architectures and hyper-parameters, while ensuringtheir performance is comparable to the ones trained on the originalgraph. As such, in the deployment stage, these trained GNNs candirectly perform inference on the original graph to support variousdownstream tasks.Despite the potential to accelerate model training, current GCmethods still fall short in their real-world practicality. By default,conventional GC methods are subsumed under a static setting,which requires that a large graph is firstly condensed to facilitateGNN training on the small graph, and the trained GNNs are then",
  "Taobao": ": The upper panel presents the evolution of the graph.The graph T expands as tasks progress. Varying colors ofnodes represent distinct classes. The lower panel shows thetest accuracy of consecutive tasks on the Yelp and Taobaodatasets. The test model is GCN, which is trained on thecondensed graph of the initial task1 and applied to evaluatesubsequent tasks without fine-tuning. The test set expandsfollowing the tasks, and the evaluation is limited to the nodesbelonging to the classes in 1. deployed on the same large graph for testing. Unfortunately, as-suming the large graph stays unchanged throughout the entireprocess severely contradicts the dynamic and evolving nature ofgraph-structured data in the real world. In fact, graphs in manyhigh throughput applications are inherently open-world ,where new nodes and classes continuously emerge and are inte-grated into the existing graph structure, as depicted in . Thisphenomenon is exemplified in citation networks , wheresome new papers explore established topics, while others ventureinto emerging areas. These papers in the novel areas are often de-veloped from previous studies and cite a range of related literature.Such addition of nodes often introduces novel patterns that aredistinctly different from those previously observed, increasing thegraphs complexity and diversity. Consequently, in addition to pre-serving performance on the initial large graph, GNNs trained oncondensed graphs are expected to exhibit adaptability to novel pat-terns that emerge within dynamic environments. For instance, GCfor neural architecture search initially condenses a snapshotof the original graph, thereafter employing the condensed graphto accelerate the searching procedure and identify optimal modelarchitecture. However, the evolving nature of graphs leads to adiscrepancy between the model deployment environment and thecondensed graph snapshot. Therefore, optimal model architectureidentified based on condensed graphs should sustain its superiorperformance over time, even as the graph evolves. In a nutshell,the critical problem that arises for GC in open-world scenarios is:How can GC methods be adapted to handle the dynamic nature ofevolving graphs, ensuring that GNNs trained on condensed graphsremain accurate and robust in the face of continual changes in graphstructures? Specifically, the application of GC within open-world graph en-vironments encounters two primary challenges. The first challengearises from the distribution shift caused by the constant additionof new nodes, which may either belong to existing categories orintroduce entirely new classes. On the one hand, these nodes typi-cally exhibit distributions that diverge from existing ones. On theother hand, their integration, particularly when involving novelclasses, can modify the distribution patterns of previously observednodes through their connections. However, the condensed graphcreated by existing GC methods, intended to act as a simple datasimulator capturing a static view of the original graphs distribu-tion, inherently constrains the generalization capacities of GNNstrained on these graphs to distribution shifts. To assess the effect ofnewly added nodes to GC, we simulate the deployment of GC ontwo real-world, progressively evolving graphs: Yelp and Taobao1.The GCN is trained on the condensed graph of the initial task1 and applied to evaluate distinct test sets in subsequent taskswithout fine-tuning. As depicted in , there was a noticeabledecline in the classification performance of test nodes in each task,highlighting the distribution shift caused by newly added nodesand the limited adaptability of GC within dynamic graphs.In the meantime, the second challenge involves intensive com-putation in the condensation process. The continuous addition ofnew nodes and subsequent distribution changes necessitate peri-odic re-condensation to refresh and realign the condensed graphwith evolving data distributions. However, the condensation pro-cess is often complex and slow to converge , resulting in atime-consuming procedure that hampers efficient life-long graphdata management. In GC approaches, the process begins by en-coding both the large original graph and the condensed graphthrough a relay model. Subsequently, the relay model, along withthe condensed graph, is updated utilizing a nested loop optimiza-tion strategy. This GC paradigm is implemented iteratively untilconvergence is achieved. However, the iterative encoding of thelarge-scale original graph, as well as the sophisticated nested loopoptimization, inherently demands substantial time and extensivecomputational resources. Consequently, these intensive computa-tions pose a significant obstacle to the serving of GC in evolvingand life-long graph systems.In light of these challenges, we propose a novel graph conden-sation approach, Open-wold graph condensation (OpenGC), facili-tating trained downstream GNNs to handle evolving graphs in theopen-world scenarios. To tackle the distribution shift issue, we pro-pose temporal invariance condensation that incorporates invariantlearning to preserve the invariant patterns across different tem-poral environments in the condensed graph. To this end, a temporaldata augmentation technique is designed to simulate the graphsevolving pattern and exploit the structure-aware distribution shiftby referring to the historic graph. It specifically considers the sim-ilarity of node pairs and targets the low-degree nodes, which aremore susceptible to the influence of newly added neighbors. Bythis means, OpenGC can significantly enhance the adaptability ofdownstream GNNs by training them on the temporally generalizedcondensed graph, thus eliminating the need to laboriously design",
  "Graph Condensation for Open-World Graph LearningKDD 24, August 2529, 2024, Barcelona, Spain": "specific generalization modules for each GNN. In response to the ex-isting sophisticated condensation procedure, we design an efficientGC paradigm that combines Kernel Ridge Regression (KRR) withnon-parametric graph convolution, circumventing the nested loopoptimization and heavy graph kernel encoding in other KRR-basedGC methods . Consequently, OpenGC is well-suited for man-aging life-long graph data and handling the continuous growth anddynamic changes of open-world graphs. The main contributions ofthis paper are threefold: New problem and insights. We are the first (to the best of ourknowledge) to focus on the practical deployment issue of GCin the evolving open-world graph scenario and point out thenecessity of learning temporally generalized condensed graphs,which is an important yet under-explored problem in GC. New methodology. We present OpenGC, a GC method that ex-plores the temporal invariance pattern within the original graph,endowing the condensed graph with temporal generalizationcapabilities. Additionally, OpenGC employs an efficient conden-sation paradigm, significantly improving the condensation speedand enabling prompt updates to the condensed graph. SOTA performance. Through extensive experimentation onboth real-world and synthetic evolving graphs, we validate thatOpenGC excels in handling distribution shifts and accelerat-ing condensation procedures, surpassing various state-of-the-art(SOTA) GC methods in performance.",
  "AD 1": "2 is the normalized adjacency matrix. A representsthe adjacency matrix with the self-loop, D denotes the degree matrixof A, and W() is the trainable weights at layer . For simplicity,we denote an -layer GNN encoder as H = (T), where H denotesthe final node embeddings utilized in downstream tasks. In thispaper, we concentrate on the node classification task, where H isfurther input into the classifier (). Consequently, the entire modelis denoted as = , encapsulating both the GNN encoder andthe classifier.",
  "Graph condensation aims to generate a small synthetic graphS = {A, X} with A R , X R as well as its label": "Y R , where . The model trained on S can achievecomparable node classification performance to the one trained onthe much larger T. To facilitate the connection between real graphT and synthetic graph S, a relay model = parameterized by is employed in the optimization process for encoding both graphs.We initially define the loss for T and S about the parameter as:",
  "(4)": "where 0 represents the initial parameters of the relay model, sam-pled from a specific distribution . The expectation on 0 aimsto improve the robustness of S to different parameter initializa-tion . D (, ) is the distance measurement. opt () is the modelparameter optimizer and the parameters of the relay model areupdated only on S. The structure of condensed graph A is parame-terized by similarity in node features and modeled by a multi-layerperceptron (MLP) as A = MLP(X).Limited distribution generalization of GC. The optimizationobjective of GC aims to align the performance of GNNs trainedon the condensed graph closely with those trained on the originalgraph. However, such objective is solely performance-driven andtakes into account merely a snapshot of the original graphs distri-bution. Consequently, GNNs trained on the condensed graph arerestricted to adapting to the condensed distribution and struggleto accommodate the dynamic distribution changes encounteredduring deployment.Intricate optimization in GC. The optimization procedure ofGC is notably intricate, owing to two primary factors. Firstly, theoptimization objective in Eq. (4) necessitates simultaneous updatesof the relay model and the condensed graph S. To tackle thisdemand, GC employs a nested loop optimization strategy, updating within the inner loop and optimizing S in the outer loop. Nev-ertheless, this bi-level optimization approach not only intensifiesthe computational demands but also introduces additional hyper-parameters, thus adding complexity to the optimization processand impeding the attainment of optimal solutions. Secondly, theupdating or periodic initialization of the relay model leads to therepetitive encoding of the original graph throughout the condensa-tion process. Due to the neighbor explosion problem, the computa-tional load for encoding the expansive original graph is substantial,and the repeated execution of the encoding operation exacerbates",
  "Problem Formulation": "In the open-world graph scenario, we consider a sequential streamof node batches {1,2, ...,}, where represents the total num-ber of batches involved and each node batch accompanied by corre-sponding edges. These node batches are progressively accumulatedand integrated into the existing graph over time to construct thetasks {1,2, ...,}, as depicted in . At task, the snapshotgraph T includes all nodes that have emerged and the graphexpands as tasks progress, i.e., T T+1. Correspondingly, thenumber of node class for task increases over time, satisfying +1.In this dynamic content, we consider the practical applicationscenario for both GC and GNN deployment. During the GC phase attask, we initially annotate a subset of the newly added nodes in T,addressing the continual integration of new classes. Subsequently,the condensed graph S is generated for the large graph T andemployed to train GNNs efficiently. In the deployment phase, GNNstrained on S are equipped with the open-set recognition method for deployment on subsequent tasks T . This approach enablesthe recognition of nodes from new classes as the unknown class,while nodes from observed classes in T are categorized into theirappropriate classes. When significant structural changes occur, suchas an influx of new class nodes or deteriorating GNN performance,the GC procedure is repeated to keep the condensed graph alignedwith the latest original graph.In this paper, our primary objective centers on enhancing boththe efficiency and the generalization ability of GC across differentdistributions. The exploration of advanced open-set recognitionmethods falls outside the scope of this study.",
  "METHODOLOGIES": "We hereby present our proposed open-world graph condensation(OpenGC). We begin with the foundation GC paradigm, which isan efficient GC approach to generate the condensed graph S forthe static original graph T. Then we move on to temporal environ-ment generation by exploring the structure-aware distribution shift.Finally, the generated environments are integrated to facilitate thetemporal invariance condensation and the pipeline of OpenGC isdepicted in .",
  "KRR-based Feature Condensation": "Based on our earlier discussions, the rationale for the intensive com-putational demand of the condensation process is the deficiencyof the relay model = , which requires iterative optimizationwithin the inner loop and repetitive encoding of the original graph.To address these challenges, we propose discarding the conven-tional relay model, which pairs GNNs with classifiers. Instead, weintroduce a novel relay model that integrates non-parametric con-volution with KRR to significantly improve the efficiency of staticgraph condensation. For the sake of simplicity, the task subscript is omitted in this subsection. Specifically, we first transform the classification task into a re-gression problem by replacing the classification neural network with KRR. The corresponding loss function in Eq. (2) is formulatedas:LT () = Y (T)W2 + W2 ,",
  "LS() =Y (S)W2 + W2 ,(5)": "where W is the learnable matrix in KRR, || || is the 2 norm, is aconstant, and ()W is the prediction of the labels. Accordingly,the condensation objective in Eq. (4) is substituted by Eq. (6) toensure that the regression model trained on S attains performancecomparable to the one trained on the T.",
  "WS = (S)T (S) (S)T + I1Y,(7)": "where I is the identity matrix. The reduction in graph size leads to aninversion matrix of dimensions , ensuring the computationremains efficient.In addition to the classifier , the graph encoder () in con-ventional GC leverages GNNs that follow the message-passingparadigm , involving the stacking of numerous propagationand transformation layers and leading to the iterative encodingissue. To alleviate this problem, we decompose the propagationand transformation processes within (), and then employ non-parametric graph convolution to encode the original graphduring the pre-processing stage. The layers convolution of theoriginal graph and the condensed graph is calculated as:",
  "(T) = (H), (S) = (H),(9)": "where () is the transformation layer parameterized by . Fi-nally, we use the identity matrix as the predefined adjacency matrixas previous works , i.e, A = I, to further simplify the con-densed graph modeling and condense the original graph structurein the node attributes. The benefit of this design is two-fold. Firstly,the predefined structure eliminates the training of the adjacencymatrix generator. Secondly, it circumvents the encoding of the con-densed graph in the condensation procedure. When conducting thedownstream tasks, the identity matrix is leveraged as the adjacencymatrix to train various GNNs.",
  "Open-set recognition": ": The pipeline of OpenGC. The graph T and historic graph T1 are encoded by non-parametric convolution andembeddings are leveraged to construct temporal environments H . The condensed graph embedding H is generated accordingto temporal invariance condensation loss L. In the deployment stage, the condensed graph is utilised to train multipleGNNs with various architectures, which are applied to sequential tasks T . condensed graph. Given the unavailability of future graph struc-tures and distributions, we refer to the historic graph and constructvarious temporal environments by simulating potential future dis-tribution shifts.Specifically, we calculate the residuals by comparing the currentembeddings at task with the embeddings from the last task 1.The residual for the node is calculated as:",
  "H, = H, H1,,(10)": "where H, and H1, are embeddings of node at task and 1,respectively. As the graph evolves, the neighbors of node are ex-tended, and the node embedding changes from H1, to H, aftergraph convolution. Therefore, H, formulates the added neigh-bors in convolution and indicates the structure-aware distributionshift in task , which can be leveraged to augment other nodeembeddings. Then, we randomly select node , which belongs tothe same class as node , and use its residual H, to modify theembedding H,. The motivation hinges on the assumption thatnodes in the same class follow a similar distribution shift pattern2.",
  "H, = H, + , H,,(11)": "where H, is the normalized residual and , controls the mag-nitude. To determine the value of ,, we take into account thecharacteristics of the target nodes and the node responsiblefor generating the residual. Firstly, a higher weight is assigned tothe target node if it has a lower degree, with the premise thatnodes with fewer connections are more prone to being influenced,leading to a pronounced distribution shift upon the addition of newneighbor nodes. Moreover, the similarity between node and isassessed to promote a more significant distribution shift amongsimilar nodes. Consequently, the magnitude is constructed as:",
  "We use the drop edge and drop feature to construct the environments if no historicclasses are available": "where is the hyper-parameter to control the base magnitudeand cosine() measures the consine similarity. is the degree cal-ibration term sampled from the Beta distribution: Beta( , 1), where denotes the degree of node , repre-sents the calibration constant. This introduces the randomness formultiple environments generation and a higher increasesthe probability of sampling a smaller .Finally, we produce several environments E for invariancecondensation, with the embeddings for each environment denotedby H = [ H,1; ...; H, ].",
  "Temporal Invariance Condensation": "With the definition of efficient graph condensation and multipleenvironment representations, we represent the final objective oftemporal invariance condensation.At task , we first pre-compute the embeddings of the originalgraph H and various environments H . To enhance the stabilityof the optimization, we incorporate a learnable temperature tocalibrate the prediction of KRR and substitute the regression losswith cross-entropy loss. The loss for the original graph embeddingH and condensed graph embedding H is expressed as:",
  "EXPERIMENTS": "We design comprehensive experiments to validate the efficacy ofour proposed OpenGC and explore the following research questions:Q1: Compared to the SOTA GC methods, can OpenGC achieve bet-ter temporal generalization performance? Q2: Can the OpenGCcondense the graph faster than other GC approaches? Q3: How dothe different components, i.e., IRM and different constraints usedin environment generation affect OpenGC? Q4: Can the condensedgraph generated by OpenGC generalize well to different GNN ar-chitectures? Q5: How do the different hyper-parameters affect theOpenGC? Q6: What are the characteristics of the visualization ofcondensed nodes?",
  "Experimental Settings": "Datasets. In dynamic real-world scenarios, newly added nodes of-ten belong to both existing and novel classes and available datasetsfail to fully capture this dynamic nature. Consequently, we con-struct four real-world graph datasets to evaluate our proposedmethod: Yelp 3, Taobao 4, Flickr and Coauthor . Follow-ing , we construct the Yelp and Taobao according to the naturaltimestamps to align with temporal sequences. For the Flickr andCoauthor datasets, which do not include temporal information, wesegment the data into tasks using randomly selected classes andnodes. Without loss of generality, we guarantee that in subsequenttasks, newly added nodes not only introduce new classes but alsobelong to all classes observed in earlier tasks. All datasets are ran-domly divided, with each task following a consistent split ratio:60% for training, 20% for validation, and 20% for testing. Moreover,the training, validation, and testing sets are continually expandedbased on the sets from previous task. The detailed descriptions ofdatasets and tasks are provided in Appendix A.3.Baselines and Evaluation Settings. We compare our proposedmethods to three SOTA GC methods: (1) GCond : the first GCmethod that employs the gradient matching and bi-level optimiza-tion techniques; (2) GCDM : an efficient GC method that utilizesdistribution matching to align the class distributions; (3) SFGC :a structure-free GC method that pre-trains a large number of GNNmodels and utilizes trajectory matching to improve the optimizationresults. To enable GNNs be capable of recognizing novel classes, we incorporate two open-set recognition techniques into each base-line: (1) Softmax : it adds the softmax as the final output layerand sets the threshold to classify the low-confidence nodes as theunknown class. (2) Openmax : it assesses the probability of anode being an outlier via Weibull fitting and then calibrates theoutput logits, enabling the identification of novel classes with thepredefined threshold.We evaluate each GC method on evolving graphs containing aseries of tasks {1,2, ...,}. At task , the graph T is condensedby GC methods, and the condensed graph is then used to traindownstream GNNs. The performance of these GNNs is evaluatedacross all subsequent tasks from to . Therefore, an uppertriangular performance matrix M R is maintained, wherethe element M, represents the accuracy of a GNN trained on thecondensed graph from task and tested on task ( ). Toquantify the overall performance of models trained on condensedgraphs, we use the mean of average performance (mAP) as theevaluation metric as follows:",
  "=M,).(16)": "For a fair comparison, we follow the conventional graph conden-sation for the condensation setting. For all compared baselines,we employ SGC , which uses non-parametric convolution, asthe relay model and use the identity matrix as the adjacency matrix.The downstream GNN is GCN unless specified otherwise.To assess the ability of GC to generalize across different GNNarchitectures, we conduct evaluations using a variety of models,including GCN , SGC , GraphSAGE and APPNP .We condense the original graph with nodes into a condensedgraph with nodes and the compress ratio is calculated by /.We choose the compress ratio of Yelp, Taobao, and Coauthor to be{1%, 2%}. For the larger dataset Flickr, we choose the compress ratioas {0.1%, 0.2%}.Hyper-parameters and Implementation. The hyper-parametersfor baselines are configured as described in their respective papers,while others are determined through grid search on the valida-tion set of the condensed task. For all datasets, a 2-layer graphconvolution is employed, and the width of the hidden layer in is set to 1024. The regularization parameter for KRR is fixed at5e-3. The learning rate for the condensation process is determinedthrough a search over the set {1e-1, 1e-2, 1e-3, 1e-4}. The parameters and are optimized from the set {0.1, 0.3, 0.5, 0.7, 1} to achieve anoptimal balance between the losses. The intervention magnitudeparameter is explored within the range {0.01, 0.1, 1, 10, 100} toidentify the most appropriate level of intervention. The number ofenvironments considered is varied from 1 to 5 to find the optimalsetting. The calibration constant used in Beta distribution is setas 10. Finally, the number of training epochs is determined usingthe early stop to prevent overfitting.Due to the absence of new class nodes in the validation set, wefollow previous work to set the threshold for open-set recog-nition approaches, configuring it to exclude 10% of the validationnodes as unknown class. To eliminate randomness, we repeat eachexperiment 5 times and report the average test score and standarddeviation.",
  "Condensed task": ": The heatmap of the differences between performance matrix (%) of OpenGC and SFGC on Yelp, Taobao, Flickr andCoauthor datasets (from left to right). Softmax is adopted and the compress ratios are 1%, 1%, 0.1%, and 1%, respectively. We use the ADAM optimization algorithm to train all the models.The codes are written in Python 3.9 and the operating system isUbuntu 16.0. We use Pytorch 1.12.1 on CUDA 11.4 to train modelson GPU. All experiments are conducted on a machine with Intel(R)Xeon(R) CPUs (Gold 6128 @ 3.40GHz) and NVIDIA GeForce RTX2080 Ti GPUs.",
  "Accuracy Comparison (Q1)": "We report the mAP for different GC methods with standard de-viation in . In this table, Whole indicates that GNNs aretrained on the original graph, achieving the highest performance.However, it suffers from substantial computational costs due to thelarge scale of the original graph. Compared to Whole, GC methodsmaintain similar performance levels even under extreme compressrate on Yelp, Taobao, and Coauthor datasets, confirming the ef-fectiveness of GC. On the Flickr dataset, the performance gap islarger compared to other datasets, attributing to the datasets signif-icant imbalance issue. When comparing the different GC methods,performance differences emerge. For example, GCond achievescomparable performances with GCDM across different datasets andcompress ratios. Benefiting from the advanced trajectory matchingstrategy, SFGC achieves consistent performance improvement com-pared to GCond and GCDM. This is due that trajectory matchingcan provide more precise optimization guidance for GC procedurecompared to gradient and distribution matching and significantlyenhance the quality of condensed graphs. Nonetheless, all thesemethods only condense the snapshot of the original graph and pre-serve the static graph information in the condensed graph, whichlimits their performance under the dynamic graph scenarios. Ourproposed OpenGC consistently outperforms other baselines. Re-markably, with substantial compress rates, OpenGC achieves results comparable to the Whole on the Taobao and Coauthor under theSoftmax setting. Furthermore, in , we present a detailedheatmap of the differences between the performance matrix M ofOpenGC and the strongest baseline SFGC. The observed patternreveals a deep red coloration along the diagonal line, indicating asignificant enhancement in performance on condensed tasks. Asthe graph evolves, the color gradually lightens and the incrementgradually weaken. This phenomenon is attributed to the increasingnumber of classes, which makes the task more complicated. Whencomparing the different compress ratios, OpenGC guarantees supe-rior GNN performance at lower compress ratios compared to thehigher compress ratios achieved by other GC methods. All theseresults underscore the efficacy of our proposed temporal invariancecondensation and the superior optimization results from the exactsolution provided by KRR.",
  "Condensation Time Comparison (Q2)": "Forods by measuring the condensation time for the largest graph inthe final task on each dataset. Besides our proposed method, wealso assess the performance of OpenGC without the environmentgeneration and invariant learning (OpenGC w/o IRM) and theresults are presented in . This assessment includes measur-ing the pre-processing time, the average condensation time per100 epochs, and the total condensation time (pre-processing timeexcluded). The corresponding accuracy results and time complexityare reported in and Appendix A.2. Firstly, the pre-processingtime of GCond, GCDM, and OpenGC w/o IRM contains the timeof non-parametric graph convolution in Eq. (8). OpenGC addition-ally involves the environment generation time. In contrast, SFGCdemands the training of hundreds of GNN models on the original",
  "KDD 24, August 2529, 2024, Barcelona, SpainXinyi Gao et al": ": The comparison of condensation time (sec.) for different GC methods. The pre-processing time (Pre. time), averagecondensing time per 100 epochs (Avg. time), and total condensing time (Total time) are measured separately. The condensationtarget is the largest graph in the final task and the compress ratios are 1%, 1%, 0.1%, and 1% for 4 datasets respectively. <1indicates that the time duration is less than 1 second.",
  "YelpTaobaoFlickrCoauthor": "OpenGC45.980.5369.810.5438.110.8273.000.60w/o IRM44.250.2869.310.1536.300.8071.780.11w/o TEG44.990.1669.470.6436.540.6372.060.40w/o degree45.170.1669.630.1936.680.3572.510.11w/o similarity45.690.3969.510.7437.270.1472.470.13 graph to serve as teacher models and provide training trajectories.This step is computationally demanding and time-consuming, sig-nificantly exceeding the time taken for condensation. Moreover,training a large number of GNNs contradicts the core motivationbehind GC, which aims for efficient training of multiple GNNs usingcondensed graphs. The average condensing time per 100 epochsreflects the complexity of the condensation process. GCond uti-lizes gradient matching to align the original graph and condensedgraph, necessitating the additional back-propagation steps for gra-dient computation with respect to model parameters. In contrast,GCDM alters the optimization objective and leverages the distribu-tion matching, eliminating the gradient calculation and achieving afaster condensation procedure. Nonetheless, GCDM still engages innested loop optimization for relay model updates. Similarly, SFGCrequires multiple updates to the relay model based on the condensedgraph to resemble the training trajectories of the teacher model.Moreover, it utilises the graph neural tangent kernel to evaluate thequality of the condensed graph, which is computationally intensiveand further increases the condensing time. Our method exhibits the highest condensation speed. Compared to OpenGC withoutIRM, the time required for environment generation in OpenGCis minimal. For example, the pre-processing on Yelp and Taobaotakes less than 1 second. Despite the larger feature dimensionsof Flickr and Coauthor, the pre-processing time remains under 10seconds. Although GCDM also demonstrates rapid condensation,its accuracy is considerably lower than our proposed method, asdetailed in .",
  "Ablation Study (Q3)": "To validate the impact of individual components, OpenGC is testedwith Softmax, selectively disabling certain components. We evalu-ate OpenGC in the following configurations: (1) without the IRMloss (w/o IRM); (2) without the temporal environment generation(w/o TEG), using edge and feature drop to randomly generateenvironments instead; (3) without the degree constraints (w/o de-gree); (4) without the similarity constraint (w/o similarity). Theresults of these settings are presented in . The removal ofIRM loss leads to a noticeable decline in accuracy, underscoringthe importance of both temporal environment generation and theinvariance constraint in improving the generalization capabilitiesof the condensed graph. The significance of temporal environmentgeneration is further highlighted by the performance drop in w/oTEG. Although new environments are generated, w/o TEG doesnot simulate the graph expanding pattern, failing to predict thefuture graphs and leading to the sub-optimal results. Further anal-ysis is conducted on the degree and similarity constraints duringtemporal environment generation. The performance in these casesis better than w/o TEG but does not reach the levels achieved byOpenGC, reinforcing the importance of incorporating node charac-teristics to enhance the environment generation.",
  "Generalizability for GNN Architectures (Q4)": "A critical attribute of GC is its ability to generalize across differ-ent GNN architectures, making the condensed graph versatile fortraining various GNN models in downstream tasks. Therefore, weevaluate different GNN models on the condensed graph, includingGCN, SGC, GraphSAGE, and APPNP. These models are then appliedto subsequent tasks and the mAP of different datasets are presentedin . According to the results, all evaluated GNN modelswere effectively trained using GC methods, achieving comparablelevels of performance. In detail, GCN and SGC exhibited superiorperformance due to these two models utilise the same convolutionkernel as the relay model. GraphSAGE performed exceptionally",
  "Hyper-parameter Sensitivity Analysis (Q5)": "The hyper-parameters and are leveraged to control the impactof the IRM loss during training, while and the number of environ-ments contribute to the extent of augmentations. In , wepresent the mAP performances w.r.t. different values of , , andthe number of environments, respectively. For , we observe thatvalues in the range of 0.3 to 0.7 yield the best performance. Highervalues may excessively weigh the IRM loss, potentially compro-mising the preservation of original graph information. Similarly,selecting an optimal value is crucial for maximizing performanceacross different datasets. Regarding , a value of 10 ensures a magni-tude balance of the residual to the original embeddings. Finally, theincrease of the number of environments can enhance performanceand it should be controlled to avoid introducing excessive noise.",
  "CONCLUSION": "In this paper, we present open-world graph condensation (OpenGC),a robust graph condensation approach that enhances the tempo-ral generalization capabilities of condensed graph. OpenGC ex-ploits the structure-aware distribution shift in the evolving graphand extracts invariant features in the original graph for temporalinvariance condensation. Moreover, OpenGC optimizes the con-densation procedure by combining kernel ridge regression andnon-parametric graph convolution, successfully accelerating thecondensation progress. Benefiting from the superiority of the gen-eralization capacity of condensed graphs and efficient optimizationprocedure, OpenGC not only enhances the GNNs ability to handlethe dynamic distribution change in real-world scenarios but alsoexpedites the condensed graph updating in life-long graph learning. This work is supported by Australian Research Council under thestreams of Future Fellowship (Grant No. FT210100624), DiscoveryEarly Career Researcher Award (Grants No. DE230101033), Discov-ery Project (Grants No.DP240101108 and No.DP240101814).",
  "Xinyi Gao, Junliang Yu, Wei Jiang, Tong Chen, Wentao Zhang, and Hongzhi Yin.2024. Graph condensation: A survey. arXiv preprint arXiv:2401.11720 (2024)": "Xinyi Gao, Wentao Zhang, Tong Chen, Junliang Yu, Hung Quoc Viet Nguyen, andHongzhi Yin. 2023. Semantic-aware node synthesis for imbalanced heterogeneousinformation networks. In Proceedings of the 32nd ACM International Conferenceon Information and Knowledge Management. 545555. Xinyi Gao, Wentao Zhang, Junliang Yu, Yingxia Shao, Quoc Viet Hung Nguyen,Bin Cui, and Hongzhi Yin. 2023. Accelerating scalable graph neural networkinference with node-adaptive propagation. arXiv preprint arXiv:2310.10998 (2023).",
  "Chuanxing Geng, Sheng-jun Huang, and Songcan Chen. 2020. Recent advancesin open set recognition: A survey. IEEE transactions on pattern analysis andmachine intelligence 43, 10 (2020), 36143631": "Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George EDahl. 2017. Neural message passing for quantum chemistry. In InternationalConference on Machine Learning. PMLR, 12631272. Zhichun Guo, Kehan Guo, Bozhao Nan, Yijun Tian, Roshni G Iyer, Yihong Ma,Olaf Wiest, Xiangliang Zhang, Wei Wang, Chuxu Zhang, et al. 2022. Graph-basedmolecular representation learning. arXiv preprint arXiv:2207.04869 (2022).",
  "William L. Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive Represen-tation Learning on Large Graphs. In Advances in Neural Information ProcessingSystems. 10241034": "Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, BowenLiu, Michele Catasta, and Jure Leskovec. 2020. Open Graph Benchmark: Datasetsfor Machine Learning on Graphs. In Advances in Neural Information ProcessingSystems. Wei Jin, Xianfeng Tang, Haoming Jiang, Zheng Li, Danqing Zhang, Jiliang Tang,and Bing Yin. 2022. Condensing Graphs via One-Step Gradient Matching. InProceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery andData Mining. 720730.",
  "Jing Long, Tong Chen, Quoc Viet Hung Nguyen, Guandong Xu, Kai Zheng,and Hongzhi Yin. 2023. Model-agnostic decentralized collaborative learning foron-device POI recommendation. In SIGIR. 423432": "Jing Long, Tong Chen, Quoc Viet Hung Nguyen, and Hongzhi Yin. 2023. Decen-tralized collaborative learning framework for next POI recommendation. ACMTransactions on Information Systems 41, 3 (2023), 125. Noel Loo, Ramin Hasani, Alexander Amini, and Daniela Rus. 2022. Efficientdataset distillation using random feature approximation. Advances in NeuralInformation Processing Systems 35 (2022), 1387713891.",
  "Qiying Pan, Ruofan Wu, Tengfei Liu, Tianyi Zhang, Yifei Zhu, and WeiqiangWang. 2023. FedGKD: Unleashing the Power of Collaboration in Federated GraphNeural Networks. arXiv (2023)": "Liang Qu, Huaisheng Zhu, Ruiqi Zheng, Yuhui Shi, and Hongzhi Yin. 2021. Im-gagn: Imbalanced network embedding via generative adversarial graph networks.In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &Data Mining. 13901398. Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H.Lampert. 2017. iCaRL: Incremental Classifier and Representation Learning. In2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017,Honolulu, HI, USA, July 21-26, 2017.",
  "Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and StephanGnnemann. 2018. Pitfalls of graph neural network evaluation. arXiv preprintarXiv:1811.05868 (2018)": "Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, and Jihong Guan. 2023. All in one:Multi-task prompting for graph neural networks. In Proceedings of the 29th ACMSIGKDD Conference on Knowledge Discovery and Data Mining. 21202131. Xiangguo Sun, Hong Cheng, Bo Liu, Jia Li, Hongyang Chen, Guandong Xu,and Hongzhi Yin. 2023. Self-supervised hypergraph representation learningfor sociological analysis. IEEE Transactions on Knowledge and Data Engineering(2023).",
  "Ying-Xin Wu, Xiang Wang, An Zhang, Xiangnan He, and Tat-Seng Chua. 2022.Discovering invariant rationales for graph neural networks. arXiv preprintarXiv:2201.12872 (2022)": "Zhe Xu, Yuzhong Chen, Menghai Pan, Huiyuan Chen, Mahashweta Das, HaoYang, and Hanghang Tong. 2023. Kernel Ridge Regression-Based Graph DatasetDistillation. In Proceedings of the 29th ACM SIGKDD Conference on KnowledgeDiscovery and Data Mining. 28502861. Yu Yang, Hongzhi Yin, Jiannong Cao, Tong Chen, Quoc Viet Hung Nguyen,Xiaofang Zhou, and Lei Chen. 2023. Time-aware dynamic graph embedding forasynchronous structural evolution. IEEE Transactions on Knowledge and DataEngineering (2023). Gilad Yehudai, Ethan Fetaya, Eli Meirom, Gal Chechik, and Haggai Maron. 2021.From local structures to size generalization in graph neural networks. In Interna-tional Conference on Machine Learning. PMLR, 1197511986. Hongzhi Yin, Liang Qu, Tong Chen, Wei Yuan, Ruiqi Zheng, Jing Long, XinXia, Yuhui Shi, and Chengqi Zhang. 2024. On-Device Recommender Systems: AComprehensive Survey. arXiv preprint arXiv:2401.11441 (2024). Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Vik-tor K. Prasanna. 2020. GraphSAINT: Graph Sampling Based Inductive LearningMethod. In International Conference on Learning Representations.",
  "AAPPENDIXA.1Algorithm of OpenGC": "The detailed algorithm of OpenGC is shown in Algorithm 1. Inpre-processing stage, we first encode both the original graph attask and 1 by using non-parametric graph convolution. Then,multiple environments are generated according to calculated em-beddings. In condensation stage, we sample a initialization for () from the distribution . Then, the KRR solution WS is cal-culated according to the condensed graph. Finally, the embeddingsof constructed environments and original graph are condensed vialoss L.",
  "A.2Time Complexity Analysis": "We show the detailed time complexity of OpenGC and comparedbaselines in . Time complexities for both the pre-processingand condensing phases are assessed separately and the condensingprocedure is further divided into the forward propagation, processexecution, loss calculation, condensed graph updating, and relaymodel updating. The relay model for all methods is layer SGCincorporating a linear layer with the hidden dimension denoted by. For the original graph, , , and are the number of nodes,edges, feature dimensions, and classes, respectively. The numberof nodes in the condensed graph is represented by and thepre-defined adjacency matrix is utilized across all methods.The pre-processing stage for GCond and GCDM incorporatesnon-parametric graph convolution. SFGC entails the training ofhundreds of teacher models and the quantity is denoted by .OpenGC incorporates environment generation during pre-processingphase, introducing an additional time complexity of .Due to the different optimization strategies utilized in GC meth-ods, we decompose the condensing procedure into 5 stages. Theprocess execution stage varies between methods, involving dif-ferent operations specific to each method. Specifically, GCondsprocess entails calculating the gradient w.r.t the relay model pa-rameters twice. GCDMs procedure involves computing the classrepresentation. SFGC necessitates updating the relay model onthe condensed graph times to generate trajectories during eachiteration. OpenGC introduced the KRR for the closed-form solu-tion, with the time complexity being O 3 + . Considering and eliminating the relay model updating, our proposedmethod achieves a more efficient condensation procedure comparedto other baselines.",
  "Return: Condensed graph S": "Yelp 5 is a large business review website where people can uploadtheir reviews for commenting business, and find their interestedbusiness by others reviews. According to reviews, we constructa business-to-business temporal graph. Specifically, we take thedata from 2016 to 2021, and treat the data in each year as a task,thus forming 6 tasks in total. In each year, we sample the 2 largestbusiness categories as classes in each task. The newly added nodesin later tasks will cover all observed classes in former task classes.We regard each business as a node and set the businesss categoryas its node label. The temporal edge will be formed, once a userreviews the corresponding two businesses within a month. Weinitialize the feature representation for each node by averaging300-dimensional GloVe word embeddings of all reviews for thisbusiness following the previous work .Taobao 6 is a large online shopping platform where items can beviewed and purchased by people online. For the Taobao dataset, weconstruct an item-to-item graph, in the same way as Yelp. The datain the Taobao dataset is a 6-day promotion season of Taobao in 2018.The data in each day is treated as a task and we sample 3 largestitem categories in each task. We regard the items as nodes and takethe categories of items as the node labels. The temporal edge willbe built if a user purchases 2 corresponding items in the promotionseason. We use the 128-dimensional embedding provided by theoriginal dataset as the initial feature of the node.Flickr is an image network where each node in the graphrepresents one image. If two images share some common properties(e.g., same geographic location, same gallery, comments by the sameuser, etc.), an edge will be established between these two images.All nodes are classified into 7 classes and node features are the 500-dimensional bag-of-word representations. We randomly choose 2classes as the initial task and each subsequent task adds a new class.Coauthor is a co-authorship graph. Nodes represent authorsand are connected by an edge if they co-authored a paper. Nodefeatures represent paper keywords for each authors papers, andclass labels indicate the most active fields of study for each author.",
  "A.4Related Work": "Graph condensation. Graph condensation is designed to reduceGNN training costs through a data-centric perspective and mostGC methods focus on improving the GNN accuracy trained onthe condensed graph. For example, SFGC introduces trajec-tory matching in GC and proposes to align the long-term GNNlearning behaviors between the original graph and the condensedgraph. GCEM focuses on improving the performance of dif-ferent GNN architectures trained on the condensed graph. It cir-cumvents the conventional relay GNN and directly generates theeigenbasis for the condensed graph to preserve high-frequencyinformation. Besides improving the quality of condensed graphs,GC has been widely used in various applications due to its excellentgraph compression performance, including inference acceleration, continual learning , hyper-parameter/neural architecturesearch and federated learning . Although GC is developedon various applications, none of them focus on the evolution ofgraphs in real-world scenarios. Our proposed method is the first to explore this practical problem and contains its significance in GCdeployment.Invariant learning for out-of-distribution generalization. Theinvariant learning approaches are proposed to reveal invariantrelationships between the inputs and labels across different dis-tributions while disregarding the variant spurious correlations.Therefore, numerous methods are developed to improve the out-of-distribution (OOD) generalization of models, which refers to theability to achieve low error rates on unseen test distributions. Forexample, Invariant Risk Minimization improves the empiricalrisk minimization and includes a regularized objective enforcing si-multaneous optimality of the classifier across all environments. RiskExtrapolation encourages the equality of risks of the learnedmodel across training environments to enhance the model sensitiv-ity to different environments. Recent works utilize invariant learn-ing in OOD graph generalization problem , and themost critical part of them is how to design invariant learning tasksand add proper regularization specified for extracting environment-invariant representations. However, these methods are all model-centric and only concentrate on enhancing the GNN model toextract the invariant features. In contrast, our proposed methodintroduces invariant learning in the data-centric GC method, en-abling the GNNs trained on the condensed graph all contain theOOD generalization."
}