{
  "Abstract": "Identifying anomalies from time series data plays an important rolein various fields such as infrastructure security, intelligent oper-ation and maintenance, and space exploration. Current researchfocuses on detecting the anomalies after they occur, which can leadto significant financial/reputation loss or infrastructure damage. Inthis work we instead study a more practical yet very challengingproblem, time series anomaly prediction, aiming at providing earlywarnings for abnormal events before their occurrence. To tacklethis problem, we introduce a novel principled approach, namelyfuture context modeling (FCM). Its key insight is that the futureabnormal events in a target window can be accurately predicted iftheir preceding observation window exhibits any subtle differenceto normal data. To effectively capture such differences, FCM firstleverages long-term forecasting models to generate a discriminativefuture context based on the observation data, aiming to amplifythose subtle but unusual difference. It then models a normality cor-relation of the observation data with the forecasting future contextto complement the normality modeling of the observation datain foreseeing possible abnormality in the target window. A jointvariate-time attention learning is also introduced in FCM to lever-age both temporal signals and features of the time series data formore discriminative normality modeling in the aforementioned twoviews. Comprehensive experiments on five datasets demonstratethat FCM gains good recall rate (70%+) on multiple datasets andsignificantly outperforms all baselines in 1 score. Code is availableat",
  "Introduction": "Identifying anomalies from time series data is highly demanded inpractice . Detecting anomalies that have already occurredis valuable, but the occurrence of the abnormal events can leadto significant financial/reputation loss or infrastructure damage.Accurately predicting these anomalies in advance, i.e., having earlywarnings of the anomalies, can effectively mitigate such adverseeffects. Time series anomaly prediction holds significant impor-tance across various fields. For instance, in the monitoring of criti-cal infrastructure systems, anomaly prediction ensures the safetyand stability of water treatment systems by preventing abnormalevents such as water supply/pollution accidents ; in machineoperation and maintenance , it provides early warningsfor potential failures in servers, hard drives, and other equipment,enabling preemptive measures to avoid financial/reputation lossesdue to the failures; in space exploration, it alerts potential opera-tional/logistic issues beforehand, providing time for handling theissues early, thereby guaranteeing smooth progression of explo-ration missions . Thus, this work focuses on addressing theanomaly prediction problem.Numerous methods have been introduced for time series anom-aly detection (TSAD), such as reconstruction-based methods , contrastive learning-based methods , one-class classification methods , and graph neural network-based methods . TSAD focuses on detecting the abnormalevents after they occur, so they assume that clear abnormal patterns",
  "1": ": Illustration of the key insight of FCM. For effectiveanomaly prediction, FCM assumes that there are subtle signs(in the bottom time series) at the observation window forfuture abnormal events in a target window. These signs aretypically too subtle to be detected by TSAD models. FCMaims to leverage long-term time series forecasting models toamplify these signs and associate them as a future contextwith the observation temporal signals for accurate anomalyprediction. exist in the observational time series data, rendering them ineffec-tive for anomaly prediction. This is because the signs of havingabnormal events in the future time points are typically subtle, suchas the data in in .To tackle this challenge, we introduce a novel principled ap-proach, namely future context modeling (FCM). Its key insightis that the future abnormal events in a target window can be ac-curately predicted if their preceding observation window exhibitsany subtle difference to normal data. To effectively capture suchdifferences, FCM first leverages long-term forecasting (LTTSF) mod-els to generate a discriminative future context basedon the current time window data, aiming to amplify those subtlebut unusual difference. This is because i) the LTTSF techniques canoften accurately predict normal time points, resulting in smooth,accurate forecasting time points, but ii) they are difficult to ac-curately forecast the abnormal time points, leading to fluctuated,exceptional time points. For example, as shown in , at thenormal time points , the forecasting output given 1 does notexhibit any abnormality, but it starts behaving abnormally at +1when observing where subtle sign of anomalies is presented.The unusual forecasting signals at +1 on the top help largelyamplify the subtle abnormality signs at current window .These forecasting time points are treated as future contexts,which are then utilized by FCM to model its normality correlationwith the observation data for foreseeing possible abnormality inthe target window. That is, FCM takes both the current observationtime points and its forecasting time points as joint input, uponwhich a joint data reconstruction model is built. Due to the subtle anomaly signs in the current time points and their amplificationvia the forecasting, the joint reconstruction error is expected to belarge if the future target window contains abnormal time points,and it would be small otherwise. Additionally, FCM also adds amodule that individually model the current time points so as tocapture any sign of future abnormality without being affected bythe forecasting time points. Thus, FCM foresees future abnormalitythrough two different yet complementary views: one view is onmodeling any subtle abnormality sign at the current time pointswhile another view is based on the normality correlation betweenthe current and forecasting time points.Furthermore, to support a more discriminative reconstructionfor future normal and abnormal time points, we propose a multi-dimensional self-attention module to obtain correlations from boththe feature dimension and the temporal dimension. The subtleanomaly signs often have difficulty establishing correlations withthe future abnormal time points in at least one of these two dimen-sions, while the normal time points are strongly correlated at boththe feature and temporal dimensions. Thus, the multi-dimensionalself-attention approach helps increase the difficulty in the joint datareconstruction when the future time window contains abnormaltime points. By contrast, it enables better data reconstruction whenthe future time points are all normal.Our contributions are summarized as follows:",
  "We explore an important yet under-explored problem, timeseries anomaly prediction, aiming to promote a more practi-cal setting for anomaly identification in time series data": "We then propose the novel anomaly prediction approachFCM that aims to learn and leverage future context to am-plify subtle abnormality signs at the observation windowand provide more discriminative features for anomaly pre-diction. To our best knowledge, this is the first work thatmodels the normality correction between the observationand forecasting time points for anomaly prediction. We further introduce a variate-time multi-dimensional self-attention module, in which the correlation of time seriesdata can be effectively modeled from both of the feature andtemporal dimensions, enabling a more discriminative jointmodeling of the current and forecasting time points in FCM. We establish evaluation protocols using five widely-usedTSAD datasets and perform comprehensive experimentsto compare our FCM with 16 anomaly prediction methods.FCM gains good recall rate (70%+) on multiple datasets andconsitently outperforms the competing methods in 1 score.",
  "Related Work2.1Time Series Anomaly Detection": "Studies on identifying anomalies in time series data are focusedon the task of detection rather than prediction. Numerous meth-ods have been introduced in this line of research, which can beroughly categorized into the following five groups. 1) ClassicalMethods: Classical methods are not specific to time series data butare generally applicable to all data types, such as OCSVM , iFor-est , and DAGMM . 2) Reconstruction-based Methods:The main idea of reconstruction-based methods is to learn the man-ifold of normal classes. When encoding and reconstructing data using autoencoders or other methods, anomalies cannot be properlyreconstructed due to their significant differences from the normalmanifold. Many reconstruction architectures have been introduced. For example, Anomaly Transformer (Anom-Trans) targets the correlation of consecutive times with a trans-former network, discovering weak relationships between anomaliesand the entire sequence. Methods such as DA-VAE , PUAD ,and MEMTO learn diverse normal patterns via prototype- ormemory-augmented reconstruction. 3) Contrastive Learning-basedMethods: These methods learn temporal normal patterns via con-trastive learning, such as DCdetector and CTAD . 4) GraphNeural Network-based Methods: They learn the structure of exist-ing relationships between variables using graph neural networksand distinguish anomalies by predicting the future values of eachsensor, such as GDN . 5) Generative Adversarial Network-basedMethods: The methods in this group focus on utilizing adversarialtraining or adversarially generated time series data to train thedetection models, such as USAD and BeatGAN .",
  "Time Series Forecasting": "Long-term time series forecasting (LTTSF) is a classic task in timeseries analysis. It involves extracting the core patterns embeddedin extensive data and estimating changes over a long period in thefuture. In recent years, numerous studies have attempted to applytransformer models to LTTSF . For example, Informer aimsto adopt distillation techniques together with self-attention to ef-fectively extract the most crucial time points for the forecasting.Autoformer draws on ideas from traditional time series analysismethods and incorporates decomposition and auto-correlation intothe network. FEDformer uses a Fourier-enhanced structure toachieve linear complexity. PatchTST treats patches as inputunits, preserving the semantics of each block in the time seriesdata, thereby utilizing the transformer structure more effectivelyto achieve good results. Subsequent works have mostly followedthe patch concept. iTransformer embeds each time series asvariable tokens, employing the attention mechanism to handlemultivariate correlations and using a feed-forward network forsequence encoding. It adopts a reversed perspective on time series,embedding the entire time series of each variable independentlyinto tokens, thereby expanding the local receptive field. These fore-casting models may be adapted for anomaly prediction via a simplereconstruction module, but they lack the designs that focus onlearning the prevalent patterns from the data, leading to ineffec-tive anomaly prediction. Limited work has been done on anomalyprediction. Jhin et al. explore the detection of early signs ofabnormality in time series data to provide a unified frameworkfor anomaly detection and prediction, but its future prediction isrestricted to very short time period, leading to a task similar toanomaly detection.",
  "Slide step": ": The studied setting. Given an observational timewindow from to +1, we aim to predict whether there wouldbe abnormal events in the target window from +1 to +2. Thetwo windows are slid with a fixed step size (see Sec. 3.1). We first formalize the anomaly prediction task for multivariatetime series data. As shown in , a sliding window is lever-aged. Let observation window be = [1, . . . ,], where = + 1. The window size is and the sliding step is . The obser-vation window serves as the input to our neural network. The nextnon-overlapping window [ (+1), . . . , (2) ] is the target window.Given an input observation window , our network determineswhether each time point in the target window is an anomaly. Thereare a total of ( )/ + 1 observation windows. The periodbefore an anomaly occurs in the target window is called operationwindow, in which we take actions to mitigate the impact if the pre-diction results have anomalies. The length of the actionable timeis adjusted by the sliding step size . Note that the step size needsto be smaller than the window size to ensure sufficient actionabletime.",
  "Overview of FCM": "This paper proposes a novel time-series anomaly prediction methodFCM. To achieve accurate abnormal event prediction, it leverageslong-term forecasting models to generate a discriminative futurecontext for modeling its normality correlation with the data inthe observation window. As shown in , FCM contains twoimportant components: a future context-aware anomaly predictionmodule and a joint variate-time (var-time) attention module. Theanomaly prediction module consists of normality modeling fromtwo views. The first view is to model the normality associationof observation window data with a forecasting future context toachieve accurate abnormal prediction through joint data reconstruc-tion. Another view is on the normality modeling of the observationdata to capture any abnormality signs without being affected bythe forecasting output. The var-time attention module constrainsanomalies by modeling from the temporal signals and feature di-mensions of time series data, increasing the reconstruction differ-ence between normal and abnormal. Below we introduce the detailsof each module.",
  "Joint Variate-Time Attention": ": Overview of FCM. The future context-aware anomaly prediction in FCM leverages the output of the forecasting taskto enhance the representation of the current window, generating a new discriminative representation of the observation datafor joint normality modeling. The var-time attention module learns the relationships between temporal signals and featurevalues of time series data to enable more discriminative normality learning. abnormality signs at the observation window. Thus, we design aLTTSF module in FCM. This module predicts the target windowby learning the continuous changes in two adjacent windows inthe training data with normal data. For current window , theforecasting results for the target window through can be definedas (+1): (2) = (1:; ), with the parameters op-timized using a mean squared error (MSE) loss:",
  ".(1)": "As shown in , although the forecasting module has signifi-cant errors in predicting the abnormal signals, there are discrimi-native forecasting outputs for normal and abnormal time points inthe target windows, i.e., smooth, accurate forecasting for normaldata signals in the target window vs. fluctuated, exceptionalforecasting results for abnormal data signals in the target window+1. Therefore, the forecasting result (+1): (2) can be seen asan amplifier that leverages the subtle abnormality signals in theobservation window to generate a discriminative future contextat +1. FCM then models the normality correlation between theobservation window data and its forecasting future context via alinear layer parameterized by through joint observation-futurecontext signal reconstruction:",
  "where [] means a concatenation of the two inputs and = (; )refers to the reconstruction of by a decoder parameterized by": "Our LTTSF module can predict accurately for most normal points.Thus, the observation-future context data consisting of (1): ()and (+1): (2) are strongly correlated, if (1): () does not con-tain future abnormality signs. This correlation is broken otherwise,since the forecasting output (+1): (2) would not follow specificdistribution patterns. As a result, the reconstruction for normalevents in the target window would be small, and it would be largefor future abnormal events.Additionally, we also learn the normality pattern from the obser-vation window individually to model the original sign anomaloussignals. This is to complement the normality modeling from theobservation-future context view. It is done by a usual reconstructionof the observation data :",
  "Joint Variate-Time Attention Mechanism": "In Sec. 3.3, we use data reconstruction to model the normality pat-terns from two different views. However, traditional reconstructionmethods may also reconstruct subtle anomaly signals in both views.To increase the difficulty of the data reconstruction that involvesabnormal signals, we propose a joint variate-time (var-time) at-tention mechanism. This mechanism learns data correlations fromboth the temporal and feature dimensions, making it difficult toestablish connections for current windows that exhibit anomalies in at least one dimension, thereby enabling more accurate anomalyprediction.First, we set the self-attention from the variate dimension. Fol-lowing the early work iTransformer , time series of each sensoris embedded into a dimension space as a token, in whichthe original data can be defined as = {0 ,1 , ..., } . We then feed the embedded information into the en-coder of the transformer. Each head of the multi-head attentionutilizes W, W, W to transform X into Q, K, V Rby matrix multiplication, with",
  "O = AV R,(7)": "which is used to enable our time series forecasting in Sec. 3.3.Correspondingly, we feed the original data to realizeanomaly prediction in the temporal dimension. It is defined as = {0 ,1 , ..., } after values of all sensors atthe same moment are embedded into -dimension space. Weestablish the anomaly prediction module based on the correlationbetween time points. Since the anomaly detection and the predictionmodules share one encoder, we can obtain Q, K, V R ,which is the same as in the forecasting module:",
  ") R.(9)": "O = AV R, which we use it to build the reconstruc-tion modules in in Sec. 3.3.This self-attention mechanism helps learn the correlations of thetemporal and feature dimensions at the same time, so O containsboth temporal and variate corresponding, make it more discrimi-native of the reconstruction errors between the normal and subtleabnormal in both reconstruction modules of our future context-aware anomaly prediction.",
  "Anomaly Prediction Using FCM": "Training. We use a sequential updating strategy to provide feed-back for the three losses involved in the task. Specifically, we updatethe network using L and L for a period of time before incor-porating L. This approach ensures that the forecasting modulesresults are relatively stable. If we incorporate L from the start, thelarge errors in time series forecasting results could adversely affectthe forecasting results.Inference. Given the observation window data (1): () , we ob-tain the forecasting result of the target window from our LTTSF module. We then concatenate the forecasting future context withthe observation data and embed it into a low-dimensional space.We use the embedding as the representation of the observationdata. The reconstruction error obtained from this representation isused as the anomaly score for anomaly prediction:",
  "Server Machine Dataset (SMD) is a five-week datasetcollected from a large Internet company, featuring 38 di-mensions and resource utilization access traces from 28 ma-chines": "Mars Science Laboratory dataset (MSL) is collected byNASA and contains sensor and actuator data from the Marsrover . It includes 55 dimensions and 27 entities, featuringtelemetry anomaly data derived from event surprise anomaly(ISA) reports from the spacecraft monitoring system. Soil Moisture Active Passive dataset (SMAP) is also col-lected by NASA and includes soil samples and telemetryinformation from the Mars rovers . It has 25 dimensionsand, compared to MSL, contains more point anomalies. Safe Water Treatment dataset (SWaT) is a 51-dimensionalsensor-based dataset collected from continuously operatingcritical infrastructure systems . It includes 11 consecu-tive days of data: 7 days of normal operation and 4 days ofperiodic attacks, totaling 41 attacks. The time granularity is1 second.",
  "PSM (Pooled Server Metrics Dataset) is a public dataset fromeBay server machines with 25 dimensions": "4.1.2Competitors. We employ sixteen anomaly detectors as com-peting methods, including the classic anomaly detection methods(OCSVM , iForest , DAGMM , and THOC ), gener-ative methods (BeatGAN and COUTA ), reconstruction-based methods (OmniAnomaly , InterFusion , LSTM-ED ,MSCRED , AnomTrans , and DCdetector ), graph neuralnetwork-based methods (GDN ), and time series analysis-basedmodels (FEDformer , Autoformer , and TimesNet ).Among these methods, thirteen anomaly detection models canbe directly applied to anomaly prediction by modeling normal-ity on the observation window data. Additionally, the remainingthree time series analysis-based models are modified and adaptedfor anomaly prediction via a data reconstruction module on theforecasting data. 4.1.3Evaluation Metrics. Time-series anomaly prediction presentsdistinct challenges compared to the well-established anomaly detec-tion task, necessitating a novel evaluation protocol. Our approachfocuses on the ability to provide early warnings of impending anom-alies, rather than merely identifying them post-occurrence. To eval-uate the predictive capability, we designate the ground-truth labels : Precision , Recall , and 1 score of anomaly prediction results on five multivariate real-world datasets. All results arerepresented in percentages (%). The best performance for each metric on each dataset is highlighted in bold, and the runner-up1 score are underlined.",
  "FCM (Ours)98.4060.5049.8680.1565.9346.8264.2445.06": "of the window immediately preceding each true anomaly as anoma-lous. Also, for continuous anomalies, we only consider the firstwindow in which the anomaly manifests. Subsequent anomaloussamples within the same sequence are omitted from evaluation.Based on prediction results and the adjusted ground-truth labels,we employ the point adjustment strategy and then calculate theprecision , recall , and 1 score . Additionally, consid-ering the imperfection of the point adjustment strategy, we incor-porate recent metrics specifically designed for time-series anomalydetection, including Affiliation Precision/Recall (Aff-P/Aff-R) ,Range-AUC-ROC/PR (R-A-R/R-A-P) , and Volume Under theSurface ROC/PR (V_ROC/V_PR).",
  "FCM (Ours)81.3692.0195.1195.5596.81": "of early warning of the future abnormal events. We select the anom-aly score threshold for identifying anomalies according to Anom-Trans . The number of channels in the hidden state is512 by default and the number of heads is 8. We use the Adamoptimizer with an initial learning rate of 104. For different datasets,we introduce the prediction feedback module into training at dif-ferent iterations. Specifically, this module is introduced at 30,000iterations for SMD, 4,000 for SMAP, 15,000 for SWaT, 2,500 for MSL,and 5,000 for PSM. All experiments were implemented in PyTorchand run on a single NVIDIA GeForce RTX 3090 GPU. : Ablation study results. Bare AP only uses an anomaly prediction module without both future context and the variate-time attention. Bare Fore only uses a long-term forecasting model. w/o Att uses two separate self-attention modules. w/o Lremoves the future context reconstruction module.",
  "Main Results": "We conduct our anomaly prediction approach and sixteen com-peting models on five multivariate real-world datasets. shows the precision, recall, and 1 score of our method FCM andits contenders. Please note that this experiment assesses anom-aly prediction results, wherein each target windows evaluation isbased on predictions from the preceding window, diverging fromthe conventional anomaly detection paradigm. Therefore, this taskis extremely challenging, resulting in generally lower 1 scoresacross all methods. Nevertheless, FCM still significantly outper-forms competing methods on all five datasets, demonstrating thesuperiority in anomaly prediction. Notably, FCM gains remarkablerecall rates, with an average exceeding 70% across five datasets.Compared to the best-performing existing methods, our approachachieve 10%, 14.5%, 9.2%, 7.9%, and 21.4% 1 score improvementacross the five datasets. In particular, on the PSM dataset, FCMillustrates significant improvement compared to the competitors,realizing an average improvement of 48.8% in 1 score. Amongthe baseline methods, BeatGAN and AnomTrans exhibit notewor-thy prediction performance. BeatGANs strength lies in capturinghigh-order patterns of normal data through adversarially generatedsamples, while AnomTrans leverages its anomaly discriminationmethod to excel in criterion correlation difference. Nonetheless, ourmethod still illustrates consistent superiority, mainly attributed toits ability to identify subtle yet unusual differences in precedingwindows and its advantage in forecasting future anomalies. Sincethe point adjustment strategy of time series anomaly detection iscontroversial, some studies propose new evaluation metrics, such asaffiliation precision/recall and the volume under the surface (VUS),which are considered to provide a more objective assessment ofdetection performance. reports the results of new indica-tors, in which we focus on the comparison between FCM and twobest-performing contenders under traditional metrics (see in the appendix C for the results of other methods). The empiricalresults suggest that FCM can also outperform these competitorsacross most evaluation scenarios.We further relax the evaluation criteria by incorporating thepreviously omitted anomaly segments as introduced in .1.3.Specifically, we consider the prediction results to be accurate if theysuccessfully trigger an alert within the extended temporal window.It can be observed from that our method also significantlyoutperforms existing state-of-the-art anomaly detectors, averagelyachieving 32.3%, 21.9%, 24.3%, 22.2%, and 17.0% improvement across five datasets, respectively. These empirical results underscore theefficacy of FCM in the common anomaly detection task, showcasingthe strong ability of FCM in capturing both subtle and substantialabnormal signals in the respective observation and target windows.",
  "Ablation Study": "This experiment aims to validate the effectiveness of several keymodules in our FCM model. By removing components of FCM,we create four ablated variants, including Bare AP, Bare Fore,w/o L, and w/o Att. presents the performance of theseablated variants against the full FCM model. We have the followingfindings: Bare AP uses a bare anomaly prediction module withoutfuture context or var-time attention. We use AnomTrans toaccomplish this variant. Bare AP shows a performance gap ofan average of 15.6% in 1 score across five datasets, comparedto the full FCM model, demonstrating the ineffectiveness ofmodeling the subtle normality in the observation windowonly. Bare Fore utilizes a long-term forecasting model to gener-ate predictions for the observation window, which are thenused directly for anomaly detection. Results indicate that theprediction results also contain key features for identifying fu-ture anomalies. It is worthwhile to enhance the identificationof future anomalies from future context. The w/o Att variant represents the complete form of ourmodel, yet utilizing two separate self-attention modules forthe anomaly prediction and time series forecasting. Com-pared to FCM, this variant demonstrates an average improve-ment of 9.2% in 1 score. It can be seen that the shared self-attention mechanism effectively learns multi-dimensionalnormality correlation information, leading to further im-proved anomaly prediction performance. The w/o L variant incorporates both the time series fore-casting module and the anomaly prediction module. In thisconfiguration, our anomaly prediction module and time se-ries forecasting module share the var-time attention mecha-nism. This variant, on five datasets, improves the 1 scoreby 6.5% on average compared to FCM. This underscores theimportance of future contextual information in obtainingdiscriminative representations of future anomalies.In conclusion, the final row in represents the completeform of our method, incorporating all four modules. The inclusion of both the anomaly prediction and time series forecasting modulesallows the model to leverage future context and learn the normalityfrom two complementary views, resulting the superior anomalyprediction results.",
  "Case Study": "To demonstrate the efficacy of our anomaly prediction method,we conduct a comparative case study to complement quantitativeanomaly detection results. This study leverages three distinct typesof anomalies commonly encountered in time series data, includinggroup anomalies, shapelet anomalies, and trend anomalies, as re-spectively shown in (a)(b)(c). These cases are sampled fromthe SWaT dataset. For each sub-figure, the upper panel displays theoriginal time series data. The middle panel illustrates the anomalyscores (in blue) predicted by our approach FCM and the threshold(represented by a red dashed line). The bottom panel depicts theprediction labels and the original detection labels. FCM success-fully assigns significantly high anomaly scores before the actualanomalies occur in these three typical cases, thereby offering valu-able early warnings. FCM can perceive subtle abnormality signs,during which the interactions between current observational timepoints and its forecasting time points are effectively modeled. Un-like traditional methods that detect anomalies only after they havemanifested, FCM ensures a proactive rather than passive response.The ability of preemptive detection is crucial as it allows for timelyintervention and mitigation of the abnormal events.",
  "Hyper-parameter Sensitivity": "We conduct sensitivity analysis to investigate the impact of hyper-parameters on anomaly prediction performance, i.e., this experi-ment varies the setting of several hyper-parameters and reportsthe corresponding 1 scores. Specifically, we investigate three keyhyper-parameters in FCM, including the window size, the slidingwindow step size, and the number of attention heads. (a) illustrates the effect of varying window sizes on theprediction results. Note that the size of both observation windowand target window is adjusted concurrently. Generally, a largerwindow is advantageous as it encapsulates more comprehensivetemporal semantics, whereas increased window size correlates withhigher computational complexity and more challenging forecastingprocess. FCM exhibits optimal performance stability within the100-250 samples range. (b) shows the impact of different sliding step sizes on pre-diction performance. For fixed-size windows, a smaller sliding stepcan extend the effective operational time, increasing the temporalresolution of the analysis. On the contrary, a larger sliding stepresults in fewer learning windows, potentially increasing predictiondifficulty due to reduced data overlap. FCM demonstrates relativelystable performance on PSM, SMD, and SwAT within the 30-70 stepsize range, but a gradual performance degradation can be observedon the MSL and SMAP datasets. The optimal temporal granular-ity might vary across datasets, reflecting differences in underlyingtemporal dynamics and anomaly characteristics. (c) elucidates the impact of the number of var-time atten-tion heads on the model performance. The MSL dataset is sensitiveto variations in the number of attention heads, while other datasets demonstrate relative stability across different configurations. Inalignment with standard Transformer architectures, an increasednumber of attention heads generally brings enhanced informationcapture capacity. Following typical setting, we also determine thateight attention heads represent an optimal configuration.",
  "Conclusion": "This paper introduces a novel method for time series anomalyprediction, FCM. It models future context for accurate anomalyprediction by learning to capture subtle abnormality signs in theobservational time points and their correlations with forecastingtime points. FCM achieves this by modeling the correlation betweenthe current and future windows through a time series forecastingmodule, in which the forecasting future time points act as the fea-ture amplifier of the current window. This helps largely enhancethe expressiveness of the abnormal features. It also introduces ajoint variate-time attention module to simultaneously learn the cor-relations between time series and between the features. Under theconstraints of the two correlations, future anomalies become moredifficult to reconstruct, resulting in easier differentiation betweenfuture normal and abnormal time points. The effectiveness of FCMis justified by extensive experiments on five real-world datasetscompared to 16 state-of-the-art competing methods. Ahmed Abdulaal, Zhuanghua Liu, and Tomer Lancewicki. 2021. Practical ap-proach to asynchronous multivariate time series anomaly detection and localiza-tion. In Proceedings of the 27th ACM SIGKDD conference on knowledge discovery& data mining. 24852494. Julien Audibert, Pietro Michiardi, Frdric Guyard, Sbastien Marti, and Maria AZuluaga. 2020. Usad: Unsupervised anomaly detection on multivariate time series.In Proceedings of the 26th ACM SIGKDD international conference on knowledgediscovery & data mining. 33953404.",
  "Manish Gupta, Jing Gao, Charu C Aggarwal, and Jiawei Han. 2013. Outlierdetection for temporal data: A survey. IEEE Transactions on Knowledge and dataEngineering 26, 9 (2013), 22502267": "Alexis Huet, Jose Manuel Navarro, and Dario Rossi. 2022. Local evaluation oftime series anomaly detection algorithms. In Proceedings of the 28th ACM SIGKDDConference on Knowledge Discovery and Data Mining. 635645. Kyle Hundman, Valentino Constantinou, Christopher Laporte, Ian Colwell, andTom Soderstrom. 2018. Detecting spacecraft anomalies using lstms and nonpara-metric dynamic thresholding. In Proceedings of the 24th ACM SIGKDD interna-tional conference on knowledge discovery & data mining. 387395. Sheo Yon Jhin, Jaehoon Lee, and Noseong Park. 2023. Precursor-of-anomaly detec-tion for irregular time series. In Proceedings of the 29th ACM SIGKDD Conferenceon Knowledge Discovery and Data Mining. 917929. Xin Jie, Xixi Zhou, Chanfei Su, Zijun Zhou, Yuqing Yuan, Jiajun Bu, and HaishuaiWang. 2024. Disentangled Anomaly Detection For Multivariate Time Series. InCompanion Proceedings of the ACM on Web Conference 2024. 931934. Ming Jin, Huan Yee Koh, Qingsong Wen, Daniele Zambon, Cesare Alippi, Geof-frey I Webb, Irwin King, and Shirui Pan. 2024. A survey on graph neural networksfor time series: Forecasting, classification, imputation, and anomaly detection.IEEE Transactions on Pattern Analysis and Machine Intelligence (2024).",
  "by point/sequential reconstruction. Advances in Neural Information ProcessingSystems 36 (2024)": "Yuxin Li, Wenchao Chen, Bo Chen, Dongsheng Wang, Long Tian, and MingyuanZhou. 2023. Prototype-oriented unsupervised anomaly detection for multivariatetime series. In International Conference on Machine Learning. PMLR, 1940719424. Zhihan Li, Youjian Zhao, Jiaqi Han, Ya Su, Rui Jiao, Xidao Wen, and Dan Pei. 2021.Multivariate time series anomaly detection and interpretation using hierarchicalinter-metric and temporal embedding. In Proceedings of the 27th ACM SIGKDDconference on knowledge discovery & data mining. 32203230.",
  "Lifeng Shen, Zhuocong Li, and James Kwok. 2020. Timeseries anomaly detectionusing temporal hierarchical one-class network. Advances in Neural InformationProcessing Systems 33 (2020), 1301613026": "Junho Song, Keonwoo Kim, Jeonglyul Oh, and Sungzoon Cho. 2024. Memto:Memory-guided transformer for multivariate time series anomaly detection.Advances in Neural Information Processing Systems 36 (2024). Ya Su, Youjian Zhao, Chenhao Niu, Rong Liu, Wei Sun, and Dan Pei. 2019. Robustanomaly detection for multivariate time series through stochastic recurrentneural network. In Proceedings of the 25th ACM SIGKDD international conferenceon knowledge discovery & data mining. 28282837. Xue Wang, Tian Zhou, Qingsong Wen, Jinyang Gao, Bolin Ding, and RongJin. 2024. CARD: Channel Aligned Robust Blend Transformer for Time SeriesForecasting. In The Twelfth International Conference on Learning Representations. Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing Ma, Junchi Yan,and Liang Sun. 2023. Transformers in time series: a survey. In Proceedings of theThirty-Second International Joint Conference on Artificial Intelligence. 67786786.",
  "Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and MingshengLong. 2022. Timesnet: Temporal 2d-variation modeling for general time seriesanalysis. arXiv preprint arXiv:2210.02186 (2022)": "Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. 2021. Autoformer: De-composition transformers with auto-correlation for long-term series forecasting.Advances in neural information processing systems 34 (2021), 2241922430. Jiang Xiao, Zhuang Xiong, Song Wu, Yusheng Yi, Hai Jin, and Kan Hu. 2018. Diskfailure prediction in data centers via online learning. In Proceedings of the 47thInternational Conference on Parallel Processing. 110. Yanwen Xie, Dan Feng, Fang Wang, Xuehai Tang, Jizhong Han, and XinyanZhang. 2019. Dfpe: Explaining predictive models for disk failure prediction. In2019 35th Symposium on Mass Storage Systems and Technologies (MSST). IEEE,193204. Haowen Xu, Wenxiao Chen, Nengwen Zhao, Zeyan Li, Jiahao Bu, Zhihan Li,Ying Liu, Youjian Zhao, Dan Pei, Yang Feng, et al. 2018. Unsupervised anomalydetection via variational auto-encoder for seasonal kpis in web applications. InProceedings of the 2018 world wide web conference. 187196.",
  "Jiehui Xu, Haixu Wu, Jianmin Wang, and Mingsheng Long. 2021. Anomalytransformer: Time series anomaly detection with association discrepancy. arXivpreprint arXiv:2110.02642 (2021)": "Yiyuan Yang, Chaoli Zhang, Tian Zhou, Qingsong Wen, and Liang Sun. 2023.Dcdetector: Dual attention contrastive representation learning for time seriesanomaly detection. In Proceedings of the 29th ACM SIGKDD Conference on Knowl-edge Discovery and Data Mining. 30333045. Chuxu Zhang, Dongjin Song, Yuncong Chen, Xinyang Feng, Cristian Lumezanu,Wei Cheng, Jingchao Ni, Bo Zong, Haifeng Chen, and Nitesh V Chawla. 2019.A deep neural network for unsupervised anomaly detection and diagnosis inmultivariate time series data. In Proceedings of the AAAI conference on artificialintelligence, Vol. 33. 14091416.",
  "Bin Zhou, Shenghua Liu, Bryan Hooi, Xueqi Cheng, and Jing Ye. 2019. Beatgan:Anomalous rhythm detection using adversarially generated time series.. In IJCAI,Vol. 2019. 44334439": "Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,and Wancai Zhang. 2021. Informer: Beyond efficient transformer for long se-quence time-series forecasting. In Proceedings of the AAAI conference on artificialintelligence, Vol. 35. 1110611115. Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. 2022.Fedformer: Frequency enhanced decomposed transformer for long-term seriesforecasting. In International conference on machine learning. PMLR, 2726827286. Bo Zong, Qi Song, Martin Renqiang Min, Wei Cheng, Cristian Lumezanu, DaekiCho, and Haifeng Chen. 2018. Deep autoencoding gaussian mixture model forunsupervised anomaly detection. In International conference on learning represen-tations.",
  "BALGORITHM": "We show the specific implementation of FCM in Alg. 1. Through theresults of time series embedding, our attention can learn the asso-ciation between variables and output the predicted results (Line 5). Through the embedding of variables at the same time point, ourattention can learn the association between time points and outputthe reconstructed results, which are used for anomaly prediction(Line 7). We control the training epoch in which we involve thereconstruction module of future context through a parameter P inStep 8."
}