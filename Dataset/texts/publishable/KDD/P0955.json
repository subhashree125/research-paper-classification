{
  "Abstract": "In recent years, graph contrastive learning (GCL) has received in-creasing attention in recommender systems due to its effectivenessin reducing bias caused by data sparsity. However, most existingGCL models rely on heuristic approaches and usually assume en-tity independence when constructing contrastive views. We arguethat these methods struggle to strike a balance between semanticinvariance and view hardness across the dynamic training process,both of which are critical factors in graph contrastive learning.To address the above issues, we propose a novel GCL-based rec-ommendation framework RGCL, which effectively maintains thesemantic invariance of contrastive pairs and dynamically adapts asthe model capability evolves through the training process. Specifi-cally, RGCL first introduces decision boundary-aware adversarialperturbations to constrain the exploration space of contrastiveaugmented views, avoiding the decrease of task-specific informa-tion. Furthermore, to incorporate global user-user and item-itemcollaboration relationships for guiding on the generation of hardcontrastive views, we propose an adversarial-contrastive learn-ing objective to construct a relation-aware view-generator. Besides,considering that unsupervised GCL could potentially narrower mar-gins between data points and the decision boundary, resulting indecreased model robustness, we introduce the adversarial examplesbased on maximum perturbations to achieve margin maximization.We also provide theoretical analyses on the effectiveness of ourdesigns. Through extensive experiments on five public datasets, wedemonstrate the superiority of RGCL compared against twelve base-line models. To benefit the research community, we have releasedour project at",
  "* Corresponding author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08",
  "Recommender Robustness; Graph Contrastive Learning; Adversar-ial Learning": "ACM Reference Format:Jiakai Tang, Sunhao Dai, Zexu Sun, Xu Chen, Jun Xu, Wenhui Yu, Lantao Hu,Peng Jiang, Han Li. 2024. Towards Robust Recommendation via DecisionBoundary-aware Graph Contrastive Learning. In Proceedings of the 30thACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD24), August 2529, 2024, Barcelona, Spain. ACM, New York, NY, USA, 15 pages.",
  "Introduction": "Recently, the intersection of graph neural networks (GNNs) andrecommender systems has emerged as a focal point of researchattention in both academia and industry . While GNNs havedemonstrated remarkable efficacy in capturing high-order connec-tivity relationships between users and items through their potentmessage propagation mechanism , the inherent data spar-sity within recommendation scenarios introduces unexpected biasin users (e.g., non-active vs. active users) and items (e.g., long-tailvs. popular items) representations, thereby impairing the overallmodel performance .To mitigate the issue of data sparsity and drawing inspirationfrom self-supervised learning (SSL), recent works have introducedGraph Contrastive Learning (GCL) into GNN-based algorithms . GCL represents a new learning paradigm that integratescontrastive learning with GNN-based recommenders, simul-taneously enhancing the alignment of positive embedding pairsand minimizing the similarity to augmented negative instances. Inthis way, GCL can effectively alleviate the problem of representa-tion degradation among low-degree nodes. In general, GCL-basedrecommenders can be classified into two categories based on howto build the contrastive samples: (1) Hardness-driven methods.These methods basically aim to construct hard enough samples tochallenge original recommender models and provide more difficultknowledge to widen the model vision. The methods in this linemainly differentiate themselves by how to define the hardness andhow to build hard enough samples. For example, SGL generateschallenging views using various strategies, such as node dropoutand edge dropout. (2) Rationality-driven methods. These meth-ods aim to maintain the rationality of the constructed samples, that",
  "RepresentationsNoise": ": An overview of two types of representative GCL-based recommenders. To facilitate the presentation, we onlyshow a single user and item with injected noise. However,in practice, the semantic-aware GCL-based methods shouldintegrate perturbations to all graph nodes. is, the augmented features and original labels should form reason-able samples. For example, SimGCL makes slight changes tothe original features, such that the augmented feature-label pairscan be still reasonable (i.e., semantically invariant).Although the aforementioned GCL-based recommenders haveshown impressive performance to some extent, we argue that thesemethods still suffer from several significant limitations. As depictedin , on the one hand, hardness-driven models blindly pur-sue the example hardness in contrastive augmentations throughmanual-designed heuristic strategies. Unfortunately, these modelsmay inadvertently remove certain crucial nodes or edges, neglect-ing how to maintain task-specific semantics. This oversight makesit challenging for recommenders to accurately capture user pref-erences and item characteristics. On the other hand, rationality-driven methods introduce slight feature perturbations to retain theunderlying semantic structure but may overlook the benefits ofintroducing hard samples on providing more diverse knowledge.Notably, both challenging positive pairs and hard negative pairsare essential to the success of GCL-based recommenders .In extreme cases, the zero-noise version of contrastive learningmay not yield significant performance gains, as verified by prior re-search . In summary, achieving an adaptive and ideal balancebetween the hardness and rationality of contrastive augmentationsfor GCL-based recommenders poses a highly intricate challenge.In this work, we aim to leverage the idea of adversarial robust-ness to facilitate the construction of optimal contrastive aug-mented data. To be specific, the goal of adversarial robustness isto promote feature invariance upon task-relevant information, as-suring the neural networks are not fooled by imperceptible dataperturbations. More importantly, it specifies the maximum per-turbation boundary that the current model can tolerate, whichexplicitly defines a feasible exploration space for conducting ex-ample augmentation. Therefore, grounded by such idea, the graphcontrastive learning can effectively balance the example hardness and rationality, both of which are crucial factors to high-qualityrepresentations. While this idea is inherently intuitive and holds in-triguing potential, its implementation still faces several challengesand obstacles. C1: prevalent contrastive augmentation approaches,assuming entity independence, struggle to maintain inherent struc-tural features as they overlook the important connections amonguser-user and item-item. C2: as an unsupervised learning algo-rithm, GCL in blindly pursuing representation uniformity mightunintentionally compromise the robust requirement, that is, nar-row margins between data points and the model decision boundary,risking unexpected decreases in the model robustness.To realize our idea and overcome the above challenges, this pa-per proposes a novel Robust Graph Contrastive Learning-basedrecommendation framework, named RGCL. Specifically, we firstcalculate the maximum perturbation magnitudes for different usersand items at each graph layer, while preserving core semantic in-formation for both user and item sides. (Rationality) Compared tomanual-designed heuristics graph contrastive learning methods, wepropose an adversarial-contrastive objective to adaptively generatechallenging positive pairs and hard negative pairs based on theglobal relationships between user-user and item-item, (Hardness)which simultaneously overcomes the limitations of the entity in-dependence assumption. (C1) At last, we optimize the joint loss ofadversarial and contrastive components to concurrently increasethe dissimilarity between different users (items) and maximize thedistances between user-item inputs and model decision boundary,further improving the robustness of the recommendation model.(C2) In summary, our contributions can be summarized as follows: We propose a model-agnostic graph contrastive learning frame-work, which utilizes dynamic decision boundary-aware adver-sarial perturbations to constrain the perturbation space of con-trastive augmented view, achieving a better balance betweencontrastive hardness and sample rationality. We develop a joint learning algorithm based on multi-view con-trastive learning and margin maximum adversarial learning tooptimize RGCL, empowering better representation uniformitywhile improving model robustness.",
  "Preliminaries2.1GNN-based Recommendation": "Formally, let U = {1,2, . . . ,} and I = {1,2, . . . , } denotethe set of users and items, respectively, where and representthe number of users and items, respectively. Considering recommen-dation scenario with implicit feedback, a binary matrix R R are typically used to record user-item interactions (e.g., clicks orpurchases), where , = 1 indicates that user has interacted withitem , otherwise , = 0. Following most GNN-based recommen-dation works , we formulate the interaction behaviorsbetween users and items as a standard bipartite graph G = {V, A},where V = U I involves all graph nodes, and the adjacent matrix",
  "GCL-based Recommenders": "In real-world scenarios, interaction behaviors between users anditems are actually highly sparse, which can lead to severe overfittingand bias problems . Graph contrastive learning (GCL), as anovel learning paradigm, helps mitigate the above problems .In specific, GCL firstly generates diverse graph views for eachuser and item (e.g., node dropout and feature masking). Then thedifferent views of the same user (item) are treated as the positivepairs, while the different views of the different instances are treatedas the negative pairs. Finally, contrastive learning loss is used tooptimize the model parameters with paired users and items, whereInfoNCE is the most commonly adopted loss. Formally, thecontrastive learning loss for the user side can be defined as follows:",
  "Adversarial Robustness": "Adversarial training (AT) stands out as one of the most promisingapproaches for bolstering adversarial robustness . Thegoal of AT is to increase model robustness by generating adversarialexamples through well-designed perturbations, which purposefullyinduce the neural network to error. Formally, the optimal perturba-tion for data sample (,) is found by maximizing the loss functionL() : = arg max L( + ,;) where represents an adver-sarial perturbation of norm smaller than . Then, the model istrained on a mixture of both original clean examples and generatedadversarial examples to enhance the robustness ability.Discussion. Adversarial robustness uncovers the root causeof the models adversarial vulnerability, that is, the non-smoothfeature space near data samples . In other words, small inputperturbations likely result in large changes in the potential seman-tics, subsequently affecting the model output, which is the basischallenge that adversarial defense algorithms strive to resolve. Ac-tually, this particularly fits well with graph contrastive learning,which aims to maximize the consistency of the given instance un-der different augmentation views. More importantly, adversarialrobustness provides the maximum boundary of feature perturba-tions that the model can tolerate (cf. Sec 3.2), which effectivelyrestrains the exploration space for contrastive augmentation andguides the generation of optimal view-generator.",
  "Our Approach: RGCL3.1Overall Framework": "The overall framework of RGCL is presented in . In specific,we calculate the maximum feature perturbations to guide the sub-sequent generation of both contrastive examples and adversarialexamples. For contrastive examples, we firstly generate two random-augmented views Z and Z using random perturbations. Besides,the third view Z, which we refer to as adversarial-contrastiveview, is generated through maximizing relation-aware contrastivefunction. On the foundation of these contrastive samples, we em-ploy multi-view contrastive learning to prompt high-quality repre-sentations. Furthermore, to safeguard the model robustness againstpotential compromises arising from the uniformity optimizationof graph contrastive learning, we generate adversarial examplesusing maximum perturbation to strenuously enlarge the distancesbetween data points and the decision boundary. Finally, the modelis updated by employing a joint optimization objective with aug-mented contrastive and adversarial data.",
  "Decision Boundary-aware Perturbation": "To build our contrastive samples, we first derive perturbations thatthe original samples can maximally tolerate to maintain user pref-erences. Ideally, the perturbations should satisfy two conditions:(1) the perturbations should be as large as possible, such that theobtained contrastive samples are hard enough (hardness require-ment). (2) The augmented samples after incorporating the pertur-bations should be still aligned with the users original preferences(rationality requirement).Different from traditional adversarial learning problems based onclassification settings, recommender system is basically a ranking",
  ": Overall framework of our proposed dynamic decision boundary-aware graph contrastive learning framework RGCL": "problem, and the perturbations should be learned to maintain userpreference rankings. To this end, we propose to learn the maximumperturbations that can maintain item pair-wise rankings. Further-more, given that different orders of graph representations possessdifferent levels of expressive capacity, that is, higher-layer represen-tations aggregate richer structure information and reflect more com-plex connectivity patterns. Consequently, we tailor the maximumperturbation for each high-order graph representation indepen-dently. In specific, for each user and a positive-negative item pair(+,), suppose their original representations are z = =0 h() ,",
  "where means the vectors p-norm. Here, pair-wise ranking": "function () is linearized around the -th representation h(), thusthe maximum perturbation ()is exactly corresponding to theorthogonal projection of h()onto the model decision hyperplane.For the sake of simplicity and better interpretation, we denotethat (h()) = (,+,)/h(). The maximum perturbation()is equivalent to solving for the directional vector from h()to the decision boundary, which is formally given as follows:",
  "(6)": "Note that we only focus on perturbing the high-order graphrepresentations for users and items, while skipping the beginningfeatures, i.e., 1 . This is because the original featurescontain the most abundant semantic information, and pollutingthese features could lead to a severe performance decrease. Onthe other hand, by perturbing higher-order representations, wesubtly and implicitly disrupt the potential semantic and structuralcharacteristics. Intuitively, it can efficaciously simulates the noiseencountered in real-world scenarios, thereby further enhancing themodel robustness. Similarly, we can obtain the graph perturbationsof item nodes from a dual perspective.",
  "Relation-aware Contrastive Learning withPerturbation Constraints": "As highlighted in Sec. 1, existing GCL-based recommenders strug-gle to achieve a harmonious balance between contrastive hardnessand rationality, both of which are pivotal to acquire high-qualityuser (item) representations. To this end, in this subsection, we metic-ulously design the relation-aware adversarial-contrastive objective,which utilizes the global relationships among user-user and item-item to create more challenging positive and hard negative pairsunder perturbation constraints. Finally, we optimize the represen-tations through multi-view contrastive learning. 3.3.1Perturbation-constrained Contrastive Augmentation.Following previous works , we adopt the random perturba-tions {r(): = 1, 2, , } for user to generate the first randomcontrastive view z as follows:",
  "Towards Robust Recommendation via Decision Boundary-aware Graph Contrastive LearningKDD 24, August 2529, 2024, Barcelona, Spain": "Here, r R following a uniform distribution (0, 1), and isa hyper-parameter to control the initial perturbation magnitude.Similarly, we could obtain the augmentation views z for item .Following that, we can get the second augmented representationsz and z in the same way but utilizing the perturbations r withdifferent random initialization for more diverse contrastive effects.However, different users and items have unique intrinsic robust-ness, which means that even imperceptible perturbations may resultin large semantic changes for fragile instances. In turn, they unin-tentionally lead to the erroneous feature-label examples, which isheavily overlooked by existing GCL methods. Therefore, we pro-pose to employ the instance-wise perturbation constrains to guidethe generation of contrastive samples, aiming to avoid lossing task-relevant semantic information and build rational view-generator.Specifically, for the -layer augmentation perturbations r() , weconstrain its exploration space by using the following projectionoperation () to obtain the constrained perturbation r() :",
  "r()= (r() ) = min((() ), max((() ), r() ),(8)": "where max(, ) and min(, ) are both wise-element operations, and() computes the absolute value of each element for the givenvector. Here, we conservatively constrain the magnitude of randomperturbation r()within a bounded () -ball, where we define ()as ||() ||. The main motivation behind Eq. (8) is that ()isthe maximum perturbation with the most attacking direction, andour conservative strategy ensures that other perturbation directionbounded within the ball could also safely maintain semantic in-variance. Consequently, we replace r()in Eq. (7) with constrainedperturbation r()for achieving contrastive rationality. 3.3.2Relation-aware Adversarial-Contrastive Augmenta-tion. To break the assumption of instance independence in tradi-tional GCL-based algorithms and simultaneously further enhancethe hardness of contrastive examples, RGCL generates the relation-aware adversarial-contrastive perturbations to fool the model byconfusing the identities among different users and items. To bespecific, we propose to maximize the following contrastive loss forgenerating instance-specific perturbations :",
  ",(9)": "and = {||() || (): U, {1, 2, . . . , }} denotes theperturbation set of user . However, as the general GNN-based rec-ommenders involve nonlinear transformations, it is extremely chal-lenging to find a closed-form solution for the above optimizationproblem. Drawing inspiration from the fast gradient sign method(FGSM) proposed in Goodfellow et al. , which assumes thatthe objective function is approximately linear around the currentmodel parameters. Building on this approximation, we can obtainan optimal max-norm constrained perturbation as follows:",
  "(11)": "where r()and r()are defined in Eq. (8) and note that they areinitialized with different random values.Compared to the random-augmented view, adversarial-contrastiveaugmentation has two main advantages: (1) The optimization objec-tive integrates global users (items) to confuse their identities, thusthe view generation process is essentially guided by the user-userand item-item relationships, resulting in relation-aware and morechallenging contrastive representations. (2) Considering differentintrinsic vulnerability among instances, our proposed adversarial-contrastive perturbations are instance-specific and dynamicallyadopted along with the model training process, thereby furtherimproving the model robustness and adaptability. 3.3.3Multi-View Contrastive Learning. In summary, based onthe above discussion, we have obtained views triplets (z, z , z )and (z, z , z ) for user and item , respectively. Then, we employmulti-view contrastive learning objective for different views of thesame instances, i.e., {z z , z z, and z z } for user, while z z , z z, and z z for item .The complete contrastive loss function is formulated as follows:",
  "Towards Margin Maximization viaAdversarial Optimization": "However, excessive pursuit of representation uniformity in GCLmay lead to reduced distances between data points and the decisionboundary, potentially compromising the model robustness. We at-tribute such dilemma is caused by the inherent deficiency that theGCLs essence is unsupervised learning paradigm, which pushesall different instances apart while ignoring task-specific semanticrelations . To tackle the above issue, we propose to use adver-sarial examples for achieving margin maximization. Specifically,we generate adversarial examples using the maximum adverasrialperturbation defined in Eq. (6), which can be formulated as follows:",
  "where and are the hyper-parameters for different loss terms": "3.5.2Complexity Analysis. Since RGCL doesnt introduce anyother trainable parameters, the space complexity and the inferencetime complexity of model remains the same as GNN backbone. Be-sides, the total training time complexity of RGCL is((|E|+2)),where and E denote the batch size and edge set, respectively. Thus,our method retains the same order of computation complexity asother state-of-the-art GCL-based methods, such as SimGCL and RocSE . Due to the limited space, please refer to Appendix Afor more detailed analysis.",
  "Theoretical Analysis4.1Hardness-aware Contrastive Learning": "The core motivation of this paper is to construct semantic pre-serving and hardness enhancing view-generator for contrastivelearning. For the former, we capitalize on the decision boundary-aware constraint to help build rationality-aware views. For thelatter, we carefully construct more challenging contrastive paireddata because their hardness significantly affects the optimizationprocess of model parameters.To further explain, we give a proof that contrastive loss is es-sentially hardness-aware learning mechanism. Specifically, tak-ing the side of users as an example, given a set of users U ={1,2, . . . ,}, we denote the similarity of user under differentaugmented views (e.g., random-augmented view or adversarial-contrastive view) as ,, and the similarity between user and as ,. The probability of being identified as is formulated as:",
  ", exp(,/),(16)": "where we can observe that the gradients of the contrastive lossw.r.t. both positive and negative pairs are proportional to the corre-sponding exponential form of their similarity scores. This meansthat smaller positive pair similarity , and larger negative pairsimilarity , will have a greater impact on the model parameteroptimization. Therefore, our proposed RGCL can learn the high-quality representations by constructing the challenging positivepairs and hard negative pairs, which fits to guide model optimiza-tion through hardness-aware contrastive learning.",
  "Theoretical Analysis of Model Robustness": "Although contrastive learning can improve the representation uni-formity and reduce the recommendation bias, it may potentiallypush data points closer to model decision boundary and eventuallydecrease model robustness due to the nature of task-unrelated un-supervised learning. To make it up, our RGCL explicitly maximizesthe margin by constructing adversarial examples based on decisionboundary-aware perturbation. Then, in this subsection, we give theexplanation on the rationality of our method.For the sake of notation simplicity, we assume that input exam-ple is denoted as . The goal of recommendation algorithm is tomake the preference probabilities for user s positive items arehigher than that for negative items, which is denoted as (;) > 0.Inspired by work , the margin between data point and decisionboundary is denoted as (;), which can be defined as follows:",
  ": Model convergence analysis w.r.t training epochson the ML-1M and Yelp datasets": "loss. Therefore, our proposed method can maximize the margin be-tween data points and the model decision boundary by generatingadversarial examples with the maximum perturbations defined inSeq. 3.2, thereby effectively improving the robustness of model. Be-sides, we give an additional robust analysis of our method from theperspective of connections between the sharpness of loss landscapeand PAC-Bayes theory. It further theoretically elaborates on themodels tolerance to parameter perturbations. The detailed analysisis presented in the Appendix B.",
  "Experimental Setup": "Datasets. We conduct extensive experiments on the following pub-lic recommendation datasets: MovieLens (ML)-1M , Alibaba ,Kuaishou , Gowalla , and Yelp. For detailed introductions andpreprocessing details of these datasets, please refer to Appendix C.1.Baseline Models. We compare RGCL with different state-of-the-art recommendation models, including traditional recommenders(BPR and NeuMF ), GNN-based recommenders (GCMC,NGCF , GCCF , and LightGCN ) and GCL-based rec-ommenders (GraphCL , SGL , LightGCL , CGI ,RocSE , and SimGCL ). The detailed introduction of allthese baseline models are referred to Appendix C.2.Evaluation Metrics. To ensure the evaluation reliability, followingstandard practice , we adopt the full-ranking strategyto mitigate the evaluation bias introduced by randomly negativesampling, which ranks all the items that are not interacted by thetest user as candidate item pool. For evaluation metrics, we adoptthe Normalized Discounted Cumulative Gain@ (NDCG@) andRecall@, where {10, 20, 50}.For better reproducibility, more implementation details are pro-vided in Appendix C.3 and",
  "Overall Performance (RQ1)": "The results of different methods on all datasets are shown in .Based on the results, we have the following observations: Compared to traditional baselines, such as BPRMF and NeuMF,all GNN-based models perform better on most datasets, whichagrees with the previous work and confirms the effectiveness ofGNNs . Among all the GNN-based methods, LightGCNusually achieves the excellent performance due to its simpleyet effective linear convolution structure. Furthermore, mostGCL-based recommenders outperform the GNN-based methods,indicating the desirable property of GCL for alleviating the biasintroduced by high-degree nodes. However, these GCL-basedmodels fail to explicitly delineate the definitions of task-relevantsemantic rationality and contrastive hardness, thus they achieveinferior balance between contrastive rationality and hardnesswhen constructing augmentation views. By comparing our approach with all state-of-the-art baselines,it is clear to see that RGCL yields a consistent boost across alldatasets. Besides, the most -values that are much less than 0.01also demonstrate the effectiveness of RGCL. We attribute themarked enhancement in performance to the excellent balance be-tween preserving semantic information and bolstering hardnessof contrastive examples, which further prompts the ability upperbound of GCL-based recommenders. Besides, we increase thedistance between sample points and decision boundary throughenhanced adversarial examples, avoiding compromises in robust-ness caused by contrastive learning.Training Efficiency. Moreover, to verify the convergence perfor-mance of RGCL, we track the Recall@20 and NDCG@20 curvesof different models w.r.t. the training epochs in . From theresults, we can observe that RGCL converges significantly fasterthan SimGCL and LightGCN. Although LightGCL also achievesgreat convergence speed, its accuracy performance is worse thanRGCL, as seen in . One possible reason is that its staticSVD contrastive view fails to keep pace with the evolving modelcapability during training, eventually limiting the improvement ofrepresentation quality. Different from these baselines, RGCL adoptsthe decision boundary-aware perturbation to guide on the examplegeneration, which adaptively adjusts the augmentation strength toreduce the inconsistency between the representation quality andthe contrastive hardness. As a result, RGCL shows both significantlygreater efficiency and efficacy.",
  "Ablation Study (RQ2)": "To further validate the importance and contribution of each compo-nent in RGCL, we devise multiple simplified variants. In specific, wecompare the following four variants: (1) in w/o cons, we drop thedecision boundary-aware perturbation constraints on contrastiveviews. (2) In w/o rand, we do not introduce random initialized per-turbation (i.e., set r as all-one vector). (3) In w/o ac, we drop therelation-aware view generator but only retain two random aug-mented views; (4) In w/o adv, we drop the adversarial regularizationterm L in the final loss. The experiment is conducted basedon the datasets of ML-1M and Yelp, while the observation andconclusion on the other datasets are similar and omitted.",
  "We present the results in , where we can see: For w/o cons": "variant, unconstrained perturbations result in a significant perfor-mance decrease, suggesting that a uniform perturbation cannoteffectively preserve that semantic information due to different in-trinsic robustness among instances. The w/o rand variant performsmuch worse than RGCL, which demonstrates that introducingsome variances for augmented views is necessary. Furthermore, ourmethod gains improvement over w/o ac variant, which reveals theimportance of challenging positive pairs and hard negative pairsHowever, only optimizing contrastive learning is still sub-optimal,which is evidenced by the lowered performance of w/o adv variantas compared with RGCL. We speculate that over-optimizing con-trastive learning for representation uniformity may potentially leadto a reduction in the distance between data points and the modelsdecision boundary, eventually deteriorating the robustness. In sum-mary, the above observations demonstrate that all the designs arecrucial to the final performance improvement.",
  "Robustness Evaluation (RQ3)": "To validate the model robustness, we conduct experimental analysisbased on different levels of user activity level and item popular-ity. For detailed user and item grouping approaches, please referto Appendix C.4. The experimental results are presented in Fig-ure 4, where we can observe that in user (item) groups with sparseinteractions, RGCL demonstrates more significant performance",
  ": Recommendation performances at different levelof data sparsity and item popularity. The black dashed linerepresents no performance improvement or decline": "improvements. This implies that RGCL effectively capture inter-est preference of inactive users and characteristic of long-taileditems. Note that the performance trends on the item side for ML-1M and Yelp datasets are different. We speculate that one possiblereason is that the proportion of long-tailed items in ML-1M is muchhigher than Yelp, which results in major contribution to the overallperformance by low-degree item groups in ML-1M.",
  "Further Analysis of RGCL (RQ4)": "In this subsection, we further conduct more detailed experimentson the RGCL method to confirm its effectiveness. Due to spacelimitation, we only show the results on ML-1M and Yelp datasetswhile the similar conclusions can be derived from other datasets. 5.5.1Analysis of the model tolerance to hyper-parameter. To validate the robustness of our method to perturbation hyper-parameter , we conduct extensive experiments of performancecomparison with SimGCL baseline with different values of . Specif-ically, we set the search range as {0.005,0.01,0.05,0.1,0.2,0.5,1.0}. Asshown in , we observe that SimGCL shows obvious per-formance fluctuations as changes. We speculate that the twofoldreasons are the following: (1) different instances have different lev-els of intrinsic robustness. However, uniform and unconstrainedperturbations may potentially destroy the semantic structure forfragile instances, ultimately leading to erroneous contrastive views.(2) For instances with better intrinsic robustness, the hardness ofcontrastive examples is insufficient, hindering the full exploita-tion of contrastive learning. In contrast, our RGCL adopts decisionboundary-aware perturbation constraints to guide the generationof both random and adversarial contrastive examples, leading to sta-ble and superior performance. This demonstrates the insensitivityof RGCL to perturbation hyper-parameter . 5.5.2Impact of the coefficient . We change to a set of prede-termined representative values presented in (a). We can seethat the recommendation performance of RGCL gradually improvesas increases, which suggests that contrastive learning can facili-tate the uniformity of node representation and learn high-qualityfeatures. Correlating with the results in and 8, it also sug-gests that the personalized characteristic of low-degree users anditems can be better captured by our algorithm. 5.5.3Impact of the layer number . To investigate the impactof the GNN layer number on model performance, we vary the hyper-parameter in the range {1, 2, 3}. From the (b), We canobserve that the performance trend of RGCL differs across differentdatasets. For example, for the ML-1M dataset, the over-smoothingissue occurs even with small value of , while for the Yelp dataset,the model shows the significant performance improvement as graphlayer number increases. 5.5.4Impact of the temperature . The temperature plays animportant role in contrastive learning . (c) shows theimpact of model performance w.r.t. different . We can see that theperformance fluctuates severely as we use different . Specifically,too large values of lead to poor performance, which is consistentwith the previous work . Conversely, too small temperaturevalues also fail to achieve optimal model performance. One possiblereason is that too small enforces the model to concentrate fewhardest examples that dominate the optimization process, which is",
  "KDD 24, August 2529, 2024, Barcelona, SpainJiakai Tang et al": "detrimental to achieve the satisfactory generalization ability. There-fore, a suitable temperature is essential to maximize the benefitsfrom graph contrastive learning.More Analysis. To comprehensively evaluate the superiorityof RGCL, we conduct more extensive experiments in Appendix toanswer the following research questions: RQ5: What is the effect of RGCL on improving the representationuniformity of users and items? (cf. Appendix D.1)",
  "Related Work": "Graph Neural Network in Recommendation. In recent years, theapplication of GNN models in recommender systems has achievedremarkable success . For example, NGCF mod-els the higher-order connectivity in user-item graph by explicitlyinjecting collaborative signals into the embedding process. Com-pared with NGCF, LightGCN simplifies the design of GCN byremoving redundant feature transformation and nonlinear activa-tion function. However, GNN-based recommenders suffer from thesparsity of user-item interactions. Although external data sources(e.g., multi-behavior data and knowledge graphs) help mitigate theabove issue, obtaining such data is often challenging and even un-available due to expensive cost or privacy protection. In contrast,graph contrastive learning, as an popular self-supervised learningparadigm, effectively overcomes the challenge of data sparsity.GCL-based Recommendation Models. Graph contrastive learn-ing (GCL) bridges the advantages of GNN models with contrastivelearning, effectively alleviating recommendation bias and simulta-neously modeling high-order connectivity. Generally, GCL methodscan be classified into hardness-driven models and rationality-drivenmethods. Specifically, for hardness-driven methods, their key taskis to construct diverse and challenging augmented views. For ex-ample, GraphCL and SGL both devises multiple heuristicstrategy to generate different contrastive views, such edge dropout and feature masking. However, these methods are prone to losingimportant semantic features since the augmentation operationsare indeed unrelated to the downstream task yet simply based onhuman-designed experiences. In contrast, rationality-driven GCLmethods alleviate the above issue by introducing slight feature per-turbations to maintain semantic consistency, such as SimGCL and RocSE . However, these methods still suffer from poten-tial issues, such as insufficient contrastive hardness and tedioustrial-and-error of hyper-parameter, resulting in suboptimal per-formance and poor flexibility. Compared with these methods, ourmethod achieves a better balance between rationality and hardnessof contrastive examples via well-designed decision boundary-awareperturbations and adversarial-contrastive view-generator.",
  "Conclusion": "In this paper, we propose a novel graph contrastive learning frame-work, named RGCL, aiming to strike a better trade-off betweenrationality and hardness for the contrastive view-generator. Specif-ically, we propose a decision boundary-aware perturbation con-straints and relation-aware adversarial-contrastive augmentationto generate contrastive examples. Besides, RGCL generates adver-sarial examples based on the adversarial perturbations to achievemargin maximization between data points and the decision bound-ary, further improving the model robustness. Finally, we design ajoint optimization objective to optimize model parameters. This work is supported in part by National Key R&D Program ofChina (2023YFF0905402), National Natural Science Foundation ofChina (No. 62102420), Beijing Outstanding Young Scientist ProgramNO. BJJWZYJH012019100020098, Intelligent Social Governance Plat-form, Major Innovation & Planning Interdisciplinary Platform forthe DoubleFirst Class Initiative, Renmin University of China, Pub-lic Computing Cloud, Renmin University of China, fund for buildingworld-class universities (disciplines) of Renmin University of China,Intelligent Social Governance Platform. The work is sponsored byKuaiShou Technology Programs (No. 2022020091).",
  "Xuheng Cai, Chao Huang, Lianghao Xia, and Xubin Ren. 2023. LightGCL: SimpleYet Effective Graph Contrastive Learning for Recommendation. arXiv preprintarXiv:2302.08191 (2023)": "Lei Chen, Le Wu, Richang Hong, Kun Zhang, and Meng Wang. 2020. Revisitinggraph based collaborative filtering: A linear residual graph convolutional networkapproach. In Proceedings of the AAAI conference on artificial intelligence, Vol. 34.2734. Wen Chen, Pipei Huang, Jiaming Xu, Xin Guo, Cheng Guo, Fei Sun, Chao Li,Andreas Pfadler, Huan Zhao, and Binqiang Zhao. 2019. POG: personalized outfitgeneration for fashion recommendation at Alibaba iFashion. In Proceedings ofthe 25th ACM SIGKDD international conference on knowledge discovery & datamining. 26622670. Eunjoon Cho, Seth A Myers, and Jure Leskovec. 2011. Friendship and mobility:user movement in location-based social networks. In Proceedings of the 17thACM SIGKDD international conference on Knowledge discovery and data mining.10821090. Gavin Weiguang Ding, Yash Sharma, Kry Yik Chau Lui, and Ruitong Huang. 2020.MMA Training: Direct Input Space Margin Maximization through AdversarialTraining. In International Conference on Learning Representations. Chongming Gao, Shijun Li, Yuan Zhang, Jiawei Chen, Biao Li, Wenqiang Lei,Peng Jiang, and Xiangnan He. 2022. KuaiRand: An Unbiased Sequential Rec-ommendation Dataset with Randomly Exposed Videos. In Proceedings of the31st ACM International Conference on Information & Knowledge Management.39533957. Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of trainingdeep feedforward neural networks. In Proceedings of the thirteenth internationalconference on artificial intelligence and statistics. JMLR Workshop and ConferenceProceedings, 249256.",
  "F Maxwell Harper and Joseph A Konstan. 2015. The movielens datasets: Historyand context. Acm transactions on interactive intelligent systems (tiis) 5, 4 (2015),119": "Wei He, Guohao Sun, Jinhu Lu, and Xiu Susie Fang. 2023. Candidate-awareGraph Contrastive Learning for Recommendation. In Proceedings of the 46thInternational ACM SIGIR Conference on Research and Development in InformationRetrieval. 16701679. Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and MengWang. 2020. Lightgcn: Simplifying and powering graph convolution network forrecommendation. In Proceedings of the 43rd International ACM SIGIR conferenceon research and development in Information Retrieval. 639648.",
  "Ziyu Jiang, Tianlong Chen, Ting Chen, and Zhangyang Wang. 2020. Robustpre-training by adversarial contrastive learning. Advances in neural informationprocessing systems 33 (2020), 1619916210": "Xuewu Jiao, Weibin Li, Xinxuan Wu, Wei Hu, Miao Li, Jiang Bian, Siming Dai,Xinsheng Luo, Mingqing Hu, Zhengjie Huang, et al. 2023. PGLBox: Multi-GPUGraph Learning Framework for Web-Scale Recommendation. In Proceedings ofthe 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.42624272. Di Jin, Luzhi Wang, Yizhen Zheng, Guojie Song, Fei Jiang, Xiang Li, Wei Lin, andShirui Pan. 2023. Dual Intent Enhanced Graph Neural Network for Session-basedNew Item Recommendation. In Proceedings of the ACM Web Conference 2023.684693.",
  "Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-mization. arXiv preprint arXiv:1412.6980 (2014)": "Zihan Lin, Changxin Tian, Yupeng Hou, and Wayne Xin Zhao. 2022. Improvinggraph collaborative filtering with neighborhood-enriched contrastive learning.In Proceedings of the ACM Web Conference 2022. 23202329. Lingyun Lu, Bang Wang, Zizhuo Zhang, Shenghao Liu, and Han Xu. 2023.VRKG4Rec: Virtual Relational Knowledge Graph for Recommendation. In Pro-ceedings of the Sixteenth ACM International Conference on Web Search and DataMining. 526534.",
  "Feng Wang and Huaping Liu. 2021. Understanding the behaviour of contrastiveloss. In Proceedings of the IEEE/CVF conference on computer vision and patternrecognition. 24952504": "Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019.Neural graph collaborative filtering. In Proceedings of the 42nd international ACMSIGIR conference on Research and development in Information Retrieval. 165174. Chunyu Wei, Jian Liang, Di Liu, and Fei Wang. 2022. Contrastive Graph StructureLearning via Information Bottleneck for Recommendation. Advances in NeuralInformation Processing Systems 35 (2022), 2040720420.",
  "Yonghui Yang, Zhengwei Wu, Le Wu, Kun Zhang, Richang Hong, Zhiqiang Zhang,Jun Zhou, and Meng Wang. 2023. Generative-Contrastive Graph Learning forRecommendation. (2023)": "Haibo Ye, Xinjie Li, Yuan Yao, and Hanghang Tong. 2023. Towards robust neuralgraph collaborative filtering via structure denoising and embedding perturbation.ACM Transactions on Information Systems 41, 3 (2023), 128. Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, andYang Shen. 2020. Graph contrastive learning with augmentations. Advances inneural information processing systems 33 (2020), 58125823. Junliang Yu, Xin Xia, Tong Chen, Lizhen Cui, Nguyen Quoc Viet Hung, andHongzhi Yin. 2023. XSimGCL: Towards extremely simple graph contrastivelearning for recommendation. IEEE Transactions on Knowledge and Data Engi-neering (2023). Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Lizhen Cui, and Quoc Viet HungNguyen. 2022. Are graph augmentations necessary? simple graph contrastivelearning for recommendation. In Proceedings of the 45th international ACM SIGIRconference on research and development in information retrieval. 12941303.",
  "AAnalysis of Training Time Complexity": "The extra training time complexity of RGCL comes from the lossterms of contrastive and adversarial components. Suppose the num-ber of nodes and edges are |V| and |E|, respectively. Let denotethe batch size, denote the embedding dimension, L denote the totallayer number. We analyze the time complexity of each componentas follows: Original loss. The time complexity of the original LightGCNmodel comes from adjacent matrix construction, graph convolu-tion computation and BPR calculation. Their time complexitiesare(|E|),(|E|) and() respectively. Therefore, the totaltime complexity is ((|E| + )). Contrastive loss. To begin with, solving for the perturbationconstraints in contrastive learning needs one pass of forward andbackward propagation, where the time complexity is (|E|).Then, constructing two random-augmented views requires twopass of forward propagation. As for adversarial-contrastive view,it also needs extra one pass of forward and backward propagation,where the time complexity of the contrastive loss paradigm is(2). Therefore, the total time complexity of the contrastivelearning component is ((|E| + 2)). Adversarial loss. The adversarial perturbations for generatingadversarial examples has already been accounted in the con-trastive loss part. Thus, in this part, we simply consider thetime complexity of forward propagation and BPR loss, whichare (|E|) and (), respectively. Therefore, the total timecomplexity of the adversarial loss is ((|E| + )).In summary, the total time complexity of the proposed RGCL is((|E|+2)), which maintains the same order of time complexityas other graph contrastive learning algorithms . However,the experimental results in demonstrates that our algorithmhas better converge and accuracy performance.",
  "BFurther Robustness ANALYSIS": "Inspired by previous work , we provide the robustness anal-ysis from the perspective of connections between sharpness ofloss landscape and PAC-Bayes theory. Generally, smoother featurespace can avoid large feature variations caused by input perturba-tions . Meanwhile, from the perspective of model optimization,flatter loss landscape can bring better model robustness. Specifically,assuming that the prior distribution Q over the model parameters,with probability at least 1 over the draw of the training data,the expected error of L can be bounded as follows:",
  "CEXPERIMENT DETAILSC.1Recommendation Datasets": "We conduct extensive experiments on the following five publiclyavailable recommendation datasets in this paper: (1) MovieLens(ML)-1M1 is a widely adopted movie recommendation dataset,containing the one million movie ratings provided by users, rangingfrom 1 to 5 stars. (2) Alibaba2 is a fashion-related dataset andprovides user behaviors related to both outfits and fashion items.(3) Kuaishou3 contains user interactions on exposed short videos,collected from the video-sharing mobile App. (4) Gowalla4 is achecking-in dataset for item recommendation, collected from alocation-based social networking website. (5) Yelp5 is a widely-used business recommendation dataset collected from yelp website,where the business venues of users are viewed as the items.To transform the explicit user ratings into implicit interactionbehavior, the interactions with ratings above three are viewed asthe positive example for rating-based datasets (i.e., ML-1M andYelp). For Yelp and Gowalla datasets, we filter users and items thathave less than fifteen interaction number to ensure the data quality.For all datasets, we randomly divide the data into training set, vali-dation set and testing set using a ratio of 8:1:1. For negative samples",
  "C.3Implementation Details": "We implement our RGCL with PyTorch framework. For faircomparison, all models are initialized with the Xavier method and optimized by the Adam optimizer . All hyper-parametersof baseline models are searched following suggestions from theoriginal papers. The batch size and embedding dimension are fixedto 4,096 and 64, respectively. The learning rate is searched from{0.0005, 0.001, 0.005, 0.01, 0.05}. The layer number of graph neuralnetwork is searched from {1, 2, 3}. We set = 0.1 in Equation (15).The loss weight is tuned from {15, 55, . . . , 12}. The initial hyper-parameter used for perturbation magnitude is chosen from{0.005, 0.01, , 1.0}. The search range of temperature coefficient is {0.05, 0.1, 0.2, 0.5, 1.0, 5.0, 10.0}. Early stopping is utilized as theconvergence criterion. Specifically, we evaluate the performance onthe validation dataset for each epoch, and stop the training processonce there is no accuracy improvement for 10 consecutive epochs.",
  "C.4Details on User and Item Grouping": "In the following, we provide the specific details of partitioning theuser and item groups in Experiment 5.4: USER: we split all users into five groups based on the numberof user interaction while keeping the total number of each usergroup the same, which are denoted as in as-cending order of interaction count. ITEM: we group all items based on their popularity into fivegroups and similarly, we keep the total number of each itemgroup the same. Specifically, we adopt the decomposed Recalland NDCG metrics defined as follows:",
  "To better understand how RGCL promotes the uniformity of repre-sentations for preserving personalized node information, we visu-alize the learned item embeddings and user embeddings in": "and , respectively. Specifically, we firstly map the learnednode representations to 2-dimensional normalized vectors usingt-SNE . Then, we use Kernel Density Estimation (KDE) tovisualize the distribution of transformed feature representations.Moreover, for a clearer demonstration, we also visualize the densityestimations of their angles, where angles are calculated using thefunction: 2(,) for each instance (,). We can observeour RGCL shows a better uniform distribution on both users anditems. This shows that RGCL can effectively learn high-qualityrepresentations by avoiding the bias caused by the dominance ofadvantaged users and items. Besides, correlating with the resultsin , RGCL achieves a win-win breakthrough in representa-tion uniformization and performance improvement compared otherbaselines, suggesting the superiority of our designs.",
  "D.2Generalization Evaluation (RQ5)": "To verify the generalization of our proposed model-agnostic frame-work, we employ RGCL framework on three other commonly usedGNN-based backbones, i.e., GCMC , NGCF and GCCF .We summarize the experimental results in . From the table,we can see that RGCL generalizes well across different GNN-basedbackbones, further demonstrating the effectiveness and flexibilityof our method. Additionally, the improvement based on the NGCFbackbone is not significant, which we attribute to the redundantweight parameters and unnecessary nonlinear feature transforma-tions of NGCF model, thus posing challenges to the model learning.",
  "D.3Case Study (RQ7)": "In this section, we present a case study to intuitively show theeffectiveness of our model to preserve the important semantic in-formation of recommendation task. From the , we canobserve that user #315 prefers horror, action, and science fictionmovies while showing less interest in comedy movies. Comparingthe SimGCL and RGCL methods, although both original rankingresults attain the correct ordering preferences for positive items andnegative items, the introduction of noise perturbation for SimGCL",
  ">": ": Case study on ML-1M dataset. The \"Score (Origin.)\"and \"Score (Pert)\" indicate predicted scores based on the orig-inal and contrastive augmented user and item embeddings,respectively. Best viewed in color. baseline leads to a reversal in the predicted scores for movies #757(liked movie) and movie #642 (disliked movie). It indicates thatSimGCL baseline cannot reasonably control perturbations to pre-serve task-relevant information, resulting in irrational contrastivesamples. In contrast, our proposed RGCL generates rational con-trastive pairs and thus effectively improves model robustness andrecommendation performance."
}