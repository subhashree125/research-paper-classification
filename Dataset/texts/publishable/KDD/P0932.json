{
  "ABSTRACT": "Multimodal machine learning models that combine visual and tex-tual data are increasingly being deployed in critical applications,raising significant safety and security concerns due to their vul-nerability to adversarial attacks. This paper presents an effectivestrategy to enhance the robustness of multimodal image captioningmodels against such attacks. By leveraging the Fast Gradient SignMethod (FGSM) to generate adversarial examples and incorporatingadversarial training techniques, we demonstrate improved modelrobustness on two benchmark datasets: Flickr8k and COCO. Ourfindings indicate that selectively training only the text decoderof the multimodal architecture shows performance comparable tofull adversarial training while offering increased computationalefficiency. This targeted approach suggests a balance between ro-bustness and training costs, facilitating the ethical deployment ofmultimodal AI systems across various domains.",
  "INTRODUCTION": "Multimodal machine learning is an advanced area of artificial intel-ligence that integrates and processes multiple types of data inputs,such as text, images, and audio, to perform tasks that mimic humancognitive abilities . This integration allows the model to leveragethe strengths of each modality, leading to richer interpretationsand more accurate predictions that could be achieved by models Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2429, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-1-4503-XXXX-X/18/06 processing the data in isolation. One of the most compelling appli-cations of multimodal learning involves the combination of visualand textual data, commonly referred to as image-text pairs. Thispairing is particularly significant as it mirrors the way humansoften receive and interpret information, making the study of thesemodels not only interesting but also aligned with natural humancommunication patterns.The increasing deployment of multimodal models in criticalapplications also raises significant safety and security concerns.These models, like all machine learning systems, are susceptibleto adversarial attacks which means intentional inputs designedto confuse the model and provoke incorrect outputs . Therobustness of multimodal models, therefore, becomes a critical areaof focus. Ensuring these models can withstand adversarial attacksis not just a technical necessity but a safety imperative, particularlywhen these models are employed in sensitive contexts such asautonomous driving, healthcare, and content moderation.Making multimodal machine learning models more robust againstadversarial attacks is a key challenge . These models, whichcombine different types of data like text and images, can be es-pecially vulnerable because of the way these data types interact.Traditional methods that focus on making each type of data robuston its own can be very demanding and might not address all theissues.In this paper, we look at a simpler and more efficient approach.Instead of trying to make the entire model robust, we focus onimproving just one part of it. We apply adversarial training tech-niques specifically to the text decoder part of our image captioningmodel. Our experiments with the Flickr8k and COCO datasets showthat this method works well. Training only the text decoder, whilekeeping the image encoder fixed, gives us results almost as goodas training the whole model. However, when we fix the text de-coder and train only the image encoder, the performance dropssignificantly. This shows that the text decoder plays a crucial rolein making the model robust against attacks.This paper helps improve ethical AI by showing a focused way tomake multimodal models stronger. This method not only improvesthe technical side of AI but also helps build public trust in AI systemsused in different areas.This paper is organized as follows: reviews related workon adversarial attacks and defenses in multimodal machine learning. describes our model architecture and the adversarialtraining method. presents the experimental setup andresults on the Flickr8k and COCO datasets. discusses whatour findings mean for developing robust and ethical AI systems.Finally, concludes the paper and suggests future researchdirections.",
  "BACKGROUND": "In , the authors investigated the vulnerability of multimodaldeep learning models to adversarial attacks, finding that even ifonly one input modality is attacked, the overall performance de-grades significantly. The authors highlight the need to explore waysto obtain robust features from multimodal data to achieve usefulinformation from each modality and improve adversarial defensemechanism. To develop adversarial defense mechanisms we ex-plored different adversarial attacks in machine learning models.One of the first and best-known adversarial attack techniques isthe Fast Gradient Sign Method (FGSM), which involves applyingperturbations to the input data in the direction of the gradient ofthe loss function with respect to the input data to produce adver-sarial examples . Another adversarial attack similar to FGSM,which creates the attack iteratively, is Projected Gradient Descent(PGD) . PGD generates adversarial examples by iteratively tak-ing small steps toward the direction of maximum loss functionand projecting the outcome back onto the allowed data range ateach step. Other well-known adversarial attack strategies are Jaco-bian Saliency Map Adversary (JSMA) and C&W Attack. JSMA isan iterative adversarial attack technique that computes a saliencymap based on the Jacobian matrix to identify the most significantpixels to perturb in order to mislead neural network classifiers .The C&W attack, proposed by Carlini and Wagner, is an powerfuland efficient optimization-based adversarial attack that finds quasi-imperceptible perturbations to input examples that reliably causemisclassification in neural networks .The authors in discussed about adversarial attack in mul-timodal machine learning model like CLIP in their paper. Theypresented new types of adversarial attacks against the multi-modalneural network model CLIP, which integrates the detection of vi-sual objects with text reading. Basic typographical manipulationslike misspellings and font changes are included in the attacks, alongwith conceptual ones that use contradicting text and image inputsto trick the model. Adversarial training is one of the most commondefense mechanism for adversarial attack in machine learning mod-els. Adversarial training is using adversarial examples as samplesin the training phase of the model. The authors in proposeda fast adversarial training algorithm that produces robust modelsat a low computational cost by recycling the gradient informationcomputed during training. The method is able to train robust imageclassifiers for the ImageNet dataset in a short time on a modesthardware setup.",
  "METHODOLOGY3.1Model Architecture": "This work uses an architecture for the image captioning task thatcombines GPT-2 and Vision Transformer (ViT) models , with theGPT-2 acting as a decoder and the ViT as an encoder. The ViTmodel divides an input image of 224 x 224 pixels into 16 x 16 pixelpatches. Then, these patches are flattened and projected linearlyinto a space with dimensions of 768. The transformer encoderreceives the image patch sequence and a [CLS] token. The encoderconsists of multiple layers of position-wise feedforward and multi-headed self-attention networks. The [CLS] token, which compilesdata from all patches, represents the whole image. This tokens final hidden state at the ViT encoders output captures the contextualizedglobal aspects of the image, which are essential for generating acaption.The decoder obtains the encoded image features from the ViTs[CLS] token, which uses them as the starting point for captiongeneration. Through many layers of masked multi-headed self-attention, the GPT-2 model processes this input, allowing eachpoint in the output sequence to attend only to earlier positionsin the sequence. This arrangement preserves the autoregressivecharacteristic by ensuring that each word in the caption is generatedsolely based on the words that came before it. The GPT-2 modelproduces a sequence of tokens that comprise the images caption.Every token is created one after the other, and the model predictsthe subsequent token by analyzing the preceding tokens and thecontextual data extracted from the image.",
  "Adversarial Attack": "In our work, we used the adversarial example technique to createadversarial attacks. Specifically, we employed the Fast Gradient SignMethod (FGSM) to generate these examples. FGSM is a method forcrafting adversarial examples, which are inputs to machine learningmodels that have been intentionally modified to cause the modelto make incorrect predictions .Given an input x and its true label , we define the loss function (, x,), where represents the parameters of the model. Thegoal is to find a small perturbation such that the perturbed inputx = x + leads to a misclassification. FGSM achieves this by usingthe gradient of the loss function with respect to the input x.First, we compute the gradient of the loss function:",
  "x = x + sign(g)": "Here, is a small constant that controls the magnitude of theperturbation. The sign function, sign(g), is applied element-wiseto the gradient, ensuring that each element of the perturbation hasthe same magnitude, . shows examples of the adversarial perturbations gener-ated using FGSM on the Flickr8k and COCO datasets. Despite itssimplicity, FGSM is effective in generating adversarial examplesthat are close to the original inputs but cause the model to makeincorrect predictions. This method is computationally efficient andserves as a useful tool for testing and improving the robustness ofmachine learning models. This table shows how adversarial attacksare imperceptible to human eyes, as shown in the second column.The third column demonstrates the perturbations by showing thedifference between the original and perturbed images. In the Dif-ference column, we can see square or patch-like patterns. Thishappens because of ViTs architecture, which processes input im-ages by dividing them into patches. FGSM leverages the gradientscomputed for each patch, generating perturbations that exploitthe vulnerabilities within these regions. underscores thenecessity for robust models capable of defying such attacks. Byincorporating adversarial examples into the training process, we",
  "In this study, we leverage two widely-used and benchmark datasetsfor the image captioning task: Flickr8k and COCO (Common Ob-jects in Context)": "3.3.1Flickr8k Dataset. The Flickr8k dataset comprises 8,000 im-ages, each accompanied by five human-annotated captions. Theimages are sourced from Flickr, a popular online photo-sharingplatform, and are categorized into 20 distinct classes. 3.3.2COCO Dataset. The COCO dataset is a larger and more chal-lenging resource for image captioning, containing 123,287 imageswith five captions per image. Unlike the Flickr8k dataset, the im-ages in COCO are sourced from various online platforms, includingFlickr, and are categorized into 80 classes.",
  "Evaluation Metric": "To evaluate the performance for image captioning task, we usedBilingual Evaluation Understudy (BLEU) score . Comparingthe generated captions to the reference captions in the datasetgenerates a similarity score known as the BLEU score. Higher scorescorrespond to a closer match. The BLEU score computes the n-gramoverlap between the models output and the ground truth. Thereis a perfect match between the generated and reference captionswhen the BLEU score is 1, from 0 to 1.",
  "(5) We reversed the process by freezing the GPT-2 decoder andusing the ViT encoder alone for adversarial training": "We conducted steps four and five three times each and calcu-lated the average of these trials. Focusing on a single modality at atime allowed us to implement more effective adversarial defensemechanisms without having to scale them across the entire model.We repeated these five steps of experiment with COCO dataset as",
  "RESULTS": "After conducting experiments on both the Flickr8k and COCOdatasets, we evaluated the performance of our multimodal machinelearning models using the BLEU score metric. The goal of theseexperiments was to find an efficient way to increase the robustnessof these models against adversarial attacks. As shown in ,our baseline model, trained on the original data, initially achievedthe best performance. However, when we generated adversarialexamples using the FGSM and trained the model with these sam-ples, its performance decreased significantly. This suggests thatour multimodal model is not robust against adversarial attacks,as generating a single adversarial example for each image in thedataset was sufficient to degrade its performance.To enhance the models robustness, we trained it with boththe original data and the adversarial examples. This adversarialtraining approach improved the models performance, indicatingthat incorporating adversarial examples during training can helpmitigate the impact of such attacks.Aiming to find an efficient way to increase the robustness of ourmultimodal model, we investigated whether training only one of itscomponents (either the image model or the text model) would besufficient. First, we froze the image model (ViT) and trained the textmodel with both the original data and the adversarial examples. Asshown in , the models performance in this case was lowerthan with full adversarial training, although the difference was notsignificant. Alternatively, we froze the text model (GPT) and trainedthe image model; in this case, we can see the performance is muchlower than the full adversarial trainings performance.",
  "Baseline BLEU Score0.2850.232Adversarial Example0.1640.143Adversarial Training0.2170.215Adversarial Training by freezing ViT0.2000.181Adversarial Training by freezing GPT0.0600.050": "We conducted similar experiments using the COCO dataset, andthe results are presented in . The baseline performance onthe COCO dataset was higher than on Flickr8k. However, whenthe model was trained solely on adversarial examples generatedfrom the COCO dataset, the performance decreased significantly,as expected. In the third phase, where we trained the model onboth the original data and the adversarial samples, the result wascomparable to the baseline models performance.When we froze the image model (ViT) and trained only the textmodel, the difference in BLEU score compared to the full adversarialtraining approach was relatively small. This observation aligns withour findings from the Flickr8k dataset.",
  "DISCUSSION": "Through experiments on both the Flickr8k and COCO datasets, wecan conclude that while the adversarial training technique improvesthe robustness of our multimodal models against adversarial attacks,their performance does not fully match the baseline levels achievedwith clean data. However, the adversarially trained models showedperformance close to the baseline, suggesting that this approachcan effectively mitigate the impact of adversarial attacks.Our findings suggest that performing adversarial training onlyin the text model can achieve comparable performance to full ad-versarial training. However, freezing the text model or the GPT-2model during training significantly degrades performance. Whenthe ViT encoder is frozen and adversarial training is performedsolely on the GPT-2 decoder, the model still benefits from the ViTencoders robust feature extraction capabilities. The ViT encodercontinues to deliver consistent and reliable image features, en-abling the GPT-2 decoder to concentrate on learning to generatetext robustly. In contrast, when the GPT-2 decoder is frozen andadversarial training is applied solely to the ViT encoder, the modelscapacity to generate coherent and contextually relevant text is ham-pered. Since the parameters of the GPT-2 decoder remain fixed, itcannot adjust to the alterations in the image features produced bythe ViT encoder during adversarial training. Consequently, this lackof adaptability in the text generation process results in a decline inperformance. In conclusion, this finding presents an opportunity toincrease computational efficiency by focusing the training effortson a text modality, with a modest trade-off in terms of performancecompared to training the entire multimodal model.",
  "CONCLUSION": "Our research has shown important steps forward in making multi-modal machine learning models more robust, especially for taskslike image captioning. We combined Vision Transformer (ViT) andGPT-2 architectures and used adversarial training methods to focuson specific parts of the model. This focused approach helps pro-tect against adversarial attacks without needing to train the entiremodel, which saves time and resources. Through our experimentswith the Flickr8k and COCO datasets, we found that applying ad-versarial training to just the text decoder can make the modelsmuch more robust. While the performance of these models withadversarial training is slightly lower than those trained only onclean data, the trade-off is minimal. This means we can improvemodel safety without losing much accuracy. Freezing the imageencoder and training the text decoder helps balance performanceand robustness efficiently. On the other hand, training the image encoder alone does not provide the same benefits, highlighting thetext decoders key role in maintaining robustness.Future work will explore changes in activation functions foradditional gains in adversarial robustness . Current and futurework supports the goals of Ethical AI by making AI systems saferand reliable. By focusing on specific parts of the model, we canmake the adversarial defense process more efficient. This targetedapproach can lead to the development of AI systems that are bothresource-efficient and secure for public use. Enhancing the robust-ness of AI models against adversarial attacks builds public trustand confidence in AI systems, especially in critical applications likehealthcare, autonomous driving, and content moderation.",
  "Nicholas Carlini and David Wagner. 2017. Towards Evaluating the Robustness ofNeural Networks. In 2017 IEEE Symposium on Security and Privacy (SP). 3957": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, GeorgHeigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformersfor image recognition at scale. arXiv preprint arXiv:2010.11929 (2020). Ivan Evtimov, Russel Howes, Brian Dolhansky, Hamed Firooz, and Cristian Can-ton Ferrer. 2020. Adversarial evaluation of multimodal models under realisticgray box assumption. arXiv preprint arXiv:2011.12902 (2020).",
  "David A Noever and Samantha E Miller Noever. 2021. Reading Isnt Believing:Adversarial Attacks On Multi-Modal Neurons. arXiv preprint arXiv:2103.10480(2021)": "Mesut Ozdag. 2018. Adversarial Attacks and Defenses Against Deep NeuralNetworks: A Survey. Procedia Computer Science 140 (2018), 152161. Cyber Physical Systems and Deep LearningChicago, Illinois November 5-7, 2018. Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z. BerkayCelik, and Ananthram Swami. 2016. The Limitations of Deep Learning in Ad-versarial Settings. In 2016 IEEE European Symposium on Security and Privacy(EuroS&P). 372387. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: amethod for automatic evaluation of machine translation. In Proceedings of the 40thAnnual Meeting on Association for Computational Linguistics (Philadelphia, Penn-sylvania) (ACL 02). Association for Computational Linguistics, USA, 311318."
}