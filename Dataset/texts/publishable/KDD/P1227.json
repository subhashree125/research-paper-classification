{
  "ABSTRACT": "Recent works demonstrate that GNN models are vulnerable toadversarial attacks, which refer to imperceptible perturbation onthe graph structure and node features. Among various GNN mod-els, graph contrastive learning (GCL) based methods specificallysuffer from adversarial attacks due to their inherent design thathighly depends on the self-supervision signals derived from theoriginal graph, which however already contains noise when thegraph is attacked. To achieve adversarial robustness against suchattacks, existing methods adopt adversarial training (AT) to theGCL framework, which considers the attacked graph as an augmen-tation under the GCL framework. However, we find that existingadversarially trained GCL methods achieve robustness at the ex-pense of not being able to preserve the node feature similarity. Inthis paper, we propose a similarity-preserving adversarial graphcontrastive learning (SP-AGCL) framework that contrasts the cleangraph with two auxiliary views of different properties (i.e., thenode similarity-preserving view and the adversarial view). Exten-sive experiments demonstrate that SP-AGCL achieves a competitiveperformance on several downstream tasks, and shows its effective-ness in various scenarios, e.g., a network with adversarial attacks,noisy labels, and heterophilous neighbors. Our code is available at",
  "Both authors contributed equally to this research.Corresponding author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from , August 06-10, 2023, Long Beach, CA 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0103-0/23/08...$15.00",
  "INTRODUCTION": "A graph is a ubiquitous data structure that appears in diverse do-mains such as chemistry, biology, and social networks. Thanks totheir usefulness, a plethora of studies on graph neural networks(GNNs) have been conducted in order to effectively exploit thenode and structural information inherent in a graph. However, real-world graphs are usually large-scale, and it is difficult to collectlabels due to the expensive cost. Hence, unsupervised graph rep-resentation learning methods such as have receivedsteady attention. Most recently, the graph contrastive learning(GCL) framework has taken over the mainstream of unsupervisedgraph representation learning . GCL aims tolearn node representations by pulling together semantically sim-ilar instances (i.e., positive samples) and pushing apart differentinstances (i.e., negative samples) in the representation space. In par-ticular, instance discrimination-based approaches , whichtreat nodes in differently augmented graphs as self-supervisionsignals, are dominant among the recent GCL methods.Although deep learning-based models on graphs have achievedpromising results, recent studies have revealed that GNNs are vul-nerable to adversarial attacks . Adversarial attack on a graphrefers to imperceptible perturbations on the graph structure andnode features that rapidly degrade the performance of GNN models.In other words, even with a slight change in the graph structure(e.g., adding/removing a few edges) and node features, GNN mod-els lose their predictive power1. Existing studies mainly focus onenhancing the robustness of the GNN models, aiming at facilitat-ing robust predictions given an attacked graph. To this end, theyintroduce novel GNN architectures or propose to learna graph structure by purifying the attacked structure .Although the aforementioned approaches have shown effective-ness in training robust GNN models, most of them rely on thelabel information of nodes and graphs (i.e., supervised setting), andthus they are not applicable when the label information is absent(i.e., unsupervised setting). However, as most real-world graphs arewithout any label information and the labeling process is costly,developing unsupervised GNN models that are robust to adver-sarial attacks is important. Recently, it has been highlighted thatunsupervised methods for graphs are also vulnerable to adversarialattacks , and some approaches have addressed the robustnessof unsupervised GNNs by adopting adversarial training (AT) to theGCL framework . Their main idea is to find the worst-caseperturbations on the graph structure and the node features, anduse the attacked graph as an adversarial view to train GNN mod-els such that the learned node representations do not vary muchdespite the adversarial messages propagated from the perturbed",
  "KDD23, August 06-10, 2023, Long Beach, CAYeonjun In, Kanghoon Yoon, and Chanyoung Park": ": (a) Gradients of the contrastive loss w.r.t A. Each point represents an element in the adjacency matrix A with thesum of node degrees of two nodes (-axis), and the raw feature similarity (-axis) between two nodes. Red points denote edgesselected for perturbations. (b) scores (in solid lines) with = 10, and the difference of the node classification (bar graph)accuracy of GRACE-AT, and SP-AGCL compared with GRACE on Citeseer and Co.CS datasets.",
  "We conduct both theoretical and empirical studies to show thatadversarially trained GCL models indeed fail to preserve thenode feature similarity": "We present a novel GCL framework, called SP-AGCL, thatachieves adversarial robustness, while preserving the node fea-ture similarity by introducing a similarity-preserving view, andan adversarial view generated from an adversarial feature mask. SP-AGCL achieves a competitive performance compared withstate-of-the-art baselines on several downstream tasks, andwe show its effectiveness in various scenarios, e.g., adversarialattacks, noisy node labels, and heterophilous neighbors.",
  "RELATED WORKS2.1Graph Contrastive Learning": "Graph Contrastive Learning (GCL) is a well-known representationlearning framework that pulls semantically similar instances andpushes semantically different instances. Inspired by Deep InfoMax, DGI presents a contrastive learning method that learnsnode representations by maximizing the mutual information be-tween the local patch and the global summary of a graph. Recently,GRACE and GCA , motivated by SimCLR , present meth-ods that contrast two differently augmented views with each other,where the different views are generated by various augmentationsto edges and node features. GRACE obtains the representationsby maximizing the agreements between the representations of thesame nodes in the two augmented views, and minimizing the agree-ments between all other nodes. In this paper, we develop a methodthat makes GCL models robust against adversarial attacks.",
  "Adversarial Attacks on Graph": "Deep learning models on graphs have been shown to be vulnerableto adversarial attacks. Nettack is a targeted attack that aims tofool the predictions for specific target nodes. Metattack presentsa non-targeted attack method based on meta-learning that deterio-rates the overall performance of GNNs. PGD and min-max methods employ loss functions such as negative cross-entropy and CW-type loss to perturb the graph structure using projected gradientdescent. However, these attack methods are designed for supervisedlearning, and thus not applicable under unsupervised settings. Mostrecently, CLGA introduces an unsupervised graph structuralattack method that flips the edges based on the largest gradient ofthe GCL objectives. In this paper, we focus on developing a GCLframework based on AT, where such adversarially attacked graphsare used as an augmentation to obtain adversarial robustness.",
  "Adversarial Defense on Graph": "Along with the growing interest in adversarial attacks on a graph,research on defense methods have also received attention. A strat-egy for defending the adversarial attack can be categorized intotwo types. The first line of research focuses on designing novelGNN architectures. Specifically, they achieve robustness by adjust-ing messages from other nodes based on uncertainty or nodefeature similarity information . Another line of research onthe adversarial defense focuses on purifying the graph structureby removing noisy edges. They denoise the graph structure withthe feature smoothness or limited label information . How-ever, the aforementioned approaches are heavily dependent on thesupervision signals (i.e., node labels), and thus their performanceeasily deteriorates with noisy labels. Most importantly, they arenot applicable when the label information is absent.Training robust GNN models in an unsupervised manner is evenmore challenging because these models receive (self-) supervisionsignals from the augmented views of the original graph that mayhave been already attacked before being augmented, which wouldnot provide helpful supervisory signals. Most recently, adversarialGCL models, which apply AT to the GCL framework, have beenadopted to unsupervised adversarial defense models. DGI-ADV adopts a training strategy that alternately performs DGI and",
  "PRELIMINARY": "Notations. Let us denote G = V, E, X as a graph, where V ={1, ..., } is the set of nodes, E V V is the set of edges, and is the number of nodes. We denote A R as the adjacencymatrix with A = 1 if , are connected, otherwise A = 0.X R and Z R are the feature and the representationmatrix of nodes, respectively, where and are the dimensions ofthe feature and representation, respectively, and x and z denote-th row of X and Z, respectively. Graph Contrastive Learning. Since SP-AGCL follows a proce-dure similar to recent GCL models, we begin by explaining the GCLframework. Particularly, we focus on the instance discriminationmodels that try to maximize the agreement of representations be-tween different views . To be specific, given the input (A, X),GCL models first generate two different views (A1, X1) and (A2, X2)by randomly dropping edges and node features. Then, a GCN layer : (A, X) Z encodes these views into the representationsZ1 = (A1, X1) and Z2 = (A2, X2). Finally, the contrastive loss isoptimized to enforce the representations of the same nodes in dif-ferent views to be close, and the representations of different nodesto be distant. Following the common multi-view graph contrastivelearning, the pairwise contrastive loss is given by:",
  "ANALYSIS ON ADVERSARIAL GCL": "In this section, we perform a theoretical analysis and an empiricalstudy to show that adversarial GCL models indeed fail to preservethe node feature similarity. First, we define adversarial attacks onGCL models and describe how AT is applied to train robust GNNmodels given an attacked graph (.1). Then, we theoret-ically analyze how a graph can be effectively attacked, followedby an empirical study to show the characteristics of adversarialattacks on graphs (.2). Lastly, based on the characteristicsof adversarial attacks and the AT procedure on GCL, we describe alimitation of existing adversarial GCL models (.3).",
  "A,X = argmaxA,X EL( (A1 + A, X1 + X), (A2, X2))(2)": "where = {(A,X)|A0 A, X0 X} is the possibleset of the perturbations, and A, X are perturbation budgets forthe edge perturbations A and the feature perturbations X, re-spectively. When A is a discrete matrix and X consists of binaryvariables, we use the 0-norm (i.e., count the number of nonzeroelements of a vector) for the distance of A and X. When X is con-tinuous variables, we use -norm. Hence, the above formulationaims to find the optimal edges and node features to flip (i.e., Aand X) for the first view that maximally increase the contrastiveloss, which in turn makes the representation of the attacked view (A1 + A, X1 + X) to be dissimilar from the representation ofthe clean view (A2, X2). This formulation can be applied to thesecond view, but here we use the above equation for simplicity. Training Adversarially Robust GCL. We explain the adversarialGCL procedure whose goal is to learn robust GCL models based onAT. The main idea of the adversarial GCL is to force the representa-tions of nodes in the clean graph to be close to those of the attackedgraphs. Using the attacked graph as an additional augmentation,the adversarial GCL minimizes the training objective",
  "min L(Z1, Z2) + 1L(Z1, Zadv)(3)": "where Zadv = (A1 + A, X1 + X) is the representation of theattacked graph with optimal perturbations, is the set of modelparameters, and 1 is a hyperparameter. We hereafter denote theoptimally attacked graph (A1 +A, X1 +X) by the adversarial view.The first term in the Eqn. (3) is the contrastive loss for the twostochastically augmented views, and the second term is the ATloss for learning robust node representations. This adversarial GCLframework regularizes the representation of the original view tobe consistent with the representation of the adversarial view.",
  "Characteristic of Adversarial Attacks on GCL": "Based on the above formulation, we theoretically analyze howa graph can be effectively attacked, which results in degradingthe performance of GCL models. Note that adversarial attacks onGNNs under the supervised setting tend to connect nodes withdissimilar features, and it has been shown to greatly change themodel prediction results . Here, we show that similar attackstrategies are still effective in degrading the performance of GCLmodels, which are trained in an unsupervised manner. 4.2.1Analyses on effective graph attacks.1) Theoretical Analysis. Consider a GCL model with a 1-layerGCN encoder without nonlinearity, and assume that there is an arbi-trary edge perturbation connecting and where NA, whichis the neighbor set of given the adjacency matrix A. Our goal is tofind a single perturbation that greatly increases the contrastive loss3. Following Eqn. (2), this goal can be cast as a problem of findingthe optimal perturbation A that makes the representation of theattacked view Z = (A1 + A, X) dissimilar from those of thesecond clean view Z2. Then, the difference in the representations",
  "Feature difference term(4)": "where e = z2 z1 is the difference between the two clean views, is a small constant less than 1, and W R is the featuretransformation matrix in the GCN encoder. Note that the trainedGCL model discriminates two different instances, and e is closeto a zero vector as the difference between the two clean views isnegligible (i.e., e 0). From Eqn. (4), we observe that the distancebetween the node representations zatkiand z2i is large 1) when thedegree of an attacked node (i.e., |NA1 |) is small, and/or 2) when thefeature of the added node through perturbation (i.e., Wx) is verydifferent from the aggregation of the neighbor features of (i.e., 1layer diffusion for ). This implies that GCL models are vulnerableto adversarial attacks that connect two nodes that are of low-degreeand exhibit low feature similarity with each other. 2) Empirical Study. We further conduct an empirical study toverify whether such perturbations are indeed effective in attackingGCL models. We first train a simple GCL model (i.e., GRACE) onCiteseer and Co.CS datasets. Then, since edges (i.e., elements ofA) with high gradients greatly change the loss, we visualize thegradient values of the contrastive loss with respect to A as shownin (a). As expected, we observe that the adversarial attacks onGRACE are mainly generated between low-degree and dissimilarnodes, which corroborates our theoretical analysis.",
  "Limitation of Existing Adversarial GCL": "As previously demonstrated, adversarial attacks on graphs tendto connect nodes with dissimilar features. Given such an attackedgraph, an adversarial GCL model aims to learn robust node repre-sentations by reducing the distance between the clean view and theadversarial view (Eqn. (2),(3)), where the adversarial view containsmore edges that connect nodes with dissimilar features (Eqn. (4)). However, although the perturbations in the adversarial view areimperceptible, the neighborhood feature distribution of each nodehas changed due to the aforementioned characteristic of adversar-ial attacks on graphs. Hence, we argue that as existing adversarialGCL models force the clean view to be close to the adversarialview while neglecting such changes in the neighborhood featuredistributions in the adversarial view, they obtain robustness at theexpense of losing the feature information, which is an unexpectedconsequence of applying AT to GCL models. Empirical Study. To verify our argument, we conduct empiricalstudies on several recent adversarial GCL models to investigatewhether AT indeed fails to preserve the node feature similarity((b)). To this end, we measure how much feature similarityin the node features X is preserved in the node representations Zlearned by adversarial GCL model. Specifically, we first construct-nearest neighbor graphs using X and Z, and denote them byANN(X) and ANN(Z), respectively, and compute the overlapping(OL) score between the two graphs as follows :",
  "|ANN(X) |(5)": "where |ANN(X) | is the number of nonzero elements in ANN(X),and ANN(Z) ANN(X) is the intersection of two matrices, i.e.,element-wise product of two matrices. Note that a high score in-dicates a high overlap between two matrices ANN(Z) and ANN(X),which implies that nodes with similar representations also havesimilar features. In other words, if the score is high, the noderepresentations contain more information about the node features. AT fails to Preserve Node Similarity. (b) shows the score of GRACE and GRACE-AT, which is the adversarially trainedGCL model built upon GRACE. We observe that the scores ofGRACE-AT is lower than those of GRACE, which does not employAT, across all the perturbation ratios, although they outperformGRACE in terms of node classification accuracy. This implies thatthe learned representations of adversarial GCL model (i.e., GRACE-AT) fail to preserve the feature information while becoming robustto structural attacks, which means that AT encourages the learnednode representations to contain less feature information. However,as demonstrated by previous studies , the node featureinformation is crucial for defending against graph structure attacks,and we argue that the robustness against structure attacks of adver-sarially trained GCL model can be further enhanced fully exploiting",
  "Similarity Preserving Adversarial Graph Contrastive LearningKDD23, August 06-10, 2023, Long Beach, CA": "the node feature information. Hence, in the following section, we pro-pose an adversarial GCL framework that aims to preserve the nodefeature similarity information while being robust to adversarialattacks. Hence, in the following section, we propose an adversarialGCL framework that aims to preserve the node feature similarityinformation while being robust to adversarial attacks.",
  "PROPOSED METHOD: SP-AGCL": "In this section, we propose Similarirty Preserving AdversarialGraph Contrastive Learning (SP-AGCL), a framework for the robustunsupervised graph representation learning. SP-AGCL achieves ad-versarial robustness, while preserving the node feature similarityby introducing two auxiliary views for contrastive learning, i,e. thesimilarity-preserving view and the adversarial view. Model Overview. The overall architecture of SP-AGCL is de-scribed in of Appendix A. First, SP-AGCL generates fourviews that contain different properties: two stochastically aug-mented views, one similarity preserving view for retaining thefeature similarity, and one adversarial view for achieving the adver-sarial robustness against graph attacks. We then encode the viewswith GCN layers. Finally, we optimize the cross-view contrastivelearning objective based on the encoded views. By optimizing thecross-view objective, the learned node representations obtain theadversarial robustness and enriched feature information.",
  "View Generation": "Given a graph (A, X), we first generate two stochastically aug-mented views (A1, X1) and (A2, X2) both of which are processedby randomly dropping edges and node features as in . Then,we construct auxiliary views, i.e., the similarity-preserving viewand the adversarial view. 5.1.1Similarity-preserving view. The similarity-preserving viewaims to preserve the node similarity information in terms of nodefeatures (i.e., X). We first construct a top- similarity matrix (i.e.,ANN(X)) based on X using the -nearest neighbor algorithm . More precisely, we select the top- most similar nodes foreach node based on the node feature matrix X. The main reasonfor introducing the similarity-preserving view (i.e., (ANN(X), X))is to provide self-supervision signals regarding the node featureinformation to other views, so that the representations of nodeswith similar features are pulled together, which in turn preservesthe node feature similarity. This is in contrast to existing adversarialGCL models that obtain robustness against adversarial attacks atthe expense of losing the node feature similarity information. It isimportant to note that as the node similarity-preserving view isconstructed solely based on the raw features of nodes regardless ofthe graph structure (i.e., structure-free), the model is particularlyrobust against structure poisoning attacks . As a result, SP-AGCL isrelatively robust even on severly attacked graph, as will be latershown in the experiments. 5.1.2Adversarial view. The adversarial view (Aadv, Xadv) is an aug-mented view generated by attacking the clean view (A1, X1) follow-ing Eqn. (2). We herein focus on generating an adversarial view thatfurther exploits the node feature information to achieve robustness of an adversarially trained model. In this regard, we present the ad-versarial feature mask that is applied along with the structural per-turbations. The most common approach for finding the structuraland feature perturbations (i.e., A and X) is to utilize the gradient-based perturbations that greatly change the contrastive loss.1) Structural Perturbations. For structural perturbations, wefollow the similar procedure as to flip edges by computing the",
  "A1 + L": "A2 = GA R ), the optimal edges to flip(i.e.,A) are determined. More precisely, for positive elements of GA,we take the corresponding edges with large gradients and add themto A1 to generate Aadv. Moreover, for negative elements of GA, wetake the corresponding edges with small gradients, and delete themfrom A to generate Aadv. Note that the number of edges to add anddelete is within the perturbation budget (i.e., A0 A).2) Adversarial Feature Mask. For node feature perturbations, astrategy similar to the structural perturbation can be adopted toflip the node features (i.e., change from 0 to 1 and from 1 to 0) as in. However, when the feature flipping strategy is applied tonode feature perturbations, the co-occurrence/correlation statisticsof nodes are significantly altered . Such a behavior may havean adverse effect on the AT by making the clean view and theadversarial view too distant from each other. Hence, to perturb nodefeatures while retaining the co-occurrence/correlation statistics ofnode features, we propose to mask (i.e., only change from 1 to0) features that greatly increase the contrastive loss. Specifically,considering that we are interested in changing the 1s in the featurematrix X to 0s, we mask the node features with small gradientsin the negative direction, since doing so will greatly increase theloss. More formally, to obtain the adversarial mask, we compute the",
  "X1 + L": "X2 = GX R ), the adversarial feature mask M is obtained. More precisely,for negative GX, we take the node features with small gradients togenerate the mask M. That is, M = 0 if the -th feature of node has a small gradient, otherwise M = 1, where the number of zerosin the mask M is within the perturbation budget (i.e., M0 X).Then, we apply M to obtain the node features of the adversarial view(i.e., Xadv) as follows: Xadv = MX1, where is hadamard productfor matrices. By masking the node features that play an importantrole in the contrastive learning, and using it as another view in theGCL framework, we expect to learn node representations that areinvariant to masked features, which encourages the GCL model tofully exploit the node feature information.3) Adversarial View Generation. Combining the result of struc-tural perturbations (i.e., Aadv) and the result of adversarial fea-ture masking (i.e., Xadv), the adversarial view can be obtained (i.e.,(Aadv, Xadv)), and this view contains both structural and featureperturbations, which makes the adversarial view more helpful forachieving adversarial robustness.Applicability to large networks. Since computing the gradientwith respect to both A and X requires expensive cost, adversarialGCL models are generally not scalable to large graphs. Hence, we",
  "SP-AGCL85.50.382.30.280.70.279.90.178.60.278.00.282.10.280.10.278.70.577.90.477.20.5": "Baselines. The baselines include the state-of-the-art unsupervisedgraph representation learning (i.e., GRACE, GCA, BGRL) and de-fense methods (i.e., DGI-ADV, ARIEL). Additionally, we includeGRACE-MLP, which uses an MLP encoder instead of a GCN en-coder, as a baseline that focuses on the node feature informationrather than the graph structure. We describe the details of baselinemodels in Appendix C.1. Evaluation Protocol. We evaluate SP-AGCL and the baselinesunder the poisoning attack and evasive attack. The poisoning at-tack indicates that the graph is attacked before the model training,whereas the evasive attack contains the perturbations only afterthe model parameters are trained on the clean graph . For eachsetting, we use the public attacked graph datasets offered by that contain both untargeted attacks metattack and targeted attacksnettack for the three citation networks. For the co-purchase andco-authorship networks, we create attacked graphs using byrepeatedly sampling 3,000 nodes and attacking with metattack dueto the large size of these datasets.Implementation Details. The implementation details are de-scribed in Appendix C.2.",
  "use the random 1:1:8 split for training, validation, and testing forthe co-purchase and the co-authorship networks": "Result. We first evaluate the robustness of SP-AGCL under non-targeted attack (metattack). In , we have the following twoobservations: 1) SP-AGCL consistently outperforms other base-lines under both the poisoning and the evasive attacks, which in-dicate that SP-AGCL effectively achieves adversarial robustnesswith similarity-preserving view and the adversarial view. 2) Theimprovements of GRACE-MLP and SP-AGCL are especially largerunder severe perturbation ratios. This implies the benefit of exploit-ing the node feature information for learning robust representationsunder severe perturbation ratios.We further investigate the performance of SP-AGCL and base-lines according to the degree of nodes. shows that SP-AGCL out-performs all the baselines on low-degree nodes, and particularlyoutperforms under severe perturbations, which corroborates ourtheoretical and empirical studies conducted in .2. That is,as adversarial attacks on GCL models tend to be generated betweenlow-degree nodes as shown in .2, it becomes particularlycrucial to preserve the node feature similarity for low-degree nodesunder severe perturbations. In this respect, as SP-AGCL focuseson preserving the node feature similarity, it is relatively more ro-bust against attacks on low-degree nodes compared with existingadversarial GCL models.",
  ": t-SNE visualization of node repre-sentations in Citeseer dataset under metat-tack": ".2.1 of the main paper, we train each model (i.e., the noderepresentations) in an unsupervised manner, and then evaluate itwith the linear evaluation protocol as in . We use the samesplit and attacked graph structure as in for the three citationnetworks, where nettack is used to generate attacks on specificnodes, which aims at fooling GNNs predictions on those attackednodes. To be specific, we increase the number of adversarial edges,which are connected to the targeted attacked nodes, from 1 to 5to consider various targeted attack setups. Then, we evaluate thenode classification accuracy on dozens of targeted test nodes withdegree larger than 10.Result. In , we observe that SP-AGCL achieves the state-of-the-art performance under targeted attack (nettack). Similar to theresults in Section. 6.2.1, the performance gap between SP-AGCL andthe baselines gets larger as the number of perturbations for each tar-geted node increases. The above result implies that exploiting morefeature information is helpful for defending the targeted attack,which verifies the effectiveness of SP-AGCL. 6.2.3Node classification under random perturbations.Setting. We randomly add fake edges into the graph structure togenerate attacked graph structure, and then evaluate each modelin the same way as above. We use the three citation networks withthe same data split as in and set the number of added fakededges as from 20% to 100% of the number of clean edges.Result. In , we observe that SP-AGCL consistently outper-forms other baselines given randomly perturbed graph structure.Specifically, SP-AGCL demonstrates its robustness under both thepoisoning and the evasive attacks setting, showing similar resultsas reported in Section. 6.2.1 and .2.2. The result impliesthat SP-AGCL is robust to random perturbations by preserving thefeature similarity and exploiting more feature information.",
  "Preserving Feature Similarity": "Analysis on Feature Similarity. To verify whether SP-AGCL pre-serves the node feature similarity, we compute scores of SP-AGCL in (b) (Refer to Eqn. (5)). We observe that SP-AGCL hasthe highest scores on Citeseer and Co.CS datasets. This impliesthat SP-AGCL preserves the feature similarity information unlikethe adversarially trained GCL model whose node representationslose the feature similarity.Benefit of Preserving Feature similarity. We further investigatethe benefits of preserving feature similarity in other downstreamtasks (i.e., link prediction & node clustering). (a) and (b) showthe result on Co.CS and Co.Physics datasets under metattack. For link prediction, we closely follow a commonly used setting and use area under curve (AUC) as the evaluation metric. Weobserve that SP-AGCL consistently predicts reliable links comparedwith other baselines across all the perturbation ratios. Furthermore,ARIEL, the state-of-the-art adversarial GCL model, shows the worstperformance. We argue that node feature information is beneficialto predicting reliable links since nodes with similar features tend tobe adjacent in many real-world graphs . Hence, our proposedadversarial feature masking and similarity-preserving view play animportant role in predicting reliable links since they make the noderepresentations retain more feature information. For node clustering,we perform-means clustering on the learned node representations,where is set to the number of classes, to verify whether the clustersare separable in terms of the class labels. We use normalized mutualinformation (NMI) as the evaluation metric. We observe that SP-AGCL consistently outperforms ARIEL in node clustering as well,which demonstrates that preserving the node feature informationis crucial as it is highly related to class information. Visualization of Representation Space. We visualize the noderepresentations of ARIEL and SP-AGCL via t-SNE to intuitivelysee the effect of preserving the node feature information in the rep-resentation space. shows the node representations of ARIELand SP-AGCL trained on a citeseer graph under 25% metattack. Weobserve that the representations of ARIEL are separable but widelydistributed resulting in vague class boundaries. On the other hand,the representations of SP-AGCL are more tightly grouped together,resulting in more separable class boundaries. We attribute such adifference to the AT of ARIEL that incurs a loss of node feature in-formation, which is preserved in SP-AGCL. Furthermore, the vagueclass boundaries of ARIEL explain the poor performance of ARIELin the node clustering task shown in (b).",
  "Experiments on Real-World Scenarios": "Node classification with Noisy Label. We compare SP-AGCL withboth supervised (i.e., RGCN , ProGNN , and SimP-GCN )and unsupervised defense methods (i.e., DGI-ADV and ARIEL) toconfirm the effectiveness of models when the label information con-tains noise. We train the models on the clean and poisoned Citeseerdataset, whose label noise rates are varied. In , we observe thatthe unsupervised methods (especially SP-AGCL) outperform thesupervised methods at relatively high noise rates (i.e., 40% and 50%).This is because the node representations of the supervised defensemethods are not well generated in terms of the downstream tasksince they heavily rely on the supervision signals obtained from the",
  ": Node classification accuracy with noisy label": "noisy node labels. Moreover, we observe that SP-AGCL outperformsunsupervised methods (i.e., DGI-ADV and ARIEL), and the perfor-mance gap gets larger as the label noise rate increases. We arguethat this is mainly because SP-AGCL better exploits feature infor-mation, which results in more robust node representations undernoisy labels demonstrating practicality of SP-AGCL in reality.",
  "SP-AGCL57.52.541.11.932.31.364.96.858.45.564.33.6": "Node classification on Heterophilous Networks. Through-out this paper, we showed that preserving node feature similar-ity is crucial when graphs are poisoned/attacked. In fact, a het-erophilous network in which nodes with dissimilar properties(e.g., node features and labels) are connected can be considered asa poisoned/attacked graph considering the behavior of adversarialattacks described in . Hence, in this section, we evaluate SP-AGCL on six commonly used heterophilous networks benchmarkin terms of node classification. In , we observe that SP-AGCL outperforms baselines on heterophilous networks. Moreover,ARIEL, which is an adversarially trained variant of GRACE, per-forms worse than GRACE. This implies that adversarial trainingfails to preserve the feature similarity, and that the feature similar-ity should be preserved when the given structural information isnot helpful (as in heterophilous networks).",
  "Ablation Study": "To evaluate the importance of each component of SP-AGCL, i.e.,similarity-preserving view (SP), and the feature perturbation inadversarial view generation (Feat. Ptb), we incrementally add themto a baseline model, which is GRACE with structural pertur-bations described in .1.2.1. As for the adversarial viewgeneration, we compare our proposed feature masking strategywith the feature flipping strategy . We have the followingobservations in . 1) Adding the similarity-preserving viewis helpful, especially under severe structural perturbations, whichdemonstrates the benefit of preserving the node feature similarity inachieving adversarial robustness against graph structural attacks. 2)When considering the adversarial view, adding the feature maskingcomponent is helpful in general, which again demonstrates the im-portance of exploiting the node feature information. 3) Comparingthe strategies for the feature perturbations, our proposed maskingstrategy outperforms the flipping strategy, even though the mask-ing strategy requires less computations. We attribute this to thefact that the feature flip greatly alters the co-occurrence/correlation",
  "Complexity Analyses": "Analysis on Computational Efficiency We compare the trainingtime and the attacked view generation time of ARIEL, and SP-AGCL on Co.Physics dataset. For a fair comparison, ARIEL and SP-AGCL utilize the same size of subgraphs during training. In and , we observe that SP-AGCL is faster than ARIEL in termsof both the training and the view generation. This implies that ourproposed adversarial view generation is scalable compared with thePGD attack used in ARIEL, which creates an adversarial viewthrough repeated iterations and complex optimization. In additionto the fast model training, shows that SP-AGCL convergesfaster than ARIEL. As a result, SP-AGCL is proven to be the mostefficient and effective method.",
  "CONCLUSIONS": "In this paper, we discover that adversarial GCL models obtain ro-bustness against adversarial attacks at the expense of not beingable to preserve the node feature similarity information throughtheoretical and empirical studies. Based on our findings, we pro-pose SP-AGCL that learns robust node representations that pre-serve the node feature similarity by introducing the similarity-preserving view. Moreover, the proposed adversarial feature mask-ing exploits more feature information. We verify the effective-ness of SP-AGCL by conducting extensive experiments on thirteenbenchmark datasets with multiple attacking scenarios along withseveral real-world scenarios such as networks with noisy labelsand heterophily.",
  "Aleksandar Bojchevski and Stephan Gnnemann. 2019. Adversarial Attacks onNode Embeddings via Graph Poisoning. In Proceedings of the 36th InternationalConference on Machine Learning": "Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020.A Simple Framework for Contrastive Learning of Visual Representations. InProceedings of the 37th International Conference on Machine Learning. Yu Chen, Lingfei Wu, and Mohammed Zaki. 2020. Iterative deep graph learningfor graph neural networks: Better and robust node embeddings. Advances inNeural Information Processing Systems 33 (2020), 1931419326. Enyan Dai, Charu Aggarwal, and Suhang Wang. 2021. Nrgnn: Learning a labelnoise resistant graph neural network on sparsely and noisily labeled graphs. InProceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & DataMining. 227236.",
  "Kaveh Hassani and Amir Hosein Khasahmadi. 2020. Contrastive Multi-ViewRepresentation Learning on Graphs. In Proceedings of International Conferenceon Machine Learning. 34513461": "R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, PhilBachman, Adam Trischler, and Yoshua Bengio. 2018. Learning deep represen-tations by mutual information estimation and maximization. arXiv preprintarXiv:1808.06670 (2018). Wei Jin, Tyler Derr, Yiqi Wang, Yao Ma, Zitao Liu, and Jiliang Tang. 2021. Nodesimilarity preserving graph convolutional networks. In Proceedings of the 14thACM International Conference on Web Search and Data Mining. 148156. Wei Jin, Yao Ma, Xiaorui Liu, Xianfeng Tang, Suhang Wang, and Jiliang Tang.2020. Graph structure learning for robust graph neural networks. In Proceedingsof the 26th ACM SIGKDD International Conference on Knowledge Discovery & DataMining. 6674. Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification withGraph Convolutional Networks. In Proceedings of the 5th International Conferenceon Learning Representations (Palais des Congrs Neptune, Toulon, France) (ICLR17). Namkyeong Lee, Dongmin Hyun, Junseok Lee, and Chanyoung Park. 2022. Rela-tional self-supervised learning on graphs. In Proceedings of the 31st ACM Interna-tional Conference on Information & Knowledge Management. 10541063.",
  "Lichao Sun, Yingtong Dou, Carl Yang, Ji Wang, Philip S Yu, Lifang He, and BoLi. 2018. Adversarial attack and defense on graph data: A survey. arXiv preprintarXiv:1812.10528 (2018)": "Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei.2015. Line: Large-scale information network embedding. In Proceedings of the24th international conference on world wide web. 10671077. Xianfeng Tang, Yandong Li, Yiwei Sun, Huaxiu Yao, Prasenjit Mitra, and SuhangWang. 2020. Transferring robustness for graph neural network against poisoningattacks. In Proceedings of the 13th international conference on web search and datamining. 600608. Shantanu Thakoor, Corentin Tallec, Mohammad Gheshlaghi Azar, Mehdi Azabou,Eva L Dyer, Remi Munos, Petar Velikovi, and Michal Valko. 2021. Large-scalerepresentation learning on graphs via bootstrapping. In International Conferenceon Learning Representations.",
  "Petar Velikovi, William Fedus, William L Hamilton, Pietro Li, Yoshua Bengio,and R Devon Hjelm. 2018. Deep graph infomax. arXiv preprint arXiv:1809.10341(2018)": "Xiao Wang, Meiqi Zhu, Deyu Bo, Peng Cui, Chuan Shi, and Jian Pei. 2020. Am-gcn: Adaptive multi-channel graph convolutional networks. In Proceedings ofthe 26th ACM SIGKDD International conference on knowledge discovery & datamining. 12431253. Jiarong Xu, Yang Yang, Junru Chen, Xin Jiang, Chunping Wang, Jiangang Lu, andYizhou Sun. 2022. Unsupervised adversarially-robust representation learning ongraphs. In Proceedings of the AAAI Conference on Artificial Intelligence. Kaidi Xu, Hongge Che, Sijia Liu, Pin-Yu Chen, Tsui-Wei Weng, Mingyi Hong,and Xue Lin. 2019. Topology Attack and Defense for Graph Neural Networks:An Optimization Perspective. In Proceedings of the Twenty-Eighth InternationalJoint Conference on Artificial Intelligence (IJCAI-19). Jinliang Yuan, Hualei Yu, Meng Cao, Ming Xu, Junyuan Xie, and Chongjun Wang.2021. Semi-Supervised and Self-Supervised Classification with Multi-View GraphNeural Networks. In Proceedings of the 30th ACM International Conference onInformation & Knowledge Management. 24662476.",
  "C.2Implementation Details": "For each experiment, we report the average performance of 10 runswith standard deviations. For GRACE, GCA, BGRL, and ARIEL,we use the best hyperparameter settings presented in their papers.For DGI-ADV, we use the default hyperparameter settings in itsimplementation.For SP-AGCL, we tune the learning rate and weight decay from{0.05, 0.01, 0.005, 0.001} and {0.01, 0.001, 0.0001, 0.00001}, respectively.For generating two augmented views, we tune drop edge/featurerates from {0.1, 0.2, 0.3, 0.4, 0.5}. For generating the adversarial view,we tune edge perturbation budget A and feature masking ratio Xfrom {0.1, 0.3, 0.5, 0.7, 0.9} and {0.0, 0.1, 0.3, 0.5, 0.7, 0.9}, respectively.For generating the similarity-preserving view, we tune of NNgraph from {5, 10}. Moreover, we tune the combination coefficient1 and 2 from {0.1, 0.5, 1,2,3,4,5}.",
  "DADDITIONAL EXPERIMENTAL RESULTS": "In this section, we conduct three additional experiments to analyzethe sensitivity of SP-AGCL to the hyperparameters including 1) theperturbation budgets of the graph structure and node features A,X, 2) the coefficients of objective 1, 2, and 3) value of the NNalgorithm in similarity-preserving view generation.D.1Sensitivity analysis on A and XWe analyze the sensitivity of the perturbation budgets for gener-ating the adversarial view. Specifically, we change the edge pertur-bation ratio A and the feature masking ratio X from 0.1 to 0.9 toconfirm that the performance of SP-AGCL is insensitive to the per-turbation ratio as AT is performed. In , we observe that theperformance of SP-AGCL is consistently better than that of ARIELin terms of the accuracy on attacked Cora and Citeseer datasets.This implies that SP-AGCL is not sensitive to the perturbation bud-gets, showing that both perturbing the graph structure and maskingthe feature are helpful for enhancing the robustness of SP-AGCL.",
  "D.2Sensitivity analysis on 1 and 2": "We analyze the sensitivity of the coefficients of the training objec-tive 1 and 2 in Eqn (6) of the main paper. Note that 1 determinesthe importance of the adversarial view, while 2 determines the im-portance of the similarity-preserving view. We conduct a grid searchfor the two hyperparameters with the values in {0.1, 0.5, 1, 2, 3, 4, 5}.In , we observe that SP-AGCL generally outperforms ARIELin terms of the accuracy on attacked Cora and Citeseer datasets,showing that SP-AGCL is not senstivie to the selection of 1 and2. However, when 2 is relatively small (i.e., 2 = 0.1, 0.5), SP-AGCL shows comparable or worse performance than ARIEL. Inother words, SP-AGCL becomes more robust when the similarity-preserving view plays more significant role in the learning objective(i.e., when 2 is large). This implies that the similarity-preservingview is important for achieving robustness of SP-AGCL.",
  "D.3Sensitivity analysis on NN": "We analyze the sensitivity of SP-AGCL over the number of near-est neighbors (i.e., value of NN) for generating the similarity-preserving view. To be specific, we increase the value of from 10to 50 (i.e., {10,15,30,50}), and evaluate the accuracy of SP-AGCL andARIEL. In , we observe that the performance of SP-AGCL isnot only insensitive to value, but also greatly outperforms ARIELregardless of the value of in terms of the accuracy on attackedCora and Citeseer datasets. Moreover, this implies that employingthe node feature similarity is helpful for learning robust represen-tations of GCL models regardless of the values."
}