{
  "Abstract": "Hyperbolic geometry have shown significant potential in modelingcomplex structured data, particularly those with underlying tree-like and hierarchical structures. Despite the impressive performanceof various hyperbolic neural networks across numerous domains,research on adapting the Transformer to hyperbolic space remainslimited. Previous attempts have mainly focused on modifying self-attention modules in the Transformer. However, these efforts havefallen short of developing a complete hyperbolic Transformer. Thisstems primarily from: (i) the absence of well-defined modules inhyperbolic space, including linear transformation layers, Layer-Norm layers, activation functions, dropout operations, etc. (ii) thequadratic time complexity of the existing hyperbolic self-attentionmodule w.r.t the number of input tokens, which hinders its scalabil-ity. To address these challenges, we propose, Hypformer, a novelhyperbolic Transformer based on the Lorentz model of hyperbolicgeometry. In Hypformer, we introduce two foundational blocksthat define the essential modules of the Transformer in hyperbolicspace. Furthermore, we develop a linear self-attention mechanismin hyperbolic space, enabling hyperbolic Transformer to processbillion-scale graph data and long-sequence inputs for the first time.Our experimental results confirm the effectiveness and efficiency ofHypformer across various datasets, demonstrating its potential asan effective and scalable solution for large-scale data representationand large models.",
  "Computing methodologies Machine learning; Knowledgerepresentation and reasoning; Mathematics of computing Geometric topology": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08",
  "Database": ": In a variety of real-world scenes, when we classifyinstances in a dataset (e.g., classifying the node in an Arxiv pa-per), we can group them into larger groups (e.g., Physics, Com-puter Science) that contain smaller subgroups (e.g., {QuantumComputing, Geometry}, {Database, Artificial Intelligence}),which may also contain even smaller sub-subgroups. Therelationships between these various levels of groups andsubgroups can be represented by dendrograms, which aretree-like structures that reveal the underlying hierarchies inthe data.",
  "Transformer; Hyperbolic geometry; Linear self-attention; Founda-tion model": "ACM Reference Format:Menglin Yang, Harshit Verma, Delvin Ce Zhang, Jiahong Liu, Irwin King,and Rex Ying. 2024. Hypformer: Exploring Efficient Hyperbolic Trans-former Fully in Hyperbolic Space. In Proceedings of the 30th ACM SIGKDDConference on Knowledge Discovery and Data Mining (KDD 24), August2529, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.",
  "KDD 24, August 2529, 2024, Barcelona, SpainMenglin Yang et al": "data can be organized into large and abstract groups that encom-pass small and specific subgroups, which can further be subdividedinto even smaller and more specific sub-subgroups, and so on. Therelationships between these groups and subgroups can be effec-tively approximated by tree-like structures . This hierarchicalrepresentation mirrors human cognitive processes , makingit an intuitive approach to data representation.Recent initiatives have explored the use of hyperbolic learningspaces to encode complex non-Euclidean data, achieving impres-sive performance in representing tree-like data . This success is attributed to the unique prop-erty of hyperbolic space, which expands exponentially comparedto the polynomial expansion of Euclidean spaces. This propertyaligns hyperbolic space with the metric of trees, making it particu-larly suitable for representing tree-like or hierarchically structureddata . Despite the growing interest in hyperbolic representa-tion and deep learning, the Transformer , a cornerstonemodel in the various domains, was seldom explored within therealm of hyperbolic space. Despite preliminary attempts in hyper-bolic Transformers , numerous challenges remain to beaddressed.Challenge (1): Insufficient definitions for operations in thehyperbolic Transformer. Prior works of HAN and HNN++ primarily concentrated on the self-attention module, yet they fellshort of constructing a comprehensive Transformer architecture,lacking basic components such as LayerNorm layer and positionalencoding layer. This is primarily due to the inadequate definitionof fundamental operations in previous studies.Challenge (2): Inefficient and ineffective definitions forlinear transformation in the hyperbolic Transformer. Whilesome techniques employ the tangent space to achieve thelinear transformation, they often necessitate frequent logarithmicand exponential mappings, heavily dependent on the tangent spaceat the origin. This leads to an increased computational load, ac-cumulation of mapping errors, and unstable training procedures.Although Chen et al. introduced a fully Lorentz linear trans-formation in hyperbolic space, it is constrained by its immutablecurvature and normalization term.Challenge (3): Absence of a linear attention mechanismin hyperbolic Transformer. The hyperbolic self-attention mech-anisms proposed by Gulcehre et al. , Shimizu et al. , andChen et al. exhibit quadratic time complexity, posing a signifi-cant challenge when handling long-sequence input and large-scalegraph data.Proposed work: In this work, we propose an efficient hyper-bolic Transformer, referred to as Hypformer. In particular, to ad-dress Challenges (1) and (2), we propose two foundational blocks,Hyperbolic Transformation with Curvatures (HTC) and HyperbolicReadjustment and Refinement with Curvatures (HRC), to build allessential modules in the hyperbolic Transformer. HTC and HRCare built on the Lorentz model of hyperbolic geometry, workingdirectly on the hyperbolic space without frequently mapping. HTCdefines the linear transformation and facilitates mapping from ahyperbolic space with one curvature to another different curvaturewhile preserving the relative distance. HRC further enables thedefinition of basic operations commonly used in the Transformer, such as LayerNorm layer, activation function, dropout, and con-catenation, within a hyperbolic context. To tackle Challenge (3),we introduce a self-attention mechanism in Hypformer with linearcomplexity, enabling efficient large-scale data processing.To validate the effectiveness of the proposed methodology, wehave undertaken extensive experiments across a diverse range oftasks. These include graph analysis , text classi-fication , and image classification . The empiricalevidence gathered from these experiments indicates that the pro-posed method significantly reduces the GPU computation costby a factor of 10 and concurrently halves the training time com-pared with the existing hyperbolic softmax attention. Furthermore,the proposed method consistently surpasses the performance ofcompetitive baselines, yielding substantial improvements on bothtree-like and non-tree-like datasets.Contributions. In summary, this study offers the following con-tributions: First, we introduce two fundamental hyperbolic blocks,HTC and HRC. Building upon these, we have formulated funda-mental modules for linear transformation, LayerNorm, activationfunction, dropout, and concatenation operations within a hyper-bolic context. Second, we propose the first hyperbolic linear at-tention mechanism, which enables the hyperbolic Transformer tobe scalable and efficient. Based on the above efforts, we constructa Hypformer1, the first comprehensive and efficient hyperbolicTransformer model fully designed to operate within hyperbolicspace. Last, we extend the hyperbolic model to handle billion-levelgraph data for the first time, laying a crucial foundation for theapplication of big data and large-scale models.",
  "Related Work2.1Hyperbolic Neural Networks": "Recent studies have demonstrated that hyperbolic space is particu-larly adept at capturing the hierarchical and tree-like structures . Building on hyperbolic space, a varietyof hyperbolic neural networks, HNN , HAN , HNN++ ,HGCN , HGNN , F-HNN , Poincar Resnet , HGTM have been developed to leverage the advantages of the hyperbolicgeometry. These neural networks have obtained an impressiveperformance in domains like computer vision , naturallanguage processing , recommender systems , graph learning and so on .",
  "Transformer and Hyperbolic Transformer": "Introduced by Vaswani et al. , Transformer models have broughtabout a paradigm shift in the field of artificial intelligence. Trans-former has made a tremendous impact in manyfields, such as language understanding , image process-ing and graph learning . A well-known concern withself-attention is the quadratic time complexity, which can hindermodel scalability in many settings. Efficient self-attention modelsare crucial in applications that model long sequences .Despite these advancements, existing Transformer architecturespredominantly operate within the Euclidean domain. There havebeen limited attempts to extend these models to hyperbolic and",
  "Hypformer: Exploring Efficient Hyperbolic Transformer Fully in Hyperbolic SpaceKDD 24, August 2529, 2024, Barcelona, Spain": "other non-Euclidean spaces. Gulcehre et al. proposed hyper-bolic attention networks, which replace the dot-product betweenthe query and key in self-attention with a function of negativehyperbolic distance. They then utilize the Einstein midpoint tocompute the attentive output with value. Similarly, Chen et al. and Shimizu et al. adopt similar strategies that result in theattentive output with key being based on the Lorentzian midpointand gyromidpoint, respectively.2 However, these methods exhibitquadratic time complexity, limiting their scalability. Besides, theyfocused more on the self-attention module and did not define theessential modules, like LayerNorm in Transformer. Recently, Choet al. proposed a fully Product-Stereographic Transformer, pre-senting a kernelized approach to non-Euclidean attention, whichis linear time complexity. However, this method heavily relies onthe tangent space, necessitating frequent mappings between thetangent space and manifolds. Ermolov et al. proposed mappingthe last layer features obtained from a Euclidean Transformer tohyperbolic space, which essentially does not establish a true Hy-perbolic Transformer. Our work aims to address these challengesand further the development of hyperbolic Transformers.",
  "Lorentz Model of Hyperbolic Geometry": "There are several isometric models of hyper-bolic geometry that have been employed in prior research. In thisstudy, we choose the Lorentz model as the foundational frameworkdue to the numerical stability it offers . Also, the proposedHypformer can be easily adapted to other hyperbolic models, asthey are isometrically equivalent.Lorentz Model. An -dimensional Lorentz model with negativeconstant curvature ( < 0) is a Riemannian manifold denotedby L,. The corresponding Riemannian metric is given by =diag(1/, 1, , 1). Each point in L, can be represented as x = x",
  "L, :=x R+1 | x, xL = 1/, > 0.(1)": "Here, x, yL = + x y = xy represents the Lorentzianinner product. Lorentz model, also known as the hyperboloid model,is an upper hyper-surface in an ( + 1) dimensional Minkowskispace with the origin point ( 1/, 0, , 0). Lorentz model has itsroots in the theory of special relativity and employs terminol-ogy borrowed from this field. The hyperboloids axis of symmetry,represented by the 0-th element , is referred to as the time-likedimension, while all other axes x are called space-like dimensions.Tangent Space of Lorentz Model. Given x L,, the tan-gent space TxL, :=u R+1 | u, xL = 0 is the orthogonalspace of L, at x with respect to the Lorentzian inner product.3 To 2In theory, the Einstein midpoint, Lorentzian centroid, and gyromidpoint are equivalentmidpoint operations projected onto each manifold .3The orthogonality condition u, xL = 0 ensures that u lies in the tangent space,preserving the manifolds geometry.",
  "+ 1": ": Illustration of hyperbolic softmax attention definedon Lorentz model. Unlike the attention mechanism in Eu-clidean space, this hyperbolic attention obtains the similaritybetween Q and K by a negative hyperbolic distance definedin Equation (5). The time complexity is quadratic w.r.t thenumber of input tokens. achieve the mapping from the Lorentz model to the tangent spaceat x, we can use the logarithmic map, logx : L, TxL,. Theexponential map defines the inverse process, expx : TxL, L,.For the details about exponential, logarithmic maps and the relevantdistance functions, please refer to Appendix A.",
  "=1 Sim Q, K V,(2)": "where WQ, WK, WV R are projection matrices and Sim(, )denotes the similarity function. Modern Euclidean Transformersprimarily use Softmax attention where similarity is calculatedas Sim(Q, K) = expQK / . In this scenario, the attentionmap is derived by computing the similarity between all query-keypairs, which results in a computational complexity of O 2.The concept of hyperbolic self-attention, as defined by previousworks , bears a similar idea to Equation (2). presents an illustration for this hyperbolic operation on Lorentzmodel. It can be expressed as follows:",
  "Lorentz Transformation": "Lorentz Tangent Space Transformation. Previous works mainly define hyperbolic linear transformations by thetangent space method, termed as LTT. Given the Lorentz embed-ding vector x and operation function , the tangent space methodmaps x to the tangent space at a local reference point by the log-arithmic map. Then, the transformation operation is applied inthis tangent space. Finally, the resulting vector is mapped back tothe Lorentz model using the exponential mapping, that is5,",
  "LTT (x; ,1,2) := exp1o ( (log2o (x))),(6)": "where o is the local reference point (generally the origin point),and the curvatures 1 and 2 could be different since they sharethe same tangent space. Using this method, previous works definethe linear transformation, neighbors aggregation, dropout, andnon-linear activation .Limitations. While this method is intuitive, it has notable limi-tations. First, parallel computation is feasible if the same referencepoint is used for the entire embedding. However, this approachcan lead to significant mapping errors for distant points due to thepoint-specific nature of the tangent space. Conversely, using local-specific points enhances accuracy but increases computational loadby requiring separate mappings. Second, frequent use of hyperbolicfunctions like cosh or cosh1 can destabilize learning. While theclamp function can mitigate this issue, its use may compromisecomputational precision.Fully Lorentz Transformation. To overcome the above limita-tions, Chen et al. defined an alternative Lorentz transformationwithout using tangent space, termed as LTF:",
  "W(x)+b (W(x) + b). Here, is the sigmoid function, b and are bias terms, > 0 controls the": "4Here we use subscript H other than L since it is not limited to Lorentz mdoel.5Some studies proposed an improved version of tangential linear transfor-mations only on the space-like dimension and then incorporated a zero value to thetransformed results, in order to respect the constraints of the tangent space at theorigin. They have a similar formula as Equation (6), which we omit for brevity. scaling range, and is the activation function. Depending on thetype of function, it can perform different operations. For instance,for dropout, the operation function is (Wx, v) = W dropout (x).Limitations. There are several limitations to this method. First,the curvature is unchangeable. Although it appears that LTF pro-vides a way to directly modify in Equation (7), this modificationresults in a loss of previously learned information, introducing dis-tortions. Direct alteration of curvature cannot guarantee the preser-vation of relative distance relationships within the learned embedding.The derivation is shown as follows:",
  "where time =x2 1/ y2 1/": "It can be observed that changing results in a non-linear trans-formation of the Lorentz distance L . Consequently, the relativedistances between data points may not be preserved as they werein the original Lorentz space. Even small changes in the param-eter can significantly affect the resulting distances, potentiallydistorting the previously learned hierarchical structure.Second, the requirement for the W matrix and normalizationterm pose another challenge. In , W is applied to both time-like and space-like dimensions , in order to achieve Lorentz boostsand rotations simultaneously. However, its introduction constrainsthe usage of certain functions. For instance, dropout, activationoperation do not necessarily interact with the matrix W. Taking theReLU activation function as an example, it only requires filtering outnegative values without needing matrix multiplication in Euclideanspace. Additionally, Chen et al. introduced a normalization termthat constrains the value within a limited range, thereby limitingthe expressiveness of the transformation.Lastly, some basic operations, such as LayerNorm and Concate-nation, cannot be achieved within this definition.",
  "Method": "The proposed method is designed to overcome the limitations ofthe existing attempts in hyperbolic Transformer, as outlined inthe . To address Challenges (1) and (2), we designed twofoundational blocks, namely HTC and HRC in .1, and4.2, respectively. To overcome Challenge (3), we developed a hy-perbolic linear attention module in .3, which equips theTransformer with linear time complexity.",
  "Hyperbolic Linear Attention": ": Illustration of Hyperbolic Linear attention. This at-tention operates in the space-like dimension (Q, K, V) andreduces the time complexity by changing the computationorder.(1) making the curvature changeable with preserving the relativeordering; (2) being disentangled with normalization term.Given a point x in Lorentz model, x L,1 (implies x R+1),and transformation matrix W R(+1) and bias R, theHTC is given as the following equation:",
  ",": "(9)where the (x; W) = W x + denotes the linear transformationwith bias addition and 1, 2 represent the curvatures before andafter the transformation. Note that HTC does not entangle a nor-malization term with this linear transformation. Besides, It is easyto prove that the defined transformation also satisfies theLorentz rotation and boost operations, described in . Theproof is similar in , we omit for brevity.The proposed HTC avoids the use of tangent space and mini-mizes the usage of logarithmic and exponential mappings in com-parison to Equation (6). When contrasted with Equation (7), thevariable curvature of the HTC enhances the flexibility of the trans-formation. This is because linear transformations generally alterthe feature dimension, and varying curvatures can express morethan a fixed one.Next, we study the theoretical aspects of the proposed HTC. Firstand foremost, we prove that HTC is closed in hyperbolic space inProposition (4.1) with different dimensions and curvatures so thatthe mapping is done correctly. Next, in Proposition (4.2), we showthat the curvature-changing strategy of the proposed HTC, alongwith the subsequent HRC, maintains the relative distance amongany points between pre and post-curvature changing.",
  "LTC(x; ; W,,) L, .(10)": "Proposition 4.2. Let z, z, z L be points in the Lorentzmodel with curvature . Consider the curvature changing trans-formations defined in HTC (Equation (7)) and HRC (Equation 13).Let z, z, z L denote the transformed points in the Lorentzmodel with curvature . The relative distances within (z, z, z) arepreserved after the curvature alteration. Specifically, if",
  "Hyperbolic Readjustment and Refinementwith Curvatures (HRC)": "Novelty. Within the Transformer, we have several basic opera-tions beyond linear transformation, which include Dropout andConcatenation, Activation function (e.g., ReLU), and LayerNorm. Weinterpret these operations within the hyperbolic space as a readjust-ment or refinement process, referred to as HRC. Similarly, given apoint x in Lorentz model, the proposed operation HRC is definedas:",
  "Hypformer": ": Framework of Hypformer. Input data (text, images, graphs) are projected onto the Lorentz model, then transformedvia Hyperbolic Linear Transformation (HTC). The result passes through the hyperbolic linear attention block with positionalencoding, followed by a Feedforward layer (built by HTC) and LayerNorm (built by HRC). This serves as an encoder which canoptionally incorporate a GNN. For classification tasks in this study, the decoder is the fully connected layer. Dropout, activation,and residual connections are omitted for brevity.",
  "where e L,2 represents the transpose of row in Q, K and V. Inthis case, represents a scaling factor, which we set as a trainableparameter in the experiments. The focused strategy is inspired": "by the work in . A > 1 sharpens the paired points, i.e., itenhances the similarity within each group while diminishing thesimilarity between the groups. Conversely, a < 1 has the oppositeeffect.This linear attention approach allows us to handle large datasetsand long sequences more efficiently while respecting the propertiesof the Lorentz model.",
  "|x + pL|.(20)": "Here, p := HTC(x) functions as a Lorentz position vector, and specifies the magnitude of p and we use 1 in our experiments. Thisdefinition calculates the midpoint between x and p, with respectto the Lorentz constraint. We add the positional encoding beforethe linear transformation in the self-attention block. We reservethe exploration of more advanced positional encoding for futureworks.",
  "Hyperbolic LayerNorm, Dropout, Activation,and Concatenation": "LayerNorm, Dropout, Activation, and Concatenation are fundamentalcomponents of the Transformer architecture. For these operations,we employ HRC in our definitions. This choice is motivated by thefact that these functions are performed within the same referencesystem and do not involve a time-like dimension. Consequently,we define our operations as follows6:HypLayerNorm(X) = HRC(X, ),",
  "Hypformer80.4 0.589.4 0.373.2 0.266.1 0.4": "where , , , and as well as represent traditional Euclidean Dropout, LayerNorm, and Activationfunctions, respectively. In general, we define as unchanged beforeand after the HRC. In the actual implementation process, for twooperations that appear consecutively, such as 1 = and2 = , we merge them into = 1 2 for computationalefficiency.",
  "Overall Architecture": "The framework of Hypformer is shown in , it can accepta variety of data types, such as text, images, and graphs. Duringthe data preparation phase, the input data is mapped to the Lorentzmodel using an exponential map7. This mapped embedding is thentransformed using a HTC layer. In the encoder part of Hypformer,the transformed data is processed through a hyperbolic linear at-tention block with hyperbolic position encoding. This is followedby the Feedforward layer implemented by HTC, and LayerNormlayer built by HRC. For graph-based inputs, we incorporate thegraph neural networks and adopt the parallel paradigm forTransformer and GNN encoder to form a graph Transformer model.The processed data is then forwarded to the decoder. The decodercan either be the similar structure of encoder, hyperbolic multino-mial logistic regression (HypMLR) or a tailored design, weleave it in future exploration. In this research, the decoder is a fullyconnected layer used for classification tasks.Time complexity. In the proposed Hypformer, the linear atten-tion module is the main computational bottleneck. The complexitycomes from two key operations. In Equation (16), we perform aspace-like inner product computation of K and V within theLorentz model, which incurs a complexity of O(2). Followingthis, we calculate the inner product of these results with Q, whichalso has a complexity of O(2). Given that << , the total",
  "This step is necessary since most data are built from Euclidean space": "computational complexity of our method is O(). When dealingwith graph inputs, the computational complexity of a GNN model istypical O( +), where represents the number of edges. Owing tothe typical sparsity of graphs (i.e., << 2), the proposed methodcan scale linearly with respect to the number of nodes in a graph.This design make Hypformer operate on graphs with billion-levelnodes.",
  "Experiments": "In this work, we propose a novel hyperbolic Transformer with linearcomplexity, which is especially well-suited for processing graph-structured data. Graphs often exhibit intricate topological and hier-archical relationships, making them an ideal testbed for evaluatingthe effectiveness of our proposed hyperbolic Transformer. As such,we primarily focus on comparing our models performance withother state-of-the-art graph models.",
  "Experiments on Large Graphs": "Experimental Settings. We first evaluate Hypformer on diverselarge-scale graphs for node classification, with node counts rang-ing from millions to billions, including ogbn-arxiv, ogbn-protein,and Papers100M (for dataset details, see Appendix C.1). To ourknowledge, this represents the first application of hyperbolic ornon-Euclidean transformations to graphs of this scale. Our com-parative analysis focuses on state-of-the-art Euclidean GNNs andgraph Transformers. We evaluate Hypformer against a spectrumof baselines, including MLP, GCN , SGC ), advanced GNNvariants (SIGN , GCN-NSampler, GAT-NSampler), recent graphTransformer architectures (GraphFormer , GraphTrans ,GraphGPS , NodeFormer , SGFormer ) and hyperbolicmodels HAN , HNN++ and F-HNN .Experimental Findings. summarizes the results ofour experiments. Hypformer consistently outperforms other mod-els across various large-scale graph datasets, demonstrating sub-stantial improvements. It is worth noting that models, such asGraphFormer , GraphTrans , and GraphGPS , HAN ,HNN++ and F-HNN , have difficulty operating effectivelyon large-scale graph data. In addition, our method significantlyoutperforms the recent approaches such as, SGFormer and Node-Former accross all tested scenarios, highlighting its superior effec-tivness. Importantly, Hypformer exhibits robust scalability, main-taining its performance advantage even on the largest dataset, ogbn-papers100M, where previous Transformer-based models have en-countered limitations.",
  "Experiments on Small/Medium Graphs": "To complement our large-scale evaluations, we assessed Hypformeron small- and medium-scale graph datasets. This additional testingallows for a more comprehensive comparison against current state-of-the-art models, including GNNs, graph transformers, and hyper-bolic approaches that may not scale effectively to larger datasets.By expanding our evaluation scope, we aim to isolate Hypformerseffectiveness in graph learning from its scalability advantages.Experimental Settings. We conducted experiments on fivesmall/medium-scale graph datasets, adhering closely to the settingsused in HGCN works . These datasets included three low-degree",
  "ModelsDiseaseAirportCoraCiteseerPubMed#Nodes1, 0442, 6652, 7083, 32719, 717#Edges1, 0432, 6645, 4294, 73288, 651": "GCN 69.7 0.481.4 0.681.3 0.371.6 0.478.1 0.2GAT 70.4 0.481.5 0.383.0 0.772.5 1.179.0 0.3SGC 69.1 0.682.1 0.580.1 0.271.9 0.178.7 0.1HGNN 81.3 3.584.7 1.077.1 0.870.0 1.078.3 1.2HGCN 88.2 0.789.3 1.276.5 0.668.0 0.678.0 1.0HGAT 90.3 0.689.6 1.077.4 0.768.6 0.378.3 1.4GraphFormer 75.2 0.088.1 1.260.0 0.561.4 0.673.3 0.7GraphTrans 89.3 3.294.3 0.677.6 0.865.1 1.477.5 0.7GraphGPS 92.8 2.794.5 0.973.0 1.462.0 1.572.8 1.4FPS-T 88.6 0.996.0 0.682.3 0.770.0 0.778.5 0.6HAN 85.1 0.892.9 0.683.1 0.572.4 0.579.0 0.6HNN++ 89.5 0.292.3 0.382.8 0.671.5 1.379.9 0.4F-HNN 92.3 1.193.0 0.781.0 0.771.2 0.477.5 0.8NodeFormer 75.9 0.980.2 0.682.2 0.972.5 1.179.9 1.0SGFormer 89.0 3.992.9 0.583.2 0.972.2 0.380.0 0.8",
  "Hypformer93.0 0.795.0 0.585.0 0.373.3 0.481.3 0.3": "hyperbolicity datasets: citeseer, cora , and PubMed , aswell as two high-degree hyperbolicity datasets: Airport and Dis-ease. The number of nodes and edges are shown in . Fordata split and processing, please refer to Appendix C.2.Experimental Findings. showcases all the experimentalresults8. Our findings suggest that the proposed method signifi-cantly surpasses both standard GNNs and hyperbolic GNN modelsby a substantial margin. Importantly, the method exhibits effec-tiveness not only in scenarios with hyperbolic datasets (like Dis-ease, Airport) but also in situations with non-hyperbolic dataset(like Cora, CiteSeer and PubMed). The existing hyperbolic GNNmodel had a notable deficiency in this non-hyperbolic datasets.However, by introducing a hyperbolic Transformer, we have suc-cessfully overcome this problem. This thanks to that Transformerspossess long-distance learning capabilities. However, on datasetssuch as Cora, Citeseer, and PubMed, the existing graph Trans-formers cannot perform well. The primary reason might be that theTransformer equals a fully linked aggregation, which will introducesubstantial noise. Nevertheless, our method employs linear-focusedattention to solve this issue effectively.",
  "Comparisons on Text and Vision Datasets": "Additionally, we apply our model to semi-supervised image andtext classification tasks on the Mini-ImageNet and 20News-Groupsdatasets. We also construct a graph using k-NN (based on input nodefeatures) to utilize graph model. These experiments are conductedclosely in Nodeformer. More comprehensive details are provided inAppendix C.3. presents the comparative results for varying values. Notably, our method outperforms in seven out of eightcases. In contrast, the performance of competing baselines modelsvarying significantly with different k values, while our methoddemonstrates greater stability.",
  "Analysis": "Scalability of Hypformer. We conducted additional tests on themodels scalability regarding the number of nodes in a single batch.The Amazon2M dataset was used, and we randomly selected asubset of nodes, with the number of nodes varying from 10K to200K. We made a comparison between softmax attention defined byEquation (3) and linear attention defined by Equation (16), keepingall other parameters the same. As depicted in , the memoryusage of the proposed method exhibits a linear increase with thesize of the graph. When the node count exceeds 40K, the softmaxattention experiences an out-of-memory (OOM) issue. However,the proposed method continues to function effectively, resulting ina 10X reduction in GPU cost.Efficiency and Effectiveness of Hypformer. The linear atten-tion designed for Hypformer enhances its efficiency significantly. presents the efficiency of both softmax attention and lin-ear attention within Hypformer.9 As indicated in , the pro-posed linear attention mechanism significantly reduces the trainingtime by half compared to the softmax attention in Hypformer. Fur-thermore, The left subfigure in presents the performancecomparison between Hypformer equipped with Softmax attention(Hypformer(S)) and Linear attention (Hypformer(L)). The resultsdemonstrate that both models perform well, with the linear atten-tion exhibiting better accuracy.Effectiveness of Curvature . In this work, we propose thatboth the HTC and HRC basic blocks involve two variable curvatures.In our experiment, we set these as trainable parameters. In the right, we compare the impact of varying and fixed curvatureon the Hypformer. Experiments show that varying can alwaysperform better than the unified one.Ablation Study. To gain a deeper understanding of the proposedHyperbolic Transformers effectiveness, we conducted an ablationstudy on three diverse datasets. We compared the performanceof the original Hyperbolic Transformer with two variants: onewithout the graph component (W/o Graph) and another without",
  "= 5 = 10 = 15 = 20 = 5 = 10 = 15 = 20": "GCN 84.86 0.4285.61 0.4085.93 0.5985.96 0.6665.98 0.6864.13 0.8862.95 0.7062.59 0.62GAT 84.70 0.4885.24 0.4285.41 0.4385.37 0.5164.06 0.4462.51 0.7161.38 0.8860.80 0.59DropEdge 83.91 0.2485.35 0.4485.25 0.6385.81 0.6564.46 0.4364.01 0.4262.46 0.5162.68 0.71IDGL 83.63 0.3284.41 0.3585.50 0.2485.66 0.4265.09 1.2363.41 1.2661.57 0.5262.21 0.79LDS OOMOOMOOMOOM66.15 0.3664.70 1.0763.51 0.6463.51 1.75NodeFormer 86.77 0.4586.74 0.2386.87 0.4186.64 0.4266.01 1.1865.21 1.1464.69 1.3164.55 0.97SGFormer86.21 0.6686.46 0.6186.73 0.8486.76 0.7268.55 0.5467.96 0.6866.44 0.8765.46 0.59",
  ": Left: Comparison of the proposed linear atten-tion and softmax attention on small/medium datasets. Right:Comparison of unified curvature (Hypformer(L, C)) and vary-ing curvature (Hypformer(L, C+))": "the Transformer component (W/o Transformer). The results of thisstudy are presented in .For the Cora dataset, a citation network, removing the graphcomponent leads to a substantial performance drop. This indicatesthe crucial role of the graph structure in capturing the relation-ships between nodes in this context. The Transformer componentalone (W/o Graph) is insufficient for effectively modeling node in-teractions. Conversely, removing the Transformer component (W/oTransformer) still yields reasonable performance, highlighting theimportance of the graph component for this dataset. In the case ofthe ogbn-proteins dataset, which represents a protein-protein inter-action network, both the graph and Transformer components con-tribute significantly to the models performance. This suggests thatthe interplay between the graph structure and the Transformersability to capture long-range dependencies is essential for accu-rately modeling the complex interactions in this biological network.For the 20news dataset, which comprises textual data, the graphis constructed from the original features and may not accuratelyreflect the true relationships between documents. In this case, the",
  "Cora64.6 0.582.8 0.385.0 0.3ogbn-proteins70.0 0.275.9 0.480.4 0.5Mini-ImageNet87.7 0.685.8 0.587.4 0.7": "model performs best when the graph component is removed (W/oGraph), indicating that the graph structure might not be as infor-mative for this particular dataset. The Hyperbolic Transformercomponent alone is sufficient to capture the semantic relationshipsbetween documents. These findings underscore the adaptability ofthe Hyperbolic Transformer to various datasets and its ability toleverage both graph structure and long-range dependencies whenappropriate.",
  "Conclusion": "In this work, we introduce a efficient hyperbolic Transformer,Hypformer. This method operates directly and fully on hyperbolicrepresentations and employs a linear attention mechanism, en-abling it to be both scalable and effective. Furthermore, this studyintroduces two basic blocks, HTC and HRC, which are foundationalin constructing hyperbolic models. Nonetheless, the research pre-sented is an initial exploration and numerous challenges warrantfurther investigation. These include the initial determination of acurvature that better reflects the data geometry, the setting of cur-vature at different levels for Hypformer, and the design of effectivedecoders for different downstream tasks. We plan to address theseissues in our future work.",
  "Acknowledgements": "We express gratitude to the anonymous reviewers and area chairsfor their valuable comments and suggestions. In this study, MenglinYang was partly supported by Tony Massini Postdoctoral Fellowshipin Data Science from Yale University. Jiahong Liu and Irwin Kingwere partly supported by the Research Grants Council of the HongKong Special Administrative Region, China (CUHK14222922,RGCGRF 2151185).",
  "Yu Chen, Lingfei Wu, and Mohammed Zaki. 2020. Iterative deep graph learningfor graph neural networks: Better and robust node embeddings. In NeurIPS,Vol. 33. 1931419326": "Yankai Chen, Menglin Yang, Yingxue Zhang, Mengchen Zhao, Ziqiao Meng,Jianye Hao, and Irwin King. 2022. Modeling scale-free graphs with hyperbolicgeometry for knowledge-aware recommendation. In WSDM. 94102. Sungjun Cho, Seunghyuk Cho, Sungwoo Park, Hankook Lee, Honglak Lee, andMoontae Lee. 2023. Curve Your Attention: Mixed-Curvature Transformers forGraph Representation Learning. arXiv preprint arXiv:2309.04082 (2023).",
  "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT:Pre-training of deep bidirectional transformers for language understanding. arXivpreprint arXiv:1810.04805 (2018)": "Ankit Dhall, Anastasia Makarova, Octavian Ganea, Dario Pavllo, Michael Greeff,and Andreas Krause. 2020. Hierarchical image classification using entailmentcone embeddings. In Proceedings of the CVPR workshops. 836837. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, GeorgHeigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformersfor image recognition at scale. arXiv preprint arXiv:2010.11929 (2020). Aleksandr Ermolov, Leyla Mirvakhabova, Valentin Khrulkov, Nicu Sebe, and IvanOseledets. 2022. Hyperbolic vision transformers: Combining improvements inmetric learning. In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition. 74097419.",
  "Stephen C Hirtle and John Jonides. 1985. Evidence of hierarchies in cognitivemaps. Memory & cognition 13, 3 (1985), 208217": "Joy Hsu, Jeffrey Gu, Gong Wu, Wah Chiu, and Serena Yeung. 2021. Capturingimplicit hierarchical structure in 3D biomedical images with self-supervisedhyperbolic representations. In NeurIPS, Vol. 34. 51125123. Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, BowenLiu, Michele Catasta, and Jure Leskovec. 2020. Open graph benchmark: Datasetsfor machine learning on graphs. In NeurIPS, Vol. 33. 2211822133.",
  "Gal Mishne, Zhengchao Wan, Yusu Wang, and Sheng Yang. 2023. The numericalstability of hyperbolic representation learning. In International Conference onMachine Learning. PMLR, 2492524949": "Sebastien Montella, Lina M Rojas Barahona, and Johannes Heinecke. 2021. Hy-perbolic Temporal Knowledge Graph Embeddings with Relational and TimeCurvatures. In Findings of the ACL: ACL-IJCNLP 2021. 32963308. Galileo Namata, Ben London, Lise Getoor, Bert Huang, and U Edu. 2012. Query-driven active surveying for collective classification. In 10th international workshopon mining and learning with graphs, Vol. 8. 1.",
  "Lucas Vinh Tran, Yi Tay, Shuai Zhang, Gao Cong, and Xiaoli Li. 2020. HyperML:A Boosting Metric Learning Approach in Hyperbolic Space for RecommenderSystems. In WSDM. New York, NY, USA, 609617": "Hao Wang, Defu Lian, Hanghang Tong, Qi Liu, Zhenya Huang, and EnhongChen. 2021. HyperSoRec: Exploiting Hyperbolic User and Item Representationswith Multiple Aspects for Social-aware Recommendation. TOIS (2021), 128. Hongyu Wang, Shuming Ma, Shaohan Huang, Li Dong, Wenhui Wang, ZhiliangPeng, Yu Wu, Payal Bajaj, Saksham Singhal, Alon Benhaim, et al. 2022. Foundationtransformers. arXiv preprint arXiv:2210.06423 (2022).",
  "Qitian Wu, Wentao Zhao, Zenan Li, David P Wipf, and Junchi Yan. 2022. Node-former: A scalable graph structure learning transformer for node classification.In NeurIPS, Vol. 35. 2738727401": "Qitian Wu, Wentao Zhao, Chenxiao Yang, Hengrui Zhang, Fan Nie, Haitian Jiang,Yatao Bian, and Junchi Yan. 2023. Simplifying and empowering transformers forlarge-graph representations. In NeurIPS, Vol. 36. Zhanghao Wu, Paras Jain, Matthew Wright, Azalia Mirhoseini, Joseph E Gonzalez,and Ion Stoica. 2021. Representing long-range context for graph neural networkswith global attention. In NeurIPS, Vol. 34. 1326613279.",
  "CData Processing and Experimental DetailsC.1Data Processing for Large-graph Data": "We employ the public splits offered by OGB for ogbn-proteinsand ogbn-arxiv datasets. Additionally, we assess our approach us-ing models on the Amazon2M item co-occurrence network, whichcomprises 2.45 million nodes and 61.86 million edges. For Ama-zon2M, we follow the same splits used in recent studies .The largest dataset we employ is ogbn-papers100M, boasting animpressive 0.11 billion nodes and 1.61 billion edges. We also adhereto the publicly available OGB splits for this dataset.",
  "C.2Data Processing for Medium-graph Data": "We used standard splits for the citation networks. For the Air-port and Disease datasets, the train/val/test splits were 70%/15%/15%and 30%/10%/60%, respectively, which is the same as . We reportthe results of five runs on the node classification task. For Diseaseand Airport, which are imbalanced, we report the F1-score. For theother datasets, we report the accuracy. Baselines. For the baselines,we compare Hypformer against the basic GNN models, including GCN , GAT and SGC . For Hyperbolic GNN models, weutilized HGCN , LGCN and HGNN as the competitors.We also compared with state-of-the-art Euclidean graph Transform-ers models viz. Graphomer , GraphTrans , GraphGPS ,NodeFormer and SGFormer . Graphormer suggested theincorporation of edge connectivities into the model by employingshortest-path distances to bias the attention mechanism. Graph-Trans introduced a permutation-invariant Transformer modulecombined with a GNN module. While, Nodeformer, GraphGPS andSGFormer each introduced linear attention mechanisms. Specifi-cally, Nodeformer employed a kernelized Gumbel-Softmax, whileGraphGPS seperated the local real-edge aggregation and the fully-connected Transformer to achieve this complexity. Besides, we alsocompared with non-Euclidean transformers, i.e., the HAN andFPS-T .",
  "C.3Data Processing for Text and Image Data": "We tested our model on two datasets without a graph structure:20News-Groups and Mini-ImageNet. For our experiment, we se-lected 30 classes from the dataset, each with 600 images with 128features extracted by a CNN. These settings closely follow the Node-former . For each dataset, we randomly allocate instances intotraining, validation, and testing sets, comprising 50%, 25%, and 25%of the data, respectively. Following existing works , we alsoconstruct a graph using k-NN (based on input node features) to facil-itate the message passing of GNN and the graph transformer. All thedatasets we used in the experiment were directly sourced, exceptfor Mini-ImageNet, for which we extracted the features ourselves.Following the approach of , we computed node embeddingsusing a CNN model with four convolutional layers followed by afully connected layer, resulting in a 128-dimensional embedding.These 128-dimensional outputs are then used as the features ofthe nodes (images) for subsequent tasks based on Graph NeuralNetworks (GNNs).",
  "C.4Implementation Details": "In the reported results, we mainly refer to findings from severalrelevant works for the baseline comparisons . For the mostrelevant studies, such as SGFormer, other Graph Transformers, wereproduce results using identical experimental settings to ensure afair comparison. It is important to note that the results of SGFormercannot be fully reproduced due to errors in its official code imple-mentation. To maintain the integrity of our analysis, we reportthe performance of SGFormer based on the available informationwhile acknowledging the discrepancy caused by the implementa-tion issues. Our experimental setup for Hypformer mainly followsthe configurations used in SGformer. Additionally, we performedparameter tuning for the input curvature and output curvature, ex-ploring values within [1.0, 2.0, 3.0]. This is grounded in our hypoth-esis that the input attributes and hidden states belong to differentcurvature spaces. While a more detailed curvature setting could beemployed, we leave this for future exploration. Furthermore, weconducted a parameter search for in Equation (19) within [1.0, 2.0,3.0]. Regarding the decoder, we created Euclidean and hyperbolicclassifiers for experiments, with the Euclidean classifier performingbetter in most cases."
}