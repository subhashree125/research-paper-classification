{
  "ABSTRACT": "Concentrated Solar Power (CSP) plants store energy by heating astorage medium with an array of mirrors that focus sunlight ontosolar receivers atop a central tower. Operating at high temperaturesthese receivers face risks such as freezing, deformation, and corro-sion, leading to operational failures, downtime, or costly equipmentdamage. We study the problem of anomaly detection (AD) in se-quences of thermal images collected over a year from an operationalCSP plant. These images are captured at irregular intervals rangingfrom one to five minutes throughout the day by infrared camerasmounted on solar receivers. Our goal is to develop a method to ex-tract useful representations from high-dimensional thermal imagesfor AD. It should be able to handle temporal features of the data,which include irregularity, temporal dependency between imagesand non-stationarity due to a strong daily seasonal pattern. Theco-occurrence of low-temperature anomalies that resemble normalimages from the start and the end of the operational cycle withhigh-temperature anomalies poses an additional challenge. We firstevaluate state-of-the-art deep image-based AD methods, whichhave been shown to be effective in deriving meaningful image rep-resentations for the detection of anomalies. Then, we introduce aforecasting-based AD method that predicts future thermal imagesfrom past sequences and timestamps via a deep sequence model.This method effectively captures specific temporal data featuresand distinguishes between difficult-to-detect temperature-basedanomalies. Our experiments demonstrate the effectiveness of ourapproach compared to multiple SOTA baselines across multipleevaluation metrics. We have also successfully deployed our solu-tion on five months of unseen data, providing critical insights to ourindustry partner for the maintenance of the CSP plant. Our code1",
  "Computing methodologies Machine learning; Anomalydetection; Unsupervised learning": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08",
  "INTRODUCTION": "The focus on renewable energies to counteract climate changehas intensified recently. However, a critical challenge in adoptingrenewable energy sources is ensuring on-demand generation anddispatchability. A promising solution to this challenge is the integra-tion of Thermal Energy Storage (TES) facilities, which temporarilystore energy by heating or cooling a storage medium, such as wateror molten salt. Concentrated Solar Power (CSP) plants effectivelyutilize TES for storing energy by heating the medium with an ar-ray of mirrors focused on solar receivers atop a central tower .These solar receivers are composed of vertical heat exchanger tubesarranged in panel form, allowing the medium to flow through them.Operating at extreme temperatures, these systems are prone toadverse effects, including the freezing of the medium (affectinga subset of vertical tubes with significantly higher temperatures),damage to heat-resistant coatings, and deformation and corrosionof the heat exchanger tubes. Therefore, meticulous monitoring ofthe process is crucial. Given the vast amount of data generatedfrom multiple sensors, manually detecting abnormal behaviours be-comes impractical. This necessitates an automated system capableof immediately identifying abnormal behaviours. The advantages ofsuch a system are twofold: it ensures smooth operation and uninter-rupted power generation by minimizing downtime, and it reducesthe risk of further equipment damage by allowing for prompt fail-ure responses. This approach also leads to an extended operationallifetime for the CSP plant.In this paper, our goal is to develop a deep image-based anomalydetection (AD) method to identify abnormal behaviours insequences of thermal images collected over a span of one year froman operational CSP plant. These images are captured at irregularintervals ranging from one to five minutes throughout the day byinfrared cameras mounted on solar receivers. Our problem is relatedto data-driven Predictive Maintenance (PdM), where the state ofequipment in industrial processes is monitored to predict futurefailures .",
  "KDD 24, August 2529, 2024, Barcelona, SpainSukanya Patra, Nicolas Sournac, and Souhaib Ben Taieb": "Samet Akcay, Dick Ameln, Ashwin Vaidya, Barath Lakshmanan, Nilesh Ahuja,and Utku Genc. 2022. Anomalib: A Deep Learning Library for Anomaly Detection.In 2022 IEEE International Conference on Image Processing (ICIP). IEEE, 17061710. Kilian Batzner, Lars Heckler, and Rebecca Knig. 2024. EfficientAD: AccurateVisual Anomaly Detection at Millisecond-Level Latencies. In Proceedings of theIEEE/CVF Winter Conference on Applications of Computer Vision (WACV). 128138. Paul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger. 2019.MVTEC ad-A comprehensive real-world dataset for unsupervised anomaly detec-tion. Proceedings of the IEEE Computer Society Conference on Computer Vision andPattern Recognition (2019), 95849592.",
  "RELATED WORK": "AD has been extensively studied over several decades . Themajor AD methods can be broadly classified into four categories -density-based, reconstruction-based, classification-based approaches,and feature embedding-based methods. Density-based methods. The density-based approach aims to es-timate the probability distribution of normal data by assuming thatnormal samples are more likely to occur under the estimated dis-tribution than anomalous samples. Traditional methods fit a model to arbitrary data distribution but encounter challengesin higher-dimensional input spaces due to the curse of dimension-ality. To overcome this, they are often applied to low-dimensionallatent representations obtained using techniques like Autoencoder(AE) and Variational Autoencoder (VAE). Neural generative mod-els, such as VAE and Generative Adversarial Networks (GANs),are deep learning-based methods that estimate a neural networksparameters to map a predetermined source distribution to the in-put data distribution. Recent AD methods such as CFlow andFastFlow further build on normalizing flows. However, stud-ies demonstrated that normalizing flows often struggle to detectanomalies and assign them a higher likelihood . Reconstruction-based methods. Reconstruction-based methodsoperate on the assumption that encoder-decoder models trainedon normal samples will exhibit poor performance for anomaloussamples. Common deep reconstruction models used include AE orVAE-based approaches, while advanced strategies involve recon-struction by memorized normality , model architecture adap-tation and partial/conditional reconstruction . Recentapproach DRM trains a discriminative network alongside thereconstruction network to localize anomalies without the need forany further post-processing steps. Generative models like GANs arealso widely employed for anomaly detection, as the discriminatorinherently calculates reconstruction loss for samples . One-class classification. Anomaly detection can be approached asa one-class classification or single-class classification problem. Unlike density-based methods, classification-based tech-niques, such as One-Class Support Vector Machine (OC-SVM) ,directly estimate a decision boundary to differentiate between nor-mal and anomalous samples. However, this task can be challengingdue to imbalanced datasets, where normal samples vastly outnum-ber anomalous samples. To address this, techniques like SupportVector Data Descriptor (SVDD) derive a tight sphericalbound. To enhance the expressivity of the classical models, deeplearning models are used to learn the features from the data . Feature Embedding-based methods There are mainly two differ-ent types of feature embedding-based anomaly detection methods:memory bank , student-teacher . The main idea ofmemory bank methods is to extract features of nominal images andstore them in a memory bank during the training phase. During thetesting phase, the feature of a test image is used as a query to matchthe stored nominal features. The performance of the memory bankmethods heavily depends on the completeness of the memory bankrequiring a large number of nominal images. In the student-teacherapproaches , the student network learns to extract features ofthe nominal samples, similar to the teacher model. For anomalousimages, the features extracted by the student network should bedifferent from the teacher network.",
  "A CASE STUDY ON DETECTINGANOMALOUS BEHAVIOURS IN CSP PLANTS": "A CSP plant consists of two main components, namely: (i) theThermal Solar Receiver and (ii) the Steam Generator. The ThermalSolar Receiver placed on top of a central tower in the plant acts asa solar furnace. On the ground surrounding the tower, an array offlat, movable mirrors called heliostats concentrate the sun rays onthe solar receiver. The receiver consists of vertical heat exchangertubes through which the heat transfer medium flows, absorbing theheat from the concentrated sun rays. Then, the absorbed thermalenergy is utilized to generate superheated steam, which runs theSteam Generator for the production of energy. In this work, wefocus on detecting anomalous behaviours of the Thermal SolarReceiver using data obtained from an operational plant.CSP plants utilize high-capacity fluids like molten salts as theheat transfer medium, which are stored in TES facilities for futureuse. This allows for the on-demand generation of energy, makingCSP plants a viable alternative to fossil fuel-based energy plants.",
  "[ii] Deformity. The metal heat exchanger tubes in the receivertend to expand due to the high temperatures. Uneven dilation ofthe tubes could eventually lead to deformity": "[iii] Stress and Metal Fatigue. The metal tubes in the receiverundergo expansion when exposed to high temperatures duringregular operation and contraction when the operation ends. Suchrepeated changes lead to metal fatigue. Additionally, the pressuregenerated from the flowing molten salts exerts stress on the tubes. [iv] Corrosion. Due to the interaction of the metal with the moltensalt flowing through the tubes, it tends to deteriorate over time.These reactions are further accelerated due to the high tempera-tures in the receiver. Hence, CSP plants require close monitoring to guarantee seam-less operation and continuous power generation. Achieving thisrequires the analysis of data collected by numerous sensors in-stalled on the Solar Receiver. Yet, the vast volume of data generatedrenders manual inspection unfeasible, thus underscoring the needfor an automated, data-driven monitoring system.",
  "Data Description": "The Thermal Solar Receiver is composed of several panels, eachfeaturing vertical heat exchanger tubes. These tubes allow the heattransfer medium to flow through, effectively absorbing heat fromthe concentrated sunlight. Infrared (IR) cameras, strategically posi-tioned around the solar receiver, capture the surface temperature,producing thermal images with dimensions of 184 608. Theseimages are captured approximately every one to five minutes, witheach images timestamp recorded. During normal operations, thetemperature of the heat transfer medium gradually increases as ittraverses the vertical tubes from one end of the panel to the other,a direct result of absorbing heat from the concentrated sunlight.Consequently, the surface temperature patterns recorded by the IRcameras are anticipated to exhibit a smooth gradient, aligning withthe mediums flow direction. Our dataset covers a year of opera-tional data without ground truth labels for the images (normal or abnormal), making it an unsupervised anomaly detection problem.Note that throughout this work, we provide normalized imagesfrom the dataset for the sake of confidentiality.Operation in CSP plants occurs across three distinct phases asdepicted in a: (i) Preheating, (ii) Filling/Draining, and (iii)Power. The molten salt used in CSP plants freezes when the temper-ature drops below a certain threshold. To avoid this, solar panelsare initially heated during the Preheating phase. Then, in the sub-sequent Filling/Draining phase, molten salt is circulated withinthe panels. The Power phase initiates as the molten salt absorbsheat from sunlight, facilitating power generation. As operationsconclude, the molten salt is drained from the panels during the Fill-ing/Draining phase. Consequently, the panels commence coolingdown, transitioning back to the Preheating. Our work focuses solelyon the Power phase, as it is crucial for power generation and sus-ceptible to damage from prolonged exposure to high temperatures.",
  "Data characteristics and modelling challenges. Through exten-sive data analysis, we identified the following additional challenges,which are essential for modelling the solar receiver data:": "[i] Non-stationarity. a presents the average surface tem-perature across a specific week, highlighting temporal variationsin the mean image temperature and demonstrating a clear patternof daily seasonality in the data. [ii] Irregular sampling. The images were captured at irregulartime intervals, as illustrated in b, which depicts the distri-bution of inter-arrival times. Additionally, the dataset lacks datafor the extended periods when the plant was not operational.",
  "Data Labelling": "To effectively assess the performance of various AD methods, wehave labelled a subset of data from the CSP plant. This endeavouris notably complex due to the plants operation across multiplephases, each characterized by unique temperature ranges. Conse-quently, this diversity leads to a range of normal and anomaloussample types, as depicted in . The challenge of identify-ing anomalies through the plants operational phases is evident ina. Notably, normal images with low temperatures at theoperations start and end (the left-most and right-most images ina) closely resemble low-temperature anomalies (the secondimage from the right in a). The distinction between thesesamples relies heavily on context.Moreover, the variable nature of anomalies adds a layer of com-plexity to the labelling process. Our approach to this challenge isinformed by a deep understanding of the CSP plants operations andexpert insights from the field. We categorize the Power phase intothree distinct segments: (i) Starting (S), where the solar receiversmean temperature begins to rise; (ii) Middle (M), where it reachesand maintains its peak; and (iii) Ending (E), where it declines asthe day concludes. In our preprocessing, we exclude days withsignificantly few samples or with a consistently low temperaturethroughout the M segment, likely indicative of sensor or systemfailures. For the S and E segments, samples showing a consistenttemperature increase (> 5C) or decrease (< -5C), respectively,are deemed normal, whereas those displaying contrary trends aremarked as anomalous. In the M segment, we apply the following",
  "four rules for labelling:": "[R1.] Difference between consecutive images. During the M seg-ment of the Power phase, we expect a stable temperature. Significantdeviations from the preceding observation indicate an abnormality.To detect such anomalies, we first compute the pixel-wise squareddifferences between every two consecutive images. For each pair,we select the 95th percentile of these pixel-wise differences as ourscore. Samples are then labelled as anomalous if their score exceedsthe 99.9th percentile of the scores for all samples in the dataset [R2.] Difference from average daily temperature. Samples withaverage temperatures that significantly deviate from the daily av-erage temperature are labelled as anomalous. To identify theseanomalies, we first compute the daily mean temperature. Then, wecalculate the difference between each images average temperatureand the mean temperature of the corresponding day, which servesas our score. Finally, samples are labelled as anomalous if their scorefalls below the 1st percentile of the distribution of scores across allsamples in the dataset. [R3.] Difference with specific daily normal samples. Rules R1and R2 are limited to the detection of low-temperature anomaloussamples. To address this, we select the first five images from theM segment of Power phase of each day to serve as a set of tem-plates for that day. We then employ a similar methodology as inRule R1, but instead of comparing an image to just the prior im-age, we compute the mean difference between the image and all",
  "Detecting Abnormal Operations in Concentrated Solar Power Plants from Irregular Sequences of Thermal ImagesKDD 24, August 2529, 2024, Barcelona, Spain": ": Illustration of the end-to-end architecture of ForecastAD. The model is trained to forecast the next image in the sequencegiven a context embedding of prior data points obtained using a sequence-to-sequence model. For (,,) D inthe context, we sum the embeddings of inter-arrival time and interval since the start of operation and concatenate itwith the image embedding. The anomaly score (,) is computed as the difference between the forecasted and original image.",
  "five templates of the corresponding day. Applying this rule allowsus to obtain sets of high-temperature normal and abnormal sam-ples, along with a diverse set of low-temperature abnormal samples": "[R4.] Freezing statistics. To identify the anomalous samples withcharacteristics such as freezing and low-temperature patches, wecompute row-wise and column-wise differences within each image.First, we calculate the maximum value of the element-wise differ-ences between two consecutive rows, which we term the horizontalscore. Next, we compute the element-wise differences between con-secutive columns and apply a Sobel filter to detect verticaledges. The mean value of the elements detected by the Sobel filteracross all columns is referred to as the vertical score. An image islabelled as anomalous if either the horizontal or the vertical scoreexceeds a predefined threshold.Given the labelling rules, we first apply them to obtain an ini-tial set of labels. Then, in collaboration with domain experts, weconduct a visual analysis of the labelled thermal images. For thevisual analysis, we also perform clustering on the labelled samplesand inspect the cluster centres in addition to analyzing each imageindividually. This thorough inspection leads to subsequent refine-ments of the labelled set, enhancing the reliability of the labels foraccurately assessing anomaly detection methods.",
  "General problem formulation": "Consider a dataset D = {(,,)}=1 consisting of = 16, 917triplets. Each X = R+ corresponds to a thermal image withdimension = , where the height is 184 and the width is 608. These images were captured at times R+, and each {0, 1} denotes the corresponding label, with 0 representing thenormal class and 1 representing the anomalous class.Let D , D and D denote disjoint training, validation and testsets, respectively, with D D D = D. D is exclusivelycomposed of normal samples, i.e., = 0 for all (,,) D .D and D include both normal and anomalous samples. Using the training set D , the AD methods aim to learn a scoringfunction (, ) : R+ R+ R that assigns an anomaly score (,)to any given point (,). By using a threshold R, this anomalyscore can then be converted into a predicted label as follows:",
  "A FORECASTING-BASED AD MODEL": "We present a new forecasting-based AD method, denoted ForecastAD,to detect anomalous operations in the Thermal Solar Receiver ofa CSP plant from irregular sequences of thermal images. The pro-posed method builds a forecasting model to reconstruct the thermalimages using past observations as context. Images that are hardto reconstruct are considered anomalous. For a given image, ourprocedure can be summarized in the following steps: (i) extract fea-ture embeddings for that image (.1), (ii) use the previous images as context and encode them using a deep sequence model(.2), and (iii) using the context, reconstruct the image witha decoder forecasting model (.3), then assign an anomalyscore based on the reconstruction error between the original andpredicted image. We provide an overview of the architecture ofForecastAD in and summarize it in Algorithm 1.",
  "Image Encoder": "Using the training data, D , we pre-train an encoder network tocapture the inherent structure of our datasets images. The imageencoder, denoted by (;) : X Z, transforms images fromthe high-dimensional input space X to a compact latent spaceZ = R, significantly reducing dimensionality where << . Weuse an autoencoder framework for image reconstruction, with adecoder network (;) : Z X to project images from thelatent space Z back to the original input space X. The autoencoderis given by = , with indicating function composition.",
  "Context Encoder": "To handle the irregular inter-arrival times = 1 betweensuccessive ()-th and ( 1)-th images, our deep sequence model in-corporates both the image sequences and their associated irregularinter-arrival times. We employ a sinusoidal encoding = sin(),inspired by the positional encoding technique in transformer mod-els . This method aligns with strategies used in Neural TemporalPoint Processes .In addition to the inter-arrival times between consecutive images,we also embed the relative time since the start of the operation 0,i.e., = 0, which provides information about the position ofan image within the operational cycle. Such temporal contexthelps in detecting challenging temperature-based anomalies, as ithelps distinguish between low-temperature anomalies occurringmid-cycle and low-temperature normal images at the start of theoperation. We use the same sinusoidal encoding for the interval as = sin(). The sum of the two time embeddings and iscombined with the image embedding to obtain the final embedding = [ ( + )], where = () represents the imageembedding and denotes the concatenation operator.We compactly encode the embeddings of the samples pre-ceding the image at timestep into a fixed-dimensional vector ,termed the context vector for the -th image. This can be accom-plished with a deep sequence model. In our implementation, weopt for an LSTM (;), parameterized by. For a given contextsequence C = {, , 1}, the hidden state is recursively up-dated from previous states as = (1, 1;), starting froma random state.",
  "Image Decoder": "To predict the -th image, we use , the past context encoding, ,the embedding of the next inter-arrival time , as well as , theembedding of the time duration since the start of the operation. Specifically, we compute = ([ ( + )];) where (;) is the decoder network. Note that the decoder networkis pre-trained along with the image encoder using the image re-construction task on the images from the training set D . Theprediction error is computed as the Frobenius norm difference be-tween the original and the forecasted image. The total training lossis obtained by averaging the prediction errors over all the trainingexamples:",
  "EXPERIMENTS5.1Baselines": "We first compare ForecastAD against simple methods, which detectanomalies based on statistical features extracted from the images.These features include the corresponding time of day, as well asthe mean, maximum, and standard deviation of the temperature, todistinguish between normal and abnormal samples. We also evalu-ate against deep image-based AD methods, namely, autoencoder,FastFlow , PatchCore , PaDiM , CFlow , DRM ,and Reverse Distillation . Deep methods have been shown to bemore effective than shallow ones for image AD , leveraging thedeep neural networks capability to extract representative featuresthrough multiple layers of abstraction.",
  "[Tr#1]": "Time of day86.99 ( 0.00)41.44 ( 0.00)79.97 ( 0.00)83.03 ( 0.00)82.86 ( 0.00)61.60 ( 0.00)61.60 ( 0.00)79.55 ( 0.00)79.41 ( 0.00)85.34 ( 0.00)85.17 ( 0.00)0.00 ( 0.00)0.00 ( 0.00)80.20 ( 0.00)80.02 ( 0.00)Negative Mean81.67 ( 0.00)46.73 ( 0.00)71.51 ( 0.00)70.49 ( 0.00)76.43 ( 0.00)10.89 ( 0.00)13.47 ( 0.00)60.83 ( 0.00)66.22 ( 0.00)74.40 ( 0.00)76.91 ( 0.00)14.33 ( 0.00)12.72 ( 0.00)65.46 ( 0.00)66.76 ( 0.00)Negative STD79.31 ( 0.00)40.76 ( 0.00)70.35 ( 0.00)73.60 ( 0.00)73.60 ( 0.00)34.10 ( 0.00)34.10 ( 0.00)67.19 ( 0.00)67.19 ( 0.00)72.49 ( 0.00)72.49 ( 0.00)14.18 ( 0.00)14.18 ( 0.00)64.66 ( 0.00)64.66 ( 0.00)Negative Max77.61 ( 0.00)44.85 ( 0.00)68.64 ( 0.00)75.10 ( 0.00)75.37 ( 0.00)14.33 ( 0.00)14.90 ( 0.00)65.24 ( 0.00)65.57 ( 0.00)75.29 ( 0.00)75.47 ( 0.00)11.80 ( 0.00)11.87 ( 0.00)65.31 ( 0.00)65.49 ( 0.00) Autoencoder98.05 ( 0.74)46.43 ( 1.61)87.87 ( 0.26)94.50 ( 1.22)94.49 ( 1.22)36.96 ( 3.72)37.19 ( 3.68)85.17 ( 0.46)85.20 ( 0.47)95.27 ( 1.06)95.26 ( 1.06)12.58 ( 0.64)12.62 ( 0.63)86.45 ( 0.54)86.46 ( 0.55)CFlow 94.68 ( 1.26)39.99 ( 2.33)82.91 ( 1.08)87.07 ( 1.67)86.98 ( 1.77)31.69 ( 1.70)32.84 ( 1.40)78.09 ( 1.45)78.20 ( 1.47)88.50 ( 1.50)88.31 ( 1.64)12.75 ( 0.30)12.55 ( 0.43)79.53 ( 1.37)79.42 ( 1.49)Deep SVDD (one-class) 52.76 ( 8.71)49.22 ( 2.66)51.85 ( 6.15)61.35 ( 1.95)58.34 ( 5.62)20.00 ( 7.87)54.10 ( 8.04)54.65 ( 2.86)57.65 ( 3.52)70.79 ( 1.77)54.97 ( 6.57)13.79 ( 0.87)13.33 ( 1.53)64.12 ( 1.60)50.54 ( 5.49)Deep SVDD (soft-boundary) 30.22 ( 6.77)49.68 ( 4.26)35.45 ( 3.84)58.18 ( 0.04)43.04 ( 5.65)7.79 ( 0.06)68.42 ( 9.38)50.01 ( 0.03)47.16 ( 3.99)73.56 ( 0.03)33.14 ( 7.34)14.37 ( 0.01)11.66 ( 1.73)66.67 ( 0.02)31.14 ( 6.59)DRM 97.70 ( 0.77)40.48 ( 2.15)87.38 ( 0.61)91.70 ( 1.08)91.81 ( 1.05)30.49 ( 3.85)30.95 ( 3.68)81.78 ( 1.39)81.94 ( 1.35)92.97 ( 0.83)93.04 ( 0.81)12.14 ( 0.92)11.94 ( 1.07)83.66 ( 0.89)83.75 ( 0.87)FastFlow 99.83 ( 0.03)47.32 ( 0.29)91.36 ( 0.25)97.39 ( 0.29)97.38 ( 0.29)42.12 ( 1.27)42.18 ( 1.29)88.43 ( 0.38)88.43 ( 0.38)97.72 ( 0.26)97.71 ( 0.26)13.22 ( 0.26)13.24 ( 0.25)89.17 ( 0.35)89.17 ( 0.35)PaDiM 99.85 ( 0.02)49.86 ( 0.47)91.23 ( 0.10)96.92 ( 0.72)96.45 ( 0.58)43.44 ( 1.91)44.76 ( 1.28)88.24 ( 0.34)88.07 ( 0.33)97.28 ( 0.65)96.86 ( 0.53)13.76 ( 0.29)13.46 ( 0.10)88.92 ( 0.43)88.66 ( 0.39)PatchCore 99.23 ( 0.08)50.58 ( 0.37)89.04 ( 0.30)95.50 ( 0.25)95.52 ( 0.25)31.29 ( 1.54)31.46 ( 1.65)85.08 ( 0.41)85.13 ( 0.43)96.21 ( 0.22)96.23 ( 0.21)15.16 ( 0.16)15.31 ( 0.19)86.77 ( 0.34)86.81 ( 0.35)Reverse Distillation 93.88 ( 1.13)41.31 ( 2.19)84.61 ( 1.54)87.01 ( 1.68)86.66 ( 1.52)35.01 ( 2.90)39.03 ( 2.51)78.58 ( 1.64)78.93 ( 1.58)89.27 ( 1.44)88.66 ( 1.31)12.91 ( 0.36)12.81 ( 0.74)81.17 ( 1.43)80.86 ( 1.40) ForecastAD99.86 ( 0.05)46.22 ( 1.06)89.89 ( 0.35)97.73 ( 0.34)97.65 ( 0.27)36.10 ( 1.19)36.62 ( 1.35)87.73 ( 0.29)87.75 ( 0.26)98.04 ( 0.29)97.97 ( 0.24)14.02 ( 0.50)14.13 ( 0.50)88.76 ( 0.25)88.75 ( 0.22)",
  "Autoencoder96.67 ( 0.77)45.92 ( 2.47)85.45 ( 1.18)96.91 ( 0.93)6.69 ( 0.35)78.61 ( 1.30)": "CFlow 84.91 ( 2.72)42.90 ( 2.71)77.38 ( 2.98)88.18 ( 2.02)6.51 ( 0.39)74.80 ( 3.24)DRM 93.52 ( 0.52)40.51 ( 1.33)85.71 ( 0.78)94.56 ( 0.44)7.62 ( 1.01)83.36 ( 1.08)FastFlow 92.38 ( 0.72)52.51 ( 1.09)89.92 ( 0.68)93.46 ( 0.60)8.87 ( 0.46)88.76 ( 0.56)PaDiM 95.99 ( 0.37)58.14 ( 1.00)92.28 ( 0.32)96.77 ( 0.32)11.50 ( 0.86)90.73 ( 0.42)",
  "endReturn: (;), (;), (;)": "architectural specifications are provided in Appendix A. The im-age encoder employed in ForecastAD mirrors the structure of thedownsampling branch in DCAE. In ForecastAD, we adopt a 4-layerLSTM network with a hidden dimension of 128 to serve as the con-text encoder . For time encoding, the sinusoidal embedding hasa dimension of 16. We adhere to the hyperparameters mentionedby the authors for the baseline methods. For ForecastAD, we useMSE and train using an Adam optimizer with a learning rate of0.001 and weight decay of 0.00001. We use a pre-processing stepfor all the experiments where the images in the dataset are resized to 256 256 to be compatible with the baselines. Unless otherwisespecified, we use a sequence length of = 30.Dataset. Our labelled dataset comprises days, which are segmentedinto training, validation, and test sets. Days featuring exclusivelynormal samples are allocated across these three sets, while thosewith anomalous samples are included in both the validation and testsets. To underscore the challenges presented by low-temperaturesamples, we adopt two training setups: (i) [Tr#1], incorporatingtraining and validation samples solely from the M phase, and (ii)[Tr#2], comprising training and validation samples from the S, M,and E phases. Importantly, the test set in both scenarios consists ofsamples spanning the S, M, and E phases. The distribution of normaland anomalous samples across S, M, and E phases for these setupsis depicted in . For ForecastAD, we generate a sequencefor each data point by selecting preceding samples. If there areless than prior samples, we duplicate the corresponding daysfirst data point to form a -length sequence. Lastly, for the firstdata point captured each day, we set the and to a small positivevalue = 1 5.Model evaluation. We evaluate the models based on the Areaunder the Receiver Operating Characteristics curve (AUROC) andthe Area under the Precision-Recall curve (AUPR). To highlightthe effectiveness of each model in distinguishing between low-temperature normal and anomalous behaviours, we utilize three testsetups containing: (i) test samples in M [Ts#1], (ii) test samples in S-E [Ts#2], and (iii) test samples in S-M-E [Ts#3]. For the experimentsbelow, we report mean over 5 runs along with one standard error.",
  "Results and Discussion": "We summarize the results over five runs for different training se-tups in . For the training setup [Tr#1], ForecastAD providescompetitive results when compared to image-based SOTA mod-els as measured by both AUROC and AUPR metrics over the testsamples in [Ts#3]. Additionally, we observe good performance forimage-based SOTA approaches in [Ts#1]. This performance canbe attributed to the training exclusively on samples from M, whichpredominantly fall within the high-temperature region where tem-poral context is less critical. However, since the models are not",
  "Ablation Study": "Importance of time-embedding and pre-training. showsthe results of an ablation study to understand the importance of and in ForecastAD. In all configurations, we always keep theimage encoding as part of the input. Firstly, we observe the lowestAUROC and AUPR scores in [Ts#2] when the context has onlythe encodings of -prior images. It emphasizes the need to addressthe challenge posed by irregular sequences and co-occurrence oflow-temperature normal and anomalous samples. Then, on con-sidering either or , we observe a significant improvement in[Ts#2]. Furthermore, incorporating both and yields the bestperformance, highlighting that both and are necessary for re-liable detection of anomalies. Lastly, we also empirically validatethe impact of pre-training the image encoder and decoder usingthe image reconstruction task. Using the pre-trained models offerssubstantial enhancements in performance when compared to arandomly-initialized backbone.",
  "( 1.09)85.81 ( 1.23)92.53 ( 0.81)96.92 ( 0.57)28.73 ( 1.70)92.97 ( 0.36)": "Effect of context length (). We report the AD performance ofForecastAD with varying context lengths in . For contextlengths 20, we do not observe any correlation between perfor-mance and context length. However, larger sequence lengths of 30or 40 yield better performance. To limit computational demands,we did not consider larger sequence lengths and chose a sequencelength of 30 for all our experiments. Effect of different architecture. In , we analyze the effectof the number of layers in LSTM and the latent dimension onthe AUROC and AUPR scores. Firstly, we observe that larger latentdimensions lead to higher scores in most cases, regardless of thenumber of layers in LSTM. Secondly, ForecastAD performs betteron [Ts#1] and [Ts#3] with a 4-layer LSTM, while a 2-layer LSTMyields better results on [Ts#2]. Based on this empirical observation,",
  "KAUROC (%)AUPR (%)[Ts#1][Ts#2][Ts#3][Ts#1][Ts#2][Ts#3]": "188.85 ( 2.55)78.26 ( 1.86)83.64 ( 1.72)92.68 ( 1.49)23.14 ( 2.63)83.22 ( 1.01)591.21 ( 0.94)87.83 ( 1.45)89.25 ( 0.81)94.44 ( 0.51)32.24 ( 2.15)89.82 ( 0.49)1094.02 ( 1.81)78.13 ( 3.91)89.82 ( 0.77)96.40 ( 0.93)23.05 ( 2.07)89.29 ( 0.86)2092.64 ( 1.21)83.22 ( 2.90)90.46 ( 1.31)95.65 ( 0.60)27.20 ( 3.45)91.39 ( 0.93) 3094.78 ( 1.09)85.81 ( 1.23)92.53 ( 0.81)96.92 ( 0.57)28.73 ( 1.70)92.97 ( 0.36)4092.66 ( 1.46)85.87 ( 0.92)91.13 ( 1.26)95.59 ( 0.73)29.24 ( 1.49)91.88 ( 0.83)5093.44 ( 0.72)87.29 ( 1.77)92.09 ( 0.45)96.06 ( 0.34)31.51 ( 1.52)92.62 ( 0.31)",
  "Interpretability of ForecastAD": "Interpretability of deep learning models is critical for high-riskapplications to enhance transparency and trustworthiness. There-fore, we extract anomaly maps from ForecastAD correspondingto each image during inference. Recall that ForecastAD is trainedwith pixel-wise regression loss, and thus, the anomaly map can becomputed as the difference between the original and forecastedimages. Based on recent works on IAD , we smoothed theanomaly maps using a Gaussian filter and normalized it using theminimum and maximum anomaly scores for the normal samplesin the validation set. In , we show the anomaly maps of4 normal and 4 anomalous test samples, along with the image forreference. It can be seen that for specific types of anomalies, such asfreezing, where we observe high-temperature streaks, ForecastADassign high anomaly scores to those regions. Therefore, it aids theinterpretability of the results from ForecastAD. To further enhancethe understanding, the anomaly maps can be complemented byplots of mean temperature to show the sudden drops or rises intemperature resulting in the samples being anomalous.",
  "Simulated Dataset": "We have prepared a simulated dataset to ensure reproducibilityand validation of the results. We use a variational autoencoder togenerate the data. Additional details about the data generation aredeferred to Appendix B. The distribution of normal and anomaloussamples across S, M, and E phases for these setups is depicted in. We have also compared our method to the baselines on thesimulated dataset. The results are reported in for trainingsetup [Tr#2] and test setup [Ts#3], which are the main focus ofour work. The results highlight the effectiveness of ForecastAD,",
  "ModelAUROC (%)AUPR (%)[Ts#1][Ts#2][Ts#3][Ts#1][Ts#2][Ts#3]": "Autoencoder87.97 ( 4.08)66.34 ( 2.49)82.00 ( 1.58)94.04 ( 1.99)24.72 ( 6.51)83.46 ( 1.61)CFlow 83.42 ( 2.97)51.32 ( 4.16)70.30 ( 2.67)90.67 ( 1.97)10.46 ( 0.80)69.42 ( 2.14)DRM 98.11 ( 0.81)61.89 ( 5.32)89.02 ( 0.81)99.02 ( 0.40)25.90 ( 4.32)88.52 ( 0.75) FastFlow 97.24 ( 0.54)52.23 ( 3.63)87.98 ( 0.67)98.43 ( 0.26)9.49 ( 0.63)87.76 ( 0.93)PaDiM 97.93 ( 0.56)56.04 ( 0.42)88.97 ( 0.44)98.76 ( 0.31)9.89 ( 0.07)88.25 ( 0.25)PatchCore 98.28 ( 0.29)66.42 ( 1.96)92.31 ( 0.31)98.81 ( 0.20)21.57 ( 2.79)92.28 ( 0.26)",
  "Deployment": "We have tested ForecastAD over five months of data from an oper-ational CSP plant. A freshly labelled dataset was curated by initiallyapplying a predefined set of labelling rules, followed by a meticu-lous review and cleanup of the dataset with guidance from domainexperts. The performance metrics of ForecastAD on this labelledset containing 8373 normal and 1,321 abnormal samples are detailedin . Furthermore, the deployment results are broken downper month over different operating stages. Please note that for somemonths, we could not compute the performance metrics as thereare no anomalous samples present in the dataset. Such cases aremarked as in the table. It is important to note that there is vari-ability in this data, such as different stages of operations (starting,ending, and middle) and varying external weather conditions. Wecan observe that ForecastAD is fairly robust in the detection ofanomalies over this period. The actionable insights derived fromForecastAD contribute to the strategic maintenance planning ofthe CSP plant, thereby enhancing the durability of its equipment.",
  "CONCLUSION": "We address the problem of anomaly detection in irregular sequencesof thermal images collected from IR cameras in an operationalCSP plant. Extensive analysis of our dataset reveals distinctivetemporal characteristics, setting it apart from established AD in-dustrial image benchmark datasets like MVTec . We empiri-cally demonstrate that image-based SOTA AD methods underper-form, especially when context is critical for anomaly detection.We also introduce a forecasting-based AD method, ForecastAD,that predicts future thermal images from past sequences and times-tamps using a deep sequence model. This method effectively cap-tures specific temporal data features and distinguishes betweendifficult-to-detect temperature-based anomalies. Experimental re-sults demonstrate the effectiveness of ForecastAD, outperformingexisting SOTA methods as measured by AUROC and AUPR. No-tably, ForecastAD exhibits significant enhancements in detectinganomalous behaviours, particularly among low-temperature sam-ples. Furthermore, ForecastAD has been successfully deployed,providing critical insights for the maintenance of the CSP plantto our industry partner. For future work, we aim to further studythe role of context and sequence lengths in anomaly detectionperformance. We also aim to extend our model to be more robustto distribution shifts inherent in industrial processes, notably byconsidering probabilistic forecasting models. This work is supported by the research project Federated Learningand Augmented Reality for Advanced Control Centers. We thankThibault GEORGES and Adrien FARINELLE from John Cockerillfor helping us understand the dataset along with the associatedabnormal behaviours.",
  "Hanqiu Deng and Xingyu Li. 2022. Anomaly detection via reverse distillationfrom one-class embedding. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition. 97379746": "Ran El-Yaniv and Mordechai Nisenson. 2006. Optimal Single-Class ClassificationStrategies. In Advances in Neural Information Processing Systems, B Schlkopf,J Platt, and T Hoffman (Eds.), Vol. 19. MIT Press. Joseph Enguehard, Babylon Health, Dan Busbridge, Adam Bozson, Claire Wood-cock, and Nils Hammerla. 2020. Neural Temporal Point Processes For ModellingElectronic Health Records. Proceedings of Machine Learning Research (7 2020),85113. Sarah M. Erfani, Sutharshan Rajasegarar, Shanika Karunasekera, and ChristopherLeckie. 2016. High-dimensional and large-scale anomaly detection using a linearone-class SVM with deep learning. Pattern Recognition 58 (10 2016), 121134. Dong Gong, Lingqiao Liu, Vuong Le, Budhaditya Saha, Moussa Reda Mansour,Svetha Venkatesh, and Anton Van Den Hengel. 2019. Memorizing Normalityto Detect Anomaly: Memory-augmented Deep Autoencoder for UnsupervisedAnomaly Detection. Proceedings of the IEEE International Conference on ComputerVision 2019-October (4 2019), 17051714. Denis Gudovskiy, Shun Ishizaka, and Kazuki Kozuka. 2021. CFLOW-AD: Real-Time Unsupervised Anomaly Detection with Localization via Conditional Nor-malizing Flows. Proceedings - 2022 IEEE/CVF Winter Conference on Applicationsof Computer Vision, WACV 2022 (7 2021), 18191828.",
  "Emanuel Parzen. 1962. On Estimation of a Probability Density Function andMode. The Annals of Mathematical Statistics 33, 3 (1962), 10651076": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury Google,Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga,Alban Desmaison, Andreas Kpf Xamla, Edward Yang, Zach Devito, MartinRaison Nabla, Alykhan Tejani, Sasank Chilamkurthy, Qure Ai, Benoit Steiner,Lu Fang Facebook, Junjie Bai Facebook, and Soumith Chintala. 2019. PyTorch:An Imperative Style, High-Performance Deep Learning Library. Advances inNeural Information Processing Systems 32 (2019). Karsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard Schlkopf, Thomas Brox,and Peter Gehler. 2022. Towards Total Recall in Industrial Anomaly Detection. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.1431814328. Lukas Ruff, Jacob R. Kauffmann, Robert A. Vandermeulen, Gregoire Montavon,Wojciech Samek, Marius Kloft, Thomas G. Dietterich, and Klaus Robert Muller.2021. A Unifying Review of Deep and Shallow Anomaly Detection. Proc. IEEE109, 5 (5 2021), 756795. Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib AhmedSiddiqui, Alexander Binder, Emmanuel Mller, and Marius Kloft. 2018. DeepOne-Class Classification. In International Conference on Machine Learning. PMLR,43934402. Bernhard Schlkopf, John C. Platt, John Shawe-Taylor, Alex J. Smola, and Robert C.Williamson. 2001. Estimating the Support of a High-Dimensional Distribu-tion. Neural Computation 13, 7 (2001), 14431471.",
  "David M.J. Tax and Robert P.W. Duin. 2004. Support Vector Data Description. Ma-chine Learning 54, 1 (2004), 4566": "Ashish Vaswani, Google Brain, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. AttentionIs All You Need. In Advances in Neural Information Processing Systems, Vol. 30.59986008. Xudong Yan, Huaidong Zhang, Xuemiao Xu, Xiaowei Hu, and Pheng-Ann Heng.2021. Learning Semantic Context from Normal Samples for Unsupervised Anom-aly Detection. Proceedings of the AAAI Conference on Artificial Intelligence 35, 4(5 2021), 31103118. Jiawei Yu, Ye Zheng, Xiang Wang, Wei Li, Yushuang Wu, Rui Zhao, and LiweiWu. 2021. FastFlow: Unsupervised Anomaly Detection and Localization via 2DNormalizing Flows. arXiv preprint arXiv:2111.07677 (11 2021). Vitjan Zavrtanik, Matej Kristan, and Danijel Skoaj. 2021. DRM - A discrimina-tively trained reconstruction embedding for surface anomaly detection. Proceed-ings of the IEEE International Conference on Computer Vision (8 2021), 83108319.",
  "BDATA GENERATION": "We generate a public dataset mirroring the properties of our privatedataset using a variational autoencoder (VAE). For daily sequencegeneration, we condition the VAE on the time of day, class (Positive,Negative, Unlabelled), and phase (Start, Middle, End). A standardconvolutional neural network (CNN) with three convolution blockswith an increasing number of feature maps is used to encode im-ages, and a multilayer perceptron (MLP) is used to encode condi-tioning variables. Then, a two-layer LSTM network produces thecontext embedding. Finally, a deconvolutional CNN, mirroring theencoders architecture, reconstructs the original image from a latentvector sampled from the latent distribution. We use a multivariateGaussian distribution with a diagonal covariance matrix as ourlatent distribution. We train the VAE over 20 epochs using Adamoptimizer, a learning rate of 1e-4, and a batch size of 16. allows us to compare the generated images with the original imagesfrom the private dataset. From the figure, we can observe that theVAE can generate normal images and a diverse set of anomalieswhich are visually similar to images in the original dataset.",
  "CSENSITIVITY TO DATA LABELLING": "Some images in the deployment set are wrongly labelled as nor-mal. Such inconsistencies result in a distribution shift between thelabelled training and deployment sets, leading to significant degra-dation of model performance. Thus, we cleaned the deploymentset by calculating the distance of each labelled normal image inthe deployment set to the images in the training set D . For this,we first obtain the context embedding for each image usingthe context encoder of ForecastAD. We then compute the distancebetween context embeddings as = min=1,...,| D | 2. Im-ages from the deployment set for which the distance exceeds apredetermined threshold are removed. shows the UMAPprojection of the context embeddings corresponding to thesamples in the training set D and the samples from the deploy-ment set that are removed during the cleaning process.",
  "[Tr#2]": "Time of day86.99 ( 0.00)41.44 ( 0.00)79.97 ( 0.00)83.03 ( 0.00)82.86 ( 0.00)61.60 ( 0.00)61.60 ( 0.00)79.55 ( 0.00)79.41 ( 0.00)85.34 ( 0.00)85.17 ( 0.00)0.00 ( 0.00)0.00 ( 0.00)80.20 ( 0.00)80.02 ( 0.00)Negative Mean81.67 ( 0.00)46.73 ( 0.00)71.51 ( 0.00)71.16 ( 0.00)77.43 ( 0.00)9.46 ( 0.00)43.27 ( 0.00)61.15 ( 0.00)71.89 ( 0.00)76.17 ( 0.00)75.96 ( 0.00)14.59 ( 0.00)13.91 ( 0.00)67.24 ( 0.00)68.54 ( 0.00)Negative STD79.31 ( 0.00)40.76 ( 0.00)70.35 ( 0.00)66.00 ( 0.00)73.60 ( 0.00)9.17 ( 0.00)45.56 ( 0.00)56.78 ( 0.00)69.05 ( 0.00)74.45 ( 0.00)70.98 ( 0.00)12.67 ( 0.00)12.84 ( 0.00)66.33 ( 0.00)64.16 ( 0.00)Negative Max77.61 ( 0.00)44.85 ( 0.00)68.64 ( 0.00)78.37 ( 0.00)74.71 ( 0.00)33.81 ( 0.00)49.86 ( 0.00)71.14 ( 0.00)70.68 ( 0.00)77.19 ( 0.00)72.26 ( 0.00)14.13 ( 0.00)14.63 ( 0.00)68.62 ( 0.00)65.87 ( 0.00) Autoencoder96.67 ( 0.77)45.92 ( 2.47)85.45 ( 1.18)93.09 ( 1.21)93.16 ( 1.40)26.02 ( 3.26)28.83 ( 2.91)82.21 ( 1.27)82.72 ( 1.39)94.15 ( 1.02)94.10 ( 1.22)13.99 ( 0.36)13.69 ( 0.52)84.26 ( 1.01)84.40 ( 1.22)CFlow 84.91 ( 2.72)42.90 ( 2.71)77.38 ( 2.98)76.78 ( 3.03)77.15 ( 2.29)29.86 ( 2.77)39.77 ( 4.33)69.17 ( 2.77)71.09 ( 2.36)81.93 ( 2.03)80.16 ( 1.93)12.83 ( 0.93)12.35 ( 1.22)74.43 ( 1.83)73.19 ( 1.94)Deep SVDD (one-class) 45.93 ( 5.07)52.24 ( 2.24)46.56 ( 4.80)58.11 ( 0.07)54.04 ( 4.88)7.85 ( 0.07)61.60 ( 2.26)49.96 ( 0.06)55.27 ( 4.34)73.51 ( 0.06)47.40 ( 4.76)14.38 ( 0.01)12.36 ( 1.67)66.63 ( 0.06)44.36 ( 4.31)Deep SVDD (soft-boundary) 29.21 ( 15.05)52.48 ( 2.83)35.44 ( 11.72)62.34 ( 4.10)38.41 ( 11.14)17.65 ( 9.91)65.04 ( 4.93)55.09 ( 5.05)42.73 ( 9.28)74.76 ( 1.16)46.42 ( 8.16)14.06 ( 0.30)14.09 ( 0.89)68.12 ( 1.42)44.14 ( 7.40)DRM 93.52 ( 0.52)40.51 ( 1.33)85.71 ( 0.78)85.00 ( 1.41)87.14 ( 0.98)28.54 ( 4.62)35.70 ( 3.60)75.85 ( 1.66)78.80 ( 0.98)88.07 ( 1.00)88.98 ( 0.88)11.84 ( 0.36)11.40 ( 0.36)79.62 ( 1.04)80.67 ( 0.85)FastFlow 92.38 ( 0.72)52.51 ( 1.09)89.92 ( 0.68)88.04 ( 0.78)87.83 ( 0.87)56.62 ( 3.06)59.03 ( 2.49)82.95 ( 1.02)83.16 ( 1.05)90.15 ( 0.61)89.86 ( 0.68)13.39 ( 0.46)13.18 ( 0.38)84.49 ( 0.78)84.45 ( 0.84)PaDiM 95.99 ( 0.37)58.14 ( 1.00)92.28 ( 0.32)88.77 ( 0.26)87.58 ( 0.26)46.25 ( 3.26)65.73 ( 1.90)81.88 ( 0.48)84.03 ( 0.38)90.92 ( 0.24)88.87 ( 0.27)14.81 ( 0.39)17.14 ( 1.42)84.09 ( 0.21)84.07 ( 0.27)PatchCore 96.78 ( 0.57)60.15 ( 1.82)91.38 ( 0.42)90.67 ( 1.24)90.34 ( 1.30)55.82 ( 4.20)56.85 ( 4.14)85.02 ( 0.61)84.91 ( 0.63)91.84 ( 1.20)91.49 ( 1.26)17.86 ( 0.88)17.86 ( 0.97)85.71 ( 0.79)85.49 ( 0.83)Reverse Distillation 87.19 ( 0.99)57.22 ( 5.77)84.04 ( 1.64)84.39 ( 1.15)82.48 ( 1.14)44.41 ( 6.44)57.02 ( 5.75)77.91 ( 1.16)78.36 ( 0.97)87.32 ( 0.91)84.92 ( 1.12)14.91 ( 1.50)15.50 ( 1.41)80.60 ( 0.71)79.53 ( 0.91) ForecastAD94.78 ( 1.09)85.81 ( 1.23)92.53 ( 0.81)88.82 ( 1.33)88.75 ( 1.28)76.68 ( 2.13)78.40 ( 1.36)86.85 ( 0.89)87.07 ( 0.98)90.03 ( 1.24)89.88 ( 1.19)35.40 ( 1.59)36.29 ( 1.40)86.83 ( 1.01)86.89 ( 1.06)"
}