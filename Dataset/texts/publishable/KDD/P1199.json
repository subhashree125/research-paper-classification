{
  "ABSTRACT": "Recommender systems are a ubiquitous feature of online platforms.Increasingly, they are explicitly tasked with increasing users long-term satisfaction. In this context, we study a content explorationtask, which we formalize as a multi-armed bandit problem withdelayed rewards. We observe that there is an apparent trade-off inchoosing the learning signal: Waiting for the full reward to becomeavailable might take several weeks, hurting the rate at which learn-ing happens, whereas measuring short-term proxy rewards reflectsthe actual long-term goal only imperfectly. We address this chal-lenge in two steps. First, we develop a predictive model of delayedrewards that incorporates all information obtained to date. Fullobservations as well as partial (short or medium-term) outcomesare combined through a Bayesian filter to obtain a probabilisticbelief. Second, we devise a bandit algorithm that takes advantageof this new predictive model. The algorithm quickly learns to iden-tify content aligned with long-term success by carefully balancingexploration and exploitation. We apply our approach to a podcastrecommendation problem, where we seek to identify shows thatusers engage with repeatedly over two months. We empirically val-idate that our approach results in substantially better performancecompared to approaches that either optimize for short-term proxies,or wait for the long-term outcome to be fully realized.",
  "This work was completed as part of an internship at Spotify": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 23, August 610, 2023, Long Beach, CA, USA. 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0103-0/23/08...$15.00 ACM Reference Format:Thomas M. McDonald, Lucas Maystre, Mounia Lalmas, Daniel Russo, and KamilCiosek. 2023. Impatient Bandits: Optimizing Recommendations for theLong-Term Without Delay. In Proceedings of the 29th ACM SIGKDD Con-ference on Knowledge Discovery and Data Mining (KDD 23), August 610,2023, Long Beach, CA, USA. ACM, New York, NY, USA, 11 pages.",
  "INTRODUCTION": "Many online platforms rely on recommender systems to assist usersin finding relevant items among vast collections of content .Applications are wide-ranging: recommender systems help individ-uals find books, movies or audio content ; they help doctorsfind medical treatments for their patients , and students findlearning resources , among many others. A key question under-pins the design of any recommender system: What is a successfulrecommendation? Across many applications, there is an ongoingshift towards defining success at longer time-horizons , aslong-term metrics are often better suited to capture users satisfac-tion and platforms goals . For example, e-commerce platformsmay want to maximize long-term revenue, subscription-based ser-vices may want to increase retention, and social platforms maywant to encourage habitual engagement measured over severalweeks or months. In the context of podcast recommendations onan online audio streaming platform, recent work has shown thatexplicitly optimizing for long-term engagement (measured over a60-day window post-recommendation) can significantly improvethe user experience . Most of the literature, however, implic-itly assumes that there is sufficient data to estimate the long-termimpact of recommendations.",
  "Content Exploration Problem": "In this paper, we focus on a specific aspect of recommender systemsand seek to address a content exploration problem. On most onlineplatforms, new content is released regularly. In order to learn aboutthat contents appeal, we must first recommend it to users. This isknown as the cold-start problem. After ensuring an adequate amountof information has been gathered, an effective system should rapidlyshift recommendations away from poor content.We formalize this task as a multi-armed bandit problem, wherewe seek to identify promising content through successive inter-actions with users . Optimizing for long-term definitions of",
  "Tis work: adaptivelymaking use of allavailable information": ": Short-term proxies enable a rapid feedback loop,but might be poorly aligned with long-term success metrics,which take longer to realize. Our method finds the optimaltradeoff by adaptively making use of all available informa-tion at a given time. success in the bandit setting is challenging, as long-term metricsareby constructiondelayed . This gives rise to an apparenttradeoff, illustrated in , between using short-term proxiesthat are observable quickly (top-left) and ensuring that actionsselected are aligned with long-term success (bottom-right). Wepropose a means of circumventing this tradeoff by exploiting theinsight that most long-term outcomes become increasingly predictableover time.Driven by practical applications, we assume that intermediateoutcomes are progressively revealed over time, from the momentthe action is selected up to the moment the full reward is observed.We call this the progressive feedback setting. We develop a proba-bilistic model that forms beliefs about the delayed rewards an armgenerates on the basis of outcomes observed so far. As time passes,uncertainty diminishes and the model is able to make increasinglyprecise predictions. To facilitate this, we contend that historicaldata from distinct but similar applications (e.g., previous contentreleases) can be used to learn the association between intermediateand long-term outcomes. In effect, we propose a meta-learningapproach that learns to infer long-term outcomes of interest fromintermediate observations, revealed progressively over time. Wethen take advantage of this reward model to address our sequentialdecision-making problem, by combining the predictive model witha bandit algorithm. The bandit uses probabilistic predictions fromthe model to efficiently balance exploration and exploitation. Evenif the first few intermediate outcomes are insufficient to perfectlyinfer the average delayed reward an arm generates, they mightbe sufficient to reveal that this arm is outperformed by others. Insuch cases, our bandit algorithm will shift effort away from thearm. Note that, in contrast to well-studied bandit settings wherefeedback is observed at once, either immediately or after a given delay, the progressive feedback setting presents distinctive chal-lenges: Information can be obtained actively, by selecting an action,or passively by letting time unfold and incrementally receiving newdata about the outcomes of actions taken in the past.Our methodology is very general, and can be applied to a widerange of problems. In this work, we consider a recently-studied pod-cast recommendation application . In this application, actionscorrespond to podcast shows, and the reward is defined as the num-ber of days a user engages with a show in the 59 days that followa successful recommendation. Intermediate outcomes consist ofbinary activity indicators for each of the 59 days, observed with thecorresponding delay. We evaluate our approach using data from theSpotify audio streaming platform, and show that a) the full rewardcan be accurately predicted after only a few days of observation,and that b) the content-exploration problem can be solved muchquicker than approaches that rely on short-term proxies or wait forthe full reward to become available.",
  "A Bayesian filtering approach to reward estimation, whichenables us to incorporate all available information in or-der to predict delayed outcomes and quantify uncertainty(.1)": "A meta-learning approach, where the prior and noise covari-ance structures that power Bayesian filtering are themselveslearned from data. The method learns across items how tomake rapid inferences about a new item (.2). The impatient bandit algorithm, a novel algorithm for theprogressive feedback setting, which uses intermediate in-formation received at each round to iteratively update theBayesian filter, and enables us to efficiently balance explo-ration and exploitation whilst providing recommendationsthat optimize for long-term engagement (.3). An application of our impatient bandit algorithm to a real-world podcast recommendation problem, presented along-side empirical results, that show that our proposed methodconsiderably outperforms approaches based on fully-delayedfeedback or short-term proxy metrics ().",
  "We start by briefly discussing relevant related work on multi-armedbandits and applications to recommender systems": "Multi-Armed Bandits. Often used to model online platforms ,multi-armed bandits (MAB) formalize a simple sequential decision-making problem, where at each round an agent selects one ofseveral possible actions and receives a corresponding reward. Thegoal is usually to maximize the sum of rewards received over a giventime horizon. The simplest and most widely studied bandit settingis the strictly sequential feedback scenario, where is immediatelyobserved . However many extensions have been proposed.One such extension is the case of parallelized actions. Rather thansimply selecting a single action at each round and receiving a singlecorresponding reward, we can also consider a scenario in whichmultiple actions can be taken at each round in parallel .This extension is also referred to as the batched bandit setting. The",
  "Optimizing Recommendations for the Long-Term Without DelayKDD 23, August 610, 2023, Long Beach, CA, USA": "Intuitively, the covariance matrices and play a critical rolein our approach. They encode the correlations between outcomesobserved at different points in time. If intermediate outcomes ob-served early on are highly predictive of later outcomes, we expectthat we can accurately estimate (and thus ) without waiting forthe full rounds required to observe . We will revisit this from anempirical perspective in .2. A Note on the Weights. Our approach assumes that the rewardis a given linear function of the trace. For example, in ,we consider a problem where the reward is defined as = ,corresponding to 1. In practice, one might try to fit long-termobjectives to a linear model, by solving a regression problem",
  "METHODOLOGY": "We present our approach to solving the content exploration problemoutlined in the introduction. We adopt the terminology of multi-armed bandits. We consider a set of actions, A = {1, . . . , },corresponding, e.g., to different recommendation candidates. Ateach round = 1, 2, . . ., we select one or more actions. For everyaction we select, we observe a reward after a delay of rounds,i.e., at round + . Informally, we seek to develop a methodologythat helps us quickly identify and exploit actions with high meanreward = E[]. We assume that the reward is a functionof intermediate observations ,1, . . . ,,, that become availableprogressively during the interval [, + ] after selecting the action.We call this the progressive feedback setting.In .1, we consider a fixed action and develop a Bayesianreward model that takes advantage of intermediate observationsto estimate the mean reward . In .2, we take advantageof historical data to estimate the parameters of the reward model,effectively instantiating a meta-learning approach. Building on thismodel, in .3, we develop a bandit algorithm that efficientlybalances exploration and exploitation in the progressive feedbacksetting. Concrete Example. While this section introduces the methodol-ogy in a generic way, it is helpful to keep a concrete application inmind. In we consider a podcast recommendation problem,where the actions A correspond to podcast shows. The reward isthe cumulative engagement with a podcast show over a period of days: = = ,.",
  "Bayesian Reward Model": "We consider a fixed action and, for conciseness, we omit fromall subscripts. Let be the sample reward and = E[] be themean reward associated to selecting the action. Define the sampletrace, = (1, . . . ,) R, as a vector containing intermediateoutcomes. We assume that is observed after rounds, and,without loss of generality, that 1 . Correspondingly,",
  "N (, ), = + , N (0, ) i.i.d.(1)": "That is, we assume a priori that the average trace correspondingto the action is sampled from a multivariate Gaussian distributionwith mean and covariance matrix , and that a sample trace isa noisy copy of , corrupted by additive zero-mean Gaussian noisewith covariance matrix , independently for each . Furthermore,we assume that we can reconstruct the reward from all intermediateobservations as",
  "= ,": "where R is a vector of weights. By the linearity of expectation,it follows that = . We treat as given, and {, , } as modelparameters. We discuss how to learn them from data in .2.Assume that we are at round and that we have selected theaction times so far, at rounds 1 . We representthe observations collected at round as a dataset of independenttraces, D = {(, ) : = 1, . . . , }. Some traces might only bepartially observed, and we use max{ : } to indexthe last element of that is observed at round . 3.1.1Iterative Belief Updates. We consider the problem of esti-mating given D. Instead of reasoning about directly, we beginby addressing the problem of estimating . We take a Bayesianapproach and seek to compute the posterior distribution",
  "| D N (, 2),": "where = and 2 = .We describe the process by which we fold in a single trace into thebelief. The full posterior can be obtained by repeating this procedureiteratively, times. For conciseness, we drop the subscript anddenote the trace and cutoff index as (, ), respectively. We denoteby :,: the submatrix obtained by taking the first rows and the first columns of a matrix . Similarly, we denote by : the first elements of the vector . Thanks to the self-conjugacy property ofthe Gaussian distribution, we can write the posterior distribution of after observing the first elements of the trace as a multivariateGaussian with mean vector and covariance matrix",
  "respectively. We refer the reader to Rasmussen et al. [28, SectionA.2] for more details on these update equations. The completeiterative procedure is provided in Algorithm 1": "A Note on Gaussian Noise. The assumption in (1) that each trace is Gaussian with mean might seem restrictive at first sight.For example, in , we consider binary observation vectors {0, 1}, for which a Gaussian is arguably a poor model. In fact,given that our ultimate goal is to infer from several traces, the",
  ": end for": "that the algorithm does not purely exploit actions that yield largerewards in the first few rounds of feedback, ignoring other, possiblybetter actions. Due to the non-zero variance of the belief on themean reward associated with each action, Thompson samplingmay select an action other than that which the greedy algorithmwould deem optimal. This mechanism trades off exploration andexploration effectively, and is known to achieve low cumulativeregret . 3.3.1Impatient Bandit Algorithm. Our approach builds on theThompson sampling algorithm, applying it to the progressive feed-back setting. In our case, the parameters simply correspond to theaverage rewards { : A}. The key to our approach is to makeuse of the reward model developed in .1 to infer beliefs(). By updating beliefs based intermediate outcomes, we enablethe sampling step in the Thompson sampling to take full advantageof all information collected up to round , and not only of fullyobserved rewards. We call the resulting procedure the impatientbandit and describe it in Algorithm 2.",
  "Training the Reward Model": "A crucial aspect of our method is the ability to take advantage ofpast data to learn the model parameters {, , }. Specifically, weassume access to historical data about a different set of actions A.In the context of a recommender system, for example, this couldbe existing content for which we already have a sufficient amountof interaction data. For each A, denote by H = {(,) : = 1, . . . , } the data corresponding to action .For each action A, we begin by computing the empiricalmean trace vector and noise covariance matrix",
  "Bandit Algorithm": "Equipped with a model capable of making inferences about thearms mean rewards given intermediate observations, we can nowdevelop a bandit algorithm that works effectively in the progressive-feedback setting, where information about the reward is revealedprogressively over multiple rounds.Although several different objectives for the bandit problem existin the literature, in this work we focus on the goal of minimizingthe cumulative expected regret. In the case of a single action beingselected at each round, we define the cumulative expected regret atround as",
  ",": "where is the number of actions per round, and , is the meanreward associated to the th action performed at round .Before describing our algorithm, we first present a brief overviewof Thompson sampling . Slivkins et al. give a generalizedformulation of Thompson sampling for bandits with immediatelyobservable rewards, which we simplify here for ease of exposition.In a strictly sequential multi-armed bandit, when an agent takesan action A, a corresponding reward ( | ) isobserved. We place a prior distribution over the model parameters. The action to be taken at each round is chosen by computing arg maxA E [ | = ], yielding a realized observation,which we then condition on to update . Rather than taking agreedy approach, whereby is the expectation of with respectto , Thompson sampling instead samples the parameters from (i.e. ). This is a subtle, but powerful difference, as it ensures",
  "APPLICATION TO PODCASTS": "We consider a concrete application of our content-exploration prob-lem to podcast recommendations on Spotify, a leading online audiostreaming platform.1 In .1, we begin by describing how ourgeneric methodology can be applied to optimizing long-term userengagement with podcasts, and present a real-world dataset of pod-cast consumption traces. In .2, we study the reward modelin isolation, and evaluate its predictive accuracy. In .3,we consider a sequential decision-making task in the progressive-feedback setting, and compare the empirical performance of ourimpatient bandit against competing approaches.To complement the experiments presented in this section, weprovide a companion software package with a reference implemen-tation of our algorithm.2 While we are unable to publicly releasethe data due to confidentiality reasons, our package includes asynthetic dataset that leads to comparable findings.",
  "Problem Formulation & Data": "Traditionally, podcast recommender systems optimize for short-term rewards, such as the click-through-rate . Recently, Maystreet al. show that explicitly optimizing podcast recommenda-tions for long-term outcomes can lead to substantial impact on areal-world, large-scale recommendation problem. They propose asystem that reasons simultaneously about the clickiness (i.e., theclick-through-rate) and the stickiness of a recommendation. Sticki-ness is defined in terms of the downstream consequences of a suc-cessful recommendation. In particular, the authors suggest countingthe number of days users engage with a podcast show discoveredthrough a recommendation over the 59 days that follows a firstlisten. In this work, we adopt their definitions and optimization met-rics, but consider a specific subset of the overall recommendationproblem. We focus on estimating stickiness (i.e., we do not modelthe click-through rate), and seek to quickly identify new podcastshows that have high average stickiness. This lets us investigatethe challenging problem of estimating long-term rewards for newcontent in isolation, without being confounded by other aspects ofthe overall recommendation problem. Maystre et al. discusshow to estimate the click-through rate, and how to personalizemodels to take into account users preferences, but they do notaddress the content exploration problem we study here.Formally, we instantiate the methodology described in as follows. The set of actions A corresponds to candidate podcastshows that are new and that we need to explore. We define thereward {0, . . . , 59} as the number of days a user engages with ashow in the 59 days that follow a successful recommendation.3 Thisreward is observed with a delay of = 60 days. We refer to the meanreward corresponding to show as the stickiness of the show. Wecollect intermediate outcomes = 1{the user engaged on day }into an activity trace {0, 1}59. Naturally, each activity indicator is observed with delay = + 1. From these definitions, itfollows that = = , where = 1 is the all-ones vector.The distinct set A and historical data H, A correspond to aset of established shows and the corresponding historical consump-tion traces, respectively. We seek to develop a bandit algorithmthat learns to maximize the long-term engagement attributable to",
  "For the purposes of this paper, note that such a long horizon crystallizes the challengesof optimizing for the long-term, and forces us to develop methods that explicitly addressthese challenges": "each recommendation. This is a clear instance of the progressivefeedback setting; Every day, actions must be taken with only partialknowledge about the outcome of decisions made in the previous 59days. 4.1.1Dataset. We consider a dataset of podcast consumption tracescollected on the Spotify audio streaming platform between Sep-tember 2021 and May 2022. The data is divided into a trainingset and an independent validation set. Each subset consists of asample of 200 podcast shows first published on the platform dur-ing a given three-month period. For each of these shows, the datacontains a representative sample of users that discover the showduring the same three-month period. For each user, we obtain alongitudinal trace that captures their engagement with the showon each day starting from the day of discovery,4 in the form of a 59-dimensional binary vector. The training and validation sets coverpodcast shows appearing during the periods SeptemberDecember2021 and JanuaryMarch 2022, respectively. Each subset covers adistinct set of shows.The podcast shows included in the dataset span a wide rangeof categories, from Arts to True Crime. (left) shows thatthe distribution of shows over categories is comparable across thetwo periods.5 In total, the dataset consists of 8.77M activity traces,corresponding to a total of 26M cumulative active-days. The numberof traces per show ranges between 2.4K and 295K, with a medianof 5.8K (, center-left). For each show, we define the ground-truth stickiness by means of the empirical average (across users)of the cumulative active-days. (center-right) shows thatthere is substantial heterogeneity in stickiness across shows, withthe lower quartile, median, and upper quartile at 2.6, 3.4, and 4.6days, respectively. This suggests that the downstream impact ofa discovery can be very different across shows. We note that thestickiness histogram is comparable across the two subsets. Finally,some categories appear to be somewhat stickier than others, butwithin-category variability is significantly larger than between-category variability (, right).",
  "Evaluating the Reward Model": "We focus first on evaluating our Bayesian reward model in isolation.We estimate , and by using the shows and consumption tracescontained in the training dataset. For each show in the validationdataset, we randomly sample 2000 user traces. From this subset, weuse traces to infer the stickiness of each show (via Algorithm 1),and we use the remaining (2000 ) traces for computing theground truth empirical stickiness. In , we visualize how thepredictive accuracy of our stickiness model varies as a function ofboth number of days observed, and number of user traces observed.We see that stickiness predictions can be relatively accurate afterobserving only 10 days of data. The predictions improve as timepasses, and having access to more user traces further increasespredictive accuracy.We now study the noise and prior covariance matrices and, respectively. We investigate how the variance of the samplereward, V[ | :, ], and the variance of the mean reward, V[ |:], are progressively explained away as increases, i.e., as wecondition on more and more days observed. Normalizing the thconditional variance by the total (unconditional) variance, we obtainthe fraction of total variance explained by the first intermediateoutcomes. Technical details are provided in Appendix B, alongsidevisualizations of the covariance matrices as heatmaps.In (left), we look at the noise covariance . The diagonalstraight line represents a hypothetical scenario where daily activityindicators are distributed independently and identically around ,resulting in us gaining a constant amount of information about foreach additional day of observed data. The empirical line correspondsto the actual covariance matrix learned by our approach. We cansee that around 10 days worth of data is sufficient to capture over50% of the aleatoric uncertainty in the reward . There are twofactors that account for this. The first is that, as time progresses,user activity reduces, so the variance is larger early on in the 60-day window; This would be the case even if activity was entirelyindependent across days. The second and more interesting factor,is that activity is correlated across days, therefore knowledge ofactivity up to a given day allows us to predict future activity. Theuncorrelated line corresponds to the hypothetical case where thediagonal of the covariance matrix matches that of , but there is no",
  ": Explained variance as a function of days of activitydata observed": "correlation (i.e., off-diagonal elements of the matrix are set to zero).The gap between the empirical and uncorrelated curves illustrateshow much information we gain by exploiting the fact that pastactivity is predictive of future activity.Similarly, in (right), we look at the prior covariance. Intuitively, this lets us explore how much of the variance of would be explained if we were to observe the first elements ofeach of a set of independent sample traces, as . We candraw similar conclusions from this plot as to those mentioned inthe context of . However the trend is even more stark here, as 50%of variance is explained by just eight days of data, and 95% of thevariance is explained within a month.",
  "Sequential Decision-Making Task": "We now turn our focus to the evaluation of the impatient banditalgorithm, and to the comparison of its empirical performance withcompeting approaches. The way observed feedback is used is oneof the main points of differentiation between the approaches weconsider. As such, we refer to our approach as progressive, since wemakes use of all observations as they are revealed over time. Wecontrast the performance of our approach to three baselines. Delayed. The case in which we solely receive full observations, days after an action is taken is referred to as the delayedfeedback baseline. This naive approach does not attempt totake advantage of intermediate outcomes. Day-two proxy. We treat the second day of activity as a proxyfor stickiness and discard all subsequent information. Thisbaseline captures an intuitive outcome that is clearly relatedto the goal of maximizing habitual engagement: Does theuser return to the show the day after discovering it? Thisbaseline is representative of short-term proxies widely usedin recommender systems, such as the click-through-rate, thedwell time, or the conversion rate . Oracle. Finally, we include an oracle baseline, which assumes thatthe full 60-day activity trace is received immediately after anaction is taken. This is clearly unrealistic, but it is useful toinclude as it provides an upper-bound on the performanceof any model.",
  ": Average per-step regret and entropy of set of actions taken at each round, for = 200 podcast shows": "These baselines have been chosen to illustrate the benefits of in-corporating progressive feedback into a bandit algorithm, and theeffectiveness of our approach in making use of this intermediateinformation. We use Thompson sampling for all of our baselinesto ensure that any performance differences are due to the mannerin which feedback is being considered, rather than to the relativestrengths and weaknesses of different families of bandit algorithms.For similar reasons, we do not compare to any works that studyother aspects of recommendation unrelated to this study, such aspersonalization.To mimic a realistic deployment setting in which the prior wouldbe computed using data from the past, we compute our prior us-ing the training data, and then run our algorithm on the unseenevaluation dataset. A single prior is computed using all availabletraces from all 200 shows in our training set, this is then used forall of the experiments in this section. We run the bandit for 180rounds (corresponding to approximately 6 months), repeating eachexperiment 10 times to generate confidence intervals for the av-erage regret. Three different experimental setups are considered,with varying numbers of actions taken per day. 4.3.1Results. (top row) visualizes the average per-stepregret for each of these experimental settings, which ideally shouldtend to zero as . Across all of the experiments, the per-formance of the delayed approach is poor as it is forced to makeuninformed decisions for the first rounds of evaluation due to theinherent delay in feedback being received. Additionally, the oracle,as expected, outperforms the other approaches due to the unreal-istic amount of information it has access to. The day-two proxyapproach performs well at first, comparably to our approach acrossthe initial month of evaluation, but past this stage the limitationsof optimizing for this proxy become clear. The proxy is not wellaligned, and the per-step regret rapidly plateaus.Our progressive approach exhibits superior performance com-pared to the competing delayed and day-two proxy approaches; infact, the performance of our approach is closer to that of the oracle. As we increase the number of actions per round, we see a slightreduction in per-step regret across all approaches. (bottom row) provides an alternative perspective on theoutcome of these experiments, visualizing the entropy of the set ofactions taken at each round. Should a bandit converge on recom-mending a single show repeatedly at each round, the entropy wouldtend to zero. The entropy plots show that, early on in the evaluationphase, our progressive algorithm tends to diversify across actionsmore than the oracle and day-two proxy. The interpretation of thisis that our approach is performing a broader exploration of theaction space, a characteristic that can be very useful in a realistic,deployment setting, which we discuss below. Not only does let us compare the empirical performance of all four approaches,it also enables us to differentiate the effects of observational noisefrom the effects of delayed feedback. For example, the large gapin per-step regret between the oracle and delayed approaches isentirely due to the delay in feedback, as both approaches receivefull user traces of length . On the other hand, the gap in per-stepregret between the oracle and day-two proxy approaches is dueto the fact that the second day of activity is a noisy proxy for thetrue stickiness, thus the day-two proxy approach tends to rapidlyconverge on a small subset of sub-optimal shows (this can be seenfrom its entropy, which quickly approaches zero).In , we present additional results for a scenario in whichwe have a smaller action space, consisting of a subset of 50 showssampled from the original evaluation dataset discussed previously.This is clearly a simpler problem setting, as evidenced by the factthat all of the approaches tend more quickly to lower values ofaverage regret in this case except for the day-two proxy feedback.Besides this observation, the results follow largely similar trendsto those seen in . Changing Show Set. In addition to considering a static library ofshows, we also briefly consider a setting where we have a libraryof shows that is constantly evolving over time. Specifically, at eachround, one randomly selected show is removed from the library and",
  "CONCLUSION & FUTURE WORK": "In this work we have introduced a new type of bandit algorithmthat efficiently optimizes for delayed rewards, assuming that in-termediate outcomes correlated with the final reward are revealedprogressively over time. This is achieved by way of a meta-learningapproach. We begin by learning the parameters of a Bayesian filterby using historical data from a related but distinct problem. Then,we combine this probabilistic reward model with Thompson sam-pling, effectively balancing exploration and exploitation. The key to our success is that the Bayesian filter is able to make accurateinferences on delayed rewards using intermediate outcomes.We have evaluated our framework empirically on a podcast con-tent exploration problem. Using real-world platform data, experi-mental results show that our approach, which utilizes all availableintermediate information to estimate a long-term reward, signifi-cantly outperforms approaches that only use short-term proxies orwait until the reward is available.We have presented a non-personalized methodology for optimiz-ing recommendations over an extended period of time. A naturalavenue for future work is extending this to a personalized setting.Conceptually, we do not foresee any major difficulty. In Appen-dix C, we sketch an contextual extension of our Bayesian filterthat conditions beliefs on user embeddings. Another avenue ofresearch would be to build a theoretical understanding of the favor-able empirical performance observed in practical applications. Canwe formally characterize the benefits of progressive feedback overdelayed rewards in terms of the average regret?Finally, we would like to emphasize that the general frameworkwe present can also benefit other application domains, beyondrecommendations on online content platforms. For example, webelieve our algorithm could be used to allocate resources in hy-perparameter optimisation problems by identifying more orless promising hyperparameter configurations in the early stagesof training from an array of intermediate validation metrics. Thiscould significantly reduce the computational cost of training largemodels and its environmental impact, which has become a majorconcern in the ML community in recent years .",
  "Shipra Agrawal and Navin Goyal. 2012. Analysis of Thompson sampling for themulti-armed bandit problem. In Conference on learning theory. JMLR Workshopand Conference Proceedings, 391": "Susan Athey, Raj Chetty, Guido W Imbens, and Hyunseung Kang. 2019. Thesurrogate index: Combining short-term proxies to estimate long-term treatmenteffects more rapidly and precisely. Technical Report. National Bureau of EconomicResearch. Maryam Aziz, Jesse Anderton, Kevin Jamieson, Alice Wang, Hugues Bouchard,and Javed Aslam. 2022. Identifying New Podcasts with High General AppealUsing a Pure Exploration Infinitely-Armed Bandit Strategy. In Proceedings of the16th ACM Conference on Recommender Systems. 134144.",
  "Olivier Chapelle and Lihong Li. 2011. An empirical evaluation of Thompsonsampling. Advances in Neural Information Processing Systems 24 (2011)": "Antoine Dedieu, Rahul Mazumder, Zhen Zhu, and Hossein Vahabi. 2018. Hier-archical Modeling and Shrinkage for User Session Length Prediction in MediaStreaming. In Proceedings of the 27th ACM International Conference on Informationand Knowledge Management. 607616. Thomas Desautels, Andreas Krause, and Joel W Burdick. 2014. Parallelizingexploration-exploitation tradeoffs in Gaussian process bandit optimization. Jour-nal of Machine Learning Research 15 (2014), 38733923. Thore Graepel, Joaquin Quinonero Candela, Thomas Borchert, and Ralf Herbrich.2010. Web-scale Bayesian click-through rate prediction for sponsored searchadvertising in Microsofts Bing search engine. Omnipress. Aditya Grover, Todor Markov, Peter Attia, Norman Jin, Nicolas Perkins, BryanCheong, Michael Chen, Zi Yang, Stephen Harris, William Chueh, et al. 2018. Bestarm identification in multi-armed bandits with delayed feedback. In InternationalConference on Artificial Intelligence and Statistics. PMLR, 833842.",
  "William R Thompson. 1933. On the likelihood that one unknown probabilityexceeds another in view of the evidence of two samples. Biometrika 25, 3-4 (1933),285294": "Thi Ngoc Trang Tran, Alexander Felfernig, Christoph Trattner, and AndreasHolzinger. 2021. Recommender systems in the healthcare domain: State-of-the-art and research issues. Journal of Intelligent Information Systems 57 (2021),171201. Katrien Verbert, Nikos Manouselis, Xavier Ochoa, Martin Wolpers, HendrikDrachsler, Ivana Bosnic, and Erik Duval. 2012. Context-Aware RecommenderSystems for Learning: A Survey and Future Challenges. Journal of IntelligentInformation Systems 5, 4 (2012), 318335.",
  "Xiangyu Zhao, Long Xia, Jiliang Tang, and Dawei Yin. 2019. Deep reinforcementlearning for search, recommendation, and online advertising: a survey. ACMSIGWEB newsletter Spring (2019), 115": "Guanjie Zheng, Fuzheng Zhang, Zihan Zheng, Yang Xiang, Nicholas Jing Yuan,Xing Xie, and Zhenhui Li. 2018. DRN: A deep reinforcement learning frameworkfor news recommendation. In Proceedings of the 2018 World Wide Web Conference.167176. Lixin Zou, Long Xia, Zhuoye Ding, Jiaxing Song, Weidong Liu, and Dawei Yin.2019. Reinforcement learning to optimize long-term user engagement in recom-mender systems. In Proceedings of the 25th ACM SIGKDD International Conferenceon Knowledge Discovery & Data Mining. 28102818.",
  "arg min(,)D ( )2": "on some historical data D. In general, the reward = will nolonger be identical to the true long-term target .In this case, the reward can be thought of as a surrogate index,as defined in Athey et al. . Provided that several assumptionshold, this approach is principled. Among others, needs to be in-dependent of the selected action given (a.k.a. the surrogacyassumption). This assumption requires to contain sufficient in-formation on as it relates to . Furthermore, the reward = learned on historical training data should generalize to data comingin during evaluation (a.k.a. the comparability assumption). In prac-tice, it might be important to test these assumptions empirically.",
  "A.1Non-Linear Extension": "The assumption that the reward is linear in the trace is not asrestrictive as it might appear at first sight. It is easy to extend themodel to capture non-linear relationships between and , whilestaying in the same linear-Gaussian framework that we rely onthroughout .As a concrete example, consider a reward that depends on R2 in a non-linear way, for example = 21 32 + 612. Wecan augment the trace into a new vector = (1,2,3,4,5) =(1,2,21,22,1 2). Now, we can represent any quadratic relation-ship between and as a linear relationship between and . Inparticular, our example yields = with = (0, 3, 1, 0, 6). Byinstantiating the reward model over instead of , we can thusmodel non-linear (quadratic) relations between intermediate out-comes and long-term reward. This idea can be extended to higher-order polynomials or (perhaps better) to regression splines ,and capture non-linear relationships in a flexible way.",
  "In , we show visualizations of the prior and noise covari-ance matrices and obtained by training the model on the data": "described in .1.1. In the prior covariance matrix we seea clear weekly trend, and whilst the entries around the first fewdays of activity dominate, there is still a rich covariance structureacross the whole 60-day period. From the noise covariance, we canconclude that the daily observations are clearly not independent,but there is still a significant degree of day-to-day variability whichis not explained.",
  "CCONTEXTUAL EXTENSION": "Our methodology can be extended to the contextual setting, and webriefly sketch this extension here. For conciseness, let us considerthe case of disjoint linear payoffs , and let us fix a single actionand omit the subscript . Instead of modeling the -dimensionalaverage trace , we now model a ( )-dimensional matrix . Weassume that the expected reward for selecting the action is ,where is a context vector (e.g., describing a users preferences) thatcan change across rounds. Intuitively, theth column of describescoefficients of the th context-dependent average intermediateoutcome.We can extend the Bayesian filter we describe in .1 tomodel a belief over the random matrix instead of the randomvector . This is achieved simply by vectorizing the matrix, thatis, vec() N (, ), with of dimension and of dimension . We can condition the belief updates on the context usingstandard closed-form formulas. For simplicity, consider a full trace observed in context ; The posterior belief update is given by",
  "+ ,": "where denotes the Kronecker product.The simple training procedure described in .2 cannot beeasily extended to the contextual case, since the quantities involvedin the averages are context-dependent. Instead, we suggest usingtype-II maximum likelihood, a standard hyperparameter selectionprocedure. We leave a detailed development of a contextual versionof our approach for future work."
}