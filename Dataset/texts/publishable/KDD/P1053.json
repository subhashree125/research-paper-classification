{
  "Abstract": "We propose AGS-GNN, a novel attribute-guided sampling algorithmfor Graph Neural Networks (GNNs) that exploits node featuresand connectivity structure of a graph while simultaneously adapt-ing for both homophily and heterophily in graphs. (In homophilicgraphs vertices of the same class are more likely to be connected,and vertices of different classes tend to be linked in heterophilicgraphs.) While GNNs have been successfully applied to homophilicgraphs, their application to heterophilic graphs remains challeng-ing. The best-performing GNNs for heterophilic graphs do not fitthe sampling paradigm, suffer high computational costs, and arenot inductive. We employ samplers based on feature-similarity andfeature-diversity to select subsets of neighbors for a node, andadaptively capture information from homophilic and heterophilicneighborhoods using dual channels. Currently, AGS-GNN is the onlyalgorithm that we know of that explicitly controls homophily in thesampled subgraph through similar and diverse neighborhood sam-ples. For diverse neighborhood sampling, we employ submodularity,which was not used in this context prior to our work. The sam-pling distribution is pre-computed and highly parallel, achievingthe desired scalability. Using an extensive dataset consisting of 35small ( 100 nodes) and large (> 100 nodes) homophilic and het-erophilic graphs, we demonstrate the superiority of AGS-GNN com-pare to the current approaches in the literature. AGS-GNN achievescomparable test accuracy to the best-performing heterophilic GNNs,even outperforming methods using the entire graph for node classi-fication. AGS-GNN also converges faster compared to methods thatsample neighborhoods randomly, and can be incorporated intoexisting GNN models that employ node or graph sampling.",
  "Introduction": "Traditional Graph Neural Networks (GNNs) rely on the homophilicproperty of the learning problem, which assumes that a signif-icant portion of a nodes neighbors share the class label of thenode. However, this assumption has been challenged in recentyears since graphs in several practical applications do not satisfyhomophily . Consequently, GNNs designed with the assump-tion of homophily fail to classify heterophilic graphs accuratelydue to noisy or improper neighborhood aggregation . A sim-ple Multi-layer Perceptron (MLP) based model that ignores the graph structure can outperform existing homophilic GNNs on het-erophilic graphs . As a result, a number ofspecial-purpose GNNs have been developed to classify heterophilicgraphs .Although GNNs have been adapted for heterophilic graphs inearlier work, their applicability is limited since they do not scaleto large graphs and are transductive. Unlike homophilic GNNs , where subgraph sampling strategies have beendeveloped for scaling, currently there are no effective sampling ap-proaches for heterophilic graphs . Recent authors have enabledscaling by first transforming node features and adjacency matrixinto lower dimensional representations, and then applying mini-batching on the combined representations . Thus inferenceon heterophilic graphs via graph sampling remains challenging.The dichotomy of classifying graphs into heterophilic or ho-mophilic graphs, as used in current literature, is blurred in practiceby the presence of locally homophilic and locally heterophilic nodes.As shown in (2), both homophilic and heterophilic graphscould have nodes with high local homophily or heterophily. How-ever, there is no systematic and scalable approach for neighborhoodsampling that distinguishes each node w.r.t. its local homophilyproperty. We propose a new sampling strategy that incorporatesboth the adjacency structure of the graph as well as the feature infor-mation of the nodes that is capable of making this distinction. For ahomophilic node, we build our sampling strategy based on a widelyused smoothing assumption that labels and features generallycorrelate positively. Heterophilic nodes of the same labels, how-ever, are expected to have the same dissimilar neighborhood ,and our sampling strategy exploits this. Thus, we generate twosets of local neighborhood samples for each node: one based onfeature similarity that potentially improves local homophily, whilethe other encourages diversity and increases heterophily. Thesetwo samples are adaptively learned using an MLP to select theappropriate one based on the downstream task. Our attribute-basedsampling strategy can be seamlessly plugged into GNNs designedfor classifying heterophilic graphs and performs well in practice.The strength of our approach, however, is that even when pairedwith GNNs designed for homophilic graphs, we obtain better accu-racies for heterophilic graphs, thus rendering an overall scalableapproach for the latter graphs. The key contributions and findingsof this work are summarized as follows:(1) We propose a novel scalable and inductive unsupervised andsupervised feature-guided sampling framework, AGS-GNN, to learnnode representations for both homophilic and heterophilic graphs.",
  "KDD 24, August, BarcelonaTrovato and Tobin, et al": "(2) AGS-GNN incorporates sampling based on similarity and diver-sity (modeled by a submodular function). We are not aware of earlierwork that uses submodularity in this context. AGS-GNN employsdual channels with MLPs to learn from both similar and diverseneighbors of a node.(3) We experimented with 35 benchmark datasets for node classifi-cation and compared them against GNNs designed for heterophilicand homophilic graphs. For both types of graphs, AGS-GNN achievedimproved accuracies relative to earlier methods () (5.3). Fur-ther, AGS-GNN also requires fewer iterations (up to 50% less) (5.4)to converge relative to random sampling.",
  "Preliminaries": "Consider a weighted graph G(V, E) with set of vertices V, and setof edges E. We denote the number of vertices and edges by |V| and |E| . The adjacency matrix of the graph will be denotedby R, and the -dimensional feature matrix of nodes by R . We denote Y = {1, 2, ,} to be the label of anode that belongs to one of the classes, and the vector Y to denote the labels of all nodes. Additionally, the graph may haveassociated edge features of dimension . The degree of node isdenoted by , the average degree by , and the set N () denotesthe set of neighboring vertices of a node . For a GNN, denotesthe number of layers, the number of neurons in the hidden layer,and the learnable weight of the -th layer.",
  "Homophily Measures": "The homophily of a graph characterizes how likely vertices withthe same labels are neighbors of each other. Many measures ofhomophily fit this definition, and Appendix 10.2 discusses seven ofthem, with the values computed on our benchmark dataset. How-ever, for conciseness, we will focus here on node homophily (H)(intuitive), and adjusted homophily (H) (handles class imbalance).The local node homophily of node is H() = |{N():= }|",
  ": The distribution of local node homophily in a ho-mophilic and a heterophilic graph. Figs. 13 and 14 in Appen-dix 10.5 show this for all datasets": "existing graphs, ignoring the structure information of the graphbut retaining the node features and class labels. Following ,to generate an undirected graph with an average degree of andnode homophily H, for each node we randomly assign H /2edges from the same class as and (1 H) /2 edges from otherclasses. We left the class distribution unbalanced, as it is in theoriginal graph, making it more challenging for GNNs since theneighborhood of a heterophilic node could potentially have morenodes from the majority class. shows the performance of GSAGE (a homophilicGNN) and ACM-GCN (a heterophilic GNN) on the syntheticgraphs generated from Squirrel and Chameleon datasets. Weuse two versions of ACM-GCN, one with three channels (low-pass,high-pass, and identity) and the other with an additional channelwith graph structure information (ACM-GCN-struc). The originalSquirrel and Chameleon datasets are heterophilic, and the ACM-GCN-struc is the best-performing. For synthetic graphs, on bothSquirrel and Chameleon, (), we see that surprisingly the worst1 score is not achieved on the graphs whose homophily value iszero but for values near 0.25. When the homophily score is high,GNNs perform well since it aggregates relevant information, but aswe observe ACM-GCN also does well at a very low homophily. Thisis because some locally heterophilic nodes become easier to classifyafter neighborhood aggregation on features (note that featuresare not considered in the definition of homophily based solelyon labels). Another intuitive reason is that when two nodes areadjacent to the same dissimilar (wrt class labels) neighbors, the highpass filters (e.g., graph Laplacian) used in ACM-GCN treat thesenodes as similar and can classify them correctly (Appendix 7.4).",
  "Similarity, Diversity, and Homophily": "Nodes with similar features tend to have similar labels . Wecarried out an experiment to validate this statement and show inAppendix 10.4 that the labels of nodes often correlate positivelywith their features. Therefore, if instead of sampling the neighborsof a node uniformly at random, we sample neighbors that aresimilar to in feature space, we are likely to increase the homophilyof the sampled subgraph.However, this strategy alone is not enough for heterophilicgraphs as they include both locally homophilic and heterophilicnodes (). Two heterophilic nodes with the same label are ex-pected to have similar class label distributions in their neighbors.",
  ": 1 Score comparison of GSAGE and ACM-GCN onsynthetic graphs generated from Squirrel (a) and Chameleon(b) datasets with varying node homophily": "In other words, the diversity in their class labels makes them sim-ilar. It has been shown that high-pass filters (e.g.,variants of graph Laplacians: = , = 1/21/2, or = 1) capture the difference between nodes, and low-passfilters (e.g., scaled adjacency matrices, = 1/21/2, or = 1) retain the commonality of node features (Appen-dix 7.4). Therefore, if we sample diverse neighbors (in feature space)and use a GNN with a high-pass filter, we expect a higher chance ofmapping two heterophilic nodes of same class to the same space (af-ter feature transformation) since they will have the same dissimilarneighborhood.A mathematical approach to ensure diversity is through submod-ular function maximization . A submodular function is a setfunction that favors a new node that is most distant in feature spaceto add to a partially sampled neighborhood. It accomplishes this bydefining a suitable marginal gain of the new element with respectto the current neighborhood, and maximizing this value. However,employing only diverse neighborhoods can also cause issues sincetwo nodes with different labels may have similar neighborhoods af-ter sampling. In this scenario, sampling based on similarity is moreappropriate. For the spectral domain, AGS-GNN considers two chan-nels: one with a sampled subgraph ensuring diversity (used witha high-pass filter) and the other with a subgraph sampled based onsimilarity (used with a low-pass filter). Similar to ACM-GCN, we canalso use an identity channel. However, spectral GNNs are difficultto scale as they often do not support mini-batching, and are trans-ductive. Hence we consider spatial GNNs for heterophilic graphs,which employ graph topology to develop aggregation strategies.However, for heterophilic graphs, both similar and dissimilar neigh-bors need to be considered, and in AGS-GNN, we achieve this throughattribute-guided biased sampling of similar and diverse samples. 2.3.1Node Homophily with Similar and Diverse neighborhood: Con-sider an ego node = {, } with feature , label , and localnode homophily H(). Let the feature and label tuples of theneighbors of be N () = {(1,1), (2,2), , (, )}. Fromthe definition of homophily, the probability of randomly select-ing a neighbor N () with the same label as the ego node isU ( = ) = H(). Here, U refers to the distribution of selectinga neighbor uniformly at random. Let (, ) be a positive similar-ity score in features between a neighbor, , and the ego node, .",
  "Assumption 2.1. For a node , the average similarity of neigh-bors with the same label as is greater than or equal to the averagesimilarity of all neighbors": "Lemma 2.1. If the probability of selecting a neighboring node isproportional to its similarity to the ego node , the local node ho-mophily of sampled neighborhood H() H(). If the samplingprobability distribution is S, then S( = ) U ( = ). To retrieve a diverse set of neighbors we can employ a submodu-lar function. An example could be the facility location function basedon maximizing pairwise similarities between the points in the dataset and their nearest chosen point, (,) = max (,),where is the ground set, is a subset, and is the similaritymeasure. In our context is the current set of selected nodes initial-ized with ego node , and ground set = N () {}. The marginalgain is = ( {},) (,) for each neighbor N () \\ .Successively, neighbors are added to the sampled neighborhood bychoosing them to have maximum marginal gain with respect to thecurrent sample of the neighborhood.",
  "Assumption 2.2. The average marginal gain of the neighbors ofa node with the same label as is less than or equal to the averagemarginal gain of all neighbors": "Lemma 2.2. If the probability of selecting a neighboring node isproportional to its marginal gain wrt the ego node, then the local nodehomophily of sampled neighborhood H() H(). If the samplingprobability distribution is D, then D ( = ) U ( = ). Proofs of Lemmas 2.1 and 2.2 are included in the Appendix 7.1and Appendix 7.2 respectively. We also report experimental verifica-tion of these properties in Appendix 7.3. From the Lemma 2.1, sam-pling neighbors based on feature-similarity improves homophily,which potentially helps to map homophilic nodes of the same labelsinto the same space. We assume features and labels to be positivelycorrelated; thus, if we ensure feature diversity among neighbors,we can also expect label diversity in the samples, reducing localhomophily (as shown in Lemma 2.2) and increasing the chances ofmapping two heterophilic nodes into the same space. We devisefeature-similarity and feature-diversity-based sampling based onthese results in the next section.",
  "Pre-computing Probability Distribution": "An ideal graph sampling process should have the following keyrequirements: (i) Nodes with strong mutual influences (in termsof structure and attributes) should be sampled together in a sub-graph . (ii) Should be able to distinguish between similar and dis-similar neighbors (especially for heterophilic graphs) . (iii) Ev-ery edge should have a non-zero probability of being sampled inorder to generalize and explore the full feature and label space.In this section, we will devise sampling strategies satisfying theserequirements. We assume that we have access to a similarity mea-sure between any two nodes in the graph. This similarity function",
  "(b) Node Sampling from training vertices with hop size 2": ": AGS-GNN framework with Node Sampling. a) Pre-computation step to rank the neighbor of vertices. b) Demonstrateshow weighted node sampling is performed based on the selection probabilities of ranked neighbors. typically depends on the problem and the dataset. For an example,in text based datasets, we may use cosine similarity of the featurevectors (e.g., TF-IDF) generated from the texts of the correspond-ing items. We may also learn the similarities when an appropriatesimilarity measure is not apparent. Once we have the similarityfunction, for each vertex , we construct a probability distributionover its neighbors as follows: (i) rank the neighboring nodes (N ())using the similarity scores to . (ii) assign weights to the adjacentedges of based on this ranking. A few choices are shown in in Appendix. (iii) normalize the weights to construct the probabilitymass function (PMF) P() of the distribution over N ().We construct the probability distribution using the rank ratherthan the actual similarity values, since the distribution using thesimilarity values could be skewed and extremely biased towards afew top items. Here we consider two choices of rankings of neigh-borhoods that are suited for homophilic and heterophilic graphs. 3.1.1Ranking based on similarity: For this case, our goal is to sam-ple subgraphs favoring similar edges to be present more frequentlyin the subgraph. To achieve that we propose to construct a prob-ability distribution over the edges based on the similarity scores.We sort the similarity values of all the neighboring vertices of avertex from high to low and assign ranking based on the orderto the adjacent edges. Thus, although the similarity function issymmetric, we may get two different ranks for each edge. Notethat the computation required to generate the rankings are localto each vertex and thus highly parallel. For cosine similarity thetime complexity to compute the ranking of the adjacent edges of avertex is O( log).Once we have the rankings, we can use these to construct dif-ferent probability distributions. Some choices of Probability MassFunctions (PMF) can be linear or exponential decay with non-zeroselection probability to the later elements in the ranking order.Another options is the step function where the top 1% neighborsof a vertex are given a uniform weight (1), the next 2% as 2,and the rest 3, where 1 > 2 > 3. The benefit of using suchfunction is that we can partially sort the top (1 +2)% of neighborsavoiding the full ordering. in Appendix 8 shows pictorialrepresentations of some of the PMFs mentioned. 3.1.2Ranking based on diversity: As discussed in section 2.3, for aheterophilic graph, in general it is desirable to construct subgraphsbased on diversity in the class labels. To accomplish that, we proposea sampling strategy based on optimizing (a nonlinear) submodularfunction. A submodular function is a set function that has thediminishing returns property. Formally, given a ground set andtwo subsets and with , a function is submodular ifand only if, for every \\, () () () (). Thequantity on either side of the inequality is called the marginal gainof an element with respect to the two sets or . For maximizingcardinality constrained submodular functions, where the solutionset () is required to be at most in size, a natural Greedy algorithmthat starts with empty solution and augments it with the elementwith highest marginal gain is (1 1/)-approximate .To see how a submodular function may behave differently thanthe (linear) similarity based function (section 3.1.1), consider apaper citation graph where the nodes are the scientific doc-uments and the feature on each node is the binary count vectorof the associate text. For a vertex , our goal is to find a set of neighboring vertices of , ( N () where || = ), such thatgiven the initial set {}, we maximize the number of unique wordcounts. This objective can be modeled as a submodular functionknown as maximum -coverage problem, and the Greedy approachwould select successive nodes with maximum marginal gains wrtto . Intuitively, the Greedy algorithm prioritizes neighbors thathave more distinct words than what has been covered throughthe selected nodes. Therefore, if different word sets correspond todifferent class labels, the final set will likely represent a diverseset. This contrasts sharply with the ranking based on similarity,where we would encourage neighbors with similar words. FacilityLocation, Feature-based functions, and Maximum Coverage are somesubmodular functions applicable in the graph context.Given a submodular function, for a vertex , we execute theGreedy algorithm on the neighbors of to compute their marginalgains, assuming is in the initial solution. We use these marginalgains to rank neighbors of and then use the ranks to construct aprobability distribution as described in section 3.1.1. To computethe solution faster, we employed a variant of the Greedy algorithmwhich is called Lazy Greedy that can reduce the number ofmarginal gain computations. Algorithm 3 and 4 in Appendix 8.3",
  "AGS-GNN: Attribute-guided Sampling for Graph Neural NetworksKDD 24, August, Barcelona": "are in PyTorch , PyTorch Geometric , and Deep Graph Li-brary (DGL) . For computing distribution based on submodularranking, we use Apricot library . We modified the Apricot codeto make the implementation more efficient. Source codes for all ourimplementations are provided anonymously on GitHub1. 5.1.3Implemented Methods: We consider graphs with H 0.5to be heterophilic. The small instances contain graphs with lessthan 100 nodes, and they fit in the GPU memory. The largerinstances are compared against only scalable homophilic and het-erophilic GNNs. We compare AGS-GNN (the Node Sampling AGS-NSvariant) with other Node Sampling methods (GSAGE ), Graph-Sampling (GSAINT ), and Heterophilic GNNs (LINKX ,ACM-GCN ). LINKX is used for only the small graphs, where theentire graph fits into GPU memory. For large graphs, we used therow-wise minibatching of the adjacency matrix (AdjRowLoader) forLINKX, which is denoted by LINKX+. Since, most of the heterophilicGNNs require entire graph and do not support mini-batching, wecompare AGS-GNN with 18 standard heterophilic and homophilicGNNs on small heterophilic graphs only. Appendix 11.2 providesdetails on these method and results.",
  "()= ( (), (1)|AGG({(1): N()}).(3)": "Here denotes feature vectors, N() ( N ()) is the subset ofneighbors of (of size ) sampled uniformly at random. AGG is anypermutation invariant function (mean, sum, max, LSTM, etc.).In AGS-GSAGE, we use the probability distribution derived insection 3.1 to sample with or without replacement from the neigh-borhood of a node. Depending on the nature of the graph, single ordual-channel GNNs might be necessary. Some homophilic graphsmay have only a few locally heterophilic nodes, where samples",
  "Target nodeSimilar sampleDiverse sampleGNN layerMLP layer": ": Computation graph with sample size = 2 and hop-size 2. a), b) samples from similarity and diversity rankingfor a single channel, c) dual channel with combined represen-tation at the target node, and d) similar and diverse weightedsamples at each sampled node. from a distribution generated by similarity only may be sufficient.However, we expect the dual-channel AGS to perform better for het-erophilic graphs since they typically have both locally homophilicand heterophilic nodes (see , and , from Appen-dix 10.5). shows some possible computation graphs. The dual-channel AGS mechanism can be incorporated easily into GSAGE.We generate two similar and diverse neighborhood samples for thetarget node, compute the transformed feature representations usingboth samples, and use MLPs to combine these representations.",
  "()= MLP(( (() |()) + () + ())).(6)": "Combining representations at the root of the computation graphas shown in c can be better than combining two different sam-ples at each node of the tree, as in d. This is to avoid overfittingand make it computationally efficient. For transductive learning,as we have already precomputed the probability distributions, theinference process works similarly to the training phase. In inductivesetting, we have to compute the probability distribution over theneighbors of a node as described in section 3.1. 3.2.1Other sampling strategies and models: AGS can also be inte-grated into ACM-GCN and other filter-based spectral GNNs. Atthe start of the epoch, we sample neighbors of each node from thesimilarity and diversity based distributions and construct two sparsesubgraphs. We can then use the subgraph based on similarity with alow-pass filter and the subgraph based on diversity with a high-passfilter. We can employ graph sampling strategies (instead of nodesampling), such as weighted random walks, and use them with ex-isting GNNs. We can also incorporate heuristics by first computingedge-disjoint subgraphs and then sampling a sparse subgraph. Wecall our graph sampling GNN AGS-GS, and its details are providedin Appendix 9 for conciseness. We call AGS-GS-RW (Appendix 9.1)the weighted random walk version, and call AGS-GS-Disjoint (Ap-pendix 9.2) the edge-disjoint subgraph-based sampling version). Forthe downstream model, there is flexibility to adapt existing modelslike ChebNet , GSAINT , GIN , GCN , and GAT .We can also use two separate GNNs in two separate channels.",
  "Computation Complexity": "The pre-computation of the probability distribution requires O( log) and O( 2) operations for similarity and facility locationbased ranking, respectively. If the similarity metric is required tobe learned the added time complexity is O(). The training andmemory complexity depend on the usage of underlying GNNs. Letthe number of hidden dimensions () be fixed and equal to = for all layers. For Stochastic Gradient Descent (SGD)-based ap-proaches, let be the batch size, and be the number of sampledneighbors per node. When a single channel is used, each nodeexpands to nodes in its computation graph, and requires 2 op-erations to compute the embedding (a matrix-vector multiplication)in every epoch. Therefore, for nodes, the per epoch training timecomplexity is O( 2). The memory complexity is O( + 2),where The first term corresponds to the space for storing the em-bedding, and the second term corresponds to storage for all weightsof neurons of size, R .For single channel node sampling (a,b), the training andmemory complexity of AGS-GNN is similar to GSAGE. The trainingand memory requirements are twice as much for dual channelsusing the same sampling size, leaving the asymptotic bounds un-changed. However, for the dual channel with computation graphscenario shown in d, where each node generates two types ofsamples of size , the per epoch computation complexity becomesO(2 2). One way to ameliorate this cost is to reduce the sampleneighborhood size by half.",
  "Related Work": "While Spatial GNNs focus on graph structure (topology) to developaggregation strategies, spectral GNNs leverage graph signal pro-cessing to design graph filters. Spectral GNNs use low-pass andhigh-pass filters to extract low-frequency and high-frequency sig-nals adaptively for heterophilic graphs. ACM-GCN is one ofthe best-performing heterophilic GNNs that uses adaptive channelswith low-pass and high-pass filters. Recently, the authors of proposed an adaptive filter-based GNN, ALT, that combines signalsfrom two filters with complementary filter characteristics to classifyhomophilic and heterophilic graphs. These methods perform wellfor small heterophilic graphs but do not scale to large graphs. In thespectral domain, AGS-GNN can be used in conjunction with theseapproaches by computing feature-similarity and feature-diversity-based sparse graphs at first before applying filters for large graphs.For applying spatial GNNs to heterophilic graphs, rather thanusing average aggregation (as in homophilic GNNs), edge-awareweights of neighbors can be assigned according to the spatial graphtopology and node labels. DMP considers node attributes asweak labels and aggregates element-wise weights from neighbors.GGCN uses cosine similarity to create signed neighbor features;the intuition is to send positive attributes to neighbors in the sameclass and negative ones to neighbors in different classes. GPNN considers ranking the neighbors based on similarity and uses a 1Dconvolution operator on the sequential nodes. Another related workis SimP-GCN , which computes the node similarity and thenchooses the top similar node pairs in terms of feature-level cosinesimilarity for each ego node, and then constructs the neighbor setusing the-NN algorithm. AGS-GNN, in contrast, uses submodularity, node-similarity, reweighting, and sampling of the subgraph insteadof reconstructing neighbor sets.When learned weight functions are considered, AGS-GNN canbe placed into the category of supervised sampling. LAGCN trains an edge classifier from the existing graph topology, similarto our regression task of weight approximation. NeuralSparse learns a sparsification network to sample a -neighbor subgraph(with a predefined ), which is fed to GCN , GraphSAGE or GAT . Unlike our heuristic-based sampler, NeuralSparse hasend-to-end training of the sampler network and GNN, and mayrequire more iterations to find appropriate sampling probabilities.There are only a few scalable GNNs for heterophilic graphs.The most notable one is LINKX , which is transductive as themodel architecture depends on node size. A recent scalable GNN,LD2 , attempts to remedy this by transforming the adjacencyand the feature matrix into embeddings as a pre-computation andthen applying feature transformation in a mini-batch fashion.",
  "Dataset, Setup, and Methods": "5.1.1Dataset: We experimented with 35 graphs of different sizesand varying homophily. We also generated synthetic graphs ofdifferent homophily and degrees while retaining the node fea-tures for ablation studies and scaling experiments. We consideredthe node classification task in our experiments. For heterophilystudies, we used: Cornell, Texas, Wisconsin ; Chameleon,Squirrel ; Actor ; Wiki, ArXiv-year, Snap-Patents,Penn94, Pokec, Genius, Twitch-Gamers, reed98, amherst41,cornell5, and Yelp . We also experiment on some recentbenchmark datasets, Roman-empire, Amazon-ratings, Mineswe-eper, Tolokers, and Questions from . We converted a fewmulti-label multiclass classification problems (Flickr, Amazon-Products) to single-label multiclass node classification, and theirhomophily values become relatively small, making them heterophilic.For homophily studies we used Cora ; Citeseer ; pubmed; Coauthor-cs, Coauthor-physics ; Amazon-computers,Ama-zon-photo ; Reddit ; Reddit2 ; and, dblp .Details of datasets, homophily measures (), homophily distri-butions (, 14), origins, splits, etc. () are in Appendix 10. 5.1.2Experimental Setup: All evaluations are repeated 10 times us-ing a split of 60%/20%/20% (train/validation/test), unless a specificsplit is specified. All experiments are executed on 24GB NVIDIAA10 Tensor Core GPU. For all benchmark models, we use the set-tings specified in their corresponding papers, and for AGS-GNN, weuse two message-passing layers ( = 2) and a hidden layer withdimension = 256. We use the Adam optimizer with a learn-ing rate of 103 and train for 250 epochs or until convergence. Amodel converges if the standard deviation of the training loss inthe most recent 5 epochs (after at least 5 epochs) is below 104. Fornode sampling, we use a batch size of 512 to 1024 with = or in the two layers unless otherwise specified. For graphsampling, we use a batch size of 6000 and a random walk step sizeof 2. For reporting accuracy, we select the model that gives the bestvalidation performance. Depending on the models, we use a dropoutprobability of 0.2 or 0.5 during training. All of the implementations",
  "Key Results": "In , we show the performance profile plot of the algorithmsw.r.t. their relative 1-score. For each graph, we compute the relative1-score for all algorithms by subtracting their 1-score from thebest one. Thus the best performing algorithm for a problem receivesa score of 0, and for all other algorithms the difference is positive.The X-axis of represents these relative values from the best-performing algorithms across the graph problems, and the Y-axisshows the fraction of problems that achieve 1-score within thebound on the difference specified by the X-axis value. Thus, theperformance plot reveals a ranking of the algorithms in terms oftheir quality. The closer a curve (algorithm) is to the Y-axis, andthe smaller the difference in the X-axis, the better its performancew.r.t. other algorithms across all the problems considered. asummarizes results from five algorithms across 17 test problems forsmall heterophilic graphs, where we observe that AGS-NS performsthe best or close-to-best for about half of the problems and hasup to 10% lower 1 scores compared to the best algorithm for theother half. ACM-GCN performs similarly to AGS-NS for these smallgraphs. While LINKX achieves comparable accuracies to AGS-NSfor most of the problems (about 80%), for a few problems it achieveslower 1 scores.For large heterophilic graphs (c), performance improvementfor AGS-NS is considerably better than all algorithms. LINKX+ per-forms second-best for 75% of the problems. In small homophilicgraphs (b), AGS-NS and GSAGE are the top two performersfor most of the problems, followed by GSAINT, ACM-GCN, andLINKX. This is expected since ACM-GCN and LINKX are tailoredfor heterophilic graphs. In large homophilic graphs (d), AGS-NS is the best-performing algorithm in terms of accuracy, followedby GSAGE. We also observe from this figure that for homophilicgraphs, LINKX+ is not competitive.",
  ": 1 score using different subgraph samples ( =). Three synthetic versions of Cora are produced ( =200 and H 0.05, 0.25, and 0.50, respectively)": "In Table. 15 (Appendix 11.2) we present performance of AGS-NS compared to 18 recent competing algorithms on small het-erophilic graphs. ACM-GCN, AGS-NS, and LINKX remain the best-performing, with AGS-NS as the leading method for these inputs.The full set of results with numerical values are provided in Appen-dix (Tables 11, 12, 13, 14, 15). These tables present the mean () andstandard deviation () of the runs. We use a one-tailed statisticalhypothesis -test to verify whether one set of values is better thanthe other. The better-performing results are highlighted in boldface.",
  "Ablation study": "We now investigate the contribution of individual components ofAGS-GNN, employing different sampling strategies and GNNs. ForGNNs, we consider GSAGE (spatial) and ChebNet (spectral).We sample ( ) neighbors for each node usingsimilarity and diversity-based sparsification using precomputedweights (detailed in section 3.2), and random sparsification thatselects a subgraph uniformly at random. Only a single sampledsubgraph is used throughout the training. For experiments, wegenerate three synthetic versions of Cora with average degree = 200, keeping the original nodes and features the same butchanging the connectivity to have strong (0.05), moderate (0.25),and weak (0.50) heterophily. The distribution of heterophily foreach node is close to uniform. shows that diversity-basedselection performs best with strong heterophily, and with spectralGNN, it even achieves accuracy better than using the entire graph.In contrast, on moderate heterophily, the similarity-based selectionperforms the best (even better than the whole graph). For weakheterophily (homophily), similar and random sparse perform alike.When we convert these into a sampling paradigm (node samplingor graph sampling), similar behavior can be seen () as weget the best performance from diversity-based selection for strongheterophily and similarity-based for moderate heterophily with awide margin than random. For weak heterophily or homophily, thesimilarity-based performs slightly better than the random sampler.Since real-world graphs have nonuniform node homophily, wegenerated a synthetic version of Cora where individual nodes have",
  "(d) Large Homophilic Graphs": ": Performance Profile: The X-axis shows the differences in 1-scores (scaled to 100) between the best algorithm for aspecific problem, and the Y-axis shows the fraction of the problems. We compare AGS to two scalable homophilic GNNs (GSAGE,GSAINT) and two heterophilic GNNs (ACM-GCN, LINKX). For small (< 100 vertices) and large ( 100 vertices) graphs, weconsider LINKX (full-batch) and LINKX+ (mini-batch), respectively. Full results are in Appendix 11.1.",
  ": 1 Score of two-channel GNNs with a differentlyweighted sampler in synthetic Cora graph with locally het-erophilic and homophilic nodes mixed (H() = [0.05, 0.50])": "different local node homophily in the range [0.05, 0.50]. shows that our proposed two channel (one for homophily and onefor heterophily) AGS-GNN performs the best, significantly outper-forming ACM-GCN and LINKX. Detailed studies on the parametersand different submodular functions used in diversity-based sam-pling are presented in Appendix 12.2. Loss of Quality (F1) wrt to the Best 0% 25% 50% 75% 100% Percentage of Problems (5 Problems) AGS-GSAGEAGS-GSAINT(rw) AGS-ChebnetAGS-GCNAGS-GAT AGS-GIN AGS-GSAGEAGS-GSAINT(rw)AGS-ChebnetAGS-GCNAGS-GATAGS-GIN",
  ": Performance Profile: X-axis is the difference in 1-scores (scaled to 100) for different GNNs coupled with AGSsampler on five benchmark heterophilic graphs": "We coupled our sampling strategy (both node sampling andgraph sampling (9)) to existing GNNs (GSAGE, ChebNet, GSAINT,GIN, GAT, and GCN) and evaluated them on five heterophilicgraphs (Reed98, Roman-empire, Actor, Minesweeper, Tolok-ers). While detailed numerical values are provided in inAppendix 12, we summarize the key results as a performance profilein . We observe that AGS with GSAGE, ChebNet, and GSAINTperformed the best.",
  "Experimental Runtime and Convergence": "shows per epoch training time of different methods underthe same settings. For large datasets such as Reddit, with singleworker thread, our current implementation of weighted samplingrequires around 3 times more than the random sampling used inGSAGE due to the dual channels and a few implementation differ-ences with PyTorchGeometric. We plan to improve our implementa-tion next. Since our precomputation is embarrassingly parallel, wecan accelerate our algorithm by increasing the number of workerthreads. Our weighted random walk, is faster than the library imple-mentation even for sequential execution. shows the numberof epochs required to converge for random sampling (GSAGE) andour weighted sampling (AGS-GNN). Using the same settings, we seethat AGS-GNN is more stable and requires fewer epochs on averageto converge than GSAGE.",
  ": Num. of epochs for AGS-GNN and GSAGE to converge": "we get improved performance in homophilic graphs and can han-dle challenging heterophilic graphs. We verify our claims throughexhaustive experimentation on various benchmark datasets andmethods. A limitation of our work is the time required for sub-modular optimization when the facility location function is used;the computation complexity is higher for dense graphs. We willoptimize implementations in our future work. We will also buildan end-to-end process for supervised sampling. Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Nazanin Alipourfard, KristinaLerman, Hrayr Harutyunyan, Greg Ver Steeg, and Aram Galstyan. 2019. Mixhop:Higher-order graph convolutional architectures via sparsified neighborhoodmixing. In International Conference on Machine Learning. PMLR, 2129. Hao Chen, Yue Xu, Feiran Huang, Zengde Deng, Wenbing Huang, SenzhangWang, Peng He, and Zhoujun Li. 2020. Label-aware graph convolutional net-works. In Proceedings of the 29th ACM International Conference on Information &Knowledge Management. 19771980.",
  "Andreas Krause and Daniel Golovin. 2014. Submodular function maximization.Tractability 3, 71-104 (2014), 3": "Xiang Li, Renyu Zhu, Yao Cheng, Caihua Shan, Siqiang Luo, Dongsheng Li, andWeining Qian. 2022. Finding global homophily in graph neural networks whenmeeting heterophily. In International Conference on Machine Learning. PMLR,1324213256. Ningyi Liao, Siqiang Luo, Xiang Li, and Jieming Shi. 2023. LD2: Scalable Het-erophilous Graph Neural Network with Decoupled Embeddings. In Thirty-seventhConference on Neural Information Processing Systems. Derek Lim, Felix Hohne, Xiuyu Li, Sijia Linda Huang, Vaishnavi Gupta, OmkarBhalerao, and Ser Nam Lim. 2021. Large scale learning on non-homophilousgraphs: New benchmarks and strong simple methods. Advances in Neural Infor-mation Processing Systems 34 (2021), 2088720902.",
  "Meng Liu, Zhengyang Wang, and Shuiwang Ji. 2021. Non-local graph neuralnetworks. IEEE Transactions on Pattern Analysis and Machine Intelligence 44, 12(2021), 1027010276": "Sitao Luan, Chenqing Hua, Qincheng Lu, Jiaqi Zhu, Mingde Zhao, ShuyuanZhang, Xiao-Wen Chang, and Doina Precup. 2022. Revisiting heterophily forgraph neural networks. arXiv preprint arXiv:2210.07606 (2022). Sitao Luan, Mingde Zhao, Chenqing Hua, Xiao-Wen Chang, and Doina Precup.2020. Complete the missing half: Augmenting aggregation filtering with diver-sification for graph convolutional networks. arXiv preprint arXiv:2008.08844(2020).",
  "Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang.2020. Geom-gcn: Geometric graph convolutional networks. arXiv preprintarXiv:2002.05287 (2020)": "Oleg Platonov, Denis Kuznedelev, Artem Babenko, and Liudmila Prokhorenkova.2022. Characterizing graph datasets for node classification: Beyond homophily-heterophily dichotomy. arXiv preprint arXiv:2209.06177 (2022). Oleg Platonov, Denis Kuznedelev, Michael Diskin, Artem Babenko, and LiudmilaProkhorenkova. 2023. A critical look at the evaluation of GNNs under heterophily:are we really making progress? arXiv preprint arXiv:2302.11640 (2023).",
  "Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How powerfulare graph neural networks? arXiv preprint arXiv:1810.00826 (2018)": "Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichiKawarabayashi, and Stefanie Jegelka. 2018. Representation learning on graphswith jumping knowledge networks. In International Conference on Machine Learn-ing. PMLR, 54535462. Zhe Xu, Yuzhong Chen, Qinghai Zhou, Yuhang Wu, Menghai Pan, Hao Yang, andHanghang Tong. 2023. Node Classification Beyond Homophily: Towards a Gen-eral Solution. In Proceedings of the 29th ACM SIGKDD Conference on KnowledgeDiscovery and Data Mining. 28622873. Yujun Yan, Milad Hashemi, Kevin Swersky, Yaoqing Yang, and Danai Koutra.2022. Two sides of the same coin: Heterophily and oversmoothing in graphconvolutional neural networks. In 2022 IEEE International Conference on DataMining (ICDM). IEEE, 12871292. Liang Yang, Mengzhe Li, Liyang Liu, Chuan Wang, Xiaochun Cao, Yuanfang Guo,et al. 2021. Diverse message passing for attribute with heterophily. Advances inNeural Information Processing Systems 34 (2021), 47514763.",
  "Yang Ye and Shihao Ji. 2021. Sparse graph attention networks. IEEE Transactionson Knowledge and Data Engineering (2021)": "Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and ViktorPrasanna. 2019. GraphSAINT: Graph Sampling Based Inductive Learning Method.In International Conference on Learning Representations. Elena Zheleva and Lise Getoor. 2009. To join or not to join: the illusion of privacyin social networks with mixed public and private user profiles. In Proceedings ofthe 18th International Conference on World Wide Web. 531540. Cheng Zheng, Bo Zong, Wei Cheng, Dongjin Song, Jingchao Ni, Wenchao Yu,Haifeng Chen, and Wei Wang. 2020. Robust graph representation learning vianeural sparsification. In International Conference on Machine Learning. PMLR,1145811468.",
  "Dengyong Zhou, Olivier Bousquet, Thomas Lal, Jason Weston, and BernhardSchlkopf. 2003. Learning with local and global consistency. Advances in NeuralInformation Processing Systems 16 (2003)": "Jiong Zhu, Ryan A Rossi, Anup Rao, Tung Mai, Nedim Lipka, Nesreen K Ahmed,and Danai Koutra. 2021. Graph neural networks with heterophily. In Proceedingsof the AAAI Conference on Artificial Intelligence, Vol. 35. 1116811176. Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and DanaiKoutra. 2020. Beyond homophily in graph neural networks: Current limitationsand effective designs. Advances in Neural Information Processing Systems 33(2020), 77937804.",
  "Homophily of similarity-based selection": "Let be an ego node of a graph with degree , local node ho-mophily H(), and label . From the definition of homophily, theprobability of selecting a neighbor N () with the same labelas the ego node uniformly at random is U ( = ) = H(),where U denotes to the distribution obtained by selecting a neigh-bor uniformly at random. If features and labels correlate positively,and similarity is computed from the features, we can expect thefollowing assumption to be valid:",
  "Assumption 7.1. For a node , the average similarity of neigh-bors with the same label as is greater than or equal to the averagesimilarity of all neighbors": "Lemma 7.1. If the probability of selecting a neighbor is proportionalto its similarity to the ego node , then the local node homophily of asampled neighborhood H() H(). If the sampling probabilitydistribution is S then S( = ) U ( = ). Proof. Consider an ego node = {, } where , and areits feature and label, respectively. Let the feature and labels of theneighboring nodes of be N () = {(1,1), (2,2), , (, )}.Let (, ) be a similarity function that measures how similarthe feature of a neighbor is to the feature of the ego node .This function returns a positive value, and higher values indicatehigher similarity.A probability distribution with probability mass function S()assigns a probability to each neighbor based on its similarity to theego node. The distribution should satisfy the following properties:",
  "max (,),(7)": "where and are as above and is a similarity measure on theelements of . The facility location function can be maximizedby a Greedy algorithm, which starts with the empty subset anditeratively adds an element that gives the largest marginal gain tothe function value until a cardinality (or other) constraint on isreached.In this context, is the current set of selected nodes initializedwith ego node , and ground set = N () {}. The marginal gainis = ( {},) (,) for each neighbor N () \\ . Theneighbors are iteratively added based on marginal gain. If nodefeatures and labels are positively correlated, then we can expectthe following assumption to be valid:",
  "Empirical verification of the Lemmas": "empirically verifies our assumptions that a subgraph selectedfrom a similar neighborhood increases the homophily, and selectinga diverse neighborhood using a submodular function decreaseshomophily, relative to the values in the original graph. For eachvertex , we take ( ) neighbors based on theranking from similarity and diversity and compare against randomselection.",
  "Similarity-basedDiversity-based": ": Empirical homophily comparison of different sam-pled subgraphs keeping 0.25 neighbors of node . Thesynthetic graphs are from squirrel with = 42. The figureshows that similar neighbors increase H, diverse samplesobtained from a submodular function decrease H, and ran-dom selection keeps H the same as in the original graph.7.4High-Pass and Low-Pass Filters for GNN We consider the following heterophilic graph in to demon-strate how high-pass filters can help in node classification in het-erophilic graphs. Assume a heterophilic graph G has 18 nodes offive different classes where nodes in the range are locallyheterophilic and are locally homophilic. We assume thegraphs node feature be the one-hot encoding of labels, thus thefeatures and labels are (perfectly) correlated. This is to show howlow-pass and high-pass filters perform in an ideal scenario withheterophily. : A heterophilic graph to demonstrate the perfor-mance of high-pass and low-pass filter. Here, nodes 0 14 arelocally heterophilic, and nodes 1517 are locally homophilic.In the figure, vertices with the same color correspond to thesame class label.",
  "( , ) = ( )": "In these matrices, for a particular node, if the average similarityof nodes with the same label as is greater than the averagesimilarity of the nodes not having the same label as , then wecan expect that node to be appropriately classified (Definition 2and Theorem 1 in ).In AGS-GNN based sampler, along with similarity based sampling,we also sample the neighborhood of vertices based on feature di-versity. The feature diversity-based sampler will attempt to create",
  "1": "2 , and the pairwise similarityweight after aggregation is, ( , ) = ( ) .The highlighted values show that this low-pass filter cannotproperly classify heterophilic nodes. Here, nodes 0 2 havepositive similarity weights to all other nodes. the same diverse neighborhood of two heterophilic nodes, thuspotentially mapping in the same vector space. In a low-pass filter,however, the neighboring nodes of the same label as the ego nodewith similar features help to map them in the same space; there-fore, the feature-similarity-based sampler will work in conjunctionwith the low-pass filter. AGS-GNN uses a dual channel of feature-similar and feature-diverse samples for each node in the graph andadaptively learns a better representation.",
  "Computation of sampling probability fordiverse samples": "We can use submodular functions to rank the neighbors of a vertex.Multiple submodular functions are available for finding a usefulsubset from a larger set; e.g., MaxCoverage, FeatureBased,GraphBased, FacilityLocation, etc. Algorithm 3 shows thepseudocode for computing sampling probabilities using ranking bydiversity. Algorithm 4 shows the pseudocode of a simplified LazyGreedy algorithm using the facility location function. For distancemeasures such as Euclidean, distance.max()-distance is usedto convert into similarity measure. Note that the ego node isalready taken as an initial set for submodular selections, and theremaining nodes are to be selected from the neighbors.Ablation studies on different submodular functions are providedin Appendix 12.2, and more details on functions and implementa-tions are provided in Apricot (url). Similar to the pseudocode ofRankBySimilarity in Algorithm 2, we can also use a step functionto reduce computation complexity by ranking only the top fewitems.",
  ": return SIMW, E": "Algorithm 5 shows the training process for the regression taskof predicting edge weights between 0 and 1. We form a mini-batchby sampling an equal number of edges and non-edges from thetraining subgraph. The target labels are computed using the labelsof each endpoint of the edges. For a few training nodes, trainingsubgraphs mostly contain isolated vertices. In such a scenario, wecan construct a new graph where all training nodes with the sameclass are connected and use that for training purposes."
}