{
  "Abstract": "Monitoring and maintaining machine learning models are amongthe most critical challenges in translating recent advances in thefield into real-world applications. However, current monitoringmethods lack the capability of provide actionable insights answer-ing the question of why the performance of a particular model reallydegraded. In this work, we propose a novel approach to explain thebehavior of a black-box model under feature shifts by attributing anestimated performance change to interpretable input characteristics.We refer to our method that combines concepts from Optimal Trans-port and Shapley Values as Explanatory Performance Estimation(XPE). We analyze the underlying assumptions and demonstratethe superiority of our approach over several baselines on differentdata sets across various data modalities such as images, audio, andtabular data. We also indicate how the generated results can leadto valuable insights, enabling explanatory model monitoring byrevealing potential root causes for model deterioration and guidingtoward actionable countermeasures.",
  "Both authors contributed equally to this research.Also with German Cancer Research Center": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for third-party components of this work must be honored.For all other uses, contact the owner/author(s).KDD 24, August 2529, 2024, Barcelona, Spain. 2024 Copyright held by the owner/author(s).ACM ISBN 979-8-4007-0490-1/24/08 : Illustration of an opaque vision system subject tohardware degradation or environment changes, e.g., speckleson the lens or stray light. Explanatory Performance Estima-tion (XPE) allows to anticipate and explain the resulting per-formance decrease by highlighting which parts of the shiftare harmful. This provides actionable insights to restore per-formance and facilitate effective model maintenance. ACM Reference Format:Thomas Decker, Alexander Koebler, Michael Lebacher, Ingo Thon, VolkerTresp, and Florian Buettner. 2024. Explanatory Model Monitoring to Un-derstand the Effects of Feature Shifts on Performance. In Proceedings of the30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining(KDD 24), August 2529, 2024, Barcelona, Spain. ACM, New York, NY, USA,12 pages.",
  "Introduction": "Deploying Machine Learning (ML) models successfully in prac-tice is a challenging endeavor as it requires models to cope wellwith complex and dynamic real-world environments . As aconsequence, monitoring and maintaining ML-models has been es-tablished as a central pillar of the modern ML-Life cycle andcommercial ML frameworks . A crucial assumption to assurethe validity of a model is that the data distribution during trainingmatches the real-time distribution during deployment. However,",
  "KDD 24, August 2529, 2024, Barcelona, Spain.Thomas Decker et al": ": Spectrogram considering frequencies of up to1024 for a male (left) and a female (right) participant pro-nouncing the digit three. The fundamental frequency bandsfor males male0and females female0are highlighted. by gender and region because they are not sufficiently representedin the training data .In this experiment, we demonstrate how XPE can help to indicatesuch a selection bias if relevant meta-information about the in-cluded groups in the source and target distribution is absent. Weanalyze the prediction task of classifying spoken digits 0 to 9from audio signals based on the dataset provided by containingreal voice recordings of 60 different speakers with varying ages,sexes, and accents. Since our method is agnostic to the model used,we follow exactly the same model choice made by the original re-searchers , which propose to use an AlexNet as a classifier.To do so, the voice recordings are transformed into 2D spectrogramsof size 227227 using the Short-time Fourier Transform (STFT), andan example is shown in . To introduce a selection bias, weconsider a model solely trained on male participants, so the modelfaced a significant bias during training with potential consequenceson the performance during operation. We evaluated this model onunseen test sets with varying fractions of female participants andasses if feature shift importance can identify the selection bias as asource for model degradation.Due to the high dimensionality of the input data ( = 51.529),we apply two general extensions to improve the scalability of allattribution methods and reduce the computational effort. First, wesolved the optimal transport problem based on intermediate net-work activations of the used AlexNet instead of the raw inputs .This allows us to reduce the effective size over which the transportneeds to be computed and increases the quality of the resultingperformance estimation significantly. Second, we computed shiftimportance at the level of frequency bands instead of all individualspectrogram values, which also reduces the computational loadwhile maintaining the interpretability of the resulting explanations.To do so, we group features by splitting the frequency axis into32 frequency bands of 32 resolution and aggregating over theentire time axis. This level of granularity is sufficient to distinguishgender-specific characteristics related to the induced selection bias.Based on these application-specific improvements, we evaluate allshift importance methods on the biased model (trained on malesonly) when facing three shift scenarios with varying bias sever-ity. First, a test set consisting of unseen male participants, such XPEXPPEL ADAxS",
  "Problem Setting": "We consider the common situation where a machine learning model : X Y has been trained to perform a prediction task in a super-vised fashion based on labeled training data {(,)}=1. Further,we assume that the training data originates from a source distri-bution denoted as (,). At some point during deployment, wesuppose that the underlying data distribution changes and further equals to the target distribution (,) with (,) (,).As common in practice, we suppose that during deployment weonly have access to unlabeled data instances { }=1 originatingfrom the marginal target distribution (). As a motivational ex-ample, consider the situation in where a black-box modelprocessing images is monitored. During deployment, the data dis-tribution changes as a consequence of hardware degradation, e.g.,speckles on the camera lens, causing a feature drift in some imageareas. However, the degree of model robustness under a distributionshift might vary across image areas and individual features mightbe more important to the system in general. Hence, a shift mightactually hurt the models performance only through very specificimage areas and revealing those provides valuable information forefficient model monitoring and maintenance.To achieve this, XPE must accomplish two things in a data andmodel-agnostic manner. First, it should estimate the models per-formance under (,) despite missing target labels. Second, itshould attribute the anticipated performance change to specific in-put features through which the distribution shift affects the model.",
  "Background and Related Work": "Optimal Transport. Optimal transport refers to the math-ematical problem of identifying the most cost-efficient way to trans-form one probability measure into another one . Consider twomeasurable spaces (X1, ) and (X2,) and a non-negative cost func-tion : X1 X2 R+. The Monge formulation aims to find adeterministic transportation map : X1 X2 solving",
  "X1(, ())()s.t.# =": "where# describes the push forward measure resulting from proba-bility mass transfer from with respect to, so#() = ( 1()).In general, the existence of such a deterministic map is not guaran-teed, but the relationship between and can also be expressedvia a probabilistic coupling. Mathematically, such a coupling rep-resents any joint distributions over (X1 X2) with marginals equalto and . This leads to a relaxed problem corresponding to theKantorovich formulation of optimal transport:",
  "X1X2(1,2)(1,2)": "Note, that a cost-optimal coupling can be shown to exist under mildtheoretical assumptions and can easily be estimated based onempirical samples via linear programming . More advancedestimation techniques have been proposed to ensure scalability, to account for known structure within the data or to make the estimation more robust . Concepts andtools originating from optimal transport theory have been appliedto various areas of machine learning concerned with modelingand relating data distributions. This covers for instance generativemodels or the evaluation of the semantic correspon-dence between documents , images andspatio-temporal data . Another related application of optimaltransport is unsupervised domain adaptation , wherethe goal is to leverage labeled data from a source domain to obtainmodels that generalize well to an unlabeled target domain. Therealso exist proper extensions for cases where a limited number of",
  "Explanatory Model Monitoring to Understand the Effects of Feature Shifts on PerformanceKDD 24, August 2529, 2024, Barcelona, Spain": "target labels is available or to improve adaptation results inthe presence of general types of data shifts . The utilizationof optimal transport that is most related to our work is given by. The authors propose to estimate interpretable transport plansto rather explain the nature of distribution shifts irrespective oftheir actual effect on a given model. Feature attribution methods. Feature attribution methods quan-tify to what extent individual input features have contributed toa model output of interest and have been established as a popularmean to make the predictions of black-box models more transparent. More specifically, such methods aim to determine an impor-tance vector such that quantifies the influence that each inputfeature had on a scalar-valued model output. Removal-basedmethods are a particular kind of attribution techniques thatachieve this by simulating the absence of features and evaluatingthe resulting prediction changes using different computational ap-proaches. A prominent example are Shapley Values , whichhave been introduced as a fair way to distribute the total outcomeof a coalition game to individual players = {1, . . .}. In this con-text, a game can be specified via a value function () : 2 Rthat quantifies the value that each possible subset or coalition ofplayers would achieve if only they would contribute. Givena value function, the Shapley Value of each player results asa weighted average of its marginal contributions over all possiblecoalitions and orders:",
  "| |(( {}) ())": "To attain feature attribution for a model and input R, indi-vidual features , . . . , resemble players and () defines the hy-pothetical model prediction where only features in [] wouldbe present. Different computational methods have been proposed toenable such a value function by simulating model predictions underpartial feature absence . A prominent one is to integrateaffected features out based on the marginal data distribution. Foran index set and decomposition = (, ), this results in",
  "() = (, )": "Based on such strategies for feature removal, different algorithmicapproaches have been developed to compute corresponding fea-ture attributions efficiently . Moreover, Shapley Values havebeen shown to satisfy desirable theoretical properties related to faircredit allocations in game theory and are model and data-agnostic.Hence, they can automatically be applied to explain any machinelearning model and any kind of model output. In particular, theycan also directly be used to explain how individual features havecontributed to the overall model performance . As a conse-quence, Shapley Values enjoy great popularity among practitioners and they have further been employed for a wide variety ofdifferent applications related to machine learning . Feature attributions for model monitoring. Major ML frameworkssuch as Amazon SageMaker Model Monitor or Google VertexAI Model Monitoring offer monitoring of feature attributionsand interpret changing importance scores as an indicator for po-tential performance degradation. Mougan et al. demonstrateon synthetic tabular examples that monitoring attribution resultscan be superior compared to monitoring input data characteristics.However, it remains unclear under which circumstances this ap-proach can reliably signal an actual performance decrease and itdoes not provide any insights regarding why the model might havedeteriorated. Another approach is to simply combine drift detectionwith attribution methods and expect a performance change if animportant feature shifts . But this can be misleading when themodel is robust to the shift occurred in these features. On top ofthat, attribution methods themselves can be sensitive to shifts andsmall perturbation , so simply evaluating them on drifteddata may produce unreliable results. Budhathoki et al. leverageShapley Values to identify potential reasons for a distribution shiftbased on causal graphs and apply this idea in the context ofmodel monitoring. While theoretically appealing, these approachesheavily rely on complete knowledge about the causal mechanismsof the true data-generating process which is infeasible to attain inpractice. Therefore, these methods cannot be utilized to supportmonitoring in general deployment scenarios. Label-free performance estimation. Reliably estimating the per-formance of a machine learning model under distribution shifts inthe absence of target labels is a significant yet considerably hardproblem. In fact, the task has been shown to be impossible whenallowing for arbitrary shifts . However, it becomes feasibleif additional assumptions can be posed either on the scope of theshift, the relationship between source and target domain or on themodel itself. For instance, importance weighting can be used toestimate the target performance if the experienced change is eitherdue to a covariate shift or resembles a label shift .In cases where the model outputs probabilistic predictions that arewell calibrated on the target domain, the correspondingperformance can be anticipated purely based on the models confi-dence . But calibration under domain shifts is particularlychallenging and hence an area of active research .",
  "In this section, we formalize the task of feature shift importance,which aims at revealing through which specific features an observeddistribution shift affects a model": "Aligning distributions via optimal transport. A natural way togain a better understanding of how a distribution shift preciselyimpacts the predictions of a machine learning model is to systemat-ically compare its individual predictions before and after the shifthappened. For this purpose, suppose that the experienced distribu-tion shift can be expressed by a functional transformation . Moreprecisely, this means that the target domain distribution can beobtained from the source distribution via push-forward opera-tion: = #. In this case, the immediate effect on a single modelprediction () that is purely induced by the distribution shift canbe analyzed by comparing the corresponding predictions () and",
  "aggregated to quantify how individual feature shifts have contributed to the anticipated model loss based on Shapley Values": "( ()). Modern deep neural networks have demonstrated impres-sive capabilities to parameterize functional transformations thatperform complex and realistic distribution shifts . However,they typically require a lot of data to be trained and might evenrely on labeled source/target pairs. During deployment, there isusually only a limited number of discrete samples from the sourceand target domain available. Therefore, optimal transport providesa more suitable way to estimate the relationships between and in practice. Suppose we have samples randomly drawn fromthe source domain = {}=1 and from the target domain = { }=1. Let be the Dirac delta function, describing a validprobability distribution concentrated at the point , then the em-pirical source and target distributions are given by:",
  "(,)(,)": "Intuitively, provides a probabilistic estimate of how samples of thesource domain are likely to look in the target domain and vice versa(see a). This equips us with an appealing tool to comprehendthe precise nature of the shift and can further be utilized to revealhow an observed shift affected a model. In this case, understandingthe impact of a distribution shift for a single prediction () couldbe achieved by comparing it with all predictions correspondingto the potential source version of as implied by the conditionalcoupling ( |) (b). Moreover, it is straightforward to trans-form a probabilistic alignment into a deterministic one by matchingeach source sample with its most related target sample. This resultsin a transform () = arg max ( |) and equivalently 1() = arg max ( |) mapping each onto its mostlikely source version. In general, the resulting coupling depends on the chosen cost function , but the squared Euclidean distance is apopular default choice for various applications including domainadaptation . In contrast to domain adaptation, we rather seekto understand how a shift impacts a model in order to determine ifand specifically why adaptation during deployment might be nec-essary. To achieve this, we propose a novel way of combining thesample-wise alignment of and implied by optimal couplingswith feature attribution methods. Shapley Values for Feature Shift Importance. In order to betterunderstand how an observed input feature shift influenced a modelprediction we propose to consider a novel coalition game wherethe value function () expresses the model prediction under theassumption that only features in experienced the shift (c). Asintroduced above, Optimal Transport allows us to identify potentialpre- and post-shift versions of data instances related to the empiri-cal source and target distributions. The corresponding results candirectly be utilized to perform the required partial distribution shiftsof a given target sample (d). Leveraging the probabilisticrelationship expressed by the transport coupling we get:",
  "() = ( 1 ()) with 1 () = (,, 1() )": "If is directly inferred from a probabilistic coupling like de-scribed above, then resembles a computationally more efficientapproximation of , which is equivalent if the conditional coupling( |) allocates all probability mass to a single source sample.When computing Shapley Values for such value functions, canbe interpreted as a measure of how the empirical shift in feature contributed to the shift-related prediction change (e). More-over, carefully comparing and with existing value functionsdescribed in reveals a close relationship. While () and () are designed to compute partial feature absence for basic",
  "Explanatory Performance Estimation (XPE)": "Anticipating performance changes during deployment. When ashift occurs, it is critical to reevaluate whether the model still per-forms sufficiently well under the new circumstances. Reliably com-puting the performance of a model would require access to cor-responding ground truth labels. Such information is typically notavailable during deployment and usually requires cumbersome man-ual efforts. However, empirically aligning labeled source samples and unlabeled target samples via transformation also equipsus with a reasonable way to anticipate the performance by suppos-ing that all linked instances exhibit the same label. More precisely,one can obtain for any a label estimate by allocatingthe known label of the linked source sample 1() . Underappropriate assumptions on the nature of the shift, this heuristicwill accurately estimate the performance in the target domain. Definition 5.1. Let X Y, (X, Y), (X Y, (X, Y)) betwo probability spaces corresponding to the source and target do-mains. For measurable sets , let (,) = sup|() ()| bethe statistical total variation distance between two probability mea-sures and . A distribution shift from to is -approximatelabel-preserving with respect to if there exists",
  "|L L | 2": "The proof and further details are given in the Appendix. Intu-itively, this establishes a continuity result, showing that if the deci-sion boundary for transported samples in the target domain is closeto the original one, then the estimation error can also be expectedto be small. This assumption might seem restrictive in general, butwe want to highlight that label-free performance estimation onlybecomes feasible in the presence of additional constraints. Relyingspecifically on this assumption resonates quite well with typicalreal-world causes for distribution shifts during deployment: Phys-ical changes in the environment, hardware degradation, or otherdata quality issues can all be considered as shifts that modify inputdata characteristics without necessarily affecting the label. Explanatory Performance Estimation using Shapley Values. Ourproposed Explanatory Performance Estimation (XPE) approachcombines transport-based label estimation with feature shift impor-tance in the following way. Given a loss function L : Y Y R+",
  "we define a new value function XPE() = L 1 (),": "which expresses directly the anticipated performance change underpartial feature shifts. The corresponding Shapley ValuesXPE finallyindicate through which specific features an observed distributionshift impacts the anticipated performance, providing valuable infor-mation regarding potential root causes of model degradation. Forclassification models that output an entire probability distributionover possible classes , we further propose a label-estimation-freevariation of XPE, which we call Explanatory Performance ProxyEstimation (XPPE). Let ( ()) = () log( ()) bethe entropy of the probabilistic model output () capturing themodels degree of uncertainty about its prediction. By consideringhigher predictive uncertainty as a proxy for potential model degra-dation we define a novel value function XPPE() = ( ( 1 ()).It computes the model uncertainty under partial shifts and the re-sulting Shapley Values XPPE signal through which features a shifthas affected the models confidence, which does not require a labelestimate. Thus, XPPE can be superior to XPE when the shift doesnot satisfy Def. 5.1 of being approximately label preserving.",
  "Experiments": "The goal of our experiments is to rigorously analyze feature shift at-tributions and their capabilities to reliably explain the model behav-ior under deployment-related distribution shifts. In total, we reportthe results on nine different datasets covering three different datamodalities with associated data shift scenarios. In .1, weanalyze four different vision tasks and demonstrate that our methodis the most effective way to understand a performance decreasecaused by shifts resembling hardware degradations. In .2we show, for a speech classification task, that only our proposedmethods can reliably signal an induced selection bias. Finally, in.3 we evaluate four tabular data sets and demonstrate thecapabilities of our method to correctly assign a performance de-cline due to data quality issues in the form of missing values. Moredetails are documented in the Appendix and code is provided at Baselines. To assess the capabilities of XPE we first formalizebaselines that are connected to existing model monitoring practices.Remember that XPE aims to evaluate by which specific features anobserved distribution shift impacts the model performance. A firstbaseline for this purpose is to simply check whether predictions inthe target domain tend to rely on other features compared to thesource domain. Let () be the outcome of standard Shapley Valuesexplaining the prediction () for an instance . Given an estimatedtransportation map , we can simply compare the explanations oftwo matched samples individually and define a local attributiondifference (LAD):",
  "AxS(, KS) = () KS": "Note, that this baseline captures the idea of considering only fea-tures that have drifted and are simultaneously important for themodel as potentially harmful gateways of the observed distributionshift mentioned in . Defining suitable metrics. Quantitatively evaluating any kind ofmodel explanation is challenging, but a desirable property is faith-fulness . It generally tries to asses if perturbing features withhigh attribution scores also cause coherent prediction changes anda variety of different related metrics have been proposed .To evaluate feature shift attributions, we reformulate the faithful-ness criterion in the following way: When features with high shiftattributions are shifted back, we expect the model performance torecover equivalently. Suppose access to the true pre-shift version 1() of a target sample as well as to the ground truth sourceand target labels and. Then, we can define Shift-Faithfulness(S-Faith) of a feature shift attribution Shift as the correlation be-tween the actual performance change under partial feature shiftand the sum of allocated shift importance:",
  "Shift, L (), L 1 (),": "Here we adapted the metric based on the notation from , wherethe correlation is computed using different feature subsets withfixed size ||. Another popular metric to measure the quality of fea-ture attributions is RemOve And Retrain (ROAR) assessing ifthe performance actually decreases when important features are re-moved and models retrained. Consequently, we propose an adaptedmetric coined remove, retrain, and shift (ROAR-S), which evalu-ates whether the performance decrease caused by a shift diminishedif features with high shift importance are removed and the modelretrained. More precisely, we define the ROAR-S score as the pro-portion of shift-induced performance decrease that remains whenfor each instance the top 5% of input features highlighted by Shift are removed, and the model is subsequently retrained. If this scoreis small, the distribution change no longer affects the performanceand the shift importance is reliable. More details about this metricare deferred to the Appendix. To quantify the actionability of ex-planations we consider the Complexity (Cpx) metric , whichis defined as the Shannon entropy of the normalized attributionvalues: Cpx(Shift) = (|Shift|/ |Shift|). This expresses the un-certainty of shift attribution results across all input features andlower values indicate that the method communicates the potentialreason for model degradation more concisely.",
  "Understanding the effects of hardwaredegradation": "For the first experiment we consider a variety of popular light-weight image datasets, including MNIST , images of fashionitems as well as various types of medical images , and simu-lated several distribution shifts mimicking potential camera-relatedhardware degradation or physical changes in the environment .This setup ensures complete knowledge about the true pre- andpost-shift pairs which is crucial for evaluating the quality of shiftattribution methods via Shift-Faithfulness. For each dataset, wefitted a LeNet model , evaluated the metrics based on 500 testsamples and used the cross-entropy loss as a performance measure.The average results are reported in and indicate that XPEalmost consistently outperforms all baselines followed by XPPE assecond best. Most of the time, all other baselines are not correlatedat all with the true performance change induced by the shift of high-lighted features. The corresponding results for Complexity implythat the explanations of XPE also tend to be the most concise given asufficient degree of faithfulness. For cases where other methods pro-vide significantly less complex results, they typically have almostno correlation with the actual performance decrease. Moreover,the label transport accuracy ( = ) was for all considered shifts> 85%. This is in line with the Theorem above as applying a corrup-tion to an image equals a functional transformation that preservesits label and shows that aligning via optimal transport is capableof apprehending the considered transformations. To confirm ourfindings, we also evaluated ROAR-S and the results demonstratethat the performance change caused by the shift is on average bestmitigated when dropping features according to XPE and XPPE. Deriving intuitive and actionable insights. Next, we would liketo demonstrate how the results obtained via XPE can yield noveland actionable insight into the model behavior under distributionshifts concerning images. To do so, we locally examine shift attri-butions on the MNIST digit and the pneumonia detection dataset(PneumM) where a certain shift had a particularly harmful effecton the prediction and seek to understand the reasons. In and we plot some of these examples and notice that forsuch instances, the shifts do indeed perturb essential image regionsin a way that suggests a different class, i.e., a different digit or apositive pneumonia classification. By consulting the different shiftattribution results to narrow down a concrete reason we see thatonly XPE consistently highlights the parts of the corruption thatactually alter the appearance of a digit towards a different one orresemble white spots in the lung area indicating pneumonia. Thisshows that also the model is mainly misled by the intuitive regions,which cannot be concluded from the other results. This informationcan help end-users take efficient and targeted corrective measuresfor their applications, such as cleaning or repairing the camera lensor removing ambient light sources. Supporting model selection. Moreover, such analysis can also beused to compare how different models handle certain distributionshifts. In we visualize some results of XPE for the LeNetanalyzed above and a Multi-Layer-Perceptron (MLP) with one hid-den layer. The plots illustrate that the LeNets predictions mainlyget harmed through the shift of features in close proximity to the",
  "PneumM": "XPE0.385.880.335.170.783.210.255.570.635.790.793.150.624.430.723.900.96XPPE0.205.870.175.490.493.190.075.510.435.780.403.140.234.400.453.890.92LAD-0.015.500.035.080.085.840.015.240.025.830.045.820.035.900.015.892.00AxS-0.015.150.074.100.103.03-0.004.760.025.480.082.980.053.97-0.003.615.05 : Average S-Faithfulness (S-Faith), Complexity (Cpx), and ROAR-S results of shift attributions methods for a LeNet ondifferent image datasets and corruptions. A higher S-Faithfulness value indicates that features highlighted by Shift are stronglycorrelated with the true performance change caused by the shift in these features. A lower Complexity value corresponds tomore concise explanations and a lower ROAR-S score signals that removing features based on Shift effectively mitigates theshift-induced performance change. : Feature shift importance on MNIST for local andglobal corruptions. The explanations given by XPE are mostintuitive, only highlighting the area of the zigzag connectingthe top parts of the 4, which changes the models predictionto a 9. A similar observation can be made for the spattercorruption, changing the prediction from 9 7. XPE alsoallows uncovering the image regions shifting the predictionfrom 2 8 given a global increase in brightness. actual digit, whereas the MLP also gets distracted by parts of thecorruptions at irrelevant peripheral regions. This implies that theLeNet model with convolutional feature extraction generally copesbetter with the considered shifts, while the MLP might has internal-ized some unstable global patterns. Such results convey valuableinsights that can help to disclose model deficiencies, perform modelselection, or guide model debugging. : Feature shift importance for pneumonia detectiondataset (PneumM) consisting of real chest X-rays. Neitherimage shows pneumonia. Only XPE consistently assigns thecontribution to the loss increase caused by the shift to ar-eas that might resemble pneumonia indicators (white spots)within the lung area.",
  "Method": "ratio [%] strong shift medium shift no shift : Ratio of feature shift importance allocated togender-specific frequencies for a biased model solely trainedon male participants and evaluated on sets with only women(strong shift), with equal gender distribution (medium shift),and with solely men (no shift) not in the training set. OnlyXPE and XPPE attribute importance proportionally to thestrength of the induced selection bias. that a difference in performance between training and test samplesshould be predominantly due to a change in non-gender-specificfrequencies, e.g., caused by different pronunciations between indi-vidual persons. Second, we evaluate the model on a test set with anequal number of male and female participants. Finally, we test themodel on a test set consisting only of women, which results in astrong gender-specific shift. To assess the quality of feature shiftimportance in this context, we build on existing domain knowledgeabout gender-specific human vocal frequencies . It suggeststhat distinctive frequency ranges are between 85 and 155for males and 165 and 255 for females. Therefore, we expectthat as the fraction of females in the test set increases, the overallshift importance allocated to gender-specific frequencies shouldalso increase proportionally, successfully indicating the severity ofthe selection bias. To investigate this, we calculate the proportionof feature shift importance within the combined male and femalefundamental vocal frequency bands (Shiftgender) relative to the total",
  "Shifttotal": "The results presented in demonstrate that only XPE andXPPE attribute importance proportionally to the strength of theinduced selection bias, while all other baselines are insensitive to thefaced distribution shift. This suggests that in cases where a selectionbias is introduced during the training of machine learning models,XPE and XPPE can detect it and prompt further investigation, evenin the absence of labels in the target domain or correspondingmeta-information.",
  "Assessing the impact of missing values intabular data": "As a final experiment, we investigate the effectiveness of shift attri-bution methods in the presence of data quality issues on four tabulardatasets from a popular evaluation benchmark . Specifically,we focus on the commonly encountered problem of facing missingvalues due to a compromised data acquisition process. This is typi-cally handled via an imputation strategy I that replaces missingvalues based on a predefined heuristic such as the feature mean. Foreach dataset, we fitted two different models (XGBoost and anMLP) and corrupted a set of = 500 unseen test samples by ran-domly selecting a column and marking 25% of values as missing,yielding a perturbed test set . We then assess how reliably theshift importance values associated with column , denoted by Shift,correspond to the true impact of the corruption. To this end, wecompute the correlation between Shiftand the loss difference dueto missing value imputation across all samples in the test set. Thisresults in a global and feature-specific version of Shift-Faithfulness,which we refer to as Global Performance-Correlation (GPC):",
  "Shift( ), L (I( )), L,": "Here, I( )) corresponds to the corrupted sample , where themissing values are filled using a mean value imputer I fitted onthe training set. While standard S-Faith evaluates the quality ofa local shift attribution result for an individual sample, GPC is aglobal measure of shift attribution fidelity for specific features ofinterest across an entire test set. The corresponding results in demonstrate that XPE and XPPE allocate an importance scoreto the corrupted column that matches the actual loss contributioncaused by the quality issue best. This consolidates the capabilitiesof our introduced methods to improve model monitoring in thepresence of common data quality issues also for tabular data.",
  "Discussion and Conclusion": "We introduced Explanatory Performance Estimation (XPE) as anovel and agnostic framework to attribute an anticipated changein model performance induced by a distribution shift to individualfeatures. Our approach requires no ground truth labels during de-ployment, which corresponds to the typical situation in practice and is applicable to any monitoring situation. We also provideda theoretical analysis and extensive experiments covering ninedatasets to demonstrate the empirical success of our method acrossten different shifts and three data modalities.Our approach is limited by the capabilities of optimal transportto model the true observed shift based on samples, which can bechallenging for high-dimensional data and may require computingthe transport based on lower-dimensional input representations(as exemplified in the audio experiment) or the use of more so-phisticated cost functions . Another way to enhance scalabilityfor individual applications is to leverage model or data-specificapproximations of Shapley Values or to incorporate additionalconcepts from unsupervised domain adaptation for aligningsource and target samples. Moreover, explaining the effect of a shiftin terms of individual input shifts might not always be the optimallevel of abstraction. Hence, combining XPE with concept-basedexplanations would be a natural extension. This could evenfurther increase the applicability of XPE as an effective method toenable explanatory model monitoring in practice.",
  "David Alvarez-Melis, Tommi Jaakkola, and Stefanie Jegelka. 2018. Structured op-timal transport. In International Conference on Artificial Intelligence and Statistics.PMLR, 17711780": "David Alvarez-Melis, Stefanie Jegelka, and Tommi S Jaakkola. 2019. Towardsoptimal transport with global invariances. In The 22nd International Conferenceon Artificial Intelligence and Statistics. PMLR, 18701879. Marco Ancona, Enea Ceolini, Cengiz ztireli, and Markus Gross. 2018. Towardsbetter understanding of gradient-based attribution methods for Deep NeuralNetworks. In 6th International Conference on Learning Representations (ICLR).",
  "Martin Arjovsky, Soumith Chintala, and Lon Bottou. 2017. Wasserstein gen-erative adversarial networks. In International conference on machine learning.PMLR, 214223": "Yogesh Balaji, Rama Chellappa, and Soheil Feizi. 2020. Robust optimal transportwith applications in generative modeling and domain adaptation. Advances inNeural Information Processing Systems 33 (2020), 1293412944. Sren Becker, Johanna Vielhaben, Marcel Ackermann, Klaus-Robert Mller,Sebastian Lapuschkin, and Wojciech Samek. 2023. AudioMNIST: ExploringExplainable Artificial Intelligence for audio analysis on a simple benchmark.Journal of the Franklin Institute (2023).",
  "Shai Ben-David and Ruth Urner. 2012. On the hardness of domain adaptation andthe utility of unlabeled target samples. In International Conference on AlgorithmicLearning Theory. Springer, 139153": "Umang Bhatt, Adrian Weller, and Jos MF Moura. 2021. Evaluating and aggre-gating feature-based model explanations. In Proceedings of the Twenty-NinthInternational Conference on International Joint Conferences on Artificial Intelli-gence. 30163022. Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly, Yun-han Jia, Joydeep Ghosh, Ruchir Puri, Jos MF Moura, and Peter Eckersley. 2020.Explainable machine learning in deployment. In Proceedings of the 2020 confer-ence on fairness, accountability, and transparency. 648657. Kailash Budhathoki, Dominik Janzing, Patrick Bloebaum, and Hoiyi Ng. 2021.Why did the distribution change?. In Proceedings of The 24th International Con-ference on Artificial Intelligence and Statistics (Proceedings of Machine LearningResearch, Vol. 130), Arindam Banerjee and Kenji Fukumizu (Eds.). PMLR, 16661674.",
  "Ian Covert, Scott M Lundberg, and Su-In Lee. 2020. Understanding globalfeature contributions with additive importance measures. Advances in NeuralInformation Processing Systems 33 (2020), 1721217223": "Marco Cuturi. 2013.Sinkhorn Distances: Lightspeed Computation of Op-timal Transport. In Advances in Neural Information Processing Systems, C.J.Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger (Eds.),Vol. 26. Curran Associates, Inc. Marco Cuturi, Michal Klein, and Pierre Ablin. 2023. Monge, Bregman and Oc-cam: Interpretable Optimal Transport in High-Dimensions with Feature-SparseMaps. In Proceedings of the 40th International Conference on Machine Learning(Proceedings of Machine Learning Research, Vol. 202), Andreas Krause, EmmaBrunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and JonathanScarlett (Eds.). PMLR, 66716682. Bharath Bhushan Damodaran, Benjamin Kellenberger, Rmi Flamary, DevisTuia, and Nicolas Courty. 2018. Deepjdot: Deep joint distribution optimaltransport for unsupervised domain adaptation. In Proceedings of the EuropeanConference on Computer Vision (ECCV). 447463. Shai Ben David, Tyler Lu, Teresa Luu, and Dvid Pl. 2010. Impossibility the-orems for domain adaptation. In Proceedings of the Thirteenth InternationalConference on Artificial Intelligence and Statistics. JMLR Workshop and Confer-ence Proceedings, 129136. Thomas Decker, Ralf Gross, Alexander Koebler, Michael Lebacher, RonaldSchnitzer, and Stefan H Weber. 2023. The Thousand Faces of Explainable AIAlong the Machine Learning Life Cycle: Industrial Reality and Current State ofResearch. In International Conference on Human-Computer Interaction. Springer,184208.",
  "John Fitch and Anthony Holbrook. 1970. Modal vocal fundamental frequencyof young adults. Archives of otolaryngology 92 4 (1970), 37982": "Rmi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z. Alaya, AurlieBoisbunon, Stanislas Chambon, Laetitia Chapel, Adrien Corenflos, Kilian Fatras,Nemo Fournier, Lo Gautheron, Nathalie T.H. Gayraud, Hicham Janati, AlainRakotomamonjy, Ievgen Redko, Antoine Rolet, Antony Schutz, Vivien Seguy,Danica J. Sutherland, Romain Tavenard, Alexander Tong, and Titouan Vayer.2021. POT: Python Optimal Transport. Journal of Machine Learning Research22, 78 (2021), 18.",
  "Gao Huang, Chuan Guo, Matt J Kusner, Yu Sun, Fei Sha, and Kilian Q Weinberger.2016.Supervised word movers distance.Advances in neural informationprocessing systems 29 (2016)": "Hicham Janati, Marco Cuturi, and Alexandre Gramfort. 2020. Spatio-temporalalignments: Optimal transport through space and time. In International Confer-ence on Artificial Intelligence and Statistics. PMLR, 16951704. Dominik Janzing, Lenon Minorics, and Patrick Blbaum. 2020. Feature relevancequantification in explainable AI: A causal problem. In International Conferenceon artificial intelligence and statistics. PMLR, 29072916.",
  "Yiding Jiang, Vaishnavh Nagarajan, Christina Baek, and J Zico Kolter. 2022.Assessing Generalization of SGD via Disagreement. In International Conferenceon Learning Representations": "Krishnaram Kenthapadi, Himabindu Lakkaraju, Pradeep Natarajan, andMehrnoosh Sameki. 2022. Model Monitoring in Practice: Lessons Learnedand Open Challenges. In Proceedings of the 28th ACM SIGKDD Conference onKnowledge Discovery and Data Mining. 48004801. Ashkan Khakzar, Soroosh Baselizadeh, Saurabh Khanduja, Christian Rupprecht,Seong Tae Kim, and Nassir Navab. 2021. Neural response interpretation throughthe lens of critical pathways. In Proceedings of the IEEE/CVF conference on com-puter vision and pattern recognition. 1352813538.",
  "Valentin Khrulkov and Ivan Oseledets. 2022. Understanding ddpm latent codesthrough optimal transport. arXiv preprint arXiv:2202.07477 (2022)": "Pieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof TSchtt, Sven Dhne, Dumitru Erhan, and Been Kim. 2019. The (un) reliability ofsaliency methods. Explainable AI: Interpreting, explaining and visualizing deeplearning (2019), 267280. Matthieu Kirchmeyer, Alain Rakotomamonjy, Emmanuel de Bezenac, and patrickgallinari. 2022. Mapping conditional distributions for domain adaptation undergeneralized target shift. In International Conference on Learning Representations.",
  "Yann LeCun, Lon Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradient-based learning applied to document recognition. Proc. IEEE 86, 11 (1998), 22782324": "Lanna Lima, Vasco Furtado, Elizabeth Furtado, and Virgilio Almeida. 2019.Empirical Analysis of Bias in Voice-Based Personal Assistants. In CompanionProceedings of The 2019 World Wide Web Conference (San Francisco, USA) (WWW19). Association for Computing Machinery, New York, NY, USA, 533538. Tianyi Lin, Nhat Ho, and Michael Jordan. 2019. On efficient optimal transport:An analysis of greedy and accelerated mirror descent algorithms. In InternationalConference on Machine Learning. PMLR, 39823991.",
  "Scott M Lundberg and Su-In Lee. 2017. A unified approach to interpreting modelpredictions. Advances in neural information processing systems 30 (2017)": "Sasu Mkinen, Henrik Skogstrm, Eero Laaksonen, and Tommi Mikkonen. 2021.Who needs MLOps: What data scientists seek to accomplish and how can MLOpshelp?. In 2021 IEEE/ACM 1st Workshop on AI Engineering-Software Engineeringfor AI (WAIN). IEEE, 109112. Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and AramGalstyan. 2021. A Survey on Bias and Fairness in Machine Learning. ACMComput. Surv. 54, 6, Article 115 (jul 2021), 35 pages. Carlos Mougan, Klaus Broelemann, Gjergji Kasneci, Thanassis Tiropanis, andSteffen Staab. 2022. Explanation Shift: Detecting distribution shifts on tabulardata via the explanation space. In NeurIPS 2022 Workshop on Distribution Shifts:Connecting Methods and Applications.",
  "Norman Mu and Justin Gilmer. 2019. MNIST-C: A Robustness Benchmark forComputer Vision. ArXiv abs/1906.02337 (2019)": "David Nigenda, Zohar Karnin, Muhammad Bilal Zafar, Raghu Ramesha, AlanTan, Michele Donini, and Krishnaram Kenthapadi. 2022. Amazon SageMakerModel Monitor: A System for Real-Time Insights into Deployed Machine Learn-ing Models. In Proceedings of the 28th ACM SIGKDD Conference on KnowledgeDiscovery and Data Mining (Washington DC, USA) (KDD 22). Association forComputing Machinery, New York, NY, USA, 36713681. Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, SebastianNowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek. 2019. Canyou trust your models uncertainty? evaluating predictive uncertainty underdataset shift. Advances in neural information processing systems 32 (2019).",
  "Tim Salimans, Han Zhang, Alec Radford, and Dimitris Metaxas. 2018. Improv-ing GANs Using Optimal Transport. In International Conference on LearningRepresentations": "David Sculley, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Diet-mar Ebner, Vinay Chaudhary, Michael Young, Jean-Francois Crespo, and DanDennison. 2015. Hidden technical debt in machine learning systems. Advancesin neural information processing systems 28 (2015). Murtuza N Shergadwala, Himabindu Lakkaraju, and Krishnaram Kenthapadi.2022. A Human-Centric Perspective on Model Monitoring. In Proceedings of theAAAI Conference on Human Computation and Crowdsourcing, Vol. 10. 173183.",
  "Hidetoshi Shimodaira. 2000. Improving predictive inference under covariateshift by weighting the log-likelihood function. Journal of statistical planningand inference 90, 2 (2000), 227244": "Dylan Slack, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju.2020. Fooling lime and shap: Adversarial attacks on post hoc explanationmethods. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society.180186. Stefan Studer, Thanh Binh Bui, Christian Drescher, Alexander Hanuschkin,Ludwig Winkler, Steven Peters, and Klaus-Robert Mller. 2021. Towards CRISP-ML (Q): a machine learning process model with quality assurance methodology.",
  "Vayer Titouan, Nicolas Courty, Romain Tavenard, and Rmi Flamary. 2019.Optimal transport for structured data with application on graphs. In InternationalConference on Machine Learning. PMLR, 62756284": "Christian Tomani and Florian Buettner. 2021. Towards trustworthy predictionsfrom deep neural networks with fast adversarial calibration. In Proceedings ofthe AAAI Conference on Artificial Intelligence, Vol. 35. 98869896. Christian Tomani, Sebastian Gruber, Muhammed Ebrar Erdem, Daniel Cremers,and Florian Buettner. 2021. Post-hoc uncertainty calibration for domain driftscenarios. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition. 1012410132.",
  "Yuguang Yan, Wen Li, Hanrui Wu, Huaqing Min, Mingkui Tan, and QingyaoWu. 2018. Semi-Supervised Optimal Transport for Heterogeneous DomainAdaptation.. In IJCAI, Vol. 7. 29692975": "Jiancheng Yang, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke,Hanspeter Pfister, and Bingbing Ni. 2023. MedMNIST v2-A large-scale light-weight benchmark for 2D and 3D biomedical image classification. ScientificData 10, 1 (2023), 41. Chih-Kuan Yeh, Cheng-Yu Hsieh, Arun Suggala, David I Inouye, and Pradeep KRavikumar. 2019. On the (in) fidelity and sensitivity of explanations. Advancesin Neural Information Processing Systems 32 (2019).",
  "Chih-Kuan Yeh, Been Kim, and Pradeep Ravikumar. 2022. Human-Centered Con-cept Explanations for Neural Networks. In Neuro-Symbolic Artificial Intelligence:The State of the Art. IOS Press, 337352": "Mikhail Yurochkin, Sebastian Claici, Edward Chien, Farzaneh Mirzazadeh, andJustin M Solomon. 2019. Hierarchical optimal transport for document represen-tation. Advances in neural information processing systems 32 (2019). Haoran Zhang, Harvineet Singh, Marzyeh Ghassemi, and Shalmali Joshi. 2023.\"Why did the Model Fail?\": Attributing Model Performance Changes to Distribu-tion Shifts. In Proceedings of the 40th International Conference on Machine Learn-ing (Proceedings of Machine Learning Research, Vol. 202), Andreas Krause, EmmaBrunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and JonathanScarlett (Eds.). PMLR, 4155041578. Wenliang Zhao, Yongming Rao, Ziyi Wang, Jiwen Lu, and Jie Zhou. 2021. To-wards interpretable deep metric learning with structural matching. In Proceed-ings of the IEEE/CVF International Conference on Computer Vision. 98879896.",
  "(5) E ( )2 ( | ()), ( |) (6) 2": "In step (1) we performed a change of variables using = ()and in (2) we used property 1 of being-approximate label-preserving.In step (3) we used Jensens inequality to pull the absolute valueinside the expectation and in step (4) we used the assumption thatL 1. In step (5) we applied the Lemma specified above andin step (6) we leveraged property 2 of being -approximate label-preserving.",
  "BDetails on Experiments": "Data and Models. For the image data experiments, we analyzedMNIST , FashionMNSIT as well as OrganaMNIST andPneumoniaMNIST from the MedMNISTv2 benchmark . For eachdataset we trained and evaluated a LeNet model . All modelshave been trained for 100 epochs with early stopping based on apatience of 10 epochs and PyTorchs Adam optimizer with a batchsize of 16 and a learning rate of 1e-3. The used audio data set ispublicly available. We downsampled the original raw sound data toa new sampling frequency of 2048 Hz and generated a spectrogramfor every sound file using Short-time Fourier Transform (STFT).The STFT is calculated using a segment length of 455 and a overlapof 445. The training and test sets each consist of 12 randomly drawnparticipants according to the by the experiment determined genderdistribution. According to the original paper of the audio data setwe also used AlexNet1 to solve the digit classification task. Themodels are trained for 40 epochs. The training has been performedusing an Adam optimizer with a batch size of 16 and a learning rate",
  "Used PyTorch implementation of AlexNet:": "of 1e-3. For the tabular data, we selected four classification datasetsand downloaded the preprocessed version provided by . Foreach dataset we performed a 80-20 train-test split and fitted anXGBoost classifier with 50 estimators of max depth 5 as well as aMulti-Layer Perceptron (MLP) with one hidden layer of size 128. Optimal Transport. In all experiments we match the same amountof samples in source and target domain using EMDTransport solverin the POT library with default parameters. Thus, the esti-mated couplings typically result in a one-to-one matching and thesubsequent estimation of XPE and XPPE based on are equivalentto those leveraging the entire coupling . Shift Attribution. To compute the necessary Shapley Values foreach shift attribution method we relied on the model-agnostic Ker-nelSHAP implementation provided by . All Shapley Values havebeen computed using a sample size of 3000 and the default choicesfor all other hyperparameters. For LAD and AxS baselines we useda background dataset of 30 random samples for tabular data and afixed baseline of zeros otherwise. For the AxS metric, we estimatedthe shift mask KS using a featurewise two-sided Kolmogorov-Smirnov test with 95% confidence threshold to signal a shift asprovided by . Shift-Faithfulness and Complexity. To compute the metrics we re-lied on the implementation provided by . For Shift-Faithfulness,we used the Faithfulness Correlation metric, specified the perturba-tion baseline to be the estimated per-shift version of each sampleand chose a sample size of 100 with a subset size || of 64. All otherhyperparameters correspond to their default choices. Moreover,we removed samples where the anticipated performance changeis only marginal as this causes either all shift attributions to bezero or causes numerical problems during the computation of thecorrelation value used in Shift-Faithfulness. ROAR-S Metric. Our implementation of Remove, Retrain, andShift ROAR-S is based on the implementation by of the ROARmetric . Given the high computational requirements of ROAR,we sub-sample the original datasets to obtain trainand testwith = 1000 samples each. trainis used to train the pre-removalLeNet model . For each shift we create the shifted dataset versionstrainand testand compute the different feature shift attribu-tions Shift. Next we remove the top 5% of the features attributedthe highest importance. This yields the post-removal sets train,test, trainand test. The new source training set trainis usedto retrain the model yielding . With cross-entropy loss L, thetest performances Ltest=1"
}