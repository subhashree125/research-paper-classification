{
  "ABSTRACT": "In this study, we demonstrate the application of a hybrid VisionTransformer (ViT) model, pretrained on ImageNet, on an electroen-cephalogram (EEG) regression task. Despite being originally trainedfor image classification tasks, when fine-tuned on EEG data, thismodel shows a notable increase in performance compared to othermodels, including an identical architecture ViT trained without theImageNet weights. This discovery challenges the traditional un-derstanding of model generalization, suggesting that Transformermodels pretrained on seemingly unrelated image data can providevaluable priors for EEG regression tasks with an appropriate fine-tuning pipeline.The success of this approach suggests that the features extractedby ViT models in the context of visual tasks can be readily trans-formed for the purpose of EEG predictive modeling. We recommendutilizing this methodology not only in neuroscience and relatedfields, but generally for any task where data collection is limitedby practical, financial, or ethical constraints. Our results illuminatethe potential of pretrained models on tasks that are clearly distinctfrom their original purpose.",
  "Both authors contributed equally to this research": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior specific permission and/or afee. Request permissions from -UC 23, August 0610, 2023, Long Beach, CA 2023 Association for Computing Machinery.ACM ISBN 978-1-4503-XXXX-X/23/08...$15.00",
  "INTRODUCTION": "With its rich, multidimensional structure, electroencephalogram(EEG) data encapsulates a wealth of information about brain activity,offering unique insights into a variety of neurological phenomena. However, the intricacies that define EEG data introduce sub-stantial hurdles towards devising efficient, yet effective predictivemodels, especially considering the incredibly costly process of datacollection . Despite the extensive usage of machine learningregression models for predicting EEG data, these models often fallshort of adequately capturing and decoding the complex structureswithin such data. As an example, the EEGEyeNet dataset pro-vides a baseline for predicting the position of a subjects gaze on ascreen given EEG data. Their results clearly demonstrate the inabil-ity of machine learning models to accurately interpret the givendata. We also explore articles that employ analogous time-seriesdata .In recent years, there has been a significant surge in the inventionand application of deep learning models to unravel this complexityand extract valuable insights from EEG data. In particular, Con-volutional Neural Networks (CNNs) and the Self-Attentionoperator have demonstrated promising results in managingthe complexity of EEG data , with several architectures beingderived from their successful counterparts in the field of ComputerVision . These deep learning techniques excel at captur-ing nonlinear patterns and relationships within data, making themparticularly suitable for modeling complex EEG signals.Our study takes the paradigm of converting successful computervision models to an EEG counterpart a step further and proposesthe use of a hybrid Vision Transformer (ViT) pretrained onthe ImageNet dataset to take on an EEG regression task. Thisapproach takes a theoretical leap from previous methods, repurpos-ing a models architecture and weights initially designed for visiontasks to grapple with the high dimensionality and inherent complex-ity of EEG data. This research is predicated on the observation thatEEG data, like image data, possesses rich, high-dimensional struc-tures that can be effectively modeled using Transformer models.Our empirical findings indicate that utilizing a pretrained ViTmodel significantly outperforms traditional regression models. Inaddition, we demonstrate that the success of the model comes fromnot just the model architecture, but also the impact of pretrainingthe ViT on the models initial weights. This success underlines thestrength of pretraining the model across disciplines, demonstratinghow models trained on abundant, easily obtained image data canbe effectively repurposed for EEG data analysis, an area where datacollection often involves considerable challenges due to practical,financial, and ethical constraints .",
  "RELATED WORK2.1Deep Learning for EEG Tasks": "EEG regression and classification tasks, fundamental to neuro-science research and applications like Brain-Computer Interfaces(BCIs), have traditionally relied on Convolutional Neural Networks(CNNs) . While CNNs have proven their use for extractinglocalized features from time-series EEG data, they struggle withcapturing the long-term dependencies, subject-independent andsession-independent patterns inherent in EEG data . This limi-tation motivates the exploration of other model architectures thatcan replace or support CNNs.Recently, hybrid methods complementing CNNs with Self-Attentionmodules have shown promising improvements in performanceby capturing both local patterns and long-term dependencies inEEG signals . In addition, recent studies in Computer Visionhave demonstrated the superiority of Transformer-focused modelsover predictive models focused on convolutional layers . Fol-lowing this trend, our study aims to test the standalone capabilitiesof Transformers (specifically Vision Transformers), expecting theViTs ability to capture global dependencies in the data to deliversuperior performance in the complex, high-dimensional EEG datapresent in eye-tracking tasks.",
  "EEG Eye-Tracking with EEGEyeNet": "Using EEG data for applications in eye-tracking presents uniquechallenges and applications, ranging from assisting with cogni-tive studies to developing assistive technologies. The recent in-troduction of the EEGEyeNet dataset , containing 47 hours ofhigh-density 128-channel EEG data synchronized with eye-trackingrecordings from 356 healthy adults, offers a valuable resource forstudying such complex relationships. This dataset not only aids instudying attention and reaction time, but it also provides a prepro-cessing pipeline and benchmarks for gaze estimation, facilitatingreproducible research every step of the way.",
  "Vision Transformers (ViTs)": "Introduced by , Vision Transformers (ViTs) have demonstratedimpressive results in image classification tasks. Unlike CNNs, whichuse spatial convolution operations for feature extraction, ViTs di-vide the input image into a grid of patches and leverage transformerarchitecture to process these patches as a sequence. This design al-lows ViTs to capture global dependencies in the input data withoutconvolution operations, making them more suited for the globally-correlated nature of EEG data.Research in similar domains indicates the applicability of ViTs.For example, studies have demonstrated ViTs effectiveness in tasksinvolving audio and video processing , hinting at itspotential for spatial-temporal analysis of EEG data. Despite itspotential, the usage of ViTs in EEG analysis is largely unexplored,prompting the need for research like the current study.",
  ": EEGEyeNet Large Grid Paradigm . Participantsare asked to fixate on particular dots in a given period": "simultaneous contemplation of all input fragments during decision-making procedures. This particular ability is critical in capturingintricate and distant dependencies, a characteristic frequently en-countered in Electroencephalogram (EEG) data. Moreover, the trans-ferability of pretrained ViT models , which are originallytrained on vast datasets, permits their fine-tuning for subsequenttasks. This property is particularly advantageous for EEG-relatedendeavors, which often grapple with a dearth of labeled data.Nonetheless, despite the demonstrable success and potential ofpretrained ViT models, it is imperative to recognize their limitationswhen deployed directly to realms that are substantially distinct fromtheir original training context. One such example is the inherentlyhigh-dimensional and noisy nature of EEG data. Hence, the presentinvestigation critically assesses the efficacy of employing a pre-trained ViT model for an EEG regression task. This explorationcould consequently lay a foundation for future scientific inquiriesin this domain.",
  "ViT2EEG: Leveraging Hybrid Pretrained Vision Transformers for EEG DataKDD-UC 23, August 0610, 2023, Long Beach, CA": "Hamdi Altaheri, Ghulam Muhammad, Mansour Alsulaiman, Syed Umar Amin,Ghadir Ali Altuwaijri, Wadood Abdul, Mohamed A Bencherif, and MohammedFaisal. 2021. Deep learning techniques for classification of electroencephalogram(EEG) motor imagery (MI) signals: A review. Neural Computing and Applications(2021), 142. Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lui, andCordelia Schmid. 2021. Vivit: A video vision transformer. In Proceedings of theIEEE/CVF international conference on computer vision. 68366846. Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, and Quoc V Le. 2019.Attention augmented convolutional networks. In Proceedings of the IEEE/CVFinternational conference on computer vision. 32863295. Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han Hu. 2019. Gcnet: Non-local networks meet squeeze-excitation networks and beyond. In Proceedings ofthe IEEE/CVF international conference on computer vision workshops. 00.",
  "Two-Step Convolution Block. We adopt the two-step convolu-tion block utilized in EEG analysis from the early layers of EEGNet": ". This block consists of two convolutional layers, with one layerfiltering the temporal dimension and the other layer filtering thechannel (spatial) dimension.The first layer employs a 1 kernel and scans across the entireinput, capturing temporal events that occur over the same channels.The kernels in this temporal convolutional layer can be regarded asvarious band-pass filters applied to the raw signals. The resultingoutput feature maps represent the frequency bands of interest forthe EEG. Batch normalization is applied on the output .Following this, a depthwise convolutional layer containinga 1 kernel is used. This layer scans over multiple channels,given the same point in time, filtering the inputs separately. Thisdepthwise convolution also acts as a frequency-specific spatial filterwhen applied to EEG feature extraction in this context.The overall operation can be viewed as splitting the input imagesinto patches, followed by a row-by-row linear projection,with the resulting column vector transformed into a scalar feature.",
  ": Transformer Encoder block by We follow thisparadigm to implement the Transformer block in EEGViT": "The decision to separate the projection for each dimension andincorporate the depthwise mechanism proves advantageous.To align with the ViT-base models encoder layers, which arepretrained on images divided into 192 patches , we carefullyselect hyperparameters for an output of 224 patch embedding vec-tors. These patches, each representing the activity of eight adjacentchannels over 36 timestamps (72ms at a 500Hz sampling rate), areset to be non-overlapping, based on experimental findings. Hyper-parameters are further tuned for optimal feature preservation andcomputational efficiency ().Lastly, to integrate our patch sequence into the pretrained ViTlayers, which use 192 patches and corresponding pretrained posi-tional embeddings, we reinitialize a sequence of positional embed-dings to match our patch sequence length.\" 3.2.2Transformer Block. We adopt the transformer encoder par-adigm (see ) and utilize the ViT-Base model architecture in to implement the transformer block in our model. This blockconsists of 12 modules, each comprising a Multi-Head Attentionlayer and a Multi-Layer Perceptron, wrapped by residual connec-tion and layer normalization. Detailed hyperparameters for theselayers can be found in .For the pretrained version of the transformer block, we loadthe weights of the encoder layers from the ImageNet-pretrainedViT-Base model, consisting of approximately 86M parameters, andreadily available on HuggingFace .The input to the transformer block is a sequence of 1D patchembeddings obtained by flattening the output feature maps of thepreceding Two-Step Convolution block. We incorporate positionalembeddings into this sequence. Additionally, inspired by ViT, we",
  ": Details of ViT-Base model adopted for the trans-former block in EEGViT": "adopt BERTs approach of appending a CLS token to the sequence.This CLS token aids in aggregating information across the sequence. The hidden state of the CLS token ultimately serves as therepresentation of the entire sequence of patch embeddings and issubsequently transformed into the target space, which correspondsto 2D coordinates on the grid, using fully connected layers.",
  "Training": "Training machine learning models effectively requires a meticulousand strategic approach to setting various parameters and configura-tions. The goal is to optimize the models learning process, enablingit to understand complex patterns in the data and deliver reliablepredictions on new, unseen data. The EEGViT model was trainedwith such attention to detail.For our study, we adopted the Mean Squared Error (MSE) lossfunction as the primary measure to guide our models learning.MSE loss function is widely used in regression problems due to itsmathematical simplicity and effectiveness. In our specific contextof EEG signal analysis, it allows the model to minimize the predic-tion error of continuous output values, essentially improving themodels ability to predict the exact EEG signal values. Although weemployed MSE for training, we decided to use Root Mean SquareError (RMSE) to measure our results.Its essential to mention that using MSE as the loss function andsubsequently using RMSE as the performance metric is a commonlyadopted approach in various scientific studies, particularly thoseinvolving EEG and eye-tracking data . The use of these metricsis not only restricted to our work but has also been used extensivelyin similar studies.This common usage is mainly due to the advantage that thesemetrics offer in evaluating the models ability to accurately predictcontinuous outcomes while minimizing the overall deviation. More-over, since these metrics are squared, they place a higher penaltyon larger errors, thereby pushing the model to improve its overallaccuracy. This attribute is extremely beneficial in sensitive analy-ses like EEG signal interpretation, where the aim is to minimizeprediction discrepancies as much as possible.Furthermore, the use of RMSE as a performance metric providesus a direct comparison with other research findings in the field ofEEG eye-tracking. This allows for a standardized evaluation acrossvarious studies, enhancing the generalizability and transferabil-ity of our findings within the field. Given that other studies havesuccessfully employed the same method, it strengthens our confi-dence in the adopted approach and provides more credibility to ourresults.In order to compare different strategies and evaluate the effec-tiveness of our proposed approach, we trained two different modelarchitectures. The first was the unmodified, non-pretrained ViT-Base model. This model, serving as a benchmark, was a simplearchitecture comprised of a single convolutional layer that used8x36 kernels (i.e. patches) to generate patch embeddings. Alongsidethis plain version, we used a pretrained version of the ViT-Basemodel trained on the ImageNet dataset , using the weights from. This pre-training approach aimed to utilize any cross-imagefeatures the ViT learned from the vast ImageNet dataset that maybe applicable to EEG data.",
  "BASELINE METHODS4.1Naive Guessing": "In an attempt to compare our performance against a general naive,untrained model, we adopt the Naive Guessing baseline from EEGEyeNet,where the mean position from the training set is used as the con-stant prediction for every test data point. According to the originalEEGEyeNet paper, this yields a RMSE distance of 123.3 mm, a stan-dard we aim to improve upon.",
  "Deep Learning Methods": "In this study, we focus on a comparison of the proposed EEGViTwith state-of-the-art CNN-based architectures, as they are widelyrecognized for their effectiveness in handling EEG data and havedemonstrated state of the art performance on the EEGEyeNet bench-mark.Convolutional Neural Network (CNN) We implemented astandard one-dimensional CNN, applying max pooling in line withEEGEyeNet.PyramidalCNN The PyramidalCNN model leverages varyingtime granularity in the CNN architecture to capture temporal fea-tures at multiple scales .EEGNet EEGNet, an EEG-specific CNN architecture that em-ploys depthwise and separable convolutions, was designed for ver-satility across Brain-Computer Interface (BCI) paradigms .InceptionTime InceptionTime, a scalable Time Series Classifi-cation (TSC) model, employs an ensemble of deep CNNs engineeredfor TSC tasks .Xception Xception, an efficient CNN architecture employingdepthwise separable convolutions, was designed to capture spatialcorrelations while enabling faster training .",
  "EEGViT Pre-trained55.4 0.2": ": Comparison of Root Mean Squared Error (RMSE)loss in millimeters for different models on the Absolute Po-sition Task. Original error is in pixels, and we convert it intomillimeters by 2 pixels/mm for better interpretation. LowerRMSE values indicate better performance as they representcloser estimations to the actual values. The values representthe mean and standard deviation of 5 runs.",
  "RESULTS": "In this study, we set out to evaluate and compare the performanceof an array of models for an EEG regression task. Our approachincluded an assessment of traditional Machine Learning models,Deep Neural Networks, as well as variants of ViT models, both withand without pretraining and the Two-Step convolution block. Wegauged the models performance using the root mean square error(RMSE) on a dedicated test set. Our comprehensive findings, whichalso encompass the naive baseline, are outlined in .",
  "Pretrained Transformer Models": "The pretrained versions of both ViT-Base and EEGViT exceededthe performance of the previous models by significantly enhancingaccuracy. More specifically, EEGViT demonstrated an exceptionalperformance boost of 10% compared to the non-pretrained ViT,and an impressive advancement of about 21% over the precedingstate-of-the-art model, as shown in .Interestingly, even though ViT-Base underwent an identical pre-training process as EEGViT, it did not showcase the same level ofprowess. ViT-Bases loss on the test set was roughly 5% higher thanEEGViTs, which deviates from their comparative performancewhen trained from scratch. This disparity can likely be attributedto the positive impact of pretraining.When the model can draw from a substantial knowledge base,the advantages of enhanced representations become more notice-able. Consequently, the Two-Step convolution representations inour model likely lead to better leverage within the Attention mech-anism and Transformer architecture. However, this difference inperformance highlights the need for further research into optimiz-ing the training process and leveraging domain-specific knowledgein EEG signal processing.",
  "DISCUSSION": "Our EEGViT model significantly outperforms conventional regres-sion models, Convolutional Neural Networks (CNNs), and the base-line ViT-Base, establishing the utility of transfer learning and theenhancement potential of image data for EEG analysis.Despite incorporating convolutional layers, our EEGViT modelssuperior performance is primarily attributable to the Transformerblock. This is highlighted when compared with baseline modelsthat do not use a Transformer block, such as EEGNet and Xception,which also employ depthwise convolutions in their early layers,yet underperform relative to EEGViT.Our findings accentuate the significance of reduced inductivebias in transitioning from Convolutional Neural Networks (CNNs)to Transformer-based architectures for processing extensive datasets.This echoes recent studies within the Computer Vision field, sug-gesting that the inherent strong inductive bias of CNNs may impedetheir efficiency, particularly with large-scale data, thereby making Transformers a preferable alternative . Our investigationmirrors this, demonstrating that an unmodified Vision Transformer(ViT) exceeds CNN performance when trained exclusively on theEEGEyeNet dataset, a trend that is amplified with pretraining. Con-sequently, future research might explore strategies to mitigate theintrinsic inductive bias of prediction models, especially in data-richenvironments.However, there is a compelling need to make these models moreinterpretable. Future work should consider transforming our modelinto an interpretable version. For instance, visualization techniquescould be incorporated to understand the activation patterns andcorrelations in the data.Our EEGViT model holds promise for a wide range of EEGdatasets. The EEGEyeNet dataset, upon which this study is based,is an extensive compilation of synchronized EEG and eye-trackingdata. Given the very nature of the input data it learns, we stronglybelieve that the methodology presented in this paper can be appliedto other, similar EEG datasets.The practical value of our findings lies in the models abilityto supplement scarce EEG data with readily available image data,thereby reducing ethical and financial barriers to EEG data collec-tion and analysis. This approach could streamline research in thefield, facilitating robust, scalable EEG studies.Additionally, our study highlights the benefits of pre-trainedmodels in the EEG domain, traditionally hampered by data scarcityand complexity. The demonstrated superior performance of the pre-trained EEGViT model suggests that the prior knowledge learnedfrom vast image datasets can effectively help when handling EEGdata complexity. Future work may include exploring pretrainingtransformers for EEG data using other modalities of data. By pro-viding an open-source version of our code1, we encourage furtherexploration and enhancements to our approach.Nevertheless, we recognize the limitations of our study, par-ticularly the use of a relatively small Vision Transformer due toresource limitations. Future research with increased resources couldpotentially scale up the model by a factor of 10 or more, potentiallyleading to further performance gains. It is critical to balance modelcomplexity, computational resources, and performance gains as weprogress in this research direction.",
  "CONCLUSION": "In conclusion, the implementation of a Hybrid Vision Transformer(ViT) pretrained on ImageNet in an EEG regression task demon-strates the potential of Transformer-based transfer learning in neu-roscience. The ViT, initially created for visual tasks, notably out-performed state-of-the-art convolution-based models in EEG dataanalysis.These findings challenge the prevalent use of convolution in EEGneural networks and imply that cross-domain pretrained Trans-former models may serve as valuable priors for EEG tasks withminor tweaks. This insight, especially relevant where data collec-tion is difficult, suggests that repurposing available image data forEEG can enrich future research in computer science and neuro-science.",
  "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:Pre-training of deep bidirectional transformers for language understanding. arXivpreprint arXiv:1810.04805 (2018)": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, GeorgHeigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformersfor image recognition at scale. arXiv preprint arXiv:2010.11929 (2020). Guangyao Dou, Zheng Zhou, and Xiaodong Qu. 2022. Time Majority Voting, aPC-Based EEG Classifier for Non-expert Users. In HCI International 2022-LateBreaking Papers. Multimodality in Advanced Interaction Environments: 24th In-ternational Conference on Human-Computer Interaction, HCII 2022, Virtual Event,June 26July 1, 2022, Proceedings. Springer, 415428. Stphane dAscoli, Hugo Touvron, Matthew L Leavitt, Ari S Morcos, GiulioBiroli, and Levent Sagun. 2021. Convit: Improving vision transformers with softconvolutional inductive biases. In International Conference on Machine Learning.PMLR, 22862296. Hassan Ismail Fawaz, Benjamin Lucas, Germain Forestier, Charlotte Pelletier,Daniel F. Schmidt, Jonathan Weber, Geoffrey I. Webb, Lhassane Idoumghar, Pierre-Alain Muller, and Franois Petitjean. 2020. Inceptiontime: Finding alexnet fortime series classification. Data Mining and Knowledge Discovery 34, 6 (2020),19361962.",
  "Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang.2021. Transformer in transformer. Advances in Neural Information ProcessingSystems 34 (2021), 1590815919": "Nora Hollenstein, Jonathan Rotsztejn, Marius Troendle, Andreas Pedroni, CeZhang, and Nicolas Langer. 2018. ZuCo, a simultaneous EEG and eye-trackingresource for natural sentence reading. Scientific data 5, 1 (2018), 113. Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, WeijunWang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017. Mobilenets:Efficient convolutional neural networks for mobile vision applications. arXivpreprint arXiv:1704.04861 (2017).",
  "Sergey Ioffe and Christian Szegedy. 2015. Batch normalization: Accelerating deepnetwork training by reducing internal covariate shift. In International conferenceon machine learning. pmlr, 448456": "Rie Johnson and Tong Zhang. 2017. Deep pyramid convolutional neural networksfor text categorization. In Proceedings of the 55th Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Papers). 562570. Ard Kastrati, Martyna Beata Pomecka, Damin Pascual, Lukas Wolf, VictorGillioz, Roger Wattenhofer, and Nicolas Langer. 2021. EEGEyeNet: a simultane-ous electroencephalography and eye-tracking dataset and benchmark for eyemovement prediction. arXiv preprint arXiv:2111.05100 (2021).",
  "Yingzhou Lu, Huazheng Wang, and Wenqi Wei. 2023. Machine Learning forSynthetic Data Generation: a Review. arXiv preprint arXiv:2302.04062 (2023)": "Nathan Koome Murungi, Michael Vinh Pham, Xufeng Dai, and Xiaodong Qu.2023. Trends in Machine Learning and Electroencephalogram (EEG): A Reviewfor Undergraduate Researchers. arXiv preprint arXiv:2307.02819 (2023). Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, GregoryChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019.Pytorch: An imperative style, high-performance deep learning library. Advancesin neural information processing systems 32 (2019). Xiaodong Qu, Peiyan Liu, Zhaonan Li, and Timothy Hickey. 2020. Multi-classtime continuity voting for EEG classification. In Brain Function Assessment inLearning: Second International Conference, BFAL 2020, Heraklion, Crete, Greece,October 911, 2020, Proceedings 2. Springer, 2433. Xiaodong Qu, Saran Liukasemsarn, Jingxuan Tu, Amy Higgins, Timothy J Hickey,and Mei-Hua Hall. 2020. Identifying clinically and functionally distinct groupsamong healthy controls and first episode psychosis patients by clustering onEEG patterns. Frontiers in psychiatry 11 (2020), 541659. Xiaodong Qu, Qingtian Mei, Peiyan Liu, and Timothy Hickey. 2020. Using EEGto distinguish between writing and typing for the same cognitive task. In BrainFunction Assessment in Learning: Second International Conference, BFAL 2020,Heraklion, Crete, Greece, October 911, 2020, Proceedings 2. Springer, 6674. Yannick Roy, Hubert Banville, Isabela Albuquerque, Alexandre Gramfort, Tiago HFalk, and Jocelyn Faubert. 2019. Deep learning-based electroencephalographyanalysis: a systematic review. Journal of neural engineering 16, 5 (2019), 051001. Yang Tang, Shuang Song, Shengxi Gui, Weilun Chao, Chinmin Cheng, andRongjun Qin. 2023. Active and Low-Cost Hyperspectral Imaging for the SpectralAnalysis of a Low-Light Environment. Sensors 23, 3 (2023), 1437. Wei Tao, Chang Li, Rencheng Song, Juan Cheng, Yu Liu, Feng Wan, and XunChen. 2020. EEG-based emotion recognition via channel-wise attention and selfattention. IEEE Transactions on Affective Computing (2020).",
  "Michal Teplan. 2002. Fundamentals of EEG measurement. Measurement sciencereview 2, 2 (2002), 111": "Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, AlexandreSablayrolles, and Herv Jgou. 2021. Training data-efficient image transformers& distillation through attention. In International conference on machine learning.PMLR, 1034710357. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin. 2017. Attention is allyou need. Advances in neural information processing systems 30 (2017). Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, TongLu, Ping Luo, and Ling Shao. 2021. pyramid vision transformer: A versatile back-bone for dense prediction without convolutions. In Proceedings of the IEEE/CVFinternational conference on computer vision. 568578.",
  "Long Yi and Xiaodong Qu. 2022. Attention-Based CNN Capturing EEG Record-ings Average Voltage and Local Change. In International Conference on Human-Computer Interaction. Springer, 448459": "Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Fran-cis EH Tay, Jiashi Feng, and Shuicheng Yan. 2021. Tokens-to-token vit: Trainingvision transformers from scratch on imagenet. In Proceedings of the IEEE/CVFinternational conference on computer vision. 558567. Yanhong Zeng, Jianlong Fu, and Hongyang Chao. 2020. Learning joint spatial-temporal transformations for video inpainting. In Computer VisionECCV 2020:16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XVI16. Springer, 528543.",
  "Linear Regression Linear Regression, a fundamental statisticalmethod, was employed with its default parameters, providing abenchmark against more complex models": "Lasso Regression Lasso Regression uses shrinkage to limit thecomplexity of the model and prevent overfitting. We adopted thedefault parameters for the Absolute Position task, given the absenceof task-specific parameters. Elastic Net Elastic Net, a blend of Ridge Regression and LassoRegression, was implemented with an alpha of 1, L1 ratio of 0.6,tolerance of 1e-05, and a gamma of 0.01. These parameters werechosen to optimize the balance between Ridge and Lasso penalties.",
  "A.3Guide for reproducibility": "We make our code publicly available at our GitHub repositoryto facilitate further research. Details on the EEGEyeNet datasetand benchmark can be found in and their GitHub repository.All training data used for experimentation is publicly availableon OSF.io. Further information on the implementation and repro-ducibility of our work can be found in our GitHub repository."
}