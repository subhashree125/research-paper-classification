{
  "ABSTRACT": "In an era marked by the increasing adoption of Large LanguageModels (LLMs) for various tasks, there is a growing focus on ex-ploring LLMs capabilities in handling web data, particularly graphdata. Dynamic graphs, which capture temporal network evolutionpatterns, are ubiquitous in real-world web data. Evaluating LLMscompetence in understanding spatial-temporal information on dy-namic graphs is essential for their adoption in web applications,which remains unexplored in the literature. In this paper, we bridgethe gap via proposing to evaluate LLMs spatial-temporal under-standing abilities on dynamic graphs, to the best of our knowledge,for the first time. Specifically, we propose the LLM4DyG bench-mark, which includes nine specially designed tasks consideringthe capability evaluation of LLMs from both temporal and spatialdimensions. Then, we conduct extensive experiments to analyzethe impacts of different data generators, data statistics, promptingtechniques, and LLMs on the model performance. Finally, we pro-pose Disentangled Spatial-Temporal Thoughts (DST2) for LLMson dynamic graphs to enhance LLMs spatial-temporal understand-ing abilities. Our main observations are: 1) LLMs have preliminaryspatial-temporal understanding abilities on dynamic graphs, 2) Dy-namic graph tasks show increasing difficulties for LLMs as thegraph size and density increase, while not sensitive to the time spanand data generation mechanism, 3) the proposed DST2 promptingmethod can help to improve LLMs spatial-temporal understandingabilities on dynamic graphs for most tasks. The data and codes arepublicly available at Github.",
  "Dynamic Graph; Large Language Model; Spatial-Temporal; Bench-mark; Evaluation; Disentanglement": "ACM Reference Format:Zeyang Zhang, Xin Wang, Ziwei Zhang, Haoyang Li, Yijian Qin, and WenwuZhu. 2024. LLM4DyG: Can Large Language Models Solve Spatial-TemporalProblems on Dynamic Graphs?. In Proceedings of the 30th ACM SIGKDDConference on Knowledge Discovery and Data Mining (KDD 24), August2529, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.",
  "INTRODUCTION": "In an era marked by the increasing adoption of Large LanguageModels (LLMs) for various tasks beyond natural language process-ing, such as image recognition , healthcare diagnostics ,and autonomous agents , there has been a growing body ofresearch dedicated to exploring LLMs abilities to tackle the vasttroves of web data. One area of particular interest is the handlingof graph data, which ubiquitously exists on the Internet. The WorldWide Web itself can be seen as a colossal interconnected graphof webpages, hyperlinks, and content. For example, social mediaplatforms like Facebook, Twitter, and Instagram generate dynamicsocial graphs reflecting user interactions and connections.To leverage the in-context learning and commonsense knowl-edge of LLMs, several pioneer works have been dedicated to adopt-ing LLMs on static graphs. For instance, Wang et al. and Guoet al. propose benchmarks to evaluate LLMs proficiency incomprehending and reasoning about graph structures, with taskslike graph connectivity, topological sort, etc, demonstrating theLLMs abilities of in-context learning and reasoning to solve staticgraph problems. Ye et al. and Chen et al. propose to fine-tune the LLMs to solve graph tasks in natural language, showingthe strong potential of LLMs to leverage text information, gen-erate human-readable explanations, and integrate commonsenseknowledge to enhance the reasoning over structures.Dynamic graphs, in comparison with static graphs, possess awealth of temporal evolution information, which is more preva-lent on the internet. For instance, on platforms such as Twitter,users engage in continuous interactions with each other, and onWikipedia, knowledge graphs are kept updated over time. On theone hand, with the additional temporal dimension, it is possible forLLMs to interpret the ever-changing relationships and information",
  "KDD 24, August 2529, 2024, Barcelona, SpainZeyang Zhang et al": "updates on dynamic graphs, which are ignored in static graphs.On the other hand, there exist additional research challenges forcapturing the graph dynamics, and evaluating LLMs proficiencyin comprehending spatial-temporal information is critical for theapplications of LLMs on dynamic graphs. Such investigations holdthe potential to shed light on broader web applications such assequential recommendation, trend prediction, fraud detection, etc.To this end, in this paper, we propose to explore the followingresearch question:",
  "How to design the prompts for dynamic graphs and tasks, wherespatial-temporal information should be taken into considerationin natural language": "To address these issues, we further propose LLM4DyG, a com-prehensive benchmark for evaluating the spatial-temporal under-standing abilities of LLMs on dynamic graphs. Specifically, wedesign nine specially designed tasks (illustrated in ) thatconsider the capability evaluation from both temporal and spatialdimensions, and question LLMs when, what or whether the spatial-temporal patterns, ranging from temporal links, and chronologicalpaths to dynamic triadic closure, take place. To obtain a deeper anal-ysis of the impacts of spatial and temporal dimensions for LLMson dynamic graphs, we make comparisons on these tasks withthree data generators (including Erds-Rnyi model , stochasticblock model , and forest fire model ), various data statistics(including time span, graph size and density), four general prompt-ing techniques (including zero/one-shot prompting, zero/one-shotchain-of-thoughts prompting ), and five LLMs (including closed-source GPT-3.5 and open-source LLMs Vicuna-7B, Vicuna-13B ,Llama-2-13B , and CodeLlama-2-13B ). Inspired by the ob-servations and dynamic graph learning literature, we further designa dynamic graph prompting technique, i.e., Disentangled Spatial-Temporal Thoughts (DST2), to encourage LLMs to process spatialand temporal information sequentially. We observe the followingfindings from conducting extensive experiments with LLM4DyG : (1) LLMs have preliminary spatial-temporal understandingabilities on dynamic graphs. We find that LLMs significantlyoutperform the random baseline on the dynamic graph tasks,and the improvements range from +9.8% to +73% on average in, which shows that LLMs are able to recognize structuresand time, and to perform reasoning in dynamic graph tasks. (2) Dynamic graph tasks exhibit increasing difficulties forLLMs as the graph size and density grow, while not sen-sitive to the time span and data generation mechanism.Specifically, the performance of GPT-3.5 in the when link task drops from 48% to 27% when the density increases from 0.3to 0.7, while the performance varies slightly as the time spanchanges for most tasks in . We also find that in thewhen connect task the performance drops from 97.7% to 17.7%when the graph size increases from 5 to 20 in . (3) Our proposed DST2 prompting technique can help LLMsto improve spatial-temporal understanding abilities. Wefind that the results of the existing prompting techniques varya lot for different tasks in . Inspired by dynamic graphliterature, our proposed DST2 encourages LLMs to first con-sider time before nodes, thus improving the performance formost tasks, particularly, from 33.7% to 76.7% in the when linktask in .",
  "We propose to evaluate LLMs spatial-temporal understandingcapabilities on dynamic graphs for the first time, to the best ofour knowledge": "We propose the LLM4DyG benchmark to comprehensivelyevaluate LLMs on dynamic graphs. LLM4DyG consists of ninedynamic graph tasks in natural language with considerations ofboth temporal and spatial dimensions, ranging from temporallinks, and chronological paths to dynamic triadic closure andcovering questions regarding when, what or whether for LLMs. We conduct extensive experiments taking into account threedata generators, three graph parameters, four general prompts,and five different LLMs. Based on the experiments, we providefine-grained analyses and observations about the evaluation ofLLMs on dynamic graphs.",
  "RELATED WORK2.1LLMs for tasks with graph data": "Recently, there has been a surge of works about LLMs for solvingtasks with graph data . He et al. proposed anapproach that LLMs not only execute zero-shot predictions butalso generate coherent explanations for their decisions. These ex-planations are subsequently leveraged to enhance the features ofgraph nodes for node classification in text-attributed graphs. Chenet al. proposes to explore LLMs-as-Enhancers and LLMs-as-Predictors for solving graph-related tasks, where the former aug-ment the GNN with LLMs, and the latter directly adopts LLMs tomake predictions. Wang et al. introduced NLGraph, a bench-marking framework tailored for evaluating the performance ofLLMs on traditional graph-related tasks. Simultaneously, Guo et al. conducted a comprehensive empirical study focused on uti-lizing LLMs to tackle structural and semantic understanding taskswithin graph-based contexts. Recent contributions in this line ofresearch include InstructGLM , a method for fine-tuning LLMsinspired by LLaMA , designed specifically for node classificationtasks. Zhang and Jiang et al. have initiated the explorationof this frontier by interfacing LLMs with external tools and en-hancing their reasoning capabilities over structured data sourcessuch as knowledge graphs (KGs) and tables. Yao et al. explores",
  "LLM4DyG: Can Large Language Models Solve Spatial-Temporal Problems on Dynamic Graphs?KDD 24, August 2529, 2024, Barcelona, Spain": ": The overall model performance (ACC%) on the dynamic graph tasks. In the data column, N denotes the number ofnodes in the dynamic graph. In the model column, Random denotes the random baseline which uniformly outputs one of thepossible solutions, and denotes the performance improvement of GPT-3.5 over the random baseline.",
  "Temporal": "Check temporal path Question: Given an undirected dynamic graph with the edges [(1, 2, 1), (0, 1, 1), (3, 4, 4)]. Did nodes 0, 1, 2 form a chronological path?Answer: Yes Find temporal path Question: Given an undirected dynamic graph with the edges [(1, 2, 0), (2, 0, 1), (2, 3, 2)]. Find a chronological path starting from node 1. Answer: Sort edge by time Question: Given an undirected dynamic graph with the edges [(2, 0, 2), (3, 4, 4), (1, 2, 0), (0, 1, 1)]. Sort the edges by time from earliest to latest.Answer: [(1, 2, 0), (0, 1, 1), (2, 0, 2), (3, 4, 4)]. Spatial-Temporal What neighbors at time Question: Given an undirected dynamic graph with the edges [(1, 2, 1), (0, 1, 1), (3, 4, 4)]. What nodes are linked with node 1 at time 1?Answer: What neighbors in periods Question: Given an undirected dynamic graph with the edges [(1, 2, 0), (2, 0, 1), (2, 3, 2)]. What nodes are linked with node 2 at or after time 1? Answer: Check triadic closure Question: Given an undirected dynamic graph with the edges [(1, 2, 0), (0, 1, 1), (2, 0, 2), (3, 4, 4)]. Did node 0, 1 and 2 form a closed triad?Answer: Yes",
  "Triadic Closure": ": An overview of the tasks in the LLM4DyG Benchmark. The tasks are designed to consider both temporal and spatialdimensions, and question LLMs in natural language when, what or whether the spatial-temporal patterns take place. The spatial-temporal patterns range from temporal links, and chronological paths to dynamic triadic closure. The tasks are classified basedon the targets of the queries. An example prompt and graph illustration are provided for each task. leveraging LLMs for graph generation, which have potentials forvarious real-world tasks like drug discoveries. Li et al. utilizesLLMs to recognize and provide the possible latent causal structuresgiven the rich domain knowledge in the textual corpus. However,these works mainly focus on static graphs, ignoring the tempo-ral nature of graphs in real-world web applications. In this paper,we propose to explore LLMs spatial-temporal understanding ondynamic graphs, which remains unexplored in the literature.",
  "LLMs have been recently applied to other related tasks, includingtime-series forecasting, recommendation, etc. Yu et al. presentsa novel study on harnessing LLMs outstanding knowledge and": "reasoning abilities for explainable financial time series forecast-ing. Chang et al. leverages pre-trained LLMs to enhance time-series forecasting and has shown exceptional capabilities as both arobust representation learner and an effective few-shot learner. Sunet al. summarizes two strategies for completing time-series (TS)tasks using LLM: LLM-for-TS that designs and trains a fundamentallarge model for TS data and TS-for-LLM that enables the pre-trainedLLM to handle TS data. Feng et al. and evaluate the tempo-ral or sequential understanding abilities of LLMs in visual tasks. Lyuet al. investigates various prompting strategies for enhancingpersonalized recommendation performance with large languagemodels through input augmentation. However, these works do notconsider the role of structures, and in this paper, we mainly focuson exploring the spatial-temporal understanding abilities of LLMson dynamic graphs.",
  "Dynamic Graph Learning": "Dynamic graphs are pervasive in a multitude of real-world applica-tions, spanning areas such as event forecasting, recommendationsystems, etc . This prevalence has promptedsignificant research interest in the development and refinementof dynamic graph neural networks . These networksare designed to model intricate graph dynamics, which incorpo-rate evolving structures and features over time. A variety of ap-proaches have been proposed to address the challenges posed bydynamic graphs. Some research efforts have focused on employingGraph Neural Networks (GNNs) to aggregate neighborhood infor-mation for each individual snapshot of the graph. Subsequently,these methods use a sequence module to capture and model thetemporal information . In contrast, other studieshave proposed the use of time-encoding techniques. These methodsencode the temporal links into specific time-aware embeddings,and then utilize a GNN or memory module to processand handle the structural information embedded in the graph. Re-cently, there are some works focusing on studying dynamic graphsunder distribution shifts . However, these methods re-quire the model to be trained every time they encounter a newdynamic graph task, limiting their widespread usage in real-worldscenarios. In this paper, we explore the potential of LLMs on solvingdynamic graph tasks with in-context learning skills and evaluatetheir spatial-temporal understanding abilities on dynamic graphs.",
  "THE LLM4DYG BENCHMARK": "In this section, we introduce our proposed LLM4DyG benchmarkto evaluate whether LLMs are capable of understanding spatial-temporal information on the dynamic graph. Specifically, we firstadopt a random dynamic graph generator to generate the basedynamic graphs with controllable parameters like time span. Then,we design nine dynamic graph tasks to evaluate LLMs abilitiesconsidering both spatial and temporal dimensions. The overallpipeline is illustrated in . Based on this pipeline, we cancontrol the data generation, statistics, prompting methods, andLLMs for each task to conduct fine-grained analyses.",
  "Dynamic Graph Data Generators": "We first adopt a random dynamic graph data generator to controlthe statistics of the dynamic graph. In default, we adopt an Erds-Rnyi (ER) model to generate an undirected graph, and randomlyassign a time-stamp for each edge. Denote a graph G = (V, E) withthe node set V = {1, 2, . . . } and edge set E = {1,2, . . . ,}.We first generate the graph with the ER model G = (, ) where is the number of nodes in the graph, and is the probability ofedge occurrence between each node pair. In this way, controlsthe graph size, and controls the graph density. After obtainingthe graph G, we assign each edge with a random timestamp ({0, 1, . . . , 1}), where controls the time span. For a generateddynamic graph, each edge = (, ,) denotes that node andnode are linked at time . We also include other dynamic graphgenerators, stochastic block (SB) model, and forest fire (FF) model.For real-world datasets, we adopt a random sampler to sampleego-graphs from the real graphs for evaluation.",
  "Dynamic Graph Tasks": "To evaluate LLMs spatial-temporal understanding abilities, wedesign nine tasks considering both temporal and spatial dimensions.The tasks are classified based on the targets of the queries, e.g., thetemporal tasks make queries about the time, the spatial tasks makequeries about the nodes, while the solutions in spatial-temporaltasks are more complex and include the spatial-temporal patternsmixed together. We introduce the definition and generation of eachtask as follows. Temporal Task 1: when link. We ask when two nodes arelinked in this task. In a dynamic graph G = (V, E), two nodesand are linked at time if there exists a temporal edge (, ,)in the edge set E. We randomly select an edge from the edgeset as the query. Temporal Task 2: when connect. We ask when two nodesare connected in this task. In a dynamic graph G = (V, E),two nodes and are connected at time if there exists a path[(,1,), (1,2,), . . . , (, ,)] from node to node at time in the edge set E. We randomly select a pair of nodes that areconnected at some time as the query. Temporal Task 3: when triadic closure (tclosure). We askwhen the three given nodes first form a closed triad in this task.Dynamic triadic closure has been shown critical for dynamicgraph analyses . In a dynamic graph G = (V, E), two nodeswith a common neighbor are said to have a triadic closure, ifthey are linked since some time so that the three nodes havelinked with each other to form a triad. We randomly select aclosed triad as the query. Note that while these temporal tasks focus on making queriesabout time, they also require the model to understand structures sothat the model can recognize when some structural patterns exist,from links and paths to dynamic triads. Next, we introduce thespatial tasks that require the model to spot the specific time anddiscover the structures.",
  "ExemplarHere is an example: Question: Given anundirected dynamic graph with the edges[(0, 1, 0), (1, 2, 1), (0, 2, 2)]. When are node0 and node 2 first connected? Answer:1": "QuestionQuestion: Given an undirected dynamicgraph with the edges [(0, 9, 0), (1, 9, 0), (2,5, 0), (1, 2, 1), (2, 6, 1), (3, 7, 1), (4, 5, 2), (4,7, 2), (7, 8, 2), (0, 1, 3), (1, 6, 3), (5, 6, 3), (0,4, 4), (3, 4, 4), (3, 6, 4), (4, 6, 4), (4, 9, 4), (6,7, 4)]. When are node 2 and node 1 firstconnected?",
  "For the tasks neighbor at time andneighbor in periods, the base-line accuracy is1 (,) . For the tasks check tclosure andcheck": "tpath, the baseline accuracies are 1/2, as the answer is either noor yes. For the tasks find tpath and check tpath, the baselineaccuracies are calculated by enumerating possible solutions andcorrect solutions for each instance. Prompting methods. To investigate how different prompting tech-niques affect the models abilities, we compare various promptingmethods, including zero-shot prompting, few-shot prompting ,chain-of-thought prompting (COT) and few-shot promptingwith COT. We adopt one example for few-shot prompting, anduse one-shot prompting as the default prompting approach. Foreach problem instance, the prompt is constructed by sequentiallyconcatenating dynamic graph instruction, task instruction, answerinstruction, exemplar prompts, and question prompts. An exampleof the prompt construction is shown in . Models. We use GPT-3.5-turbo-instruct as the default LLM, andwe also include other LLMs like Vicuna-7B, Vicuna-13B, Llama-2-13B and CodeLlama-2-13B. For all models, we set temperature = 0for reproducibility. We adopt accuracy as the metric for all tasks.",
  "We first compare GPT-3.5 on each task with different graph sizes,where is set to 5, 10 and 20 respectively. From , we havethe following observations": "Observation 1. LLMs have preliminary spatial-temporalunderstanding abilities on dynamic graphs.As shown in , on average, GPT-3.5 has shown significantperformance improvement (from +9.8% to +73.0%) over the baselinefor all tasks, indicating that LLMs indeed understand the dynamicgraph as well as the question in the task, and are able to exploitspatial-temporal information to give correct answers instead ofguessing by outputting randomly generated answers. Overall, wecan find that LLMs have the ability to recognize time, structures,and spatial-temporal patterns. Observation 2. Most dynamic graph tasks exhibit increas-ing difficulty for LLMs as the graph size grows.As shown in , for most tasks, the performance of GPT-3.5drops as the graph size increases. For example, the performancedrops from 97.7% to 17.7% on when connect task, and 42.3% to 2.0%on neighbor in periods task. This phenomenon may be due to twofactors: 1) From the task perspective, the solution space is enlargedso that it is harder for any model to obtain the correct solution, e.g.,the accuracy of the random baseline also drops significantly on sortedge task. 2) From the model perspective, it is harder for the modelto retrieve the useful information inside the data since the inputspace is enlarged, e.g., on when connect task, the performancedrops drastically while the solution space remains the same. Thisobservation shows that it is worthy of exploring handling largerdynamic graph contexts with LLMs.We then compare GPT-3.5 on each task with different time span and density , where is set to 10, 20, and 30 respectively, and is set to 0.3, 0.5, and 0.7 respectively. From , we have thefollowing observations. Observation 3. For LLMs, the difficulties of dynamic graphtasks are not sensitive to the time span but sensitive to thegraph density.As shown in , for most tasks, the model performance isclose as the time span increases while the density remains thesame. If we keep the time span the same and increase the density, the model performance drops for most tasks. One exception isthe task find tpath where the model performance increases as thetwo factors increase. Another interesting finding from the heatmapis that LLMs are relatively more sensitive with the time span intemporal tasks while the density in spatial tasks, possibly due tothe different points of focus for these tasks. It can be also observedin spatial-temporal tasks, where the model performance mainlychanges along with the diagonal of the time span and density .To investigate how the performance of LLMs varies when thetask requires additional temporal information other than only struc-tural information, we make comparisons with different time span T = 10 T = 20 T = 30 when link when connect when tclosure T = 10 T = 20 T = 30 neighbor at time neighbor in periods check tclosure p = 0.3 p = 0.5 p = 0.7 T = 10 T = 20 T = 30 check tpath p = 0.3 p = 0.5 p = 0.7 find tpath p = 0.3 p = 0.5 p = 0.7 sort edge",
  ": Performance comparisons (ACC%) on the dynamicgraph tasks with different density and time span . (Bestviewed in color)": "Time span T ACC (%) N = 5N = 10N = 20 : Performance of GPT-3.5 on the neighbor at timetask as the time span increases with different networksizes . Note that when = 1, the data degenerates to a staticgraph, since there is only one timestamp on the graph.",
  "and graph size on the neighbor at time task. We have thefollowing observation": "Observation 4. Temporal information adds additional dif-ficulties to LLMs in comparisons with static graphs.As shown in , GPT-3.5 has a drastic performance dropwhen the time span increases from 1 to 2. The possible reason isthat the task is changed from static to dynamic, serving as a morechallenging setting, since the model has to capture the additionaltemporal information. Similar to the results from , themodel performance is not sensitive to the time span when the taskis already a dynamic graph problem.",
  "Results with different prompting methods": "We then make comparisons with different prompting methods, in-cluding zero-shot prompting, one-shot prompting, zero-shot chain-of-thoughts, and one-shot chain-of-thoughts. From Tab. 3, we havethe following observations. Observation 5. General advanced prompting techniquesdo not guarantee a performance boost in tackling spatial-temporal information.As shown in , some advanced prompting methods likezero-shot COT and one-shot COT achieve higher performance thanother prompting methods in the tasks when tclosure, find tpathand sort edge. Note that these tasks involve more complex dynamicgraph concepts or have to tackle a large time span, which showsthat the chain-of-thoughts method can, to some extent, activatethe models reasoning ability by thinking step by step on complextasks. However, no prompting methods consistently achieve thebest performance on all tasks, which calls for the need to designspecial advanced prompting methods to boost LLMs performancein handling spatial-temporal information on dynamic graphs. ACC % when link when connect when tclosure ACC % neighbor at time neighbor in periods check tclosure ACC % check tpath find tpath sort edge RandomVicuna-7B Vicuna-13BLlama-2-13B CodeLlama-2-13BGPT3.5",
  "We then make comparisons with different LLMs, including GPT-3.5,Llama-2-13B, Vicuna-7B, Vicuna-13B and CodeLlama-2-13B. From, we have the following observations": "Observation 6. LLMs abilities on dynamic graph tasks arerelated to the model scale.As shown in , smaller LLMs like vicuna-7B and Llama-2-13B have performance lower than GPT-3.5 for all tasks, and evenlower than random baseline for several tasks like when connectand check tclosure. Overall, for these tasks, larger LLMs havebetter performance.To further investigate whether the lower performance stemsfrom the incompetence of understanding instructions or perform-ing reasoning, we show the valid rate of the answers given bydifferent LLMs in several tasks. An answer is judged as valid if itmeets the requirement of the answer template and can be parsedby the evaluator program. From , we find that 1) smallermodels have significantly lower valid rates for some tasks, e.g.,21% of Llama-2-13B in the task neighbor at time, demonstratingtheir limitations in understanding human instructions for dynamicgraph tasks. 2) In some tasks, the smaller models have high validrates, while having significantly lower performance than GPT-3.5,showing their limitations in reasoning for dynamic graph tasks. Observation 7. Training on codes may help LLMs tackleon dynamic graph tasks.As shown in , compared with Llama-2-13B, CodeLlama-2-13B shows significantly better results in most tasks. In particular,CodeLlama-2-13B even outperforms GPT-3.5 in the task whenlink. Note that in comparison with Llama-2-13B, CodeLlama-2-13B is further pretrained on a large corpus of code data, whichshows the potential of improving the performance of LLMs ondynamic graph tasks by training with codes. One possible reasonis that the code data covers more implicit knowledge of structures",
  "Results with different data generators": "We make comparisons with various prompting methods and dy-namic graph generation models, including ErdsRnyi (ER) model,Stochastic Block (SB) model, and Forest Fire (FF) model, on thewhen link task. To keep the number of edges similar, we set theclass number as 2, the in-class probability as 0.4, the cross-classprobability as 0.2 for SB model, and the forward burning probabilityas 0.5 for FF model. Observation 8. General prompting methods have consis-tent performance with different dynamic graph generatorsin the same task.As shown in , the one-shot prompt method consistentlyachieves the best performance with different dynamic graph gener-ation models in the when link task. The results indicate that theevaluation of different prompting methods on dynamic graphs maynot be closely related to the dynamic graph generators.",
  "Exploring advanced dynamic graph prompts": "In this section, we aim to explore advanced dynamic graph prompt-ing techniques to improve the reasoning ability of LLMs on dynamicgraphs. The chain-of-thoughts prompting is shown as a generaladvanced prompting technique to activate LLMs complex reason-ing abilities, while it does not effectively improve performance ondynamic graphs as shown in .To have further developments, we draw inspiration from dy-namic graph learning literature where most works tackle spatial-temporal information separately, e.g., to tackle time first and thenstructures, or to tackle structures first and then time. Intuitively,this thought breaks down the complex spatial-temporal informationinto two separate dimensions so that the difficulty can be decreased.To this end, we propose Disentangled Spatial-Temporal Thoughts(DST2) to improve LLMs reasoning abilities on dynamic graphs,that is to instruct LLMs to sequentially think about the nodes ortime. Specifically, we design several prompts and add the promptsafter the task instruction in the one-shot prompt, which are denotedas v1 to v4 respectively in . Observation 9. The prompting of instructing LLMs to sep-arately tackle spatial and temporal information significantlyimproves the performance.As shown in , the prompt v4 achieves the accuracy of76.7% in the when link task, significantly surpassing the one-shotprompt (33.7%), showing that guiding the LLM to handle time beforenodes may help the model improve the spatio-temporal understand-ing ability on dynamic graphs. For spatial tasks, it seems that itwould be better for the LLM to think about spatial informationbefore temporal information ( e.g., the prompt v1 achieves 69.3%in the check tclosure task). While our proposed methods provideperformance gains in most tasks, there exist some tasks that arenot positively affected. Designing specific prompting methods forLLMs on dynamic graphs is still an open research question.",
  "CONCLUSION": "In this paper, we propose a novel LLM4DyG benchmark to evalu-ate LLMs spatial-temporal understanding capabilities on dynamicgraphs, which remains unexplored in literature. The proposedbenchmark encompasses nine specially devised tasks, which assessthe capabilities of LLMs to handle both temporal and spatial in-formation on dynamic graphs. The evaluation procedure involvesa diverse range of LLMs, prompting techniques, data generators,and data statistics. We also propose Disentanlged Spatio-TemporalThoughts (DST2) as an advanced prompting method to enhancereasoning capabilities by guiding LLMs to think about time andstructures separately. Through comprehensive experiments, weprovide nine fine-grained observations that would be helpful forunderstanding LLMs reasoning abilities on dynamic graphs. Wehope that future work can be developed based on our proposedbenchmark and observations. This work is supported by the National Key Research and Devel-opment Program of China No. 2023YFF1205001, National NaturalScience Foundation of China (No. 62222209, 62250008, 62102222,62206149), Beijing Key Lab of Networked Multimedia and BeijingNational Research Center for Information Science and Technologyunder Grant No. BNR2023RC01003, BNR2023TD03006. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, YanaHasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al.2022. Flamingo: a visual language model for few-shot learning. Advances inNeural Information Processing Systems 35 (2022), 2371623736.",
  "Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2013.Representationlearning: A review and new perspectives. IEEE transactions on pattern analysisand machine intelligence 35, 8 (2013), 17981828": "Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski,Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, PiotrNyczyk, et al. 2024. Graph of thoughts: Solving elaborate problems with largelanguage models. In AAAI, Vol. 38. 1768217690. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, ChrisHesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, JackClark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, andDario Amodei. 2020. Language Models are Few-Shot Learners. In Advances inNeural Information Processing Systems. 18771901. Lei Cai, Zhengzhang Chen, Chen Luo, Jiaping Gui, Jingchao Ni, Ding Li, andHaifeng Chen. 2021. Structural temporal graph neural networks for anomaly de-tection in dynamic graphs. In Proceedings of the 30th ACM international conferenceon Information & Knowledge Management. 37473756.",
  "Hong Chen, Yudong Chen, Xin Wang, Ruobing Xie, Rui Wang, Feng Xia, andWenwu Zhu. 2021. Curriculum Disentangled Recommendation with Noisy Multi-feedback. NeurIPS 34 (2021), 2692426936": "Hong Chen, Xin Wang, Yipeng Zhang, Yuwei Zhou, Zeyang Zhang, Siao Tang,and Wenwu Zhu. 2024. DisenStudio: Customized Multi-subject Text-to-VideoGeneration with Disentangled Spatial Control. arXiv:2405.12796 [cs.CV] Hong Chen, Yipeng Zhang, Simin Wu, Xin Wang, Xuguang Duan, Yuwei Zhou,and Wenwu Zhu. 2023. Disenbooth: Identity-preserving disentangled tuning forsubject-driven text-to-image generation. In The Twelfth International Conferenceon Learning Representations. Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and PieterAbbeel. 2016. Infogan: Interpretable representation learning by informationmaximizing generative adversarial nets. NeurIPS 29 (2016). Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei,Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, and Jiliang Tang. 2023. Ex-ploring the Potential of Large Language Models (LLMs) in Learning on Graphs.arXiv preprint arXiv:2307.03393 (2023). Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang,Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica,and Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with90%* ChatGPT Quality. Weilin Cong, Yanhong Wu, Yuandong Tian, Mengting Gu, Yinglong Xia, MehrdadMahdavi, and Chun-cheng Jason Chen. 2021. Dynamic Graph RepresentationLearning via Graph Transformer Networks. arXiv preprint (2021). Songgaojun Deng, Huzefa Rangwala, and Yue Ning. 2020. Dynamic knowledgegraph based multi-event forecasting. In Proceedings of the 26th ACM SIGKDDInternational Conference on Knowledge Discovery & Data Mining. 15851595. Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, andJie Tang. 2022. GLM: General Language Model Pretraining with AutoregressiveBlank Infilling. In Proceedings of the 60th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers). 320335.",
  "Paul W Holland, Kathryn Blackmond Laskey, and Samuel Leinhardt. 1983. Sto-chastic blockmodels: First steps. Social networks 5, 2 (1983), 109137": "Jun-Ting Hsieh, Bingbin Liu, De-An Huang, Li F Fei-Fei, and Juan Carlos Niebles.2018. Learning to decompose and disentangle representations for video prediction.Advances in neural information processing systems 31 (2018). Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, De-vendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,Guillaume Lample, Lucile Saulnier, et al. 2023.Mistral 7B.arXiv preprintarXiv:2310.06825 (2023). Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin Zhao, and Ji-RongWen. 2023. Structgpt: A general framework for large language model to reasonover structured data. arXiv preprint arXiv:2305.09645 (2023).",
  "Jure Leskovec, Jon Kleinberg, and Christos Faloutsos. 2007. Graph evolution:Densification and shrinking diameters. ACM transactions on Knowledge Discoveryfrom Data (TKDD) 1, 1 (2007), 2es": "Haoyang Li, Peng Cui, Chengxi Zang, Tianyang Zhang, Wenwu Zhu, and YishiLin. 2019. Fates of Microscopic Social Ecosystems: Keep Alive or Dead?. InProceedings of the 25th ACM SIGKDD International Conference on KnowledgeDiscovery & Data Mining. 668676. Haoyang Li, Xin Wang, Zeyang Zhang, Haibo Chen, Ziwei Zhang, and WenwuZhu. 2024. Disentangled Graph Self-supervised Learning for Out-of-DistributionGeneralization. In International conference on machine learning. PMLR. Haoyang Li, Xin Wang, Ziwei Zhang, Jianxin Ma, Peng Cui, and Wenwu Zhu. 2021.Intention-aware sequential recommendation with structured intent transition.IEEE Transactions on Knowledge and Data Engineering (2021).",
  "Yijian Qin, Ziwei Zhang, Xin Wang, Zeyang Zhang, and Wenwu Zhu. 2022. NAS-Bench-Graph: Benchmarking Graph Neural Architecture Search. In Advances inNeural Information Processing Systems": "Emanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, FedericoMonti, and Michael Bronstein. 2020. Temporal graph networks for deep learningon dynamic graphs. arXiv preprint arXiv:2006.10637 (2020). Baptiste Rozire, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xi-aoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jrmy Rapin, ArtyomKozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Fer-rer, Aaron Grattafiori, Wenhan Xiong, Alexandre Dfossez, Jade Copet, FaisalAzhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, andGabriel Synnaeve. 2023.Code Llama: Open Foundation Models for Code.arXiv:2308.12950 [cs.CL]",
  "networks. In Proceedings of the 13th International Conference on Web Search andData Mining. 519527": "Matthias Schfer, Martin Strohmeier, Vincent Lenders, Ivan Martinovic, andMatthias Wilhelm. 2014. Bringing up OpenSky: A large-scale ADS-B sensornetwork for research. In IPSN-14 Proceedings of the 13th International Symposiumon Information Processing in Sensor Networks. IEEE, 8394. Youngjoo Seo, Michal Defferrard, Pierre Vandergheynst, and Xavier Bresson.2018. Structured sequence modeling with graph convolutional recurrent net-works. In International Conference on Neural Information Processing. Jitesh Shetty and Jafar Adibi. 2004. The Enron email dataset database schema andbrief statistical report. Information sciences institute technical report, University ofSouthern California 4, 1 (2004), 120128.",
  "Jie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, and Zhong Su. 2008. Arnet-Miner: Extraction and Mining of Academic Social Networks. In KDD08. 990998": "Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, LauraGutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. 2023. Large language modelsin medicine. Nature medicine 29, 8 (2023), 19301940. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-AnneLachaux, Timothe Lacroix, Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, et al. 2023. Llama: Open and efficient foundation language models. arXivpreprint arXiv:2302.13971 (2023).",
  "Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, and Kannan Achan.2020. Inductive representation learning on temporal graphs. arXiv preprintarXiv:2002.07962 (2020)": "Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin,Chenxu Lv, Da Pan, Dian Wang, Dong Yan, et al. 2023. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305 (2023). Menglin Yang, Min Zhou, Marcus Kalander, Zengfeng Huang, and Irwin King.2021. Discrete-time Temporal Network Embedding via Implicit HierarchicalLearning in Hyperbolic Space. In Proceedings of the 27th ACM SIGKDD Conferenceon Knowledge Discovery & Data Mining. 19751985.",
  "Ziwei Zhang, Haoyang Li, Zeyang Zhang, Yijian Qin, Xin Wang, and WenwuZhu. 2023. Large Graph Models: A Perspective. Advances in Neural InformationProcessing Systems GLFrontiers Workshop (2023)": "Zeyang Zhang, Xingwang Li, Fei Teng, Ning Lin, Xueling Zhu, Xin Wang, andWenwu Zhu. 2023. Out-of-Distribution Generalized Dynamic Graph NeuralNetwork for Human Albumin Prediction. In IEEE International Conference onMedical Artificial Intelligence. Zeyang Zhang, Xin Wang, Yijian Qin, Hong Chen, Ziwei Zhang, Xu Chu, andWenwu Zhu. 2024. Disentangled Continual Graph Neural Architecture Searchwith Invariant Modularization. In International Conference on Machine Learning.",
  "Your task is to answer whether three nodes in thedynamic graph formed a closed triad. A closed triadis composed of three nodes which have linked witheach other some time.checktpath": "Your task is to answer whether a path is chronologi-cal in the dynamic graph. The time of the edges in achronological path from source node to target nodemust not decrease, e.g., is a chronologicalpath in the dynamic graph [(2, 3, 0), (3, 5, 1)]find tpathYour task is to find a chronological path in the dy-namic graph. The time of the edges in a chronolog-ical path from source node to target node must notdecrease, e.g., is a chronological path in thedynamic graph [(2, 3, 0), (3, 5, 1)]sort edgeYour task is to sort the edges in the dynamic graphby time from earlest to latest.",
  "NLGraph0.190.330.42GPT4Graph0.300.350.47Ours0.450.430.47": "2500 members of the OpenSky network since January 1st, 2020.The nodes represent airports and there is an edge between twonodes at time , if on day there is a flight between two airports.We compare the performance (ACC) of static baseline meth-ods NLGraph , GPT4Graph in the spatial task neighborsat time. As shown in , our method has a significant per-formance over the baselines on larger-scale real-world dynamicgraphs, which can be credited to our consideration of temporalinformation on dynamic graphs. Though we focus on evaluatingLLMs spatial-temporal understanding abilities on dynamic graphs,the proposed method out-performs the recent baselines for staticgraphs in real-world datasets.",
  "Original0.590.56Random indexes0.620.55People names0.680.62": "Comparisons of static baselines on spatial tasks. We compare thestatic baselines on spatial tasks. As shown in , ourmethod out-performs the recent static baselines, showing the effec-tiveness of our method for tackling problems on dynamic graphs. Links prediction tasks on real-world datasets. we add experimentsfor link prediction tasks to further compare the baselines forpredictive tasks. As shown in , our method out-performsstatic baselines NLGraph and GPT4Graph with a large margin,demonstrating the effectiveness of handling spatial-temporal infor-mation on dynamic graphs. Effects of time formatting types. We add experiments with dif-ferent types of timestamps, including original, UNIX timestamp,Date, using Codellama2-13B and two tasks. The results are shownin the table 11. The original timestamp is an integer ranging from 1to , where T is the time span. The UNIX timestamp ranges from2010-01-01 00:00:00 to 2020-01-01 00:00:00. The date is formatted as%Year%Month%Day, e.g., 20200101. The results show that usingUNIX timestamp or date for time formatting reduces the perfor-mance, which may be due to the increased complexity for LLMs toinfer the ordering between time. Effects of node formatting types. We add experiments with dif-ferent types of node names, including original, random indexes,people names, using Codellama2-13B and two tasks. The results areshown in the table 12. The original node formatting uses integersranging from 0 to to represent the nodes, where refers to thenumber of nodes, while the random indexes adopt integers rangingfrom 0 to 1e8. For the formatting of people names, several namesare adopted to represent nodes, e.g., Aiden, Priya, Dmitri. Oneinteresting phenomenon is that using names to represent nodesmay improve the performance, which may be due to that LLMsmay be more familiar with names than integers. The results areconsistent with previous literature.",
  "BIMPLEMENTATION DETAILS": "Task prompts. For each problem instance, the prompt is con-structed by a unified template including DyG instructions describ-ing a dynamic graph, task instructions describing the task, answerinstructions describing the answer template, examplars (in few-shotlearning) and questions. An example is illustrated in andwe provide the task instruction for each task in .",
  "CADDITIONAL RELATED WORKS": "Disentangled Representation Learning. Disentangled representa-tion learning seeks to identify and clarify the distinct latent fac-tors underlying observable data, with each factor represented asa unique vector . These factors are crucial for unravelingthe intrinsic processes shaping data formation and for generatingrobust representations for subsequent applications. This approachhas demonstrated its utility in various fields, including computervision , and graph representation learning . Pan explores and evaluate thein-context learning in large language models by disentangling theeffects of task recognition and task learning. Qin et al. inte-grates tailored disentangled graph neural network layers to capturecomplex structural relationships within text for better performanceand interpretability. In this paper, we focus on studying the spatial-temporal understanding abilities of LLMs on dynamic graphs, andexplore a disentangled prompt to improve performance via lettingthe model think of spatial and temporal dimensions separately. Chain-of-Thoughts. The concept of chain-of-thoughts (CoT) has garnered significant attention in recent years. CoT refers tothe process by which a model generates intermediate reasoningsteps that lead to the final answer, thereby mimicking human-likereasoning patterns, which has been shown to improve the perfor-mance of language models on complex tasks that require multi-stepreasoning. Subsequent research has built upon these findings, ex-ploring various techniques to optimize CoT prompting and extendits applicability to a broader range of tasks . Bestaet al. model the information generated by an LLM as an arbitrarygraph, enhancing thoughts using feedback loops. Zhang et al. design an automated process for generating CoT prompts. Sincethe enhanced CoT in our framework still have mixed results, onepossible future direction is design automated CoT with automatedgraph techniques . We also leave extending theframework to recently released LLMs in future works."
}