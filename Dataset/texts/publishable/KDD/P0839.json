{
  "Abstract": "Geographic health disparities pose a pressing global challenge, par-ticularly in underserved regions of low- and middle-income nations.Addressing this issue requires a collaborative approach to enhancehealthcare quality, leveraging support from medically more devel-oped areas. Federated learning emerges as a promising tool for thispurpose. However, the scarcity of medical data and limited compu-tation resources in underserved regions make collaborative trainingof powerful machine learning models challenging. Furthermore,there exists an asymmetrical reciprocity between underserved anddeveloped regions. To overcome these challenges, we propose anovel cross-silo federated learning framework, named FedHelp,aimed at alleviating geographic health disparities and fortifying thediagnostic capabilities of underserved regions. Specifically, FedHelpleverages foundational model knowledge via one-time API accessto guide the learning process of underserved small clients, address-ing the challenge of insufficient data. Additionally, we introduce anovel asymmetric dual knowledge distillation module to managethe issue of asymmetric reciprocity, facilitating the exchange of nec-essary knowledge between developed large clients and underservedsmall clients. We validate the effectiveness and utility of FedHelpthrough extensive experiments on both medical image classifica-tion and segmentation tasks. The experimental results demonstratesignificant performance improvement compared to state-of-the-artbaselines, particularly benefiting clients in underserved regions.",
  "*These authors contributed equally to this work": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 25, August 37, 2025, Toronto, ON, Canada 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-1245-6/25/08",
  "Federated Learning, Healthcare Disparity, Medical Diagnosis": "ACM Reference Format:Jiaqi Wang*, Ziyi Yin*, Quanzeng You, Lingjuan Lyu, and Fenglong Ma.2025. Asymmetrical Reciprocity-based Federated Learning for ResolvingDisparities in Medical Diagnosis. In Proceedings of the 31st ACM SIGKDDConference on Knowledge Discovery and Data Mining V.1 (KDD 25), August37, 2025, Toronto, ON, Canada. ACM, New York, NY, USA, 12 pages.",
  "Introduction": "Geographic health disparities pose a fundamental challenge to coun-tries worldwide . These disparities underscore the un-equal distribution of health resources, access to healthcare services,and health outcomes across different geographic regions. Particu-larly in low- and middle-income nations, rural areas often grapplewith significant challenges related to healthcare infrastructure, ac-cess to medical professionals, and essential health services .This lack of access can result in higher rates of preventable diseases,maternal and child mortality, and overall poorer health outcomesin these regions. Furthermore, factors such as limited funding, lackof education, and a shortage of expertise in rural areas hinder theirability to invest in cutting-edge technologies. Consequently, find-ing collaborative ways to enhance healthcare quality in underservedregions with the support of medically developed areas is an urgentand essential social issue.Federated learning, a technique widely employed in the medicaldomain, presents a potential solution to this challenge by enablingcollaborative training of robust machine learning models with-out centralizing healthcare data .However, many existing approaches necessitate clients to employidentical models, a requirement unsuitable for our context whereunderserved regions face economic constraints in procuring high-cost computational resources. In essence, these regions or clientscan only afford small-sized models. In contrast, medically devel-oped areas typically utilize large models, thereby resulting in thechallenge of heterogeneous models in federated learning. While nu-merous strategies have been proposed totackle the challenges posed by heterogeneous federated learning,these approaches still suffer from the following challenges: C1: Limited medical data in underserved regions. Several fac-tors, including inadequate access to healthcare facilities, a lack of",
  ": Client accuracy comparison between client-wiselocal training, FedAvg, and the proposed FedHelp on the Fed-ISIC19 dataset. The size of each client can be found in Sec-tion 4.2": "awareness regarding the importance of medical record-keeping, andinsufficient technology for maintaining electronic health records,contribute to the scarcity of medical data in underserved regions. Inour federated learning framework, each region is treated as a client.As depicted in , small clients typically exhibit significantlylower performance than larger ones in local training. While ap-plying the existing federated learning algorithm FedAvg doesimprove performance, the benefits are primarily observed in clientswith relatively small datasets (clients 4 and 5), excluding both thelargest and smallest ones. Consequently, it is crucial to explorenew learning strategies that can benefit ALL clients in our setting.Without such inclusive approaches, there is little incentive for med-ically developed large clients to contribute to the improvement ofunderserved small clients. C2: Asymmetrical reciprocity among clients. Traditional fed-erated learning methodologies treat all clients equally, aiming tocollaboratively train a shared global model or personalized clientmodels. However, this study deviates from the norm by focusing onharnessing the abundant resources of medically developed areas toenhance the diagnostic performance of underserved regions usingcompact models without the need to share their data. In essence,the small clients emerge as the primary beneficiaries. While largeclients can gain insights from small client information throughmodel aggregation, they still act as valuable resource providers.Thus, the collaboration in this scenario exhibits asymmetric reci-procity among clients, which aligns with our observations in Fig-ure 1. However, effectively modeling this asymmetric reciprocityposes a novel challenge in federated learning.To tackle these challenges simultaneously, we introduce agroundbreaking cross-silo federated learning framework calledFedHelp1, specifically tailored to combat geographic healthdisparities and bolster the diagnostic capabilities of underservedregions, as illustrated in . To tackle the first challenge(C1) encountered during the training of small clients, we advocateharnessing knowledge from foundational models via only one-time API access using public data rather than private medical datain .2. This acquired knowledge serves as a guiding lightin training small surrogate models in .3. Meanwhile, the",
  "The source code is available at": "second challenge (C2) arises when training large clients. To circum-vent escalating communication costs, we advocate distilling a proxymodel for each large client, mirroring the network structure of smallclients. This proxy model acts as an intermediary, facilitating theexchange of information between large and small clients. Specifi-cally, we have devised an innovative asymmetrical dual knowledgedistillation strategy to address the challenge of asymmetrical reci-procity in .4. Subsequently, the small surrogate and proxymodels are uploaded to the server for aggregation in .5.To the best of our knowledge, this is the first work to lever-age federated learning techniques to mitigate the global issue ofgeographic health disparities, thereby augmenting healthcare qual-ity in underserved regions. In particular, we introduce a uniqueframework, FedHelp, designed to address the challenge of smalldata through accessing expansive foundation models via API inan efficient way and tackle the distinctive hurdle of asymmetricalreciprocity via the proposed dual knowledge distillation strategy.We conduct comprehensive experiments encompassing multi-classand binary medical image classification tasks as well as 2D and 3Dsemantic segmentation tasks, comparing the results with state-of-the-art baselines. The experimental outcomes unequivocally affirmthe efficacy of the FedHelp framework.",
  "Related Work2.1Federated Learning": "Federated learning , aiming to collaboratively train a ma-chine learning model without sharing clients private data, has beenapplied to the medical domain . Comparedwith traditional federated learning frameworks , per-sonalized federated learning focuses on the performance of thelocal clients , achieving superior performance. How-ever, either traditional federated learning or personalized federatedlearning models require that all the clients share the identical modelstructure. Though several heterogeneous federated learning frame-works have been proposed to solve thisissue, they are not typically designed to address the challengesin the medical domain. While one recent medical-related workProxyFL proposes a decentralized federated learning methodwith each client managing a small model, it neglects the essentialdistinctions between private and proxy models. Moreover, this de-centralized approach leads to increased communication expenses.",
  "Dual Knowledge Distillation": "Knowledge distillation treats the large model as a teacher, whichpasses knowledge to a small student model to enhance its perfor-mance. The most relevant work is bidirectional or dual knowledgedistillation , enabling the teacher and student to learn knowl-edge from each other. In , the bidirectional distillation tech-nique is utilized to solve the top-k ranking research problem andmachine translation . Although a few studies apply bidi-rectional knowledge distillation in federated learning to conductthe tasks of distracted driving detection , medical relation ex-traction , and the IoT system , they all treat both teacher andstudent models equally yet ignore the importance of asymmetricalreciprocity.",
  ": Overview of the proposed FedHelp framework. CE/KL denotes the cross-entropy loss/KullbackLeibler divergence": "FedType is the most relevant work, addressing the asym-metrical reciprocity between small proxy models and large clientmodels. However, our model, FedHelp, differs from FedType inseveral key aspects. First, the approach to calculating loss values isfundamentally different. FedType relies on training two conformalmodels to estimate uncertainty setsone for the large client modeland another for the proxy model. This process introduces additionalcomputational overhead and requires tuning more hyperparame-ters. In contrast, FedHelp adopts a more straightforward strategyby directly leveraging logit ranks to select top classes, eliminatingthe need for extra conformal models. Second, FedHelp employs adistinct loss function specifically designed for small clients, as de-scribed in Eq. (3), to address challenges such as small data sizes andlow data quality. FedType, on the other hand, applies the same lossfunction uniformly across all clients, disregarding these specificchallenges. This fundamental difference highlights the adaptabilityof FedHelp to heterogeneous client conditions.",
  "The goal of this work is to enable the training of federated learningunder the settings of clients with different capacities. Let C ={1, , } be the small client set in underserved regions, where": "represents the number of small clients. Let C = {1, , }denote the large client set, where denotes the number of largeclients. Each client stores a training dataset D for a small clientor D for a large client. In our setting, the size of D is far greaterthan that of D . To increase the diagnostic ability of small clients,we propose a novel yet general framework FedHelp consisting ofthree key components: knowledge acquisition, small client training,large client training, and global model learning, in .The knowledge acquisition module aims to generate logits orprobability distributions from foundation models {F1, , F}for public data D, which are further used to guide the learning of clients. Note that the public data, including their image types andlabels, may differ from those stored on clients. For small clients,we design a new knowledge-guided surrogate training strategy tohandle the issue of data insufficiency. Since the large clients holdhigh-quality and plenty of training data, we propose a novel asym-metrical dual knowledge distillation technique to distill large clientmodels and lightweight proxy models. The lightweight models fromboth small and large clients will be uploaded to the server for globalmodel learning. Next, we use the medical image classification taskas an example and provide the details of each component.",
  "One-time Knowledge Acquisition fromFoundation Models": "It is well-known that foundation models usually outperformbasic deep learning models on many tasks due to their large ca-pacity. Unfortunately, more and more such models are packed asapplication programming interfaces (APIs) and not open-sourcedlike GPT-4. An ideal way to use these APIs is to directly upload asmall set of data to their cloud servers, which helps us to fine-tunecustomized models and return them to users. However, in our set-ting, medical data are extremely sensitive and cannot be sent tothird parties. Thus, it is challenging to obtain customized modelswithout sharing private medical data.To solve this challenge, we acquire knowledge from large foun-dation models with the help of public data D, where all clients canaccess them. Assume that all the clients can also access the APIs offoundation models {F1, , F} and request the probability distri-butions for the public data. The returned probability distributionswill be treated as knowledge for guiding the clients training.",
  "KDD 25, August 37, 2025, Toronto, ON, CanadaWang et al": "Noel CF Codella, David Gutman, M Emre Celebi, Brian Helba, Michael AMarchetti, Stephen W Dusza, Aadi Kalloo, Konstantinos Liopyris, Nabin Mishra,Harald Kittler, et al. 2018. Skin lesion analysis toward melanoma detection: Achallenge at the 2017 international symposium on biomedical imaging (isbi),hosted by the international skin imaging collaboration (isic). In 2018 IEEE 15thinternational symposium on biomedical imaging (ISBI 2018). IEEE, 168172. Nathan J Doogan, Megan E Roberts, Mary Ellen Wewers, Erin R Tanenbaum,Elizabeth A Mumford, and Frances A Stillman. 2018. Validation of a new con-tinuous geographic isolation scale: A tool for rural health disparities research.Social Science & Medicine 215 (2018), 123132. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, GeorgHeigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformersfor image recognition at scale. arXiv preprint arXiv:2010.11929 (2020). Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. 2020. Personalized fed-erated learning with theoretical guarantees: A model-agnostic meta-learningapproach. Advances in Neural Information Processing Systems 33 (2020), 35573568.",
  "Large Clients: Asymmetrical DualKnowledge Distillation": "Different from small clients, large clients have sufficient data andcomputation resources to train complex deep learning models. Thus,it is unnecessary to utilize public data as small clients do, avoidingintroducing noise during model training due to the different datadistributions. However, uploading and downloading these largemodels consume a large amount of communication. To make thewhole system more communication-efficient, we propose to distillsmall proxy models for these clients, which are further used inglobal model learning.Intuitively, the large capacity and strong predictive ability of thelarge model w allows it to distill a small, powerful proxy model wwith traditional knowledge distillation techniques. The large modelw can be treated as a teacher, and the proxy model w can be seenas a student. However, such a simple approach aims to learn effec-tive student models but ignores the importance of the proxy model. In fact, the proxy model w contains two kinds of information. Thefirst part is from the forward knowledge distillation, and the secondis from other clients via the global model aggregation, which canbe found in .5. In other words, w carries diverse criticalinformation aggregated from small clients C = {1, , } and other large clients C = {1, ,1,+1, , }.To tackle this issue, we design a novel asymmetrical dual knowl-edge distillation strategy, which enables the transfer of informationin a bidirectional way forward and backward. The forward direc-tion allows the information transfer from the large model w to the",
  "where is the number of data in D. Since the large model wis usually more powerful than the proxy one w, mandatorily dis-": "tilling knowledge from w to w in the backward direction willintroduce noise for the large model training. To address this prob-lem, we propose a ranking-based knowledge distillation to imitatethe behavior of the proxy model for the large model.Intuitively, if the behaviors of the two models are similar, theirprediction logits should also be similar. To avoid introducing extranoise by forcing the large models logits to be similar to those ofsmall ones, we propose to use the value rank of classes in the logitsto imitate behaviors. The ranks only exhibit the relative magnitudeof probabilities instead of real values, which can be treated as aloose constraint.Specifically, for a given data x, , the proxy model w can gener- ate a logit or a class probability distribution w (x, ). Since our goalis to transfer the diversity information from the proxy model to thelarge one and avoid introducing too much extra information, in ourproposed RKD, we only focus on the top-ranked classes in w (x, ).To make the large model imitate the behavior of the proxy model,we enforce to improve the ranks of these top classes in w (x, )using the following loss:",
  "exp(w (x, )[]),(6)": "where denotes the top-ranked class indexes, and representsthe remaining classes. Note that the only function of Eq. (5) is touse the top class ranks generated by the proxy model to guidethe improvement of the corresponding class probabilities learnedby the large model. This constraint relaxes the hard constraint ofthe traditional knowledge installation and enables the large modelto imitate the behaviors of the proxy model.FedHelp can also train the large model w (x, ) using the labeled",
  "The surrogate models {w1, , w } from small clients and the": "proxy models { w1, , w } from large clients will be uploaded tothe server to exchange parameter information. Since these modelshave identical network structures, we can use any existing modelaggregation approaches, such as FedAvg , to learn the globalmodel. The global model will be distributed to each client for theiterative update until FedHelp converges. Note that the proposedFedHelp is a general framework and can also be used for othertasks, such as the medical image segmentation task, and the detailscan be found in Appendix A. Besides, the whole training procedureof the proposed FedHelp can be found in Appendix B.",
  "large clients and ResNet20 for small clients and use the publicdata D as a part of model input": "Note that we do not list pFedHR as a baseline since it main-tains a personalized model for each client on the server. However,FedHelp and baselines do not have such a constraint. The details ofeach model and its implementation can be found in Appendix C. 4.1.2Implementation. For the medical image classification task,we employ two foundation models trained on CLIP , includingViT-L/14 and RN50x16 2. The public dataset D is CIFAR-100 , and the number of public data is 10,000. The proxy modelis ResNet20. We set = 0.1, = 0.2, = 1, and = 0.2.We use accuracy as the evaluation metric. We set the size of top-ranked classes in Eq. (5) as 3 for multi-class classification and 1for binary classification. With the early stopping mechanism, weset the maximum communication rounds to 100. Furthermore, ourproposed model offers flexibility in the choice of public datasets. Inour main results, we utilize CIFAR-100, a non-medical dataset. InSec. 4.4, we present results from the medical public dataset NCT-CRC-HE-100K.",
  "Melanoma Classification": "The Fed-ISIC193 dataset , consisting of 23,247 dermoscopyimages, is used to classify eight different types of melanoma. Usingthe data partition of FLamby , we divide the six clients into threelarge and three small ones, where the number of training/testingdata is 9,930/2,483, 3,163/791, 2,690/673, 655/164, 351/88, and 180/45,respectively. The cross-silo setting requires all clients to be involvedin training at each communication round. 4.2.1Performance Comparision. displays the experimentalfindings on the Fed-ISIC19 dataset. Notably, our proposed FedHelpoutperforms all baseline models, particularly showcasing remark-able improvement on small clients. For the smallest client, the ob-served percentage improvement is substantial, reaching up to 47.1%. 2We selected these two models to simulate foundation model APIs as they werepretrained on the CIFAR-100 dataset, which serves as the public data in the medicalimage classification experiment.3The links of the datasets used in the experiments can be found in Appendix D.",
  ": Averge client accuracy of two ablation studies onmelanoma classification": "Additionally, a consistent trend is observed across all approaches,indicating better performance on large clients compared to theirsmaller counterparts. This aligns with expectations, as larger clientsinherently possess more data and even employ larger models, asseen in the heterogeneous baselines.Although heterogeneous baselines leverage larger client modelsand incorporate additional public data, their performance remainscomparable to most homogeneous models. This observation under-scores that the aggregation approaches of heterogeneous modelsmight not be well-suited for our setting, potentially due to smallclients impeding the learning progress of large clients.In contrast to all other approaches, our proposed FedHelp notonly integrates foundation models to enhance the learning of smallclients but also introduces a novel asymmetric dual knowledgedistillation method to boost the learning of large clients. As a result,it achieves the highest performance. These results unequivocallydemonstrate the effectiveness of the proposed FedHelp. 4.2.2Abalation Study. We conduct two experiments to assess theeffectiveness of our model design.(1) Knowledge Enhancement with Foundation Models (Sec-tion 3.3). Given the integration of two foundation model APIs toenhance the learning of small clients, the first ablation study aimsto validate the utility of these APIs. Two baselines are employed forcomparison: The first baseline, denoted as FedHelp, indicates theabsence of any APIs, while the remaining components are identicalto FedHelp. FedHelp signifies the utilization of only one API,ViT, during the small client learning process. The results are shownin (a). Our observations show that both FedHelp andFedHelp outperform FedHelp, indicating that the incorporation offoundation models is beneficial for enhancing the training of smallclients. Furthermore, a positive correlation between the number ofAPIs and performance is evident when comparing FedHelp andFedHelp. Notably, even when removing all APIs (FedHelp), thedrop in performance is smaller than the increase observed in .This result confirms that the primary performance improvement",
  "Training large models with FedHelp511.67Training large models only using the CE loss434.00Training large models with the BKD loss479.67": "stems from the designed asymmetric dual knowledge distillation,rather than the use of foundation model APIs.(2) Asymmertic Dual Knowledge Distillation (.4).In our model design, we propose a novel asymmetric dual knowl-edge distillation approach to benefit the learning of both smalland large clients. We use three baselines in this ablation studyto validate the effectiveness of the proposed strategy, includingone-directional knowledge distillation methods (i.e., FedHelp (for-ward) and FedHelp (backward)) and a symmetric dual knowledgedistillation approach FedHelp. The results are shown in (b). These results suggest that the three distillation approaches arenot optimal for our setting. Solely employing forward knowledgedistillation (FedHelp ) effectively guides the learning of small mod-els, but the transfer of knowledge from small to large models islacking. Conversely, the outcome of FedHelp indicates a poten-tial lag in small model training compared to large model training,despite the inclusion of diverse information. Notably, even whenusing FedHelp, the foundation model APIs enhance our modelsperformance beyond the best baseline, as shown in . Incontrast, traditional dual knowledge distillation (FedHelp) outper-forms both FedHelp and FedHelp, emphasizing the importanceof exchanging knowledge between large and small clients. How-ever, it highlights the need for a well-designed approach to modelknowledge transfer from small to large clients. 4.2.3Resource Usage Analysis. Previous experiments have un-equivocally illustrated the efficacy of our proposed approach,FedHelp. Nevertheless, leveraging the logits from public data forsmall client training and integrating asymmetrical reciprocity learn-ing for large clients could further optimize resource utilization. Thisexperiment aims to quantitatively assess resource consumption byscrutinizing both computation and communication costs.(1) Computation Costs. Training time is a quantitative metricfor assessing the efficiency of different approaches. In this experi-ment, we calculate the average training time across three epochs forvarious methods, with the results presented in . Here, BKDdenotes the utilization of symmetric knowledge distillation duringmodel training. Analysis reveals that the training time for FedHelpsurpasses that of baseline methods for both large and small clients.Nevertheless, the incremental training time is moderate, represent-ing a 6.67% increase compared to the large client using the BKDloss and a 9.83% extension relative to the small client using the CEloss. Despite this, our approach demonstrates substantial perfor-mance enhancements, particularly for small clients, showcasing animprovement of up to 47.02%, as illustrated in . Importantly,",
  "Pneumonia Classification": "4.3.1Performance Comparision. We perform a binary classifica-tion evaluation for pneumonia using 5,863 chest x-ray images .The dataset is divided among two large clients and four smallclients, with the distribution of training and testing data as follows:3,134/374, 1,048/124, 422/49, 317/37, 213/24, and 109/12, respectively.The experimental results are presented in . Similar patternsto those observed in emerge, once again affirming the effec-tiveness of FedHelp. 4.3.2Hyperparameter Analysis. We conduct a key hyperparameterstudy of our proposed approach, focusing on for the surrogatemodel in Eq. (3) and for the large model in Eq. (7). We keep allother settings consistent with in the paper. The averageaccuracy is shown in . As the value of increases from0.2 to 0.6, the performance initially improves and then declines.This trend may be due to the fact that appropriate utilization ofpublic data enhances performance, but an over-reliance on it, at theexpense of local data, can have a negative impact. Regarding , theperformance declines slightly as it increases from 0.2 to 0.6. This could be because a larger allows for more backward knowledgetransfer from the surrogate model to the large model, which maybe less quality than the knowledge passed from the large models tothe surrogate models.",
  "Public Dataset Selection": "We also experimented to assess the sensitivity of public datasetselection. Initially, we fine-tuned the two foundation models onthe NCT-CRC-HE-100K dataset, utilizing the last 10,000 images asthe public data. The outcomes of the Fed-ISIC19 and pneumoniaclassification tasks are detailed in .Compared to other heterogeneous baselines utilizing the samemedical public data, our proposed model FedHelp demonstratessuperior performance on each client and yields higher averageresults with the medical public data. When compared with theresults using the CIFAR-100 dataset as the public data (refer to and ), the performance of FedHelp also experiencesa marginal boost. This enhancement is attributed to the medicalpublic data sharing more features similar to the local data, providingvaluable knowledge that enhances the training of the small localclient models. For baselines, the performance of FedMD and FedGHexhibits significant improvement when using public medical data.This enhancement is attributed to the similarity between the publicdata and private data, contributing to better consensus and therebyboosting local training.Consequently, our experimental results underscore that our pro-posed approach is not heavily reliant on choosing the public dataset.However, incorporating medical data does have a slight positiveimpact on performance, particularly when local clients are engagedin medical-related tasks.",
  "Datasets": "The 2D lung segmentation dataset contains 704 images, and we dis-tribute them to two large and one small clients with the followingnumber of training/test data: 285/31, 285/31, and 65/7, respectively.The 3D brain T1 magnetic resonance images (MRIs) dataset is ex-tracted from the Information extraction from Images (IXI) database.We still follow the data partition of Fed-IXI used by FLamby and treat two clients as large and the third one as small. The numberof training/testing images is 249/62, 145/36, and 59/15, respectively.The public dataset D used in this experiment is the dermoscopiclesion image dataset in the 2016 ISIC Challenge, consisting of 900binary mask images.",
  "Baselines and Implementations": "Except for the baselines used in , we also add a federatedlearning-based medical image segmentation approach FedSM as a homogeneous baseline. We use MedSAM as the foundationmodel API4 for segmentation tasks. We set = 0.1, = 0.2, = 1, = 0.2, 0 = 10, and = 5. We use accuracy as theevaluation metric. We set the size of top-ranked classes in Eq. (13)as 1 for the two segmentation tasks. The evaluation metrics arepixel accuracy and the Dice coefficient, following . Toensure a fair comparison, an early stop mechanism is employed for",
  "Performance Analysis": "presents the experimental results for two segmentationtasks. It is evident that the proposed FedHelp consistently outper-forms all baseline models. Notably, FedSM , designed specifi-cally for medical image segmentation tasks, demonstrates superiorperformance compared to our baselines. Similar to the medical clas-sification task findings, homogeneous models tend to outperformheterogeneous ones in other baselines. We also randomly selectone input image from each client and visualize the segmentationresults in . These results reaffirm the effectiveness andgeneralization ability of FedHelp.",
  "Ablation Study": "Similar to the medical classification, we also conduct ablation stud-ies to validate the utility of each proposed module. Since onlyone API, MedSAM , is used in this task, we keep FedHelponly to validate the influence of foundation models. FedHelp ,FedHelp, and FedHelp are kept to validate the proposed asym-metric dual knowledge distillation strategy. The observations of 2Dlung segmentation (shown in ) are similar as discussed in.2.2.",
  "Conclusion": "This paper addresses the challenge of geographic health dispari-ties in underserved regions with the aim of enhancing healthcarequality, by employing advanced federated learning techniques. Weintroduce a novel framework, named FedHelp, which harnesses the capabilities of foundation models to mitigate data insufficiencyissues in underserved regions. Additionally, we propose a novelasymmetric dual knowledge distillation strategy to address theasymmetrical reciprocity among clients. Our experiments encom-pass both medical image classification (binary and multi-class la-bels) and segmentation tasks (2D and 3D). The experimental re-sults confirm the effectiveness and utility of the proposed FedHelp,demonstrating its potential to ameliorate geographic health dispar-ities. We are confident that this work will not only yield substantialbenefits within the medical domain but will also deliver great valueto businesses.",
  "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge ina neural network. arXiv preprint arXiv:1503.02531 (2015)": "Wenke Huang, Mang Ye, and Bo Du. 2022. Learn from others and be yourself inheterogeneous federated learning. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition. 1014310153. Fatih Ilhan, Gong Su, and Ling Liu. 2023. ScaleFL: Resource-Adaptive FederatedLearning With Heterogeneous Clients. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition. 2453224541. Hirofumi Inaguma, Tatsuya Kawahara, and Shinji Watanabe. 2021. Source andTarget Bidirectional Knowledge Distillation for End-to-end Speech Translation. In2021 Conference of the North American Chapter of the Association for ComputationalLinguistics: Human Language Technologies, NAACL-HLT 2021. Association forComputational Linguistics (ACL), 18721881. Meirui Jiang, Holger R Roth, Wenqi Li, Dong Yang, Can Zhao, Vishwesh Nath,Daguang Xu, Qi Dou, and Ziyue Xu. 2023. Fair Federated Medical Image Seg-mentation via Client Contribution Estimation. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition. 1630216311.",
  "Qinbin Li, Bingsheng He, and Dawn Song. 2021. Model-contrastive federatedlearning. In Proceedings of the IEEE/CVF conference on computer vision and patternrecognition. 1071310722": "Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar,and Virginia Smith. 2020. Federated optimization in heterogeneous networks.Proceedings of Machine learning and systems 2 (2020), 429450. Quande Liu, Cheng Chen, Jing Qin, Qi Dou, and Pheng-Ann Heng. 2021. Feddg:Federated domain generalization on medical image segmentation via episodiclearning in continuous frequency space. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition. 10131023. Ruixuan Liu, Fangzhao Wu, Chuhan Wu, Yanlin Wang, Lingjuan Lyu, HongChen, and Xing Xie. 2022. No one left behind: Inclusive federated learning overheterogeneous devices. In Proceedings of the 28th ACM SIGKDD Conference onKnowledge Discovery and Data Mining. 33983406.",
  "Jun Ma, Yuting He, Feifei Li, Lin Han, Chenyu You, and Bo Wang. 2023. SegmentAnything in Medical Images. arXiv preprint arXiv:2304.12306 (2023)": "Xiaosong Ma, Jie Zhang, Song Guo, and Wenchao Xu. 2022. Layer-wised modelaggregation for personalized federated learning. In Proceedings of the IEEE/CVFconference on computer vision and pattern recognition. 1009210101. Neil J MacKinnon, Vanessa Emery, Jennifer Waller, Brittany Ange, Preshit Am-bade, Munira Gunja, and Emma Watson. 2023. Mapping health disparities in 11high-income nations. JAMA network open 6, 7 (2023), e2322310e2322310. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, andBlaise Aguera y Arcas. 2017. Communication-efficient learning of deep net-works from decentralized data. In Artificial intelligence and statistics. PMLR,12731282. Matias Mendieta, Taojiannan Yang, Pu Wang, Minwoo Lee, Zhengming Ding,and Chen Chen. 2022. Local learning matters: Rethinking data heterogeneity infederated learning. In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition. 83978406. Jean Ogier du Terrail, Samy-Safwan Ayed, Edwige Cyffers, Felix Grimberg,Chaoyang He, Regis Loeb, Paul Mangold, Tanguy Marchand, Othmane Mar-foq, Erum Mushtaq, et al. 2022. FLamby: Datasets and Benchmarks for Cross-Silo",
  "Federated Learning in Realistic Healthcare Settings. Advances in Neural Informa-tion Processing Systems 35 (2022), 53155334": "Peihan Qi, Xiaoyu Zhou, Yuanlei Ding, Zhengyu Zhang, Shilian Zheng, and ZanLi. 2022. Fedbkd: Heterogenous federated learning via bidirectional knowledgedistillation for modulation classification in iot-edge system. IEEE Journal ofSelected Topics in Signal Processing 17, 1 (2022), 189204. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,et al. 2021. Learning transferable visual models from natural language supervision.In International conference on machine learning. PMLR, 87488763. Sashank Reddi, Rama Kumar Pasumarthi, Aditya Menon, Ankit Singh Rawat, FelixYu, Seungyeon Kim, Andreas Veit, and Sanjiv Kumar. 2021. Rankdistil: Knowledgedistillation for ranking. In International Conference on Artificial Intelligence andStatistics. PMLR, 23682376. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolu-tional networks for biomedical image segmentation. In Medical Image Computingand Computer-Assisted InterventionMICCAI 2015: 18th International Conference,Munich, Germany, October 5-9, 2015, Proceedings, Part III 18. Springer, 234241.",
  "Ertong Shang, Hui Liu, Zhuo Yang, Junzhao Du, and Yiming Ge. 2023. FedBiKD:Federated Bidirectional Knowledge Distillation for Distracted Driving Detection.IEEE Internet of Things Journal (2023)": "Yiqing Shen, Yuyin Zhou, and Lequan Yu. 2022. Cd2-pfed: Cyclic distillation-guided channel decoupling for model personalization in federated learning. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.1004110050. Dianbo Sui, Yubo Chen, Jun Zhao, Yantao Jia, Yuantao Xie, and Weijian Sun.2020. Feded: Federated learning via ensemble distillation for medical relationextraction. In Proceedings of the 2020 conference on empirical methods in naturallanguage processing (EMNLP). 21182128.",
  "Canh T Dinh, Nguyen Tran, and Josh Nguyen. 2020. Personalized federatedlearning with moreau envelopes. Advances in Neural Information ProcessingSystems 33 (2020), 2139421405": "Minxue Tang, Xuefei Ning, Yitu Wang, Jingwei Sun, Yu Wang, Hai Li, and Yi-ran Chen. 2022. FedCor: Correlation-based active client selection strategy forheterogeneous federated learning. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition. 1010210111. Philipp Tschandl, Cliff Rosendahl, and Harald Kittler. 2018. The HAM10000dataset, a large collection of multi-source dermatoscopic images of commonpigmented skin lesions. Scientific data 5, 1 (2018), 19. Jiaqi Wang, Qi Li, Lingjuan Lyu, and Fenglong Ma. 2024. pFedClub: ControllableHeterogeneous Model Aggregation for Personalized Federated Learning. In TheThirty-eighth Annual Conference on Neural Information Processing Systems.",
  "Jiaqi Wang and Fenglong Ma. 2023. Federated learning for rare disease detection:a survey. Rare Disease and Orphan Drugs Journal 2 (2023), 22": "Jiaqi Wang, Cheng Qian, Suhan Cui, Lucas Glass, and Fenglong Ma. 2022. Towardsfederated covid-19 vaccine side effect prediction. In Joint European Conference onMachine Learning and Knowledge Discovery in Databases. Springer, 437452. Jiaqi Wang, Xiaochen Wang, Lingjuan Lyu, Jinghui Chen, and Fenglong Ma.2024. FEDMEKI: A Benchmark for Scaling Medical Foundation Models viaFederated Knowledge Injection. In The Thirty-eighth Annual Conference on NeuralInformation Processing Systems. Jiaqi Wang, Xingyi Yang, Suhan Cui, Liwei Che, Lingjuan Lyu, Dongkuan Xu, andFenglong Ma. 2023. Towards Personalized Federated Learning via HeterogeneousModel Reassembly. In Thirty-seventh Conference on Neural Information ProcessingSystems. Jiaqi Wang, Xingyi Yang, Suhan Cui, Liwei Che, Lingjuan Lyu, Dongkuan DKXu, and Fenglong Ma. 2024. Towards personalized federated learning via hetero-geneous model reassembly. Advances in Neural Information Processing Systems36 (2024). Jiaqi Wang, Chenxu Zhao, Lingjuan Lyu, Quanzeng You, Mengdi Huai, andFenglong Ma. 2024. Bridging Model Heterogeneity in Federated Learning viaUncertainty-based Asymmetrical Reciprocity Learning. In Proceedings of the 41stInternational Conference on Machine Learning (Proceedings of Machine LearningResearch, Vol. 235), Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, AdrianWeller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (Eds.). PMLR,5229052308. Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, andRonald M Summers. 2017. Chestx-ray8: Hospital-scale chest x-ray database andbenchmarks on weakly-supervised classification and localization of commonthorax diseases. In Proceedings of the IEEE conference on computer vision andpattern recognition. 20972106. Xiaochen Wang, Jiaqi Wang, Houping Xiao, Jinghui Chen, and Fenglong Ma.2024. FEDKIM: Adaptive Federated Knowledge Injection into Medical Foun-dation Models. In Proceedings of the 2024 Conference on Empirical Methods inNatural Language Processing, Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung",
  "Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA,81418154": "Yanping Wang, Jun Zhu, Chunhua He, Xiaohong Li, Lei Miao, and Juan Liang.2012. Geographical disparities of infant mortality in rural China. Archives ofDisease in Childhood-Fetal and Neonatal Edition 97, 4 (2012), F285F290. Jennifer Weisent, Barton Rohrbach, John R Dunn, and Agricola Odoi. 2012. So-cioeconomic determinants of geographic disparities in campylobacteriosis risk:a comparison of global and local modeling approaches. International journal ofhealth geographics 11 (2012), 116. An Xu, Wenqi Li, Pengfei Guo, Dong Yang, Holger R Roth, Ali Hatamizadeh, CanZhao, Daguang Xu, Heng Huang, and Ziyue Xu. 2022. Closing the generalizationgap of cross-silo federated medical image segmentation. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition. 2086620875.",
  "Huaao Zhang, Shigui Qiu, and Shilong Wu. 2021. Dual knowledge distillation forbidirectional neural machine translation. In 2021 International Joint Conferenceon Neural Networks (IJCNN). IEEE, 17": "Xu Zhang, Yinchuan Li, Wenpeng Li, Kaiyang Guo, and Yunfeng Shao. 2022. Per-sonalized federated learning via variational bayesian inference. In InternationalConference on Machine Learning. PMLR, 2629326310. Yao Zhou, Jun Wu, Haixun Wang, and Jingrui He. 2022. Adversarial robustnessthrough bias variance decomposition: A new perspective for federated learning. InProceedings of the 31st ACM International Conference on Information & KnowledgeManagement. 27532762.",
  "B. Algorithm Flow": "To provide a clear illustration of FedHelp, we outline the algorith-mic flow in Algorithm 1. Notably, (1) we consolidate medical imageclassification and segmentation within Algorithm 1; (2) the smallclient update and large client update can be executed in parallel;and (3) obtaining logits of public data by querying the foundationmodels can be performed before model training, as they remainconstant. Importantly, FedHelp optimizes communication costs byexclusively uploading and downloading small models, as indicatedin lines 23 and 26.",
  "We use the following heterogeneous baselines:": "FedMD employs transfer learning and knowledge dis-tillation, utilizing labeled public data on the server. Eachclient is required to train their local model using both publicand private datasets. Subsequently, the clients transmit theirclass scores from the public dataset to the server, which thencomputes a consensus and sends it back to the clients forupdating their models locally. FedGH allows clients to use individual feature extractorswhile sharing a uniform global header. Specifically, clientstrain their local models on personal data and send back boththe representations and labels for each category to the server,facilitating the update of the global header. Following this,clients substitute their personal headers with the updatedglobal header for making inferences. FCCL aims to tackle heterogeneity in federated learningby creating a cross-correlation matrix using unlabeled publicdata, aiding domain shift adaptation. It utilizes knowledgedistillation in local updates to combat catastrophic forgettingwhile maintaining privacy. Clients send logits to the serverfor aggregation and then use the consensus logits to steertheir local training processes. FedKEMF focuses on training diverse local models, facil-itating effective knowledge integration, and implementingresource-conscious models. Clients transmit their networkmodels to the server for a comprehensive knowledge distilla-tion process. Subsequently, the server generates personalizedmodels for each client and redistributes these for local up-dates."
}