{
  "ABSTRACT": "Transformers have gained popularity in time series forecasting fortheir ability to capture long-sequence interactions. However, theirmemory and compute-intensive requirements pose a critical bottle-neck for long-term forecasting, despite numerous advancements incompute-aware self-attention modules. To address this, we proposeTSMixer, a lightweight neural architecture exclusively composedof multi-layer perceptron (MLP) modules. TSMixer is designed formultivariate forecasting and representation learning on patchedtime series, providing an efficient alternative to Transformers. Ourmodel draws inspiration from the success of MLP-Mixer models incomputer vision. We demonstrate the challenges involved in adapt-ing Vision MLP-Mixer for time series and introduce empiricallyvalidated components to enhance accuracy. This includes a novel de-sign paradigm of attaching online reconciliation heads to the MLP-Mixer backbone, for explicitly modeling the time-series propertiessuch as hierarchy and channel-correlations. We also propose a Hy-brid channel modeling approach to effectively handle noisy channelinteractions and generalization across diverse datasets, a commonchallenge in existing patch channel-mixing methods. Additionally,a simple gated attention mechanism is introduced in the backboneto prioritize important features. By incorporating these lightweightcomponents, we significantly enhance the learning capability ofsimple MLP structures, outperforming complex Transformer mod-els with minimal computing usage. Moreover, TSMixers modulardesign enables compatibility with both supervised and maskedself-supervised learning methods, making it a promising buildingblock for time-series Foundation Models. TSMixer outperformsstate-of-the-art MLP and Transformer models in forecasting by aconsiderable margin of 8-60%. It also outperforms the latest strongbenchmarks of Patch-Transformer models (by 1-2%) with a signifi-cant reduction in memory and runtime (2-3X). The source code ofour model is officially released as PatchTSMixer in the HuggingFace[Model] [Examples].",
  "INTRODUCTION": "Multivariate time series forecasting is the task of predicting thefuture values of multiple (possibly) related time series at future timepoints given the historical values of those time series. It has wide-spread applications in weather forecasting, traffic prediction, indus-trial process controls, etc. This decade-long problem has been wellstudied in the past by several statistical and ML methods , .Recently, Transformer -based models are becoming popular forlong-term multivariate forecasting due to their powerful capabil-ity to capture long-sequence dependencies. Several Transformerarchitectures were suitably designed for this task in last few yearsincluding Informer , Autoformer , FEDformer , andPyraformer . However, the success of the Transformer in thesemantically rich NLP domain has not been well-transferred to thetime series domain. One of the possible reasons is that, thoughpositional embedding in Transformers preserves some orderinginformation, the nature of the permutation-invariant self-attentionmechanism inevitably results in temporal information loss. Thishypothesis has been empirically validated in , where an embar-rassingly simple linear (DLinear) model is able to outperform mostof the above-mentioned Transformer-based forecasting models.Furthermore, unlike words in a sentence, individual time pointsin a time series lack significant semantic information and can beeasily inferred from neighboring points. Consequently, a consider-able amount of modeling capacity is wasted on learning point-wisedetails. PatchTST has addressed this issue by dividing the inputtime series into patches and applying a transformer model, result-ing in superior performance compared to existing models. However,PatchTST employs a pure channel1 independence approach whichdoes not explicitly capture the cross-channel correlations. PatchTSThas demonstrated that channel independence can enhance perfor-mance compared to channel mixing, where channels are simplyconcatenated before being fed into the model. This simple mixingapproach can lead to noisy interactions between channels in theinitial layer of the Transformer, making it challenging to disentan-gle them at the output. CrossFormer , another recent effort onPatch Transformers with an improved channel mixing technique,also faces this issue (see Appendix ). Therefore, there is anecessity to explicitly model channel interactions to seize oppor-tunistic accuracy improvements while effectively reducing the high",
  "KDD 23, August 610, 2023, Long Beach, CA, USAVijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, & Jayant Kalagnanam": "volume of noisy interactions across channels. In addition, thoughPatchTST reduces the timing and memory overhead via patching,it still uses the multi-head self-attention under the hood, whichis computationally expensive even when it is applied at the patchlevel.Recently, a series of multi-layer perceptron (MLP) models underthe umbrella of MLP-Mixers\" have been proposed in the computervision domain . These models are lightweight and fast,and achieve comparable or superior performance to vision Trans-former models while completely eliminating the need for computingintense multi-head self-attention . Moreover, MLP-Mixer by itsdefault architecture does not disturb the temporal ordering of theinputs which makes it a natural fit for time-series problems andresolves the concerns raised in DLinear . At this point, we askthe following important question - Can MLP-Mixer models yieldsuperior performance for multivariate time series forecasting? If so,what are the required time series customizations?Towards this, we show that the adoption of MLP-Mixer for timeseries is not trivial, i.e., just applying the vanilla MLP-Mixer withsome input and output shape modification will not make it a power-ful model, and will have suboptimal performance compared to thestate-of-the-arts. Hence, we propose TSMixer, a novel MLP-Mixerarchitecture for accurate multivariate time series forecasting. LikePatchTST, TSMixer is also patching-based and follows a modulararchitecture of learning a common backbone to capture the tem-poral dynamics of the data as a patch representation, and differentheads are attached and finetuned based on various downstreamtasks (Ex. forecasting). Backbone is considered task-independentand can learn across multiple datasets with a masked reconstruc-tion loss while the heads are task and data-specific.",
  "Key highlights of TSMixer are as follows:": "(1) TSMixer is a patching-based, lightweight neural architecturedesigned solely with MLP modules that exploits variousinherent time-series characteristics for accurate multivariateforecasting and representation learning. (2) TSMixer proposes a novel design paradigm of attaching &tuning online reconciliation2 heads to the MLP-Mixer back-bone that significantly empowers the learning capability ofsimple MLP structures to outperform complex Transformermodels while using less computing resources. This studyis the first of its kind to highlight the benefits of infusingonline reconciliation approaches in the prediction head ofMixer style backbone architectures for time-series modeling. (3) Specifically, TSMixer proposes two novel online reconcili-ation heads to tune & improve the forecasts by leveragingthe following intrinsic time series properties: hierarchicalpatch-aggregation and cross-channel correlation. (4) For effective cross-channel modeling, TSMixer follows a novelHybrid\" channel modeling by augmenting a channel indepen-dent backbone with a cross-channel reconciliation head. Thishybrid architecture allows the backbone to generalize across 2This reconciliation is different from the standard reconciliation in hierarchicalforecasting . Here, reconciliation targets patch-aggregation and cross-channelcorrelation and it is done online during training. We call it online\" as it is learned aspart of the overall loss computation and not as a separate offline process. diverse datasets with different channels, while the reconcilia-tion head effectively learns the channel interactions specificto the task and data. This approach effectively handles noisyinteractions across channels, which is a challenge in existingpatch channel-mixing methods. (5) For effective long sequence modeling, TSMixer introduces asimple Gated Attention3 that guides the model to focus onthe important features. This, when augmented with Hierar-chical Patch Reconciliation Head and standard MLP-Mixeroperations enables effective modeling of long-sequence in-teraction and eliminates the need for complex multi-headself-attention blocks. (6) Finally, the modular design of TSMixer enables it to workwith both supervised and masked self-supervised learningmethodologies which makes it a potential building block fortime-series foundation models . We conduct a detailed empirical analysis on 7 popular public datasets,wherein. TSMixer outperforms all existing benchmarks with ex-tremely reduced training time and memory usage. Snapshot (relativeMSE improvements) of the primary benchmarks are as follows:",
  "RELATED WORK": "Transformer-based time series models: The success of Trans-formers in the NLP domain has inspired time series researchersto come up with TS Transformer-based models. Due to the quadratictime and memory complexity of Transformer architectures, onemajor difficulty in adopting NLP Transformers for time series taskslies in processing much longer input sequence lengths (). To ad-dress this, several compute-aware time series Transformer modelssuch as Informer , Autoformer , Pyraformer and FED-former have been proposed, which modifies the self-attentionmechanisms to achieve () or ( log()) complexity.Patch-based time series models: The above-mentioned worksmostly feed granular-level time series into the Transformer model,hence, they focus on learning attention over every time point. Incontrast, PatchTST and CrossFormer enable patching beforefeeding it to the Transformers for learning representation acrosspatches. PatchTST follows the channel independence approach andin contrast, CrossFormer follows the channel-mixing approach.MLP-Mixer in vision domain: Recently, multi-layer percep-trons (MLP) strength has been reinvented in the vision domain . The idea of is to transform the input image through aseries of permutation operations, which inherently enforces mix-ing of features within and across patches to capture short andlong-term dependencies without the need for self-attention blocks.MLP-Mixer attains similar performance as CNN and Transformer.To extend upon, in gMLP , authors propose to use MLP-Mixer",
  ": High-level model architecture": "with Spatial Gating Unit (SGU)3 and ResMLP proposes to useMLP-Mixer with residual connections.MLP based sequence / time series models:4 Similar effortshave recently been put in the time series domain. Zhang et. al. proposed LightTS that is built upon MLP and two sophisticateddownsampling strategies for improved forecasting. The DLinearmodel also employs a simple linear model after decompos-ing the time series into trend and seasonality components. DLin-ear questions the effectiveness of Transformers as it easily beatsTransformer-based SOTAs. Li et. al. proposed MLP4Rec, a pureMLP-based architecture for sequential recommendation task, tocapture correlations across time, channel, and feature dimensions.",
  "METHODOLOGY3.1Notations": "Throughout the paper, we use the following variable names: :a multivariate time series of length and channels or timeseries, : input sequence length, : forecast sequence length(a.k.a. horizon), : batch size, : number of patches, : patch length, : hidden feature dimension, : expansion feature dimension, :number of MLP-Mixer layers, M: learned DNN model, : numberof output forecast patches, : context length, : patch length forcross-channel forecast reconciliation head, : patch-aggregatedprediction, : patch-aggregated ground truth, rec: actual baseprediction, rec: base ground truth, : scale factor. To ensure betterunderstanding, we provide the shape of a tensor as its subscript inthe text, and in square brackets in the architectural diagrams. Wedenote a linear layer in the neural network by A() for compactness.The multivariate forecasting task is defined as predicting futurevalues of the time series given some history:",
  "Training methodologies": "TSMixer has two training methodologies: supervised and self super-vised. The supervised training follows the prediction workflowas shown in the right part of . First, the input history timeseries goes through a sequence of transformation (normalization,patching, and permutation). Then, it enters the TSMixer backbonewhich is responsible for the main learning process. The predic-tion head converts the output embeddings of the backbone to thebase forecasts, . The model can be trained to minimize the meansquared error (MSE) of the base forecasts: L, = 22.We introduce two extra online forecast reconciliation heads which,if activated, can tune the base forecasts and produce more accu-rate forecasts by leveraging cross-channel and patch-aggregationinformation. When any or both of these reconciliation heads areactivated, a customized MSE-based objective function is employedon the tuned forecasts. A detailed discussion is provided in Sec-tion 3.3.7.The self-supervised training is performed in two stages. First, themodel is pretrained (see the pretrain workflow in ) with aself-supervised objective. Then, the pretrained model is finetunedthrough the prediction workflow for a supervised downstreamtask. Self-supervised pretraining has been found to be useful for avariety of NLP , vision , and time series tasks . Similar toBERTs masked language modeling (MLM) in the NLP domain,we employ a masked time series modeling (MTSM) task. The MTSMtask randomly applies masks on a fraction of input patches, and themodel is trained to recover the masked patches from the unmaskedinput patches. Other input transformations in the pretrain work-flow are the same as in the prediction workflow. The MTSM taskminimizes the MSE reconstruction error on the masked patches.The modular design of TSMixer enables it for either supervisedor self-supervised training by only changing the model head (andkeeping backbone the same).",
  "Model components": "Here we discuss the modeling components that are introducedto a vanilla MLP-Mixer to have improved performance. The high-level architecture is shown in . For stochastic gradientdescent (SGD), each minibatch, , is populated from by amoving window technique. The forward pass of a minibatch alongwith its shape is shown in 3.3.1Instance normalization. The input time series segmentgoes through reversible instance normalization (RevIN) . RevINstandardizes the data distribution (i.e., removes mean and dividesby the standard deviation) to tackle data shifts in the time series. 3.3.2Patching. Every univariate time series is segregated intooverlapping / non-overlapping patches with a stride of . For self-supervised training flow, patches have to be strictly non-overlapping.The minibatch is reshaped into , where de-notes the patch length, and is the number of patches (hence, = ( )/ + 1). The patched data is then permuted to and fed to the TSMixer backbone model. Patching re-duces the number of input tokens to the model by a factor of , and",
  "hence, increases the model runtime performance significantly ascompared to standard point-wise Transformer approaches": "3.3.3TSMixer backbone. Three possible backbones are shownin a. The vanilla backbone (V-TSMixer) flattens the channeland patch dimensions ( ) on the input before passing to thenext layer. This approach is commonly followed in Vision MLPMixing techniques and hence acts as a vanilla baseline. We proposetwo new types of backbone: channel independent backbone (CI-TSMixer) and inter-channel backbone (IC-TSMixer). They differin their MLP mixer layer architectures. CI-TSMixer backbone isinspired from the PatchTST model, in which the MLP Mixerlayer is shared across channels which forces the model to sharelearnable weights across the channels. This results in a reductionof model parameters. Moreover, CI-TSMixer enables us to employTSMixer for self-supervised modeling over multiple datasets eachhaving a different number of channels, which V-TSMixer cannot.In IC-TSMixer, an extra inter-channel mixer module is activated inthe backbone to explicitly capture inter-channel dependencies. Allthese backbones will be compared with extensive experimentation.Note that all the TSMixer backbones start with a linear patchembedding layer (see a). It transforms every patch inde- pendently into an embedding: = A. The weightand bias of A() are shared across channels for CI-TSMixer and IC-TSMixer backbones, but not for V-TSMixer. Since channels arecompletely flattened in V-TSMixer, A() in V-TSMixer does nothave any notion of multiple channels (i.e. c = 1). 3.3.4MLP Mixer layers. TSMixer backbone stacks a set of mixerlayers like encoder stacking in Transformers. Intuitively, each mixerlayer (b) tries to learn correlations across three differentdirections: (i) between different patches, (ii) between the hiddenfeature inside a patch, and (iii) between different channels. The for-mer two mixing methods are adopted from the vision MLP-Mixer ,while the last one is proposed particularly for multivariate time series data. The inter patch mixer module employs a shared MLP(weight dimension = ) to learn correlation between differentpatches. The intra patch mixer blocks shared MLP layer mixesthe dimensions of the hidden features, and hence the weight matrixhas a dimension of . The proposed inter channel mixer(weight matrix size = ) mixes the input channel dimensions,and tries to capture correlations between multiple channels in amultivariate context. This inter-channel mixer has been proposed inMLP4Rec for the event prediction problem and we investigateits applicability for the time-series domain. Please note that inter-channel mixer block is included only in the IC-TSMixer backboneand not with the CI-TSMixer and V-TSMixer backbones (b).The input and output of the mixer layers and mixer blocks aredenoted by . Based on the dimension under focus ineach mixer block, the input gets reshaped accordingly to learn cor-relation along the focussed dimension. The reshape gets revertedin the end to retain the original input shape across the blocks andlayers. All of the three mixer modules are equipped with an MLPblock (Appendix Figure. 6a), layer normalization , residual con-nection , and gated attention. The former three components arestandard in MLP-Mixer while the gated attention block is describedbelow. 3.3.5Gated attention (GA) block. Time series data often hasa lot of unimportant features that confuse the model. In orderto effectively filter out these features, we add a simple Gated At-tention after the MLP block in each mixer component. GAacts like a simple gating function that probabilistically upscalesthe dominant features and downscales the unimportant featuresbased on its feature values. The attention weights are derived by: = softmaxA. The output of the gated atten-tion module is obtained by performing a dot product between theattention weights and the hidden tensor coming out of the mixermodules: = (Appendix Figure. 6b). Augmenting GAwith standard mixer operations effectively guides the model to focus",
  "on the important features leading to improved long-term interac-tion modeling, without requiring the need for complex multi-headself-attention": "3.3.6Model heads. Based on the training methodology (i.e. su-pervised or self-supervised), we either add prediction or pretrainhead to the backbone. Both heads employ a simple Linear layerwith dropout after flattening the hidden features across all patches(Appendix Figure. 7). By default, heads share the same weightsacross channels. The output of the prediction head is the predictedmultivariate time series ( ), while the pretrain head emits amultivariate series of the same dimension as the input ( ). 3.3.7Forecast online reconciliation. Here we propose twonovel methods (in the prediction workflow, see ) to tunethe original forecasts, , based on two important characteristicsof time series data: inherent temporal hierarchical structure andcross-channel dependency. Any or both of them can be activatedin our TSMixer model to obtain reconciled forecasts.Cross-channel forecast reconciliation head: In many scenar-ios, the forecast for a channel at a certain time might be dependenton the forecast of another channel at a different point in time onthe horizon. For example, in retail domain, sales at a future timepoint might be dependent on the discount patterns around that time.Hence, we introduce a cross-channel forecast reconciliation headwhich derives an objective that attempts to learn cross-dependencyacross channels within a local surrounding context in the forecasthorizon. demonstrates its architecture.First, every forecast point is converted into a patch (of length )by appending its pre and post-surrounding forecasts based on thecontext length (). Then, each patch is flattened across channelsand passed through a gated attention and linear layer to obtain arevised forecast point for that patch. Thus, all channels of a forecastpoint reconcile its values based on the forecast channel values in thesurrounding context leading to effective cross-channel modeling.Residual connections ensure that reconciliation does not lead toaccuracy drops in scenarios when the channel correlations are very",
  ": Online hierarchical patch reconciliation head": "noisy. Since the revised forecasts have the same dimension as theoriginal forecasts, no change to the loss function is required. Fromexperiments, we observe that the hybrid\" approach of having achannel-independent backbone augmented with a cross-channelreconciliation head provides stable improvements as compared toother channel-mixing approaches. Also, this architecture helps inbetter backbone generalization as it can be trained with multipledatasets with a varying number of channels while offloading thechannel-correlation modeling to the prediction head (which is taskand data-dependent).Online hierarchical patch reconciliation head:5. Time se-ries data often possess an inherent hierarchical structure, eitherexplicitly known (e.g., hierarchical forecasting datasets ), or asan implicit characteristic (e.g., an aggregation of weather forecastsfor seven days denotes an estimate of weekly forecast, an aggrega-tion of sales forecast over all stores in a state denotes state-levelsales, and so on). In general, aggregated time series have betterpredictability and a good forecaster aims at achieving low forecasterror in all levels of the hierarchy (). Here, we propose anovel method to automatically derive a hierarchical patch aggrega-tion loss (online during training) that is minimized along with thegranular-level forecast error. shows the architecture. Theoriginal forecast is segregated into number of patches, eachof length . We denote this as . Now, is also passed through alinear layer to predict the hierarchically aggregated forecasts at thepatch-level: = A . Then, we concatenate and at the patch level and pass it through another linear trans-formation to obtain reconciled granular-level forecast: rec. Thus,the granular-level forecasts get reconciled at a patch level basedon the patch-aggregated forecasts leading to improved granular-level forecasts. Residual connections ensure that the reconciliation 5This reconciliation is different from the reconciliation in hierarchical forecast-ing . Here, hierarchy is defined as an aggregation over a patch, and the reconcilia-tion is done online during training.",
  "LBUrec, (2)": "where, is ground-truth future time series, is the aggregatedground-truth at patch-level, BU refers to bottom-up aggregation ofthe granular-level forecasts to obtain the aggregated patch-levelforecasts , and is scale factor. For MSE loss, = ()2.More intuitively, this loss tries to tune the base forecasts in a waysuch that they are not only accurate at the granular-level, but alsoaccurate at the aggregated patch-level. Note that a pre-defineddataset-specific hierarchical structure can be enforced here, but itis left for future studies.",
  "EXPERIMENTS4.1Experimental settings": "4.1.1Datasets. We evaluate the performance of the proposedTSMixer model on 7 popular multivariate datasets as depicted in. These datasets have been extensively used in the litera-ture for benchmarking multivariate forecasting modelsand are publically available in . We follow the same data loadingparameters (Ex. train/val/test split ratio) as followed in . 4.1.2Model Variants. A TSMixer variant is represented usingthe naming convention -TSMixer(). can either be Vanilla (V), or Channel Independent(CI), or Inter Channel (IC). can be a combinationof Gated Attention (G), Hierarchical Patch Reconciliation head (H),and/or Cross-channel Reconciliation head (CC). Common variantsare:",
  "Other model variants can be formed using the same naming con-vention. Unless stated explicitly, the cross-channel reconciliationhead uses the default context length of 1": "4.1.3Data & Model Configuration. By default, the followingdata and model configuration is used: Input Sequence length =512, Patch length = 16, Stride = 8, Batch size = 8, Forecastsequence length {96, 192, 336, 720}, Number of Mixer layers = 8, feature scaler = 2, Hidden feature size = (32),Expansion feature size = (64) and Dropout = 0.1.Training is performed in a distributed fashion with 8 GPUs, 10CPUs and 1000 GB of memory. For ETT datasets, we use a lowerhardware and model configuration with high dropout to avoid over-fitting, as the dataset is relatively small ( = 3, = 0.7, = 1). Supervised training is performed with 100 epochs. In self-supervised training, we first pretrain the backbone with 100 epochs.After that, in the finetuning phase, we freeze the backbone weightsfor the first 20 epochs to train/bootstrap the head (also known aslinear probing), and then, we finetune the entire network (backbone+ head) for the next 100 epochs. We choose the final model basedon the best validation score. Since overlapping patches have tobe avoided in self-supervised methodology, we use reduced patchlength and stride with the same size there (i.e. = 8, = 8). Thisfurther updates the hidden feature and expansion feature size by (i.e. = 16, = 32 ) for self-supervised methodology. Everyexperiment is executed with 5 random seeds (from 42-46) and themean scores are reported. Standard deviation is also reported forthe primary results. We use mean squared error (MSE) and meanabsolute error (MAE) as the standard error metrics. 4.1.4SOTA Benchmarks. We categorize SOTA forecasting bench-marks into the following categories: (i) Standard Transform-ers: FEDformer , Autoformer and Informer , (ii) PatchTransformers: PatchTST and CrossFormer , (iii) MLPsand Non-Transformers: DLinear , LightTS and S4 ,(iv) Self-supervised models: BTSF , TNC , TS-TCC ,CPC , and TS2Vec .",
  "In this section, we compare the accuracy and computational im-provements of TSMixer with the popular benchmarks in supervisedmultivariate forecasting": "4.2.1Accuracy Improvements. In , we compare the ac-curacy of TSMixer Best variant (i.e. CI-TSMixer-Best) with SOTAbenchmarks. Since we observe similar relative patterns in MSEand MAE, we explain all the results in this paper using the MSEmetric. TSMixer outperforms standard Transformer and MLP bench-marks by a significant margin (DLinear: 8%, FEDformer: 23%, Auto-former: 30% and Informer: 64%). PatchTST (refers to PatchTST/64in ) is one of the strongest baselines, and TSMixer marginallyoutperforms it by 1%. However, TSMixer achieves this improve-ment over PatchTST with a significant performance improvementw.r.t. training time and memory (Section. 4.2.2). For exhaustive re-sults with the individual best TSMixer variants, refer to Appendix. Also, Appendix A.4.1 highlights the superior performanceof TSMixer with other secondary benchmarks (LightTS, S4 andCrossFormer).",
  "CI-TSMixer-Best % improvement (MSE)8%1%23%30%64%": ": Comparing TSMixer with popular benchmarks in supervised long-term multivariate forecasting. The best results are inbold and the second best is underlined. PatchTST results are reported from . All other benchmarks are reported from (stride) . In contrast, TSMixer not only enables patching but alsocompletely eliminates the self-attention blocks. Hence, TSMixershows significant computation improvement over PatchTST andother Transformer models. In , we highlight the speed-upand memory comparison of TSMixer over PatchTST. For analy-sis, we capture the following metrics: (i) Multiply-Add cumulativeoperations on the entire data per epoch (MACs), (ii) Number ofmodel parameters (NPARAMS), (iii) Single EPOCH TIME and (iv)Peak GPU memory reached during a training run (MAX MEMORY).For this experiment, we trained TSMixer and PatchTST modelsin a single GPU node with the same hardware configuration in anon-distributed manner to report the results. To ensure fairnessin comparison, we use the exact model parameters of PatchTSTand TSMixer which were used in the error metric comparison re-ported in . In , we capture the average improvementof TSMixer over PatchTST for each performance metric acrossthe three larger datasets (Electricity, Weather, Traffic) with =96.Since CI-TSMixer is purely MLP based, it significantly reduces theaverage MACs (by 4X), NPARAMS & MAX MEMORY (by 3X),and EPOCH TIME 6 (by 2X). Even after enabling gated attention 6MACs and EPOCH TIME are highly correlated and in general create a similarrelative impact across models. However, since PatchTST involves high parallelismacross the attention heads, we observe a different relative impact w.r.t. MACs andEPOCH TIME. and hierarchy reconciliation, CI-TSMixer(G,H) still shows a good re-duction in MACs & NPARAMS (by 3X), and EPOCH TIME & MAXMEMORY (by 2X). It is important to note that, when cross-channelreconciliation is enabled [i.e CI-TSMixer(G,H,CC)], the number ofparameters becomes very high in TSMixer as compared to PatchTST.The reason is that the number of parameters in the cross-channelreconciliation head is dependent on the number of channels in thedataset that leads to this scaling effect. For example, since Electric-ity and Traffic datasets have a very high number of channels (i.e.321 and 862 respectively), the number of parameters of the modelalso scales up, whereas weather (with only 21 channels) did not en-counter any such scaling effect. Even PatchTST should show a simi-lar scaling effect if the channel correlation is enabled in it. However,even with increased parameters, CI-TSMixer(G,H,CC) still showsa notable reduction in MACs (by 3X) and EPOCH TIME & MAXMEMORY (by 2X). The reason is that parameter scaling affectsonly the reconciliation head and not the backbone which primarilyconstitutes the total training time and memory. Thus, TSMixer andits variants can easily produce improved results over PatchTST insignificantly less training time and memory utilization.",
  ": Effect of CI, Gated Attention and Hierarchy Recon-ciliation over Vanilla TSMixer (MSE)": "4.3.1Effect of CI, Gated Attention & Hierarchy Patch Rec-onciliation. depicts the improvements of various enhance-ment components in TSMixer over V-TSMixer in three datasets:ETTH1, ETTM1, and Weather (for space constraint). V-TSMixer rep-resents the vanilla model where all channels are flattened and pro-cessed together (similar to vision MLP-Mixer ). CI-TSMixer outper-forms V-TSMixer by 9.5% by introducing channel independence (CI)in the backbone instead of channel flattening. By further addinggated attention (G) and hierarchy reconciliation (H) together [i.e. CI-TSMixer(G,H)], we observe an additional 2% improvement lead-ing to a total of 11.5% improvement w.r.t. V-TSMixer. On anal-ysis with all datasets (Appendix ), we observe that CI-TSMixer(G,H) outperforms V-TSMixer by an avg. of 19.3%. In gen-eral, adding G and H together leads to more stable improvementsin CI-TSMixer as compared to just adding G or H.",
  ": Channel mixing technique comparison (MSE)": "CI-TSMixer outperforms V-TSMixer by 11.5%, and by adding cross-channel reconciliation head (CC), the accuracy further improves by2% leading to an overall improvement of 13.5% for CI-TSMixer(G,CC-Best) (i.e. channel independent backbone with CC head) ContextLength () is an important hyperparameter in the CC head, whichdecides the surrounding context space across channels to enablereconciliation, and this parameter has to be decided based on the un-derlying data characteristics. For this experiment, we varied from1 to 5 and selected the best which is depicted as CI-TSMixer(G,CC-Best) in . For more exhaustive results on various and alldatasets, refer to the Appendix Table. 13. Moreover, we observefrom that CI-TSMixer(G,CC-Best) performs better comparedto the IC-TSMixer which applies cross-channel correlation insidethe backbone. In addition, CrossFormer proposes an alternativepatch cross-channel correlation approach which TSMixer signifi-cantly outperforms by 30% (Appendix Table. 9). Thus, the hybrid\"channel modeling approach of having a channel-independent back-bone augmented with a cross-channel reconciliation head is relativelyrobust to the noisy interactions across the channels. Also, from thearchitectural perspective - this approach helps in pretraining thebackbone on multiple datasets with a varying number of channels.This cannot be achieved trivially with channel-mixing backbones.",
  "In this section, we compare TSMixer with popular benchmarks onmultivariate forecasting via self-supervised representation learning": "4.4.1Accuracy Improvements. In , we compare the fore-casting results of TSMixer with self-supervised benchmarks. ForBTSF, TNC, TS-TCC and CPC - we report the results from . Forself-supervised PatchTST, we report ETTH1 from and calcu-lated it for Weather as it was unavailable. We use the space com-monly reported across and . In the self-supervised work-flow, the TSMixer backbone is first pre-trained to learn a genericpatch representation, and then the entire network (backbone + head)is finetuned for the forecasting task. By training TSMixer with thisapproach, we observe from Table. 6 that CI-TSMixer-Best achievesa significant improvement (50-70% margin) from existing forecast-ing benchmarks learned via self-supervision (such as BFSF, TNC,",
  ": Forecasting via Representation Learning (MSE)": "TS-TCC, CPC). Similar trends were also observed with TS2Vec (Appendix ). Also, CI-TSMixer-Best beats self-supervisedPatchTST by a margin of 2%. However, as explained in Section. 4.2.2,we achieve this improvement over PatchTST with a significant re-duction in time and memory. For a drill-down view of CI-TSMixer-Best, refer to Appendix Table. 14 4.4.2Pretrain strategies in Self Supervision. In Table. 7, wedo a detailed analysis of the self-supervised approach on 3 largedatasets (Electricity, Traffic, and Weather) with 3 different pretraindata creation strategies as follows: (i) SAME (same primary datafor pretrain and finetune), (ii) ALL (pretrain with all data [ETT,Electricity, Traffic and Weather] and finetune with the primarydata), (iii) TL (transfer learning: pretrain with all data except pri-mary data and finetune with the primary data). Since we learnacross multiple datasets, we use a bigger model size for this ex-periment (i.e. = 12, = 3) for better modeling capacity. FromTable. 7, we observe that all three considered data strategies workequally well with marginal variations across them. However, thebenefits of transfer learning across multiple datasets are not verynotable, as we observe in other domains (like vision and text). Onaverage, CI-TSMixer-Overall-Best(SS) [which indicates the best re-sult across the considered data strategy variants] shows improvedperformance w.r.t. self-supervised PatchTST (reported from )and supervised TSMixer by 1.5%. Thus, enabling self-supervisionbefore the forecasting task helps in improving forecast accuracy.Furthermore, it helps in faster convergence for downstream tasks. 4.4.3Patch Representations. To understand the semantic mean-ing of the learned patch representations, we randomly choose 5patch embeddings (i.e. output of TSMixer backbone after pretrain-ing) from ETTH1 and fetched its nearest 50 embeddings to form 5clusters in the patch embedding space (Figure. 5(b)). We then fetchtheir associated patch time series and plot in Figure. 5(a). From thefigure, we observe that nearby patch representations highly corre-late to the patch time series of similar shapes and patterns, therebylearning meaningful patch representations that can effectively helpin the finetuning process for various downstream tasks. Please referto Appendix Figure. 8 for visualizations on more datasets.",
  "CONCLUSIONS AND FUTURE DIRECTIONS": "Inspired by the success of MLP-Mixers in the vision domain, thispaper proposes TSMixer, a purely designed MLP architecture withempirically validated time-series specific enhancements for mul-tivariate forecasting and representation learning. Especially, weintroduce a new hybrid architecture of augmenting various rec-onciliation heads and gated attention to the channel-independentbackbone that significantly empowers the learning capability ofsimple MLP structures to outperform complex Transformer models.Through extensive experimentation, we show that TSMixer outper-forms all popular benchmarks with a significant reduction in com-pute resources. In future work, we plan to extend TSMixer to otherdownstream tasks (such as classification, anomaly detection, etc.)and also improve the transfer learning capabilities across datasets.We also plan to investigate Swin, Shift, and other newerMixer variants for its applicability in time-series.",
  "R.J. Hyndman and G. Athanasopoulos (Eds.). 2021. Forecasting: principles andpractice. OTexts: Melbourne, Australia. OTexts.com/fpp3": "Arindam Jati, Vijay Ekambaram, Shaonli Pal, Brian Quanz, Wesley M. Gif-ford, Pavithra Harsha, Stuart Siegel, Sumanta Mukherjee, and ChandraNarayanaswami. 2022. Hierarchy-guided Model Selection for Time Series Fore-casting. Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, andJaegul Choo. 2022. Reversible Instance Normalization for Accurate Time-SeriesForecasting against Distribution Shift. In International Conference on LearningRepresentations. Muyang Li, Xiangyu Zhao, Chuan Lyu, Minghao Zhao, Runze Wu, and RuochengGuo. 2022. MLP4Rec: A Pure MLP Architecture for Sequential Recommendations.In Proceedings of the Thirty-First International Joint Conference on Artificial Intelli-gence, IJCAI-22, Lud De Raedt (Ed.). International Joint Conferences on ArtificialIntelligence Organization, 21382144. Track.",
  "Aron van den Oord, Yazhe Li, and Oriol Vinyals. 2018.RepresentationLearning with Contrastive Predictive Coding.CoRR abs/1807.03748 (2018).arXiv:1807.03748": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, LlionJones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.At-tention is All you Need. In Advances in Neural Information Process-ing Systems, Vol. 30. Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. 2021. Autoformer:Decomposition Transformers with Auto-Correlation for Long-Term Series Fore-casting. In Advances in Neural Information Processing Systems.",
  "Tianping Zhang, Yizhuo Zhang, Wei Cao, Jiang Bian, Xiaohan Yi, Shun Zheng, andJian Li. 2022. Less Is More: Fast Multivariate Time Series Forecasting with LightSampling-oriented MLP Structures": "Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,and Wancai Zhang. 2021. Informer: Beyond Efficient Transformer for Long Se-quence Time-Series Forecasting. In The Thirty-Fifth AAAI Conference on ArtificialIntelligence, Vol. 35. 1110611115. Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. 2022.FEDformer: Frequency enhanced decomposed transformer for long-term seriesforecasting. In Proc. 39th International Conference on Machine Learning (Baltimore,Maryland).",
  "AAPPENDIXA.1Datasets": "We use 7 popular multivariate datasets provided in for fore-casting and representation learning. Weather7 dataset collects 21meteorological indicators such as humidity and air temperature.Traffic8 dataset reports the road occupancy rates from differentsensors on San Francisco freeways. Electricity9 captures the hourlyelectricity consumption of 321 customers. ETT10 (Electricity Trans-former Temperature) datasets report sensor details from two elec-tric Transformers in different resolutions (15 minutes and 1 hour).Thus, in total we have 4 ETT datasets: ETTM1, ETTM2, ETTH1, andETTH2.",
  "A.2Supplementary Details": "TSMixer follows to determine the optimal learning rate duringtraining for better convergence. Early stopping with patience 10 isapplied during training. TSMixer library is built using PyTorch andmulti-GPU node training is enabled via Pytorch DDP11. TSMixercan easily scale and cater to the needs of large-scale forecasting andcan be deployed across multiple nodes in the Kubernetes clusters.",
  "A.3Supplementary Figures": "This section covers the supplementary figures that are referred inthe paper for better understanding. Figure. 6(a) depicts the stan-dard MLP block used in the mixer layer. Figure. 6(b) explains thegated attention block used in the mixer layer to downscale thenoisy features. Figure. 7 highlights the architecture followed in theprediction and pretrain head. In the self-supervised pre-trainingworkflow, pre-train head is attached to the backbone. In other work-flows (such as supervised, or finetuning), the prediction head isattached.Figure. 8 shows the correlation between patch time series andits associated embeddings in different datasets.",
  "This section explains the supplementary results which are not re-ported in the main paper due to space constraints": "A.4.1Benchmarking LightTS, S4, CrossFormer and TS2Vec.This section compares and contrasts TSMixer with LightTS ,S4 , CrossFormer and TS2Vec . Considering the spaceconstraints and the lower performance of these benchmarks ascompared to our reported primary benchmarks (like PatchTST,DLinear), we mention these in the appendix instead of the mainpaper. Table. 8 and Table. 9 compare and contrast TSMixer withLightTS, S4 and CrossFormer in a supervised workflow. Table. 10compares and contrasts TSMixer with TS2Vec in the self-supervisedworkflow. Since the baseline papers reported the results in a dif-ferent forecast horizon ( ) space, we report their comparison inseparate tables as per the commonly available forecast horizons.",
  "% MSE improvement over V-TSMixer18%18.5%18.1%19.3%": ": Effect of CI, Gated Attention and Hierarchy Reconciliation over Vanilla TSMixer on alldatasets. CI-TSMixer outperforms V-TSMixer by 18% and by adding gated attention (G) and HierarchyReconciliation head (H), the accuracy further improves by 1.3%, leading to a total of 19.3% improvement.It is important to note that, we observe stable improvements when (G) and (H) are used togetherinstead of just (G) or (H).",
  "% improvement over V-TSMixer18%13.2%19%": ": Detailed MSE Analysis of various Channel Mixing techniques with different context lengths.Context length is varied from (1) to (5) and the minimum is selected as (Best) in this table. From thecomplete data analysis, we observe that CI-TSMixer outperforms V-TSMixer by 18%, and by addinga cross-channel reconciliation head (CC), the accuracy further improves by 1% leading to a totalimprovement of 19%. In contrast - IC-TSMixer outperforms V-TSMixer but not CI-TSMixer ."
}