{
  "ABSTRACT": "Personalization of playlists is a common feature in music stream-ing services, but conventional techniques, such as collaborativefiltering, rely on explicit assumptions regarding content qualityto learn how to make recommendations. Such assumptions oftenresult in misalignment between offline model objectives and onlineuser satisfaction metrics. In this paper, we present a reinforcementlearning framework that solves for such limitations by directlyoptimizing for user satisfaction metrics via the use of a simulatedplaylist-generation environment. Using this simulator we developand train a modified Deep Q-Network, the action head DQN (AH-DQN), in a manner that addresses the challenges imposed by thelarge state and action space of our RL formulation. The resultingpolicy is capable of making recommendations from large and dy-namic sets of candidate items with the expectation of maximizingconsumption metrics. We analyze and evaluate agents offline viasimulations that use environment models trained on both publicand proprietary streaming datasets. We show how these agents leadto better user-satisfaction metrics compared to baseline methodsduring online A/B tests. Finally, we demonstrate that performanceassessments produced from our simulator are strongly correlatedwith observed online metric results.",
  "music playlist generation; reinforcement learning; recommendersystems; simulation": "ACM Reference Format:Federico Tomasi, Joseph Cauteruccio, Surya Kanoria, Kamil Ciosek, MatteoRinaldi, and Zhenwen Dai. 2023. Automatic Music Playlist Generation viaSimulation-based Reinforcement Learning. In Proceedings of the 29th ACMSIGKDD Conference on Knowledge Discovery and Data Mining (KDD 23), KDD 23, August 610, 2023, Long Beach, CA, USA 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.This is the authors version of the work. It is posted here for your personal use. Notfor redistribution. The definitive Version of Record was published in Proceedings ofthe 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 23),August 610, 2023, Long Beach, CA, USA,",
  "INTRODUCTION": "Generating personalized playlists programmatically enables a dy-namic form of music consumption that users expect from majormusic streaming platforms. Machine learning (ML) methods arecommonly used to power personalized playlist experiences thatattempt to optimize for both content quality and users musicalpreferences. Music playlist personalization and generation methodsoften rely on methods such as collaborative filtering or sequence modeling . These methods make strong contentquality assumptions that can lead to various practical limitations.For example, explicit feedback based collaborative filtering assumesthat a \"good\" playlist consists of tracks to which a user would assigna high rating and, as a result, often struggles to consider other im-portant factors such as acoustic coherence, the context of a listeningsession, and the potential presence of optimal item sequences. Con-versely, in industrial music streaming systems, quality is assessedthrough metrics derived from user activity, such as average stream-ing time or the number of days during which user was active duringa given period. The limitations arising from modeling assumptionsin conventional methods can lead to a mismatch between offlinemetrics and user satisfaction metrics. For example, a collaborativefiltering method may return a playlist with the highest predictedratings but contains a mixture of adult and kids music, which usu-ally does not lead to good user satisfaction. This makes developinga good playlist generation system challenging in practice.Reinforcement learning (RL) offers an orthogonal approach thatdoes not require explicit quality assumptions but can instead in-teract with users to learn a playlist generation model that directlyoptimizes for satisfaction. RL agents interact with users to exploreand identify important factors that drive playlist quality as assessedby consumption metrics and, as such, can overcome the target-to-metric disconnect of conventional playlist personalization methods.In order to apply RL to music playlist generation, the genera-tion problem needs to be formulated as a Markov decision process(MDP), in which states encode contextual information summarizinga user listening session, the action space is the space of all the possi-ble playlists and the reward is the desired user satisfaction metric. Amajor challenge of this formulation is the combinatorially complexaction space. For example, the task of generating a playlist of 30",
  "KDD 23, August 610, 2023, Long Beach, CA, USAFederico Tomasi et al": "We thank all the present and past team members involved in theprojects that led to this publication for their hard work and ded-ication: Justin Carter, Elizabeth Kelly, Raj Kumar, Quan Yuan fortheir work on the backend and ML components and setting theinfrastructure up for online tests; Jason Uh for guiding the projectand coordinating online tests; Dmitrii Moor, for his initial contri-bution to the world model concept; Mehdi Ben Ayed, for creatingthe infrastructure for RL experimentation and first offline and on-line tests; Marc Romeyn, Vinod Mohanan, Victor Delepine, MayaHristakeva for laying the foundation of the work in its initial phase.",
  "RELATED WORK": "Music recommendation is a topic of increased interest in recentyears . Recommending music on a streaming platform is anon-trivial challenge. It is especially difficult when compared toother types of content because implicit music preferences of usersare hard to model in a generalized fashion . Moreover, musicstreaming users are more likely to listen to the same songs severaltimes (in comparison to buying the same items on e-commerce plat-form or watching the same movies on video streaming platformsbeing relative less likely), so the trade-off between explorationand exploitation (i.e., recommending new tracks or recommendingtracks the users are familiar with) becomes harder to balance. RLapproaches provide theoretically valid and practically proven mech-anisms to solve such problems and, as such, have been extensivelyexplored for music recommendation tasks .Prior work aims at modeling user preferences as part of the RLprocedure itself, however; there is no generally accepted procedureused to translate user musical preferences into an actionable re-ward for agents. Some approaches require that users score eachsong in the dataset (e.g., ), which would be infeasible in largescale applications. Others relax this requirement and divide songsin the catalog into predefined bins before assigning each user toone or multiple clusters (e.g., ), which, conversely, may not befine-grained enough to capture the nuances of the user preferences.Traditional methods that use a static notion of preference are alsolimiting as user preferences are inherently contextual, so the sametypes of songs may be relevant in particular situations but not inothers (e.g., sleep music while exercising). Current work that at-tempts to use RL methods for music recommendation also fails tosufficiently account for the sparsity of user-item signals in lean-back interaction modalities. Moreover, due to the low-friction iteminteraction cost of music to users, it is challenging to use implicitor explicit signals to derive a generally valid content rating .This work combines an accurate model of the environment (acritical component of our offline RL formulation) with RL agentsbest suited for non-myopic decisions. Our model provides us witha generalizable mechanism via which we can approximate complexuser behaviors and translate them into a reward function that theagent can effectively use to learn a satisfactory policy. This, coupledwith our agent design, allows us to generate playlists customized todifferent types of users in a manner that maximizes their expectedsatisfaction as determined by our metrics.Previous RL for music playlist generation methods consider the action space to be a single track, which is similar to theMDP of our simulated environment. The major difference is thatthese methods learn an agent directly from recorded user listening",
  "Automatic Music Playlist Generation via Simulation-based Reinforcement LearningKDD 23, August 610, 2023, Long Beach, CA, USA": "completion counts form the SWM than a random policy. This isclearly demonstrated by the fact that the CWM-GMPC has the high-est average return among the three methods tested. CWM-GMPCorders tracks optimally according to the simulated responses ofCWM. Since the agent is trained against the CWM in simulationthis provides a bound on the performance of the agent.This bound manifests in the agent. The policy is able to out-perform a random shuffle, implying that the agent policy is hascaptured some information while training in simulation with theCWM that enables it to provide a better track ordering than random.We also plot the distribution of rewards in . The distribu-tions show how a random policy mostly fails to return a satisfactorylist of tracks, further validating the assumption that the order oftracks is indeed meaningful for a good listening experience. Thepolicy learned by our proposed agent is able to provide a betterexperience than the random policy for the majority of the users,concentrating the majority of (simulated) rewards between 2 and 4(which correspond on average to at most 7 and 9 tracks completedin the session of length 20). We can notice a spike in position 0 ofsimulated rewards for the agent policy which is not shared fromthe CWM-GMPC. This highlights that, for a few users and sessions,the agent fails to predict an appropriate sequence which leads tothe simulation outcome of all tracks being skipped.",
  "PROBLEM FORMULATION": "Consider the problem of automatic playlist generation in a musicstreaming platform. The catalog is composed of millions of songs(often called tracks) created by a diversity of artists. Each trackand artist is associated with a variety of features that summarizenuances useful when recommending or otherwise modeling content(traits intrinsic to the songs themselves, such as audio features likebeats-per-minute or key, or aggregate user consumption traits).In this work, we assume that we have access to such features foreach recommendable track. Further details on how to generatesuch features can be found in previous works (e.g., ). Here weconsider the automatic playlist generation problem, where usersdecide on a general super-set of content they are interested in(such as a genre, e.g., indie pop), and the platform is tasked withgenerating a playlist that satisfies users given this initial interest.Having a catalog of millions of tracks, it is unlikely that such aplaylist already exists especially for tail content types. Further,each user experiences music differently, hence automatic playlistgeneration is a relevant and practical problem for music streamingplatforms to create the best personalized experience for each user.",
  ": Reinforcement learning loop in a simulation-basedenvironment": "In order to design a method to best pick a list of tracks for theuser, we first design a user behavior model that estimates how a userwould respond to these tracks. Using this model we can optimize theselection of tracks in such a way as to maximize a (simulated) usersatisfaction metric. To model users, we assume that each user has animplicit prior preference for tracks belonging to certain categories.For example, one user may usually like rock music, while anotheruser may prefer hip hop. However, we do not assume that (static)genre preference is the only factor that influences which tracks auser wants to listen to. Instead, we consider multiple other factors,such as contextual preference based on time of day, device typeand so on (for example, music patterns during commute hours maydiffer than music patterns before sleep, or during working hours).Further, we consider a subset of tracks that are available foreach playlist based on the general preference of the user, whichwe refer to as the candidate pool. For example, based on the genreindie pop we utilize a pool of candidates that have been previouslyassigned to this genre. Each candidate pool has a dimensionalitymuch lower that the size of the whole catalog of available tracks inthe platform. Utilizing these components we solve the recommenderproblem using a model-based RL framework that we detail in thenext section.",
  "METHODOLOGY": "illustrates the main training loop in a model-based RLframework . The environment consists of a world model thatmodels the user behavior in response to an action A attimestep , which is also referred to as a user behavior model in theprevious text. This model captures the state transitions given theaction selected from the agent and the state information visible tothe agent.More precisely, the environment models a transition functionT : S A S and a reward function R : S A R. The agentis tasked with learning a policy that selects actions to perform onthe environment based on the current state of the environment.The policy is hence represented as a function : S A, thatmaps a states to actions. The first state is sampled from the initialstate distribution (0) by the environment and observed to theagent. After the action is selected by the policy, the environmentreturns the next state +1 T (|,).We consider a recommendation task, in particular for contentprogramming in a music streaming domain. In such framework, theaction is the item (i.e., the song) recommended. The environmentconsumes the action proposed by the agent and uses the world",
  "World Model Design": "To accurately learn the policy in an offline setting, the agent needs tointeract with a model that mimics a real recommendation scenariosas close as possible to the same conditions the agent will encounterwhen deployed online. To accomplish this, we design a world modelthat includes the information of the candidate pools, the user stateinformation, as well as the current track features. The world model(environment) models a transition function using historical data,and uses this function, once trained, to produce a new state fromthe action selected by the agent as well as the reward that resultsfrom the specific action.The world model is responsible for starting the session (start ofepisode), keeping track of list of tracks recommended to the user,for terminating the session (end of episode). The combination ofall of such components is how the world model simulates a usersession. Further, in order to model the reward, the world modelutilizes a user model to predict the user response to an item. Theuser model is designed to be as accurate as possible in its ability topredict how users would have behaved in response to the actionsselected by the agent, by distilling user preference. Such user modelis a supervised classifier trained using data collected from real usersduring past online experiments. We use a randomized policy tocollect ground truth interaction data for training. Specifically, werandomly shuffle the songs in a number of music listening sessionsand collect outcome information for each track streamed by theuser, such as the percentage of the track completed by the user(if streamed) and the presentation position of the track within thelistening session.To these data we join user information (i.e., features related touser interest and past interaction with the platform), which we callcontext features, and content information (i.e., information relatedto the content that the user is interacting with), which we callitem features. The model uses these features to predict a set of userresponses for each track. In practice, we perform this task by traininga classifier with multiple outputs, each one mapped to a response. The model is optimized to predict three different (related) userresponses: (i) whether the user will complete the candidate track,(ii) whether the user will skip the candidate track, (iii) and whetherthe user will listen to the track for more than a specific number ofseconds (specified a priori). The actual implementation of the usermodel changes the complexity and the modeling power to mimicreal users. While different parameterizations of the user modelare possible, we design two different user models, a sequentialand non-sequential model, which we refer to as SWM and CWM,respectively.The sequential model is a sequence of LSTM cells1 that capturethe order of tracks within the same episode and use the user re-sponse at track 1 to predict the user response at track . Morespecifically, the sequential user model defines a (auto-regressive)sequential model of the form:",
  "( |0, . . . ,1,0, . . . ,,),(1)": "where represents the item features of the track at the-th position, represents the user response that corresponds to track and represents the context information (user information in addition toother information about the current session). The parameterizationof () uses a LSTM to capture the sequential information from 0to 1. Empirically, we consider a sequential 3-layer model with(500, 200, 200) LSTM units.For the non-sequential user model, we construct a series of denselayers where the information in the input (features) is limited tofeatures that summarize the track and the user with no other infor-mation on the session itself (such as position of the track withinthe session, or user responses of previous tracks in the session)provided to the model. The non-sequential user model takes thefollowing form:",
  "( |,),(2)": "where response to track does not depend on previous tracks,and the probability decomposes into independent components.Each model has its own advantages. The non-sequential model issimple and fast which makes it easier to use for agent training. Thesequential model is more accurate in the classification task due toits access to sequential information coupled with the fact that userresponses to tracks in sequence are often correlated. Empirically, wefound that the use of a sequential model (with intrinsically higherstochasticity) makes training the agent far more difficult and thuscan be less beneficial in practice. Additionally, the non-sequentialmodel allows us to easily compute the maximum theoretical rewardsince it is not dependent on the actual order of songs shown tothe users. For this reason, during offline evaluation we are ableto understand how far the agent is from the maximum expectedreward.At each action picked by the agent, then, the world model collectsthe information about the specific track (track features) as well ascontext information (user response history, features of previoustracks in the session and so on) to pass to the user model and 1Different parametrizations can be applied to capture the sequential information, suchas transformers . However, detailed comparisons about different world modelimplementations are outside of the scope of the paper.",
  "Action Head DQN Agent": "A highly diverse set of musical interests across users and usergroups coupled with the potential for varied and sometimes evenorthogonal listening behavior or objectives (e.g., consuming famil-iar or unfamiliar content) on the part of individual users requires arecommender system that can both generalize and adapt. RL meth-ods meet these requirements and we propose a simple and efficientRL solution based on Deep Q- Learning (DQN) .The DQN agent uses a deep neural network to predict the recom-mendation quality (Q) of each item (action) in a listening session.The main idea behind the Q network is that each available track isassigned to a specific value (a Q value), and the track with the high-est value is then selected by the agent. The reward as returned bythe environment after each action is used to update the Q network.An intrinsic limitation in our use case is that the pool of can-didate tracks is dynamic and may depend on contextual or userinformation. In particular, the agent needs to be able to generalizeits selection strategy across different candidate pools. This require-ment results in our DQN differing from typical formulations wherethe Q network, given a state representation as input, produces anarray of quality predictions where each array element maps to aspecific action. This is not applicable in our case since the itemspace as defined by the pool is constantly changing, and there-fore, so is the output space of the Q network and its mapping tospecific actions. Simultaneously, we need to train a single agentthat can operate across a diverse set of candidate pools that inpractice can have little to no overlap. Empirically choosing to traina single agent makes more efficient use of our training data andgenerated episodes. Practically, it also means that in production weneed to maintain only one agent and not one agent per pool, user,or pool-user combination.Because of such constraints we developed what we refer to as anaction head (AH) DQN agent (). Our AH-DQNs Q networktakes as input the current state and the list of feasible actions.The network will produce a single Q value for each action, andthe one with the highest Q value is selected as the next actionto apply. This ensures that a single Q network can be optimizedindependently from the set of available items to recommend during",
  ": Action-head network. Both observation and actionfeatures are taken in input in order to estimate the Q value": "an episode. We model the different environments as a contextualMDP , to ensure generalization across users and candidate poolsthus allowing for the existence of one single agent. Intuitively, thisapproach generalizes well to arbitrary and even dynamic candidatepools. The downside is that we need to make a forward pass throughthe Q network for each allowed action in the candidate pool topredict its Q value. However, as the candidate pool is much smallerthan the catalog of items to recommend, this is still efficient to doin practice. Furthermore, this step could be efficiently parallelizedas the forward passes through the Q network, for each action at astep, are independent from each other.Our goal is to optimize the following policy: AH : SA A,which takes as input the state and the list of available actions,denoted A, and selects the best action according to the computedQ values:",
  "(,) = (,) + max (+1,)(3)": "where is the current state, (,) is the reward estimate, and+1 is the next state.The state of a user S encodes all the information available tothe agent at time. S is the set of possible states. The state variableobeys the Markov property (+1|,) = (+1|1,1, . . . ,,),since encodes all the information in (1,1, . . . ,1,1). Reward and User Simulator. Our reward function R : A Rassociates the recommended track at step with a measure on howsuccessful the recommended track (or sequence of tracks up toand including time-) is with respect to some user outcome. In ourexperiments, we measure the performance of the recommendersystem based on the probability of a track to be completed by theuser, so R() . In our model-based RL framework, theuser behavior is approximated using a user model (as described inabove) and, in this setting, the probability of completion estimatedby the model serves as a proxy for the real user preference. Then,for each episode we compute the sum of completion probabilitiesas predicted by the user model until episode termination, which is",
  "EXPERIMENTS": "We tested our model on both public and proprietary streamingdatasets. We evaluate public streaming performance in an offlinesetting only. For our online experiments, we train a user model onproprietary streaming data of similar structure to the public dataset,train the agent as described, and deploy a fixed agent policy in anonline A/B testing setting.",
  "Public Streaming Dataset": "We first use a public music dataset from Spotify including 160million music listening sessions to empirically evaluate our pro-posed model-based RL formulation. Features that describe tracks(such as acoustic properties and popularity estimates) and sessionsare provided in the dataset. We organize our data by ordering ses-sion interactions over time. At each interaction there are indicatorsin the dataset that note if the track was completed or skipped ator before three track-time markers: 1,2,3. Using these data wedesign simulation environment to train and test our agent. First wetrain a response model that takes as input a sequence of tracks upto time 1, {, = 0, . . . , 1} and an action-track at . It thenpredicts the probability of a skip outcomes 1,2,3 for .We regard each listening session as a list of tracks from a uniqueplaylist that the agent aims at creating. As described in , auser (associated to its listening session) will initiate a play sessionby selecting a musical sub-context (such as a genre). Each sub-context has a pool of associated items (tracks) from which we drawcandidates to generate a playlist. The user will then interact withthe playlist until the selected tracks are exhausted (one source ofepisode termination)2.We impose some design specifics for our simulation. First, wesample sessions of length 20 from the data. At the first step theenvironment observation is composed of the first 5 tracks in theplay session in the order observed. At each step the agent actionis to pick one track from the remaining 15: the response modelthen provides a predicted outcome for this selection (probability ofcompletion). The agent is rewarded for picking tracks with a lowlikelihood of being skipped as determined by the response modeland the episode is terminated after 15 steps. As such the maximumachievable reward in our simulations is the maximum of the sum ofcompletion probabilities for each session, which is upper-boundedby the size of candidates in the playlist, i.e., 15.We use a non-sequential user model (CWM), to simulate userresponses during agent training. As no user features are availablein the public dataset (users are anonymized for privacy), we utilizeonly on the track features and past user responses in the sessionto compose our feature space. Specifically, the user model utilizesfirst 5 tracks in the play session, represented by their features, as acontext feature to estimate the outcome of the remaining 15 tracks,each represented by their respective features. 2Note that, in online scenarios, the user may quit the session prior to consuming allcandidates in the playlist. For simplicity we use a fixed size termination criterion duringoffline experiments and evaluation, but other termination criteria remain possible.",
  ": Reward distributions for the compared policies onthe public streaming dataset as estimated by the SWM": "We then train the agent as introduced in .2 against theCWM. As mentioned in .2 this allows for more tractableand faster training than more complex user models. We train theQ-network using an element-wise mean squared error TD loss.Such loss is implemented using an additional target network withthe same structure of the original Q network and its own param-eters and layers, which is updated with a period of 50 that em-pirically stabilizes the training. We compare our method to twobaselines: (i) Random: a model to randomly sort the tracks by shuf-fling the 15 remaining tracks in the session with equal probability;and (ii) CWM-GMPC: a combination of the non-sequential usermodel and a greedy ranking policy, as described in .2 Fi-nally, we use an independent metric model to estimate the policyperformance of the agent against the random and CWM-GMPCbaselines. To better simulate user behavior during offline evalua-tion, we use the sequential world model (SWM) as introduced in.2. includes the result summary of our comparisons betweenour agent and the two baselines using SWM as the evaluation pro-cedure. Note that each policy results in relatively modest simulatedaverage returns (between 1 and 2.5). This is because of two reasons.First, the dataset does not contain any user preference features andany non-sequential model has access to only track features duringinference. Second, the action space is made up of only 15 prede-fined tracks and the goal is to sort them accurately. Our trainedpolicies are non-sequential, their ability to achieve highly accu-rate results on such a task will be somewhat limited. We note thateven the CWMs greedy optimal ordering produces better predicted",
  "Online Experiments": "We tested our model-based RL approach online at scale to assess theability of the agent to power our real-world recommender systems.Our expectation is that the RL agent, trained offline against a non-sequential world model, should be able to produce satisfactoryuser-listening experiences assuming the world model accuratelyreflects user preferences. As the behavior of the agent is directlydependent on the accuracy of the user model, we also tested ouragent online against the user model by itself (CWM-GMPC). Experiment Settings. Similar to the experimental settings on thepublic dataset, we consider the case of a recommender systemtasked with generating the best list of tracks for a user to listen togiven a particular context (time of day, type of music requested, andso on). Our recommender system has a similar objective in onlineexperiments: to maximize user satisfaction. As a proxy for usersatisfaction, we consider the completion count (and rate), i.e., theamount of tracks recommended by the policy that are completed bythe user (and as a fraction on how many are started). For this taskwe first train an agent offline using a non-sequential world model,CWM, and then deploy the agent online to serve recommendationsto the users. In our production setting each user targeted by thetest selects a playlist which is backed by a predefined track pool,and the system responds with an sub-selected ordered list of tracksof a size less than that of the pool.The outcomes of our user model are primarily consumption-focused and summarize the probability of user-item interactions.Specifically, the CWM variant in this case has been optimized forthree targets: completion, skip and listening duration3. The rewardfor the agent is computed as the sum of the probability of completion",
  "(ii) Cosine Similarity: a model which sorts the tracks based onthe cosine similarity between predefined user and track em-beddings;": "(iii) CWM-GMPC: the user model ranking, which sorts the tracksbased on the predicted probability of completion from theuser. This is the same GMPC construction we use in thepublic dataset but the user-model targets have changed tomatch our real-world setting (as described above). All policies take as inputs features of the user that made therequest and the pool-provided set of tracks with their associatedfeatures. The goal of each policy is to select and order a list oftracks from the pool that maximizes expected user satisfaction,which we measure by counting the number of tracks completedby the user. Note that the number of tracks that the user actuallyinteracts with (i.e., that the user starts listening to) may be lowerthan the recommended number (e.g., if the user leaves the listeningsession early). For this reason, in what follows we measure not onlythe number of completed tracks, but also the rate of completion(the amount of tracks completed with respect to those that wereactually started).To study the effectiveness of our proposed approach we con-ducted a live A/B test on our production platform. We comparedthe previously described 4 policies to the default playlist gener-ation model of our production platform by running a week longtest. Users were randomly divided into five test cells, assigningthe default model to control group, and assigning the four policiesto their respective treatment groups. We collected a large sampleof interaction data covering 2 million users, interacting with 2.8million unique tracks across 4 million distinct sessions.",
  ": Simulated performance (estimated by the SWM)with respect to online reward (with true user responses) forthe compared four different policies": "offline performance estimates for each model alongside online re-sults for the policies previously listed. Just like with the publicdataset evaluation, we use an independent metric model in offlinesimulation to estimate policy performance, the sequential worldmodel (SWM) described in .1.For each policy we generate a list of recommended tracks for eachevaluation episode. For each list we then compute the completionprobabilities using the SWM and sum them. Finally, we averagethe total completion probabilities across the evaluation episodes tocalculate the average reward for a policy.Our simulator for real-world applications allows for a variety ofsettings including (but not limited to) the ability to simulate policyperformance for different users and content. Different simulationsettings and varying simulated episodes initializations can lead todifficult to summarize, multi-modal performance estimates from oursimulator. For these results simply calculating the average rewardof a policy across all episodes can fail to correctly summarize itsperformance because of skew, outliers, and the multi-modal distri-butions. highlights the bimodal distribution of the rewardsfor simulated episodes that can result from two common in userlistening behaviors: lean-back (majority of tracks are completed)and active skipping (majority of tracks are skipped, or incomplete).The offline performance metric is then computed using a normal-ized average of the modal returns from episode reward as follows.A Gaussian mixture model is used to separate the reward distri-butions, then values are logged and min-max scaled to priorto each policy being assessed. A non-parametric bootstrap is usedto obtain confidence intervals on our performance estimates foreach policy. Such a procedure yields a more accurate reflection of apolicys simulated performance.The offline performance expectations of the evaluated policiesalign with their online performance (). Unsurprisingly, arandom policy fails to perform well in both simulated and onlinesettings. The Cosine Similarity model has better performance thanrandom, but it lacks the rich user and item features available tothe other non-random policies. We see that the offline performancebetween the Action Head Policy (the agent) and CWM-GMPC is",
  ": Percent difference in average completion count per-session relative to control": "essentially the same. This is to some extent expected since theoptimal policy for agent trained against the pointwise world modelis, in fact, the greedy policy CWM-GMPC (.2). Onlineresults show a slight gap between these policies, but the differenceis statistically indistinguishable. Online Evaluation. Tables 2 and 3 include results on the CWM-GMPC and RL agent performance online, respectively. Online ex-periments that directly serve the user model (CWM-GMPC) demon-strate its performance is statistically indistinguishable from controlfor essentially all metrics of interest (). Note that of the met-ric comparisons in , most importantly, the probability of auser skipping or completing a track is statistically indistinguishablebetween the user model and control. This, along with the offline-online correlation analysis in , shows how our approach tomodel user behavior is accurate in practice.",
  ": Online reward with respect to simulated perfor-mance with the addition of SWM online estimate": "The performance of our trained agent and the additional compar-ative policies relative to control for completion count per session isshown in and completion rate per session in . Ourexpectation is that an agent trained against our world-model in anoffline setting should at least mimic its online performance. Ourresults show that, online, our user model is statistically indistin-guishable from control, so we expect to have similar results for ouragent performance. Hence, the objective of our online analysis is todemonstrate no statistically significant difference between control(playlist generator implemented in production) and the behaviorof our agent. This is validated in our results, where both our usermodel ranking and the agent trained in simulation show resultsstatistically indistinguishable from control, further validating ourapproach.We also report the complete metric spread for the agent rela-tive to control in . Note that although point estimates aredifferent, the confidence intervals for the metric relative differ-ence to control for the agent and user model ranking are moreor less entirely overlapping for each metric. This highlights howthe performance of CWM-GMPC and agent are statistically indis-tinguishable on real users, validating our approach to automaticplaylist generation. Offline Analysis to Predict Online Results. One of the main goalsof this work is to develop the ability to experiment with, train,and evaluate agent policies without exposing users to sub-optimalrecommendations. We have seen how the agent trained against apointwise user model (which we refer to as AH-CWM) is able tomimic its optimal policy online. However, the question of perfor-mance for the agent trained against more complex, sequential usermodels (SWM) with unknown optimal policies still remains. Train-ing such agents brings additional complexity, as a sophisticateduser model is required to be developed alongside an agent that cantrain against it effectively. In real scenarios, the implementationand online testing of a new policy is not straightforward, and needsto be backed up by some degree of assurance that it will not lead toa detrimental experience for the user to minimize the risk of usersabandoning the platform.For this reason, we consider our offline-online correlation anal-ysis of by adding the estimated offline performance as-sessment of an agent trained against the SWM (AH-SWM). Fig-ure 8 shows the offline performance of AH-SWM policy along withan estimate of its online performance derived via extrapolation.Such offline-online correlation analysis is fundamental in orderto approximate what is the expected improvement that could betranslated online. Based on these predictions we hypothesize anonline improvement over both CWM-GMPC and AH-CWM policyperformance. We argue this type of analysis of being relevant inpractical applications of recommender system where online de-ployment requires sufficient expected improvement over existingbaselines.",
  "CONCLUSION": "In this paper we presented a reinforcement learning frameworkbased on a simulated environment that we deployed in practiceto efficiently use RL for playlist generation in a music streamingdomain. We presented our use case which is different from standardslate recommendation task where usually the target is to select atmaximum one item in the sequence. Here, instead, we assumeto have a user-generated response for multiple items in the slate,making slate recommendation systems not directly applicable.By making use of a learned world model that simulates userresponses to the actions selected by the agent, we were able to trainagents offline and evaluate their policies prior to exposure to realusers. Online results show that even without further training usingonline interactions the learned policy does not result in loss of usersatisfaction with respect to other baselines.A further research direction is to improve the user behaviormodel for better agent training. The performance of the trainedagent is influenced by the prediction accuracy of the user behaviormodel. We can explore various ways to improve the user behav-ior model such as designing better user representations, exploringdifferent neural network architectures such as transformers or in-creasing the robustness of prediction via techniques like dropoutor ensemble methods.",
  "Binbin Hu, Chuan Shi, and Jian Liu. 2017. Playlist recommendation based on re-inforcement learning. In International Conference on Intelligence Science. Springer,172182": "Eugene Ie, Vihan Jain, Jing Wang, Sanmit Narvekar, Ritesh Agarwal, Rui Wu,Heng-Tze Cheng, Morgane Lustman, Vince Gatto, Paul Covington, Jim McFad-den, Tushar Chandra, and Craig Boutilier. 2019. Reinforcement Learning forSlate-based Recommender Systems: A Tractable Decomposition and PracticalMethodology. (May 2019). arXiv:1905.12767 [cs.LG] Rosilde Tatiana Irene, Clara Borrelli, Massimiliano Zanoni, Michele Buccoli, andAugusto Sarti. 2019. Automatic playlist generation using Convolutional NeuralNetworks and Recurrent Neural Networks. In 2019 27th European Signal ProcessingConference (EUSIPCO). 15.",
  "Dietmar Jannach, Markus Zanker, Alexander Felfernig, and Gerhard Friedrich.2010. Recommender systems: an introduction. Cambridge University Press": "Mesut Kaya and Derek Bridge. 2018. Automatic Playlist Continuation usingSubprofile-Aware Diversification. In Proceedings of the ACM Recommender Sys-tems Challenge 2018 (Vancouver, BC, Canada) (RecSys Challenge 18, Article 1).Association for Computing Machinery, New York, NY, USA, 16. Suzana Kordumova, Ivana Kostadinovska, Mauro Barbieri, Verus Pronk, and JanKorst. 2010. Personalized implicit learning in a music recommender system. InUser Modeling, Adaptation, and Personalization: 18th International Conference,UMAP 2010, Big Island, HI, USA, June 20-24, 2010. Proceedings 18. Springer, 351362.",
  "Thomas M Moerland, Joost Broekens, Aske Plaat, Catholijn M Jonker, et al. 2023.Model-based reinforcement learning: A survey. Foundations and Trends inMachine Learning 16, 1 (2023), 1118": "Amir Hossein Nabizadeh, Alpio Mrio Jorge, Suhua Tang, and Yi Yu. 2016.Predicting User Preference Based on Matrix Factorization by Exploiting MusicAttributes. In Proceedings of the Ninth International C* Conference on ComputerScience & Software Engineering (Porto, Portugal) (C3S2E 16). Association forComputing Machinery, New York, NY, USA, 6166. Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard L Lewis, and Satinder Singh.2015. Action-conditional video prediction using deep networks in atari games.Advances in neural information processing systems 28 (2015). Keigo Sakurai, Ren Togo, Takahiro Ogawa, and Miki Haseyama. 2020. MusicPlaylist Generation Based on Reinforcement Learning Using Acoustic FeatureMap. In 2020 IEEE 9th Global Conference on Consumer Electronics (GCCE). 942943.",
  "Shun-Yao Shih and Heng-Yu Chi. 2018.Automatic, Personalized, andFlexible Playlist Generation using Reinforcement Learning.(Sept. 2018).arXiv:1809.04214 [cs.CL]": "Jagendra Singh and Vijay Kumar Bohat. 2021. Neural Network Model for Rec-ommending Music Based on Music Genres. In 2021 International Conference onComputer Communication and Informatics (ICCCI). IEEE, 16. Peter Sunehag, Richard Evans, Gabriel Dulac-Arnold, Yori Zwols, Daniel Visentin,and Ben Coppin. 2015. Deep Reinforcement Learning with Attention for SlateMarkov Decision Processes with High-Dimensional States and Actions. (Dec.2015). arXiv:1512.01124 [cs.AI] Peter Sunehag, Richard Evans, Gabriel Dulac-Arnold, Yori Zwols, Daniel Visentin,and Ben Coppin. 2015. Deep reinforcement learning with attention for slatemarkov decision processes with high-dimensional states and actions. arXivpreprint arXiv:1512.01124 (2015).",
  "Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An intro-duction. MIT press": "Andreu Vall. 2015. Listener-Inspired Automated Music Playlist Generation. InProceedings of the 9th ACM Conference on Recommender Systems (Vienna, Austria)(RecSys 15). Association for Computing Machinery, New York, NY, USA, 387390. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin. 2017. Attention is allyou need. Advances in neural information processing systems 30 (2017)."
}