{
  "Abstract": "Ratings of a user to most items in recommender systems are usuallymissing not at random (MNAR), largely because users are free tochoose which items to rate. To achieve unbiased learning of theprediction model under MNAR data, three typical solutions havebeen proposed, including error-imputation-based (EIB), inverse-propensity-scoring (IPS), and doubly robust (DR) methods. How-ever, these methods ignore an alternative form of bias caused bythe inconsistency between the observed ratings and the users truepreferences, also known as noisy feedback or outcome measure-ment errors (OME), e.g., due to public opinion or low-quality datacollection process. In this work, we study intersectional threats tothe unbiased learning of the prediction model from data MNAR andOME in the collected data. First, we design OME-EIB, OME-IPS, andOME-DR estimators, which largely extend the existing estimatorsto combat OME in real-world recommendation scenarios. Next, wetheoretically prove the unbiasedness and generalization bound ofthe proposed estimators. We further propose an alternate denoisingtraining approach to achieve unbiased learning of the predictionmodel under MNAR data with OME. Extensive experiments are con-ducted on three real-world datasets and one semi-synthetic datasetto show the effectiveness of our proposed approaches. The code isavailable at",
  "Corresponding author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain. 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08",
  "Introduction": "Recommender systems (RS) are designed to generate meaningfulrecommendations to a collection of users for items or productsthat might interest them, which have made a number of significantadvancements in recent years . Nevertheless, directuse of these advanced models in real-world scenarios while ignoringthe presence of numerous biases in the collected data can lead tosub-optimal performance of the rating prediction model .Among these, the problem of missing data is particularly prevalent,as users are free to choose items to rate and the ratings with a lowervalue are more likely to be missing , leading to the collecteddata in RS is always missing not at random (MNAR) . MNARratings pose a serious challenge to the unbiased evaluation andlearning of the prediction model because the observed data mightnot faithfully represent the entirety of user-item pairs .To tackle this problem, previous studies evaluate the perfor-mance of a prediction model by computing the prediction inaccuracy:the average of the prediction errors (e.g., the squared difference be-tween a predicted rating and the potentially observed rating) for allratings . To unbiasedly estimate the prediction inaccuracywhen the ratings are partially observable, three typical approacheshave been proposed, including: (1) The error-imputation-based(EIB) approaches , which compute an imputed error foreach missing rating. (2) The inverse-propensity-scoring (IPS) ap-proaches , which inversely weight the prediction error foreach observed rating with the probability of observing that rating.(3) The doubly robust (DR) approaches , which use boththe error imputation model and the propensity model to estimatethe prediction inaccuracy, and the estimation is unbiased wheneither the imputed errors or the learned propensities are accurate.Despite the widespread use of these methods, they ignore analternative form of bias caused by the inconsistency between theobserved ratings and the users true preferences, also known asnoisy feedback or outcome measurement errors (OME). Similar toselection bias, OME also arises from systematic bias during the datacollection. For example, the collected user feedback may differ fromthe true user preferences due to the influence of public opinions .Meanwhile, low-quality data collection such as recommender at-tacks or carelessly filling out the after-sales assessment canalso result in noisy user feedback . Therefore, to make currentdebiased recommendation techniques more applicable to real-world",
  "KDD 24, August 2529, 2024, Barcelona, Spain.Haoxuan Li, Chunyuan Zheng, Wenjie Wang, Hao Wang, Fuli Feng, and Xiao-Hua Zhou": "less than three to 0 and otherwise to 1. KuaiRec is a fully exposedlarge-scale industrial dataset, which has 4,676,570 video watch-ing ratio records from 1,411 users to 3,327 items. We binarize therecords less than one to 0 and otherwise to 1. We adopt three com-mon metrics, i.e., AUC, NDCG@K, and Recall@K for performanceevaluation. For Coat and Music, K is set to 5. For KuaiRec, K isset to 50. All the experiments are implemented on PyTorch withAdam as the optimizer. For all experiments, we use GeForce RTX3090 as the computing resource. Logistic regression is used as thepropensity model for all the methods requiring propensity. Thelearning rate is tuned in {0.001, 0.005, 0.01, 0.05} and the batch sizeis tuned in {64, 128, 256} for Coat and {2048, 4096, 8192} for Musicand KuaiRec. We tune the embedding dimension in {4, 8, 16, 32}for Coat and {8, 16, 32, 64} for Music and KuaiRec. Moreover, wetune the weight decay rate in . Baselines. We use matrix factorization (MF) as the base modeland compare our methods to the following debiasing methods:EIB , IPS , SNIPS , CVIB , DAMF , DR ,DR-JL , MRDR-JL , DR-BIAS , DR-MSE , DIB ,MR , SDR , TDR , IPS-V2 and DR-V2 . We alsocompare denoising methods including surrogate loss minimization(OME) , T-MF , R-MF , and LCD-MF . Performance Comparision. shows the prediction perfor-mance with varying baselines and our methods. First, most of thedebiasing and denoising methods have better performance com-pared to the Naive method, which shows the necessity of debiasingand denoising. Meanwhile, our methods exhibit the most competi-tive performance in all three datasets, significantly outperformingthe baselines including debiasing methods and denoising methods.",
  "its measurement error parameters estimation in the context ofselection bias interact to affect the debiased recommendations.The contributions of this paper are summarized as follows": "We formulate OME caused by the inconsistency between theobserved ratings and the users true preferences, and establish anevaluation criterion for the unbiased learning of the predictionmodel under OME, named true prediction inaccuracy. We develop OME-EIB, OME-IPS, and OME-DR estimators forunbiasedly estimating the true prediction inaccuracy of the pre-diction model under MNAR data with OME, and theoreticallyanalyze the biases and generalization bounds of the estimators. We further propose an alternating denoise training approach toestimate the measurement error parameters and achieve unbiasedlearning of the prediction model under MNAR data with OME,which corrects for data MNAR and OME in parallel.",
  "Preliminaries2.1Task Formulation without OME": "Let U = {1, . . . , } be a set of users, I = {1, . . . ,} a set ofitems, and D = U I the collection of all user-item pairs. The po-tentially observed rating matrix R R comprises potentiallyobserved ratings ,, which represents the observed rating if user had rated the item . In RS, given the user-item features ,, theprediction model (,) parameterized by aims to accuratelypredict the ratings of all users for all items, and then recommend tothe user the items with the highest predicted ratings, as describedin . The prediction matrix R R comprises the predictedratings (,) obtained from this prediction model. If the ratingmatrix R had been fully observed, then the prediction inaccuracy Pof the prediction model can be measured using",
  "(,)D,,": "where , = ( (,),,) is the prediction error, and (, )is a pre-defined loss, such as the mean square error (MSE), i.e.,, = ( (,) ,)2. Let O {0, 1} be an indicator matrix,where each entry , is an observation indicator: , = 1 if the rat-ing , is observed, and , = 0 if the rating , is missing. GivenR as the set of the observed entries in the rating matrix R, therating prediction model aims to train the prediction model that min-imizes the prediction inaccuracy P. Nonetheless, as users are freeto choose items to rate, leading to the collection of observationaldata that is always missing not at random (MNAR) ,e.g., the ratings with a lower value are more likely to be miss-ing. Let O =(,) | (,) D,, = 1 be the set of user-itempairs for the observed ratings, the direct use of the naive estima-tor EN = EN( R, R) =1| O|(,)O , that computes on theobserved data would yield severely biased estimation .",
  "(,)D(,, + (1 ,) ,),": "which is an unbiased estimator of the prediction inaccuracy whenthe imputed errors are accurate, i.e., , = ,.The inverse-propensity-scoring (IPS) approaches firstlearn , as the estimate of the propensity , = P(, = 1 | ,),i.e., the probability of observing the rating, then inversely weight theprediction error for each observed rating with the learned propen-sity, and estimate the prediction inaccuracy with",
  "Methodology3.1Task Formulation under OME": "Despite many methods have been proposed for achieving unbi-ased learning to tackle the data MNAR problem ,they ignore an alternative form of bias caused by the inconsis-tency between the observed ratings and the users true prefer-ences, also known as noisy feedback or outcome measurementerrors (OME). Both data MNAR and OME arise from systematicbias during the data collection. In RS, two common scenarios thatcause incorrect user feedback signals include the influence of publicopinions , and the low-quality data collection such as recom-mender attacks or carelessly filling out the after-sales assess-ments . Formally, we denote R as the users true preferencematrix, with , as its entries, which may deviate from the poten-tially observed ratings ,. Let P(, = 0 | , = 1) = 01 andP(, = 1 | , = 0) = 10 be the false negative rate and falsepositive rate, respectively, where 01 + 10 < 1, then we have",
  "01 10": "The bias of the OME-DR estimator includes the three terms: (1)The first term shares a similar form to the bias of the previous IPSestimator and leads to smaller bias when , ,. (2) The secondterm is novel for OME, specifically focusing on the estimated falsenegative rate 01 that corresponds to the positive samples , = 1.Moreover, we find that 11 = 1 and 01 = 0 when the the estimatedfalse negative rate 01 is accurate, i.e., 01 = 01, which results insmaller bias when , ,. (3) The third term is similar to thesecond term, but instead focuses on the estimated false positiverate 10 that corresponds to the negative samples , = 0, and alsoresults in smaller bias when , ,. Given the importance ofbias derivation for constructing estimators under OME, we providea proof sketch as below (see Appendix A for more details).",
  "01 10,": "where , is the imputed error for estimating ,, and OME-EIB isan unbiased estimator of the true prediction inaccuracy when theimputed errors are accurate, i.e., , = ,. Since the proof is nottrivial, we postpone to show unbiasedness of OME-EIB estimatorand the following OME-IPS and OME-DR estimators in Sec. 3.4.Similarly, the OME-IPS estimator estimates P = P( R, R) with",
  "which is an unbiased estimate of the true prediction inaccuracywhen either the imputed errors or the learned propensities areaccurate, i.e., , = , or , = ,": "3.3Identification and Estimation of 01 and 10The proposed OME-EIB, OME-IPS, and OME-DR estimators requireknowledge of the known error parameters 01 and 10, which areusually not directly available from the collected data. By buildingupon the existing weak separability assumption , we present adata-driven identification and estimation method of 01 and 10.We impose the following weak separability assumption that",
  "inf(,)D P(, = 1 | ,) = 0andsup(,)DP(, = 1 | ,) = 1,": "which also known as mutual irreducibility in the observa-tional label noise literature. This does not require the true ratingsto be separable, i.e., P(, = 1 | ,) {0, 1} for all user-itempairs, but instead stipulates that there exist \"perfectly positive\"and \"perfectly negative\" samples among the entire user-item pairs.In real-world recommendation scenarios, the weak separabilityassumption is easily satisfied, providing there exists at least one\"perfectly positive\" feedback and one \"perfectly negative\" feedbackamong the thousands of collected ratings. For example, in the movierating scenario, \"perfectly positive\" feedback refers to at least one ofthe users who made a positive review of the movie is fully reliable,and we do not need to know who that user exactly is.By noting the linkage between the observed and the true ratings",
  "Since the OME-DR estimator degenerates to the OME-IPS estimatorwhen , = 0, and degenerates to the OME-EIB estimator when, = 1, without loss of generality, we only analyze the explicit bias": "form of the OME-DR estimator. Following existing literature , we assume that the indicator matrix O contains independentrandom variables and each , follows a Bernoulli distribution withprobability ,. In addition, due to the presence of OME, we alsoconsider the randomness of the potentially observed ratings ,given the true ratings ,, e.g., , = 1 with probability 1 01given , = 1, and , = 0 with probability 01 given , = 1.",
  "Corollary 3.2 (Double Robustness). Given 01 = 01 and10 = 10, the OME-DR estimator is unbiased when either imputederrors E or learned propensities P are accurate for all user-item pairs": "The above result is obtained via substituting either , = , or, = , into the bias of the OME-DR estimator in Theorem 3.1.Given the estimated error parameters 01 and 01, we obtain theoptimal prediction model under OME by minimizing the OME-DRestimator over a hypothesis space F of the prediction models R = arg min F{EOMEDR( R, R; 01, 10)}. We next derive the generalization bound of the optimal predictionmodel in terms of the empirical Rademacher complexity . Thebasic idea of proving the performance guarantee under OME is toexploit the inheritance of the Lipschitz continuity from the tureprediction loss ( (,),,) to the surrogate loss ( (,),,).",
  "Alternating Denoise Training Approach": "We further propose an alternating denoise training approach toachieve unbiased learning of the prediction model, which correctsfor data MNAR and OME in parallel. Specifically, we first train apropensity model using the observed MNAR data. Based on theestimated propensities, the denoising prediction and imputationmodels are alternately updated, which also facilitates the accurateestimations 01 and 10 of the error parameters 01 and 10. Propensity Estimation via Logistic Regression. Since unbiasedratings are difficult to be collected in the real-world scenarios ,following previous studies , we adopt logistic regressionto train a propensity model , = (, + + ) parameter-ized by = (, 1, . . . , ,1, . . . ,) using the observed MNARdata, where () is the sigmoid function, R + is the weight",
  "ROTATESKEWCRSONETHREEFIVE": "Naive 0.125 0.0020.179 0.0010.175 0.0020.241 0.0020.264 0.0030.299 0.003OME ( 01, 10)0.087 0.0060.163 0.0020.104 0.0140.161 0.0130.179 0.0140.213 0.014OME (01, 10)0.024 0.0030.136 0.0020.105 0.0040.067 0.0040.076 0.0040.100 0.004 MRDR 0.098 0.0030.089 0.0010.099 0.0020.172 0.0040.190 0.0040.194 0.004SDR 0.097 0.0020.089 0.0010.102 0.0020.172 0.0030.191 0.0030.194 0.003TDR 0.092 0.0020.080 0.0010.103 0.0020.171 0.0030.189 0.0030.195 0.003 EIB 0.366 0.0010.256 0.0010.150 0.0010.562 0.0010.605 0.0010.635 0.001OME-EIB ( 01, 10)0.362 0.0010.255 0.0010.144 0.0010.554 0.0010.598 0.0010.627 0.001OME-EIB (01, 10)0.357 0.0010.253 0.0010.144 0.0010.546 0.0010.589 0.0010.618 0.001 IPS 0.110 0.0020.116 0.0020.134 0.0030.212 0.0040.231 0.0040.254 0.004OME-IPS ( 01, 10)0.060 0.0030.096 0.0020.075 0.0170.111 0.0060.125 0.0070.144 0.007OME-IPS (01, 10)0.013 0.0030.068 0.0030.052 0.0040.034 0.0050.038 0.0060.050 0.005 DR 0.106 0.0020.087 0.0010.104 0.0020.190 0.0030.209 0.0030.216 0.003OME-DR ( 01, 10)0.056 0.0040.067 0.0010.045 0.0180.090 0.0060.102 0.0060.106 0.007OME-DR (01, 10)0.009 0.0030.039 0.0020.022 0.0030.013 0.0040.016 0.0050.012 0.004",
  "(,)D, log( ,) + (1 ,) log(1 ,)": "Denoising Prediction Model Training. Based on the learnedpropensities ,, imputed errors , (), and initial estimates 01and 10, we train the denoising prediction model by minimizingthe estimated true prediction inaccuracy from the proposed OME-DR estimator EOMEDR(,; 01, 10). To obtain more accurateestimates 01 and 10 from the training loop, we then compute theminimum and maximum predicted ratings in the training batchto update (<,<) and (>,>), respectively. The choice not toemploy a separate noisy rating prediction model for computing(<,<) and (>,>) is guaranteed by the monotonicity betweenP(, = 1 | ,) and P(, = 1 | ,) (see Sec. 3.3 for proofs),where the latter has a larger gap between the minimum and themaximum, making it easier to distinguish (<,<) and (>,>). Denoising Imputation Model Training. Given a pre-trainedmodel (,) for estimating P(, = 1 | ,) via existing meth-ods , we update the estimates 01 and 10 by computing1 (>,>) and (<,<), respectively. We finally train thedenoising imputation model , () by minimizing the loss",
  "Semi-Synthetic Experiments": "To investigate if the proposed estimators are able to estimate thetrue prediction inaccuracy accurately in the presence of OME andMNAR effect, several semi-synthetic experiments are conducted ona widely-used dataset MovieLens-100K (ML-100K). We first adoptMF to complete the five-scaled rating matrix R. However, the com-pleted matrix will have an unrealistic rating distribution, thus we sort the matrix entries in ascending order and assign a positive feed-back probability of 0.1 for the lowest 1 proportion, a positive feed-back probability of 0.3 for the next 2 proportion, and so on to obtaina true positive feedback probability, for each user-item pair. Theadjusted probability matrix contains , {0.1, 0.3, 0.5, 0.7, 0.9}with proportion , respectively.Second, following the previous studies , we use sixmatrices below as the prediction matrix R: ROTATE: Set predicted , = , 0.2 when , 0.3, and, = 0.9 when , = 0.1. SKEW: Predicted , are sampled from the Gaussian distributionN ( = ,, = (1 ,)/2), and clipped to the interval [0.1, 0.9]. CRS: If the true , 0.6, then , = 0.2. Otherwise, , = 0.6. ONE: The predicted matrix R is identical to the true positivefeedback probability matrix, except that randomly select , = 0.1with total amount |{(,) | , = 0.9}| are flipped to 0.9. THREE: Same as ONE, but flipping , = 0.3 instead. FIVE: Same as ONE, but flipping , = 0.5 instead.Next, we assign the propensity , = min(4,6, ) for eachuser-item pair and obtain the estimate propensities",
  ", Bern(,), (,) D,, Bern(,), (,) D,": "where Bern() denotes the Bernoulli distribution. Then we flip thefeedback matrix according to the 01 and 10 to generate a binarynoise feedback matrix R. The absolute relative error (RE) is used forevaluation, which is defined as RE(E) = |P E ( R, R)|/P,where E denotes the estimate prediction inaccuracy. The smallerthe RE, the more accurate the estimation. In addition, we provideboth results for using estimated 01 and 10 to estimate P anddirectly using true 01 and 10 to estimate P.",
  "Note: * means statistically significant results (p-value 0.05) using the paired-t-test compared with the best baseline": "Performance Comparison. We compare the proposed methodswith the Naive method , the EIB method , the IPS method ,and the DR-based methods . The RE results are shownin with 01 = 0.2 and 10 = 0.1. First, most debiasing anddenoising methods have lower RE than the Naive method. In addi-tion, our proposed methods stably and significantly outperform thecorresponding debiasing baseline methods. Meanwhile, the OMEmethods are only able to denoising, thus they still have a large RE,even the true is used for estimation. This shows the effectivenessof our method in the presence of both the OME and MNAR effects. In Depth Analysis. We explore the effect of the proportion ofobserved data on the estimation accuracy of 01 and 10, and theresults when 01 = 0.2 and 10 = 0.1 are shown in . We takeROTATE as an example, and the same phenomenon is observed for the other five prediction matrices. First, our methods stably outper-form the baseline methods in all scenarios. Second, the estimationerror decreases significantly as the proportion of observed dataincreases. Moreover, even with the estimation error, our estimationresults are only slightly worse than using the real . This furthervalidates the stability and practicality of our methods.",
  "Real-World Experiments": "Datasets and Experiment Details. We verify the effectiveness ofour methods on three real-world datasets: Coat , Music and KuaiRec . Coat includes 6,960 MNAR ratings and 4,640missing-at-random (MAR) ratings from 290 users to 300 items. Mu-sic has 311,704 MNAR ratings and 54,000 MAR ratings of 15,400users to 1,000 items. For Coat and Music, we binarize the ratings",
  "Related Work6.1Debiased Recommendation": "The data collected in recommender systems are often systematicallysubject to varying types of bias, such as conformity bias , itempopularity bias , latent confounders , and positionbias . In order to achieve unbiased learning of the predic-tion model, three typical approaches have been proposed, includ-ing: (1) The error-imputation-based (EIB) approaches ,which compute an imputed error for each missing rating. (2) Theinverse-propensity-scoring (IPS) approaches , whichinversely weight the prediction error for each observed rating withthe probability of observing that rating. (3) The doubly robust (DR)approaches , which use both the error imputation modeland the propensity model, and the unbiased learning of the pre-diction model can be achieved when either the error imputationmodel or the propensity model is accurate. Based on the aboveEIB, IPS, and DR estimators, in terms of learning paradigms, recentstudies have investigated flexible trade-offs between bias and vari-ance , parameter sharing in multi-task learning ,and the use of a few unbiased ratings to improve the estimation ofthe prediction inaccuracy . In terms of statisti-cal theory, recent studies have developed targeted DR (TDR) and conservative DR (CDR) to combat the inaccurate imputederrors, StableDR to combat sparse data, and new propensityestimation methods based on balancing metrics . Inaddition, proposes to minimize the propensity-independent generalization error bound via adversarial learning. extends DRto multiple robust learning. These methods have also been appliedto sequential recommendation and social recommendation .Furthermore, methods based on information bottlenecks and representation learning have also been proposed fordebiased recommendation. In this work, we extend the previousdebiasing methods to a more realistic RS scenario, in which theobserved ratings and the users true preferences may be different.",
  "Outcome Measurement Error": "Outcome measurement error (OME) refers to the inconsistencybetween the observed outcomes and the true outcomes, also knownas noisy outcomes in social science , biostatistics , andpsychometrics . The error models or transition matrices inOME establish the relation between the true outcomes and theobserved outcomes, including uniform , class-conditional, and instance-dependent structures of outcomemisclassification. Many data-driven error parameter estimationmethods are developed in recent machine learning literature . Meanwhile, given knowledge of measurement errorparameters, recent studies propose unbiased risk minimizationapproaches for learning under noisy labels .Despite the prevalence of OME in real-world recommendationscenarios, there is still only limited work focusing on denoising inRS . Recently, by noticing that noisy feedback typicallyhas large loss values in the early stages, proposes adaptivedenoising training and proposes self-guided denoising learningin implicit feedback. However, the existing methods are mostlyheuristic and require the fully observed noisy labels, which preventsthe direct use of such methods for RS in the presence of missingdata. To fill this gap, we extend the surrogate loss-based methodsto address the data MNAR and OME in parallel.",
  "Conclusion": "In this study, we explored the challenges characterized MNAR datawith noisy feedback encountered in real-world recommendationscenarios. First, we introduced the concept of true prediction inaccu-racy as a more comprehensive evaluation criterion that accounts forOME, and proposed OME-EIB, OME-IPS, and OME-DR estimatorsto provide unbiased estimations of the true prediction inaccuracy.Next, we derived the explicit forms of the biases and the general-ization bounds of the proposed estimators, and proved the doublerobustness of our OME-DR estimator. We further proposed an al-ternating denoise training approach that corrects for MNAR datawith OME. The effectiveness of our methods was validated on bothsemi-synthetic and real-world datasets with varying MNAR andOME rates. The potential limitations includes the usage of weak sep-arability assumption, and accurate measurement error parametersestimation for ensuring unbiasedness. In future research, we aimto develop unbiased learning methods applicable to more complexOME forms, study alternative practical assumptions for identifyingand estimating the error parameters, and extend the applicabilityof the proposed methods to a wider range of RS settings.",
  "Adith Swaminathan and Thorsten Joachims. 2015. The self-normalized estimatorfor counterfactual learning. In NeurIPS": "Brendan Van Rooyen et al. 2015. Machine learning via transitions. (2015). Brendan Van Rooyen, Aditya Menon, and Robert C Williamson. 2015. Learningwith symmetric label noise: The importance of being unhinged. In NeurIPS. Hao Wang, Tai-Wei Chang, Tianqiao Liu, Jianmin Huang, Zhichao Chen, Chao Yu,Ruopeng Li, and Wei Chu. 2022. ESCM2: Entire space counterfactual multi-taskmodel for post-click conversion rate estimation. In SIGIR.",
  "Proof. The proof can be found in Lemma 26.4 of": "Theorem 3.4 (Generalization Bound). Given 01 and 10 with01 + 10 < 1, suppose ( (,),,) is -Lipschitz in (,) forall ,, and , , | ( (,),,)| for all ,, then withprobability 1 , the true prediction inaccuracy P( R, R) of theoptimal prediction matrix using the OME-DR estimator with imputederrors E and learned propensities P has the upper bound"
}