{
  "ABSTRACT": "With the rapid advancements of large language models (LLMs), in-formation retrieval (IR) systems, such as search engines and recom-mender systems, have undergone a significant paradigm shift. Thisevolution, while heralding new opportunities, introduces emergingchallenges, particularly in terms of biases and unfairness, whichmay threaten the information ecosystem. In this paper, we present acomprehensive survey of existing works on emerging and pressingbias and unfairness issues in IR systems when the integration ofLLMs. We first unify bias and unfairness issues as distribution mis-match problems, providing a groundwork for categorizing variousmitigation strategies through distribution alignment. Subsequently,we systematically delve into the specific bias and unfairness issuesarising from three critical stages of LLMs integration into IR sys-tems: data collection, model development, and result evaluation.In doing so, we meticulously review and analyze recent literature,focusing on the definitions, characteristics, and corresponding mit-igation strategies associated with these issues. Finally, we identifyand highlight some open problems and challenges for future work,aiming to inspire researchers and stakeholders in the IR field andbeyond to better understand and mitigate bias and unfairness issuesof IR in this LLM era. We also consistently maintain a GitHub repos-itory for the relevant papers and resources in this rising directionat",
  "Information Retrieval, Large Language Model, Bias, Fairness": "ACM Reference Format:Sunhao Dai, Chen Xu, Shicheng Xu, Liang Pang, Zhenhua Dong and JunXu. 2024. Bias and Unfairness in Information Retrieval Systems: New Chal-lenges in the LLM Era. In Proceedings of the 30th ACM SIGKDD Conferenceon Knowledge Discovery and Data Mining (KDD 24), August 2529, 2024,Barcelona, Spain. ACM, New York, NY, USA, 11 pages.",
  "INTRODUCTION": "Information Retrieval (IR) systems strive to navigate the era ofinformation overload, facilitating users in acquiring informationmore efficiently and effectively . The integration ofLarge Language Models (LLMs) has fundamentally redefined IRsystems, including the introduction of LLM-generated data as newIR data sources, the shift from passive retrieval to proactive genera-tion as core paradigms, and adopting LLMs as results evaluatorsfor IR systems . These advancements, however, bring forthnew challenges in bias and unfairness, affecting the reliability ofIR systems and potentially contributing to societal issues like echochambers and cognitive interference . For in-stance, researchers found that LLMs often retrieve informationthat deviates from facts and is biased towards LLM-generated con-tent . Moreover, LLMs frequently manifest stereo-types and discriminatory content to users and amplify disparitiesbetween items of different socio-economic statuses .Recently, much effort has been made around bias and unfairnessin the context of LLMs and IR systems. However, the literature iscurrently fragmented and often lacks a unified definition of theseconcepts. This ambiguity hampers the development of systematicstrategies to address these issues effectively. To this end, our sur-vey aims to provide a comprehensive and unified perspective thateffectively summarizes the emerging challenges and opportunitiesrelated to bias and unfairness in the intersection between LLMsand IR systems. Generally, both bias and unfairness issues can be",
  "Current Version": ": Overview of three stages of the intersection between LLMs and IR systems. (a) LLMs-generated content as new datasources for IR. (b) Incorporating LLMs to enhance or as IR models. (c) Adopting LLMs as results evaluators in IR systems. regarded as a distribution mismatch problem. Specifically, bias un-derscores the fact that the predicted information lacks objectivityand truthfulness, highlighting the mismatch with the objective tar-get distribution. Unfairness reveals that the predicted informationfails to align with the social values between humans and machines,leading to a mismatch with the subjective target distribution ofhuman values. This perspective not only unifies the nature of theseissues but also streamlines the exploration of mitigation strategies.Our survey begins with a brief overview of how LLMs are in-tegrated into IR systems, setting the stage for understanding theemergence of new bias and unfairness challenges across three piv-otal stages of the IR lifecycle: data collection, model development,and result evaluation. Then we propose a unified perspective onbias and unfairness, categorizing them as distribution mismatchproblems. Based on this unified view, we categorize mitigationstrategies into two principal groups: data sampling, including dataaugmentation and data filtering, and distribution reconstruction,encompassing rebalancing, regularization, and prompting. Follow-ing this taxonomy, we delve into a detailed analysis of several typesof bias and unfairness phenomena that arise with the integrationof LLMs into IR systems, spanning the aforementioned stages. Oursystematic review encompasses a comprehensive examination ofthese issues and their respective mitigation strategies in recent stud-ies, providing a holistic view of the current landscape and guidingfuture efforts in eliminating bias and ensuring fairness for moretrustworthy IR systems.Difference with Existing Surveys. Several recent surveyshave reviewed and discussed the issues of bias and fairness withinIR , primarily focusing on works publishedbefore the advent of LLMs. With LLMs becoming increasingly preva-lent, a new subset of surveys has paid attention tothe bias and fairness challenges presented by LLMs themselves. Ad-ditionally, some other recent surveys have examinedhow integrating LLMs can enhance and transform traditional IRsystems, highlighting some opportunities arising from this integra-tion. Compared to these surveys, our work stands apart by offeringa comprehensive survey of the emerging and pressing issues of biasand fairness at the intersection of IR and LLMs, employing a novelunified perspective to review the cause and mitigation strategies. Summary of Contributions. (1) We provide a novel unifiedperspective for understanding bias and unfairness as distributionmismatch problems, alongside a detailed review of several types ofbias and unfairness arising from integrating LLMs into IR systems.(2) We systematically organize mitigation strategies into two keycategories: data sampling and distribution reconstruction, offeringa comprehensive roadmap for effectively combating bias and unfair-ness with state-of-the-art approaches. (3) We identify the currentchallenges and future directions, providing insights to facilitate thedevelopment of this potential and demanding research area.",
  "Background": "As shown in , the advent of LLMs has reshaped the wholepipeline of IR systems, typically in the following three stages: datacollection, model development, and result evaluation.LLMs-generated content as new data sources for IR. Theemergence of LLMs has significantly accelerated the growth ofArtificial Intelligence Generated Content (AIGC), marking a newera in content creation. Unlike traditional Professional and UserGenerated Content (PGC and UGC) sources, AIGC can be producedautomatically at scale, potentially dominating the content land-scape. . However, AIGC also reshapes the distributionof the IR data, resulting in new concerns about bias and fairness.Incorporating LLMs to enhance or as IR models. The im-pressive emergent capabilities of LLMs in understanding, reasoning,and generalization have motivated significant efforts to integratethem into the development of next-generation IR systems .On one hand, LLMs have been deployed to refine key componentsof traditional IR systems , enhancing their effective-ness and efficiency. On the one hand, beyond enhancing existingframeworks, LLMs also introduce a novel paradigm by acting asgenerative search and recommendation agents ,directly generating responses to fulfill user queries.Adopting LLMs as results evaluators in IR systems. Hu-man evaluation plays a pivotal role in IR systems, particularly in",
  "Bias and Unfairness in Information Retrieval Systems: New Challenges in the LLM EraKDD 24, August 2529, 2024, Barcelona, Spain": "conversational search and recommendation, such as directly assess-ing the quality of responses from generative LLM-based IR mod-els . However, human evaluation comes with significantchallenges, including high costs and a lack of reproducibility. LLMs,with their advanced language modeling and understanding capa-bilities, offer new possibilities for conducting evaluations of thesecomplex tasks without human evaluation . Thisshift not only streamlines the evaluation process but also mitigatesthe substantial costs associated with human evaluations, furtherfacilitating the development of IR systems.",
  "Distribution Alignment Perspective": "While the reshaping of the above IR stages by LLMs has introducednumerous new opportunities, it has also given rise to many newand pressing issues related to bias and unfairness. In this section,we formulate the problems of bias and fairness from a distributionalignment perspective, offering a unified framework for understand-ing these challenges and inspiring mitigation strategies.Formally, when a user interacts with a typical IR system, he/shemay optionally provide his/her personalized information require-ment along with their personalized attributes (typically indicatedas user profile ) to the system. Subsequently, the goal of the IR sys-tem is to retrieve the target information (e.g., documents, items,or advertisements, et al. ) for this user, with the users informa-tion requirements and optional interaction history as the input = {,, }. Let = () be the predicted result either from IRmodels or directly generated by LLMs, where () is the model.Let () be the distribution of predicted results for all users, wecan unify the bias and unfairness problem as a distribution mismatchproblem with the ground truth distribution (), where is thetarget result, which can be defined as follows:",
  "() ().(1)": "Based on the Equation 1, the bias and unfairness problem can beexplained as follows: Bias stems from systemic deviations occurring at variousstages of the IR process, from data collection to model design andevaluation. These systemic issues results in the predicted distri-bution, (), diverging from the target distribution, (), whichideally represents objective and factual realities. Unfairness deeply rooted in cultural and societal notionsof fairness, aims to align the predicted distribution () with asubjective target distribution (). It reflects human values andsocial contracts and evolves with the progress of time.",
  "Taxonomies of Mitigation Strategies": "Based on our unified view, we can further systematically catego-rize mitigation strategies from the view of distribution alignment.Specifically, the goal of mitigation strategies is to align the distri-bution of retrieved information with a target distribution definedby either objective criteria or subjective social values. As shown in, we outline two primary categories and their sub-strategies:(1) Data Sampling focuses on directly modifying the data: Data Augmentation serves as distribution completion, enrich-ing the dataset with additional, often synthetic, data to approximate",
  ": Illustration of different types of mitigation strate-gies from a unified view of distribution alignment": "the target distribution more closely. Techniques such as counterfac-tual imputation and the incorporation of external knowledge areemployed to fill in the gaps in the existing dataset, thus aiming torecover the real distribution more accurately. Data Filtering acts as distribution truncation, selecting datasubsets that align with the target distribution, ensuring the modelsoutput is representative of desired target outcomes. Techniqueslike re-ranking and constrained beam search fall under this cate-gory, serving as post-processing methods to ensure the retaineddistribution segment matches the target distribution.(2) Distribution Reconstruction aims at adjusting the pre-dicted distribution: Rebalancing transforms the predicted distribution throughtechniques like reweighting or resampling, to reflect the targetdistribution more accurately. Common strategies include adjustingthe loss weights for various groups to achieve equilibrium, therebyrealigning the predicted distribution with the target distribution. Regularization narrows the predicted distribution by intro-ducing constraints that encourage the model to learn the target dis-tribution more faithfully. It encompasses both implicit approaches,such as adversarial learning, and explicit ones, like regularizationtechniques, to modify the distribution directly. Prompting extracts the best aligned distribution by directlyemploying specific prompts. This approach guides LLMs to generateoutputs more likely from the target distribution, facilitating analignment with the desired target distribution.Through the lens of distribution alignment, these strategies offera structured and coherent approach to eliminate bias and ensurefairness of IR systems in IR systems. In the following sections, wewill conduct a detailed review of various emerging issues related tobias and unfairness, their mitigation solutions, and their integrationwithin the distribution alignment framework.",
  "In this subsection, we categorize the bias caused by data collectioninto two groups: source bias and factuality bias": "3.1.1Source Bias. Source bias emerges when the incorporation ofLLM-generated content into the corpus of the IR systems: Definition. Information retrieval models tend to rank contentgenerated by LLMs higher than content authored by humans.Specifically, as LLMs fuel the rapid expansion of LLM-generatedcontent, the corpus for IR systems now increasingly encompassesa mix of human-written and LLM-generated texts. Recent stud-ies highlight that modern retrieval models, especiallythose leveraging neural matching techniques, tend to favor LLM-generated content over human-authored content with similar se-mantics. This preference stems from the unique representationsembedded in LLM-generated content, which neural retrieval modelscan capture and thus assign a higher estimated relevancy score . More severely, this source bias is further amplified whenLLM-generated content is included in model training, presentinga challenge to the information ecosystem . Zhou et al. further explores this escalation of source bias in user, data, andrecommender systems feedback loop. Tan et al. further thatthis bias will extend from retrievers to the readers and Chen et al. uncover this bias in the RAG systems.To counteract source bias, recent studies have introduced debi-ased constraints into the training objectives of IR retrieval mod-els . This strategy aims to correct the skewed rele-vancy predictions favoring LLM-generated content, ensuring fairtreatment between human-written and LLM-generated content.By adopting a distribution alignment perspective, such mitigationefforts strive to recalibrate the IR models relevancy distributiontowards an ideal state, where documents are judged equally basedon their semantic content rather than their source. 3.1.2Factuality Bias. As AIGC increasingly becomes a part of thedata sources for IR systems, it inevitably introduces a significantamount of non-factual or hallucinated content. This introductionalters the distribution of IR system data, thereby leading to biasesin the retrieval process. Definition. LLMs may produce content that does not alignwith recognized factual information of the real world.Many studies have shown that LLMs are at risk of generatingfactual errors. For instance, TruthfulQA has highlighted thatlanguage models generate many false answers that mimic popularmisconceptions in question-answering tasks and have the potentialto deceive humans. Besides, merely scaling up models is not promis-ing for improving truthfulness, which means that LLMs still facechallenges in generating factually correct content. Lee et al. show LLMs are susceptible to generating text with nonfactual in anopen-ended generation because of the uniform randomness at ev-ery sampling step. FActScore finds that LLMs lag significantlybehind humans in ensuring the factual consistency of long-form text generation. Other studies reveal that LLMs also exhibitfactual hallucination in natural language inference tasks. In addi-tion to the above studies, many large-scale benchmarks also indicate that LLMs exhibit factuality bias in multi-task andmulti-domain scenarios.Previous studies find that the flawed data source and inferior datautilization are two important causes of factuality bias . Specifi-cally, some low-quality, factual errors, and long-distance repetitionin the training texts harm the factual correctness of the text gener-ated by LLMs . The coverage of knowledge by trainingdata also limits the correctness of LLMs in generating knowledgein some rare or specialized fields . In addition to thetraining data, LLMs usually resort to shortcuts to generate the textsdepending on position close and co-occurred words rather thanunderstand the knowledge itself and always fail to recallthe knowledge that has been memorized .To mitigate factuality bias, in the training of LLMs, some methodsfocus on providing high-quality and factually correct training datafor LLMs . In the inference, previous studies can be dividedinto two categories. One is with the help of an external factualknowledge base such as retrieval-augmented generation . The other is to improve the ability of LLMsthemselves such as Self-Consistency and Dola .",
  "Bias in Model Development": "Incorporating LLMs into IR models introduces inherent random-ness in the generation results, potentially leading to inconsistentoutcomes. In this subsection, we categorize the bias in model de-velopment into four groups: position bias, popularity bias, input-hallucination bias, and context-hallucination bias. 3.2.1Position Bias. Position bias emerges notably in scenarioswhere LLMs are utilized directly as retrieval or recommender sys-tems , characterized by the preference of documents oritems based on their input positions: Definition. LLM-based IR models tend to give preference todocuments or items from specific input positions.Recent works have highlighted that the order of candidate docu-ments or items can significantly impact the performance of LLM-based IR models, while conventional IR models are often not affectedby the changing of input orders . For instance, LLM-based models often show a preference for content positioned at thebeginning or end of a list, neglecting the contributions of items inthe middle. This lost in the middle suggests that these LLM-basedmodels may not fully utilize the context provided by items thatdont occupy prominent positions in the input sequence .There are a number of works on mitigating position bias, and wecan categorize them into three lines based on our distribution align-ment framework. (1) Prompting: This approach involves carefullydesigned prompts to encourage the models to disregard the inputsorder . Nonetheless, due to LLMs prompt sensitivity, this re-quires precise and task-specific prompt engineering across varioustasks and domains. (2) Data Augmentation: This approach has beenexplored in numerous studies, which is a form of data augmentationthat involves random shuffling of candidates followed by aggrega-tion to determine the final ranking. . Forinstance, Tang et al. introduced a permutation self-consistency",
  "Egocentric Bias": "method, offering theoretical guarantees under certain conditions,enabling models to produce and aggregate potential outcomes fromvarious candidate permutations, enhancing result stability. (3) Rebal-ancing: This method counteracts position bias by adjusting the priordistribution sensitive to positions. It recalibrates the models output,addressing the inherent bias towards item positions .While these strategies offer pathways to counteract positionbias, they present challenges, notably the increased computationaldemand associated with processing multiple permutations . Future work should aim to balance effectiveness with effi-ciency to develop more stable and unbiased LLM-based IR systems. 3.2.2Popularity Bias. Popularity bias has been a widely studiesissue in traditional IR models, with extensive research highlightingits effects on the so-called Matthew effect issue . This biasis characterized by the long-tail phenomenon, where a minorityof popular items garners a majority of user interactions. Conse-quently, models trained on such skewed data tend to favor thesepopular items, often over-representing them in results and furtherexacerbating their dominance . However, the advent ofLLM-based IR models introduces new dimensions to the challengeof popularity bias, which can be defined as follows: Definition. LLM-based IR models tend to prioritize candidatedocuments or items with high popularity levels.Unlike conventional models, LLM-based IR models do not merelyreflect the popularity distributions of the target finetuning dataset.They are also inclined to retrieve or recommend items popularin the pre-training corpora of the LLMs . For instance, thevast training data of LLMs, encompassing a wide array of content,means that certain documents or items may be over-represented,influencing the model to prefer these familiar documents or itemsin retrieval and recommendation tasks. As a result, LLM-based IRmodels may not only exhibit the existing popularity bias foundwithin the target finetuning dataset but also introduce a new biasbased on the contents prevalence in their pre-training data. Thisextension of popularity bias in LLM-based IR models presents amore complex problem.To combat popularity bias in LLM-based IR models, existingmethods explore two main solutions: (1) Data Augmentation: Wanget al. proposes two data augmentation strategies to diver-sify the dataset by incorporating more underrepresented content,aiming to balance the final recommendation results. (2) Prompt-ing: Alternative methods involve crafting specific instructions to directly intervene in the LLMs output, such as encouraging an eq-uitable mix of popular and long-tailed items in results. However, addressing this expanded notion of popularity bias inLLM-based IR systems requires new strategies in future work thataccount for both the inherent biases of the training data and theadditional biases introduced by the LLMs pre-training corpora. 3.2.3Instruction-Hallucination Bias. Instruction-hallucination biasemerges when LLMs are used as retrievers, rerankers, or recom-menders but do not fully follow the users instructions: Definition. Content generated by LLM-based IR models maydeviate from the instructions provided by users.Recent studies reveal that LLMs often struggle to adhere fullyto users instructions across various natural language processingtasks, such as dialogue generation , question answering andsummarization . These instructions comprise the usersintent or input task (e.g., reranking a document list) and the specificcontent or object (e.g., the document list) targeted by the task.Deviations from the input task suggest that LLMs may misinterpretthe tasks users intend to execute. For instance, when deployed asrecommenders, LLMs might not grasp users requests for itemswith particular characteristics, leading to recommendations thatdo not match the request . Similarly, contradictions with theinput content reveal that in tasks like reranking, LLMs may produceresults that are inconsistent with, or even absent from, the giveninstructions, showcasing a gap in understanding and fulfilling thespecified requirements .The key to mitigating the instruction-hallucination bias is toenhance the instruction following the ability of large languagemodels. For example, some works propose high-quality instructionfine-tuning datasets such as Natural Instructions , Public Poolof Prompts , and Self-Instruct , etc. Besides, other stud-ies try to further align content generated by LLMs with humanpreferences via reinforcement learning from human feedback . 3.2.4Context-Hallucination Bias. This bias emerges when LLMsare used as recommenders or re-rankers in scenarios with long andrich context, which can be defined as: Definition. LLMs-based IR models may generate content thatis inconsistent with the context.There have been many studies showing that LLMs run the riskof generating content that is inconsistent with the context, espe-cially in scenarios where the context is very long and multi-turn",
  "KDD 24, August 2529, 2024, Barcelona, SpainSunhao Dai, Chen Xu, Shicheng Xu, Liang Pang, Zhenhua Dong, and Jun Xu": "are more prone to generating unfair outcomes for items when com-pared to traditional models . Moreover, certain works have also found that LLMs will also recommend unfair job oppor-tunities to users. The embedding of item unfairness in the modeldevelopment process can contribute to increased item polarizationthrough reinforcement, potentially creating echo chambers thatlimit users exposure to diverse perspectives .Efforts to address item unfairness encompass a range of strate-gies: Zu et al. have employed prompt-based learning to trainGPT-2, leveraging this approach to generate distractors for fill-in-the-blank vocabulary items; some studies propose toutilize decoding strategies to decrease the probability of existing to-kens/items; and Friedrich et al. suggests integrating a fairnessterm into the LLMs-based diffusion process to introduce a shiftingfairness consideration, aimed at generating new items that are lesslikely to contain discriminatory elements. Moreover, Jiang et al. advocates for the re-weighting of different items to effectivelymitigate unfairness in recommendation tasks. Other studies propose some prompt-aware methods to mitigate provider fairness.",
  "When adopting LLMs as result evaluators in IR systems, the follow-ing three types of bias emerge, including selection bias, style bias,and egocentric bias": "3.3.1Selection Bias. A primary challenge in utilizing LLMs as eval-uators is that they are sensitive to the order/ID tokens of candidateresponses, a phenomenon known as selection bias: Definition. LLM-based evaluators may favor the responses atspecific positions or with specific ID tokens.For example, Zheng et al. have demonstrated that gpt-3.5-turbo exhibits a preference for choice C, while llama-30B shows apreference for choice A across various benchmarks. Other workshave revealed a common tendency among LLMs to favor responsespositioned at the first position . Selectionbias may stem from an imbalance in how answers of differentpositions or with distinct ID options are represented in the trainingdata . Further investigation reveals that this bias tendsto amplify when LLM-based evaluators are uncertain about theprediction between the top-ranked choices.To address selection bias, several approaches have been ex-plored: (1) Prompting: Simple prompt-based methods, such as in-corporating few-shot examples or employing chain-of-thought andreference-guided judgment, have been proposed . (2)Data Augmentation: Strategies like position or token switchingaim to eliminate selection bias by diversifying the evaluation con-text . While these methods can enhancethe robustness of evaluations, they are often time-consuming andcostly. (3) Rebalancing: Zheng et al. utilized probability de-composition techniques to estimate the prior distribution of specificpositions or tokens associated with responses, which helps in align-ing evaluations closer to objective standards. Wang et al. have developed a calibration framework that integrates Human-in-the-Loop to calculate balanced position diversity entropy for finalselection. Despite the availability of these strategies, effectively mit-igating selection bias in LLM-based evaluators remains a challenge,requiring further research for more efficient solutions. 3.3.2Style Bias. Style bias can be viewed as a form of aestheticbias, where the appeal of presentation overshadows the substance,leading to a preference for responses that, while polished in appear-ance, might harbor factual inaccuracies: Definition. LLM-based evaluators may favor the responseswith specific styles (e.g., longer responses).For instance, several studies have identified a clear preferencein LLMs for longer responses over shorter ones, emphasizing form over the actual quality of the content . Moreover,Chen et al. have observed an inclination towards content withvisually engaging elements, such as emojis or references, evenwhen such content may include factual errors. Huang et al. suggest that this bias may stem from the training process of LLMs,which emphasizes generating fluent and verbose responses, therebyinadvertently leading them to prefer these characteristics whenemployed as evaluators.Addressing style bias remains challenging, with current strate-gies mainly counteract overemphasis on stylistic features throughprompts. However, these measures are often insufficient, highlight-ing the need for modifications in LLM architecture and trainingapproaches to mitigate this bias effectively in future work. 3.3.3Egocentric Bias. With LLMs being extensively utilized in thedevelopment of IR models, egocentric bias has emerged as a newbias during the automated evaluation conducted by LLMs , which can be defined as follows: Definition. LLM-based evaluators prefer the responses gener-ated by themselves or LLMs from the same family.A recent work has identified that language model-drivenevaluation metrics, such as BARTScore , T5Score , andGPTScore , inherently favor texts produced by their underlyingLMs, especially in summarization tasks. Liu et al. and Zhenget al. further highlighted that when acting as evaluators, LLMsdemonstrate a clear bias towards outputs generated by themselvesover those from other models or human contributors. This biascould stem from that the LLM may share the same for both themodel development phase and result evaluation phase .The emergence of egocentric bias introduces the risk of self-reinforcement for LLMs, particularly when they are further trainedusing rewards from LLM-based evaluations. This scenario can leadto LLMs overfitting to their own evaluation criteria, intensifyingself-preference in next-generation model . Current strategiesfor mitigating egocentric bias primarily involve employing diverseLLMs as evaluators to foster peer discussions , thereby reducingthe preference for any specific LLM and enhancing the robustnessof evaluation outcomes. However, this strategy inevitably increasesthe evaluation costs. Future research must explore more efficientsolutions to ensure fair and unbiased evaluation.",
  "Fairness Concepts": "Sociological researches acknowledge multiple cultural variationsin perceptions of fairness . In IR systems, achieving fair-ness often entails ensuring that the retrieved documents or recom-mended items align with cultural values , including princi-ples such as gender equality , addressing disadvan-tages , and avoiding discriminatory language .Researchers have revealed that various multi-stakeholders in-volved in IR systems , such as users and items, often havedistinct perspectives on fairness considerations. In IR, user fair-ness and item fairness are often associated with two sociologicalconcepts: equality and distributive justice .",
  "Item Unfairness": "User Fairness. Everyone should be treated the same and pro-vided the same resources to succeed. This implies that the IR sys-tems should deliver equitable and non-discriminatory informationservices to different users. Item Fairness. The resources should be equally distributedbased on needs. This implies that the IR systems should afford moreopportunities (e.g., exposures) to weaker items, striving to equalizethe opportunities across diverse items.",
  "In this section, we will elucidate the underlying causes of unfairnessin the data collection process and subsequently outline currentmitigation strategies to address these issues": "4.2.1User Unfairness. In the context of user unfairness, one pri-mary cause stems from the existing taxonomic, discriminatory, andoffensive content in the training data, disproportionately affect-ing specific groups. The inclusion of these contents within theexisting material can be attributed to historical and cultural rea-sons , or they may be generated by LLMs . Forexample, when training LLMs, it is common to encounter discrimi-natory content . Discrimination against certain groupscan also stem from unbalanced data collection, where the insuf-ficient representation of diverse perspectives leads to unfair out-comes . The presence of unbalanced data can contribute tothe perpetuation of historical and cultural stereotypes or systematic influences .Previous works have employed various methods to mitigateunfairness during data collection by redistributing the existingdocument corpus. Specifically, some work cre-ate matched pairs (e.g., male or female) to ensure a more equi-table dataset and other methods add non-toxic exam-ples for groups. Other approaches suggest using down-weighting samples containing social group or discriminated infor-mation. Moreover, other studies propose to filter outdiscriminated or taxonomic content from web-scale datasets. Fi-nally, instruction fine-tuning or RLHF has also been shown to beeffective in promoting fairness . 4.2.2Item Unfairness. For item unfairness, one primary cause islikely unbalanced data collection, where the insufficient represen-tation of certain items leads to disparities in the IR process .Another reason raised is that LLMs cannot only retrieve existingitems but also generate new items, contributing to the potentialintroduction of novel content and perspectives .However, these newly generated items may still encounter discrim-ination issues . To mitigate item unfairness during data collection, several stud-ies have developed methods to generate non-discriminatory items.For instance, Zou et al. and Rathod et al. suggest usingspecific templates to enrich training data with a variety of safe andequitable question-answer pairs, thereby improving LLM-based IRmodels training. Guenole et al. introduce pseudo-item dis-crimination techniques for filtering out non-discriminated items.Additionally, other studies advocate for employing fairness-aware prompts to produce newly non-discriminatory items. Fur-thermore, Jiang et al. proposes to re-weight different itemsamples to enhance item fairness.",
  "In this section, we will analyze the causes of unfairness in the modeldevelopment phase and explore mitigating strategies to addressand minimize these disparities": "4.3.1User Unfairness. When adapting LLMs as information re-trievers, researchers have observed that the extensive knowledgegained during pre-training may introduce risks of user unfair-ness , highlighting the need for careful consid-eration and mitigation strategies in deploying such models. Stud-ies have shown that utilizing explicit user-sensitiveattributes like gender or race in LLMs may lead to the generationof discriminated recommendation results or unfair answers to spe-cific questions. Moreover, it has been observed that LLMs can learnimplicit attributes, such as user names and email addresses, andutilize them to generate discriminated content .To address user unfairness in the model development phase, priorresearch has suggested mitigating unfairness during the fine-tuningprocess. Some studies investigate how different intersectionalprompts affect recommendation fairness and UP5 proposeto conduct prompt tuning to get an effective fair-aware prompt. Hanet al. , Zayed et al. proposes to set different weights of lossfor different samples containing discriminated content. Meanwhile,various studies explore the incorporationof a fairness-aware regularizer to assist LLMs-based models ingenerating more equitable content. Wang et al. proposesto remove unfair information from LLMs-based embeddings bygenerating adversarial examples. There are also some works that recommend employing a filtering-list approach orcomparing model outputs with safe samples to proactively preventthe inclusion of discriminatory words and enhance the fairness ofthe generated content.",
  "Unfairness in Result Evaluation": "To assess the fairness performance of IR models effectively, it isessential to measure the distributions of social values, which repre-sent the fairness objectives inherent in the evaluation process .However, human evaluation demands significant labor resources.Therefore, recently, LLMs have been employed to simulate humanor real systems to facilitate evaluation processes efficiently. 4.4.1User Unfairness. Similarly, user unfairness typically ariseswhen LLMs-based evaluators fail to accurately simulate the be-haviors exhibited by real humans. For example, Zhang et al. propose leveraging psychological knowledge to assess the simu-lated human ability of LLMs, but they discover that LLMs frequentlyexhibit certain group behavior towards certain human groups.To enhance the capability of LLMs as fair evaluators for IR sys-tems, previous studies have devised several methods. Approachesinclude designing or learning specific prompts informed by psy-chological insights to better simulate diverse human groups , augmenting training data with additional humanpersonality information to refine LLMs as evaluators , andadopting innovative techniques like the unsupervised constructedpersonalized lexicon (UBPL) to manipulate their individual char-acteristics . Furthermore, Bai et al. proposes four stages toidentifying discrimination patterns in queries. 4.4.2Item Unfairness. In evaluating item unfairness, when turningfrom discriminant style to generation style, a primary concern arisesfrom attributing credit to the generated items ,as achieving item fairness necessitates tracing this credit back tothe item provider for a comprehensive assessment.To tackle these challenges, several studies have de-veloped IR agents that use world knowledge to fairly distribute itemcredits, aiming to reduce biases and enhance fairness. Shi et al. employs the MIN-K% Prob technique to check whether an itemexists in the training data. Meanwhile, Akyrek et al. , Grosseet al. leverage influence functions and embedding similarities",
  "CONCLUSION AND FUTURE DIRECTIONS": "This survey has delved into the emergence of new bias and un-fairness challenges within IR systems in this LLM era. We haveestablished a unified framework to understand these issues as dis-tribution mismatch problems and systematically categorized miti-gation strategies into data sampling and distribution reconstructionapproaches. Through an in-depth review of fifteen types of biasand unfairness, along with their corresponding mitigation strate-gies, we provide a comprehensive overview of the current progress.Despite the considerable attention given to this topic, we identifysome important problems for further exploration.Biases and Unfairness in IR Feedback Loops. In real IRsystems, the interaction between users, models, and informationforms feedback loops that impact each other over time. These loopscan significantly shape user perceptions and preferences basedon the information they are exposed to. This interaction, in turn,influences the training data, creating a cycle that may reinforceexisting biases and unfairness. Novel strategies to interrupt thefeedback loops are essential for mitigating these issues.Unified Mitigation Framework. Current methods primarilyaddress individual instances of bias or unfairness, but in the future,we should consider unified solutions. This is because various typesof bias and unfairness are not isolated. Presenting a unified frame-work can facilitate a deeper understanding of these relationships,enabling methods for addressing different types of bias and unfair-ness to complement each other. Our proposed unified perspectiveoffers a potential direction to address these issues simultaneously.Theoretical Analysis and Guarantees. The current explo-ration of bias and unfairness within the intersection between LLMsand IR systems has predominantly been through empirical studies.However, there is a critical need for robust theoretical analysis toaugment these empirical findings. Future efforts should focus ondeveloping more rigorous analytical frameworks.Better Benchmarks and Evaluation. Most benchmarks cur-rently utilized to study bias and unfairness within simulated envi-ronments. There is a crucial need for collecting large-scale, real-world datasets to enhance the evaluations and broaden researchhorizons. Additionally, as LLMs increasingly draw upon existingonline data to train subsequent generations, dynamic benchmarksare needed to be constructed. Consequently, future work can focuson exploring a systematic evaluation protocol for different bias andunfairness issues. This work was funded by the National Key R&D Program of China(2023YFA1008704), the National Natural Science Foundation ofChina (No. 62377044, No.62276248), Beijing Key Laboratory of BigData Management and Analysis Methods, Major Innovation & Plan-ning Interdisciplinary Platform for the Double-First Class Initia-tive, PCC@RUC, funds for building world-class universities (disci-plines) of Renmin University of China, and the Youth InnovationPromotion Association CAS under Grants No.2023111."
}