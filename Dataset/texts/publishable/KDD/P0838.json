{
  "ABSTRACT": "This paper introduces a recurrent neural network approach for pre-dicting user lifetime value in Software as a Service (SaaS) applica-tions. The approach accounts for three connected time dimensions.These dimensions are the user cohort (the date the user joined),user age-in-system (the time since the user joined the service) andthe calendar date the user is an age-in-system (i.e., contemporane-ous information). The recurrent neural networks use a multi-cellarchitecture, where each cell resembles a long short-term memoryneural network. The approach is applied to predicting both acqui-sition (new users) and rolling (existing user) lifetime values for avariety of time horizons. It is found to significantly improve medianabsolute percent error versus light gradient boost models and BuyUntil You Die models.",
  "INTRODUCTION": "Services without fixed fees, such as Software as a Service (SaaS)models (e.g., free-to-play gaming), represent a significant globalindustry. In SaaS, it is crucial to evaluate user value to calculatethe return on advertising spend (ROAS) as a predictive functionof user acquisition and user engagement. Direct measurement ofROAS, while straightforward, is impractical as it may take months/ years to realize the revenue of a user but marketing decisionshave to be made sooner. Unlike traditional products with a definedprice, in the software as a service model, value is determined bythe user, not the business. Thus it is a common practice to predictthe value of a user given their early interactions with the service.Creating these predictions can be non-trivial. This work explores aRecurrent Neural Network (RNN) - based approach to predictinguser value.Predicting user value has two unique challenges. First the uservalue matures over time. That is, if a prediction is needed for totalvalue at age-in-system at each time the prediction must be updatedand the uncertainty decreased. Second there are three connectedtime dimensions in addition to user level features. These dimensionsare the user cohort (the time the user onboarded to the service),user age-in-system (the time since the user joined the service) andthe date of the prediction (contemporaneous information; i.e., thestate of the service at the time of prediction).",
  "=0+1, 1": "Let 0 be the forecast age-in-system, 1, . . . , as forecast hori-zons, and 0+1,0+ and as the rolling LTV with horizon. Typicalforecast horizons include 1 week, 1 month, 1 quarter, 1 half yearetc. The forecast may use any information that is available to theforecaster up to the date 0, including 0, 01, and so on.While in principle the data can be arranged in a columnar matrix,it is more easily understandable in wide format. gives anexample of simulated data. The diagonal striped pattern highlightsthat each cohort achieves an age-in-system at a different date. Fig-ure 1 shows the daily value for an expanded simulated data setboth as a function of age-in-system and date. The event (green) andoutage (blue) data are distributed over age-in-system values buthave only one date value each.Thus to accurately predict the value of users it necessary to ac-count for both cohort effects (i.e, users that have been onboarded at",
  ": The daily user value as a function of age-in-system(left), and as a function of observation date (right). The colorsindicate the system status (blue is an outage, green is anevent)": "the same date tend to have similar characteristics, and consequentlysimilar monetization patterns), the contemporaneous effects (theevents of a given date / time have an impact for all users active atthat time), the age-in-system at which the prediction is to be made,and the potentially user level features (these can and in general dochange with age-in-system). It should be noted that while it willlikely never be possible to forecast contemporaneous effects (e.g., itis extremely difficult to know that a non scheduled system outageis going to occur at a given time ) it is still necessary to model themto ensure that the cohort and age effects are correctly accountedfor.",
  "RELATED WORK": "Traditional approaches to predicting non-contractual user levelcustomer values rely on recency, frequency and monetary value(RFM) from the users past history to extrapolate future purchas-ing behaviors. A prominent model family in this class is a set ofparametric generative models aptly named Buy Till You Die (BTYD) . The BTYD approach breaks the customer value forecastinto 3 separate modeling objects, the transaction frequency, thetransaction amount, and the duration of staying active, and modelseach independently by different distributions with user level hetero-geneous parameters following common prior distributions acrossall the users, and finally combine them to calculate the forecastvalue. While the approach is elegant and parsimonious, there are a number of drawbacks that lead to worse performance and limitedscope in practical situations. First, the assumption that the trans-action amount, the transaction frequency, and the active durationare independent from each other rarely holds in reality. Frequentcustomers typically are more satisfying customers or have betterintent in using the service and they stay with the service longer.Second, it is hard to incorporate useful features, such as seasonality,into a BTYD model, as the model itself is a pure vintage basedmodel. Thirdly, for new users with less transactions, the modeldoes not have enough data to discriminate between potentiallyhigh value users and low value ones. Finally, if there are severallines of business, e.g. Uber Ride and Uber Drive, each has to bemodeled independently, with a loss of available information. There-fore a more expressive model form is often necessary to leverage awider range of signals available to the forecaster.Supervised machine learning models that directly use the cus-tomer value as the target are the natural candidates to remedy theaforementioned issues with the BTYD models. Popular choices in-clude regularized regression models, random forest and gradientboosting trees. Along with a flexible model form with no distribu-tion assumptions, they need a large amount of features to increaseforecast accuracy. On the other hand, these models inherently as-sume independent observations. Therefore to model the customervalue process, a time series object, one needs to heavily rely on fea-ture engineering to capture the temporal dependency. Specifically,one often needs to build out features at different lags explicitly forsuch models. This may over inflate the feature space and run intoruntime performance issues.",
  "METHOD4.1The Recurrent Neural Network Approach": "This work proposes the use of a Recurrent Neural Network (RNN)with Long Short-Term Memory (LSTM)-style cells. For the acqui-sition model, the target variables represent the sum of user dailyvalues (UV) from their current age-in-system to graduation.Thatis, for every new day, part of the total UV is actualized during thejourney of the user from 0 days age-in-system to 90 days age-in-system. Hence, the unknown part that needs to be predicted arethe UV residuals; i.e., the amount of UVs that are not actualizedyet. For the rolling model the target is the sum of a users next UVfor a specified number (varying over use-case) of weeks.",
  "Architecture": "Generally, an RNN uses a number of cells, in our case these arestandard LSTMs or more advanced LSTM-like cells. It is easy tocode an RNN that can plug-in various cells, while keeping the sameoverall architecture, for example one depicted on .It is usually advantageous to use dilated RNNs In those net-works, cells use not just the most recent state, but delayed one, by2 or more steps, like in layers 2, 3, 5, and 6 in . Finally, auseful trick is to use Res-net like shortcuts between groups ofcells, called here blocks.",
  "Predicting Customer Lifetime Value Using Recurrent Neural Net": ": An example RNN composed with 6 layers(cells),in two blocks, with the Res-net style shortcut over blocknumber 2. Additionally, some layers are dilated, and thereis a final linear adaptor layer that converts the top-mostcell output into the needed size. Each dot represents a cell orinput or output in a particular time step. multilayer dilated RNN. As in the dRNN output is split intoreal output , that goes to the next layer, and a controlling output, that is an input to the gating mechanism in following time steps.This last feature (the controlling output) deals with an importantlimitation of the LSTM, and most other cells: the output at step isreused at step + (where >= 1) as a controlling input. But thereis no reason to assume it is optimal for these two vectors to be thesame.The cell uses two states, c-state (also called cell state), which isclose to standard LSTM or GRU state, and -state, which is the con-trolling state. At each time step, the whole input is a concatenationof , 1, , where is a standard input at a time (eitherfrom a previous layer or an input to the RNN), 1 is the mostrecent -state, and is the delayed state ( >= 2). The cell alsouses a fusion gate to create a weighted combination of previousand delayed -states. It tends to be more accurate than standardLSTM and GRU cells on forecasting tasks .",
  "Data PreProcessing": "When forecasting at a user level, apart from standard steps of datapreparation, normalization etc., there is also the issue of data spar-sity. In particular, most users purchase infrequently. Looking backone or several weeks, it can be common to find no transactions.The data set is skewed for such zero-records. To account for thissubsampling of zero-records is used. The process is illustrated in.The subsampling causes uneven RNN steps, to compensate forthis calendar features, such as week of the year and number ofweeks since the last step are added.",
  "RESULTS": "The RNN architecture is applied to two problems: acquisition LTV(used at Uber) and rolling LTV (used at Meta). As these problemshave different data and use cases different metrics are used to evalu-ate each case. For acquisition LTV root mean squared error is used.For rolling LTV a modified symmetric median percent error is used.In both cases the RNN models are evaluated against several othermodels; these models are different for both use cases.",
  "Acquisition LTV (Uber)": "The Acquisition LTV was evaluated using root mean squared error(RMSE). Two RNNs with 2 LSTM cells are used. First a model thatused only convensional features is tested (the model is denoted2-cell RNN). This model is not found to fit the data as well as anxgboost model. Second a 2-cell RNN model was fit with the aditionof embedding type features (encodings of cities) is tried (denoted2-cell embedding). The 2-cell embedding model preforms betterthan the xgboost model. The details are shown in Tab. 2.",
  "Rolling LTV (Meta)": "We implement the Rolling LTV forecast model for the Meta Questusers. The Meta Quest is the consumer virtual reality (VR) ecosys-tem where users make app/in-app purchases from hundreds ofapps for a diverse VR experience. We forecast gross revenues fromapp/in-app purchases at the user level with 1/4/13/26 week forward-looking horizons. The data consists of user level time series up to 4years long, with 100+ features including demographical, behavioral,and seasonal variables. Feature embedding is also used to reducethe dimension for large categorical variables.For Meta Quest user level Rolling LTV forecast, an issue withstandard MAPE as the accuracy metric is the prevalence of zeroactual values. The relative error size measured by MAPE with zeroor close-to-zero actual values is by definition large even for anerror of small absolute size. This is not meaningful in most usecases. Therefore to avoid over penalizing small errors when theactual values are zero or close-to-zero, we propose a modificationof SMAPE, adjusted SMAPE (aSMAPE), for a given forecast horizon,",
  "max{,,} + ,,": "where is the total number of users in the forecast. Here is a usecase dependent dollar amount where we set a floor on the absoluteerror for the corresponding relative error metric. For example, ifwe set at $1.00, with 10% aSMAPE we treat any absolute errorunder $0.10 no worse than $0.10. The use of SMAPE over MAPEhas an added benefit that SMAPE is bounded, which is importantfor a user level accuracy metric.A comparison of out-of-sample aSMAPE errors between 3 modelson a set of Meta Quest users is shown in . Here we set = $1.00. We find that nRNN model consistently outperform BTYDand LGB models.",
  "CONCLUSION": "The article discusses an RNN approach to model customer lifetimevalue. It further demonstrates the architecture and technical detailson how a variant of neural-net forecasting architecture can beapplied in LTV modeling practically. The proposed method providesbetter forecast accuracy and flexibility, compared to the traditionalmethod such as BTYD and a tree-based approach based on thestudies conducted in two types of LTV model from Uber TechnologyInc. and Meta.",
  "Shiyu Chang, Yang Zhang, Wei Han, Mo Yu, Xiaoxiao Guo, Wei Tan, Xi-aodong Cui, Michael Witbrock, Mark A Hasegawa-Johnson, and Thomas S": "Huang. 2017.Dilated Recurrent Neural Networks. In Advances in Neu-ral Information Processing Systems, I. Guyon, U. Von Luxburg, S. Ben-gio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.),Vol. 30. Curran Associates, Inc. Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau,Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning PhraseRepresentations using RNN Encoder-Decoder for Statistical Machine Translation.(2014). DOI:"
}