{
  "ABSTRACT": "Lifelong sequential modeling (LSM) has significantly advanced rec-ommendation systems on social media platforms. Diverging fromsingle-domain LSM, cross-domain LSM involves modeling lifelongbehavior sequences from a source domain to a different target do-main. In this paper, we propose the Lifelong Cross Network (LCN),a novel approach for cross-domain LSM. LCN features a Cross Rep-resentation Production (CRP) module that utilizes contrastive lossto improve the learning of item embeddings, effectively bridgingitems across domains. This is important for enhancing the retrievalof relevant items in cross-domain lifelong sequences. Furthermore,we propose the Lifelong Attention Pyramid (LAP) module, whichcontains three cascading attention levels. By adding an intermedi-ate level and integrating the results from all three levels, the LAPmodule can capture a broad spectrum of user interests and ensuregradient propagation throughout the sequence. The proposed LAPcan also achieve remarkable consistency across attention levels,making it possible to further narrow the candidate item pool of thetop level. This allows for the use of advanced attention techniquesto effectively mitigate the impact of the noise in cross-domain se-quences and improve the non-linearity of the representation, allwhile maintaining computational efficiency. Extensive experimentsconducted on both a public dataset and an industrial dataset fromthe WeChat Channels platform reveal that the LCN outperformscurrent methods in terms of prediction accuracy and online perfor-mance metrics.",
  "INTRODUCTION": "Click-through rate (CTR) prediction stands as a fundamental taskin many real-world applications. The precision of CTR predictionheavily relies on comprehending the users intentions towards thepotential candidates. In recent years, deep neural networks (DNNs)have made significant strides in improving the accuracy of CTRprediction. They accomplish this by extracting representations ofuser interests from behavior sequences, specifically in relation tothe candidate items .Nowadays, social media platforms such as TikTok, YouTube, andWeChat Channels, presents billions of items to users every day.The complexity of user interactions on these platforms has signifi-cantly increased, posing new challenges in modeling user behaviorsequences. On one hand, the volume of data has expanded exponen-tially, with some sequences extending to a length of lifelong. Onthe other hand, users often engage with a variety of content types,leading to behavior sequences that contains items from differentdomains. This complexity becomes particularly pronounced whenfocusing on CTR prediction for smaller-scale domains.In order to overcome the lack of direct user data in the targetdomain, it becomes imperative for models to extract user interestsfrom behavior sequences in an auxiliary source domain. For in-stance, as illustrated in 1, within WeChat Channels, the medianbehavior sequence length for live content is a mere 500, whichis a fraction of the length for video content. Therefore, for livecontent CTR prediction, it is imperative to employ cross-domainlifelong sequential modeling (LSM) of video item interactions. Thisapproach ensures comprehensive prediction that captures the usersintegrated interests across the platform.A common approach to managing lifelong sequences involvessegmenting the modeling into two units: the General Search Unit(GSU) and the Exact Search Unit (ESU) . The GSUs role is tosift through the sequence to identify items that are most relevantto the candidate items. Subsequently, the ESU is responsible forextracting user interest representations from the items identified",
  "KDD 24, 2024, ConferenceRuijie and Zhaoyang, et al": "Guorui Zhou, Chengru Song, Xiaoqiang Zhu, Ying Fan, Han Zhu, Xiao Ma,Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep Interest Network forClick-Through Rate Prediction. arXiv:1706.06978 [stat.ML] Yongchun Zhu, Kaikai Ge, Fuzhen Zhuang, Ruobing Xie, Dongbo Xi, Xu Zhang,Leyu Lin, and Qing He. 2021. Transfer-Meta Framework for Cross-domainRecommendation to Cold-Start Users. In Proceedings of the 44th International",
  "RELATED WORK": "The modeling of user behavior sequences plays a central role inunderstanding user intentions, and extensive research has been ded-icated to this field. It has been observed that models tend to achievebetter performance when longer sequence lengths are incorporated. With the exponential growth of data, the concept of lifelongsequential modeling (LSM) has emerged, aiming to extract user in-terests with respect to the candidate items over extensive sequencelengths. One notable approach addressing LSM is SIM , whichdivides the process into a General Search Unit (GSU) and an ExactSearch Unit (ESU), thereby managing lifelong sequences with re-duced computational demands. Subsequent research has built uponthis framework, leading to notable advancements . How-ever, these methods have not fully addressed the unique challengesassociated with cross-domain LSM, and may experience perfor-mance declines due to insufficient transfer from the source to thetarget domain.There are a diverse array of studies contributing to the fieldof cross-domain recommendation (CDR). A significant body of re-search advocates for the training of independent models withina source domain . Additionally, the variational auto-encoder (VAE) framework has been employed in several work tolearn domain-invariant embeddings . Another approachinvolves the integration of Meta Networks . Sequential mod-eling has also seen the incorporation of CDR techniques. Notably,PSJNet and -Net have adopted gating mechanisms toachieve this integration. In parallel, DA-GCN and C2DSR have leveraged Graph Neural Networks (GNNs), and RecGURU has utilized Transformer architectures. However, most of thesestudies focus on short sequence modeling, which limit their appli-cability to LSM due to the inherent differences in sequence length.Contrastive learning has seen significant advancements in thefields of computer vision and natural language processing . It has also been adapted to enhance the Click-ThroughRate (CTR) prediction tasks . In thecontext of sequential modeling, MISS utilizes contrastive learn-ing to refine interest representation, while AQCL introducesan auxiliary loss to learn item relationships from sparse trainingsamples. CL4CTR aims to enhance item representation qualitythrough contrastive learning, yet it requires the management of alarge embedding table and triplets during training, which may beimpractical in real-world applications. In this paper, we harness theprinciples of contrastive learning to impose additional supervision,enabling the model to learn item embeddings that can facilitate thebridging of items across different domains.",
  "PRELIMINARIES": "Cross-domain LSM involves modeling sequences of user behaviorsfrom one domain (the source) and applying results gained to adifferent domain (the target). The objective of cross-domain LSM isto extract a representation of user interests based on their lifelongsequences in the source domain, which can then be used to enhancethe accuracy of click-through rate (CTR) predictions in the targetdomain. A key aspect of this scenario is that the same user base isactive in both domains, yet there is no item overlap between them.Specifically, to formalize the modeling, we consider three distinctcategories of features for each user:",
  "METHODOLOGY": "In this paper, we propose a novel approach, the Lifelong CrossNetwork (LCN) for cross-domain LSM. The LCN is comprised oftwo major components: the Cross Representation Production (CRP)module and the Lifelong Attention Pyramid (LAP) module.The CRP module is a jointly trained sub-network with the ob-jective of learning item embeddings that can bridge items acrossdomains. These embeddings are then utilized within the main net-work to enhance its ability to identify the most relevant items withrespect to a given candidate item from a lifelong sequence in thesource domain. The LAP module is structured with three levels ofcascading attentions, each processing the top ranked items of theprevious level. The progressive nature of the LAP module ensuresa more context-aware extraction of interest representations thatare directly relevant to the target item.We show an overview of the proposed LCN in .",
  "Cross Representation Production": "A lifelong behavior sequence records a users interactions overan extended period. When considering a particular item, only afraction of this sequence may hold predictive value for the usersclick-through rate (CTR) on that item. This is particularly true forcross-domain lifelong sequences, where the items originate from adifferent domain than the target item. It is crucial for the model to identify the most relevant items within the sequence to optimizemodel capacity and computational efficiency.Typically, Lifelong Sequence Modeling (LSM) is segmented intotwo units: a General Search Unit (GSU) and an Exact Search Unit(ESU). The role of GSU is to sift through the lifelong sequence andidentify the items most relevant to the candidate item. Its effec-tiveness highly depends on the quality of the item embeddingsutilized. Previous approaches re-use item embeddings learned dur-ing training of the model, which are proved to perform well withinthe training datas distribution. However, when the candidate itemand the sequence items belong to different domains, the embed-dings must be developed to bridge the gap between source andtarget domains. Achieving this is non-trivial, given that the modelis primarily trained on the target domain data.To overcome this challenge, we propose a Cross RepresentationProduction (CRP) module that jointly refines cross-domain itemembeddings along with the main network. Drawing inspirationfrom contrastive learning, the CRP module pairs positive and nega-tive examples from users short-term behavior sequences and. It then imposes extra supervision on the item embeddings toenhance learning of relationships between items across domains. 4.1.1Positive and Negative Sampling. The construction of sam-ple pairs is fundamental to contrastive learning. Within our CRPmodule, we achieve this by selecting items from users short-termbehavior sequences. This selection process is grounded in the un-derstanding that a users interests tend to remain stable acrossdifferent domains, especially within a short-term period of time.Consequently, items within a users short-term behavior sequenceare likely to exhibit similar characteristics, regardless of their do-main of origin. This consistency enables us to uniformly sampleboth positive and negative pairs across domains.As depicted in , we have designed three distinct typesof positive pairs for each user. To begin with, we select two items,1 and 2, from the target domains short-term sequence ,forming a positive pair within the target domain. Similarly, wechoose two items, 1 and 2, from the source domains short-term sequence , creating a source domain positive pair. Thesepairs are intended to encourage the model to bring item embeddingsof similar items closer within their respective domains. To learnalignment between similar items across domains, we construct across-domain positive pair by selecting one item each from and , denoted as 1 and 2.Regarding negative sample pairs, we employ a parallel approachto the positive sampling, but with items from different users behav-ior sequences within the same training batch. This results in threetypes of negative sample pairs: <1, 2>, <1, 2> and<1, 2>. For each type, we sample pairs per training batch,which are then used as the negative counterparts for all positivepairs within that batch. 4.1.2Loss Function. The CRP module employs a contrastive lossfunction designed to enforce the model to minimize the cosinedistance between item embeddings for positive pairs within a givenbatch. Each type of positive pairs is associated with a correspondingloss function as follows:",
  "Lifelong Attention Pyramid": "Classic LSM framework segments interest extraction into two units:the General Searching Unit (GSU) and the Exact Searching Unit(ESU), with the latter generally employing more complex attentiontechniques than the former. Although the GSU stage significantlynarrows down the item pool for the ESU stage, the attention utilizedin ESU are often less sophisticated than state-of-the-art (SOTA)attention techniques to maintain computational efficiency.In the context of cross-domain LSM, items from the source do-main in the behavior sequence and the candidate items from thetarget domain do not overlap, and the contextual and behavioralpatterns can vary greatly between domains. Consequently, cross-domain lifelong sequences may introduce more noise, calling forbetter consistency across different searching stages and the use ofmore advanced attention techniques to interpret the sequences.To address these challenges, we propose the Lifelong AttentionPyramid (LAP) module. This module extends the traditional two-stage framework into a three-level attention pyramid, featuringcascading levels of attention that aim to refine and streamline thesearch process within the lifelong sequence. By achieving betterconsistency across levels, the LAP reduces the number of itemsprogressing to the top level. This reduction allows for the appli-cation of more advanced attention techniques at the top level tofilter noise and enhance the non-linearity of the representation.We detail each of the levels, named the Complete-Scope Attention(CSA), the Median-Scope Attention (MSA), and the Focused-ScopeAttention (FSA), in the following sections. 4.2.1The Complete-Scope Attention (CSA). As the first level of at-tention, the Complete-Scope Attention (CSA) mirrors the functionof the GSU from previous frameworks. Within the CSA, a broadyet general search is executed across the entire lifelong sequence.The objective is to ensure that every item within the sequenceis accounted for, thereby excluding the least relevant items fromsubsequent levels. Meanwhile, the CSA can provide a preliminary",
  "Cross-Domain LifeLong Sequential Modeling for Online Click-Through Rate PredictionKDD 24, 2024, Conference": "The results of experiments on public and industrial datasetsreveal that the proposed LCN significantly improves the modelscapacity to handle cross-domain LSM. The results also indicate theadaptability of the CRP and LAP modules for their effectivenesswhen integrated with different LSM backbones or applied to LSMwithin a single domain. As for future work, we aim to refine the in-tegration of the CRP module with the main network and investigateenhancements to the LAP modules inter-level connections. Martn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al.2016. Tensorflow: Large-scale machine learning on heterogeneous distributedsystems. arXiv preprint arXiv:1603.04467 (2016). Jiangxia Cao, Xin Cong, Jiawei Sheng, Tingwen Liu, and Bin Wang. 2022. Con-trastive Cross-Domain Sequential Recommendation. In Proceedings of the 31stACM International Conference on Information & Knowledge Management. 138147.",
  "struct a reduced sub-sequence1, which is subsequently fedinto the second level of attention": "4.2.2The Median-Scope Attention (MSA). In contrast to previ-ous two-stage methods, our approach introduces an intermediateMedian-Scope Attention (MSA) level to effectively bridge the firstand third levels. This addition is inspired by the observation thata significant majority of attention scores, often more than 90%,tend to concentrate on a mere 20% of the items in a sequence. Byincorporating the MSA, we distribute some of the CSAs functionof omitting less relevant items to this intermediate level, enhancingthe overall consistency of the LAP module and further narrowingthe item pool for the final attention level.In the MSA, we incorporate additional contextual informationabout the items to perform a secondary attention search. It is impor-tant to note that the contextual details used here, such as interactionspecifics like viewing duration, are consistent with the informationthat will be utilized in the final level. This consistency is crucial, asthese contextual elements have been shown to significantly benefitsequential modeling .Formally, let1 = {1,2, ...,1} represents the contex-tual information of the items provided by the CSA. The attentionscore at this level is calculated using the following formulas:",
  "=1 ( ( || ))(10)": "where is the projection matrix.Following the first level, we form a sub-sequence2 by se-lecting the top-2 items according to their rankings. Theseselected items will then serve as the input for the final level. 4.2.3The Focused-Scope Attention (FSA). The objective of the finalattention level mirrors that of the ESU, aiming to deliver a detailedand targeted interest representation with respect to the candidateitem, based on the most relevant items filtered through the previousattention levels. The Focused-Scope Attention (FSA) benefits from asmaller set of items, enabling the use of a more advanced attentiontechnique to increase the non-linearity of the representation.We employ an attention similar to the decoder of the multi-headtransformer to extract interests from various perspectives.Formally, for a candidate item and the sub-sequence2, theoutput of the -th head in the multi-head attention is computed as:",
  "=1()(13)": "At the end of the LAP module, we integrate the three interest rep-resentations, , and , from each attention levelto generate a representation that encapsulates a broad spectrum ofuser interests as reflected in the lifelong sequence. This integrationalso ensures full gradient propagation throughout the sequence,potentially enhancing the consistency between the different atten-tion levels. The implications and benefits of this approach will befurther explored and discussed in the experimental section.The entire LCN, including the CRP module, is designed for end-to-end training. The final loss function for LCN is an combinationof the CTR loss in Equation 2 and the CRP loss in Equation 6:",
  "EXPERIMENTS": "To assess the performance of the proposed LCN, we carried outextensive experiments and compared it against previous methodsusing both a public dataset and an industrial dataset. Additionally,we implemented online A/B testing to further validate the effec-tiveness of LCN. The details of our experimental setup, along withthe results and discussions, are introduced in this section.",
  "SDIM : Introduces a novel Hash Attention technique tointegrate GSU with ESU": "TWIN : An approach that employs dimension compres-sion of cross features to enhance the consistency betweenGSU and ESU.For training these models, we adhered to the parameter configu-rations recommended in their respective original publications ortheir open-source implementations. 5.1.3Metrics. For offline evaluation, we employed the followingthree key metrics: Area Under the Curve (AUC), Grouped AreaUnder the Curve (GAUC) and the value of the Logarithmic Loss(logloss) for the CTR prediction tasks.For online A/B testing, the evaluation was standardized by usingthe CTR and stay time of users on the items presented. We alsomonitored the inference latency as a secondary metric. 5.1.4Parameter Settings. The core architecture of our networkconsists of a straightforward two-layer feed-forward neural net-work. For the public dataset, the dimensions of the layers are con-figured to 256 and 128, while for the industrial dataset, we scaleup to 512 and 256 to accommodate its larger size. The user profilefeatures {} comprises profile information such as age, gender,and income level. We standardized the maximum length of theshort-term behavior sequence to 50 for both datasets. All features,including item and user ids, are assigned distinct embedding spaces.",
  "Module Analyses": "We detail experiments designed to assess the efficiency of the twomajor components of LCN. These experiments are carried out onthe industrial dataset, which is more appropriate for our topic givenits substantial data volume and extended sequence length. 5.2.1Analyses of CRP. For the CRP module, we performed anablation study alongside an embedding analysis to quantitativelyand qualitatively assess its impact. Furthermore, we demonstratethe effectiveness of the CRP module when being incorporated withprevious LSM frameworks.Ablation Study. The outcomes of the experiments are presentedin . In the experiment where the contrastive loss for thecross-domain sample pairs L was omitted, the process of sam-pling cross-domain positive and negative pairs was also excluded.Additionally, in the case where no loss components were applied,the CRP module was entirely removed.The results reveal that even without L, the CRP module con-tributes to a considerable enhancement over the initial baseline.This improvement suggests that the supervision provided by thecontrastive loss aids the model in more effectively capturing the re-lationships between items within the embeddings, thereby refiningthe search quality in the GSU.The best performance was achieved when all three contrastivelosses were employed. This underscores the importance of incorpo-rating an additional contrastive loss to learn the similarities betweenitems across the source and target domains, which is crucial forcross-domain LSM. It appears that training the model solely withthe target domains CTR loss is insufficient for aligning embeddingsacross different domains. This misalignment is a primary factorin the degraded performance of previous LSM methods in cross-domain LSM, as the GSU struggles to deliver stable and precisesearch results without embeddings that encapsulate cross-domaininformation. The CRP module addresses this, significantly improvethe efficiency of cross-domain LSM.Representation Quality. To provide a clearer insight into thequality improvement of the learned item embeddings, we visualizedthe clustering of item embeddings, as depicted in . Specif-ically, we employed the T-SNE technique to distill the firsttwo principal components from the learned embeddings, treatingthese values as coordinates in a two-dimensional space. By plot-ting these coordinates and color them according to item categories,",
  "TWIN0.72120.63550.2348TWIN0.72650.63910.2259": "we created a visual map that illustrates the embeddings ability todifferentiate between item categories, which is a direct measure ofthe embeddings capacity to pair similar items.We can observe that while embeddings learned without the CRPmodule can cluster items in the target domain with a quality com-parable to those learned with the CRP module, there is a markeddifference in the source domain, where embeddings refined withthe CRP module exhibit superior clustering. This suggests that theadditional contrastive loss introduced by the CRP module signifi-cantly enhances the quality of the embeddings in the source domain.This enhancement is a key factor in the performance improvementsachieved by implementing the CRP module.Backbone Substitution. We extended our evaluation of theCRP module by integrating it into multiple LSM frameworks. Inthese experiments, the CRP module was utilized to refine itemembeddings, which were then re-used in the GSU and ESU of therespective frameworks. The results are summarized in .The results indicate that the CRP module consistently enhancesthe performance of all tested frameworks in managing cross-domainLSM. This improvement is largely attributed to the superior qual-ity of the item embeddings generated for search purposes. Theseembeddings are proven to be beneficial regardless of variations inthe underlying framework architecture.It is important to note that in all experiments, the CRP modulewas trained jointly along with the main network. This means thatwhile CRP introduces additional losses during the training phase,it does not impose any extra computational cost during inference.",
  "SIM Soft0.70360.64950.2669ETA0.70300.64850.2692SDIM0.70250.64850.2688TWIN0.70440.65060.2664LCN / CRP0.70760.65310.2648": "of the MSA level, which effectively bridges the CSA and FSA. Itis also worth noting that a significant enhancement was achievedwhen integrating (concatenating) the results from all three attentionlevels as the final output of LAP. This is mainly because that theintegration facilitates gradient flow throughout the sequence, whichis crucial for improving consistency across different levels.We further conducted a set of experiments to assess the influenceof 1 on consistency. The results, depicted in -(b), revealthat consistency is augmented with larger values of 1 and tendsto plateau once 1 exceed a quarter of the maximum sequencelength, corroborating the model performance trends observed inthe previous section. This indicates that the interplay between MSAand FSA is highly consistent, owing to the contextual informationincluded in MSA. This also highlights the value of MSA, whichis key to maintain consistency across levels while considerablynarrowing the item pool needed in the FSA.Adaptability Test. We conducted experiments to assess theadaptability of the proposed LAP in the context of single-domainLSM. For these experiments, we omitted CRP module and relied onuser interactions within the original source domain (video content)as the training labels. The results are summarized in .The results indicate that LAP continues to outperform previ-ous methods in the context of single-domain LSM, though with asmaller margin of improvement compared to cross-domain LSM.This reduced gain is likely due to the relative ease of modelinglifelong sequences within a single domain, where the data distribu-tion and behavioral patterns among items in the sequence and thecandidate items tend to be more consistent.",
  "LCN-200-50 / CRP0.72370.63790.23300.60980.57860.1619LCN / LAP0.72660.64010.22960.61280.58080.1615LCN-200-500.72940.64230.22230.61430.58200.1610LCN-500-1000.72970.64250.22200.61450.58200.1610": "Across all metrics and datasets, the proposed LCN consistentlyoutperforms the other methods. It is important to note, however,that the margin of improvement on the public dataset is narrowerthan that on the industrial dataset. This is largely due to the publicdatasets smaller sequence lengths and data volume. The perfor-mance difference on the industrial dataset is likely a more accuratereflection of the models true generalization capabilities and itseffectiveness in real-world recommendation scenarios.",
  "Online Testing": "To further validate the efficiency of the proposed LCN, we con-ducted an online A/B test to assess the quality of its recommen-dations. We utilized LCN-200-50 to maintain a balance betweencomputational efficiency and performance. Over a seven-day pe-riod, we collected user feedback to calculate online metrics. Theresults were significant, with group B achieving a relative increaseof +2.93% in CTR and +3.27% in stay time. Moreover, the inferencelatency for group B was only 3 ms longer than that of group A,which is a negligible trade-off considering the substantial gains inuser experience delivered.",
  "CONCLUSIONS": "In this paper, we propose the Lifelong Cross Network (LCN) forcross-domain lifelong sequential modeling (LSM). LCN is composedof two components: the Cross Representation Production (CRP)module and the Lifelong Attention Pyramid (LAP) module. TheCRP module is a sub-network that is supervised by contrastive lossto learn item embeddings that can bridge similar items across do-mains. The LAP module is structured with three levels of cascadingattentions to extract interest representations from the lifelong se-quence with respect to the candidate item. By integrating these twomodules, the proposed LCN can effectively identify relevant itemsacross domains, achieving a highly consistent and computationalefficient interest extraction process within the lifelong sequence inthe source and improving the CTR prediction in the target domain.",
  "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding.arXiv:1810.04805 [cs.CL]": "Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of trainingdeep feedforward neural networks. In Proceedings of the thirteenth internationalconference on artificial intelligence and statistics. JMLR Workshop and ConferenceProceedings, 249256. Wei Guo, Can Zhang, Zhicheng He, Jiarui Qin, Huifeng Guo, Bo Chen, RuimingTang, Xiuqiang He, and Rui Zhang. 2022. MISS: Multi-Interest Self-SupervisedLearning Framework for Click-Through Rate Prediction. arXiv:2111.15068 [cs.IR] Guangneng Hu, Yu Zhang, and Qiang Yang. 2018. CoNet: Collaborative CrossNetworks for Cross-Domain Recommendation. In Proceedings of the 27th ACMInternational Conference on Information and Knowledge Management (CIKM 18).ACM.",
  "Diederik P. Kingma and Jimmy Ba. 2017. Adam: A Method for Stochastic Opti-mization. arXiv:1412.6980 [cs.LG]": "Chenglin Li, Mingjun Zhao, Huanming Zhang, Chenyun Yu, Lei Cheng, Guo-qiang Shu, BeiBei Kong, and Di Niu. 2022. RecGURU: Adversarial Learning ofGeneralized User Representations for Cross-Domain Recommendation. In Pro-ceedings of the Fifteenth ACM International Conference on Web Search and DataMining (WSDM 22). ACM. Meng Liu, Jianjun Li, Guohui Li, and Peng Pan. 2020. Cross domain recom-mendation via bi-directional transfer graph collaborative filtering networks. InProceedings of the 29th ACM international conference on information & knowledgemanagement. 885894.",
  "Weiming Liu, Xiaolin Zheng, Mengling Hu, and Chaochao Chen. 2022. ExploitingVariational Domain-Invariant User Embedding for Partially Overlapped CrossDomain Recommendation. arXiv:2205.06440 [cs.IR]": "Jianxin Ma, Chang Zhou, Hongxia Yang, Peng Cui, Xin Wang, and Wenwu Zhu.2020. Disentangled self-supervision in sequential recommenders. In Proceedingsof the 26th ACM SIGKDD International Conference on Knowledge Discovery & DataMining. 483491. Muyang Ma, Pengjie Ren, Yujie Lin, Zhumin Chen, Jun Ma, and Maarten de Rijke.2019. -Net: A Parallel Information-Sharing Network for Shared-Account Cross-Domain Sequential Recommendations. In Proceedings of the 42nd InternationalACM SIGIR Conference on Research and Development in Information Retrieval(Paris, France) (SIGIR19). Association for Computing Machinery, New York, NY,USA, 685694. Wentao Ouyang, Xiuwu Zhang, Lei Zhao, Jinmei Luo, Yu Zhang, Heng Zou,Zhaojie Liu, and Yanlong Du. 2021. Minet: Mixed interest network for cross-domain click-through rate prediction. In CIKM. 26692676.",
  "Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, andAlexei A. Efros. 2016.Context Encoders: Feature Learning by Inpainting.arXiv:1604.07379 [cs.CV]": "Qi Pi, Weijie Bian, Guorui Zhou, Xiaoqiang Zhu, and Kun Gai. 2019. Practiceon Long Sequential User Behavior Modeling for Click-Through Rate Prediction.In Proceedings of the 25th ACM SIGKDD International Conference on KnowledgeDiscovery &amp; Data Mining (KDD 19). ACM. Pi Qi, Xiaoqiang Zhu, Guorui Zhou, Yujing Zhang, Zhe Wang, LejianRen, Ying Fan, and Kun Gai. 2020.Search-based User Interest Modelingwith Lifelong Sequential Behavior Data for Click-Through Rate Prediction.arXiv:2006.05639 [cs.IR] Jiarui Qin, Weinan Zhang, Xin Wu, Jiarui Jin, Yuchen Fang, and Yong Yu. 2020.User Behavior Retrieval for Click-Through Rate Prediction. In Proceedings ofthe 43rd International ACM SIGIR Conference on Research and Development inInformation Retrieval (SIGIR 20). ACM. Aghiles Salah, Thanh Binh Tran, and Hady Lauw. 2021. Towards Source-AlignedVariational Models for Cross-Domain Recommendation. In Proceedings of the 15thACM Conference on Recommender Systems (Amsterdam, Netherlands) (RecSys21). Association for Computing Machinery, New York, NY, USA, 176186. Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang.2019. BERT4Rec: Sequential Recommendation with Bidirectional Encoder Repre-sentations from Transformer. arXiv:1904.06690 [cs.IR] Wenchao Sun, Muyang Ma, Pengjie Ren, Yujie Lin, Zhumin Chen, ZhaochunRen, Jun Ma, and Maarten de Rijke. 2021. Parallel Split-Join Networks for Shared-account Cross-domain Sequential Recommendations. arXiv:1910.02448 [cs.IR]",
  "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. Attention Is AllYou Need. arXiv:1706.03762 [cs.CL]": "Fangye Wang, Yingxu Wang, Dongsheng Li, Hansu Gu, Tun Lu, Peng Zhang, andNing Gu. 2023. CL4CTR: A Contrastive Learning Framework for CTR Prediction.In Proceedings of the Sixteenth ACM International Conference on Web Search andData Mining. 805813. Ruoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong,and Ed Chi. 2021. DCN V2: Improved Deep &amp; Cross Network and PracticalLessons for Web-scale Learning to Rank Systems. In Proceedings of the WebConference 2021 (WWW 21). ACM."
}