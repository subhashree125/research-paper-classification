{
  "ABSTRACT": "Scale-calibrated ranking systems are ubiquitous in real-world ap-plications nowadays, which pursue accurate ranking quality andcalibrated probabilistic predictions simultaneously. For instance, inthe advertising ranking system, the predicted click-through rate(CTR) is utilized for ranking and required to be calibrated for thedownstream cost-per-click ads bidding. Recently, multi-objectivebased methods have been wildly adopted as a standard approachfor Calibrated Ranking, which incorporates the combination of twoloss functions: a pointwise loss that focuses on calibrated abso-lute values and a ranking loss that emphasizes relative orderings.However, when applied to industrial online applications, existingmulti-objective CR approaches still suffer from two crucial limita-tions. First, previous methods need to aggregate the full candidatelist within a single mini-batch to compute the ranking loss. Suchaggregation strategy violates extensive data shuffling which haslong been proven beneficial for preventing overfitting, and thus de-grades the training effectiveness. Second, existing multi-objectivemethods apply the two inherently conflicting loss functions ona single probabilistic prediction, which results in a sub-optimaltrade-off between calibration and ranking.To tackle the two limitations, we propose a Self-Boosted frame-work for Calibrated Ranking (SBCR). In SBCR, the predicted rankingscores by the online deployed model are dumped into context fea-tures. With these additional context features, each single item canperceive the overall distribution of scores in the whole ranking list,so that the ranking loss can be constructed without the need forsample aggregation. As the deployed model is a few versions olderthan the training model, the dumped predictions reveal what wasfailed to learn and keep boosting the model to correct previouslymis-predicted items. Moreover, a calibration module is introducedto decouple the point loss and ranking loss. The two losses areapplied before and after the calibration module separately, which",
  "Corresponding Author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08...$15.00 elegantly addresses the sub-optimal trade-off problem. We con-duct comprehensive experiments on industrial scale datasets andonline A/B tests, demonstrating that SBCR can achieve advancedperformance on both calibration and ranking. Our method has beendeployed on the video search system of Kuaishou, and results in sig-nificant performance improvements on CTR and the total amountof time users spend on Kuaishou.",
  "INTRODUCTION": "As one of the most popular video-sharing apps in China, Kuaishoustrongly relies on its leading personalized ranking system, whichserves hundreds of millions of users with fingertip connection tobillions of attractive videos. Once the ranking system receives arequest from a user (aka. query in literature), it will predict a rank-ing score for each of the retrieved candidate videos (aka. items,documents). These ranking scores are not only used in sorting thecandidate items to fit the users personalized interest, but also es-sential for many downstream applications. For example, we use theclick-through rate (CTR) to guide ads bidding and the probabilityof effectively watching to estimate the video quality. This suggeststhat industrial ranking systems should emphasize two matters si-multaneously: 1). the relative orders between scores, namely theranking quality evaluated by GAUC and NDCG , and 2). theaccurate absolute values of scores which should be calibrated tosome actual likelihood when mapped to probabilistic predictions.To meet this practical demand in industrial ranking systems,there have been emerging studies on a paradigm known as Cali-brated Ranking (CR) . The standard approach of CR usuallyincorporates a multi-objective loss function: a pointwise loss thatfocuses on calibrated absolute values and a ranking loss that em-phasizes relative orderings. Sculley combines regression and",
  "KDD 24, August 2529, 2024, Barcelona, SpainZhang and Liu, et al": "where I ( ) indicates whether lies inside the-th interval. NamelyI ( ) = 1 if [0.01, 0.01 ( +1)) and 0 otherwise. And , {0, ..., 100} are the function values at all interval ends. Obviously,0 = 0 and 100 = 1. And the other 99 which control the functionproperty will be adjusted during modeling training.Second, to keep the knowledge learnt previously from pairwiseloss, our calibration module should preserve the relative orders ofitems under the same query. Namely, given any two predictions, < ,, the calibrated probability cali,, cali,,. We thusrequire the piece-wise linear function to be non-decreasing, i.e.,0 1 ... 100. And the parameters 1, ...,99 are learnt onlyfrom the features of the current query, such as the user features(user id, gender, age, behavior sequences) and context features(timestamp, page index, date). So Eq. 11 only includes as the inputbut excludes any item features in . The learning process is defined:",
  "-0.25%": ": The performance comparison of different datashuffling strategies. Evaluation metrics include: Logloss,NDCG@10 (widely used in ranking), and the total amount oftime users spend on Kuaishou (the most important metricin our online A/B test). Extensive item-level data shuffling(upper) significantly outperforms the query-level data shuf-fling (bottom) where the whole candidate item list retrievedfor a single request is aggregated in a single mini-batch. The-oretical explanation will be discussed in Sec 3.2.2. This ex-perimental result validates the advantage of extensive datashuffling and motivates us to propose a novel ranking lossthat enables extensive shuffling. 1 We highlight the two limitations of conventional multi-objectiveCR approaches in industrial online applications, namely,the contradiction with extensive data shuffling and the sub-optimal trade-off between calibration and ranking. We propose a novel SBCR framework that successfully ad-dresses the two limitations. In SBCR, ranking quality is em-phasized without the need for data aggregating, and the twoobjectives are decoupled to avoid the conflict. We validate SBCR on the video search system of Kuaishou.Extensive offline experiments show that our method canachieve advanced performance on both calibration and rank-ing. In online A/B tests, SBCR also outperforms the strongproduction baseline and brings significant improvements onCTR and the total amount of time users spend on Kuaishou.SBCR has now been deployed on the video search system ofKuaishou, serving the main traffic of hundreds of millionsof active users.",
  "RELATED WORK": "We mainly focus on the related work concerning these aspects: CTRPrediction, Learning-to-Rank (LTR), and Calibrated Ranking.CTR Prediction aims to predict a users personalized click ten-dency, which is crucial for nowadays information systems. In thelast decades, the field of CTR prediction has evolved from traditionalshallow models, e.g. Logistic Regression (LR), Gradient BoostingDecision Tree (GBDT) , to deep neural models e.g. Wide &Deep , DCN . Most researches are dedicated to improvingmodel architectures: Wide & Deep and DeepFM combinelow-order and high-order features to improve model expressive-ness, and DCN replace FM of DeepFM with Cross Network.DIN and SIM employ the attention mechanism to extractuser interest. Despite recent progress, the loss function is still notwell-explored and the dominant pointwise LogLoss cant wellsatisfy the ranking quality highly desired in practice .",
  "A Self-boosted Framework for Calibrated RankingKDD 24, August 2529, 2024, Barcelona, Spain": ": The comparison of SOTAs on the production dataset. The best results are highlighted in bold and the second-bestare underlined. * indicates significant improvement with p-value=0.05 than previous best-performing Point + ListCE. Shuffledenotes compatibility with extensive sample-level data shuffling. SBCR outperforms all due to 2 key advantages: compatibilitywith extensive data shuffling and effective structure to decouple ranking and calibration losses.",
  "Preliminaries": "The aim of Calibrated Ranking is to predict ranking scoresfor a list of candidate items (or documents, objects) properly givena certain query (request with user, context features). Besides therelative order between the ranking scores emphasized by conven-tional LTR , CR also focuses on the calibrated absolute values ofscores simultaneously. To be specific, ranking scores should notonly improve ranking metrics, such as GAUC and NDCG , but",
  "the function value of an interval endL_self boost loss_self boost loss for one sample": "also be scale-calibrated to some actual likelihood when mapped toprobabilistic predictions.Formally, our goal is to learn a scoring function : Q X R,given a training dataset D consisting of a list of samples (,,).Here, we denote Q as the query space, Q as a query , X asthe item feature space, X as a candidate item, and Y as aground-truth label under specific business settings. For example,in our video search system at Kuaishou, can indicate whether avideo is clicked, liked, or watched effectively (beyond a predefinedtime threshold). Without loss of generality, we assume the labelspace Y = {1, 0} throughout this paper.The model first predicts a ranking score (also known as logit),",
  "(,,)D log , + (1 ) log(1 ,),(3)": "which is shown to be calibrated since the minima is achievedat the point , = E[|,].The ranking loss is usually defined on pairs of training samples.We denote D+ = {|(,,) D, = 1}, D = {|(,,) D, = 0} as the set of positive and negative items associate to thesame query . And the pairwise loss is defined to promote a largemargin between items across the two sets,",
  "D+, Dlog(, ,),(4)": "where the outer average is taken over all queries in D 2, denotesthe number of unique queries, and the inner average is taken overpairs inside each query. Although achieving high ranking quality,this pairwise loss suffers from the miscalibration problem due totranslation-invariant .To combine the advantages of both, a multi-objective loss isdefined asL = L + (1 )L,(5) where (0, 1) controls the trade-off between the quality ofcalibration and ranking.Note that besides the pairwise loss, there are many other widelyused ranking losses, such as listwise softmax and listwiseApproxNDCG . Similarly, these listwise losses also sacrifice cali-bration for ranking performance, and could be used as a componentin multi-objective CR. We skip the discussion for conciseness. 3.2.2Limitations of Existing Multi-Objective CR. Despite beingextensively studied, existing multi-objective CR approaches stillsuffer from two crucial limitations: contradiction to extensive datashuffling and sub-optimal trade-off between calibration and rankingabilities.In the default setting of most industrial training systems, samplesare first extensively shuffled, and then divided into mini-batches forsequential gradient descent. This shuffling simulates the Indepen-dent and Identically Distribution (IID), and consequently preventsthe overfitting problem caused by aggregated similar samples in-side each mini-batch. We show the performance gain from datashuffling in our experiments in .While in multi-objective CR, extensive data shuffling is not ap-plicable due to the definition of the pairwise loss. Specifically, tocalculate (), we need to go through all positive-negative pairsassociated with , indicating that all samples with the same query",
  "We slightly abuse the notation for conciseness of Eq. 4: D represents any uniquequery in the training dataset, namely, {| (,, ) D}": "have to be gathered inside a single mini-batch. As a result, in train-ing pairwise (or listwise) losses, data can only be shuffled at querylevel, not sample level. This aggregation of similar samples (un-der the same query, with identical context, user features) insideeach mini-batch contradicts the IID assumption and thus heavilydegrades the performance gain from data shuffling.Another limitation of conventional CR lies in the trade-off natureof the multi-objective loss. To be specific, we introduce the followingtheorem.",
  "Ultimately, we have derived an infeasible equation, ,1 +,2 =0, indicating that the two losses have distinct optimal solution": "Intuitively, L emphasizes the relative order of items, whilefailing to predict the absolute probabilistic value accurately. AndL vice versa. In Eq 5, however, the two inherently conflictinglosses are applied on the prediction ,, and thus the best trade-offmay be sub-optimal for both calibration and ranking. In addition,this trade-off also sensitively depends on , leading to a challenginghyper-parameters choosing issue. In contrast, a better design woulddecouple the ranking and calibration losses with separated networkstructures and gradient cut-off strategies, which elegantly addressesthe objective conflict.",
  "Self-Boosted Calibrated Ranking": "To address the two limitations, our proposed SBCR mainly consistsof two modules: 1). a self-boosted ranking module (SBR) that en-ables extensive data shuffling, while achieving high ranking qualitylike what conventional pairwise loss does, and 2). an auxiliary cali-bration module that decouples the ranking and calibration lossesand thus successfully addresses the sub-optimal trade-off problem.We describe our model architecture in . 3.3.1The Self-Boosted Ranking Module. As mentioned earlier, ()is calculated on all possible positive-negative item pairs associatedwith , making it inseparable for sample-level data shuffling. Toaddress this issue, we propose a novel loss function,",
  "+": ": The architecture of the proposed Self-Boosted framework for Calibrated Ranking. Middle: SBCR consists of twomodules: a self-boosted ranking module (SBR) trained by a multi-objective loss (pointwise and self-boosted pairwise loss) and acalibration module. Left: the details of the proposed self-boosted pairwise loss. Using dumped ranking scores s from the onlinedeployed model, we enable both comparisons between samples associated with the same query and extensive sample-level datashuffling. Right: the proposed calibration module that decouples the ranking and calibration objectives to avoid the conflict.",
  "Since only one items feature is used as the input, it isimpossible to run the network forward and backward for theother items associated with": "We now dig into the details of _ that solves this conflict.Our system mainly consists of two parts: 1). an online Server thatreceives a users request, makes real-time responses by scoring andranking candidate items, and dumps logs including the users feed-back. and 2). a near-line Trainer that sequentially trains the scoringfunction (, ) on latest logs. For every few minutes, the Servercontinuously loads the latest scoring function from the Trainer. Wedenote the deployed model on the Server as (, ), which would bea bit older than the training model (, ).Formally, when the Server receives a query and candidates[1, ...,], it",
  "(3) finally dumps the scores s R and labels y R intocontext features of the current query": "Although only adding negligible cost, s and y actually providerich knowledge on the score distribution to enhance the modelsranking ability. More importantly, the dumped scores from an oldermodel reveal what the Trainer failed to learn and further directthe following update to pay extra attention to the previously mis-predicted samples. We thus term this Self-Boosted.",
  "log( s ,)I(y ),(10)": "where the indicator I() = 1 if > 0 and 0 otherwise. We slightlyabuse notations by broadcasting scalars to fit vectors dimensions.Remarks: Similar to , our _ also compares itemsunder the same query and encourages large margins. It is differentthat the self-boost mechanism keeps lifting the model performanceby focusing on previously missed knowledge. We take the firstterm in _ for example. After masked by the indicator, itpromotes the margins between the positive items score , and itsnegative peers dumped scores s. If the items under were poorlypredicted previously, the unmasked elements in s would be larger,resulting in a larger loss value. To achieve a satisfactory margin,, would be lifted more aggressively in the backpropagation. 3.3.2The Calibration Module. The probabilistic prediction trainedusing multi-objective CR (Eq. 5) usually suffers from the sub-optimaltrade-off between calibration and ranking ability.To address this issue, we propose a calibration module to de-couple the two losses. The pairwise loss and point loss are appliedbefore and after the calibration module separately, which elegantlyaddresses the objective conflicting problem.The calibration module maps into a calibrated one:",
  "cali = ( ;),(11)": "where cali (0, 1) is the calibrated probability and is the pro-posed calibration module. We make several considerations in thedesign of ():First, to be flexible enough to capture various functional distribu-tions, () is set as a continuous piece-wise linear function. Withoutloss of generality, we partition the function domain (0, 1) into 100equal-width intervals and set () to be a linear function inside eachinterval:",
  "(14)": "3.3.3The Overall Architecture of SBCR and Training Tricks. Ourmodel consists of two networks, as summarized in Fig 2.The first deep network defines the scoring function (,) withtwo groups of inputs: 1). , features shared by samples under thesame query, including user features, long-term user behaviors andcontext features; 2). , features of a specific item. In the industrialranking system of Kuaishou, we adopt QIN as (,) due toits SOTA performance. This network is trained using,",
  "L_ = L + (1 )L_ .(15)": "We replace the conventional pairwise loss in Eq. 5 by our proposedself-boosted pairwise loss, which enables sample-level shuffling.Our second deep network defines the function for calculating theinterval heights a in Eq. 13, which is trained by Eq. 14. In our system,the network consists of 4 FC layers of size (255, 127, 127, 100).To avoid a sub-optimal trade-off between calibration and ranking,we further stop the gradient back propagation for all inputs to thecalibration module (i.e., ,). Thus (,) focuses on the rankingquality and the calibration module only deals with calibration. Wewill discuss more on parameter sensitivity in our experiments.",
  "EXPERIMENTS": "In this section, to validate the effectiveness of our proposed SBCRframework, we compare SBCR with many state-of-the-art CR al-gorithms. We also provide an in-depth analysis to investigate theimpact of each building block in SBCR. Experiments are conductedbased on the Kuaishou video search system, including both offlineevaluations on the billion-scale production dataset, and online A/B",
  "Experiment Setup": "4.1.1Datasets. In our offline experiments, all compared algo-rithms are initialized from the same checkpoint, trained onlineusing 5 days data, and then frozen to test on the 6th days data. Thedataset is collected from the user log on the video search system ofKuaishou and the statistics of the dataset are shown in Tab. 2. Ourmethod is designed for the online training systems that are widelydeployed in industrial scenarios, so we only conduct experimentson the real production dataset. 4.1.2Implementation Details. In our experiments, we adopt QIN as our architecture for all compared methods. With efficient userbehavior modeling, QIN is a strong production baseline latest de-ployed on the KuaiShou video search system. For feature engineer-ing, ID features are converted to dense embeddings and concate-nated with numerical features. All models are trained and testedusing the same optimization settings, i.e., Adam optimizer, learningrate of 0.001, and batch size of 512. All models are trained with oneepoch following Zhang et al. , which is widely adopted in theproduction practice. For the relative ranking weight (1 )/, wechose the best one from [0.01, 0.1, 1.0, 10, 100] for each comparedalgorithm and report the performance of them in their own optimalhyper-parameter settings for a fair comparison. For our proposedSBCR, we simply set the relative weight to 1 consistently across allexperiments. 4.1.3Evaluation Metrics. In this work, we consider both rankingand calibration performances. For evaluating the ranking perfor-mance, we choose NDCG@10 and GAUC (Group AUC). NDCG@10is consistent with other metrics like NDCG@5. GAUC is widelyemployed to assess the ranking performance of items associatedwith the same query, and has demonstrated consistent with onlineperformance in previous studies . GAUC is computed by:",
  "(17)": "where I is defined in the same way as Eq. 12. Among the threecalibration metrics, LogLoss provides sample-level measurement,whereas ECE and PCOC provide subset level and dataset level mea-surement, respectively. There are some other metrics like Cal-N andGC-N and here we mainly follow the setting of calibrated rank-ing for a fair comparison. Lower LogLoss or ECE indicatesbetter performance, and PCOC is desired to be close to 1.0.These five metrics serve as reliable indicators for evaluating bothranking and calibration abilities. 4.1.4Compared Methods. As in , we include several im-portant baseline methods for a comprehensive comparison. Thesebaseline methods are divided into two groups based on whetherthe loss function is single-objective or multi-objective. The Single-objective group consists of these four methods:",
  "Main Experimental Results": "The main experimental results are shown in . The methodsare compared on both ranking and calibration metrics. From theresults, we have the following observations:First, in the single-objective group, pointwise achieves the bestcalibration performance but inferior ranking performance. In con-trast, RankNet and ListNet outperform the Pointwise model on theranking ability, at the expense of being completely uncalibrated.Among these methods, ListCE can reach a tradeoff for it makesregression compatible with ranking, but it still suffers from poorcalibration. This validates the necessity of calibrated ranking.Second, the multi-objective methods incorporate the two lossesand achieve a better trade-off. And several recent studies (JRCand Point + ListCE) have achieved encouraging progress on bothranking and calibration metrics. Note that LogLoss is a stronger cali-bration metric than PCOC and ECE. For example, if a model predicts",
  "PointAggre0.72790.56140.97590.0057+ PairShuf (SBR)0.73150.56141.01470.0056": "averaged prediction over the whole dataset, the model achievesperfect PCOC and ECE but poor LogLoss. Considering LogLossas the main calibration metric, all compared multi-objective meth-ods still suffer from sub-optimal trade-off between calibration andranking, when compared to the best Single-objective methods, vali-dating our second motivation. When comparing the three variantsof , we find that cal-softmax achieves slightly better NDCGbut suffers from unacceptable worst calibration performance, whilemulti-objective gets the best trade-off between ranking and calibra-tion. Our observation are also consistent with the previous resultsreported in the original paper.Third, our proposed SBCR outperforms all compared algorithmson both ranking and calibration performance. This validates thetwo key advantages of SBCR, i.e., compatibility with extensive data-shuffling and effective structure to avoid trade-off between rankingand calibration. Specifically, SBCR is trained with the dumped pre-dictions by the online deployed model, making it the only methodthat needs no sample aggregation when computing the rankingloss. And a calibration module is introduced to decouple the pointloss and ranking loss to address the sub-optimal trade-off problem.The gain of the two key advantages will be further analyzed insection 4.3.1 and 4.3.2.",
  "Ablation Study and Analysis": "We conduct ablation studies to investigate the contribution of eachSBCR building block: the self-boosted pairwise loss to address thedata-shuffling problem, and the calibration module to address thetrade-off problem. We also include hyper-parameter analysis toshow the sensitivity. 4.3.1Analysis on Data-shuffling. As mentioned, extensive data-shuffling that simulates the Independent and Identical Distribu-tion has long been proven beneficial in preventing the overfittingproblem. We re-validate the performance gain of data-shuffling onboth point loss and point + pair loss. For point + pair loss, Point +RankNet is used as the aggregation method and our SBR (Eq.15) is used as the shuffling method, results shown in Tab. 4.Shuffling achieves consistent improvement over sample aggre-gation, which validates that extensive data-shuffling is essentialfor performance and supports our first motivation. Conventionalmulti-objective CR algorithms require the aggregation of the wholecandidate list in a single mini-batch for computing the ranking loss,",
  "which is incompatible with extensive data-shuffling. SBCR solvesthis problem and achieves superior performance": "4.3.2The Impact of Calibration Module. In order to analyze theimpact of the Calibration Module, we compared several methods inTab. 5. ListNet-Platt applies Platt-scaling post-processing aftera ListNet model, which is a strong baseline. Hence, we compared ourcalibration module with Platt-scaling on the same ListNet model.We also apply Platt-scaling and ours upon the same SBR model asdefined in Eq. 15.As shown in Tab. 5, we observed that both Platt-scaling andours preserve the relative orders and show the same GAUC. Ourproposed calibration module achieves consistently better calibra-tion ability on both ListNet and SBR, which validates the strongadaptability of our method. 4.3.3Effects of Hyper-Parameter. The only hyper-parameter intro-duced in our method is , the trade-off parameter between pointloss and self-boosted pair loss in Eq. 15. We define (1 )/ as therelative ranking weight and examine the sensitivity in .First, surprisingly, an extremely small value of relative rankingweight is not optimal for calibration and an extremely large valueis not optimal for ranking. This validates that the two losses collab-orate with each other. Point loss is necessary for ranking, especiallyin queries where all items are negative and pair loss is necessaryfor calibration by giving auxiliary guild for model training.Second, SBCR is robust to the setting of . When the relativeweight is set 10 times larger /smaller, the performance of SBCR isstill comparable to that of SOTA. We own this robustness to ourcalibration module which is specially designed to be monotonicfor a given . Namely, it preserves the learned orderings from ourranking module. Optimizing the calibration module will not degradethe ranking performance.Third, we still observe slightly trade-off between calibration andranking. This trade-off has been greatly improved by the calibrationmodule. We tried to remove the calibration module and found thatwhen (1 )/ = 100, ECE will be increased to 0.1796, which is 35times worse than the current case.",
  ": The sensitivity of relative ranking weight (1 )/for SBCR (Eq. 15). Higher GAUC and lower ECE indicatebetter performance": "algorithm reported in , Point+ListCE. All three algorithmsshare the same backbone QIN , and the same features, modelstructures and optimizer. The online evaluation metrics are CTR,View Count and User Time Spend. View count measures the totalamount of video users watched, and time spend measures the totalamount of time users spend on viewing the videos.As shown in Tab. 6, SBCR contributes to the +4.81% increase inCTR, +3.15% increase in View Count and +0.85% increase in TimeSpend. Note that the 0.2% increase is a significant improvement inour system. SBCR is also efficient for online serving. Compared toour baseline QIN+Point, the only additional module is the calibra-tion network that adds 1.41% parameters and 0.96% floating pointoperations, which is negligible for the model.Note that there is an important issue that is commonly facedin the real production systems. Usually, there are several differentalgorithms under A/B test simultaneously. So the dumped scoresused for training SBCR are actually from deployed models in dif-ferent A/B tests, not only SBCRs corresponding deployed model.This is not equivalent to the standard self-boosted mechanism inSec 3. We should check the impact.In A/B test, the pointwise baseline serves the main traffic, thusSBCR is mostly trained with dumped scores from the pointwisebaseline. In this sub-optimal implementation, we still observe signif-icant gain (the 3rd two in ). And in A/B backtest (last row in), SBCR serves the main traffic and the old pointwise modelserves the small traffic. The dumped scores are mostly from SBCRitself. We observe a larger improvement compared to that in AB test:an additional gain of +0.63% View Count and +0.43% Time Spent.We conclude that the dumped scores from other models also work,with a slightly smaller gain compared to standard self-boost. Thisis because the mis-predicted samples by other models are also hardand informative and focusing on other strong models mistakes isalso beneficial.",
  "QIN + Point (baseline)---QIN + Point + ListCE+1.01%+0.80%+0.14%QIN + SBCR (AB)+4.81%+3.15%+0.85%QIN + SBCR (AB back)+5.70%+3.78%+1.28%": "extensive data shuffling and the sub-optimal trade-off between cali-bration and ranking, which contributes to significant performancegain. SBCR outperformed our highly optimized production baselineand has been deployed on the video search system of Kuaishou,serving the main traffic of hundreds of millions of active users.Note that we restricted our calibration module () to piece-wiselinear function since it is easy to guarantee the monotonicity, whichis necessary to preserve the relative orders of items under the samequery. A promising future direction is to improve the flexibility of by upgrading it to monotonic neural networks. Aijun Bai, Rolf Jagerman, Zhen Qin, Le Yan, Pratyush Kar, Bing-Rong Lin, Xuan-hui Wang, Michael Bendersky, and Marc Najork. 2023. Regression CompatibleListwise Objectives for Calibrated Ranking with Binary Relevance. In Proceed-ings of the 32nd ACM International Conference on Information and KnowledgeManagement. 45024508. Sebastian Bruch, Xuanhui Wang, Michael Bendersky, and Marc Najork. 2019.An analysis of the softmax cross entropy loss for learning-to-rank with binaryrelevance. In Proceedings of the 2019 ACM SIGIR international conference on theoryof information retrieval. 7578.",
  "Christopher JC Burges. 2010. From ranknet to lambdarank to lambdamart: Anoverview. Learning 11, 23-581 (2010), 81": "Christopher J. C. Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, NicoleHamilton, and Gregory N. Hullender. 2005. Learning to rank using gradientdescent. In Proceedings of the 22nd international conference on Machine learning,Vol. 119. 8996. Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. 2007. Learningto rank: from pairwise approach to listwise approach. In Proceedings of the 24thinternational conference on Machine learning. 129136.",
  "Sougata Chaudhuri, Abraham Bagherjeiran, and James Liu. 2017. Ranking andcalibrating click-attributed purchases in performance display advertising. InProceedings of the ADKDD17. 16": "Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al.2016. Wide & deep learning for recommender systems. In Proceedings of the 1stWorkshop on Deep Learning for Recommender Systems. ACM, 710. Chao Deng, Hao Wang, Qing Tan, Jian Xu, and Kun Gai. 2021. Calibrating userresponse predictions in online advertising. In Machine Learning and KnowledgeDiscovery in Databases: Applied Data Science Track: European Conference, ECMLPKDD 2020, Ghent, Belgium, September 1418, 2020, Proceedings, Part IV. Springer,208223.",
  "Tom Fawcett. 2006. An introduction to ROC analysis. Pattern Recognit. Lett. 27, 8(2006), 861874": "Fredric C Gey. 1994. Inferring probability of relevance using the method oflogistic regression. In Proceedings of the 17th Annual International ACM-SIGIRConference on Research and Development in Information Retrieval. 222231. Bin Gu, Victor S. Sheng, KengYeow Tay, Walter Romano, and Shuo Li. 2015.Incremental Support Vector Learning for Ordinal Regression. IEEE Transactionson Neural networks and learning systems 26, 7 (2015), 14031416.",
  "Melbourne, Australia., 27822788": "Tong Guo, Xuanping Li, Haitao Yang, Xiao Liang, Yong Yuan, Jingyou Hou,Bingqing Ke, Chao Zhang, Junlin He, Shunyu Zhang, et al. 2023. Query-dominantUser Interest Network for Large-Scale Search Ranking. In Proceedings of the 32ndACM International Conference on Information and Knowledge Management. 629638. Jui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia, David Zhang, Philip Pronin,Janani Padmanabhan, Giuseppe Ottaviano, and Linjun Yang. 2020. Embedding-based retrieval in facebook search. In Proceedings of the 26th ACM SIGKDDInternational Conference on Knowledge Discovery & Data Mining. 25532561.",
  "Tie-Yan Liu et al. 2009. Learning to rank for information retrieval. Foundationsand Trends in Information Retrieval 3, 3 (2009), 225331": "Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. 2015. Obtainingwell calibrated probabilities using bayesian binning. In Proceedings of the AAAIconference on artificial intelligence, Vol. 29. Feiyang Pan, Xiang Ao, Pingzhong Tang, Min Lu, Dapeng Liu, Lei Xiao, andQing He. 2020. Field-aware calibration: a simple and empirically strong methodfor reliable probabilistic predictions. In Proceedings of The Web Conference 2020.729739. Rama Kumar Pasumarthi, Sebastian Bruch, Xuanhui Wang, Cheng Li, MichaelBendersky, Marc Najork, Jan Pfeifer, Nadav Golbandi, Rohan Anil, and StephanWolf. 2019. Tf-ranking: Scalable tensorflow library for learning-to-rank. InProceedings of the 25th ACM SIGKDD International Conference on KnowledgeDiscovery & Data Mining. 29702978. Qi Pi, Guorui Zhou, Yujing Zhang, Zhe Wang, Lejian Ren, Ying Fan, XiaoqiangZhu, and Kun Gai. 2020. Search-based user interest modeling with lifelongsequential behavior data for click-through rate prediction. In Proceedings of the",
  "David Sculley. 2010. Combined regression and ranking. In Proceedings of the 16thACM SIGKDD international conference on Knowledge discovery and data mining.979988": "Xiang-Rong Sheng, Jingyue Gao, Yueyao Cheng, Siran Yang, Shuguang Han,Hongbo Deng, Yuning Jiang, Jian Xu, and Bo Zheng. 2023. Joint optimizationof ranking and calibration with contextualized hybrid model. In Proceedingsof the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.48134822. Yukihiro Tagami, Shingo Ono, Koji Yamamoto, Koji Tsukamoto, and Akira Tajima.2013. Ctr prediction for contextual advertising: Learning-to-rank approach. InProceedings of the Seventh International Workshop on Data Mining for OnlineAdvertising. 18.",
  "Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross networkfor ad click predictions. In Proceedings of the ADKDD17. 17": "Ruoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong,and Ed Chi. 2021. Dcn v2: Improved deep & cross network and practical lessonsfor web-scale learning to rank systems. In Proceedings of the web conference 2021.17851797. Fen Xia, Tie-Yan Liu, Jue Wang, Wensheng Zhang, and Hang Li. 2008. Listwiseapproach to learning to rank: theory and algorithm. In Proceedings of the 25thinternational conference on Machine learning, Vol. 307. 11921199. Le Yan, Zhen Qin, Xuanhui Wang, Mike Bendersky, and Marc Najork. 2022. ScaleCalibration of Deep Ranking Models. In Proceedings of the 28th ACM SIGKDDInternational Conference on Knowledge Discovery and Data Mining. 43004309. Zhao-Yu Zhang, Xiang-Rong Sheng, Yujing Zhang, Biye Jiang, Shuguang Han,Hongbo Deng, and Bo Zheng. 2022. Towards Understanding the OverfittingPhenomenon of Deep Click-Through Rate Models. In Proceedings of the 31st ACMInternational Conference on Information & Knowledge Management. 26712680. Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, YanghuiYan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-throughrate prediction. In Proceedings of the 24th ACM SIGKDD International Conferenceon Knowledge Discovery & Data Mining. ACM, 10591068."
}