{
  "Abstract": "Cross-lingual Cross-modal Retrieval (CCR) is an essential task inweb search, which aims to break the barriers between modality andlanguage simultaneously and achieves image-text retrieval in themulti-lingual scenario with a single model. In recent years, excel-lent progress has been made based on cross-lingual cross-modalpre-training; particularly, the methods based on contrastive learn-ing on large-scale data have significantly improved retrieval tasks.However, these methods directly follow the existing pre-trainingmethods in the cross-lingual or cross-modal domain, leading to twoproblems of inconsistency in CCR: The methods with cross-lingualstyle suffer from the intra-modal error propagation, resulting in in-consistent recall performance across languages in the whole dataset.The methods with cross-modal style suffer from the inter-modaloptimization direction bias, resulting in inconsistent rank acrosslanguages within each instance, which cannot be reflected by Re-call@K. To solve these problems, we propose a simple but effective1-to-K contrastive learning method, which treats each languageequally and eliminates error propagation and optimization bias. Inaddition, we propose a new evaluation metric, Mean Rank Vari-ance (MRV), to reflect the rank inconsistency across languageswithin each instance. Extensive experiments on four CCR datasetsshow that our method improves both recall rates and MRV withsmaller-scale pre-trained data, achieving the new state-of-art1.",
  "Corresbonding author: codes can be accessed at": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08",
  "Introduction": "Recently, significant progress has been made in the cross-modality, and the cross-lingual domains, leading toincreased interest in the more general cross-lingual cross-modalscenarios. In the cross-lingual cross-modal domain, Cross-lingualCross-modal Pre-training (CCP) is first explored,followed by Cross-lingual Cross-modal Retrieval (CCR) as the first downstream task independently studied. CCR aims toachieve image-text retrieval in multi-lingual scenarios with a singlemodel, preventing the high latency associated with text translationfrom other languages to English in real-time web searches.In general, modern dense retrieval matches the results for aquery by a particular distance metric (e.g., Euclidean distance orcosine similarity), which implies that the dense retrieval meth-ods should push queries and those semantically similar candidateitems closer than other random pairs in the high-dimensional space.Thus, the core of the retrieval task lies in aligning the semanticspaces of queries and candidate sets, regardless of whether theyare in different languages or different modalities. Recent studiesshow that contrastive learning based on pairwise data is effectivein cross-lingual and cross-modal retrieval tasks. For example, CLIP, which is only pre-trained by aligning different modalities us-ing contrastive learning, has achieved remarkable performancesin zero-shot cross-modal retrieval; on the other hand, aligning therepresentations from different modalities (or different languages)before fusing them can reduce the difficulty of fusion and signifi-cantly improve the performance of downstream cross-modal tasksincluding retrieval, question answering and reasoning . As a re-sult, the existing works in CCP directly pieced the alignment ideasin cross-modal or cross-lingual domains, feeding pairwise data intothe encoder at a time, such as an image-text pair and a bi-lingual",
  ": Two inconsistency problems exist in the currentcross-lingual cross-modal pre-training methods, leading toinconsistent recall and ranking in cross-lingual cross-modalretrieval separately": "text pair. Specifically, the existing methods use the following twoideas to align different modalities: (1) considering English as theanchor for bridging vision with other languages, which means thatthe images are aligned to the English texts only, while the texts inother languages are aligned to the English texts only or (2)considering the images being aligned with the texts in a randomlanguage at a time during pre-training .However, the desirable alignment process is more complex incross-lingual cross-modal scenarios. Intuitively, the semantics ofthe texts in multiple languages need to be aligned jointly with thosefrom vision, which cannot be achieved with pairwise data. With thetheoretical derivations and empirical studies (), we findthat applying either of the two above ideas to CCP will result in twoproblems of inconsistency (). Specifically, regarding Englishas the bridge in inter-modal may cause error propagation, resultingin an inconsistent performance on Recall@K of different languagesin CCR; aligning the image with only the text in a random languageat a time may lead to the optimization direction bias, resultingthe inconsistent ranks of different languages within an instance.Highlighting that the latter problem is more insidious since it cannotbe directly reflected by Recall@K, which is almost the only reportedevaluation metric of CCR .To solve the above problems, in this paper, we propose a sim-ple but effective contrastive paradigm for CCP, 1-to-K contrastivelearning. Specifically, when pre-training the images and texts ina mini-batch ratio of not 1 to 1 but 1 to K (K 2), each image isaligned simultaneously with K texts in different languages. Underthis paradigm, all languages are aligned with vision at once, and nolanguage is used as the bridge between vision and other languages, eliminating intra-modal error propagation and inter-modal opti-mization direction bias in principle. In addition, two commonly usedpre-training tasks for capturing fine-grained correlation betweenmodalities, Multi-lingual Image-Text Matching (MITM) andCross-modal Masked Language Modeling (CMLM) , can beeasier superimposed on the novel contrastive paradigm with thehelp of hard negative sampling. Based on the three pre-trainingtasks, we propose a pre-trained model, CCR. For the evaluation ofCCR, as a complement to Recall@K, we propose a new evaluationmetric, Mean Rank Variance (MRV), to reflect the rank inconsis-tency of the different languages in an instance. Extensive experi-ments on four public CCR datasets demonstrate that our methodhas effectively solved the above two problems and achieved newstate-of-the-art.The contributions of this paper can be summarized as follows:",
  "We propose a simple but effective 1-to-K contrastive para-digm as an alternative to the traditional 1-to-1 contrastiveparadigm in CCR to solve these problems": "We propose Mean Rank Variance (MRV) to better reflect re-trieval performance across languages and modalities, whichis used to replenish Recall@K and evaluate the rank consis-tency across languages in each dataset sample. We propose CCR, a CCP model with the novel 1-to-K con-trastive paradigm. We pre-train four variants of CCR withthe different language numbers and data scales. The largestvariant CCR10-E, which is still pre-trained with fewer lan-guage numbers and data scale than all baselines, achievesnew SOTA on four CCR datasets.",
  "Cross-Lingual Cross-Modal Pre-Training": "Cross-lingual Cross-modal Pre-training (CCP) is gen-eralized from cross-modal pre-training and cross-lingualpre-training , which aims to develop a representation learn-ing model that captures the relationship in different modalities anddifferent languages simultaneously. Current methods can be broadlydivided into three categories based on their model architectures. Cross-Lingual Style. The first class of methods follows themodel architecture in the cross-lingual domain, where a pre-trainedcross-modal model (e.g. CLIP ) is required. Then, the pre-trainedmodel is tuned to a cross-lingual version by aligning the representa-tions of English texts and non-English texts while freezing both thevisual and English textual backbone. The representatives of thesemethods are multi-lingual CLIPs . The idea behind thesemethods is using English as a bridge between vision and otherlanguages.",
  "Improving the Consistency in Cross-Lingual Cross-Modal Retrieval with 1-to-K Contrastive LearningKDD 24, August 2529, 2024, Barcelona, Spain": ": Performance comparison on four retrieval datasets, where IR means text-to-Image Retrieval and TR means image-to-Text Retrieval. Consistent with standard evaluation protocols, Recall@1 on xFlickr&CO, Accuracy on WIT, and averageRecall@K with K=1,5,10 on Multi30K and COCO are reported. We only calculate MRV on xFlickr&CO and Multi30K becausethere is no one-to-many relationship between images and texts in WIT, whereas the texts in COCO are from different sources.",
  "Cross-Lingual Cross-Modal Retrieval": "Cross-lingual Cross-modal Retrieval (CCR) isone of the downstream tasks that have been focused on in cross-lingual cross-modal scenarios. MURAL demonstrates that highperformance in CCR can be achieved through pre-training withcontrastive learning over large-scale datasets. Fei et al. pre-train only a fusion encoder for CCR using pre-extracted imageregion features. More recently, IGLUE , a cross-lingual cross-modal benchmark, was proposed with two new retrieval datasets,xFlickr&CO and WIT. In addition, IGLUE explores several cross-modal pre-training models (such as ViLBERT and xUNITER), and evaluates them on two new datasets by directly translat-ing the texts in other languages to English, demonstrating that thesemodels serve as strong baselines. Carlsson et al. apply cross-lingual teacher learning to transfer CLIP to other languages. Wanget al. proposed a noise robustness CCR method to improve theperformance when training on the noisy translated data.To the best of our knowledge, our work in this paper is the firstexploration of the consistency in cross-lingual cross-modal retrieval.In addition, our newly proposed 1-to-K contrastive learning pre-training task and the evaluation metric MRV have not previouslyappeared in CCR and related fields.",
  "Preliminary": "In the loss functions for alignment, there may be only the anchorwith its positive samples (e.g., Mean Squared Error (MSE)) andthe optional negative samples (e.g., InfoNCE Loss , which iscommonly used in contrastive learning). When these loss func-tions are used, the anchor is optimized by the alignment direction,which points from the anchor to the positive sample. Intuitively, the alignment direction brings the anchor and positives together inthe semantic space.In advance, we give the required notation for the follow-up con-tent in this section. For simplicity, we only consider the case whereone image needs to be aligned with two texts from two differentlanguages, and the subsequent conclusions can be easily gener-alized to more languages. Let , and denote the normalizedrepresentations of the image, the text in language , and the textin language , respectively. We define = (, ), = (, )and = (, ), where (., .) represents the angle of two samedimensional representations.",
  "Inconsistency in Recall@K": "Theoretical Analysis. The methods following the cross-lingualarchitecture implicitly rely on English as a bridge in inter-modalalignment between the other language and vision. In this setting,we consider the situation in which the other language text represen-tation is the anchor, where it is aligned to its positive sample, theEnglish text representation. However, in theory, it should be alignedto the image representation. Without loss of generality, if we regardlanguage as English and language as another language, thenthe practical alignment direction is , while correct alignmentdirection is ((a)). Then we have the following results:",
  "Lemma 3.1. Suppose that is the angle between the practical andcorrect alignment direction of . If and only if English texts can bealigned well with images, i.e. tends to 0, then will converge to 0": "Empirical Observation. We find the inter-modal alignmentprocess so tough that English texts cannot be aligned well withimages. Specifically, the loss value can drop by 5 to 6 orders ofmagnitude in the text-modal (uni-modal) scenario , while it isonly 2 orders of magnitude in cross-modal contrastive learning ((b)). It means that the alignment between English texts andimages is not ideal, and if English texts are used to connect imagesand texts in other languages, there will be a risk of error propagationon intra-modal alignment, resulting in a worse alignment betweennon-English texts and images.Impact of inconsistency. As this problem persists during pre-training, the impact of this problem is global and can be revealed bythe uneven performance under the different language settings. As itis shown by the results of M3P and UC2 in , the performancegap among different language scenarios is clear even though theinstance number per language has been kept nearly consistentduring pre-training .",
  "Inconsistency in Rank": "Theoretical Analysis. The methods that follow the cross-modalarchitecture consider each language separately aligned to the vision,thus avoiding error propagation in intra-modal. However, theysuffer from another local problem of inconsistency.In this setting, we consider the situation that the image is theanchor, where its optimal alignment coordinates should satisfy: (1)min((, ) + (, )) and (2) (, ) = (, ). Combining thetwo conditions above, should be drawn to the midpoint of theminor arc corresponding to and , i.e., the correct alignment",
  "(b)": ": Theoretical analysis and empirical observation forinconsistency in Rank. (a) An illustration of Lemma 3.2,where the green arrow represents the correct alignment direc-tion, while the red arrow represents the practical alignmentdirection. (b) A Visualization of T-SNE with 10 instancesrandomly sampled in xFlickr&CO. The representations areobtained by Swin Transformer and the first half (firstsix layers) of XLM-R following the setting in CCLM . pre-training model, CCR, is further presented to combine 1-to-Kcontrastive learning with other common pre-training tasks in aunified framework in .3; Finally, a new evaluation metriccalled Mean Rank Variance (MRV) is proposed in .4, whichevaluates the rank consistency across languages in a instance.",
  "Notation": "Let = (,1,2, ...,) denote a multi-lingual image-text dataset,consisting of the instance (,1,2, ...,) , where indexesthe instance, is the image in this instance, is the text in the-th language in this instance, and refers to the total numberof languages in the dataset. If it is clear from the context, we willremove the subscript or for brevity.",
  "-to-K Contrastive Learning": "To solve both two problems in the previous section, the key is thatthe texts in all languages should be aligned with the semanticallysimilar images all at once. Obviously, it is not possible to do this byaligning pairs of data. Even if uniformly sampling one from the textsin all languages and combining it with the corresponding image toform an image-text pair, the second problem remains. Therefore,the effective way is to form the texts in all languages and the imagedirectly into a tuple as the input. Therefore, we propose a 1-to-Kcontrastive learning approach to solve this problem. For simplicity,let and represent the normalized text and image representations,respectively. Then, the optimization objective of 1-to-K contrastivelearning can be formulated as follows:",
  "exp( /) + , exp( /)(2)": "where is the number of languages and is the number of negativeinstances. It is worth noting that there exists literature on multiplepositive contrastive learning in other fields , where all posi-tive items are accumulated in the numerator and the probability ofthe overall positive terms probability is calculated to be infinitelyconvergent to 1. Instead, we further set the label of each positiveitem to 1/K to ensure equal contribution from each language.Note that increasing the number of multi-lingual texts used asinput to the encoders only results in a small increase in GPU mem-ory and training time since the text encoders are usually morelightweight than image encoders in CCP and most of the com-putations involved are matrix operations that support parallelism.The changes in memory usage and training time before and afterapplying 1-to- contrastive learning are detailed in Appendix D.",
  "Pretraining Model: CCR": "Based on the proposed 1-to-K contrastive learning, we further pro-pose a CCP model named CCR. Specifically, we combine 1-to-Kcontrastive learning with two other common CCP tasks and balancepositive and negative samples by hard sample mining. As shownin the middle of , we adopt the common framework incross-lingual cross-modal pretraining , which consists ofa multi-lingual text encoder (), a visual encoder () and a fusionencoder (, ) with image-to-text cross-attention. 4.3.1Hard Sample Mining. Incorporating cross-attention betweenthe image representation and the text representations in all lan-guages can greatly increase the pre-training time. Therefore, weuse the hard sample mining strategy proposed by Li et al. forboth positive and negative samples. This method allows the modelcan only focus on how to reconstruct the hardest positive samplesin the CMLM task and distinguish the hardest negative samples inthe MITM task. In subsequent sections, we use posto represent",
  "the hard positive sample for texts and negand negto representthe hard negative sample for texts and images, respectively. Pleaserefer to Appendix C.3 for sampling details": "4.3.2Multi-lingual Image-Text Matching (MITM). The MITM taskis a binary classification task that aims to identify whether the se-mantics of a given image-text pair match. This task is often regardedas an image-text bi-directional prediction problem. Specifically, inthe image-to-text direction, the model is trained to select the rightone from the hard positive and hard negative text samples. Let clsbe the representation output by the fusion encoder, then the lossfunction of MITM can be expressed as",
  "W exp(( mask,))(5)": "where : (R W) R1 is a score function to evaluate thematching degree of a given contextual representation with a giventoken, + is the original token of the masked location and W isthe vocabulary list. We use the special token [MASK] to replace15% of the tokens in each text, following BERT .",
  "Evaluation Metric: Mean Rank Variation": "While Recall@K is the common metric used in CCR, it only canreflect the overperformance on a single language. In this section,we introduce a new evaluation metric, Mean Rank Variation (MRV),to measure the rank consistency in different languages within aninstance. illustrates the difference between MRV and Re-call@K in their calculation methods. MRV for K languages can becomputed in both Image-to-Text Retrieval (TR) and Text-to-ImageRetrieval (IR) tasks. For example, in the TR task, given an image and a text set in a particular language {}=1, the similaritiesbetween the image and the text set are computed first. Then thetext set is sorted by these similarities in ascending order and therank of is denoted as . For each , we can loop through from 1 to to obtain {}=1, and average them to obtain . Similarly, in the IR task, we denote the rank of retrieving theimage using the text as and the average rank obtainedby retrieving using all languages as . Thus, MRV for languages, which is denoted as MRVK, can be expressed as",
  "| |2(7)": "Note that there is no trade-off between Recall@K and MRVK,which means that when Recall@1=1 holds for all K languages,MRVK=0 also holds. MRVK is more likely to reflect the alignmentconsistency of local semantic space. Such consistency is significantin certain scenarios, such as cross-border e-commerce, to ensureconsistency in the results retrieved when the queries are in differentlanguages but have the same semantics.",
  "Experiment5.1Experiment Setup": "5.1.1Pre-training Datasets. For pre-training, we mainly use Con-ceptual Captions 3M (CC3M) , which currently has only 1.8million image-text pairs from the web due to the inaccessibilityof image hyperlinks. To verify the scalability of our approach, wefurther introduce 3 additional cross-modal web datasets, includ-ing SBU Caption , Visual Genome and COCO . For thetranslated version of the texts, we use the 6-language (English,German, French, Czech, Japanese, and Chinese) translated textsin CC3M provided by UC2 as well as the same 6-languagetranslated texts in the other three datasets, provided by CCLM for fair comparisons. To further verify the generalizability of ourmethod to more languages, we use the M2M-100-large model to translate the English text in the datasets into an additional 4languages (Spanish, Indonesian, Russian, and Turkish), followingQiu et al. . Therefore, the total number of text languages used",
  "for evaluation is 10, which covers all languages in xFlickr&CO. Weplan to open-source these translated texts for research": "5.1.2Baseline. CCR proposed in this paper is mainly an improve-ment of the training optimization objective in the pre-trainingphase, so we mainly compare it with other CCP models, includingxUNITER , UC2 , M3P , TD-MML and CCLM .These methods have been briefly described in .1, while formore details on them, please refer to Appendix B.1.",
  "CCR10-E pre-trained using CC3M, COCO, VG and SBU with10-language texts": "5.1.4Evaluation Datasets and Protocols. We evaluate our meth-ods on four popular CCR datasets, including xFlickr&CO , WIT, Multi30K , and COCO . Although the images inxFlickr&CO are derived from the original Flickr30K and COCO,the multi-lingual texts in xFlickr&CO are manually re-annotated.Therefore, the performance on xFlickr&CO may not be stronglycorrelated with that on Multi30K and COCO. For both xFlickr&COand WIT, we evaluate our models using two protocols: fine-tuningon the English train set (Zero-Shot) and fine-tuning on 100 instancesof other languages based on English fine-tuned models (Few-Shot).For Multi30K and COCO, we also use two evaluation protocols:fine-tuning on the English train set (Zero-Shot) and fine-tuning oneach language train set (Fine-Tune). Note that the results on WITunder the few-shot scenario are not reported because IGLUE does not provide the corresponding evaluation protocol. For moredetails, please refer to Appendix B.2.",
  "Implementation Details": "Following , the image encoder is initialized using the 12-layerSwin Transformer , and the multi-lingual encoder and fusionencoder are initialized using the pre-trained XLM-R , whichconsist of 6 layers for each. We provide a detailed comparison ofthe model architecture and initialization sections between CCRand other baselines in Appendix B.1. Also, keeping consistent with for a fair comparison, in Eqn. (1) and (2) are set as 0.07. TheAdamW optimizer with 1e-4 learning rate, 0.01 weight decay,and first 3% linearly warm-up steps is used. The batch size on eachGPU is set to 64. The pre-training experiments were conducted on 2NVIDIA A100s, while fine-tuning was done on 1 A100. We pre-trainall models for 30 epochs. With the acceleration of PyTorch DDP ,it takes approximately 4 days to pre-train for 30 epochs on CC3Mwith 6 languages. In addition, we provide the hyper-parametersused for fine-tuning all four datasets in Appendix C.2.",
  "Few-shot fine-tune English fine-tuned model on target languages (Few-Shot)Single-language fine-tune": "xUNITER2.7M+100G14.3013.54-----------M3P3.3M+100G13.2112.26---87.482.167.365.0-88.675.880.1UC23.3M+19.8M19.7917.59---87.283.877.674.2-88.184.987.3CCLM-3M2.8M+54.8M65.3163.9112.93--90.489.690.088.82.4192.392.192.4 CCR61.8M+10.8M29.2828.7215.24--88.588.188.687.32.2891.691.091.8CCR6-E3.3M+19.8M33.1933.417.81--90.890.591.490.51.3092.592.692.5CCR101.8M+18.0M55.9155.2418.11--84.283.684.682.53.9090.089.790.4CCR10-E3.3M+33.0M73.7473.277.62--91.490.691.290.21.1292.592.592.5 between the WIT test set and the pre-training data of CCLM. Un-less otherwise noted, we use ISO 639-1 Abbreviations to representspecific languages in subsequent tables. The table mapping thetwo-letter codes to the specific language is provided in AppendixA for convenience. Recall Rates. With a smaller scale pre-trained data (#imagesand #texts) and fewer language numbers than the baselines, CCR10-E achieves SOTA results under both zero-shot and few-shot (orfine-tuning) setting for all CCR datasets, demonstrating the goodgeneralizability and transferability of CCR among different lan-guages. When comparing the performance difference among thefour variants of CCR, we can find that (1) CCR10 use more lan-guages compared to CCR6, causing it to improve the performanceon the newly added languages while hurting Recall@K of the orig-inal languages existing in CCR6, possibly due to the increaseddifficulty of alignment across more languages; (2) CCR6-E achieveshigher Recall@K and lower MRV on the original languages com-pared to CCR6 after introducing more pre-training data. Consistency Evaluation of Recall@K. Recall that one of theinconsistency problems leads to inconsistent recall@K in differ-ent languages. As seen in , all baselines perform better inEnglish than in other languages on Multi30K and COCO becauseEnglish is used as a bridge between the visual and other languagesduring their pre-training. Benefitting from the 1-to-K contrastiveparadigm, all four variants of CCR maintain significantly smallerinter-language gaps on these two datasets. Among them, CCR10-E maintains the smallest performance gap across languages on Multi30K and COCO in the zero-shot scenario, even though thisscenario is more favourable for English-related retrieval. More sur-prisingly, when CCR is fine-tuned in each language separately, theperformance gap on various languages almost disappears, whichreflects the promising application of CCR in practical applications. Consistency Evaluation of Rank. Recall that the other prob-lem results in the inconsistency of rank. The motivation behindproposing MRV is that Recall@K cannot reflect such differencesacross languages within an instance. Therefore, we calculate MRVfor four languages (EN, DE, JA, and ZH) on xFlickr&CO and fourlanguages (EN, DE, FR, and CS) on Multi30K, which are denotedas MRV4 in . We also report MRV4 of all compared modelsexcept TD-MML based on the checkpoints obtained from the officialIGLUE GitHub repository 2. It can be found that MRV4 for CCLM,which uses 1-to-1 contrastive learning, has improved substantiallycompared to M3P and UC2, while CCR can improve further andachieve the lowest MRV. Similar to Recall@K, adding more lan-guages (CCR6 CCR10 and CCR6-E CCR10-E) will result in ahigher MRV due to the capacity constraints of the model and theelevated difficulty of the optimization objective.",
  "CCR10-E72.977.8990.82.5391.1-w/o KCL68.1411.0284.66.4588.8-w/o H-MITM70.958.2987.43.8990.3-w/o H-CMLM69.489.4585.94.7889.6": "H-MITM: Hard sample mining for MITM is replaced with randomuniform sampling from the candidate set; w/o H-CMLM: Hardsample mining for CMLM is replaced with uniform sampling fromthe candidate set.Due to space constraints, we only report results for CCR6 andCCR10-E under the zero-shot setting in . Note that the othertwo variants also show a similar trend. As can be seen from theresults, each pre-training task and sampling approach proposed tocontribute to the improvement in both Recall@K and MRV4. Morespecifically, 1-to-K contrastive learning has the largest improve-ment for all metrics, while 1-to-1 contrastive learning is still betterthan the results without contrastive learning. Hard sample miningpositively affected both MITM and CMLM downstream tasks.",
  "Further Study": "5.5.1Pure Contrastive Learning. In fact, CCR is proposed to en-sure that the models parameter number and pre-training tasks aresimilar to other baselines. However, neither MITM and CMLM tasksnor the fusion encoder is necessary for the retrieval task. Therefore,we further compare the effect of 1-to-K and 1-to-1 contrastive learn-ing on Recall@K and MRV with the fusion encoder removed, whileother settings remain consistent with CCR6. As seen from (a), 1-to-K contrastive learning can still lead on both xFlickr&COand Multi30k. 5.5.2Loss and Performance. To better understand why our methodworks, we record the 1-to-1 contrastive loss and 1-to-K contrastiveloss during the pre-training process of CCR6 and CCR6 -w/oKCL, respectively. In addition, we evaluate the checkpoints every5 epochs on Multi30K under zero-shot setting and plot the resultsin (c). The figure shows that 1-to-K contrastive learningperforms better at all evaluated checkpoints. Attributed to the ab-sence of directional bias, when pre-training with 1-to-K contrastivelearning, the corresponding loss values remain lower than thosewhen using 1-to-1 contrastive learning. 5.5.3T-SNE Visualization. A T-SNE visualization similar to that in.3 is shown in (d) and (e), which contains10 instances randomly sampled in xFlickr&CO. Comparing to 1-to-1 contrastive learning, 1-to-K contrastive learning enables higherdiscrimination between instances and a more balanced distribution IR R@1TR R@1MVR4 301-to-K CL1-to-1 CL",
  "Case Study": "After manually analyzing the wrong cases in xFlickr&CO, whichare not correct under some language settings, we summarized twotypical causes of matching errors: fine-grained semantic match-ing errors and pseudo-negative samples. We give some cases foreach of them in . Since images are more presentable andcomprehensible than texts, we only use the error cases from thetext-to-image retrieval (IR) task. The first four cases demonstratea fine-grained semantic matching error. For example, the conceptof headband in the first case is so specialized that the image canmatch all other features when retrieved using German (DE) andTurkish (TR). The last two cases show a pseudo-negative sampleerror, where the images retrieved actually match the text semantics,but these matching relationships are missing annotations in thedataset. For example, in the fifth case, both images retrieved forthe \"hockey game\" matched the textual description, yet only one islabelled as correct in the xFlickr&CO dataset.",
  "fast food displayed on a table with sandwich and soup": ": Six wrong cases of text-to-Image Retrieval (IR) on xFlickr&CO. We only provide the English text in each instance as areference, and the images are actually retrieved from the text corresponding to the labelled language at the top of each column.The green and red boxes outside the images represent the correct and incorrect images.",
  "Discussion": "The Novelty of 1-to-K Contrastive Learning. The proposed modi-fication is not groundbreaking but based on traditional 1-to-1 con-trastive learning. However, recall that 1-to-1 contrastive learning,which has been carried over from the cross-lingual or cross-modaldomains, is still the dominant paradigm in CCP. The call to changea tasks pre-training paradigm is usually tough. Changing to 1-to-Kcontrastive learning is minimal yet effective and easily applicableto the existing CCR models based on SimSiam networks. The Significance of the Consistency in CCR. Maintaining con-sistency in CCR is important. For example, in a cross-border e-commerce business, consistency in recall across languages ensuresthat the entire retrieval system can be supported by a single funda-mental model. Further, the query with the same semantics issued bydifferent native-speaking customers should be expected to returnthe same results, meaning there needs to be good consistency inrank across different languages within an instance. If we evaluatethe retrieval model with Recall@K on each language only, the trueperformance of the CCR model will not be reflected. Further Consistency. Ensuring equal contributions across lan-guages in all aspects is challenging. For instance, XLM-R, CCRscross-lingual encoder, is trained on the 2.5TB CommonCrawl Cor-pus encompassing 100 languages. Discrepancies in data sizes be-tween high-resource and low-resource languages within this cor-pus, like the 100GB English data versus the 0.1GB Sundanese data, impede XLM-R from achieving uniform performance across lan-guages. Balancing language contributions during pre-training couldhelp narrow the performance gap but would require substantialcomputational resources, which we will explore in future studies.",
  "Conclusion": "In this paper, we first analyze the two problems of inconsistencyexisting in the current CCP methods and point out their impacton CCR via theoretical analysis and empirical studies. Then wepropose a 1-to-K contrastive paradigm and a CCP model, CCR,based on it, which equally aligns all languages with vision at once,effectively improving the consistency in CCR. In addition, a newevaluation metric, MRV, is proposed to portray the consistency ofeach language rank within each instance. Exclusive experiments onthe four CCR datasets show that our model scales well and achievesnew SOTA on both Recall@K and MRV.",
  "KDD 24, August 2529, 2024, Barcelona, SpainZhijie Nie, Richong Zhang, Zhangchi Feng, Hailang Huang, & Xudong Liu": "Hangbo Bao, Wenhui Wang, Li Dong, Qiang Liu, Owais Khan Mohammed, KritiAggarwal, Subhojit Som, Songhao Piao, and Furu Wei. 2022. VLMo: Unifiedvision-language pre-training with mixture-of-modality-experts. Advances inNeural Information Processing Systems 35 (2022), 3289732912. Emanuele Bugliarello, Fangyu Liu, Jonas Pfeiffer, Siva Reddy, Desmond Elliott,Edoardo Maria Ponti, and Ivan Vuli. 2022. IGLUE: A benchmark for transferlearning across modalities, tasks, and languages. In International Conference onMachine Learning. PMLR, 23702392. Fredrik Carlsson, Philipp Eisen, Faton Rekathati, and Magnus Sahlgren. 2022.Cross-lingual and Multilingual CLIP. In Proceedings of the Thirteenth LanguageResources and Evaluation Conference. European Language Resources Association,Marseille, France, 68486854. Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. 2021. Concep-tual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-TailVisual Concepts. In CVPR. Computer Vision Foundation / IEEE, 35583568. Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Pi-otr Dollr, and C Lawrence Zitnick. 2015. Microsoft coco captions: Data collectionand evaluation server. arXiv preprint arXiv:1504.00325 (2015). Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan,Yu Cheng, and Jingjing Liu. 2020. Uniter: Universal image-text representationlearning. In European conference on computer vision. Springer, 104120. Zewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham Singhal, Wenhui Wang,Xia Song, Xian-Ling Mao, He-Yan Huang, and Ming Zhou. 2021. InfoXLM:An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training. In Proceedings of the 2021 Conference of the North American Chapterof the Association for Computational Linguistics: Human Language Technologies.35763588. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guil-laume Wenzek, Francisco Guzmn, douard Grave, Myle Ott, Luke Zettlemoyer,and Veselin Stoyanov. 2020. Unsupervised Cross-lingual Representation Learn-ing at Scale. In Proceedings of the 58th Annual Meeting of the Association forComputational Linguistics. 84408451.",
  "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT:Pre-training of deep bidirectional transformers for language understanding. arXivpreprint arXiv:1810.04805 (2018)": "Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Sid-dharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaud-hary, et al. 2021. Beyond english-centric multilingual machine translation. TheJournal of Machine Learning Research 22, 1 (2021), 48394886. Hongliang Fei, Tan Yu, and Ping Li. 2021. Cross-lingual Cross-modal Pretrain-ing for Multimodal Retrieval. In Proceedings of the 2021 Conference of the NorthAmerican Chapter of the Association for Computational Linguistics: Human Lan-guage Technologies. Association for Computational Linguistics, Online, 36443650. Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple ContrastiveLearning of Sentence Embeddings. In Proceedings of the 2021 Conference on Em-pirical Methods in Natural Language Processing. 68946910. Aashi Jain, Mandy Guo, Krishna Srinivasan, Ting Chen, Sneha Kudugunta, ChaoJia, Yinfei Yang, and Jason Baldridge. 2021. MURAL: Multimodal, MultitaskRepresentations Across Languages. In Findings of the Association for Computa-tional Linguistics: EMNLP 2021. Association for Computational Linguistics, PuntaCana, Dominican Republic, 34493463.",
  "Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-semantic alignments forgenerating image descriptions. In Proceedings of the IEEE conference on computervision and pattern recognition. 31283137": "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, JoshuaKravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al.2017. Visual genome: Connecting language and vision using crowdsourced denseimage annotations. International journal of computer vision 123, 1 (2017), 3273. Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong,and Steven Chu Hong Hoi. 2021. Align before fuse: Vision and language repre-sentation learning with momentum distillation. Advances in neural informationprocessing systems 34 (2021), 96949705. Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li,Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, et al. 2020. PyTorchdistributed: experiences on accelerating data parallel training. Proceedings of theVLDB Endowment 13, 12 (2020), 30053018. Xirong Li, Chaoxi Xu, Xiaoxu Wang, Weiyu Lan, Zhengxiong Jia, Gang Yang,and Jieping Xu. 2019. COCO-CN for cross-lingual image tagging, captioning,and retrieval. IEEE Transactions on Multimedia 21, 9 (2019), 23472360. Fangyu Liu, Emanuele Bugliarello, Edoardo Maria Ponti, Siva Reddy, Nigel Collier,and Desmond Elliott. 2021. Visually Grounded Reasoning across Languages andCultures. In Proceedings of the 2021 Conference on Empirical Methods in Natural",
  "Ilya Loshchilov and Frank Hutter. [n. d.]. Decoupled Weight Decay Regularization.In International Conference on Learning Representations": "Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019. Vilbert: Pretrain-ing task-agnostic visiolinguistic representations for vision-and-language tasks.Advances in neural information processing systems 32 (2019). Minheng Ni, Haoyang Huang, Lin Su, Edward Cui, Taroon Bharti, Lijuan Wang,Dongdong Zhang, and Nan Duan. 2021. M3P: Learning universal representationsvia multitask multilingual multimodal pre-training. In Proceedings of the IEEE/CVFconference on computer vision and pattern recognition. 39773986.",
  "Vicente Ordonez, Girish Kulkarni, and Tamara Berg. 2011. Im2text: Describingimages using 1 million captioned photographs. Advances in neural informationprocessing systems 24 (2011)": "Maxime Portaz, Hicham Randrianarivo, Adrien Nivaggioli, Estelle Maudet,Christophe Servan, and Sylvain Peyronnet. 2019. Image search using multi-lingual texts: a cross-modal learning approach between image and text. Ph. D.Dissertation. qwant research. Chen Qiu, Dan Oneat,, Emanuele Bugliarello, Stella Frank, and Desmond Elliott.2022. Multilingual Multimodal Learning with Machine Translated Text. In Find-ings of the Association for Computational Linguistics: EMNLP 2022. Associationfor Computational Linguistics, Abu Dhabi, United Arab Emirates, 41784193. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,et al. 2021. Learning transferable visual models from natural language supervision.In International conference on machine learning. PMLR, 87488763. Bin Shan, Yaqian Han, Weichong Yin, Shuohuan Wang, Yu Sun, Hao Tian, HuaWu, and Haifeng Wang. 2022. ERNIE-UniX2: A Unified Cross-lingual Cross-modalFramework for Understanding and Generation. arXiv preprint arXiv:2211.04861(2022).",
  "Jiaming Song and Stefano Ermon. 2020. Multi-label contrastive predictive coding.Advances in Neural Information Processing Systems 33 (2020), 81618173": "Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and MarcNajork. 2021. Wit: Wikipedia-based image text dataset for multimodal multi-lingual machine learning. In Proceedings of the 44th International ACM SIGIRConference on Research and Development in Information Retrieval. 24432449. Weijie Su, Xizhou Zhu, Chenxin Tao, Lewei Lu, Bin Li, Gao Huang, Yu Qiao,Xiaogang Wang, Jie Zhou, and Jifeng Dai. 2022. Towards All-in-one Pre-trainingvia Maximizing Multi-modal Mutual Information. arXiv preprint arXiv:2211.09807(2022). Yonglong Tian, Dilip Krishnan, and Phillip Isola. 2020. Contrastive multiviewcoding. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK,August 2328, 2020, Proceedings, Part XI 16. Springer, 776794.",
  "Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.Journal of machine learning research 9, 11 (2008)": "Yabing Wang, Jianfeng Dong, Tianxiang Liang, Minsong Zhang, Rui Cai, and XunWang. 2022. Cross-Lingual Cross-Modal Retrieval with Noise-Robust Learning.In Proceedings of the 30th ACM International Conference on Multimedia. 422433. Yuya Yoshikawa, Yutaro Shigeto, and Akikazu Takeuchi. 2017. STAIR Captions:Constructing a Large-Scale Japanese Image Caption Dataset. In Proceedings ofthe 55th Annual Meeting of the Association for Computational Linguistics (Volume2: Short Papers). Association for Computational Linguistics, Vancouver, Canada,417421. Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. 2014. From imagedescriptions to visual denotations: New similarity metrics for semantic infer-ence over event descriptions. Transactions of the Association for ComputationalLinguistics 2 (2014), 6778.",
  "This section details the baselines used for comparison and com-pares key information about their architectures and pre-trainingprocesses in": "xUNITER . is a multi-lingual variant of UNITER , whichfollows the architecture of UNITER and the parameters are initial-ized with XLM-Rbase . It also has a twin, mUNITER, which isinitialized using mBERT . Considering that xUNITER worksbetter, we ignore the results of mUNITER in this paper. xUNITERand mUNITER are pre-trained using image-English text pairs andparallel corpus alternately composed of batch. UC2 . presents the first MT-augmented pre-training modelthat pivots primarily on images and complementary on Englishto learn cross-lingual cross-modal representation from large-scaleof multi-lingual image-to-text pairs. Two new pre-training tasks,Masked Region-to-Token Language Modeling and Visual Transla-tion Language Modeling, are proposed to facilitate the model toobtain better alignment between vision and different languages. M3P . combines multi-lingual pre-training and multi-modalpre-training into a unified framework via multitask Learning. multi-modal code-switched training is proposed to further alleviate theissue of lacking enough labeled data for non-English multi-modaltasks and avoid the tendency to model the relationship betweenvision and English text. TD-MML . uses translated data for multi-lingual multi-modallearning, which are applied in both pre-training and fine-tuningdata with the existing CCP model. In order to prevent the modelfrom learning from low-quality translated texts, two metrics areproposed for automatically removing the low-quality translationtexts from the resulting datasets. CCLM . is a CCP framework that unifies cross-lingual pre-training and cross-modal pretraining with shared architectures andobjectives. Contrastive learning is introduced for cross-modal andcross-lingual alignment, respectively.",
  "B.2Evaluation Dataset": "xFlickr&CO. is a novel dataset purposed by ICLUE andcollected by combining 1000 images from Flickr30K and COCOrespectively. The existing captions from and are used forEnglish and Japanese, while the captions are from crowd-sourcefor the other 6 languages. WIT. means Wikipedia-based Image-Text dataset col-lected instances from the websites of Wikipedia in 108 languages.For training, a subset of 500K captions is randomly sampled fromthe English training set of WIT. For evaluation, the WIT test datareleased as part of its corresponding Kaggle competition3 is used. Multi30K. extends Flickr30K from English to German, Frenchand Czech. It contains 31,783 images obtained from Flickr and pro-vides five captions per image in English and German, and onecaption per image in French and Czech. Dataset splits are definedas the original Flickr30K. COCO. extends the original COCO Caption by translatingthe captions into Japanese and Chinese. The Japanese and Chinesesubsets consist of 820k and 20k captions respectively. Followingprevious work, we use the same train, dev, and test splits for Englishand Japanese as defined by Karpathy and Fei-Fei . For Chinese,we use the COCO-CN split .",
  "C.2Hyperparameter Setting": "For zero-shot xFlickr&CO and WIT, we first fine-tune the model onthe English training set, and then evaluate zero-shot and few-shotperformance in other languages. Following , for both zero-shotand few-shot experiments, we use AdamW optimizer with 1 = 0.9and 2 = 0.999; weight decay is set to 0.01; learning rate scheduleris linear. The all hyper-parameters used are shown in .",
  "C.3The Method of Hard Negative Sampling": "For positive samples, given an image , its associated set of texts(1,2, ...,) can be regarded as positive samples. Among thesetexts, the hardest positive sample pos can be identified as the textthat aligns worst with the image, and the degree of alignment canbe estimated by computing the cosine similarity between the imageand text representations. Accordingly, we can sample the indexpos of the hardest positive sample from a specific distribution ,which can be expressed as",
  "(8)": "where is a multinomial distribution.For negative samples, if the image and the text from differenttuples are well aligned, they can be regarded as hard negative sam-ples for each other. Also, we estimate the degree of alignment usingthe cosine similarity and sample the index of the negative examplefrom a multinomial distribution. Thus, the process of obtaining thehard negative image can be expressed as",
  "C.4The Method of Rank": "We obtain the representations from the text encoder and imageencoder outputs and rank the candidates by cosine similarity. ForCCR and ablation models containing the fusion encoder, we re-rank only the top candidates using the Fusion encoder to betteradapt to the web-scale data. Specifically, we use the projection headused for the multi-lingual image-text matching task to predict thematch probability between the query and each shortlisted candidateand re-rank the candidates regarding this probability only. In ourexperiment, is 256 for COCO and 128 for the other three datasets.",
  "DTime and Memory Comparison": "We compare the models training time and GPU memory consump-tion for different language numbers of translated texts, which arereported in . The results in the table are the average resultsmeasured while keeping other external conditions constant as muchas possible. It is easy to find that both training time and memoryusage increase linearly with the number of languages. Specifically,the training time increases by 4.2 min per language for 1 Epoch,while the memory footprint increases by 710 MB per language perNvidia A100 40GB."
}