{
  "Abstract": "Retrieval-augmented generation (RAG) is a framework enablinglarge language models (LLMs) to enhance their accuracy and re-duce hallucinations by integrating external knowledge bases. Inthis paper, we introduce a hybrid RAG system enhanced througha comprehensive suite of optimizations that significantly improveretrieval quality, augment reasoning capabilities, and refine nu-merical computation ability. We refined the text chunks and tablesin web pages, added attribute predictors to reduce hallucinations,conducted LLM Knowledge Extractor and Knowledge Graph Ex-tractor, and finally built a reasoning strategy with all the references.We evaluated our system on the CRAG dataset through the MetaCRAG KDD Cup 2024 Competition. Both the local and online evalu-ations demonstrate that our system significantly enhances complexreasoning capabilities. In local evaluations, we have significantlyimproved accuracy and reduced error rates compared to the base-line model, achieving a notable increase in scores. In the mean-while, we have attained outstanding results in online assessments, We participated in Meta CRAG KDD Cup 2024 as Team ElectricSheep, securing thirdplace in Task 1 and achieving first place in five of the seven question types in Task 2among over 2, 000 participants and 5, 500 submissions. For access to the competition de-tails and the leaderboard, please refer to the following URLs: and Yuan is the leader of the team ElectricSheep.The project was conducted as part of Gongbo Suns research internship at the StateKey Laboratory of Multimedia Information Processing, Peking University.Ming Zhang is the advisor of the team. Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for third-party components of this work must be honored.For all other uses, contact the owner/author(s).Conference KDDCup Workshop of SIGKDD 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s).",
  "Large Language Models, Language Generation, Retrieval-AugmentedGeneration, Reasoning": "ACM Reference Format:Ye Yuan, Chengwu Liu, Jingyang Yuan, Gongbo Sun, Siqi Li, and MingZhang. 2024. A Hybrid RAG System with Comprehensive Enhancementon Complex Reasoning. In KDDCup Workshop of SIGKDD 24: KDDCupWorkshop of the 30th ACM SIGKDD Conference on Knowledge Discovery andData Mining, August 2529, 2024, Barcelona, Spain. ACM, New York, NY,USA, 13 pages.",
  "Introduction": "Pre-trained large language models (LLMs), such as Llama3 ,GPT-4 , Mistral-7B , Gemini family and Qwen-2 ,have demonstrated impressive advancements in the field of NaturalLanguage Processing (NLP), notably in the Question-Answeringtask. This success is built on foundational knowledge, includingfactual knowledge , relational knowledge , and linguisticknowledge , acquired by LLMs through exposure to the largescale internet corpus during training . Prior studies have shownthe ability of LLMs in knowledge internalization and generation, which knowledge is implicitly stored within modelsparameters and retrieved during the generation process.",
  "Conference KDDCup Workshop of SIGKDD 24, August 2529, 2024, Barcelona, SpainYe, et al": "Open Domain. The question_type was divided into the followingcategories: simple question, simple question with some condition,set question, comparison question, aggregation question, multi-hop question, post-processing question, and false premise question.However, the question_type attribute is hard to predict becauseit often needs reasoning through all references, so we didnt ap-ply attribute prediction to it. Additionally, the static_or_dynamicattribute, which pertains to timeliness, was classified into four cat-egories: real-time, fast-changing, slow-changing, and stable. More-over, we found that the boundaries between these four categoriesare not clear, so we only classified this attribute into two cate-gories: static and dynamic. For each attribute, we implemented twomethods for question classification: one leveraging the in-contextlearning capability of large language models and the other employ-ing support vector machines (SVM).In-Context Learning. Large language models demonstrate robustnatural language understanding and strong multi-task generaliza-tion abilities. We prompt the model with classification instructionsand 5 demonstrations for categories, instructing it to classify subse-quent questions. The demonstrations are randomly selected fromthe publicly available CRAG dataset. All prompts utilized in theclassification process can be found in Appendix D.1.To enhance classification reliability, we adopted a self-consistencystrategy involving multiple samplings from the large languagemodel. The category that appeared most frequently among the sam-pled results was designated as the classification for the question.SVM. We also tried to train an SVM classifier using the CRAGpublic dataset to reduce computational overhead. We used theall-MiniLM-L6-v2 model to get the sentence embeddings, whichare used to train the SVM. We observed that the SVM model canget higher accuracy in predicting the attributes with less time andcomputation consumption. However, we didnt have time to mergethe code into our final version for evaluation. So, consequently,the submitted version relied on the few-shot learning approachwith the large language model for classification, and we leave theimprovement for this module as future work. We will show thedetailed results analysis and comparison in .1.",
  "Related Works": "Numerous techniques have been proposed to address the aforemen-tioned issues. For example, formal verification can help to reducehallucination and output the verified reasoning process.Moreover, there are some efficient training methods thatcan help the model adapt to domain-specific knowledge. Whilenumerous approaches have been proposed in the literature, themajority are tailored to address specific issues within a limitedrange of scenarios, making them unsuitable for direct applicationto CRAG tasks. We present a compilation of recent research to high-light the diverse design choices within the broader RAG researchcommunity. Drawing inspiration from recent advancements in thefield, we have developed a novel design that integrates multiplestrategies.We built upon previous research to structurethe conventional RAG workflow into four phases: pre-retrieval,retrieval, post-retrieval, and generation.",
  "Pre-retrieval": "The pre-retrieval phase involves indexing, query manipulation, anddata modification. That is, compiling and indexing external knowl-edge sources to enhance the efficiency of subsequent searchingexecution, refining the query to better match the external data dis-tribution and modifying data sources to create a coherent represen-tation that encapsulates relevant knowledge for further reasoning.Indexing: Text indexing has emerged as a prominent area of re-search within the field of data mining, leading to the developmentof numerous methodologies. Traditional indexing methods includethe inverted index , suffix tree , and term frequency-inversedocument frequency (TF-IDF) index . These indexing methodsutilize term frequency so that they may neglect the semantic in-formation inherent in the text. Recent indexing methods employlanguage models such as BERT and Sentence-T5 to encodetext into vectors within latent space. These vectors are used toconstruct vector indexes that enhance similarity search throughtechniques including product quantization (PQ) , HierarchicalNavigable Small World (HNSW) , and Navigating Spreading-outGraph (NSG) .Query Manipulation: The manipulation algorithms discussedbelow contribute to developing an improved metric for evaluatingthe similarity between user queries and external data. It is desir-able to formulate a fully specified, context-independent query thateffectively articulates user intent and aligns with the distributionof external knowledge sources. Early research primarily utilizes ann-gram language model derived from hundreds of millions of websearch sessions or employs a pointer-generator network to",
  "A Hybrid RAG System with Comprehensive Enhancement on Complex ReasoningConference KDDCup Workshop of SIGKDD 24, August 2529, 2024, Barcelona, Spain": "although with zero-shot CoT reasoning. To balance the hallucina-tion and knowledge from the LLM itself, we only treat the outputof this module as one of the references. We have carefully designedprompts to ensure that the model neither overly relies on documentreferences nor excessively trusts the LLMs knowledge. .6will provide a more detailed introduction.",
  "Retrieval": "The retrieval phase primarily involves searching for pertinent mate-rials from external knowledge sources and ranking them accordingto their relevance to the user query.Search: Searching for relevant documents efficiently from vastexternal knowledge sources, such as the entire Internet, can beexceedingly challenging. Previous research on RAG that aims toretrieve local data typically employs local vector databases, suchas Elasticsearch , or utilizes libraries like FAISS for efficientsimilarity search and vector clustering. On the other hand, researchfocused on general-purpose RAG typically utilizes external websearch engines directly; for instance, WebGPT employs theBing API.Ranking: The ranking metric establishes the priority of each doc-ument housed in external knowledge sources. The metric used toevaluate the similarity between a potentially manipulated queryand a document can be classified into two categories: sparse re-trieval and dense retrieval. Sparse retrieval metrics, such as BM25 and Term FrequencyInverse Document Frequency (TF-IDF) , rely on word frequency, whereas dense retrieval metricsutilize embedded vector, including Euclidean distance , cosinesimilarity , and Dense Passage Retrieval (DPR) .",
  "Post-retrieval": "The post-retrieval phase includes re-ranking and filtering, whichfurther refines the ranking results, and filtering out materials thatare irrelevant to the querying topic.Re-ranking: Although vector similarity-based document retrievalcan be executed efficiently and effectively, including in parallel or distributed systems , its capacity to reveal semanticrelationships between queries and documents remains constrained. This is where re-ranking becomes significant: typically big-ger models, which offer greater accuracy but lower efficiency, canprecisely reorder the limited set of documents retrieved. Some ap-proaches embed the query-document pair in a single pass,facilitating cross-attention between sequences. These approachesenhance the evaluation of mutual information between the texts. Other studies employ large language models as few-shot annota-tors to generate data for training a cross-attention re-ranker , orinvestigate the auto-regressive generation of re-ranking results toleverage inter-document relationships .Filtering: Filtering seeks to remove redundant parts from the re-trieval results, which may either be of poor quality or exhibit lowrelevance to the user query . Current studies employ large lan-guage models to evaluate the utility of retrieved information andto critique whether all verification-worthy statements are substan-tiated by the associated documents , or condense retrieveddocuments into a textual summary, thereby minimizing inferencecosts .",
  "Generation": "The curated reference materials are subsequently processed in thegeneration phase to produce the final results. Additionally, variousenhancement techniques and customizations can be applied to thesame phrase.Enhancing: Enhancing, also known as retrieval fusion, seeks toimprove the performance of large language model generations byutilizing retrieved documents. Typically, the retrieved documentsare concatenated with the query in the expectation that the lan-guage models will generate coherent results based on the referencematerials . Other methods utilize sophisticated prompttemplates that integrate multiple pieces of information or em-ploy techniques such as context compression, summarization, andfiltering to achieve more efficient inference . Severalstudies have also investigated the integration of encoded retrievalvectors into input features .Customization: During the customization process, it is crucial tometiculously refine the models outputs to ensure alignment be-tween the models responses and the users intended queries. Thisrefinement will result in final answers that are concise, informative,and well-formatted. These approaches include generating reflec-tion tokens for self-evaluation of the models output and imple-menting a graph-text contrastive learning method to enhance thealignment between generated content and retrieved references.",
  "CRAG Benchmark": "CRAG benchmark is a factual question-answering benchmarkwith thousands of QA pairs and 50 real-world web pages for eachdata. The benchmark also provides a mock API for KnowledgeGraph(KG) searching. The benchmark is set for the KDD Cup 2024with 2, 706 data items available in public, half of which are forpublic validation. There are 5 domains and 8 question types in thebenchmark, and each data item has a static_or_dynamic labelthat indicates whether the answer to a question changes and theexpected rate of change, which can be used to analyze the modelstrengths and weaknesses. In order to mimic real-world applicationscenarios, each generated response is limited to 30 seconds on anAWS G4dn.12xlarge instance, which is equipped with 4 NVIDIA T4GPUs providing a total of 64 GB of GPU memory during inference.The benchmark is split into three tasks in the competition: Re-trieval Summarization, Knowledge Graph and Web Retrieval, andEnd-to-End Retrieval Augmented Generation, which are from sim-ple to complex. We introduce the three tasks as follows:",
  "System Design": "The complete design of our system is shown in . There are6 critical modules in our system, including (1) web page processing,(2) attribute predictor, (3) numerical calculator, (4) LLM knowledgeextractor, (5) KG Module, and (6) reasoning module. We have en-hanced the systems capabilities in information extraction, reducinghallucinations, numerical calculation accuracy, and higher-orderreasoning through these modules. Additionally, we have imple-mented special handling for corner cases. We will introduce thesemodules as follows.",
  "Web Page Processing": "Web pages serve as a shared information source for all three tasks,containing a substantial amount of potentially valuable informa-tion that can aid the model in task completion. As a result, webpage processing is a critical component of system design, directlyimpacting both the quality of the extracted information and theaccuracy of subsequent language model generations.However, despite the abundance of information presented innatural language on web pages, extracting this information is notstraightforward. This complexity is due to the frequent presence ofsignificant amounts of noise that does not contribute relevant infor-mation necessary for task completion. Such noise can unnecessarilyprolong the models processing and reasoning time, potentially lead-ing to misinterpretations. The types of noise encountered includedecorative HTML tags used for typography, JavaScript code, and in-ternal comments within the web page. While some HTML tags maycontain semantic information that aids in paragraph segmentationor title identification, the useful information is not easy to extract.Moreover, there is some structured information in the HTML liketables, which will do harm to the text quality if they are improperlyhandled through methods such as truncation or splicing other texts.So we process the raw web page into two parts: text chunks andtables, to get the references with higher quality.Text Chunks Processing. To address the challenges posed by thecomplexity of modern HTML web pages, we adopt trafilatura, aPython library specifically designed for gathering text from the Web.This library effectively mitigates noise generated by recurring ele-ments such as headers, footers, and links. We also clean all the tablesby identifying the <table> tag, which will be processed specially.To enhance robustness, we employ the classic BeautifulSoup li-brary as a fallback option for a small subset of web pages thattrafilatura cannot process. After extracting text using these twotools, we utilize functions provided by the Blingfire library tosegment the text into individual sentences.The meaning of a sentence is often influenced by its contextualplacement; therefore, the information within a single sentence isoften incomplete. We organize sentences into chunks based on thefollowing rules to enhance semantic coherence in subsequent re-trieval processes. First, we truncate individual sentences that exceeda predetermined length threshold to ensure they remain within anappropriate length. Next, we implement a keyword-based approachto identify whether a sentence is a question, utilizing indicatorssuch as the 5W1H interrogatives at the beginning and the presenceof question marks at the end. All keywords utilized in this processcan be found in Appendix B. Sample data analysis has shown thatquestions are often immediately followed by their correspondinganswers. Therefore, we concatenate questions with their subse-quent text until we reach the pre-assigned length threshold. Finally,we connect any remaining ungrouped sentences in sets of 3.Tables Processing. Given that the extracted text data typically doesnot include tables, we have employed BeautifulSoup to extracttables from web pages and convert them into Markdown format.We hypothesize that exposure to numerous documents formattedin Markdown during the model training phase will improve themodels understanding and interpretation of this format. Finally,",
  "Transformation": ": The design of our web page processing. We utilizedTrafilatura and BeautifulSoup to extract plain text and ta-bles from web pages. Following this extraction, we employedBlingfire to segment the plain text into sentences, whichwere then grouped into chunks based on heuristic methods.Additionally, the tables were converted into Markdown for-mat for further processing. we cleaned the empty tables to reduce the noise. The source codefor table transformation is listed in Appendix C.Text Embedding & Ranking Metrics. Regarding the rankingmetrics and methodologies for retrieval, it is important to note thatin the CRAG tasks, each question is associated with a relativelysmall number of candidate articles (five for each question in theinitial two tasks). Consequently, we have adopted a straightfor-ward approach. We utilized the sentence-t5-large model to generate vector embeddings for both the text chunks and thequeries, and we employed cosine similarity as the metric to rankthe relevance of the text chunks. The cosine similarity betweenthe user query embedding and a text chunk embedding q and c is",
  "qc": "where q c is the dot product of embedding vectors q and c, andq and c are their respective magnitudes. Our experimentsindicated that employing LLMs for complex query manipulationmethods did not significantly improve retrieval accuracy; instead,it resulted in substantial computational overhead. Therefore, weprimarily leveraged the LLM knowledge extractor, as detailed below,to enhance the quality of the references.",
  "Attribute Predictor": "Large language models demonstrate considerable variability in theirperformance across various question-answering tasks. Within thecontext of the CRAG tasks, answering aggregation questions andmulti-hop questions poses greater challenges than addressing sim-ple questions. This difficulty arises from the models need to notonly possess robust information retrieval capabilities but also tointegrate multiple sources of information and engage in reasoningprocesses. Moreover, when dealing with questions related to slow-changing and fast-changing facts, the model must exhibit temporalawareness, which adds complexity to the generation of accurateresponses . To tackle these challenges, we have developed anattribute predictor that assesses the type of each specific questionand the rate of underlying factual change, aiming to optimize per-formance across all question types.There are three useful attributes in the benchmark: domain,question_type, and static_or_dynamic. Following the descrip-tions outlined in the CRAG dataset, we classified the domain intofive categories: Finance, Sports, Music, Movies, and Encyclopedia",
  "Numerical Calculator": "Previous research has highlighted the phenomenon of halluci-nation in large language models, particularly concerning theirperformance in precise numerical calculations . Within theframework of the CRAG tasks, answers to aggregation questions,multi-hop questions, and post-processing questions are not directlyavailable in the retrieved content. Instead, these tasks require themodel to derive the final answer through a series of reasoning steps,which often involve precise numerical computations, especially inthe domain of finance and sports. This requirement poses additionalchallenges to the models capacity to generate accurate results.To address this issue, we employed an approach that leverages ex-ternal tools, drawing inspiration from previous research .We encourage the large language model to articulate the reasoningsteps necessary to solve the problem as mathematical expressionswhile delegating the actual numerical calculations to an externalPython interpreter. We specifically integrate retrieved text chunksand tables that may contain numerical information into the models prompts and employ prompt techniques that encourage the modelto generate valid Python expressions directly. Detailed specifica-tions of the prompts are provided in the Appendix D.2 In the sub-mitted version, we utilized multiple sampling and processed thegenerated Python expressions using the eval function.Note that the program code generated by the LLM may includemalicious code, and executing such code directly poses a potentialthreat to system stability. To mitigate such risk, the best practice forensuring system security is to use ast.literal_eval or to executethe code within a sandbox environment. We leave the safety andproper termination of sandbox execution as future work.",
  "LLM Knowledge Extractor": "Through extensive training on diverse corpora, LLMs have ac-quired substantial knowledge and demonstrated robust question-answering capabilities across a wide range of domains. Chancesare that the reference documents are not beneficial in generatingaccurate responses to the models queries. This may be attributed tothe possibility that the retrieved materials are outdated, or they mayinclude irrelevant or even misleading information. Consequently,these reference documents do not enhance the models capacityto produce answers but harm it. Furthermore, during the genera-tion phase, we employ the Llama 3 instruction-tuned models thatare optimized for dialogue use cases and possess a robust capa-bility for following instructions. Our findings indicate that whenreference documents are provided within the prompt, the modelexhibits a significant tendency to extract answers from these docu-ments, regardless of whether the documents contain the necessaryinformation. This inclination may be a result of the instructionalfine-tuning process. In contrast, LLMs that operate without any ref-erences are capable of accurately answering these questions. In lightof this observation and drawing inspiration from prior research, we developed a large language model knowledge extractor.This extractor leverages the knowledge-rich responses generatedby the large language model as part of the reference materials forenhanced reasoning.The process of extracting knowledge from the model closelyresembles the normal model generation process. It similarly utilizeszero-shot indications, which include prompts requiring the modelto assess whether a given query pertains to a false-premise issueand to generate more concise responses. However, a notable distinc-tion exists in the lack of reference documents sourced from externalknowledge bases within the prompts, as well as the exclusion of mul-tiple sampling, which is intended to reduce computational overhead.In this way, the LLM could respond solely based on the knowledgeinternalized within its parameters during the training. Our find-ings suggest that this approach results in favorable performance onquestions classified as slow-changing and stable. We anticipate thatthis approach will effectively align the knowledge embedded in theLLMs parameters with external reference documents, thereby mit-igating the issue of the models excessive dependence on externallyretrieved information. Moreover, we also use the zero-shot CoT tolet the model reasoning by itself for more accurate knowledge. Theprompt template is shown in Appendix D.3However, as described previously, letting the model directly an-swer the questions will introduce hallucinations in its knowledge,",
  "Knowledge Graph Module": "In addition to web references, Task 2 and Task 3 also provide amock API for querying the provided knowledge graph (KG). As astructured knowledge base, a KG provides accurate information.However, the generation of a KG query is crucial to determiningwhether the system can retrieve the correct answer. We startedfrom the KG baseline, which extracted the entities in the querywith an LLM, and generated the query by manual rules. The qualityof rule-based queries is limited by the complexity of the rules, andhard to scale. So we tried a function-calling method, which makesall the mock APIs as the input of the LLM, and lets it generate aproper function calling. However, due to limitations in time andresources, we were unable to optimize the models and prompts forthe function-calling method, resulting in suboptimal performance.Therefore, we reverted to the KG baseline method and did notmake further improvements in the submitted version. We show theprompts for the function-calling methods in Appendix D.4.",
  "Reasoning Module": "After all the previously introduced processing methods, we gettext chunks, tables, triplets from KG, and knowledge from LLMweights as the references. We carefully designed a prompt templateto let the LLM do reasoning from all these references and get thefinal answer. We control the reasoning process by output formatdemonstration and zero-shot CoT, which is useful for multi-hopquestions. Leveraging the strong instruction-following capabili-ties of Llama3-70B-Instruct, weve successfully maintained steadyprogress in controlling reasoning tasks. We designed several rulesto constrain the reasoning path and output format, including thatthe output should be precise, and guide the model reasoning byasking intermediate questions in the prompt. The complete promptis shown in Appendix D.5.",
  "Handling Corner Cases": "In addition to the main modules mentioned above, we have also han-dled many corner cases, including (1) identifying invalid questions;(2) encouraging the model to answer I dont know for unsureanswers to reduce hallucination; and (3) dealing with outputs thatdo not comply with the instruction format. We will introduce ourdesign to handle these corner cases as follows.Invalid Questions. There are some questions that have falsepremises, which means the query is contradictory to the fact. Forthese questions, the model should output invalid questions. Toidentify this type of question, the model needs to carefully ana-lyze the references provided. We add special rules in the reasoningprompt shown in Appendix D.5Reduce Hallucination. We employed two approaches to alleviatehallucinations: attribute prediction and reasoning. We found thatthe time-changing questions, which would be labeled as dynamic",
  ": The main results of our designed system evaluatedin the public test dataset": "by attribute predictor, are hard for our system and we do not haveenough time and resources to improve them. So we manually letthe system answer I dont know for these questions. Moreover,we added several rules and prompt engineering techniques in thereasoning module to let the model answer I dont know whenit is unsure. Ultimately, we configured the system to exclusivelyoutput \"I dont know\" and refrain from adding any additional wordswhenever \"I dont know\" is included in the initial response.Incorrect Format. Cause we didnt conduct constrained samplingfor the reasoning output, there is the possibility that the model willoutput answers that can not be parsed. To handle this situation,we design a backup summarization agent to summarize the finalanswer precisely and concisely based on the reasoning modulesoutput when the parse fails. The prompt for this module is shownin Appendix D.6",
  "Experiments": "We conducted ample experiments to verify the effectiveness ofeach module. The main results of our local evaluation are shownin , where we got a lot of improvement in the public testset compared with the baseline of Task 1. We got a 15.8% scorewhere we greatly reduced the hallucination ratio and changed thesehallucinations into answering I dont know. We show detailedanalysis and ablation studies of our evaluation results. In the finalprivate test, we got a 21.8% score in Task 1. We will also showan analysis of the private evaluation. We will use Official RAGBaseline as the baseline model of Task 1 in the following tables.",
  "Detailed Analysis": "To gain a deeper understanding of the strengths and weaknessesof our system across various aspects, we conducted a meticulousanalysis of the evaluation results on the public test set. shows the detailed scores in Task 1 setting.For the domain attribute, we perform well in areas such asmovies, music, and open topics, but our performance is lackingin finance and sports. This is due to the fact that these two domainsrequire the model to have the ability to answer dynamic informa-tion that changes over time. To prevent hallucinations, our modelopts to refuse to answer such queries. The performance regard-ing the attribute of dynamism reaches the same conclusion: as thedynamism of the model increases, the effectiveness of our systemgradually declines. Regarding the score distribution across ques-tion types, our analysis indicates that the system exhibits superiorperformance on tasks requiring complex reasoning, which benefitsfrom the robust functionality of our integrated reasoning module.",
  ": Ablation results of our system. The modules aregradually added to the system. The results are tested in Task1 and Task 2, which are only different in using KG": "beneficial modules. Consequently, comprehensive ablation studies,which would involve removing each component from the finalsystem, were not conducted. Instead, we documented the rationalebehind the inclusion of each module and its resulting improvements. outlines the primary construction pathway of our system.We started from the baseline model of Task 11 and did a lot ofrefinement and added modules to it. As shown in , each mod-ule we added would increase the final score. The main optimizationdirections are reducing hallucinations and increasing correctness.",
  "Analysis For Private Evaluation": "Although the competition organizers have not provided a compre-hensive and detailed analysis of the results on the private leader-board, we can still present the currently published results and an-alyze our system. shows the score for Task 1 in privateevaluation. Our system achieved scores close to the champion inTask 1 but fell significantly behind in Task 2 and Task 3. We be-lieve this is due to our underutilization of the information from theknowledge graph. shows the prizes we won from 5 out of 7question types in Task 2. We find that our system performs well onquestion types that require complex reasoning, such as aggregationand multi-hop, which we attribute to our reasoning module.",
  ": The evaluation results in the private test set": "Furthermore, illustrates the scores across various at-tributes, revealing that our focus has been on static and slow-changing questions, neglecting the challenging time-varying ones.This deficiency in handling dynamic questions has also led to sub-optimal performance in financial question types. The score resultsalign with those from our internal assessment; however, we en-countered a higher number of incorrect responses in the moviedomain and on simple question types. We hypothesize that thisdiscrepancy may stem from variations in question distribution be-tween our local evaluation and online test datasets. Additionally,our performance in popularity aligns well with the expectationsset forth in the CRAG Benchmark .",
  "Conclusion": "With the designed RAG system, we finally got 3rd place in task1 and got the prize for 5 out of 7 question types in task 2. Thefinal evaluation scores can be found in the Winners Announcement.The table extractor, reasoning module, and calculator module havedemonstrated substantial enhancements over the baseline system.Additionally, the ICL attribute predictor has significantly reducedhallucinations in responses to difficult questions.",
  "Discussions": "Many aspects of our system can be improved. For example, weonly used two tower models for retrieval, while re-ranker modelsare more suitable for task 1. Moreover, a two-stage retrieval andre-ranker system should be used for task 3. We didnt optimize theKG information retrieval in task 2, which can be improved a lot inthe future. The current methods for handling tables are relativelysimple. Some tables are useless or too large with a lot of noise,but we didnt handle these cases. There should be retrieval andstructural query methods specifically designed for tables. This project is partially supported by the National Key Research andDevelopment Program of China with Grant No. 2023YFC3341203as well as the National Natural Science Foundation of China withGrant Number 62276002.We thank Prof. Ming Zhangs mentorship and support to theteam and Alan Wus assistance and discussions.",
  "Gautier Izacard and Edouard Grave. 2021. Leveraging Passage Retrieval with Gen-erative Models for Open Domain Question Answering. arXiv:cs.CL/2007.01282": "Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni,et al. 2023. Atlas: Few-shot learning with retrieval augmented language models.Journal of Machine Learning Research 24, 251 (2023), 143. Bernard J Jansen, Danielle L Booth, and Amanda Spink. 2009. Patterns of query re-formulation during web searching. Journal of the american society for informationscience and technology 60, 7 (2009), 13581371.",
  "Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and MikeLewis. 2019. Generalization through memorization: Nearest neighbor languagemodels. arXiv preprint arXiv:1911.00172 (2019)": "Omar Khattab and Matei Zaharia. 2020. Colbert: Efficient and effective passagesearch via contextualized late interaction over bert. In Proceedings of the 43rdInternational ACM SIGIR conference on research and development in InformationRetrieval. 3948. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks.Advances in Neural Information Processing Systems 33 (2020), 94599474.",
  "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, et al.2022. Training language models to follow instructions with human feedback.arXiv:cs.CL/2203.02155": "Yu Pan, Ye Yuan, Yichun Yin, Jiaxin Shi, Zenglin Xu, et al. 2024. PreparingLessons for Progressive Training on Language Models. In Proceedings of the AAAIConference on Artificial Intelligence, Vol. 38. 1886018868. Yu Pan, Ye Yuan, Yichun Yin, Zenglin Xu, Lifeng Shang, et al. 2023. Reusingpretrained models by multi-linear operators for efficient training. Advances inNeural Information Processing Systems 36 (2023), 32483262. Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, LukeZettlemoyer, et al. 2023. Art: Automatic multi-step reasoning and tool-use forlarge language models. arXiv preprint arXiv:2303.09014 (2023).",
  "Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, et al. 2023. Fresh-LLMs: Refreshing Large Language Models with Search Engine Augmentation.arXiv:cs.CL/2310.03214": "Haiming Wang, Ye Yuan, Zhengying Liu, Jianhao Shen, Yichun Yin, et al. 2023. Dt-solver: Automated theorem proving with dynamic-tree sampling guided by proof-level value function. In Proceedings of the 61st Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Papers). 1263212646. Haoyu Wang, Tuo Zhao, and Jing Gao. 2024. BlendFilter: Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and Knowl-edge Filtering. arXiv preprint arXiv:2402.11129 (2024).",
  "\"system_prompt\": \"You will be provided with a question. Your task is to identify whether this": "question is a static question or a dynamic question. A static question is that theanswer is fixed and will not change over time. A dynamic question is that the answerwill change over time or needs time information. You **MUST** choose from one of thefollowing choices: [\\\"static\\\", \\\"dynamic\\\"]. You **MUST** give the question typesuccinctly, using the fewest words possible.\\nHere are some examples:\\n\" + \\"
}