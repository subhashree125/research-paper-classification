{
  "ABSTRACT": "With the rise of e-commerce and short videos, online recommendersystems that can capture users interests and update new items inreal-time play an increasingly important role. In both online andoffline recommendation, the cold-start problem due to interactionsparsity has been affecting the recommendation effect of cold-startitems, which is also known as the long-tail problem of item distri-bution. Many cold-start scheme based on fine-tuning or knowledgetransferring shows excellent performance on offline recommenda-tion. Yet, these schemes are infeasible for online recommendationon streaming data pipelines due to different training method, com-putational overhead and time constraints. Inspired by the abovequestions, we propose a model-agnostic recommendation algorithmcalled Popularity-Aware Meta-learning (PAM), to address the itemcold-start problem under streaming data settings. PAM dividesthe incoming data into different meta-learning tasks by predefineditem popularity thresholds. The model can distinguish and reweightbehavior-related and content-related features in each task based ontheir different roles in different popularity levels, thus adapting torecommendations for cold-start samples. These task-fixing designsignificantly reduces additional computation and storage costs com-pared to offline methods. Furthermore, PAM also introduced dataaugmentation and an additional self-supervised loss specificallydesigned for low-popularity tasks, leveraging insights from high-popularity samples. This approach effectively mitigates the issueof inadequate supervision due to the scarcity of cold-start samples.Experimental results across multiple public datasets demonstrate",
  "Corresponding author": "Yunze Luo, Jingchi Wang, and Kaigui Bian are affiliated with School of CS, AI Innova-tion Center, National Engineering Laboratory for Big Data Analysis and Applications,State Key Laboratory of Multimedia Information Processing, Peking University. Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 25, August 3-7, 2025, Toronto, Canada 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-1-4503-XXXX-X/18/06",
  "INTRODUCTION": "Platforms that depends on online recommendation service, includ-ing short video, e-commerce, and streaming media, play an indis-pensable role in peoples lives. Online recommender systems needto capture interest shifts in the system through periodic updateswhile taking into account the historical information of users anditems. Many previous works have demonstrated superiorperformance in the realm of online recommendation. These onlinesystems are characterized by a large amount of data, strict time-consuming constraints, changes in global information (e.g. userinterests, item popularity) over time and the requirement ofone epoch streaming training. A common way to train the systemstreamingly is continuously optimizing the model to capture in-formation shifts. The same network parameters are applied for allusers and items, and the parameters are updated using data arrivingduring the period to avoid being stale.Another inevitable recommendation scenario is the cold-startproblem caused by sparse consumption data of new users or items.The fact that low-popularity items comprise a huge portion of thetotal items and their interaction data represents only a small frac-tion of the overall consumption data is also concerning.As a consequence of this long-tail distribution, the model excels in",
  "SIGKDD 25, August 3-7, 2025, Toronto, CanadaYunze Luo et al": "recommending popular items but struggles when it comes to rec-ommending cold items. On the flip side, interactions with popularitems provide a more accurate reflection of user interests and itemfeatures. Even recommendations for cold-start items heavily relyon the patterns learned by the model from these interactions.Such cold-start problem has garnered considerable attention andhas been addressed through various solutions in recommendationscenarios. The representation-based approach considersitems as tasks and generates personalized parameters for differentitems to enhance the effectiveness of cold-start item recommen-dation, such as meta-learning and knowledge transfer .The side-information-based approach focuses on usingcontent-based side information and pay less attention to the itemspopularity-related information, which also have different impor-tance in the recommendation of cold and popular items. However,the methods above are still challenging to apply in online scenarioswith streaming data.The cold-start problem in online recommendations faces twomain challenges. On the one hand, in the combination of online andcold-start recommendation, the Matthew effect makes the sys-tem optimize the distribution of popular items further. Because thepopular items make up the majority of the training data leading tobetter performance on popular items, thus the data further increasesthe proportion of popular data over time. This effect exacerbates theproblem of cold-starts in online systems. Meanwhile, the require-ment of training streaming data with one epoch makes the existingcold-start methods inapplicable, and the time consumption andcomputational demand make generating personalized parametersfor items impractical in online scenarios.In this work, we have introduced a novel model-agnostic Popu-larity-Awared Meta-learning framework (PAM) for solving the itemcold-start problem under streaming data in online systems. Firstly,we propose a fixed task segmentation with predefined popularitythresholds in the gradient-based meta-learning . We leveragemeta-learnings ability to share meta-parameters between tasksand generate personalized parameters for different tasks. In thisway, we optimise the performance of cold-start tasks without losingthe interest information of streaming data, and the personalizedparameters also avoid making the system over-fitting popular itemdata, solving the Matthew cold-start problem in online scenarios. InPAM, tasks of varying popularity levels can share meta-knowledgeand utilize popularity-related features and content-related featureswith different weights. Additionally, the fixed task segmentationenables efficient online inference without fine-tuning on each itemby storing different tasks parameters in advance, thus adapts tostreaming training scenarios while addressing the computationaloverhead and time constraints.Besides, we also design a cold-start task enhancer to furtherutilize various information from popular items in the system tooptimize the cold-start task. The enhancer classifies the embeddingsinto behavior-based and content-based (because of their differentroles in the recommendation of cold-start and popular items) andcomprises two auxiliary tasks based on these two types of embed-dings: 1) The data augmentation module constructs low-popularitysamples using high-popularity interaction data, thereby directlyincreasing the number of cold-start samples; 2) the self-supervised module replay historical cold-start interactions leveraging well-trained item embeddings as supervision, thereby promoting thefeature extraction capability for cold-start tasks.Our main contributions can be summarized as follows:",
  "Cold-Start Recommendation": "Solutions to the cold-start problem are usually categorized into side-information-based methods and fine-tuning-based methods .Cold-start recommendation methods based on side information arediverse . The more side-information is availableto the system in online recommendation, the better the perfor-mance of such methods. Approaches based on fine-tuning are alsosuitable for cold-start scenarios, where features are extracted in ad-vance through pre-training or meta-knowledge and can be quicklyadapted to the task to achieve better results when faced with a newscenario with fewer samples . In recent years, many hybridmethods that introduce side-information into few-shot learninghave also emerged. As a method that utilizes a meta-learning frame-work to introduce content information to make recommendationsfor items of different popularity, PAM is likewise a hybrid methodthat combines both. Meta-learning is also an efficient solution toadapt quickly to personalized parameters with a few samples, andwe will describe the various methods in the subsequent subsections.",
  "Online Recommendation": "The goal of online recommendation is to assist users in discoveringitems they may be interested in real-time, and be able to distributenew items in the system promptly. Unlike sequential recommenda-tions, which focus on the historical behavior of the system, onlinerecommendations also have to take into account real-time interestshifts and be able to generate recommendation parameters in a lowtime-consuming process. As mentioned earlier, the large amountof online data and the streaming training method leads to manyschemes that generate specialized parameters for sequential rec-ommendation in offline systems to be infeasible. There are manytypes of common approaches, including CTR prediction based Inc-CTR and DDP , and FIRE that proposes a incrementalrecommendation algorithm from the perspective of graph signalprocessing. Various types of meta-learning algorithms introducedin the next subsection also shows potential in online recommenda-tions. However, they still struggle with the long-tail distribution ofonline data and performance poorly on cold-start items.",
  "Meta-Learning": "Meta-learning , also known as learning to learn, is an emerg-ing few-shot learning method that seeks to quickly adapt to thecorresponding task with a few-shot data to achieve better recom-mendation results. The meta-learning training process divide thetraining set into support and query set, the former are used to gen-erate unique parameters and the latter are used to calculate loss fortraining. Since MeLU , meta-learning has also demonstratedits applicability in the field of recommender systems due to thehigh overlap between few-shot learning and cold-start recommen-dation scenarios . In recent work,ClusterSeq preserves secondary user preferences through asequential recommendation system based on meta-learning clus-tering. Online recommendation is also another scenario where thetraining process of meta-learning is applicable because of its char-acteristic of making recommendation model updates in a temporalorder . As an example, SML trains a meta-model to output new parameters based on the parameters of thepast moments and the data of the current moment. FORM adapts user-specific tasks and leverages the online meta leader tooptimize online training performance.",
  "PRELIMINARY3.1Notations": "In this work, we consider item cold-start recommendation task instreaming data scenario. The data arrives by time period {0, 1, ,, }, each period of data consists of user-item interactions(,) U I that happened during . The recommender systemis similarly updated by time, with , , representing differentparts recommender at time . The system maintains the embeddingmatrix parameters and the network weight parameters . Wealso define the initialization of the network weights as . Matricesand vectors are denoted by symbols in bold font. For a particularinteraction data (,), N represents the number of views ofthe item, i.e., the number of times the item has been clicked by theuser prior to this moment in time. In particular, for some pre-giventhreshold , items with popularity below that threshold arecalled cold-start items, and others are called popular items.",
  "Problem Statement": "Our goal is to consider the item cold-start problem in online recom-mender systems. In traditional online recommendation schemes,the system periodically retrains the system. To ensure that varioustypes of historical data in the system are preserved and the real-timeinterest is captured, the generation of the current recommendationsystem needs to encapsulate the past information and searchfor better performance on the preceding data period:",
  "We would highlight that PAM is a model-agnostic online trainingapproach. We utilize a dual-tower structure to ensure that it isuniform across all frameworks in this paper": "4.1.1Embedding layer. (,) , : The purpose of the em-bedding layer is to characterize the different information aboutthe user and the item, convert it into a vector, and output it tothe hidden layer for information extraction. Specifically, the em-bedding layer consists of multiple feature embedding matrices ofusers and items. For user , the system maintains multiple one-hotvectors for different features, which extract the users informationfrom the feature embedding matrix and then concatenate it intothe embedding vector:",
  "= = (4)": "4.1.2Hidden layer. , , : After the embedding vectorsare generated, they are fed into the hidden layer and ultimatelyoutput the top representations of the user and item. In our dual-tower model, the user and the item have their own fully connectedlayer parameters, and the embedding vector will be mapped tothe top representation by fully connected network layers = 1() and = 1(), where the -th layercan be represented as:",
  "where , are the weight matrix and bias vector of the -thhidden layer, and represents a ReLU activation function": "4.1.3Output layer. , : The output layer calculates thepredicted scores for the corresponding user-item pairs based onthe top-level representations of the users and items. We utilize theInfoNCE loss form to calculate the prediction and loss value.The prediction of interaction is calculated by sampling otherinteractions in the batch as negative samples:",
  "Popularity-Aware Meta-Learning": "As the cold-start item samples constitute only a small fractionof the online traffic, their influence on model updating is limited.Consequently, the recommender system may exhibit satisfactoryperformance for popular items but may perform poorly for cold-start items. However, directly excluding samples from popular itemscould result in reduced performance for all items. This is becausesamples from popular items also contain user interest-related in-formation, and the exclusion leads to an information loss.Hence, it naturally occurred to us to partition the tasks accordingto popularity and employ gradient-based meta-learning for recom-mendation. This popularity-aware segmentation enables us to strikea balance by avoiding excessive emphasis on highly popular itemswhile still leveraging the valuable information from interactionswith high-popularity items. Meanwhile, the segmentation is fixedduring online training and it offers benefits from two perspectives.Firstly, it mitigates the overhead associated with traditional meta-learning patterns, which treat every data instance as a separate task,thus requiring task-specific parameter updating and storage. Thesemethods incur unacceptable costs for online application. Secondly,as mentioned in , the meta-learner extracts versatile features forall tasks, and after fine-tuning on specific task, the model can reusethese features with different weights for fast adaption. In PAM,items with the same popularity are recommended with identicalfeatures, while tasks with different popularity share lower-levelembedding features. In other words, cold-start items can leveragemore popularity-independent features, i.e., content features, whilepopular items can rely more on popularity-related features, i.e.,historical feedback features. 4.2.1Fixed Task Segmentation. Specifically, for a designated num-ber of tasks and an interaction (,), the number of tasks towhich this interaction belongs is calculated by a piece-wise con-stant function:",
  "where represents the -th task, and is a metric reflects popu-larity, such as the number of click or sales volume. The function is defined by a set of pre-determined thresholds": "4.2.2Local Updates. The parameters in PAM follow a bi-level op-timization scheme, consisting of local updates and global updates.Before the training process starts, we initialize the global recom-mender parameters , , , . After the arrivalof a data batch , the batch will be divided into multiple parts{1 , , } according to the popularity of the item by function .Each portion of data is further divided into and , stand-ing for support and query set. For the -th task, the personalizedrecommender parameters can be obtained by local updates:",
  "L (, |)(9)": "where represents the inner loop learning rate, and L (|)represents the loss function of task on data set with an initial-ization of (see Eq. (7)). The recommender {, } will be usedto serve the items in task at the following time moment +1. Notethat the interaction data between the support set and the query sethave no overlap, hence it is meaningless to update the embeddingparameters of users and items in the local update, and the updatedparameters contain only weight parameters of the networks inEq. (9). We utilize the LSLR methods, maintaining an adaptivelearning rate for each network weight to alleviate overfitting andadvance the performance. 4.2.3Global Updates. The goal of the meta optimization is to mini-mize the sum of the losses of the query set over its parameters in each task . Since all of the parameters are involved inthe computation of the target function, the global update will beperformed on all of the parameters:",
  "=1L ( , |)(11)": "where represents the personalized weight of tasks. Because ofthe fixed division of the tasks, we can leverage the importance ofthe task and assign distinctive weights. For instance, the cold-starttask, involving less popular items, can be assigned a higher weight.By fixed tasks division as described above, we solve the problemof long-tailed distribution of training data by dividing the cold-started long-tailed part of the item distribution into separate tasks.",
  "Cold-start Task Enhancer": "Apart from task partitioning in meta-learning, incorporating addi-tional supervision signals can prevent the neglect of cold-start tasksduring online training. Thus, we devise a cold-start task enhancerthat effectively transfers information from popular items and theirassociated interaction feedback. This module further improve therecommendations in the cold-start task. 4.3.1Cold-start Embedding Simulation. We categorize the vari-ous types of feature embedding of items into two types: behavior-based embeddings and content-based embeddings , be-cause these types of embeddings play different roles in the recom-mendation of cold-start and popular items, as shown in Sec. 5.4.We then further divide the behavior-based embeddings into IDembedding and sequential embeddings, this is because that ID em-bedding directly stores the information of the items and we utilizeit for the cold-start embedding simulation, the details of whichwill be described later. The system maintains a cold-start embed-ding parameter , for each item , whenever it appears in the data as a cold-start item, the system stores its behavior-basedembeddings (ID embedding and sequential embeddings) into theparameter in addition to updating its own embedding param-eter . For each item that appears in the popular task , itsbehavior-based embeddings in the cold-start period have certainlybeen stored. Therefore, we can simulate a cold-started item at thecurrent moment with the help of the stored embedding as well as other content-based feature embedding :",
  "and fully simulates a cold-start item embedding at current time": "4.3.2Data Augmentation. With relatively low traffic proportion,the cold-start task in meta-learning still has limited training op-portunity. Data argumentation can significant increase the numberof cold-start samples, so we propose a simulation-based data argu-mentation. We treat the simulated cold-start embedding from theprevious subsection as a new part of cold-start task data with thereal popular interaction data labels , and train the meta-learningmodel to update the cold-start task parameters, which can partiallyaugment the data to improve the effect.",
  "where is calculated by support sets ,": "4.3.3Self-supervised Instructor. Furthermore, we propose lever-aging the well-learned embeddings of popular items and achieveinstruction for the cold-start task by training a mapping networkto fit the transformation process of the cold-start embedding tothe embedding from popular phase. For the simulated cold-startitem embedding , we have the ID embedding of its popular stateID , which has more information due to multiple updates. Tothis end, we reinforce the ability of the other fully connected layersto extract information by replacing the last fully connected layer ofthe cold-start task network parameters , with a unique map-ping parameter , that makes the output top embeddingas similar as possible to its true ID embedding:",
  "{+1, +1, +1 } {, , } {,, }L .(18)": "4.4.2Online Serving. During the training process, the model isfine-tuned on the cold-start data to generate parameters for thecold-start task . We save that parameter and serve it for therecommendation of cold-start items at the next moment. In thiscase, PAM is able to store the parameters prepared for the cold-startitem in advance without real-time personalized fine-tuning, thusavoiding the time cost of online fine-tuning altogether. Comparedto traditional meta-learning, we avoid the time-consuming servingof personalization on each item, and also reduce the storage spaceoverhead of maintaining the respective parameters for each item.The training and serving process of PAM is described by Alg. 1.",
  "EXPERIMENTS": "We conducted experiments to answer the following RQs: RQ1: Howdoes PAMs performance on online cold-start recommendationsimprove over existing frameworks? RQ2: How much of a perfor-mance enhancement do the various components of PAM provide?RQ3: Why PAM perform better on multitasking? RQ4: How do thevarious hyperparameters of PAM affect the performance results?RQ5: How does PAM perform in online A/B testing?",
  "MovieLens43,18151,1426,840,091203.09Yelp1,987,929150,3466,990,2801,3120.02Book351,487581,7176,402,7282,8080.03": "data is simulated, including MovieLens , Yelp and Book .Details of datasets are shown in A.For the label processing of the datasets, the user ratings lie be-tween 0 and 5, while we consider the interaction data with ratingsabove 3 as positive samples and the rest as negative samples. Theinformation of datasets is conducted in . 5.1.2Baselines. We compare our proposed PAM with the follow-ing baseline, where all scenarios leverage the previously mentionedtwo-tower model structure and the feature inputs remain the same.- Periodical Fine-tuning (PF) or incremental updating, is afrequently used type of update in online systems, which performsan update on recommender system using the incoming data peri-odically to serve the next moment.- s2Meta employs a novel meta-learning task division ap-proach that considers item scenarios as tasks to achieve betterrecommendation results for new scenarios.- IncCTR is a practical incremental method that lever-ages knowledge distillation for training, use the current knowledgestored in model to guide the retraining.- SML proposes a meta-model that takes the previous mo-ments recommender and current moments data as inputs andgenerate the next moments recommender for serving through aCNN-based network.- ASMG better captures long-term interests through his-torical recommenders to generate models for serving through ameta-model generator that introduces GRU.- MeLON generates more stable online recommendationmodels by maintaining dynamic learning rates for different param-eters and distinguishing the importance of interaction data.- IMSR allows the traditional adaptive interest capturemodel to better capture new interests of users periodically by in-troducing unique interest shifters..",
  "Online Item Cold-Start Recommendation with Popularity-Aware Meta-LearningSIGKDD 25, August 3-7, 2025, Toronto, Canada": "5.1.3Evaluation Metrics. Similar to ASMG , we divide eachdataset equally into 31 periods, and each period is further parti-tioned into batches for training. To simulate the characteristics ofonline streaming data, we do not use a pre-training scheme, and allmodels will be consistently streamed starting from the data of thefirst period. We start testing from the 24th period, and the modelsobtained from each training period will be tested on cold-start datafrom the next period. The average of the metrics from all the testingphases will be reported. For specific metrics, we use the Recall@Kand NDCG@K metrics to measure the models effectivenesson cold-start items. Since ranking the items scores for all users istime-consuming, for a cold-start item, we take the interacted useras 1 positive sample and all the non-interacted users in the currentbatch as the negative samples (not less than 900 due to the itemslow popularity), and compute the above metrics. 5.1.4Hyperparameters. For the definition of the tasks, to simulatean online cold-start scenario, we designated the items with the low-est 5% of popularity in the interaction data as cold-start items .The cold-start thresholds of popularity on the three datasetsare 50, 20, and 15, respectively. We divide the remaining data into 4tasks to distinguish popular items of different natures.During the training process, we use Adam as the optimizerfor gradient descent with set to 0.001. The outer loop learningrate is set to 0.001. The batch size is 1024, the weights of tasks are set to 2 for the cold-start task and 0.5 for the other tasks, andthe weights of the loss function are set to 1, 3, and 2, respectively.",
  "Overall Performance (RQ1)": "5.2.1Comparison of Cold Items. demonstrates results ofthe top-K metrics on cold-start items for various methods. Fromthe result of the experiment, we have the following observations.Our proposed PAM significantly outperforms other baselineson cold-start items. This demonstrates the effectiveness of PAMsunique meta-learning scheme targeted at solving long-tailed dis-tributions of item popularity in online streaming data scenarios.The PAM performs better on smaller K-value metrics compared toother baseline methods, and the improvement is relatively smallas the K-value increases. This suggests that the PAM method canaccurately recommend cold-start items to the users who favor it themost. In contrast, the baseline method ranks the positive-sampleusers relatively far down the list, as reflected in the considerablerise in the metrics as the K-value rises.SML and ASMG methods focus on extracting information fromhistorical recommender systems and balancing the weights of his-torical information with real-time interests to optimize recommen-dations at the next moment. However, because data distributiontends to be popular videos, focusing on historical information onlyaggravates the interest bias of the model. Besides, this model reliesmore on the efficacy of the initialized model, which results in poorerperformance on cold-start items in the streaming no pre-trainingscheme. At the same time, ASMG has a high storage space over-head (leads to OOM on the Yelp dataset), which becomes anotherlimitation for its application in online systems. IMSR is also an ap-proach that balances periodic retraining of historical with currentinformation, and thus has a similarly reliance on the efficiency ofpre-training, and performs poorly in schemes without pre-training. In addition, the emphasis on historical information extraction alsoleads to a worse performance on cold-start recommendations.IncCTR, as a knowledge distillation scheme that utilizes themodels self-supervised signals, also references historical informa-tion. Its low dependence on initialized parameters leads to a betterfit to online scenarios and a slight improvement over PF, achievingstate-of-the-art for cold-start items on some datasets. However, itsway of generating additional labels increases the computationaltime consumption and partially reduces the models efficiency.s2Meta is a scheme for task segmentation and meta-learningtraining of arriving data by categories of items. However, it onlyimproves slightly relative to the PF baseline when no new itemcategories enter the system and performs slightly inferior on thecontent-information-rich MovieLens dataset, as the rest of the base-line can similarly achieve similar information extraction throughcontent-based embedding.MeLON performs superiorly on some recall metrics and worseon NDCG metrics. As a scheme that can adjust the learning ratein both directions on interactions and parameters to accommo-date necessary samples, MeLON can better find the direction ofgradient updates in less cold-start data and thus performs betterin the MovieLens dataset. However, as the number of cold-startinteractions increases in other datasets, its inability to find the com-monality of cold-start items may bring inefficiency in updating. 5.2.2Comparison of Popular Items. Meanwhile, we compare themetrics of PAM on popular items, the results are showed in .It can be seen that the performance of the other baseline methodson popular items has improved compared to the cold-start items,reflecting the shift of the parameters toward the popular items dueto more feedback on popular items. Meanwhile, PAM does not per-form as well on popular as on cold-start items, demonstrating thatthe preference of parameter settings for cold-start tasks success-fully optimizes on cold-start items. Notably, PAM still outperformssome baselines on popular items, demonstrating the superior per-formance of popular task parameters detached from the cold-startdata for recommending items in the corresponding task.",
  "Ablation Study (RQ2)": "We compare PAM with several variants: PAM-M refers to themethod without the cold-start task enhancer, PAM-S and PAM-Adenote the method that adds only the self-supervision and dataaugmentation modules, respectively, and PAM-F is the methodthat implements the complete cold-start task enhancer.We can see that both the optimization method for cold-start taskinformation extraction with self-supervised signals and the dataaugmentation method for simulated cold-start data significantlyenhance the PAM on all datasets. This reflects the nature of PAMsexcellent task segmentation, which makes further optimization ona single task feasible.For PAM-S, it utilizes self-supervised signals using the ID embed-ding information of popular items to guide the cold-start task. Com-pared with PAM-M, the improvement on the MovieLens dataset isrelatively minor, while it has a significant optimization effect onthe Yelp dataset. This may be because the high concentration ofcontent-based features in the MovieLens dataset leads to the cold-start task itself being able to extract more information. In contrast,",
  "R@5R@10R@20N@5N@10N@20": "PF0.29490.35320.42850.24740.26610.2850s2Meta0.29620.35440.42870.24880.26740.2861IncCTR0.29320.35150.42470.24660.26530.2838SML0.27290.33350.40490.22630.24590.2643ASMG0.26590.32490.39990.22100.23990.2588MeLON0.32050.42610.51390.24420.26890.3173IMSR0.29140.35810.43880.23170.25660.2821PAM0.33400.40310.48400.27710.29940.3198 the decentralized content-based features in the Yelp dataset makethe guidance approach more effective.The PAM-A approach leverages the idea of data augmentation.Simulating cold-start consumption data using historical behavior-based embedding of popular items delivers a significant optimiza-tion compared to PAM-M across all datasets. It also shows that thesparseness of feedback on cold-start items is a significant and essen-tial reason for the poor recommendation of cold-start items. Thus,the increase in data volume further enhances the task segmentationto improve the performance of cold-start items.PAM-F combines all the proposed optimization methods for thecold-start task and ultimately brings significant improvements inthe cold-start item metrics compared to the baseline. It is worthnoting that the improvement effect of PAM-F is weakened comparedwith the combination of the respective improvement of PAM-Sand PAM-A, which only incorporate a single loss. This is becauseboth PAM-S and PAM-A optimize the cold-start task, but they use",
  "Breakdown Analysis (RQ3)": "In this section, we conduct a breakdown analysis of PAMs abilityto personalize parameters on tasks with different item popularity.To validate this, we summarize the following experiments. First,we saved the network parameters of cold-start and popular tasksgenerated by the corresponding task data. We sampled 200 batchesand computed the top representations in Sec 4.1.2 of cold-startand popular items using the corresponding parameters. Afterward,we categorized the input embedding into two types like in Eq. (13):behavior-based embedding and content-based embedding .We mask the behavior-based and content-based embedding of thecold-started and popular items to 0, respectively, and computethe top representations again as inputs using the correspondingnetwork parameters. The masked top representations are furthercompared with unmasked top representations. illustrates thesquared error of the top representations of the cold-start and popu-lar items before and after masking the two types of embeddings.In the task of cold-start items, the differences in the top repre-sentations of the network outputs before and after masking thebehavior-based embedding were minor. In contrast, some dimen-sions of the top representations changed considerably before andafter masking the content-based embedding. In cold-start items,content-based embedding stores less information due to the few-shot interactions in cold-start items. Hence, the parameters gen-erated by the cold-start task can effectively extract the contentinformation, which gives more importance to content-based ratherthan behavior-based embedding and improves the recommendationeffect of the cold-start task.",
  ": Squared errors of top representations of cold-startitems and popular items before and after masking for differ-ent types of embedding inputs": "In contrast, the top representations of the popular item taskschanged considerably before and after both embedding masks, andmasking the behavior-based embedding introduced more significanterror. This suggests that the network parameters of the popularitem task can effectively utilize the information in both embeddingand focus more on behavior-based embedding (e.g., ID embedding).We can also that masking the content-based embedding, bothon the cold-start task and on the hot task, affects the exact samedimensions of the top-level representation. illustrating the similarextraction methods of content information of the cold-start andpopular item tasks.",
  "PAM+41.39%+60.45%+4.26%+6.34%": "the cold-start task is set in an appropriate range, it can effectivelyimprove the performance on cold-start items. If the weight of thecold-start task is set too low, the loss on the cold-start parameterwill take a minor percentage of the overall loss during the globalupdate, thus making the parameter less specialized for the cold-starttask. On the other hand, if the weights of the cold-start tasks are settoo high, then as mentioned before, most of the information in thepopular data will be lost, leaving the parameters shared betweenthe tasks poorly updated, leading to the same inferior performance. 5.5.2Impact of modules loss weight. We similarly analyzed theeffect of the loss weights of different modules on the effectiveness ofPAM. It can be seen that for the data-enhanced modules in (b),the enhancement brought are more stable and the weights only haveminor effect on the metric performance. Besides, the self-supervisedguidance module, although performance stably, fluctuates slightlymore than the data enhancement module, as shown in (c).",
  "Online A/B Tests (RQ5)": "To evaluate the performance of PAM in an online system, we de-ployed the PAM in a commercial, billion-user-scale, online recom-mender system and compared it to the PF baseline. For companyprivacy, we dont report the implementation details and the realperformance of the original retrieval stage. Instead, we report theperformance gain ratio improved by our approach PAM. In theonline system, we counted the following metrics, including the rateof items appearing in users recommendations (Show%), and theratio of users liking (LTR), commenting (CMTR), and collectingvideos (CLTR). The results of the experiments in show thatthe high accuracy of PAM for item recommendation enables itemsto be better recommended to users who favor the item.",
  "CONCLUSION": "In this work, we find the problem of poor results of online recom-mender systems on cold-start items due to the long-tailed distri-bution of item popularity, as well as the inapplicability of existingcold-start schemes in scenarios with streaming data. To this end, wepropose a unique PAM by partitioning the data into tasks by itempopularity and optimizing the performance of cold-start task param-eters while sharing information between tasks. Furthermore, wedesign a novel cold-start task enhancer to optimize the performanceof cold-start tasks further by leveraging the popular item informa-tion. The conducted experiments demonstrate the superiority ofthe PAM approach, and in the future, we will further investigatedifferent schemes to optimize the online cold-start problem.",
  "Homanga Bharadhwaj. 2019. Meta-Learning for User Cold-Start Recommenda-tion. In 2019 International Joint Conference on Neural Networks (IJCNN). 18": "Gaode Chen, Ruina Sun, Yuezihan Jiang, Jiangxia Cao, Qi Zhang, Jingjian Lin,Han Li, Kun Gai, and Xinghua Zhang. 2024. A Multi-modal Modeling Frameworkfor Cold-start Short-video Recommendation. In Proceedings of the 18th ACMConference on Recommender Systems. 391400. Gaode Chen, Xinghua Zhang, Yijun Su, Yantong Lai, Ji Xiang, Junbo Zhang, andYu Zheng. 2023. Win-win: a privacy-preserving federated framework for dual-target cross-domain recommendation. In Proceedings of the AAAI Conferenceon Artificial Intelligence, Vol. 37. 41494156.",
  "Robin Devooght, Nicolas Kourtellis, and Amin Mantrach. 2015. Dynamic MatrixFactorization with Priors on Unknown Values. arXiv:1507.06452 [stat.ML]": "Manqing Dong, Feng Yuan, Lina Yao, Xiwei Xu, and Liming Zhu. 2020. MAMO:Memory-Augmented Meta-Optimization for Cold-start Recommendation. InProceedings of the 26th ACM SIGKDD International Conference on KnowledgeDiscovery & Data Mining (Virtual Event, CA, USA) (KDD 20). Association forComputing Machinery, New York, NY, USA, 688697. Zhengxiao Du, Xiaowei Wang, Hongxia Yang, Jingren Zhou, and Jie Tang.2019.Sequential Scenario-Specific Meta Learner for Online Recommenda-tion. In Proceedings of the 25th ACM SIGKDD International Conference onKnowledge Discovery & Data Mining (Anchorage, AK, USA) (KDD 19). As-sociation for Computing Machinery, New York, NY, USA, 28952904.",
  "Yupeng Hou, Jiacheng Li, Zhankui He, An Yan, Xiusi Chen, and Julian McAuley.2024. Bridging Language and Items for Retrieval and Recommendation. arXivpreprint arXiv:2403.03952 (2024)": "Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and LarryHeck. 2013. Learning deep structured semantic models for web search usingclickthrough data. In Proceedings of the 22nd ACM international conference onInformation & Knowledge Management. 23332338. SeongKu Kang, Junyoung Hwang, Dongha Lee, and Hwanjo Yu. 2019. Semi-Supervised Learning for Cross-Domain Recommendation to Cold-Start Users.In Proceedings of the 28th ACM International Conference on Information andKnowledge Management (Beijing, China) (CIKM 19). Association for ComputingMachinery, New York, NY, USA, 15631572.",
  "Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Opti-mization": "Hoyeop Lee, Jinbae Im, Seongwon Jang, Hyunsouk Cho, and Sehee Chung.2019. MeLU: Meta-Learned User Preference Estimator for Cold-Start Recom-mendation. In Proceedings of the 25th ACM SIGKDD International Conferenceon Knowledge Discovery & Data Mining (Anchorage, AK, USA) (KDD 19). As-sociation for Computing Machinery, New York, NY, USA, 10731082. Yuanfu Lu, Yuan Fang, and Chuan Shi. 2020. Meta-learning on HeterogeneousInformation Networks for Cold-start Recommendation. In Proceedings of the26th ACM SIGKDD International Conference on Knowledge Discovery & DataMining (Virtual Event, CA, USA) (KDD 20). Association for Computing Machin-ery, New York, NY, USA, 15631573. Haokai Ma, Zhuang Qi, Xinxin Dong, Xiangxian Li, Yuze Zheng, and XiangxuMengand Lei Meng. 2023. Cross-Modal Content Inference and Feature Enrich-ment for Cold-Start Recommendation. arXiv:2307.02761 [cs.IR] Mohammmadmahdi Maheri, Reza Abdollahzadeh, Bardia Mohammadi, MinaRafiei, Jafar Habibi, and Hamid R. Rabiee. 2023.ClusterSeq: Enhanc-ing Sequential Recommender Systems with Clustering based Meta-Learning.arXiv:2307.13766 [cs.IR]",
  "Krishna Prasad Neupane, Ervine Zheng, Yu Kong, and Qi Yu. 2022.A Dy-namic Meta-Learning Model for Time-Sensitive Cold-Start Recommendations.arXiv:2204.00970 [cs.IR]": "Xingyu Pan, Yushuo Chen, Changxin Tian, Zihan Lin, Jinpeng Wang, He Hu,and Wayne Xin Zhao. 2022. Multimodal Meta-Learning for Cold-Start Sequen-tial Recommendation. In Proceedings of the 31st ACM International Conferenceon Information & Knowledge Management (Atlanta, GA, USA) (CIKM 22). As-sociation for Computing Machinery, New York, NY, USA, 34213430. Deepak Kumar Panda and Sanjog Ray. 2022. Approaches and algorithms tomitigate cold start problems in recommender systems: a systematic literaturereview. J. Intell. Inf. Syst. 59, 2 (oct 2022), 341366. Haoyu Pang, Fausto Giunchiglia, Ximing Li, Renchu Guan, and Xiaoyue Feng.2022. PNMTA: A Pretrained Network Modulation and Task Adaptation Ap-proach for User Cold-Start Recommendation. In Proceedings of the ACM WebConference 2022 (Virtual Event, Lyon, France) (WWW 22). Association for Com-puting Machinery, New York, NY, USA, 348359. Danni Peng, Sinno Jialin Pan, Jie Zhang, and Anxiang Zeng. 2021. Learningan Adaptive Meta Model-Generator for Incrementally Updating RecommenderSystems. In Proceedings of the 15th ACM Conference on Recommender Systems(Amsterdam, Netherlands) (RecSys 21). Association for Computing Machinery,New York, NY, USA, 411421.",
  "Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2019. Representation Learningwith Contrastive Predictive Coding. arXiv:1807.03748 [cs.LG]": "Manasi Vartak, Arvind Thiagarajan, Conrado Miranda, Jeshua Bratman, andHugo Larochelle. 2017. A meta-learning perspective on cold-start recommenda-tions for items. In Proceedings of the 31st International Conference on NeuralInformation Processing Systems (Long Beach, California, USA) (NIPS17). CurranAssociates Inc., Red Hook, NY, USA, 69076917. Maksims Volkovs, Guangwei Yu, and Tomi Poutanen. 2017. DropoutNet: Address-ing Cold Start in Recommender Systems. In Proceedings of the 31st InternationalConference on Neural Information Processing Systems (Long Beach, California,USA) (NIPS17). Curran Associates Inc., Red Hook, NY, USA, 49644973.",
  "Zhikai Wang and Yanyan Shen. 2023. Incremental Learning for Multi-InterestSequential Recommendation. In 2023 IEEE 39th International Conference onData Engineering (ICDE). 10711083": "Zhenchao Wu and Xiao Zhou. 2023. M2EU: Meta Learning for Cold-start Recom-mendation via Enhancing User Preference Estimation. In Proceedings of the46th International ACM SIGIR Conference on Research and Development inInformation Retrieval (SIGIR 23). Association for Computing Machinery, NewYork, NY, USA, 11581167. Jiafeng Xia, Dongsheng Li, Hansu Gu, Jiahao Liu, Tun Lu, and Ning Gu. 2022. FIRE:Fast Incremental Recommendation with Graph Signal Processing. In Proceedingsof the ACM Web Conference 2022 (Virtual Event, Lyon, France) (WWW 22).Association for Computing Machinery, New York, NY, USA, 23602369. Ruobing Xie, Yalong Wang, Rui Wang, Yuanfu Lu, Yuanhang Zou, Feng Xia,and Leyu Lin. 2022. Long Short-Term Temporal Meta-learning in Online Rec-ommendation. In Proceedings of the Fifteenth ACM International Conference",
  "on Web Search and Data Mining (Virtual Event, AZ, USA) (WSDM 22). As-sociation for Computing Machinery, New York, NY, USA, 11681176": "Chen Yang, Jin Chen, Qian Yu, Xiangdong Wu, Kui Ma, Zihao Zhao, ZhiweiFang, Wenlong Chen, Chaosheng Fan, Jie He, Changping Peng, Zhangang Lin,and Jingping Shao. 2023. An Incremental Update Framework for Online Recom-menders with Data-Driven Prior. In Proceedings of the 32nd ACM InternationalConference on Information and Knowledge Management (Birmingham, UnitedKingdom) (CIKM 23). Association for Computing Machinery, New York, NY,USA, 48944900. Junliang Yu, Min Gao, Jundong Li, Hongzhi Yin, and Huan Liu. 2018. Adap-tive Implicit Friends Identification over Heterogeneous Network for SocialRecommendation. In Proceedings of the 27th ACM International Conferenceon Information and Knowledge Management (Torino, Italy) (CIKM 18). As-sociation for Computing Machinery, New York, NY, USA, 357366. Yang Zhang, Fuli Feng, Chenxu Wang, Xiangnan He, Meng Wang, Yan Li, andYongdong Zhang. 2020. How to Retrain Recommender System? A SequentialMeta-Learning Method. In Proceedings of the 43rd International ACM SIGIRConference on Research and Development in Information Retrieval (VirtualEvent, China) (SIGIR 20). Association for Computing Machinery, New York,NY, USA, 14791488.",
  "Yujia Zheng, Siyi Liu, Zekun Li, and Shu Wu. 2020. Cold-start Sequential Recom-mendation via Meta Learner. arXiv:2012.05462 [cs.IR]": "Yongchun Zhu, Ruobing Xie, Fuzhen Zhuang, Kaikai Ge, Ying Sun, Xu Zhang,Leyu Lin, and Juan Cao. 2021. Learning to Warm Up Cold Item Embeddingsfor Cold-start Recommendation with Meta Scaling and Shifting Networks. In Proceedings of the 44th International ACM SIGIR Conference on Research andDevelopment in Information Retrieval (Virtual Event, Canada) (SIGIR 21). As-sociation for Computing Machinery, New York, NY, USA, 11671176. ADATASETS SETTINGSWe selected three public datasets that containing timestamp infor-mation.MovieLens is a dataset of user ratings for movies. Weselected one of the MovieLens 25M datasets and retained the data forthe period 2014-2018 for training, containing 6,840,091 interactionsbetween 43,181 users and 51,142 movies.Yelp is a dataset containing user reviews of various businessesspanning over 10 years, containing 6,990,280 interactions between1,987,929 users and 150,346 businesses.Book is a part of the Amazon dataset, which collects bookreviews from purchasers on the Amazon e-shopping site. We se-lected data from the period 2012-2023 for training and retainedusers with 10 or more interactions and items with 3 and more in-teractions, remaining 6,402,728 interactions between 351,487 usersand 581,717 books."
}