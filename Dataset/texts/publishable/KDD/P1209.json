{
  "ABSTRACT": "Order execution is a fundamental task in quantitative finance, aim-ing at finishing acquisition or liquidation for a number of tradingorders of the specific assets. Recent advance in model-free rein-forcement learning (RL) provides a data-driven solution to the orderexecution problem. However, the existing works always optimizeexecution for an individual order, overlooking the practice thatmultiple orders are specified to execute simultaneously, resulting insuboptimality and bias. In this paper, we first present a multi-agentRL (MARL) method for multi-order execution considering practicalconstraints. Specifically, we treat every agent as an individual oper-ator to trade one specific order, while keeping communicating witheach other and collaborating for maximizing the overall profits.Nevertheless, the existing MARL algorithms often incorporate com-munication among agents by exchanging only the information oftheir partial observations, which is inefficient in complicated finan-cial market. To improve collaboration, we then propose a learnablemulti-round communication protocol, for the agents communicat-ing the intended actions with each other and refining accordingly.It is optimized through a novel action value attribution methodwhich is provably consistent with the original learning objectiveyet more efficient. The experiments on the data from two real-worldmarkets have illustrated superior performance with significantlybetter collaboration effectiveness achieved by our method.",
  "These authors contributed equally to this research.This work was conducted during the internship of Yuchen Fang and Zhenggang Tangat Microsoft Research Asia.Corresponding author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 23, August 610, 2023, Long Beach, CA, USA 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0103-0/23/08...$15.00",
  "Reinforcement Learning, Quantitative Finance, Multi-agent Rein-forcement Learning, Order Execution, Financial Trading": "ACM Reference Format:Yuchen Fang, Zhenggang Tang, Kan Ren, Weiqing Liu, Li Zhao, Jiang Bian,Dongsheng Li, Weinan Zhang, Yong Yu, and Tie-Yan Liu. 2023. LearningMulti-Agent Intention-Aware Communication for Optimal Multi-OrderExecution in Finance. In Proceedings of the 29th ACM SIGKDD Conference onKnowledge Discovery and Data Mining (KDD 23), August 610, 2023, LongBeach, CA, USA. ACM, New York, NY, USA, 13 pages.",
  "INTRODUCTION": "In quantitative finance, the primary goal of the investor is to maxi-mize the long-term value through continuously trading of multipleassets in the market . The process consists of two parts, port-folio management, which dynamically allocate the portfolio acrossthe assets, and order execution whose goal is to fulfill a number ofacquisition or liquidation orders specified by the portfolio man-agement strategy, within a time horizon, and close the loop of in-vestment . Figure (1a) presents the trading process within onetrading day. The trader first updates the target portfolio allocationfollowing some portfolio management strategy. Then, the ordersshown in the red dotted zone need to be executed to accomplishthe actual portfolio adjustment. We focus on order execution taskin this paper, which aims at simultaneously finish executing mul-tiple orders during the time horizon while maximizing the overallexecution profit gain.The challenge of order execution lies in two aspects. First, thenumber of orders changes according to the portfolio allocationfrom day to day, which requires the order execution strategy to bescalable and flexible to support large and various number of orders.Second, cash balance is limited and all acquiring operations will",
  ": An example of multi-order execution and motiva-tion of collaboration within it": "consume the limited cash supply of the trader, which can only bereplenished by the liquidating operations. The lack of cash supplymay lead to missing good trading opportunities, which urges oneto achieve balance between acquisition and liquidation, to avoidconflicted trading decisions that would cause cash shortage andpoor trading performance. Figure (1b) illustrates a typical exam-ple of a conflicted trading decision that results in cash imbalanceand low execution utility. The execution of acquisition orders areforced to postpone due to cash shortage until the liquidation orderssupplementing the cash, leading to missing the best acquisitionopportunity. We have observed similar evidences in real-worldtransactions and made more analysis in the experiment.Although there exists many works for order execution, few ofthem manage to address the above three challenges. Traditionalfinancial model based methods and some recently developedmodel-free reinforcement learning (RL) methods only opti-mize the strategy for single-order execution without consideringpractice of multi-order execution, which would result in low trad-ing efficacy. Moreover, it is not applicable to directly transfer theexisting methods to multi-order execution since utilizing only oneagent to conduct the execution of multiple orders would lead toscalability issue as the action space of one individual agent growsexponentially with the number of orders. Also, it is either not flexi-ble enough for the execution of varying number of orders .To resolve the above challenges, we treat multi-order executionas a multi-agent collaboration problem and utilize a multi-agentreinforcement learning (MARL) method where each agent acts toexecute one individual order to factorize the joint action space forscalability to varying order numbers , and all agents collab-orate to achieve higher overall profits with less decision conflicts.However, the existing MARL solutions for general multi-agentcollaboration are not suitable for the multi-order execution envi-ronment where the actions of one agent can significantly influencethe others through shared cash balance, further affecting the finalperformance as the financial market changes drastically. The main-stream methods, which build the communication channel amongagents to promote collaboration , only allow agents to shareinformation of their partial observation, which can not directlyreflect the intention of agents, thus harming the collaboration per-formance. models the intentions of agents as imagined futuretrajectories and share the intentions through communication, inorder to achieve better collaboration performance. However, itrequires predicting environment dynamics and future actions of others to generate the imagined trajectory, which is intractableespecially in the noisy and complicated financial market. Also, theagents herein can not respond to the intentions of others, i.e., chang-ing the actions they intended to take after receiving the messages,until the next timestep, which makes the intention message lesshelpful for achieving well coordination at the current timestep.In this paper, we propose a novel multi-round intention-awarecommunication protocol for communicating the intended actionsamong agents at each timestep, which is optimized through anaction value attribution method. Specifically, we first model theintention of agents as the actions they intended to take at currenttimestep and share the intentions between agents throughcommunication. Then, during multiple rounds of communication,the agents are allowed to coordinate with each other and achievebetter balance between acquisition and liquidation, whereafter thelast intended actions are taken as the final decisions of the currenttimestep. Thus, note that the intended actions of agents shouldbe gradually refined for better collaboration during multi-roundcommunication. To ensure this, we propose a novel action valueattribution method to directly optimize and refine the intendedactions at each round, which has been proved as unbiased to theoriginal decision making objective yet more sample efficient.Our contributions are three-fold as discussed below. We illustrate the necessity of simultaneous optimization for all or-ders in multi-order execution task. To the best of our knowledge,this is the first work formulating this problem as a multi-agentcollaboration task and utilize MARL method to solve it. We formulate the intention of agents as the actions they intendedto take and propose a novel action value attribution methodto optimize the intended actions directly. We are the first toexplicitly refine the intended actions of agents in a cooperativescenario, which may shed some light on the researches aboutgeneral multi-agent reinforcement learning. Our proposed intention refinement mechanism allows agentsto share and modify their intended actions before the final deci-sions are made. The experiment results on two real-world stockmarkets have demonstrated the superiority of our approach onboth trading performance and collaboration effectiveness.",
  "RELATED WORK2.1RL for Order Execution": "Reinforcement learning (RL) based solutions are proposed for or-der execution due to its nature as a sequential decision makingtask. Early works extend traditional control theory basedmethods and rely on unrealistic assumptions for the market,thus not performing well in real-world situations. Several follow-ing works adopt a data-driven mindset and utilizedmodel-free RL methods to learn optimal trading strategies. However,all these methods target on individual order execution and do notconsider practical constraints under multi-order execution as shownin Figure (1b), leading to sub-optimal or impractical trading behav-iors. Although MARL has been widely adopted in financial area formarket simulation and portfolio management ,these is no existing method utilizing MARL directly for order exe-cution. To our best knowledge, this is the first work using MARLfor multi-order execution task with practical constraint.",
  "Communication in General MARL": "Communication is an essential approach to encourage collabora-tion between agents for multi-agent collaboration problems. Earlyworks design pre-defined communication protocolsbetween agents. DIAL and CommNet first proposed dif-ferentiable communication mechanism with deep neural networks.The following works can be divided into two sub-groups. The firstgroup focuses mainly on who to communicate, i.e., throttlingthe communication channel rather than a fully connected globalone . While the second concentrates on how to delivermessages, i.e., designing different network structures for betterinformation passing and aggregation in communication channels. However, the messages shared in all these communica-tion methods contain only the information from the observationsof agents and do not explicitly reflect their intentions, leading tocatastrophic discordance. Our work is orthogonal to these methodssince we focus on what to communicate and the correspondingoptimization algorithm, which can be easily adapted to the commu-nication structure in the works mentioned above to make up fortheir shortcomings.",
  "Intention Modeling": "Explicit modeling of the intentions of agents has been used in The-ory of Mind (ToM) and Opponent Modeling (OM) methods. ToM-net captures mental states of other agents and predicts their fu-ture action. OM uses agent policies to predict the intendedactions of opponents. But all these works are conducted under acompetitive setting and require agents to infer each others inten-tion, which could be inaccurate considering the instability natureof MARL . first conducts intention sharing between agentsthrough communication mechanisms under cooperative settings.However, the agents are not allowed to modify their action afterreceiving others intention at the current step, thus still sufferingfrom discordance. Also, this method requires forecasting the statetransitions of the environment of the next few steps, which maysuffer from compounding error problems, especially in finance areawhere the environment is extremely noisy. While our method canimprove the joint action of agents gradually through the intentionsharing and refinement process, and does not use predicted statetransitions or any additional environment information.",
  "Multi-Order Execution": "We generalize the typical settings of order execution in previousworks to multi-order execution, where a set of orders needto be fulfilled within a predefined time horizon. As shown in Fig-ure (1a), we take intraday order execution as a running examplewhile the other time horizons follow the same schema. Thus, foreach trading day, there are a set of orders to be traded for assets, respectively, which are denoted as a tuple (,0, d, M). Itincludes the trading directions of the orders d = (1, . . . ,) where {1, 1} stands for liquidation and acquisition, respectivelyand order index ; the amount of shares to trade for eachasset M = (1, . . . , ); and the initial cash balance of the tradingaccount 0 which is shared by all the trading operations duringthe day as explained below. For simplicity, we assume that thereare timesteps in the time horizon, i.e., one trading day. At eachtimestep , the trader should propose the volumes to trade,denoted as q = (1 , . . . , ). The market prices of the correspond-ing assets are p = (1 , . . . , ), which would not be revealed tothe trader before her proposing the trading decision at . Duringthe trading process, for the trader, liquidating operations replen-ish the cash balance while acquiring operations consume it. Asa result, the balance of trading account varies every timestep as = 1 + =1 ( ).Different from the previous works for order execution that onlyoptimize for a single order, the objective of multi-order execution isto maximize the overall profits while fulfilling all the orders withoutrunning out of cash, which can be formulated as follows:",
  "Multi-Order Execution as a Markov DecisionProcess": "The order execution problem can be formulated as a Markov deci-sion process (MDP) as (, S, A, , ,), where each agent executesone order and all the agents share a collective goal. Here S is thestate space, A is the action space for the agent. is the numberof agents corresponding to the order number. Note that, for differ-ent trading days, order number varies dynamically and can belarge, which makes the joint action space extremely huge for onesingle-agent RL policy to learn to execute multiple orderssimultaneously. Thus, in multi-agent RL schema for multi-orderexecution, we treat each agent as the individual operator for execut-ing one order. Each agent has a policy (;) which produces thedistribution of actions and is parameterized by . The actions ofall agents = (1, ...,) sampled from each corresponding policyis used to interact with the environment and (, ) is the rewardfunction. (, ) is the transition function of environment whichgives the next state by receiving the action. Our goal is to optimizea unified policy = {1, ..., } with parameter = {1, ..., } tomaximize the expected accumulative reward",
  "Number of agents . We define the agent to be the operator for asingle asset, and each agent would be responsible for the executionof the corresponding order": "State space S. The state S describes the overall informationof the system. However, for each agent executing a specific order, the state observed at timestep contains only the historicalmarket information of the corresponding asset collected just beforetimestep and some shared trading status. The detailed descriptionof the observed state information has been illustrated in AppendixA.2. Action space A. Agent proposes the action A after observ-ing at {1, . . . , }. Following , we define discrete actionspace as {0, 0.25, 0.5, 0.75, 1} which corresponds to the propor-tion of the target order . The trading volume = will beexecuted at timestep . Moreover, =1 = 1 has been satisfiedby fixing = (1 1=1 ) to ensure all orders are fully fulfilled.The similar setting has been widely adopted in the related literature. We also conduct experiments on different action spaces andpresent the results in Appendix B.2, which shows that this actionsetting has performed sufficiently well.We should note cash limitation here. If the remained cash balanceis not adequate for all acquisition agents, then the intended tradingvolume of them are cutoff, by environment, evenly to the scale thatthe remained cash balance will just be used up after this timestep.For instance, if the actions of all the acquisition agents require twiceas much as the remained cash, their viable executed volume will benormalized to half, to ensure cash balance 0 for 1 . Reward function. Different from the previous works for orderexecution that optimize the performance of each order individually,we formulate the reward of all agents in multi-order execution asthe summation of rewards for execution of each individual order,which includes three parts: the profitability of trading, the penaltyof market impact and the penalty of cash shortage where the cashbalance has been used up and the acquisition actions are limited.First, to account for the profitability during order executioncaused by actions, following , we formulate this term of rewardas volume weighted execution gain upon the average price as",
  "where = 1": "=1 is the average market price of asset over thewhole time horizon. Note that, as discussed in , incorporating in the reward function at timestep will not cause informationleakage since the reward has not been included in the state thuswould not influence the actions of our agent. It would only takeeffect in back-propagation during training.Second, to avoid potential market impacts, i.e., too large tradingvolume that may harmfully influence the market, we follow therecent works and propose a quadratic penalty for trading",
  "=1(,;) .(8)": "Assumptions. There are two main assumptions adopted in thispaper. Similar to , (i) the temporary market impact has beenadopted as a reward penalty, i.e., , and we assume that the marketis resilient and will bounce back to equilibrium at the next timestep.(ii) We either ignore the commissions and exchange fees as thisexpense is relatively small fractions for the institutional investorsthat we mainly aim at.",
  "METHODOLOGY": "In this section, we first briefly introduce the main challenges ofsolving multi-order execution task. Then, we illustrate how theprevious communication based MARL methods fail facing thesechallenges by describing the design and problems of general frame-work of existing method, further clarifying the motivation of ourimprovements. Finally, we introduce the details of our multi-roundintention-aware communication with action refinement, includingthe optimization method.",
  "Problems of General Multi-agentCommunication Framework": "There are two main challenges for solving multi-order executiontask with MARL method: (1) The agents have to extract essentialinformation from their own observation, e.g., judge whether it is agood opportunity to trade to derive a high profit. (2) The acquisitionand liquidation orders should be coordinated with each other tomaintain a reasonable cash balance without severe cash shortage.This requires the agents to be aware of the situation and decisionintention of the other agents at the current timestep, and adjusttheir decisions to avoid potential conflicts, e.g., congested cashconsumption. However, the existing multi-agent communicationmethods are limited by a common inflexible framework and can notsolve all these challenges, as summarized and discussed as below.Note that, the following procedure is conducted at each timestep,thus, we omit the subscript of timestep for simplicity in conditionthat the context is of clarity.",
  "Finally, the actions of agents are generated with a decision makingmodule () as ().(11)": "The previous works on multi-agent communication have beensummarized above. They all focus on improving the structure ofcommunication channels , either throttling the channel for moreefficient communication or improving the informationpassing ability with a more complicated channel design .However, none of these approaches breaks through the aboveframework, and we claim that two main problems exist within thisframework. First, all hidden representations only contain theinformation of partial observations of the agents but not the actionsthey intended to take, making it harder for agents to reach good col-laboration. Second, though multiple rounds of communications areconducted during this process, the agents only make decisions onceafter the final round of communication. They have no opportunityto refine their actions afterward, leading to discordance as it is hardfor agents to reach an agreement immediately in a complicatedenvironment like the finance market. These problems combinedmaking existing methods fail to solve the challenge (2) mentionedabove, thus not suitable for multi-order execution task.",
  "In this section, we first describe the framework of our proposedintention-aware communication (IaC) method. Then, we discussthe optimization details including the intended action refinement": "4.2.1Decision making with multi-round intention communication.To solve the problems mentioned above, we propose the intention-aware communication, which can be divided into two parts: obser-vation extraction and multi-round communication with decisionmaking. Following the above convention, we by default omit thesubscript in notations without causing misunderstanding sincethis procedure is conducted during each timestep. The whole pro-cess has been illustrated in . Observable information extraction. Similar to the frameworkdescribed in .1, from the view of the -th agent, at eachtimestep, an information extractor is utilized to extract the pat-terns from the input and encode them as an initial hidden repre-sentation 0 of agent from its observation following Eq. (9). Multi-round communication with decision making. Our cen-tral improvement lands in the multi-round communication process.Instead of designing more complicated communication channels,we focus on what to communicate and share the intended actionsof agents during each round of communication. Also, we make itpossible for agents to constantly refine their actions according to",
  "#!#\"": ": The policy framework of our proposed intention-aware communication method. It illustrates the decision-making process of all agents at timestep. Totally rounds ofintention-aware communication is conducted among agentsto derive the final decision . Intended actions will be madeafter the -th round and fed to the next communication. Allthe modules of different rounds share the same parameters, thus, few more parameters are introduced.",
  "(), 1 , 1 ,(13)": "where 0 is a dummy action whose value can be arbitrarily assigned,() is the communication channel where agents exchange infor-mation and update their hidden states for one round from to+1, and () is the decision making module. The intended actionsof the last round are used as the final actions actually executed inthe environment as = in our method.Note that, our proposed method is different from the generalframework of the previous communication-based MARL methodsdescribed in .1, which only share the information extractedfrom the partial observations of agents and make the (final) deci-sions only once after the last round of communication. The novelparts of our proposed method are emphasized with dashed linesand striped blocks in .The communication channel () is scalable to varying numberof agents following previous methods , 4.2.2Intention optimization with action value attribution. The in-tended actions generated after-th communication round shouldprovide the intention of every agent, which reflects the instant intu-ition of decision making at the current timestep. Thus, the intendedactions should by design reflect the true intentions of agents andbe exchanged with each other through the next round of communi-cation to further facilitate collaborative decision making, until thefinal round as = . To achieve this, we propose an auxiliary ob-jective for each round of intended actions. All auxiliary objectives",
  "KDD 23, August 610, 2023, Long Beach, CA, USAYuchen Fang et al": "are optimized together with the original objective () defined inEq. (3) to keep all intended actions highly correlated to the finalgoal and progressively refine decisions upon round and round.We first introduce some definitions before describing the de-sign of the auxiliary objective in detail. Recall that we set the con-text of agents at timestep , we define value function () == (, ) as the expected cumulative reward we couldget starting from state following our policy , and action value(, ) = (, ) + ( (, )) as the expected cumulativereward if we take action at timestep following policy .Denoting the intention generation process during the -th roundof communication as , i.e., (|, 1) = ((1||1)),we optimize by defining an auxiliary objective function to max-imize the expected cumulative reward (, ) as if we took as the final actions instead of , for all and 1 encounteredduring interacting with the environment. Note that, when = ,this objective is consistent with our original objective () as are the final actions that are used to interact with the environment.Thus, all our objectives can be uniformly denoted as",
  "Action value attribution. We further clarify the intuition of our": "auxiliary objective from a perspective of attributing the credit offinal decision making across the whole process of intention re-finement during each round of communication. First, aiming atencouraging the agents to find better actions than the intendedactions exchanged during last round of communication, we defineanother auxiliary objective for optimizing as () = E,1E (|,1; ) [(, ) (, 1)].(18)Taking derivatives of Eq. (18) w.r.t. and considering Eq. (16),we can easily find that () = (). Thus, consideringthe consistency between () and () we mentioned above, (), i.e., the auxiliary objective defined over the decisions of thelast round, is also consistent with our original target (). Also, as =1((, ) (, 1)) = (, ), we can see that theoriginal optimization goal (), i.e., here, has been distributedto each for 1 . We can tell that what we are doing isdesigning an action value attribution method where the value ofthe last decision is attributed to all the intended actions.Optimization with action value attribution decomposes the fi-nal optimization objective to each round of intention-aware com-munication, which not only alleviates the burden of multi-agentcommunication optimization, but also improves decision makinggradually through learning to promote action value at each roundas shown in Eqs. (16) and (18). Action value estimation. The last detail is to estimate the ac-tion value (, ) to calculate () in Eq. (16). Normally, for = , (, ) can be directly calculated from the original sam-pled trajectory as is the final decision utilized to interact withthe environment. While, for 1 < , we train an action valueestimation model (, ) upon the trajectories collected by interact-ing with the environment using the actual decision . Note that,this procedure does not require the environment to provide any ad-ditional information about the intended actions, which guaranteesgeneralizability for wider applications of MARL.Overall speaking, we optimize the objective functions ()for all communication rounds simultaneously, thus, the final lossfunction to minimize w.r.t. the parameter is defined as",
  "=1 () .(19)": "As for implementation, we use PPO algorithm to optimize allthe intended actions and the final decisions.The overall decision-making and optimization process of ourproposed Intention-Aware Communication method is presentedin Algorithm 1. The detailed network structures of the extractor(), communication channel(), decision module () and actionvalue estimator (, ) are presented in Section A.6.",
  "Datasets": "All the compared methods are trained and evaluated based onthe historical transaction data of China A-share stock market USstock market from 2018 to 2021 collected from Yahoo Finance1.All datasets are divided into training, validation, and test sets ac-cording to time and the statistics of all datasets are presented inAppendix A.1 For both stock markets, we conduct three datasetswith a rolling window of one-year length and stride size, denotedas CHW1, CHW2, CHW3 and USW1, USW2, USW3. For eachtrading day, several sets of orders with the corresponding initialcash budgets {(,0, d, M)} are generated according to a widelyused portfolio management strategy Buying-Winners-and-Selling-Losers implemented in , and each set of intraday exe-cution orders includes the information about the asset name, theamount and trading type of each order, as discussed in Sec. 3. Theorders are the same for all the compared methods for fairness. With-out loss of generality, all the orders in our datasets are restrictedto be fulfilled within a trading day, which is 240-minute length forChinese stock market and 390-minute for US stock market. Thedetailed trading process has been described in Appendix A.3.",
  "Evaluation Settings": "5.2.1Compared methods. Since we are the first to study the simul-taneous optimization for multi-order execution, we first compareour proposed method and its variants with traditional financialmodel based methods and some single-agent RL baselines proposedfor order execution problem. Note that, for single-agent RL methodsthat are optimized for single-order optimization, instead of usingthe summation of rewards for all orders as the reward function,the agent is only optimized for the reward of each individual order(,;) as defined in Eq. (7), aligned with how these methodsare originally proposed. Then, to illustrate the effectiveness of ourproposed novel intention-aware communication mechanism, wecompare our method with those general RL works incorporatingcommon MARL algorithms. All methods are evaluated with thesame multi-order execution procedure described in Appendix A.3. TWAP (Time-Weighted Average Price) is a passive rule-based method which evenly distributes the order amount acrossthe whole time horizon, whose average execution price wouldbe the average market price as defined in Eq. (4).",
  "IS incorporates intention communication in MARL, whichforecasts the future trajectories of other agents as intentions": "IaC is our proposed method, which utilizes an intention-awarecommunication mechanism to increase the cooperative tradingefficacy for multi-order execution. Specifically, for comprehen-sive comparison, we conduct experiments on two variants of ourmethod, IaCT and IaCC, which utilize the implementation ofcommunication channel as TarMAC and CommNet, respectively. For all the compared methods, the hyper-parameters are tuned onthe validation sets and then evaluated on the test sets. For RL-basedmethods, the policies are trained with six different random seedsafter determining the optimal hyper-parameters and the means andstandard deviations of results on test sets are reported. The detailedhyper-parameter settings are presented in Appendix A.5. All RL-based methods share the same network structures for extractor(), communication network () (if exists) and decision module(), thus the sizes of parameters are similar, for fair comparison.",
  "as EG = (strategy": ") 104. Here strategy is the average exe-cution price of the evaluated strategy as defined in Eq. (2). Note thatEG is proportional to the reward + described in Eq. (4) and hasbeen widely used in order execution task . EG is measured inbasis points (BPs), where one basis point is 1. To better illustratethe profit ability, we also report the additional annualized rate ofreturn (ARR) brought by the order execution algorithm, relativeto the same portfolio management strategy with TWAP executionsolution whose average execution price is and EG= 0, whosedetailed calculation is presented in Appendix A.4. Following ,we also report the gain-loss ratio (GLR) of EGE [EG |EG>0]E [EG |EG<0] andpositive rate (POS) P[EG > 0] across all the orders in the dataset.All the above metrics, i.e., EG, ARR, GLR and POS, are better whilevalue getting higher.Moreover, as a reasonable execution strategy should managethe cash resources wisely to avoid shortages, our last evaluationmetric is the average percentage of time of conflict (TOC) wherethe agents conduct conflicted actions and suffer from short of cashduring the execution, defined as 100% E[=1 1( = 0)]/ .TOC is better when the value is lower. Generally, a high TOC valueindicates that the acquisition orders are often limited by the cashsupply, which would usually results in suboptimal EG results. Notethat, although the portfolio management strategy responsible togenerate daily orders can hold more cash, i.e., allocating a largerinitial cash balance 0 for each set of orders to offer more budgetsfor acquisition and reduce TOC, a large cash position would lowerthe capital utilization and cause a lower profit rate. Thus, the initialcash budget 0 would not be very large which requires multi-orderexecution to actively coordinates among liquidation and acquiringto well manage cash resources, i.e., achieving low TOC value.",
  "IaCT8.110.52*2.050.13*0.550.021.060.011.010.22*7.990.29*2.020.070.540.011.070.03*1.010.298.230.56*2.080.14*0.550.02*1.070.011.380.27": ": The results of all the compared methods on five rolling windows of two real-world markets. () means the higher(lower) value is better. For learning-based methods, we report the mean value standard deviation over six random seeds. Thebest results of learning-based methods are highlighted with bold format. * indicates p-value < 106 in significance test . improve the trading performance on the test environment com-pared to the other baselines and achieve the highest profits, i.e., EG,POS and GLR on all datasets. Also, the intention-aware communi-cation mechanism brings a significant reduction in the TOC metric,which proves that sharing intended actions through communicationoffers much better collaboration performance than the previousMARL methods. (2) Almost all the MARL methods with multi-orderoptimization achieve higher profits and lower TOC than the RLmethods optimized for single-order. It has illustrated the necessityof jointly optimizing multi-order execution and encouraging thecollaboration among agents. (3) Although IS also shares the inten-tions of agents through communication, it achieves worse resultsthan IaC, indicating that the refinement of intended actions formultiple rounds within a single timestep is important for agents toreach good collaboration in a complicated environment. Also, in-tention communication in IS requires predicting future trajectoriesof all agents, which might not accurately reflect the true inten-tion of other agents. Moreover, it suffers from large compoundingerror in noisy financial environments. (4) All the financial modelbased methods achieve TOC equal to zero since they do not activelyseek best execution opportunity but mainly focus on reducing mar-ket impacts , which may easily derive low TOC value yet low(poorer) EG performance. (5) There exists a huge gap between theperformances of RL-based methods on China A-share market andUS stock market. The reason may be the relatively larger daily pricevolatility in China stock market as shown in Appendix B.3.5.4Extended Investigation To further present the necessity of jointly optimizing multiple or-ders and the improvement in collaboration efficiency achieved bythe proposed intention-aware communication mechanism, we in-vestigate the statistics of the market data and compare the transac-tion details of our IaC strategy and other baselines. The analysis inthis section is based on the trading results on the test set of CHW1and USW1 while the other datasets share similar conclusions. Collaboration is necessary when conducting multi-order ex-ecution. We further clarify the necessity of collaboration for multi-order execution by illustrating the trading opportunity of acquisi-tion and liquidation orders over the trading day. a illustrates",
  "the general price trend of all the assets by showing the averageof price deviation at each minute = E [": "] as defined inEq. (4). It shows that, on average, there exists acquisition oppor-tunity (lower price) at the beginning of the trading day, while theopportunities for liquidation (higher price) does not come until themiddle of the trading day. Generally, the opportunities for liquida-tion come later than those of acquisition, which requires carefulcollaboration between buyers and sellers during execution and fur-ther call for multi-order optimization solutions as the fulfillment ofacquisition orders depends on the cash supplied by liquidation. Multi-order optimization improves significantly against single-order execution. We compare the execution details of IaC andPPO to show that jointly optimizing multiple orders is necessary toachieve high profit in multi-order execution. Figure (3b) shows howIaCC and PPO distribute the given order across the trading day onaverage. The bars exhibit the ratio of acquisition and liquidationorders fulfilled at every minute on average, i.e., E [ ]. The hollowred bars show the number of orders buyers intend to fulfill, andthe solid bars show what they actually trade considering the cashlimitation. We find from Figure (3b) that PPO tends not to liquidatemuch at the beginning of the day, as it is not a good opportunityfor liquidation, as shown in Figure (3a), leading to slow cash replen-ishment. Although PPO intends to buy many shares in the first 30minutes of the day when the price is low, it is severely limited bya cash shortage. It has to postpone the acquisition operations andloses the best trading opportunities during the day. On the contrary,IaCT coordinates both acquisition and liquidation more activelyand efficiently fulfills most liquidation orders in the first hour oftrading thus guaranteeing sufficient cash supply for acquisitionorders, which shows the improvement on collaboration brought byoptimizing all orders simultaneously through our method. The intended actions have been gradually refined duringintention-aware communication in IaC. To illustrate the refineprocess of intended actions, we directly take ,, generated afterthe -th round of communication (1 = 3) from thetrained policy of IaCC and IaCT, as the final actions at each timestep, and evaluate them on the test sets. We report the profitability",
  ": The price trend of assets in the order and the corresponding trading situation of compared methods": "performance (EG) and collaboration effectiveness (TOC) of theseintended actions in Figure (3c) to exhibit the collaboration ability ofthe agents after each round of action refinement. We can tell that (1)all intended actions achieve good TOC performance and reasonableEG, which reflects that even the intended actions proposed beforethe final actions have reflected the intentions of the agents thussubsequently offer clear information for better collaboration. (2) TheEG gets higher and the TOC gradually reaches optimal while theagents communicate, indicating that the agents manage to improvetheir intended actions based on the intentions of each other forbetter collaboration. We also conduct case study on the transactiondetails of the intended actions after each round of communicationin Appendix B.4 to further show the refinement of intended actionsduring our intention-aware communication.",
  ": (a) Convergence analysis of intended actions alongwith communication rounds (1 = 5); (b) Analysis oftotal numbers () of communication rounds": "The intended actions have converged after multi-round com-munication in IaC. Figure (4a) shows the average difference be-tween the intended actions of neighboring rounds with totally = 5communication rounds. It indicates that the intended actions gen-erally reaches convergence as the agents communicate for multiplerounds. Figure (4b) illustrates the influence of the total communica-tion round number on the EG performances of our IaC method. The performances first improve sharply and then remain stablewhen 3, which indicates that when intention-aware commu-nication is sufficient ( 3), additional communication wouldnot bring significant improvements. These observations reflect thestability and robustness of our proposed IaC method.",
  "SOFTWARE FOR ORDER EXECUTION": "We developed a financial decision-making toolkit Qlib.RL based onQlib , to support order execution scenario. It offers APIs forreceiving orders from upstream portfolio management systems andoutputs detailed execution decisions to downstream trading pro-gram. Qlib.RL supports simultaneously execution of 1,000 orders ona machine with a NVIDIA P40 GPU and an Intel Xeon 8171M CPU,where all execution decisions would be given within 50 millisecond,which is significantly faster than the required decision time intervalthat is 1 minute in our practice. As for training, Qlib.RL retrains thepolicy every 2 months with the latest data and applies a rolling man-ner to maintain promising performance, which is an acceptable costin this scenario. The corresponding codes and benchmark frame-work with data can be referred to",
  "CONCLUSION AND FUTURE WORK": "In this paper, we formulate multi-order execution task as a multi-agent collaboration problem and solve it through an intention-aware communication method. Specifically, we model the intentionof agents as their intended actions and allow agents to share andrefine their intended actions through multiple rounds of commu-nication. A novel action value attribution method is proposed tooptimize the intended actions directly. The experiment results haveshown the superiority of our proposed method.In the future, we plan to conduct joint optimization with order ex-ecution and portfolio management, and adapt our intention-awarecommunication methodology to wider RL applications.",
  "J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson, Counterfactualmulti-agent policy gradients, in Proceedings of the AAAI Conference on ArtificialIntelligence, vol. 32, 2018": "K. Zhang, Z. Yang, H. Liu, T. Zhang, and T. Basar, Fully decentralized multi-agentreinforcement learning with networked agents, in International Conference onMachine Learning.PMLR, 2018, pp. 58725881. L. Wang, L. Han, X. Chen, C. Li, J. Huang, W. Zhang, W. Zhang, X. He, and D. Luo,Hierarchical multiagent reinforcement learning for allocating guaranteed displayads, IEEE Transactions on Neural Networks and Learning Systems, 2021.",
  "A.3Multi-order execution procedure": "We describe the detailed multi-order execution procedure of eachday in this section. The procedure is illustrated in . Theorders to execute, consisting of the assets, the amount to trade andthe trading directions of the orders, are first given by an upstreamportfolio management strategy. All orders should be fulfilled beforethe time horizon ends, i.e., within one trading day. Note that, weassume it possible for all orders to be fully fulfilled before the endof the time horizon, which is usually guaranteed by the portfoliomanagement strategy. The trading day is divided evenly into timesteps in total. For the -th asset, at the beginning of timestep, the state is generated by the environment and fed into theagent policy , and the corresponding action is proposed bythe policy. The proposed volume to execute = calculatedbased on action will be executed at this timestep . Also, thecash consumed by all acquisition operations and replenished byliquidation operations is calculated at each timestep to update thecash balance .Specifically, in all of our experiments, we set the time for eachperiod as 30 minutes, thus = 8 for China A-share stock market and = 13 for US stock market. Without loss of generality, following, within each timestep, we use TWAP as a lower level strategyto conduct actual execution at each minute, in other words, weequally allocate some volume on every minute in timestep from. Note that, one can also replace TWAP with any other orderexecution strategies.",
  ": The overall structure of the extractor network .The hidden state before the first round of communication 0is generated by a temporal extractor from the observation": "of historical market information, we encode it with a temporalextractor, which is composed by a Gated Recurrent Unit (GRU)and two fully-connect (FC) layers with ReLU () = (0,) asthe activation function. The structure of extractor is shared by allRL-based compared methods. Communication channel (). As we mentioned before, ourintention-aware communication method does not require specificform of communication channel and can be combined with arbi-trary communication implementations in previous works. In thispaper, we follow and use fully connected network andself-attention module as communication channel. Althoughgraph neural networks (GNN) have been used as communication channels in recent works , which is also suitable in ourmethod, we did not utilize GNN as there is no clear graph structurebetween agents in multi-order execution. Nevertheless, GNN canalso be incorporated with our methods in other scenarios.",
  "the last activation function is used to generate action distribution = ()": "Value function. We use the hidden representation 0 generatedby the extractor () together with the actions to estimate theaction value. 0 and are fed into a three-layer MLP to derive theaction value estimation (, ).To improve scalability and flexibility of order execution withvarious order number, all agents share the same network parameterwith each other which is a common approach in many previousMARL works . Each experiment has been conducted on themachine with an NVIDIA P40 GPU and an Intel Xeon 8171M CPUwithin 170 hours. And our proposed IaC methods, with a muchfaster convergence speed, achieve the best performance within 35hours of training, as shown in Appendix B.1. BIN-DEPTH ANALYSISB.1Learning AnalysisWe illustrated the learning situation of the compared methods onvalid sets of CHW1 and USW1 in . We can tell that, (1)with our intention-aware communication method, both IaCC andIaCT converge fastest achieving the highest reward among all thecompared methods. (2) The convergence speeds of PPO and DDQNare quick, as they are optimized for single-order execution, which isless complicated thus easier to optimize than multi-order executionoptimization. However, the final performances are limited and sub-optimal due to lacking of collaboration. (3) The performance of IShas larger variance on multiple runs than TarMAC and CommNet,which might be the result of the unstable prediction of the imaginedtrajectories under noisy financial scenario.",
  "B.2Influence of MDP settings": "We investigate the influence of MDP settings to the trading perfor-mance. We take action space A as an example and test differentaction space settings for PPO, TarMAC and IaCT on CHW1 andUSW1. Specifically, apart from the action space defined in Sec. 3.2,we conduct experiments on other two action space settings. Theresults are presented in .We can tell from the results that (1) on all these action space set-tings, IaCT achieves the highest EG and lowest TOC, which showsthat the performance improvement brought by our IaC method isconsistent. (2) When the size of action space gets smaller, e.g., from",
  ": The learning curves (mean standard deviation ofperformance over six random seeds) on CHW1 and USW1.Here step means policy interaction with the environment": "{0, 0.25, 0.5, 0.75, 1} to {0, 0.33, 0.67, 1}, the strategies tend to achievehigher average EG performances. However, the performances getmore unstable, and the TOC results get worse (higher). It is under-standable as the agents have to trade more at one timestep, thushaving a larger chance to trade much at a good opportunity ofthe day while suffering from large variance. In the meantime, theagents fail to conduct fine-grained execution and collaboration,resulting in worse, i.e., higher, TOC results.",
  "B.3Influences of market situation": "There exists a significant gap between the EG results on ChinaA-share stock market and US stock market for all the comparedmethods. We conduct analysis on the overall market situationof these two markets to explain this phenomenon. Specifically,we calculate the average volatility (AV) and average strength ofmomentum (ASM) of stock prices on each trading day as AV =1|D||D|=1 std( ) , ASM =1|D||D|=1 1=1 | 1| , where stdmeans standard deviation, |D| is the number of orders in the datasetand is the average market price of asset we defined in Eq. (2).The statistics are shown in . We can see that both the ASMand the AV of datasets of China A-share market are larger than thatof US stock market, indicating that the price movements on Chinesestocks tends to be larger and have more obvious trends, whichmakes it easier for RL policies to find good trading opportunities.",
  "B.4Case study": "illustrates the detailed trading situation of intended actionafter different rounds of communication of IaCT on USW1 in onetrading day. It shows that the agents sacrifice the profit of liqui-dation orders a little, to trade earlier for cash supply to maximizethe overall profit of both acquisition and liquidation. The agentsmanages to generally refine their actions and achieve better collabo-ration through multiple rounds of intention-aware communication.From , the case study clearly presents the collaborative ef-fectiveness of the learned policy, which has illustrated the efficacyof our proposed method."
}