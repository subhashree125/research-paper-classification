{
  "Amazon Science": "AbstractMethods to evaluate Large Language Model (LLM) responses and detect inconsistencies, also known as hallucinations, withrespect to the provided knowledge, are becoming increasingly important for LLM applications. Current metrics fall short intheir ability to provide explainable decisions, systematically check all pieces of information in the response, and are often toocomputationally expensive to be used in practice. We present GraphEval: a hallucination evaluation framework based onrepresenting information in Knowledge Graph (KG) structures. Our method identifies the specific triples in the KG that areprone to hallucinations and hence provides more insight into where in the response a hallucination has occurred, if at all,than previous methods. Furthermore, using our approach in conjunction with state-of-the-art natural language inference(NLI) models leads to an improvement in balanced accuracy on various hallucination benchmarks, compared to using the rawNLI models. Lastly, we explore the use of GraphEval for hallucination correction by leveraging the structure of the KG, amethod we name GraphCorrect, and demonstrate that the majority of hallucinations can indeed be rectified.",
  ". Introduction": "As the size and power of LLMs have drastically increasedover recent years, so has the number of potential appli-cations. Arguably, one of the biggest blockers to imple-menting these models in practice is their tendency tohallucinate - returning seemingly plausible, but untrue,responses. Here, we focus on the problem of detectinghallucinations with respect to the provided context thatthe LLM should use as its source of knowledge; detectinghallucinations that have deviated from the LLMs originaltraining data is out of the scope of this work. In appli-cations where certainty in a response is critical, such asmedical diagnosis, the existence of hallucinations thatarise from a given context is especially limiting. There-fore, it is of utmost importance to develop successfulmethods to detect these hallucinations and, when it isof interest to address or correct them, provide clarity onwhich aspect of the response is likely a hallucination.The importance of this issue is reflected in the amount ofresearch being published on the topic - see Ji et al. for a recent survey of this area.",
  "Copyright for this paper by its authors. Use permitted under Creative Commons LicenseAttribution 4.0 International (CC BY 4.0)": "before hallucinations were at the forefront of the prob-lem. Methods have evolved a great deal from traditionalN-gram based metrics, such as BLEU and ROUGE, to much more intricate LLM-based evaluation met-rics with user-defined evaluation criteria, such as G-Eval. More recently, techniques to mitigate the preva-lence of hallucinations in generated outputs leveragingRetrieval Augmented Generation (RAG) and rea-soning on knowledge graphs (KGs) have beenproposed. The former suggested the concatenation of rel-evant contextual data into the prompt to ground the LLMresponse, while the latter enforced a more robust reason-ing process through providing grounding information inKG structures . As successful as these approaches havebeen, they do not fully circumvent the need to evaluateLLM outputs. Inspired by current research harnessing KGs to providegrounded LLM responses, we propose GraphEval - a hal-lucination detection framework based on the represen-tation of information in KG structures. To the best ofour knowledge, we are the first to apply KGs to an LLM-based hallucination evaluation framework, and in doingso we provide a higher level of insight into where inthe output a hallucination has occurred than any previ-ous metrics. Additionally, we demonstrate how usingour method in conjunction with current state-of-the-arthallucination detection methods improves their classi-fication accuracy on various benchmarks. Finally, weconsider the problem of hallucination correction and weintroduce GraphCorrect, showcasing how GraphEval can",
  ". Problem statement": "In this work we focus on the closed-domain hallucina-tion detection problem: the situation where we have atextual output from an LLM which is generated usingsome grounding context included in the prompt. In thiscase, the goal is for the LLM to use the provided contextas its only source of knowledge. The open-domain prob-lem, which is with respect to all factual knowledge inthe world, is not explored here but is briefly discussed in. We consider hallucination detection to be a binary classifi-cation problem, with 0 corresponding to the LLM outputbeing factually consistent given the provided context,and 1 corresponding to the output containing at leastone inconsistency. We can assess hallucination evalua-tion methods using a benchmarking dataset containingground-truth labels (usually human-annotated) to de-termine whether a given context-output pair containsfactual inconsistencies. Throughout the paper we usethe terms factual, consistent, grounded and faithful in-terchangeably to mean containing no hallucinations withrespect to the context. Finally, we explore the problem of hallucination correc-tion, wherein we do not use any directly labeled dataset.Instead, we utilize hallucination detection frameworks tofirst identify hallucinations to correct, and subsequentlyrepurposing them to evaluate the corrected outputs. It isimportant to note that our exploration of hallucinationcorrection only serves as an extension to our evaluationframework and is not the primary focus of this study.",
  ". Related work": "Historically, N-gram based metrics such as BLEU and ROUGE have been the most widely used metricsfor natural language evaluation. However, these met-rics have been shown to perform poorly at the task offactual inconsistency detection . In more recentyears, embedding-based metrics such as BERTScore have been favoured over N-gram based metrics. Thesemethods measure the similarity between two pieces oftext by comparing the contextualised embedding from atransformer model, such as BERT . Both N-gram and embedding-based metrics base theirscores on how similar the text to be evaluated is to somereference text. This similarity objective often fails to cap-ture the intricacies of the hallucination detection problem. Therefore, researchers have begun to develop new meth-ods that are more acutely tuned to detecting inconsisten-cies between an LLM output and its grounding context.Maynez et al. identified the crossover between thetextual entailment score in NLI tasks and consistency pre-diction. This was a breakthrough at the time, producinghigher correlation with faithfulness than any previousmetrics, and paved the way for further research that cap-italised on NLI data and models . Very recently, attention has turned to leveraging LLMsthemselves to evaluate the consistency of LLM outputs.SelfCheckGPT and ChatProtect approach theproblem by considering the self-consistency within sam-pled outputs. Since they require the generation of a largenumber of responses from the LLM, many consider thesemethods prohibitively computationally expensive. Other LLM-based hallucination evaluation methods, suchas G-Eval and GPTScore , employ a different LLMfor evaluation than the one used to generate the LLMresponse that needs to be evaluated. G-Eval allows user-defined evaluation criteria and uses automated chain-of-thought prompting and form-filling to assign scores.GPTScore treats the task as conditional generation, lever-aging models like GPT-3 to assign higher probabilities tohigh-quality outputs by prepending evaluation instruc-tions to the LLM prompt. Unlike NLI models trained onbinary classification data, these methods produce scoresthat are harder to interpret as probabilities and oftenrequire additional steps for inconsistency classification. Recent hallucination detection methods,such asFactScore and SAFE , utilize large languagemodels to break down the response into atomic or in-dividual facts for evaluation. These approaches haveenabled precise identification of where hallucinations oc-cur within the LLM response. Each fact is automaticallyverified against a comprehensive knowledge source likeWikipedia or scientific literature in the case of FactScore,or through the use of a search engine in the case of SAFE. FactGraph is the only factuality evaluation methodwe are aware of that utilises graph-like structures. Themethod is focused solely on the detection of inconsisten-cies in the summarization problem, decomposing boththe summary and the supporting documents into whatthey call structured meaning representations (MRs). TheseMRs describe the core semantic concepts and relations,which the authors claim to be more suitable for factualityevaluation than the raw text. : A visualisation of the GraphEval approach. First, the LLM output is fed into the KG construction prompt to producethe KG depicted on the right. Next, each individual triple in the KG is fed into an out-of-the-box hallucination detectionmethod, such as an NLI model, and compared to the provided context for inconsistencies. Finally, any triples that are flaggedas inconsistent are returned to the user, along with the overall hallucination decision.",
  ". GraphEval: Our evaluationmethod": "GraphEval is based around the idea of representing infor-mation in a structured manner through KGs, and aims toaddress the lack of explainability of previous hallucina-tion detection approaches, i.e. which concrete pieces ofinformation in particular are inconsistent. Formally, a KG is a collection of triples ={(1, , 2) }, where and denotethe set of entities and relationships, respectively. In theGraphEval setting, both entities and relationships aresimply pieces of text. We do not make use of commonextensions to this simple setting, such as entity and rela-tionship types, or attached properties.",
  "a visualisation of this process in using a realexample from one of the benchmarks described in .1": "Regarding stage 1, we provide a short review of LLM-based KG construction methods in , along withresults from our implementation. For stage 2, we leverageexisting techniques and employ an out-of-the-box NLImodel for this task. A benefit of this approach is that itgives us the opportunity to make a direct comparisonbetween the performance of the raw NLI model and themodel supplemented with our KG approach. In essence,our method is a pre-processing step, the output of whichcan be fed into any hallucination detection method; wechoose NLI models as they are computationally cheapcompared to LLM-based models, yet still achieve state-of-the-art results. By feeding each triple into an NLI model,along with the grounding context, we obtain a probabilityof containing a hallucination for each triple. Finally, weclassify the example as inconsistent if at least one tripleproduces a probability greater than 0.5. Similar approaches to ours have been proposed in re-cent literature. SummaC also uses NLI-based modelsto detect inconsistencies in LLM-generated summaries.However, it distinguishes itself by segmenting both thecontext and the summary into their respective sentences,and then by passing each context-summary pair into theNLI model. This approach presents challenges in main-",
  "QAGS-X23948.5%18318": "Statistics relating to the evaluation benchmarks used. The label ratio is the ratio of factually consistent examples to inconsistentexamples. The average output and context length are the average number of words in each. taining entity references across sentences; for instance,\"John Doe\" may only be referred to as \"he\" in anothersentence. Similarly, FactScore faces the same limita-tion. Our method circumvents this issue by organisingentity relationships with a KG. While FactGraph also makes use of graph structuresin their consistency evaluation process, the method dif-fers from GraphEval in a few major respects. Firstly,their approach can only be applied to the summarisationproblem; whereas GraphEval can easily be applied tovarious domains such as Summarisation, Question An-swering, Common Sense Reasoning and many others.Secondly, FactGraph does not employ LLMs anywhere intheir framework, missing out on recent advances in thefield. Finally, their approach aims to decompose both theLLM output and the provided context into the underlyingcore semantic concepts and relations, before comparingeach of the graph structures. GraphEval, on the otherhand, only represents the LLM output as a KG and aimsto preserve as much of the information contained in theraw text as possible.",
  "We present a systematic way of checking allpieces of information contained in the LLM out-put": "Our method only requires one call to an LLM, inthe KG construction phase, and does not requirethe (usually) large context documents to be input,as in all previous LLM-based metrics. This makesGraphEval less computationally expensive thanother LLM-based methods. Our method returns the specific triples that arenot grounded in the context, providing explain-ability for the decision and identifying which sec-tion of the output should not be trusted. We lever-age this feature for hallucination correction andpropose a new method called GraphCorrect, de-scribed in .",
  ". Relation extraction - the process of identifyingsemantic relationships between entities": "Previously, researchers addressed each stage individually,but with the increasing power of LLMs, theres been ashift towards end-to-end systems. Kumar et al. sug-gest employing two LLM components: one for namedentity recognition and another one for both relation clas-sification and direction. Similarly, Grapher utilizes apre-trained LLM for entity extraction and relation predic-tion. However, these methods require users to providepossible relations. More recent methods like PiVE and AutoKG use LLM prompting strategies for KGconstruction without additional user input. The aforementioned methods do not make use of some ofthe emergent abilities of LLMs, such as in-context learn-ing and the chain-of thought prompting strategy. Wedecide to leverage these emergent abilities, and take asimple prompt engineering approach to our KG construc-tion step. The techniques used can be summarised as thefollowing:",
  "The final prompt used in our experiments can be foundin the Appendix. We highlight to the reader that our KGconstruction method is not the main contribution of our": "work, which is rather the application of KG constructionto the hallucination detection problem. The major benefitof our KG construction approach is its ease of implemen-tation with any LLM. Furthermore, it is less computation-ally intensive than methods like PiVE, which performsmultiple iterations of improvements to the generated KG.Of course, users may conduct the KG construction stageof GraphEval using their method of choice; the exper-iments in this paper exhibit the capability of a simpleprompting strategy.",
  ". GraphCorrect: Correction ofhallucinations with GraphEval": "While the primary focus of this work lies in hallucinationdetection, GraphEvals breakdown of LLM outputs intotriples easily allows for its extension to correct hallucina-tions within the given context. To achieve this, we firstidentify all triples within the KG that are likely to con-tain hallucinations (i.e. those with a probability greaterthan 0.5, if any). We then employ the following two-stepprocedure on each identified triple: Step 1 - Input the given triple along with thecontext into an LLM to correct for the potentialhallucinations within the triple. This results in anewly generated corrected triple.Step 2 - Input the identified triple, its correctedcounterpart and the initial LLM output. Selec-tively replace the information from the original(hallucination-containing) triple with the infor-mation from the new triple in the initial LLMoutput. We name this LLM hallucination correction method asGraphCorrect. The final prompts used in our experimentsfor both step 1 and step 2 can be found in the Appendix Band C respectively. This systematic approach to halluci-nation correction offers several benefits. First, it tackleseach identified hallucination separately, increasing thechances of all perceived hallucinations being corrected.Furthermore, it offers the advantage of exclusively alter-ing the segments of the original text that are suspectedto contain a hallucination, leaving other elements un-touched and ensuring overall high similarity with theoriginal text. Finally, breaking down the entire processinto intermediate steps ensures that the original contextand the initial LLM output never undergo simultaneousprocessing within an LLM. This guarantees safeguardsagainst both the addition of extra information and theloss of information in the LLM output.",
  ". Benchmarks": "We conducted two sets of experiments: one focusing onhallucination detection to highlight GraphEvals perfor-mance and another on hallucination correction to show-case the advantages of GraphCorrect. For both scenarios,we utilized the SummEval , QAGS-C and QAGS-X benchmarks - currently the most prevalent bench-marks in relevant academic literature. All three are con-cerned with detecting hallucinations in LLM-generatedsummaries and are human-annotated for factual con-sistency with respect to the grounding context. contains some statistics pertaining to each of thesedatasets. SummEvalThe SummEval dataset consists of humanevaluations on 16 summarization model outputs from100 articles from the CNN/DailyMail dataset . Eachsummary is labelled on a Likert scale from 1-5 on 4 cat-egories: consistency, coherence, fluency and relevance.We follow the TRUE benchmark in taking the con-sistency scores and mapping a score of 5 to being fullyconsistent, and anything lower to being inconsistent. QAGSThe QAGS-C and QAGS-X datasets are builtfrom the CNN/DailyMail and the XSum datasets,respectively. The human annotators examined the sum-maries one sentence at a time, and determined the factualconsistency of each sentence comparing it to the originalarticle. Three annotators assessed each sentence and themajority decision was recorded. Again, we follow theTRUE benchmark in considering a summary to be factu-ally consistent if and only if all sentences are consideredconsistent.",
  ". NLI models in GraphEval": "As mentioned in , we employ NLI models toperform the second stage of GraphEval - checking theconsistency of each individual triple with respect to thecontext. We conduct experiments using the three mostpopular NLI-based hallucination detection models avail-able on HuggingFace 1. HHEMBased on the DeBERTaV3 model and ini-tially trained on NLI data, the hallucination evaluationmodel created by Vectara 2 is further fine-tuned ondatasets annotated for consistency. The datasets used",
  "for fine tuning were: FEVER , Vitamin C andPAWS . This model is considerably smaller than thefollowing two models, requiring only 738 MB of memory,and thus has a significantly shorter run-time": "TRUEThe TRUE model is based on a T5-XXL model and is trained similarly to the model described inthe TRUE paper . Instead of the ANLI dataset usedin that paper, this model is trained on the same datasetsas HHEM, plus the following: SNLI , MNLI andScitail . This model requires 45.5 GB of memory. TrueTeacherGekhman et al. leverage the ability ofLLMs to evaluate hallucinations by generating syntheticdata through annotating model-generated summaries.They then use this synthetic data to further fine-tune themodel from , leading to state-of-the-art performanceon the TRUE benchmark. This model is the same size asthe TRUE model.",
  ". Experimental settings": "In all experiments conducted in this study necessitatingthe utilization of an LLM, we use Claude 2 3, an LLMfrom Anthropic, through the Amazon Bedrock API 4. Weuse the default settings for the LLM: temperature = 1,top_p = 1, top_k = 250. We also refer the reader to theAppendix for the prompts used in this work.",
  ". Hallucination detection with GraphEval": "We present our results of hallucination detection for thethree NLI models, and their GraphEval counterparts, in. We report the balanced accuracy as our evalu-ation metric, which corrects for the class imbalance inthe SummEval benchmark. In the case of using the NLImodel directly, we classify the example as containing ahallucination if the NLI model returns a probability ofmore than 0.5. When combining the NLI model withGraphEval, we classify the example as containing a hallu-cination if at least one triple fed to the NLI model returnsa probability of more than 0.5. We see that adding theGraphEval pre-processing step to each of the NLI mod-els almost always improves the balanced accuracy score,sometimes by a considerable amount, such as the resultsfor the SummEval and QAGS-C benchmarks in . On average (weighting by the number of samples ineach dataset), adding the GraphEval pre-processing stepimproves the balanced accuracy by 6.2 (SE=1.3).",
  "Balanced accuracy scores for hallucination detection of NLImodels (HHEM, TRUE, TrueTeacher) and their GraphEvalcounterparts on the SummEval, QAGS-C and QAGS-X bench-marks": "We hypothesise that the negligible difference betweenthe base NLI model and the model supplemented withGraphEval for the QAGS-X dataset is due to the averagelength of the generated text (only 18 words, comparedwith 49 and 63 for QAGS-C and SummEval respectively,see 1). This highlights an important aspect of where themost value can be found in our method. When the LLMoutput is very short, there are less likely to be multiplefacts that need to be checked for consistency (whichcan easily be done without the use of a KG) and theintricacies of the short sentence might even be lost inthe KG construction phase. On the other hand, when theLLM output is very long, current methods struggle totest each individual fact against the context, and this iswhen GraphEval thrives. It should be noted that even when the results for GraphE-val are comparable to the baseline methods, the benefitof using GraphEval is the identification of the specifictriple(s) that are inconsistent with the provided context.",
  ". Hallucination correction with GraphCorrect": "Identifying the particular triple(s) likely to harbor a hallu-cination enables straightforward correction using Graph-Correct, as described in . For each of the evalu-ation frameworks proposed here (HHEM + GraphEval,TRUE + GraphEval, and TrueTeacher + GrapEval), wecompared GraphCorrect to a basic prompting strategyfor hallucination correction, serving as a baseline. Theprompt used in this baseline approach, referred to as theDirect Prompt henceforth, is provided in Appendix D. For each framework, we initially identify hallucinations,correct only the LLM outputs suspected of containing hal-lucinations using either GraphCorrect or Direct Prompt,and then reapply the evaluation framework to detect hal-lucinations in the corrected LLM outputs. Note that thisprocedure only allows us to measure what we presume tobe corrected hallucinations, given the potential for errorsin the evaluation frameworks utilized here. We report the",
  "QAGS-X0.6430.7970.4860.6940.5980.784": "Average ROUGE-1, ROUGE-2 and ROUGE-L scores measuring similarity between original and corrected summaries usingDirect Prompt and GraphCorrect across different datasets and hallucination detection frameworks. percentage of believed corrected hallucinations in . A score of 0% suggests no corrected hallucinationsaccording to the given framework, while a score of 100%indicates correction of all hallucinations as per the givenframework. GraphCorrect outperforms the promptingstrategy proposed here by significantly correcting formore hallucinations on all tasks apart from two relatedto the QAGS-X dataset. As on the hallucination detectiontask, we hypothesise these results are correlated with theaverage length of the text, with GraphCorrect bringingmost value in longer texts with a more complex structureto unravel and correct. Additionally, as previously stated, GraphCorrect offersthe advantage of only modifying the segments of textin the LLM outputs susceptible to hallucinations, whileleaving other sections unaltered, thereby maintaininghigh overall similarity with the original text. This charac-teristic is illustrated in by assessing the ROUGE-1,ROUGE-2, and ROUGE-L metrics between the original",
  "QAGS-X71.169.3": "Percentage of believed corrected hallucinations using a di-rect prompting strategy and GraphCorrect on the SummEval,QAGS-C and QAGS-X benchmarks. The hallucinations werefirst detected by HHEM + GraphEval, TRUE + GraphEval andTrueTeacher + GraphEval respectively, and then correctionswere evaluated by the same metric. summaries and the corrected versions for both GraphCor-rect and Direct Prompt across all experimental scenariosexamined in this study. GraphCorrect systematically gen-erates texts that are closer in similarity to the originalLLM outputs compared to its counterpart.",
  ". Discussion": "Our work focuses on detection of hallucinations in closed-domain tasks, where we are interested only in consis-tency with respect to the provided context. The GraphE-val framework could be extended to open-domain halluci-nation detection by employing agents, as in AutoKG ,to first retrieve relevant external sources as the groundinginformation to check against. We expect that in the near future, more research will beconducted on the construction of KGs from unstructuredtext, which will provide improvements to the first stage ofour procedure and ultimately the evaluation performance.Even as LLMs alone become more powerful, this willcontinue to contribute to improvements in GraphEvalsperformance. We observe that, in the knowledge graph constructionphase of our procedure, it is possible that some informa-tion loss may occur. However, as shown by the resultsin .4, our method rarely leads to a reduction inbalanced accuracy. Furthermore, when it is comparableto the baseline methods, we have the added explainabilityof identifying the specific triples where the hallucinationhas occurred. We believe our hallucination correction framework(GraphCorrect) shows promise and an interesting av-enue for future work. However, the effectiveness of theapproach described in this work should be assessed man-ually, rather than relying on the convoluted use of hallu-cination evaluation frameworks (which only yield mea-",
  ". Conclusion": "We introduce GraphEval, a simple and effective pre-processing step for improving the explainability and per-formance of LLM hallucination detection metrics. Ourmethod leverages LLMs ability to extract informationfrom unstructured text and construct knowledge graphs,whose triples can be fed into out-of-the-box hallucinationdetection methods. We demonstrate that GraphEval in conjunction withstate-of-the-art NLI models leads to an average improve-ment in balanced accuracy of 6.2 (SE=1.3) on three popu-lar hallucination benchmarks. Furthermore, our methodindicates which triples, in the KG representation of theLLM output, are inconsistent. To the best of our knowl-edge, this is the first application of KGs to an LLM-basedhallucination evaluation framework and we believe thesuccess of GraphEval will only grow as KG constructionmethods also improve. Finally, we examined the issue of hallucination correctionand showed that GraphCorrect can effectively address themajority of hallucinations found in LLM outputs whilemaintaining extremely high similarity with the originaltexts.",
  "Garima Agrawal, Tharindu Kumarage, Zeyad Al-ghamdi, and Huan Liu. 2024.Can KnowledgeGraphs Reduce Hallucinations in LLMs? : A Survey.arXiv:2311.07914 [cs.CL]": "Samuel R. Bowman, Gabor Angeli, ChristopherPotts, and Christopher D. Manning. 2015. A largeannotated corpus for learning natural language in-ference. In Proceedings of the 2015 Conference onEmpirical Methods in Natural Language Process-ing, Llus Mrquez, Chris Callison-Burch, and JianSu (Eds.). Association for Computational Linguis-tics, Lisbon, Portugal, 632642. Jacob Devlin, Ming-Wei Chang, Kenton Lee, andKristina Toutanova. 2019. BERT: Pre-training ofDeep Bidirectional Transformers for Language Un-derstanding. In Proceedings of the 2019 Conferenceof the North American Chapter of the Associationfor Computational Linguistics: Human LanguageTechnologies, Volume 1 (Long and Short Papers), JillBurstein, Christy Doran, and Thamar Solorio (Eds.).Association for Computational Linguistics, Min-",
  "Pengcheng He, Xiaodong Liu, Jianfeng Gao, andWeizhu Chen. 2021.DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED AT-TENTION. In International Conference on LearningRepresentations": "Karl Moritz Hermann, Tomas Kocisky, EdwardGrefenstette, Lasse Espeholt, Will Kay, Mustafa Su-leyman, and Phil Blunsom. 2015. Teaching ma-chines to read and comprehend. Advances in neuralinformation processing systems 28 (2015). Or Honovich, Roee Aharoni, Jonathan Herzig, Ha-gai Taitelbaum, Doron Kukliansy, Vered Cohen,Thomas Scialom, Idan Szpektor, Avinatan Hassidim,and Yossi Matias. 2022. TRUE: Re-evaluating Fac-tual Consistency Evaluation. In Proceedings of theSecond DialDoc Workshop on Document-groundedDialogue and Conversational Question Answering,Song Feng, Hui Wan, Caixia Yuan, and Han Yu(Eds.). Association for Computational Linguistics,Dublin, Ireland, 161175. Or Honovich, Leshem Choshen, Roee Aharoni, EllaNeeman, Idan Szpektor, and Omri Abend. 2021.2: Evaluating Factual Consistency in Knowledge-Grounded Dialogues via Question Generation andQuestion Answering. In Proceedings of the 2021Conference on Empirical Methods in Natural Lan-guage Processing, Marie-Francine Moens, XuanjingHuang, Lucia Specia, and Scott Wen-tau Yih (Eds.).Association for Computational Linguistics, Onlineand Punta Cana, Dominican Republic, 78567870.",
  "Tushar Khot, Ashish Sabharwal, and Peter Clark.2018. SciTail: A Textual Entailment Dataset fromScience Question Answering. In AAAI": "Abhijeet Kumar, Abhishek Pandey, Rohit Gadia,and Mridul Mishra. 2020.Building KnowledgeGraph using Pre-trained Language Model for Learn-ing Entity-aware Relationships. In 2020 IEEE Inter-national Conference on Computing, Power and Com-munication Technologies (GUCON). 310315. Philippe Laban, Tobias Schnabel, Paul N. Bennett,and Marti A. Hearst. 2022. SummaC: Re-VisitingNLI-based Models for Inconsistency Detection inSummarization. Transactions of the Association forComputational Linguistics 10 (2022), 163177. Patrick Lewis, Ethan Perez, Aleksandra Piktus,Fabio Petroni, Vladimir Karpukhin, Naman Goyal,Heinrich Kttler, Mike Lewis, Wen-tau Yih, TimRocktschel, et al. 2020. Retrieval-augmented gener-ation for knowledge-intensive nlp tasks. Advancesin Neural Information Processing Systems 33 (2020),94599474.",
  "for Computational Linguistics, Online, 19061919": "Igor Melnyk, Pierre Dognin, and Payel Das. 2021.Grapher: Multi-Stage Knowledge Graph Construc-tion using Pretrained Language Models. In NeurIPS2021 Workshop on Deep Generative Models andDownstream Applications. Sewon Min, Kalpesh Krishna, Xinxi Lyu, MikeLewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023.Factscore: Fine-grained atomic evaluation of fac-tual precision in long form text generation. arXivpreprint arXiv:2305.14251 (2023). Niels Mndler, Jingxuan He, Slobodan Jenko, andMartin Vechev. 2024. Self-contradictory Halluci-nations of Large Language Models: Evaluation,Detection and Mitigation. In The Twelfth Inter-national Conference on Learning Representations. Shashi Narayan, Shay B. Cohen, and Mirella Lapata.2018. Dont Give Me the Details, Just the Summary!Topic-Aware Convolutional Neural Networks forExtreme Summarization. In Proceedings of the 2018Conference on Empirical Methods in Natural Lan-guage Processing, Ellen Riloff, David Chiang, JuliaHockenmaier, and Junichi Tsujii (Eds.). Associationfor Computational Linguistics, Brussels, Belgium,17971807. Kishore Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu. 2002.Bleu: a Method for Auto-matic Evaluation of Machine Translation. In Pro-ceedings of the 40th Annual Meeting of the Associa-tion for Computational Linguistics, Pierre Isabelle,Eugene Charniak, and Dekang Lin (Eds.). Associ-ation for Computational Linguistics, Philadelphia,Pennsylvania, USA, 311318. Colin Raffel, Noam Shazeer, Adam Roberts, Kather-ine Lee, Sharan Narang, Michael Matena, YanqiZhou, Wei Li, and Peter J. Liu. 2020. Exploring theLimits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Re-search 21, 140 (2020), 167. Leonardo F. R. Ribeiro, Mengwen Liu, IrynaGurevych, Markus Dreyer, and Mohit Bansal.2022. FactGraph: Evaluating Factuality in Sum-marization with Semantic Graph Representations.arXiv:2204.06508 [cs.CL] Tal Schuster, Adam Fisch, and Regina Barzilay.2021. Get Your Vitamin C! Robust Fact Verifica-tion with Contrastive Evidence. In Proceedings ofthe 2021 Conference of the North American Chap-ter of the Association for Computational Linguis-",
  "tics: Human Language Technologies. Associationfor Computational Linguistics, Online, 624643": "James Thorne, Andreas Vlachos, Oana Cocarascu,Christos Christodoulopoulos, and Arpit Mittal.2018. The FEVER2.0 Shared Task. In Proceedings ofthe Second Workshop on Fact Extraction and VERifi-cation (FEVER). Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020.Asking and Answering Questions to Evaluate theFactual Consistency of Summaries. In Proceedings ofthe 58th Annual Meeting of the Association for Com-putational Linguistics, Dan Jurafsky, Joyce Chai, Na-talie Schluter, and Joel Tetreault (Eds.). Associationfor Computational Linguistics, Online, 50085020. Jerry Wei, Chengrun Yang, Xinying Song, YifengLu, Nathan Hu, Dustin Tran, Daiyi Peng, RuiboLiu, Da Huang, Cosmo Du, et al. 2024. Long-formfactuality in large language models. arXiv preprintarXiv:2403.18802 (2024). Adina Williams, Nikita Nangia, and Samuel Bow-man. 2018. A Broad-Coverage Challenge Corpusfor Sentence Understanding through Inference. InProceedings of the 2018 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,Volume 1 (Long Papers), Marilyn Walker, Heng Ji,and Amanda Stent (Eds.). Association for Compu-tational Linguistics, New Orleans, Louisiana, 11121122. Linyao Yang, Hongyang Chen, Zhao Li, Xiao Ding,and Xindong Wu. 2024. Give us the facts: Enhanc-ing large language models with knowledge graphsfor fact-aware language modeling. IEEE Transac-tions on Knowledge and Data Engineering (2024).",
  "( \" system \" ,": "\" \" \"You are anexpertate x t r a c t i n ginformationins t r u c t u r e dformatstob u i l daknowledge graph .Step 1 E n t i t yd e t e c t i o n :I d e n t i f ya l le n t i t i e sinthe rawt e x t .Make surenottomiss any out .E n t i t i e sshouldbeb a s i candsimple ,theyareakintoWikipedianodes .Step 2 Coreferencer e s o l u t i o n :Finda l le x p r e s s i o n sinthet e x tt h a tr e f e rtothe samee n t i t y .Makesuree n t i t i e sarenotd u p l i c a t e d .Inp a r t i c u l a rdo noti n c l u d ee n t i t i e st h a tare mores p e c i f i cv e r s i o n sthemselves ,e . g .\" ad e t a i l e dviewofj u p i t e r satmosphere \"and\" j u p i t e r satmosphere \" ,onlyi n c l u d ethemosts p e c i f i cversionofthee n t i t y .Step 3 R e l a t i o ne x t r a c t i o n :I d e n t i f ysemanticr e l a t i o n s h i p sbetweenthee n t i t i e syou havei d e n t i f i e d . Format :Returntheknowledge graphasal i s toft r i p l e s ,i . e .[ \" e n t i t y1 \" ,\" r e l a t i o n1 2\" ,\" e n t i t y2 \" ] ,inPython code .\" \" \" ,) ,( \" human \" ,\" Usethegivenformattoe x t r a c tinformationfromthefo ll owi nginput :<input >{input } </ input >.Skipthepreamble andoutputther e s u l tas al i s twithin <python > </ python > tags . \" ,) ,( \" human \" ,\" \" \" ImportantTips :1 .Make surea l linformationi sincludedintheknowledge graph .2 .Eacht r i p l emust onlycontainthrees t r i n g s !None ofthes t r i n g sshouldbe empty .3 . Do nots p l i tupr e l a t e dinformationi n t os e p a r a t et r i p l e sbecauset h i scouldchangethemeaning .4 .Make surea l lb r a c k e t sandquotationmarksarematched .5 .Beforeadding at r i p l eto",
  "theknowledge graph ,checktheconcatenatedt r i p l emakes senseas asentence .I fnot ,d i s c a r di t .\" \" \" ,) ,( \" human \" ,\" \" \" Hereare some exampleinputand outputp a i r s": "## Example1 .Input :\" The Walt Disney Company ,commonly known asDisney ,i san Americanm u l t i n a t i o n a lmass media andentertainmentconglomeratet h a ti sheadquarteredattheWaltDisneyStudioscomplexinBurbank ,C a l i f o r n i a . \"Output :<python >[ [ \" The Walt Disney Company \" ,\"headquarteredat \" , \" WaltDisneyStudioscomplexinBurbank ,C a l i f o r n i a \" ] ,[ \" The Walt Disney Company \" ,\"commonly known as \" ,\" Disney\" ] ,[ \" The Walt Disney Company \" ,\"i n s t a n c eof \" ,\" Americanm u l t i n a t i o n a lmass media andentertainmentconglomerate \" ] ]</ python > ## Example2 .Input :\" Amanda Jackson was borninS p r i n g f i e l d ,Ohio , USA onJune1 ,1 9 8 5 .She was ab a s k e t b a l lplayerf o rthe U . S. women s team . \"Output :<python >[[ \" Amanda Jackson \" ,\" bornin \" ,\"S p r i n g f i e l d ,Ohio , USA \" ] ,[ \" Amanda Jackson \" ,\" born on \" ,\"June1 ,1 9 8 5 \" ] ,[ \" Amanda Jackson \" ,\" occupation \" ,\" b a s k e t b a l lplayer \" ] ,[ \" Amanda Jackson \" ,\" playedf o r \" ,\"U . S . women sb a s k e t b a l lteam\" ] ]</ python >",
  "## Example3 .Input :\" Musice x e c u t i v eDarius Van Armanwas borninPennsylvania . HeattendedGonzagaCollege": "High School andi sa humanbeing . \"Output :<python >[[ \" Darius Van Arman \" ,\"occupation \" ,\" Musice x e c u t i v e\" ] ,[ \" Darius Van Arman \" ,\" bornin \" ,\"Pennsylvania \" ] ,[ \" Darius Van Arman \" ,\" attended \" ,\" GonzagaCollegeHigh School\" ] ,[ \" Darius Van Arman \" ,\"i n s t a n c eof \" ,\" human being \" ] ]</ python > ## Example4 .Input :\" I t a l yhad3 . 6 x times morecasesofcoronavirusthanChina . \"Output :<python >[[ \" I t a l y \" ,\" had3 . 6 x times morecasesofcoronavirusthan \" ,\"China \" ] ]</ python >\" \" \" ,) ,",
  "B. Hallucination correction (step 1)": "\" \" \"You are anexpertate x t r a c t i n ginformationins t r u c t u r e dformatsfromt e x t .Thefo ll owi ngt r i p l ec o n t a i n sf a c t u a l l yi n c o r r e c tinformation .Correcti tbased on theprovidedcontext ,ImportantTips :1 . A t r i p l ei sdefinedas[ \"e n t i t y1 \" ,\" r e l a t i o n1 2\" ,\"e n t i t y2 \" ] .2 . A t r i p l emust onlycontainthrees t r i n g s ! None ofthes t r i n g sshouldbe empty .3 .The concatenatedt r i p l emustmake senseas asentence .4 .Onlyreturnthec o r r e c t e dt r i p l e ,nothinge l s e .",
  "C. Hallucination correction (step2)": "\" \" \"Inthefollowingcontext ,r e p l a c etheinformationoftheoldt r i p l ewiththeinformationofthe newone .Do not make anyotherm o d i f i c a t i o ntothecontext .Onlyreturnthe new context .<context >{ summary } </ context >< o l d _ t r i p l e >{ o l d _ t r i p l e } </ o l d _ t r i p l e ><new_triple >{ new_triple } </ new_triple >\" \" \"",
  "D. Hallucination correctionwithout a KG": "\" \" \"Thefollowing summary containsf a c t u a l l yi n c o r r e c tinformation .Correcti tbased on thecontext ,butdon tchangeotherp a r t softhesummary .Onlyreturnthec o r r e c t e d summary ,nothinge l s e .<summary >{ summary } </ summary><context >{ context } </ context >Remember ,do minimalchangestotheo r i g i n a lsummary ,don t makei tlongerand keepas much ofi tasyou cane x a c t l ythe same .\" \" \""
}