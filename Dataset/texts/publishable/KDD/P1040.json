{
  "ABSTRACT": "Collaborative Filtering (CF) typically suffers from the significantchallenge of popularity bias due to the uneven distribution of itemsin real-world datasets. This bias leads to a significant accuracy gapbetween popular and unpopular items. It not only hinders accurateuser preference understanding but also exacerbates the Matthew ef-fect in recommendation systems. To alleviate popularity bias, exist-ing efforts focus on emphasizing unpopular items or separating thecorrelation between item representations and their popularity. De-spite the effectiveness, existing works still face two persistent chal-lenges: (1) how to extract common supervision signals from popularitems to improve the unpopular item representations, and (2) howto alleviate the representation separation caused by popularity bias.In this work, we conduct an empirical analysis of popularity biasand propose Popularity-Aware Alignment and Contrast (PAAC) toaddress two challenges. Specifically, we use the common super-visory signals modeled in popular item representations and pro-pose a novel popularity-aware supervised alignment module tolearn unpopular item representations. Additionally, we suggestre-weighting the contrastive learning loss to mitigate the repre-sentation separation from a popularity-centric perspective. Finally,we validate the effectiveness and rationale of PAAC in mitigatingpopularity bias through extensive experiments on three real-worlddatasets. Our code is available at",
  "Corresponding Authors": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0490-1/24/08",
  "Collaborative Filtering, Popularity Bias, Supervised Alignment, Re-weighting, Contrastive Learning": "ACM Reference Format:Miaomiao Cai, Lei Chen, Yifan Wang, Haoyue Bai, Peijie Sun, Le Wu, MinZhang, and Meng Wang. 2024. Popularity-Aware Alignment and Contrastfor Mitigating Popularity Bias. In Proceedings of the 30th ACM SIGKDDConference on Knowledge Discovery and Data Mining (KDD 24), August2529, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.",
  "INTRODUCTION": "Modern recommender systems play a crucial role in mitigatinginformation overload . Collaborative filtering (CF)is widely used in personalized recommendations to help users finditems of potential interest. CF-based methods primarily learn userpreferences and item characteristics by aligning the representationsof users and the items they interact with . Despite theirsuccess, CF-based methods often face popularity bias , result-ing in significant accuracy gaps between popular and unpopularitems . Popularity bias stems from the limited supervisorysignals for unpopular items, causing overfitting during trainingand reducing performance on the test set. It hinders the accurateunderstanding of user preferences, decreasing recommendationdiversity . Whats even worse is that popularity bias mayexacerbate the Matthew effect, where popular items become evenmore popular due to frequent recommendations .Mitigating popularity bias in recommendation systems, as de-scribed in Figure. 1, presents two primary and significant chal-lenges. The first challenge arises from insufficient representationsof unpopular items during training, leading to overfitting and poorgeneralization performance. The second challenge, representation",
  ": Popularity bias presents two challenges: (1) Over-fitting caused by limited supervisory signals for unpopularitems, and (2) Representation separation in item embeddingsdriven by popularity bias": "separation, occurs when popular and unpopular items are modeledinto different semantic spaces, exacerbating bias and reducing rec-ommendation accuracy. Next, we will explore these challenges anddiscuss potential solutions to mitigate popularity bias.Due to the limited supervisory signals for unpopular items, theirrepresentations are insufficient leading to overfitting. During train-ing, representation alignment focuses on users and the items theyhave interacted with . However, due to limited interactions,unpopular items are often modeled around a small number of users.This focused modeling can lead to overfitting due to the insufficientrepresentations of unpopular items. As shown in the left part ofFigure.1, we divide items into popular and unpopular groups basedon the Pareto principle . And then evaluate their performancein both training and testing sets (measured using @100).The results reveal that traditional methods achieve higher accu-racy for unpopular items during training but significantly loweraccuracy during testing, indicating clear overfitting. To address thisissue, previous studies have tried to boost the training weights orprediction scores for unpopular items, such as IPS , MACR ,and others . However, as shown in Figure.1, overfittingstill exists even with augmented supervisory weights for unpopularitems. This may be because unpopular items still lack sufficientsupervisory signals, leading to inadequate representation capability.Therefore, how to enhance the representation modeling ofunpopular items remains a challenge.Recent studies indicate that popularity bias causes representationseparation in item embeddings . Specifically, the model rep-resents popular and unpopular items in different semantic spacesaccording to their popularity levels. As shown in the right partof Figure.1, we train LightGCN on the Yelp2018 dataset1, ran-domly selected users and items, and visualize them using t-SNE dimensionality reduction. The blue dots represent unpopular items,the yellow dots represent popular items, and the orange dots rep-resent users. As seen, there is a clear distinction in the positionsof unpopular and popular items in the representation space. Userrepresentations show a preference for popular items, exacerbatingpopularity bias. Existing methods try to alleviate representation sep-aration by either removing the correlation between item represen-tations and popularity or enhancing overall consistency through contrastive learning . However, blindly removingpopularity information can harm recommendation accuracy .While contrastive learning methods improve recommendation per-formance, they often worsen representation separation by pushingpositive and negative samples apart. When negative samples followthe popularity distribution , most are popular items. Optimiz-ing for unpopular items as positive samples pushes popular itemsfurther away, intensifying representation separation. Conversely,when negative samples follow a uniform distribution , mostare unpopular items. Optimizing for popular items as positive sam-ples separates them from most unpopular items, again worseningrepresentation separation. Therefore, how to effectively solverepresentation separation is also crucial.In this work, we conduct a analysis of popularity bias and pro-pose Popularity-Aware Alignment and Contrast (PAAC) to addresstwo challenges. Our model PAAC primarily consists of the follow-ing two modules: (1) Supervised Alignment Module: To enhancethe representations of unpopular items with more supervision sig-nals, we use common supervisory signals modeled in popular itemrepresentations and propose a popularity-aware supervised align-ment module. Intuitively, items interacted with by the same usershare similar characteristics. By leveraging similar characteristicsmodeled in popular item representation, we propose to align therepresentations of popular and unpopular items interacted with bythe same user. (2) Re-weighting Contrast Module: To better alle-viate representation separation, we propose a re-weighting contrastmodule from a popularity-centric perspective. Considering the influ-ence of various popularity levels on recommendation performanceas positive and negative samples, we introduce hyperparameters and to control the weighting of samples with different itempopularity levels. Our contributions can be summarized in threekey points: To provide more supervisory signals for unpopular items, weleverage common characteristics modeled in popular item rep-resentations and propose a popularity-aware supervised align-ment module to enhance the unpopular item representations.",
  "PRELIMINARY2.1Collaborative Filtering": "The core of CF-based models is to learn user preferences and itemcharacteristics by aligning user and item representations basedon their interactions . Based on these representations, thetrained model predicts potential interactions for recommendation .Specifically, let (| | = ) and (| | = ) represent the sets ofusers and items, respectively. In the implicit feedback setting, theobserved interactions are represented by the matrix R 0, 1 ,where R, = 1 indicates an interaction between user and item ,and R, = 0 indicates no interaction. To better learn user prefer-ences and item characteristics, we use LightGCN as the encoder.It employs Graph Convolution Networks (GCNs) to learn high-order",
  "Popularity-Aware Alignment and Contrast for Mitigating Popularity BiasKDD 24, August 2529, 2024, Barcelona, Spain": "threshold, which tends to overrepresent popular items in somemini-batches, our approach dynamically divides items in each mini-batch into popular and unpopular categories. Specifically, the top% of items are classified as popular and the remaining (100 )%as unpopular, with varying. This strategy prevents the overrepre-sentation typical in fixed distribution models, which could skew thelearning process and degrade performance. To quantify the effectsof these varying ratios, we examined various division ratios for pop-ular items, including 20%, 40%, 60%, and 80%, as shown in Table. 3.The preliminary results indicate that both extremely low and highratios negatively affect model performance, thereby underscoringthe superiority of our dynamic data partitioning approach. More-over, within the 40%-60% range, our models performance remainedconsistently robust, further validating the effectiveness of PAAC.",
  "where and represent positive and negative items respectively,": "and are the item representations after different data augmenta-tions, and > 0 is the temperature coefficient. In this work, we usenoise perturbation for data augmentation, a simpler and more effec-tive method than graph augmentation . Although effective,CL-based methods tend to exacerbate representation separation byincreasing the distance between positive and negative samples, asillustrated in .",
  "THE PROPOSED MODEL": "To address the existing challenges in mitigating popularity bias, wepropose Popularity-Aware Alignment and Contrast (PAAC), as illus-trated in Figure. 2. We leverage the common supervisory signalsin popular item representations to guide the learning of unpop-ular representations and propose a popularity-aware supervisedalignment module. Additionally, we introduce a re-weighting mech-anism in the contrastive learning module to address representationseparation from a popularity-centric perspective.",
  "Supervised Alignment Module": "During training, the alignment of representations typically empha-sizes users and items that have interacted , often resultingin items being closer to interacted users than non-interacted onesin the representation space. However, due to the limited interac-tions of unpopular items, they tend to be modeled based on a smallsubset of users. This narrow focus might lead to overfitting, as therepresentations of unpopular items may not adequately capture their characteristics. As illustrated in , how to enhanceunpopular representation modeling remains a challenge.The difference in the number of supervisory signals is crucialin learning representations for popular and unpopular items. Inparticular, popular items benefit from an abundance of supervi-sory signals throughout the alignment process, facilitating effectivelearning of their representations. In contrast, unpopular items witha limited number of supervised users are more prone to overfitting.This is due to insufficient representation learning for unpopularitems, highlighting the impact of supervisory signal distributionon representation quality. Intuitively, items interacted with by thesame user share some similar characteristics. In this part, we lever-age common supervisory signals in popular item representationsand introduce a popularity-aware supervised alignment method toenhance unpopular item representations.Specifically, we first filter items with similar characteristics basedon the users interests. For any user , we refer to the set of itemsthey interact with as :",
  ", () > (),(4)": "where and are disjoint, and the popularity of eachitem in the popular group is greater than that of any item in theunpopular group . This means that popular items receive moresupervisory information than unpopular items, leading to poorerrecommendation performance for unpopular items.To address the challenge of inadequate representation learningfor unpopular items, we leverage the assumption that items inter-acted with by the same user exhibit some similar characteristics.Specifically, we use similar supervisory signals in popular itemrepresentations to enhance the representations of unpopular items.Inspired by previous works , we align the representations ofitems in and to provide more supervisory informationto unpopular items and enhance its representation, as follows:",
  "( ,) + ( ,)": ": An Illustration of our proposed Popularity-Aware Alignment and Contrast (PAAC), which consists of the SupervisedAlignment Module and the Re-weighting Contrast Module. Supervised Alignment Module leverages the common supervisionsignal in popular representations to guide the learning of unpopular representations. Re-weighting Contrast Module addressrepresentation separation from a popularity-centric perspective. inadvertently worsen this separation. When negative samples fol-low the popularity distribution , dominated by popular items,optimizing for unpopular items as positive samples enlarges thegap between popular and unpopular items in the representationspace. Conversely, when negative samples follow a uniform dis-tribution , focusing on popular items separates them from themajority of unpopular ones, worsening the representation gap. Ex-isting studies use the same weights for positive and negativesamples in the contrastive loss function, without considering differ-ences in item popularity. However, in real-world recommendationdatasets, the impact of items varies due to dataset characteristicsand interaction distributions. Neglecting this aspect could lead tosuboptimal results and exacerbate representation separation.Inspired by previous works , we propose to identify dif-ferent influences by re-weighting different popularity items. To thisend, we introduce re-weighting different positive and negative sam-ples to mitigate representation separation from a popularity-centricperspective. We incorporate this approach into contrastive learningto better optimize the consistency of representations. Specifically,we aim to reduce the risk of pushing items with varying popularityfurther apart. For example, when using a popular item as a positivesample, our goal is to avoid pushing unpopular items too far away.Thus, we introduce two hyperparameters to control the weightswhen items are considered positive and negative samples.To ensure balanced and equitable representations of items withinour model, we first propose a dynamic strategy to categorize itemsinto popular and unpopular groups for each mini-batch. Insteadof relying on a fixed global threshold, which often leads to theoverrepresentation of popular items across various batches, weimplement a hyperparameter . This hyperparameter readjusts theclassification of items within the current batch. By adjusting the hyperparameter , we maintain a balance between different itempopularity levels. This enhances the models ability to generalizeacross diverse item sets by accurately reflecting the popularitydistribution in the current training context. Specifically, we denotethe set of items within each batch as . And then we divide into a popular group and an unpopular group basedon their respective popularity levels, classifying the top % of itemsas :",
  ", () > (),(6)": "where and are disjoint, with consistingof the top % of items in the batch. In this work, we dynamicallydivided items into popular and unpopular groups within each mini-batch based on their popularity, assigning the top 50% as popularitems and the bottom 50% as unpopular items. This radio not onlyensures equal representation of both groups in our contrastivelearning but also allows items to be classified adaptively based onthe batchs current composition.After that, we use InfoNCE to optimize the uniformity ofitem representations . Unlike traditional CL-based methods,we calculate the loss for different item groups. Specifically, weintroduce the hyperparameter to control the positive sampleweights between popular and unpopular items, adapting to varyingitem distributions in different datasets:",
  "( /) ,": "(9)where the parameter ranges from 0 to 1, controlling the negativesample weighting in the contrastive loss. When = 0, it meansthat only intra-group uniformity optimization is performed. Con-versely, when = 1, it means equal treatment of both popularand unpopular items in terms of their impact on positive samples.The setting of allows for a flexible adjustment between prioritiz-ing intra-group uniformity and considering the impact of differentpopularity levels in the training. We prefer to push away itemswithin the same group to optimize uniformity. This setup helpsprevent over-optimizing the uniformity of different groups, therebymitigating representation separation.The final re-weighting contrastive objective is the weighted sumof the user objective and the item objective:",
  "Experiments Settings": "4.1.1Datasets. In our experiments, we use three widely publicdatasets: Amazon-book2, Yelp20183, and Gowalla4. We retainedusers and items with a minimum of 10 interactions, consistent withprevious works . A detailed description can be found inAppendix A.1.Note that the traditional dataset splitting methods fail to assessthe effectiveness in mitigating popularity bias because the test setsstill follow the long-tail distribution . In such cases, themodel might perform well during testing even if it heavily relieson popularity for recommendations . Hence, the conventionaldataset splitting is not appropriate for evaluating whether the modelsuffers from popularity bias . To this end, we follow previous",
  "KDD 24, August 2529, 2024, Barcelona, SpainMiaomiao Cai, et al": "works to extract an unbiased dataset where the item distribution inthe test set follows a uniform distribution . Specifically,we retain a fixed number of interactions for each item in the test set,amounting to approximately 10% of the entire dataset. Additionally,to avoid exposing the test distribution, we randomly selected 10% ofinteractions from the dataset as the validation set, and the remainingas the training set. 4.1.2Baselines and Evaluation Metrics. We implement thestate-of-the-art LightGCN to instantiate PAAC, aiming to in-vestigate how it alleviates popularity bias. We compare PAAC withseveral debiased baselines, including re-weighting-based modelssuch as IPS and -AdjNorm , decorrelation-based modelslike MACR and InvCF , and contrastive learning-basedmodels including Adap- and SimGCL . For detailed descrip-tions of these models, please refer to Appendix A.2.We utilize three widely used metrics, namely @, @,and @, to evaluate the performance of Top- recommen-dation. @ and @ assess the number of target itemsretrieved in the recommendation results, emphasizing coverage. Incontrast, @ evaluates the positions of target items in theranking list, with a focus on their positions in the list. Note that weuse the full ranking strategy , considering all non-interacteditems as candidate items to avoid selection bias during the teststage . We repeated each experiment five times with differentrandom seeds and reported the average scores.",
  "Overall Performance (RQ1)": "As shown in Table. 1, we compare our model with several base-lines across three datasets. The best performance for each metric ishighlighted in bold, while the second best is underlined. Our modelconsistently outperforms all compared methods across all metricsin every dataset. Our proposed model PAAC consistently outperforms all base-lines and significantly mitigates the popularity bias. Specifi-cally, PAAC enhances LightGCN, achieving improvements of282.65%, 180.79%, and 82.89% in @20 on the Yelp2018,Gowalla, and Amazon-Book datasets, respectively. Comparedto the strongest baselines (SimGCL or Adap-), PAAC deliversbetter performance. The most significant improvements areobserved on Yelp2018, where our model achieves an 8.70% in-crease in @20, a 10.81% increase in @20, and a 30.2%increase in @20. This improvement can be attributed toour use of popularity-aware supervised alignment to enhancethe representation of less popular items and re-weighted con-trastive learning to address representation separation from apopularity-centric perspective. The performance improvements of PAAC are smaller on sparserdatasets. For example, on the Gowalla dataset, the improve-ments in @20, @20, and @20 are 3.18%, 5.85%,and 5.47%, respectively. This may be because, in sparser datasetslike Gowalla, even popular items are not well-represented dueto lower data density. Aligning unpopular items with thesepoorly represented popular items can introduce noise into the",
  "model. Therefore, the benefits of using supervisory signals forunpopular items may be reduced in very sparse environments,leading to smaller performance improvements": "Regarding the baselines for mitigating popularity bias, the im-provement of -Adjnorm is relatively limited compared to thebackbone model (LightGCN) and even performs worse in somecases. This may be because -Adjnorm is specifically designedfor traditional data-splitting scenarios, where the test set stillfollows a long-tail distribution, leading to poor generalization.IPS and MACR mitigate popularity bias by excluding item pop-ularity information. InvCF uses invariant learning to removepopularity information at the representation level, generallyperforming better than IPS and MACR. This shows the impor-tance of addressing popularity bias at the representation level.Adap- and SimGCL outperform the other baselines, emphasiz-ing the necessary to improve item representation consistencyfor mitigating popularity bias. Different metrics across various datasets show varying im-provements in model performance. Adapt- performs well on@20 and @20, while SimGCL excels in @20.This suggests that different debiasing methods may need dis-tinct optimization strategies for models. Additionally, we ob-serve varying effects of PAAC across different datasets, withperformance improvements of 9.76%, 7.35%, and 4.83% on theYelp2018, Amazon-Book, and Gowalla datasets, respectively.This difference could be due to the sparser nature of the Gowalladataset. Conversely, our model can directly provide supervisorysignals for unpopular items and conduct intra-group optimiza-tion, consistently maintaining optimal performance across allmetrics on the three datasets.",
  "Ablation Study (RQ2)": "To better understand the effectiveness of each component in PAAC,we conduct ablation studies on three datasets. Table. 2 presents acomparison between PAAC and its variants on recommendationperformance. Specifically, PAAC-w/o refers to the variant wherethe re-weighting contrastive loss of popular items is removed, fo-cusing instead on optimizing the consistency of representations forunpopular items. Similarly, PAAC-w/o denotes the removal ofthe re-weighting contrastive loss for unpopular items. PAAC-w/o refers to the variant without the popularity-aware supervised align-ment loss. Its worth noting that PAAC-w/o differs from SimGCLin that we split the contrastive loss on the item side, L, intotwo distinct losses: Land L. This approach allows us toseparately address the consistency of popular and unpopular itemrepresentations, thereby providing a more detailed analysis of theimpact of each component on the overall performance.From Table. 2, we observe that PAAC-w/o outperforms SimGCLin most cases. This validates that re-weighting the importance ofpopular and unpopular items can effectively improve the modelsperformance in alleviating popularity bias. It also demonstrates theeffectiveness of using supervision signals from popular items toenhance the representations of unpopular items, providing more op-portunities for future research on mitigating popularity bias. More-over, compared with PAAC-w/o , PAAC-w/o results in muchworse performance. This confirms the importance of re-weighting",
  "PAAC0.0494*0.0574*0.0375*0.1232*0.1321*0.0848*0.0701*0.0724*0.0556*": "popular items in contrastive learning for mitigating popularity bias.Finally, PAAC consistently outperforms the three variants, demon-strating the effectiveness of combining supervised alignment andre-weighting contrastive learning. Based on the above analysis, weconclude that leveraging supervisory signals from popular itemrepresentations can better optimize representations for unpopu-lar items, and re-weighting contrastive learning allows the modelto focus on more informative or critical samples, thereby improv-ing overall performance. All the proposed modules significantlycontribute to alleviating popularity bias.",
  "Debias Ability (RQ3)": "To further verify the effectiveness of PAAC in alleviating popularitybias, we conduct a comprehensive analysis focusing on the recom-mendation performance across different popularity item groups.Specifically, 20% of the most popular items are labeled Popular,and the rest are labeled Unpopular. As shown in Figure. 3, wecompare the performance of PAAC with LightGCN, IPS, MACR,and SimGCL using the @20 metric across different popular-ity groups. We use to denote the accuracy gap between the twogroups. From Figure. 3, we draw the following conclusions: Our proposed PAAC significantly enhances the recommenda-tion performance for unpopular items. Specifically, we observean improvement of 8.94% and 7.30% in @20 relative toSimGCL on the Gowalla and Yelp2018 datasets, respectively.This improvement is due to the popularity-aware supervised",
  "alignment method, which uses supervisory signals from popu-lar items to improve the representations of unpopular items": "PAAC has successfully narrowed the accuracy gap between dif-ferent item groups. Specifically, PAAC achieved the smallest gap,reducing the @20 accuracy gap by 34.18% and 87.50% onthe Gowalla and Yelp2018 datasets, respectively. This indicatesthat our method treats items from different groups fairly, ef-fectively alleviating the impact of popularity bias. This successcan be attributed to our re-weighted contrast module, whichaddresses representation separation from a popularity-centricperspective, resulting in more consistent recommendation re-sults across different groups. Improving the performance of unpopular items is crucial for en-hancing overall model performance. Specially, on the Yelp2018dataset, PAAC shows reduced accuracy in recommending pop-ular items, with a notable decrease of 20.14% compared toSimGCL. However, despite this decrease, the overall recommen-dation accuracy surpasses that of SimGCL by 11.94%, primarilydue to a 6.81% improvement in recommending unpopular items.This improvement highlights the importance of better recom-mendations for unpopular items and emphasizes their crucialrole in enhancing overall model performance.",
  "Hyperparameter Sensitivities (RQ4)": "In this section, we analyze the impact of hyperparameters in PAAC.Firstly, we investigate the influence of 1 and 2, which respectivelycontrol the impact of the popularity-aware supervised alignmentand re-weighting contrast loss. Additionally, in the re-weightingcontrastive loss, we introduce two hyperparameters, and , tocontrol the re-weighting of different popularity items as positiveand negative samples. Finally, we explore the impact of the groupingratio on the models performance. 4.5.1Effect of 1 and 2. As formulated in Eq. (11), 1 controlsthe extent of providing additional supervisory signals for unpopularitems, while 2 controls the extent of optimizing representationconsistency. Figure. 4 illustrates how the relative performance tothe best baseline @20 varies with 1 and 2 on the Yelp2018and Gowalla datasets. Horizontally, with the increase in 2, theperformance initially increases and then decreases. This indicatesthat appropriate re-weighting contrastive loss effectively enhancesthe consistency of representation distributions, mitigating popular-ity bias. However, overly strong contrastive loss may lead the modelto neglect recommendation accuracy. Vertically, as 1 increases,the performance also initially increases and then decreases. Thissuggests that suitable alignment can provide beneficial supervisorysignals for unpopular items, while too strong an alignment may in-troduce more noise from popular items to unpopular ones, therebyimpacting recommendation performance. 4.5.2Effect of re-weighting coefficient and . To mitigaterepresentation separation due to imbalanced positive and negativesampling, we introduce two hyperparameters into the contrastiveloss. Specifically, controls the weight difference between positive 00.20.40.60.81.0 (= 0.6,1 = 1000,2 = 10) 0.000 0.020 0.040 0.060 NDCG@20 NDCG@20 0.000 0.025 0.050 0.075 HR@20 Yelp2018 HR@20 00.20.40.60.81.0 ( = 0.8,1 = 1000,2 = 10) 0.033 0.035 0.036 0.037 0.039 NDCG@20 NDCG@20 0.050 0.053 0.055 0.058 0.060 HR@20 Yelp2018 HR@20 00.20.40.60.81.0 (= 0.2,1 = 50,2 = 5) 0.020 0.040 0.060 0.080 0.100 NDCG@20 NDCG@20 0.050 0.075 0.100 0.125 0.150 HR@20",
  "%0.04670.05450.03500.11760.12700.0818": "samples from popular and unpopular items, while controls theinfluence of different popularity items as negative samples.In our experiments, while keeping other hyperparameters con-stant, we search and within the range {0, 0.2, 0.4, 0.6, 0.8, 1}.Figure. 5 illustrates how performance changes when varying and on two datasets, with horizontal lines representing the bestbaseline. As and increase, performance initially improves andthen declines. The optimal hyperparameters for the Yelp2018 andGowalla datasets are = 0.8, = 0.6 and = 0.2, = 0.2, respec-tively. This may be attributed to the characteristics of the datasets.The Yelp2018 dataset, with a higher average interaction frequencyper item, benefits more from a higher weight for popular items aspositive samples. Conversely, the Gowalla dataset, being relativelysparse, prefers a smaller . This indicates the importance of consid-ering dataset characteristics when adjusting the contributions ofpopular and unpopular items to the model.Notably, and are not highly sensitive within the range ,performing well across a broad spectrum. Figure. 5 shows thatperformance exceeds the baseline regardless of values when otherparameters are optimal. Additionally, values from [0.4, 1.0] onthe Yelp2018 dataset and [0.2, 0.8] on the Gowalla dataset surpassthe baseline, indicating less need for precise tuning. Thus, and achieve optimal performance without meticulous adjustments,focusing on weight coefficients to maintain model efficacy. 4.5.3Effect of grouping ratio . To investigate the impact ofdifferent grouping ratios on recommendation performance, we de-veloped a flexible classification method for items within each mini-batch based on their popularity. Instead of adopting a fixed global",
  "RELATED WORK5.1Popularity Bias in Recommendation": "Popularity bias is a common issue in recommender systems whereunpopular items in the training dataset are rarely recommended . Many methods have been proposed to analyze andreduce performance differences between popular and unpopularitems. These methods can be broadly categorized into three types. Re-weighting-based methods aim to increase the trainingweight or scores for unpopular items, redirecting focus away frompopular items during training or prediction . For instance,IPS adds compensation to unpopular items and adjusts theprediction of the user-item preference matrix, resulting in higherpreference scores and improving rankings for unpopular items. -AdjNorm enhances the focus on unpopular items by controllingthe normalization strength during the neighborhood aggregationprocess in GCN-based models. Decorrelation-based methods aim to effectively remove thecorrelations between item representations (or prediction scores)and popularity. For instance, MACR usescounterfactual reasoning to eliminate the direct impact of popu-larity on item outcomes. In contrast, InvCF operates on theprinciple that item representations remain invariant to changes inpopularity semantics, filtering out unstable or outdated popularitycharacteristics to learn unbiased representations. Contrastive-learning-based methods aim to achieve overalluniformity in item representations using InfoNCE , pre-serving more inherent characteristics of items to mitigate pop-ularity bias . This approach has been demonstrated as astate-of-the-art method for alleviating popularity bias. It employsdata augmentation techniques such as graph augmentation or fea-ture augmentation to generate different views, maximizing positivepair consistency and minimizing negative pair consistency to pro-mote more uniform representations . Specifically, Adap-adjusts user/item embeddings to specific values, while SimGCLintegrates InfoNCE loss to enhance representation uniformity andalleviate popularity bias.",
  "Representation learning is crucial in recommendation systems, es-pecially in modern collaborative filtering (CF) techniques. It createspersonalized embeddings that capture user preferences and item": "characteristics . The quality of these representa-tions critically determines a recommender systems effectivenessby precisely capturing the interplay between user interests anditem features . Recent studies emphasize two funda-mental principles in representation learning: alignment and uni-formity . The alignment principle ensures that embed-dings of similar or related items (or users) are closely clusteredtogether, improving the systems ability to recommend items thatalign with a users interests . This principle is crucial whenaccurately reflecting user preferences through corresponding itemcharacteristics . Conversely, the uniformity principle ensures abalanced distribution of all embeddings across the representationspace . This approach prevents the over-concentration ofembeddings in specific areas, enhancing recommendation diversityand improving generalization to unseen data .In this work, we focus on aligning the representations of pop-ular and unpopular items interacted with by the same user andre-weighting uniformity to mitigate representation separation. Ourmodel PAAC uniquely addresses popularity bias by combininggroup alignment and contrastive learning, a first in the field. Unlikeprevious works that align positive user-item pairs or contrastivepairs, PAAC directly aligns popular and unpopular items, leveragingthe rich information of popular items to enhance the representa-tions of unpopular items and reduce overfitting. Additionally, weintroduce targeted re-weighting from a popularity-centric perspec-tive to achieve a more balanced representation.",
  "CONCLUSION": "In this work, we analyzed popularity bias and proposed PAAC tomitigate popularity bias. We assumed that items interacted withby the same user share similar characteristics and used this ob-servation to align representations of both popular and unpopularitems through a popularity-aware supervised alignment approach.This provided more supervisory information for unpopular items.Note that our hypothesis of aligning and grouping items basedon user-specific preferences offers a novel alignment perspective.Additionally, we addressed the issue of representation separationin current CL-based models by introducing two hyper-parametersto control the weights of items with different popularity levelsas positive and negative samples. This approach optimized rep-resentation consistency and effectively alleviated separation. Ourmethod, PAAC, was validated on three public datasets, proving itsrationale and effectiveness.In the future, we will explore deeper alignment and contrastadjustments tailored to specific tasks to further mitigate popular-ity bias. We aim to investigate the synergies between alignmentand contrast and extend our approach to address other biases inrecommendation systems. This work was supported in part by grants from the National Key Re-search and Development Program of China (Grant No. 2021ZD0111802),the National Natural Science Foundation of China (Grant No. 72188101,U21B2026), the Fundamental Research Funds for the Central Uni-versities, and Quan Cheng Laboratory (Grant No. QCLZD202301).",
  "Yifan Wang, Peijie Sun, Min Zhang, Qinglin Jia, Jingjie Li, and Shaoping Ma. 2023.Unbiased Delayed Feedback Label Correction for Conversion Rate Prediction.KDD (2023), 24562466": "Tianxin Wei, Fuli Feng, Jiawei Chen, Chufeng Shi, Ziwei Wu, Jinfeng Yi, andXiangnan He. 2020. Model-Agnostic Counterfactual Reasoning for EliminatingPopularity Bias in Recommender System. KDD (2020), 17911800. Chenwang Wu, Xiting Wang, Defu Lian, Xing Xie, and Enhong Chen. 2023.A causality inspired framework for model interpretation. Proceedings of the29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (2023),27312741.",
  "AAPPENDIXA.1Datasets": "We conducted experiments on three public datasets, with the pro-cessed dataset statistics summarized in Table. 4. Additionally, toanalyze the imbalance in item distribution, we recorded the numberof interactions for the most and least popular items and calculatedthe average number of interactions per item. We also used theGini coefficient to reflect the disparity in item popularity distribu-tion . The results are presented in Table. 5.Amazon-Book. Amazon is a frequently utilized dataset for itemrecommendations. From this collection, we specifically selected theAmazon-Book dataset.Yelp2018. This dataset is sourced from the 2018 edition of theYelp Challenge, where local businesses such as restaurants and barsare considered items.Gowalla. This is a check-in dataset from Gowalla, that containsuser location data shared through check-ins.",
  "A.3Hyper-Parameter Settings": "We initialize parameters using the Xavier initializer and use theAdam optimizer with a learning rate of 0.001. The embeddingsize is fixed at 64. For all datasets, we set the batch size is 2048, andthe 2 regularization coefficient 3 is 0.0001. For our model, we fine-tune the popularity-aware supervised alignment coefficient 1 in {1,5, 10, 50, 100, 300, 400, 500, 100}, the popularity-aware contrastiveregularization coefficient 2 in {0.1, 1, 5, 10, 20}, and the re-weightinghyperparameters and in {0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0}. Additionally,we set the grouping ratio x=50. This means we dynamically divideditems into popular and unpopular groups within each mini-batchbased on their popularity, assigning the top 50% as popular itemsand the bottom 50% as unpopular items. This approach ensuresequal representation of both groups in our contrastive learning andallows items to be adaptively classified based on the batchs currentcomposition. Moreover, we carefully search the hyper-parametersfor all baselines to ensure fair comparisons.",
  "A.4Debias Ability": "A.4.1Conventional Test. As mentioned in .1.1, tradi-tional evaluation methods fail to accurately measure a modelsability to mitigate popularity bias, as test sets often exhibit a long-tail distribution. This can misleadingly suggest high performancefor models that favor popular items. To address this significant issue,we utilize an unbiased dataset for evaluation, ensuring a uniformitem distribution in the test set, following established precedents.Despite this, traditional performance metrics remain essential foreffectively assessing model performance. Therefore, we also exam-ine PAACs results under conventional experimental settings. Theseresults, detailed in Table. 6, demonstrate that our method competesclosely with the best baselines, affirming its efficacy under standardevaluation conditions. A.4.2Impact on Embedding Separation. To evaluate the im-pact of PAAC on embedding separation, we conduct detailed quan-titative and qualitative analyses from various perspectives.Quantitative Analysis. Following the Pareto principle ,items are categorized into Popular and Unpopular groups. Toquantify the differences in the distribution of embeddings between",
  ",": "where 1 and 2 are the embedding vectors. Table. 7 shows the re-sults for MMD and CS, demonstrating that PAAC significantly low-ers the disparity between the distributions of popular and unpopularitemsas evidenced by reduced values in both metricstherebyeffectively enhancing embedding separation. Qualitative Analysis. The effectiveness of PAAC is further vali-dated through a comprehensive visual assessment using t-SNE ,a widely used technique for dimensionality reduction and visu-alization. This analysis involved visualizing the embeddings of arandomly selected subset of 400 items from the dataset. By plottingthese embeddings, we are able to visually compare the performanceof PAAC against a baseline model, LightGCN. provides a clear illustration of this comparison. Thefigure demonstrates that PAAC achieves a significantly more uni-form distribution of embeddings compared to LightGCN. In partic-ular, both popular and unpopular items are more evenly dispersedthroughout the embedding space under PAAC, rather than clus-tering in separate regions. This uniform distribution is indicativeof reduced embedding separation, a crucial factor in mitigatingpopularity bias. The visual assessment, therefore, provides strongqualitative evidence that complements our statistical analyses, re-inforcing the overall robustness and efficacy of PAAC in promotingfairer and more balanced representation of items in the embeddingspace."
}