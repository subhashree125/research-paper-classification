{
  "ABSTRACT": "Although pre-trained language models (PLMs) have recently ad-vanced the research progress in mathematical reasoning, they arenot specially designed as a capable multi-task solver, sufferingfrom high cost for multi-task deployment (e.g., a model copy fora task) and inferior performance on complex mathematical prob-lems in practical applications. To address these issues, in this paper,we propose JiuZhang 2.0, a unified Chinese PLM specially formulti-task mathematical problem solving. Our idea is to maintain amoderate-sized model and employ the cross-task knowledge sharingto improve the model capacity in a multi-task setting. Specially,we construct a Mixture-of-Experts (MoE) architecture for model-ing mathematical text, so as to capture the common mathematicalknowledge across tasks. For optimizing the MoE architecture, wedesign multi-task continual pre-training and multi-task fine-tuningstrategies for multi-task adaptation. These training strategies caneffectively decompose the knowledge from the task data and estab-lish the cross-task sharing via expert networks. In order to furtherimprove the general capacity of solving different complex tasks,we leverage large language models (LLMs) as complementary mod-els to iteratively refine the generated solution by our PLM, viain-context learning. Extensive experiments have demonstrated theeffectiveness of our model.",
  "Equal contribution.Corresponding author": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 23, August 610, 2023, Long Beach, CA, USA 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0103-0/23/08...$15.00",
  "Chinese pre-trained language model, Mathematical problem solving": "ACM Reference Format:Wayne Xin Zhao, Kun Zhou, Beichen Zhang, Zheng Gong, ZhipengChen, Yuanhang Zhou, Ji-Rong Wen, and Jing Sha, Shijin Wang, Cong Liu,Guoping Hu. 2023. JiuZhang 2.0: A Unified Chinese Pre-trained LanguageModel for Multi-task Mathematical Problem Solving. In Proceedings of the29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining(KDD 23), August 610, 2023, Long Beach, CA, USA. ACM, New York, NY,USA, 13 pages.",
  "INTRODUCTION": "Recently, the mathematical reasoning capacity of machines hasbeen largely empowered by the progress of pre-trained languagemodels (PLMs) . By pre-training on large-scale mathe-matical corpus with specially designed tasks, PLMs can understandthe mathematical formulas and logic to a certain extent , achiev-ing better performance on a variety of math-related tasks.Despite the progress, existing PLM based approaches still havetwo major limitations in real-world math-related applications. (1)Limited task performance: due to the limit of model capacity andpre-training data, PLMs are less capable of understanding complexmathematical problems, thus suffering from performance degra-dation on difficult tasks. (2) Large maintenance cost: an online ap-plication often supports multiple math-related tasks (e.g., similarproblem retrieval and knowledge point classification), while PLMsneed to be fine-tuned task by task when dealing with differentdownstream tasks, taking a significant cost of maintaining multi-task solvers (e.g., a model copy for a task).By exploring the scaling laws, large language models (LLMs)1 can overcome the above issues to some extent with strongermathematical reasoning ability. While, they are very costly to betuned for task or domain adaptation. Although in-context learn-ing can be applied to solve different tasks in an efficient way(with no need for fine-tuning), it is still difficult to adapt them to",
  "KDD 23, August 610, 2023, Long Beach, CA, USAWayne Xin Zhao et al": "3.2.1MoE Extension for Knowledge Sharing. MoE is a widelyused technique to increase model capacity by incorporating multi-ple expert networks (the same architecture yet different parame-ters). While, we employ MoE to decouple and share mathematicalknowledge across tasks: common knowledge for related tasks canbe captured in one specific expert and less irrelevant knowledgeacross different tasks is distributed among multiple experts. MoE Layer for Mathematical Text. In our approach, we only ex-tend the deep shared encoder (capturing the essential mathematicalknowledge) with MoE, but not the shallow decoders (supportingdifferent types of tasks). As the encoder is composed of multiplebidirectional Transformer layers, we incorporate the MoE layerto substitute for the original feed-forward layer. Each MoE layerconsists of a routing network () and multiple expert networks{ ()}=1, where denotes the number of expert candidates. Toreuse the encoded knowledge from JiuZhang, we utilize the pa-rameters of its feed-forward layer to initialize the parameters ofthe expert networks, which can also improve the training stability.Since a mathematical problem is usually related to diverse knowl-edge points, we adopt a token-wise routing mechanism todecouple its associated mathematical knowledge, by assigning ex-perts individually for each token. Given an input mathematicaltext = {1, ,}, in each Transformer layer, the multi-headself-attention layer first produces the aggregated representationsof all these tokens {1, , }. Then, for each token, the routingnetwork estimates the probability distribution over the experts:",
  "variety of math-related applications . In the following, wewill review the related study in three major technical approaches": "Traditional NLP Approaches. Since mathematical problems aredescribed in natural language, it is straightforward to cast the un-derstanding of mathematical problems as a natural language pro-cessing (NLP) task. A major difficulty lies in the understanding ofthe formulas and logic that mathematical text contains. Thus, earlyNLP approaches typically extract the features for understanding thetext and formulas, e.g., semantic parser and operator tree .In recent years, a surge of methods introduce the deep neural net-work into mathematical problem understanding. They generallyleverage advanced NLP models, e.g., RNN and Transformer ,to encode the mathematical text into meaningful representations. PLM Based Approaches. Inspired by the success of PLMs in NLPtasks, researchers employ PLMs to deal with mathematical prob-lems , showing the superiority in understanding and model-ing of mathematical texts. Basically, these methods continually pre-train PLMs (e.g., BERT ) with a specific math corpus, and designproper pre-training strategies to capture the semantics of the formu-las and logics conveyed in the mathematical texts, e.g., text-formularepresentation alignment , basic-to-advanced curriculumpre-training and unified multi-task learning . However,existing PLM approaches cannot well solve complex mathematicalproblems and also have a high cost in multi-task deployment. LLM Based Approaches. In contrast to PLMs with moderate sizes,large language models (LLMs) are introduced to solvemathematical problems . Further, external modules ortools are used to assist LLMs in complex math problem solving, e.g.,program interpreter . Since it is very costly to tune LLMs,in-context learning has been widely used to solve differenttasks, e.g., chain-of-thought (CoT) method that uses multi-stepreasoning . Based on CoT, several improvements have beenproposed for mathematical reasoning, including selecting moreappropriate samples , designing better instructions ,generating multiple results for ranking and decomposingproblem into sub-problems . However, it is hard for LLMs toadapt to the domains or tasks with large differences from the pre-training setting , e.g., Chinese mathematical problem solving.Besides, our model is built on MoE architecture , which aimsto scale up the model capacity with controllable computational cost.For MoE architectures, it is important to design suitable expertnetwork , routing mechanism and training strate-gies . While, our work has presented a novel applicationof MoE for dealing with mathematical tasks, with specific improve-ments. Our work is also related to multi-task learning based onlanguage models , while our focus is to share mathematicalknowledge across. We design specific architecture and correspond-ing training strategies for mathematical problem solving, whichdistinguishes it from prior work on multi-task learning.",
  ". Therefore, choose B": ": The overview of our model JiuZhang 2.0, consisting of two major parts: MoE extension with multi-task training basedon the PLM (the primary role) and iterative refinement via LLM (the complementary role). The red bold tokens are errorsgenerated by JiuZhang, which are corrected by LLM in the later iterative refinement process.",
  "Backbone Model: JiuZhang": "We first introduce the backbone model JiuZhang for mathe-matical problem understanding. Unlike general-purpose PLMs (e.g.,BERT ), JiuZhang considers the pre-training corpus of mathe-matical text, in which each text consists of a sequence of tokens(either a text word or a math symbol) corresponding to a mathe-matical problem (including both problem statement and possiblesolution), denoted as = {1,2, ,}. Next, we introduce theoriginal architecture and pre-training tasks for JiuZhang . Architecture. Since both understanding and generation capacitiesare needed for mathematical problem solving, JiuZhang adopts anarchitecture consisting of one shared encoder and two task-specificdecoders: one decoder for understanding tasks ( -decoder) and theother decoder for generation tasks (-decoder). It employs bidi-rectional Transformers to implement the shared encoder and the -decoder, and an auto-regressive Transformer to implement the-decoder. In order to enhance the representation ability, the sharedencoder is built with more layers than the two decoders (i.e., 10layers v.s. 2 layers). Given a mathematical text = {1, ,}, theshared encoder can produce contextualized token representations{h()1, h()2, , h()} (-layer architecture) by capturing mathe-matical semantics from the input text. Then, the -decoder and-decoder will solve the understanding and generation tasks basedon the contextualized representations, respectively. Pre-training Tasks. In the former version, JiuZhang sets up threetypes of pre-training tasks and schedules them in a curriculum learn-ing approach. The basic course is constructed based on maskedtoken prediction following general-purpose PLMs, with two pre-training tasks of masked language modeling () and denoisedauto-encoder (). The advanced course is constructed based on specific considerations of mathematical text, including mathe-matical logic recovering and solution checking. For mathematicallogic recovering, we introduce the pre-training tasks of shuffled sen-tences recovering () and shuffled formulas recovering (),in order to enhance the understanding of mathematical logic; for so-lution checking, we introduce the pre-training tasks of dual-decodersolution checking ( and ), which improve the modelsability to detect and correct errors in its own generated outputs.These pre-training tasks can gradually adapt JiuZhang to mathe-matical problem solving. Due to space limit, please refer to originalpaper for more details.Although JiuZhang can better model mathematical text com-pared with general-purpose PLMs, it is not specially designed formulti-task mathematical problem solving. In order to enhance themulti-task capacity, we next introduce two important improve-ments, namely MoE extension with multi-task training (.2)and iterative refinement with LLM (.3). In the following,we introduce the two parts in detail.",
  "MoE Extension with Multi-task Training": "By leveraging a corpus of mathematical text, JiuZhang implicitlycaptures mathematical knowledge with specially designed pre-training tasks. While, such information is encoded via a wholemodel (i.e., the shared encoder), and it is difficult to transfer math-ematical knowledge across different tasks. To better decomposeand share the mathematical knowledge, we propose to enhancethe backbone model with Mixture-of-Experts (MoE) extension,and introduce multi-task continual pre-training and multi-task fine-tuning strategies based on MoE-enhanced architecture.",
  "=1() ().(2)": "Sparsely Routing with Jitter Noise. To save the computationalcost in MoE layers, we introduce the sparse activation mecha-nism to selectively utilize expert networks for each token.Specifically, according to the estimated probability distribution(), we first rank all the expert networks and then select the top-ones ( ) in Eq. (2) to derive the token representation. Here, weset = 1, i.e., only the most related expert will be routed for eachtoken. In this way, for each token, the computational cost of the ex-pert network is roughly the same as the original feed-forward layerof JiuZhang. More detailed analysis about inference latency can befound in Appendix B. However, prior studies have found thatsuch a sparse expert assignment approach would deterministicallychoose the best-ranking expert, causing the expert network easyto overfit. Therefore, we introduce randomness into the expert se-lection process by using the jitter noise in the routing network.We multiply the estimated probability distribution in Eq. (1) by ajitter noise (a randomly scaling distribution vector) as:",
  "Multi-task Pre-training for MoE Adaptation. In order to sup-port the MoE architecture, we design multi-task continual pre-training strategies for adapting to the multi-task setting": "Multi-task Continual Pre-training. The goal of multi-task pre-training is to decouple and transfer mathematical knowledge viaexpert sharing, according to task supervision. Since there is no taskdata during the pre-training stage, we consider reusing the originalpre-training tasks of JiuZhang discussed in .1, includingmasked token prediction ( and ), mathematical logicrecovering ( and ) and solution checking ( and ).Instead of using a curriculum learning way as in , we treat thesix pre-training losses as equal optimization goals, and set a multi-task pre-training objective:",
  "= + + + + + .(4)": "Note that our model has been initialized with the parameters of theformer JiuZhang, so that it also implicitly benefits from the curricu-lum learning strategy proposed in the previous paper . While,based on the MoE-based architecture, we employ these pre-trainingtasks to decouple and share mathematical knowledge across tasks. Auxiliary Losses for Improved Optimization. For MoE methods,there are two major training problems that affect the performance,i.e., the unbalanced load among experts and the training in-stability . To alleviate these problems, we adopt two auxiliarylosses as the regularizers in our approach. Specially, theunbalanced load problem refers that certain experts are extremelyfrequently routed, which may cause the overfitting problem onthese experts and the underfitting problem on other experts. There-fore, we aim to improve the unbalanced routing among all experts.Formally, we encourage the accumulated estimated probabilitiesfor each expert to be uniform, denoted as:",
  "=1 ,(5)": "where is the number of tokens dispatched to the-th expert, andis the accumulated routing score estimated by the routing networkfor the -th expert, and is the coefficient to control the influence.According to , this loss encourages uniform routing since itwould be minimized under a uniform distribution. Further, thetraining instability problem is often caused by the large volatilityof the probability scores in the routing network. In order to controlthe volatility, we adopt the -loss that encourages the routinglogits of all tokens (size ) to remain small as:",
  "inter-task relationships. Thus, we design a multi-task fine-tuningstrategy, which boosts the capacity of our MoE architecture byleveraging the data of all (available) downstream tasks": "Unifying the Fine-tuning Tasks. For multi-task fine-tuning, wecombine the available training data from multiple downstreamtasks for jointly optimizing our model. Since these tasks that weconsider are math related, they tend to rely on common mathe-matical knowledge for task solving, which can be captured via theMoE-based architecture. However, the formats of the input andoutput data for downstream tasks are generally different, makingit hard to be jointly fine-tuned. Recall that our backbone model hasincluded two specific decoders that can handle both understandingand generation tasks for mathematical text. Thus, we unify themath-related tasks into two general formats, either understandingor generation. Specially, for all text classification tasks, we mergethe annotation labels and consider an extended multi-label setting,where the label dictionary covers the labels from all classificationtasks. In this way, we can equip our -decoder with a multi-labelclassifier head to simultaneously accomplish all these classificationtasks. Further, for all text generation tasks, we adopt a standardsequence-to-sequence format and utilize the -decoder to solvethem. To better distinguish the different tasks for our model, giventhe training data from tasks, we also devise task prompt em-beddings, denoted as {1, , }. For each instance, we insert itstask prompt embedding after the [CLS] token embedding. Routing with Task Prompt. During multi-task fine-tuning, asthe task type may be useful to determine the selection of differentexperts with specific mathematical knowledge, we further revisethe routing mechanism by incorporating task-level instruction.Specially, in each MoE layer, we add the input token representation with the representation of the task prompt , to compose theinput of the routing layer for estimating the probability distributionover the experts as:",
  "Iterative Refinement via LLM": "Although MoE extension is employed to enhance the backbonemodel, we keep a moderate-sized model (i.e., 276 for = 4) withan affordable cost for downstream applications. Due to the limit inmodel size and pre-training data, it still has difficulty in generatingsolution text for some complex mathematical problems. Our solu-tion is to leverage large language model (LLM) with strongergeneral modeling capacities for refining the generation results ofour PLM. To achieve this, we first design a retrieval strategy to se-lect the most relevant exemplars for constructing the prompts, andthen devise an iterative prompting method that utilizes in-contextlearning to gradually correct the generated results. 3.3.1Constructing Prompts Using Retrieved Samples. Since existingLLMs are mainly English-focused, they cannot sufficiently capturethe necessary mathematical knowledge to effectively accomplishmath-related tasks in Chinese (see experiments in .2). Thus,instead of directly solving the tasks, LLM plays a complementaryrole in our approach for refining the generated results of our PLM. Specifically, given a mathematical problem , we first utilize thePLM (.2) to generate the solution text , and then employthe LLM via in-context learning to refine into with improvedquality. To provide effective guidance on the LLM, we construct theprompts with retrieved relevant exemplars and specially designednatural language instructions. Retrieving Exemplars. As empirical studies have revealedthat the exemplars in the prompts of LLMs are important to the taskperformance, we retrieve relevant instances from the training dataas the exemplars. Since exemplar finding is essentially an unsuper-vised text retrieval task, we further employ SimCSE to enhancethe representation capacity of our backbone model for semanticmatching. Following SimCSE, we incorporate the dropout mecha-nism to augment positive representations and utilize the contrastivelearning objective for training. In the retrieval stage, given the targetproblem and the training data set as the retrieval candidate pool,we first encode all the mathematical problems into dense vectorsby our backbone model, and then select the top-ranking problemsas relevant exemplars, denoted as = {,}=1, where isthe associated solution text for problem . Note that we do notuse the solution text for the target problem, while only utilizingthe solution texts of the problems from training data. Building Prompts. In order to guide the LLM to refer to the re-trieved exemplars for revising the generated result from our PLM,we utilize the in-context learning method with specially designedprompts. Specifically, the input of the LLM consists of four parts,i.e., the given question , the generated result , the retrieved exem-plars = {,}=1, and a natural language instruction . Weconcatenate the above four parts into a long sentence, to composethe prompt template as:",
  "where the instruction can be flexibly set according to differenttasks. We will discuss how to set it in the following part": "3.3.2Iterative Prompting for Result Refinement. Generally, the gen-erated results from the PLM may contain a variety of mistakes (e.g.,inconsistent logic and language typos), and it is hard for the LLM tocompletely check and correct all these mistakes at once. Therefore,we devise a three-stage iterative refining strategy that graduallyimproves the generated results following a coarse-to-fine manner.Concretely, based on the prompt template in Eq. (8), we design threespecific instructions for the three stages, which guide the LLM torefine the generation results from the three perspectives of overalllogic, deduction process and language expressions, respectively. Wepresent the above instructions in the Appendix ().Further, to better cooperate with the above instructions, we alsorevise the way of retrieving exemplars in the three stages:",
  "UnseenGenerationJCAG8,0001,0001,000JBAG8,0001,0001,000": "To accomplish the goal for each individual stage, we find that itneeds multiple iterations for LLM to produce ideal outputs. Thus,we perform -step ( = 3) iterations for each stage. At each step,the refined output ( ) will be used as the input of the next step(+1) to compose the prompt and the retrieved exemplars can alsobe updated according to new query (+1). In this way, we caniteratively refine the generated results until the expected goal isfulfilled at each stage, and finally generate high-quality results.",
  "EXPERIMENTS4.1Experimental Settings": "We utilize the same pre-training corpus of JiuZhang , consistingof 1,276,952 high-school math problems collected from Zhixuewang,and each problem is associated with the problem type, problemstatement and solution text. We preprocess these collected texts inthe same way as JiuZhang. Evaluation Tasks. We consider two different settings for eval-uation, namely seen tasks and unseen tasks, referring to the taskdata that are used and not used, respectively, during multi-task fine-tuning. We split each task dataset into training/development/testsets. The statistics of these tasks are shown in . Seen tasks consist of six tasks based on high-school math prob-lems, including (1) two question answering tasks, i.e., Multiple-Choice Question Answering (MCQ) and Blank-Filling QuestionAnswering (BFQ); (2) two analysis generation tasks, i.e., Multiple-Choice Analysis Generation (CAG) and Blank-Filling Analysis Gen-eration (BAG); and (3) two classification tasks, i.e., Knowledge PointClassification (KPC) and Question Relation Classification (QRC).For these tasks, we perform multi-task fine-tuning with all trainingsets, select the model checkpoint with the best average performanceon development sets, and then evaluate the results on test sets. Unseen tasks consist of two analysis generation tasks based onjunior high school math problems, i.e., Junior-high-school Multiple-Choice Analysis Generation (JCAG) and Junior-high-school Blank-Filling Analysis Generation (JBAG), which are not used in multi-task fine-tuning for our model. For the two tasks, we performtask-specific fine-tuning, i.e., the multi-task fine-tuned model isseparately optimized, tuned and evaluated for each task.We use the evaluation metrics following JiuZhang . For clas-sification tasks (KPC and QRC), we adopt Accuracy and F1-macro as the evaluation metrics. For question answering tasks (MCQ andBFQ), we adopt Accuracy for evaluation. For generation tasks (CAG,BAG, JCAG and JBAG), we use BLEU-4 , ROUGE-2 and ROUGE-L to evaluate the quality of the generated analysis, and alsoadopt Accuracy to evaluate the generated answers. Baseline Methods. We select the following four types of baselines: Non-pretraining methods consist of classic neural network meth-ods for text classification or generation, i.e., TextCNN , TextR-CNN , Seq2Seq and Transformer . Pre-trained language models have been pre-trained on large-scale general corpus. We select BERT-Base , BART-Base ,RoBERTa-wwm , CPT and Mengzi . For generationtasks, we fine-tune RoBERTa-wwm in a UniLM way , and utilizebi-directional attention for input and unidirectional attention foroutput to implement the Seq2Seq based training and inference. Continual pre-training methods further pre-train PLMs ondomain-specific corpus (our collected math corpus), and also adoptspecially designed pre-training tasks. We select MathBERT ,DAPT-BERT , DAPT-CPT, COMUS , JiuZhang . Sinceour approach is also related to multi-task learning , we alsoadd a variant that extends JiuZhang in a multi-task trainingstrategy, MTDNN for fine-tuning. Chain-of-thought (CoT) methods add explanations to the exem-plars in the input prompt of LLMs, to better guide them to generatecorrect answer . We employ CoT on GPT-3 and CodeX ,i.e., GPT3-CoT and CodeX-CoT.Note that CoT methods rely on intermediate reasoning stepsof the sampled exemplars in input to guide the solving of mathproblems, which are not available in the two classification tasks ofKPC and QRC. While, in MCQ, BFQ, CAG and BAG tasks, we canutilize the analysis text to derive the intermediate reasoning steps,hence we only report the results of CoT methods on the four tasks. Implementation Details. For GPT3-CoT and CodeX-CoT, wefollow the standard chain-of-thought way to construct the inputprompts , and the numbers of sampled exemplars are set to5 and 8, respectively, since GPT-3 has a smaller maximum inputlength than CodeX. During training, we use AdamW as theoptimizer with the learning rate of 3e-5, and warm up the learningrate for the first 5% steps then decay the weight with a ratio of 0.01.The coefficients of the auxiliary loss (Eq. (5)) and the -loss (Eq. (6))are 1e-3 and 1e-4, respectively. For the MoE structure, we set thenumber of experts = 4 and the number of activated experts = 1.For continual multi-task pre-training, we pre-train our model witha batch size of 256 for 700000 steps. For multi-task fine-tuning, wefine-tune our model with a batch size of 32 for 80 epochs and adoptthe routing mechanism with task prompt. For iterative refinement,we use CodeX as the LLM and retrieve top-8 similar problemsfrom the training set as exemplars for each input problem. Moredetails are reported in Appendix A.",
  "JiuZhang 2.0 (w/o IRL)73.561.289.979.8": "First, continual pre-training methods (i.e., COMUS, DAPT-CPT,JiuZhang, JiuZhang-MTDNN) achieve better performance thangeneral-purpose PLMs such as BART and CPT. The reason is thatthese methods have been continually pre-trained on the math cor-pus, which can learn useful mathematical knowledge from suchtexts. Among these continual pre-training methods, the two meth-ods based on JiuZhang (i.e., JiuZhang and JiuZhang-MTDNN) mostlyoutperform all other methods. It is mainly because that JiuZhangincorporates three types of pre-training tasks, which is further pre-trained in a curriculum learning way. While, JiuZhang-MTDNNrevises the fine-tuning process of JiuZhang by adopting multi-tasklearning, which can improve the performance on MCQ and BFQ,but has worse performance on KPC and QRC tasks. A possiblereason is that there exists negative interference among these tasks during multi-task learning. Besides, COMUS also performs well onthe KPC task. Since the KPC task requires a deep understanding ofthe formulas in mathematical problems for predicting the knowl-edge points, COMUS specially designs graph neural networks andmemory networks for modeling the formulas.Second, the chain-of-thought methods based on powerful LLMs(i.e., GPT3-CoT and CodeX-CoT) overall perform worse than con-tinual pre-training methods on generation metrics (i.e., BLEU-4,ROUGE-2 and ROUGE-L). The reason might be that these LLMsmainly focus on English tasks, and cannot well adapt to Chinesemath-related tasks. In contrast, these continual pre-training meth-ods have been trained over the math corpus, thus having an adapta-tion capacity in downstream tasks. While, for the Accuracy metric,chain-of-thought methods perform relatively better than other base-lines. It shows that LLMs are more skilled in accurately predictingthe answer, since they have a stronger mathematical reasoningcapacity due to the huge model size and large-scale pre-trainingcorpus (also including large amounts of mathematical texts).Finally, our proposed JiuZhang 2.0 outperforms all the baselinesin most cases. By integrating the MoE architecture with multi-tasktraining, our model can better capture the mathematical knowledgeacross various math-related tasks. Even without iterative refine-ment via the LLM, our model (i.e., JiuZhang 2.0 w/o IRL) can stilloutperform all the baselines. After incorporating the iterative refine-ment via the LLM, the performance of our approach can be furtherimproved, especially on the Accuracy metric. It demonstrates thatour approach can further benefit from the mathematical reasoningcapacity of the LLM. In this way, JiuZhang 2.0 can combine boththe advantages of the PLM and LLM: PLM can be tuned for domainadaptation to Chinese math-related tasks, while LLM has strongerreasoning and generation capacities. 4.2.2Evaluation on Unseen Tasks. Since multi-task fine-tuningcannot cover all math-related tasks, we continue to examine theperformance of our model on new tasks that are not seen before.In order to enlarge the domain gap between existing and newtasks, we select the two tasks of multiple-choice analysis generation",
  "BAG Acc": ": Ablation study of our approach on CAG and BAG tasks. indicates that the corresponding technique is removedfrom our model, while the rest are kept. We abbreviate the terms Multi-task Continual Pre-Training, Multi-Task Fine-Tuning,Mixture-of-Experts, and Task embedding in Routing network as MTPT, MTFT, MoE and TR respectively. (JCAG) and blank-filling analysis generation (JBAG) from juniorhigh schools, which has a different distribution with those fromhigh schools (in multi-task fine-tuning). For these two unseen tasks,we fine-tune our model (task by task) on them after multi-taskfine-tuning, as the same way in the baselines.From , we can see that the overall experimental findingsare similar to those discussed in .2.1, where we have theoverall performance order: PLMs < continual pre-training methods< JiuZhang < JiuZhang 2.0 w/o IRL < JiuZhang 2.0. In particular,the variant of JiuZhang 2.0 w/o IRL also performs better than allthese baselines, since it employs MoE extension with multi-tasktraining, thus having an improved ability for capturing commonmathematical knowledge across tasks. Further, by adopting theiterative refinement via LLMs (IRL), our JiuZhang 2.0 achieves asignificant improvement on the Accuracy metric (i.e., 55.60 63.20on JCAG, 38.10 53.80 on JBAG). The results show that theproposed IRL strategy can effectively leverage the strong generationand reasoning capacities of LLMs via in-context learning, whichcan gradually improve the generation quality of our PLM.",
  "Detailed Analysis": "4.3.1Ablation Study. In JiuZhang 2.0, we have proposed a series ofimprovement techniques for enhancing the capacity for mathemati-cal problem solving. Next, we study how each technique contributesto the model performance. We keep the complete model with all im-provement techniques as a reference, then remove one specific tech-nique each time, and compare the performance with and withoutit. We consider the following variants: (1) MoE removes the MoEextension, (2) MTPT removes multi-task continual pre-training,(3) MTFT removes multi-task fine-tuning, and (4) TR removesthe task embedding from the routing network. Note that MoE",
  ": Varying the number of experts () in our approach": "can be considered as an implementation of the multi-task learningmethod with JiuZhang as the backbone model. We reportBLEU-4 and Accuracy of these variants on the CAG and BAG tasks.From , we observe that removing any of these improve-ments would lead to performance degradation, which indicates theeffectiveness of these proposed techniques in mathematical prob-lem solving. In particular, the removal of multi-task pre-training orfine-tuning leads to a larger performance drop, which shows thetwo training strategies are more important to improve the modelperformance. These two tasks are well suited to the MoE architec-ture, and they can help capture the mathematical knowledge viathe expert networks. 4.3.2Hyper-parameters Analysis. In our MoE architecture, thereare two major hyper-parameters to tune, i.e., the number of experts and the number of activated experts in the MoE layers. Next,we investigate the effect of each hyper-parameter on our approach.We conduct the analysis experiments on CAG and BAG tasks andreport the results on BLEU-4 and Accuracy metrics for the twohyper-parameters in and , respectively.",
  "Ratio53.5 %46.5%": "First, the increase in the number of experts does not necessarilyimprove the performance of our approach (), especially inthe Accuracy metric. A possible reason is that the MoE architectureintroduces additional parameters, which is more likely to overfiton the training set. Besides, using more experts also leads to largercomputational costs. In our experiments, to balance the effective-ness and efficiency, we set = 4, i.e., using four expert networks,which generally gives a good performance. Second, more activatedexperts are not useful to improve the model performance, evenleading to performance degradation (). A possible reason isthat activating more experts would cause interference among them,resulting in the conflict utilization of experts. In contrast, by setting = 1, we can not only achieve a relatively better performance, butalso save the computation cost of activated expert networks. 4.3.3Analysis on the MoE Architecture. A major contribution ofour model lies in the architecture extension with MoE. By settingmultiple expert networks, we can effectively share the mathemat-ical knowledge learned from the math corpus across tasks, so asto improve multi-task mathematical problem solving. These ex-perts are expected to capture and decompose specific mathematicalknowledge for different math tasks. Next, we present an analysisexperiment about the encoded knowledge at each expert network.As shown in , we select three mathematical texts from twotasks, and show the routed expert for each token (toke-level routing)in different background colors. It can be observed that our routingnetwork can effectively decompose the mathematical knowledgeand route them to the corresponding experts. For example, thetrigonometric functions (e.g., and ) are routed to expert #3,while the (background or formal) words and numbers are mainlyassigned to expert #1 and expert #2, respectively.",
  "KPCA seagoing ship starts from A, sails in a straight line at a speedof 40 nautical miles per hour in the direction of 40": "the function of automatic math problem solving on Zhixuewangfor conducting online / test. Given a math problem (e.g., blank-infilling problem), this function aims to automatically generate theanswer with a detailed analysis of the solving process. Here, wecompare our JiuZhang 2.0 with the original JiuZhang , and bothmodels are fine-tuned by the training data provided by this app.For comparison, we sample a small population of requests of thisfunction, and a user will be asked to select her/his preferred answerand analysis provided by the two models in each request. reports the winning ratio of the two methods. As we cansee, our proposed JiuZhang 2.0 performs better than the baselineJiuZhang. The major reason is that our model adopts the multi-tasktraining with MoE layers to better capture the shared knowledgeacross multiple math-related tasks, and also leverages LLMs toiteratively refine the generated results. In this way, our model cangenerate more accurate answers and high-quality analysis.",
  "CONCLUSION": "In this paper, we proposed JiuZhang 2.0, a unified Chinese PLM formulti-task mathematical problem solving. Different from previousPLM approaches for math domain, we focus on improving the multi-task capacity of PLMs, especially on complex tasks. For this purpose,we designed a MoE-based encoder for modeling the mathematicaltext, aiming to share the mathematical knowledge across differenttasks. To support the MoE architecture, we specially designed multi-task continual pre-training and multi-task fine-tuning strategiesfor learning the shared knowledge via expert networks. Further, weleveraged the powerful LLMs as a complementary role to iterativelyrefine the generation results by our PLM, with the elaboratelydesigned prompts. Experimental results (both offline evaluation andonline / test) have demonstrated that our approach is superiorto competitive baselines on a variety of math-related tasks.",
  "ACKNOWLEDGEMENT": "This work was partially supported by National Natural ScienceFoundation of China under Grant No. 62222215, Beijing Natural Sci-ence Foundation under Grant No. 4222027, and Beijing OutstandingYoung Scientist Program under Grant No. BJJWZYJH012019100020098.And this work is also partially supported by the Outstanding In-novative Talents Cultivation Funded Programs 2021 of RenminUniversity of China. Xin Zhao is the corresponding author.",
  "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural MachineTranslation by Jointly Learning to Align and Translate. In ICLR 2015": "Arnab Banerjee, Srijoy Paul, Tisu Priya, Anamika Rohit, and Nibaran Das. 2023.A Deep Learning-Powered Voice-Enabled Math Tutor for Kids. In Recent Trendsin Image Processing and Pattern Recognition: 5th International Conference, RTIP2R2022, Kingsville, TX, USA, December 1-2, 2022, Revised Selected Papers. Springer,406417. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shot learners. NeurIPS (2020). Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de OliveiraPinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman,et al. 2021. Evaluating large language models trained on code. arXiv preprintarXiv:2107.03374 (2021). Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. 2022. Programof thoughts prompting: Disentangling computation from reasoning for numericalreasoning tasks. arXiv preprint arXiv:2211.12588 (2022).",
  "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding. InNAACL": "Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, JianfengGao, M. Zhou, and Hsiao-Wuen Hon. 2019. Unified Language Model Pre-trainingfor Natural Language Understanding and Generation. ArXiv abs/1905.03197(2019). Iddo Drori, Sunny Tran, Roman Wang, Newman Cheng, Kevin Liu, LeonardTang, Elizabeth Ke, Nikhil Singh, Taylor L Patti, Jayson Lynch, et al. 2021. Aneural network solves and generates mathematics problems by program syn-thesis: Calculus, differential equations, linear algebra, and more. arXiv preprintarXiv:2112.15594 (2021).",
  "Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple ContrastiveLearning of Sentence Embeddings. ArXiv abs/2104.08821 (2021)": "Zheng Gong, Kun Zhou, Xin Zhao, Jing Sha, Shijin Wang, and Ji-Rong Wen. 2022.Continual Pre-training of Language Models for Math Problem Understandingwith Syntax-Aware Memory Network. In ACL. 59235933. Shashank Gupta, Subhabrata Mukherjee, Krishan Subudhi, Eduardo Gonzalez,Damien Jose, Ahmed Hassan Awadallah, and Jianfeng Gao. 2022. Sparsely Acti-vated Mixture-of-Experts are Robust Multi-Task Learners. ArXiv abs/2204.07689(2022).",
  "Suchin Gururangan, Ana Marasovi, Swabha Swayamdipta, Kyle Lo, Iz Beltagy,Doug Downey, and Noah A. Smith. 2020. Dont Stop Pretraining: Adapt LanguageModels to Domains and Tasks. In ACL": "Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, EricTang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problemsolving with the math dataset. arXiv preprint arXiv:2103.03874 (2021). Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, EricTang, Dawn Xiaodong Song, and Jacob Steinhardt. 2021. Measuring MathematicalProblem Solving With the MATH Dataset. ArXiv abs/2103.03874 (2021).",
  "Siwei Lai, Liheng Xu, Kang Liu, and Jun Zhao. 2015. Recurrent convolutionalneural networks for text classification. In AAAI": "Yihuai Lan, Lei Wang, Qiyuan Zhang, Yunshi Lan, Bing Tian Dai, Yan Wang,Dongxiang Zhang, and Ee-Peng Lim. 2021. MWPToolkit: An Open-Source Frame-work for Deep Learning-Based Math Word Problem Solvers. arXiv preprintarXiv:2109.00799 (2021). Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, AbdelrahmanMohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART:Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,Translation, and Comprehension. In ACL. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, HenrykMichalewski, Vinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, ImanolSchlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, andVedant Misra. 2022. Solving Quantitative Reasoning Problems with LanguageModels. ArXiv abs/2206.14858 (2022).",
  "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever,et al. 2019. Language models are unsupervised multitask learners. OpenAI blog1, 8 (2019), 9": "Yunfan Shao, Zhichao Geng, Yitao Liu, Junqi Dai, Fei Yang, Li Zhe, Hujun Bao, andXipeng Qiu. 2021. CPT: A Pre-Trained Unbalanced Transformer for Both ChineseLanguage Understanding and Generation. arXiv preprint arXiv:2109.05729 (2021). Noam M. Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le,Geoffrey E. Hinton, and Jeff Dean. 2017. Outrageously Large Neural Networks:The Sparsely-Gated Mixture-of-Experts Layer. ArXiv abs/1701.06538 (2017).",
  "Shuming Shi, Yuehui Wang, Chin-Yew Lin, Xiaojiang Liu, and Yong Rui. 2015.Automatically solving number word problems by semantic parsing and reasoning.In EMNLP. 11321142": "Sowmya S. Sundaram, Sairam Gurajada, Marco Fisichella, Deepak P, andSavitha Sam Abraham. 2022. Why are NLP Models Fumbling at Elementary Math?A Survey of Deep Learning based Word Problem Solvers. ArXiv abs/2205.15683(2022). Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin. 2017. Attention is allyou need. In Advances in neural information processing systems. 59986008.",
  "Zhuosheng Zhang, Aston Zhang, Mu Li, and Alexander J. Smola. 2022. AutomaticChain of Thought Prompting in Large Language Models. ArXiv abs/2210.03493(2022)": "Zhuosheng Zhang, Hanqing Zhang, Keming Chen, Yuhang Guo, Jingyun Hua,Yulong Wang, and Ming Zhou. 2021. Mengzi: Towards Lightweight yet IngeniousPre-trained Models for Chinese. arXiv preprint arXiv:2110.06696 (2021). Wayne Xin Zhao, Kun Zhou, Zheng Gong, Beichen Zhang, Yuanhang Zhou, JingSha, Zhigang Chen, Shijin Wang, Cong Liu, and Ji-Rong Wen. 2022. JiuZhang: AChinese Pre-trained Language Model for Mathematical Problem Understanding.In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery andData Mining. 45714581.",
  "Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang,": "Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang,Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A Survey of LargeLanguage Models. arXiv preprint arXiv:2303.18223 (2023). Wei Zhong and Jimmy Lin. 2021. Pya0: A python toolkit for accessible math-aware search. In Proceedings of the 44th International ACM SIGIR Conference onResearch and Development in Information Retrieval. 25412545. Denny Zhou, Nathanael Schrli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang,Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. 2022. Least-to-mostprompting enables complex reasoning in large language models. arXiv preprintarXiv:2205.10625 (2022). Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Ruyi Gan, Jiaxing Zhang,and Yujiu Yang. 2022. Solving Math Word Problem via Cooperative Reasoninginduced Language Models. ArXiv abs/2210.16257 (2022).",
  "AIMPLEMENTATION DETAILS": "We report the detailed parameter settings of our approach through-out the experiments in . In addition to the above settings,we conduct all the experiments on 8 RTX 3090 24G GPUs, wherethe multi-task continual pre-training and multi-task fine-tuningtook about 72 and 12 hours, respectively. During multi-task fine-tuning, we construct the model inputs of all downstream tasks asfollows, and the task embedding will be inserted after the [CLS]token embedding.KPC, MCQ, BFQ, CAG, BAG, JCAG and JBAG: [CLS] [SEP].QRC: [CLS] 1 [SEP] 2 [SEP].For the iterative refinement via the LLM, we also design threetypes of instructions and adopt three ways to construct queries forretrieval, for the three iterative stages, respectively. We show thedetails in .Besides, we also present the Algorithm 1 and Algorithm 2, to bet-ter show the multi-task training and iterative refinement processesof our approach, respectively.",
  "BINFERENCE LATENCY ANALYSIS": "In our approach, although we scale up the number of parametersin the PLM by incorporating the MoE layers, the sparsely routingmechanism can ensure that only the top-1 most related expert willbe activated, leading to relatively less increased computational cost.To investigate it, we conduct the analysis experiments to comparethe inference latency per batch of our model with two baselines",
  "Because 10 = 13 , so 11 + 12 + 13 = 0 , so 12 = 0 , and 1 = 22 , So = 2 . So choose A": "using different model structures, i.e., BART and CPT, in the BAGtask. During inference, we adopt greedy search to decode and setthe batch size to 16. As shown in , compared to CPT, theinference latency of our model is slightly increased. It indicatesthe effectiveness of the sparse routing mechanism to guarantee theefficiency of our approach. Besides, we can see that BART requiresdouble the inference time of CPT and our approach. The reason isthat CPT and JiuZhang 2.0 adopt an unbalanced model architecturewith a shallower decoder than BART (2 layers VS. 6 layers), whichcan save the computation cost on the cross-attention layers of thedecoder.",
  "To give a qualitative analysis of our proposed approach, we performa case study that shows the generated analysis from our approach.We select two examples from the CAG and BAG tasks, respectively,": "and also show the generated analysis by two best performed meth-ods, i.e., JiuZhang and CodeX-CoT.As shown in , although CodeX-CoT and JiuZhang havegenerated a detailed multi-step reasoning process consisting aboutthe two problems, they both make mistakes in the intermediatesteps. For the first example, we can see that CodeX-CoT obtainsa wrong intermediate conclusion 13 = 0 by mistakenly simpli-fying the summation of two arithmetic progressions, which maybe caused by the unfamiliarity of the knowledge about arithmeticprogressions. JiuZhang makes a small mistake in calculation, i.e.,22 + 11 = 0 = 2, leading to the wrong answer. It also re-flects the lack of mathematical computation common sense aboutJiuZhang. As a comparison, we can see that our approach can gener-ate more proper analysis and successfully produce the true answers.It indicates the effectiveness of our approach in solving complexmathematical problems."
}