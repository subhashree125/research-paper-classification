{
  "Abstract": "Time series forecasting always faces the challenge of concept drift,where data distributions evolve over time, leading to a declinein forecast model performance. Existing solutions are based ononline learning, which continually organize recent time series ob-servations as new training samples and update model parametersaccording to the forecasting feedback on recent data. However,they overlook a critical issue: obtaining ground-truth future valuesof each sample should be delayed until after the forecast horizon.This delay creates a temporal gap between the training samplesand the test sample. Our empirical analysis reveals that the gapcan introduce concept drift, causing forecast models to adapt tooutdated concepts. In this paper, we present Proceed, a novel proac-tive model adaptation framework for online time series forecasting.Proceed first estimates the concept drift between the recently usedtraining samples and the current test sample. It then employs anadaptation generator to efficiently translate the estimated drift intoparameter adjustments, proactively adapting the model to the testsample. To enhance the generalization capability of the framework,Proceed is trained on synthetic diverse concept drifts. Extensive ex-periments on five real-world datasets across various forecast modelsdemonstrate that Proceed brings more performance improvementsthan the state-of-the-art online learning methods, significantly fa-cilitating forecast models resilience against concept drifts. Code isavailable at",
  "Time series forecasting, Online learning, Concept drift": "ACM Reference Format:Lifan Zhao and Yanyan Shen. 2025. Proactive Model Adaptation AgainstConcept Drift for Online Time Series Forecasting. In Proceedings of Pro-ceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery andData Mining (KDD 25). ACM, New York, NY, USA, 13 pages. Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from 25, August 37, 2025, Toronto, Canada. 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-1-4503-XXXX-X/18/06",
  "Introduction": "Time series forecasting has been a prevalent task in numerousfields such as energy , retail , climate and finance . Re-cent years have witnessed a surge of deep learning-based forecastmodels that take past time series observations topredict values in the next steps, where is called forecast horizon.Due to the dynamic nature of the environment, latent concepts thatinfluence observation values (e.g., social interest , stock marketsentiment ) often change over time. This ubiquitous phenom-enon is known as concept drift . In the presence of conceptdrift, future test data may not follow a similar data distributionas the historical training data, causing degradation in forecastingperformance .Online learning is commonly used to mitigate the effects of con-cept drift. The key consideration is to continually transform thenewly observed time series into a set of training samples and adjustmodel parameters by minimizing the forecast errors on the newtraining samples. In addition to the standard fine-tuning technique,recent works proposed more advanced model adaptationtechniques, which focus on how to effectively adapt to recent databy utilizing forecasting feedback (e.g., errors or gradients) on thenew training samples. Among them, FSNet monitors the gra-dients in previous fine-tuning, transforms them into parameteradjustments, and tailors a new forecast model to the current train-ing samples. OneNet is an online ensembling network thatgenerates ensemble weights to combine two forecast models anddynamically adjusts the ensemble weights and the models param-eter weights according to the forecast errors. However, existingonline learning methods overlook the fact that the ground truthof each prediction is not available immediately but is delayed afterthe forecast horizon.As illustrated in , an online learning task inherently hasa -step feedback delay in time series forecasting, resulting in atemporal gap (at least steps) between available training samplesand the test sample. Formally, at each online time , the currenthorizon window Y, which have not been observed yet, has overlapwith Y+1, . . . , Y1. These samples cannot provide completeforecasting feedback and supervisions for the performant, prevail-ing time series forecasting models which predict -step valuesdirectly . Hence, the available training samples for on-line learning at time is D = {(X, Y) | }, which hasa temporal distance to the test sample (X, Y). The temporal gapissue suggests that addressing concept drift by fitting the forecastmodel to the recent training samples may be insufficient, sinceconcept drift may also occur over the -step temporal gap betweenthe recent training samples and the current test sample.",
  "feedback delay= horizon = 3": ": (a) Example of feedback delay when = 3. At on-line time = 5, we use observation values X5 = {v1, , v5}to forecast future values Y5 = {v6, v7, v8}, while the groundtruth is known until = 8 and the feedback of forecastingperformance arrives with 3-step delay. (b) A temporal gapalways exists between new training samples D and the testsample X, where concept drift may occur. To investigate the impact of this temporal gap on forecastingperformance, we examine two online learning strategies using real-world time series datasets (detailed in ). The first strategyfine-tunes model parameters using the latest available training sam-ple at time , which is (X, Y ). The second strategy omits the feedback delay and adapts the model using (X1, Y1),which is infeasible in practice due to future information leakage.The empirical results show that the average forecast error of the firststrategy is approximately double that of the second strategy, indi-cating that even the latest observed time series pattern differs fromthe test sample notably. Moreover, the performance gap betweenthe two strategies becomes more significant with a longer horizon,suggesting that the concept drift becomes more pronounced overlonger time intervals. These findings highlight the presence of asubstantial concept drift between the practical training samplesand the test sample, limiting the effectiveness of current onlinelearning techniques that adapt model parameters to the potentiallyoutdated concepts and leave the concept drift unresolved.In this paper, we aim to answer the question: how can we effec-tively adapt the forecast model to each test sample and boost onlinetime series forecasting performance against ongoing concept drifts?A straightforward idea is to estimate a sketch of the latent conceptof the test data and customize model parameters for the estimatedconcept. However, it is challenging to train a model adapter (e.g., amapping function) that directly generates model parameters basedon the estimated concept of the test sample. First, test data may beara new concept that is out of the distribution of the historical timeseries. There is no experience in generating optimal parametersfor such an out-of-distribution concept. Second, a parameterizedmodel adapter that maps concepts to model parameters (or theirupdates) essentially lies in a parameter space of R, where isthe concept embedding dimension and is the number of modelparameters. Since advanced time series forecasting models ofteninvolve a high quantity of parameters (e.g., 1 million ), the",
  "model adapter is hard to optimize and easily suffer from overfittingdue to the huge parameter size and limited test data.To address these issues, we propose Proceed, a PROaCtivE": "modEl aDaptation framework that responds to the concept driftbefore forecasting the test sample. In scenarios where optimalmodel parameters changes with time, we posit that such parameterchanges are affected by the drift across a latent concept space, i.e.,there may exist learnable relationships between the direction anddegree of concept drift and those of parameter changes. In lightof this, we propose to infer parameter changes based on conceptdrift, rather than directly generating a whole model. Specifically,Proceed begins with a forecast model that have learned new train-ing samples. Given a test sample, Proceed exploits latent featuresthat encapsulate the concept drift between the training samplesand the test sample. In response to the undergoing concept drift,Proceed generates parameter adjustments via bottleneck layers,which yield a compact set of adaptation coefficients that rescale allmodel parameters. In this way, our solution eschews direct mappingfrom concept drift to per-parameter adjustments, instead optingfor a more nuanced and efficient strategy. We expect the adaptedmodel with rescaled parameters to approach the optimum for thetest sample. To enhance the generalization ability of Proceed, weshuffle historical data to synthesize diverse concept drifts, on whichwe train Proceed to learn the relationships between concept driftsand desirable parameter adjustments. When confronted with a re-occurring concept drift that has been learned among the syntheticconcept drifts, Proceed can tailor the model to the test sample.The contributions of our work can be summarized as follows. We highlight that time series forecasting has an inherent feedbackdelay issue, and we provide an empirical analysis to demonstratethe presence of concept drift between the newly acquired trainingsample and the test sample. Empirically, such concept drift ismore significant with a longer forecast horizon, while it remainsunresolved in existing online learning methods. We propose Proceed, a proactive model adaptation frameworkfor online time series forecasting under concept drift. Proceedfirst estimates the latent representation of concept drift and thencustomizes parameter adjustments that adapt the parameters tothe concept of the test sample in advance of online prediction. To improve the generalization ability, we randomly shuffle his-torical data and synthesize diverse concept drifts to train ourframework. In this way, Proceed has the potential to handle newconcepts during the online phase. Extensive experiments on five real-world time series datasetsdemonstrate that Proceed remarkably reduces the average fore-cast error of different forecast models by a large margin of 21.9%.Moreover, Proceed outperforms existing online model adapta-tion methods by an average of 10.9% and keeps high efficiency.",
  "Proactive Model Adaptation Against Concept Drift for Online Time Series ForecastingKDD 25, August 37, 2025, Toronto, Canada": "At time , a forecast model F parameterized by takes the pastobservations X = [v+1, , v] R as input features topredict the future -step values, denoted as Y = [v+1, , v+ ] R . The training objective is to optimize the model parameters such that the loss function Y Y 22 is minimized, where Y R denotes the predicted values in the horizon window. Typically, in multi-step forecasting where > 1, there are twoprimary strategies to generate the predictions Y at each time .The first strategy performs iterative forecasting. That is, the modelforecasts one step ahead and uses its prediction to forecast thenext step, repeating this process for the entire horizon. Despite itssimplicity, this strategy suffers from significant error accumulationover long horizons . The second strategy adopts direct forecast-ing where the model makes all the predictions Y simultaneously.Note that the two strategies require different output modules orlayers in the forecast model. Recent works have shownthat direct forecasting tends to outperform the iterative method,particularly for longer forecast horizons. Hence, this paper adoptsthe direct forecasting strategy where a sample X is consideredvalid for training if all the -step values in Y are known .In online forecasting scenarios, time series data are observedsequentially. Due to the dynamic nature of real-world processes,underlying data distributions are subject to constant change. Conse-quently, a forecast model trained on historical data may encounterdifficulties when confronted with new, evolving patterns. This issueis referred to as the concept drift challenge, which can substantiallyaffect model performance over time . To mitigate concept drifts,it is crucial to adapt the forecast model continuously to assimilatenew concepts presented in the incoming time series. Formally, theonline model adaptation problem is defined as follows. Definition 2 (Online Model Adaptation). At time , onlinemodel adaptation activates a model adapter A that produces adaptedmodel parameters based on available observations {v1, , v },where is expected to be close to the optimal parameters . Then,we forecast Y by F (X;). At a high level, existing model adaptation methods select some observed data D {((X, Y) | } as train-ing samples and utilize the forecasting feedback (e.g., forecastingerrors and gradients w.r.t. D) to update the model parameters.Their adapted parameters tend to align with the patterns or con-cepts present in D. However, it is crucial to recognize that theconcepts present in D may not necessarily reflect that of thetest sample (X, Y) due the horizon time span . To this end, themodel optimized on D might still be susceptible to concept drift,potentially resulting in suboptimal predictions at time (see detailsin ).In what follows, we provide empirical analysis that illustrates thepresence of concept drift between D and the test sample (X, Y),revealing the limitations of existing model adaptation techniques.",
  "Datasets": "Following prior works , we use five real-world time seriesdatasets, including ETTh2, ETTm1, Weather, ECL, and Traffic. Weprovide their detailed descriptions in Appendix C.1 and statisticalinformation in . We also adhere to the evaluation settings ofFSNet and OneNet , where each dataset is chronologicallydivided into training/validation/test sets by the ratio of 20:5:75. 3.2Baseline VariantsWe consider the following there time series forecasting models. FSNet : FSNet is built upon a TCN backbone and furtherdevelops an advanced updating structure that facilitates fastadaptation to concept drift. OneNet : OneNet is an ensemble model that dynamicallycombines two forecasters, one focused on temporal dependenceand one focused on channel dependence. We follow its officialimplementation where the two forecasters are built upon FSNets. PatchTST : PatchTST is one of the state-of-the-art time seriesforecasters, which models temporal patterns by Transformer.We pre-train the forecast models on the training set and then per-form online learning across the validation set and test set. For eachof the models, we compare two online learning techniques below. Practical: At each time , before forecasting Y, we perform pre-dictions on X and use the Mean Square Error (MSE) betweenthe ground truth Y and the prediction Y to update themodel by one-step gradient descent. Optimal: At each time , before forecasting Y, we assume theground truth Y1 is available without any delay. We calculate theprevious forecast loss on the last sample X1 (i.e., MSE betweenY1 and Y1) to update the model by one-step gradient descent.It is important to notice that the Optimal strategy uses the mostrelevant and recent information for the prediction on the test sampleX. However, it is infeasible in practice as it requires knowledge offuture data. The performance gap between this ideal strategy andthe Practical strategy reveals the presence of concept drift that isnot adequately addressed by existing online learning techniques. Tomaintain a fair comparison, both strategies fine-tune all the modelparameters using one training sample each time.",
  "Key Observations": "shows the MSE results of two strategies. We have the follow-ing major observations. First, the Practical strategy performs worsethan Optimal by an average of 107% on different models. This sig-nificant difference suggests that the most recently observed patternin (X, Y ) substantially differs from (X1, Y1) and failsto reflect the concept of the test sample (X, Y). In other words,the adapted forecast models are still vulnerable to the concept driftcaused by the feedback delay issue. Second, we can observe thegeneral tendency for the performance gap to become more signif-icant as the forecast horizon increases. This could be attributedto the increasing temporal distance between X and X. Third,OneNet demonstrates the best performance when utilizing the Op-timal strategy. However, its effectiveness drastically diminishes inall the cases when employing the Practical strategy, even becomingthe worst on the ETTm1 and Traffic datasets. The reason is thatOneNet is an ensemble model with a large number of parameters,which increases the overfitting risk on the new training samples.Fourth, PatchTST, recognized as the leading time series forecastingmodel, reports much smaller MSE than the other models. Whenusing the Practical strategy, PatchTST outperforms all the othermodels. However, it still falls short of the performance achievedby OneNet under the Optimal strategy. To sum up, there is stillconsiderable room for performance improvement if we can addressthe possible concept drift between each test sample and the latestavailable training data during online learning.",
  "Solution Overview": "The ultimate goal of Proceed is to close the gap between the newlyacquired training data and the test sample and boost performanceagainst concept drift caused by the feedback delay issue. To achievethis, a simple solution is to extract the latent concepts from all his-torical samples and learn a mapping function between each conceptand optimal model parameters w.r.t. the historical sample. As such,one can extract concept from each test sample and use the mappingfunction to directly generate possibly optimal parameters. How-ever, the online phase may include new concepts that are out of thehistorical data distribution. The simple solution can fail in this case,since it has not learned the relationship between out-of-distribution(OOD) concepts and corresponding optimal parameters.To address the problem, we propose to map concept drifts toparameter shifts. We assume that the direction and degree of thedrift over the concept space can reflect a possible direction andmagnitude of parameter shifts over the parameter space, informinghow to make an adaptation. Specifically, given a model that fits newtraining samples, Proceed exploits latent features from the trainingsamples and the current test sample, estimates the undergoingconcept drift, and accordingly predicts parameter shifts.Following existing online model adaptation methods ,we use one training sample for model updating at each time ,i.e., D = (X, Y ). With the model updated on D, weestimate the concept drift between D and X, predict potentialshifts in the parameter space, and accordingly make parameteradjustments. Formally, our Proceed solution consists of four keysteps at each time as listed below. (1) Online Fine-tuning. Given a forecast model F parameterizedby 1, we redo forecasting by Y = F (X ; 1).Next, we use the forecast error Y Y 22 to update 1 into by gradient descent. The subscript of indi-cates that the parameters have fit (X, Y ). (2) Concept drift estimation. Given D and the test sampleX, we feed them into two concept encoders E and E thatextract concept representations denoted as c R andc R , respectively. Then, we estimate the hidden state ofthe concept drift between X and X by , where = c c .",
  "(4) Online forecasting. Finally, the adapted model yields predic-tions by Y = F (X; ). In the next time + 1, the parameterswill be reset to": "The rationale of our solution is that we can synthesize diverseconcept drifts based on historical data to train our model adapter. shows an example of the historical training data includingfour samples with their concepts denoted by c1, , c4 respectively.Our idea is to shuffle the order of samples and train our modeladapter on the synthetic concept drifts between each pair of samples.Though the concepts of the future test samples (i.e., c5, , c7) areout-of-distribution, the concept drift patterns 46 and 57 aresimilar to 14, 23 which have been learned in training data.Given such reoccuring concept drifts, we can generate appropriateparameter shifts by experience.",
  "Concept Drift Estimation": "In the literature, there are numerous concept drift detection meth-ods that estimate the degree of concept drift to decide whento adapt the model. The degree can be estimated by the changesin forecast error , distribution distance , prediction uncer-tainty , etc. Nevertheless, the degree as a scalar in R is notinformative and cannot indicate how to adapt the model. Therefore,we propose to model a high-dimensional representation vector inR that characterizes both the degree and the direction of theconcept drift, where is the representation dimension.First, we devise a concept encoder E that extracts concept repre-sentation c from the latest training sample D = (X, Y ).Let X() R denote the time series of the -th variate. Givenall observations of variates at time , we encode c by thefollowing equation:",
  "c = E(X) = Average{MLP(X())}=1 R .(2)": "It is noteworthy that E has the potential to estimate hidden state ofY. When and 2, X itself contains a sequence of hori-zon windows, i.e., {Y, , Y }. In this case, MLP can learnthe hidden state of each observed horizon windows, exploit the tem-poral evolution pattern across horizon windows, and extrapolatethe hidden state of the next horizon window Y.Next, we estimate the concept drift between time and time by the concept difference, i.e., = c c R .",
  "Proactive Model Adaptation": "As illustrated in , we assume the concept representationspace and the parameter space to have some learnable relation-ships, where the estimated concept drift can indicate thedirection that the parameters should shift to.Technically, it is non-trivial to decode the concept drift repre-sentation into an appropriate parameter shift. As the parameterspace is often of a huge dimension, it is tough to search for anoptimal mapping function between the concept space and the pa-rameter space. Also, a simple mapping function may require toomany additional parameters, leading to costly memory overhead.For instance, when = 100 and the number of model parametersis 1 million, a trivial method is to learn a fully connected layerof 100 million parameters that maps to parameter shifts",
  "KDD 25, August 37, 2025, Toronto, Canada.Lifan Zhao, & Yanyan Shen": "Massimo Caccia, Pau Rodrguez, Oleksiy Ostapenko, Fabrice Normandin, MinLin, Lucas Page-Caccia, Issam Hadj Laradji, Irina Rish, Alexandre Lacoste, DavidVzquez, and Laurent Charlin. 2020. Online Fast Adaptation and KnowledgeAccumulation (OSAKA): a New Approach to Continual Learning. In NeurIPS. Mouxiang Chen, Lefei Shen, Han Fu, Zhuo Li, Jianling Sun, and Chenghao Liu.2024. Calibration of Time-Series Forecasting: Detecting and Adapting Context-Driven Distribution Shift. In Proceedings of the 30th ACM SIGKDD Conference onKnowledge Discovery and Data Mining (Barcelona, Spain) (KDD 24). Associationfor Computing Machinery, New York, NY, USA, 341352. Yuntao Du, Jindong Wang, Wenjie Feng, Sinno Pan, Tao Qin, Renjun Xu, andChongjun Wang. 2021. AdaRNN: Adaptive Learning and Forecasting for TimeSeries. In Proceedings of the 30th ACM International Conference on Information &Knowledge Management. ACM. Wei Fan, Pengyang Wang, Dongkun Wang, Dongjie Wang, Yuanchun Zhou,and Yanjie Fu. 2023. Dish-TS: A General Paradigm for Alleviating DistributionShift in Time Series Forecasting. In AAAI Conference on Artificial Intelligence.",
  "R(in+out) is initialized by zeros, () Rin, and () Rout": "To further reduce parameters, we share W()1and W()2acrosslayers of the same type (e.g., up projection layers in all Transformerblocks), while we learn an individual bias term b() to customizedistinct coefficients for each layer. Compared with a fully connectedlayer, we reduce the total parameters of the adaptation generatorfrom O(Linout) to O( (L + + in + out)), where L is thenumber of model layers.Finally, we derive the adapted parameters used in online fore-casting at time by:",
  "Mini-batch Training": "Given abundant historical data, we shuffle them to synthesizediverse concept drifts and train our model adapter on them. Toimprove the training efficiency, we randomly select multiple sam-ples as a mini-batch, adapt the forecaster towards each sampleconcurrently, and compute the average forecast loss.Let B = {X,, Y, }+=+1 represent the -th mini-batch of samples collected from different time. For the last mini-batch B1,our concept encoder E extracts latent features of all samples andtheir average c1 is considered as the concept representation ofB1. For each X, in B, we estimate the concept drift from B1to X, individually and generate the corresponding adaptation co-",
  "efficients (), , (), . Note that we do not iteratively make different": "versions of adapted models (i.e., {, }+=+1) which consumesmore GPU memory and more time cost. Instead, we only preserveone model obtained from the last mini-batch (denoted as 1) andsimultaneously handle multiple data batches.For brevity, we omit the superscript () in the following equa-tions. Let h, Rin denote the intermediate inputs of the -th",
  "= , 0 (, h,).(5)": "This equation also stands for other parameters such as convolutionfilters and bias terms. As such, we can directly compute ()0Has usual, where H = {, h, }+=+1. Correspondingly, wemultiply the layer outputs with , for each sample in parallel.As such, we obtain the predictions of all samples in B, denotedas { Y, }+=+1. Next, we compute the average forecast loss on 1+=+1 Y, Y, 22 and use the gradients to update 1 into and train other additional parameters, including the conceptencoders and the adaptation generator. The updated model and theupdated adapters will be used in the next mini-batch. During theonline phase, we only fine-tune the forecast model and keep theparameters of the model adapter frozen because the ground truthof the test sample is not available.As our model adapter learns the transition probability ( |, X, X), we can follow the proof in the previous work toshow that it is feasible for in Eq. (4) to yield lower forecastingerror than (see Appendix A).",
  "Discussion": "4.5.1Comparison with Existing Methods. Prior approaches to on-line time series forecasting make model adaptation passively basedon feedbacks in forecasting previous samples, which mainly focuson learning recent data patterns. In , we compare exist-ing methods and Proceed in terms of the training data and themodel adaptation techniques at each online time . Most methodsuse one newly acquired training sample, while SOLID selectsmore recent samples that share similar lookback windows with thetest sample and are assumed to share similar concept. SOLID andProceed simply fine-tune the forecast model by gradient descent,while FSNet and OneNet further generates additional parameteradjustments based on the forecasting feedback. Since the feedbackis delayed by steps and cannot reveal the test concept, we pro-pose a novel step called proactive model adaptation which aims tomitigate the effects of concept drift between the training sampleand the test sample. As this core step is orthogonal to existing meth-ods, Proceed can incorporate the data augmentation techniqueof SOLID and the feedback-based model adaptation techniques ofFSNet and OneNet. 4.5.2Time Complexity Analysis. Though the lookback windowsize and the number of variates could be large, our concept driftestimation has a linear complexity w.r.t. them, i.e., O() with arelatively small hyperparameter . As for proactive model adapta-tion, the time complexity is O( ( + in + out) + inout), whichis agnostic to the number of variates. Note we set a rather smallbottleneck dimension (e.g., 32). Hence, our framework is friendlyto large-scale multivariate time series with a large . Throughoutthe online phase, we first adapt the model parameters by Eq. (4),and online forecasting is performed with no additional cost.",
  "Experimental Settings": "Datasets. As introduced in Sec. 3.1, we use five popular bench-marks, of which more details are provided in Appendix C.1. Weconform to the dataset split ratio of FSNet and OneNet, i.e., we splitthe datasets by 20:5:75 for training, validation, and testing. Therationale is that online learning is of great practical value in scenar-ios of limited training data, when pretrained forecast models areinadept at handling new concepts during long-term online service. Forecast Models. As our framework is model-agnostic, we pre-train three popular and advanced forecasting models, includingTCN , PatchTST , and iTransformer . We report theforecasting errors of these pretrained models without any onlinelearning method (Method=\" \\\" in -4). Online Learning Baselines. We compare Proceed with GD, anave online gradient descent method, and the state-of-the-artonline model adaptation methods, including FSNet, OneNet, andSOLID. We reimplement FSNet and OneNet by adopting the Practi-cal strategy described in Sec. 3.2. Furthermore, FSNet is speciallydesigned for TCN, while OneNet is based on online ensemblingand is model-agnostic. We implement multiple variants of OneNetby combining each forecast model with an FSNet. As for SOLID, itsoriginal work learns linear probing with the pretrained parametersat each update time and does not inherit the fine-tuned parametersfrom the last update. As our datasets have a much long online phase,we implement a variant called SOLID++ that continually fine-tunesall model parameters across the online data. Empirically, SOLID++performs better than SOLID in our evaluation setting.",
  "Overall Comparison": "5.2.1Effectiveness. To verify the effectiveness of our proposedframework, we compare the predictive performance of Proceedand other baselines on the five popular datasets. We repeat eachexperiment with 3 runs and report the average results. As shownin -4, Proceed achieves the best performance in most cases,reducing the average forecast errors of all models without online",
  "PROCEEDPROCEED": ": Efficiency comparison on the Traffic dataset ( = 24).The horizontal axis is the online forecasting latency (mil-lisecond) between updating the model and obtaining onlinepredictions. The vertical axis is the average MSE on test data.The size of each circle represents the peak amount of GPUmemory occupation (GB). learning by 42.3%, 10.3%, 12.9% for TCN, PatchTST, and iTrans-former, respectively. Also, it is worth mentioning that all forecastmodels have been enhanced by RevIN , a data adaptation ap-proach to concept drift by reducing the distribution shifts w.r.t.the mean and standard deviation of time series. Since all onlinelearning methods can still achieve remarkable improvements, itsuffices to support our claim that time series forecast models needmodel adaptation to handle more complex concept drifts.Based on the same forecast model, Proceed outperforms theexisting online model adaptation methods by 12.5%, 13.6%, 6.7%against FSNet, OneNet, and SOLID++, respectively. In particular,we can observe that TCN with existing online learning methodsstill lags behind a frozen PatchTST. By contrast, TCN enhanced byProceed can outperform the frozen PatchTST and iTransformerin some cases of ETTh2, ETTm1, and Weather datasets. Note thatthese 3 datasets have relatively more significant concept drift asshown in . This indicates that Proceed has the capability ofhandling the concept drift during online learning.",
  "iTransformer": "\\0.7240.8220.9440.4780.5660.5840.4730.5810.6760.2890.3150.3440.2570.2670.280GD0.6690.7880.9430.4220.4880.5290.4440.5520.6510.2870.3130.3410.2510.2620.272OneNet0.6630.7750.9160.4490.5250.5460.4110.5260.6330.2890.3160.3450.2520.2640.277SOLID++0.6730.7890.9290.4210.4880.5270.4470.5540.6530.2880.3140.3430.2480.2600.269Proceed0.6330.7530.8890.3980.4610.5000.3780.4950.6020.2810.3060.3310.2310.2430.254 5.2.2Efficiency. As depicted in , we compare the GPU mem-ory occupation and inference latency of different model adaptationmethods on Traffic, the largest time series dataset. We record theonline latency, including feedback-based model adaptation and anyother additional steps. Here, SOLID only fine-tunes the final layerrather than the whole model, reducing the gradient computationcost. More specifically, SOLID and SOLID++ computes gradientson several selected training samples iteratively instead of concur-rently, so as to save GPU memory. By contrast, Proceed only usesthe latest training sample and thereby achieves the lowest latency.Moreover, Proceed is faster and more lightweight than the onlineensembling method OneNet.",
  "Visualization of the Representation Space": "To verify our assumption that online data have OOD concepts, weadopt t-SNE to visualize the representations of concepts c andconcept drifts on the historical training data and onlinedata. As shown in a, there are a great number of OOD conceptson the online data, which are distinct from any historical concept.",
  "Ablation Study": "For a fine-grained investigation into the role of our proposed com-ponents, we conduct more ablation studies by introducing fivevariants as follows. (1) feedback-only: this variant only performsgradient descent based on feedback from D; (2) G(c): this vari-ant generates adaptation based on the concept of the test sampleonly instead of estimating concept drift; (3) E(X ): this variantuses the same encoder E to extract concepts c and c from look-back windows X and X, respectively; (4) diff. W()1 , W()2 : thebottleneck layers that generate adaptation coefficients are totallydifferent across the model layers.As shown in , we have four major observations. First,Proceed outperforms the nave feedback-based model adaptationmethod by a large margin, demonstrating the necessity of proactivemodel adaptation in reducing the distribution gap between newtraining data and test data. Second, the variant w/ G(c), whichgenerates adaptation only based on the estimated test concept, re-sults in a considerably higher MSE than Proceed based on conceptdrift. This is due to the fact that some OOD concepts may takeplace in the online phase and challenge the adaptation generator.By contrast, our concept drift-based design can make the inputs ofG in-distribution as much as possible, reducing difficulties in gener-ating adaptation. Third, the variant w/ E(X ) has suboptimalperformance compared with Proceed with two different conceptencoders. One reason is that we can only infer limited informationabout Y from X, and it would be better to leverage theobserved Y. Another possible reason is that the model beforeproactive model adaptation does not overfit D, while we antici-pate the adapted model being optimal for (X, Y). Thus, E shouldencode a little information of the latest training sample into c ,while a different encoder E should encode all information of thetest sample into c. Fourth, the variant learning different W()1and W()2do not significantly outperform Proceed with weights sharedacross some layers. Considering the non-stationarity of time series,an over-parameterized generator G has higher overfitting risks ontraining data. Thus we only set the bias term b()1to be differentacross model layers, yielding distinct, layer-specific adaptations.",
  "Conclusion": "In this work, we highlight that online time series forecasting inher-ently has a temporal gap between each test sample and availabletraining data, where concept drift may well occur. Through em-pirical study, we found that this gap hinders the effectiveness ofexisting model adaptation methods which passively rely on the feed-back in forecasting recent data. To address this issue, we propose anovel online model adaptation framework named Proceed, whichproactively adapts the forecast model to the test concept beforeforecasting each test sample. Specifically, Proceed first fine-tunesthe model on the latest acquired training sample, then extractslatent features from time series data to estimate the undergoing",
  "feedback-only4.5060.5901.1404.9650.3687.3%G(c)4.2180.5581.0304.8420.3521.2%E(X )4.3960.5571.0304.7760.3521.7%diff. W()1 , W()24.2540.5471.0204.7670.3520.4%Proceed4.2160.5431.0174.7520.352-": "concept drift, and efficiently maps the estimated drift into parame-ter adjustments that are tailored for the test sample. Furthermore,we synthesize diverse concept drift and optimize Proceed withgeneralization capacity of mapping concept drift to beneficial pa-rameter adjustments. Extensive experiments on five real-world timeseries datasets demonstrate that our proposed Proceed remarkablyreduces the forecast errors of various forecast models and surpassesthe state-of-the-art online learning methods. Our proactive modeladaptation method provides a new direction in addressing contin-uous concept drift for online systems, which we wish to help anyother prediction tasks with a feedback delay issue. This work is supported by the National Key Research and Devel-opment Program of China (2022YFE0200500), Shanghai MunicipalScience and Technology Major Project (2021SHZDZX0102), andSJTU Global Strategic Partnership Fund (2021 SJTU-HKUST). Wewould like to thank Shaofeng Cai for his valuable advice on thepreliminary version of the paper.",
  "Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, and Qi Tian. 2023.Accurate medium-range global weather forecasting with 3D neural networks.Nature 619, 7970 (2023), 533538": "Oussama Boussif, Ghait Boukachab, Dan Assouline, Stefano Massaroli, TianleYuan, Loubna Benabbou, and Yoshua Bengio. 2023. Improving day-ahead SolarIrradiance Time Series Forecasting by Leveraging Spatio-Temporal Context. InAdvances in Neural Information Processing Systems, Vol. 36. Curran Associates,Inc., 23422367. Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and SIMONECALDERARA. 2020. Dark Experience for General Continual Learning: a Strong,Simple Baseline. In Advances in Neural Information Processing Systems, Vol. 33.Curran Associates, Inc., 1592015930. Joos-Hendrik Bse, Valentin Flunkert, Jan Gasthaus, Tim Januschowski, DustinLange, David Salinas, Sebastian Schelter, Matthias Seeger, and Yuyang Wang. 2017.Probabilistic demand forecasting at scale. Proceedings of the VLDB Endowment10, 12 (Aug. 2017), 16941705.",
  "Steven C.H. Hoi, Doyen Sahoo, Jing Lu, and Peilin Zhao. 2021. Online learning:A comprehensive survey. Neurocomputing 459 (Oct. 2021), 249289": "Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, andJaegul Choo. 2022. Reversible Instance Normalization for Accurate Time-SeriesForecasting against Distribution Shift. In International Conference on LearningRepresentations. Wendi Li, Xiao Yang, Weiqing Liu, Yingce Xia, and Jiang Bian. 2022. DDG-DA: Data Distribution Generation for Predictable Concept Drift Adaptation.Proceedings of the AAAI Conference on Artificial Intelligence 36, 4 (jun 2022),40924100.",
  "Daojun Liang, Haixia Zhang, Jing Wang, Dongfeng Yuan, and Minggao Zhang.2024. Act Now: A Novel Online Forecasting Framework for Large-Scale StreamingData. arXiv:2412.00108 [cs.LG]": "Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, andMingsheng Long. 2024. iTransformer: Inverted Transformers Are Effective forTime Series Forecasting. In The Twelfth International Conference on LearningRepresentations. Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long. 2022. Non-stationaryTransformers: Exploring the Stationarity in Time Series Forecasting. In Advancesin Neural Information Processing Systems, Vol. 35. 98819893. Zhiding Liu, Mingyue Cheng, Zhi Li, Zhenya Huang, Qi Liu, Yanhu Xie, andEnhong Chen. 2023. Adaptive Normalization for Non-stationary Time SeriesForecasting: A Temporal Slice Perspective. In Thirty-seventh Conference on NeuralInformation Processing Systems.",
  "Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. 2023. A ComprehensiveSurvey of Continual Learning: Theory, Method and Application. (Jan. 2023).arXiv:2302.00487 [cs.LG]": "Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, andDoyen Sahoo. 2024. Unified Training of Universal Time Series Forecasting Trans-formers. In Proceedings of the 41st International Conference on Machine Learning(Proceedings of Machine Learning Research, Vol. 235), Ruslan Salakhutdinov, ZicoKolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and FelixBerkenkamp (Eds.). PMLR, 5314053164. Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, Xiaojun Chang, and ChengqiZhang. 2020. Connecting the Dots: Multivariate Time Series Forecasting withGraph Neural Networks. In Proceedings of the 26th ACM SIGKDD International",
  "Conference on Knowledge Discovery & Data Mining. ACM": "Xiaoyu You, Mi Zhang, Daizong Ding, Fuli Feng, and Yuanmin Huang. 2021.Learning to Learn the Future: Modeling Concept Drift in Time Series Predic-tion. In Proceedings of the 30th ACM International Conference on Information &Knowledge Management. ACM. Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. 2023. Are TransformersEffective for Time Series Forecasting? Proceedings of the AAAI Conference onArtificial Intelligence 37, 9 (jun 2023), 1112111128. YiFan Zhang, Qingsong Wen, Xue Wang, Weiqi Chen, Liang Sun, Zhang Zhang,Liang Wang, Rong Jin, and Tieniu Tan. 2023. OneNet: Enhancing Time SeriesForecasting Models under Concept Drift by Online Ensembling. In Thirty-seventhConference on Neural Information Processing Systems. Lifan Zhao, Shuming Kong, and Yanyan Shen. 2023. DoubleAdapt: A Meta-learning Approach to Incremental Learning for Stock Trend Forecasting. InProceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and DataMining (Long Beach, CA, USA) (KDD 23). Association for Computing Machinery,New York, NY, USA, 34923503. Lifan Zhao and Yanyan Shen. 2024. Rethinking Channel Dependence for Multi-variate Time Series Forecasting: Learning from Leading Indicators. In The TwelfthInternational Conference on Learning Representations. Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,and Wancai Zhang. 2021. Informer: Beyond Efficient Transformer for Long Se-quence Time-Series Forecasting. Proceedings of the AAAI Conference on ArtificialIntelligence 35, 12 (may 2021), 1110611115. Tian Zhou, Peisong Niu, Xue Wang, Liang Sun, and Rong Jin. 2023. One FitsAll: Power General Time Series Analysis by Pretrained LM. In Thirty-seventhConference on Neural Information Processing Systems. Jiaqi Zhu, Shaofeng Cai, Fang Deng, Beng Chin Ooi, and Wenqiao Zhang.2023. METER: A Dynamic Concept Adaptation Framework for Online Anom-aly Detection. Proceedings of the VLDB Endowment 17, 4 (Dec. 2023), 794807.",
  "ATheoretical Analysis": "Inspired by a recent work , we can prove that it is feasible forthe forecast model with proactive model adaptation to have lowerforecasting error than one without proactive model adaptation.In practice, we initialize the parameters W()2as an all-zeromatrix, and the adaptation coefficients stem from zeros. Thus, wehave A(; 0) = , where A denotes the model adpater and 0denotes the initialized parameters of A. Given randomly shuffledhistorical data, we train the adapter parameters into that is close toan optimum and approximates the transition probability ( |, X, X). Let denote A( ; ), denote the optimal modelparameters for forecasting Y, and = A( ; ). The adaptedmodel parameters generated by the well-learned model adapterare expected to get closer to the optimal model parameters thanthose generated by A(; 0). Formally, we have A( ; ) A( ; ) < A( ; 0) A( ; ),(6)i.e., < .(7)As we synthesize various concept drifts to train the model adapter,we believe that most concept drifts on test data have been learned inpast experience, thereby satisfying Eq. (7) during the online phase.Assuming that the forecast model F has Lipschitz constant withupper bound L and lower bound L w.r.t. its parameters, we have",
  "F (X; ) F (X; ) < L .(9)": "Note that F is usually a neural network as a continuous functionof . We define S( , ) as a sphere centering at with a radius R+. When approaches 0, due to the continuity of F , the upperbound and lower bound of Lipschitz constant within S( , ) willbecome closer and finally identical, i.e., lim0+ L/L =1. Moreover, we have known that / > 1in Eq. (7). Therefore, there exists a constant > 0 such thatL/L < / , where , S( , ). Thus it is possible to satisfy the following inequality:",
  "BRelated WorksB.1Online Model Adaptation": "Online model adaptation, or online learning , has been a popu-lar learning paradigm that updates models on new data instantlyor periodically. In the general field, most efforts focus on address-ing the catastrophic forgetting issue stemming from excessiveupdates, and numerous continual learning methods have beendeveloped to retain acquired knowledge of past data by rehearsalmechanisms and regularization terms , which can be seam-lessly incorporated into our framework. Most recently, SOLID proposed to adapt the forecast model by several selected trainingsamples which are assumed to have a similar context (i.e., conceptin this paper) to each test sample. SOLID relies on heuristicmeasures about context similarity and is not fully aware of unob-served contexts. The selected training samples may not share theexactly same concept with the test sample. Thus it is desirable tofurther adopt our proposed framework. We would like to leavethe combination of training data sampling and proactive modeladaptation as future work.After the submission of our manuscript, there are two concur-rent researches that also noticed the information leakage inFSNet and OneNet. Since the ground-truth -step values of the lastprediction are not fully observed, both works propose to generatepseudo labels of the unobserved values, concatenate the pseudolabels with observed labels to simulate complete ground truth, andcalculate the forecast errors to update the model. The potentialdrawback is that the pseudo label generator is still susceptible toconcept drift and may produce low-quality pseudo labels, leavingthe concept drift issue unresolved.On top of online learning, rolling retraining is another learn-ing paradigm that periodically re-trains a new model from scratchon all historical data, while the cost of frequent retraining is un-affordable. In practice, we can perform rolling retraining once a",
  "B.2Data Adaptation": "Apart from model adaptation, data adaptation is another main-stream approach to concept drift in time series forecasting, whichis orthogonal to model adaptation. The goal of data adaptation is tonormalize historical training data and future test data into a com-mon data distribution , reducing overfitting risks. Amongthem, RevIN is the most popular method that applies instancenormalization to each time series lookback window and restoresthe statistics to the corresponding predictions, making each samplefollow a similar distribution. Such normalization-based data adap-tation methods mainly focus on the statistical changes in the meanand deviation of time series, while they overlook the distributionshifts in more complex temporal dependencies between time stepsand spatial dependencies between variates . Meanwhile, theforecast model may underfit the normalized time series, as the re-moved statistics can serve as informative signals for prediction .",
  "C.2Evaluation Settings": "To the best of our knowledge, there is no open-source evaluationframework that considers the temporal gap in online learning fortime series forecasting. Though Zhang et al. noticed the feed-back delay, they sidestep this issue by performing one predictiontask every time steps and performing the next task until theground truth is available. Concretely, they use X: to forecastX+1:+ and then use X+:+ to forecast X++1:+2 . In con-trast, the common practice is to perform forecasting at each timestep between and + , continually updating the previous predic-tions of X+:+. The reason is that short-term forecasting istypically easier than long-term forecasting. Therefore, in this work,",
  ": Pipeline of Proceed at each online time . CDE:concept drift estimation; PMA: proactive model adaptation": "we implement a more realistic evaluation framework for onlinelearning where we perform forecasting at each time step, whichremains consistent with the traditional evaluation setting.As for the ETTm1 dataset, popular works (e.g., Informer ,PatchTST , and iTransformer ) usually use 69,680 time stepsof the dataset. In contrast, FSNet and OneNet only used a quarter ofit, i.e., 17,420 steps, and the training set only contains 3484 samples.This setting is in favor of small models (e.g., TCN) but is challengingfor the state-of-the-art time series forecast models. Therefore, wetake all 69,680 time steps following PatchTST and iTransformer.As such, the ETTm1 dataset in our experiment has the greatestnumber of samples, enriching the diversity of our datasets.",
  "C.3Implementation Details": "We conduct experiments on four Nvidia 4090 24GB GPUs. We raneach experiment 3 times with different random seeds and reportedthe average results. We follow the official implementation of theforecast models and the online learning methods, using their recom-mended model hyperparameters (e.g., the number of layers) and theAdam optimizer. In cases where default hyperparameter values arenot provided, we conduct a grid search to find the optimal hyper-parameters that yield the best performance. Following FSNet andOneNet, the lookback length for TCN is set to be 60. For PatchTSTand iTransformer, we search for the optimal lookback length andset to be 512. We enhance TCN and FSNet by RevIN whichis widely adopted by the state-of-the-art forecast models to reducedistribution shifts, lowering the forecasting loss in our experiments.As for the Optimal and Practical variants, we report the lowesterrors when applying or not applying RevIN to FSNet and OneNet.Empirically, the Optimal variant without RevIN achieves betterperformance in some cases.For Proceed, we search the concept dimension in {50, 100,150, 200} and the bottleneck dimension in {24, 32, 48}. The MLP",
  "DPipeline of Proceed": "depicts the pipeline of Proceed during the online phase,which is a bit different from the steps introduced in Sec. 4.1. Givenmodel parameters 1 that have been updated with previoustraining data D(1), our model adapter additionally estimates theconcept drift between D(1) and D and adapts the model toD, yielding forecast feedback for updates. The reason behind it isthat we jointly train the forecast model and the model adapter (seeSec. 4.4), while our training algorithm does not enforce the forecastmodel to perform well alone. Consequently, the forecast modelwithout could forget some predictive skills, while the parameteradjustments from the model adapter may incorporate the skillsforgot by the model. Thus it is necessary for the inference processto keep consistent with the training phase, i.e., we always makepredictions by integrating the forecast model and our model adapteras a whole. Nevertheless, in the feedback-based adaptation step,we only finetune the forecast model but keep the model adaptersparameters frozen. Otherwise, the model adapter may overfit thepatterns of the one-step concept drift between D(1) and D,losing generalization ability.",
  "EFurther Experimental ResultsE.1Performance Gap in terms of MAE": "shows the MAE errors of two variants of online learningintroduced in Sec. 3. Likewise, we define MAE as the performancegap w.r.t. MAE. We can observe a general trend similar to ,where a longer horizon can enlarge the performance gap betweenthe Optimal and the Practical variants. The ECL and Traffic datasetsdo not have significant concept drift between each test sample andits preceding training sample. By contrast, MSE is more significantthan MAE. We conjecture that online learning mainly benefitsthe forecast models in some rare or extreme cases of time seriesevolution where the frozen models predictions deviate far fromthe ground truth and result in large MSE.",
  "iTransfo.GD2.463 3.759 6.152 0.459 0.602 0.692 0.851 1.131 1.406Proceed2.228 3.571 5.868 0.426 0.569 0.659 0.734 1.001 1.299": "In this section, we introduce variants that update the modelwith feedback on Y and partial ground truth { Y} 1=1 , whereY = . Since each update involves training sam-ples, the GPU memory overhead becomes about times than one in-volving Y only. We leave out results on ECL and Traffic datasets,where the GPU memory overhead exceeds the limit of our Nvidia4090 GPUs (24GB). As shown in , Proceed consistentlyoutperforms online gradient descent method under this setting.Compared with , using both Y and partial ground truthfor feedback-based adaptation can improve the performance, whilethe improvement is insignificant. This indicates that samples withpartial ground truth cannot reveal the concept of the test sample, ne-cessitating proactive model adaptation. Though the partial groundtruth can be beneficial to our method, the finetuning cost scales upwith (e.g., up to either 96 latency or 96 GPU memory). Thus,it would be much more efficient if using Y only. Notably, ourproposed method using Y only can still outperform GD using training samples in most cases. . E.3Variants of the Concept EncoderIn this section, we study different kinds of operations on multivari-ate time series using our concept encoder. Let c, denote the latentconcept feature vector of the -th variate. Apart from the averageintroduced in Eq. (1), we implement two variants as follows: linear transformation: we learn a linear layer that transformsthe concatenation of all latent features, i.e., = . weighted sum: we learn an individual weight R for each-th variate, and = .In , we compare the performance of the variants in terms ofthe average MSE with 24,48,96. The results demonstrate thatthe average operation is simple yet effective. One possible reasonis that all variates are typically assumed to have equal importanceduring both training and evaluation."
}