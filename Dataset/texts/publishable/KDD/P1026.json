{
  "Abstract": "We consider the problem of estimating the spectral density of the normalized adjacency matrixof an n-node undirected graph. We provide a randomized algorithm that, with O(n2) queriesto a degree and neighbor oracle and in O(n3) time, estimates the spectrum up to accuracyin the Wasserstein-1 metric.This improves on previous state-of-the-art methods, including anO(n7) time algorithm from [Braverman et al., STOC 2022] and, for sufficiently small , a 2O(1) time method from [Cohen-Steiner et al., KDD 2018]. To achieve this result, we introduce a newnotion of graph sparsification, which we call nuclear sparsification. We provide an O(n2)-queryand O(n2)-time algorithm for computing O(n2)-sparse nuclear sparsifiers. We show that thisbound is optimal in both its sparsity and query complexity, and we separate our results fromthe related notion of additive spectral sparsification. Of independent interest, we show that oursparsification method also yields the first deterministic algorithm for spectral density estimationthat scales linearly with n (sublinear in the representation size of the graph).",
  "where 1(NG) n(NG) are the eigenvalues of Gs normalized adjacency matrix, NG RV V": "Note that (1) is equivalent to requiring that the Wasserstein-1 distance between the uniform dis-tribution on 1, . . . , n and the uniform distribution on 1, . . . , n is less than . We also considera more general problem, which we call the matrix SDE problem, where the goal is to estimateeigenvalues of a given symmetric matrix with spectral norm bounded by 1, in the same metric, (1). Outside of applications in network science, SDE has seen recent interest from the machine learningcommunity due to its connections to learning distributions based on moment estimates [KV17;JMSS23] and learning graph structure from random walk traces [CKSV18].The matrix SDEproblem also has applications in deep learning [GKX19; PSG18]. Algorithms for related (oftenstronger) notions of spectral approximation, like relative-error eigenvalue histograms, or point-wiseeigenvalue estimates, have been widely studied for applications like approximating matrix normsand spectral sums [Mus+18; Bha+23; SW23; Bha+24]. Excitingly, at least for unweighted graphs, it has been shown that Problem 1.1 can be solved intime that is sublinear in the size of Gs representation, which can be as large as (n2). Given aneighbor oracle that when queried with a V and i Z>0 outputs the degree of a and the i-thvertex incident to a (if there is one) in O(1) time, there are randomized SDE algorithms running in2O(1) [CKSV18] and O(n7) time [BKM22].1 Recent work also studies query complexity lowerbounds for different oracles [JMSS23]. However, large gaps remain: The best query lower boundfor neighbor oracles is just (1/2) [JMSS23]. Our work is motivated by these advances, the general importance of sublinear time graph algorithms[CRT05; Czu+03; CS10; ORRR12; BKM14; ERS18; MOP01], and progress on sublinear timealgorithms for other spectral problems like expander testing [CS07; NS10; GR11] and spectralclustering [Glu+21; Mou21]. We ask: Is it possible to improve on existing SDE algorithms? Isit possible to efficiently obtain new sparse approximations, i.e., sparsifications, of G that facilitatemore efficient spectral density estimation? Finally, all previous sublinear time SDE algorithmsmake critical use of randomness. Are there sublinear time deterministic algorithms? In this paper, we provide an affirmative answer to each of these questions. For unweighted graphs,we provide a randomized O(n3)-time algorithm for solving Problem 1.1 and a deterministicn 2 O(1)-time algorithm. The randomized method improves on the O(n7)-time method fromBraverman et al. [BKM22] and on the 2O(1)-time method from Cohen-Steiner et al. [CKSV18]for small values of . To the best of our knowledge, our deterministic method is the first to achieve",
  "The runtime of the method from [CKSV18] is independent of n; instead of outputting a list of n eigenvalues, itoutputs a list of O(1/) distinct eigenvalue magnitudes and corresponding multiplicities": "a sub-quadratic dependence on the number of nodes, n. Moreover, we obtain an algorithm withthe same complexity for weighted graphs provided that, when queried with a V and i Z>0, theoracle outputs the weighted-degree of a, the weight of the i-th largest edge incident to a (with tiesbroken arbitrarily), and the endpoint of this edge. Finally, we show that these complexities areobtainable even in a weaker random walk model for accessing G.",
  "A Sparsification Approach to Spectral Density Estimation": "Our results follow a natural two-stage approach to approximating the spectral density of graphs.First, in sublinear time, we construct a sparse matrix that approximates NG RV V , the normal-ized adjacency matrix of G, and has o(n2) entries. Second, we apply and adapt existing spectraldensity estimation methods to efficiently approximate the spectrum of the sparse approximation.This approach decouples the information theoretic problem of how to approximate NG using a smallnumber of queries from the computational problem of how to efficiently approximate eigenvalues. Concretely, our approach motivates the following natural question: What notions of graph approx-imation (ultimately, sparsification) are sufficient for preserving the spectrum in the sense of (1) andcan be obtained in sublinear time? We make progress on this problem by introducing a new notionof -additive nuclear approximation and presenting algorithms that obtain near-optimal sparsityand query complexity for producing such approximations. Formally, we define: Definition 1.2 (Additive Nuclear Approximation and Sparsification). M RV V is an -additivenuclear approximation to n-vertex graph G = (V, E, w) if NG M n. M is an -additivenuclear sparsifier of G if it is also s-sparse, i.e., has at most s non-zero entries, for s = o(n2).2 In Definition 1.2, NG M denotes the nuclear norm, i.e., the sum of the singular values ofNG M. Importantly, additive nuclear approximation is sufficient for solving the SDE problem.For symmetric matrices, A, B Rnn, let W1(A, B) def= 1 nni=1 |i(A)i(B)| denote Wasserstein-1distance between the probability distributions p and q induced by the real-valued eigenvalues of Aand of B. A short proof (see .6, Fact 1.17) establishes that W1(A, B) 1",
  "Our main algorithmic result is that nuclear sparsifiers with O(n2) non-zero entries can be com-puted deterministically in O(n2) time. Formally, we assume the following access to G:": "Definition 1.3 (Adjacency query model). We say we have adjacency query access to a weightedgraph G = (V, E, w) if V is known and there is an O(1) time procedure, GetNeighbor(a, i), thatwhen queried with any a V and i Z>0 outputs degG(a) and, if there is one, the i-th largestedge {a, b} and w{a,b} (with ties broken arbitrarily). If the a has < i edges, the oracle returns . Adjacency queries are easily supported by standard graph data structures. For example, an ad-jacency list where each nodes neighbors are stored in an array supports adjacency queries forunweighted graphs. To support weighted graphs, it suffices to sort these arrays by edge weight.4 2Note that M itself is not required to be the adjacency matrix of a graph and our algorithms will in general returnmatrices that are not. However, in all cases, we show that one can modify our algorithm to ensure that the outputis indeed a normalized adjacency matrix. See Lemma 7.1 in for details.3When A and B are diagonal with monotonically decreasing diagonal entries then W1(A, B) =1nA B. Inthis sense, nuclear approximation is a natural strengthening of approximation in the Wasserstein-1 distance.4While this assumption on the ordering of the array might be considered non-standard, it is easy to supportusing a basic data structure for storing the graph provided in which we sort the edges by weight. We also show",
  "Within the query model established, we prove the following main result on constructing sublinearsize nuclear sparsifiers in . Our method works even for weighted graphs": "Theorem 1.4 (Sublinear Time Nuclear Sparsification). There is a deterministic method (Algorithm1) that, for any (0, 1), returns a 2n2-sparse -additive nuclear sparsifier for any undirectedweighted graph G, and runs in O(n2) time in the adjacency query model (Definition 1.3). We obtain Theorem 1.4 using a greedy approach that deterministically adds edges from G to thesparsifier based on their weight and end-point degrees. Importantly, we leverage that this approachis deterministic to obtain the first deterministic sublinear time SDE algorithms. Before discussingthese SDE algorithms, in .2 we highlight that deterministic sublinear time methods likeTheorem 1.4 are not possible for stronger and more well-studied notions of graph sparsification.",
  "Comparison to Prior Work on Sparsification": "A significant line of work studies spectral graph sparsification, which generalizes cut sparsification[BK96], and has applications in linear system solving, combinatorial graph algorithms, and beyond[ST11; SS11; BSS12; Kap+17; LS18]. Concretely, a spectral sparsifier is defined as: Definition 1.5 (Spectral Sparsifier, [ST11]). Given > 0 and a graph G with (unnormalized)Laplacian matrix L, a symmetric matrix L Rnn is an -spectral sparsifier of L if, for all x Rn,(1 )xLx x Lx (1 + )xLx. Definition 1.5 involves the Laplacian, whereas we focus on the (normalized) adjacency matrix. Nev-ertheless, it is straightforward to show that the spectrum of the normalized adjacency matrix ofan -spectral sparsifier is -close to that of G in Wasserstein distance. However, it is impossibleto obtain -spectral sparsifiers with o(n2) adjacency queries (see Theorem 11 of Lee [Lee13] with = 1/n). Consequently, we consider notions of spectral sparsification that are weaker than Defini-tion 1.5, but stronger than nuclear sparsification (Definition 1.2). In particular, we introduce thefollowing notion of additive error spectral sparsification, which strictly strengthens Definition 1.2:5",
  "Definition 1.6 (Additive Spectral Sparsifier). A symmetric matrix M RV V is an -additivespectral sparsifier of G if M NG2": "We show in that an O(n2 log n)-sparse -additive spectral sparsifier can also be obtainedfor all weighted graphs in O(n2 log n) time using standard random sampling methods.6 Giventhis result, it is natural to ask whether we can obtain a deterministic algorithm for additive spectralapproximation with a linear dependence on n, as we do for nuclear sparsification in Theorem 1.4.Interestingly, we prove that this is impossible:",
  "-additive spectral sparsifier (Definition 1.6), even for unweighted graphs": "in Theorem 1.13 that we can extend our result to a more standard random walk model, still obtaining a nuclearsparsifier with just O(n2) queries.5Definition 1.6 is related to two notions of sparsification previously studied by Lee [Lee13] and Agarwal et al.[AKLP22]. Additive spectral sparsification is stronger than probabilistic (, )-spectral sparsification [Lee13] becausewe require x NGx xx xMx x NGx + xx to hold simultaneously for all vectors x Rn with highprobability. Additive spectral sparsification is also a stronger notion than (, )-cut sparsification [AKLP22], becausewe require the additive guarantee to hold for all vectors x rather than just for cuts.6The O(n2 log n) sparsity can be improved to O(n2) at the cost of additional poly(log n/) runtime factorsusing near-linear time -spectral sparsification methods [JRT23; LS18].",
  "Applications to Sublinear Time Spectral Density Estimation": "A main application of our result on nuclear sparsification (Theorem 1.4) is faster deterministicand randomized sublinear time SDE algorithms for graphs. In the randomized setting, a long lineof work in computational chemistry, applied math, and, recently, computer science [Ski89; SR94;WWAF06; LSY16] studies linear time methods for spectral density estimation. In Chen et al.[CTU21] and Braverman et al. [BKM22] it was proven that common randomized algorithms likethe stochastic Lanczos quadrature method and moment matching can approximate the spectraldensity of any symmetric matrix A up to -accuracy in Wasserstein distance using roughly O(1)matrix-vector multiplications with A.7 At a high level, these methods solve the SDE problem byapproximating the first q = O(1) moments of As eigenvalue density, which can be expressedas Tr(A), Tr(A2), . . . , Tr(Aq). They return a distribution that matches or nearly matches thosemoments, so will be increasingly close to the true spectral density as q increases. For each i 1, . . . , q, estimating Tr(Ai) can be reduced to roughly O(i) matrix-vector products with A throughthe use of stochastic trace estimation methods like Hutchinsons estimator [Hut90; MMMW21]. We obtain our main result on the SDE problem by simply applying such methods (concretely, The-orem 1.4 from Braverman et al. [BKM22]) to our nuclear norm sparsifier, which can be multipliedby a vector in O(n2) time. Formally, we obtain the following result (proven in ):",
  "Theorem 1.8 (Randomized Sublinear Time SDE). There is a randomized algorithm that solvesthe SDE problem (Problem 1.1) with probability 99/100 in On3 time in the adjacency querymodel (Definition 1.3)": "Theorem 1.8 directly improves on an O(n7) time method from Braverman et al. [BKM22], whichonly applied to unweighted graphs, and takes a different approach from ours. That work leveragesrandomized methods for approximating matrix-vector products instead of sparsification. For suffi-ciently small , we also improve on the 2O(1) time method from Cohen-Steiner et al. [CKSV18],which has no dependence on n.8",
  "Theorem 1.9 (Deterministic Sublinear Time SDE). There is a deterministic algorithm that solvesthe SDE problem (Problem 1.1) in n 2O(1 log(1) time in the adjacency query model (Defini-tion 1.3)": "We prove Theorem 1.9 by showing that it is possible to exactly compute the ith eigenvalue moment,TrMi, for our particular nuclear norm sparsifier M in O(2i log(1))) time via direct computationof the diagonal entries of TrMi. Crucially, we leverage the fact that our nuclear norm sparsifiersguaranteed by Theorem 1.4 are uniformly sparse: not only is the total number of non-zero entriesin M small, but every row of the sparsifier has a bounded number of non-zero entries (specifically,O(1/2)). Once the eigenvalue moments are computed, we again appeal to the moment matchingmethod from Braverman et al. [BKM22], as in the proof of Theorem 1.8. Previously, the best 7The method from Braverman et al. [BKM22] uses O(min(1, 2 log4(1)/n)) matrix-vector multiplications,which is O(1) for sufficiently large n.8Initial evidence (lower bounds in restricted models) suggests that it may not be possible to improve the dependence in Cohen-Steiner et al. [CKSV18] to sub-exponential while maintaining no dependence on n [JMSS23].",
  "Lower Bounds for Nuclear Sparsification": "In this section, we prove Theorem 1.10 which shows that our sparsification result of Theorem 1.4 isnearly optimal in terms of sparsity. The proof proceeds in two steps. First, in .1, we restrictto the case where = (1/n), showing that with constant probability, the unnormalized adjacencymatrix A of a random Erds-Rnyi graph cannot be approximated well in the nuclear norm by anysparse matrix B with nnz(B) = o(n2/ log2 1) = o(n2/ log2 n), i.e., A B = (n1.5). Second, in .2 we extend the argument in .1 to a lower bound on nuclear sparsifica-tion of normalized adjacency matrices for a broader range of to obtain Theorem 1.10. Concretely,we show that there exists a fixed constant 0 such that, whenever (1/n) < 0, we can tileseveral random Erds-Rnyi graphs in order to construct a graph G that cannot be approximatedin nuclear norm by any matrix with o(n2/ log2 1) entries.",
  "Notably, Theorem 1.11 even applies to randomized algorithms": "It is interesting to ask if the same query complexity is optimal for spectral density estimation itself(Problem 1.1). Currently, the best query lower bound, due to a recent result of Jin et al. [JMSS23],is just (1/2). That work also proves a lower bound of (21/) under a more restrictive randomwalk query model (see .5). However, large gaps still remain in understanding the optimalquery complexity and running times for spectral density estimation. Addressing these gaps is anexciting direction for future work.",
  "Random Walk Query Model": "Finally, motivated by the SDE algorithms of Braverman et al. [BKM22] and Cohen-Steiner etal. [CKSV18], we consider a weaker graph access model than the adjacency query model. Thealgorithms in Braverman et al. [BKM22] and Cohen-Steiner et al. [CKSV18] assume access to Gvia random walks, i.e., that in O(k) time we can sample a random walk v0, v1, ..., vk, where v0is a uniformly random node in G and vi is chosen from the neighbors of vi1 with probabilityproportional to edge weight.",
  "returns a uniformly random vertex a V and if a has no neighbors, or an edge {a, b} selectedwith probability proportional to its weight, along with the degree of a and b": "While our algorithm discussed in .1 is not directly implementable in this more restrictivequery model, in , we present an alternative, randomized algorithm that achieves identicalquery and time-complexities in the model: Theorem 1.13. There is an algorithm (Algorithm 3) that, for any (0, 1), returns with probabil-ity 2/3 an O(n2)-sparse -additive nuclear sparsifier for any undirected weighted graph G usingO(n2) queries in the one-step random walk query model (Definition 1.12). We also obtain an algorithm for -additive spectral sparsification in the one-step random walk querymodel. We obtain the following via a natural application of matrix concentration. Specifically, wedirectly use an algorithm and theorem from Cohen et al. [Coh+17]. Theorem 1.14. There is an algorithm (Algorithm 4) that, for any (0, 1), returns with prob-ability 2/3 an O(n2 log n)-sparse -additive spectral sparsifier for any undirected weighted graphG using just O(n2 log n) queries in the one-step random walk query model (Definition 1.12). Theorem 1.14 achieves the stronger notion of -additive spectral sparsification, at the cost of anextra factor of log n in the sparsity, query complexity, and runtime as compared to Theorem 1.13.We show that this log n factor is unavoidable in the following sense:",
  "Theorem 1.15. For a fixed constant (0, 1/8) and c (0, 1) any algorithm requires (n log n)one-step random walk model queries to output an -additive spectral sparsifier with probability c": "Proof. For each i [n], let Xi be an independent Bernoulli random variable.We construct arandom bipartite graph on 2n nodes GX = (V, E) based on X def= {X1, . . . , Xn}. Let V = A Bdenote the vertices of GX, such that |A| = |B| = n. Let a1, . . . , an denote the vertices in A, andb1, . . . , bn denote the vertices in B. If Xi = 1 we add (ai, bi) to E. Let A denote the adjacency matrix of GX. Let I def= {i : (ai, bi) E}. The RandomNeighbor queryeither returns a vertex ai or bi for i [n] \\ I, or an edge (ai, bi) for i I. If the RandomNeighborquery returns a vertex ai (or bi) for i I, then (ai, bi) E. Therefore the RandomNeighbor querycorresponds to sampling a pair (ai, bi) for i [n]. The pair (ai, bi) for i [n] corresponds to couponsin the coupon collector problem. Suppose there exists an algorithm that takes T n log n log(1/c)n RandomNeighbor queries andoutputs a spectral sparsifier A of A such thatA A2 with constant probability c. Then, wewill show that we can solve the coupon collector problem in T n log n log(1/c)n queries withconstant probability c, which leads to a contradiction. This follows because we can determine all the edges in the graph GX, and hence all the coupons(ai, bi) for i [n], from A by taking x R2n indexed by ai and bi, for i [n]. We take x to be theall 0 vector except at two coordinates where it is 1, i.e., xai = xbi = 1, and xk = 0 for k = ai or bi.We get that if 3/4 x Ax 5/4 then (ai, bi) E and if 1/4 x Ax 2/4, then (ai, bi) E.",
  "Paper Organization and Preliminaries": "Paper Organization. presents our nuclear sparsification and SDE algorithms in theadjacency query model. and present our sparsity and query lower boundsfor nuclear sparsification. presents a lower bound against deterministic algorithms for-additive spectral sparsification. covers our results in the more restrictive one-steprandom walk model. shows how our algorithms for constructing a nuclear sparsifier canbe modified to obtain a graphical nuclear sparsifier (i.e., a matrix that is a nuclear sparsifier and isalso the normalized adjacency matrix of some graph.)",
  "and the Frobenius norm is AFdef=i,j A(i, j)2 ni=1 |i(A)|2. For matrices A, X Rmn we": "define the matrix inner-product, A, X def= i,j A(i, j)X(i, j).We use Loewner order notationA 0 or 0 A to denote that a symmetric A is positive semidefinite (PSD), i.e., that A hasnon-negative eigenvalues. A B denotes that A B is PSD. Other Notation.We let Ber(p) denote a Bernoulli random variable with parameter p, i.e.,Ber(p) = 1 with probability p and 0 otherwise.G(n, p) denotes the Erds-Rnyi model withparameter p, i.e., a graph distributed as G(n, p) has (i, j) E with probability p for every i < j [n],independently at random.",
  "Fact 1.17. For A, B Snn, if A B n, then W1(A, B)": "Proof. Let min be the smallest eigenvalue among the eigenvalues of A and B, so that AminI andB minI are both positive semidefinite, where I denotes an n n identity matrix. The singularvalues of A minI and B minI equal the eigenvalues of those matrices. So, we can directlyapply Mirskys singular value perturbation inequality [Mir60, Theorem 5] to conclude that:",
  "Additive Nuclear Sparsifiers": "In this section, we present our main algorithm (Algorithm 1) for constructing O(n2)-sparse -additive nuclear norm sparsifiers (Theorem 1.4). We first prove a structural result, which leads toour greedy sparsification procedure. Specifically, we show that the normalized adjacency matrixcan be sparsified by eliminating edges with small weights compared to the degree of its endpoints.",
  "deg(v). Because max {deg(v), deg(v)} deg(v), theloop executes for a superset of the edges in E that have v as an endpoint": "Next, we bound the runtime. There are n iterations of the for loop, each of which requires anO(1) time oracle call. Additionally, every iteration adds 2 non-zero entries to N at the cost of oneoracle call and other O(1) time operations. So, the total runtime is O(n + nnz( N)). As shown inTheorem 2.1, nnz( N) = O(n2). Consequently, the algorithm runs in O(n2). Note that the nuclear sparsifier N returned by Algorithm 1 is not guaranteed to be the normalizedadjacency matrix of any undirected graph. As access to a graphical sparsifier may be desirable, weshow how to modify Algorithm 1 to obtain a graphical nuclear sparsifier in Lemma 7.1.",
  "2n, then computing an": "2 accurate SDE for N immediately yields an accurate SDEfor NG. If we compute the SDE for N using the existing linear time method from Braverman et al.[BKM22], which runs in roughly O(nnz( N)1) time, then we immediately obtain our Theorem 1.8,i.e., that there is an O(n3) time randomized algorithm for approximating the spectral density ofany weighted graph G.",
  ". In particular, it is well known that, iftwo distributions p and q have the same first 2c": "(uncentered) moments for a fixed constant c,then W1(p, q) /2 [KV17; BKM22].9 So, given moments of an unknown distribution p (here,the spectral density of N), we can find a distribution approximating p in Wasserstein distanceby simply returning any distribution that (approximately) matches those moments.Formally,Braverman et al. [BKM22] show in their Lemma 3.4 that, given ps first 2c/ moments, a distributionapproximating p to /2 error Wasserstein distance can be found in poly(1/) time. Furthermore,by their Theorem B.1, this distribution can be converted to a uniform distribution over a set of napproximate eigenvalues in O(n + 1/) time.",
  "i=1ji = TrNj,": "where 1, . . . , n are Ns eigenvalues. As N is symmetric with at most 82 non-zeros per row/column(guaranteed by Theorem 2.1), for any positive integer k, Nk has at most (82)k non-zeros perrow/column.Thus, givenNk, the matrixNk+1 =Nk N can be computed in n(82)k+1 =n 2O(k log 1",
  "Sparsification Lower Bound for Erds-Rnyi Adjacency Matrices": "Our sparsity lower bound is based on showing that, with constant probability, all pairs of unnor-malized adjacency matrices in a sample of roughly 2O(n2) independent random Erds-Rnyi graphsare far in nuclear norm roughly n1.5 far. On the other hand, the set of s-sparse matrices can bewell-approximated in nuclear norm by a finite set of size approximately 2s. So, if s = o(n2), thereare simply not enough choices of s-sparse matrices to approximate all graphs in our sample.",
  "In the proof of this result, we leverage the following (simplified) matrix concentration result toshow that two Erds-Rnyi random graphs tend to be far apart in the nuclear norm": "Fact 3.1 (Guionnet and Zeitouni [GZ00], Theorem 1.1 with f(x) = |x|). Let C Rnn be a fixedsymmetric matrix whose entries have magnitude at most 1. Let X Rnn be a random symmetricmatrix with Xij = Cijij, where ij = ji, and {ij : 1 i j n} are independent randomvariables supported on . Then, for any > 16/n,",
  "Moreover, since = 2/n, for any B Bc, we have a matrix S Sc with S B2F n(/2)2 1/n.It follows that S B n S BF 1/n": "With the above lemmas in place, we are now ready to prove Theorem 3.4, which shows that,with good probability, the unnormalized adjacency matrix of an Erds-Rnyi graph cannot beapproximated well in the nuclear norm by sparse matrices. Theorem 3.4. Let A be the (unormalized) adjacency matrix of an Erds-Rnyi random graphG(n, 1/2), where n C for a universal constant C. For a universal constant c, with probability atleast 3/4, there is no matrix B with nnz(B) cn2/ log2 n such that A B n1.5/500.",
  "Our goal is to show that a large fraction of A1, . . . , Am cannot be well-approximated by a matrixwith cn2/ log2 n non-zero entries": "Specifically, suppose for z matrices Aj1, . . . , Ajz, there are matrices B1, . . . , Bz, each with at mostcn2/ log2 n non-zero entries, such that Bi Aji n1.5/500. Observe that, since Aji Ajk >n1.5/200 for all i, k [z], by triangle inequality, we have that BiBk > n1.5/1000 for all i, k [z].Moreover, we know that every entry in each Bi is bounded in magnitude by n1.5. If it was not, thensince every entry in Aji is bounded by 1, we would have Aji Bi Aji BiF n1.5 1. Then, applying Lemma 3.3, for every Bi, i [z], there is a matrix Si Sc such that Si Bi 1/n n1.5/2000. By triangle inequality, since Bi Bk > n1.5/1000 for all i, k [z], it mustbe that Si = Sk for all i, k [z]. Accordingly, we have that z |Sc|. So at most |Sc| of the mmatrices from A1, . . . , Am can be approximated by a matrix with cn2/ log2 n non-zeroes, where |Sc|is as defined in Lemma 3.3.",
  "Proof of Theorem 1.10": ".1 proves the Theorem 1.10 for the special case of = (1/n), but for unnormalizedadjacency matrices.In this section, we extend the Theorem 3.4 for larger values of and fornormalized adjacency matrices to prove the Theorem 1.10. Our lower bound construction followsby considering a block-diagonal matrix of size n n, with n/b 1 many b-by-b blocks and eachblock satisfying the properties in Theorem 3.4.",
  "In the following lemma, we show that the nuclear norm of a block diagonal matrix is at least thesum of the nuclear norms of the blocks of the matrix": "Lemma 3.5 (Block-wise Nuclear Norm Bound). For any symmetric A Rnn and S1, . . . , Sk [n]that partition [n], i.e. i[k]Si = [n] and Si Sj = for all i = j, it is the case that A ik ASi,Si, where ASi,Si denotes the principal submatrix of A indexed by Si. Proof. By Fact 1.16 we know that there are symmetric Xi S|Si||Si| for all i [k] such thatXi2 1 and ASi,Si, Xi = ASi,Si for all i [k]. Let X Snn be the block-diagonal matrixdefined as XSi,Si = Xi for all i [k] and all other entries of Y set to 0. Note that for all x Rn,",
  "Next, we show that if the blocks of a block diagonal matrix cannot be well approximated by a sparsematrix, then the block diagonal matrix also cannot be well approximated by a sparse matrix": "Corollary 3.6 (Corollary of Lemma 3.5). Let n, k, b Z0 be such that k = n/b 1. LetE Rnn be a block diagonal matrix E = diag(E1, E2, . . . , Ek, R), where E1, . . . , Ek Rbb and Ris an arbitrary matrix of size (n kb). For any pair of constants , > 0, let Ei, for each i [k],be such that for any sparse matrix B with nnz(B) , Ei B > . Then, for any matrix Bwith nnz(B) (k/2) , we have that E B > n/(4b).",
  "Proof. Let B be any n n matrix with nnz(B) (k/2) . For i [k], define Bi as the submatrixof B with entries corresponding to Ei.Let I def= {i [k] : nnz(Bi) } be the index set of": "sparse blocks Bi.We have that |I| k/2.This is because if |I| < k/2, then nnz(B) i[k]\\I nnz(Bi) > (k/2) , leading to a contradiction. Therefore, for each i I, Ei Bi .Therefore, by Lemma 3.5, we have that E B (k/2) n/(4b). Using Corollary 3.6, we get the following lower bound on the sparsifiability of normalized adjacencymatrices in the nuclear norm by setting b = O1/2 and noting that (from Theorem 3.4) Erds-Rnyi random graphs satisfy the properties of the matrices E1, . . . Ek in Corollary 3.6. Theorem 1.10 (Sparsity Lower Bound). For any 0, where 0 (0, 1) is a fixed constant andany integer n 1/2, there is a graph G on n nodes, with normalized adjacency matrix NG, suchthat any matrix M satisfying NG M n must have (n2/ log2 1) non-zero entries. Proof. Let G1, . . . , Gk denote the Erds Rnyi random graphs of size b C from Theorem 3.4. LetA1, . . . , Ak denote the adjacency matrix of G1, . . . , Gk respectively. We define the graph G to be agraph of size n which is a disjoint union of G1, . . . , Gk, where k = n/b 1, and an arbitrary graphwith adjacency matrix R on (n kb) vertices, such that the degree of each of the (n kb) nodes isat least 1. Let AG denote the adjacency matrix of G. We get that AG = diag(A1, A2, . . . , Ak, R).Note that the matrix AG satisfies the conditions of the matrix E from Corollary 3.6, with Ei = Ai,i [k], = cb2/ log2 b, and = b1.5/500. This is true because Theorem 3.4 guarantees that, foreach i [k], any matrix B with nnz(B) cb2/ log2 b, Ai B > cb1.5, where c = 1/500.",
  "Nuclear Sparsification Query Complexity Lower Bound": "In this section, we prove Theorem 1.11, which asserts that our O(n/2) query algorithm from also achieves optimal query complexity for nuclear approximation, up to polylogarithmicfactors in n. Interestingly, our lower bound applies in an even more general query model whichsupports edge queries in addition to neighbor queries. We formalize this model below: Definition 4.1 (Generalized adjacency query model). We say we have generalized adjacency queryaccess to a weighted graph G = (V, E, w) if, in addition to the access under the adjacency querymodel (Definition 1.3), there is an O(1) time procedure, GetEdge(u, v) that when queried with anyu, v V returns True if {u, v} E and False otherwise. Our main result in this section is that any deterministic algorithm that makes o(n2/ log2 1)queries to the generalized adjacency query model (Definition 4.1) is unable to distinguish betweena random Erds-Rnyi graph and the graph which is the complement restricted to the edges whichwere not queried. Theorem 1.10 then follows by an application of Yaos principle. In the remainder of the section, we let QALG(G) denote the sequence of queries a deterministicalgorithm ALG makes to the generalized adjacency query model on input G. We let EALG(G) todenote the set of edges revealed by these queries. That is, for each query of the form GetEdge(u, v) QALG(G), we include {u, v} EALG(G); meanwhile, for each query of the form GetNeighbor(a, i) QALG(G), we include the edge {a, b} returned by the generalized adjacency query model (if any) inEALG(G). Our formal result is as follows: Lemma 4.2. There exists an 0 (0, 1) and an absolute constant C such that for any (0, 0),there exists a distribution q over graphs on n C2 nodes such that the following holds: let ALGbe any deterministic algorithm that on any n-node unweighted graph G = (V, E) makes at most|QALG(G)| Cn2/ log2 1 queries to the generalized adjacency query model (Definition 4.1) foran absolute constant C and outputs some XALG(G) Rnn. Then",
  "Proof. We describe a distribution q over graphs on n = 2kb n0 vertices, where b = C2 for asufficiently small constant C and k = n0/2b": "The hard distribution:Specifically, the vertex set of our graph will be split into 2k sets of size b,which we denote by V 1,1, V 1,2, V 2,1, V 2,2, . . . , V k,1, V k,2. For r [k], let vr,1jand vr,2jdenote the j-thvertex in V r,1 and V r,2, respectively. For each i = j [b] and r [k], we draw Xi,j,r Ber(1/2).If Xi,j,r = 1, we add the edges {vr,1i , vr,1j }, {vr,2i , vr,2j } to E.If Xi,j,r = 0, we add the edges",
  "Indistinguishable graph pairs:For any realization G = (V, E) q, let G = (V, E) denote thecomplement of G. That is, for any nodes u, v, (u, v) E if and only if (u, v) / E. Observe that G": "is itself distributed as q. Additionally, define G = (V, E) as follows: Initialize E = E. Then, for alli, j, r, if either {vr,1i , vr,2j } or {vr,2i , vr,1j } are in EALG(G), then delete both {vr,1i , vr,1j } {vr,2i , vr,2j } fromE and add {vr,1i , vr,2j } and {vr,2i , vr,1j }; likewise if either {vr,1i , vr,1j } or {vr,2i , vr,2j } are in EALG(G),then delete both {vr,1i , vr,2j } and {vr,2i , vr,1j } from E and add {vr,1i , vr,1j } and {vr,2i , vr,2j }. In other",
  ". For any G, QALG(G) = QALG( G), and consequently XALG(G) = XALG( G)": "The last claim follows from the fact that, not only are G and G identical on any edge accessed by aGetEdge or GetNeighbor query, but since both graphs are regular with the same degree, the degreeinformation returned by any GetNeighbor also matches. With the above claims in place, it will suffice to show that, for any deterministic algorithm thatmakes a small number of queries, G and G are far in the nuclear norm. Accordingly, ALG cannotwell approximate both graphs simultaneously. Concretely, we will show that there exists a setH supp(q) such that PGq {G H} 17/32 and, for every G H,",
  "nnz(P) 4 |QALG(G)| Cn2/ log2(1) ckb2/ log2(b)": "for any fixed constant c. This is because each query in QALG(G) corresponds to at most one edgein EALG(G) and therefore corresponds to at most two edge changes (one edge deleted from and oneedge added) to G relative to G and correspondingly, corresponds to four entries modified in theadjacency matrix of G relative to G. Consequently, by Corollary 3.6 and (6), we have that:AG",
  "With Lemma 4.2 in place, our main query complexity lower bound for nuclear sparsification followsdirectly from Yaos min-max principle": "Theorem 1.11 (Query Lower Bound). For any 0 and any integer n C/2 where 0 (0, 1)and C > 1 are fixed constants, any algorithm that returns an -additive nuclear sparsifier for anyinput G with probability 3/4 requires (n2 log2(1)) GetNeighbor queries. Proof. Any randomized algorithm that makes at most Q Cn2 log2(1) for a fixed constantC can be formulated as a distribution over deterministic algorithms that each makes at most Qqueries. As such, if a randomized algorithm succeeds with probability > 3/4 on inputs from thehard distribution q guaranteed by Lemma 4.2, there must be a deterministic algorithm that succeedswith probability > 3/4 and uses an equal number of queries. Such an algorithm is ruled out byLemma 4.2, so the theorem follows by contradiction.",
  "In this section, we discuss query lower bounds for deterministic algorithms that produces an -additive spectral sparsifiers (Definition 1.6). Concretely, here we prove an (n2) lower bound on": "(a) Any edge included in E will always be one of the four edges depicted above for some k, j [m]. For eachk, j [m], G1 will contain the edges depicted here between the vertices {vki , vjt }i,t and these will be theonly edges incident to these 8 vertices. (b) If for some k, j [m], E does not contain the edges depicted in a, then G2 will contain theedges depicted here between the vertices {vki , vjt }i,t and these will be the only edges incident to these 8vertices.",
  ": Illustration of construction of E, G1 and G2 in the proof of Theorem 5.1": "the number of queries to the generalized adjacency query model required by any deterministic additive spectral norm approximation algorithm on undirected graphs. In this section, for a graphG = (V, E), we use LGdef= I NG to denote its normalized Laplacian. Our proof proceeds by constructing a resisting oracle. We will show that whenever the deterministicalgorithm makes o(n2) queries to the generalized adjacency model, there exist two graphs G1, G2on n nodes that have constant nuclear norm distance, along with an oracle (Algorithm 2) that issimultaneously a valid implementation of the generalized adjacency query model (Definition 4.1) onboth G1 and G2. Consequently, no deterministic algorithm that makes o(n2) queries to a generalizedadjacency model can distinguish between G1 and G2 and therefore must fail to output a nuclearsparsifier on either G1 or on G2. Theorem 5.1. Let XALG(G) be the output of any deterministic algorithm ALG which, given aninput graph G on n nodes, makes at mostn2128 queries to the generalized adjacency query model(Definition 4.1). Then, for any m 1 and n 4m, there exists an undirected graph G on n nodesalong with a vector v RV , such thatv LGv XALG(G) > 1",
  "Proof. Set V = V1 V2 V3 V4 where |Vi| = m. We use vri to denote the r-th vertex in Vi. LetT [n2], and consider the resisting oracle defined in Algorithm 2, and let E = A(T) (see Lines 1,9, and 15)": "Let E1 = {{vk1, vj2}, {vk3, vj4} : {vki , vjt } / E, i, t } and E2 = {{vk1, vj3}, {vk2, vj4} : {vki , vjt } /E, i, t }. Let G1 = (V, E1 E) and G2 = (V, E2 E). See for an illustration. Notethat G1, G2 are both m-regular by construction. Moreover, by construction, Algorithm 2 is a validimplementation of the generalized adjacency query model (Definition 4.1) on both G1 and G2.Hence XALG(G1) = XALG(G2).",
  "Sparsification in the Random Walk Model": "In this section, we show how our notion of nuclear sparsification (and indeed, -additive spectralsparsification) can also be achieved in the one-step random walk model (Definition 1.12), whichis a more restrictive query model than the one considered by Braverman et al. [BKM22].In.2, we also present a lower bound that separates the query complexity achievable forspectral sparsification from what is achievable with nuclear sparsification.",
  "Algorithms in the Random Walk Model": "We begin by presenting algorithms for sparsification under the one-step random walk query model.In Theorem 1.13, we show that by taking T = O(n2) in Algorithm 3, with constant probability,we can obtain a nuclear sparsifier. In Theorem 1.14, we show that by taking T = O(n2 log(n))in Algorithm 4, we can obtain an -additive spectral sparsifier. Theorem 1.13. There is an algorithm (Algorithm 3) that, for any (0, 1), returns with probabil-ity 2/3 an O(n2)-sparse -additive nuclear sparsifier for any undirected weighted graph G usingO(n2) queries in the one-step random walk query model (Definition 1.12).",
  "Setting T = 3n2 suffices to succeed with probability 2/3": "In the following Theorem, we note that by using a result of Cohen et al. [Coh+17] we can alsoachieve the stronger notion of -additive spectral sparsification in the one-step random walk accessmodel (Definition 1.12.) Theorem 6.1 (Theorem 3.9 (Simplified), [Coh+17]). Let A Rnn be a symmetric matrix whereno row or column is all zeros. Let , p (0, 1). Let r = A1 where 1 is the all ones vector. Let Eijdenote a matrix whose (i, j)-th entry is 1 and rest of the entries are zero, and D be a distributionover Rnn such that X D takes value",
  "PR1/2( A A)R1/22 p": "Using the above theorem, we show that with an additional multiplicative O(log(n))-factor in thenumber of queries, the Algorithm 4 we can obtain the stronger notion of -additive spectral sparsifierin the one-step random walk access model (Definition 1.12.) Theorem 1.14. There is an algorithm (Algorithm 4) that, for any (0, 1), returns with prob-ability 2/3 an O(n2 log n)-sparse -additive spectral sparsifier for any undirected weighted graphG using just O(n2 log n) queries in the one-step random walk query model (Definition 1.12).",
  "Separation Between Spectral and Nuclear Sparsification": "In this section, we show that there exists a constant c such that any algorithm for constructingan c-additive spectral sparsifier must make at least (n log n) queries to the one-step random walkgraph query oracle. Our result relies on the following classical result on the sample complexity forthe coupon collector problem [MU17]. Lemma 6.2 (Coupon Collector). Consider a collection of n 1 different coupons from whichcoupons are drawn independently, with equal probability, and with replacement. Let T be a randomvariable which denotes the number of trials needed to see all the n different coupons. Then, for anyconstant c > 0P [T n log n cn] exp(c) .",
  "Therefore, any algorithm that takes T n log n log(1/c)n oracle queries, fails to produce aspectral sparsifier, with probability greater than c": "Note that for the random graphs GX considered in the proof of Theorem 1.15, the degree of eachnode is at most 1; consequently, on these random graphs, the k-step random walk query model is nomore informative than a 1-step random walk model. Therefore, the lower bound of Theorem 1.15holds for the k-step query model as well.",
  "From Nuclear Sparsifiers to Graphical Nuclear Sparsifiers": "Throughout this paper, we have constructed nuclear sparsifiers that are matrices (Definition 1.2)but may not necessarily be the normalized adjacency matrices of any graph. In this section, weprove the following lemma, which shows how to convert our nuclear sparsifiers into an graphicalnuclear sparsifier (i.e., a nuclear sparsifier which is the normalized adjacency matrix of some graph)using no additional queries and only O(n/2) additional runtime. We leverage the fact that a closeranalysis of the proofs of Theorems 2.1 and 1.13 actually achieve an stronger notion of sparsificationthan nuclear sparsification (Definition 1.2) in that they output -additive nuclear sparsifiers Msuch that M NG2 M NG2F n2.",
  "In the following, for a matrix A Rnn, we let A[1:j] denote the leading principal submatrix oforder j": "Lemma 7.1. Let (0, 1) and G = (V, E, w) be a graph on n nodes. Without loss of generality,assume the vertices are ordered such that deg(i) deg(j) for i < j. Let M Rnn be an matrixsuch that NG M2F n2. Let Q def= D1/2G MD1/2Gand e Rn1 be a vector with eidef= deg(i) n1j=1 (Q)i,j. Let G be a graph with adjacency and degree matrix",
  "Acknowledgements": "Yujia Jin and Ishani Karmarkar were supported in part by NSF CAREER Grant CCF-1844855,NSF Grant CCF-1955039, and a PayPal research award. Yujia Jins contributions to the projectoccurred while she was a graduate student at Stanford. Christopher Musco was partially supportedby NSF Grant CCF-2045590. Aaron Sidford was supported in part by a Microsoft Research FacultyFellowship, NSF CAREER Grant CCF-1844855, NSF Grant CCF-1955039, and a PayPal researchaward. Apoorv Vikram Singh was partially supported by NSF Grant CCF-2045590.",
  "[BH16]Afonso S. Bandeira and Ramon van Handel. Sharp nonasymptotic bounds on thenorm of random matrices with independent entries. In: The Annals of Probability44.4 (2016), pp. 24792506": "[BGKS20]Jess Banks, Jorge Garza-Vargas, Archit Kulkarni, and Nikhil Srivastava. Pseu-dospectral Shattering, the Sign Function, and Diagonalization in Nearly Matrix Mul-tiplication Time. In: 61st Annual IEEE Symposium on Foundations of ComputerScience (FOCS). 2020. [BSS12]Joshua Batson, Daniel A. Spielman, and Nikhil Srivastava. Twice-Ramanujan spar-sifiers. In: SIAM Journal on Computing 41.6 (2012). Preliminary version in the 41stAnnual ACM Symposium on Theory of Computing (STOC), pp. 17041721.",
  "[BKM14]Petra Berenbrink, Bruce Krayenhoff, and Frederik Mallmann-Trenn. Estimatingthe number of connected components in sublinear time. In: Information ProcessingLetters 114.11 (2014), pp. 639642": "[Bha+23]Rajarshi Bhattacharjee, Gregory Dexter, Petros Drineas, Cameron Musco, and ArchanRay. Sublinear Time Eigenvalue Approximation via Random Sampling. In: Pro-ceedings of the 50th International Colloquium on Automata, Languages and Program-ming (ICALP). 2023, 21:121:18. [Bha+24]Rajarshi Bhattacharjee, Gregory Dexter, Cameron Musco, Archan Ray, SushantSachdeva, and David P. Woodruff. Universal Matrix Sparsifiers and Fast Deter-ministic Algorithms for Linear Algebra. In: Proceedings of the 15th Conference onInnovations in Theoretical Computer Science (ITCS). 2024.",
  "[CRT05]Bernard Chazelle, Ronitt Rubinfeld, and Luca Trevisan. Approximating the mini-mum spanning tree weight in sublinear time. In: SIAM Journal on computing 34.6(2005), pp. 13701379": "[CTU21]Tyler Chen, Thomas Trogdon, and Shashanka Ubaru. Analysis of stochastic Lanczosquadrature for spectrum approximation. In: Proceedings of the 38th InternationalConference on Machine Learning (ICML). 2021. [Coh+17]Michael B. Cohen, Jonathan Kelner, John Peebles, Richard Peng, Anup B Rao,Aaron Sidford, and Adrian Vladu. Almost-linear-time algorithms for markov chainsand new spectral primitives for directed graphs. In: Proceedings of the 49th AnnualACM Symposium on Theory of Computing (STOC). 2017, pp. 410419. [CKSV18]David Cohen-Steiner, Weihao Kong, Christian Sohler, and Gregory Valiant. Ap-proximating the Spectrum of a Graph. In: Proceedings of the 25th ACM SIGKDDInternational Conference on Knowledge Discovery and Data Mining (KDD). 2018,pp. 12631271. [Czu+03]Artur Czumaj, Funda Ergn, Lance Fortnow, Avner Magen, Ilan Newman, RonittRubinfeld, and Christian Sohler. Sublinear-time approximation of Euclidean mini-mum spanning tree. In: Proceedings of the 14th Annual ACM-SIAM Symposium onDiscrete Algorithms (SODA). 2003, pp. 813822.",
  "[CS10]Artur Czumaj and Christian Sohler. Sublinear-time Algorithms. In: Property Test-ing: Current Research and Surveys. Springer Berlin Heidelberg, 2010, pp. 4164": "[DBB19]Kun Dong, Austin R Benson, and David Bindel. Network density of states. In:Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Dis-covery and Data Mining (KDD). 2019, pp. 11521161. [ERS18]Talya Eden, Dana Ron, and C Seshadhri. On approximating the number of k-cliquesin sublinear time. In: Proceedings of the 50th Annual ACM Symposium on Theoryof Computing (STOC). 2018, pp. 722734. [EG17]Nicole Eikmeier and David F. Gleich. Revisiting Power-Law Distributions in Spectraof Real World Networks. In: Proceedings of the 23rd ACM SIGKDD InternationalConference on Knowledge Discovery and Data Mining (KDD). 2017, pp. 817826.",
  "[FDBV01]Ills J Farkas, Imre Dernyi, Albert-Lszl Barabsi, and Tamas Vicsek. Spectra ofreal-world graphs: Beyond the semicircle law. In: Physical Review E 64.2 (2001),p. 026704": "[GKX19]Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An Investigation into Neu-ral Net Optimization via Hessian Eigenvalue Density. In: Proceedings of the 36thInternational Conference on Machine Learning (ICML). 2019. [Glu+21]Grzegorz Gluch, Michael Kapralov, Silvio Lattanzi, Aida Mousavifar, and ChristianSohler. Spectral clustering oracles in sublinear time. In: Proceedings of the 32ndAnnual ACM-SIAM Symposium on Discrete Algorithms (SODA). 2021, pp. 15981617. [GR11]Oded Goldreich and Dana Ron. On Testing Expansion in Bounded-Degree Graphs.In: Studies in Complexity and Cryptography. Miscellanea on the Interplay betweenRandomness and Computation. Springer Berlin Heidelberg, 2011, pp. 6875.",
  "[Hut90]Michael F. Hutchinson. A stochastic estimator of the trace of the influence matrixfor Laplacian smoothing splines. In: Communications in Statistics-Simulation andComputation (1990)": "[JRT23]Arun Jambulapati, Victor Reis, and Kevin Tian. Linear-Sized Sparsifiers via Near-Linear Time Discrepancy Theory. In: Proceedings of the 35th Annual ACM-SIAMSymposium on Discrete Algorithms (SODA). 2023, pp. 51695208. [JMSS23]Yujia Jin, Christopher Musco, Aaron Sidford, and Apoorv Vikram Singh. Moments,Random Walks, and Limits for Spectrum Approximation. In: Proceedings of the 36thAnnual Conference on Computational Learning Theory (COLT). 2023, pp. 53735394. [Kap+17]Michael Kapralov, Yin Tat Lee, Cameron Musco, Christopher Musco, and AaronSidford. Single Pass Spectral Sparsification in Dynamic Streams. In: SIAM Journalon Computing 46.1 (2017), pp. 456477.",
  "[NS10]Asaf Nachmias and Asaf Shapira. Testing the expansion of a graph. In: Informationand Computation 208.4 (2010), pp. 309314": "[ORRR12]Krzysztof Onak, Dana Ron, Michal Rosen, and Ronitt Rubinfeld. A near-optimalsublinear-time algorithm for approximating the minimum vertex cover size. In:Proceedings of the 23rd Annual ACM-SIAM Symposium on Discrete Algorithms(SODA). 2012, pp. 11231131. [PSG18]Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli. The emergence of spec-tral universality in deep networks. In: Proceedings of the 21st International Confer-ence on Artificial Intelligence and Statistics (AISTATS). 2018."
}