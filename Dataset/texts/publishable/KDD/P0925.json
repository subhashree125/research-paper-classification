{
  "ABSTRACT": "We introduce the Robustness of Hierarchically Organized TimeSeries (RHiOTS) framework, designed to assess the robustness ofhierarchical time series forecasting models and algorithms on real-world datasets. Hierarchical time series, where lower-level forecastsmust sum to upper-level ones, are prevalent in various contexts,such as retail sales across countries. Current empirical evaluationsof forecasting methods are often limited to a small set of benchmarkdatasets, offering a narrow view of algorithm behavior. RHiOTSaddresses this gap by systematically altering existing datasets andmodifying the characteristics of individual series and their interre-lations. It uses a set of parameterizable transformations to simulatethose changes in the data distribution. Additionally, RHiOTS incor-porates an innovative visualization component, turning complex,multidimensional robustness evaluation results into intuitive, easilyinterpretable visuals. This approach allows an in-depth analysis ofalgorithm and model behavior under diverse conditions. We illus-trate the use of RHiOTS by analyzing the predictive performanceof several algorithms. Our findings show that traditional statisticalmethods are more robust than state-of-the-art deep learning algo-rithms, except when the transformation effect is highly disruptive.Furthermore, we found no significant differences in the robustnessof the algorithms when applying specific reconciliation methods,such as MinT. RHiOTS provides researchers with a comprehen-sive tool for understanding the nuanced behavior of forecastingalgorithms, offering a more reliable basis for selecting the mostappropriate method for a given problem.",
  "ACM Reference Format:Luis Roque, Carlos Soares, and Lus Torgo. 2024. RHiOTS: A Framework forEvaluating Hierarchical Time Series Forecasting Algorithms. In Proceedings": "Permission to make digital or hard copies of part or all of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for third-party components of this work must be honored.For all other uses, contact the owner/author(s).KDD 24, August 2529, 2024, Barcelona, Spain 2024 Copyright held by the owner/author(s).ACM ISBN 979-8-4007-0490-1/24/08",
  "INTRODUCTION": "In time series forecasting, modeling inter-temporal dependenciesbetween observations is a crucial task to capture the dynamics of theunderlying process. Furthermore, considering cross-series informa-tion, i.e., the relationship between multiple related time series, canenhance the performance of traditional univariate algorithms .Such relationships occur in many real-world situations and oftenrepresent hierarchical structures, such as in data concerning GrossDomestic Product , epidemics , and sales demand . Co-herent forecasts , which satisfy these hierarchical relations,are often required in such applications. To improve performance,state-of-the-art algorithms for hierarchical time series (HTS) fore-casting rely on both the autocorrelation of each time series and thecross-series correlations .An important characteristic of HTS forecasting algorithms istheir robustness to changes in time series relationships, such asvariations in seasonality, trends, cross-series dependencies, andvolatility. These phenomena significantly impact predictive per-formance and may lead to inaccurate forecasts and suboptimaldecision-making. Nevertheless, conventional evaluation method-ologies, typically based on a small number of datasets and metrics,cannot assess the robustness of HTS forecasting algorithms indynamic scenarios where relationships between time series andtemporal dependencies change over time. We note that the con-cerns about the adequacy of evaluation methods are not limitedto HTS forecasting. In fact, there is a growing discussion on thelimitations of empirical evaluations in time series analysis (e.g.,anomaly detection ). We argue that similar issues can be raisedfor time series forecasting since both suffer the exact root cause:most papers evaluate algorithms on one or more of a handful ofpopular benchmark datasets (e.g. ).This work introduces the Robustness of Hierarchically Orga-nized Time Series (RHiOTS) framework. RHiOTS is designed toevaluate the robustness of HTS forecasting models and algorithmsfacing real-world data distribution changes. Our framework system-atically quantifies the robustness by imposing controlled realistictransformations, mimicking common patterns observed in actualdatasets. These semi-synthetic datasets retain essential characteris-tics of the original data while introducing new dynamics, servingas a robust baseline to assess algorithmic performance against datavariations. RHiOTS has an innovative visualization component,",
  "KDD 24, August 2529, 2024, Barcelona, SpainLuis Roque, Carlos Soares, and Lus Torgo": ": The chart on the left shows the performance of forecasting algorithms against multiple transformations for theTourism dataset. The chart on the right averages the performance ranks of forecasting algorithms for all datasets. metrics presented in the table do not assess the robustness of thedifferent algorithms.Still, there are some insights that we can extract from this typeof standard comparison. First, and contrary to our expectations,the ETS model does not show meaningful differences when usingthe naive reconciliation strategy or MinT. The global deep learningmodels performed well on the bottom series, especially TFT, but noton the aggregate-level ones. The GPHF model consistently producesgood predictions for aggregated levels, but it is less accurate on thebottom level ones. Once again, can we generalize these results, orare they the result of our experimental setup?After using RHiOTS across all datasets and averaging the results,we can more confidently answer Q6 and Q7 for all the algorithms.We dont see relevant differences between ETS-BU and ETS-MinT,and thus, using hierarchical information does not seem to produceincreased robustness. On the predictive performance of algorithms,we consistently observe simpler algorithms (classical ones) yieldingbetter results across transformations than more complex ones. Theradar chart on the right in presents the average ranks ofvarious forecasting algorithms across all datasets and controlledby transformation. It serves as a generalization of the chart onthe left of the same figure. The chart shows that deep learningalgorithms employing BU strategies, such as DeepAR and TFT,often underperform other approaches. The only exception is whenfaced with high-intensity magnitude warping. This is expected, ascomplex algorithms are better equipped to handle such behavior.The underperformance of these algorithms is not simply due to theirnaive reconciliation strategies, as evidenced by the resilience of theETS + BU algorithm. GPHF ranks between deep learning algorithmsand classical ones. Classical approaches, ETS + BU and ETS + MinT",
  "BACKGROUND AND NOTATION": "We are working with a collection of related univariate time series,Z = {z, N, = 1, . . . ,}. The training values can be writtenas z1: = where R denotes the value of timeseries at time and represents the last training point. When theinterpretation is unambiguous and to simplify the notation in spe-cific sections, we refer to z as the observed time series. The trainingrange is denoted by {1, 2, ..., }, while { + 1, + 2, . . . , +} is theprediction range and is the forecast horizon. Point predictions aredefined as z+1:+.HTS organizes a collection of time series, represented as z,within a hierarchical structure denoted by . The hierarchy isa tree-like structure, where each node represents a group or a",
  ": Simple example of a hierarchically organized timeseries dataset that comprises sales data from a retailer in theUS": "time series z. The root of the tree represents the total aggregate ofall the time series in the dataset, denoted as z. The leaf nodes ofthe tree represent individual time series z. Each time series z isassociated with one or more groups in the hierarchy. Thus, wecan write 1, ..., where z = z. The subset of groupsa time series z belongs to is denoted by = { : }. denotesthe set of all groups, and its cardinality is denoted by ||.We illustrate this concept in using an example from aretailer operating in the United States. It consists of two groups:The first, denoted by , represents the states in the United States(US), with elements and , representing California (CA) and NewYork (NY), respectively. The second group, denoted by , representsproduct categories and has elements and , corresponding totrousers and t-shirts, respectively. At the bottom level, the hierar-chy would generate four distinct time series: z , z , z , and z .Each non-leaf node (group ) in the hierarchy represents the ag-gregation of the time series associated with its children nodes. Forinstance, if represents the group of all stores in different states ofthe US, and z represents the sales data of an individual product ina specific store, then the time series at node is z, = z z.Similarly, the time series for each element of is z = z.The objective of HTS forecasting is twofold: firstly, to minimizethe forecast error for each series within the hierarchy, and sec-ondly, to ensure that forecasts at all levels of the hierarchy areconsistent with each other. Formally, the forecast error for a se-ries over a prediction horizon is defined as +1:+ = { = | { + 1, + 2, . . . , + }}, where denotes the fore-casted value and denotes the actual value at time . The con-sistency constraint requires that for any aggregated level withinthe hierarchy, represented by a group , the aggregated forecastsz+1:+ = z+1:+, should equal the sum of forecastedvalues for all time series belonging to that group. This ensuresthat the forecasts respect the hierarchical structure, maintainingintegrity and coherence across all levels of aggregation within thedataset.",
  "( 1)=2 | 1|": "It is equally important to assess the forecasting performance ataggregated levels. Hence, we also compute the MASE metric forevery group element . This is achieved by aggregating the timeseries at the bottom level that belongs to a specific group element.For example, for group element in our previous example, the ag-gregated time series z is calculated as z = z +z . Subsequently,the MASE metric for z is computed.To evaluate the forecast accuracy at the group level, we calculatethe average MASE across all elements that belong to the group. Forexample, for group , we would consider z and z . First, we definethe aggregated time series for each group as z = : z.Then, we compute the MASE for each of these aggregate timeseries z and finally take the average across all in the group , =1| | .At the most aggregated level (the top of the hierarchy), we sumthe values of all the bottom-level time series and evaluate the pre-dictive performance based on these values.Analyzing the distance between time series is not a trivial task.We adopt Dynamic Time Warping (DTW) following the approachrecommended by for short time series. DTW evaluates se-quence similarity by computing a matrix of distances betweenelements and determining the optimal alignment path that mini-mizes the total distance (Eq. 1). This alignment, or warp path, ismathematically formulated as:",
  "(1)": "where an alignment path of length is a sequence of index pairs((0, 0), ..., (1, 1)) and (z, z) is the set of all admissiblepaths.In terms of time series transformations, we denote them by thefunction T, to the original time series z. Such transformations canmodify individual components of a time series and alter the inter-relations among multiple time series within the dataset. Differenttransformation functions will have different sets of parameters. Forsimplicity of notation, we introduce the concept of a parameter set, representing a specific parameter set used in the transformation.Formally, we denote the resulting transformed series as z,, wherez, = T (z,), and symbolizes the particular parameter set .We denote the resulting dataset as Z,,, where {1, 2, . . . , }denotes the transformation applied, is the set of parameters used,and represents the number of samples generated from the sametransformation and set of parameters.",
  "RELATED WORK": "Hierarchical Time Series Forecasting Methods. Univariatemethods like ARIMA and ETS are common for individual timeseries analysis and can be used for HTS when using simple rec-onciliation strategies . General approaches like those found in are methods used to forecast a set of time seriesfrom the same domain. It means that they are capable of learningrelationships among time series. However, these approaches do notincorporate any hierarchical structure, missing out on leveraging itto improve forecasting accuracy.Alternatively, several methods explicitly model the hierarchi-cal nature of HTS datasets. Early approaches by , followedby refinements in , involve fitting and reconciling indepen-dent forecasts at all hierarchy levels. Non-linear models (e.g., )improve their ability to capture complex patterns in the data byemploying optimization and regularization techniques during themodel training process. Another approach uses Gaussian Processesand does not require any reconciliation since the hierarchical struc-ture is an input to the model itself .Evaluating Hierarchical Time Series Forecasting. The eval-uation of HTS forecasting models has received limited attention inrecent literature. Exceptions. For example, and , compare theperformance of state-of-the-art reconciliation models on a limitedset of real-world datasets. Despite considering various forecastingmodels, the fact that the experimental setup does is so limited anddoes not provide rich data variations, our ability to generalize islimited. Recently, a practical guide addressing concepts like scale,units, sparsity, forecast horizon, multiple evaluation windows, anddecision context has been proposed . However, the guide fallsshort in objectively quantifying performance differences betweenmodels beyond basic accuracy assessment on benchmarks.Time Series Augmentation. Data augmentation involves gen-erating synthetic data that covers unexplored input space whilepreserving correct labels . This technique has proven effective indomains such as computer vision, where methods like AlexNet have leveraged augmented data for image classification. On theother side, the unique properties of time series data, such as itstemporal dependencies and intricate dynamics, create a set of chal-lenges.Autoregressive models, while effective for linear, stationary timeseries, face limitations with complex data and computational de-mands . Generative models, including Generative Adversar-ial Networks (GANs) and Variational Autoencoders (VAEs), haveemerged as powerful tools for augmenting time series data. GANs,particularly TimeGAN, and extensions of this work, create realis-tic synthetic samples by learning temporal dynamics, albeit withtraining challenges . VAEs and their conditional variants(CVAEs) generate new data from a latent space, offering potential de-spite some control issues over the generated samples .Sliding window methods risk overfitting by focusing too nar-rowly on local patterns, while decomposition methods generatelimited variety from extracted dataset features like trends .Simpler transformations like jittering, scaling, magnitude, andtime warping have been shown to help in specific tasks and do-mains . They are simple to implement and increase datavariety. While they may not capture complex patterns, they providesome control over the transformations since they are parametrictransformations.",
  "RHIOTS": "We introduce RHiOTS, a framework designed to assess the ro-bustness of HTS forecasting algorithms. Traditional evaluations,which primarily focus on predictive accuracy using a limited setof benchmark problems, fail to adequately assess the resilienceof an algorithm to minor variations in individual time series ortheir interrelations. RHiOTS addresses these issues by offering anuanced analysis of the stability of forecasting algorithms againstsuch changes, providing deeper insights into their robustness andreliability.",
  "Framework": "RHiOTS serves two primary objectives. The first goal is to provide acomprehensive understanding of the behavior of models. We do thisby systematically applying various transformations to the data andthen evaluating the performance of models. This process facilitatesa detailed assessment of the robustness of each model, controlledby transformation and intensity. It enables practitioners to identifythe most effective model for the unique aspects of their problemand potential variations in data properties.For the second goal, RHiOTS aims to support the reliable se-lection of an appropriate algorithm for a given forecasting task.By analyzing how different algorithms perform across a range oftransformed datasets, the framework offers insights into whichalgorithms are most adaptable and effective under varying condi-tions. This allows for a more effective and informed comparison ofalgorithms and improves generalization.RHiOTS applies transformations to each individual time series,z = T (z,). The transformations are applied to the time seriesin the leaf nodes of the hierarchy only. The aggregated levels of thehierarchy are recomputed after the transformation, i.e., the totalsum of the observations of a specific group (or for the top-levelseries) is computed based on the transformed individual series,z = :z z .The next step in RHiOTS is to assess the robustness of HTSforecasting models by connecting the variations introduced bytime series transformations with the variation in forecasting per-formance. To quantify the variation in forecasting performance, wecompute the forecasting error of the model on the original dataset and the various transformed versions. Given that each transforma-tion represents a type of variation (e.g., jittering represents noisein the measurement of the values), the analysis of the variation offorecasting performance for different versions of a transformationgives a systematic perspective on the robustness of a method tothat type of variation (e.g., the robustness of the method to noise).",
  "Time Series Transformations": "We apply random-based transformations to the original time se-ries, represented by z, = T (z,,), where T denotes a transfor-mation function and indicates its governing parameters. Thesetransformations can affect individual time series components andrelationships between series in a dataset. However, they should besmooth and continuous to preserve a meaningful relationship be-tween the original and transformed series. This ensures that similarparameters produce closely related transformed series.Jittering is a magnitude domain transformation that can be de-fined as the addition of a random noise component to the values of atime series , = +, where , N (0, 2). the standard devia-tion , of the added noise is a transformation parameter. Applyingjittering to the time series using the addition of i.i.d. Gaussian noiseis widely used in the literature to simulate realistic sources of mea-surement error or irregularity in time series data . For example,consider a retail store where sales data suddenly becomes moreerratic due to unexpected external factors such as local constructionaffecting customer traffic.Another transformation used is scaling, which involves modify-ing the amplitude of the series by a random scalar value , = where N (0, 2,). Once again, , defines the standard devia-tion of the multiplicative effect for each parameter set of the trans-formation, dataset, and series. Scaling the time series data using amultiplicative factor simulates realistic changes in the magnitudeof the time series. For example, consider a retail store experiencingincreased variability in sales due to various promotional campaignsexecuted by the store itself and its competitorsWe also applied magnitude warping . This method causes asmooth, continuous, nonlinear transformation of time series data.It can be written as , = (,) where N (1, 2,). Note",
  "RHiOTS: A Framework for Evaluating Hierarchical Time Series Forecasting AlgorithmsKDD 24, August 2529, 2024, Barcelona, Spain": "Future research can focus on developing finer transformationcontrols, enabling a more targeted analysis of their impact on perfor-mance. Meta-learning techniques can help more effectively identifyrelationships between transformation parameters and model perfor-mance. RHiOTS, combined with these research directions, will helpbuild a more comprehensive evaluation framework for hierarchicaltime series forecasting models. Acknowledgements. This work was partially funded by projectsAISym4Med (101095387) supported by Horizon Europe Cluster 1:Health, ConnectedHealth (n. 46858), supported by Competitive-ness and Internationalisation Operational Programme (POCI) andLisbon Regional Operational Programme (LISBOA 2020), underthe PORTUGAL 2020 Partnership Agreement, through the Euro-pean Regional Development Fund (ERDF) and NextGenAI - Cen-ter for Responsible AI (C645008882-00000055), supported by IAP-MEI, and also by FCT plurianual funding for 2020-2023 of LIACC(UIDB/00027/2020_UIDP/00027/2020)",
  "that interpolates a cubic spline with knots u = 1, ...,. Each": "knot comes from a distribution N (1, 2,), with the number ofknots and the standard deviation , as parameters. The cubicspline is fitted to the original data points, and the transformed datais obtained by scaling the original magnitude using the evaluatedcubic spline function values. The proposed transformation methodmodulates the magnitude of the time series, maintaining its overallstructure and smoothness. For example, the summer season salestime series of a store near a beach area during unusually hightemperatures could experience magnitude warping.Finally, the last transformation considered was time warping.Time warping is a process that stretches or compresses the timeaxis of a time series. Similarly to magnitude warping, we definetime warping using a cubic spline to interpolate the values of thetime series at a set of evenly spaced time points. The time pointscan be chosen such that they are spaced more closely together inregions where the time series is stretched and more widely spacedin regions where it is compressed. This will effectively stretch orcompress the time axis of the time series while preserving the shapeof the time series itself. Time warping is defined as , = , ( )",
  "Time warping can potentially impact the seasonality of a time series": "by stretching or compressing the time axis of the series. This cancause the periodic patterns in the series, such as seasonal cycles ortrends, to become more or less prominent, depending on how thetime axis is altered. For example, changes in weather cycles couldsignificantly impact the seasonality of specific stores and products.",
  "EXPERIMENTS": "The primary objective of our experimental setup is to illustrate howRHiOTS can be used to analyze the robustness of both models andalgorithms within the context of Hierarchical Time Series (HTS)forecasting.The first step we are interested in is how different transforma-tions affect the distance between time series in the dataset (Q1).HTS algorithms rely on these dependencies to improve their uni-variate estimates. (1) Models: In selecting models for a specific problem, stan-dard evaluation methods test those models on the availabledata. The assumption is that existing data is representativeof new, unseen data. RHiOTS can be used to test the robust-ness of those models to variations in the data (e.g., noise).We investigate the effects of different types of perturbationson prediction error (Q2) and how the prediction error ofHTS forecasting models changes with manipulation of de-pendencies across time and between series (Q3). Then, we",
  "Datasets": "The empirical evaluation uses three public datasets: the Tourism dataset from the Australian Bureau of Statistics, the M5 datasetbased on Walmart sales, and the Police dataset from Houstonpolice criminal reports. These datasets encompass various timegranularities, frequencies, lengths, and hierarchical structures, il-lustrating the usefulness of RHiOTS in evaluating the robustnessof time series forecasting models in different scenarios.To efficiently conduct the experiments across all datasets andmodels, we downsampled the M5 dataset by reducing its frequencyfrom daily to weekly. Additionally, for the M5 and Houston datasets,we selected a subset of 500 time series with high count levels. Pre-liminary experiments indicate that this selection has no significantimpact on evaluating the different algorithms using RHiOTS. In theinterest of space, we do not discuss them here.",
  "DeepAR + BU: DeepAR produces probabilistic forecastsbased on training an auto-regressive recurrent network modelon related time series . The hierarchy of the dataset is": ": Ranking of the performance of forecasting methodsunder magnitude warping transformation for the Tourismdataset, from original data (orig) to the most intense trans-formation (v5). Performance rank is indicated by proximityto the center, with 0 being the best. It shows that the perfor-mance of all algorithms deteriorates with increased transfor-mation intensity, highlighted by the significant reorderingof ranks and crossing of lines.",
  "Results and Discussion": "We start by addressing Q1 by evaluating the impact of our pro-posed transformations (.2) in the distance distributionsbetween the generated and original datasets. Remember that HTSalgorithms rely on the dependencies between time series to improvethe univariate estimates. Thus, we are interested in measuring howthese relations are disrupted when applying each transformationcontrolled by its magnitude. Looking at , we are systemati-cally generating rich variations of the original datasets. Also, thebehavior is consistent across all datasets.If we look closer to , for every transformation, the effect islimited and increases with the magnitude of the transformation, asexpected. The transformation with the largest effect on the distanceis magnitude warping, potentially having a greater impact on theperformance of the algorithms that rely on the relations between",
  "PoliceETS + BU0.980.820.840.770.830.85ETS + MinT0.950.850.830.840.821.34DeepAR + BU1.030.850.880.770.860.92TFT + BU0.651.640.952.011.007.09GPHF0.990.870.840.760.850.77": ": Results (MASE) for the original datasets considered in the experiments. Bold values represent the lowest error acrossmodels. The errors are calculated for each group, for bottom and top-level series. the series. The jittering transformation reduces the spread of thedistribution, i.e., it pushes the number of series that are very similaror dissimilar to each other to be closer to the original mean distance.Finally, as expected, the time warping transformation does notproduce a relevant impact in the distance. We are using DTWto measure the distance, which already accounts for time-basedwarping effects. In this case, we are interested in the fact thatalthough the distance does not change, there are warping effects inthe time dimension that could impact performance.Regarding Q2, suggests that predictive performancevaries significantly by transformation applied. Also, and answer-ing Q3, increasing the magnitude of transformations has not onlydifferent effects but, in some cases, opposite ones. laysout a comparative analysis of different HTS forecasting models(columns) in terms of their sensitivity to transformations (rows) inthe time series data. We control by different levels of the hierarchywithin the Tourism dataset such as bottom, purpose, region,and so on. Note that as the x-axis increases, the magnitude of thetransformation applied also increases. Thus, a flatter line would sug-gest that a model is less sensitive to that particular transformation,maintaining a consistent performance despite the increasing inten-sity of data alteration. As anticipated, magnitude warping is themost disruptive transformation, and its impact increases with themagnitude. For all models, we can see that there is a positive slope,i.e., consistently worse results when the parameters of the transfor-mation increase. Interestingly, for jitter, scaling, and time warping,the performance of all models stays flat or actually improves as weincrease the magnitude of the transformation (especially for jitter).Comparing the robustness of the different models, we observe thatGPHF is the model that shows more consistent behavior, whichmeans less impact in terms of performance.Regarding Q4, and the left radar chart on helpus compare the robustness of the different models. The classicalmethods yield better results than the more complex counterparts inmost cases. The only exception is when the transformation is too disruptive, such as magnitude warping, where we cannot spot anymeaningful difference between the predictive performance of thedifferent models. The radar chart in provides a visualizationof the ranking of different forecasting models in handling the mostdisruptive transformation magnitude warping across variousmagnitudes of this transformation. Each axis represents a differentset of parameters of the magnitude warping transformation, startingfrom the original data (orig) and progressing through increasinglyintense transformations (v0 through v5). The distance from thecenter indicates the rank of the method performance, with a rank of1 being the closest to the center (best performance) and higher ranksbeing further away (worse performance). The extensive crossingof lines as the magnitude increases shows the disruptive effect ofthe transformation. Thus, we cannot say that there is a model thatwould be more robust to this transformation when applied to thisdataset in particular. We then expanded the visualization to alltransformations to have a global perspective on the robustness ofthe models (see the left radar chart on ). ETS + MinT methodappears to be the most stable model across most transformations(except for magnitude warping, as already discussed). It also seemsto suggest that using hierarchical information brings value to themodel performance since ETS-BU is consistently worse than ETS+ MinT. Most interestingly, classical methods seem to be the mostrobust when applied to this dataset.When answering Q5, we start by applying a standard method-ology to estimate the forecasting accuracy, which is often used totest algorithms (e.g., to compare a newly proposed algorithm withexisting ones). In this analysis, researchers compare the predictiveperformance of algorithms on a small number of datasets. We re-produce what this analysis would look like, as shown in . Wecan see that the results are somewhat consistent for the Tourismand Police datasets. If M5 was not considered, this could lead us togeneralize on unreliable information. The main outcome is that weget mixed results, which are hard to generalize. Furthermore, the",
  "CONCLUSIONS AND FUTURE WORK": "We propose RHiOTS, a novel framework for generating semi-synthetictime series datasets with controlled dependencies. We demonstrateits effectiveness in evaluating the performance of HTS models andalgorithms under various conditions. Our empirical study usingRHiOTS confirms that model and algorithm performance variesdepending on the perturbations applied to the data and providesinsights into the expected effects based on the type of perturbation.First, we show that RHiOTS creates rich variations of the origi-nal datasets regarding the correlations between time series. Whenevaluating models, the predictive performance varies substantiallybased on the transformation applied. As we increase the magnitude,performance degrades quickly in transformations like magnitudewarping, while it improves in cases such as jitter. When evaluat-ing algorithms, we start by applying the conventional benchmarkanalysis of comparing predictive performance across three datasets.As the results were inconclusive, we extended the approach to useRHiOTS. One takeaway is that we do not see meaningful differencesin robustness when applying specific reconciliation methods, suchas MinT. The second is that classical algorithms are more robustthan more complex deep learning counterparts. Deep learning algo-rithms only showed more robustness when the transformation washighly disruptive, such as high-intensity magnitude warping. Theaggregated findings from our visualizations and analyses suggestthat if a single model must be chosen without prior knowledge ofpotential distortions in a dataset, the ETS model stands out as themost robust option.",
  "George Athanasopoulos, Roman A. Ahmed, and Rob J. Hyndman. 2009. Hierarchi-cal forecasts for Australian domestic tourism. International Journal of Forecasting25, 1 (2009), 146166": "George Athanasopoulos, Puwasala Gamakumara, Anastasios Panagiotelis, Rob J.Hyndman, and Mohamed Affan. 2020. Hierarchical Forecasting. In MacroeconomicForecasting in the Era of Big Data: Theory and Practice, Peter Fuleky (Ed.). SpringerInternational Publishing, Cham, 689719. George Athanasopoulos, Puwasala Gamakumara, Anastasios Panagiotelis, Rob J.Hyndman, and Mohamed Affan. 2020. Hierarchical Forecasting. In MacroeconomicForecasting in the Era of Big Data (1st ed.), Peter Fuleky (Ed.). Springer, 689719. George Athanasopoulos and Rob Hyndman. 2006. Modeling and forecastingAustralian domestic tourism. Monash University, Department of Econometrics andBusiness Statistics, Monash Econometrics and Business Statistics Working Papers 29(01 2006). George Athanasopoulos and Nikolaos Kourentzes. 2020. On the Evaluation ofHierarchical Forecasts. Monash Econometrics and Business Statistics WorkingPapers 2/20. Monash University, Department of Econometrics and BusinessStatistics. Christoph Bergmeir, Rob J. Hyndman, and Jos M. Bentez. 2016. Bagging ex-ponential smoothing methods using STL decomposition and BoxCox trans-formation. International Journal of Forecasting 32, 2 (2016), 303312. Hong Cao, Vincent Tan, and John Pang. 2014.A Parsimonious Mixture ofGaussian Trees Model for Oversampling in Imbalanced and Multimodal Time-Series Classification. IEEE transactions on neural networks and learning systems25 (12 2014), 22262239.",
  "Valentin Flunkert, David Salinas, and Jan Gasthaus. 2017. DeepAR: ProbabilisticForecasting with Autoregressive Recurrent Networks. CoRR abs/1704.04110(2017). arXiv:1704.04110": "Simone Gitto, Carmela Di Mauro, Alessandro Ancarani, and Paolo Mancuso. 2021.Forecasting national and regional level intensive care unit bed demand duringCOVID-19: The case of Italy. Plos one 16, 2 (2021), e0247726. Maxime Goubeaud, Philipp Jouen, Nicolla Gmyrek, Farzin Ghorban, LucasSchelkes, and Anton Kummert. 2021. Using Variational Autoencoder to augmentSparse Time series Datasets. In 2021 7th International Conference on Optimizationand Applications (ICOA). 16. Maxime Goubeaud, Philipp Jouen, Nicolla Gmyrek, Farzin Ghorban, LucasSchelkes, and Anton Kummert. 2021. Using Variational Autoencoder to augmentSparse Time series Datasets. In 2021 7th International Conference on Optimizationand Applications (ICOA). 16.",
  "Brian Kenji Iwana and Seiichi Uchida. 2021. An empirical survey of data aug-mentation for time series classification with neural networks. PLOS ONE 16, 7(jul 2021), e0254841": "Yanfei Kang, Rob Hyndman, and Feng Li. 2020. GRATIS: GeneRAting TIme Serieswith diverse and controllable characteristics. Statistical Analysis and Data Mining:The ASA Data Science Journal 13 (05 2020). Juan Pablo Karmy and Sebastin Maldonado. 2019. Hierarchical time seriesforecasting via Support Vector Regression in the European Travel Retail Industry.Expert Systems with Applications 137 (2019), 5973.",
  "Eamonn J. Keogh and Shruti Kasetty. 2004. On the Need for Time Series DataMining Benchmarks: A Survey and Empirical Demonstration. Data Mining andKnowledge Discovery 7 (2004), 349371": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. ImageNet Clas-sification with Deep Convolutional Neural Networks. In Advances in NeuralInformation Processing Systems, F. Pereira, C.J. Burges, L. Bottou, and K.Q. Wein-berger (Eds.), Vol. 25. Curran Associates, Inc. Shanika L Wickramasuriya, George Athanasopoulos, and Rob Hyndman. 2018.Optimal Forecast Reconciliation for Hierarchical and Grouped Time SeriesThrough Trace Minimization. J. Amer. Statist. Assoc. (03 2018), 145.",
  "Souhaib Ben Taieb, James W. Taylor, and Rob J. Hyndman. 2021. HierarchicalProbabilistic Forecasting of Electricity Demand With Smart Meter Data. J. Amer.Statist. Assoc. 116, 533 (2021), 2743. arXiv:": "Qingsong Wen, Liang Sun, Fan Yang, Xiaomin Song, Jingkun Gao, Xue Wang,and Huan Xu. 2021. Time Series Data Augmentation for Deep Learning: ASurvey. In Proceedings of the Thirtieth International Joint Conference on ArtificialIntelligence (IJCAI-2021). International Joint Conferences on Artificial IntelligenceOrganization. Renjie Wu and Eamonn J. Keogh. 2023. Current Time Series Anomaly De-tection Benchmarks are Flawed and are Creating the Illusion of Progress.IEEE Transactions on Knowledge and Data Engineering 35, 3 (2023), 24212429. Jinsung Yoon, Daniel Jarrett, and Mihaela van der Schaar. 2019. Time-seriesGenerative Adversarial Networks. In Advances in Neural Information ProcessingSystems, H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch-Buc, E. Fox, andR. Garnett (Eds.), Vol. 32. Curran Associates, Inc."
}