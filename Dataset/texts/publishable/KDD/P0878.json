{
  "ABSTRACT": "Acquiring new customers is a vital process for growing businesses.Prospecting is the process of identifying and marketing to potentialcustomers using methods ranging from online digital advertising,linear television, out of home, and direct mail. Despite the rapidgrowth in digital advertising (particularly social and search), re-search shows that direct mail remains one of the most effectiveways to acquire new customers. However, there is a notable gap inthe application of modern machine learning techniques within thedirect mail space, which could significantly enhance targeting andpersonalization strategies. Methodologies deployed through directmail are the focus of this paper.In this paper, we propose a supervised learning approach foridentifying new customers, i.e., prospecting, which comprises howwe define labels for our data and rank potential customers. Thecasting of prospecting to a supervised learning problem leads toimbalanced tabular data. The current state-of-the-art approach fortabular data is an ensemble of tree-based methods like randomforest and XGBoost. We propose a deep learning framework fortabular imbalanced data. This framework is designed to tackle largeimbalanced datasets with vast number of numerical and categoricalfeatures. Our framework comprises two components: an autoen-coder and a feed-forward neural network. We demonstrate theeffectiveness of our framework through a transparent real-worldcase study of prospecting in direct mail advertising. Our resultsshow that our proposed deep learning framework outperforms thestate of the art tree-based random forest approach when applied inthe real-world.",
  "ACM Reference Format:Sadegh Farhang, William Hayes, Nick Murphy, Jonathan Neddenriep andNicholas Tyris. 2024. A Deep Learning Approach for Imbalanced Tabular": "Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior specific permission and/or afee. Request permissions from , August 2529, 2024, Barcelona, Spain 2024 Association for Computing Machinery.ACM ISBN 978-1-4503-XXXX-X/18/06...$15.00 Data in Advertiser Prospecting: A Case of Direct Mail Prospecting. In KDD24: AACM SIGKDD Conference on Knowledge Discovery & Data Mining,August 2529, 2023, Barcelona, Spain. ACM, New York, NY, USA, 10 pages.",
  "INTRODUCTION": "Companies and brands use advertising for different purposes; ac-quiring new customers, creating brand awareness, introducing newproducts , and so forth. To achieve these goals, advertisers use dif-ferent mediums such as email, TV, newspaper, direct mail, onlineadvertisement, etc. Each of these techniques has its own pros andcons. Email advertising campaigns and surveys are less costly tosend but have lower success compared to say for example directmail advertising and surveys . Previous research has shownthat direct mail advertisement is better for brand recall since itrequires less cognitive effort to process than digital media . Wefocus on direct mail advertising in this paper.Direct mail has two types of advertising: customer relationshipmanagement (CRM) and prospecting. Customer relationshipmanagement uses ads to stay in touch with and motivate customersto make future purchases. The goal when prospecting is to findnew customers for the company by using targeted ads. Potentialcustomers are unknown when prospecting, therefore the potentialuniverse that ads can be served to is large (e.g., in the US, we havearound 131.43 million households ). Budget limitations and thedesire for profitable ad campaigns will necessitate a more targetedapproach. Therefore, it is essential to have an efficient frameworkto acquire new customers with the minimum budget spent.Even though direct mail advertising is one of the most effec-tive ways to acquire new customers, to the best of our knowledge,machine learning has not been widely used and studied in directmail advertising. In this paper, we focus on direct mail advertising,specifically on prospecting. Prospecting is challenging since anadvertiser must identify new potential customers and target them.To identify and target new customers, we propose an approach thatmodels the problem as a supervised learning task. In our proposedframework, we discuss in detail how we define labels and rank thecustomers throughout the population. Our proposed approach hasdelivered strong production performance for a variety of differentFortune 500 companies. Casting the problem of direct mail advertis-ing as a supervised learning problem results in a tabular imbalancedinput dataset (which is when the size of different classes or groupsare not equal). The majority of classes distributions can dominatethe training process, which will result in poor performance forminority classes.Previous research corroborated the fact that random forest, XG-Boost, and ensemble methods are the first choice in practice and",
  "KDD, August 2529, 2024, Barcelona, SpainSadegh Farhang, et al": "words, the ranking shows the priority that a household would haveduring a campaign mailing.We evaluated the real-world performance of two companies, i.e.,Company and Company . These companies were selected asthey are distinct from those used in the development and testing ofour framework. This serves to demonstrate the generality of ourframework and its ability to be applied to different products andaudiences. Moreover, Company operates in the home servicessector, while Company is in the home decoration sector.In our Evaluation, we have tested different audiences within eachcompany for both our proposed deep learning framework, whichis denoted by DL-AE, and random forest, denoted by RF in .Testing different audiences with different sizes for a company willlead us to find new potential customers since each audience consistsof a special sector of the companys customers.",
  "RELATED WORK": "Deep learning methods for tabular data can be divided into threecategories: data transformation, specialized architectures, and reg-ularization techniques . Data transformation methods convertcategorical and numerical data to an input that a deep learningmodel can effectively process. This can include transforming tab-ular data into a format similar to an image that captures spatialdependencies, as demonstrated by the successful use of convolu-tional neural networks in image analysis . This approach hasbeen applied to gene expression profiles and molecular descriptionsof drugs, both of which have feature dependencies. However, itmay not work well for independent features or when dependen-cies cannot be captured. Another approach is to use an encoderrepresentation of data such as VIME to find a new and in-formative representation of the data for predictive tasks. This hasbeen shown to improve performance compared to baseline mod-els like XGBoost , but is limited to continuous data . In ourframework, we use both the encoder representation and raw repre-sentation of data. Note that before now all of these methods have only been tested on small datasets and not yet shown to be scalableto real-world applications.The majority of the work in this field is concentrated in special-ized architectures for tabular data. One approach combines classicmachine learning methods, often decision trees, with neural net-works . For example, Frosst and Hinton use theoutputs of a deep neural network and the ground truth labels totrain soft decision trees, which are highly interpretable but sacrificeaccuracy. Another specialized architecture is network-on-network(NON) which consists of three components: a field-wise net-work, an across-field network, and an operation fusion network.Each column has its own field-wise network to extract specific infor-mation. The optimal operations are chosen by the across-field net-work and connected by the operation fusion network. For datasetswith many features, training individual networks for every columnand optimizing operations can be computationally intensive andtime-consuming.Transformers, previously used successfully in other domains,have been applied to tabular data as well. One such architectureis TabNet . TabNet uses sequential decision steps to encodefeatures and determine relevant ones for each sample using sparselearned masks. Despite good performance, some inconsistencies inexplanations have been noted .The third category of methods for tabular data focuses on regu-larization techniques . Regularization is necessary dueto the extreme flexibility of deep learning models for tabular datathat can sometimes lead to overfitting. A Regularization LearningNetwork (RLN) was proposed to address the observation that asingle feature in tabular data can significantly impact the prediction.A RLN is made more efficient and sparse by using regularization co-efficients to control the weight of each feature. This approach uses anew \"counterfactual loss\" to improve performance and the resultingsparse network can also be used for feature importance analysis.The evaluation of a RLN relies mainly on numerical datasets anddoes not address categorical data.In many real-world datasets such as the prospecting case inour study, the number of features and samples are extensive andcontain both categorical and numerical data. Additionally, imbal-anced datasets are often encountered in these problems. Hence,using specialized regularization techniques for tabular data is notappropriate. Furthermore, due to the large size of these datasets, weaim to implement a fast-training architecture that maintains highperformance. While one-time training might be feasible for sometasks, training a prospecting model must occur independently andfrom scratch for each company. To address these challenges wepresent our framework utilizing an autoencoder and feed-forwardprediction network.",
  "PROPOSED FRAMEWORK": "In this section, we present our machine learning approach forprospecting in advertising by formulating it as a supervised learningproblem and introducing our deep learning architecture.Our proposed framework is based on deep learning for tabu-lar data, which has been compared to tree-based methods such asrandom forests in previous studies, e.g., . Deep learning ap-proaches have yet to outperform tree-based methods for small to",
  "A Deep Learning Approach for Imbalanced Tabular Data in Advertiser Prospecting: A Case of Direct Mail ProspectingKDD, August 2529, 2024, Barcelona, Spain": "medium-sized datasets but our problem of prospecting has somekey differences to previous theoretical work. First, our datasets aremuch larger. Second, our datasets are imbalanced, whereas previousconclusions were made from balanced datasets. Third, a deep learn-ing framework provides more options for future optimizations inprospecting, such as fine-tuning the model with transfer learning.",
  "Prospecting": "The goal of prospecting is twofold: first, to find new customers for acompany, and second, to prioritize potential customers in an orderthat minimizes the cost of the prospecting campaign. To achievethese goals, we propose a supervised learning solution.Each company has a record of its previous customers who havepurchased one of their products. For each of these customers, aset of features can be created, ranging from demographic informa-tion to spending habits and interests. The set of features is bothnumerical and categorical. This list of customers and their featuresis referred to as an audience. The type of features that are usefulfor one product or service being advertised may differ from another.Although creating these features for a customer is not the focus ofthis paper, some companies, such as Epsilon , Visa , Master-card , Acxiom , and Experian provide such data whileabiding by privacy laws and best practices. Further discussion onprivacy considerations can be found in .3.The process of creating features for a companys universe, or thepopulation it is targeting for ads, is similar to creating features forits previous customers. The universe can vary based on the com-panys marketing goals, ranging from a city to a country or multiplecountries. In this framework, the entire US population is used asan example in experimental results. However, this framework isgeneral and not limited by the size of the universe. The population,after excluding the companys existing audience of customers, con-sists of a mix of potential new customers and individuals who arenot as likely to become new customers. The size of the populationis usually much larger compared to the audience, with the US pop-ulation being around three hundred million while the size of theaudience is typically not more than one million consumers.We model targeted advertisements as a binary classificationproblem. Therefore, we define two classes: Class 1, which consistsof customers in the companys audience list, and Class 0, whichconsists of a sample of the population minus the audience. We usethese two classes to train a model, which is then used to classify therest of the population and determine the target audience for the ads.It is possible that some individuals in the Class 0 sample are actuallypotential customers and should be labeled as Class 1. However,due to the small proportion of customers in the population, thenumber of incorrect Class 0 labels is negligible and is not expectedto significantly impact the performance of our proposed framework.The important aspect of our Class 0 labels is that they represent asample of the universe.When we sample data from our population minus our audience,there are two considerations:",
  "Sample size Sampling method": "We aim to balance the size of our sample with the population.On one hand, the sample needs to be large enough to be represen-tative of the non-customer portion of the population. On the otherhand, a sample that is too large may contain an increased numberof instances with the wrong negative label. This would result ina highly imbalanced dataset that is harder to train a supervisedlearning model on. We choose a sample size that strikes a balancebetween being representative and not so large the quantity of falsenegatives approaches the size of the audience. The actual samplesize is chosen through experimentation while taking into accountthe imbalance in the dataset.There are several methods to sample from the population minusthe audience. In this paper we choose to sample uniformly fromthe entire population without any restrictions.",
  "Framework": "In this subsection, we introduce our proposed deep-learning archi-tecture for tabular data. shows the detail of our proposedarchitecture. Our proposed architecture consists of two main com-ponents: (1) the autoencoder creates an encoded representation ofdata to distinguish between the minority class and the majorityclass, and (2) the feed-forward prediction neural network that usesthe combination of encoded representation of data and data itself forfinal prediction. It is worth mentioning that in our feed-forward pre-diction neural network, we use a customized loss function for ourimbalanced dataset, which we cover in more detail in .2.2.",
  ": Deep learning framework for tabular data": "3.2.1Autoencoder. An autoencoder is a type of unsupervised neu-ral network designed to learn an efficient representation of unla-beled data. It consists of two parts: an encoding network and adecoding network.Encoding: The encoding network takes in the input sample andcompresses it into a lower dimensional representation througha series of linear and non-linear transformations. = (;),(1)where is encoding neural network with weight parameters.Decoding: The decoding network then takes the encoded repre-sentation and tries to reconstruct the original input by applyinga reverse process of linear and non-linear transformations.",
  "= (4)": "= (;)(5)where denotes the input features to the feed-forward neuralnetwork, i.e., the concatenation of input features and the encodedfeatures. The weight parameters of the feed-forward neural networkare represented by , and the predicted probability of label 1 isrepresented by .Cost Function. Imbalanced data, where different classes havevarying sizes, is a common challenge in machine learning. There areseveral approaches to address this issue, such as re-sampling or cost-sensitive mechanisms. Over-sampling involves repeating samplesfrom the minority class or generating new samples via methodslike interpolating neighboring samples or data augmentationusing generative models . Other approaches involve assigningweights to each samples loss based on the data distribution , and the most common method is to select weights for eachclass as the inverse of the class frequency . A more advancedapproach to weight selection involves learning weights from abalanced sample of data . In this paper, we focus on the latterapproach of assigning weights to the samples loss function andusing the inverse of the class frequency for weight selection.Since we are modeling the prospecting scenario as a classificationproblem, we use binary cross entropy for our loss function. Duringtraining, our objective is to minimize the following loss function:",
  "The current state-of-the-art approach for classification in tabulardata favors tree-based models such as random forest and XGBoostfor small to medium datasets (around 10K samples) . However,": "in many practical applications, the datasets are much larger, oftenexceeding 100K samples and sometimes reaching millions, as is thecase in our study of prospecting via direct mail. In this study, wecompare the real-world performance of our proposed deep learningframework, consisting of an autoencoder and feed-forward neuralnetwork, to a random forest model regarding conversion rate andfound that our framework outperformed the random forest.As mentioned earlier, tree-based algorithms provide state-of-the-art performance for tabular data . In , Grinsztajn etal. compare tree-based algorithms like random forest with otherapproaches and different datasets thoroughly and show that tree-based approaches outperform deep learning and other traditionalmethods. As a result of this previous work, we only compare ourproposed framework to the random forest model in our evaluation.",
  "EXPERIMENTAL RESULTS": "In this section, we present the experimental results of our proposedframework for addressing the issue of prospecting in imbalanceddatasets using real-world data. First, we provide a brief overviewof our datasets, including their creation and evaluation criteria.Next, we describe the process of finalizing the proposed modelstructure and hyperparameter tuning through experimentation. Wealso discuss the selection of sample size. Finally, we evaluate theperformance of our proposed model in a real-world scenario.",
  "Dataset": "Each country has its own privacy laws. Since we are a companybased in the United States, we restrict ourselves to the United Statesprivacy laws. However, our proposed method is general and canbe used in other countries. The data we use for our experimentsis provided by companies like Acxiom and Experian, which offerfeatures for each individual or household and are compliant withfederal and state privacy laws and best practices. To protect theprivacy of individuals, the data provided was de-identified duringthe model development process. For a further discussion on privacy,see .3.While these data providers supply the features for our input, theydo not provide the labels for our binary classification. As mentionedearlier, each company seeking to do prospecting provides a list oftheir customers, i.e., the audience. We use this list to create the label1 data by combining it with the features provided by data providerslike Acxiom and Experian . To create the label 0 data, wesample from the population that is not included in the audience list.By combining both label 0 and label 1 data, we create our dataset.The term \"ratio\" is used to denote the sample size in relation to theaudience size. For example, a ratio of 4 means that the sample sizeis equal to 4 times the audience size.In our experimental results, we use audience lists from six com-panies, referred to as , , , , , and , to build our dataset,train the model, and evaluate the proposed model. However, forconfidentiality reasons, the actual names of these six advertisersare not disclosed. In the following sections, we provide a generaldescription of these companies without revealing their specificidentities.",
  "Company : A home decor company with an audience sizeof 446, 606": "Note that the audiences of these six companies vary in terms ofboth size and product type. We select these companies to demon-strate the effectiveness of our proposed framework across differentproduct verticals. To show how this framework can be generalized,we use companies , , , and for training, model building, andhyperparameter tuning. Then, we evaluate the performance of ourfinalized framework in a real-world scenario using companies and .For consistency, we use the same data provider throughout theexperiment. However, we do not disclose which provider we usedamong those mentioned.We evaluate our proposed framework in two ways. First, we usetraditional evaluation metrics such as accuracy, precision, recall,and the 2 score. Since our data is imbalanced, we focus on precision,recall, and the 2 score, which is a weighted harmonic mean ofprecision and recall. We split our dataset into training and testingsets, and evaluate the performance of our model based on thesemetrics. Recall is particularly important when prospecting becausewe do not want to miss any potential customers. Therefore, weconsider the 2 score which places more emphasis on recall.Second, we evaluate our proposed framework in a real-worlddirect mail scenario. Based on our trained model, we measure thenumber of consumers exposed to a direct mail ad who then make apurchase within a specific time frame (attribution window). Theseresults are presented in .5.Our proposed deep learning structure consists of two parts: anautoencoder and a feed-forward neural network. To train the frame-work, we first train the autoencoder alone to ensure that the encoderproduces good compact representations of the input features. Then,we use the trained autoencoder in the framework and freeze itsweights during the training of the feed-forward neural network.This separate training and freezing of the autoencoder has sev-eral benefits. First, it is faster to train the autoencoder alone than itis to simultaneously train both the feed-forward neural networkand the autoencoder using a joint loss function. Second, we foundthat using the pre-trained autoencoder and fine-tuning it duringthe training process of the feed-forward neural network does notresult in improved performance. Finally, jointly training the au-toencoder and the feed-forward neural network would introducean additional hyper-parameter to tune, i.e., a weight parameterspecifying the contribution of each loss function, which we aim toavoid for simplicity.",
  "Our architecture comprises two parts: an autoencoder and a feed-forward neural network. First, we determine the optimal encoded": "size for the autoencoder. We use a symmetrical structure for theencoder and decoder, with the first hidden layer in the encoderhaving 256 neurons and each subsequent layer size being halveduntil we reach the desired encoded size. We then reconstruct theinput from the encoded output and compare the decoder outputto the original input using Euclidean distance. We have 734 inputfeatures, so an encoded representation less than 16 may not besufficient and a representation larger than 128 may not be compactenough. Thus, we consider encoded output sizes of 16, 32, 64, and128. We compare the precision, recall, and 2 measures for thesefour sizes and choose the size with the best performance withoutoverfitting. For the training of our autoencoder, we use the Adamoptimizer and train for 100 epochs until the distance metric plateaus.Based on the results, we select an encoded size of 32 for our encoder.The results for the four sizes are shown in . We choose anencoded size of 32 as the 2 measure is slightly better for this size.Our proposed deep learning framework shows different levels ofperformance for different companies in terms of recall and precision.For example, companies and have lower precision values (below50%) compared to companies B and D (above 50%). The same istrue for recall values, with companies B and D having about 10%higher recall compared to companies A and C. One reason for thisvariation is the difference in products offered by these companies,some of which have more general products that can be purchased bya larger customer base, making it more challenging to build specificprospecting models. As a result, our framework tends to producemore false positives (lower precision) to ensure that no potentialcustomers are missed (high recall). Additionally, all four companieshave an imbalanced dataset with varying class frequencies, whichaffects the weight assigned to each sample in the loss function. Thisleads to a reasonable rate of true positives while having a tendencyto predict more positives and higher weights for class 1 predictions,resulting in more false positives.The second component, the feed-forward neural network, takesas input the concatenation of the input features (734) and the outputof the encoder (32). We test three different network architecturesto find the best one for our problem.The first architecture (architecture 512) has 512 neurons in thefirst layer and each subsequent layer reduces the number of neu-rons by half until reaching 64 neurons. The second architecture(architecture 2048) starts with 2048 neurons in the first layer andreduces the number of neurons by half in each subsequent layeruntil 64 neurons. The third architecture (architecture 4096) has4096 neurons in the first layer followed by 64 neurons. All threearchitectures have a final layer of 1 neuron for binary prediction.Batch normalization, the ReLU function, and dropout are usedin all layers of these three architectures.Due to space limitations, we do not cover in detail the perfor-mance of these three structures. However, the performance of thesearchitectures was evaluated in the same manner as the autoencoderevaluation to determine which one provides the best performancewhile also having low overfitting and reasonable training time. Asa result, we select architecture 4096 in our framework.In our model, we use a dropout probability of 0.5. For the opti-mizer, we test Adam, AdamW , and SGD with momentum .Based on our results, we observe that SGD with momentum givesthe best performance with lower overfitting. We select a learning",
  "Ratio": "The ratio of the sample size to the target audience size is investigatedin this section. The dataset for our prospecting task is created byusing a sample of the population excluding the target audience asthe negative class (label 0). We call the size of the sample relativeto the audience size the ratio. When the ratio is greater than 1, thedataset becomes imbalanced, meaning that the number of sampleswith label 0 exceeds the number of samples with label 1. To preventthe majority class (label 0) from dominating the learning processand lowering the accuracy of the minority class (label 1), weightsare assigned to each samples loss based on the inverse of the classfrequency. The results for this investigation can be seen in (see Tables 6, 7, 8, and 9 in Appendix A for both training and testresults).According to the results in the figure, as the ratio increases theprecision for both the test and training sets decreases. This outcomeis expected because as the ratio increases, there are more samplesfor class with label 0. The weight factor (inverse of class frequency)in the loss function yields more positive predictions to maintain ahigh recall value, but also results in more false positives, which isundesirable. However, the more data we use for training the moregeneralizable we make the model. Therefore, when choosing theratio, it is important to balance these two factors and opt for ahigher ratio as long as the precision does not excessively degrade.As mentioned previously, ratios 1 and 2 are not chosen as theylead to small datasets, which limit the models ability to generalize.Additionally, precision values for ratios greater than 5 are too low.Hence, we focus on ratios 3, 4, and 5 and do extensive analysis forthese three ratios, see for test performance and fortraining set performance. For companies , , and , the highestrecall is achieved with a ratio of 4. Although company has betterrecall with ratio 3, the difference is minimal and the recall value forratio 4 is very close. Despite a slight decrease in precision whenincreasing the ratio from 3 to 4, we still opt for ratio 4 as it offers",
  "Random Forest Comparison": "shows a comparison of precision, recall, and 2 score for ourproposed deep learning framework (denoted as DL-AE) and randomforest (denoted as RF) for four different companies, i.e., companies, , , and . Our framework outperforms random forest in termsof recall and 2 score for all four companies, while random forestprovides better precision. As we prioritize recall over precision,our framework is considered more suitable for our use case. Theperformance of both frameworks varies based on the company,which is dependent on the product and its audience. These resultscoupled with the conversion rate analysis demonstrate that ourproposed deep learning framework outperforms random forest inreal-world scenarios.",
  ": Comparison of random forest (RF) versus our pro-posed framework (DL-AE) for test test": "Since the focus of this paper is on proposing the deep learningmethod for tabular data, we do not provide the details of how wetune our random forest framework. In this paper, we only providethe parameters of our random forest model; however, similar todeep learning, we have done rigorous testing in order to find thebest parameters for our random forest framework. The depth ofour random forest model ranges from 9-18, with 200-500 trees, anda minimum between 5 - .01 percent (training data size) instancesper node.",
  "Real-World Performance": "In this section, we evaluate our proposed framework in the realworld. We aim to see how our proposed framework works forprospecting via direct mail. In a prospecting campaign, the specificnumber of mail pieces that are sent to different households is deter-mined by the companys budget and referred to as the campaignsreach. The number of targeted households that end up purchas-ing a product from that company within a specified time window(attribution window) are considered converters. Each individualtransaction from one of these converters is considered a conver-sion. To calculate the performance for a prospecting campaign,we are mainly interested in the conversion rate (denoted by CVR),or the number of conversions (denoted by #CNV) divided by thereach.The output of our proposed framework gives a probability foreach sample showing the likelihood of that sample being a prospect-ing customer. Based on these probabilities, we can rank the wholepopulation and distribute reach according to this ranking. In other",
  ": Comparison of random forest (RF) versus our pro-posed framework (DL-AE) in real-world": "In , the size of audience 1 and 2 are 329, 110 and 390, 273respectively. For company , audience 1 and 2 size are 446, 606and 347, 179. For privacy reasons for these companies, we do notreveal the difference between these audiences except for their size.As we can see in , the real-world performance of ourproposed framework is better than the random forest.In addition to considering the conversion rate of a couple ofcampaigns for some companies, we consider the performance ofcompany for multiple campaigns over the course of 6 monthsand compare how often our proposed architecture performs betterthan RF in practice. Over the course of 6 months, we have run 46campaigns for this company, and the reach ranges from 20,000 to500,00 based on the goal of the campaign and budget for that spe-cific campaign. When evaluating the performance of 46 differentcampaigns using conversion rate, our proposed DL framework out-performed the random forest model 30 times and matched randomforest performance 9 times. In production, the random forest modelonly performed better than our proposed DL framework 7 times.The above analysis corroborates the consistent performance of ourproposed DL approach over time compared to the random forest,which is known as the best choice for tabular data.",
  "Label and Ground Truth": "To address the issue of prospecting, we selected our label 0 by ran-domly sampling from the population excluding current customers.While some individuals in this sample may have never been exposedto the product, making it uncertain if they are truly label 0, we chosethis method as it is infeasible to determine with certainty whetheran individual will never purchase the product. There is a possibilitythat they may become customers in the future. The random samplefrom across the entire United States allows us to conceptually learnhow our clients customers compare to the average American.Additionally, testing our framework in the real world and calcu-lating conversions does not necessarily indicate that non-convertersare not customers. It only shows that they didnt choose to purchasethe product during the current attribution window. Factors suchas a different attribution window, increased ad exposure, alteredmessaging, or alternative offers may influence their decision toconvert in the future.It is important to note that there is no standard conversion ratethat is considered acceptable. Conversion rates vary depending ona companys marketing goals, which can change from companyto company and season to season. Whether the conversion rateof a specific campaign is reasonable depends on the companysobjectives and current market conditions.",
  "Interpretability": "Given the uncertainty surrounding label 0, we evaluate the inter-pretability of our model using Shapley values . We calculate theimpact of individual features on the prediction for a set of samplesranked at the top of our models predictions. This analysis demon-strates that features related to the product have the greatest impacton the prediction. In other words, we aim to confirm that our modelis functioning correctly and picking up relevant product-relatedfeatures. Our analysis suggests that the uncertainty regarding label0 does not negatively impact the accuracy of our model. However,due to proprietary data restrictions from our data providers, wecannot reveal the specific features or products involved. We onlymention this investigation to ensure that our model is functioningcorrectly.",
  "CONCLUSION": "This paper presents a novel deep learning framework for handlingimbalanced tabular data in an applied real world scenario. Theframework consists of two components: an autoencoder and a feed-forward neural network, which are designed to efficiently handlelarge datasets with numerous features. The performance of theproposed framework is evaluated through a real-world case studyof direct mail prospecting advertisement. The study investigatesimportant architecture selections such as the encoder size, feed-forward neural network architecture, and ratio, and compares theperformance of the proposed model to that of a tree-based randomforest model. The results show that the proposed framework out-performs the random forest in traditional metrics such as precisionand recall, as well as in real-world performance in the prospectingcampaign.The proposed framework is general in nature and could be ap-plied to other binary classification tasks. The authors intend tofurther explore this potential in future work by evaluating theframework on different tasks with varying feature sizes and num-bers, and comparing its performance to tree-based models likerandom forest and XGBoost.",
  "Mastercard Inc. 2023. Mastercard. [On-line; accessed 3-June-2024]": "Visa Inc. 2023. VISA. [Online; accessed 4-June-2024]. Herman Kahn and Andy W Marshall. 1953. Methods of reducing sample size inMonte Carlo computations. Journal of the Operations Research Society of America1, 5 (1953), 263278. Mohammad Mahdi Kamani, Sadegh Farhang, Mehrdad Mahdavi, and James ZWang. 2020. Targeted data-driven regularization for out-of-distribution gener-alization. In Proceedings of the 26th ACM SIGKDD International Conference onKnowledge Discovery & Data Mining. 882891. Salman H Khan, Munawar Hayat, Mohammed Bennamoun, Ferdous A Sohel, andRoberto Togneri. 2017. Cost-sensitive learning of deep feature representationsfrom imbalanced data. IEEE transactions on neural networks and learning systems29, 8 (2017), 35733587.",
  "Tse-Hua Shih and Xitao Fan. 2008. Comparing response rates from web and mailsurveys: A meta-analysis. Field methods 20, 3 (2008), 249271": "Gowthami Somepalli, Micah Goldblum, Avi Schwarzschild, C Bayan Bruss, andTom Goldstein. 2021. Saint: Improved neural networks for tabular data via rowattention and contrastive pre-training. arXiv preprint arXiv:2106.01342 (2021). Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and RuslanSalakhutdinov. 2014. Dropout: a simple way to prevent neural networks fromoverfitting. The journal of machine learning research 15, 1 (2014), 19291958. Baohua Sun, Lin Yang, Wenhan Zhang, Michael Lin, Patrick Dong, CharlesYoung, and Jason Dong. 2019. Supertml: Two-dimensional word embeddingfor the precognition on structured tabular data. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition Workshops. 00. Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. 2013. On theimportance of initialization and momentum in deep learning. In Internationalconference on machine learning. PMLR, 11391147.",
  "Gilmer Valdes, Wilmer Arbelo, Yannet Interian, and Jerome H Friedman. 2021.Lockout: Sparse regularization of neural networks. arXiv preprint arXiv:2107.07160(2021)": "Jinsung Yoon, Yao Zhang, James Jordon, and Mihaela van der Schaar. 2020. Vime:Extending the success of self-and semi-supervised learning to tabular domain.Advances in Neural Information Processing Systems 33 (2020), 1103311043. Yitan Zhu, Thomas Brettin, Fangfang Xia, Alexander Partin, Maulik Shukla,Hyunseung Yoo, Yvonne A Evrard, James H Doroshow, and Rick L Stevens. 2021.Converting tabular data into images for deep learning with convolutional neuralnetworks. Scientific reports 11, 1 (2021), 11325."
}