{
  "Abstract": "When faced with a large number of productreviews, it is not clear that a human can remem-ber all of them and weight opinions representa-tively to write a good reference summary. Wepropose an automatic metric to test the preva-lence of the opinions that a summary expresses,based on counting the number of reviews thatare consistent with each statement in the sum-mary, while discrediting trivial or redundantstatements. To formulate this opinion preva-lence metric, we consider several existing meth-ods to score the factual consistency of a sum-mary statement with respect to each individualsource review. On a corpus of Amazon prod-uct reviews, we gather multiple human judg-ments of the opinion consistency, to determinewhich automatic metric best expresses consis-tency in product reviews. Using the resultingopinion prevalence metric, we show that a hu-man authored summary has only slightly betteropinion prevalence than randomly selected ex-tracts from the source reviews, and previousextractive and abstractive unsupervised opinionsummarization methods perform worse thanhumans. We demonstrate room for improve-ment with a greedy construction of extractivesummaries with twice the opinion prevalenceachieved by humans. Finally, we show that pre-processing source reviews by simplification canraise the opinion prevalence achieved by exist-ing abstractive opinion summarization systemsto the level of human performance.",
  "Introduction": "Opinion summarization has emerged as a commer-cial application of multi-document summarization,with the goal of outputting the most salient opin-ions expressed in a collection of customer reviewsof a given product or service (Kim Amplayo et al.,2022). Practitioners have recognized the difficultyof obtaining a large training set of summaries thatadequately represent a large number of opinions ofvarious products, and so have developed unsuper- vised systems for opinion summarization (Brain-skas et al., 2020; Iso et al., 2021; Isonuma et al.,2021; Angelidis et al., 2021).Although systems may be trained without ref-erence summaries, they are still central to systemevaluation. However, for large scale data, adequatehuman references may not be only expensive butactually impossible, because the human cannot re-member all the source text at once. A commonlyused test set of Amazon data summarizes only setsof eight product reviews (Brainskas et al., 2020).The newer SPACE dataset collects summaries forsets of one hundred product reviews, by dividingthe work of selecting important statements amongdifferent annotators, and combining the selectedstatements in an aggregation step (Angelidis et al.,2021). Even if this is adequate for a hundred re-views, it will not scale when a human has to com-bine even more statements. The largest dataset,AmaSum (Brainskas et al., 2021), uses the opin-ions of professional reviewers as references, whichutilize many sources of information including thewriters own product tests, and may not be basedon opinion counting.For single-document summarization, researchershave introduced reference-free metrics to quantifyfactual consistency of a summary with its sourcedocument (Ernst et al., 2021; Fabbri et al., 2022;Laban et al., 2022), but factuality still is largelyneglected in multi-document and opinion summa-rization. We argue that a statement in an opinionsummary should not only be factual in the sense ofbeing logically implied by one document (review),but prevalent in the sense of being logically impliedby many documents.The main contribution of this paper is to intro-duce an automatic, reference-free metric for opin-ion prevalence in product reviews. Additionally,we present a new dataset, ReviewNLI, of hu-man consistency judgments in product reviews. Wecompare automatic metrics for opinion consistency",
  "arXiv:2307.14305v1 [cs.CL] 26 Jul 2023": "on ReviewNLI, choosing the best to form the basisof our opinion prevalence metric. We quantify theadvantage of human summaries above randomlyselected review extracts, but show that it is possibleto write summaries with twice the opinion preva-lence of a human reference summary. Finally, weintroduce a preprocessing technique that improvesthe opinion prevalence of some abstractive opinionsummarization systems to human levels.The ReviewNLI data, code for our opinion preva-lence metric, and code for our preprocessing tech-nique are released.1",
  "Related Work": "Some work on opinion summarization assumesthat a review is annotated with ratings for eachaspect within a finite set, that aspect seed terms arechosen, or that an aspect-based sentiment analysismodel is available, and coverage of these aspectsand sentiments guides the structure of the outputsummary (Di Fabbrizio et al., 2013; Angelidis andLapata, 2018; Suhara et al., 2020). However, evenif such information is available, within each aspectand/or sentiment, the task remains of aggregatingthe source review texts relevant to that aspect orsentiment into part of the summary. We seek tomeasure the degree to which a summary reflectsthe source reviews. We quantify this in a generalway that does not require specific annotations.Opinion prevalence is not measured in recentopinion summarization work, but some notions ofconsistency are. Iso et al. (2022) automaticallymeasured the consistency between source reviewsand the generated summary based on the similar-ity of the contextual embedding. This is a weakerrelationship than logical entailment, and score canbe earned for alignment to any source opinion,whether it is frequent or infrequent. Suhara et al.(2020) and Isonuma et al. (2021) manually evalu-ated faithfulness of their summaries against sourcereviews. However, they formulate faithfulness asa ternary classification problem (fully, partially, ornot supported) which considers a summary fullyfaithful if even one source review supports it. Stan-dards for labeling and agreement with the majoritydecisions were not reported. Recently Hoskinget al. (2023) evaluated faithfulness automaticallywith SummaC (Laban et al., 2022), again on thebasis of whether any supporting review existed, without regard to the frequency of opinions.Similarly to other summarization tasks, it is com-mon to manually evaluate fluency, coherence, in-formativeness, and redundancy of an opinion sum-mary (Isonuma et al., 2021; Brainskas et al., 2020;Amplayo and Lapata, 2020). We do not suggestthat opinion prevalence should replace these. Otherdesiderata have been suggested as well. For exam-ple, Iso et al. (2022) writes that systems may re-flect the important opinions in reviews, e.g., This isa nice place to eat. The staff are nice and friendly.,but they may not generate summaries that grab theattention of their readers and attempts to match thestyle of individual reviews in the summary, thoughthis is evaluated qualitatively. A reader might findthe style of an anecdotal review more pleasant toread, but particular anecdotes generally occur onlyonce. There may be tradeoffs between opinionprevalence and desiderata such as style matching,and we leave that investigation for future work.As one construction of extractive summarieswith high opinion prevalence, we use a greedysearch strategy similar to the one introduced bySaggion (2005) to extract summaries that opti-mized ROUGE. Later research used an A* searchto extract summaries with even better ROUGE re-call (Aker et al., 2010), but for opinion prevalencethere is interaction among added sentences to mea-sure redundancy, so such a search may be ineffi-cient.",
  "Data": "To select among possible automatic metrics foropinion consistency, we used the Amazon inputsource reviews and human summaries collectedin Brainskas et al. (2020). This dataset containsa development set of 28 products and a test setof 32 products, each with eight customer reviewsin English. Three human-authored reference sum-maries appear for each product, for use in evalua-tion. These reference summaries have been used toevaluate several abstractive unsupervised opinionsummarization systems, including Brainskas et al.(2020), Iso et al. (2021), and Isonuma et al. (2021).We use the Punkt tokenizer of NLTK 3.6.22 tosplit the reference summaries into sentences. Pair-ing each sentence from the first reference summary",
  "2www.nltk.org": "for each product with each of the source reviews,we asked qualified crowdworkers to judge whetherthe sentence was mostly implied or not by thegiven customer review (a binary decision). Detailsabout the instructions and qualification procedureare given in Appendix A. We obtained decisionsfrom three crowdworkers for each sentence/reviewpair, and took the majority decision as groundtruth. We release these decisions as the dataset,ReviewNLI.There are several reasons this opinion consis-tency judgment may be subjective. The same basicissue with a product may be described with slightlydifferent details. Moreover, a single sentence of asummary may combine various pieces of informa-tion, only some of which appear in a given sourcereview. Through the instructions, examples, andqualification test, we strived to achieve a mutualunderstanding of how to resolve these ambiguities.For instance, we clarified that a source review thatsaid We liked the clean rooms would not entail asummary that said The hotel room was clean andbright because the rooms may have been clean butvery dark.",
  ": Worker agreement with majority labels: accu-racy, false positive rate on negative examples, and falsenegative rate on positive examples": "Through these efforts, we achieved a rate ofagreement between worker labels and the majoritydecision that is on par with popular mainstreamnatural language inference (NLI) datasets. compares overall labeler accuracy with respect tothe majority, and the false positive and false nega-tive rates, across our dataset (ReviewNLI), and thedevelopment sets of SNLI (Bowman et al., 2015)and (matched) MultiNLI (Williams et al., 2018).For fair comparison, SNLI and MultiNLI are re-laxed to binary problems, classifying entailmentversus non-entailment (neutral or contradiction).Labelers achieve between 92% and 93% accuracyon all datasets. On ReviewNLI, this accuracy wasfairly uniform across workers: each worker whocontributed more than 100 labels had at least 90%accuracy.As in SNLI and MultiNLI, roughly one third of",
  "Experiments": "We evaluate how accurately existing metrics forsummary-source consistency can be thresholdedto predict the human judgments of opinion consis-tency found in ReviewNLI. None of the metricshas been trained on data from the review domain.ROUGE (Lin, 2004) is a popular, model-freemetric. Here, to test summary-source consistency,we take the ROUGE-1 precision rather than recallor F1, as we expect words of the summary sentenceto be mostly contained in the source review if en-tailed, whereas most words in the source revieware not expected in any one summary sentence.We apply stemming and use the implementation inHuggingface Datasets 2.3.2.3 SuperPAL (Ernst et al., 2021) is proposed asa semantic alternative to ROUGE for measuringsummary-source alignment, by using OpenIE in-formation extraction (Stanovsky et al., 2018) toextract propositions from summaries and sources,and using a RoBERTa (Liu et al., 2019) model firstfine-tuned on MultiNLI and further fine-tuned to de-cide which pairs of propositions align. We considertaking the maximum or average alignment scoreover all proposition pairs generated for a sourcereview / summary sentence pair.QAFE (Fabbri et al., 2022) generates question /answer (QA) pairs based on a summary sentence,and compares the answers expected from the sum-mary sentence to the answers predicted by a QAmodel using the source review as context. We takethe F1 score over the QA pairs generated as themeasure of consistency.SummaC (Laban et al., 2022) combines NLIscores obtained across units of a specific granu-larity between a source and a summary. We usethe zero-shot system, taking entailment probabilitywithout subtracting contradiction probability. Af-ter validating different models and granularities onthe development set, we found that the MultiNLI(Roberta large) model with document granularityhad the best performance.As our task is unbalanced binary classification,for reasons similar to those discussed in (Labanet al., 2022), we choose balanced accuracy to selectthe best metric, while also reporting AUC (Bradley,1997). Balanced accuracy is defined in terms of",
  ": Predicting majority human judgment of opinionconsistency on ReviewNLI": "SummaC performs the best. Its best balancedaccuracy is achieved at a threshold which judgessummaries as consistent if the model predicts evena weak (.04) probability of entailment, indicatingthat the consistent pairs in the ReviewNLI task arerelated in a much weaker way than strict logicalentailment. We observed a 7% difference betweenbalanced accuracy at the document and sentencegranularities. At sentence granularity, a maximumwas taken among sentence-based NLI judgments,and this did not perform well when informationwas split across source sentences.Although SuperPAL also leverages an NLI clas-sifier, it did not perform as well as SummaC, sug-gesting that its extracted propositions were not use-ful enough to classify the relation of summary andsource review. This held even though we tried twoways (maximum and average) of aggregating theproposition pair judgments.ROUGE performs worse than every method ex-cept QAFE. Even at its best balanced accuracy,QAFE only classifies 21.5% of majority-judgedconsistent opinion pairs as consistent. We suspectthat QAFE is too factoid oriented to be useful inconfirming opinions, and this is confirmed by qual-itative examples of QA pairs such as What typeof shoes run a bit narrow? which focus on detailsof the shoes that should be shared by all instancesof the same product, instead of the conclusion thatthe shoes are narrow.",
  "Scoring Opinion Prevalence": "Given a binary classifier C(x, y) which returns 1if a text x logically implies a text y and 0 if not,we consider how to define opinion prevalence of asummary S with sentences y1, . . . , yn with respectto a set of reviews R = {x1, . . . , xm} of a productp.The opinion prevalence should reward opinionsthat are expressed by multiple source reviews. Fora given sentence y, this could be formulated as",
  "i=1C(xi, y).(1)": "As we consider summaries with n > 1 sentences,there are two masks we want to apply to the classi-fier values. One mask should stop us from countingopinions that were already mentioned; otherwisethe one most frequent opinion could be repeatedfor a better prevalence than adding the second mostfrequent opinion to the summary. For yk, this masklooks like j<k(1 C(yj, yk)), which becomeszero if any prior yj implies yk.The other mask should stop us from mentioningconclusions that are so trivial that they follow fromthe fact the consumer bought the indicated product,without telling us anything about the consumersexperience. Let t be the sentence, I bought a p.For example, if p is a Reebok mens basketballsneaker, obvious conclusions like It is a shoeor I wear it might be logically entailed by everysingle review, even though they are not interesting.Therefore we mask C(xi, y) with 1 C(t, y).Attaching these masks to equation 1 and averag-ing over sentences in the summary, we obtain ourdefinition of opinion prevalence:",
  "j<k(1 C(yj, yk))(4)": "The opinion prevalence is a quantity between 0and 1. Based on the results of the previous sec-tion, we instantiate the classifier C with the Sum-maC MultiNLI document granularity model inthe experiments that follow. For our ReviewNLItest set, we collected the product names from theamazon.com and amazon.ca websites usingthe item numbers, and used them to compute the trivial masks. For a dataset where product namesare unavailable, it may be necessary to omit thismask (i.e. assume C(t, y) = 0).Opinion prevalence is a new quantity, not mea-sured by previously proposed metrics. If we as-sumed that humans could measure prevalence orthat human summaries were optimal, we couldcompare its correlation with human judgments toother metrics. Instead, we have compared the accu-racies with respect to human judgments for variouschoices of the classifier C, and simply formulatedhow outputs from the best C should be counted.",
  "Opinion Prevalence in Human andMachine Summaries": "Opinion prevalence provides an automatic metricto score the output of an opinion summarizationsystem, without requiring any reference summaries.Because of the mental overhead required to remem-ber, compare, generalize, and count opinions, thereis no reason to expect that a human-authored sum-mary consists of an optimal set of most frequentlyimplied statements. We investigated the opinionprevalence achieved by the existing human refer-ence summaries corresponding to the test set ofReviewNLI. shows the results.Do humans extract prevalent opinions anybetter than extracting sentences at random? Weconstructed three random summaries for each prod-uct, by concatenating sentences selected at randomwithout replacement from all the reviews. To makethe length comparable to the human summaries,we stopped adding sentences after the length incharacters exceeded the length of the first humansummary minus the half the length of its last sen-tence. shows a consistent lead in prevalencefor the human summaries over these random sum-maries.How do opinion prevalences of existing sum-marization systems compare? Having establishedthese baselines, we measure the opinion preva-lence achieved by various unsupervised opinionsummarization systems. We test three abstractivesystems with published models and source codethat were trained on Amazon reviews and evalu-ated on the Amazon test set which we used to createReviewNLI, where aspect information is unavail-able: CopyCat (Brainskas et al., 2020), COOP(Iso et al., 2021), and RecurSum (Isonuma et al.,2021). CopyCat uses a hierarchical variational au-toencoder (Bowman et al., 2016) to encode reviews, and decodes the mean of their latent vectors, with adecoder that has access to a pointer-generator mech-anism (See et al., 2017). COOP avoids averagingthe latent vectors from its autoencoder, conductingan expensive search over the power set of sourcereviews to find a combination of latent vectors thatdecodes to a review with best word overlap withthe source reviews. RecurSum applies a recursiveGaussian Mixture Model, in which latent vectorsare sampled conditioned on topics, to balance thecoverage of summaries across multiple topics.We also test an extractive system, QuantizedTransformer (QT) (Angelidis et al., 2021).4 QTdiscretizes a transformer encoding of source re-view sentences and samples the resulting opinionclusters proportionately to their popularity. There-fore, we might expect it to be well aligned with theobjective of opinion prevalence.We ran each of the four systems on the Amazontest set and measured the opinion prevalence of thecollected outputs, using the provided precomputedmodels. Results are shown in .None of the existing systems reaches the rangeof human performance, but the gap is not large.QT performs best, perhaps because popularity isconsidered in the sampling procedure. RecurSumand CopyCat have worse opinion prevalence thana random summary. Perhaps the topic coverageobjective of RecurSum is at odds with selecting themost frequent opinions, regardless of topic. Amongabstractive systems, only COOP exceeds the preva-lence of random summaries. Without the searchover the power set of input reviews (taking all inputreviews to be selected), its prevalence drops intothe range of random summaries.Are the results explained by summary length?A summarization algorithm that actually optimizedfor opinion prevalence may be able to achievehigher prevalences with summaries of shorterlength. Indeed, reordering sentences and truncat-ing the least prevalent ones can yield a higherprevalence score:Suppose Prev(R, {yi})Prev(R, {yj}) > 0 and C(yi, yj) = 0 for alli < j. Then if n < n, it is easy to show thatPrev(R, {yk}nk=1) Prev(R, {yk}nk=1).However, this phenomenon appears not to ex-plain any of the comparative results in . 4We test QT on Amazon, but trained per the repo instruc-tions on 500 entities from SPACE. Training on Amazon per-forms worse than random (.1784) in testing, perhaps suggest-ing it is hard to learn a good quantization from the greatervariety of products and fewer reviews per product.",
  "ExtractiveRandom summary 1.1931299.25.0.1572278.76.7Random summary 2.1887299.34.1.1682272.16.8Random summary 3.1791284.44.6.1647275.57.0QT.2039263.56.1.1774248.39.1Greedy.4744294.85.3.4886281.57.4": ": Opinion prevalences, lengths in characters, and numbers of sentences for human summaries, randomsummaries, and unsupervised opinion summarization systems. The simplified columns show opinion prevalencewhen simplification preprocessing is added (see .2). All summary outputs were computed by us usingprovided pretrained models. To the contrary, the existing abstractive systemsget lower opinion prevalence than the human sum-maries, while having fewer characters and fewersentences. Each instance of simplification (see Sec-tion 6.2), which preprocesses the input sentencesto be shorter, leads to a greater number of outputsentences, while usually maintaining the same char-acter length and increasing the opinion prevalence.Investigating the order of the implication counts ofeach statement in the output summaries, we suspectthat only our greedy system would be able to takeadvantage of this phenomenon to achieve higherscores, but we have set it to match the characterlength of the human and random summaries (whichare longest).",
  "Greedy Summaries": "We build a more prevalent extractive summary witha greedy search strategy over the sentences in thesource reviews. After computing the prevalencesof each statement in the source reviews, we addthe most frequently implied nontrivial statementsthat are not implied by previously added statements. The procedure is detailed in Algorithm 1.We tested this method and found its averageopinion prevalence to be much higher than anysystem considered, and twice the level of humans().Actual output summaries of Algorithm 1 withthe highest prevalence are shown in . Quali-tatively, they appear nontrivial and nonredundant.We observed a few self-contradictory summaries,based on conflicting opinions from different sourcereviews, but this problem appeared in the abstrac-tive and extractive outputs we evaluated too. (Uti-lizing the contradiction classification of the NLIclassifier, it may be possible to mitigate this prob-lem too.) The complete set of outputs are availablein our data release.On eight source reviews, this method ran fasterthan RecurSum (see Appendix C), but as thenumber of GPU queries grows quadratically withthe number of source reviews, more scalable ap-proaches should be investigated. We present themethod merely to indicate the large potential towrite summaries with better opinion prevalence.",
  "PrevalenceSummary": ".7000This handle is highly recommended. Would highly recommend it for lighter cameras.It looks good and is solidly built. A low cost device reduced jittery videos. It stayspretty secure whether I use it with the mount or on my flip video camera or snapshot..6875The product is excellent. It does not irritate my skin and it does the job. It lasts so long.Better product for less money. It works much better than common over the counterdeodorants. It lasts longer than the deodorant I used to use..6000You have to get the recipe and technique right for tortillas. This tortilla maker is lovely.A tortilla press needs to be hot to keep the tortilla from shrinking. Many indicated thatusers were using this machine to press and cook tortillas. It takes some trial and errorsto make it work right.",
  ": After using an ASSET model as preprocessing, the three product summaries output by Algorithm 1 withthe highest prevalence": "tences may be long and complex, combining manydifferent observations. This complexity may makeit difficult to extract and relate the common asser-tions from different reviews.To make the job of the summarization systemeasier, we consider using a text simplificationmodel to preprocess the input sentences of thesource reviews. To illustrate the potential of thisapproach, we train a T5 base model (Raffel et al.,2020) on the ASSET dataset (Alva-Manchego et al.,2020). ASSET collects simplifications from tenhuman annotators applying a number of transfor-mations, including rewriting a long sentence asseveral shorter sentences. ASSET source sentencesare based on Wikipedia and do not involve productreview data.ASSET contains only a development and testset, so we use its development set for training.The 2,000 development examples provide 20,000source/target pairs. Our model is trained for 30 epochs with maximum source and target lengths64, using learning rate 5e5, and Adam (Kingmaand Ba, 2015) parameters 1 = .9, 2 = .999, and = e8.For each sentence in our source reviews, we sam-ple ten sequences from the ASSET model withnucleus sampling (Holtzman et al., 2020) usingp = .9 and take the first of the outputs with thelargest number of sentences. We apply each of theopinion summarization systems to the source re-views rewritten by replacing the sentences in thismanner. This preprocessing could be run contin-ually as the source reviews are collected, beforesummarization time.Results are shown in the Simplified column of, and sample outputs with the highest preva-lence are shown in . COOP and CopyCatimprove by 3.7% and 4.6%, reaching the range ofhuman performance. RecurSum changes little, andmay be limited more by the topic tree that struc- Algorithm 1. Greedy extractive summariesInput: Reviews R = {x1, . . . , xm}Input: Trivial statement tInput: Target minimum length NCollect all sentences s1, . . . , sn from Rforeach j = 1, . . . , nif C(t,sj) = 0thenforeach i = 1, . . . , mCompute C(xi, sj)Let i1, . . . , in be {1, . . . , n} sortedso that Prev(R, {sij}) Prev(R, {sij+1})Let I Let j 0while iI len(si) < N and j < nLet j j + 1if C(t, xij) = 0 and C(xi, xij) = 0 for i Ithen Let I I {ij}Output: Summary {si}iI tures its output. We hope these findings encouragethe design of opinion summarization systems basedon finer granularities.Surprisingly, QT and random summaries getworse with preprocessing, and the greedy sum-maries from Algorithm 1 improve very little. Afterthe preprocessing, the outputs of these algorithmscan no longer be considered extractive, and thereis no guarantee that a statement will be supportedby at least one review.",
  "Conclusion": "If practitioners are concerned with reflecting a di-verse set of the most common opinions in an opin-ion summary, we recommend that they evaluateopinion prevalence. Mimicking human summarieswill not draw systems nearer to this goal becausehumans achieve less than half the prevalence that ispossible. Some systems can make easy progress byreplacing their use of sentences with finer grainedinformation; preprocessing with a simplificationmodel achieves this with no changes to the under-lying system.Sometimes opinion summaries are expected tocover particular aspects, such as atmosphere, ser-vice, and food at a restaurant, or to balance positiveand negative opinions. Although we do not con-sider this setting here, our metric could be usedrestricted to statements relevant to each aspect orsentiment. No single metric captures all desiderata of anopinion summary. We recommend opinion preva-lence be used in combination with other metrics toevaluate attributes such as fluency, coherence, andinformativeness.We introduced a greedy summarizer (Algorithm1) to show the existence of summaries with twicethe opinion prevalence of humans, though a lineartime construction of them remains a challenge forfuture research. The success of the text simplifi-cation preprocessing gives an easy improvementwhich could be added to systems designed to em-bed opinions at the sentence granularity.",
  "Limitations": "Although triviality masking provides some protec-tion against generic statements, not every uninfor-mative statement will be implied by the trivial state-ment. Rankings of systems by opinion prevalencestill should be supplemented by human assessmentsof informativeness and redundancy.We have only evaluated opinion prevalence onone dataset, but we expect opinion prevalence ofhuman summaries to be even worse on datasetswith more source reviews per summary.",
  "Ethics Statement": "An opinion summarization system poses the riskthat a consumer may select a product while miss-ing information that they could have found if theyread more of the reviews directly. Such a systemalso may hallucinate or pose misleading summariesof content. The summary may not represent themost prevalent opinions, though the present workprovides a means to ameliorate that risk.On the positive side, opinion summaries mightprovide information from more reviews than theconsumer could have read in a short amount oftime. It is in this hope that we are developingtechnologies for such summarization systems.Consumers may want to search reviews for tailrisks, such as a product catching on fire. Because(hopefully) these events are uncommon experi-ences, a summarization method targeting prevalentopinions may overlook such reports. Faults of aproduct might be found in a body of mostly posi-tive reviews by conditioning on negative sentimentbefore applying the method. For particular risks,simply using an NLI classifier to compare productreviews to the statement of that risk might exposethose risk possibilities. Ahmet Aker, Trevor Cohn, and Robert Gaizauskas.2010.Multi-document summarization using A*search and discriminative learning. In Proceedingsof the 2010 Conference on Empirical Methods inNatural Language Processing, pages 482491, Cam-bridge, MA. Association for Computational Linguis-tics. Fernando Alva-Manchego, Louis Martin, Antoine Bor-des, Carolina Scarton, Benot Sagot, and Lucia Spe-cia. 2020. ASSET: A dataset for tuning and evalua-tion of sentence simplification models with multiplerewriting transformations. In Proceedings of the 58thAnnual Meeting of the Association for ComputationalLinguistics, pages 46684679, Online. Associationfor Computational Linguistics. Reinald Kim Amplayo and Mirella Lapata. 2020. Un-supervised opinion summarization with noising anddenoising. In Proceedings of the 58th Annual Meet-ing of the Association for Computational Linguistics,pages 19341945, Online. Association for Computa-tional Linguistics. Stefanos Angelidis, Reinald Kim Amplayo, YoshihikoSuhara, Xiaolan Wang, and Mirella Lapata. 2021.Extractive opinion summarization in quantized trans-former spaces. Transactions of the Association forComputational Linguistics, 9:277293. Stefanos Angelidis and Mirella Lapata. 2018. Sum-marizing opinions: Aspect extraction meets senti-ment prediction and they are both weakly supervised.In Proceedings of the 2018 Conference on Empiri-cal Methods in Natural Language Processing, pages36753686, Brussels, Belgium. Association for Com-putational Linguistics. Samuel R. Bowman, Gabor Angeli, Christopher Potts,and Christopher D. Manning. 2015. A large anno-tated corpus for learning natural language inference.In Proceedings of the 2015 Conference on Empiri-cal Methods in Natural Language Processing, pages632642, Lisbon, Portugal. Association for Compu-tational Linguistics. Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, An-drew Dai, Rafal Jozefowicz, and Samy Bengio. 2016.Generating sentences from a continuous space. InProceedings of the 20th SIGNLL Conference on Com-putational Natural Language Learning, pages 1021,Berlin, Germany. Association for Computational Lin-guistics.",
  "Arthur Brainskas, Mirella Lapata, and Ivan Titov. 2021": "Learning opinion summarizers by selecting informa-tive reviews. In Proceedings of the 2021 Conferenceon Empirical Methods in Natural Language Process-ing, pages 94249442, Online and Punta Cana, Do-minican Republic. Association for ComputationalLinguistics. Kevin Clark, Minh-Thang Luong, Quoc V. Le, andChristopher D. Manning. 2020. Electra: Pre-trainingtext encoders as discriminators rather than generators.In International Conference on Learning Representa-tions.",
  "Giuseppe Di Fabbrizio, Ahmet Aker, and RobertGaizauskas. 2013. Summarizing online reviews us-ing aspect rating distributions and language modeling.IEEE Intelligent Systems, 28(3):2837": "Ori Ernst, Ori Shapira, Ramakanth Pasunuru, MichaelLepioshkin, Jacob Goldberger, Mohit Bansal, andIdo Dagan. 2021. Summary-source proposition-levelalignment: Task, datasets and supervised baseline.In Proceedings of the 25th Conference on Computa-tional Natural Language Learning, pages 310322,Online. Association for Computational Linguistics. Alexander Fabbri, Chien-Sheng Wu, Wenhao Liu, andCaiming Xiong. 2022. QAFactEval: Improved QA-based factual consistency evaluation for summariza-tion. In Proceedings of the 2022 Conference of theNorth American Chapter of the Association for Com-putational Linguistics: Human Language Technolo-gies, pages 25872601, Seattle, United States. Asso-ciation for Computational Linguistics.",
  "Noisy pairing and partial supervision for opinionsummarization": "Hayate Iso, Xiaolan Wang, Yoshihiko Suhara, StefanosAngelidis, and Wang-Chiew Tan. 2021. Convex Ag-gregation for Opinion Summarization. In Findingsof the Association for Computational Linguistics:EMNLP 2021, pages 38853903, Punta Cana, Do-minican Republic. Association for ComputationalLinguistics. Masaru Isonuma, Junichiro Mori, Danushka Bollegala,and Ichiro Sakata. 2021. Unsupervised abstractiveopinion summarization by generating sentences withtree-structured topic guidance. Transactions of theAssociation for Computational Linguistics, 9:945961. Reinald Kim Amplayo, Arthur Brazinskas, YoshiSuhara, Xiaolan Wang, and Bing Liu. 2022. Be-yond opinion mining: Summarizing opinions of cus-tomer reviews.In Proceedings of the 45th Inter-national ACM SIGIR Conference on Research andDevelopment in Information Retrieval, SIGIR 22,page 34473450, New York, NY, USA. Associationfor Computing Machinery.",
  "Diederik P Kingma and Jimmy Ba. 2015. Adam: Amethod for stochastic optimization. In InternationalConference on Learning Representations": "Philippe Laban, Tobias Schnabel, Paul N. Bennett, andMarti A. Hearst. 2022. SummaC: Re-visiting NLI-based models for inconsistency detection in summa-rization. Transactions of the Association for Compu-tational Linguistics, 10:163177. Mike Lewis, Yinhan Liu, Naman Goyal, MarjanGhazvininejad, Abdelrahman Mohamed, Omer Levy,Veselin Stoyanov, and Luke Zettlemoyer. 2020.BART: Denoising sequence-to-sequence pre-trainingfor natural language generation, translation, and com-prehension. In Proceedings of the 58th Annual Meet-ing of the Association for Computational Linguistics,pages 78717880, Online. Association for Computa-tional Linguistics.",
  "H. Saggion. 2005. Topic-based summarization at duc2005. In Document Understanding Conference": "Abigail See, Peter J. Liu, and Christopher D. Manning.2017. Get to the point: Summarization with pointer-generator networks. In Proceedings of the 55th An-nual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 10731083, Vancouver, Canada. Association for Computa-tional Linguistics. Gabriel Stanovsky, Julian Michael, Luke Zettlemoyer,and Ido Dagan. 2018. Supervised open informationextraction. In Proceedings of the 2018 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Volume 1 (Long Papers), pages 885895,New Orleans, Louisiana. Association for Computa-tional Linguistics. Yoshihiko Suhara, Xiaolan Wang, Stefanos Angelidis,and Wang-Chiew Tan. 2020. OpinionDigest: A sim-ple framework for opinion summarization. In Pro-ceedings of the 58th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 57895798, Online. Association for Computational Lin-guistics. Adina Williams, Nikita Nangia, and Samuel Bowman.2018. A broad-coverage challenge corpus for sen-tence understanding through inference. In Proceed-ings of the 2018 Conference of the North AmericanChapter of the Association for Computational Lin-guistics: Human Language Technologies, Volume1 (Long Papers), pages 11121122, New Orleans,Louisiana. Association for Computational Linguis-tics.",
  "ACrowdworker Protocol": "We recruited crowdworkers from Amazon Mechan-ical Turk.5 Workers were required to live in theUS, Great Britain, or Australia, have a 90% ap-proval rate, and have 1,000 approved HITs. Wefurther restricted the workers to have demonstratedsatisfactory performance on our previous HITs. Ad-ditionally, they had to classify seven out of eightexamples on a qualification test in agreement withour answers.Workers were compensated 40 cents for eachHIT, which consisted of a single source reviewand a single opinion summary. For each sentencein the summary, the workers judged whether thesentence was mostly or fully supported by thereview or partially or totally unsupported by thereview (a binary decision). A rough estimate ofthe average time a worker spent on each assignmentis 90 seconds, so that crowdworkers could earn $16per hour, which is above minimum wage in everystate of the United States except the District ofColumbia.6",
  "Detailed instructions were as follows:": "Youre given an Amazon review. De-cide whether the summary statementsgiven afterwards are mostly supported bythe review you read, or not. If significantparts of the summary statement are notsupported (e.g. the summary says Thehotel room was clean and bright andthe review just says We liked the cleanrooms), select partially unsupported.Please see the examples for guidance.",
  "It might be a good idea to order asize bigger because they can be alittle tight in the waist. - NOT SUP-PORTED": "Overall, these tights are definitelyrecommended - SUPPORTED - In-ferred from the reviewers strongsatisfaction.Example 2: My son is 3 and this fitshim perfectly. Hell probably be able towear it for the next two years if hed like.Its cute too. The hat is thin, but com-pletes the outfit. And the candy pocketis huge. Perfect! Im so glad we boughtthis costume over any other Thomas cos-tume.Summary statements: What an impressive Thomas theTrain costume! - SUPPORTED -The reviewer certainly seems im-pressed. We arent quite sure aboutwhether its a Thomas the Traincostume, but we allow that it proba-bly is on the basis of Thomas cos-tume.",
  "BArtifacts": "The development and test sets of Amazon reviewsand human summaries used for evaluation are dis-tributed with Copycat by Brainskas et al. (2020)under the MIT license.7 The upstream Amazondata is not clearly licensed, but we believe our re-search use is consistent with previous research usesof the data, including Brainskas et al. (2020), Isoet al. (2021), and Isonuma et al. (2021). The Re-viewNLI logical consistency judgments will alsobe distributed under the MIT license.Among consistency metrics, SuperPAL8 (Ernstet al., 2021) and SummaC9 (Laban et al., 2022) areApache 2.0 licensed. QAFE10 (Fabbri et al., 2022)is BSD licensed. ROUGE is run by the Hugging-face Datasets library 11 which wraps the Google Re-search implementation12 both of which are Apache2.0 licensed.Among summarization systems, Copycat, Recur-Sum13 (Isonuma et al., 2021), and QT14 (Angelidiset al., 2021) are MIT licensed. COOP15 (Iso et al.,2021) is BSD licensed.The worker IDs are unique identifiers of theAmazon crowdworkers who contributed the opin-ion consistency judgments, and we will apply aone-way hash to them before releasing them in Re- 7github.com/abrazinskas/Copycat-abstractive-opinion-summarizer8github.com/oriern/SuperPAL9github.com/tingofurro/summac10github.com/salesforce/QAFactEval11github.com/huggingface/datasets12github.com/google-research/google-research/tree/master/rouge13github.com/misonuma/recursum14github.com/stangelid/qt15github.com/megagonlabs/coop",
  "CComputational Resources": "Experiments were run on Nvidia GeForce GTX1080Ti GPUs. Unless otherwise noted, modelswere pretrained and one GPU was used for evalua-tion.For the consistency metrics, QAFE uses a BARTlarge (Lewis et al., 2020) model for question gen-eration (400M parameters) and an Electra large(Clark et al., 2020) model for question answering(335M parameters). SuperPAL uses a Roberta largemodel (355M parameters) for proposition align-ment, and we suspect its OpenIE model for propo-sition extraction has about 1.8M parameters basedon file size. The SummaC model that performedbest was Roberta large (355M parameters).For the summarization systems, we counted atotal of 41M parameters in the Copycat checkpoint,21M parameters in the COOP checkpoint, and 23Mparameters in the RecurSum checkpoint. Using oneGTX 1080 Ti GPU, we trained the 27M parameterQT model on the 1.1M reviews (for 11K hotels)in the SPACE corpus (Angelidis et al., 2021) asinstructed, in just under four hours.Running the consistency metrics between eachinput review and the first human summary on the 32product test set using the same GPU, wall clock runtimes (including latency) were as follows: Super-PAL took 6 minutes and 9 seconds; QAFE took 3minutes and 3 seconds; and SummaC took 1 minuteand 23 seconds. The opinion prevalence scores ofthe first human summaries of the 32 products werecomputed in 33 seconds, which is faster than Sum-maC because of early stopping when a trivial orpreviously implied statement is encountered.Running the summarization systems on the testset, wall clock run times were as follows: Copycattook 24 seconds; COOP took 1 minute 36 seconds;RecurSum took 5 minutes and 17 seconds; and QTtook 10 seconds. Computing the greedy extrac-tive summaries (Algorithm 1) took 2 minutes 48seconds.For the ASSET simplification model (297M pa-rameters), 30 epoch training was chosen due tothe small training (development) set after artifacts(insertions of nonsense words) were observed inthe output of a 3 epoch model, but other trainingparameters were left at Huggingface defaults andno careful model selection was performed."
}