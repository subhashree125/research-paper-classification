{
  "ABSTRACT": "Spatio-temporal prediction aims to forecast and gain insights intothe ever-changing dynamics of urban environments across bothtime and space. Its purpose is to anticipate future patterns, trends,and events in diverse facets of urban life, including transportation,population movement, and crime rates. Although numerous effortshave been dedicated to developing neural network techniques foraccurate predictions on spatio-temporal data, it is important to notethat many of these methods heavily depend on having sufficientlabeled data to generate precise spatio-temporal representations.Unfortunately, the issue of data scarcity is pervasive in practicalurban sensing scenarios. In certain cases, it becomes challenging tocollect any labeled data from downstream scenarios, intensifyingthe problem further. Consequently, it becomes necessary to builda spatio-temporal model that can exhibit strong generalizationcapabilities across diverse spatio-temporal learning scenarios.Taking inspiration from the remarkable achievements of largelanguage models (LLMs), our objective is to create a spatio-temporalLLM that can exhibit exceptional generalization capabilities acrossa wide range of downstream urban tasks. To achieve this objective,we present the UrbanGPT, which seamlessly integrates a spatio-temporal dependency encoder with the instruction-tuning para-digm. This integration enables LLMs to comprehend the complexinter-dependencies across time and space, facilitating more compre-hensive and accurate predictions under data scarcity. To validate theeffectiveness of our approach, we conduct extensive experiments onvarious public datasets, covering different spatio-temporal predic-tion tasks. The results consistently demonstrate that our UrbanGPT,with its carefully designed architecture, consistently outperformsstate-of-the-art baselines. These findings highlight the potentialof building large language models for spatio-temporal learning,particularly in zero-shot scenarios where labeled data is scarce. ACM Reference Format:Zhonghang Li1,2, Lianghao Xia1, Jiabin Tang1, Yong Xu2,, Lei Shi3, LongXia3, Dawei Yin3 and Chao Huang1. 2024. UrbanGPT: Spatio-TemporalLarge Language Models. In Proceedings of ACM (Conference). ACM , 11 pages.",
  "Conference, 2024,2024. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00": "aspects of urban life. This has significant implications in the fieldof urban computing, where the ability to predict transportationpatterns can optimize traffic flow, reduce congestion, and enhanceoverall urban mobility . Moreover, anticipating populationmovement can aid in effective urban planning and resource alloca-tion . Additionally, the ability to forecast crimes can greatlycontribute to enhancing public safety . Spatio-temporal predic-tion plays a vital role in shaping smarter and more efficient cities,ultimately leading to an improved quality of urban life.It is important to highlight the various types of neural networkarchitectures commonly adopted in this domain of spatio-temporalprediction. These architectures are designed to capture and modelthe complex relationships between spatial and temporal dimensionsin the data. One widely employed architecture is the ConvolutionalNeural Network (CNN) , which is effective in extract-ing spatial features by applying convolutional filters across theinput data. Another line of spatio-temporal neural networks is theRecurrent Neural Network (RNN) family . Those spatio-temporal RNNs are well-suited for capturing temporal dependenciesby maintaining a memory state that can retain information overtime. Recently, there has been a surge in the use of Graph NeuralNetworks (GNNs) for spatio-temporal prediction . GNNsexcel in modeling complex spatial relationships in data representedas graphs, where each node corresponds to a spatial location andedges capture the connections between them.While current spatio-temporal neural network techniques haveproven to be highly effective, it is crucial to acknowledge theirstrong dependence on having an abundance of labeled data in orderto generate accurate predictions. However, the pervasive problemof data scarcity in practical urban sensing scenarios poses a sig-nificant challenge. For example, deploying sensors throughout theentire urban space to monitor citywide traffic volume or air qual-ity is impractical due to the high cost involved . Moreover,the challenge of limited labeled data availability extends to spatio-temporal forecasting across different cities, in which acquiringlabeled data for each target city becomes a daunting task .These issues emphasize the pressing need for novel solutions thataddress data scarcity and enhance the generalization capabilities ofspatio-temporal models in various smart city applications.Inspired by the remarkable progress of large language models(LLMs), our primary goal is to create a spatio-temporal LLM thatpossesses outstanding generalization capabilities across a widearray of urban tasks. Leveraging the reasoning abilities inherent inLLMs, we aim to expand their success into the domain of spatio-temporal analysis. Our objective is to develop a model that caneffectively comprehend and forecast intricate spatial and temporalpatterns, enabling it to excel in various urban scenarios.While it is of utmost importance to develop a versatile spatio-temporal model capable of effectively handling diverse downstream",
  "arXiv:2403.00813v3 [cs.CL] 19 May 2024": "Instructions: Given the historical data for taxi flow over 12 time steps in a specific region of New York City, the recorded taxi inflows are , and the recorded taxi outflows are [96 91 108 102 83 78 76 74 106 93 86 57] Please predict the taxi inflow and outflow for next 12 time steps Labels: Taxi Inflow: Taxi Outflow: . Ours: Based on the given information, the predictions of taxi inflow and outflow in this region are Prediction: Inflow: Outflow: .",
  "Instructions": "LLAMA-70B Ground true Prediction: Inflow: Outflow: Baseline LLM: There is a noticeable spike in taxi inflows and outflows around 10:00-11:00, which may indicate a surge in demand for taxi services I have created a predictive model to forecast the taxi flow for the next 12 time steps: Prediction: Inflow: Outflow: UrbanGPT",
  "Time Steps": "Outflow Predicted value BaselineLLMOursGround_true : The superior predictive performance of the pro-posed UrbanGPT compared to the large language model(LLaMA-70B) and the spatiotemporal graph neural network(STGCN) in a zero-shot traffic flow prediction scenario. tasks, aligning the spatio-temporal context with the knowledgespace of large language models (LLMs) and enabling them to com-prehend the complex dependencies across time and space presentsignificant challenges. These hurdles call for meticulous modeldesign to bridge the gap between the unique characteristics ofspatio-temporal data and the knowledge encoded within LLMs.Contributions. In light of these challenges, we propose UrbanGPT,a large language model specifically tailored for spatio-temporalprediction. At the core of UrbanGPT lies a novel spatio-temporalinstruction-tuning paradigm that seeks to align the intricate de-pendencies of time and space, with the knowledge space of LLMs.Within our UrbanGPT framework, we start by incorporating aspatio-temporal dependency encoder, which utilizes a multi-leveltemporal convolutional network. This encoder enables the modelto capture the intricate temporal dynamics present in the spatio-temporal data across various time resolutions. Then, our modelinvolves aligning textual and spatio-temporal information to em-power language models in effectively injecting spatio-temporalcontextual signals. This is achieved through the utilization of alightweight alignment module that projects the representationsof spatio-temporal dependencies. The result is the generation ofmore expressive semantic representations by integrating valuableinformation from both textual and spatio-temporal domains.Through the incorporation of spatio-temporal information dur-ing the instruction-tuning process, the language model gains profi-ciency in understanding and processing the intricate relationshipsand patterns found in spatio-temporal data. By leveraging the in-sights obtained from the spatio-temporal domain, the languagemodel becomes better equipped to capture the nuances and com-plexities of spatio-temporal phenomena. This, in turn, enables themodel to make more reliable and insightful predictions across vari-ous urban scenarios, even when faced with limited data availability.To showcase the superior predictive performance of our pro-posed model, we compare it with the large language model (LLaMA-70B) and the spatio-temporal graph neural network (STGCN) in a zero-shot traffic flow prediction scenario guided by textual instruc-tions, as depicted in . The large language model, LLaMA,effectively infers traffic patterns from the input text. However, itslimitations in handling numeric time-series data with complex spa-tial and temporal dependencies can sometimes lead to oppositetraffic trend predictions. On the other hand, the pre-trained base-line model demonstrates a strong understanding of spatio-temporaldependencies. However, it may suffer from overfitting to the sourcedataset and underperform in zero-shot scenarios, indicating its lim-ited generalization capabilities beyond existing spatio-temporalprediction models. In contrast, our proposed model achieves a har-monious integration of domain-specific spatio-temporal knowledgeand language modeling capabilities. This enables us to make moreaccurate and reliable predictions under data scarcity.In summary, our main contributions can be outlined as follows: To the best of our knowledge, this is the first attempt to developa spatio-temporal large language model capable of predictingdiverse urban phenomena across different datasets, especiallyunder conditions of limited data availability. We propose UrbanGPT, a spatio-temporal prediction frameworkthat empowers large language models (LLMs) to comprehendthe intricate inter-dependencies across time and space. This isachieved through the seamless integration of a spatio-temporaldependency encoder with the instruction-tuning paradigm, ef-fectively aligning the spatio-temporal context with LLMs. Extensive experiments conducted on three benchmark datasetsprovide compelling evidence of our proposed UrbanGPTs excep-tional ability to generalize in zero-shot spatio-temporal learningscenarios. These findings highlight the models robust general-ization capacity, demonstrating its effectiveness in accuratelypredicting and understanding spatio-temporal patterns, even inscenarios where no prior training data is available.",
  "PRELIMINARIES": "Spatio-Temporal Data. Spatio-temporal data is commonly col-lected and can be represented as a three-dimensional tensor X R . Each element X,, in the tensor corresponds to the valueof the -th feature at the -th time interval in the -th region. Toprovide an example, lets consider predicting taxi traffic patterns inan urban area. In this scenario, the data can represent the inflowand outflow of taxis in a specific region (e.g., the -th spatial area)during a given time period from to 1 (e.g., a 30-minute interval).Spatio-Temporal Forecasting. In spatio-temporal prediction tasks,a common scenario involves forecasting future trends using histor-ical data. Specifically, the goal is to predict the data for the next time steps based on information from the preceding steps.",
  "X+1:+ = (X +1: )(1)": "The function () represents a spatio-temporal prediction modelthat has been trained effectively using historical data. Spatio-temporalprediction tasks can be divided into two main categories: regressionprediction, which involves predicting continuous values like trafficflow or taxi demand , and classification prediction, where thegoal is to classify events such as crime occurrence prediction .To optimize the model (), different loss functions are utilized based on the specific characteristics of the spatio-temporal scenar-ios.Spatio-Temporal Zero-Shot Learning. Despite the effectivenessof current spatio-temporal learning approaches, they often en-counter difficulties in effectively generalizing across a wide range ofdownstream spatio-temporal learning scenarios. In this study, ourfocus is on addressing the challenge of spatio-temporal zero-shotscenarios, where we aim to learn from previously unseen data indownstream spatio-temporal prediction datasets or tasks. This canbe formally defined as follows:",
  "METHODOLOGY3.1Spatio-Temporal Dependency Encoder": "Although large language models demonstrate exceptional profi-ciency in language processing, they face challenges in comprehend-ing the time-evolving patterns inherent in spatio-temporal data. Toovercome this limitation, we propose enhancing the capability oflarge language models to capture temporal dependencies withinspatio-temporal contexts. This is accomplished by integrating aspatio-temporal encoder that incorporates a multi-level tempo-ral convolutional network. By doing so, we enable the model toeffectively capture the intricate temporal dependencies across vari-ous time resolutions, thereby improving its understanding of thecomplex temporal dynamics present in the spatio-temporal data.Specifically, our spatio-temporal encoder is composed of two keycomponents: a gated dilated convolution layer and a multi-levelcorrelation injection layer. Lets formalize this architecture as:",
  "()= ( W() E()+ b() ) ( W() E()+ b() ) + E ()(3)": "We begin with the initial spatio-temporal embedding, denoted asE R . This embedding is obtained by enhancing the originaldata X through a linear layer. To address the issue of gradientvanishing, we utilize a slice of E, denoted as E R , whichis determined by the size of the dilated convolutional kernel. Thisslice is employed for performing residual operations. To performthe residual operations, we use 1-D dilated convolution kernelsW and W R , along with the corresponding biasterms b and b R . The sigmoid activation function isapplied to control the degree of information preservation during therepeated convolution operation. After the gated temporal dilatedconvolutional layer encoding, we are able to effectively capture thetemporal dependencies across multiple time steps, resulting in thegeneration of temporal representations.These representations contain different levels of temporal de-pendencies, reflecting various granularity-aware time-evolving pat-terns. To preserve these informative patterns, we introduce a multi-level correlation injection layer. This layer is designed to incorpo-rate correlations between different levels and is formalized as:",
  "S()= (W() ()+ b() ) + S(1)(4)": "We have the convolution kernel W R and the biasvector b R . These are employed after layers of encoding.A simple non-linear layer is employed to merge the results fromequations 3 and 4, and the final spatio-temporal dependency repre-sentations of are denoted as R . To address the diverseset of urban scenarios that may arise downstream, our proposedspatio-temporal encoder is designed to be independent of graphstructures when modeling spatial correlations. This is particularlycrucial because in zero-shot prediction contexts, the spatial rela-tionships between entities may be unknown or difficult to ascertain.By not relying on explicit graph structures, our encoder can effec-tively handle a broad spectrum of urban scenarios, where spatialcorrelations and dependencies can vary or be challenging to definein advance. This flexibility enables our model to adapt and performwell, ensuring its applicability in a wide range of urban contexts.",
  "Spatio-Temporal Instruction-Tuning": "3.2.1Spatio-Temporal-Text Alignment. In order to enable lan-guage models to effectively comprehend spatio-temporal patterns,it is crucial to align textual and spatio-temporal information. Thisalignment allows for the fusion of different modalities, resultingin a more informative representation. By integrating contextualfeatures from both textual and spatio-temporal domains, we cancapture complementary information and extract higher-level se-mantic representations that are more expressive and meaningful.To achieve this objective, we utilize a lightweight alignment mod-ule to project the spatio-temporal dependencies representations. This projection involves the use of parameters W R and b R, where represents the commonly used hiddendimension in language models (LLMs).The resulting projection, denoted as H R , are rep-resented in the instructions using special tokens as: <ST_start>,<ST_HIS>, ..., <ST_HIS>, <ST_end>. Here, <ST_start> and <ST_end>serve as identifiers marking the beginning and end of the spatio-temporal token. These identifiers can be included in the large-scale language model by expanding its vocabulary. The placeholder<ST_HIS> represents the spatio-temporal token and corresponds tothe projection H in the hidden layer. By employing this technique,the model gains the ability to discern spatio-temporal dependen-cies, thereby enhancing its proficiency in successfully performingspatio-temporal predictive tasks within urban scenes. 3.2.2Spatio-Temporal Prompt Instructions. In scenarios in-volving spatio-temporal prediction, both temporal and spatial in-formation contain valuable semantic details that contribute to themodels understanding of spatio-temporal patterns within specificcontexts. For instance, traffic flow in the early morning differs sig-nificantly from rush hour, and there are variations in traffic patternsbetween commercial and residential areas. As a result, we recognizethe potential of representing both temporal and spatial informationas prompt instruction text. We leverage the text understandingcapabilities of large language models to encode this information,enabling associative reasoning for downstream tasks.In our UrbanGPT framework, we integrate multi-granularitytime information and spatial details as instruction inputs for thelarge language model. Time information includes factors such as",
  "Gate": "Spatio-Temporal dependency encoder LLMs space Lightweight alignment module Spatio-Temporal Instruction-Tuning Given the historical data and the corresponding tokens<ST_HIS> for taxi flow The recording time This regionis located Please generate the predictive tokens for Based on the given information, the predictive tokensof taxi inflow and outflow in this region are <ST_PRE> Large Language ModelsInflowOutflow City A City B Instruction-TuningZero-shot prediction <ST_HIS> <ST_HIS> Multi-tasks",
  ": The overall architecture of the proposed spatio-temporal language model UrbanGPT": "Temporal Information: The recording time of the historical data is 'January 7, 2020, 08:30,Tuesday to January 7, 2020, 14:00, Tuesday, with data points recorded at 30-minute intervals'.Spatial Information: Here is the region information: This region is located within the StatenIsland borough district and encompasses various POIs within a three-kilometer radius,covering Public Safety, Education Facility, Residential categories.Task description: Now we want to predict the taxi inflow and outflow for the next 12 timesteps during the time period of 'January 7, 2020, 14:30, Tuesday to January 7, 2020, 20:00,Tuesday, with data points recorded at 30-minute intervals'.",
  ": Illustration of spatio-temporal prompt instructionsencoding the time- and location-aware information": "the day of the week and the hour of the day, while regional in-formation encompasses the city, administrative areas, and nearbypoints of interest (POI) data, among others. By incorporating thesediverse elements, UrbanGPT is capable of identifying and assimi-lating spatio-temporal patterns across different regions and time-frames. This enables the model to encapsulate these insights withincomplex spatio-temporal contexts, thereby enhancing its abilityfor zero-shot reasoning. The design of the instructional text forspatio-temporal information is illustrated in . 3.2.3Spatio-Temporal Instruction-Tuning of LLMs. Whenit comes to incorporating detailed spatio-temporal textual descrip-tions, the next stage is to fine-tune large language models (LLMs)using instructions to generate spatio-temporal forecasts in textualformat. However, this approach poses two challenges. Firstly, spatio-temporal forecasting typically relies on numerical data, which dif-fers in structure and patterns from natural language that languagemodels excel at processing, focusing on semantic and syntactic rela-tionships. Secondly, large language models are typically pre-trainedusing a multi-classification loss to predict vocabulary, resulting in aprobability distribution of potential outcomes. This contrasts withthe continuous value distribution required for regression tasks.To address these challenges, UrbanGPT adopts a different strat-egy by refraining from directly predicting future spatio-temporalvalues. Instead, it generates forecasting tokens that aid in the pre-diction process. These tokens are subsequently passed through aregression layer, which maps the hidden representations to generatemore accurate predictive values. The formulation of the regression",
  "Y, = W3[(W1H, ), (W2, )](5)": "The prediction result, denoted as Y, R, is obtained using therectified linear unit activation function, represented by . The hid-den representations of the forecasting tokens, denoted as , R, are introduced as a novel term in the vocabulary of large lan-guage models (LLMs). The regression layer is formulated usingweight matrices W1 R , W2 R , and W3 R2,where represents the concatenation operation. While the prob-ability distribution of the forecasting tokens remains relativelystable, their hidden representations contain rich spatio-temporalcontextual attributes that capture dynamic spatio-temporal interde-pendencies. This enables our model to provide precise predictionsby leveraging this contextual information.",
  "Model Optimization": "Building upon the baseline model , we adopt the absolute errorloss as our regression loss function. This choice allows us to effec-tively handle predictions across a wide range of urban scenarios.Additionally, we introduce a classification loss as a joint loss tocater to diverse task requirements. To ensure optimal performance,our model optimizes different losses based on the specific task in-puts. For instance, we utilize the regression loss for tasks such astraffic flow prediction, while employing the classification loss fortasks like crime prediction. This approach enables our model toeffectively address the unique challenges posed by each task anddeliver accurate predictions in various urban scenarios.",
  "Here, represents a sample from Y, and represents the totalnumber of samples, which is calculated as the product of , , and. We use various loss functions in our model, including L for": "binary cross-entropy loss, L for regression loss, and the cross-entropy loss adopted in our spatio-temporal language models. Tocapture probability distributions from the prediction, we employthe sigmoid function denoted by . Each of these loss functionsplays a specific role in our model, allowing us to effectively handleclassification, regression, and language modeling tasks as needed.",
  "Experimental Setting": "4.1.1Dataset Description. To evaluate the effectiveness of theproposed model in predicting spatio-temporal patterns across var-ious urban computing scenarios, we conducted experiments us-ing four distinct datasets: NYC-taxi, NYC-bike, NYC-crime, andCHI-taxi. These datasets encompass a wide range of data sourcesto capture the dynamic nature of urban environments, includingrecords of taxi travel, bike trajectories, crime incidents in New YorkCity, and taxi travel data in Chicago. To facilitate our analysis, wepartitioned the cities into grid-like regions based on latitude andlongitude information. Within specific time intervals, we aggre-gated statistical measures for each region. For example, this in-volved calculating the number of taxi inflows and outflows withina 30-minute period in region A, or determining the count of theftincidents within a day in region B. Furthermore, Points of Interest(POIs) data can be obtained through APIs provided by map services,utilizing the latitude and longitude of different regions. For morecomprehensive data descriptions, please refer to the Appendix. 4.1.2Evaluation Protocols. In order to investigate the capabili-ties of large language models in analyzing diverse spatio-temporaldata across different regions, we selected a subset of taxi, bike, andcrime data from various areas of New York City as our training set.",
  "Supervised Learning Scenarios. We evaluated the model usingfuture data from the same regions as the training set": "For regression tasks, we maintained a consistent training and test-ing methodology across all baseline models. When it came to classi-fication tasks involving crime data, we utilized binary cross-entropyas the loss function for training and testing the models. Our ex-periments were conducted using the robust vicuna-7b as thefoundational large language model for UrbanGPT. For a more com-prehensive understanding of our methodology and experimentalsetup, please refer to the appendix for detailed information. 4.1.3Evaluation Metrics. For regression tasks, we employedMAE (Mean Absolute Error) and RMSE (Root Mean Square Error)as evaluation metrics. These metrics quantify the discrepanciesbetween the predicted outcomes and the actual labels, with lowervalues indicating superior performance . In the case of clas-sification tasks, we utilized Recall and Macro-F1 as evaluation met-rics to assess performance. Recall measures the models ability tocorrectly identify positive instances, while Macro-F1 is a compre-hensive performance metric that combines precision and recall toprovide an overall measure of classification accuracy . 4.1.4Baseline Model. We conducted a thorough comparisonwith 10 advanced models to establish baselines for our proposedmethod. (i) In the category of RNNs-based spatio-temporal forecast-ing methods, we compared our proposed method with AGCRN ,DMVSTNET and ST-LSTM . These approaches leverageRNNs for modeling and prediction. (ii) The GNNs-based spatio-temporal models primarily utilize graph neural networks to cap-ture spatial correlations and integrate temporal encoders to capturespatio-temporal relationships. The models we compared againstin this category include GWN , MTGNN , STSGCN ,TGCN , and STGCN . (iii) In the attention-based spatio-temporal models category, the methods employ attention mecha-nisms to model spatio-temporal correlations. The models we com-pared against in this category are ASTGCN and STWA .",
  "Zero-Shot Prediction Performance (RQ1)": "In this section, we thoroughly evaluate the predictive performanceof our proposed model in zero-shot scenarios. The results of ourevaluation are presented in and visualized in . Ourobjective is to assess the models effectiveness in predicting spatio-temporal patterns in geographical areas that it has not encounteredduring training. This evaluation encompasses both cross-regionand cross-city settings, allowing us to gain insights into the modelsgeneralization capabilities across different locations. 4.2.1Prediction on Unseen Regions within a City. Cross-region scenarios entail using data from certain regions within acity to forecast future conditions in other regions that have notbeen encountered by the model. Through a thorough analysis ofthe models performance in these cross-region predictions, we candraw attention to three significant observations:i) Superior Zero-shot Predictive Performance. The results pre-sented in highlight the exceptional performance of ourproposed model in both regression and classification tasks on vari-ous datasets, surpassing the baseline models in zero-shot prediction.The success of our model can be attributed to two key factors. Spatio-Temporal-Text-Alignment. The alignment of spatio-temporal contextual signals with the text comprehension abil-ities of language models plays a pivotal role in the success ofour proposed model. This fusion enables the model to effectivelycapitalize on both the encoded urban dynamics from the spa-tiotemporal signals and the comprehensive understanding oftextual context provided by the LLMs. By leveraging these twoessential aspects, our model achieves the remarkable ability togeneralize its prediction capabilities in zero-shot scenarios. : Our models performance in zero-shot prediction is evaluated on three diverse datasets: NYC-taxi, NYC-bike, andNYC-crime, providing a comprehensive assessment of its predictive capabilities in unseen situations.",
  "UrbanGPT6.1616.926.8321.782.025.162.015.030.670.340.690.42": "Spatio-Temporal Instruction-Tuning. This adaptive tuningprocess empowers the LLM to effectively integrate crucial in-formation from the instructions, enhancing its comprehensionof the complex relationships and dependencies between spatialand temporal factors. By seamlessly merging the spatio-temporalinstruction-tuning with the spatio-temporal dependency encoder,our proposed model, UrbanGPT, successfully preserves universaland transferable spatio-temporal knowledge. Consequently, themodel becomes capable of capturing the fundamental patternsand dynamics that govern spatio-temporal phenomena, enablingit to make precise predictions in downstream zero-shot scenarios. ii) Enhanced Urban Semantic Understanding. Urban seman-tics offer valuable insights into the diverse dimensions of spatialand temporal characteristics. Our approach involves training ourmodel on a wide range of datasets, enriching its understandingof spatio-temporal dynamics across different timeframes and geo-graphical locations. In contrast, baseline models tend to prioritizeencoding temporal and spatial dependencies, neglecting the nu-anced semantics that differentiate regions, timeframes, and datacategories. By incorporating comprehensive semantic awarenessinto our UrbanGPT, we significantly enhance its ability to makeaccurate zero-shot predictions in previously unseen regions.iii) Improved Performance in Sparse Data Scenarios. Predict-ing spatio-temporal patterns in sparse data environments is chal-lenging as models tend to overfit when data points are scarce. Thischallenge is particularly notable when predicting crimes, where datais often sparse but crucial for accurate predictions. Baseline mod-els struggle in cross-regional prediction tasks under these sparseconditions, resulting in low recall scores that indicate potential over-fitting. To overcome this limitation, our model integrates spatio-temporal learning with large language models (LLMs) using aneffective spatio-temporal instruction-tuning paradigm. By incorpo-rating rich semantic insights, our approach enhances the modelsspatio-temporal representations, enabling it to effectively handlesparse data and achieve improved prediction accuracy. 4.2.2Cross-City Prediction Task. To assess the performance ofour model in cross-city prediction tasks, we conducted tests on theCHI-taxi dataset, which was not seen during the training phase. Theresults, depicted in , yielded the following observations: Consistency in Multi-step Prediction: Our model consistentlyoutperforms the comparison method at each time step. Notably,it maintains a significant advantage in both short-term and long-term spatio-temporal prediction, demonstrating the robustnessof our proposed model in cross-city prediction scenarios. Effective Knowledge Transfer Across Cities: The predictionresults obtained from the CHI-taxi dataset validate the supe-rior forecasting capabilities of our model in cross-city scenarios.This enhancement can be attributed to the integration of spatio-temporal encoders with the spatio-temporal instruction-tuningparadigm. By incorporating these components, our model ef-fectively captures universal and transferable spatio-temporalpatterns, allowing it to make accurate predictions. Additionally,by considering different geographical information and temporalfactors alongside the learned transferred knowledge, our modelsuccessfully associates spatio-temporal patterns exhibited bysimilar functional areas and historical periods. This comprehen-sive understanding provides valuable insights for making precisezero-shot predictions in cross-city scenarios.4.3Classical Supervised Prediction Task (RQ2) This section examines the predictive capabilities of our UrbanGPTin end-to-end supervised prediction scenarios, as presented in Ta-ble 2. We will discuss the results from two perspectives below.Enhanced Long-Term Forecasting Abilities: We examined themodels effectiveness in long-term spatio-temporal forecasting byutilizing test datasets that spanned broader time intervals. For in-stance, we trained the model using data from 2017 and evaluated itsperformance on data from 2021. The results of these experimentsdemonstrate that our UrbanGPT possesses a significant advantageover the baselines, highlighting its superior ability to generalizeacross different temporal landscapes. This capability reduces theneed for frequent retraining or incremental updates, making themodel more aligned with real-world applications. Additionally, theexperiments have confirmed that incorporating additional textualknowledge does not hinder model performance or introduce noise,thus further validating the feasibility of utilizing large languagemodels to enhance spatio-temporal forecasting tasks.Spatial Semantic Understanding: Accurately capturing spatialcorrelations is crucial in the realm of spatio-temporal prediction.",
  "UrbanGPT2.506.781.716.683.117.103.016.94": "Traditional methods often employ graph networks or attentionmechanisms to analyze these correlations. Models lacking dedi-cated spatial correlation modules, like LSTM, tend to underperformas they overlook the spatial context. In contrast, our model com-pensates for the absence of explicit spatial encoders by integratingextensive geographic and points of interest (POIs) data within thetextual input. This approach enables the model to comprehend theshared characteristics of areas with similar functions at a highersemantic level. Consequently, it deduces patterns of correlationbetween various functional zones and effectively represents theinterconnections among different regions.",
  "Ablation study (RQ3)": "This section investigates the impact of different key componentson the performance of our model, as illustrated in . Ourrigorous testing primarily revolves around the zero-shot scenariousing the NYC-taxi dataset. Through our analysis, we have distilledthe benefits offered by the different modules into four key points. (1) Impact of Spatial and Temporal Context: -STC. By re-moving time and spatial information from the instruction text,we observed a noticeable decline in the models performance.This can be attributed to the lack of temporal information, forcingthe model to rely solely on spatio-temporal encoders for encodingtime-related features and performing prediction tasks. Furthermore,the absence of spatial information hindered the models ability tocapture spatial correlations, making it challenging to analyze thedistinct spatio-temporal patterns of different areas. (2) Impact of Instruction-Tuning with Diverse Datasets: -Multi. We conducted our training solely on the NYC-taxi data toexamine whether incorporating multiple datasets would providevaluable insights to the LLMs in zero-shot scenarios. -STC-Multi-STEOurs-T2P NYC-taxi inflow 6.0 6.2 6.4 6.6 6.8 7.0 MAE NYC-taxi inflow 16.8 17.2 17.6 18.0 RMSE NYC-taxi outflow 6.6 6.8 7.0 7.2 7.4 7.6 7.8 8.0 MAE NYC-taxi outflow 21.5 22.0 22.5 23.0 23.5 24.0 24.5 RMSE",
  ": Ablation study of our proposed UrbanGPT": "However, this restricted training approach limited the modelsability to fully uncover the spatio-temporal dynamics of cities due tothe absence of diverse urban indicators. As a result, the models per-formance was suboptimal. By integrating diverse spatio-temporaldata from multiple sources, our model can effectively capture theunique characteristics of different geographical locations and theirevolving spatio-temporal patterns. (3) Impact of Spatio-Temporal Encoder: -STE. In this vari-ant, we disable the spatio-temporal encoder to investigate itseffect on aligning the large language model with encoded urbandependency dynamics into latent embedding space.The results clearly indicate that the absence of the spatio-temporalencoder significantly hampers the performance of the large lan-guage model in spatio-temporal prediction scenarios. This under-scores the crucial role played by the proposed spatio-temporalencoder in enhancing the models predictive capabilities. (4) Regression Layer Incorporation in Instruction-Tuning:T2P. We explicitly instructed UrbanGPT to generate its predic-tions in a textual format. However, the suboptimal performanceindicates the challenges in utilizing LLMs for precise numericalregression tasks, as opposed to employing regression layers.The primary challenge stems from the models dependence onmulti-class loss for optimization during training, leading to a mis-match between the models probabilistic output and the continuousvalue distribution necessary for spatio-temporal forecasting. Tobridge this gap, we incorporated a regression predictor into ourmodel, significantly improving its capacity to generate more precisenumerical predictions for regression tasks.",
  "Model Robustness Study (RQ4)": "In this section, we focus on evaluating the robustness of our Ur-banGPT across different spatio-temporal pattern scenarios. We cate-gorize regions based on the magnitude of numerical variations, suchas taxi flow, during a specific time period. Lower variance indicatesstable temporal patterns, while higher variance suggests diversespatio-temporal patterns in active commercial zones or denselypopulated areas. Our findings, shown in , reveal that most STGCNAGCRNGWNSTWAMTGNNOurs [0-0.25](0.25-0.5](0.5-0.75](0.75-1.0]0.0 0.5 1.0 1.5 2.0 2.5 3.0 [0.0, 0.75] (0.75, 1.0] NYC-taxi inflow RMSE [0-0.25](0.25-0.5](0.5-0.75](0.75-1.0]0.0 0.5 1.0 1.5 2.0 [0.0, 0.75] (0.75, 1.0] NYC-taxi outflow RMSE [0-0.25](0.25-0.5](0.5-0.75](0.75-1.0]0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 [0.0, 0.75] (0.75, 1.0] CHI-taxi inflow RMSE [0-0.25](0.25-0.5](0.5-0.75](0.75-1.0]0.0 0.5 1.0 1.5 2.0 2.5 [0.0, 0.75] (0.75, 1.0] CHI-taxi outflow RMSE",
  ": Robustness study of the UrbanGPT model": "models perform well in regions with lower variance, where patternsremain relatively stable. However, the baseline model struggles inregions with high variance, particularly within the (0.75, 1.0] range,resulting in inaccurate predictions. This limitation may stem fromthe baseline models difficulty in inferring spatio-temporal patternsin unseen regions during zero-shot scenarios. In practical applica-tions, accurate prediction of densely populated or bustling areasis crucial for urban governance, such as traffic light control andsecurity scheduling. Our UrbanGPT demonstrates significant per-formance improvement in the (0.75, 1.0] interval, highlighting itspowerful zero-shot prediction capability with our method.",
  "Case Study": "In our case study, we thoroughly evaluate several large languagemodels (LLMs) for zero-shot spatio-temporal prediction. We em-phasize the challenges these models face in directly understandingspatio-temporal patterns from numeric geo-series data. In con-trast, we showcase the exceptional performance of our proposedUrbanGPT framework in capturing universal spatio-temporal pat-terns and its ability to generalize effectively across various zero-shotspatio-temporal forecasting scenarios. For a more comprehensiveunderstanding, please refer to the Appendix.",
  "RELATED WORK": "Deep Spatio-temporal Prediction Models. Deep spatio-temporalprediction methods have gained prominence in deep learning dueto their impressive performance. These models typically consist oftwo components: temporal dependency modeling and spatial corre-lation encoding. Early models like D-LSTM and ST-resnet used RNNs and convolutional networks to model temporal andspatial dependencies. Graph neural networks (GNNs) proved tobe a natural fit for spatial correlation modeling, as seen in modelslike STGCN and DCRNN , which utilized graph structuresbased on node distances. Techniques such as learnable region-wisegraph structures and dynamic spatio-temporal graph net-works have further enhanced spatial correlation modeling.Furthermore, researchers have explored approaches such asmulti-scale temporal learning and multi-granularity tempo-ral learning to encode temporal dependencies. These strategiesenable the capture of features like long-term and short-term correla-tions as well as periodicity. These advancements have significantly contributed to the progress of spatio-temporal prediction. However,it is worth noting that the majority of these studies are tailoredfor supervised contexts, with limited research and developmentfocused on zero-shot spatio-temporal forecasting. This representsan important area that requires further exploration.Spatio-Temporal Pre-training. Spatio-temporal pre-training tech-niques have recently received significant research attention. Thesetechniques primarily focus on generative and contrastive pre-training models to enhance the predictive performance of down-stream tasks. There has also been extensive exploration of pretrain-finetune frameworks for few-shot learning scenarios , aim-ing to improve knowledge transferability by aligning source andtarget data. However, these approaches require training or fine-tuning on target data and lack zero-shot prediction capabilities. Inthis work, we address the challenge of data scarcity in downstreamurban scenarios by proposing UrbanGPT. Our model demonstratesthe ability to generalize well across various scenarios, mitigatingthe need for extensive training or fine-tuning on target data.Large Language Models. The emergence of large language mod-els has recently attracted significant attention due to theirunprecedented machine performance in tasks like text understand-ing and reasoning. These models have become a hot topic, demon-strating the potential to advance from intelligent algorithms toartificial intelligence. Open-source large language models such asLlama , Vicuna , and ChatGLM have been released,leading to researchers exploring their application in various fieldsto enhance transfer learning capabilities with domain knowledgefrom these models. In the computer vision domain, researchershave combined multimodal large language models with promptlearning methods to achieve zero-shot predictions in downstreamtasks . Furthermore, the capabilities of LLMs in graphreasoning , recommendation and traffic anal-ysis have been extensively studied. However, the utilizationof large language models for zero-shot spatio-temporal predictiontasks in the field of urban intelligence remains largely unexplored.",
  "CONCLUSION": "We present the UrbanGPT, a spatio-temporal large language model,to generalize well in diverse urban scenarios. To seamlessly alignthe spatio-temporal contextual signals with LLMs, we introduce aspatio-temporal instruction-tuning paradigm. This empowers theUrbanGPT with the remarkable ability to learn universal and trans-ferable spatio-temporal patterns across various types of urban data.Through extensive experiments and meticulous ablation studies,we demonstrate the exceptional effectiveness of the UrbanGPTsarchitecture and its key components. However, it is important toacknowledge that while the results are promising, there are stilllimitations to be addressed in future studies. As a first step, we areactively engaged in collecting a more diverse range of urban datato enhance and refine the capabilities of our UrbanGPT across abroader spectrum of urban computing domains. Additionally, un-derstanding the decision-making process of our UrbanGPT is of im-portance. While the model demonstrates exceptional performance,providing interpretability and explainability is equally essential.Future research efforts will focus on empowering our model withthe ability to interpret and explain its predictions.",
  "Lei Bai, Lina Yao, Can Li, Xianzhi Wang, and Can Wang. 2020. Adaptive GraphConvolutional Recurrent Network for Traffic Forecasting. In NeurIPS. 1780417815": "Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski,Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Pi-otr Nyczyk, and Torsten Hoefler. 2024. Graph of Thoughts: Solving ElaborateProblems with Large Language Models. arXiv:2308.09687 Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, et al. 2020.Language Models Are Few-Shot Learners. In NeurIPS. 18771901. Ziwei Chai, Tianjie Zhang, Liang Wu, Kaiqiao Han, Xiaohai Hu, Xuanwen Huang,and Yang Yang. 2023. Graphllm: Boosting graph reasoning ability of large lan-guage model. arXiv preprint arXiv:2310.05845 (2023).",
  "Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin,and Chao Huang. 2023. Graphgpt: Graph instruction tuning for large languagemodels. arXiv preprint arXiv:2310.13023 (2023)": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-AnneLachaux, Timothe Lacroix, Baptiste Rozire, Naman Goyal, Eric Hambro, FaisalAzhar, et al. 2023. Llama: Open and efficient foundation language models. arXivpreprint arXiv:2302.13971 (2023). Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-mine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-ale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXivpreprint arXiv:2307.09288 (2023). Beibei Wang, Youfang Lin, Shengnan Guo, and Huaiyu Wan. 2021.GSNet:Learning Spatial-Temporal Correlations from Geographical and Semantic Aspectsfor Traffic Accident Risk Forecasting. AAAI (2021), 44024409.",
  "A.1Experimental Details Description": "A.1.1Dataset Details. We collected data on taxi flows, bicycleflows, and crime incidents in New York City for training and evalu-ation. The NYC-taxi dataset contains 263 regions, with each regionmeasuring approximately 3km x 3km. The time sampling inter-val for this dataset is 30 minutes. The NYC-bike and NYC-crimedatasets consist of 2162 regions, each represented by a 1km x 1kmgrid. The time sampling interval for NYC-bike is also 30 minutes,while for NYC-crime, it is 1 day. All datasets cover the time periodfrom Jan 1, 2016, to Dec 31, 2021, in NYC. The CHI-taxi datasetincludes 77 regions, with each region measuring approximately4km x 4km. This dataset includes all taxi data from January 1, 2021,to December 31, 2021, with a time sampling interval of 30 minutes. A.1.2Hyperparameters Settings. The parameters for the di-lation convolution kernel in the time encoder are set as follows:, , and are all set to 32, with a dilation factor of 1. Forour prediction task, we aim to predict the next 12 steps of databased on the previous 12 steps. Both the history length () andprediction length () are set to 12. The projection layer parametersare configured with set to 64 and set to 4096. Lastly, the hiddenlayer parameter for the regression layer is set to 128. A.1.3Further Experimental Setup Descriptions. During theinstruction-tuning phase, we randomly selected 80 regions fromthe three datasets in New York City as training data. Its importantto note that the region indices were kept consistent for the NYC-bike and NYC-crime datasets. The training sets had specific timeintervals: for NYC-taxi datasets, it ranged from January 1, 2017,to March 31, 2017; for NYC-bike datasets, it covered April 1, 2017,to June 30, 2017; and for NYC-crime datasets, it spanned fromJanuary 1, 2016, to December 31, 2018. For the pretraining of thespatio-temporal dependency encoder and the baseline models, weutilized the same data for training to optimize the parameters. Themaximum epoch count was set at 100. In the testing phase, weconducted the following evaluations: (i) Zero-Shot Prediction. We also selected an additional 80regions from the New York City datasets as unseen test data. Forthe NYC-bike and NYC-taxi datasets, we used the first two weeksof data in 2020 for testing. As for the NYC-crime dataset, we usedthe entire year of 2020 for testing. In the case of the Chicago citydataset, we evaluated the model using all the data from Dec 2021. (ii) Classical Supervised Prediction: For evaluation purposes,we chose the data with the longest time interval, which involvedtesting the model using all the data from the NYC-bike and NYC-taxi datasets specifically for the month of December 2021.",
  "AGCRN: RNNs are employed to capture temporal correlations,allowing for the representation of the evolving patterns over time": "DMVSTNET: In this method, RNNs are utilized to effectivelymodel temporal dependencies, capturing the patterns that evolveover time. Furthermore, convolutional networks and fully con-nected layers are employed to capture local spatial correlationsand establish meaningful spatial relationships.Attention-based Spatio-Temporal Approaches: ASTGCN: In this method, attention mechanisms are employedto capture multi-granularity temporal correlation features. STWA: The model incorporates personalized temporal andspatial parameters into the attention module, allowing for themodeling of dynamic spatio-temporal correlations.Spatio-Temporal GNNs: GWN: It incorporates a learnable graph structure and 1-Dconvolutions to effectively learn spatio-temporal dependencies.",
  "A.2Case study": "In this section, we assess the effectiveness of different large lan-guage models (LLMs) in zero-shot spatio-temporal prediction sce-narios, as illustrated in and . The instructions pro-vided to the models are clearly indicated in blue font. The resultsdemonstrate that various LLMs are capable of generating predic-tions based on these instructions, thereby highlighting the effective-ness of the prompt design. For instance, ChatGPT relies on historicalaverages rather than explicitly incorporating temporal or spatialdata in its predictions. Llama-2-70b analyzes specific time periodsand regions, but it encounters challenges in encoding numericaltime-series dependencies, resulting in suboptimal predictive perfor-mance. On the other hand, Claude-2.1 effectively summarizes andanalyzes historical data, leveraging peak-hour patterns and pointsof interest to achieve more accurate traffic trend predictions.Our proposed UrbanGPT seamlessly integrates spatio-temporalcontextual signals with the reasoning capabilities of large languagemodels (LLMs) through a spatio-temporal instruction-tuning para-digm. This integration leads to remarkable improvements in predict-ing numerical values and spatio-temporal trends. These findingsunderscore the potential and effectiveness of our framework incapturing universal spatio-temporal patterns, making zero-shotspatio-temporal prediction practical and achievable.",
  ": We examine the zero-shot predictions of different LLMs for bicycle flow in NYC with the provided instructions": "Instructions: Given the historical data for bike flow over 12 time steps in a specific region of New York City, the recorded bike inflowsare , and the recorded bike outflows are . The recording time of thehistorical data is January 14, 2020, 12:00, Tuesday to January 14, 2020, 17:30, Tuesday, with data points recorded at 30-minute intervals.Here is the region information: This region is located within the Staten Island borough district and encompasses various POIs within aone-kilometer radius, covering Education Facility, Cultural Facility, Commercial, Transportation Facility categories. We now aim topredict the bike inflow and outflow for the next 12 time steps during the time period of January 14, 2020, 18:00, Tuesday to January 14,2020, 23:30, Tuesday, with data points recorded at 30-minute intervals. To improve prediction accuracy, a spatio-temporal model isutilized to encode the historical bike data as tokens <ST_Start><ST_HIS><ST_HIS><ST_End>, where the first and the second tokenscorrespond to the representations of bike inflow and outflow. Please conduct an analysis of the traffic patterns in this region, taking intoaccount the provided time and regional information, and then generate the predictions (the predictive tokens for regression).",
  "Ground true: Based on the given information, the predictions of bike inflow and outflow in this region are and": "Llama-2-70b Responds: Based on the provided historical data, I have ... Here are my predictions for the bike inflow and outflow for thenext 12 time steps:Bike Inflow: . Bike Outflow: .These predictions are based on the following analysis: Time of Day: The traffic patterns during this time are likely to be influenced bythe evening rush hour, with increased bike inflows and outflows during the peak hours...Regional Information: ... This suggests thatthere may be a mix of both commuter and recreational bike traffic in the area... ChatGPT Responds: To analyze the traffic patterns in the given region and make predictions for the next 12 time steps, we can calculatethe average bike inflow and outflow based on the historical data... Now, we can generate predictions for the next 12 time steps: PredictedBike Inflow: [11.67 11.67... 11.67]. Predicted Bike Outflow: [9.58 9.58... 9.58]... Claude-2.1 Responds: Based on the provided historical data, this region in Staten Island seems to experience fluctuating yet relativelylow bike traffic flow during the daytime on Tuesdays. The bike inflows range from 6 to 15 per 30-minute interval, while outflows rangefrom 4 to 15.Given the region covers education, cultural, commercial, and transportation facilities within a 1km radius, the bike trafficlikely corresponds to people traveling to and from these points of interest. The fluctuations could reflect class/work schedules andtransportation needs throughout the day.For the evening prediction period of 6PM to 11:30PM, we may expect some further fluctuationsbut generally declining bike traffic as the day turns to night. People would be finishing up activities and heading home. I would predictthe following bike inflows and outflows for the next 12 time steps: and ...",
  ": Zero-shot predictions of different LLMs for taxi flow in NYC with the provided instructions (without spatial information)": "Instructions: Given the historical data for taxi flow over 12 time steps in a specific region of New York City, the recorded taxi inflowsare , and the recorded taxi outflows are . The recording time of thehistorical data is January 5, 2020, 20:30, Sunday to January 6, 2020, 02:00, Monday, with data points recorded at 30-minute intervals. Nodescription is available for this region. Now we want to predict the taxi inflow and outflow for the next 12 time steps during the timeperiod of January 6, 2020, 02:30, Monday to January 6, 2020, 08:00, Monday, with data points recorded at 30-minute intervals...",
  "Llama-2-70b Responds: Based on the provided historical data, I have analyzed the traffic patterns... Taxi Inflow: Taxi Outflow: . The predictions are based on the following factors": "ChatGPT Responds: To generate predictions for the next 12 time steps, we can use the average values as a baseline...Predicted taxiinflows: [53.33 53.33... 53.33]. Predicted taxi outflows: [40.83 40.83... 40.83]... Claude-2.1 Responds: The data covers evening hours on a Sunday and early morning hours on a Monday. We can expect lower taxidemand during later Sunday evening, with some pickup around end-of-weekend times and early Monday commute times. The regionis unspecified, so we cannot infer any location-specific usage patterns. Inflow ranges from 49-111 taxis over the 30 minute intervals.Outflow ranges from 19-100 taxis. This indicates the region likely serves both residential and commercial purposes.Prediction: Inflows: . Outflows: ..."
}