{
  "Abstract": "Diffusion models have achieved remarkable results in multiple domains of generative modeling.By learning the gradient of smoothed data distributions, they can iteratively generate samplesfrom complex distributions, e.g., of natural images. The learned score function enables theirgeneralization capabilities, but how the learned score relates to the score of the underlyingdata manifold remains largely unclear.Here, we aim to elucidate this relationship bycomparing the learned scores of neural-network-based models to the scores of two kinds ofanalytically tractable distributions: Gaussians and Gaussian mixtures. The simplicity of theGaussian model makes it particularly attractive from a theoretical point of view, and weshow that it admits a closed-form solution and predicts many qualitative aspects of samplegeneration dynamics. We claim that the learned neural score is dominated by its linear(Gaussian) approximation for moderate to high noise scales, and supply both theoreticaland empirical arguments to support this claim. Moreover, the Gaussian approximationempirically works for a larger range of noise scales than naive theory suggests it should, and ispreferentially learned by networks early in training. At smaller noise scales, we observe thatlearned scores are better described by a coarse-grained (Gaussian mixture) approximationof training data than by the score of the training distribution, a finding consistent withgeneralization. Our findings enable us to precisely predict the initial phase of trained modelssampling trajectories through their Gaussian approximations. We show that this allowsone to leverage the Gaussian analytical solution to skip the first 15-30% of sampling stepswhile maintaining high sample quality (with a near state-of-the-art FID score of 1.93 onCIFAR-10 unconditional generation). This forms the foundation of a novel hybrid samplingmethod, termed analytical teleportation, which can seamlessly integrate with and accelerateexisting samplers, including DPM-Solver-v3 and UniPC. Our findings strengthen the fieldstheoretical understanding of how diffusion models work and suggest ways to improve thedesign and training of diffusion models.",
  "Introduction": "Diffusion models (Sohl-Dickstein et al., 2015; Song & Ermon, 2019; Ho et al., 2020; Song et al., 2021) haverevolutionized the field of generative modeling by achieving remarkable performance across diverse domains,including image (Rombach et al., 2022), audio (Kong et al., 2021; Chen et al., 2021; Popov et al., 2021), andvideo (Harvey et al., 2022; Ho et al., 2022; Blattmann et al., 2023) generation. Despite these successes, whydiffusion models perform as well as they do is poorly understood. Two major open questions are as follows.",
  "Denoiser along Diffusion Sampling Trajectory": ": Gaussian score well-approximates the learned neural score. A. Schematic illustrating ourworks main claim. In the high noise regime, the neural score is well-approximated by both the Gaussian/linearscore and the score of the training set; in the low noise regime, neural scores are better described by theGaussian model. B. Visual demonstration of the effect. Denoiser outputs along PF-ODE trajectories weresimilar at high noise, regardless of the score model (neural score, Gaussian score, or delta mixture score); atsmall noise scales, neural denoiser outputs more closely resemble those of the Gaussian model. First, given that samples are generated via a dynamic process from noise, when do different sample featuresemerge, and what controls which features appear in the final sample? Second, given the empirical fact thatdiffusion models often generalize beyond their training data, what distribution do they learn instead? Central to understanding these models is characterizing the score function, the dynamic vector field learnedby a neural network during training. By modeling the gradient of the smoothed data distribution, the scorefunction guides the iterative sample generation process. Curiously, it has been observed to deviate from itstheoretically expected behavior: it may not be the gradient of a mixture of data points, and it may not evenbe a gradient field at all (Wenliang & Moran, 2023). Such deviations raise fundamental questions about thenature of what networks learn and how they generalize training data. How does the learned score function compare to that of the underlying data manifold, especially given thatneural networks only have access to a finite set of training examples in practice? Critically, a network thatlearns the exact score function of the training seti.e., a mixture of delta functions centered on trainingdatacan only reproduce training examples, and hence cannot generalize. This observation suggests thatneural networks optimize for a balance between learning the score function of the training set and capturingaspects of the underlying data manifold. However, the precise characteristics of the learned score function,and how the development of this balance proceeds over the course of training, remain largely unexplored. One simple possibility is that diffusion models generalize in part by learning the score function associatedwith certain summary statistics of the traning set, like its mean and covariance matrix. If this were true, onewould expect the score function to be that of a Gaussian model, i.e., linear in its state features. While it isdecidedly not true that learned neural scores are Gaussian, we find that this is much closer to being true thanone might expect. To show this, we first mathematically characterize the properties of the Gaussian model.Next, we compare the Gaussian models score function to real score functions, and find that the Gaussianmodel well-approximates the behavior of neural network models for moderate to high noise levels (A).Finally, we show how this insight can be applied to accelerate sampling from diffusion models (B).",
  "Mathematical formulation": "In this section, we review the mathematics of diffusion models and define the idealized score models of interest.To streamline notation and match popular implementations of diffusion models, we use the EDM frameworkof Karras et al. (2022). This choice implies no loss of generality, as a simple reparametrization allows one tomap to other formalisms (Song et al., 2021; Ho et al., 2020). See Appendix D.2 for more details on this point.",
  "Basics of score-based modeling": "Let pdata(x) be a data distribution in RD. The core idea of diffusion models is to corrupt this distributionwith (usually Gaussian) noise, and to learn to undo this corruption; this way, one can sample from thegenerally complex distribution p(x) by first sampling from a Gaussian, and then iteratively removing noise.The noise-corrupted distribution is defined as",
  "dx = tt s(x, t) dt(2)": "where t denotes time and t denotes the noise scale at time t (with t its derivative)1. The key ingredientof this process is the score function s(x, ) := x log p(x; ), i.e., the gradient of the noise-corrupted datadistribution. To generate a sample, one samples xT N(x; 2T I) with a large T = max, and then integratesEq. 2 backward in time until tmin = min 0. Since we are interested in understanding these dynamics indetail, we must carefully study the dynamic vector field s(x, ). There are various ways to learn a parameterized score approximator s(x, ) (Yang et al., 2023), includingscore matching (Hyvrinen, 2005) and sliced score matching (Song et al., 2020b), but the current most popularand performant approach is denoising score matching (Raphan & Simoncelli, 2006; Vincent, 2011; Song &Ermon, 2019). The corresponding objective, which is minimized by the score function of the training set, is",
  ".(4)": "Sample generation dynamics are most usefully understood in terms of this denoiser, which supplies an estimatex0 = D(x, ) of the PF-ODEs endpoint given the current state x and noise scale 2. The mathematicalquestion we are most interested in becomes: how does x0 evolve throughout sample generation? 1Although EDM conventionally chooses t = t, we do not assume this to keep our results slightly more general.2We will use the terms denoiser output D and endpoint estimate x0 interchangeably in what follows.",
  "Delta Mixture | sigma = 0.5": ": Structure of Gaussian, Gaussian mixture, and delta mixture scores. Upper row:Examples of 2D probability density functions for Gaussian, Gaussian mixture model (GMM), and deltamixture distributions. Lower row: The respective score vector fields. For mixture models, the space iscolored based on the weighting function wi(x) from Eq. 7, with a unique color (red, blue, or green) assignedto each Gaussian component. Connection to alternative frameworks.In Eq. 2, the norm of xt changes significantly over time. Manyalternative frameworks prefer a probability flow ODE or SDE where the norm of the state remains morestable, such as DDPM (Ho et al., 2020), DDIM (Song et al., 2020a), and VP-SDE (Song et al., 2021). Asnoted by Karras et al. (2022), these formulations produce dynamics equivalent to Eq. 2 when xt is rescaledby a time-dependent factor xt xt = txt and time is reparametrized, allowing their solutions to be directlymapped to one another. For simplicity, we present most of our results (except some in Sec. 3.3) without thetime-dependent scaling factors; however, with minor modifications, these results are applicable to modelswith state scaling. Detailed results for alternative formulations are provided in Appendices D.3 and B.2.",
  "Idealized score models of interest": "In this paper, we will try to understand the score functions learned by real diffusion models by comparingthem to the score functions of certain idealized distributions. We consider three such idealized score models:the Gaussian model, which assumes only the overall mean and covariance of the data are learned; the scoreof the delta mixture distribution, which is the Exact score of the training dataset, assuming training data areprecisely memorized and that no generalization occurs; and the Gaussian mixture model (GMM), which liessomewhere between the previous two models. Below, we write down some basic but important properties ofeach of these idealized score models. Gaussian model.Suppose the target distribution is a Gaussian distribution with mean RD andcovariance RDD. Since the effective dimensionality of data manifolds is often much lower than thedimensionality of the ambient space (Turk & Pentland, 1991; Hinton & Salakhutdinov, 2006; Camastra &Staiano, 2016) (consider, e.g., the dimensionality of image manifolds versus the dimensionality of pixel space),we allow to have rank r D. At noise scale , the score is that of N(, + 2I), which reads",
  "Published in Transactions on Machine Learning Research (12/2024)": "From this, it is immediately clear that the correction term is not completely arbitrary. First, J is alwaysnegative. Second, its time course is bowl-shaped: it begins close to zero (since T 0), becomes morenegative, then ends close to zero (since 0 0). It achieves its most negative value roughly when t andt",
  "x i22 ) .(11)": "This models optimal denoiser is a weighted combination of training examples. The noise scale acts like atemperature parameter: in the large limit, the score points towards the data mean; in the 0 limit, thescore pushes with infinitely strong force towards the training example closest to the current state x. Thus,generating samples using this score always (in the absence of numerical errors) reproduces training examples. This score model is special, since it is the exact score of the finite training dataset (without augmentation), i.e.,the score function minimizing the score matching objective (Eq. 3). Since we expect a sufficiently expressivescore approximator to converge to this model in the absence of additional influences (e.g., regularizationand early stopping), comparing learned scores to this model speaks to the question of generalization. If ascore approximator learns something other than this, then the model has been implicitly regularized andgeneralizes beyond the training set to some extent.",
  "Exact solution and interpretation of the Gaussian score model": "In this section, we present the exact solution to the Gaussian model. The utility of this model is that it issimple enough that it can be solved exactly, and hence one can precisely quantify various aspects of samplegeneration dynamics. First, we briefly describe the solution method (Sec. 3.1-3.2), and then we discuss thequalitative insights we can obtain from it (Sec. 3.3). In Sec. 4-5, we will show how this admittedly simplemodel relates to real diffusion models.",
  "Solution method: Exploiting a decomposition of the covariance matrix": "Since the covariance is symmetric and positive semidefinite, it has a compact singular value decomposition = UUT , where U = [u1, ..., ur] is a D r semi-orthogonal matrix and Rrr is diagonal withk := kk > 0 for all k. The columns of U, uk, are the principal component (PC) axes along which theGaussian mode varies, and their span comprises the data manifold of the Gaussian model.",
  "xT := (I UUT )(xT )ck(T) := uTk (xT )": "The solution has three components: (i) the distribution mean, (ii) an off-manifold component, and (iii) anon-manifold component. The distribution mean term does not change throughout sample generation. Theoff-manifold component shrinks to zero as t 0. The on-manifold component, which is determined by themanifold-projected difference between x and , evolves independently according to (t, ) along each PCdirection.",
  "/(2t + 2t), which quantifies the amplification effect of a perturbationalong PC uk at time t (Eq. 23)": "Solution with data scaling term.The popular VP-SDE (Song et al., 2021) is an alternative to the EDMformulation, and its forward process is characterized by the transition probability p(xt|x0) = N(tx0, 2t I).Using the VP-SDE is equivalent to introducing a time-dependent scaling term t in Eq. 2. Thus, we canobtain the solution by substituting xt xt/t and t t/t. The solution reads",
  ": Geometry of sample generation trajec-tories for the VP-SDE": "The closed-form solution of the Gaussian model pro-vides the exact relationship between the covariancematrix and sample generation dynamics. Since thedynamics are separable along each PC, we only needto study the functions and to describe how thevariance of the data distribution along each PC in-fluences dynamics along that direction. Here, wehighlight interesting consequences of the Gaussianmodels exact solution related to four aspects ofsample generation: 1) the determination of the finalsample, 2) the geometry of trajectories, 3) the featureemergence order, and 4) the effect of perturbations. In this subsection, we describe these consequencesin the context of the VP-SDE formulation, whichincludes the additional t data-scaling factor. Thesignal scale t is assumed to decrease with time, thenoise scale is assumed to increase with time, and wealso assume 2t + 2t = 1 at all times. We visualize t, t along with the functions (t, ), (t, ) to provideintuition about the Gaussian solution ().",
  "k=1(0, k) ukuTk (xT T ) ,(21)": "is reminiscent of linear filtering. The final location of x0 along each axis uk is determined by the projectionof the (mean subtracted) initial noise onto that axis, amplified by the standard deviation (0, k) k.Thus, the subtle alignment between the initial noise pattern and the covariance of the data distributiondetermine what is generated. Note that even if the initial overlap is dominated by particular features, due tothe amplification effect of (0, k) the final sample may be dominated by other features that vary more.",
  "ck(T)uk ,": "i.e., that xt dynamics tend to look like a rotation or a spherical interpolation within the 2D plane formed byx0 and xT , up to on-manifold correction terms (see Appendix D.4 for the derivation and more discussion).The correction terms tend to be small; assuming r D, and that the typical overlap between the initialnoise and any given eigendirection is roughly 1/",
  "D 1 .(22)": "Feature emergence order.The solution of the denoiser trajectory D(t) (Eq. 20) implies that the outputsof the denoiser remain on the data manifold throughout sample generation. This explains why the outputs ofwell-trained models resemble noise-free images, rather than images contaminated with noise. Initially, as all (T, ) 0, the expected outcome D(T) starts close to the distribution mean . For aface dataset, this might resemble the so-called generic face (Langlois & Roggman, 1990). The denoiseroutput moves along each PC direction according to the function (C). The sigmoidal shape of (t, )indicates that the feature associated with a given PC changes the most when t t , i.e., when thenoise variance matches the scaled signal variance. After that point, the feature stabilizes, suggesting a\"critical period\" of development for each feature (D). Moreover, since the critical period happens whent 1/",
  "+ , features appear in order of descending variance. The highest-variance features appear earliest,and as generation proceeds more and more lower-variance features are unveiled": "This has interesting implications for image diffusion models. Natural images have more variance in lowfrequencies than high frequencies (Ruderman, 1994). Furthermore, for face images, semantic featureslikegender, head orientation, skin color, and backgroundshave higher variance than subtle features such asglasses, facial, and hair textures (Wang & Ponce, 2021). Hence, facts about natural image statistics andsample generation dynamics together explain why features like scene layout are specified first in the endpointestimateor equivalently why generation is usually, outline-first, details later. Effect of perturbations.Finally, we examined the effect of perturbations on feature commitment. Supposeat time t (0, T) the off-manifold directions are perturbed by x and the on-manifold direction uk isperturbed by an amount ck. The effect of the perturbation on the generated sample x0 is",
  "The far-field Gaussian structure of real diffusion models": "Most distributions interesting enough to train generative models on are not Gaussian, so how is the exactsolution of the Gaussian model related to real diffusion models trained on complex datasets? Our centralclaim is that, at moderate to high noise scales, the score function of an arbitrary bounded point cloud isnearly indistinguishable from that of a Gaussian with matching mean and covariance. We call such structurefar-field; we borrow the term from physics, where it describes a region far enough from the source of (e.g.,electromagnetic) waves that the size and shape of the source can be neglected. First, we provide theoretical arguments to support this claim (Sec. 4.1). Second, we present two empiricalanalyses that support it. In Sec. 4.2 and 4.3, we study how well the Gaussian model approximates the scorefunction, sampling trajectories, and samples generated by pre-trained diffusion models. In Sec. 4.4, we gobeyond the Gaussian model and systematically compare the learned neural score with the scores of GMMswith varying numbers of components and covariance matrix ranks. In the following section (Sec. 5), we trackthe neural score field throughout training and examine when different types of score structure emerge.",
  "Theoretical basis for far-field Gaussian score structure": "Consider a point cloud {yi}Ni=1 RD. When the noise level is high and/or the query point is far from thepoint clouds support, we claim that the score of this distribution is nearly indistinguishable from that of aGaussian with the same mean and covariance. Here, we provide a theoretical argument for this claim.",
  "Is the Gaussian approximation really only accurate when": "tr? In the next subsection, we empiricallyshow that it works quite well even for somewhat lower noise scalesand in particular when the noise scaleis smaller than the top eigenvalue of the covariance matrix (). The surprisingly broad range of noisescales for which the Gaussian approximation works well is what led us to note its unreasonable effectiveness.",
  "Learned score vectors are empirically well-approximated by the Gaussian model": "Motivated in part by the theoretical argument from the previous subsection, we empirically validated theclaim that the score function of pre-trained diffusion models is well-described by the Gaussian model inthe far-field/high-noise regime. To do this, we examined the score functions of three pre-trained models (ofCIFAR-10, FFHQ64, and AFHQv2-64) from Karras et al. (2022). For each dataset, we computed the scoresof several tractable approximations of the data:",
  "We characterized the average deviation as a function of noise scale for each type of idealized score ()": ": Gaussian score dominates the learned neural score at high noise scales. Fractionof squared error as a function of noise scale between neural score function and various analytical scoreapproximations. Note the log scale on the y-axis. Upper panel, all noise levels. Lower panel, zoom in tolow-noise regime. For reference, vertical lines show the square roots of the 10 largest covariance eigenvalues. Gaussian score predicts the learned score at high noise.For all three datasets, at most noise levels( > 1.08), the Gaussian score explains almost all variance (> 99%) of the neural score. As expected, itexplains more variance than the isotropic model, which does not incorporate covariance information. Further,for CIFAR-10, at all levels, the 10-class Gaussian mixture score predicts the neural score slightly better thanthe Gaussian score, which shows that adding more modes indeed helps capture the details of the score field.Surprisingly, at most noise levels ( > 1.08) the 10-mode model improved the fraction of explained variance",
  "Gaussian model empirically captures early sample generation dynamics": "If the Gaussian model produces score vectors similar to those learned by neural networks, then their samplingtrajectories may (at least initially) be similar too. On the other hand, it is also possible that initially smalldifferences may accumulate over the course of sample generation, and hence that real sampling trajectoriesdiffer substantially from those predicted by the Gaussian model4. To test this, we compared the solutions ofPF-ODE based on the idealized score models to the sampling trajectories of neural diffusion models withdeterministic samplers. We used the exact solution for the Gaussian model (Eq. 15), Heuns 2nd-ordermethod to integrate the neural scores, and an off-the-shelf RK4 integrator to integrate the mixture scores. Gaussian model predicts early diffusion trajectories.We found that the early phase of reversediffusion is well-predicted by the Gaussian analytical solution (blue trace in , 16), whose trajectoryremained close to those of the trained diffusion models (MSE < 0.01) before diverging. For CIFAR-10, thepoint of divergence was around 9 sampling steps ( 1.92), and for FFHQ and AFHQ it was around 17steps ( 4.37). This roughly corresponds to the scale where the Gaussian approximation of the score fieldbreaks down, and the score needs to be approximated by that of a more complicated distribution (). The exact score model predicts early sample generation dynamics slightly better than the Gaussian model(green trace in , 16), but diverges earlier ( < 15) from the neural trajectory, and by a much largeramount than the Gaussian model, consistent with our earlier score function observations ().",
  "Distance between analytical and EDM samplesCIFAR10 (rescaled to 224 pix)C": ": Gaussian model predicts early denoiser trajectories and low-frequency features ofsamples. A. The denoiser output D(xt, t) along a sampling trajectory of the EDM model and the Gaussiansolution with the same initial condition xT . B. Samples generated by the EDM model, Gaussian solution,and the exact delta mixture scores from the same initial condition. C. Image samples from the Gaussianand GMM models are closer to actual diffusion samples than samples from the delta score model. Gaussian model predicts low-frequency aspects of diffusion samples.We visualized the samplesgenerated by the neural and the idealized score models ( A,B), and found that the Gaussian modelssamples closely replicated several characteristics of those produced by trained models, including their globalcolor palette, background, the spatial layout of objects, and face shading, among other features. This observation is consistent with our earlier theoretical results given well-known facts about natural imagestatistics. These characteristics represent relatively low-spatial-frequency information, which contains farmore variance than high-frequency information (Ruderman, 1994). As predicted by our Gaussian modelresults (C), and empirically shown by Ho et al. (2020), high-variance image features are determinedin the early phase of sampling, during which the neural trajectory is accurately described by the Gaussianmodel. Consequently, the Gaussian model can predict the layout of the final image; given that it doesnot fully describe learned neural scores, especially in the low-noise regime, it is unsurprising that it is lesssuccessful at predicting high-frequency details like edges and textures. Gaussian model predicts diffusion samples more accurately than exact delta score.Interestingly,the samples generated by the exact delta score visually deviate even more from the EDM samples thanthose generated by the Gaussian model ( B). We quantified this observation using the pixelwise meansquared error (MSE) and Perceptual Similarity (LPIPS) metrics, with further details provided in AppendixA.3. Across all metrics, samples from pre-trained diffusion models more closely resembled Gaussian-generatedsamples than samples produced using the exact delta score ( C). This discrepancy was significant,albeit less pronounced, when using the perceptual similarity metric, except in the case of the AlexNet-basedLPIPS distance on the AFHQ dataset, where the difference was not significant (). These findings indicate, perhaps contrary to expectation, that the Gaussian model in many ways provides abetter quantitative approximation of the behavior of real diffusion models than the score of the training set.This is true in various senses, and appears to be true whether one uses pixel space or perceptual metrics.",
  "Beyond Gaussian: Low-rank Gaussian mixture scores as a model of learned neural scores": "Our previous analyses demonstrated that learned score functions closely match the Gaussian model at highnoise levels, but not at low noise levels. To study the behavior of the learned score function at lower noiseor,equivalentlycloser to the data manifold, we need to go beyond the single Gaussian. As a next step, westudied the score of a Gaussian mixture model whose covariance was allowed to be low-rank. We asked 1) towhat extent does adding Gaussian components help explain the learned neural score? and 2) what is theminimum rank required for good score approximation?",
  "Score Residual of EDM vs GMM score | CIFAR10 uncond EDM pretrained": ": Deviation of learned neural score from Gaussian mixture model with varying com-ponents and ranks. Upper: Residual explained variance (EV) plotted as a function of the number ofGaussian modes on the x-axis, with each colored line representing a different rank. Each panel compares thescore at a certain noise scale . Lower: An alternative view of the same data plot, with the rank of thecovariance matrix on the x-axis. See for analogous MNIST results. We systematically compared the structure of neural scores to the scores of GMMs fit to the same trainingdata (). We varied both the number of modes and the ranks of the associated covariance matrices;for the details of the fitting procedure, see Appendix A.5. In brief, we performed k-means clustering on thetraining set and utilized the empirical mean and covariance of each cluster to define Gaussian components.The fraction of samples within each cluster was used to define the weights of components. To obtain low-rankcovariance matrices, we computed the eigendecomposition of the empirical covariances and retained the top rdirections. Neural score is best explained by Gaussian mixture with moderate number of modes.First,note that both the Gaussian model and exact score models are special cases of the GMM; the former hasK = 1 and the latter has K = N. This leads us to expect that making K larger than 1 helps approximatethe learned neural score, but only up to a point. Indeed, we found that at most noise scales ( > 0.05), increasing the number of Gaussian modes reduces thedeviation between the neural score and the analytical score. Further, measured by the increase of explainedvariance, the benefit of adding Gaussian modes increases as we lower the noise level: it rises from 108 to102 (A). But this trend does not hold indefinitely. At smaller noise scales, augmenting the Gaussianmodes beyond a certain numberfor instance, 200-500 for MNIST and 100 for CIFAR-10increases the gap",
  "A.B.C": ": Impact of number of modes and covariance rank on neural score approximation. A.Contribution of multiple mixture components beyond one Gaussian (full rank). Improvement of the fractionof Exp. Var. is plotted on the y-axis; the vertical line means certain mixture models perform even worsethan the single Gaussian. B. Minimal rank required for each noise level. Each line shows the minimumrank for different numbers of GMM components. Minimal rank is the smallest rank value that increasesthe residual Exp. Var. within 10% beyond the full rank model. C. Tradeoff between Gaussian rank andmode number. Residual Exp. Var. plotted as a function of mode number and rank, with the red dashed linedenoting models with same explained variance. Required covariance rank increases with lower noise level.We found that, in the high-noise regime,a low-rank Gaussian is sufficient to predict the score: for example, when = 10, the explained variance isidentical for Gaussian mixtures with full rank and rank 100 ( lower panel; note that the curve hasan elbow). As the noise scale decreases, the minimum required rank gradually increases: the elbow ofthe explained variance curve moves rightward ( B). This phenomenon can be understood throughthe Gaussian score equation (Eq. 12). In the diagonal matrix , entries for which k 2 are renderednegligible, ask k+2 0, i.e., those dimensions become effectively invisible at that noise scale. As the noisescale is reduced, more principal dimensions of the covariance matrix are unveiled and become visible to thescore function, so the covariance matrixs rank effectively increases. Trade-off between mode number and local rank.Interestingly, at small noise scales, one can trade offbetween the number of components and the rank of each component. For the CIFAR-10 dataset, at = 0.1,a GMM with 200 components with rank 384 is as effective at explaining the neural score as a GMM with10 components with rank 1024 (C, see also horizontal lines in ). This suggests an interestinggeometric picture of the score. Though globally the data distribution and its score appear to be high rank,locally the data distribution and the score are effectively low rank; this picture is reminiscent of a nonlinearmanifold comprised of many glued-together linear manifolds. Visual comparison of Gaussian mixture score versus neural score in low noise regime.Thedeviation between Gaussian mixture scores and neural scores in the low noise regime raises the questionabout the spatial nature of this difference. To explore this, we visualized various neural and idealized scorefields close to the image manifold. We selected three data samples to establish a 2D plane, and then evaluatedand projected each score vector field onto this plane. Various qualitative comparisons of this sort (involvingboth inter- and intra-category planes, and a mix of training and test examples) are illustrated in Figures 11,20, 21, and 22. At a moderate noise level (e.g., = 2.0), the vector field created by the delta point cloud or a large numberof Gaussian mixtures could roughly recapitulate the score field structure of the neural score ( upperpanel). Intriguingly, at lower noise levels ( 0.1), the neural score revealed a complex geometric structure,with the score vector field nearly vanishing inside the lines and triangles (simplexes) interpolating the data",
  "sigma=0.10": ": Visual comparison of the score vector field s(x, ) of pre-trained diffusion modelsand the Gaussian mixture model. All panels visualize score functions on the same 2D domain, namelythe plane spanned by three test set examples not seen during training. Red dots mark the 2D coordinatesof these three samples. The x-y axes correspond to an orthonormal coordinate system on the plane, withthe units denoting L2 distance on the plane. In this case, all three examples are of the MNIST digit 2.Arrows visualize the projection of score vectors onto this plane, and the heatmap visualizes the L2 norm ofthe projected vector on the plane. The first panel shows the neural score vector after training; the otherpanels show idealized score models of decreasing complexity (number of Gaussian modes). See , 21,and 22 for additional similar visualizations. points, as depicted in the lower panel of . This contrasts with the Gaussian mixture and delta mixturescores, which aligned with theoretical expectations of piece-wise linear vector fields demarcated by linear orquadratic hypersurfaces. This phenomenon is reminiscent of continuous attractors in the dynamical systems literature (Samsonovich &McNaughton, 1997; Khona & Fiete, 2022), and suggests that the probability flow ODE could converge alongthese simplices where the vector field vanishes, which may be one mechanism that supports generalization.",
  "Learning of far-field Gaussian score structure": "In the preceding sections (Sec. 4.2, 4.3, 4.4), our analyses focused on the structure of the score of traineddiffusion models, but did not touch on the question of how that structure emerges during training. Giventhat overparameterized function approximators like neural networks are thought to learn simpler structurefirst, it stands to reason that Gaussian/linear score structure may be learned relatively early. Is this true? To test this idea, we trained neural network score approximators on different datasets using the trainingprocedure described by Karras et al. (2022). Throughout training, we sampled query points xt from thenoised distribution, and compared the neural score to the three idealized score models from Sec. 2.2. As we have observed, at higher noise scales ( 10), all score approximators are similar to the neural scoreafter convergence. During training, the neural score function steadily converges to these approximators as well(, ). Intriguingly, at lower noise scales, the network displays non-monotonic learning dynamicsfor simpler scores. Specifically, the neural score initially aligns with simpler score models (the Gaussian modeland a GMM with few modes) before starting to deviate from them. This pattern was consistently observedacross various training sets (refer to for CIFAR-10, AFHQ, FFHQ). On the other hand, the neuralscore approached more complex score approximators (the delta mixture) monotonically. This effect suggestsan initial tendency to fit the score of simpler distributions. This tendency manifested slightly differently on the MNIST dataset (). Here, the score networkapproaches different GMM score approximators in order of increasing complexity. Namely, the deviationbetween the neural score and simpler Gaussian scores decreased and reached a floor, before moving on tomore complex models, such as 2-mode and 5-mode Gaussian mixtures. Although the non-monotonic effectwas less pronounced, it supports the idea that the network initially maximizes the explainable variance forsimpler distributions before progressing to more complex Gaussian mixtures. We visually demonstrate this phenomenon by plotting the evolution of the neural score field during trainingon the MNIST dataset (). At a lower noise scale ( = 1.0), the neural score field starts by resemblinga Gaussian-like (single basin) vector field, then splits into multiple basins, so that it resembles the score of amulti-modal distribution.",
  "sigma=1.00": ": Visual comparison of neural score vector field s(x, ) with idealized scores throughouttraining. Layout and score domain are the same as in . The upper row panels depict the neuralscore vector field at different training epochs; the lower row panels depict the scores of idealized models ofincreasing complexity. See , 28, 29 for similar plots on the plane spanned by training examples. Our finding is consistent with the general observation that the learning dynamics of overparameterizedneural networks exhibit a spectral bias (Bordelon et al., 2020; Canatar et al., 2021), and tend to capturelow-frequency aspects of input-output mappings before high-frequency ones (Rahaman et al., 2019; Xu et al.,2022). In this setting, the relevant mapping is the one defined by the score s(x, ) or denoiser D(x, ). Forlower noise scales , the Gaussian score is smoother and lower-frequency in x space than the score of GMMs,so it is perhaps unsurprising that Gaussian structure is learned preferentially by the network early in training.",
  "Application: Accelerating sampling via teleportation": "Above, we have shown that the Gaussian models exact solution provides a surprisingly good approximationto the early sampling trajectory. We can exploit this fact to accelerate diffusion by analytical teleportation( A). By this, we mean replacing a certain number of initial PF-ODE integration steps with a singleevaluation of the Gaussian analytical solution at some intermediate time t (or equivalently noise scale skip).In this way, one can nontrivially reduce the number of neural function evaluations (NFEs) required to generatea sample. In principle, this speedup can be combined with any deterministic or stochastic sampler. Experiment 1.First, we showcase its effectiveness using the optimized second-order Heun sampler fromKarras et al. (2022), which yields near state-of-the-art image quality and efficiency. See Alg. 1 and AppendixA.7, B.2 for method details and additional experiments with DDIM (Song et al., 2020a). We evaluated our proposed hybrid sampler on unconditional diffusion models trained on CIFAR-10, FFHQ-64,and AFHQv2-64. We sampled 50,000 images using both the Heun sampler and our hybrid sampler withvarious numbers of skipped steps (or equivalently, different times t), and evaluated the Frechet InceptionDistance (FID) in each case. We found that we can consistently save 15-30% of NFES at the cost of a lessthan 3% increase in the FID score ( B,C), even when competing with the optimized sampling methodfrom EDM (Karras et al., 2022). For CIFAR-10 and AFHQ, skipping steps can even reduce the FID score by 1%, resulting in a highlycompetitive FID score of 1.93 for CIFAR-10 unconditional generation. (For the full set of results, see Sec.B.8, , and Tab. 4-6.) Comparing to , we observe that the number of skippable stepsroughly corresponds to the noise scale at which the neural score deviates substantially from its Gaussianapproximation.",
  "init noise": ": Leveraging closed-form solution to accelerate sample generation. A. Schematic descriptionof our analytical teleportation method. B. Sampled image as a function of number of skipped steps; ourhybrid method combines Heuns method with a Gaussian model prediction. C. Image quality (FID score) ofthe hybrid method as a function of NFE and number of skipped steps (see Appendix A.8). : Teleportation improves sampling speed while maintaining FID scores across datasets.Noise in the table refers to the noise scale that analytical teleportation skips to, i.e., t = skip in Alg. 1. See and Tab. 4-6 for a more complete set of results.",
  "Teleportation6732.72.5615916.82.0262512.91.9345511.72.841518.02.359215.32.123": "Experiment 2.To further demonstrate the generality of our method, we evaluated our hybrid sampler withseveral popular deterministic samplers: dpm_solver++, dpm_solver_v3, heun, uni_pc_bh1, uni_pc_bh2 Luet al. (2022); Zheng et al. (2023); Zhao et al. (2024) on the same pre-trained EDM models. We systematicallyvaried the skip noise scale skip and the number of sampling steps nstep. We found results consistent with the first experiment across all samplers (): Gaussian teleportationcan improve sample generation time (i.e., reduce the number of required neural function evaluations) without",
  "reducing sample quality. Further, given a fixed budget of sampling steps, using Gaussian teleportation canimprove sample quality (i.e., reduce FID score)": "Our method compares favorably to the dpm_solver_v3 approach to speeding up sampling proposed by Zhenget al. (2023), which exploits model-specific statistics. They achieved FIDs of 12.21 (5 NFE) and 2.51 (10NFE) on unconditional CIFAR-10. Using our teleportation technique, we achieved FIDs of 9.63 (5 NFE,skip = 20.0), 2.45 (10 NFE, skip = 40.0), and 2.22 (12 NFE, skip = 20.0). Similar results hold for theAFHQv2 dataset (see full results in -32). Our intuition is that substituting the easy-to-approximate(i.e., mostly linear) part of the score field with its Gaussian approximation allows the sampler to spendmore steps on the harder-to-approximate, nonlinear score field close to the data manifold. If this intuitionis correct, it suggests that by spending the neural function evaluation budget more wisely, we can improvesample quality. For the FFHQ-64 dataset, we observed similar results to Experiment 1: analytical teleportation slightlyincreased FID given a fixed number of sampling steps (see full results in ). For example usingdpm_solver++, 40 NFE with no skipping yields an FID of 2.46, while skipping to noise scale skip = 20.0yields FIDs of 2.65 (40 NFE) and 2.77 (25 NFE). Though these changes in FID are tiny, they show that forthe face dataset, the neural score model may deviate from the Gaussian approximation in an interesting wayin the high noise regime.",
  "Related work": "Diffusion models and Gaussian mixtures.There has been increasing interest in characterizing diffusionmodels associated with tractable target distributions, and specifically in Gaussian and Gaussian mixturemodels.Shah et al. (2023) and Gatmiry et al. (2024) focused on learning to generate samples fromGaussian mixtures using diffusion models, and Shah et al. (2023) found a connection between gradient-basedscore matching and the expectation-maximization (EM) algorithm and derived convergence guarantees.Concurrently, Pierret & Galerne (2024) derived the exact solution to the reverse SDE and PF-ODE for aGaussian model, which allowed them to compare Wasserstein errors for any sampling scheme. But to ourknowledge, no existing work has provided the in-depth comparisons between idealized score models andneural scores that we present here. Consistency of noise-to-image mapping.Recently, many researchers have noticed that the mappingfrom initial noise to images is highly consistent across independently-trained diffusion models, and evenacross models trained on non-overlapping splits of the same dataset (Zhang et al., 2023; Kadkhodaie et al.,2023). This phenomenon can be partially explained by two of our findings: (1) the noise-to-image mappingis largely determined by the Gaussian structure of the data (Eq. 16), and (2) Gaussian structure seems tobe preferentially learned by neural networks (Sec. 5). If different splits of the dataset have almost identical",
  "Gaussian approximations, one expects different networks to learn similar linear score structure, and hencepossibly similar noise-to-image mappings": "Non-isotropic Gaussian initial states.Though most diffusion models sample initial states from anisotropic Gaussian, Lee et al. (2021) explored sampling from a non-isotropic Gaussian whose mean andcovariance depend on the training set. Although our work does not involve any modification to the scorematching objective, we also find that one can save computation by choosing a different initial state, i.e., thestate xt N(t, 2t + 2t I) predicted by the Gaussian model. In a sense, during the intial phase of samplegeneration, diffusion models convert their isotropic/white initial states to preconditioned non-istropic states. Score field smoothness and generalization.Several recent papers have also observed that traineddiffusion models learn score fields that are smoother than the scores of their training distributions, which inprinciple is helpful for generalization (Kadkhodaie et al., 2023; Scarvelis et al., 2023). These findings areconsistent with our observation that diffusion models learn smooth score functions that more closely resemblethe scores of Gaussian mixture models with a moderate number of modes.",
  "Discussion": "In summary, even for real diffusion models trained on natural images, in the high-noise regime the neural scoreis well-approximated by a Gaussian model; in the low-noise regime, Gaussian mixture models approximateneural scores better than the score of the training distribution. We mathematically characterized the samplingtrajectories of the Gaussian model, and found that it recapitulates various aspects of real sampling trajectories.Finally, we leveraged these insights to accelerate the initial phase of sampling from diffusion models. Below,we mention additional implications of our results for the training and design of diffusion models: Noise schedule.As the early time evolution of the sample distribution is well-predicted by the Gaussianmodel, we in principle do not need to sample high noise levels to train a denoiser D(x, ). Instead, samplingcan directly begin (or warm start) from the non-isotropic Gaussian distribution N(t, 2t + 2t I). Model design.Since it is empirically true that neural scores are dominated by Gaussian/linear structure athigh noise levels, we can directly build this structure into the neural network to assist learning. For example,we can add a linear by-pass pathway based on the covariance of training distribution, and let the neuralnetwork only learn the nonlinear residual not accounted for by the Gaussian model term. Training distribution.As the neural networks first need to learn the data covariance structure by scorematching, we may be able to assist learning by reshaping the training distribution. One hypothesis is that ifwe pre-condition the target distribution by whitening its spectrum, then the neural score may converge faster.If true, this may explain the higher efficiency of latent diffusion models (Rombach et al., 2022): with KLregularization, the autoencoder not only compresses the state space, but also whitens the image distributionby morphing it to be closer to a Gaussian distribution.",
  "Limitations and future work": "Higher-resolution and conditional diffusion models.Our experiments focused on lower-resolutionimage generative models. Generalizing our results to higher-resolution models, or popular text-to-imageconditional diffusion models (Rombach et al., 2022), involves overcoming difficulties related to covarianceestimation. For high-dimensional models, estimating covariances is substantially harder given a limitednumber of training examples; for conditional models, especially for text-to-image models, there may not evenexist training data corresponding to a specific prompt, making direct covariance estimation impossible. Structure of neural scores close to image manifold.We focused on characterizing the linear structurethat dominates neural scores in the high noise regime, but did not claim to precisely understand neuralscores at smaller noise scales, i.e., when the sample is closer to the data manifold. Even though the scores",
  "Francesco Camastra and Antonino Staiano. Intrinsic dimension estimation: Advances and open problems.Information Sciences, 328:2641, 2016. ISSN 0020-0255. doi:": "Abdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan. Spectral bias and task-model alignment explaingeneralization in kernel regression and infinitely wide neural networks. Nature Communications, 12(1):2914, May 2021. ISSN 2041-1723. doi: 10.1038/s41467-021-23103-1. URL Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan. Wavegrad:Estimating gradients for waveform generation. In International Conference on Learning Representations,2021. URL",
  "Emile Pierret and Bruno Galerne. Diffusion models for gaussian distributions: Exact solutions and wassersteinerrors. arXiv preprint arXiv:2405.14250, 2024": "Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail Kudinov. Grad-tts: A diffusionprobabilistic model for text-to-speech. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38thInternational Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research,pp. 85998608. PMLR, 1824 Jul 2021. URL Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Hamprecht, Yoshua Bengio,and Aaron Courville. On the spectral bias of neural networks. In International Conference on MachineLearning, pp. 53015310. PMLR, 2019.",
  "Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprintarXiv:2010.02502, 2020a": "Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. InH. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch-Buc, E. Fox, and R. Garnett (eds.), Advancesin Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.URL Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score matching: A scalable approach to densityand score estimation. In Ryan P. Adams and Vibhav Gogate (eds.), Proceedings of The 35th Uncertaintyin Artificial Intelligence Conference, volume 115 of Proceedings of Machine Learning Research, pp. 574584.PMLR, 2225 Jul 2020b. URL Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.Score-based generative modeling through stochastic differential equations. In International Conference onLearning Representations, 2021. URL",
  "A.3Measuring Image Similarity": "LPIPS is am image distance metric trained to mimic human perceptual judgment of image similarity. Weused LPIPS models with all three pretrained backbones: AlexNet, VGG, SqueezeNet. Note that, the imageswe were measuring has different resolutions, 32 pixels for CIFAR10 and 64 pixels for FFHQ and AFHQ.Consistent with the FID measurement, for all datasets, we resized the image to 224-pixel resolution beforesending them to the LPIPS model. We found that, without resizing, the mismatch of image size (e.g. 32pixel for CIFAR) to the convnet can bias the image distance by the boundary artifact, and obscure the effect.",
  "A.4Sampling Diffusion trajectory with Gaussian Mixture scores": "For the Gaussian score, we are able to compute the whole sampling trajectory analytically. But for a Gaussianmixture with more than one mode, we need to use numerical integration. We evaluated the numerical scorewith the Gaussian mixture model defined as above and integrated Eq.2 with off-the-shelf Runge-Kutta 4integrator (solve_ivp from scipy) from sigma 80.0 to 0.0. We also chose (t) = t. To compare the trajectorywith the one sampled with the Heun method, we evaluated the trajectory at the same discrete time steps asthe Karras et al. (2022) paper. The ith noise level is the following,",
  "Given large and high-dimensional image datasets, we used a fast and heuristic method for fitting the GaussianMixture model": "We performed mini-batch k-means clustering on the training data (sklearn.cluster.MiniBatchKMeans)with batch size 2048 and fixed random seed 0 to cluster the dataset to K cluster. For each cluster i, thenumber of samples belonging to this class is Ni. We compute the empirical mean i and covariance matrixi of all samples belonging to this cluster. Then we define the Gaussian mixture model as",
  "For mini-EDM training runs, hyperparameters are": "MNIST, used batch size 128, Adam optimizer with a learning rate of 2E-4, we densely sampledcheckpoints for the first 25000 steps. The channel multiplier is set to 1 2 3 4, with the base channelcount 16, no attention, and there is 1 layer per block. CIFAR10, used batch size 128, Adam optimizer with a learning rate of 2E-4, checkpoints for thefirst 50000 steps. The channel multiplier is set to 1 2 2, with the base channel count 96, attentionresolution is 16, and there are 2 layers per block.",
  "For EDM training runs, hyperparameters are": "CIFAR10, used batch size 256, Adam optimizer with a learning rate of 0.001, checkpoints for the first5000K images (around 20000 steps). EDMPrecond class network was used, the channel multiplierwas set to 2 2 2, with the base channel count 128. The augmentation probability was 0.12. AFHQ and FFHQ, used batch size 256, Adam optimizer with a learning rate of 2E-4, checkpointsfor the first 5000K images (around 20000 steps). EDMPrecond class network was used, the channelmultiplier was set to 1 2 2 2, with the base channel count 128. augmentation probability was 0.12.",
  "DatasetMNISTCIFARCIFARAFHQFFHQ": "Batch128128256256256Optim.AdamAdamAdamAdamAdamLR2E-42E-40.0012E-42E-4Steps2500050000200002000020000Chan Mult.1 2 3 41 2 22 2 21 2 2 21 2 2 2Base Chan.1696128128128Attn Res.None16161616Lyr per B12444Aug.ProbNoneNone0.120.120.12Net. ClassSongUNetSongUNetSongUNetSongUNetSongUNetCode Basemini-EDMmini-EDMEDMEDMEDM : Hyperparameters for diffusion model (mini-EDM and EDM) training runs Abbreviations:Batch = Batch Size, Optim. = Optimizer, LR = Learning Rate, Steps = Steps/Images, Chan Mult. =Channel Multiplier, Base Chan = Base Channel Count, Attn Res = Attention Resolution, Lyr per B =Layers per Block, Augm Prob = Augmentation Probability, Net. Class = Network Class",
  "A.7Hybrid sampling method": "As we stated in the paper, the Hybrid sampling scheme can be combined with any deterministic sampler (e.g.DDIM Song et al. (2020a), PNDM Liu et al. (2022)). In our two main benchmark experiments, we combinedit with the Heun sampler, and a few recent samplers in DPMSolver-v3 benchmark. Experiment 1In the first experiment, we used the following strategy for choosing skip, inspired by thebaseline Heun method. The Heun method samples a sequence of nstep noise levels [0, 1, 2, ...nstep], where0 = max and nstep = min. For each initial condition xT , we use the analytical solution to evaluate xt orintegrate the probability flow ODE to time t, where we chose t as the i-th noise level i (Eq. 40). Then we",
  "will skip the first i step in the Heun method and start at initial state xt. In this manner, when we skip tothe ith noise level, we will save 2i neural function evaluations": "Experiment 2In the second experiment, we systematically varied both the skipping noise level skip andthe number of sampling steps nstep for a bunch of recent deterministic diffusion samplers: dpm_solver++,dpm_solver_v3, heun, uni_pc_bh1, uni_pc_bh2. We used the implementation of these samplers in the codebase of DPMSolve-v3, used in their benchmark experiments used the same models as in Experiment 1, namely the EDM modelspre-trained on CIFAR10, AFHQ, FFHQ datasets. For each initial condition xT , we use the analytical solution to evaluate xt at t = skip, and then run thesediffusion samplers with setting max = skip and nstep steps. We systematically vary nstep and skip on agrid.",
  "A.8FID score computation and baseline": "We used the same code for FID score computation as in Karras et al. (2022). For each sampler, we sampledthe same initial noise state xT with random seeds 0-49999. We computed the FID score based on 50,000samples. For our baseline, we picked the same model configurations as reported in Tab. 2, specifically, Variancepreserving (VP) config F, the unconditional model for CIFAR10, FFHQ 64 and AFHQv2 64. The defaultsampling steps are 18 steps (NFE = 35) for CIFAR10, 79 steps (NFE = 79) for FFHQ and AFHQ.",
  "B.1Detailed validation results": "Here we presented the full results for all datasets: approximating neural scores with analytical scores (),deviations between sampling trajectory () and denoiser trajectory () guided by neural score andanalytical score. : Deviation between the denoiser D(xt, t) of EDM neural network and the analyticalscore. The thick line denotes the mean over initial conditions; the shaded area denotes 25%, 75% quantilerange over the initial conditions.",
  "B.2Additional validation experiments with DDIM": "Teleportation resultsFor the MNIST and CIFAR-10 models, we can easily skip 40% of the initial stepswith the Gaussian solution without much of a perceptible change in the final sample (E). Quantitatively,for CIFAR10 model, we found skipping up to 40% of the initial steps can even slightly decrease the FrechetInception Distance score (FID), and hence improve the quality of generated samples (F). For models ofhigher resolution datasets like CelebA-HQ, we need to be more careful; skipping more than 20% of the initialsteps will induce some perceptible distortions in the generated images (E bottom), which suggests thatthe Gaussian approximation is less effective for larger images. The reason may have to do with a low-qualitycovariance matrix estimate, which could arise from the number of training images being small compared tothe effective dimensionality of the image manifold.",
  "B.3Extended results for Generated Image Similarity Comparison": ": Comparing image samples generated from pre-trained EDM model versus an-alytical score models.Distance between the samples from the EDM and analytical score modeld(x0,EDM(xT ), x0,analytical(xT )) from the same initial noises xT were plotted. Each panel showed onetype of image metric d, MSE or LPIPS with different backbones AlexNet, VGG, SqueezeNet. Gaussian modelconsistently predicts EDM samples better than the exact delta score model, except for AlexNet based LPIPSin AFHQv2.",
  "Score Residual of EDM vs GMM score | MNIST mini EDM": ": Deviation of the Learned Neural Score from the Score of a Gaussian Mixture Modelwith Varying Numbers of Components and Ranks, Fitted on Training Data. Same plotting schemeas but for MNIST.Upper: Residual explained variance plotted as a function of the number of Gaussian modes on the x-axis,with each colored line representing a different rank. Each panel compares the score at a certain noise scale .Lower: The same data plotted alternatively, with the number of ranks on the x-axis.",
  "B.8Extended results of FID scores for Gaussian analytical teleportation": ": Image quality as a function of skipping steps for hybrid sampling approach. Note themain x-axes are the number of Neural Function Evaluation (NFE); the secondary x-axes are the number ofskipping steps from the Heun sampler; the tertiary-axes are the time or noise level skip at which we evaluatethe Gaussian solution. See Tab.6,4,5 for numbers.",
  "The effects of applying the Gaussian analytical teleportation on the FID scores are presented below (,Tab.5,4,6)": "For CIFAR-10 model, we found teleportation not only reduces the number of required neural function calls,but also lowers the FID score. Admittedly, the FID score effect is quite small on the CIFAR-10 model wetried: it goes from 1.958 (no teleportation) to 1.933 (5 steps replaced by teleportation), which is around a 1%decrease. Visually, the difference in sample images is mostly negligible; there are some changes, but they areplausible changes in image details. Hence, teleportation saves 29% of NFE and improves the sample quality,without changing the model or the sampler. Further, skipping 6 steps will save 34% NFE but increase theFID score by less than .2%. For AFHQv2-64, the teleportation can save 25-30% of NFE (10-12 steps skipped) or reduce the FID by 2%(6 steps skipped). For FFHQ64, the teleportation saves 10-15% of NFE (4-6 steps skipped) with around 3%increase in FID.",
  "The full table of FID scores as a function of the number of skipped steps is shown below": "The fact that replacing NN evaluations with the analytical solution can even improve the already low FIDscore (albeit very slightly in the tests we just ran) is intriguing, and it is not immediately obvious why this istrue. One possibility is that, early in reverse diffusion, the neural network might be a worse or noisier versionof the Gaussian score.",
  "FID Heatmap by Steps and Skip Value Dataset: cifar10 Solver: uni_pc_bh2": ": Image quality as a function of skipping noise scale skip and sampling step numberfor hybrid sampling approach (CIFAR10). y-axis of heatmap (Skip Value) denotes the skip (edmconvention, min = 0.002, max = 80); x-axis denotes sampling steps. Each panel features hybrid samplerwith one deterministic sampler: Heun, DPM_solver++, DPM_solver_v3, UniPC-bh1, UniPC-bh2.",
  "D.1Detailed derivation of the solution of the Gaussian Diffusion": "In this section, we derive the analytic solution xt for the probability flow ODE in EDM formulation (Eq.2)with Gaussian score. We will assume that the score function corresponds to a Gaussian distribution whosemean is and covariance matrix is . We will also assume, given that images data are usually thought ofas residing on a low-dimensional manifold within pixel space, that the rank r of the covariance matrix maybe less than the dimensionality D of state space. Let = UUT be the eigendecomposition or compactSVD of the covariance matrix, where U is a D r semi-orthogonal matrix whose columns are normalized (i.e.UT U = Ir), and is the r r diagonal eigenvalue matrix. Denote the kth column of U by uk and the kthdiagonal element of by k.",
  "k=1ck(t)uk .(99)": "We can see that there are three terms: 1) t, an increasing term that scales up to the mean of thedistribution; 2) d(t)y(T), a decaying term downscaling the residual part of the initial noise vector, which isorthogonal to the data manifold; and 3) the ck(t)uk sum, each term of which has independent dynamics.",
  "D.4Derivation of rotational dynamics": "In this section, we derive various results quantifying how reverse diffusion trajectories are rotation-like. Inparticular, under certain assumptions, we will show that the dynamics of the state xt looks like a rotationwithin a 2D plane spanned by x0 (the reverse diffusion endpoint) and xT (the initial noise). We will derivethe formula by assuming that the training set consists of a single Gaussian mode, but will explain why thisassumption may not be strictly necessary.",
  "Notice that |J| < 1, regardless of": "Although we derived this formula by assuming a single Gaussian mode, its form does not actually dependon any properties of the mode. This suggests that, as long as the score function landscape looks locallyGaussian, the formula may still be applicable. For example, suppose the learned image distribution is aGaussian mixture. Even though the mean and the covariance of the nearest modethe one which we expectto dominate the score functionmay regularly change throughout reverse diffusion, even in a discontinuousway, the rotation equation should stay the same.",
  "k=1J(t; k)2ck(T)2 .(108)": "Recall that ck(T) is the coefficient of the original noise seed xT N(0, I) along the direction uk. AssumingD is large, the norm of the noise seed is approximately 1. Since there is a priori no relationship between xTand uk, we expect that xT uk 1/ D. (Suppose we express xT in terms of a set of D orthonormal basisvectors. Given that its norm is 1, and that it has no special relationship with any basis vector, the overlapbetween xT and each vector must be about 1/",
  "This function has saturating behavior, and remains high (in the sense of being k) until around the timewhen tk": "1 2t. But consistent with our other results (see and the associated formulas),this is roughly around the time of feature commitment, or more specifically when the sigmoid-shapedcoefficients of x0 begin to transition to their final value. So the rotation formula only becomes useful fordetermining the endpoint after enough time has passed that one is sufficiently close to the endpoint. Using multiple xt does not help reduce the error in applying the rotation formula, since the error equationabove is monotonic in time. In other words, averaging over multiple recent xt is strictly worse than just usingthe rotation formula and the most recent (i.e. greatest number of reverse diffusion time steps) xt. As a final note: since we have the form of the correction terms, why not use that as additional information?We could do this, but this only works for the Gaussian case, where we already have access to the full solution!And knowing these terms at all times is also roughly equivalent to knowing the full trajectory. So in summary,viewing reverse diffusion trajectories as 2D rotations is a useful geometric picture, but it is less quantitativelyuseful than the full analytical solution to the Gaussian model, e.g. for accelerating sampling.",
  "2tConst(122)": "This means the solution of xt can be interpreted as a spherical interpolation between the hypothetical initialstate Const and denoiser state D. We used the word rotation in a very loose sense: this trajectory will be anarc on spherical surface when D and const have the same norms and orthogonal to each other. These criteriaare not always true in actual diffusion trajectories.",
  "2t xT .(132)": "In words: through a rotation, xt interpolates between x0 and const. This solution paints the picture that thestate is constantly rotating towards the estimated outcome x0, with Eq. 132 describing that hypotheticaltrajectorys shape. But as the target x0 is moving, the actual trajectory will be similar to Eq. 132 only onshort time scales, and not on longer time scales. This idea is visualized by the circular dashed curves in .",
  "D.8Arguments of the approximation of score field of Gaussian and point cloud": "In this section, we will argue from a theoretical perspective, the score field of a bounded point cloud isequivalent to a Gaussian with matching mean and covariance, when the noise level is high and when the querypoint is far from a bounded point cloud. We are going to use a technique inspired by multi-pole expansion inelectrodynamics Griffiths (2005).",
  "iyiyTi T(148)": "these are the first moment and the second central moment of the distribution. Let us assume this point cloudis bounded by a sphere of radius r around , i.e. yi r. Since Nj yj = Ntr, this bounds thecovariance spectrum tr r2. The Gaussian distribution with matching first two moments is N(, ). Weare going to show that when the noise level and x are both much larger than the standard deviationof the distribution, the score at noise level of the original dataset (point cloud) is equivalent to the score ofthis Gaussian distribution."
}