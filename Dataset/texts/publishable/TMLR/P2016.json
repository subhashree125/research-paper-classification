{
  "Abstract": "Representation learning, and interpreting learned representations, are key areas of focus inmachine learning and neuroscience. Both fields generally use representations as a means tounderstand or improve a systems computations. In this work, however, we explore surprisingdissociations between representation and computation that may pose challenges for suchefforts.We create datasets in which we attempt to match the computational role thatdifferent features play, while manipulating other properties of the features or the data. Wetrain various deep learning architectures to compute these multiple abstract features abouttheir inputs. We find that their learned feature representations are systematically biasedtowards representing some features more strongly than others, depending upon extraneousproperties such as feature complexity, the order in which features are learned, and thedistribution of features over the inputs. For example, features that are simpler to computeor learned first tend to be represented more strongly and densely than features that aremore complex or learned later, even if all features are learned equally well. We also explorehow these biases are affected by architectures, optimizers, and training regimes (e.g., intransformers, features decoded earlier in the output sequence also tend to be representedmore strongly).Our results help to characterize the inductive biases of gradient-basedrepresentation learning. We then illustrate the downstream effects of these biases on variouscommonly-used methods for analyzing or intervening on representations.These resultshighlight a key challenge for interpretabilityor for comparing the representations of modelsand brainsdisentangling extraneous biases from the computationally important aspects ofa systems internal representations.",
  "Introduction": "A key goal of machine learning research is learning representations that allow effectively performing down-stream tasks (Bengio et al., 2013). A growing subfield aims to interpret the mechanistic role that theserepresentations play in the models behaviors (Geiger et al., 2021; Olsson et al., 2022; Geiger et al., 2024;Nanda et al., 2023; Conmy et al., 2023; Merullo et al., 2023; Wu et al., 2024), or to alter those representa-tions to improve model alignment, interpretability, or generalization (Sucholutsky et al., 2023; Bricken et al.,2023a; Zou et al., 2023). Likewise, neuroscience studies the neural representations that a system developsand how they relate to its behavior (Churchland & Sejnowski, 1990; Baker et al., 2022). Each field focuseson representations as a means to understand or improve a systems computationsthat is, its more abstractpatterns of behavior on a task, and how those behaviors are implemented.",
  "Shapematch": ": An overview of our approach.(top left) We train networks to compute multiple features ofan input, on controlled datasets where we systematically manipulate various properties of the input andtarget output distribution; for example, to compute easier- and harder-to-compute features of the input.(top center) We study how these properties bias the feature representations the model learnsfor example,towards representing easier-to-compute features more strongly. (top right) We explore the impacts of thosebiases on downstream areas like interpretability and cognitive neuroscience. (bottom) We consider thesephenomena across a range of experiment domains, including boolean functions and various language andvision tasks, using standard architectures for each domain. However, the relationship between representation and computation is nontrivial. Thus, using representationsto understand or improve computation depends fundamentally on the linking assumptions we make abouttheir relationship. Here, we attempt to shed some empirical light on this linking, by characterizing howcomputational role, and various aspects of the features and extraneous details of the learning process, affectthe representations that deep neural networks learn. To do so, we train various modelsMLPs, ResNets and Transformersto compute multiple independentabstract features about an input, through separate outputs (). This approach provides a controlledsetting in which each abstract input feature plays an equivalent computational role: contributing to predictingexactly one output value. In this controlled setting, we manipulate various properties like feature complexity,the order in which features are trained, and their prevalence and distribution within the inputs. We show that,although these properties do not substantially affect the computational role that each feature plays in themodels input-output behavior, they substantially bias the structure of the models learned representations.",
  "Published in Transactions on Machine Learning Research (9/2024)": "In we show visualizations of the first six principal components of the models. The easy featuresalways occupy at least the first two principal components. In the case that there is only a single feature,the hard feature is separated along the third principal component. As more easy features are added, theyoccupy successively more principal components, and the harder feature is correspondingly shifted one PChigher for each easy feature thats added. Given this, it is somewhat surprising that two PCs are devoted tothe single easy feature in the one easy-feature case, when just one would suffice. In we show t-SNE visualizations of the same networks.It is noticeably easier to see the hardfeature separating the clusters in these plots than in the top few principal components, which demonstratesthe benefits of a nonlinearity and locality bias of t-SNE. Nevertheless, as more easy features are added, itbecomes hard to tell that there is another feature separating the clusters, particularly in the case of 4 easyfeatures. Taking a step back, how would the results of this section extrapolate to more realistic settings? If we imaginea more realistic problem regime in which tens, hundreds, or perhaps even thousands of features contribute toa models predictions, it seems quite likely that some of the computationally-important-but-complex featureswould be entirely swamped by the noise of the easier features in any typical feature visualization.",
  "Other properties like the prevalence of a feature in the training dataset, or the output position inwhich the model is trained to report it, can likewise bias feature representations": "These broad patterns of biases are common across a range of models and optimizers; they seem tobe rather general patterns of gradient-based deep representation learning rather than artifacts ofa particular architecture. However, we note there are complex interactions between architecture,optimizer, and task, which can shift or even reverse the biases in some cases. These feature representation biases have downstream implications for interpreting learned represen-tations, comparing them between different systems, or trying to alter them. Conclusions or resultsmay be biased by relatively extraneous details of the learning process. In Sections 7 & 8 we set these results in the broader context of related work, such as prior work on thecomputational- and feature-level inductive biases of deep learning, and challenges of interpretability. Ourresults may have implications for both artificial intelligence and cognitive (neuro)science.",
  "Input stimulus: a raw input I provided to a model, for example a tokenized string, a vector of booleanvalues, or an image": "Feature: an abstract, task-relevant property of an input; e.g., whether to be or not to be appears withina sentence, or whether XOR of the first and third boolean value is true. Thus, a feature can bedefined as the result of applying some function to the input: feature value = f(I). Generally, thefeatures we discuss are those a model is explicitly trained to output. Computation: the behavior (i.e. input-output mapping) of a system, described at the abstract level ofthe task-relevant features. For example, if we train a vision model to report the shape and color ofan object in an image, and it does so even on a held-out test set of images, we say that the modelis computing the features shape and color. That is, the task of the model would be to output thevector [fshape(I), fcolor(I)] where the feature functions identify the respective values. Computational role of a feature: the downstream contribution of a feature to the desired output be-havior, once that features value has been computed. For example, suppose the network is trainedto label sentences with whether they contain the word cake, and whether they are grammatical,through separate outputs ([fcake(I), fgrammatical(I)]). We say these features play an equivalent com-putational role (direct output) once they are computed, even though computing the value of onefeature might be much easier than the other.",
  "Representation: a distributed activity pattern across neural units within (part of) a model. We will denotethe representation the model produces for an input I as (I)": "Feature Representation: the portion of a representation (e.g. a vector subspace) that can be predictedfrom knowing the value of a particular feature for the current input. (Note that we do not herestipulate that this subspace must be causally implicated in the models behavior, unlike, e.g., Cao,2022. We make this choice for practical reasons, to avoid verifying causality throughout training.However, we do verify the causal role of the representations in some cases in 6.1.)",
  "Methods": "In we show an overview of our approach. We primarily train networks to output classifications ofmultiple features, through separate output units (e.g. from an MLP), or as a sequence (from a Transformer).We construct the datasets such that the features are statistically independent from one another. In thissetting, we know the correct computation that the model should learn. We train models on large enoughdatasets, and for long enough, that they achieve high accuracy (>95%, and often 100%) on the held outtest set for all features (except where explicitly noted in the text). Thus, we can ensure that the modelsare computing each feature in a way that broadly generalizes in accordance with our understanding of thatfeature. We investigate whether particular properties of the features change the way that those features are repre-sented. These properties included feature complexity, feature prevalence in the training dataset, or featureposition in the output sequence. We therefore create families of training datasets which systematically ma-nipulate these properties. We also create held-out validation and test datasets corresponding to each trainingdataset, for analysis and to ensure that the model generalizes as expected. The train/validation/test datasetsare sampled IID. To analyze the results, we extract internal representations from the trained model for each stimulus inthe validation or test set, and then inspect those representations are driven by each feature. Our mainanalyses focus on the variance explained for each feature. To compute this quantity, we fit linear regressionswhose inputs are the value of a feature, and whose outputs are the activations of each unit in the modelsrepresentations. We measure the variance explained by a feature as the R2 resulting from the correspondingregression, when the regression is fit using the validation set and tested on the test set. More formally, if (Ii) Rn denotes the models learned representation vector for an input Ii,1 and f(Ii)is the corresponding feature value, then via linear regression we find the optimal linear predictor W f of themodels representations on the validation set V from that feature:",
  "|| (Ii) ||2": "where the total variance EIiT || (Ii) ||2 is simply the variance of the representation units across all inputsin the dataset.2 When comparing representation variance explained over the course of training, we normalizeeach timepoint relative to the total variance at the end of training (i.e., in the equation above, we replacethe total variance term || (Ii) ||2 in the denominator with the final value ||final (Ii) ||2 at all timepoints),to plot all values on a consistent scale. 1With the mean representation across the dataset subtracted out, for notational convenience when writing the variance.2Note that when we compare multiple features, this measure could in principle overcount variance by attributing the samevariance in the representations to both features. However, because we have constructed the datasets such that the features arestatistically independent, we do not have this issue in practice.",
  "Value": "feature easyhard : Test accuracy (left) and representation variance (right) over learning in an MLP trained to outputeasy and hard features. The easy feature (reading out a single input) is learned quite rapidly, but the hardfeature (sum of 4 inputs mod 2) is learned more slowly; nevertheless, by the end of training the model achievesperfect accuracy for both features on a held-out test set. Despite this, the easy feature occupies substantiallymore of the penultimate layer representation variance than the hard feature. It is particularly interestingto note that long after the easy feature has been mastered, the representation variance attributable to itcontinues to grow, especially when the harder feature performance is most rapidly improving. (Representationvariance is plotted normalized by the total representation variance at the end of training, to show the gradualincrease over time. See Appendix 20 for an unnormalized version. Lines are averages across 10 seeds, intervalsare 95%-CIs.)",
  "MLPs on binary features": "We begin with some simple experiments on MLPs trained to compute binary features from vectors of 32binary inputs. This initial setting is motivated by the original results of Hermann & Lampinen (2020) onRSA, and provides a simple domain where the complexity of relationships between input features and theiroutputs is clearly defined.",
  "An initial look at complexity bias": "We first train a 4-layer MLP to simultaneously output the values of two features embedded amongst othernoisy inputs: (1) an easy, linear feature (reading out the value of an input unit) (2) a harder feature (the summod 2 of a non-overlapping set of 4 inputs).3 That is, suppose the input vector I is composed of k booleaninputs I = [A, W, X, Y, Z, N1, ...Nk5] {0, 1}k, where the first 5 will be relevant to one of the features andthe remaining N1...Nk5 are purely noise that the model must learn to ignore. Then the features are definedrespectively as:",
  "We study the representations of these two features at the penultimate layer of the model, immediately beforethe output logits": "In we show the basic pattern of results, which illustrates many of the phenomena that we study below.In the left panel, we plot model accuracy on a held-out test set over training for each feature (see AppendixB.1 for the corresponding loss curves). The easy linear feature is learned quite rapidly, almost instantly on 3The nonlinear feature is harder by a variety of overlapping measures, including the minimum depth and width ReLUnetwork that can compute the feature, the number of updates required to learn it, the dataset size required to learn this featurein a way that generalizes, etc.",
  "Varying features and accounting for many ways they can be represented": "If learning order alone does not explain the dominance of the easy feature, what aspects of feature complexitymight? To explore this, we trained models on a larger set of hard features, ranging in difficulty from simpleAND of groups of inputs to more complex ones. In analyzing these models we considered another propertyof nonlinear features: there are more reasonable ways to compute them. For example, XOR(X,Y) can bedecomposed as AND(OR(X,Y), NOT AND(X,Y)) or as OR(AND(X, NOT Y), AND(NOT X, Y)), whichyield different patterns of representations (cf. Fig. A.14 in Hermann & Lampinen, 2020). By contrast, allsimple ways of encoding the linear feature are linearly equivalent. Could it be that the model is stronglyrepresenting components of the hard feature, even if it is not representing the hard feature itself? To test this possibility, we measured the variance explained by all patterns of inputs relevant to the hardfeature. That is, we enumerated all the possible hard-feature-relevant input patterns and then created aone-hot embedding for each of them. We then regressed from this set of labels onto the representations.The advantage to this analysis is that any function of the hard inputs alone is linear in this input-patternspaceit is essentially a tabular function representation where there is a separate entry for each possibleinput patternand thus it can account for any linear and non-linear patterns of encoding the hard input,as long as they do not mix in other irrelevant inputs. (Note that the easy feature inherently captures allrelevant input patterns, so there is no need to perform an equivalent analysis for the easy feature.) The results of this analysis are shown in . In sum, accounting for the variance of all input patternsdoes not by itself explain the gap between easy and hard features; if the features are trained simultaneouslythe gap remains quite large. However, if the hard feature is pretrained, and the analyses are performedwith respect to all hard input patterns, the gap vanishes, and there is even a slight bias towards the hardfeature in some cases. Thus, the easy-hard gap seems to be explained by a combination of hard featuresbeing learned later, and the fact that there are more ways to represent harder features.",
  "Feature prevalence also biases representations": "In the above experiments, all features were balanced to appear equally often in the dataset. However, distri-butions of properties in naturalistic data tend to be skewed (e.g. Piantadosi, 2014), and these distributionalproperties can shape the solutions that models learn (e.g. Chan et al., 2022). Thus, we explored whetherfeature prevalence affects the representational biases we observed above. To do so, we constructed datasets containing two linear features (easy), and two sum-mod-2 features (hard),where the first feature of each type was present (i.e. had a label of 1) in 50% of the data (common), but",
  "Feature": "Representation variance explained (%) : Representation variance explained without controlling for length at the final encoder layer (left),and pre-logits decoder layer (right) for transformers trained to identify semantic and structural features ofsentences from Dyck-(20,10). Results are broadly qualitatively similar to the length-controlled results in , but the correlations with length change the numerical values. (Bars are averages across 32 seeds, intervalsare 95%-CIs.) 0.0 0.2 0.4 Test accuracy (%)",
  "The effects of architecture and hyperparametersgenerally weak but complex interactions": "How general are our findings? In Appx. B.7 we explore the effects of MLP architecture and other hyper-parameters. We summarize the results here. First, deeper MLPs tend to show larger representational gapsbetween easy and hard features; model width has little effect. Despite the fact that nonlinearities stronglyinfluence the structure of learned representations (Saxe et al., 2019; Alleman et al., 2024), our general patternof findings is robust to choice of nonlinearity (tanh or rectifier). Optimizers and dropout: There is, however, a surprising interaction between dropout and choice ofoptimizer at convergence. Without dropout, all optimizers show similar effects, with large gaps betweeneasy and hard features. However, we show in Appx. B.7.1 that dropout yields strikingly different effectswith different optimizers at convergence; while SGD and AdaGrad show consistent gaps between easy andhard features, other common optimizers like Adam show qualitatively different effects with dropout, and infact show a slight hard feature bias at moderate dropout rates. However, this is only true at convergenceearlier in training, even when the hard features have been learned and generalize quite well, the easy featuresremain dominant.Furthermore, we also show that transformers trained on more complex datasets (seebelow) show more consistent feature representation biases across dropout levels.",
  "Transformers on sequence tasks": "The majority of models employed in modern deep learning are not MLPs. In recent years, Transformer(Vaswani et al., 2017) models have come to dominate, in language and beyond. We therefore assessed whethertransformers show similar effects to those we observed in simpler architectures, and whether the additionalproperties of language-like datafor example, positions and structure of tokens within a sequencelikewisebias feature representations. To do so, we instantiated several increasingly language-like training regimes. The simplest are character-level sequence-to-sequence language datasets that bear some similarity to those used above, but with a fewadditional properties. These datasets involve mapping a sequence of letters to a sequence of classificationsof non-overlapping subsequences of the original sequence. In particular, we consider three types of features.The simplest is exact-match vs. non-match to a particular letter sequence (analogous to the linear featureabove), for example 1 if the sequence is A,B,C and 0 otherwise. We also tested two increasingly nonlinearfeatures: matching all-but-one of the letters in the sequence (XOR-like), and the sum of matches modulo 2(analogous to the most complex feature above). In addition, we evaluate the effect of the number of letters inthe target sequence, and the position within the output sequence. Specifically, we created datasets containing8 features (exact match of 1-4 token sequences, all-but-one for 2-4 token subsequences, and sum-mod-2 for 4token subsequences), along with distractor characters in between. We sampled the feature order randomlyfor each dataset. We trained encoder-decoder transformers on 128 of these datasets and evaluated the representation varianceexplained in the last representation of the encoders (after concatenating all token representations), and thevariance explained immediately before the logit layer of the decoder when outputting that feature. That is,unlike in the MLP experiments above, in the decoder the representations of different features are no longer",
  "being analyzed simultaneously on the same representations, instead they are analyzed at different positionsin the output sequence": "In we show the results. There are several interesting phenomena to note. First, none of the featuresexplains very much of the overall variance in the penultimate encoder representations. (Note that the varianceexplained can be negative due to failure to generalize, because we fit the regressions using a validation set andcompute variance explained on the test set.) Nevertheless, there are clear biases based on both complexityand the number of tokens involved in computing the feature; generally features involving more tokens arerepresented more strongly, while more complex features are represented less strongly. In the penultimate decoder layer, we observe results that are qualitatively similar to those observed in MLPs;simpler features explain more variance than more complex ones. Number of tokens does still have some effect,but the pattern is less clear. Moreover, we observe substantial output order effects on representations in thedecoder ()in general, features earlier in the sequence occupy more variance.",
  "Dyck languages: structural and token-level properties dominate in different model components": "We next trained transformers on a set of more complex tasks involving processing hierarchical structures fromstring inputs. More specifically, these tasks are based on the Dyck languagesstrings of balanced brackets.These languages have been used in a variety of prior works studying generalization and interpretability inRNNs (Hewitt et al., 2020) and Transformers (Yao et al., 2021; Murty et al., 2023; Friedman et al., 2023a).Like several prior works, we create tasks based on the versions of these languages with bounded depth(Hewitt et al., 2020). We sample strings from Dyck-(20,10); that is, balanced strings over 20 brackets withnesting depth at most 10. However, in contrast to prior works (which have mostly focused on language modeling), we train transformersto output a variety of classifications of these sentences, including structure-independent token-level seman-tic features (a binary classification of the first bracket type), analogous token-level but structure-dependentfeatures (classifying the type of the first bracket at maximum depth), or purely structural features (the max-imum depth, number of root brackets, and maximum branching factor) of the sequence. These features donot capture the full structure of the sentences, but allow for more controlled experiments and analyses than",
  "(b) ResNets": ": Learning curves for test accuracy and variance explained for CNNs and ResNets trained onthe color-texture-shape tasks. (a) For CNNs Color, which is learned first, quickly comes to dominate atthe spatial pooling layer; the penultimate layer produces relatively less bias. (b) For Resnets, color brieflydominates, but is quickly overtaken by shape. (Lines are averages across 32 seeds, areas are 95%-CIs.)",
  "Relational vision tasks show similar complexity biases": "All the visual features we considered above involve computations that are arguably simpler (disjunctivetemplate matching) than the relational features (like XOR or grammatical structure) that we considered inour MLP and Transformer experiments. Thus, we performed some experiments where we trained modelson images with three objects, and asked them to classify the properties of one object, but to report therelations (same or different) among the properties of the other two objects. We reduced the basic categoryclassifications to binary labels (arbitrary splits of each type), to equalize the number of classes of basic andrelational categories (as we observed above that the number of classes affects variance). In we show the results. As would be expected, the easier basic feature representations tend tocarry more variance than the harder relational ones at the final spatial pooling layer in both architectures.Surprisingly, both model classes show consistent biases for color and texture over shape, for the basic features.All biases are very weak by the penultimate layer; moreso than we observed above. In general, the biases areweaker in this setting than with more output classes; together with a supplemental experiment (Appendix) showing that binary classifications show a weaker bias in the setting of the original classificationsabove, these results suggest that feature biases may be stronger with larger output spaces, which accordswith our transformer results showing that number of labels itself can be a strong biasing factor.",
  "Why should we care? Evaluating downstream impacts": "Computing representational variance explained by features is not a standard analysis. However, the varianceexplained by the features has various consequences. Some of these consequences are mathematically neces-sary, and we illustrate them purely for clarity, but some we merely observe empirically. In this section, weillustrate these downstream implications in causal interventions, interpretability, representational similarityanalyses, and use of representations for downstream tasks.",
  "The representations identified are causally sufficient and specific": "The above analyses are correlational. However, most philosophical accounts (e.g. Cao, 2022; Baker et al.,2022) define representations partly in terms of their causal role in a computation. An increasing number ofapproaches in interpretability similarly use causal interventions to achieve more faithful interpretations ofmodel internal representations (e.g. Geiger et al., 2021; Meng et al., 2022; Geiger et al., 2024). Here, we verifyin one case that the representations identified in our analyses indeed play a causal role in the computations.",
  "To do so, we take an MLP model (from the experiments) at the end of training, manipulate itsinternal representations, and evaluate the causal effect on its output behavior": "To manipulate model representations of a feature, we take the representation pattern predicted when thatfeature is present or absent and subtract those two patterns to create a present-absent difference vector. Wethen add (or subtract) a scalar multiple of this difference vector to the representation of each stimulus inthe test dataset, and evaluate whether by doing so we can flip the label the model predicts for that featureon that stimulus. This approach is equivalent to approaches that create steering vectors from activationdifferences (e.g. Turner et al., 2023). We show the results in . If we intervene along the subspace defined by the easy feature differencevector, we can flip the models labels of the easy feature for all items, without altering any of its labels forthe hard feature. Similarly, if we intervene along the direction of the hard feature difference vector, we canflip the hard labels without altering the easy ones (unless we make an extremely large intervention). Thus,the representations identified in our regressions are causally sufficient and specific. Moreover, these results provide another way to see that the hard features occupy less variance than the easyfeature much smaller interventions on the representations suffice to flip the models hard feature labels.These results might also have downstream consequences for each features robustness to perturbation.",
  "Impact on model simplification for interpretability": "Many approaches to mechanistic interpretability involve simplifying the models representations, for examplevisualizing their top principal components (PCs). However, these simplifications may exacerbate featurebiases; if the top PCs are biased towards certain features, lower-variance features may be missed in theinterpretation. Thus, these representation simplifications may not faithfully capture the computations of theoriginal model. We describe one way in which this could occur here, motivated by the experiments of Friedman et al. (2023a).That work shows that certain simplifications may be less faithful out-of-distribution and thus interpretationsbased on these simplifications may not give generalizable understanding of a models computations. Forexample, keeping only the top principal components of a transformer models keys and queries can yieldaccurate model behavior in distribution, but fail to generalize appropriately to more challenging test sets,",
  "Featureevaluated": "LinearAND(AND,AND)XORAND(XOR,XOR)SUM % 2 : Training MLPs on four features, sampled from a set of varying complexity. This figure showsthe effect on test accuracy (top) and representation variance explained (bottom) of a certain type of feature(colors) from the number of features of another type (panels). For example, the bottom left panel showsthe representation variance of each feature type, as a function of of how many of the other features in thedataset are linear. As the number of linear features in a dataset increases, the variance explained by otherfeatures decreases noticeably. Conversely, the presence of more hard features in the dataset tends to inflatethe variance of all simpler features. Features of intermediate difficulty, such as XOR, tend to inflate simplerfeatures, but suppress equally or more complex ones. (Note that due to small dataset size, the hardestfeature is only generalized reliably when all other features are linear. Results from training on 128 datasetswith randomly sampled sets of four features. Lines are averages, errorbars are 95%-CIs.)",
  "suggesting that the original model may use subtler features in more challenging cases, that the simplificationsomit": "Though our experimental settings are somewhat different, our results hint at a possible basis for thosephenomena. In particular, if the features required to solve the harder OOD splits are more complex, theymay occupy relatively less variance, and therefore be more readily lost when keeping only the top principalcomponents. We show a simple demonstration of this from one of our MLP models in : if we keeponly two principal components from the models representations, the model can still identify the easy featureperfectly, while hard feature accuracy remains at chance until we keep at least four principal components.(In Appx. B.11 we show that dropping the top PCs yields the opposite pattern, with hard features provingmore robust).",
  "Impact on feature visualization": "Given the above observations, clearly low-dimensional visualization may be biased towards certain features.We showed a simple example of this in , in which the top PCs are entirely clustered by the easyfeature. In Appx. B.12 we elaborate these results, showing that the hard feature appears in the next PC.However, if models are trained with more and more easy features, the hard feature is pushed into higherand higher PCs. Visualizing with t-SNE yields somewhat better results with only a single easy featuredueto t-SNEs nonlinear embedding and emphasis on neighborsbut once enough easy features are added itsimilarly becomes difficult to identify the hard feature in low-dimensional t-SNE plots as well, while theeasy features show extremely clear clusters. Thus, feature visualizations may be inherently biased towardssimpler features, those that are learned earlier, etc.",
  "Representationalsimilarity": ": Representational similarities among MLPs trained to compute different pairs of features. Thefeature representation biases produce strong biases in which networks appear similar; in some cases, networksmay appear more similar to ones that are computing entirely different features than they do to anothernetwork computing the same features. (Results are computed across 5 seeds per feature pair, using Euclideandistances and Pearson correlation.)",
  "Impact on Representational Similarity Analysis": "Representational Similarity Analysis (RSA; Kriegeskorte et al., 2008) is a technique for comparing two sys-tems (e.g. a model and the brain) based on the (dis)similarity structure of their internal representations. Thebasic idea is that while two systems representations are not directly comparable, their similarity structuresare. RSA and related techniques (e.g. CKA; Kornblith et al., 2019) are commonly used in neuroscience tocompare models to brains, and in AI for model analysis (Sucholutsky et al., 2023). However, several priorworks have noted that RSA may not perfectly reflect computational similarity (Maheswaranathan et al.,2019; Hermann & Lampinen, 2020; Dujmovi et al., 2023; Friedman et al., 2023b); our findings likewisehighlight the potential for representational similarity to dissociate from computational similarity. To illustrate this, in we perform an RSA analysis between many pairs of MLP networks. EachMLP is trained to output two features of varying difficulty. Ideally, the RSA would be more similar on thediagonal (for networks trained to perform the same two tasks) than off the diagonal, and those on the blockdiagonals (trained with just one feature that is the same) would be more similar than those off. However,the feature representation biases lead to a substantially more biased structure. For example, two modelsthat are both computing SUM%2 over two sets of features will appear to be less similar to one another thaneach appears to be a network that is solely computing simple linear features. These results use a Euclideanmetric for RSA, which is naturally sensitive to variance. However, in Appx. B.13 we show that the resultsare qualitatively similar with cosine distance as the metric.",
  "Featureeasyhard": "Classifier hiddens()(128)(128, 128) : Representational biases affect the general-ization behavior of downstream ML models that buildon the representations. This figure shows the resultsof training classifiers (linear or nonlinear) on top ofthe representations of an MLP model to predict a fea-ture that is stochastically related to both the easy andhard features. If both features are highly predictive,the classifiers are strongly biased toward relying on theeasier feature. (Results are similar for both linear andnonlinear classifiers, so we collapse across them.) A key motivation for representation learning is touse the representations to help solve downstreamtasks.Here, we assess how feature biases in thepretrained representations will affect the behaviorof the downstream model. To do so, we take several MLP models trained above,and train (non)linear classifiers on top of their rep-resentations, to predict a label that is probabilisti-cally related to both easy and hard features inde-pendently. More specifically, we create a range ofdatasets where the label is independently predictedby the easy and hard features with varying probabil-ity, but where both features are equally predictive.We then test the extent to which the classifiers relyon each feature by testing on datasets where the fea-tures are decorrelated, and only one feature predictsthe label. This setting is similar to some of the ex-periments of Hermann & Lampinen (2020) and Her-mann et al. (2023), but beginning from multi-taskpretrained representations rather than raw inputs. In this setting, we show in that the biasesin the feature representations affect the downstreammodel behavior. Specifically, we plot the degree towhich the classifier outputs prefer each featurewhen they are pitted against one another, acrossvalues of feature predictivity. If the features havelow predictivity, the classifier relies equally on bothfeatures. However, as the features predictivity increases, the classifiers exhibits increasing bias towards theeasier feature.",
  "Related work": "Implicit inductive biases towards simplicity: Highly over-parameterized deep networks sometimesgeneralize well even when they could memorize the entire dataset (Zhang et al., 2021). To make sense ofthese observations, a variety of works have proposed an implicit inductive bias for simplicity, either in thearchitectures themselves (Valle-Perez et al., 2018; Huh et al., 2021; Rahaman et al., 2019) or in combinationwith gradient-based learning dynamics (Tachet et al., 2018; Arora et al., 2019; Advani et al., 2020; Kalimeriset al., 2019; Lampinen & Ganguli, 2018; Shah et al., 2020; Lu et al., 2023; Xu et al., 2023). In particular,Valle-Perez et al. (2018) emphasize that networks with random parameters are biased towards computingsimpler functions, which may contribute to the relative ease of learning simpler featuresand the greatersparsity in representations of more complex features. However, most of these prior works have focused on the case where the features redundantly serve a singletask, rather than the case where the network is computing all of them, and have measured this bias at thelevel of the input-output behavior, rather than in the networks internal representations. Our results suggestthat the impact of these biases persists in the networks internal representations even in a setting whereit has learned to compute the more complex features equally well. Moreover, these representational biasesmay even help to amplify other inductive biases because, once some features are more strongly represented,they will be favored in later layers learning (cf. 6.4); thus, they may be part of the mechanism of the",
  "Discussion": "We have attempted to systematically characterize the internal representations of deep learning models thatare trained to compute multiple features of their input. We found that these representations can be biasedtowards one feature or another by a variety of factors such as the relative complexities of the features,their positions and extents, their prevalences, and their learning order. Some of these biases interact witharchitecture and optimizer in complex ways, while others are relatively consistent. We believe characterizing these biases is valuable for multiple reasons. First, there is inherent scientific valuein exploring the complex phenomena of gradient-based representation learning that are not, to the best ofour knowledge, fully understood. Some of our observations echo the previously-observed simplicity bias ofdeep learning architectures, but at a representational rather than behavioral level. These representationalsimplicity biases appear to be partly driven be learning order, and partly by the fact that there are inherentlymore ways to represent a more nonlinear feature. Second, our findings have important practical implicationsfor interpretability and neuroscience methods that rely on analyzing model representations. Implications for interpretability: Machine-learning researchers often want to interpret model represen-tations to understand the models (Olah et al., 2018; Geiger et al., 2021; Olsson et al., 2022; Bills et al.,2023)either for scientific reasons, or to attempt to assess their safety and reliability, or to improve modelbehavior (Zou et al., 2023; Muttenthaler et al., 2024). However, many of these analyses implicitly or ex-plicitly prioritize features that carry high variance in the representations. This prioritization is explicit inthe case of using methods like PCA to analyze model representations. Indeed, as we suggested above, our",
  "Limitations & future directions": "There are a number of limitations to our work that would be useful to address in the future. First, we focusedon simple, synthetic datasets. While this choice allowed complete control over the tasks, the features, andtheir correlations, it is possible that biases would be different on more realistic datasets with larger setsof entangled features. However, concurrent work from Fel et al. (2024) finds compatible results for largermodels trained on ImageNet. Moreover, we focused on classification because, again, it made the computational solution more straightfor-ward to analyze. It would be interesting to consider how these biases differ under other objectivesit is",
  "possible that continuous functions yield qualitatively different feature learning than discrete classifications,6": "though an increasing amount of work has converged on some kind of (sequential, soft) classification, e.g.language modeling. However, a limitation of our tasks relative to language modeling is that the represen-tations we consider are not richly contextual. Transformers can learn to restructure feature representationsaccording to the task context (Li & McClelland, 2023); in future work it would therefore be important tostudy how representation biases interact with more richly contextual computation. Moreover, the models we used were of the toy scale used in a range of mechanistic interpretability work(e.g. Nanda et al., 2023; Yao et al., 2021). While we did not see dramatic effects of parameter count inthe regimes we considered, we did observe somewhat larger biases in deeper models, and it is possible thatqualitatively different biases would emerge in larger models; thus it would be useful to study these effects atscale in future work. Furthermore, we found that the biases we observed can be shifted in nontrivial ways byhyperparameter choices (3.6). However, we did not observe these interactions in Transformers trained onmore complex datasets. We therefore expect that in standard training settings, where models are trained oncomplex tasks without full convergence, the representational biases would be more similar to those observedin earlier stages of training. Nevertheless, future work should investigate these interactions. Finally, our main analyses relied on linear regression. While these analyses can account for many formsof representationparticularly in 3.3, where the augmented all-input-patterns space can account for anyfunction of the hard inputs alonethey are limited to information that is not linearly available from thataugmented space. For example, if the hard features were represented in superposition (cf. Elhage et al.,2022) with other noise inputs, a linear analysis from solely the hard feature space could not account for that.In general, it is difficult to completely test the possibility that some representation non-linearly encodessome information (if it were easy, then encryption would not work). However, the fact that we observe thatthe majority of the total variance in the system is explained by these linear analyses, and that they aresufficient for causally changing the behavior of the system in a targeted way (6.1), supports the idea thatlinear analyses provide useful insight.7 Finally, linear analyses are empirically justifiable in the context oftheir frequent use in interpretability and model steering research (e.g. Liu et al., 2022; Turner et al., 2023;Oikarinen & Weng, 2024), and more general arguments that information tends to be represented linearly(Park et al., 2024). Nevertheless, more deeply exploring the space of nonlinear representations within modelsremains an important direction for future work.",
  "Conclusions": "We have trained deep learning models to compute multiple features of their inputs. We found that, evenonce all features have been learned, the models representations are substantially biased towards or awayfrom features by properties like their complexity, the order in which they were learned, their prevalence inthe dataset, or their position in the output sequence. These representational biases may relate to some of theimplicit inductive biases of deep learning. More pragmatically, representational biases could pose challengesfor interpreting learned representations, or comparing them between different systemsin machine learning,cognitive science, and neuroscience.",
  "Acknowledgements": "We thank Roma Patel, Aaditya Singh, Spencer Frei, Dan Friedman, Lukas Muttenthaler, Asma Ghandehar-ioun, Ed Grefenstette, and the anonymous reviewers for comments and suggestions on earlier versions of thepaper. 6Note, however, that discrete features as input and output can nevertheless be best served by more continuous internalrepresentations. For example, the nonlinear hard feature we use in the first experiments is naturally computed by first summing4 binary values, giving a sum between 0 and 4 as an intermediate step.7Though note that just observing the causal sufficiency of linear features does not rule out the possibility that some nonlinearfeature is playing a role that gets preempted by the intervention on the linear feature (cf. Mueller, 2024).",
  "Tolga Bolukbasi, Adam Pearce, Ann Yuan, Andy Coenen, Emily Reif, Fernanda Vigas, and Martin Wat-tenberg. An interpretability illusion for bert. arXiv preprint arXiv:2104.07143, 2021": "James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin,George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: compos-able transformations of Python+NumPy programs, 2018. URL Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner,Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer,Tim Maxwell, Nicholas Joseph, Zac Hatfield-Dodds, Alex Tamkin, Karina Nguyen, Brayden McLean,Josiah E Burke, Tristan Hume, Shan Carter, Tom Henighan, and Christopher Olah. Towards monose-manticity: Decomposing language models with dictionary learning. Transformer Circuits Thread, 2023a. Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner,Cem Anil, Carson Denison, Amanda Askell, et al. Towards monosemanticity: Decomposing languagemodels with dictionary learning. Transformer Circuits Thread, pp. 2, 2023b.",
  "Rosa Cao. Putting representations to use. Synthese, 200(2):151, 2022": "Stephanie Chan, Adam Santoro, Andrew Lampinen, Jane Wang, Aaditya Singh, Pierre Richemond, JamesMcClelland, and Felix Hill. Data distributional properties drive emergent in-context learning in trans-formers. Advances in Neural Information Processing Systems, 35:1887818891, 2022. Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Hieu Pham, Xuanyi Dong, ThangLuong, Cho-Jui Hsieh, Yifeng Lu, et al. Symbolic discovery of optimization algorithms. Advances inNeural Information Processing Systems, 36, 2024.",
  "Karl Deisseroth. Optogenetics: 10 years of microbial opsins in neuroscience. Nature neuroscience, 18(9):12131225, 2015": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchicalimage database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee,2009. Marin Dujmovi, Jeffrey S Bowers, Federico Adolfi, and Gaurav Malhotra. Obstacles to inferring mechanisticsimilarity using representational similarity analysis. bioRxiv, 2023. doi: 10.1101/2022.04.05.487135. URL Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, ZacHatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, et al. Toy models of superposition. arXivpreprint arXiv:2209.10652, 2022.",
  "Dan Friedman, Andrew Lampinen, Lucas Dixon, Danqi Chen, and Asma Ghandeharioun. Interpretabilityillusions in the generalization of simplified models. arXiv preprint arXiv:2312.03656, 2023a": "Dan Friedman, Andrew Kyle Lampinen, Lucas Dixon, Danqi Chen, and Asma Ghandeharioun. Comparingrepresentational and functional similarity in small transformer language models. In UniReps: the FirstWorkshop on Unifying Representations in Neural Models, 2023b. Atticus Geiger, Hanson Lu, Thomas Icard, and Christopher Potts. Causal abstractions of neural networks.In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances inNeural Information Processing Systems, volume 34, pp. 95749586. Curran Associates, Inc., 2021. Atticus Geiger, Zhengxuan Wu, Christopher Potts, Thomas Icard, and Noah Goodman. Finding alignmentsbetween interpretable causal variables and distributed neural representations. In Causal Learning andReasoning, pp. 160187. PMLR, 2024. Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and WielandBrendel. Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy androbustness. arXiv preprint arXiv:1811.12231, 2018. Robert Geirhos, Jrn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, MatthiasBethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelli-gence, 2(11):665673, 2020.",
  "John Hewitt, Michael Hahn, Surya Ganguli, Percy Liang, and Christopher D Manning. Rnns can generatebounded hierarchical languages with optimal memory. arXiv preprint arXiv:2010.07515, 2020": "Irina Higgins, Loic Matthey, Arka Pal, Christopher P Burgess, Xavier Glorot, Matthew M Botvinick, ShakirMohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variationalframework. ICLR (Poster), 3, 2017a. Irina Higgins, Nicolas Sonnerat, Loic Matthey, Arka Pal, Christopher P Burgess, Matko Bosnjak, MurrayShanahan, Matthew Botvinick, Demis Hassabis, and Alexander Lerchner. Scan: Learning hierarchicalcompositional visual concepts. arXiv preprint arXiv:1707.03389, 2017b.",
  "Yuxuan Li and James McClelland. Representations and computations in transformers that support gener-alization on structured tasks. Transactions on Machine Learning Research, 2023": "Xu Liu, Steve Ramirez, Petti T Pang, Corey B Puryear, Arvind Govindarajan, Karl Deisseroth, and SusumuTonegawa. Optogenetic stimulation of a hippocampal engram activates fear memory recall. Nature, 484(7394):381385, 2012. Ziming Liu, Ouail Kitouni, Niklas S Nolte, Eric Michaud, Max Tegmark, and Mike Williams. Towardsunderstanding grokking: An effective theory of representation learning. Advances in Neural InformationProcessing Systems, 35:3465134663, 2022.",
  "Jack Merullo, Carsten Eickhoff, and Ellie Pavlick.Circuit component reuse across tasks in transformerlanguage models. In The Twelfth International Conference on Learning Representations, 2023": "Milton Montero, Jeffrey Bowers, Rui Ponte Costa, Casimir Ludwig, and Gaurav Malhotra. Lost in latentspace: Examining failures of disentangled models at combinatorial generalisation. Advances in NeuralInformation Processing Systems, 35:1013610149, 2022. Milton Llera Montero, Casimir JH Ludwig, Rui Ponte Costa, Gaurav Malhotra, and Jeffrey Bowers. Therole of disentanglement in generalisation. In International Conference on Learning Representations, 2020.",
  "Shikhar Murty, Pratyusha Sharma, Jacob Andreas, and Christopher D Manning. Grokking of hierarchicalstructure in vanilla transformers. arXiv preprint arXiv:2305.18741, 2023": "Simon Musall, Matthew T Kaufman, Ashley L Juavinett, Steven Gluf, and Anne K Churchland. Single-trialneural dynamics are dominated by richly varied movements. Nature neuroscience, 22(10):16771686, 2019. Lukas Muttenthaler, Lorenz Linhardt, Jonas Dippel, Robert A Vandermeulen, Katherine Hermann, An-drew Lampinen, and Simon Kornblith. Improving neural network representations using human similarityjudgments. Advances in Neural Information Processing Systems, 36, 2024.",
  "Remi Tachet, Mohammad Pezeshki, Samira Shabanian, Aaron Courville, and Yoshua Bengio. On the learningdynamics of deep neural networks. arXiv preprint arXiv:1809.06848, 2018": "Frederik Truble, Elliot Creager, Niki Kilbertus, Francesco Locatello, Andrea Dittadi, Anirudh Goyal, Bern-hard Schlkopf, and Stefan Bauer.On disentangled representations learned from correlated data.InInternational conference on machine learning, pp. 1040110412. PMLR, 2021. Greta Tuckute, Jenelle Feather, Dana Boebinger, and Josh H McDermott. Many but not all deep neuralnetwork audio models capture brain responses and exhibit correspondence between model stages and brainregions. Plos Biology, 21(12):e3002366, 2023.",
  "Guillermo Valle-Perez, Chico Q Camargo, and Ard A Louis. Deep learning generalizes because the parameter-function map is biased towards simple functions. arXiv preprint arXiv:1805.08522, 2018": "Sjoerd Van Steenkiste, Francesco Locatello, Jrgen Schmidhuber, and Olivier Bachem. Are disentangledrepresentations helpful for abstract visual reasoning? Advances in neural information processing systems,32, 2019. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser,and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30,2017.",
  "Sarah Wiegreffe and Yuval Pinter. Attention is not not explanation. arXiv preprint arXiv:1908.04626, 2019": "Zhengxuan Wu, Atticus Geiger, Thomas Icard, Christopher Potts, and Noah Goodman. Interpretability atscale: Identifying causal mechanisms in alpaca. Advances in Neural Information Processing Systems, 36,2024. Zhiwei Xu, Yutong Wang, Spencer Frei, Gal Vardi, and Wei Hu. Benign overfitting and grokking in relunetworks for xor cluster data. In The Twelfth International Conference on Learning Representations, 2023. Yihao Xue, Siddharth Joshi, Eric Gan, Pin-Yu Chen, and Baharan Mirzasoleiman.Which features arelearnt by contrastive learning? On the role of simplicity bias in class collapse and feature suppression.In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and JonathanScarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 ofProceedings of Machine Learning Research, pp. 3893838970. PMLR, 2329 Jul 2023. URL",
  "ASupplemental methods": "All models were trained in JAX (Bradbury et al., 2018) using the flax (Heek et al., 2023) library; networkswere generally constructed from the corresponding flax examples.All representation regressions duringtraining were fit using scikit-learn (Pedregosa et al., 2011). Plots were created with the Tidyverse (Wickhamet al., 2019). Plot color palettes are loosely based on Harrower & Brewer (2003).",
  "A.1Hyperparameters & architectural details": "In we report hyperparameter values for all main experiments. Note that different choices for somehyperparameters for the MLP experiments are systematically tested in Section B.7, for example a larger setof optimizers. Hyperparameter values were generally chosen from prior work, or from the flax (Heek et al.,2023) examples; they were changed if the models exhibited training instability or failed to learn.",
  "Additional architectural details are provided below for each model": "MLP: The network contained four hidden layers, with sizes , followed by a final outputlayer. Initialization was variance-scaling with scale 1. The nonlinearity at the hidden layers was a leakyrectifier with negative slope 0.01. The model was trained using a sigmoid cross-entropy loss. Transformer: The network was an encoder-decoder transformer (Vaswani et al., 2017) as implemented inthe flax examples, with 4 layers in each component, 4 heads per layer, an embedding size of 128 (and thusQ/K/V dimension of 32), and an MLP dimension of 512. The implementation uses the layernorm on thebranch input rather than the residual connection, following Radford et al. (2019). The model uses standardsinusoidal position embeddings. The model was trained using a softmax cross-entropy loss, as in standardlanguage modeling. CNN: The architecture was a standard CNN that alternates convolution with mean pooling. Convolutionfilter sizes were (6, 4, 3, 2), channels were (128, 128, 128, 256, 256), strides were (3, 1, 1, 2, 1), and poolingsizes and strides were (3, *, *, 2, *). All convolutions and the MLP hidden layer used rectifier nonlinearities.The network was trained using a softmax cross-entropy loss for multi-way classification.",
  "A.2.1Boolean": "For the Boolean experiments, the inputs were vectors with 64 boolean values, and the outputs were vectorswith 2 boolean values. Some number of the inputs were devoted to each of the features, while the rest wereset randomly. The inputs that were relevant for one feature did not overlap with those relevant for anotherfeature. For our main experiments we redundantly encoded the linear features across four input units, tomatch the number used for the hardest features; however, in we show this redundant encoding doesnot substantially affect the results.",
  "A.2.2Language": "All language problems were posed as sequence-to-sequence tasks, where an input sequence was provided andthe model was asked to output a sequence of classifications of features of that input. As the position offeatures in the output sequence affects their variance, the output sequence order was randomized for eachtraining run. Letter strings: The datasets and features were similar to the boolean tasks, except for a larger input vo-cabulary, and the sequential structure of the inputs. The input sentences were constructed from a vocabularyof 9 letters (A-I). There were 5 input letters provided per feature, with the relevant letters appearing at thebeginning of that chunk, thus there was at least one irrelevant (noise) letter at the end of each featuresrelevant inputs. Since 8 features were used, input sequences were length 8 5 = 40. The input letters wereinitially sampled randomly, but then labels were assigned to each input to balance each feature (indepen-dently) across the dataset, and the relevant input letters were then resampled until the desired label wasreached (rejection sampling). The features used were applied to chunks of 1-4 letters, with the feature beingeither exact match to a letter string, all-but-one letter matching (XOR-like), or the sum of the letter matchesmod 2. Naturalistic language: The sentences used for this work were sampled from a simple grammar thatgenerates grammatical English sentences, but within a relatively constrained range. The grammar focuseson two types of sentence structureswith either nested or successive noun-verb dependencieswhich areloosely inspired by prior work on grammar processing in neural models (Lakretz et al., 2021). The primitivesof the grammar are provided below; the items with a question mark were introduced with some probability(0.5 for the adjectives, 0.33 for all others). The valence and tense (present or past) of the sentence were setto be the same across all of the adjectives/verbs in the sentence.",
  "positive : [good , cool , awesome , wonderful ],negative : [bad , uncool , terrible , worthless ],neutral : [acceptable , bland , generic , tolerable ],}": "LANG_AGENTS = [{singular : person , plural : people },{singular : child , plural : children },{singular : adult , plural : adults },{singular : dog , plural : dogs },{singular : horse , plural : horses },{singular : owl , plural : owls },{singular : robot , plural : robots },]",
  "bythe + x for x in [park , school , house , office , lake , forest ] + [z for y inLANG_AGENTSfor z in y.values ()]]": "From these sentences, we trained the model to output a variety of features: whether the sentence was writtenin present or past tense, the valence, the structure type (nested or sequential), whether each of the nounswas singular or plural, and their identities. The latter features require parsing the sentence to determine itsstructure, and to identify which noun appears where while ignoring the other noun (and any that occur indistractor location clauses). Dyck: For the Dyck language experiments we used sentences sampled from Dyck-(20,10)that is, 20 brackettypes and a max depth of 10. We also enforced a length limit of 64 tokens. We sampled each sequence onetoken at a time: first we selected whether it would be an open or close bracket. If the type was unconstrained,we set a probability of 70% that it would be the same type as the previous bracket; e.g., that an open bracketwould be followed by another open bracket. This biased sampling yields more interesting structures. If thetoken was at 0 or max depth, or close to the max length, the type was constrained appropriately to fit thelanguage constraints. If the token was selected to be an open bracket the bracket class was randomly sampled from the set of 20bracket classes. If it was a close bracket, it was chosen to match the corresponding opening bracket. After the first token, when the depth reached 0, the sentence was terminated with 50% probability; otherwise,a new opening bracket was sampled. Thus, this dataset contains sentences of varying lengths. We trained the model to compute 5 features about the sentence: the class of the first token, the class of thefirst token that appeared at the max depth of the sentence, the maximum depth itself, the number of rootnodes (i.e. opening brackets at depth 0), and the max branching factor (i.e. the max number of openingbrackets contained within another pair, or the number of root nodes if it was larger). Note that unlike thenaturalistic language tasks above, the sentence length is correlated with various features such as the numberof root nodes. Thus we control for length in our analyses.",
  "A.2.3Vision": "In we show examples of each of the 10 colors, textures, and shapes that we used in the visionexperiments. Note that, while our experiments were inspired by Hermann & Lampinen (2020) we do not useprecisely the same feature sets that they did; our textures and shapes are somewhat simpler and the factthat we do not randomly rotate them likely makes the classification problem slightly easier. However, we donot expect this to substantially alter the results. In we show example stimuli for the basic and relational tasksnote that in the latter case, thebasic features are reported for the object in the left column of the image, and the relational features are",
  "B.2Representation variance explained curves with and without normalization": "In the learning curves we generally plot variance explained normalized by total variance at the end of training.In we compare this to the values normalized by current total variance. While the qualitative biasesare similar, the alternative normalization makes the earlier learning dynamics more visible. Furthermore, italso shows that at initialization, the easy feature explains more variance than the hard featurebut all hardinput patterns explain even more. Representation variance explained (% of current total variance) Representation variance explained (% of final total variance) Training steps",
  "B.3MLP learning curves by training order": "In we show the learning curves for easy and hard features, across different training orders. Whenthe hard feature is pretrained, it occupies more variance than the easy feature initially, but the easy featurequickly overtakes it once it starts to be learned. However, when considering all input patterns associatedwith the hard feature, the effect of pretraining is much clearer; and the curve more clearly shows the inflationin the later phase of learning that is observed with the easy feature in the other cases.",
  "B.4Per-unit representation variance analysis": "In this section we present analyses of how each of the 64 units in the penultimate layer of the model representseach feature for a single run of the basic model trained on the linear and sum mod 2 tasks. In weshow snapshots of the variance explained by each feature for each unit, at various salient stages in training.In we show individual learning curves for all the units. The qualitative patterns are similar to theaggregate case, but there are interesting individual effects to observe. Most notably, the units that end upcarrying the most variance about the difficult feature are also those that initially carry the most varianceabout the easy feature, and the variance they carry about the easy feature is inflated as the hard featureis learned. Thus, most of the individual units also continue to carry more variance due to the easy feature.Only a subset of the initially-easy-selective neurons acquire some hard-relevant signal later (which likelycontributes to the sparsity difference observed below). However, a few neurons that initially carried at leastas much signal about the hard inputs as easy ones do tend to become somewhat more selective for the hardinput features as the harder task is learned. Furthermore, many of the units carry non-trivial signal aboutother inputs (or nonlinear patterns of the feature-relevant inputs) that is also amplified as the other features",
  "are learned either because those inputs are weakly correlated with a task feature within the dataset bychance, or just due to the learning dynamics": "Hard test 95% (42250)Hard test 100% (43500)Final (69000) Initial (0)Easy test 95% (500)Easy test 100% (1000)Hard begins learning (27000) 0.0 0.5 1.0 1.5 2.0 0.00 0.02 0.04 0.06 0.0000 0.0025 0.0050 0.0075 0.0100 0.000 0.001 0.002 0.003 Unit (sorted by final total variance) Unit variance attributed to feature feature easyhardhard all inputpatternsother inputs : The distribution of variance explained by each feature across the 64 units of an MLP at variousstages of training. (The ordering of units is consistent across the plots, and is determined by the final plot.)",
  "Epoch": "Unit variance attributed to feature (log) feature easyhardhard all inputpatternsother inputs : Raw representation variance plotted across learning for each of the 64 units in the penultimatelayer of an MLP. Note that this figure plots raw variance explained, without normalizing by the total varianceof the unit, to facilitate comparing across units. Note also that the vertical axis is log-scaled; where thevariance explained on the test set was 0 it is omitted from the plot.",
  ": Harder features also yield sparser representations. (Lines are means across 10 seeds, errorbarsare 95%-CIs)": "We also find that more complex features tend to be represented more sparsely in the models representations(). To measure sparsity, we evaluate the hidden representations, and then standardize element-wise sothat each unit has variance 1. We then fit a logistic regression decoding the feature, and evaluate the entropyof the softmax of the absolute value of the regression coefficients. If the regression integrates informationmore uniformly across all the units, this entropy value will be higher, while if fewer units are contributing,the entropy value will be lower; thus, we subtract this measure from the maximum entropy (a uniformdistribution) and divide by the maximum entropy to calculate a 0-1 sparsity score for each feature. By thismeasure, the harder features are noticeably sparser in the models representations, even in the case that theyare pretrained.",
  "B.6More features of varying complexity": "While our main MLP experiments only trained models to compute two features, deep networks trained onreal tasks will compute many more. To move slightly closer to this regime, we trained networks to predictfour features (N.B. we also use more features in our language and vision experiments), which we randomlysampled from a set of five features of varying complexity. We trained the models to predict all featuressimultaneously, and as above evaluated test accuracy and variance explained by each feature. We show the results in . In general, the patterns are roughly what would be expected from thetwo-feature experiments above: adding more simple features to the dataset tends to suppress more complexfeatures, while adding more complex features inflates simpler ones. Note that we cannot disentangle theseeffects fully here, but see the learning curves above Appx. B.3 for evidence that both inflation and suppressionhappen in the two feature case. Features of intermediate difficulty tend to inflate simpler features but interferewith more complex ones.",
  "B.7The effects of MLP architecture and other hyperparameters": "How specific are our findings to the particular MLP architecture we evaluated? In we show the effectof depth/width/aspect ratios (i.e. ratios of layer sizes) of the MLP on our findings; in short, deeper MLPsshow somewhat larger gaps, width and aspect ratio appear to have little effect. Residual MLPs also showqualitatively similar biases (), in contrast to our finding that ResNets have strikingly different biasesto CNNs (11). Prior work has found that tanh and rectifier networks have qualitatively different representation-learningproperties (Alleman et al., 2024), thus we performed a similar comparison here (). Overall patternsare similar, but we do note a slightly smaller gap in general with tanh networks. This observation may fitwith the results of Alleman et al., who found that tanh network representations were generally more drivenby the output structure, while rectifier network representations were driven more by input structure; thatfinding might suggest a somewhat smaller gap for tanh networks in our setting, since the output structureis identical for both features, but the input structure differs. In the main experiments we encoded the easy feature redundantly across 4 input units (to match the numberof inputs used for the hard feature); in we show that this redundant encoding of the easy featureslightly increases the gap, but does not change the qualitative pattern of results. In our main experiments, we used variance scaling initializers with an initialization scale of 1.0 for thenetwork weights (that is, we initialized the weights to approximately preserve the variance of signals throughthe network at the beginning of training). However, we show in that smaller initialization scalesyield somewhat larger biases towards the easy features.",
  "B.7.1Dropout and optimizer interact for MLP architectures": "The results we observed in the main experiments were with SGD. In , we show that various otheroptimizers (e.g. Kingma & Ba, 2014; Loshchilov & Hutter, 2017; Chen et al., 2024) show similar effectswithout dropout.However, in we show that enabling dropout has different effects on differentoptimzers at convergence. For SGD and adagrad, the bias direction is consistent across dropout rates, thoughthe bias magnitude shows a minimum at moderate dropout rates, but for other optimizers the biases reverse",
  "Optimizer": "Representation variance explained (%) feature easyhardhard all inputpatterns : Effect of optimizer on the results without dropout. Patterns are qualitatively similar with alloptimizers. However, we see more dramatic interactions with dropout enabled, see Fig. B.7.1. (Bars aremeans across 10 seeds, errorbars are 95%-CIs.) or disappear at moderate or high dropout rates. However, this effect only occurs at convergence; earlier inlearning the easy features dominate, even after the hard features are generalized fairly well. Furthermore, in we show that the feature biases in Transformers trained on sequence datasets are more consistent,even with dropout enabled.",
  "B.8Transformer sequential learning curves": "In we show learning curves for transformers trained on the letter-sequence tasks, with one featureenabled at a time. We observe some representation biases due to feature order, more so at the encoder layer.In fact, there seems to be relatively little change in the feature ranking at the encoder layer once the first fewfeatures are learned. One possible explanation is that the encodings may be good enough to not requiresubstantial further learning to identify other features at other positions, thus leaving a lingering bias. At the decoder layer, we see more complex patterns. We use 100 epochs to smoothly transition to adding theloss on the new feature, but even so we observe substantial interference, which increases with each subsequentfeature. We also see some decay towards the end of learning each feature, likely due to the weight decay.The magnitude of bias appears to vary as different features are trained. Note that because the transformersprocessing is shared across time points, variance explained for a given feature tends to increase even beforeits loss weight is enabled. The weight sharing may also contribute to the overall inflation of the varianceexplained by each feature as learning progresses (the staircase pattern), even once that feature has beencompletely learned. Encoder R^2 (%)Penultimate R^2 (%) 0.5 1.0 Training epochs Representation variance explained (%)",
  "Featuretrainingorder": ": Learning curves for sequential training in Transformers. Feature order biases are noticeable, butappear weaker than in the MLPs, at least at the penultimate layer. However, several other qualitativelyinteresting patterns appear, including very little change in the encoder after the first few features are learned,and decay, interference, and recovery in the decoder. (Lines are averages across 48 seeds. Dashed linesindicate when the loss begins to ramp up for each feature.)",
  "B.10.1ResNet representational biases for shape are affected by the number of class labels and thepresence of distractors": "We noted a surprising shift in representational biases of ResNets between our two vision settings: favoringshape in one case, but favoring color and texture in another. Here, we disentangle the contributors to thiseffect, by removing some of the confounding changes. First, in a we show that these changes are not driven solely by the shift to binary labels; if we simplyreplace the original single-object classification task with the binary-label version, without changing the inputdistribution or adding new objects, we still see shape biases (though they are somewhat less pronounced). Second, we keep training only on binary classifications of the basic features, but switch to the input distribu-tion of the relational tasks; that is, we effectively introduce two distractor objects, which the model simplyhas to ignore (because we train it only to report binary classifications of the first object). Surprisingly, thebiases shift in this case to favor color and texture over shape (b). Thus, because the biases were reduced with binary labels in the original setting, and flipped in the multi-object setting, it seems that both changes may contribute to the bias shift, but with a larger contributionfrom the distractor objects.",
  "(b) With distractor objects": ": Representational biases of ResNets trained to report binary classifications of the color, shape, andtexture of a single object shift depending on the other objects in the image. (a) with no other objects in theimage, the representations still show shape bias, but more weakly than with the original 10-way classifications(b). (b) With distractor objects in the image, the biases shift to favor color and texture. (Lines areaverages across 32 seeds, areas are 95%-CIs.)",
  "B.11Keeping all but the top-k principal components": "In we show that if we drop the top principal components from the models representations (rather thanpreserving them, as is typical), the predictions of the hard feature are robust to dropping more components,because the top components are occupied with the easy feature. 12345678910 11 12 13 14 15 Top representation SVD components removed Accuracy (%) feature easyhard",
  "(i) 3 easy, PCs 5/6": "PC2 PC1 Easy Features(0, 0, 0, 0)(0, 0, 0, 1)(0, 0, 1, 0)(0, 0, 1, 1)(0, 1, 0, 0)(0, 1, 0, 1)(0, 1, 1, 0)(0, 1, 1, 1)(1, 0, 0, 0)(1, 0, 0, 1)(1, 0, 1, 0)(1, 0, 1, 1)(1, 1, 0, 0)(1, 1, 0, 1)(1, 1, 1, 0)(1, 1, 1, 1)Hard Feature01",
  "(j) 4 easy, PCs 1/2": "10.07.55.02.50.02.55.07.510.0 PC4 PC3 Easy Features(0, 0, 0, 0)(0, 0, 0, 1)(0, 0, 1, 0)(0, 0, 1, 1)(0, 1, 0, 0)(0, 1, 0, 1)(0, 1, 1, 0)(0, 1, 1, 1)(1, 0, 0, 0)(1, 0, 0, 1)(1, 0, 1, 0)(1, 0, 1, 1)(1, 1, 0, 0)(1, 1, 0, 1)(1, 1, 1, 0)(1, 1, 1, 1)Hard Feature01",
  "(k) 4 easy, PCs 3/4": "PC6 PC5 Easy Features(0, 0, 0, 0)(0, 0, 0, 1)(0, 0, 1, 0)(0, 0, 1, 1)(0, 1, 0, 0)(0, 1, 0, 1)(0, 1, 1, 0)(0, 1, 1, 1)(1, 0, 0, 0)(1, 0, 0, 1)(1, 0, 1, 0)(1, 0, 1, 1)(1, 1, 0, 0)(1, 1, 0, 1)(1, 1, 1, 0)(1, 1, 1, 1)Hard Feature01",
  "(l) 4 easy, PCs 5/6": ": Principal components visualizations of penultimate representations from MLPs that are trained tooutput a fixed hard feature (sum % 2), together with different numbers of easy linear features (increasing from1-4 across rows). As easy features are added, their labels (colors) spread across more principal components(columns), and thus the dimension along which the hard features (shapes) separate is pushed into higherand higher principal components. With 1 easy feature (top row), the first two PCs are clustered accordingto the easy feature (a), but the hard feature is clustered along the third principal component (b). When twoeasy features are learned, the third principal component is used to help represent them, and the hard featureis pushed into the fourth principal component (e). When three or four easy features are learned, the hardfeature is pushed into the 5th or 6th PCs, respectively (i & l). There is a remarkably consistent progressionthat as each easy feature is added, another PC is captured, and the hard feature is pushed one PC higher.",
  "(c) 3 easy features": "t-SNE 1 t-SNE 2 Easy Features(0, 0, 0, 0)(0, 0, 0, 1)(0, 0, 1, 0)(0, 0, 1, 1)(0, 1, 0, 0)(0, 1, 0, 1)(0, 1, 1, 0)(0, 1, 1, 1)(1, 0, 0, 0)(1, 0, 0, 1)(1, 0, 1, 0)(1, 0, 1, 1)(1, 1, 0, 0)(1, 1, 0, 1)(1, 1, 1, 0)(1, 1, 1, 1)Hard Feature01",
  "(d) 4 easy features": ": t-SNE visualization of the penultimate representations from MLPs that are trained to output afixed hard feature (sum % 2), together with different numbers of easy linear features (increasing from 1-4across panels). With a small number of easy features, the clustering according to both the easy feature values(colors) and the hard features (shapes) emerges quite clearly. The existence of the hard feature is generallymore readily visible in these t-SNE plots than in examining just the top few principal components (cf. ). However, as more easy features are introduced, they come to dominate the representation space, andwith 4 easy features it would be difficult to discriminate the hard features separating most clusters if theywere not already labelled with shapes.",
  "B.13RSA result resilience": "In we show that the RSA results in the main text are qualitatively similar if different metrics areused for the analysis, or if Spearman rank correlation is used to compare the dissimilarity matrices. pearsonspearman cosineeuclidean (Linear, Linear) (Linear, AND) (Linear, XOR) (Linear, AND(XOR,XOR)) (Linear, SUM%2) (AND, Linear) (AND, AND) (AND, XOR) (AND, AND(XOR,XOR)) (AND, SUM%2) (XOR, Linear) (XOR, AND) (XOR, XOR) (XOR, AND(XOR,XOR)) (XOR, SUM%2) (AND(XOR,XOR), Linear) (AND(XOR,XOR), AND) (AND(XOR,XOR), XOR) (AND(XOR,XOR), AND(XOR,XOR)) (AND(XOR,XOR), SUM%2) (SUM%2, Linear) (SUM%2, AND) (SUM%2, XOR) (SUM%2, AND(XOR,XOR)) (SUM%2, SUM%2) (Linear, Linear) (Linear, AND) (Linear, XOR) (Linear, AND(XOR,XOR)) (Linear, SUM%2) (AND, Linear) (AND, AND) (AND, XOR) (AND, AND(XOR,XOR)) (AND, SUM%2) (XOR, Linear) (XOR, AND) (XOR, XOR) (XOR, AND(XOR,XOR)) (XOR, SUM%2) (AND(XOR,XOR), Linear) (AND(XOR,XOR), AND) (AND(XOR,XOR), XOR) (AND(XOR,XOR), AND(XOR,XOR)) (AND(XOR,XOR), SUM%2) (SUM%2, Linear) (SUM%2, AND) (SUM%2, XOR) (SUM%2, AND(XOR,XOR)) (SUM%2, SUM%2) (Linear, Linear) (Linear, AND)(Linear, XOR) (Linear, AND(XOR,XOR)) (Linear, SUM%2) (AND, Linear) (AND, AND)(AND, XOR) (AND, AND(XOR,XOR)) (AND, SUM%2) (XOR, Linear) (XOR, AND)(XOR, XOR) (XOR, AND(XOR,XOR)) (XOR, SUM%2) (AND(XOR,XOR), Linear) (AND(XOR,XOR), AND)(AND(XOR,XOR), XOR) (AND(XOR,XOR), AND(XOR,XOR)) (AND(XOR,XOR), SUM%2) (SUM%2, Linear) (SUM%2, AND)(SUM%2, XOR) (SUM%2, AND(XOR,XOR)) (SUM%2, SUM%2) (Linear, Linear) (Linear, AND)(Linear, XOR) (Linear, AND(XOR,XOR)) (Linear, SUM%2) (AND, Linear) (AND, AND)(AND, XOR) (AND, AND(XOR,XOR)) (AND, SUM%2) (XOR, Linear) (XOR, AND)(XOR, XOR) (XOR, AND(XOR,XOR)) (XOR, SUM%2) (AND(XOR,XOR), Linear) (AND(XOR,XOR), AND)(AND(XOR,XOR), XOR) (AND(XOR,XOR), AND(XOR,XOR)) (AND(XOR,XOR), SUM%2) (SUM%2, Linear) (SUM%2, AND)(SUM%2, XOR) (SUM%2, AND(XOR,XOR)) (SUM%2, SUM%2) Network A tasks Network B tasks 0.00 0.25 0.50 0.75 1.00"
}