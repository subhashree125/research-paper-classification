{
  "Abstract": "We consider the AdaDelta adaptive optimization algorithm on locally Lipschitz, positivelyhomogeneous, and o-minimally definable non-smooth neural networks, with either theexponential or the logistic loss. We prove that, after achieving perfect training accuracy,the resulting adaptive gradient flows converge in direction to a Karush-Kuhn-Tucker pointof the margin maximization problem, i.e. perform the same implicit regularization as theplain gradient flows. We also prove that the loss decreases to zero and the Euclidean normof the parameters increases to infinity at the same rates as for the plain gradient flows.Moreover, we consider generalizations of AdaDelta where the exponential decay coefficientsmay vary with time and the numerical stability terms may be different across the parameters,and we obtain the same results provided the former do not approach 1 too quickly and thelatter have isotropic quotients. Finally, we corroborate our theoretical results by numericalexperiments on convolutional networks with MNIST and CIFAR-10 datasets.",
  "Introduction": "Understanding when, why, and how training overparameterized neural networks by gradient-based algorithmsachieves good generalization (Zhang, Bengio, Hardt, Recht, and Vinyals, 2021; Belkin, Hsu, Ma, andMandal, 2019) remains one of the central questions in machine learning, despite several years of vibrantresearch. Much progress on the question has been made by investigating implicit regularization (or implicitbias) (Neyshabur, Bhojanapalli, McAllester, and Srebro, 2017): the mysterious preference of the trainingalgorithms for interpolators that perform well at test time. Buiding on the seminal work of Soudry, Hoffer, Nacson, Gunasekar, and Srebro (2018), one of the mostcelebrated results in the field was obtained by Lyu and Li (2020); Ji and Telgarsky (2020): that after achievingperfect training accuracy, gradient flow implicitly regularizes locally Lipschitz, positively homogenous, ando-minimally definable non-smooth networks so that their parameters converge in direction to a marginmaximization KKT point. This precise bias towards margin maximization for this wide class of networks hasbeen the basis of numerous theoretical works (cf. Vardi (2023)), as well as remarkable practical methods suchas the reconstruction of training data by Haim, Vardi, Yehudai, Shamir, and Irani (2022); Buzaglo, Haim,Yehudai, Vardi, Oz, Nikankin, and Irani (2023).",
  "Published in Transactions on Machine Learning Research (12/2024)": "the training loss shrinks faster when the numerical stability hyperparameters have random components(i.e. for AdaDeltaN and AdaDeltaNS), however remains within a constant multiplicative band of the trainingloss when those hyperparameters are isotropic; the results are not substantially affected by whether the exponential decay coefficients follow the increasingschedule, i.e. they are similar for AdaDelta and AdaDeltaS, and also for AdaDeltaN and AdaDeltaNS. VGG on CIFAR-10.Following Lyu & Li (2020), we finally considered the 14-layer VGG-16 (Simonyan andZisserman, 2015) with biases only at the first level, and in order consisting of: 64-channel convolutional thenReLU, repeated 2 times; max-pooling; 128-channel convolutional then ReLU, repeated 2 times; max-pooling;256-channel convolutional then ReLU, repeated 3 times; max-pooling; 512-channel convolutional then ReLU,repeated 3 times; max-pooling; 256-channel convolutional then ReLU, repeated 3 times; max-pooling; 10-widthfully connected. Each convolutional layer is 3 3-filter, and each max-pooling is 2-kernel 2-stride. We trained the network on CIFAR-10 (Krizhevsky, 2009) from the default PyTorch random initialization for1000 epochs using the cross-entropy loss: in a finer regime with batch size 100, and learning rate 0.1 for allfive algorithms; and in a coarser regime with batch size 250, and learning rate 0.25 for all five algorithms.The results are shown in fig. 3.",
  "we corroborate these theoretical results in three empirical settings, ranging from a simple visualization toa 14-layer convolutional network on CIFAR-10": "Further related work.The implicit regularization of adaptive optimization algorithms was the focus ofseveral other recent works. Wang, Meng, Zhang, Sun, Chen, Ma, and Liu (2022) proved that momentumdoes not affect the implicit bias towards margin maximization for linear classification. Tarzanagh, Li, Zhang,and Oymak (2023) showed margin maximization when optimizing attention by gradient descent. Cattaneo,Klusowski, and Shigida (2024) studied the implicit bias of Adam and RMSProp by backward error analysis.Zhang, Zou, and Cao (2024) established that Adam without a numerical stability term maximizes the -normmargin for linear logistic regression. Some of the prior empirical works on generalization of adaptive methodsare Keskar and Socher (2017); Zaheer, Reddi, Sachan, Kale, and Kumar (2018); Luo, Xiong, Liu, and Sun(2019); Chen, Zhou, Tang, Yang, Cao, and Gu (2020). Convergence properties of the popular Adam algorithm have been considered further in works that includethe following. Wang, Fu, Zhang, Zheng, and Chen (2023) proved a tight upper bound on Adams iterationcomplexity. Wang, Zhang, Zhang, Meng, Sun, Ma, Liu, Luo, and Chen (2024) studied convergence ofrandomly reshuffled Adam with diminishing learning rate and without assuming bounded smoothness, andshowed that it can be faster than stochastic gradient descent with diminishing learning rate. Thilak, Littwin,Zhai, Saremi, Paiss, and Susskind (2024) identified and investigated a slingshot phenomenon in late-stageAdam and related it to grokking (Power, Burda, Edwards, Babuschkin, and Misra, 2022). Xie and Li (2024)showed that Adam with decoupled weight decay implicitly performs constrained optimization. 1In an adaptive algorithm, for each model parameter, the learning rate is adapted by a separate coefficient. The adapterconsists of those coefficients, and is updated at every step of the algorithm.2Here the difference between RMSProp and Adam without momentum is that the latter involves a bias-correction coefficientin the computation of the exponentially decaying average.3",
  "Basic notation.We write: [n] for the set {1, . . . , n}; u, v for the inner product of vectors u and v;v =": "v, v for the Euclidean length of a vector v; vi for the ith component of a vector v; 0, 1, , etc.for the vectors whose dimension is inferred from the context and whose all components are equal to thespecified value, so that for all i we have 0i = 0, 1i = 1, i = , etc. Provided u and v are vectors of equal dimensions, we write uv, u/v, v2, v, etc. for the component-wiseproduct, quotient, square, square root, etc. operations, so that for all i we have (uv)i = uivi, (u/v)i = ui/vi,(v2)i = (vi)2, (v)i = vi, etc. Similarly, we write u < v, u v, etc. for the component-wise less, less thanor equal, etc. relations, so that we have u < v i: ui < vi, u v i: ui vi, etc. Local Lipschitz continuity and the Clarke subdifferential.Suppose a function f : Rk R is locallyLipschitz, i.e. every point v Rk has a neighborhood U such that f is Lipschitz continuous on U. ByRademachers theorem (cf. e.g. Borwein and Lewis (2010, Theorem 9.1.2)), then f is differentiable almosteverywhere. The Clarke subdifferential of f at a point v is the convex hull",
  "It is nonempty and compact for all v, and equals the singleton {f(v)} if f is continuously differentiableat v (Clarke, 1975). It consists of subgradients, which we may refer to simply as gradients": "O-minimal structures and definable functions.An o-minimal structure S is a family {Sk}k=1 suchthat: each Sk is a set of subsets of Rk; S1 is the set of all finite unions of open intervals and points; each Skcontains the zero sets of all polynomials on Rk; each Sk is closed under finite union, finite intersection, andcomplement; each Sk+k contains the Cartesian products of all sets in Sk and Sk; each Sk contains theprojections of all sets in Sk+1 onto the first k components. A function f : Rk Rk is definable in S if andonly if its graph is a set in Sk+k. For every o-minimal structure, the collection of all definable functions is closed under algebraic operations,composition, inverse, maximum, minimum, etc. (cf. e.g. Ji & Telgarsky (2020, Appendix B)). Moreover, byWilkies theorem (Wilkie, 1996), there exists an o-minimal structure in which the exponential function isdefinable.",
  "+ hk + gk+1L(wk)": "Predictor and loss functions.We assume the following properties of the predictor function (w, x) withparameters w Rp, inputs x Rd, and scalar outputs. The L-positive homogeneity means that scaling theparameters w by any > 0 scales the output by L, i.e. (w, x) = L(w, x). Assumption 1. For some L > 0 and some o-minimal structure S in which the exponential function isdefinable, for each x we have that (w, x) as a function of w is: (i) locally Lipschitz; (ii) L-positivelyhomogeneous; (iii) definable in S. This assumption admits neural networks that are constructed from a wide variety of layer types, includingfully connected, convolutional, ReLU, Leaky ReLU, Square ReLU, cube activation, and max-pooling, whichmay be composed arbitrarily. Points of nondifferentiability, such as at 0 in the case of the ReLU nonlinearity,are permitted because we assume only local Lipschitzness instead of continuous differentiability and we workwith the Clarke subdifferential instead of the gradient. However, due to the L-positive homogeneity (ii),skip connections are excluded, biases are excluded except at the first layer, and activations based on theexponential function (e.g. sigmoid activation) are excluded. The homogeneity exponent L is the degree towhich the parameters are multiplied when computing the network output this may be determined bystarting with exponent 0, then going forwards through the layers and e.g.: adding 1 to the exponent if thelayer is fully connected, not changing the exponent if the layer is ReLU, doubling the exponent if the layer isSquare ReLU, etc.",
  "ezfor the exponential loss;log(1 + ez)for the logistic loss": "Now we make use of the definability of the exponential function in the o-minimal structure S from Assumption 1.(The exponential function was effectively excluded by the homogeneity clause (ii) from the building of thepredictor function (w, x).) Since the individual loss function (in both cases, exponential and logistic) islocally Lipschitz and definable in S, the same is true of the total loss function L. Note however that the lossfunctions we consider are not homogeneous. Generalized AdaDelta flow trajectories.The starting point of our analysis is the adaptive gradientdescent of Procedure 1, which is a generalization of AdaDelta (Zeiler, 2012) by allowing: the learning rate tobe specified (this is already the case in the PyTorch implementation3), the exponential decay coefficients tovary with time (e.g. by following a specified schedule), and both those coefficients and the numerical stabilityterms to be specified differently across the vector components. The focus of our theoretical study is the adaptive gradient flow that corresponds to the adaptive gradientdescent with an infinitesimal learning rate. To arrive at its definition, we first restate the equations ofProcedure 1 as follows. We eliminate the auxiliary variable k+1, we suppose the hyperparameters k obey",
  "+ g(t + )L(w(t))": "Now, regarding these equations as determining the endpoints g(t + ), h(t + ), w(t + ) of the next linesegments in some continuous polygonal curves g, h, w: [0, ) Rp born from the adaptive gradient descentof Procedure 1 with learning rate (cf. e.g. Elkabetz and Cohen (2021)), letting tend to 0 we obtain thatthe limits of their directions are given by the right-hand sides of eqs. (1) to (3), which we take to be thederivatives that determine the adaptive gradient flow we are seeking. We therefore analyze trajectories g, h, w: [0, ) Rp of the two exponentially decaying averages and of theparameters, which are arcs (i.e. absolutely continuous on every compact subinterval) and which obey theadaptive gradient flow of Process 2 for almost all t 0.",
  "(1 (t))dt = . (ii) g(0) 0 and h(0) 0. (iii) There exists a time t0 suchthat L(w(t0)) < (0)": "This is a mild regularity assumption. Part (i) ensures that the exponential decay coefficients are not scheduledto approach 1 so fast that they stop the learning, and for example it is satisfied by any constant coefficientssmaller than 1. Part (ii) just requires that the exponentially decaying averages of squared gradients andsquared adapted gradients are initialized as nonnegative (in original AdaDelta they are initialized to zero).Part (iii) assumes that the dataset is separable by the network, moreover that for every parameters trajectory wthat we consider arising from the generalized AdaDelta flow, there exists a time t0 at which a separation(i.e. the perfect training accuracy) is achieved; this commonly occurs when training overparameterized neuralnetworks by gradient based algorithms (cf. e.g. Zhang et al. (2021)). We also remark that, for both theexponential and the logistic individual loss function (z), if f(z) is defined by (z) = ef(z), then f(z) ismonotonically increasing, and so the condition L(w(t0)) < (0) is equivalent to f 1(log(1/L(w(t0)))) > 0. Admittance of a chain rule.Since the total loss function L is locally Lipschitz and definable in S (forboth the exponential and the logistic individual loss functions), and the trajectory w of the parameters is anarc, we are able to use the following fact applied to them. Proposition 1 (Davis, Drusvyatskiy, Kakade, and Lee (2020, Theorem 5.8)). If f : Rk R is locallyLipschitz and definable in an o-minimal structure, then it admits a chain rule: for all arcs v: [0, ) Rk,almost all t 0, and all u f(v(t)), we have df(v(t))/dt = u, dv(t)/dt.",
  "Main result": "We prove that continuous generalized AdaDelta obeys the same tight rates for convergence of the loss andgrowth of the parameters as were established for plain gradient flow by Lyu & Li (2020, Corollary A.11), andalso implicitly regularizes the parameters to converge in direction to a KKT point of a variant of the margin",
  "4": "w2.If the quotient / of the numerical stability hyperparameters is isotropic (i.e. j/j are equal for all j [p]),then this modification does not alter the directions of the KKT points, so the implicit regularization isthe same as was established for plain gradient flow by Lyu & Li (2020, Theorem A.8) and Ji & Telgarsky",
  "w2subject to: y(i)(w, x(i)) 1 for all i [n]": "Remark 3. The growth of the Euclidean length w(t) of the parameters to infinity during the trainingprocess is an artifact of the L-positively homogeneous predictor function and the strictly decreasing individualloss function (both exponential and logistic) indeed, once perfect training accuracy is achieved, scaling theparameters w(t) by any > 1 decreases the total loss. Theorem 2 however establishes that this growth isslow, bounded above and below by the Lth root of log t. Note also that the convergence shown in Theorem 2to a KKT point of the skewed normalized margin maximization problem is directional, and so independent ofthe growth of w(t).",
  "Hence v(t) = w(t)/ = (t) L(w(t)) = (t) L(v(t)) for almost all t 0, as required. FromAssumption 2.(iii), we have L(v(t0)) = L(w(t0)) < (0), also as required": "Therefore, to establish Theorem 2, it remains to prove that limt (t) = 1 and that d log (t)/dt is Lebesgueintegrable. The remainder of this section consists of a sequence of lemmas, which culminates in Lemma 8that shows the former fact and Lemma 9 that shows the latter fact. The lemmas in particular make useof Assumption 2.(ii) on the initialization of the arcs of the exponentially decaying averages g(t) and h(t),and of Assumption 2.(i) on the unboundedness of the integrals of the complements of the exponential decaycoefficients, to show that each component of the skewed adapter (t) monotonically (either from below orfrom above, depending on g(0) and h(0), and on the numerical stability terms and ) tends to 1 as thetime t tends to infinity.",
  "Lemma 6. For all t 0 we have g(t) 0 and h(t) 0": "Proof. Suppose g(t)j < 0 for some t 0 and j [p]. By Assumption 2.(ii) and since gj : [0, ) R is an arc,there exists t0 < t such that g(t0)j = 0 and for all (t0, t] we have g()j < 0. Recalling that j : [0, ) , from eq. (1) we obtain that g()j 0 for almost all (t0, t]. Hence g(t)j = g(t0)j + tt0 g()j d 0,which is a contradiction.",
  "Supposing h(t)j < 0 for some t 0 and j [p], we obtain a contradiction analogously, using eq. (2)": "Consider an arbitrary jth component of the skewed adapter. By Assumption 2.(ii), its initial value (0)jis positive. The proof of the third lemma shows that there are three cases: if (0)j < 1 then (t)j ismonotonically nondecreasing and bounded above by 1, if (0)j = 1 then (t)j is constant, and if (0)j > 1then (t)j is monotonically nonincreasing and bounded below by 1.",
  "Proof. Consider an arbitrary j [p]": "For part (i), suppose t 0 is such that Lemma 5 applies. If 0 < (t)j < 1 then 1 (t)2j > 0, so fromLemmas 5 and 6, and by recalling that 0 (t)j 1, we have d(1 (t)2j)/dt 0, i.e. d(t)2j/dt 0. Butd(t)2j/dt = 2(t)j(t)j, and so (t)j 0. In the remaining two cases, namely if (t)j = 1 or (t)j > 1,the reasoning is analogous.",
  "AdaDelta: Its standard PyTorch implementation3, with the exponential decay coefficient = 0.9, and thenumerical stability term = 105": "AdaDeltaS: This is AdaDelta amended to have the exponential decay coefficients follow the schedulek = 1 0.1/(1 + 100k/K), where K is the total number of steps. Thus 1 k follows a harmonic sequence,increasing the coefficient from 0 = 0.9 at the first step to K1 = 0.999 at the last step, which lessensaggressiveness of the decay in computing the averages of the squared gradients and the squared adaptedgradients along the training. AdaDeltaN: This is AdaDelta amended to have the numerical stability terms different in the numerator andthe denominator of the adaptor, and different across the network parameters. At the start of the training,each component of and of is sampled independently from 105+X, where X is a centered Gaussian withstandard deviation 1 for the smaller two networks we consider, and with standard deviation 0.25 for thelargest network. AdaDeltaNS: This combines the extensions in AdaDeltaS and AdaDeltaN, i.e. has exponential decaycoefficients that follow the specified schedule as well as numerical stability terms whose components areinitialized randomly as above. We performed experiments in the following three gradually more complex settings. The total compute for theperceptron setting was around 10min on a mid-range CPU; whereas one run of all five algorithms for thesmaller convolutional setting took roughly 2h, and for the larger convolutional setting took roughly 12h, inboth cases on a mid-range GPU.",
  "equals v (w, x), where is the Leaky ReLU nonlinearity with inactive gradient 0.5": "The dataset(x(1), y(1)), . . . , (x(100), y(100))consists of 50 points sampled from (cos 0.5, sin 0.5) + u inde-pendently and labelled 1, and 50 points sampled from (cos 0.5, sin 0.5) + u independently and labelled 1,where u is distributed uniformly on the square [0.6, 0.6]2. This toy setting is convenient for three-dimensional visualizations of the adapters reciprocal square roots.We trained the network for K = 5000 full-batch epochs using the exponential loss, with learning rate 0.1 forSGD and learning rate 1 for the four variants of AdaDelta. The learning rate for SGD was chosen so that itachieves perfect training accuracy after a similar number of epochs as the four versions of AdaDelta. Werepeated the training 100 times, where in each round the five algorithms used the same randomly initializedparameters and numerical stability terms (if applicable). The results are shown in fig. 1.",
  "all rounds achieve 100% training accuracy by around 256 epochs, in line with the separation Assump-tion 2.(iii);": "the final normalized margin, which here equals mini y(i) vK (wK, x(i))/(v2K + wK2), is within ahigher and much narrower range for SGD and with isotropic numerical stability hyperparameters (i.e. forAdaDelta and AdaDeltaS), confirming the prediction of Theorem 2 that, without the isotropy, the implicitregularization may not be towards margin maximization;",
  "+gK": "+hK1 , has alarge variance of its direction when the numerical stability hyperparameters have random components (i.e. forAdaDeltaN and AdaDeltaNS), and it is the limit of this direction that according to the proof of Theorem 2determines the nature of the implicit regularization. Four-layer convolutional network on MNIST.Also following Wang et al. (2021), we then consideredthe network from Mdry, Makelov, Schmidt, Tsipras, and Vladu (2018) with biases removed, and in orderconsisting of: 32-channel 55-filter convolutional, ReLU, 2-kernel 2-stride max-pooling, 64-channel 33-filterconvolutional, ReLU, 2-kernel 2-stride max-pooling, 1024-width fully connected, ReLU, and 10-width fullyconnected. We trained the network on MNIST (LeCun, Bottou, Bengio, and Haffner, 1998) from the default PyTorchrandom initialization for 500 epochs using the cross-entropy loss: in a finer regime with batch size 100,learning rate 0.01 for SGD, and learning rate 0.1 for the four variants of AdaDelta; and in a coarser regimewith batch size 1000, learning rate 0.1 for SGD, and learning rate 1 for the four variants of AdaDelta. Thelearning rates for SGD were chosen so that it achieves perfect training accuracy after a similar number ofepochs as the four versions of AdaDelta. The results are shown in fig. 2.",
  "all algorithms achieve perfect training accuracy by around 250 epochs, in line with the separationAssumption 2.(iii);": "the normalized margin, whose values here are relatively small partly due to the division by the 14th power ofthe network norm, grows significantly higher for SGD and with isotropic numerical stability hyperparameters,with the exception in the coarser regime of AdaDelta whose training loss shrinks considerably slower comparedwith AdaDeltaS this is in line with Theorem 2, whose prediction of implicit regularization towards marginmaximization depends both on the isotropy and on convergence to zero training loss; after all algorithms achieve perfect training accuracy, their test accuracies remain roughly constant andwithout substantial differences among them (approximately within 81 1% in the finer regime, or 79 1% inthe coarser regime), with the exception in the coarser regime of AdaDelta and AdaDeltaN whose traininglosses shrink considerably slower compared with AdaDeltaS and AdaDeltaNS; whether the exponential decay coefficients follow the increasing schedule does not substantially affect theresults in the finer regime, however in the courser regime AdaDeltaS and AdaDeltaNS shrink the training lossconsiderably faster than AdaDelta and AdaDeltaN, leading to higher normalized margins as well as bettergeneralization.",
  "Conclusion": "Our main result, Theorem 2 which holds under Assumptions 1 and 2, indicates that AdaDelta may be used inpractical methods that rely on the optimization algorithm implicitly regularizing the network parameters toconverge in direction to a margin maximization KKT point, which places AdaDelta in the same category ase.g. RMSProp and Adam without momentum (Wang et al., 2021, Theorem 7). This relies on the isotropy ofthe quotient of the numerical stability hyperparameters, without which according to Theorem 2 the implicitregularization of AdaDelta may not be towards margin maximization, as is the case for e.g. AdaGrad (Wanget al., 2021, Theorem 6). Relaxing Assumption 1 on the predictor function is challenging even without considering adaptive algorithms,e.g. the definability excludes pathological examples such as based on the Mexican hat function (cf. Lyu &Li (2020, Appendix J)).",
  "SGDAdaDeltaAdaDeltaSAdaDeltaNAdaDeltaNS": ": The results of training a 14-layer convolutional network on CIFAR-10, with batch sizes and learningrates for the right-hand column that are 2.5 times larger than for the left-hand column. The solid lines showthe median values, and the shaded areas are between the 25th and 75th percentiles, over 19 runs."
}