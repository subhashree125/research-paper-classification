{
  "Abstract": "Kaplan et al. (2020) (Kaplan) and Hoffmann et al. (2022) (Chinchilla) studied the scalingbehavior of transformers trained on next-token language prediction.These studies pro-duced different estimates for how the number of parameters (N) and training tokens (D)should be set to achieve the lowest possible loss for a given compute budget (C). Kaplan:Noptimal C0.73, Chinchilla: Noptimal C0.50. This paper finds that much of this discrep-ancy can be attributed to Kaplan counting non-embedding rather than total parameters,combined with their analysis being performed at small scale.Simulating the Chinchillastudy under these conditions produces biased scaling coefficients close to Kaplans. Hence,this paper reaffirms Chinchillas scaling coefficients, by explaining the primary cause of Ka-plans original overestimation. As a second contribution, the paper explains differences inthe reported relationships between loss and compute. These findings lead us to recommendthat future scaling studies use total parameters and compute. 1",
  "Introduction": "Kaplan et al. (2020) (Kaplan) and Hoffmann et al. (2022) (Chinchilla) provided two influential studiesmeasuring the impact of scale in large language models (LLMs). Both informed large-scale efforts on how totrade off model parameters (N) and training tokens (D) for a given compute budget (C), but with conflictingadvice. Kaplans finding that Noptimal C0.73, Doptimal C0.27 led to the conclusion that big models maybe more important than big data, and LLMs trained in the ensuing years committed more resources to",
  "Published in Transactions on Machine Learning Research (11/2024)": "Scheme 4. The best learning rate is chosen per model. Six models trained per size at differentbudgets [0.25D, 0.5D, 0.75D, 1.0D, 1.5D, 2.0D], and cosine annealing applied. (Chinchilla studyused this.) Result 2. shows that optimization scheme has a smaller impact on scaling coefficients than switchingfrom NT to N\\E. Using a single set of models with no annealing (scheme 2) produces the same coefficientsas using the more computationally expensive scheme 4. Counter to Chinchillas comment that moving fromKaplans scheme 3 to scheme 4 would reduce the scaling coefficient, our experiment suggests the opposite isthe case, increasing from 0.46 to 0.49. This might explain our slight overestimation of the scaling coefficientsin Eq. 21 & 22.",
  "Concretely, this paper offers the following contributions": "We develop an analytical approach comparing the scaling relationships reported in Kaplan andChinchilla (). This approach finds that Kaplans reported relationship is locally consistentwith Chinchillas, if non-embedding parameters are used, and at smaller scale. Secondly, we study the reported relationships between compute and loss (). Again non-embedding parameters and smaller scale models are the cause of Kaplans biased estimate, combinedwith the absence of an offset term in their compute-loss form.",
  "Set Up": "Neural scaling laws.Kaplan et al. (2020) & Hoffmann et al. (2022) investigated empirically modelingthe relationships between the number of parameters (N), training tokens (D), training compute (C) andloss (L) in transformers for language modeling. The main functional form of relationship considered was apower law, y = axb, which is widely used throughout the sciences to describe a relationship between twoquantities (x & y) that holds over many orders of magnitude, e.g. (Kello et al., 2010). Definition of N & C. One difference between the two studies is the definition of N & C. Kaplan studiedrelationships in terms of non-embedding parameters (N\\E) and non-embedding compute (C\\E), excluding thecontributions of the embedding layers for the vocabulary and position indices (NE). By contrast, Chinchillastudied total parameters (NT ) and total compute (CT ). We define,",
  "NE = (h + v)d,(2)": "where d is the dimension of the transformer residual stream, v is vocab size, h is context length (only includedwhen positional embeddings are learned). Using the common approximation for training compute FLOPsC = 6ND (a factor of 6 covers a forward and backward pass), we define total and non-embedding compute,",
  "Analysis Overview": "Our analysis uses information and data from the Chinchilla and Kaplan studies to estimate the scaling lawsthat would emerge if the Chinchilla relationship had been expressed in terms of N\\E & C\\E, and this hadbeen done over the smaller model sizes used in Kaplan, as summarized in . We will see that for large NT , NE becomes a negligible portion of the models parameters and compute cost.Hence in the large parameter regime the two coefficients directly conflict with each other. At smaller valuesof NT , NE is not negligible (this is the regime considered in Kaplans study 768 to 1.5B parameters). Wefind that at the smaller end of this range, the relationship between N \\E & C\\E is not in fact a power law.However, fitting a local power law at this small scale, produces a coefficient that is close to Kaplans, andhence roughly reconciles these two results.",
  "NT = 12ld2 + NE,(12)": "where l is number of layers. Whilst Kaplan do not list their model configurations, they do study varyingaspect ratio A = d/l for a fixed size model. They find that models of a given size perform similarly over arange of aspect ratios, and this is not affected by model scale (their ). Hence, we could assume asizing scheme with fixed aspect ratio (A 40 appears sensible from their plots). Assuming this sizing allowsus to state (with l = d/A in Eq. 12),",
  "/3": ": Total parameter count vs. non-embedding parameter count for the suite of models sizes used inthe Chinchilla study, along with our fitted approximation. Note the curvature at model sizes below around200M parameters. Chinchilla perspective. We empirically fit a function NT = N\\E + N \\E (note the learnable exponent)to the Chinchilla model configurations listed in Table A9 of Hoffmann et al. (2022) for a range of NT (44Mto 16B). We compute NE from Eq. 2, using the reported vocab size of 32,000, but ignore the context length2,048 since Chinchilla used non-learnable position embeddings (though their inclusion effects coefficients onlyslightly). shows the configurations and relationship from a model fitted with numpys polyfit, which producescoefficients, = 47491 & = 0.34. The exponent has come out close to 1/3, and an implied aspect ratioA = 39.2 (inferred from ). Hence, this further supports the form in Eq. 11.",
  "= C\\E.(19)": "This shows that in general the relationship between N \\E & C\\E is not a power law. However, we can considera local power law approximation. That is, for some particular value of N\\E, there is some constant g givinga first order approximation (denoted by ) N \\E Cg\\E, where g is defined",
  "Step 4. Simulate synthetic data from the Chinchilla loss model over the model sizes used Kaplan. Fit a localpower law for N \\E in terms of C\\E": "By reading g off , we could estimate a local power law and hence scaling coefficient for a given valueof N \\E. However, its not clear what N \\E point value is representative of the Kaplan study. We opt fora more faithful estimation procedure, generating synthetic training curves from Eq. 18 across the range ofmodel sizes used in Kaplan, and fit coefficients using models falling on the compute efficient frontier. Thiswill also verify our analytic expression for N \\E & C\\E in Eq. 19. shows the synthetic training curves generated. We simulated 20 models with N\\E ranging from790 parameters to 1.58B (Kaplan reports using model sizes ranging in size from 768 to 1.5 billion non-embedding parameters). For other constants in Eq. 18, we adopt the Epoch AI specification (Eq. 10) and = 47491, though we report final results for the Chinchilla specification (Eq. 9) also.",
  "Experiment 1. Firstly, we verify whether scaling coefficients come out close to Chinchillas and Kaplanswhen using NT and N\\E respectively": "We trained five models of sizes, NT [0.8M, 1.6M, 2.1M, 3.3M, 4.6M] on the BookCorpus dataset. We usedthe GPT-2 tokenizer with vocab size of 50,257, and a context length of 16 (whilst much smaller than typical,our experiments suggest scaling coefficients are not affected by context length). Chinchillas Method 1 wasused to fit scaling coefficients, with the approximation C = 6ND. Models were trained for updates , batchsize was 65,536 tokens per update, fortotal training tokens D [262M, 262M, 262M, 524M, 524M]. The best learning rate for each model size waschosen [0.001, 0.005, 0.01, 0.05] and no annealing was applied.",
  "Chinchilla spec: LT E C0.155T(29)": "(See Section A.3 for Chinchillas compute coefficient.) Similar to the compute-parameter scaling coefficient,on the surface Kaplans coefficient of 0.057 appears quite far from Chinchillas of 0.1550.178. However,we will again show that by beginning from the Chinchilla study, and adjusting for Kaplans non-embeddingcompute, smaller scale, and additionally their compute-loss form, these two coefficients can be roughlyreconciled.",
  "which are roughly inline with Kaplans reported coefficient of L\\E C0.057\\E": "visualizes the fit of the Chinchilla compute-loss form vs. the Kaplan compute-loss form, when onecounts total compute vs. non-embedding compute. We see that Kaplans form provides a good fit of thedata in the non-embedding compute plot at small scale, over the range of model sizes they considered. Wespeculate that this might be the motivation for Kaplans selection of this simpler compute-loss form.",
  "Related work": "Following early works formalizing how language models improve with parameters, data, and training compute(Rosenfeld et al., 2019; Kaplan et al., 2020; Hoffmann et al., 2022), there has been investigation into whetherthese scaling laws arise in other domains (Henighan et al., 2020), and to explain their existence from atheoretical standpoint (Hutter, 2021; Maloney et al., 2022; Bahri et al., 2024). Closer in spirit to our paper are several concurrent works that have investigated the influence of variousdesign decisions on scaling law analyses.Su et al. (2024) revisit the methodology used to find scalingcoefficients.Hgele et al. (2024) found that multiple independent cosine schedules could be reproducedmore efficiently through a constant learning rate with multiple short decays, or stochastic weight averaging.Our finding is subtly different; a simple fixed learning rate will recover very similar compute-parameterscaling coefficients as multiple cosine schedules. Bi et al. (2024) study the effect of various hyperparameterson scaling laws. They observe that different text datasets produce slightly different optimal coefficients,with cleaner data leading to more parameter-hungry scaling behavior, which they speculate could partiallyexplain the difference between Kaplan and Chinchilla coefficients. Porian et al. (2024) provide a concurrent work with the same objective as our paper explaining the differ-ences between the Kaplan and Chinchilla coefficients. Through a set of large-scale experiments reproducingKaplans study, they determine that responsibility for the discrepancy can be attributed, in decreasing orderof significance, to; 1) Kaplan counting non-embedding rather than total compute. 2) Kaplan using a fixed-length warmup period that was too long for smaller models, making them appear less efficient. 3) Kaplan notfully tuning optimization hyperparameters. We see these findings as complimentary to our own. We havebeen able to identify the primary first-order reason using only information that was publicly available inthe two papers, with a fully analytical approach. (Tiny-scale experiments were run post-hoc as verification.)This illustrates the promise of applying mathematical approaches to the empirical science of scaling.",
  "Discussion": "This paper aimed to explain the difference between the Kaplan and Chinchilla scaling coefficients. We foundtwo issues in Kaplans study that combined to bias their estimated scaling coefficients; their choice to countonly non-embedding parameters, and studying smaller sized model sizes. This means there is curvature inthe true relationship between N\\E & NT (). At larger values of NT , the embedding parametercounts become negligible, NT = N\\E, and differences would not arise. Alternatively, had Kaplan studiedrelationships directly in terms of NT , this issue would also not arise, even at this smaller scale (confirmedby our Experiment 1 finding NT C0.49Teven for NT < 5M). The form Kaplan used to predict loss fromcompute further contributed to differences in the reported compute-loss scaling coefficients. Inconsistency across scaling studies. Existing literature on scaling is not consistent in its use of non-embedding vs. total compute. Some studies (Henighan et al., 2020; Gordon et al., 2021; Ghorbani et al.,2021; Fernandes et al., 2023; Hu et al., 2024; Bi et al., 2024; Su et al., 2024) follow Kaplans approach,using non-embedding parameters or compute, while others (Clark et al., 2022; Que et al., 2024; Wanget al., 2023) adhere to the Chinchilla approach, using total parameters. Our work indicates that this choicecan substantially alter scaling exponents, complicating cross-study comparisons. Similarly, the choice ofcompute-loss equation varies through the literature. Studies such as (Clark et al., 2022; Brown et al., 2020;Smith & Doe, 2024) opt for the Kaplan compute-loss form without offsets. In contrast, (Henighan et al.,2020; Gordon et al., 2021; Fernandes et al., 2023; Hu et al., 2024; Wang et al., 2023) employ the Chinchillacompute-loss form with non-zero offsets. Again, our work suggests that these methodological differences canlead to significant variations in scaling predictions and interpretations. The lack of a standardized approach in scaling studies risks making comparisons misleading and insights lessclear. We see our work as helping to understand certain decisions made in previous studies that should bestandardized. Concretely, we advise future studies to report total, rather than non-embedding, parameters,and to include an offset in the compute-loss fitting models. We discuss motivation for these choices below.Furthermore, our initial evidence does not support using multiple cosine decays per model size we find asingle fixed learning rate per model size is sufficient for measuring compute-optimal parameter coefficients. Why should embedding parameters contribute to scaling behavior? Several works evidence thatembedding parameters capture meaningful language properties. Word embeddings can be factorized intosemantically interpretable factors (even the shallow Word2vec) (Mikolov et al., 2013a;b; Arora et al., 2018).LLMs learn linear embeddings of space and time across scales (Gurnee & Tegmark, 2024). Developing suchmeaningful embedding structures allows LLMs to perform high-level language operations, such as arithmetic(McLeish et al., 2024). Therefore, if one believes that the embedding layer does more than just translatetokens to a vector of the correct dimension, we see no reason to exclude them in the parameter count. Why should a non-zero offset be used in loss-compute predictions? The Chinchilla compute-lossform with a non-zero offset (Eq. 27), is a more appropriate form from the perspective of statistical learning.This approach accounts for the concept of irreducible risk (Hernandez et al., 2023), which posits a lowerbound on achievable loss regardless of model or dataset size. This may arise from various factors: inherentbiases or limitations in the learning algorithm, or noise in the original task.As a concrete example inlanguage modeling, the best a model can do for the prediction of the first token in a sequence, is to estimatethe marginal distribution of all tokens, which leads to a non-zero loss. Limitations. We acknowledge several limitations of our analysis. We have aimed to capture the primaryfirst order reason for the difference between the Kaplan and Chinchilla scaling coefficients.But thereare multiple other differences between the two studies that likely also affect scaling coefficients (); datasets (Kaplan used OpenWebText2, Chinchilla used MassiveText), transformer details (Kaplan usedlearnable position embeddings while Chinchillas were fixed, also differing tokenizers, vocabularly sizes),optimization scheme (Kaplan used scheme 3, Chinchilla scheme 4, also differing warmup schedules), differ-ences in computation counting (Kaplan used C = 6ND, Chinchillas Method 1 & 2 used a full calculation).However, our work suggested these factors impact coefficients in a more minor way.",
  "Tamay Besiroglu, Ege Erdil, Matthew Barnett, and Josh You. Chinchilla scaling: A replication attempt.arXiv preprint arXiv:2404.10102, 2024": "Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong,Qiushi Du, Zhe Fu, et al. Deepseek llm: Scaling open-source language models with longtermism. arXivpreprint arXiv:2401.02954, 2024. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee-lakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, GretchenKrueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter,Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christo-pher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models arefew-shot learners. In Advances in Neural Information Processing Systems, volume 33, pp. 18771901.Curran Associates, Inc., 2020. Aidan Clark, Diego de Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann, BogdanDamoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al. Unified scaling laws for routed languagemodels. In International conference on machine learning, pp. 40574086. PMLR, 2022. Patrick Fernandes, Behrooz Ghorbani, Xavier Garcia, Markus Freitag, and Orhan Firat. Scaling laws formultilingual neural machine translation. In International Conference on Machine Learning, pp. 1005310071. PMLR, 2023. Behrooz Ghorbani, Orhan Firat, Markus Freitag, Ankur Bapna, Maxim Krikun, Xavier Garcia, CiprianChelba, and Colin Cherry. Scaling laws for neural machine translation. arXiv preprint arXiv:2109.07740,2021. Mitchell A Gordon, Kevin Duh, and Jared Kaplan. Data and parameter scaling laws for neural machinetranslation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,pp. 59155922, 2021.",
  "Wes Gurnee and Max Tegmark. Language models represent space and time, 2024": "Alexander Hgele, Elie Bakouch, Atli Kosson, Loubna Ben Allal, Leandro Von Werra, and Martin Jaggi. Scal-ing laws and compute-optimal training beyond fixed training durations. arXiv preprint arXiv:2405.18392,2024. Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun,Tom B Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive generative modeling.arXiv preprint arXiv:2010.14701, 2020. Danny Hernandez, Jared Kaplan, Tom Henighan, Sam McCandlish, Tom B. Brown, Andrew Carr, DieterichLawson, Jacob Steinhardt, Chris Olah, and Dario Amodei.Scaling laws for neural language models.Proceedings of the National Academy of Sciences, 120(26), 2023. doi: 10.1073/pnas.2311878121. URL Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimallarge language models. arXiv preprint arXiv:2203.15556, 2022.",
  "Marcus Hutter. Learning curve theory. arXiv preprint arXiv:2102.04074, 2021": "Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray,Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprintarXiv:2001.08361, 2020. Christopher T. Kello, Gordon D.A. Brown, Ramon Ferrer i Cancho, John G. Holden, Klaus Linkenkaer-Hansen, Theo Rhodes, and Guy C. Van Orden. Scaling laws in cognitive sciences. Trends in CognitiveSciences, 14(5):223232, 2010. ISSN 1364-6613. doi: URL",
  "Alexander Maloney, Daniel A Roberts, and James Sully. A solvable model of neural scaling laws. arXivpreprint arXiv:2210.16859, 2022": "Sean McLeish, Arpit Bansal, Alex Stein, Neel Jain, John Kirchenbauer, Brian R. Bartoldson, BhavyaKailkhura, Abhinav Bhatele, Jonas Geiping, Avi Schwarzschild, and Tom Goldstein. Transformers can doarithmetic with the right embeddings, 2024. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representationsof words and phrases and their compositionality.In C.J. Burges, L. Bottou, M. Welling, Z. Ghahra-mani, and K.Q. Weinberger (eds.), Advances in Neural Information Processing Systems, volume 26. Cur-ran Associates, Inc., 2013a. URL Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space word repre-sentations. In Lucy Vanderwende, Hal Daum III, and Katrin Kirchhoff (eds.), Proceedings of the 2013Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-guage Technologies, pp. 746751, Atlanta, Georgia, June 2013b. Association for Computational Linguistics.URL"
}