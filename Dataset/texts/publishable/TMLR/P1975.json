{
  "Abstract": "Given a finite set of sample points, meta-learning algorithms aim to learn an optimaladaptation strategy for new, unseen tasks. Often, this data can be ambiguous as it mightbelong to different tasks concurrently. This is particularly the case in meta-regression tasks.In such cases, the estimated adaptation strategy is subject to high variance due to thelimited amount of support data for each task, which often leads to sub-optimal generalizationperformance. In this work, we address the problem of variance reduction in gradient-basedmeta-learning and formalize the class of problems prone to this, a condition we refer toas task overlap. Specifically, we propose a novel approach that reduces the variance of thegradient estimate by weighing each support point individually by the variance of its posteriorover the parameters. To estimate the posterior, we utilize the Laplace approximation, whichallows us to express the variance in terms of the curvature of the loss landscape of ourmeta-learner. Experimental results demonstrate the effectiveness of the proposed methodand highlight the importance of variance reduction in meta-learning.",
  "Introduction": "Meta-learning, also known as learning-to-learn, is concerned with the development of intelligent agents capableof adapting to changing conditions in the environment. The central idea of meta-learning is to learn a priorover a distribution of similar learning tasks, enabling fast adaptation to novel tasks given only a limitednumber of data points. This approach has proven successful in many domains, such as few-shot learning (Snellet al., 2017), image completion (Garnelo et al., 2018b), and imitation learning tasks (Finn et al., 2017b). One instance of this is Gradient-Based Meta-Learning (GBML), which was introduced in Finn et al. (2017a).GBML methods employ a bi-level optimization procedure, where the learner first adapts its parameters",
  "Published in Transactions on Machine Learning Research (10/2024)": ": The space of task-parameters adapts to the Hessian. Sum of the logarithm of the lossfor 3 support points over different parameters for the sine experiment. Values increase from white to darkblue. The red cross and the red diamond indicate the prior and the posterior, orange points are the singletask-adapted parameters. Top row: Results for CAVIA. Bottom row: Results for LAVA, included is also thecovariance for each support point. A derivation can be found in appendix A.4. We propose to model the probability of the adapted parametersgiven each support point as a Gaussian distribution, N(i, i), by means of Laplaces approximation (Kasset al., 1991). In particular, we define the maximum a posteriori (MAP) estimate to be the GBML originalformulation i.e., i = 0 0L(0, DSi ). The variance of the adaptation can then be defined as the inverseof the Hessian of the loss function evaluated in the adapted parameters i.e., i = H1i. This approximationallows us to rewrite the estimate of the overall adapted parameters in eq. (7) as the product of these Gaussians.The most probable set of parameters given the provided support data will thus be the mean of the resultingGaussian distribution:",
  "fxi yi22 ,s.t. = A(DS ).(1)": "The performance is tightly bound to the error in the estimator A which in part arises from uncertainty inthe support data. In this work, we consider the uncertainty that is induced when data points can belong tomultiple tasks, which we denote as task overlap. For ease of notation, let f(x) = f(, x) denote the true data-generating process.Definition 1 (Task Overlap). We define task overlap as the condition for which x X the map f(, x) :T Y is non-injective.",
  "From a probabilistic perspective, this property is equivalent in stating that the conditional task-distributionp( | x, y) will have a non-zero covariance for all (x, y) X Y": "An example of task overlap can be seen by considering a sine wave regression problem. Let the distributionof tasks be sinusoidal waves with changing amplitude and phase i.e., y = A sin(x + ). In this case, thespace of tasks is defined by the union of possible amplitudes and phases and thus has dimension 2. A singlesample in the form of a tuple (x, y) is, however, insufficient to identify this task unambiguously. In fact,there exists an infinite number of viable amplitudes and phases for which the sine can pass through such apoint, fig. 1 on the left. In the terms above, there is no injective map between (x, y) and (A, ). The setof all possible solutions induced by this single point defines a distribution of solutions in the task-adaptedparameters space. All of these solutions are equally probable if conditioned on this single point. Whenmultiple (and different) samples are considered for the adaptation step, the inference of the sine wave can beexact. There exists a unique sine wave that passes through all of these points at the same time. In termsof task parameters, this can be seen as considering the intersection of the solutions induced by each point;see fig. 1 on the right. When using a single-point estimate, the information about this distribution of possibleoptimal task parameters is lost.",
  "Gradient-Based Meta-Learning": "A notable family of methods to solve the problem described in eq. (1) are GBML algorithms. In these,learning the adaptation process is formulated as a bi-level optimization procedure. A set of meta-parameters0 is learned such that the inference process A corresponds to a single gradient descent step on the losscomputed using the support data and 0:",
  "= 0 L(, DS )=0 ,(2)": "where is a scalar value denoting the learning rate. The resulting estimate of the task-adapted parametersis then optimized to approximate the underlying task function using eq. (1). This, effectively, defines anoverall optimization procedure on the meta-parameters 0. Gradient-based Meta-Learning does not requireadditional parameters for the adaption procedure, as it operates in the same parameter space as the learner.Moreover, it is proven to be a universal function approximator (Finn & Levine, 2018), making it one of themost common models for meta-learning. As noted in previous work (Grant et al., 2018), the bi-level formulation of GBML can be interpreted froma Bayesian view. We are interested, for each task, in finding a set of adapted parameters that maximize",
  "i=1Vari,(4)": "which in the case of task overlap is non-zero. In fig. 4 we depict the variance of this estimate for a simplesinusoidal regression task, together with a comparison to our proposed variance-reduced estimation describedin section 3. As can be seen, for standard GBML, the variance of the estimated parameters cannot get belowthe variance induced by the finite sampling of the support data. On the other hand, LAVAs estimation has adecreasing variance as training progresses. To address the problem of high variance, we leverage upon a general method for reducing the variance of asum of random variables (Hartung et al., 2011). Let 1, . . . , N be a set of random variables with the samemean and different covariance i for each i = 1, . . . , N. Let = Ni=1 Wii denote a weighted averagewith weights Wi Rdd. We wish to estimate W i that yield a with the minimum variance. We can definethis variance reduction problem as:",
  "Method": "In this section, we describe a novel method to model the estimation of the task-adapted parameters moreaccurately in the task overlap regime. Given a finite set of support points DS, we want to approximate theoptimal posterior of eq. (3). Let p(|xi, yi) denote the posterior w.r.t to one data point. Given the conditionsof task overlap, (definition 1), this induces a distribution in parameter space. Another way of interpretingthis posterior is that each data point provides evidence of what the possible true function could be. The fullposterior p(|DS) will thus lie at the intersection of all marginal posteriors. Given the i.i.d. assumption, thisimplies that:",
  "i=1Hii.(8)": "One interpretation of GBML methods such as MAML (Finn & Levine, 2018) is that the posterior estimateassumes an equal covariance across all marginal posteriors, which in turn implies that the MAP estimate equalsthe average of the parameters. In fact, in the case of no task overlap, eq. (8) becomes equivalent to eq. (2).This follows from the fact that we perform a single gradient step to approximate arg max p(|DS). Due tolinearity of the gradient operator, arg max p(|DS) corresponds to the average of {arg max p(|xi, yi)}Ni=1.However, in the case of anisotropic uncertainty in the adapted parameters, as in the task overlap regime, thisleads to a sub-optimal estimation. On the other hand, when the Laplace approximation faithfully describesthe underlying distribution, the estimate proposed in eq. (8) corresponds to the minimum variance estimator.This can be seen from proposition 1, where i = H1i. The final assumption to make the results coincide isthat the expectation over the adapted parameters is the same, i.e. Ep()[i] = for all i = 1, . . . , N. This",
  "Compared to simple parameter averaging defined in eq. (4), we achieve a lower variance for any support setDS p(). Thus, on expectation over all tasks, we achieve a variance-reduced estimate": "The overall training procedure follows from GBML methods. We optimize the set of parameters 0 throughthe objective of eq. (1) as a bi-level optimization procedure where is now defined according to eq. (8).Differently from GBML, the Laplace approximation allows for reshaping of the parameter space of the learnedmodel. Embedding this new adaptation process in the optimization allows for a precise approximation. Tominimize the loss, the model has to shape the parameter space such that, locally, the Laplace approximationcan exactly estimate the posterior. This imposes certain constraints on the structure of the parameter space",
  "ANILCAVIALAVA": ": Evaluation of Computation Time: We evaluate the performance of the different methods(measured in terms of MSE) as a function of the computational time (in seconds) across different supportsizes. The results show that, considering the same execution time (x-axis), LAVA outperforms both CAVIAand ANIL, with lower MSE. This result holds across all evaluated support sizes. arising from task overlap. This learned parameter space can be precisely shaped to allow for an accurateLaplace Approximation due to the flexible nature of parametric neural networks. Consider once again thesine wave regression example. Each support point gives evidence for a space of possible solutions. Whenconsidering a model with a parameter space of dimension 2, the space of solutions in the adapted parametersis a one-dimensional curve. The optimal posterior corresponds to an intersection of the curves induced byeach support point. In contrast to other GBML methods, LAVA allows for straightening out the region ofparameters that minimize the inner loss function. This in turn allows for shaping of the parameter space thatenables a precise Laplace Approximation. We illustrate this in fig. 2.",
  "Computational Implementation": "A limitation of the described method lies in an increased time complexity. Computing the Hessian on eachsingle support point can severely affect the training time of methods that already require complex second-ordercalculations like in GBML, especially for over-parameterized neural networks. To minimize the computationalburden, we consider performing adaptation in a subspace of the parameter space. In most of the experiments,we opt for the implementation of ANIL (Raghu et al., 2020), which performs adaptation over the last layer ofa neural network as they argue that most of the adaptation takes place in the final layers. This allows us tocompute the Hessian in closed form, drastically reducing the computational cost and keeping it independentof the dimensionality of the input space. Nevertheless, the Hessian computation can result in an increase in computational time and LAVA is, in fact,more expensive than the standard GBML model. However, this computational complexity increase is pairedwith stronger performances. LAVA provides a more effective adaptation technique as one of its main featuresis the efficient use of the limited information given by the support. In this regard, LAVA provides a bettertrade-off between performances and computational complexity. To better analyze this trade-off we comparedLAVAs performances against two GBML baselines in the Sine regression experiment by varying the numberof inner loop adaptation steps. As shown in fig. 3, LAVA has a computational complexity comparable toCAVIA (Zintgraf et al., 2019) with 2 inner loop gradient steps and ANIL with 3. In appendix A.8 we providea description of how to compute the Hessian when the loss is in the form of eq. (1). Computing second-order derivatives, however, is known to be numerically unstable for neural net-works, (Martens, 2016). We found that a simple regularization considerably stabilizes the training. FollowingWarton (2008), we take a weighted average between the computed Hessian and an identity matrix beforeaggregating the posteriors. For all of our experiments, we substitute each Hessian Hi in eq. (8) with thefollowing:",
  "Related Work": "Gradient-Based Meta-Learning methods were first introduced with MAML (Finn et al., 2017a) andthen expanded into many variants. Among these, Meta-SGD (Li et al., 2017) includes the learning rate as ameta parameter to modulate the adaptation process, Reptile (Nichol & Schulman, 2018) gets rid of the innergradient and approximates the gradient descent step to the first-order. In CAVIA (Zintgraf et al., 2019), theadaptation is performed over a set of conditioning parameters of the base learner rather than on the entireparameter space of the network. Other works instead make use of a meta-learned preconditioning matrix invarious forms to improve the expressivity of the inner optimization step (Lee & Choi, 2018; Park & Oliva,2019; Flennerhag et al., 2020). Bayesian Meta-Learning formulates meta-learning as learning a prior over model parameters. Most ofthe work in this direction is concerned with the approximation of the intractable integral resulting from themarginalization of the task parameters. This has been attempted using a second-order Laplace approximationof the distribution (Grant et al., 2018), variational methods (Nguyen et al., 2020), MCMC methods (Yoonet al., 2018), or Diffusion models (Pavasovic et al., 2023). While these Bayesian models can provide abetter trade-off between the posterior distribution of the task-adapted parameters and the likelihood ofthe data (Chen & Chen, 2022), they require approximating the full posterior and marginalizing over it.LLAMA (Grant et al., 2018) proposes to approximate the integral around the MAP estimate through theLaplace Approximation on integrals. Given one gradient step, the posterior estimate is still performed byaveraging. As we have shown, the averaging fails to account for inter-dependencies between the supportpoints arising from task overlap. Model-based Meta-Learning relies on using another model for learning the adaptation. One such approachis HyperNetwork (Ha et al., 2022), which learns a separate model to directly map the entire support data tothe task-adapted parameters of the base learner. In Gordon et al. (2019), this is implemented using amortizedinference, while in Kirchmeyer et al. (2022), the task-adapted parameters are context to the base learner.Alternatively, the HyperNetwork can be used to define a distribution of candidate functions using the few-shotadaptation data (Garnelo et al., 2018b;a) and additionally extend it using an attention module (Kim et al.,2019). Lastly, memory modules can be iteratively used to store information about similar seen tasks (Santoroet al., 2016) or to define a new optimization process for task-adapted parameters (Ravi & Larochelle, 2017).All of these methods can potentially solve the aggregation of information problem implicitly as the supportdata are processed concurrently. However, the learned model is not model-agnostic and introduces additionalparameters.",
  "Experiments": "To begin with, we test the validity of using the Laplace approximation to compute the task-adapted parametersfor a simple sine regression problem. Additionally, we show how LAVA exhibits a much lower variance in theposterior estimation in comparison to standard GBML. In appendix A.5, we demonstrate the unbiasedness ofGBML and evaluate the noise robustness of GBML and LAVA against a model-based approach. We further evaluate our proposed model on dynamical systems tasks of varying complexity in regard tothe family of functions and dimensionality of the task space as well as regression of two real-world datasets.We compare the results of our model against other GBML models. In particular, our baselines includeANIL (Raghu et al., 2020), CAVIA, (Zintgraf et al., 2019) as a context-based GBML method, LLAMA (Grantet al., 2018), VFML and VR-MAML (Wang et al., 2021; Yang & Kwok, 2022) as a variance-reduced meta-learning methods, and MetaMix (Chen et al., 2021) as a meta-data augmented method. Experimental detailsare given in the appendix A.1.",
  "Lava |Ds|=10": ": Estimators variance.Variance of the task-adapted parameters given the same task but differentsupport data points. Left: For CAVIA. Center: For LAVA. Right: Log variance of the distribution of theadapted parameters during training. can accurately capture distributions in parameter space. The other aspect we investigate is measuring thevariance reduction in our estimate. In the sine experiment, the dimensionality of the true task parametersis two, allowing us to visualize the learned parameter space. To this end, we train both LAVA and GBMLwith a conditional model (akin to CAVIA (Zintgraf et al., 2019)) on the sine-wave regression problem with acontext vector of dimension two. Given a single (x, y) tuple, there exists a one-dimensional space of sine waves that pass through that point.This makes the aggregation challenging and thus allows us to test the benefits of approximating this subspacewith the Laplace Approximation. Laplace Approximation AssumptionThe first ablation aims at testing the quality of the Laplaceapproximation in modeling the distribution of the task parameters given each single data point. After themodels have converged, we visualize the loss landscape induced by different support data points when usingdifferent task-adapted parameters. In particular, we sample a support dataset of three (x, y) tuples andcompute the logarithm of the mean squared error between the prediction of both models and the true ysampled from a grid of tasks. The idea is that each data points loss is minimized by a continuous space ofparameters. When summing up the losses of these support points, in fact, very well-defined valleys can benoticed in the loss landscape. We illustrate the distribution of this sum of losses for a grid of task-adaptedparameters in fig. 2 as a heat map for CAVIA (top row) and LAVA (bottom row). Additionally, the priorcontext (red cross), the single task adapted parameters (orange dots) and the aggregated final posterior (reddiamond) are also shown. For our method, we provide a visualization of the Hessian matrix for each singletask-adapted parameter. Variance EstimationFor the second ablation, we evaluate the variance of the posterior estimate forboth CAVIA and LAVA using the sine regression framework. The variance of the estimator describes thedifference between the task-adapted parameters from the sampling of different support sets from the sametask. For this experiment, we fix the sine task and sample 100 different support sets each with 10 (x, y)tuples. For each of these support points, we compute the resulting task-adapted parameters. We depict thespread of these distributions in fig. 4 (left and center) for CAVIA and LAVA respectively. Note that the scaleof the parameter space as well as the inner learning rate is the same for the two methods. The right of fig. 4illustrates the log variance of the task-adapted parameters during training for different support sizes. As canbe seen in the figure, at the beginning of training, the variance increases suddenly as the models learn to usethese parameters to solve the meta-learning problem. However, while CAVIAs context parameters varianceremains high during the rest of the training, our method learns to reduce it consistently.",
  "Cartpole: An inverse dynamics problem of an actuated cartpole with varying mass": "For each system, we consider samples from its vector field as our target data. We train all models for 300epochs and compare their MSE on reconstructing the vector field, we present the results in table 1. For CAVIAand ANIL, we perform a grid search over up to 8 inner optimization steps and select the best-performingone. For a single gradient step, LAVA outperforms both CAVIA and ANIL even with a large number ofadaptation steps. This increase in performance can be attributed to LAVAs use of second-order informationthrough the curvature of the loss landscape. Further details about the dataset construction and choice ofparameters are given in appendix A.1. To further evaluate the top-performing models, we visualize roll-out trajectories devised from integrating thelearned vector field in fig. 5. We can note that LAVA provides strong roll-out prediction in almost all cases.",
  "and synthetic Frequency-modulated radio signal subject to noise in the RadioML 2018.01A dataset (OSheaet al., 2018)": "Air Quality:We consider the Beijing Air Quality Dataset (Zhang et al., 2017) which is a time-series datasetcontaining recordings of air quality across 12 monitoring sites. The dataset contains hourly measurementsduring the time period from 01-03-2014 to 28-2-2017. We consider each monitoring site a separate task. Eachtask can be viewed as an interpolation task. A random contiguous subsequence of size ns + nq is selectedwhich is then split randomly into a support- and query-set. For each measurement, a time-variable t isappended, indicating a positional embedding in the time-series. The task is then predicting the air-qualitymeasurement from the given time t. We present the MSE in the left-most column of table 2. In this task,LAVA clearly outperform the baselines. RadioML:Lastly, we evaluate the performances of LAVA compared with the baselines on regressingfrequency-modulated radio signals. To do so, we make use of the RadioML 2018.01A dataset describedin OShea et al. (2018). These are both synthetic and real-world radio signals recorded over-the-air andsubject to noise. We devise the task as being able to regress each signal given a few data point recordings. Fortesting, we withhold 25% of the signals. We present the MSE in table 2 on the right-most column. Regressionof these signals poses a considerable challenge for meta-learning methods. The change in frequency betweensignals exacerbates the task overlap conditions of the task. The results highlight this, as LAVA is the onlymethod able to correctly regress the signals.",
  "Discussion and Conclusion": "In this paper, we characterized the problem of task overlap for under-determined inference frameworks. Wehave shown how this is a cause of high variance in the posterior parameters estimate for GBML models.In this regard, we have proposed LAVA, a novel method to address this issue that generalizes the originalformulation of the adaptation step in GBML. In particular, the task-adapted parameters are reformulatedas the average of the gradient step of each single support point weighted by the inverse of the Hessianof the negative log-likelihood. This formulation follows from the Laplace approximation of every singleposterior given by each support data point, resulting in the posterior being the mean of a product of Gaussiandistributions. Empirically we have shown how our proposed adaptation process suffers from a much lowervariance and overall increased performance for a number of experiments. For regression tasks, the assumption of task overlap is of particular interest. On the other hand, classificationtasks are inherently discrete and do not suffer from the problem of task overlap to the same extent asregression-like problems. Nonetheless, for completion, we provide in appendix A.10 the results of a few-shotclassification experiment on mini-Imagenet. In this experiment, LAVA performs similarly to the other GBMLbaselines, confirming the different nature of the overall problem. However, the discrete nature of classificationproblems presents an avenue for future work in the possibility of incorporating adaptation over a categoricaldistribution of parameters. A second limitation is the computational burden of computing the Hessian. Asdescribed in section 3.1, and further discussed in appendix A.8, we overcome this limitation by restricting theadapted parameters to the last layer only (akin to ANIL (Raghu et al., 2020)). This allows us to compute",
  "the Hessian in closed form. We show that the time complexity of our model is comparable to 2 inner steps ofCAVIA and 3 inner steps of ANIL while showcasing superior performance": "An interesting extension to the proposed method would be to explore techniques to approximate the Hessian.This would allow us to extend the adapted parameters to the whole model. Possible approximations couldinclude the Fisher information matrix or the Kronecker-factored approximate curvature (Martens & Grosse,2015) to estimate the covariance of the Laplace approximation. Alternatively, it might be interesting to explorethe direction of fully learning this covariance by following an approach similar to model-based methods. Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul, BrendanShillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient descent. Advancesin neural information processing systems, 29, 2016. Lisha Chen and Tianyi Chen. Is bayesian model-agnostic meta learning better than model-agnostic metalearning, provably? In International Conference on Artificial Intelligence and Statistics, pp. 17331774,2022. Yangbin Chen, Yun Ma, Tom Ko, Jianping Wang, and Qing Li. Metamix: Improved meta-learning withinterpolation-based consistency regularization. In 2020 25th International Conference on Pattern Recognition(ICPR), pp. 407414, 2021. Chelsea Finn and Sergey Levine. Meta-learning and universality: Deep representations and gradient descentcan approximate any learning algorithm. In International Conference on Learning Representations, 2018.",
  "Chelsea Finn, Tianhe Yu, Tianhao Zhang, Pieter Abbeel, and Sergey Levine. One-shot visual imitationlearning via meta-learning. In Conference on robot learning, pp. 357368, 2017b": "Sebastian Flennerhag, Andrei A Rusu, Razvan Pascanu, Francesco Visin, Hujun Yin, and Raia Hadsell.Meta-learning with warped gradient descent. In International Conference on Learning Representations,2020. Marta Garnelo, Dan Rosenbaum, Christopher Maddison, Tiago Ramalho, David Saxton, Murray Shanahan,Yee Whye Teh, Danilo Rezende, and SM Ali Eslami. Conditional neural processes. In Internationalconference on machine learning, pp. 17041713, 2018a.",
  "A.1Experimental Details": "For all of the experiments and all the baselines, we fix the architecture of the meta-learner f to be amulti-layer perceptron with 3 hidden layers of 64 hidden units together with ReLU activations. We use a metabatch size of 10 tasks and train all the models with Adam (Kingma & Ba, 2014) optimizer with a learningrate of 103. We use the inner learning rate = 0.1 for the adaptation step and MSE as the adaptation loss.All experiments were run for 5 different seeds to compute mean and standard deviations. For LLAMA we use = 106, for PLATIPUS we scale the KL loss by 0.1, for BMAML we use 10 particles and use MSE ratherthan the chaser loss for a fair comparison. Other experiment-specific details include:",
  "M(q)q + C(q, q) q + g(q) = Bu": "Here, q is a generalized coordinate vector, M is a matrix describing the mass, C is a force matrixand g(q) is the gravity vector. The system takes external force as input contained in u which getsmapped into general forces by the matrix B. The inverse dynamics task involves predicting thecontrol inputs u given a target trajectory {q(s)}. We generate trajectories of an actuated cartpolewith varying mass M and train on the inverse-dynamics task.",
  "A.1.2RadioML": "The RadioML 2018.01A dataset (OShea et al., 2018) consists of measurements of over-the-air radio commu-nications signals. The original dataset consists of signals with different modulation techniques and varyingsignal-to-noise (SNR) ratios. For the experiment presented in the paper, we restrict the dataset to signalswith frequency modulation and an SNR greater or equal to 20 dB. Each signal is composed of two channelsdescribing the In-phase and Quadrature components of the signal (I/Q). Here we consider the first 100time-steps of each of these signals as the curve to be regressed.",
  "Exp(x|) [L(, x)] = Exp(x|) [L(, x)] = L(, )": "Moreover, we measure empirically the bias of GBML and LAVA estimators. As a comparison, we include afully learned network implemented as a HyperNetwork (Ha et al., 2022) that takes as input the entire supportdataset and outputs the adapted parameters directly. Both the adaptation and the aggregation are learnedend-to-end together with the downstream task. We train these three models until convergence on the sine regression dataset. Then, we measure theirperformance on each task corrupted by Gaussian noise with a standard deviation of 3 on the support labels.The experiment is designed to test how the performance changes when increasing the support size. Weshow the difference in the loss between adaptation with and without noise for the three models and fordifferent support sizes in fig. 6. Thus, we are effectively testing the ability of these estimators to recover theperformances of the noiseless adaptation. Ideally, an unbiased estimator converges to the correct posteriorwith enough samples as long as the noise has zero mean. As can be seen in the figure, GBML methods aremuch more robust to these kinds of perturbations, while learned networks are not unbiased.",
  "A.7Geometry of the Parameter Space": "In order to minimize the loss described in eq. (1) using LAVAs adaptation (eq. (8)), we implicitly make anassumption over the geometry of the task space. In this section, we expand on this assumption and providean example of the limitations of using Gaussians to represent the distribution of solutions in parameter space.In fact, the model has to shape the parameter space such that, locally, the Laplace approximation can exactlyestimate the posterior. This imposes certain constraints on the structure of the parameter space arising fromtask overlap.",
  ": end while": "defines a cover of the task space T . As the Laplace approximation estimates the space with a normaldistribution, we require the underlying fiber Mx,y to be simply connected, or in other words, each element(x, y) induces a path-connected space of model parameterizations in which every path can be continuouslydeformed into another one. An example of a space of tasks with a non-trivial topology could be an annulus or a disk with a hole inside it.Such a space can arise in problems of goal-oriented navigation. Consider an agent in a 2-dimensional plane Pwhere the task provides the control inputs to reach a given goal position specified by a coordinate (x, y) P.Given trajectories provided by expert demonstrations, infer the goal-conditioned policy. In this setting, eachsupport point corresponds to a position of the agent, and the loss is defined as the L2 distance to the goal. Ifthe goal can be situated anywhere on the plane P, then a single support point (xs, ys) defines a space ofpossible goal positions as a circle around the current position. By continuity, this implies that the space ofpossible parameters Mxs,ys that yield the possible policies must be non-simply connected as well. This posesproblems for the Laplace approximation, as the Gaussian distribution implicitly holds an assumption on thetopological properties.",
  "A.9.1Condition Number": "We measure the condition number of the Hessian for LAVA under both the conditional setting and whenonly updating the final layer. We calculate the condition number (H) for H = Ni=1 Hi and H calculatedsimilarly with the approximate Hessians defined in eq. (9). We present the results across different supportsizes in table 3 below.",
  ": Condition and Approximate Condition Numbers for Support Size 10": "The condition number increases with the number of adaptation parameters increases. A high conditionnumber signifies that the matrix is close to being singular, meaning it is difficult to invert. In the contextof matrix inversion, this implies that small errors in the input data can lead to disproportionately largeerrors in the output, making the inversion process highly sensitive and potentially unreliable. In the table,we also present the condition number at convergence. This approximate condition number is derived fromthe approximate Hessian, as defined in eq. (9). Initially, the regularization promotes an even distributionof singular values, but as training progresses, the matrices become increasingly skewed. This behavior isexpected, as the Hessian for each support point Hi should be elongated in specific directions, resulting in arelatively high condition number.",
  "A.9.2Additional Sine Results": "Here we provide additional results for the sine regression experiment. Using the same experimental settingsdescribed in section 5.1 and appendix A.1, we present MSE and standard deviations for 5 seeds for LAVAand baselines in table 4. In particular, we provide results for support size equal to 1 as an ablation. The MSEvalue is noticeably higher as one support point is not sufficient to identify the underlying task accurately.Nevertheless, results show comparable performances between LAVA and ANIL when |DS| = 1 as eq. (8)becomes equivalent to the adaptation described in eq. (2). Qualitative results comparing the effect of differentsupport points and the addition of Gaussian noise on the support are shown in fig. 8. The figure shows howthe prior of the model (Green dotted line) evolves into the posterior for LAVA (orange curve) and ANIL(blue curve), this is compared with the ground truth curve (green dashed line). The effects of task overlap",
  ": Results Mini-Imagenet with support sizes 1 and 5": "We further experiment with classification on the Mini-Imagenet dataset (Vinyals et al., 2016). We use thetraining-set split as used in Ravi & Larochelle (2017) which leaves 64 classes for training, 16 for validationand 20 for test. We experiment with 5-way classification in either a 1-shot or 5-shot setting. We train themodels for 1000 epochs and perform model selection by choosing the one with the best performance on thevalidation set. We present results on the test set in table 5. Standard classification benchmarks such as Mini-Imagenet test the capability of the model to incorporatehigh-dimensional data in the form of images. Some of the methods are optimized toward image data andattempt to efficiently learn a well-structured representation space of images, such that the adaptation reducesto modifying decision boundaries. In particular, few-shot image classification problems in this form areinherently discrete problems that do not suffer as extensively from the task overlap assumption as outlinedin definition 1. In this context, the Gaussian assumption on the parameter distribution is not an accurateone, as each element of the support induces a multi-modal distribution in the parameter space. Thus variancereduction becomes inaccurate, and unbiased methods such as ANIL prove better. Nevertheless, LAVAmanages to get competitive performances on this benchmark as well."
}