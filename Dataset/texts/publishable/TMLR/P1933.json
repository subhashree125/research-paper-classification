{
  "Abstract": "The convergence of the conjugate gradient method for solving large-scale and sparse linearequation systems depends on the spectral properties of the system matrix, which can beimproved by preconditioning. In this paper, we develop a computationally efficient data-driven approach to accelerate the generation of effective preconditioners. We, therefore,replace the typically hand-engineered preconditioners by the output of graph neural networks.Our method generates an incomplete factorization of the matrix and is, therefore, referred toas neural incomplete factorization (NeuralIF). Optimizing the condition number of the linearsystem directly is computationally infeasible. Instead, we utilize a stochastic approximationof the Frobenius loss which only requires matrix-vector multiplications for efficient training.At the core of our method is a novel message-passing block, inspired by sparse matrixtheory, that aligns with the objective of finding a sparse factorization of the matrix. Weevaluate our proposed method on both synthetic problem instances and on problems arisingfrom the discretization of the Poisson equation on varying domains.Our experimentsshow that by using data-driven preconditioners within the conjugate gradient method weare able to speed up the convergence of the iterative procedure. The code is available at",
  "Introduction": "Solving large-scale systems of linear equations is a fundamental problem in computing science. Availablesolving techniques can be divided into direct and iterative methods. While direct methods, which rely oncomputing the inverse or factorizing the matrix, obtain accurate solutions they do not scale well for large-scaleproblems. Therefore, iterative methods, which repeatedly refine an initial guess to approach the exactsolution, are used in practice when an approximation of the solution is sufficient (Golub & Van Loan, 2013). The conjugate gradient method is a classical iterative method for solving equation systems of the form Ax = bwhere the matrix A is symmetric and positive definite. Further, the matrix is typically assumed to be sparsein the sense that it contains few non-zero elements (Carson et al., 2023). Problems of this form arise naturallyin the discretization of elliptical PDEs, such as the Poisson equation, and a wide range of optimizationproblems, such as quadratic programs (Pearson & Pestana, 2020; Potra & Wright, 2000). The convergencespeed of the method thereby depends on the spectral properties of the matrix A (Golub & Van Loan, 2013).Therefore, it is common practice to improve the spectral properties of the equation system by preconditioningbefore solving it. Despite the critical importance of preconditioners for practical performance, it has provennotoriously difficult to find good designs for a general class of problems (Saad, 2003).",
  "Published in Transactions on Machine Learning Research (09/2024)": "The average time to solve the problems from the small Poisson dataset is 0.45 seconds which is slightlyfaster than the preconditioners without any additional fill-ins but on par with the additional results shownin . For the larger problem instances, the direct method scales slightly better in our experimentsrequiring an average time of 2.7 seconds to solve the problem instances. In comparison, the best solver usingthe conjugate gradient method requires an average of 4.4 seconds (IC) and 4.5 seconds (NeuralIF) respectivelyover all problem instances. Thus, overall the difference between the two implementations is not huge butsparse direct solvers using GPU show a slightly better scaling in our experiments. Note, however, that the numerical comparison presented here between the different methods is in favor ofthe sparse direct solvers. While different preconditioner are evaluated using the same solver implementation,it makes sense to compare the timing between the different preconditioning techniques even though thesolver itself is not optimized for performance. Further, as described in Section D.1 additional techniques suchas reordering of rows and columns can be applied to obtain better preconditioners by applying additionalheuristics. These techniques are already integrated into the sparse direct solvers applied here. The fullimplementation of the sparse direct solver is optimized in a low-level language while our CG method isexecuted in Python, leading to slower solving times.",
  "Conjugate gradient method": "The conjugate gradient method (CG) is a well-established iterative method for solving symmetric andpositive-definite (abbreviated spd and denoted by S++n) systems of linear equations of the form Ax = b. Thealgorithm does not require any matrix-matrix multiplications, making CG particularly effective when dealingwith large-scale and sparse matrices (Saad, 2003). The method creates a sequence of search directions pifrom the corresponding Krylov subspace which are orthogonal with respect to the inner product induced bythe matrix A, i.e. pTi Apj = 0 for i = j. In other words, the search directions are mutually conjugate to eachother. These search directions are used to update the solution iterate xk. Since it is possible to compute theoptimal step size in closed form for each search direction, the method is guaranteed to converge within n steps(Shewchuk, 1994). Typically, the initial search direction p0 is chosen as the gradient of the correspondingquadratic program given by b Ax0 (Nazareth, 2009). In Algorithm 1 the preconditioned conjugate gradientmethod is shown. The original conjugate gradient algorithm can be recovered by setting P = I. ConvergenceThe convergence of CG to the true solution x depends on the spectral properties of thematrix A. Using the condition number (A), a linear worst-case bound of the error in the number of takenCG steps k is given by (Carson et al., 2023)",
  ".(1)": "However, in practice the CG method often converges significantly faster and the convergence also depends onthe distribution of eigenvalues and the initial residual. Clustered eigenvalues hereby lead to faster convergence.Further, a large condition number does not always imply a slow convergence of the iterative scheme. On theother hand, a small condition number leads to a fast convergence (Carson et al., 2023). A common approachto accelerate the convergence is to precondition the linear equation system to improve its spectral propertiesleading to faster convergence (Benzi, 2002). PreconditioningThe underlying idea of preconditioning is to compute a cheap approximation of the inverseof A that is used to improve the convergence properties. A common way to achieve this is to approximate thematrix P A with an easily invertible preconditioning matrix P , which allows us to compute the precondi-tioned search directions for each iteration in line 5 and line 10 of Algorithm 1 efficiently. This constraint can beachieved by constructing a (block) diagonal preconditioner or finding a (triangular) factorization (Benzi, 2002). Finding a good preconditioner requires a trade-off between the time required to compute the preconditionerand the resulting speed-up in convergence (Golub & Van Loan, 2013). The Jacobi preconditioner simplyapproximates A with a diagonal matrix. The incomplete Cholesky (IC) preconditioner is a more advanced,but widely adopted, method. As the name suggests, the idea is to approximate the Cholesky decompositionof the matrix. The IC(0) preconditioner restricts the non-zero elements in the obtained triangular factor Lto exactly the non-zero elements in the lower triangular part of A. Thus, no fill-ins during the factorizationare allowed. More general versions allow additional fill-ins of the matrix based on the position or thevalue of the matrix elements (Benzi, 2002) or allow flexible positions of non-zero elements such as as themodified incomplete Cholesky (MIC) preconditioner (Lin & Mor, 1999). The chosen amount of fill-in valuesdetermines how well the incomplete factorization approximates the original matrix and dictates how muchthe preconditioner accelerates the convergence of the iterative scheme. However, adding more fill-in elementsincreases the computational complexity of computing the incomplete factorization. Furthermore, it is desirableto know the amount of fill-in beforehand to avoid memory allocation problems (Scott & Tma, 2023). Finding new preconditioners is an active research area but is often done on a case-by-case basis. Newly devel-oped methods are often tailor-made for specific problem classes and often do not generalize to new problemdomains. General purpose preconditioners such as incomplete factorization methods and algebraic multigrid ap-proaches are, nevertheless, popular and sufficiently effective for many problems in practice. Therefore, improve-ments upon these methods are of great interest for a broad community (Benzi, 2002; Pearson & Pestana, 2020). Stopping criterionIn practice, due to numerical rounding errors, the residual is only approaching butnever reaching zero. Further, the true solution x is typically not available and therefore, equation (1) can",
  "Graph neural networks": "Graph neural networks (GNN) belong to an emerging family of neural network architectures well-suited tomany real-world problems with a natural graph structure (Velikovi, 2023). A (directed) graph G = (V, E) isa tuple consisting of a set of nodes V and directed edges E connecting two nodes in the graph E V V . Weassign every node v V a node feature vector xv Rp and respectively every directed edge eij, connectingnodes i and j, an edge feature vector zij Rm. The widely adopted message-passing GNNs consist of multiple layers updating the node and edge featurevectors of the graph iteratively using permutation-invariant aggregations over the neighborhoods and learnedupdate functions (Bronstein et al., 2021). Here, we follow the blueprint presented by Battaglia et al. (2018)to describe the update functions for a simple message-passing GNN layer. In each layer l of the networkthe edge features are updated first by the network computing the features of the next layer l + 1 as",
  "z(l)ij , x(l)i , x(l)j,(2)": "where is a parameterized function. The outputs of this function are also referred to as messages. Then,for each node i V the features from its neighboring edges, in other words, the incoming messages, areaggregated using a suitable aggregation function. Typical choices include sum, mean and max aggregations.Any such permutation-invariant aggregation function is denoted here by . The aggregation of incomingmessages over the neighborhood N of node i, which is defined as the set of adjacent nodes in the graphN(i) = {j | (i, j) E}, is computed as",
  "x(l)i , m(l+1)i.(4)": "The node and edge update functions and are typically parameterized using neural networks. Theequations (2)(4) describe the iterative scheme of message passing that is implemented by many popularGNNs. By choosing a permutation-invariant function in the neighborhood aggregation step (3) and sincethe update functions only act locally on the node and edge features, the learned function represented bythe GNN itself is permutation equivariant and can handle inputs of varying sizes (Battaglia et al., 2018). Many known algorithms share a common computational structure with this message-passing scheme makingGNNs a natural parameterization for learned variants. This correspondence is typically referred to asalgorithmic alignment in the literature (Dudzik & Velikovi, 2022).",
  "Method": "In this section, we formulate the learning problem for a data-driven preconditioner and derive an efficientloss function. Then, we introduce a problem-tailored GNN architecture, and demonstrate how to extend theframework to yield flexible sparsity patterns and analyze its complexity. Learning problemOur final goal is to learn a mapping f : S++n S++nthat takes an spd matrix A andpredicts a suitable preconditioner P that improves the spectral properties of the system and therefore alsothe convergence behavior of the conjugate gradient method. In order to ensure convergence, the output of the learned mapping needs to be spd and is in practice oftenrequired to be sparse due to resource constraints. To ensure these properties, we restrict the mapping",
  "s.t. (A)ij = 0 if Aij = 0, (A) L+n .(6b)": "Equation (6a) aims to minimize the distance between the learned factorization and the input matrix using theFrobenius norm distance. It is, however, also possible to use a different distance metric instead (Vemulapalli& Jacobs, 2015). When considering more advanced preconditioners, the no fill-in sparsity constraint (6b) canbe relaxed slightly allowing more non-zero elements in the preconditioner. However, it is desirable that therequired storage for the preconditioner is known beforehand and does not increase too much compared to theoriginal system (Benzi, 2002). Scalable trainingA practical problem with objective (6a) is that we cannot compute the expectationsince we lack access to the distribution of A.On the other hand, we have access to training dataA1, A2, . . . , An A, so we consider the empirical counterpart of equation (6) empirical risk minimization where the intractable expected value is replaced by the sample mean which we can optimize using stochasticgradient descent to obtain an approximation of .",
  "B2F = trace(BTB) wT BT Bw = Bw22,(7)": "where the elements in the vector w are iid normal distributed random variables.By setting B =(A)(A)T A we obtain an unbiased estimator of the loss which requires only matrix-vector products(Martinsson & Tropp, 2020). The resulting objective is similar to the loss proposed by Li et al. (2023) but doesnot rely on computing the true solution to the problem beforehand, making it computationally more efficientas it avoids solving linear equation systems before the training. In Appendix D.2, we show that using thisapproximation as the loss function leads to similar results as training using the full Frobenius norm as the loss. Model architectureDue to the strong connection of graph neural networks with matrices and numericallinear algebra (Moore et al., 2023), graph neural networks are a natural choice to parameterize the function .Furthermore, this allows us to enforce the sparsity constraints (6b) directly through the network architectureand avoids scalability issues arising in other network architectures since GNNs can exploit the sparse problemstructure through the architecture directly.",
  ".(8)": "This activation function forces the diagonal elements to be strictly positive. Multiplying the final edgeembedding by the constant term one-half before applying the non-linear activation avoids numerical problemsand, based on our experiments, improves the convergence during training.Due to the connection toincomplete factorization methods, we refer to our model as neural incomplete factorization (NeuralIF). Additional fill-ins and droppingsSimilar to the incomplete Cholesky method with additional fill-ins(Saad, 2003), processing on the graph can be applied to obtain both static and dynamic sparsity patterns forthe learned preconditioner. Static level of fill-ins can be obtained by adding additional edges to the graph priorto the message passing during the symbolic phase. This corresponds to relaxing constraint (6b) to allow morenon-zero elements. Adding all remaining edges to the graph is computationally intractable and has limited ap-plicability to large-scale problems. Therefore, we use heuristics inspired by the level-based fill-ins for incompletefactorization methods, for example, by allowing all entries corresponding to the sparsity pattern of A2. To obtain a dynamic sparsity pattern, we add a weighted 1-penalty on the elements in the learned precon-ditioning matrix (A)1 to the training objective (6a). This encourages the network to produce sparseoutputs (Jenatton et al., 2011). During inference, we drop elements with a small magnitude to obtain aneven sparser preconditioner. This can lead to faster overall solving times since the step to find the searchdirection in line 10 of Algorithm 1 scales with the number of non-zero elements in the preconditioner (Daviset al., 2016). Both of these approaches can also be combined to obtain a learned preconditioner similar to thedual threshold incomplete factorization (Saad, 1994). By including additional edges, the forward pass of the GNN becomes computationally more expensive. There-fore, it is important to avoid increasing the non-zero elements too much to maintain the computational efficiency.Dropping additional elements, on the other hand, can be achieved with nearly no overhead during inference. Inference and complexityWhen the trained model is used during inference to generate preconditioners,the output P(A) = (A)(A)T A is used within Algorithm 1 to find the preconditioned search directionrequiring only a single forward pass through the model. For the complexity analysis, we assume that the number of non-zero elements in the sparse matrix A is onthe order of the matrix size i.e. nnz(A) = O(n), which is a common assumption when working with sparsematrices (Scott & Tma, 2023). Further, the embedding size of the node and edge features in the hiddenlayers is constant and, therefore, omitted from the discussion below.",
  "Results": "The overall goal of preconditioning techniques is to reduce total computational time required to solve thelinear equation system up to a given precision (here, we choose 106) measured by the residual norm asdescribed in . The time used to compute the preconditioner beforehand, therefore, needs to be tradedoff with the achieved speed-up through the usage of the preconditioner. We compare the different methodsbased on both the time required to compute the preconditioner (P-time) and the time needed to solve thepreconditioned linear equation system using the CG method (CG-time) which is related to the number ofiterations required. However, the type and the sparsity of the obtained preconditioner also influences thetime-per-iteration. For instance in factorized preconditioners the sparse triangular solve method scales withthe number of non-zero elements (Davis et al., 2016) while diagonal preconditioners can be applied in a fullyparallelized fashion. The implementation details for the conjugate gradient method and different baselinesand methods are described in Appendix C. We consider two different datasets in our experiments. The first dataset consists of synthetically generatedproblems where we can easily control the size and sparsity of the generated problem instances. The otherclass is motivated by problems arising in scientific computing by discretizing the Poisson PDE on varyinggrids using the finite element method. The details for the dataset generation can be found in Appendix A. For numerical experiments we use a single NVIDIA-Titan Xp with 12 GB memory. For baseline preconditioners,which are not able to be accelerated directly using GPUs, we use 6 Intel Core i7-6850K @ 3.60 GHz processorsfor the computations. The (preconditioned) conjugate gradient method is always run on the CPU to ensure afair comparison between the performance of the preconditioners. Both of our models as well as the data-drivenNeuralPCG baseline are trained for a total of 50 epochs. However, convergence can usually be observedsignificantly earlier. For the synthetic dataset we use a batch size of 5, while for the problems arising from thePDE discretization we only use a batch size of 1 due to resource constraints. This leads to a total training timeof 40 minutes for the synthetic dataset and 55 minutes for the PDE dataset for each model. For all trainingschemes we utilize early stopping based on the validation set performance as an additional regularizationmeasure. However, training can be further accelerated by applying different validation strategies and stoppingtraining once convergence is observed. Further acceleration of our neural network-based preconditioner can be obtained by batching the probleminstances.This allows parallelization of the computation for the preconditioners leading to a smallerprecomputation overhead. However, to ensure a fair comparison in our experiments, we handle each problemindividually in our experiments.",
  "Synthetic problems": "The results and statistics about the preconditioned systems from the experiments using the synthetic datasetof size n = 10 000 with 1% non-zero elements are shown in . As a baseline, we include the standardCG implementation without any additional preconditioner. We can see that our learned preconditioner issignificantly faster to compute while maintaining similar performance in terms of reduction in number ofiterations as the incomplete Cholesky (IC) method without additional fill-ins. This makes our approach, interms of total solving time, on average 25% faster than incomplete Cholesky. Compared to the data-driven NeuralPCG (Li et al., 2023) method, our method is faster to compute duringinference which leads to a smaller P-time, since the number of parameters is significantly smaller. Further, weare able to reduce the number of required iterations to solve the problem using preconditioned CG significantlymore. In summary, our data-driven approach shows the ability of the learned preconditioner to accelerate thesolving procedure both compared to classical methods and other data-driven preconditioning techniques. Dynamic sparsity patternThe results for the learned preconditioner with additional sparsity, throughdropping elements by value as described in , are shown as NeuralIF-sp. Even though the output ofthe model contains only around half of the elements compared to the methods without fill-ins, the performancein terms of iterations does not suffer significantly. This can be exploited within the forward-backward solveswhich scale with the number of non-zero elements. Therefore, the overall time to solve the system is onaverage shortest using the sparsified NeuralIF preconditioner across all tested preconditioning methods. Solving timesThe distribution of total solving times is shown in , where each point represents asingle test problem instance that is solved with each respective method. The time shown includes both thetime required to compute the preconditioner (P-time) and the time to run the preconditioned CG method(CG-time). Overall, the variance of solving time between different problems is small for all considered methodswhich is due to the fact that the generated problems are very similar. The Jacobi and incomplete Choleskypreconditioner are effective for most problems. In comparison, the NeuralPCG method is not able to speedup the computational time compared to the other preconditioning baselines. We can see that both of ourNeuralIF preconditioners outperform the other preconditioning methods on nearly all problem instances butsimilar to other methods exhibit slightly worse performance on a few problem instances. Eigenvalue distributionIn , the ordered eigenvalues of the preconditioned linear equation systemfor one test problem are shown. Here, we can see that NeuralIF is especially able to reduce large eigenvaluescompared to all the other preconditioners. However, compared to incomplete Cholesky the smaller eigenvaluesof the NeuralIF preconditioned system decrease earlier and the smallest eigenvalue of the system is slightlysmaller. This results in a slightly worse convergence behavior in the limit of our data-driven method. NoneJacobiIC(0)NeuralPCGNeuralIF (ours) NeuralIF-sp (ours) 1.0 1.5 2.0 2.5 3.0 Total solving time in seconds",
  "PreconditionerCond. number Sparsity P-time CG-time (its.) Total time": "None60 834.17--2.79 (935.99)2.79Jacobi33 428.8699.99%0.0051.83 (689.82)1.84IC(0)4 707.1799.49%0.2471.16 (260.64)1.40NeuralPCG7 240.7299.49%0.1231.54 (318.86)1.66NeuralIF (ours)4 921.7699.49%0.0281.16 (267.08)1.19NeuralIF-sp (ours)5 581.4499.77%0.0211.01 (286.02)1.03 BreakdownOne inherent limitation of the incomplete Cholesky method is that it suffers from breakdownin 4% of the synthetic test problems due to the numerical instabilities of the method (Benzi, 2002). Theseinstances are not included in solving time in but are very costly in practice since it requires a restart ofthe solving procedure. In contrast, our data-driven approach consistently generates a suitable preconditioner.",
  "Poisson PDE problems": "In the second problem, we are focusing on problems arising from the discretization of Poisson PDEs usingthe finite element method. In order to train our learned preconditioner efficiently, we create a subset of smalltraining problems with matrices of size between 20 000 and 100 000 with up to 500 000 non-zero elements.The results for these problems are summarized in . Here, MIC is the modified incomplete Choleskymethod with the same number of non-zero elements as IC(0) but potentially different locations of the non-zeroelements which is determined dynamically (Lin & Mor, 1999). Allowing a more flexible sparsity pattern canlead to better results but additional pre-computation time is required to determine the position of non-zeroelements. Among all preconditioners without fill-ins, our data-driven method performs best as it is veryefficient to compute and reduces the required iterations vastly. Additional fill-insAmong the preconditioners with additional fill-ins, the MIC+ preconditioner furtherallows additional elements based on the number of non-zero elements in each row of the matrix which improvesthe preconditioner at the cost of a more expensive pre-computation time. Our NeuralIF(1) preconditioner isobtained by allowing non-zero elements in the non-zero locations of the matrix A2 which is a widely-adopted : Results for the small Poisson PDE problems from the training distribution using a subset of thecolumns shown in . The first set of method does not use fill-ins while in the second part additionalnon-zeros are added based on the position and value of the non-zero elements.",
  "(b) Solving time (CG-time)": ": Comparison of the computational time required for the incomplete Cholesky and NeuralIFpreconditioner with respect to the matrix size measure in number of non-zero elements on both the instancesfrom the training distribution and problem instances outside of the training domain. We are using 600problem instances from the generated Poisson PDE problems. The generated outputs have by constructionthe same number of non-zero elements as the input matrix.",
  "matrix with the time that is required to compute the preconditioner (P-time, a) as well as the timerequired to run the preconditioned conjugate gradient method (b)": "The results show that our model exhibits a very good size generalization as it is able to produce efficientpreconditioners even for matrices which are a magnitude of size bigger than the training instances. Further,we can see that the learned preconditioner scales better in terms of P-time for larger matrices than theincomplete Cholesky method and the variance for the preconditioner computation is decreased.",
  "Discussion & Limitations": "Here, we discuss several limitations and potential improvements with our proposed framework. However, weemphasize that many of these problems affect all learning-to-optimize methods and are not specific to ourlearned preconditioner. TrainingTo train data-driven preconditioners, we assume that there is a sufficiently large set of probleminstances from of a distribution A that share some similarity. In practice, not all problem domains give rise tosuch a distribution. Further, the time invested in the model training needs to be amortized over the speedupobtained during the inference phase of the learned preconditioner. However, this model training replaces theotherwise time-consuming manual tuning of preconditioners and is not a specific issue with our method but alimitation of the general learning-to-optimize framework (Chen et al., 2022). This also motivates followingour self-supervised learning approach since it allows us to train the model on unsolved problem instances.The trained model can then, in principle, be applied to accelerate solving the training instances making iteasier to amortize the model training. ConvergenceWhile the method works well in our numerical experiments and general convergence isensured, no guarantees about the convergence speed can be made and it is likely that for some problemclasses the usage of the learned preconditioner leads to an increase in overall solving time. That said, as seenin the experiments also classical preconditioners suffer from this problem and our method avoids pitfalls suchas breakdowns in incomplete Cholesky (Benzi, 2002). Future workA promising future research direction is to learn more flexible sparsity patterns used forpreconditioners alongside the values to fill in. This can be achieved by changing the graph structure usedfor message passing but requires additional care due to the combinatorial nature of the problem. Further,the incomplete factorization loss used to train our model is only a heuristic and not directly related to thecomplex convergence behavior of the conjugate gradient method. Extending the approach to directly takeinto account the downstream task instead of relying on the heuristic factorization approach has the potentialto further improve the learned preconditioner and lead to faster convergence (Bansal et al., 2023).",
  "Related work": "Data-driven optimization or learning-to-optimize (L2O) is an emerging field aiming to accelerate numericaloptimization methods by combining them with data-driven techniques (Amos, 2023; Chen et al., 2022).For example, a neural network can be trained to directly predict the solution to an optimization problem(Grementieri & Galeone, 2022) or replace some typically hand-crafted heuristic within a known optimizationalgorithm (Bengio et al., 2021). L2O using GNNsGraph neural networks have been recognized as a suitable computational backendfor problems in data-driven optimization and linear algebra (Moore et al., 2023). Chen et al. (2023) studythe expressiveness of GNNs with respect to their power to represent linear programs on a theoretical level.Using the Coates graph representation, Grementieri & Galeone (2022) develop a sparse linear solver for linearequation systems. They represent the linear equation system as the input to a graph neural network which istrained to approximate the solution to the equation system directly. However, no guarantees for the solution",
  "can be obtained. In contrast, our method benefits from the convergence properties of the preconditionedconjugate gradient method": "Following an AutoML approach, Tang et al. (2022) instead try to predict a good combination of solver andpreconditioner from a predefined list of techniques using supervised training. The NeuralIF preconditionerinstead focuses on further accelerating the existing CG method and does not need any explicit supervisionduring training. Sjlund & Bnkestad (2022) use the Knig graph representation instead to acceleratelow-rank matrix factorization algorithms by representing the matrix multiplication as a concatenation graphsimilar to our approach. However, their architecture utilizes a graph transformer while our approach worksin a fully sparse setting. This avoids the scalability issues of transformer architectures and makes it bettersuited for large-scale problems. Data-driven CGIncorporating deep learning into the conjugate gradient method has been utilized inseveral ways previously. Kaneda et al. (2023) suggest replacing the search direction in the conjugate gradientmethod with the output of a neural network. This approach can, however, not be integrated with existingsolutions for accelerating the CG method and does not allow further improvements through preconditioning.Furthermore, to ensure convergence all previous search directions need to be saved and the full GramSchmidtorthonormalization needs to be computed in every iteration making it prohibitively expensive. There have also been some earlier approaches to learning preconditioners for the conjugate gradient methodfollowing similar ideas as our proposed NeuralIF method. Ackmann et al. (2021) use a fully-connectedneural network to predict the preconditioner for climate modeling using a supervised loss. Sappl et al. (2019)use a convolutional neural network (CNN) to learn a preconditioner for applications in water engineeringby optimizing the condition number directly. However, both of these approaches are only able to handlesmall-scale problems and their architecture and training is limited due to their poor scalability comparedto our suggested approach. Utilizing CNN architectures, predicting sparsity patterns for specific typesof preconditioners (Gtz & Anzt, 2018) and general incomplete factorizations (Stanaityte, 2020) has alsobeen suggested previously. In comparison, our learned method is far more general and can be applied to asignificantly larger class of problems. Even though CNNs are widely adopted in the previous approaches,they are not well-suited to represent the underlying problem given by a matrix in contrast to GNNs. Morerecently, learned preconditioners have also been developed in the more general context of the GMRES solverfor non-symmetric and indefinite matrices using graph neural networks (Husner et al., 2024). PreconditioningIncomplete factorization methods are a large area of research. In most cases, the aim is toreduce computational time and memory requirements by ignoring elements based on either position or value.However, if done too aggressively, the method may break down due to numerical issues and require expensiverestarts. Perturbation and pivoting techniques can mitigate but not completely eliminate such problems (Scott& Tma, 2023). In contrast, our method does not suffer from breakdown as the positive-definiteness of theoutput is ensured a posteriori. Numerical efficiency can also be improved by reordering to reduce fill-in (for IC()with > 0) or by finding rows of the matrix that can be eliminated in parallel (Gonzaga de Oliveira et al., 2018). Chow & Patel (2015) formulate incomplete factorization as the problem of finding a factorization that isexact on the given sparsity pattern as a feasibility problem that can be solved approximately. In contrast, ourmethod only approximates the Frobenius norm minimization in equation (6) without enforcing constraints,which reduces pre-computation times. However, both approaches are quite similar in the sense that bothmethods aim to minimize the difference of the sparse factorization and the original matrix. While ourmethod is using a fully-amortized learned optimization approach to minimize the full Frobenius norm distance(Amos, 2023), Chow & Patel (2015) directly optimize the residuals of non-zero elements in the approximatefactorization. The latter approach also allows further acceleration using methods from distributed computing(Anzt et al., 2018) but typically requires highly optimized implementations to achieve competitive results. For problems arising from elliptical PDEs, multigrid preconditioning techniques have shown to be veryeffective. These type of methods utilize the underlying geometric structure of the problem to obtain apreconditioner (Pearson & Pestana, 2020). Multigrid methods have also been successfully combined withdeep learning techniques (Azulay & Treister, 2022). A potential drawback of this type of preconditioner is,",
  "Conclusions": "In this paper we introduce NeuralIF, a novel and computationally efficient data-driven preconditioner forthe conjugate gradient method to accelerate solving large-scale and sparse linear equation systems. Ourmethod is trained to predict the sparse Cholesky factor of the input matrix following the widespread idea ofincomplete factorization preconditioners (Benzi, 2002). To obtain a computationally efficient loss function, wederive a stochastic approximation of the Froebnius loss based on Hutchinsons trace estimator (Hutchinson,1989). This allows us to train our model using matrix-vector multiplications only which can be efficientlyimplemented for the large-sclae sparse matrices. We use the problem matrix A as an input to a graph neural network, which processes the input and producesthe desired approximate factorization. The network architecture aligns with the objective to minimize thedistance between the learned output and the input matrix A based on insights from graph theory using theFrobenius norm as a distance measure. Our experiments show that the proposed method is competitiveagainst general-purpose preconditioners both on synthetic and real-world problems and allows the creation ofboth dynamic and static sparsity patterns. Our work shows the large potential of data-driven techniquesin combination with insights from numerical optimization and the usefulness of graph neural networks as anatural computational backend for problems arising in computational linear algebra (Moore et al., 2023). We would like to thank Daniel Gedon, Fredrik K. Gustafsson, Sebastian Mair and Peter Munch for theirfeedback and discussions.This work was supported by the Wallenberg AI, Autonomous Systems andSoftware Program (WASP) funded by the Knut and Alice Wallenberg Foundation. The computations wereenabled by the Berzelius resource provided by the Knut and Alice Wallenberg Foundation at the NationalSupercomputer Centre. Jan Ackmann, Peter Dben, Tim Palmer, and Piotr Smolarkiewicz. Machine-Learned Preconditioners forLinear Solvers in Geophysical Fluid Flows. In EGU General Assembly Conference Abstracts, EGU GeneralAssembly Conference Abstracts, pp. EGU215507, April 2021.",
  "Jorge Nocedal and Stephen J Wright. Numerical optimization. Springer, 1999": "Nicolas Nytko, Ali Taghibakhshi, Tareq Uz Zaman, Scott MacLachlan, Luke N. Olson, and Matt West. Opti-mized sparse matrix operations for reverse mode automatic differentiation. arXiv preprint arXiv:2212.05159,2022. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deeplearning library. Advances in neural information processing systems, 32, 2019.",
  "The (preconditioned) conjugate gradient method is a standard iterative method for solving large-scale systemsof linear equations of the formAx = b(9)": "where A is symmetric and positive definite (spd) i.e. A = AT and xTAx > 0 for all x = 0. We also writeA S++nto indicate that A is a n n spd matrix (Golub & Van Loan, 2013). The method is especiallyefficient when the matrix A is large-scale and sparse since the required matrix-vector products shown inAlgorithm 1 can efficiently exploit the structure of the matrix in this case (Saad, 2003). In this section, we specify different problem settings leading to distributions A over matrices of the desiredform such that the conjugate gradient method is a natural choice for solving the problem. Given a distributionwe accessing via a number of samples, our goal is to train a model using the empirical risk minimizationobjective shown in the main paper to compute an incomplete factorization of a given input and utilize theresulting output as an effective preconditioner in the CG algorithm. In total, we are testing our method on two different datasets but vary the parameters used to generate thesedatasets. The first problem dataset considers synthetic problem instances. The other test problem arisesin scientific computing where large-scale spd linear equation systems can be obtained naturally from thediscretization of elliptical PDEs. The size of the matrices considered in the experiments is between n = 10 000 and n = 500 000 for all datasetsin use. Throughout the paper we assume that the problem at hand is (very) sparse and spd. Thus, thepreconditioned conjugate gradient method is a natural choice for all these problems. We ensure that thematrices are different by using unique and non-overlapping seeds for the problem generation. The datasetsare summarized in . For the graph representation used in the learned preconditioner, the matrix sizecorresponds to the number of nodes in the graph and the number of non-zero elements corresponds to thenumber of edges connecting the nodes. In the following, the details for the problem generation are explainedfor each of the datasets. : Summary of the datasets used with some additional statistics on size of the matrices and the numberof corresponding non-zero elements. Samples refer to number of generated problems in the train, validationand test set respectively.",
  "A.1Synthetic problem": "The set of random test matrices is constructed by choosing a sparsity parameter p which indicates the expectedpercentage of non-zero elements in the matrix. It is also possible to specify p itself as a distribution. We choosethe non-zero probability such that the resulting spd matrices which are generated with equation equation (10)have around 99% total sparsity (therefore the generated problems have 1 million non-zero elements). We thencreate the problem by sampling a matrix A with p(Aij = 0) = p and p(Aij|Aij = 0) N(0, 1). In order toensure the final matrix is spd we compute the test sample as",
  "u(x) = uD(x)x (11b)": "where f(x) is the source function and u = u(x) is the unknown function to be solved for. By discretizing thisproblem using the finite element method and writing it in matrix form a system of linear equations of theform Lx = b is obtained where the stiffness matrix L is sparse and spd. For large n, this problem thereforecan be efficiently solved using the conjugate gradient method given a good preconditioner. In order to obtain a distribution A over a large number of Poisson equation PDE problems, we consider a setof two-dimensional meshes that are generated by sampling from a normal distribution and creating a meshbased on the sampled points. We consider three different cases for the mesh generation: (a) we form theconvex hull of the sampled points and triangulate the resulting shape, (b) we sample two sets of points andtransform them into convex shapes as before. The second samples have a significantly smaller variance. Thefinal shape that is used for triangulation is then obtained as the difference between the two samples. Finally,(c) we generate a simple polytope from the sampled points. An example for each of the generated shapes isshown in . The sampled meshes are discretized using triangular basis elements with the finite-element method andrefined using the scikit-fem python library (Gustafsson & McBain, 2020). Due to the discretization based onthe provided mesh the size of the generated matrix is variable. Based on the stiffness matrices arising fromthe discretized problems the NeuralIF preconditioner is trained.",
  "BConnection to sparse approximate inverse preconditioners": "Instead of modeling the preconditioner as a sparse approximation of the factorization of A, as in the incompletefactorization setting, sparse approximate inverse preconditioners (SPAI) directly approximate the inverse ofA (Scott & Tma, 2023). In other words, the goal of SPAI preconditioners is finding a matrix M such thatM A1 subject to sparsity constraints. Similar to our objective function in equation (6a), the objective of",
  "minMS I MAF(12)": "where M is restricted to have a predetermined sparsity pattern S (Benzi & Tuma, 1999). The solution tothis problem can be computed efficiently but the output is neither guaranteed to be symmetric nor positivedefinite. Instead, the factorization of M can be obtained by reformulating the problem leading to the class offactorized sparse approximate preconditioners. In this class of methods, the distance between the factorizedpreconditioner and the unknown Cholesky factor of A1 is computed (Benzi et al., 1996; Scott & Tma, 2023).",
  "C.1Model architecture": "Here, we summarize in detail the architecture choices for our NeuralIF preconditioner and provide details onthe implementation and training of the method. In total, our model consists of 1 780 learnable parameterswhich is significantly less than previous approaches and allows for a fast inference time. Node and edge featuresIn total eight features are used for each node thus xi R8. The first fivenode features listed in are implemented following the local degree profile introduced by Cai & Wang(2019). The dominance and decay features shown in the table are originally introduced by Tang et al. (2022).Additionally, we use the position of the node in the matrix as the final input feature. As edge feature, only the scalar value of the non-zero matrix entries are used. In deeper layers of the network,when skip connections are introduced, the edges are augmented with the original matrix entries. Thiseffectively leads to having two edge features in these layers: the computed edge embedding of the currentlayer z(l)ij as well as the original matrix entry aij. However, from these only a one-dimensional output is",
  "computed z(l+1)ijas described in the following": "Message passing blockAt the core of our method, we introduce a novel message passing block whichaligns with the objective of our training. The underlying idea is that matrix multiplication can be representedin graph form by concatenating the two Coates graph representations as described previously (Doob, 1984).The pseudocode for the message passing scheme is shown in Algorithm 2. The message passing block consistsof two GNN layers as introduced in which are concatenated to mimic the matrix multiplication. To",
  ": Input: Graph representation of the spd system of linear equations Ax = b.2: Output: Lower-triangular sparse preconditioner for the linear system which is an incomplete factorization": "3: NeuralIF preconditioner computation:4: Compute node features xi shown in : Apply graph normalization6: Split graph adjacency matrix into index set for the lower and upper triangular parts, L and U.7: for each message passing block l in 0, 1, . . . , N 1 do8: update using the lower-triangular matrix part",
  ": Return lower triangular matrix with elements z(N)ijfor i j": "align the block with our objective, we only consider the edges in the lower triangular part of the matrix inthe first layer (see lines 811 in Algorithm 2) while in the second layer of the block only edges from the uppertriangular part of the matrix are used (lines 1417). However, the same edge embedding for the two steps areused (line 13). In the final step of the message passing block, we introduce skip connections and concatenatethe edge features in the current layer with the original matrix entries (lines 1823). Note that, since thematrix A is symmetric it holds that (i, j) L (j, i) U where U and L are the index set corresponding tothe non-zero elements in the upper and lower triangular matrix.",
  "Here, we denote the updates after the first message passing layer with the superscript (l + 1": "2) to explicitlyindicate that it is an intermediate update step. The final output from the block is then computed based onanother message passing step which utilizes the computed intermediate embedding. Further, NL(i) denotesthe neighborhood of node i with respect to the edges in the lower triangular part L and NU(i) the ones fromthe upper triangular part U respectively which are utilized for the message passing. There are always at leastthe diagonal elements in the neighborhood of each node and therefore, the message computation is well defined. Network designBoth the parameterized edge update and the node update functions in every layerare implemented using two layer fully-connected neural networks. The inputs to the edge update network are formed by the node features of the corresponding edge and the edge features themselves leading to atotal of 8 + 8 + 1 = 17 inputs in the first message passing step and one additional input in later steps. Theedge network outputs a scalar value. For each network, 8 hidden units are used and the tanh activationfunction is applied as a non-linearity. To compute the aggregation over the neighborhood , the mean andsum aggregation functions are used respectively in the first and second step of the message passing block which is applied component-wise to the set of neighborhood edge feature vectors. The node update function takes the aggregation of the incoming edge features as well as the current node feature as an input (1 + 8 = 9).",
  "C.2Baseline implementation": "Conjugate gradient methodThe conjugate gradient method is implemented both in the preconditionedform (as shown in Algorithm 1) and, as a baseline, without preconditioner using PyTorch only relying onmatrix-vector products which can be computed efficiently in sparse format (Paszke et al., 2019). In order toensure an efficient utilization of the computed preconditioners, the forward-backward substitution to solvefor the search direction is implemented using the triangular solve method provided in the numml packagefor sparse linear algebra (Nytko et al., 2022). The conjugate gradient method is run until the normalizedresidual reaches a threshold of 106, see for details. Baseline preconditionersThe Jacobi preconditioner is due to its computationally simplicity, directlyimplemented in PyTorch and scipy using a fully vectorized implementation. It can be seen in the numericalexperiments that the overhead for computing this method is very small compared to the more advancedpreconditioners and does not effect the overall runtime significantly. The incomplete Cholesky preconditioners and its modified variants with dynamic sparsity patterns andadditional fill-ins are implemented using the highly efficient C++ implementation of ILU++ with providedbindings to python (Mayer, 2007; Hofreither, 2020). Due to the sequential nature of the computation, it is ofcritical importance that the utilized implementation is efficient. The data-driven NeuralPCG baseline which uses a simple encoder-decoder GNN architecture (Li et al., 2023)is implemented in PyTorch and PyTorch Geometric following a very similar approach to NeuralIF. The hyper-parameters for the model are chosen as specified in the paper: using 16 hidden units with a single hidden layerfor encoder, decoder and GNN update functions and a total of 5 message passing steps. This results in a total of10 177 parameters in the model which means the parameter count is over 5 times higher than our proposed ar-chitecture which makes inference using the NeuralPCG model more expensive as we observe in the experiments.Due to computational limits, the batch size is reduced during training compared to the original paper. The results obtained from the model are similar to the one presented by Li et al. (2023). However, in ourexperiments we observed a higher pre-computation time which can be explained by the fact that differenthardware acceleration is used. Further, in our experiments different sparsity patterns with more non-zeroelements and larger matrices are used showing the poor computational scaling of the NeuralPCG modelfor large problem instances. In terms of resulting number of conjugate gradient iterations, our experimentsmatch the results from the paper. ComputeTo avoid overhead due to the initialization of the CUDA environment, we run a model warm-upwith a dummy input, before running inference tests with our model. The (preconditioned) conjugate gradientmethod including the sparse forward-backward triangular solves for the utilized preconditioner is run ona single-thread CPU. The learned preconditioner is computed using a single NVIDIA-Titan Xp GPU.",
  "DAdditional Results": "Here, we present additional results and extensions of the baseline NeuralIF preconditioner presented in thispaper. We both describe how to include reordering into the learned preconditioner as well as show morein-depth results. In Section D.2, we conduct an ablation study, probing the different loss functions proposedto train the learned preconditioner. Finally, we show the convergence behavior of the PCG algorithm for thedifferent preconditioners and compare the performance with sparse direct methods.",
  "D.1Matrix reordering": "It is possible to directly combine our learned preconditioner with existing reordering methods. Classicalreordering schemes such as COLMAD are executed in the symbolic phase before obtaining the values forthe non-zero elements in the preconditioner (Gonzaga de Oliveira et al., 2018). Since our message passingscheme implicitly takes into account the ordering of the nodes, the nodes can simply be relabeled prior torunning the forward pass of the neural network which influences the split into lower and upper triangularmatrices that are used for the message passing. This ordering could also be learned alongside the values tofill in which is an interesting area for future research.",
  "D.2Comparison of loss functions": "In this section, we compare the different loss functions proposed to train the learned preconditioner. Computingthe condition number as proposed in Sappl et al. (2019) as a loss function, is computationally infeasible inour approach since the underlying problems are large-scale compared to the problems encountered in theprevious work. For the matrices used in our experiments, computing the condition number in the forwardprocess is often computationally very expensive and therefore, the approach is omitted. We compare insteadthe full Frobenius norm as a loss as shown in objective (6a) and the stochastic approximation we suggestusing Hutchinsons trace estimator as shown in equation (7). Our loss is self-supervised and thus it is sufficient to have access to the problem data Ai, bi but not thesolution vector xi. This saves significant time in the dataset generation. Further, it allows us to train apreconditioner on an existing dataset without solving all problems beforehand or even applying the learnedpreconditioner on the training data. The full Frobenius loss is also self-supervised but requires a matrix-matrixmultiplication which leads to a large memory consumption and expensive forward and backward computationleading to longer training times. Further, matrix-vector multiplications are significantly easier to optimizeand build a cornerstone of the CG algorithm allowing for example matrix-free implementations. The stochastic and full Frobenius loss functions are compared in . We are training all models for20 epochs and use a batch size of 1 on the synthetic dataset. Thus, the number of parameter updates inthe training for all models is the same.Using the full Frobenius norm as a loss function is significantlymore computationally expensive compared to the stochastic approximation and, further, requires significantlymore memory which means we can not apply it to large-scale problems. Training a model for 20 epochs withthe full loss takes roughly 5 hours, while using the stochastic approximation or supervised loss, training isfinished in around 30 minutes without decreasing the obtained preconditioner performance. Further, the : Comparison of NeuralIF preconditioner performance on the synthetic problem set trained usingdifferent loss functions. The validation loss for both training loss functions is the Frobenius norm distancegiven in equation (6a). Time-per-epoch lists the training time for one epoch of training data (consisting of1 000 samples), using a batch size of 1, in seconds.",
  "reduced memory consumption allows us to increase the batch size in other experiments leading to an evenlarger speedup in training time and additional variance reduction during the training process": "Even though we use a stochastic approximation to train our model, the validation loss computed as the fullobjective does not increase significantly. Both the training loss and the validation loss for the different lossfunctions are compared in . Even though the variance for the stochastic approximation is significantlyhigher as expected, the validation performance of the two loss functions is nearly identical. One explanationfor this is that the noise in the stochastic gradient descent method is already very large influencing theoptimization procedure. Further, the added noise might help prevent overfitting on the training samplesleading to a better performance of the stochastic loss in terms of validation loss.",
  "D.3PCG convergence": "Here, additional results for the synthetic dataset are shown. Notably, shows the convergence ofthe different (preconditioned) CG runs on a single problem instance from the synthetic dataset where boththe residual which is also used as a stopping criterion and the usually unavailable distance between thetrue solution and the iterate are shown. Further, for each problem the worst-case bound depending on thecondition number (A) from equation (1) is displayed. We can see that the distance to the true solution isbounded by the theoretical (A)-bound and monotonically decreasing. The residual itself does not need tobe decreasing and often increases in the first few iterations in our experiments. The distribution of the eigenvalues of the linear equation system shown in in the main paperfor this problem instance directly influence the convergence behavior of the method. However, as it iscommonly observed in the CG method, the (A)-bound only describes the convergence behavior locallyand overall, the algorithm converges significantly faster than the worst-case bound (Carson et al., 2023).We can, however, observe that the better conditioning still improves the convergence significantly and theobtained speedup is proportional to the speedup obtained in the worse-case bound making the conditionnumber at least for the problem instances considered in this dataset a good measure for the performanceof the conjugate gradient method. Further, we can see that our learned NeuralIF preconditioner is especiallyeffective in the beginning of the CG algorithm and is able to outperform IC(0) for the first 100 iterationsshowing a significantly different convergence behaviour in this region.",
  "D.4Comparison with sparse direct solvers": "Here, we compare the performance of the (preconditioned) conjugate gradient method with applying sparsedirect solvers to the problem instances (Scott & Tma, 2023). We are using the sparse Cholesky decompositionwith GPU support of the CHOLMOD routine available from the SuiteSparse package with the Pythoninterface provided by cholespy. We apply the supernodal factorization strategy and use nested dissectionreordering to the matrix (Nicolet et al., 2021; Chen et al., 2008). Synthetic problemsFor the synthetic dataset the average time to solve each problem using the directsolver is 91 seconds which is significantly longer than the conjugate gradient based solvers presented in. This is due to the fact that no structured sparsity in the problems is present that can be efficientlyexploited during the symbolic-solving phase due to the construction of the problems. Therefore, usingiterative methods is far superior in this case even though the performance of the CG method is not tuned. Poisson problemsIn contrast, problems arising from the discretization using the finite-element methodare well known to be efficiently solvable using sparse direct methods due to their inherent sparsity structure.However, by taking domain knowledge into account it is possible to accelerate the CG solver further usingthe popular multigrid methods which constructs the preconditioner based on the properties of the problemdomain as previously discussed."
}