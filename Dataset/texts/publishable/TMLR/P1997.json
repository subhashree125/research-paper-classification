{
  "Abstract": "In-context learning allows adapting a model to new tasks given a task description at test time.In this paper, we present IMProv - a generative model that is able to in-context learn visualtasks from multimodal prompts. Given a textual description of a visual task (e.g. Left:input image, Right: foreground segmentation), a few input-output visual examples, or both,the model in-context learns to solve it for a new test input. We train a masked generativetransformer on a new dataset of figures from computer vision papers and their associatedcaptions, together with a captioned large-scale image-text dataset. During inference time, weprompt the model with text and/or image task example(s) and have the model inpaint thecorresponding output. We show that training our model with text conditioning and scalingthe dataset size improves in-context learning for computer vision tasks by over +10% AP forForeground Segmentation, over +5% gains in AP for Single Object Detection, and almost20% lower LPIPS in Colorization. Our emperical results suggest that vision and languageprompts are complementary and it is advantageous to use both to achieve better in-contextlearning performance.",
  "Introduction": "In-context learning (ICL) (Brown et al., 2020; Chan et al., 2022; Xie et al., 2021), also known as few-shotprompting, is an exciting new paradigm in machine learning that allows a model to adapt to novel downstreamtasks without fine-tuning or changing the models weights. In natural language processing (NLP), ICLis considered an emergent property of large language models (Brown et al., 2020; Touvron et al., 2023;Chowdhery et al., 2022) and it was first introduced in the seminal paper of GPT-3 (Brown et al., 2020). Afew-shot prompt typically includes examples of (input, output) pair(s). The few-shot performance of large",
  "language models has been shown to achieve competitive results on NLP tasks, sometimes surpassing priorstate-of-the-art fine-tuning approaches (Brown et al., 2020)": "In computer vision, the full potential of in-context learning (ICL) is still far from being realized. To enable amodel to perform in-context learning during test time, there are two key challenges that need to be addressed.Firstly, the models architecture should be designed in such a way that it can effectively process promptsfrom various vision tasks. This means it should be capable of receiving task instructions and/or input-outputexamples as inputs to the model. Secondly, a different approach to training these models is required. Whilein natural language processing (NLP), the emergence of ICL has been facilitated by utilizing large-scale,non-curated data, in computer vision, even generative models trained on billions of non-curated text-imagepairs have failed to achieve similar results. A possible approach to enable test-time few-shot prompting for computer vision tasks is to train a multi-taskinpainting model (Wang et al., 2022; 2023b; Bar et al., 2022). For example, previous approaches (Wang et al.,2022; 2023b) adopted a fully supervised paradigm to train a model over a predetermined set of vision tasks.However, this line of study requires a handful of manual annotations and thus struggles to scale and generalizewell to unseen vision tasks. Instead of explicitly designing the tasks, Bar et al. (2022) took a differentunsupervised approach by proposing to learn from unstructured Computer Vision Figures data, where imageshave implicit task supervision and grid-like structure. However, using vision-only prompting (Bar et al., 2022)suffers from ambiguities and is limited in its ability to describe a specific visual task. To alleviate these difficulties, we propose to multimodal ICL by prompting using input that consists of bothpixels and text. Intuitively, these two modalities can work in synergy to enhance the understanding of theworld and its complexities. For example, during a conversation, people use language to communicate ideasand vision to perceive facial expressions and conversational gestures (Cassell et al., 1999; McNeill, 2019). Forprompting, conditioning vision models on text can enable describing instructions in an efficient manner andreduce ambiguities without the necessity for multiple high-quality visual examples. Equipped with this intuition, we train a model, dubbed IMProv, to inpaint randomly masked regions giventhe rest of the image and a caption as context Our training does not require explicit definitions of tasksand annotations for each task. To demonstrate multimodal learning can be boosted by larger dataset, wecollected a new dataset of image-text pairs from Semantic Scholar, which is three times larger than thelargest existing computer vision figures dataset. We train a new model by performing inpainting on randomlymasked images from a combination of the newly constructed data and LAION 400M (Schuhmann et al., 2021).",
  "Reconstructed tokens": ": IMProv Architecture. During training, the input image is patchified, masked, and fed to themodel together with the associated caption CLIP (Radford et al., 2021) embeddings. For each masked token,the decoder outputs a distribution over a frozen pretrained VQGAN (Esser et al., 2021) codebook. Themodel is trained with cross-entropy loss. At test-time, our model exhibits emerging capabilities such as zero-shot prompting for vision tasks, e.g.,performing foreground segmentation with only a textual description of the task without any image examples. We explore the outcomes of interchanging and combining image and textual prompts in our model. We findthat when using both modalities our model achieves improved ICL performance compared to past vision-onlyapproaches (), improving average precision by over 10% in Foreground Segmentation, over 4% forsingle object detection, and closing over 40% of the gap between current ICL approaches to state-of-the-art1-shot training approaches that utilize supervised base-class training data. Beyond visual recognition, IMProvcan be applied to general vision tasks including edge estimation, depth estimation, and conditional imagesynthesis as shown in .",
  "We introduce a model with Inpainting-based Multimodal Prompting capabilities for vision tasks (IMProv).It receives both text and masked input image as context and outputs a reconstructed image": "Given an input image x RHW 3, a binary mask m {0, 1}HW , and a sentence t K V where V isthe vocabulary and K in the sentence length, the goal of our inpainting model f is to generate a new imagey RHW 3, with the masked regions filled according to the input image context and the sentence:",
  "y = f(x, m, t)(1)": "Our model f has an encoder-decoder structure like MAE-VQGAN (Bar et al., 2022), where the encoderand decoder are Vision Transformers (Dosovitskiy et al., 2020). In contrast to Bar et al. (2022); He et al.(2021), after every self-attention layer, we add a cross-attention layer between image tokens and textualtokens, thereby effectively allowing each image token to attend to text token:",
  "Published in Transactions on Machine Learning Research (09/2024)": "Zhuotao Tian, Hengshuang Zhao, Michelle Shu, Zhicheng Yang, Ruiyu Li, and Jiaya Jia. Prior guidedfeature enrichment network for few-shot segmentation. IEEE transactions on pattern analysis and machineintelligence, 2020. 13 Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundationlanguage models. arXiv preprint arXiv:2302.13971, 2023. 1, 12",
  "+ Class NameLeft - input image, right: Black and whiteforeground background segmentation of a horse": "At inference time, prompting the trainedmodel can be done via text, via a visualprompt, or by combining both. To promptthe model via visual prompt we apply thesame task formulation of Bar et al. (2022)- we form a grid-like image composed oftask input-output example(s) (e.g. inputimages and their segmentation masks),and a new query image, and apply themodel to inpaint the corresponding resultfor the query. To prompt the model viatext, we provide to f a description of thetask (e.g. \"Left: input images, Right: foreground/background segmentation results\"). Formally, Let S = {(xi, yi)}ni=1 be the set of input-output examples where xi is an image and yi is thecorresponding vision task output, let t be a textual task description and let xq be a query image. We introducean arrangement function g1 that arranges S and xq into a grid (visual prompt), denoted by xvp and providesa mask m for the inpainting function:",
  "CVF78,22720,764noarxivS2CV268,118261,225YesSemantic Scholar": "The grid-like visual prompt images inpainted by IM-Prov have a different distribution from natural im-ages.Vision task descriptions (e.g.\"left: input,right: segmentation mask\"), paired with images, donot appear often in widely used language-and-vision datasets. Thus, a model that was trained on thesedatasets will have trouble completing the inpainting task successfully due to the distribution shift. To mitigatethis domain gap, we collect a new dataset of figures, paired with their associated captions, extracted fromcomputer vision papers. Our dataset, The Semantic Scholar Computer Vision dataset (S2CV), is collected from computer visionpapers that appear on Semantic Scholar website. This website contains papers from 40 conferences andjournals from the years 2010 to 2022. We extracted pairs of figures and their captions from each paper onthe website, resulting in 1,417,398 pairs. We then filtered out figures that do not include images (e.g. plotof loss curve). Finally, the filtered S2CV dataset includes 268,118 captioned figures and is 3 times largerthan the largest existing figures dataset, the Computer Vision Figures dataset (CVF; Bar et al. (2022)). Seecomparison in , full details about S2CV in the dataset datasheet (Gebru et al., 2021), and providedexamples in . We also extend the existing CVF by repeating its data collection process and extracting the captions of thefigures in the dataset. This results in 78,227 image-text pairs. This dataset, CCVF (Captioned-CVF), servesas a baseline in our experiments.",
  "Experiments and Results": "We train a IMProv with a ViT-L backbone on a combination of our CCVF, S2CV dataset and LAION-400M (Schuhmann et al., 2021). During the training process, we create mini-batches by randomly selectinghalf of the data from the LAION-400M dataset and the other half from CCVF and S2CV, ensuring that themodel learns from a diverse set of figure-like images. We evaluate IMProv on a variety of computer vision tasks. By default, our visual prompt consists of a 2 2grid where the bottom-right quarter is masked, the top row contains the input-output example, and thebottom-left image represents the query. The visual example and the textual prompt are defined according tothe task (see .2).",
  "Implementation Details": "During training, we utilize images and their associated captions. We follow the resized cropping and flippingaugmentations of He et al. (2021) and train on 224 224 crops. We use AdamW (Loshchilov & Hutter, 2017)optimizer with a learning rate of 2e4 and weight decay of 0.05. We train our models on one machine with 8A100 GPUs with a batch size of 2048 for 150k iterations. Our learning-rate schedule consists of 2k linearwarm-up steps followed by a cosine learning rate decay. We use a pre-trained frozen CLIP ViT-L/14 modelas our text encoder and a pre-trained VQGAN codebook with a vocabulary size of 1024, provided by Esseret al. (2021) and a spatial dimension reduction of 16. During training, we drop the text conditioning with aprobability of 0.1.",
  "Downstream Computer Vision Tasks": "Next, we include the evaluation results of IMProv on a wide range of computer vision tasks. When trainedon CCVF/ S2CV and LAION 400M (Schuhmann et al., 2021), IMProv significantly improves ICL performanceover a wide range of computer vision downstream tasks when compared to vision-only ICL approaches. Foreground Segmentation. In this task, the goal is to segment the query image into two classes - foregroundand background. The input-output example is a random image with its corresponding binary segmentationmask (e.g. black for the background and white for the foreground). We define the textual prompt to be: Left- input image, right - Black and white foreground-background segmentation of {class}, where the {class} isthe class of the foreground object, annotated in Pascal-5i. We follow the evaluation protocol of Bar et al.",
  "(2022) and test IMProv on four splits of Pascal-5i dataset (Shaban et al., 2017). Results are reported in": "Object Detection. Similar to the task of Foreground Segmentation, our objective here is to performbinary segmentation of the object present in the query image. However, this task is more challenging as theinput-output examples contain a rectangle-shaped mask, derived from a bounding box which is less accuratecompared to a fine detailed segmentation mask. We define the textual prompt to be: Left - input image,right - Black and white foreground background segmentation of {class} of rectangle shape where the {class}is the class of the foreground object. We use the Pascal VOC 2012 dataset (Everingham et al., 2015), which",
  "Left - input image, right: Black and white foreground background segmentation of {class_name}": ": Detailed textual prompts IMProv performance. We experiment with textual prompts withvaried amounts of detail, e.g., from no text to instructions that include the task, specific locations, and objectclass names. See examples of full-text prompts in . Please see the supplementary material for moreresults. : Comparison between previous visual prompting results to multimodal prompting oncomputer vision tasks. For Foreground Segmentation and Single Object Detection, we report the mIOUscore. For Colorization, we report the MSE and LPIPS. Training dataset appears in parentheses.",
  "consists of images along with their associated detection boxes. Our results are reported in in terms ofthe mean Intersection over Union (mIOU) metric": "Colorization. The goal is to map a gray-scale image to a color image. The example pair is a gray-scaledimage and the corresponding color image. We define the textual prompt to be: Colorization results: Left- input image, Right - Colorized image of class where the {class} is the class of object present in theimage. We randomly sampled 1000 example pairs and image query from ImageNet (Russakovsky et al., 2015)validation set and converted them to gray-scale to obtain gray-scale and color version for each image. MSEand LPIPS (Zhang et al., 2018) Results are reported in . Other Tasks. We evaluate our models on the dataset created by Wang et al. (2023c), which includesaround 310k image-caption pairs that were automatically annotated by using state-of-the-art pre-trainedmodels for a wide range of vision tasks. Specifically, each image is annotated with depth and normal mapsobtained from Midas (Ranftl et al., 2022), segmentation maps obtained from Uniformer (Li et al., 2022),and object boundary maps detected by HED (Xie & Tu, 2015). For each vision task X, we evaluate ourmodel on two tasks - X-to-images and images-to-X. As each task has a different evaluation metric, and as ourmodel produces image outputs, we simplify the evaluation by comparing the generated image to the renderedannotation of the task by calculating LPIPS (Zhang et al., 2018). We report the results in and plotqualitative results in .",
  "MAE-VQGAN (CVF)23.52IMProv(CCVF)26.13IMProv(CCVF + LAION)36.29": "Dataset Ablation. We report our results and compare themwith prior works in . IMProv trained on a combinationof LAION-400M and our S2CV dataset outperforms the priorwork (Bar et al., 2022) trained solely on the CVF dataset bymore than 12 points in mIOU. It demonstrates IMProv couldbenefit from training on additional amounts of unlabeled images. Textual Prompts Ablation. We experiment with textual andvisual prompts that have different relevance to the query imageand task. For the visual prompt, we choose the input-outputexamples using three different retrieval strategies: (1) RandomClass, a random example pair in which the class of the foreground object is chosen randomly, (2) SameClass, where a random example pair of the same class is chosen randomly, and (3) The example is chosen viaNearest Neighbor from all the images with the same foreground object class, using the model from Zhanget al. (2023). We evaluate our IMProv model on Pascal 5i with and without a textual prompt that containsTask, Location, and Class Name. Firstly, we compare against Bar et al. (2022) under (1) Random Class visual prompt setting and reportresults in . In this setting, the visual prompts describe the task (e.g., segmentation) but are not",
  "Supervised ICL (InstructPix2Pix)0.650.600.590.620.640.610.620.55IMProv (S2CV+LAION)0.610.520.510.460.590.500.560.48IMProv (S2CV+LAION+InstructPix2Pix)0.550.430.470.370.540.460.510.44": "curated from the same class (the setting in ), or chosen via nearest neighbors as in Zhang et al. (2023).Using non-curated visual prompts is most realistic, as finding a perfectly aligned visual example might be ashard as solving the original input. The result shows that conditioning on text improves average mIoU by 3points when using reasonable non-curated visual prompts. Moreover, IMProv trained on a combination ofLAION and our CCVF dataset further boost the mIoU by 10 points.",
  ": Using textual prompts improvesperformance and reduces the need for carefulselection of visual prompts": "In we plot the results under different textual prompts.We find that the textual prompts play a big role in the perfor-mance of the model (see ). To dive deeper into the effectof textual prompt, we plot the relation between textual promptand visual prompt in . It shows adding text promptsimproves the results for any type of visual prompt, from theleast related Random Class examples to the most relevant Near-est Neighbors examples. In addition, we find that by using text,it is possible to achieve similar performance with lower-qualityvisual examples (using the Same Class example rather thanthe Nearest Neighbor (Zhang et al., 2023)). Similarly, higher-quality visual examples improve the results for all the testedtextual prompts. Interestingly, the results suggest a trade-offbetween the two modalities - high-quality textual prompts canalleviate the need for carefully chosen visual prompts, and viceversa. Moreover, as shown in , when the textual prompt isinconsistent with the visual example, the model may follow themore certain textual instruction. We include additional resultsfor different combinations of visual and textual prompts in theSupplementary Material. Does Structured Data Improve In-Context Learning?The key insight of our approach is to train IMProv on unstructured data in a fully unsupervised mannerwithout parsing the image or the text. Here we experiment with training IMProv on additional structureddata in a fully supervised manner. We use the dataset of Brooks et al. (2022), which consists of 310kinput-output image editing pairs and their corresponding descriptions. For training, we use random pairsas our input-output examples. We embed them into a grid structure, in a similar manner to the structurewe use at test-time. The grid images that we construct for training consist of 1 2 and 2 2 images byrandomly selecting 1 or 2 input-output examples for each caption in the original dataset. We test our models on a held-out set of vision tasks. As shown in , we find that training on structuredsupervised data alone leads to poor generalization and ICL performance on the test tasks. Training on bothunstructured S2CV and LAION-400M, together with the structured data improves ICL results on the testtasks compared to our base model. Comparison to Finetuning and Few-Shot Baselines. We compare IMProv to classic 1-shot baselines,which we view as an upper bound of our approach. Approaches like FWB (Nguyen & Todorovic, 2019) andCyCTR (Zhang et al., 2021) utilize a fully labeled base classes train set (2086 to 5883 on different Pascal 5i splits) with architectures that are optimized for foreground segmentation (e.g, by utilizing higher resolutions).We also compare to MAE-VQGAN (Bar et al., 2022) that performs visual prompting without text and to",
  ": Compare to Stable Diffusion (SD) on Segmentation. The result is marked in red": "finetuning baselines with K = {1, 4, 16} training examples for each target class. The results in indicate that IMProv closes over 40% of the accuracy gap between MAE-VQGAN to supervised one-shotapproaches. This demonstrates the potential of our approach to scale with more data. Comparison to existing text-to-image works. We also compare our IMProv against state-of-the-arttext-image generative models, i.e. Stable Diffusion Rombach et al. (2021). We input the same text promptand visual prompt to Stable Diffusion version 1 (SD1) and 2 (SD2) inpainting models, with 50 inferencesteps and 7.5 classifier free guidance scale. The quantitative results are shown in . Comparedto IMProv, SD fails to generate black/white segmentation mask, and couldnt leverage visual prompt to",
  ": Failure case analysis. Compare to Bar et al. (2022), IMProv could succeed on some cases tosome extend e.g. \"Task ambiguity. But we still fail on one of the \"Non-aligned input-output\" cases": "find the corresponding objects. We also compare them on the letter generation task in . AlthoughSD sometimes could generate the correct letter, it still fails to follow the visual prompt to generate whitebackground and red font color. Moreover, as shown in , when the textual prompt is inconsistent withthe visual example, the model may follow the more certain textual instruction.",
  "VisualPrompt58.3Painter62.3IMProv (CCVF)62.8IMProv (S2CV)68.9": "Compare to supervised approach. Compared to a supervisedprompting approach like Painter(Wang et al., 2023a), IMProv cangeneralize to a larger variety of tasks.We demonstrate this in, showing that while Painter fails to adapt to the colorizationtask, IMProv performs reasonably well.We additionally reportthe comparison of Painter and our method in . Our modeloutperforms Painter by a large margin. Its also worth noting thatchanging the training dataset from CCVF to S2CV improves themIoU significantly, which further indicates the effectiveness of ourproposed S2CV dataset.",
  "Grid 2x242.68----Grid 3x337.2139.78---Grid 4x416.4218.3430.7832.5633.11": "Different grid size With a 2x2 grid, only a visualprompt is applicable. We also report different gridsizes with different numbers of visual prompts. Wereport the results on Pascal VOC segmentation in and . In each row, keeping grid sizefixed, as we increase the number of visual promptexamples, the mIoU increases. On the other hand, due to the limitation of 224x224 resolution, when the gridsize is larger, image in each shot becomes smaller, so one-shot accuracy drops. We acknowledge that thecurrent model could only take a fixed image size, as many other pre-train image encoders. However, if morecomputation budgets are available, this issue could be mitigated with higher training resolution. Failure case analysis We compare with failure cases of Bar et al. (2022) in . For some of thecases, our IMProv could successfully address them with text prompts, e.g. the cat colorization and bottlesegmentation of Task ambiguity. And our model could successfully generate the image that moves theorange to the center, though fails the other Non-aligned input-output example.",
  "Related Work": "Prompting in NLP. The ability to prompt a language model to solve a specific task, also known as ICL, is arecently discovered property of generative language models that were trained on a large corpus of text (Brownet al., 2020; Touvron et al., 2023; Chowdhery et al., 2022; Bubeck et al., 2023). Brown et al. (2020) haveshown that existing NLP tasks can be described in unstructured text, and then fed into a large languagemodel to complete the missing part without any finetuning (Radford et al., 2019; Brown et al., 2020). Morerecently different approaches to prompting have emerged including Prompt Engineering (Brown et al., 2020;Lu et al., 2021), Prompt Ensembling (Jiang et al., 2020), Prompt Prefix Tuning (Li & Liang, 2021; Lesteret al., 2021), and Chain of Thought Prompting (Wei et al., 2022). The Flamingo (Alayrac et al., 2022) modelextends language-only models, and conditions the model on both image and text. Our approach is differentin that our model outputs pixels and not text. Therefore, it is suited to solve a variety of computer visiontasks that can be represented in pixel-space, like semantic segmentation or image colorization.",
  "Discussion": "We presented an approach for multimodal prompting of inpainting models. To unlock in-context learningcapabilities in such models, we had to collect a specific dataset of figures with associated captions. To furtherscale this approach, we believe that other sources of unstructured data - visual, textual, and multimodal -should be incorporated during the training phase. To understand the feasibility of such a data collection effort,one must be able to predict and quantify the effect of different dataset sizes and types on the downstreamin-context learning capabilities of models trained on them. We plan to investigate it in future work.",
  "Hyojin Bahng, Ali Jahanian, Swami Sankaranarayanan, and Phillip Isola. Exploring visual prompts foradapting large-scale models. arXiv preprint arXiv:2203.17274, 1(3):4, 2022. 13": "Amir Bar, Xin Wang, Vadim Kantorov, Colorado J Reed, Roei Herzig, Gal Chechik, Anna Rohrbach, TrevorDarrell, and Amir Globerson. Detreg: Unsupervised pretraining with region priors for object detection.arXiv preprint arXiv:2106.04550, 2021. 13 Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei Efros. Visual prompting via imageinpainting. Advances in Neural Information Processing Systems, 35:2500525017, 2022. 2, 3, 4, 5, 8, 9, 11,12, 13, 20",
  "Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editinginstructions. arXiv preprint arXiv:2211.09800, 2022. 9": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, GretchenKrueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language modelsare few-shot learners. CoRR, abs/2005.14165, 2020. URL 1, 2, 12 Sbastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, PeterLee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Earlyexperiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023. 12 Justine Cassell, David McNeill, and Karl-Erik McCullough. Speech-gesture mismatches: Evidence for oneunderlying representation of linguistic and nonlinguistic information. Pragmatics & cognition, 7(1):134,1999. 2 Stephanie Chan, Adam Santoro, Andrew Lampinen, Jane Wang, Aaditya Singh, Pierre Richemond, JamesMcClelland, and Felix Hill. Data distributional properties drive emergent in-context learning in transformers.Advances in Neural Information Processing Systems, 35:1887818891, 2022. 1",
  "Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative imagetransformer. arXiv preprint arXiv:2202.04200, 2022. 13": "Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, KevinMurphy, William T. Freeman, Michael Rubinstein, Yuanzhen Li, and Dilip Krishnan. Muse: Text-to-imagegeneration via masked generative transformers, 2023. 13 Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generativepretraining from pixels. In International Conference on Machine Learning, pp. 16911703. PMLR, 2020. 13 Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, PaulBarham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modelingwith pathways. arXiv preprint arXiv:2204.02311, 2022. 1, 12",
  "Alexei A. Efros and Thomas K. Leung. Texture synthesis by non-parametric sampling. In IEEE InternationalConference on Computer Vision, pp. 10331038, Corfu, Greece, September 1999. 13": "Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1287312883,2021. 3, 4, 5, 13 M. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The pascalvisual object classes challenge: A retrospective. International Journal of Computer Vision, 111(1):98136,January 2015. 6",
  "Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprintarXiv:2101.00190, 2021. 12, 13": "Guilin Liu, Fitsum A. Reda, Kevin J. Shih, Ting-Chun Wang, Andrew Tao, and Bryan Catanzaro. Imageinpainting for irregular holes using partial convolutions. In Proceedings of the European Conference onComputer Vision (ECCV), September 2018a. 13 Guilin Liu, Fitsum A Reda, Kevin J Shih, Ting-Chun Wang, Andrew Tao, and Bryan Catanzaro. Imageinpainting for irregular holes using partial convolutions. In Proceedings of the European conference oncomputer vision (ECCV), pp. 85100, 2018b. 13",
  "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language modelsare unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 12": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, GirishSastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learningtransferable visual models from natural language supervision. In Marina Meila and Tong Zhang (eds.),Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings ofMachine Learning Research, pp. 87488763. PMLR, 1824 Jul 2021. URL 3, 4, 18 Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer,2020. 13 Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, andIlya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pp.88218831. PMLR, 2021. 13",
  "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolutionimage synthesis with latent diffusion models, 2021. 10, 13": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, AndrejKarpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large ScaleVisual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211252, 2015.doi: 10.1007/s11263-015-0816-y. 7 Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, AarushKatta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. 2, 5 Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, TheoCoombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy,Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. Laion-5b: An open large-scaledataset for training next generation image-text models, 2022. 18",
  "Boyu Yang, Chang Liu, Bohao Li, Jianbin Jiao, and Qixiang Ye. Prototype mixture models for few-shotsemantic segmentation. In European Conference on Computer Vision, pp. 763778. Springer, 2020. 13": "Chao Yang, Xin Lu, Zhe Lin, Eli Shechtman, Oliver Wang, and Hao Li. High-resolution image inpaintingusing multi-scale neural patch synthesis. In Proceedings of the IEEE conference on computer vision andpattern recognition, pp. 67216729, 2017. 13 Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu,Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. arXiv preprintarXiv:2110.04627, 2021a. 13 Yingchen Yu, Fangneng Zhan, Rongliang Wu, Jianxiong Pan, Kaiwen Cui, Shijian Lu, Feiying Ma, XuansongXie, and Chunyan Miao. Diverse image inpainting with bidirectional and autoregressive transformers. InProceedings of the 29th ACM International Conference on Multimedia, pp. 6978, 2021b. 13",
  "ABroader Impact Statement": "Using In-Context Learning to solve various vision tasks using one model has the potential to reduce the costof training and inference of future models, facilitate access to this technology, and democratize its exploration.Nevertheless, while scaling the training dataset to a large corpus of non-curated text-image pairs improvesthe in-context learning results, it might also introduce potential biases that must be mitigated prior to anycommercial deployment of the model. A comprehensive discussion about ethical considerations can be foundin Schuhmann et al. (2022).",
  "Text prompt with location, task and class information - Left - input image, right - Black and whiteforeground/background segmentation of {class}": "and present quantitative and qualitative results for different combinations of visual andtextual prompts. We find that more informative textual prompts improve the results for all visual prompts.Similarly, higher-quality visual examples improve the results for all the tested textual prompts. Interestingly,the results suggest a trade-off between the two modalities - high-quality textual prompts can alleviate theneed for carefully chosen visual prompts, and vice versa. We argue that using non-curated visual prompts ismost realistic, as finding a perfectly aligned visual example might be as hard as solving the original input.",
  "Illustration of obtaining feature vector for causality loss optimization with respect to different tasks. (a) For classication task, spatial average pooling is": "performed over the entire feature map ($h\\\\times w \\\\times c)$ to obtain a feature vector of $c$ dimensions (see Section \\\\ref{subsubsec:cla}). (b) For segmentation task (pixel level classication), entropy is calculated for each pixel (see Section \\\\ref{subsubsec:seg}). (c) For detection task (region level classication), spatial average pooling is performed over each groundtruth bounding box (bbox) region to obtain a feature vector of $c$ dimensions (see",
  "MAE-VQGAN (CVF)24.6625.1524.3619.9123.52IMProv(CCVF)26.1527.3829.3721.6226.13IMProv(CCVF + LAION)37.0940.6836.9130.4936.29": "So in we report the mIoU on Pascal VOCsegmentation, where we compare against Bar et al.(2022) under Different Class Random Sample vi-sual prompt setting.In this setting, the visualprompts describe the task (e.g., segmentation) butare not curated from the same class (as in ),or chosen via nearest neighbors. The result showsthat conditioning on text significantly improves theprompting performance when using reasonable non-curated visual prompts.",
  ": Trade-off between the number ofvisual prompt and textual prompts": "Trade-off between the number of visual prompt andtextual prompts We plot the mIoU w.r.t the numberof support in . We run the experiments undergrid 4x4 setting in , with Random Class supportimage. Similar to Bar et al. (2022), as we increase thenumber of support visual examples, mIoU goes up. It isworth noting that when there are only 1 or 2 examples, thetext prompt does not help. It is because the model couldntgenerate meaningful segmentation with a small numberof visual supports due to the resolution (see fordetails). Under the setting that visual support numbergreater than 3, the text prompt consistently improves theresults. It implies that our text prompt is orthogonal tothe number of visual support pairs.",
  "Other Vision Tasks. We provide more qualitative resultsfor both Image-to-X task and X-to-Image task in and respectively, \"X\" can be any of the following: segmentation, edges, depth and normal": "Generalizability to new vision tasks Our model was never trained on specific vision tasks, and insteadwas trained on unstructured and non-annotated figures from computer vision papers. The data we train on isnot constructed explicitly for specific tasks, and even if it is trained on some task-specific figures, it is usuallynot presented the way we test our model, as we randomly crop the figures during training. Therefore, webelieve the test task is generalized from different task combination instead of replicating from training task.Similarly to(Bar et al., 2022), our model can generalize to unseen diverse tasks as shown in ."
}