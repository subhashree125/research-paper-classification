{
  "Abstract": "Conventional object recognition models operate under closed-set assumptions presumingthat the training dataset is sufficiently comprehensive that any object detected during infer-ence can be assigned to some known prior class. This assumption is flawed and potentiallydangerous for real-world applications such as driving scene perception where diverse ob-jects and unexpected behaviours should be expected. In order to progress towards trustedautonomous platforms object recognition models need Open Set Recognition (OSR) meth-ods capable of identifying unknown classes while maintaining good performance on knownclasses. Existing OSR methods are mostly designed for image data and utilize generativemodels which are hard to train. In this paper, we propose S2CA, a Supervised ContrastiveClass Anchor learning method which leverages contrastive learning principles to effectivelyreject unknown classes by increasing intra-class compactness and inter-class sparsity ofknown classes in feature space. We train a feature encoder through contrastive learningwhile ensuring that features of known classes form compact clusters, and then transfer thetrained encoder to the OSR task. During inference, the model rejects unknown classes basedon class-agnostic information in feature space and class-related information in logit space.The proposed OSR method is simple yet powerful. It is not only suitable for image-basedobject recognition models, but can also be used for a variety of lidar-based object recog-nition models. We demonstrate superior performance of S2CA when compared with stateof the art methods on two widely used driving scene recognition datasets, i.e., KITTI andnuScenes. The source code is available at",
  "Introduction": "Autonomous driving has received a great deal of attention in recent years due to its potential for eliminatinghuman error, which is the major cause of most road accidents.To ensure safe and efficient navigation,autonomous vehicles must be able to perceive their surroundings using onboard sensors and understand thedriving scene. This involves localizing and recognizing surrounding objects. Conventional object recognition models typically make the assumption of a closed set, requiring that allobject categories which the model will encounter are available in the training set (Scheirer et al., 2013).When the recognition model encounters an unknown object, it will misclassify it as one of the known classes.In the real world, autonomous vehicles encounter a wide variety of objects. It is not feasible to include allobject categories in the training set. For example, autonomous driving datasets designed for the northernhemisphere do not typically contain kangaroos, yet in Australia kangaroos frequently appear on the road.",
  "Published in Transactions on Machine Learning Research (09/2024)": "Shuaifeng Zhi, Yongxiang Liu, Xiang Li, and Yulan Guo.Toward real-time 3d object recognition: Alightweight volumetric cnn framework using multitask learning. Computers & Graphics, 71:199207, 2018.ISSN 0097-8493.doi: Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million imagedatabase for scene recognition. IEEE transactions on pattern analysis and machine intelligence, 40(6):14521464, 2017. Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min Chen. Bbn: Bilateral-branch network with cumulativelearning for long-tailed visual recognition.In 2020 IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR), pp. 97169725, 2020. doi: 10.1109/CVPR42600.2020.00974.",
  "Therefore, the closed-set assumption will inevitably have an adverse impact on the autonomous drivingsystem": "An ideal driving scene perception system should be able to detect all object instances on the road surface,then reject unknown objects and classify the known ones. Unknown objects can then be stored for laterlabelling, e.g. by a human supervisor. Scheirer et al. (2013) define the problem of rejecting unknown classes(U) and simultaneously classifying known classes (K) as open-set recognition (OSR). Unknown classes aresometimes referred to as unknown unknowns, because they are not only unseen but also have no sideinformation (e.g., semantic attributes or known properties) during training (Oza & Patel, 2019; Geng et al.,2021). An intuitive OSR method is to set a threshold for the classification confidence score (e.g. softmax probability)of the most probable class produced by the model (Hendrycks & Gimpel, 2017). However, previous researchhas shown that samples in U can also produce equally high softmax scores (Bendale & Boult, 2015). State-of-the-art OSR methods often involve generative models (Ge et al., 2017; Neal et al., 2018; Oza & Patel,2019; Perera et al., 2020; Kong & Ramanan, 2021), which help separate known and unknown classes inthe feature space. While improved feature learning leads to better OSR performance, the complexity andchallenges of training generative models limit their application in open-set recognition in diverse scenarios.Miller et al. (2021) proposed Class Anchor Clustering (CAC), which forces known classes to form compactclusters around class anchors in logit space and rejects unknown objects based on their distance to knownclusters. However, the cluster compactness achieved by minimizing the CAC loss can limit the networksability to learn discriminative features (Miller et al., 2021), which is critical for achieving acceptable OSRperformance (Perera et al., 2020). A further limitation of existing OSR methods relates to their applicationto a single data modality, i.e., RGB images, whereas OSR performance with other data modalities, such aslidar, has not been evaluated in previous work.",
  "Open-set Recognition": "Open-set recognition has long been an attractive topic in the computer vision community. Early studies(Scheirer et al., 2013; 2014; Jain et al., 2014; Scherreik & Rigling, 2016) often involve traditional machinelearning models, such as SVM, and use the probabilistic methods to threshold the prediction score. OpenMax (Bendale & Boult, 2015) is one of the first OSR methods based on deep neural networks, whichdetermines the center of each known class in the logit space and then creates a statistical model of thedistances between correctly classified samples. It uses extreme value theory (EVT) to detect outliers byfitting a weibull function to the tail of the distance distribution. Many subsequent works involve genera-tive models. G-OpenMax (Ge et al., 2017) trains a Generative Adversarial Network (GAN) to synthesizeopen-set samples and uses these to estimate open-set class activations. Neal et al. (2018) propose a dataaugmentation technique called counterfacutal image generation. This approach uses GANs to generate im-ages that are visually similar to known ones, but are actually from unknown classes. The network is trainedto have low confidence scores for these known unknowns. Kong & Ramanan (2021) propose OpenGAN,which is suitable for both image classification and semantic segmentation. Generative models can do morethan synthesize known unknown classes. In (Oza & Patel, 2019), the auto-encoder only learns feature repre-sentations from known classes, which leads to higher reconstruction errors for unknown classes. Yoshihashiet al. (2019) integrate classification and input reconstruction in model training, which enhances the learnedfeature representation. Perera et al. (2020) combine the auto-encoder with a multi-task classifier trained byminimizing a self-supervision loss and a classification loss. The self-supervision learning branch incorporatesrandom transformations to improve the descriptiveness of feature representations. Miller et al. (2021) pro-pose a simple OSR method, which forces known classes to form tight clusters around class anchors in logitspace and reject unknown objects based on their distance to known classes. While this model is easier totrain than the previous generative models, anchoring class centers at equal distances can restrict the learningof semantically meaningful features (Miller et al., 2021).",
  "samples can be completed unrelated to the training set, whereas OSR aims to reject valid but unknownclasses (Gillert & von Lukas, 2021)": "OOD detection methods usually map the input x onto a single scalar that indicates OOD-ness. Assumingthat the network is more confident about known objects, the OOD-ness leads to softmax scores that arehigher for known classes than the unknown ones, making it a baseline OOD detection method (Hendrycks& Gimpel, 2017). Unknown objects tend to have a uniform softmax distribution, leading to higher entropy(Chan et al., 2021). Liang et al. (2018) propose ODIN, which adds small perturbations to the input therebyincreasing the softmax score gap between known and unknown objects. Hendrycks et al. (2022) use themaximum logit as the OOD score, and show that it outperms the softmax baseline (Hendrycks & Gimpel,2017). KullbackLeibler divergence between the softmax probabilities and class templates has also been usedas the OOD score (Hendrycks et al., 2022). Liu et al. (2020) measure OOD-ness of objects based on theHelmholtz free energy of the network output. Lee et al. (2018) use the Mahalanobis distance (MD) betweenthe feature vector and the closest class-conditional Gaussian distribution to measure OOD-ness. Wang et al.(2022) propose Virtual-logit Matching (ViM), which combines class-agnostic information in the feature spaceand the class-dependent information in the logit space for OOD detection.",
  "Contrastive Learning": "Contrastive learning aims to learn a representation of data such that similar instances are close together inthe representation space, while dissimilar instances are far apart. He et al. (2020) propose the MomentumContrast (MoCo) for large-scale unsupervised learning, which converts the image-based contrastive learninginto a dictionary look-up problem. Chen et al. (2020) propose SimCLR, which maximizes the similaritybetween augmented versions of the same image (positive pairs) and minimizes the similarity between differ-ent images (negative pairs). SimCLR employs an MLP projection head after the encoder network, whichintroduces non-linear transformations to the feature space. It also incorporates a normalization step afterthe projection head, which ensures that the representations have unit length, facilitating the use of cosinesimilarity to measure similarity between pairs. Khosla et al. (2020) propose Supervised Contrastive Learning(SCL), which extends SimCLR (Chen et al., 2020) to fully supervised scenarios. SCL contrasts all samplesfrom the same class as positives against the negatives from the remainder of the batch, thereby formingtight class clusters in the feature space. Contrastive learning methods (He et al., 2020; Chen et al., 2020;Khosla et al., 2020) usually train an encoder with contrastive loss first, then freeze the encoder and traina linear classifier. Wang et al. (2021) argue that this two-stage learning may not be the most effective ap-proach in a fully supervised setting, as it can reduce the compatibility between the feature encoder and theclassifier. To address this issue, Wang et al. (2021) propose a hybrid framework that allows simultaneouslearning of features and classifiers for long-tail image classification. Ming et al. (2023) show that contrastivelearning reduces intra-class sparity and improves inter-class repulsion in the feature space, which benefitsdistance-based OOD detection.",
  "Proposed Method": "Given the various data modalities and the different model architectures used for object recognition in drivingscenes, we propose to perform OSR in feature space. This makes our method efficient and applicable todifferent data modalities and encoder networks. shows an overview of the proposed S2CA framework. We train a feature encoder with ContrastiveClass Anchor learning, which combines Supervised Contrastive Learning (SCL) (Khosla et al., 2020) andClass Anchor Clustering (CAC) (Miller et al., 2021). Contrastive Class Anchor learning aims to train theencoder so that the known classes show intra-class compactness and inter-class sparsity in the feature space.In this way, feature representations of known and unknown classes are separated, allowing us to model thedistribution of known classes and reject unknown classes.",
  "Contrastive Learning Branch": "Supervised Contrastive Learning (SCL) (Khosla et al., 2020) encourages objects of the same class to betightly clustered in the feature space, while clusters belonging to different classes are far apart.Thesecharacteristics are beneficial to the OSR task. The intra-class compactness of known classes helps maintaina robust recognition performance for known objects, while the inter-class sparsity makes it easier to rejectunknown objects based on their distance to the known classes. Let x and y denote the input instance and the corresponding label, with y N known classes. For eachinstance, S2CA creates two augmented views, xi and xj, of x from a predefined family T . For image input,augmentation may include random cropping, random flipping and color jittering (for range images, we onlyuse random flipping). For lidar point clouds, we apply geometric transformations such as random translation,random rotation and random scaling. For a mini-batch of size B, the corresponding augmentations aredenoted as {xi}Bi=1 and {xj}Bj=1.",
  "Classifier Learning Branch": "In the classifier learning branch, the feature embedding h is passed to a linear classification layer fc toproduce the N-class logit vector s. The CAC loss (Miller et al., 2021) is defined based on the distancebetween the logit vectors s and a set of anchored class centers C = (c1, , cn) with a predefined magnitude set equally for all classes. Let d denote a vector of Euclidean distances between s and each anchored classcenter in C, and f() denote the classification function, the CAC loss is written as:",
  "Transition from Contrastive Learning to Classifier Learning": "To ensure compatibility between the feature encoder and the classifier, we follow a curriculum strategy (Zhouet al., 2020) to adjust the weighting of the two branches. The model will first focus on supervised contrastivelearning (Khosla et al., 2020) and then smoothly transitions to classifier learning as the training progresses.The final objective function is the combination of Eq. (2) and Eq. (3).",
  "LS2CA = LSCL + (1 )LCAC(4)": "Note that LCAC in Eq. 4 is the sum of each individual loss in the mini-batch (like the batch form of theSCL loss shown in Eq. 2). is a parameter that decreases as the training progresses. For each epoch ti, = 1 ti tmax where tmax is the maximum epoch number. Initially, is large, and the model focuses onoptimizing the SCL loss to learn semantically meaningful features. As the training progresses, is reducedand the model smoothly shifts its focus to the CAC loss, which fine-tunes the model from a global perspective. The combination of SCL and CAC has several advantages. While using the CAC loss alone can restrict thediscriminative power of the learned features (Miller et al., 2021), the SCL helps the encoder learn a richerfeature space. The SCL also benefits from the CAC. As a batch contrastive algorithm, SCL only contrastssamples in the same mini-batch. To achieve better performance, SCL requires larger batch sizes or more",
  "Model Inference": "For model inference, we use virtual logit (Wang et al., 2022) to extract information in the feature space andcombine it with ordinary logit to form the OOD score and reject unknown objects. Unlike the distance-based rejection score used by Miller et al. (2021), which ignores the class-agnostic information in the featurespace, the virtual logit matching (ViM) combines the class-agnostic information in the feature space withthe class-dependent information to reject unknown objects and classify the known ones.",
  "Virtual Logit": "The virtual logit is the scaled norm of the projection of the feature vector h to the orthogonal complementof the principal subspace of the feature space. To simplify the subsequent logit computation, we offset hwith a new origin o = (W T )+b, where W is the weight in fc, b is the bias and ()+ is the Moore-Penroseinverse. This transforms the feature vectors to a new bias-free coordinate system. To distinguish these fromthe previous features h, the feature vectors in the new coordinate system are denoted h. We further denotethe feature matrix of the training set as H.",
  "HT H = QQ1(5)": "where contains the eigenvalues in descending order, and the first D columns of Q define the D-dimensionalprincipal subspace P. Let P be the orthogonal complement of P, and hP the projection of h to P .After this projection, known data tend to have a smaller norm, whereas unknown data exhibit a larger norm.This facilitates the differentiation between known and unknown samples. The virtual logit is then obtainedas (Wang et al., 2022):lv = hP (6)",
  "Inference Pipeline": "During inference, the final classification layer fc is replaced by a new module fv, which produces a logitvector of length N +1, that is N normal logits and one virtual logit. fv is able to invoke both the encoder feand the classification layer fc. It uses the encoder fe to extract the feature matrix H from the training setX, project H into a bias-free coordinate system and calculate the D dimensional principal subspace P. We",
  "Implementation Details": "For training, we use the Cosine Annealing Learning Rate (Loshchilov & Hutter, 2017), with a maximumlearning rate of 0.1 and a minimum of 104. In the initial epochs, the learning rate decreases slowly to speedup the contrastive learning process. The learning rate begins to drop rapidly midway through the trainingto facilitate classifier learning. Towards the end of the training, the learning rate becomes very small, sothat the classifier can be fine-tuned. The training takes up to 200 epochs, and is optimized by stochasticgradient descent (SGD) with a momentum of 0.9. For the SCL loss, the temperature is set to 0.1 and thebatch size is limited to 256. For the CAC loss, the anchor magnitude is set to 10 and the weight variable is set to 0.3. For the virtual logit, the dimensionality of the principal subspace is set to 64.",
  "Comparison with State-of-the-art Methods": "We compare the performance of S2CA with existing OSR and OOD detection methods, for which sourcecode is available from PyTorch-OOD library (Kirchheim et al., 2022) or GitHub. Specifically, we selectSoftMax (Hendrycks & Gimpel, 2017), ODIN (Liang et al., 2018), MD (Lee et al., 2018), Energy (Liu et al.,2020), Entropy (Chan et al., 2021), MaxLogit (Hendrycks et al., 2022), KL-M (Hendrycks et al., 2022), ViM(Wang et al., 2022), and CAC (Miller et al., 2021). The selected methods are compatible with different datamodalities and encoder networks.",
  "Encoder Networks": "To evaluate the open-set recognition performance with both image and lidar data several encoder networkswere used as shown in Tab. 1. For RGB image data, we use a 40-layer wide residual network with a wideningfactor 2 (WRN-40-2) (Zagoruyko & Komodakis, 2016) for feature extraction. For lidar data, we use encodernetworks operating on raw point clouds, range images, and 3D voxel grids. For range images, we use the sameWide ResNet as for RGB image data. For lidar data represented as 3D voxel grids, we use VoxNet (Maturana& Scherer, 2015) and LightNet (Zhi et al., 2018) as the encoder networks. For lidar point clouds, we use",
  "Datasets": "We use two public autonomous driving datasets for evaluation: nuScenes (Caesar et al., 2019) and KITTI(Geiger et al., 2013). For the object detection task, nuScenes has 23 classes, while KITTI (Geiger et al., 2013)has 8 classes. Each dataset was divided into three parts: xtrain, xtest and xOOD. xtrain and xtest containedsamples of known classes for training and testing. xOOD refers to unknown classes (out of distribution). Weselected the classes with the fewest annotations as unknown. In nuScenes, unknown classes were stroller,personal mobility vehicle, animal, and ambulance. In KITTI, unknown classes were truck, misc (miscellaneousobjects, such as trailers and segways), tram, and sitting person. We first located each object using the givenbounding box and then extracted these from the image and lidar point cloud data. Heavily occluded orcompletely invisible objects were filtered out. Following Neal et al. (2018), we calculate the openness of",
  "Evaluation Metrics": "The effectiveness of an open set recognition model can be evaluated by its overall classification accuracy orF-score when detecting unknown classes in mixed data. However, these combined metrics are affected notonly by the quality of the OSR model, but also by the calibration parameters chosen (Neal et al., 2018).As a result, we evaluate the performance of OSR models using two metrics: Area Under the ROC Curve(AUROC) and Closed Set Accuracy. Area Under the ROC Curve (AUROC)AUROC uses all possible thresholds to plot an ROC curve ofthe True Postive Rate (TPR) against False Positive Rate (FPR). Ideally, known objects should have a lowerOOD-ness score, whereas unknown objects should have a higher score. If all unknown classes have higherscores than the known ones, the TPR will be 1 and the FPR will be 0 after trying all thresholds, resultingin a perfect AUROC value of 1. Lower AUROC values indicate that unknown objects are mixed with theknown ones.",
  "Open-set Recognition Performance": "Tab. 2 shows the performance of the OSR methods with different encoder networks as measured by AUROC.It can be seen that overall S2CA achieves leading performance on both datasets across different data modalityand encoder networks.Interestingly, the OSR methods perform well with lidar data when paired withsuitable encoder networks. Point-based encoder networks generally outperform projection-based and voxel-based models. The range image representation of lidar data does not seem to benefit the open-set recognitionperformance, especially when the lidar point cloud is sparse. This can be seen from the performance of therange image-based models on nuScenes, which is noticeably worse than on KITTI. This is because nuSceneslidar data are captured by a 32-channel lidar, so the point cloud is relatively sparse and the range imageshave a lower resolution as compared to KITTI which uses a 64-channel lidar. Notably, ODIN (Liang et al.,2018) and MD (Lee et al., 2018) perform quite poorly on nuScenes range images. This can be attributedto the addition of noise during preprocessing step in these methods, which has a negative impact on therecognition performance. The comparable performance of the OSR methods with different image-based andlidar-based encoder networks implies that the power of open-set recognition lies in the feature learning. Themodality of the data is not as crucial as the information it carries.",
  ". Distribution of OOD scores for known and unknown samples obtained by the Softmax (inverted) baseline(a), and the proposed S2CA (b). For fair comparison, all scores are normalized to": "The superior performance of the proposed S2CA can be attributed to its ability to learn discriminativefeatures, which results in OOD scores that are higher for unknown classes. shows the probabilitydensity distributions of the normalized OOD scores for both known and unknown classes in the KITTIdataset. For the SoftMax baseline (Hendrycks & Gimpel, 2017) in a, both distributions exhibit along-tailed pattern and there is a high degree of overlap between them, whereas S2CA ( b) presents anoticeable difference between the two distributions, making it easier to determine an appropriate thresholdto distinguish the known and unknown classes effectively. The significant difference between the two figuresexplains the superior performance of S2CA in the OSR task.",
  ". Closed-set accuracy of S2CA with different encoder networks compared with the cross-entropy baseline onnuScenes (a) and KITTI (b)": "shows the closed-set accuracy of S2CA as compared with the conventional cross-entropy baseline. It canbe seen that for all encoder networks the S2CA framework maintains a comparable closed-set performance asthe conventional cross-entropy baseline. This again indicates that the ability of S2CA to extract rich featuresfrom the data is not only useful for rejecting unknown objects, but also helps maintain a high classificationaccuracy on known classes.",
  ". Examples of failure cases. Van (a) is a known class, while Truck (b) is unknown": "A close examination of the errors reveals that most misclassified samples are objects that are similar inappearence and are too far from the sensor. shows an example of a known (van) and an unknownobject (truck) which were misclassified by S2CA. In such cases, where the unknown object looks very similarto a known class, rejecting the unknown objects and correctly classifying the known ones is challenging.",
  "S2CA - The complete S2CA framework": "Tab. 3 illustrates the results of the ablation study.The complete S2CA framework with differentencoder networks consistently outperforms all competing baseline models in terms of AUROC. It is evidentthat ViM is a key contributor to open-set performance. We can see a substantial improvement in AUROCbetween CLS and CLS V. The SoftMax score in CLS depends solely on the information in the logit space,and OOD samples tend to have smaller SoftMax probabilities than in-distribution samples. In contrast,ViM incorporates class-agnostic information in the feature space and class-dependent information in thelogit space, which better reflects OOD-ness (Wang et al., 2022).However, ViM neglects the varianceof maximum logits in the calculation of the scaling parameter per model, weakening the importance ofinformation in the feature space in the final decision-making process. This limitation is effectively addressedby CAC loss, which forces known objects to form tight clusters in logit space. Our results show that CACalone provides limited improvements in open-set performance, with AUROC scores of CLS V. and C.C.Vshowing small differences.We can also observe that contrastive learning leads to overall improvementsin AUROC. By incorporating supervised contrastive learning, Cont.CLS surpasses the performance ofboth CLS V. and C.C.V. Furthermore, the complete S2CA framework also uses CAC loss to mitigateoverconfident classification and address the limitations associated with the ViM score.The cumulativeeffect of these components makes the S2CA framework superior to all baseline methods, underscoring itspotential as a comprehensive and effective approach to open-set recognition.",
  "CLS74.0775.6876.8877.9272.575.4CLS V.80.3876.5676.6877.4185.2777.39C.C.V.80.7176.1276.4477.5885.8377.56Cont. CLS 80.3377.5877.2378.5686.2180.04S2CA81.2379.6477.3979.2887.4881.94": "Following Sun et al. (2021), we experiment with different projector dimensions and temperatures for con-trastive learning. The results for the RGB image classifier trained with S2CA are shown in Tab. 4. As itcan be seen, the choice of hyperparameters has a minor impact on the performance of S2CA. Tab. 5 shows the performance of post-training OOD scores applied to the RGB image classifier trained withS2CA. ViM (Wang et al., 2022) has the best performance since it combines class-agnostic information in thefeature space and the class-dependent information in the logit space for OOD detection.",
  "Evaluation on Out-of-Distribution Benchmarks": "Our results generally support the hypothesis that the combination of contrastive learning and class anchorclustering provides a convincing solution to the problem of open set recognition in driving scenes. Similarfindings have been reported for general OOD benchmarks by another recent work, CIDER (Ming et al.,2023), which also adopts the concept of supervised contrastive learning and class anchor (or prototype). To evaluate the performance of S2CA on OOD benchmarks, we follow the experimental setup of CIDER(Ming et al., 2023), and use CIFAR100 (Krizhevsky et al., 2009) as the in-distribution dataset to trainS2CA with a ResNet-34 backbone (He et al., 2016). OOD test datasets include SVHN (Netzer et al., 2011),Places365 (Zhou et al., 2017), Textures (Cimpoi et al., 2014), LSUN (Yu et al., 2015), and iSUN (Xu et al.,2015). As shown in Tab. 6, S2CA achieves similar results to CIDER, with a slightly higher overall AUROCand a slightly worse FPR. When compared to other contrastive learning-based methods, such as ProxyAnchor(Kim et al., 2020), CSI (Tack et al., 2020), SSD+ (Sehwag et al., 2021), and KNN+ (Sun et al., 2022), S2CAdemonstrates the best overall performance on both metrics. While both CIDER and S2CA use contrastive learning and class anchors, they have subtle differences.S2CA places fixed class anchors in the logit space, as the ground truth label of known classes, whereasCIDER estimates the prototypes (anchors) directly from sample embeddings using EMA in the feature space.Furthermore, S2CA jointly optimizes SCL and CAC loss. In contrast, CIDER uses maximum likelihoodestimation (MLE) loss and dispersion loss to promote intra-class compactness and inter-class sparsity in thefeature space. Also S2CA uses the virtual logit as OOD score, taking advantage of information from bothfeature space and logit space, whereas CIDER adopts k-NN cosine similarity as the OOD score, which onlyuses information in the feature space.",
  "Theoretical Analysis": "Softmax probability as a baseline OOD detection method achieves modest success, because the neural net-work is less confident with unknown input. This observation supports previous results, such as (Hendrycks &Gimpel, 2017) and (Kang et al., 2024), which also show that the predictions of the neural network convergeto a constant value when inputs become more OOD. However, as shown by Pearce et al. (2021), since neural",
  "Conclusion": "We proposed an effective and adaptive OSR method based on contrastive class anchor learning. Our ex-perimental results show that the proposed method achieves state-of-the-art performance with different datamodalities and encoder networks. Future work will investigate failure detection, which focuses on detectingmisclassification, covariate shifts (corruption shift and domain shift), and new class shifts (Jaeger et al.,2023). In the future work, we will work on a unified failure detection method, which detects both misclas-sified known samples and out-of-distribution samples under various domains. For real-world application,S2CA classifier can be integrated into a two-step object detector to make a complete open-set object detec-tion pipeline, for different data modalities. For image input, S2CA can be applied to methods such as VOS(Du et al., 2022), which appends open-set classifier after Region Proposal Network (RPN) (Ren et al., 2015).For LiDAR input, object proposals can be obtained via ground segmentation and clustering (Nunes et al.,2022).",
  "Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun.Vision meets robotics: The kittidataset. International Journal of Robotics Research (IJRR), 2013": "Chuanxing Geng, Sheng-Jun Huang, and Songcan Chen. Recent advances in open set recognition: A survey.IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(10):36143631, 2021. doi: 10.1109/TPAMI.2020.2981604. Alexander Gillert and Uwe Freiherr von Lukas.Towards combined open set recognition and out-of-distribution detection for fine-grained classification. In 16th International Joint Conference on ComputerVision, 2021.",
  "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. InProceedings of the IEEE conference on computer vision and pattern recognition, pp. 770778, 2016": "Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.Momentum contrast for unsuper-vised visual representation learning. In 2020 IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pp. 97269735, 2020. doi: 10.1109/CVPR42600.2020.00975. Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples inneural networks. In International Conference on Learning Representations (ICLR), pp. 10101019, 2017. Dan Hendrycks, Steven Basart, Mantas Mazeika, Mohammadreza Mostajabi, Jacob Steinhardt, and DawnSong. Scaling out-of-distribution detection for real-world settings. In International Conference on MachineLearning (ICML), 2022. Paul F Jaeger, Carsten Tim Lth, Lukas Klein, and Till J. Bungert. A call to reflect on evaluation prac-tices for failure detection in image classification. In The Eleventh International Conference on LearningRepresentations, 2023. URL",
  "Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. In ICLR, 2017": "Daniel Maturana and Sebastian Scherer. Voxnet: A 3d convolutional neural network for real-time objectrecognition. In 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp.922928, 2015. doi: 10.1109/IROS.2015.7353481. Claudio Michaelis, Benjamin Mitzkus, Robert Geirhos, Evgenia Rusak, Oliver Bringmann, Alexander S.Ecker, Matthias Bethge, and Wieland Brendel. Benchmarking robustness in object detection: Autonomousdriving when winter is coming. arXiv preprint arXiv:1907.07484, 2019. Dimity Miller, Niko Snderhauf, Michael Milford, and Feras Dayoub. Class anchor clustering: A loss fordistance-based open set recognition. In 2021 IEEE Winter Conference on Applications of Computer Vision(WACV), pp. 35693577, 2021. doi: 10.1109/WACV48630.2021.00361. Yifei Ming, Yiyou Sun, Ousmane Dia, and Yixuan Li. How to exploit hyperspherical embeddings for out-of-distribution detection? In The Eleventh International Conference on Learning Representations, 2023.URL",
  "Tim Pearce, Alexandra Brintrup, and Jun Zhu. Understanding softmax confidence and uncertainty, 2021": "Pramuditha Perera, Vlad I. Morariu, Rajiv Jain, Varun Manjunatha, Curtis Wigington, Vicente Ordonez,and Vishal M. Patel. Generative-discriminative feature representations for open-set recognition. In CVPR,pp. 1181111820, 2020. doi: 10.1109/CVPR42600.2020.01183. Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.Faster r-cnn:Towards real-time ob-ject detection with region proposal networks.In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama,and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 28. CurranAssociates,Inc.,2015.URL",
  "Yiyou Sun, Yifei Ming, Xiaojin Zhu, and Yixuan Li. Out-of-distribution detection with deep nearest neigh-bors. ICML, 2022": "Jihoon Tack, Sangwoo Mo, Jongheon Jeong, and Jinwoo Shin. Csi: Novelty detection via contrastive learningon distributionally shifted instances. In Advances in Neural Information Processing Systems, 2020. Haoqi Wang, Zhizhong Li, Litong Feng, and Wayne Zhang. Vim: Out-of-distribution with virtual-logitmatching. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp.49114920, 2022. doi: 10.1109/CVPR52688.2022.00487. Peng Wang, Kai Han, Xiu-Shen Wei, Lei Zhang, and Lei Wang. Contrastive learning based hybrid networksfor long-tailed image classification.In 2021 IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pp. 943952, 2021. doi: 10.1109/CVPR46437.2021.00100. Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment anduniformity on the hypersphere. In Proceedings of the 37th International Conference on Machine Learning,ICML20. JMLR.org, 2020. Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, and Justin M. Solomon.Dynamic graph cnn for learning on point clouds. ACM Trans. Graph., 38(5), oct 2019. ISSN 0730-0301.doi: 10.1145/3326362. URL Pingmei Xu, Krista A Ehinger, Yinda Zhang, Adam Finkelstein, Sanjeev R Kulkarni, and Jianxiong Xiao.Turkergaze: Crowdsourcing saliency with webcam based eye tracking. arXiv preprint arXiv:1504.06755,2015. Ryota Yoshihashi, Wen Shao, Rei Kawakami, Shaodi You, Makoto Iida, and Takeshi Naemura. Classification-reconstruction learning for open-set recognition. In 2019 IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR), pp. 40114020, 2019. doi: 10.1109/CVPR.2019.00414. Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun: Con-struction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprintarXiv:1506.03365, 2015."
}