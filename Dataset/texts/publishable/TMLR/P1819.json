{
  "Abstract": "On tabular data, a significant body of literature has shown that current deep learning(DL) models perform at best similarly to Gradient Boosted Decision Trees (GBDTs), whilesignificantly underperforming them on outlier data Gorishniy et al. (2021); Rubachev et al.(2022); McElfresh et al. (2023). However, these works often study problem settings whichmay not fully capture the complexities of real-world scenarios. We identify a natural tabulardata setting where DL models can outperform GBDTs: tabular Learning-to-Rank (LTR)under label scarcity. Tabular LTR applications, including search and recommendation, oftenhave an abundance of unlabeled data, and scarce labeled data. We show that DL rankerscan utilize unsupervised pretraining to exploit this unlabeled data. In extensive experimentsover both public and proprietary datasets, we show that pretrained DL rankers consistentlyoutperform GBDT rankers on ranking metricssometimes by as much as 38%both overalland on outliers.",
  "Introduction": "The learning-to-rank (LTR) problem aims to train a model to rank a set of items according to their relevanceor user preference (Liu, 2009). An LTR model is typically trained on a dataset of queries and associated querygroups (i.e., a set of potentially relevant documents or items per query), as well as an associated (generallyincomplete) ground truth ranking of the items in the query group. The model is trained to output the optimalranking of documents or items in a query group, given a query. LTR is a core ML technology in many realworld applicationsmost notably in search contexts including Bing web search (Qin & Liu, 2013), Amazon",
  "Published in Transactions on Machine Learning Research (11/2024)": "Fraction of training qgs labeled 0.26 0.28 0.30 0.32 0.34 0.36 0.38 0.40 0.42 Outlier-NDCG MSLRWEB30K GBDTNo Pretrain DLPretrained DL Fraction of training qgs labeled 0.46 0.48 0.50 0.52 0.54 0.56 0.58 0.60 Outlier-NDCG Yahoo Set1 GBDTNo Pretrain DLPretrained DL Fraction of training qgs labeled 0.60 0.62 0.64 0.66 0.68 0.70 0.72 0.74 Outlier-NDCG Istella_S GBDTNo Pretrain DLPretrained DL : Simulated crowdsourcing of labels: We compare Outlier-NDCG () pretrained rankers,non-pretrained DL rankers, and GBDT rankers as we change the percentage of training query groups thatare labeled. To the left of the black dotted line, pretrained rankers perform the best. Points are averages overthree trials. To the left of the black dotted vertical line, pretrained rankers are (1) significantly better onoutliers than GBDTs at the p = 0.05 level using a one-sided t-test, and (2) on average better on outliers thanall other non-pretrained methods. : Train and validation dataset statistics on MSLRWEB30K, Yahoo Set1, and Istella_S after binarylabel generation with target = 4.25. The test set is left untouched. Labeled QGs are those query groups thathave at least one item with y = 1.",
  "Learning-To-Rank problem and its metrics": "In the LTR problem, samples are query groups (QGs) consisting of a query (e.g. shopping query) and Lpotentially relevant items (e.g. products) for this query. Although an LTR dataset contains multiple QGs,for the sake of simplicity, the notation in this section only handles a single QG. The i-th relevant item inthis QG is represented by a feature vector xi Rd, which captures information about this query-item pair,subsuming any query related features. If and only if the QG is labeled, such as the ones used in supervisedtraining or testing, we also have an associated scalar relevance label yi, which could be a binary, ordinal, orreal-valued measurement of the relevance of this item for this query (Qin et al., 2021). The objective is tolearn a function that ranks these L items such that the items with highest true relevance are ranked at thetop. Most LTR algorithms formulate the problem as learning a scoring function f : Rd R that maps the featurevector associated with each item to a score, and then ranking the items by sorting the scores in descendingorder. To measure the quality of a ranking induced by our scoring function f on a labeled test QG, acommonly-used (per-QG) metric is NDCG (normalized cumulative discounted gain) ():",
  "DCG(, {yi}Li=1) ,(1)": "where f : [L] [L] ([L] {1, . . . , L}) is a ranking of the L elements induced by the scoring function f on{xi}Li=1 (i.e. (i) is the rank of the i-th item), while is the ideal ranking induced by the true relevancelabels {yi}Li=1, and discounted cumulative gain (DCG) is defined as DCG(, {yi}Li=1) = Li=12yi1 log2(1+(i)).Here, () indicates that larger metric values are preferred. Note that for summarizing this metric over the fulltest set, we simply take the average across all the QGs in the set. Typically, a truncated version of NDCG isused that only considers the top-k ranked items, denoted as NDCG@k. In the rest of our paper, we will referto NDCG@5 as NDCG; this will be our evaluation metric. We consider a natural label-scarce LTR setting with large amounts of unlabeled data and a limited supply oflabels for a subset of this data. The labels that do exist can arise from multiple sources, such as crowdsourced",
  "Outlier-NDCG for outlier performance evaluation": "Motivated by a recent observation that GBDTs outperform DL models in datasets with irregularities McElfreshet al. (2023), we also study their performance on outliers. In interactive ML systems like search, performingwell on outlier queries is particularly valuable as it empowers users to search for more outlier queries, whichin turn allows the modeler to collect more data and improve the model. To this end, we evaluate the outlierperformance by computing the average NDCG on outlier queries groups and refer to it as Outlier-NDCGfor brevity. In practice, some outlier queries may already be known, and the modeler can define the outlierdatasets accordingly, like in our proprietary dataset. For example, since industry data pipelines often havemissing data/features one could identify samples with missing features as outliers. When outliers are unknown,detecting them (especially in higher-dimensional data) is challenging. Hence, we create a heuristic algorithmby assuming outliers are rare values that are separated from the bulk of the data. Using it we systematicallyselected outlier query groups for the public datasets that we use. More details about the outliers are providedin Appendix A.2.2. Separately measuring the performance on the outliers is justified by our later observationthat performance on the full dataset and the outliers can significantly differ (Tables 2 and 3).",
  "Design: Unsupervised pretraining for LTR": "Qualitatively, the ranking relevance of tabular records to a query is typically determined by a combination offields. For example, if a person searches for a product (say a pair of sandals), we can think of the fields in theLTR problem as representing properties of the results, such as material, color, and price. Qualitatively, weexpect that if a pair of records are the same except for small differences between fields (e.g., missing a field),then the two products should generally have similar rankings in the LTR problem. In this section, we discusshow existing methods fail to capture this qualitative intuition. Semi-supervised learning for GBDTs: While to the best of our knowledge there are no methodsto perform unsupervised pretraining in GBDTs, one can use semi-supervised methods like consistencyregularization (Jeong & Shin, 2020; Sajjadi et al., 2016; Miyato et al., 2018; Oliver et al., 2018; Berthelotet al., 2019), pseudo-labeling/self-training (Rizve et al., 2021; Pseudo-Label, 2013; Shi et al., 2018; Yarowsky,1995; McClosky et al., 2006), and PCA (Duh & Kirchhoff, 2008) to address limited size of labeled data.In consistency regularization, one increases the size of the training set by using data augmentations. Inpseudo-labeling/self-training, a model trained on the available labeled data is used to label unlabeled data.Then, the final model is trained on the resulting labeled dataset. In PCA, one projects the features alongdirections of maximum variability in the union of labeled and unlabeled datasets. Since prior work has foundthat consistency regularization decreases the performance of GBDTs (Rubachev et al., 2022), we evaluateonly pseudo-labeling and PCA. None of these methods explicitly ensures that semantically similar recordshave similar representations in the learned models. Pretraining methods for tabular data: To address this gap, there have been many unsupervisedpretraining methods proposed for tabular data, which can be applied to the tabular LTR setting. In thispaper, we will evaluate (1) SCARF (Bahri et al., 2021) and (2) DACL+ (Verma et al., 2021) which arecontrastive learning methods (Chen et al., 2020), (3) VIME-self (Yoon et al., 2020) and (4) SubTab (Ucaret al., 2021) which are autoencoder based methods. The four most commonly-evaluated baselines in thepretraining literature for tabular data are Ucar et al. (2021); Hajiramezanali et al. (2022); Levin et al.(2022); Majmundar et al. (2022). Our results suggest that many of these methods outperform GBDTs in thelabel-scarce regime (.1), in accordance with the main theme of this paper. However, the consistentlybest-performing methods in our experiments were more basic, domain-agnostic pretraining methods, whichcan be viewed as more primitive building blocks of some of the above methods. Surprisingly, they use lessdomain knowledge of tabular data than these more sophisticated methods.",
  "{(q, i, a) = (q, i, a)} exp(cos(z(a)q,i , z(a)q,i)/)(2)": "a = 1 a, is the temperature parameter, Lq is the number of items in q-th query group, andcos(z, z) = z, z/zz denotes the cosine similarity. After pretraining, the encoder h is used in down-stream applications. SimCLR achieves superior performance in many domains (Chen & He, 2021; Wang et al.,2022b;a). It is known that contrasting a pair of data samples which are hard to distinguish from each other,called hard negatives can help an encoder learn good representations (Robinson et al., 2020; Oh Song et al.,2016; Schroff et al., 2015; Harwood et al., 2017; Wu et al., 2017; Ge, 2018; Suh et al., 2019). SimCLR simplycontrasts all pairs of data samples in the batch (including from other query groups) with the assumption thata large enough batch is likely to contain a hard negative. SimSiam (Chen & He, 2021): Similar to SimCLR, for each data point xq,i, this method produces stochastically-augmented positive pairs and their projected representations. However, we pass these representations furtherthrough a predictor pred(), to get p(a)q,i = pred(z(a)q,i ) for a = 0, 1. Finally, we maximize the similarity between the projected and predicted representations: cos(p(0)q,i , sg(z(1)q,i )) + cos(p(1)q,i , sg(z(0)q,i )), where sg is astop-gradient. Unlike SimCLR, there are no negatives, i.e., the loss function does not push the representationof an augmented sample away from that of other samples augmentations. The asymmetry in the loss due tothe stop gradient and the predictor prevents representation collapse to make negatives unnecessary Tian et al.(2021); Zhang et al. (2021); Wang et al. (2022b). If the number of items in each query group is L, then thetime/space complexity for SimSiam is only O(BL) per batch, whereas it is O(B2L2) for SimCLR. Therefore,SimSiam is more efficient and it can scale to larger batchsizes and data. Pretraining method for LTR: Motivated by (a) SimCLRs high complexity and (b) the efficacy ofcontrasting with hard negatives, we additionally propose SimCLR-Rank, an LTR-specific alternative. Recallthat the SimCLR loss for an item in equation 2 uses all the other items in the batch as negatives. Goodpretraining strategies usually exploit the structure of the data, e.g. text sequences motivates masked tokenprediction Devlin et al. (2018). Studying the query group structure of LTR data, we notice that high-qualityhard negatives are freely available as the other items from the same query group. These items are retrievedand deemed potentially similarly relevant for this query by an upstream retrieval model, making these itemsharder negatives than other items in the batch. So, we propose SimCLR-Rank, which modifies SimCLR tocontrast only with the other items in the same query group, as illustrated in . Formally, we modifythe per-augmentation loss (a)q,i as:",
  "Contrastive Learning": ": In SimCLR and its variants, positive pairs, or augmented versions of the same data sample, aretrained to have similar embeddings. Positive-negative pairs, or augmented samples originating from twodifferent data points or classes, are trained to have distant embeddings. In vanilla SimCLR, each positive pairis contrasted with all other items in the batch, denoted by the data points contained in the brown dashedline above. In SimCLR-Rank, each positive pair is contrasted with only the items in the same Query Group(QG), denoted by the data points inside the red solid line above.: Pretraining methods: A comparison across unsupervised pretraining methods on NDCG,averaged over 3 trials. Here only 0.1% of the query groups have labels for finetuning. Bold numbersdenote the best in a column, and underlined numbers are within the margin of error of the best. Given thatSCARF/DACL+/SimCLR are too slow to scale to large LTR datasets (for example on the industrial-scaleproprietary online shopping dataset), we find that SimCLR-Rank and SimSiam are the best pretrainingmethods for tabular LTR.",
  "MethodMSLR ()Yahoo Set1 ()Istella ()": "Supervised GBDT0.2801 0.00020.5083 0.01970.5450 0.0000Semi-supervised GBDT0.2839 0.00040.5061 0.02670.4656 0.0310SimCLR-Rank + GBDT0.3165 0.00500.5504 0.01350.5397 0.0098SimSiam + GBDT0.3158 0.00300.5620 0.01670.5297 0.0051 SCARF (Bahri et al., 2021)0.3807 0.00160.5884 0.01290.5542 0.0024DACL+ (Verma et al., 2021)0.3833 0.00370.5887 0.00120.5626 0.0024VIME-self (Yoon et al., 2020)0.3834 0.00110.5839 0.00580.5514 0.0025SubTab (Ucar et al., 2021)0.3748 0.00250.5814 0.00620.5082 0.0050SAINT (Somepalli et al., 2021)0.3355 0.00430.5890 0.00750.5560 0.0066SimCLR (Chen et al., 2020)0.3827 0.00270.5837 0.00930.5602 0.0055",
  "Public datasets results": "For the public datasets (MSLRWEB30K, Yahoo Set1, Istella_S; details given in .1), we simulatelabel scarcity occurring in two scenarios: (1) crowdsourcing of labels for a limited number of query groups,and (2) using scarce implicit binary user feedback (e.g. clicks) as labels. 1We emphasize that we do not prescribe any specific deep learning architecture choice; the choice for the best DL modelmay depend on the specific LTR task. Our proposed workflow of pretraining deep models in LTR is agnostic to the specificchoice of DL architecture. Prior work has proposed DL architectures designed for click-through rate (CTR)/LTR based onfactorization machines (Rendle, 2010), such as DeepFM and DCNv2 (Guo et al., 2017; Wang et al., 2021), which we compareto in .1.2 and .1.2 against our pretrained models. Note that it is unclear currently how to do unsupervisedpretraining using these CTR/LTR specific architectures at the moment.2Code is provided at",
  "We simulate a setting where labels are crowdsourced for only some query groups due to limited resources": "Dataset. For each of the three public datasets, we vary the fraction of labeled query groups in the trainingset in {0.001, 0.002, 0.005, 0.1, 0.5, 1.0}. Note that within each labeled query group all items are labeled. Methodology. We repeat the methodology from .1 for the GBDT and the SimCLR-Rank andSimSiam pretraining methods. For each dataset and labeling fraction, we report the test metric of the bestGBDT ranker among semi-supervised GBDTs and supervised GBDTs under GBDT. Similarly, under nopretrain DL we report the best supervised DL model amongst tabular ResNet (Gorishniy et al., 2021),DeepFM (Guo et al., 2017), and DCNv2 (Wang et al., 2021) under the name no pretrain DL. Finally, wereport the best pretrained ranker amongst SimCLR-Rank and SimSiam under the name pretrained DL.Pretrained DL rankers use tabular ResNet (see Appendix A.2.3 for more details). During hyperparametertuning, we tune for NDCG and report the resulting test NDCG and test Outlier-NDCG. Results. We compare pretrained rankers, non-pretrained DL rankers, and GBDTs on public datasets in. The results for Outlier-NDCG are provided in in Appendix A.3.4. We find that pretrainedrankers outperform non-pretrained methods (including GBDTs) on NDCG and Outlier-NDCG across allpublic datasets, up to a dataset-dependent fraction of QGs labeled, as shown in (for NDCG) and (for Outlier-NDCG). We also note that GBDTs have better NDCG than non-pretrained DL modelsin most of these regimes. This shows that there exist scenarios with a limited supply of labeled query groups,where self-supervised pretraining is the factor that allows deep learning models to outperform GBDTs. Weprovide a detailed comparison between semi-supervised and supervised GBDT models in Appendix A.4.",
  "Simulated implicit binary user feedback": "Here we simulate a scenario when users of an LTR system provide binary implicit feedback for the items, likethe clicking of an advertisement on a webpage. This kind of feedback is usually infrequent in an LTR system,as it is the result of a user choosing to spend more time or resources on an item in a presented list. Therefore,most query groups will not have any labels, and even labeled query groups will only have a few positivelylabeled items. On the other hand, such labels are often cheaper to collect than crowdsourced labels, so theyare used in many industry use cases. Dataset.To simulate this scenario, we follow the methodology from Yang et al. (2022), to generateindependent stochastic binary labels for each item from its true relevance label for training and validationsets of each of the public datasets. Note that we still use the true (latent) relevance labels in the test set forevaluation. This models a scenario where we observe binary labels that are noisy observations of a true latentlabel, but the task is to use these noisy labels to learn to rank according to the true labels. The details ofthis binarizing transformation are provided in Appendix A.2.4. In this transformation, a parameter, target,implicitly controls how sparse the binary labels are (larger is more sparse). We select target = 4.5, to obtain8.9%, 5.3%, and 25.2% labeled query groups which contain at least one positive label in MSLR, Yahoo Set1,Istella_S datasets, respectively. We present detailed data statistics in of Appendix. Methodology. We reuse the methodology of .1.1, except we use a linear scoring head for pretrainedmodels to improve stability and performance (see Appendix A.2.3). Similar to .1.1, we report thebest of multiple models for each method and dataset (see .1.1 for details). Results. In , we present the test NDCG and Outlier-NDCG metrics of the models under thissetting. We see that pretrained rankers significantly outperform GBDTs and non-pretrained DL rankers inthis simulated binary user feedback setting with up to, even with 25.2% labeled query groups (Istella_S).For MSLRWEB30K and Yahoo Set1 datasets, the second best model is the GBDT, and the non-pretrainedDL ranker has a much worse Outlier-NDCG. This aligns with recent observations that GBDTs performbetter than DL models in supervised training with datasets containing irregularities McElfresh et al. (2023).However, pretraining helps DL rankers close this gap, and at times beat GBDTs in outlier performance.We also present results for target {4.25, 5.1} in Appendix A.3.5. There, we find that making the labelssparser (target = 5.1) increases the relative improvement of pretrained DL rankers, while in a less sparse",
  "In this section we test whether the results from the simulated label scarce settings above, especially .1.2,translate to a large-scale real-world LTR problem": "Dataset. Our proprietary dataset is derived from the online shopping logs of a large online retailer, numberingin the tens of millions of query groups. In this dataset, query groups consist of items presented to a shopperif they enter a search query. We assign each item a purchase label: label is 1 if the shopper purchased theitem, or 0 if the shopper did not purchase the item. Hence, this is a real-world instance of the implicit binaryuser feedback setting simulated in .1.2. Further, only about 10% of the query groups have itemswith non-zero labels. See Appendix A.2.1 for more dataset details. Methodology. We compare the performance of pretrained DL modelsSimCLR-Rank and SimSiamwithGBDT and non-pretrained DL models. SimSiam and SimCLR-Rank are first pretrained on all the querygroups before being finetuned on the labeled query groups with some non-zero purchase labels. The GBDTand the non-pretrained models are only trained on the labeled data. More details about these models andtraining strategies is given in Appendix A.2.5. Results. Our results are summarized in and all numbers are given as relative percentage improvements(%) over the GBDT ranker. First, note that all the DL models outperform GBDT models in terms ofNDCG. Amongst them, the pretrained model using SimSiam performs the best with a gain of over 5.5%. Thisis substantial, given that in large-scale industry datasets, even a 2% improvement in NDCG is consideredsignificant. In terms of Outlier-NDCG, the story is very different. Here, the non-pretrained DL model and",
  "Ablations: Choices for pretrained models in LTR": "Our experiments suggest that the best pretraining and finetuning strategies for the LTR problem are differentfrom the known best practices for image or text tasks. We make a few observations. (1) SimSiam andSimCLR-Rank perform significantly differently depending on the dataset. (2) Combining models pretrainedby SimCLR-Rank and SimSiam can produce a model that is competitive against SimCLR-Rank and SimSiamacross all datasets, producing a unified approach that may be performant across many settings. (3) Fullfinetuning of the model performs much better than linear probing, which only finetunes a linear layer overfrozen pretrained embeddings. In the remainder of this section, we elaborate on these findings in more detail. (1) SimCLR-Rank and SimSiam perform substantially differently depending on the dataset.SimSiam and SimCLR-Rank perform significantly differently depending on the dataset, which is differentfrom the observations of Chen & He (2021), who proposed SimSiam and found that it performs similarly toSimCLR derivatives. We next compare SimCLR-Rank with SimSiam at different levels of label scarcity. Fraction of training qgs labeled 0.36 0.38 0.40 0.42 0.44 0.46 NDCG MSLRWEB30K SimCLR-Rank + SimSiamSimCLR-RankSimSiam Fraction of training qgs labeled 0.56 0.58 0.60 0.62 0.64 0.66 NDCG Yahoo Set1 SimCLR-Rank + SimSiamSimCLR-RankSimSiam Fraction of training qgs labeled 0.54 0.56 0.58 0.60 0.62 0.64 0.66 NDCG Istella_S SimCLR-Rank + SimSiamSimCLR-RankSimSiam : Comparison of SimCLR-Rank and SimSiam pretraining strategies on three public datasets withvarying fraction of labeled query groups. Neither method consistently dominate the other in all the datasets.However, by combining SimCLR-Rank and SimSiam encoders, we can produce models that are competitiveacross all datasets. Data points are averages over 3 trials. Methodology. The dataset and methodology follows that of .1.1, except we let (1) SimCLR-Rank usethe Gaussian augmentation with scale 1, and (2) SimSiam use the zeroing augmentation with probability 0.1.These augmentations generally perform well for their respective methods. Experiments illustrating the effectof each augmentation method are included in Appendix A.3.6. Results. In , SimCLR-Rank performs better on MSLRWEB30K and Istella_S, while SimSiamperforms better on Yahoo Set1 when labels are scarce. shows the t-SNE projections of sub-sampleditem embeddings learned by SimCLR-Rank, SimSiam, and pre-final layer of a fully supervised model forYahoo Set1. SimSiam exhibit qualitatively more clustered embeddings than SimCLR-Rank, particularly forhigh relevance labels. Similar t-SNE plots for other datasets are in Appendix A.3.3. (2) Combining models pretrained by SimCLR-Rank and SimSiam produces a model that hasthe strengths of both. To make steps towards a single recommendation for unsupervised pretraining inLTR, we unify SimCLR-Rank and SimSiam into a single method SimCLR-Rank + SimSiam. Methodology. To produce a SimCLR-Rank + SimSiam model, we finetune a linear layer over the embeddingsfrom a finetuned SimCLR-Rank concatenated with the embeddings from a finetuned SimCLR-Rank model.More details about how we combine the models is given in Appendix A.3.7.",
  "Conclusion": "We study the learning-to-rank (LTR) problem with tabular data under a scarcity of labeled dataa commonscenario in real-world practical LTR systems. Prior works on supervised learning with tabular data haveshown that GBDTs outperform deep learning (DL) models, especially on datasets with outliers. In this paper,we find that in the label-scarce setting, DL pretraining methods can exploit available unlabeled data to obtainnew state-of-the art performance. Through experiments with public LTR datasets and a real-world large-scale",
  "LP0.1803 0.00330.2304 0.0332MP0.1749 0.00230.2969 0.0009FF0.3149 0.01190.2892 0.0025": "online shopping dataset, we show that pretraining allows DL rankers to outperform GBDT rankers, especiallyon outlier data. Finally, we provide guidelines for pretraining on LTR datasets. There remain several openquestions around pretraining for tabular and LTR data. It is still unknown whether we can achieve knowledgetransfer across different tabular datasets, as is commonly done in image and text domains. It will also beuseful to investigate LTR-specific pretraining methods which can further improve the ranking performance.",
  "GF and CH gratefully acknowledge the support of Google, Intel, the Sloan Foundation, and Bosch": "Qingyao Ai, Keping Bi, Jiafeng Guo, and W Bruce Croft. Learning a deep listwise context model for rankingrefinement. In The 41st international ACM SIGIR conference on research & development in informationretrieval, pp. 135144, 2018. Qingyao Ai, Xuanhui Wang, Sebastian Bruch, Nadav Golbandi, Michael Bendersky, and Marc Najork.Learning groupwise multivariate scoring functions using deep neural networks. In Proceedings of the 2019ACM SIGIR international conference on theory of information retrieval, pp. 8592, 2019.",
  "Dara Bahri, Heinrich Jiang, Yi Tay, and Donald Metzler. Scarf: Self-supervised contrastive learning usingrandom feature corruption. arXiv preprint arXiv:2106.15147, 2021": "David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raffel.Mixmatch: A holistic approach to semi-supervised learning. Advances in neural information processingsystems, 32, 2019. Vadim Borisov, Tobias Leemann, Kathrin Seler, Johannes Haug, Martin Pawelczyk, and Gjergji Kasneci.Deep neural networks and tabular data: A survey. IEEE transactions on neural networks and learningsystems, 2022.",
  "Amanda Bower, Hamid Eftekhari, Mikhail Yurochkin, and Yuekai Sun. Individually fair ranking. arXivpreprint arXiv:2103.11023, 2021": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.Advances in neural information processing systems, 33:18771901, 2020. Sebastian Bruch, Masrour Zoghi, Michael Bendersky, and Marc Najork. Revisiting approximate metricoptimization in the age of deep neural networks. In Proceedings of the 42nd international ACM SIGIRconference on research and development in information retrieval, pp. 12411244, 2019.",
  "Christopher JC Burges. From ranknet to lambdarank to lambdamart: An overview. Learning, 11(23-581):81,2010": "Shaofeng Cai, Kaiping Zheng, Gang Chen, HV Jagadish, Beng Chin Ooi, and Meihui Zhang. Arm-net:Adaptive relation modeling network for structured data. In Proceedings of the 2021 International Conferenceon Management of Data, pp. 207220, 2021. Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. Learning to rank: from pairwise approach tolistwise approach. In Proceedings of the 24th international conference on Machine learning, pp. 129136,2007.",
  "Nikita Kitaev, Steven Cao, and Dan Klein.Multilingual constituency parsing with self-attention andpre-training. arXiv preprint arXiv:1812.11760, 2018": "Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large languagemodels are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022. Ananya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy Liang. Fine-tuning candistort pretrained features and underperform out-of-distribution. In International Conference on LearningRepresentations, 2022. Branislav Kveton, Ofer Meshi, Masrour Zoghi, and Zhen Qin. On the value of prior in online learning torank. In International Conference on Artificial Intelligence and Statistics, pp. 68806892. PMLR, 2022.",
  "Kyungmin Lee and Jinwoo Shin. Rnyicl: Contrastive representation learning with skew rnyi divergence.Advances in Neural Information Processing Systems, 35:64636477, 2022": "Roman Levin, Valeriia Cherepanova, Avi Schwarzschild, Arpit Bansal, C Bayan Bruss, Tom Goldstein,Andrew Gordon Wilson, and Micah Goldblum. Transfer learning with deep tabular models. arXiv preprintarXiv:2206.15306, 2022. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, HeinrichKttler, Mike Lewis, Wen-tau Yih, Tim Rocktschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:94599474, 2020.",
  "Bhaskar Mitra, Nick Craswell, et al. An introduction to neural information retrieval. Foundations andTrends in Information Retrieval, 13(1):1126, 2018": "Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a regular-ization method for supervised and semi-supervised learning. IEEE transactions on pattern analysis andmachine intelligence, 41(8):19791993, 2018. Jaehyun Nam, Woomin Song, Seong Hyeon Park, Jihoon Tack, Sukmin Yun, Jaehyung Kim, and JinwooShin. Semi-supervised tabular classification via in-context learning of large language models. In Workshopon Efficient Systems for Foundation Models@ ICML2023, 2023a.",
  "Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy Lin. Multi-stage document ranking with bert.arXiv preprint arXiv:1910.14424, 2019": "Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. Deep metric learning via lifted structuredfeature embedding. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.40044012, 2016. Avital Oliver, Augustus Odena, Colin A Raffel, Ekin Dogus Cubuk, and Ian Goodfellow. Realistic evaluationof deep semi-supervised learning algorithms. Advances in neural information processing systems, 31, 2018.",
  "Feifei Pan, Mustafa Canim, Michael Glass, Alfio Gliozzo, and James Hendler. End-to-end table questionanswering via retrieval-augmented generation. arXiv preprint arXiv:2203.16714, 2022": "Liang Pang, Jun Xu, Qingyao Ai, Yanyan Lan, Xueqi Cheng, and Jirong Wen.Setrank: Learning apermutation-invariant ranking model for information retrieval. In Proceedings of the 43rd internationalACM SIGIR conference on research and development in information retrieval, pp. 499508, 2020. Gustavo Penha, Arthur Cmara, and Claudia Hauff. Evaluating the robustness of retrieval pipelines withquery variation generators. In European conference on information retrieval, pp. 397412. Springer, 2022.",
  "Ivan Rubachev, Artem Alekberov, Yury Gorishniy, and Artem Babenko. Revisiting pretraining objectives fortabular deep learning. arXiv preprint arXiv:2207.03208, 2022": "Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transformations andperturbations for deep semi-supervised learning. Advances in neural information processing systems, 29,2016. Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face recognitionand clustering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.815823, 2015. Maximilian G Schuh, Davide Boldini, and Stephan A Sieber. Twinbooster: Synergising large language modelswith barlow twins and gradient boosting for enhanced molecular property prediction. arXiv preprintarXiv:2401.04478, 2024.",
  "Ravid Shwartz-Ziv and Amitai Armon. Tabular data: Deep learning is not all you need. Information Fusion,81:8490, 2022": "Gowthami Somepalli, Micah Goldblum, Avi Schwarzschild, C Bayan Bruss, and Tom Goldstein. Saint:Improved neural networks for tabular data via row attention and contrastive pre-training. arXiv preprintarXiv:2106.01342, 2021. Yangqiu Song and Dan Roth. Unsupervised sparse vector densification for short text similarity. In Proceedingsof the 2015 conference of the North American chapter of the association for computational linguistics:human language technologies, pp. 12751280, 2015. Yumin Suh, Bohyung Han, Wonsik Kim, and Kyoung Mu Lee. Stochastic class-based hard example miningfor deep metric learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pp. 72517259, 2019. Yi Sui, Tongzi Wu, Jesse Cresswell, Ga Wu, George Stein, Xiaoshi Huang, Xiaochen Zhang, Maksims Volkovs,et al. Self-supervised representation learning from random data projectors. arXiv preprint arXiv:2310.07756,2023. Baohua Sun, Lin Yang, Wenhan Zhang, Michael Lin, Patrick Dong, Charles Young, and Jason Dong. Supertml:Two-dimensional word embedding for the precognition on structured tabular data. In Proceedings of theIEEE/CVF conference on computer vision and pattern recognition workshops, pp. 00, 2019.",
  "Yuandong Tian, Xinlei Chen, and Surya Ganguli. Understanding self-supervised learning dynamics withoutcontrastive pairs. In International Conference on Machine Learning, pp. 1026810278. PMLR, 2021": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation andfine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Talip Ucar, Ehsan Hajiramezanali, and Lindsay Edwards. Subtab: Subsetting features of tabular data for self-supervised representation learning. Advances in Neural Information Processing Systems, 34:1885318865,2021.",
  "Zifeng Wang and Jimeng Sun. Transtab: Learning transferable tabular transformers across tables. Advancesin Neural Information Processing Systems, 35:29022915, 2022": "Chao-Yuan Wu, R Manmatha, Alexander J Smola, and Philipp Krahenbuhl. Sampling matters in deepembedding learning. In Proceedings of the IEEE international conference on computer vision, pp. 28402848,2017. Chen Wu, Ruqing Zhang, Jiafeng Guo, Wei Chen, Yixing Fan, Maarten de Rijke, and Xueqi Cheng. Certifiedrobustness to word substitution ranking attack for neural ranking models. In Proceedings of the 31st ACMInternational Conference on Information & Knowledge Management, pp. 21282137, 2022a.",
  "Chen Wu, Ruqing Zhang, Jiafeng Guo, Yixing Fan, and Xueqi Cheng. Are neural ranking models robust?ACM Transactions on Information Systems, 41(2):136, 2022b": "Chen Wu, Ruqing Zhang, Jiafeng Guo, Maarten De Rijke, Yixing Fan, and Xueqi Cheng. Prada: practicalblack-box adversarial attacks against neural ranking models. ACM Transactions on Information Systems,41(4):127, 2023. Fen Xia, Tie-Yan Liu, Jue Wang, Wensheng Zhang, and Hang Li. Listwise approach to learning to rank: theoryand algorithm. In Proceedings of the 25th international conference on Machine learning, pp. 11921199,2008. Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student improvesimagenet classification. In Proceedings of the IEEE/CVF conference on computer vision and patternrecognition, pp. 1068710698, 2020. Enqiang Xu, Yiming Qiu, Junyang Bai, Ping Zhang, Dadong Miao, Songlin Wang, Guoyu Tang, Lin Liu, andMingMing Li. Optimizing e-commerce search: Toward a generalizable and rank-consistent pre-rankingmodel. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development inInformation Retrieval, SIGIR 24, pp. 28752879, New York, NY, USA, 2024. Association for ComputingMachinery. ISBN 9798400704314. doi: 10.1145/3626772.3661343. URL",
  "Hai-Tao Yu. Pt-ranking: A benchmarking platform for neural learning-to-rank, 2020": "Chaoning Zhang, Kang Zhang, Chenshuang Zhang, Trung X Pham, Chang D Yoo, and In So Kweon. How doessimsiam avoid collapse without negative samples? a unified understanding with self-supervised contrastivelearning. In International Conference on Learning Representations, 2021. Peng Zhang, Dawei Song, Jun Wang, and Yuexian Hou. Bias-variance decomposition of ir evaluation. InProceedings of the 36th international ACM SIGIR conference on Research and development in informationretrieval, pp. 10211024, 2013.",
  "Bingzhao Zhu, Xingjian Shi, Nick Erickson, Mu Li, George Karypis, and Mahsa Shoaran. Xtab: Cross-tablepretraining for tabular transformers. arXiv preprint arXiv:2305.06090, 2023": "Yitan Zhu, Thomas Brettin, Fangfang Xia, Alexander Partin, Maulik Shukla, Hyunseung Yoo, Yvonne AEvrard, James H Doroshow, and Rick L Stevens. Converting tabular data into images for deep learningwith convolutional neural networks. Scientific reports, 11(1):11325, 2021. Honglei Zhuang, Zhen Qin, Rolf Jagerman, Kai Hui, Ji Ma, Jing Lu, Jianmo Ni, Xuanhui Wang, andMichael Bendersky. Rankt5: Fine-tuning t5 for text ranking with ranking losses. In Proceedings of the46th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp.23082313, 2023.",
  "A.1Detailed discussion of related work": "Learning-To-Rank. In our paper, we focus on the traditional LTR setting where the features are all numeric(tabular data). However, there is a line of work in LTR where raw text is also an input. In this case, one canleverage large language models in the ranking setting (Zhuang et al., 2023; Han et al., 2020; Yates et al.,2021; Nogueira et al., 2019; Mitra et al., 2018). In tabular LTR problems, the dominant models currently used are gradient boosted decision trees (GBDTs)(Friedman, 2001), which are not deep learning models. GBDT models, which perform well on tabular data,",
  "Deep tabular models. Given the success of neural methods in many other domains, there have been manyattempts to adapt deep models to the tabular domain": "Borisov et al. (2022) categorize existing techniques for using deep neural networks over tabular data into fourtypes: (1) Encoding-based methods such as VIME (Yoon et al., 2020), SCARF (Bahri et al., 2021), IGTD(Zhu et al., 2021), and SuperTML (Sun et al., 2019); (2) Novel hybrid architectures such as DeepFM(Guo et al., 2017), xDeepFM (Lian et al., 2018), and many others Cheng et al. (2016); Frosst & Hinton (2017);Ke et al. (2018; 2019); Popov et al. (2019); Luo et al. (2020); Liu et al. (2020); Ivanov & Prokhorenkova(2021); Luo et al. (2021); (3) Transformer-based architectures including SAINT (Somepalli et al., 2021),TabNet (Arik & Pfister, 2021), FT-Transformer Gorishniy et al. (2021), TabTransformer (Huang et al., 2020),and ARM-Net Cai et al. (2021)); (4) Regularized DNNs (Shavitt & Segal, 2018; Kadra et al., 2021).For instance, TANGOS introduced special tabular-specific regularization to try to improve deep modelsperformance (Jeffares et al., 2023). We also note that a more recent work, Tree-hybrid simple MLP (Yanet al., 2024a), proposes to combine MLPs and GBDTs together for the tabular prediction problem. Self-supervised learning.Self-supervised learning (SSL) or unsupervised pretraining has improvedperformance in settings where there is a significant source of unlabeled data like text (Devlin et al., 2018)and images (Chen et al., 2020). In SSL, deep models are first pretrained on perturbed unlabeled data usingself-supervised tasks to learn useful representations for the data. Then these models are finetuned for adownstream task with labeled data. Finetuning often takes one of two forms: (1) linear probing (a popularfinetuning strategy in text and images (Chen & He, 2021; Chen et al., 2020; Peters et al., 2019)),wherewe freeze the pretrained model and only update the linear head during supervised finetuning, and (2) fullfinetuning, where we update the whole model during supervised finetuning (Devlin et al., 2018). Sometimes amix of the two is used (Kumar et al., 2022). The core idea behind prominent SSL approaches like SimSiamand SimCLR is to carefully perturb input training samples, and train a representation that is consistentfor perturbations of the same sample. This provides robustness to natural perturbations and noise in data(Hendrycks et al., 2019). Inspired by the success of pretraining and self-supervised learning in images and text, several works showhow to apply SSL to unlabeled tabular data. One strategy is to corrupt tabular data and train a deep modelto reconstruct it (Yoon et al., 2020; Majmundar et al., 2022; Ucar et al., 2021; Nam et al., 2023b; Lin et al.,2023; Syed & Mirza, 2023; Hajiramezanali et al., 2022). Another approach is to use contrastive losses, whichhave been highly successful in the image domain (Chen et al., 2020). These methods are also applicable toour case because tabular data, like image data, is often composed of fixed-dimensional real vectors (Vermaet al., 2021; Bahri et al., 2021; Lee & Shin, 2022; Hager et al., 2023; Liu et al., 2023; Darabi et al., 2021).Concurrent work has also investigated training tabular-specific LLMs for tabular tasks (Yan et al., 2024b;Schuh et al., 2024). Concurrent to us, Holzmller et al. (2024) also proposes pretraining (with a modifiedarchitecture) to improve tabular deep learning performance. Rubachev et al. (2022) evaluate a variety ofdifferent pretraining methods for tabular learning across many different datasets, finding that there is not aclear state of the art. Robustness in LTR. There has been prior work on studying worst-case behavior (robustness) of rankers(Voorhees, 2005; Zhang et al., 2013; Goren et al., 2018; Wu et al., 2022b;a; Penha et al., 2022). Some previousmetrics measure a models robustness against adversarial attack (Goren et al., 2018; Wu et al., 2022a; 2023).Others measure the models per-query performance variance on a dataset (Voorhees, 2005; Zhang et al., 2013;Wu et al., 2022b). Our outlier metric, Outlier-NDCG, is a departure from previous work because it is notdirectly a measure of robustness and it is possible for a model to perform better on outlier data. Transfer learning. We summarize some related work on transfer learning in tabular domain. One directionrevolves around pretraining models on common columns across many datasets (Zhu et al., 2023; Wang &",
  "A.2.1Dataset statistics": "In table , we provide the details of our three public datasets and the proprietary real-world dataset.Note that for the public dataset experiments we sub-sample the pre-training dataset in different ways (asdescribed in Sections 3, 4.1.1 and 4.1.2) to create the training dataset and simulate scarcity of labeled samples.However, we use the same validation dataset without and with the labels for the self-supervised pretrainingand supervised learning stages, respectively. Note that in all these datasets, the higher the label value thehigher the relevance of the item.",
  "A.2.2Outliers query group details": "Here we detail how we select the outlier query groups for the Outlier-NDCG metrics (.1). For ourproprietary online shopping dataset, there are known outlier features that cause low quality predictions.These outliers arise from noise introduced in various stages in the data pipeline. We have mitigation strategiesin place against these feature outliers, but they are often imperfect. Because we already know some of theoutliers features, we deem any query group with items with outlier features as an outlier. These outlierscomprise roughly 10% of the test set, which is still a large number of query groups (much larger than thenumber of query groups in the outlier datasets of the public datasets).",
  ": An example of outlier detection in Istella_S (Lucchese et al., 2016) for our Outlier-NDCG metric": "Because different datasets have differently-sized typical gaps, we tune G for each dataset (MSLR, Yahoo,Istella) such that the resulting percentage of outlier queries is as close to 1% of the test set as we can get.MSLRWEB30K has G = 5, with 0.65% (40/6072) outlier queries, Yahoo has G = 20, with 1.4% (30/2147)outlier queries, and Istella has G = 32, with 0.46% (34/7202) outlier queries. 1% is a hyperparameter thatcan be tuned according to the users goals.",
  "Pretrained DL model": "Pretraining: (1) the encoder used for pretraining is the tabular ResNet (Gorishniy et al., 2021) with threeResNet blocks, with the final linear layer removed, (2) pretraining is done on the entire dataset with learningrate 0.0005 using Adam (Kingma & Ba, 2014) (3) with SimSiam/SimCLR-Rank/SimCLR-Sample, we tunedamong four different augmentations for each pretraining method: randomly zeroing out features (Zeroing)with probabilities 0.1 or 0.7, and Gaussian noise with scale 1.0 or 2.0, (4) for SCARF we tune the corruptionprobability in {0.3, 0.6, 0.9}, (5) for VIME-self we tune the corruption probability in {0.3, 0.5, 0.7}, (6) forDACL+ we tune the mixup amount in {0.3, 0.6, 0.9}, (7) for SubTab we divide input features into 4 subsetswith 75% overlap, and tune the masking probability in {0.3, 0.6, 0.9}, (8) we pretrain for 300 epochs forall methods, and (9) we use a batch size of roughly 200000 items for SimCLR-Rank, SimSiam, VIME-self,and SubTab as they are less GPU memory intensive while using a batch size of roughly 2000 items forSimCLR/DACL+/SCARF which are more GPU memory intensive. We also evaluate SAINT (Somepalli et al., 2021), a transformer-based pretrainining method.For thearchitecture, we follow (Somepalli et al., 2021), using L = 4 layers, dropout of 0.1, attention heads h = 4,with a self-attention query/key/value dimension of 16 and a intersample attention query/key/value dimensionof 64. For augmentation, we follow (Somepalli et al., 2021) and use a CutMix mask parameter of 0.3 andMixUp parameter of 0.2. Again following (Somepalli et al., 2021) we weight the denoising loss 10 times morethan the contrastive loss. For all other minor architectural details, we follow the code of (Somepalli et al.,2021). We pretrain for 5 epochs with a batch size of roughly 100 items due to GPU memory contraints and",
  "also the time cost (one epoch of saint took 300x longer than an epoch of SimCLR-Rank in MSLRWEB30K,6000x longer in Yahoo Set1, and 1000x longer in Istella). We use a learning rate of 5e-5": "Finetuning: (1) finetuning is done on the labeled train set by adding a three-layer MLP to the top of thepretrained model and training only this head for 100 epochs and then fully finetuning for 100 epochs usingAdam with a learning rate of 5e-5, (2) we use an average batch size of roughly 1000 items (may vary based onquery group size), (3) we use the LambdaRank loss (Burges, 2010), (4) we use the validation set to performearly stopping (i.e. using the checkpoint that performed best on the validation set to evaluate on the test set). For SAINT we follow the finetuning process of (Somepalli et al., 2021) in using the output of the CLSembedding as the representation for an item, and tune the learning rate in {5e-5, 1e-5, 5e-6} as we found theperformance to be sensitive to this setting. Implicit binary user feedback setting. We reuse the methodology of .1.1, except we use ascoring head of one linear layer (as opposed to a three layer MLP) for pretrained models. We found that thisimproved stability and performance in the binary label setting. x1 x2 SimCLR-Rank t-SNE MSLRWEB30K 0.01.02.03.04.0 x1 x2 SimSiam t-SNE MSLRWEB30K 0.01.02.03.04.0 x1 x2 Supervised t-SNE MSLRWEB30K 0.01.02.03.04.0 : We plot the t-SNE plots of embeddings produced by three different encoders for the MSLRWEB30Kdataset: (1) pretrained by SimCLR-Rank, (2) pretrained by SimSiam, (3) trained via supervised training onthe entire training set on roughly 1000 samples from MSLR. Marker size and color indicates relevance. Wefind (1) SimCLR-Rank/SimSiam cluster different relevances effectively but do not order them as well as thesupervised encoder. (2) SimCLR-Rank produces more spread-out embeddings with less defined clusters thanSimSiam. x1 x2 SimCLR-Rank t-SNE Istella_S 0.01.02.03.04.0 x1 x2 SimSiam t-SNE Istella_S 0.01.02.03.04.0 x1 x2 Supervised t-SNE Istella_S 0.01.02.03.04.0 : We plot the t-SNE plots of embeddings produced by three encoders: (1) pretrained by SimCLR-Rank,(2) pretrained by SimSiam, (3) trained via supervised training on the entire training set on roughly 1000samples from Istella_S. Marker size and color indicates relevance. We find (1) SimCLR-Rank/SimSiam clusterdifferent relevances effectively but do not order them as well as the supervised encoder. (2) SimCLR-Rankproduces more spread-out embeddings with less defined clusters than SimSiam. No Pretrained DL model. There are a few rankers we evaluated for the no-pretrain DL baseline: (1) a3-layer tabular ResNet from (Gorishniy et al., 2021) with a three-layer MLP on top of it, and was trained",
  "A.2.4Details of Binary Label generation used in .1.2": "Following the methodology from Yang et al. (2022), we generate stochastic binary labels y from the relevancelabels r as y = 1{t r + G1 > t target + G0} where t is a temperature parameter, G1, G0 are standardGumbels, and target is a parameter controlling how sparse the binary labels are. Higher target leads tohigher label sparsity. Yang et al. (2022) show that y is 1 with probability (t (r target)) where is thesigmoid function. We set t = 4 as in Yang et al. (2022). For a given target, we produce produce a newdataset for each of MSLRWEB30K, Yahoo Set1, and Istella_S where we convert the relevance labels in thetraining/validation sets to binary labels and keep the true relevance labels in the test set.",
  "A.2.5Large industry-scale dataset experimental details": "For our DL ranker, first we pass the features (numerical and categorical) through some featurizers. Then weuse a simple 4 hidden layer MLP, with and ReLU activation, to predict a single scalar score for each (query,item) pair. We also use dropout and batch normalization in input and hidden layers. For the SimCLR pretraining stage, we remove the last linear layer of the above ranker to create the mainencoder to produce the pretrained embeddings. Projector encoder here is a 2 hidden layer MLP, with ReLUactivation and no bias, which outputs an input dimensional embedding. We also do not use any normalizationor dropout in this projector. For SimSiam we use the same embedder and a similar projector. The only difference in the projector beingbatch normalization in the hidden layers and batch normalization without affine learnable parameters in theoutput layer. Additionaly, SimSiam also employs a predictor MLP, with a single hidden layer and ReLUactivation, and with the same input and output sizes. It uses batch normalization and no bias in the hiddenlayer and no input and hidden dropouts. For pretraining, we used an augmentation which randomly zeroed out some features. We also tried randomlyswapping some features between items in the same batch or query group, but they didnt perform as well aszeros augmentation. During the fine-tuning stage we discard the projectors and predictor and add back thelinear layer to complete the non-pretrained ranker structure. DL rankers are trained/fine-tuned using PiRankloss Swezey et al. (2021), batchsize of 1000, AdamW optimizer with learning rate of 0.00001, weight decay of0.01 and cosine scheduling with warmup. All training stages and experiments were conducted on machineswith 4 GPUs, where we ran 100k iterations with ddp and picked the best checkpoint based on validationmetric. GBDT rankers were trained with LambdaRank using the lightgbm package. They were tuned with Bayesianhyperparameter optimization strategy by tuning number of leaves, learning rate, and minimum data in leafin a reasonably large range with a maximum of 150 jobs. We used default values of the library for the otherhyperparameters.",
  "A.3.3Additional Results on SimCLR-Rank vs SimSiam": "In this section we give additional t-SNE plots of the embeddings learned by SimCLR-Rank and SimSiamencoders, and the pre-final layer of the fully supervised ranker (trained with all labels) on MSLRWEB30Kdataset (), and Istella_S dataset (). For the Yahoo Set1 dataset, SimSiam performs the best,but for the above two datasets SimCLR-Rank is the best.",
  "A.3.5Additional Results for Simulated Implicit Binary user feedback setting": "In this part of the appendix we provide additional results on binary label generation as detailed in .1.2.In Tables 9 and 10 we give the dataset statistics when using target = 4.25 and target = 5.1, respectively. Forthese two settings, the GBDT baseline does not contain semi-supervised runs as we found that semi-supervisiontends to generally degrade GBDT performance; see Appendix A.4. The No Pretrain baseline uses thetabular ResNet architecture. In Tables 12 and 13 we give the comparison between pretrained and non-pretrained rankers following themethodology in .1.2 and find that GBDTs perform the best when binary labels are less sparse(target = 4.25) and pretrained DL rankers perform the best when binary labels are sparser (target = 5.1).",
  "A.3.7Combining SimCLR-Rank and SimSiam": "Here we describe how we constructively combined the SimCLR-Rank and SimSiam to get the competitiveresults in .3. Following the setting of Appendix A.2.3, we train (1) a SimCLR-Rank model withgaussian augmentation and noise of scale 1.0, and (2) a SimSiam model with zeroing augmentation andcorruption probability 0.1. After pretraining the SimCLR-Rank and SimSiam models, we finetune each of them on the labeled data andkeep the checkpoints with the best validation NDCG. Then we combine these two models by concatenatingthe final embedding layers togetherif each models penultimate embedding dimension is h (h = 136 for us),then the concatenated embedding is of dimension 2 h. Then we pass this concatenated model through abatchnorm (to normalize the scales of each embedding to prevent scale issues), pass it through dropout layerof probability 0.7 (if the dataset is MSLRWEB30K or Istella_S) or 0.0 (if the dataset is Yahoo Set1), andmap this vector (of dimension 2 h) to a score of dimension 1 using a linear layer. We finetune only the finallinear layer.",
  "A.4Semi-supervised GBDT": "Here we provide a more detailed comparison between Supervised GBDTs, GBDT + pseudolabeling, andGBDT + PCA + pseudolabeling in for the simulated crowdsourcing of labels setting and in for the simulated implicit binary user feedback setting. We follow the experimental methodology inAppendix A.2.3. We find that for the most part, semi-supervised learning does not benefit GBDT rankers.",
  "A.5Quantitative measures of embedding quality": "We provide a quantitative measure of embedding quality in this section. We attempt to verify that the focus onin-query group negatives helps improve embedding quality. To do this, we compare the embeddings producedby a SimCLR-Rank model and a SimCLR-Sample model (SimCLR-Sample is the same as SimCLR-Rankexcept the negatives are randomly taken from the batch rather than being in-query group), both under zeroesaugmentation with p = 0.1, and measure how spread out the embeddings are within a query group for eachmethod. In , we find that SimCLR-Rank embeddings are consistently more spread out within aquery group than SimCLR-Sample, showing that in-query group focus leads to a measurable impact on thedistinguish-ability of embeddings within a query group. This may help explain why SimCLR-Rank performsbetter than SimCLR-Sample.",
  "Outlier-NDCG ()": "Zeroing (p=0.1)0.3149 0.01190.5200 0.01330.6348 0.0164Zeroing (p=0.7)0.3002 0.00600.5081 0.00970.6201 0.0104Gaussian noise (scale=1.0)0.2585 0.03710.4893 0.00840.6145 0.0160Gaussian noise (scale=2.0)0.2929 0.00210.5163 0.01010.5905 0.0146 : The ratio of the in-query group mean absolute deviation over the overall mean absolute deviation ofembeddings from SimCLR-Rank and SimCLR-Sample. These embeddings were calculated over the validationset of each dataset. We find that SimCLR-Rank has a higher ratio, meaning the embeddings are more spreadout within a query group.",
  "SimCLR-Rank0.99530.99040.9915SimCLR-Sample0.99450.98810.9797": "Fraction of training qgs labeled 0.275 0.300 0.325 0.350 0.375 0.400 0.425 0.450 0.475 NDCG MSLRWEB30K GBDTGBDT+PCAGBDT+pseudoGBDT+PCA+pseudo Fraction of training qgs labeled 0.500 0.525 0.550 0.575 0.600 0.625 0.650 0.675 NDCG Yahoo Set1 GBDTGBDT+PCAGBDT+pseudoGBDT+PCA+pseudo Fraction of training qgs labeled 0.45 0.50 0.55 0.60 0.65 NDCG Istella_S GBDTGBDT+PCAGBDT+pseudoGBDT+PCA+pseudo : A comparison of Supervised GBDT, GBDT + PCA, GBDT + pseudolabeling, and GBDT + PCA+ pseudolabeling. We find that semi-supervised learning may sometimes benefit GBDT rankers, though it isnot very consistently beneficial."
}