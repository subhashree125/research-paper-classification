{
  "Abstract": "How can balance be quantified in game settings? This question is crucial for gamedesigners, especially in player-versus-player (PvP) games, where analyzing the strength rela-tions among predefined team compositionssuch as hero combinations in multiplayer onlinebattle arena (MOBA) games or decks in card gamesis essential for enhancing gameplayand achieving balance. We have developed two advanced measures that extend beyond thesimplistic win rate to quantify balance in zero-sum competitive scenarios. These measuresare derived from win value estimations, which employ strength rating approximations viathe Bradley-Terry model and counter relationship approximations via vector quantization,significantly reducing the computational complexity associated with traditional win valueestimations. Throughout the learning process of these models, we identify useful categoriesof compositions and pinpoint their counter relationships, aligning with the experiences ofhuman players without requiring specific game knowledge. Our methodology hinges on asimple technique to enhance codebook utilization in discrete representation with a deter-ministic vector quantization process for an extremely small state space. Our framework hasbeen validated in popular online games, including Age of Empires II, Hearthstone, BrawlStars, and League of Legends.The accuracy of the observed strength relations in thesegames is comparable to traditional pairwise win value predictions, while also offering a moremanageable complexity for analysis. Ultimately, our findings contribute to a deeper un-derstanding of PvP game dynamics and present a methodology that significantly improvesgame balance evaluation and design.",
  "Introduction": "In the dynamic landscape of player-versus-player (PvP) games, team compositions, or \"comps,\" such as herocombinations or decks formed before matches commence, are pivotal (Costa et al., 2019; de Mesentier Silvaet al., 2019; Reis et al., 2021). The gaming industry, now approximately a 200 billion US dollar market(Kristianto, 2023), thrives on the diversity and engagement offered by these compositions, reflecting playersindividuality and sustaining market competitiveness (Figueira et al., 2018; Fontaine et al., 2019). However,",
  "Published in Transactions on Machine Learning Research (09/2024)": "3. Combo-Reliant Archetypes (Category 3): These decks hinge on key cards to enable combina-tions but lack exceptionally fast draw engines. Naga Priest and Dragon Druid, which require a mixof Naga and spells, or Dragon cards in hand respectively, are typical of this category. 4. Midrange Archetypes with Strong Creatures (Category 4): This category thrives on deploy-ing minions with both high attack and health, sustaining board presence to win. Earthen Paladinis a prime example, using minions that can endure and dominate board trades. Their area of effectspells also gives them an advantage against swarm-based strategies (Category 7). 5. Midrange Value Decks (Category 5): These decks aim to overwhelm the opponent with thesheer quality of their cards, not necessarily through a quick victory but through sustained pressureand superior trades. 6. Diverse Midrange Archetypes (Category 6): A mix of decks that dont fit neatly into onearchetype but generally win by card value. They often dont have as stark counter relationships andtend to have more even matchups. 7. Aggro and Snowball Archetypes (Category 7): These decks look to establish an early boardpresence and snowball to victory. They perform well against decks that struggle to clear multiplethreats, such as general aggro decks (Category 9). 8. Value-Oriented Decks with Combo Finishers (Category 8): This group features decks thatmaintain board control with ample resources and are capable of executing a one-turn-kill (OTK)combo in the late stages of the game. Notably, Control Warrior can build up a significant armorstack to unleash a massive hit, while Rainbow Mage is known for its combo potential to achieve anOTK.",
  "Game Balance": "Game designers are tasked with devising engaging mechanisms and numerical frameworks that enhanceplayer experiences (Schell, 2008). Developing an immersive game loop not only encourages participationbut also assists players in forming a mental model of the games mechanics (Sellers, 2017). Designers oftenapply the Yerkes-Dodson law to optimize player satisfaction, suggesting an optimal arousal level for peakperformance that aligns in-game challenges with player skill progression (Dodson, 1915).This dynamicinteraction is crucial for maintaining players in a state of mental flow (Cskszentmihlyi, 1990), where gamebalance plays a pivotal role in sustaining appropriate levels of difficulty and challenge. As a critical research field within game design and operations (Schell, 2008; Novak et al., 2012; Sellers,2017), game balance significantly influences player engagement through diverse strategies and playstyles.It extends beyond mere difficulty adjustments to encompass strategy, matchmaking, and game parametertuning (Becker & Grlich, 2020).Understanding balance definitions and metrics is vital for effectivelyaddressing these components. Traditional metrics such as win rate, win value difference, and game scoreshave driven the evolution of game balancing techniques, refining the interplay between game mechanics andplayer satisfaction (Jaffe et al., 2012; Budijono et al., 2022; Mahlmann et al., 2012). In PvP scenarios, win value estimation is a common approach, with values often normalized to scales like or to simplify payoff calculations between competitors (Budijono et al., 2022). However, calibratingthe strength of a composition with win values typically requires comparisons against multiple opponents(Fontaine et al., 2019). While strength rating systems like Elo, TrueSkill, and Matchmaking Rating canidentify strength from a single scalar rating and suggest greater strength with higher ratings (Elo, 1966;Herbrich et al., 2006; Pramono et al., 2018), capturing intransitivity or cyclic dominance in scalar ratingsis challenging. This necessitates multi-dimensional ratings (Chen & Joachims, 2016; Balduzzi et al., 2018),which reintroduce complexity into balance analysis.Thus, this paper aims to propose a solution thatconsiders intransitivity while maintaining feasible complexity in balance analysis. Acquiring accurate game data is also crucial for balance analysis, often involving the deployment of rule-basedagents during early development phases and integrating human testers later to capture realistic gameplaydata. Advances in artificial intelligence, demonstrated by AlphaZeros performance in board games, haveenabled learning-based agents to contribute to game balance data collection (Tomaev et al., 2022). Com-munity discussions about strategies also provide valuable insights, often grounded in game theory principlesor defining some empirical relationship graphs by humans to explain the game scenarios (Schmitz, 2022;Hernndez et al., 2020). Although the entropy of a strategy reaching Nash equilibrium can serve as a mea-sure of strategic balance (Pendurkar et al., 2023), computing Nash equilibrium policies at the game actionlevel in complex games is resource-intensive, posing a challenge for practical application in the game design",
  "Proposition 2.2. We say composition c1 dominates c2 over all compositions c if Win(c1, c) > Win(c2, c)": "When Proposition 2.2 is true, c2 is considered useless in terms of win values because c1 can perform betterthan c2 in all cases. If game designers can validate all compositions with Proposition 2.2, they can analyzegame balance by identifying overly strong or useless compositions.If some compositions are very weakor meaningless, leading to most compositions being able to defeat them 100% of the time, thus violatingthe domination relation, designers can either manually eliminate these compositions from the set of allcompositions c or iteratively eliminate dominated compositions by running Proposition 2.2 several times. However, the time complexity of validating Proposition 2.2 is O(N 3) over N team compositions with apairwise win value estimation Win(c1, c2). We will try to reduce this complexity with approximations later.",
  "Learning Rating Table and Counter Table": "For understanding the strength relations between compositions (comps) and performing efficient balanceanalysis, we need to quantify the strength and counter relationships first. Our methodology begins with theapplication of the Bradley-Terry model to allocate a scalar value representing the strength of each comp basedon win estimations. This process is elaborated upon in .1. To tackle the issue of cyclic dominance orintransitivity of win values efficiently, epitomized by the Rock-Paper-Scissors dynamic, we devise a countertable. This involves examining the variances between actual win outcomes from specific comps and thepredictions made by the Bradley-Terry model, a process detailed in .2. Furthermore, the overarchingframework that integrates these components into our learning process is delineated in .3.",
  "Neural Rating Table": "Win rates in PvP games, while useful as a conventional metric, do not fully encapsulate the actual strengthsof individual players or team compositions. A players or comps true prowess is better reflected in theirability to triumph over comparable opponents, as victories against both weaker and stronger opponentscontribute equally to the win rate but signify different levels of strength. The Elo rating system, commonlyutilized in chess and similar two-player zero-sum games, offers a scalar strength rating for entities, aligningwith the principles of the Bradley-Terry model (Elo, 1966; Bradley & Terry, 1952). This model predicts theprobability of player i defeating player j, as delineated in Equation 1:",
  "Neural Counter Table": "Within the framework of adapting the Bradley-Terry model through neural networks, we can list the strengthof all N team compositions with a space complexity of O(N). However, the precision of strength relationsprovided by this method may not match the precision afforded by direct pairwise comparisons for eachcomposition, a process that inherently bears a space complexity of O(N 2). While theoretically feasible vianeural networks, such direct prediction incurs a high space complexity, making it challenging to check theseratings and analyze balance, especially with a large N. The phenomenon of cyclic dominance or intransitivity of win values, a common challenge in analyzinggame balance, introduces further complications. An N N counter table, which would record adjustmentsfrom rating predictions to enhance accuracy, becomes impractical due to its high space complexity andcognitive load. In practice, players intuitively grasp counter relationships without the need for exhaustivememorization of large tables. To navigate this, we propose a more manageable M M counter table thatserves as an approximation of the full N N relationships, where M represents a manageable number ofdiscrete categories. Beginning with a minimum of 3 to capture basic cyclic dominance patterns, M can beadjusted to strike a balance between prediction accuracy and table interpretability. For the task of learning discrete categories, we employ Vector Quantization (VQ), a technique of neuraldiscrete representation learning celebrated for its effectiveness (van den Oord et al., 2017). It acts as anend-to-end analog to K-means clustering within neural networks, primarily introduced in the context ofVQ-VAE (van den Oord et al., 2017; Baevski et al., 2020). Our goal diverges from traditional autoencoderobjectives; rather than reconstructing inputs, our focus is on developing a counter table that learns from theresiduals between direct win predictions and those derived from the Bradley-Terry model.",
  "Neural Discrete Representation Learning": "Before introducing our design for learning the counter table, we first discuss a popular discrete representationlearning method with neural networks, VQ-VAE (van den Oord et al., 2017). In scenarios that require discreterepresentation, clustering is a common approach, with k-means clustering being a widely used method forseveral decades (MacQueen et al., 1967).This clustering idea is based on finding k reference points inthe feature space to represent corresponding groups of actual features using the nearest neighbor method.However, obtaining an effective feature space from raw observations for this clustering process is a criticalproblem. With the growth of deep learning, variational autoencoder (VAE) (Kingma & Welling, 2014) wasproposed to learn effective latent feature spaces for several tasks.",
  "Lrec = E[D(o, o)],Lvq = E[D(sg[ze], zq)],Lcommit = E[D(ze, sg[zq])](5)": "Here, D is a distance function, with mean square error (MSE) being a common choice. ze and zq are thelatent codes before and after nearest neighbor replacement, respectively. The sg[] denotes the stop-gradientoperator. The standard VQ-VAE minimizes the combined loss function L = Lrec +Lvq + Lcommit, where is a weight term that encourages the encoder to produce latent codes closer to discrete representations.",
  "Applying Vector Quantization to the Counter Table": "After a brief understanding of vector quantization with neural networks, we extend this idea of discreterepresentation to our counter table application. Given the symmetrical nature of residual win values and ouraim to classify compositions into M discrete categories, we utilize Siamese network architectures for boththe learning of discrete representations and the prediction of residual win values, as illustrated in .The residual win value, Wres, is defined as:",
  "Learning Procedure": "The methodology underlying the construction of the rating and counter tables is encapsulated in the learningframework depicted in . This framework requires a dataset consisting of match results, including theteam compositions of the competing sides alongside the ultimate win-lose outcomes. The representation ofthese compositions is adaptable, ranging from simple binary encodings to more nuanced feature descriptions,according to the preferences and requirements set by game designers. Crucially, the derivation of the Neural Counter Table C is predicated on the prior establishment of the NeuralRating Table R. This sequential approach ensures that the foundational ratings of team compositions areaccurately determined before their interrelations and counter dynamics are analyzed. The development andrefinement of these tables pave the way for the introduction of novel measures aimed at enhancing diversityand balance within the gaming environment. A comprehensive discussion of these newly introduced balancemeasures is forthcoming in , while the effectiveness and precision of the rating and counter tableswill be evaluated in .",
  "Accuracy of Strength Relations": "With our rating table and counter table, we can approximate the win value of a match given two compositionsand identify the strength relations for balance. In this section, we examine the accuracy of strength relationsusing these tables across different games and investigate the impact of the hyperparameters N and M oncounter table training. There are 5 models for each method in our experiments, each trained from a differentrandom seed. The results in the tables are the average values of these models.",
  "Simple Combination Game": "A simple combination game was designed with 20 elements, each assigned a score equal to its index from1 to 20. A comp consists of three distinct elements. The score sc of a comp c is the sum of its elementsscores, and the win-lose outcome is binary, sampled via the probability function P(c1 > c2) =(sc1)2",
  "Advanced Combination Game": "This game combines the simple combination game with Rock-Paper-Scissors rules. The primary rule mirrorsthe simple combination game, with an additional rule: T = sc mod 3, assigning T as the counter category ofthe comp, with 0/1/2 corresponding to Rock/Paper/Scissors. A winning Rock-Paper-Scissors comp receivesa +60 score bonus during comp score calculation. There are still C203 = 1140 possible compositions inthis setting. The dataset consists of 100,000 matches generated uniformly.",
  "Age of Empires II (AoE2)": "Age of Empires II is a popular real-time strategy game. We utilized the statistics (as of January 2024) fromaoestats1, an open-access statistics website. The game features 45 civilizations (comps) in 1v1 random mapmode across all Elo ranges. Thus, there are 45 compositions in this game. Further combinations ofcivilizations in team mode or specific maps are not discussed in this paper. The dataset contains1,261,288 matches.",
  "Hearthstone": "For Hearthstone, a popular collectible card game, we accessed the statistics (as of January 2024) fromHSReplay2, an open-access statistics website. We considered 91 named decks as compositions for standardranking at the Gold level. Therefore, only up to 91 compositions were used, and the detailed heroor card selections within these compositions were not considered for simplicity. The datasetcomprises 10,154,929 matches.",
  "Brawl Stars": "Brawl Stars is a popular Multiplayer Online Battle Arena (MOBA) game. We focused on the Trio Modesof Brawl Stars, where teams of three compete for victory. Data were sourced from the \"Brawl Stars Logs& Metadata 2023\" on Kaggle, initially collected via the public API. With 64 characters, 43 maps, and 6modes, the composition count could reach C643 43 6, i.e., the maximum number of compositionscan reach 10,749,312. The dataset includes 179,995 matches, with 94,235 unique compositions observed.",
  "League of Legends": "For League of Legends, a renowned MOBA game with 5-on-5 team competition, we used the \"League ofLegends Ranked Matches\" dataset from Kaggle, which features 136 champions. Thus, the maximumnumber of compositions is C1365= 359, 933, 112. The dataset covers 182,527 ranked solo games with348,498 unique compositions observed, which implies that almost all compositions are different.",
  "Comparisons of Strength Relation Prediction": "To better understand whether win value predictions can provide accurate strength relations, we examinethe accuracy of the strength relation classification task (weaker/same/stronger) rather than the predictionerror of win values.For example, if there are two compositions, A and B, and the oracle win value isWin(A,B)=0.55, the actual outcome we care about is whether A is stronger than B. Now, consider twoapproximations: Win(A,B)=0.49 and Win(A,B)=0.62. It is clear that Win has a smaller absolute error(0.06) compared to Win (0.07), but Win suggests the wrong strength relation. In this strength relation classification task, if the win value falls within the range of [0.499, 0.501], we designatethe prediction as the same; a value below 0.499 indicates weaker, and a value above 0.501 indicates stronger.The classification label is calculated based on the average pairwise win value in the dataset. For example,if CA has a 60% average win value against CB in the dataset, CA is deemed stronger when calculatingaccuracy. All models are approximated with neural networks and trained for 100 epochs on datasets using5-fold cross-validation.A linear decay learning rate from 0.00025 to 0 over 100 epochs with the Adamoptimizer is employed. We then compare the following five methods of win value prediction and providetheir definitions with formulations in Section A.4.1: 1. WinValue: Predicts the win value for a given composition and compares the win values of the twocompositions to determine the winner. If the absolute value of the win value difference is not greaterthan 0.1%, they are considered to be at the same level. This is a common method in game statisticsthat does not require maintaining a large table.",
  "League of Legends51.150.953.651.151.0": "2. PairWin: Directly predicts the pairwise win value. Some game statistics provide this kind of resultwhen the number of compositions is not too large, and it is a straightforward measure to examinecounter relationships. If we have sufficient match results, this method represents the upper boundof strength relation accuracy. 3. BT: Utilizes linear approximation to perform the Bradley-Terry model. This method assumes therating of a composition can be derived from the sum of the element ratings within the composition.Common generalized Bradley-Terry models for team setups or Elo ratings in team games use thiskind of approach (Coulom, 2007).",
  ". NCT: Enhances NRT with an additional neural counter table of size M M": "presents the accuracy of the strength comparison task.Notably, NCT with M = 81 achievesaccuracy comparable to PairWin across all games. In games with complex compositions, such as BrawlStars and League of Legends, a non-linear approximation is essential for estimating comp strength. Forgames with explicit counter relationshipssuch as Rock-Paper-Scissors, the Advanced Combination Game,and Age of Empires II, which exhibit a significant accuracy discrepancy between PairWin and NRTourcounter table offers a viable solution. The simplistic win value estimation approach, WinValue, usually doesnot provide the best strength predictions. These results affirm the precision of our rating and counter tablesin predicting win values. Notably, in , as the parameter M increases, so does accuracy, allowing fordetailed tracing and analysis of complex counter relationships through the counter table. We provide furtherdiscussions regarding the results of counter tables in Appendix A.3. Additionally, we can observe these accuracies to analyze the properties of the obtained gaming results. If thedifference in accuracy between training and testing is small, it implies that there is no significant overfittingand the strength relations can be generalized to matches not observed. However, in cases where there is",
  "League of Legends50.951.151.150.951.051.0": "clear overfitting, such as in League of Legends, it suggests the need for more match results to generalizethe known strength relations to unknown scenarios. Since League of Legends includes a hero ban and pickphase before a match starts, players tend to prefer compositions that can counter their opponents and aimto improve their win rate. This increases the difficulty of win rate prediction for unobserved cases. The largepossible composition space also increases the complexity of accurate model training. For such game cases, werecommend collecting more game results for training. Nonetheless, we can still analyze the strength relationsin the training datasets, as our goal is to understand these relations. Initial insights from observed cases canbe valuable for early game balancing, although conclusions drawn from these datasets may not necessarilyapply to unobserved scenarios.",
  "Counter Table Utilization": "Given the need for a counter table for strength relation analysis, we adopt a vector quantization processin our NCT training. There is an issue with low codebook utilization when the state space is extremelysmall. We introduced a VQ Mean Loss to maximize the utilized M. For vector quantization, as describedin .2, the standard hyperparameters are set to N = 0.01 and M = 0.25. We explore differentconfigurations of these hyperparameters in Age of Empires II using NCT M=27 since it is a game requiringa large counter table for better strength relation accuracy, as shown in Tables 3 and 4. We found that thecommonly suggested N = 0.25 (van den Oord et al., 2017) leads to low utilization. Thus, we selected anearly zero coefficient N = 0.01 to perform regular VQ training. However, even with N = 0.01, if we donot introduce the VQ Mean Loss (set M = 0), the utilized M is still far from the upper bound of 27. Wesuggest using M = 0.25 for better accuracy and codebook utilization. Also, a coefficient greater than 1 forVQ Mean Loss is not reasonable since it suggests gravitating the mean embedding more than the nearestembedding, which breaks the original idea of VQ and results in worse performance.",
  "Tabular Version of Baselines": "In .2, all baselines we used are trained from neural networks for better generalizing unseen compo-sitions. One may ask why not conduct a simple tabular approach like common rating or statistical analysis;thus, we also report the tabular version of WinValue, PairWin, and also Elo rating and a multidimensionalvariant, mElo2 (Balduzzi et al., 2018).",
  "We test the following five types of methods:": "1. WinValue NT: It is the same as WinValue, but using a tabular method to get predictions. Inother words, this method averages the game results to directly report the average win values insteadof using an approximation from a neural network. For those unseen compositions on any side ofplayers in the test dataset, we set the prediction to undefined and always get a wrong predictionsince there is no default strength value in the method. 2. PairWin NT: It is the same as PairWin, but using a tabular method to get predictions. Inother words, this method averages the composition-by-composition game results to give the predic-tion. For those unseen matches, we also set the prediction to undefined and always get a wrongprediction. We can imagine that this method would have the best training accuracy but very badgeneralizability since training with a tabular approximator can have minimal approximation errorbut no generalization. 3. Elo NT: We apply the standard Elo rating method on compositions.Each composition is aplayer, and the initial rating is 1000. The constant K for updating the Elo rating is 16. We useNRT as the baseline of Elo since they are all derived from the Bradley-Terry model but use differentimplementations. 4. mElo2: We implement the mElo2 proposed by Balduzzi et al. (2018), which assigns each compositiona scalar rating r and a two-dimensional vector c. The initial rating is 1000, and the update step Kis 16. For the initial vectors of c, we follow a public implementation provided by Lazewatsky (2024),uniformly sampled from a real value range .",
  "League of Legends51.1 0.150.9 051.1 6.350.251.0": "adjustment. The results in show that tabular methods can have better prediction on training datasince they have less approximation error. However, they do not have generalizability and may fall into severeoverfitting. If we focus on popular online games, using the neural network version of PairWin or NCT=81as the win value predictor is still a better choice.",
  "New Balance Measures": "The creation of rating and counter tables allows us to devise new ways to measure balance in games, goingbeyond simple win rates to consider domination relations as described in Proposition 2.2. In games wheretwo players compete against each other, a common goal is to equalize win rates, aiming for each player tohave a win rate near 50%. This is easier to achieve in real-time games with symmetric settings for eachplayer, but it is harder in turn-based games because the player who goes first often has an advantage (Beau& Bakkes, 2016). The main challenge in balancing is determining which player has the upper hand and theextent of their advantage. We propose two new ways to measure balance based on estimated win values andexplain how to calculate these measures using our approximations to reduce computational complexity.",
  "x1+y >x2": "x2+y when x1 > x2 > 0. According to Proposition 5.2, if there is only one compositionctop with the highest rating, it is considered the best choice before considering counter strategies. Thisinformation is often sufficient for some balancing methods, such as identifying and adjusting the strongestcomp (Fontaine et al., 2019). The time complexity of identifying ctop can be done in O(N) over N comps,",
  "end for": "Deriving from Proposition 5.8 and Assumption 5.7, Proposition 5.9 simplifies the determination of dominationwithin the same category, and Proposition 5.10 establishes a transitive relationship in domination. Thesepropositions collectively support Lemma 5.11: Lemma 5.11. Given a rating table R following the Bradley-Terry model and a counter table C covering ccomps across M categories, all non-dominated comps can be identified among the highest-rated ones in eachof the M categories. After finding the top comp in each category (O(N + M)), we use Lemma 5.11 to identify non-dominatedcomps. This methodology is detailed in Algorithm 2, calculating the Top-B Balance measure in O(N+M 3),where a larger B implies more balanced game content.",
  "Top-D Diversity Measure": "We are examining how many different game compositions (comps) players might prefer to play. More choicescan enrich the game content for fun and also help designers generate revenue by selling these comps. Wewant to know which comps players will pick based on their chances of winning. Here are some definitionsand assumptions for this measure.",
  "R(c)+R(ctop) + G 0.5": "By accepting Definition 5.3, Assumption 5.1, Assumption 5.4, Assumption 5.5, and Proposition 2.2,Lemma 5.6 is true, since comps that meet this condition are considered not dominated by any other comps.We use Algorithm 1 to count how many comps meet this condition, and this number represents the gamesTop-D Diversity measure, where a larger D implies more diverse game content. The time complexity ofthis algorithm is O(N) over N comps. Without the property of a single scalar rating, the time complexityto check and define win value gaps on pairwise compositions is O(N 2).",
  "Case Study of Top-D Diversity and Top-B Balance": "With our new balance measures, previous works on game balancing, as mentioned in Sections 1 and 2, cannow incorporate these measures to adjust game mechanisms beyond merely achieving a 50% win rate inPvP scenarios. In this section, we conduct two case studies using our measures for direct balance changesuggestions in Age of Empires II and Hearthstone, employing our first model of rating and counter tablesin the experiments to demonstrate an application. The actual ratings and counter categories of these tablescan be found in Appendix A.3. We also discuss the use case and information for suggesting balance updateswith our methods and existing approaches, including win rate observations and entropy-based methods.",
  "Case Study on Age of Empires II": "In Age of Empires II, there are 45 civilizations as compositions in 1v1 mode. We first examine the Top-DDiversity measure in (a). The top composition identified was the Romans with a strength of 1.08145.The result on Top-D Diversity suggests that to enhance general balance, setting the win values standarddeviation larger than 4% is reasonable.Such adjustments could be implemented through matchmakingmechanisms, map randomness, game rule variations, etc.A lower randomness level implies imbalance,",
  "forcing almost every player to choose Romans, especially since it is part of the DLC (requiring purchasefor competitive advantage)": "Regarding Top-B Balance in (b), when we assume there are 9 categories considering counter rela-tionships, it showed that one category is dominated. According to our ad-hoc analysis in Appendix A.3.1,it is an economic powerhouse category, and the top civilization in this category is Poles. This indicatesthe potential necessity to enhance civilizations within this category on their economic bonuses to improvebalance, as even the best among them, Poles, is dominated by other top civilizations. When we extend the size of the counter table to M = 27, Aztecs and Chinese were identified as dominated.This suggests a review of the counter relationships within their category might be warranted to discusspotential improvements. Generally, the balance is commendable, with 24 non-dominated civilizations. When we assume the counter relationships can be very complex and it is worthwhile for expert players tomemorize a large counter table, we find that there is no truly dominated civilization in the M = 81 setting.All civilizations are assigned to distinct categories and show their advantages in specific matchups. Theaccuracy of strength relations in M = 81 is 98.9%. This evidence shows that Age of Empires II is balancedwhen counter relationships are meticulously examined. The game features a complex counter loop, allowingfor a counter civilization to almost any other. However, there is still room for balance improvement forbeginner players if we do not want such a large counter table. These insights provide game developers with guidance on addressing balance weaknesses in future updates.They also offer players a deeper understanding that even a generally strong civilization, like Romans, hasspecific counter civilizations. Traditional game balance techniques, which often aim for a fair win rate (e.g.,50% in 2-player zero-sum games), might overlook the intricacies of counter relationships and game theorywhen merely weakening strong comps or strengthening weak ones.",
  "Case Study on Hearthstone": "When we examine another game, Hearthstone, we first discard decks with fewer than 100 game records inour dataset to ensure the analysis is not biased by outliers. There are 58 decks remaining after our filtering. From the Top-D Diversity in (a), the balance is not good since even with a 4% tolerant win valuegap, there are only 3 decks considered not dominated. The top deck is Treant Druid with a strength of1.48915 (please see for the strengths of other decks), and it is clearly too strong. When we check Top-B Balance in (b) under M = 3, Treant Druid dominates the others. If weextend the counter table to M = 9 (we provide an ad-hoc category analysis in Appendix A.3.2), the balanceis surprisingly good with 9 Top-B Balance. This is because there is a clear counter deck, Aggro Paladin(1.17278), to Treant Druid. Even though Aggro Paladin has a general strength of only 1.17278, it hasa strong counter ability to Treant Druid, and all top decks in M = 9 have their advantages. When we use a larger counter table, M = 27 or M = 81, the game is balanced enough since the differencebetween Used M and Top-B Balance is not significant.This analysis suggests that for this version ofHearthstone, Treant Druid requires specific adjustments. The counter relationships are balanced.",
  "Discussion on Different Types of Balance Measures": "When considering game balance measures, the major concerns are the type of information these measuresprovide and how this information can help modify the game mechanics. We focus on measures with fewersubjective factors, specifically those related to improving players chances of winning. Measures dependenton player preferences, such as use rate, learning difficulty, popularity, and other factors, are not included inthe main discussion. Starting with the most common measure, win rates, it is very clear and general in PvP games. Whether usinga simple win rate or a more detailed win rate with specific opponent compositions, making each compositionhave similar strength is a common idea (Becker & Grlich, 2020). Achieving an average 50% win rate foreach composition in an average case without considering its opponents is a specific solution. Designers cancheck the win rate of each composition and increase the strength of those below 50%, commonly referredto as \"buffing\" in the player community. For compositions with over 50% win rates, especially those with alarge advantage, designers may try to weaken their strength, referred to as \"nerfing\" in the player communityor develop new specific compositions to counter them. However, these changes may not always align withplayers desires for entertainment or satisfaction with game depth. Players often pursue diversity or wantto demonstrate individuality with different strategy settings (Rheinberg, 2020). Keeping every setting orcomposition at the same strength level can violate this intention. Therefore, studying different distributionsof strength over win rates can provide various solutions, not just achieving 50% average case win rates. OurTop-D Diversity and Top-B Balance measures also rely on win rates but focus on different aspects like thetolerant win value gap and the size of meaningful counter relationships. Using another possible balance measure, the entropy of strategy, especially strategies that reach Nash equi-librium (Pendurkar et al., 2023), can be considered another application of win rates. A policy reachingNash equilibrium implies its opponent cannot change itself to gain more benefits. When balance is definedby maximizing the entropy of this policy, it suggests increasing the strength of low sampling probabilitystrategies and reducing the advantages of high sampling probability strategies. This idea is similar to thegoal of achieving 50% win rates or the same average case strength but defined on a more complex relationshipwith Nash equilibrium. However, this idea may have the same limitation as achieving 50% win rates sinceit cannot explicitly differentiate those compositions that share the same strength relations: for example,making all strategies nearly the same. Therefore, Pendurkar et al. (2023) also proposed using a parameterregularization term to trade off the choice of entropy, which may guide parameters to the same strength,and the inherent diversity of game settings. Previous measures can help with game balance; however, they cannot guarantee that the updates would nottend to reduce the inherent diversity of the game mechanics since there is usually a global optimal solutionthat sets all parameters for different compositions nearly the same. Although this result is very balanced, it isalso boring for players since there is almost one actual composition decorated as several different compositionsor at least sharing the same distribution of win values. If we analyze the potential effect of these balancechanges from the players experience angle, it would converge to a case with not improved balance since theactual game content would be very similar to one strong composition dominating the game since we do nothave to study which composition is better, all have the same strength, except the mechanism or playstyle ofeach composition is different. This raises the question: what result are players pursuing for game balance?",
  "Conclusion and Future Works": "The quantification of balance in competitive scenarios is crucial for analyzing how many participants aremeaningful. This paper focuses on a special case of two-team zero-sum competitive scenarios, PvP gamecompositions. With our approximations of rating and counter relationships, domination relationships cannow be quantified efficiently.In the past, most balancing techniques have primarily relied on win rateanalysis. Our experiments, conducted in popular online games, underscore the efficacy and practicality ofour approach. We believe our work enhances the tools available for game balancing, leading to more balancedand engaging player experiences. There are still many topics to explore further in the realm of PvP game compositions. For example, wehave only considered pre-built compositions, but measuring the balance of elements that form a compositionis also important for games with a vast number of element combinations, such as the individual cards in",
  ": An example of extending the classical Rock-Paper-Scissors to more complex cases": "Hearthstone, the specific tech tree in Age of Empires II, or even the equipment in League of Legends. Forgames where it is difficult to enumerate all compositions, considering composition building first is essential. Expanding our focus to broader applications, our approach can be applied to domains where the assessmentof competitor strength is crucial in competitive scenarios, such as sports, movie preference, peer grading,elections, and language model agents (Chen & Joachims, 2016; Zheng et al., 2023). In the realm of cutting-edge artificial intelligence research, our approach could offer insights into multi-agent training with counterrelationships to exploit weaknesses (Vinyals et al., 2019) and potentially benefit fields like AI safety for attackand defense analysis (Amodei et al., 2016). Thus, our methods hold promise for a wide range of applications,marking a step forward in the quantitative analysis of competitive dynamics.",
  "Broader Impact Statement": "Our rating and counter tables are learned models and do not guarantee that the results will always bethe same. It is necessary to carefully check and train several models for critical applications to ensure theresults are not based on random guesses. Our balance measures focus on helping pinpoint the advantages andweaknesses of each composition rather than identifying a single dominating composition. However, if misused,this approach could potentially aid in creating market monopolies rather than improving balance.It isimportant to use these measures ethically and with the intention of promoting fair competition. Additionally,our methodology is built on a scalar rating system and tested on two-team zero-sum symmetric games, whichmay limit the applicability of our balance measures to other types of games, especially those without a winrate. We appreciate all the valuable feedback and insights from everyone who reviewed this research, whichgreatly helped in refining and improving this work. Additionally, we would like to thank Gamania DigitalEntertainment Co., Ltd. (Gamania) for inspiring the research topics related to game balance in our earlystudies.",
  "Ralph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block designs: I. the method ofpaired comparisons. Biometrika, 1952": "Jane Bromley, James W. Bentz, Lon Bottou, Isabelle Guyon, Yann LeCun, Cliff Moore, Eduard Sckinger,and Roopak Shah. Signature verification using A \"Siamese\" time delay neural network. InternationalJournal of Pattern Recognition and Artificial Intelligence (IJPRAI), 7, 1993. Nathaniel Budijono, Phoebe Goldman, Jack Maloney, Joseph B Mueller, Phillip Walker, Jack Ladwig,and Richard G Freedman. Ludus: An optimization framework to balance auto battler cards. In AAAIConference on Artificial Intelligence (AAAI), 2022.",
  "Aron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. InAdvances in Neural Information Processing Systems (NeurIPS), 2017": "Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Michal Mathieu, Andrew Dudzik, Junyoung Chung,David H. Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss,Ivo Danihelka, Aja Huang, L. Sifre, Trevor Cai, John P. Agapiou, Max Jaderberg, Alexander Sasha Vezh-nevets, Rmi Leblond, Tobias Pohlen, Valentin Dalibard, David Budden, Yury Sulsky, James Molloy,Tom Le Paine, Caglar Gulcehre, Ziyun Wang, Tobias Pfaff, Yuhuai Wu, Roman Ring, Dani Yogatama,Dario Wnsch, Katrina McKinney, Oliver Smith, Tom Schaul, Timothy P. Lillicrap, Koray Kavukcuoglu,Demis Hassabis, Chris Apps, and David Silver. Grandmaster level in starcraft ii using multi-agent rein-forcement learning. Nature, 2019. Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, JasonBaldridge, and Yonghui Wu. Vector-quantized image modeling with improved VQGAN. In InternationalConference on Learning Representations (ICLR), 2022. Jiahui Zhang, Fangneng Zhan, Christian Theobalt, and Shijian Lu. Regularized vector quantization for tok-enized image synthesis. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2023. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena.In Advances in Neural Information Processing Systems(NeurIPS), 2023.",
  "A.1Enhancing Codebook Utilization Through VQ Mean Loss": "We tackle the issue of underutilization in codebooks for Vector Quantization (VQ) by introducing the VQMean Loss technique. This method significantly improves the use of the embedding space in VQ models. Asdepicted in , traditional VQ-VAE approaches depend on nearest neighbor selection for determininglatent vectors, which can lead to sparse utilization of the codebook. The integration of VQ Mean Lossobviates the need for meticulous codebook initialization or hyperparameter fine-tuning to overcome lowutilization problems.",
  "A.2Illustrative Examples of Top-D Diversity Measure and Top-B Balance Measure": "We elucidate the Top-D Diversity Measure with an example in . When utilizing a single scalar forstrength measurement of compositions, identifying the composition with maximum strength (denoted as Ain the figure) is straightforward. By defining an acceptable win value gap G, for instance, G = 5%, whichmay represent a games error margin, compositions with a rating equal to or greater than 1.47 are consideredplayable. Such a demarcation is akin to tier tables commonly created within gaming communities, suggestingthat our rating table could facilitate the construction of insightful analyses or tier tables.",
  ": The rating table for Age of Empires II, depicting the overall strength and viability of eachcivilization": "The rating table () complements the counter table by offering a quantitative assessment of eachcivilizations overall strength. This allows for a broader perspective beyond direct match-ups, giving insightinto how each civilization fares in the general meta. The counter table not only outlines how civilizations within the same group perform against each other butalso illustrates the inherent strengths and weaknesses they possess against other groups, shaped by theirdistinct technologies, units, and economic bonuses. For instance, the Heavy Cavalry Group is susceptible to the powerful Anti-Cavalry capabilities of civilizationslike the Hindustanis or Goths. They may also find themselves at a disadvantage against Infantry-dominatedfactions but can leverage their cavalrys mobility to gain an upper hand against civilizations that rely heavilyon gunpowder or archery for ranged attacks. Goths, while typically facing a disadvantage in matchups against",
  "A.3.2Hearthstone": "The 99 Hearthstone counter table () represents a strategic breakdown of deck matchups, reflectinghow various playstyles interact within the game. Each category is defined by distinct tactical approaches, andthe table illustrates the expected performance of these categories against one another, with green indicatinga favorable matchup and red indicating a disadvantage.",
  ": The 9 9 counter table for Hearthstone, detailing the strengths and weaknesses of various deckarchetypes": "1. High-Value Control Archetypes (Category 1): These decks, often Singleton with no duplicatecards, rely on high-value individual cards and powerful board clears, such as the notorious RenoJackson hero card, to win by value over time. Examples include Highlander Shaman, Paladin, andDemon Hunter, as well as Control Priest and Blood Death Knight. 2. Tempo-Dependent Decks (Category 2): These archetypes require a specific mana curve toplay cards efficiently and can dominate when curving out correctly. Decks like Treant Druid andBig Rogue, which need to establish a board and then buff it, fall into this category.",
  ". Aggressive Paladin Archetypes (Category 9): These decks spread the board with multiplethreats early on and aim to end the game before the opponent stabilizes": "The analysis reveals that control archetypes (Category 1) are effective against high-value creature decks(Category 4) due to their abundance of removal options. However, they struggle against decks with late-game OTK capabilities (Category 8) because such decks can bypass control strategies with a sudden wincondition.Tempo (Category 2) decks falter against stable midrange (Category 4) due to board clearsdisrupting their momentum, while also struggling against the faster-paced aggro decks (Category 9) thatcan establish a quicker board presence. Combination-reliant decks (Category 3) tend to underperform against aggressive Paladin strategies (Category9) due to the Paladins ability to conclude games before combos can be assembled. Meanwhile, the snowballpotential of decks in Categories 4 and 7 makes them strong against tempo decks but vulnerable to controlarchetypes with multiple board clears. Value-oriented decks with combo finishers (Category 8) excel against control decks by circumventing theirgradual value game with a sudden win condition, yet they might struggle against decks with large minionsthat their additional resources cant efficiently counter. Through the counter table, Hearthstone players can better strategize their deck choices and gameplay,considering the prevalent matchups in the current meta. The table thus serves as a critical tool for playersaiming to optimize their strategies and achieve a higher win rate in competitive play.",
  "A.3.3Brawl Stars": "In games with complex combinations for building compositions like Brawl Stars, it requires much more gameknowledge to explain the possible meaning of counter categories from learning. For example, if we considerthe composition number of only three heroes in Brawl Stars, there are C643= 41664 available compositions.It is not easy to check all these compositions and further check their pairwise relationships. According to, the counter relationship is not clear in Brawl Stars with three heroes and the corresponding gamemodes and maps. Using NRT can achieve 95.9% average accuracy and adding an M = 81 counter tableonly improves 1.3% extra accuracy. This implies that the scalar rating value already reflects most strengthrelationships and there is not much significant information that an extra counter table can help with. Thus,we trained another composition setting in Brawl Stars. We only considered two-hero combinations as thecompositions, and for each game match, we split it into C32 C32 = 9 game matches as the training andtesting dataset. The corresponding accuracies of this setting are listed in . In this setting, the accuracy improvement with the M = 9 counter table is 3.5%, implying that somemeaningful categories are identified for describing counter relationships. We selected the best model of the",
  ". Quick Burst and Control: Heroes like Chester, Shelly, and Buzz possess quick burst and controlabilities": "Since there are C642available compositions and our limited game knowledge, we do not guarantee thatthese analyses are entirely correct, but we can find some interesting relationships. High Mobility andSustained Output has an advantage over the Long-Range Control and Area Damage category due totheir high mobility but may be countered by Sustained Output and Support in modes requiring prolongedengagement. The three categories with versatility: 2, 6, and 8 tend to form a cycle where 2 > 6 > 8 > 2. These kinds of ad-hoc explanations may help game designers to change the game mechanisms from a moregeneral aspect. If there is a game that is very hard to explain the counter table or tables diverge whentrained from different random seeds, we may need more detailed attributes of the combination elements tosummarize a reasonable update direction.",
  "Hearthstone: 91-dimensional one-hot vector for deck naming": "Brawl Stars: 115-dimensional vector encoding the complex dynamics of Trio Modes. This includesa binary encoding for the presence of 64 unique heroes, one-hot encodings for 43 distinct maps plusan indicator for any map not listed, and a similar encoding scheme for the 6 game modes and anyunrecorded mode. This encoding captures the essence of team compositions, map strategies, andgame modes, essential for predicting match outcomes in Brawl Stars.",
  "A.4.1Formulations of Strength Relation Methods": "For a more formal definition of the methods used in the strength relation classification task, we use thefollowing definitions. For each match outcome, we have two compositions, A and B, and a strength relationlabel with three valid states: weaker/same/stronger. The ground truth of each matchup A, B is determinedby the tabular PairWin method, which simply counts the average win value of this composition matchup.For example, if a matchup A, B has 100 game results with 60 wins, 30 losses, and 10 ties, the win value isx = (60 1.0 + 30 0.0 + 10 0.5)/100 = 0.65. For x > 0.501, the strength relation label is stronger; forx < 0.499, the strength relation label is weaker; and for 0.499 x 0.501, the strength relation label issame. WinValue: Given a win value estimator WinV alue(C) of composition C without considering its op-ponent, if WinV alue(A)WinV alue(B) > 0.001, the prediction is stronger. If WinV alue(B)WinV alue(A) > 0.001, the prediction is weaker. If |WinV alue(A) WinV alue(B)| 0.001,the prediction is same. PairWin: Given a win value estimator PairWin(C1, C2) of compositions C1 and C2 in a matchup,if PairWin(A, B) > 0.501, the prediction is stronger. If PairWin(A, B) < 0.499, the predictionis weaker. If 0.499 PairWin(A, B) 0.501, the prediction is same."
}