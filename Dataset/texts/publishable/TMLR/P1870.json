{
  "Abstract": "Continuous-time dynamic graphs (CTDGs) are essential for modeling interconnected,evolving systems. Traditional methods for extracting knowledge from these graphs oftendepend on feature engineering or deep learning. Feature engineering is limited by the man-ual and time-intensive nature of crafting features, while deep learning approaches suffer fromhigh inference latency, making them impractical for real-time applications. This paper intro-duces Deep-Graph-Sprints (DGS), a novel deep learning architecture designed for efficientrepresentation learning on CTDGs with low-latency inference requirements. We benchmarkDGS against state-of-the-art (SOTA) feature engineering and graph neural network methodsusing five diverse datasets. The results indicate that DGS achieves competitive performancewhile inference speed improves between 4x and 12x compared to other deep learning ap-proaches on our benchmark datasets. Our method effectively bridges the gap between deeprepresentation learning and low-latency application requirements for CTDGs.",
  "Introduction": "Graphs serve as a foundational structure for modeling and analyzing interconnected systems, with appli-cations spanning in computer science, mathematics, and life sciences. Recent studies have emphasized thecritical role of dynamic graphs, which capture evolving relationships in systems like social networks andfinancial markets (Costa et al., 2007; Zhang et al., 2020; Zhou et al., 2020; Majeed & Rauf, 2020; Febrinantoet al., 2023). Graph structure representation is crucial for encoding complex graph information into low-dimensional em-beddings that are usable by machine learning models. This task is particularly challenging for dynamicgraphs.Traditional graph feature engineering methods rely on manually crafted heuristics to capturegraph characteristics, necessitating domain knowledge and considerable time to engineer and test new",
  "Published in Transactions on Machine Learning Research (11/2024)": "feasible, which in turn can help to learn long-term dependencies and to use online learning. To validate theeffectiveness and applicability of DGS, we conducted a thorough evaluation using two internal AML datasetsand additional datasets from various fields, thereby demonstrating its versatility. Future work includes exploring alternative normalization functions or activation functions beyond softmaxand incorporating advanced optimization algorithms such as Adam to replace the current use of stochasticgradient descent for updating DGS parameters during forward-mode AD. Additionally, a significant enhance-ment under consideration is enabling input-dependent adaptability for the parameters and , aiming toimprove the models responsiveness to varying input features and enhance overall performance, similar toapproaches used in gated recurrent neural networks. Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: A next-generation hyperparameter optimization framework. In Proceedings of the 25th ACM SIGKDD interna-tional conference on knowledge discovery & data mining, pp. 26232631, 2019. Atilim Gunes Baydin, Barak A Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. Automaticdifferentiation in machine learning: a survey. Journal of Marchine Learning Research, 18:143, 2018.",
  "We introduce Deep-Graph-Sprints (DGS), a method for real-time representation learning of CTDGs,optimizing latency and enhancing the ability to capture long-term dependencies, see Sections 3": "We benchmark DGS against SOTA methods, in node classification and link prediction tasks, demon-strating on par predictive performance while achieving significantly faster inference speedups upto 12x faster than TGN-attn (Rossi et al., 2020), and up to 8x faster than both TGN-ID (Rossiet al., 2020) and Jodie (Kumar et al., 2019). Detailed results are provided in .",
  "Background: Overview of Automatic Differentiation Modes": "Deep learning depends significantly on credit assignment, a process identifying the impact of past actions onlearning signals (Minsky, 1961; Sutton, 1984). This process is essential for reinforcing successful behaviorsand reducing unsuccessful ones. The capability of assigning credit in deep learning models depends on the differentiability of learning signalsenabling the use of Jacobians for this purpose (Cooijmans & Martens, 2019). A key technique in this contextis automatic differentiation (AD), a computational mechanism for the derivation of Jacobians through apredefined set of elementary operations and the application of the chain rule, applicable even in programswith complex control flows (Baydin et al., 2018). In AD, depending on the direction of applying the chainrule, three strategies stand out: forward mode, reverse mode (often termed backpropagation), and mixedmode. Forward mode involves multiplying the Jacobians matrices from input to output. Reverse mode, atwo-phase process, first executes the function to populate intermediate variables and map dependencies, thencalculates Jacobians in reverse order from outputs to inputs (Baydin et al., 2018). Mixed mode combinesthese approaches. Temporal models, such as recurrent neural networks (RNNs) and GNNs for temporal graphs, pose specificchallenges for backpropagation due to their memory-intensive requirements. The memory complexity forstoring intermediate states across a history significantly impacts the feasibility of full backpropagation.",
  "Method": "In this section we detail our low latency node representation learning method, namely Deep-Graph-Sprints(DGS). We start by explaining the main components that form its architecture and then we detail eachone of them. Furthermore, we detail the training paradigm that distinguishes our method, highlighting itsmemory demands and ability to capture long-term dependencies compared to existing approaches throughits RTRL-based approach. Additionally, we explain our method during inference, demonstrating how itachieves low latencies.",
  ". Neural Network (NN): responsible for decision-making processes, such as classification. It uses theembedding provided by the ER component to generate a task specific output": "The ER component is particularly noteworthy for its role in updating the embeddings of nodes or edges,thereby enriching them with detailed attributes and relationships context within the network. These em-beddings are then input into the neural network, which is tailored to specific applications. For instance, innode classification, the network evaluates each node associated with a new edge, with the score reflectingthe networks interpretation from the representations provided by the ER component.",
  "St = St1 + (1 )(1 )( Ft) + St1(1)": "Here, St denotes the state of a node at time t, evolving from its previous state St1 and the state of itsimmediate neighbor St1, while also incorporating the current edges features Ft, encoded through the function, which bucketizes these features. The function maps numerical features into a one-hot encodedvector based on predetermined buckets (e.g., buckets corresponding to specific percentiles). This bucketiza-tion provides a deterministic representation of the input features, which requires manual tuning to define thebuckets and results in sparse, high-dimensional feature vectors. The coefficients and are scalar forgettingfactors that modulate the impact of neighborhood and past information on the current state. Although Graph-Sprints demonstrates rapid processing capabilities and performs on par with leading tech-niques, it faces several limitations in practical applications. These include the complex and time-consumingtuning processes required for its feature extraction and decision-making components. For instance, to tunethe forgetting coefficients or to define the bucketed features edges of the function for every feature. More-over, the models expressivity is constrained by the uniform application of scalar forgetting coefficients acrossall features, limiting its ability to capture the unique temporal dynamics of each feature. Finally, the use oflarge number of buckets per feature significantly increases the memory requirements especially in datasetswith many features. In contrast, our proposed methodology, while drawing inspiration from Graph-Sprints,goes beyond traditional feature engineering by employing a dynamic learning mechanism for embeddings.The state update equation in our methodology is illustrated in Equation 2:",
  "(2)": "In this equation, we introduce vectorized forgetting coefficients, and , each corresponding in dimensional-ity to the state vector, and modulate the weighting between current and historical information, and betweenself and neighborhood information, respectively. Each dimension of and corresponds to unique forgettingrates for each embedded feature in the state vector, which itself consists of a vector as we will detail below.",
  "The embedding matrix W is tasked with mapping input features into a vector of the same size as the statevector": "Inspired by the Graph-Sprints feature representations, where the buckets that belong to the same featuresum to one, we utilize the softmax function () to achieve analogous representations.Moreover, we employsoftmax temperature scaling (Guo et al., 2017) where higher temperatures result in softer distributions. DGSenhances model expressiveness and optimizes memory usage by incorporating multiple softmax functions,each applied to a segment of the product between the embedding matrix W and the feature vector Ft. Thenotationmi=1 denotes the concatenation of the results obtained by applying the m softmax functions. Eachfunction is applied to the product of the i-th portion of the embedding matrix, Wi, and the features valuesFt, as illustrated in . This strategy not only aids in reducing computational and memory demandsby limiting the dependency of each state element to a specific segment of the embedding matrix and therebylowering the Jacobians dimensionality but also introduces a modular structure. : Schematic illustration of state calculation based on Equation 2. This example demonstrates thecomputation of node state at time t with a state size of s = 6, three softmaxes (m = 3), and thus two rowsper softmax from the embedding matrix W (h = s/m = 2). The number of input features is f = 4.",
  "Neural Network (NN)": "The NN component is a feedforward neural network, that encompasses multiple layers. The configurationof this component is subject to optimization depending on the task at hand. This optimization includesdecisions such as the number of layers, the size of each layer, and the incorporation of normalization layers. Although Equation 1 (GS) and Equation 2 (DGS) present similarities.There are several aspects thatdistinguish Graph-Sprints and DGS methods. GS is a feature engineering approach with fixed embeddings,while DGS employs deep learning with learnable embeddings. Moreover, parameter optimization in GS isseparate, whereas DGS allows end-to-end optimization. compares both methods.",
  "long-term dependencies (as detailed in ), and ensuring efficient learning while accommodating thememory constraints and structural complexities of graph data": "In typical ML scenarios, the complexity of forward-mode AD limits its applicability. Nonetheless, forward-mode AD is applicable in situations requiring a manageable number of Jacobian computations, offeringefficient Jacobian propagation through computational graphs. In the DGS method, the feasibility of forward-mode AD is supported by two main factors. First, DGS is dominated by elementwise multiplications, wheredifferent elements of a state vector are not mixed together. Second, the implementation of multiple softmaxfunctions limits the dependency of each state element to a segment of the embedding matrix W, thus reducingthe computational and memory requirements.",
  "and S": ", achieving a computational and memory complexity of O(s), wheres represents the size of the state vector. In contrast, performing these operations via matrix multiplication,implying and are (s s) matrices, would increase the complexity to O(s3). Regarding the embeddingmatrix W, with dimensions s f (embedding size by the number of features), the application of a singlesoftmax function over the entire embedded vector would result in Jacobians SW with dimensions f s2,leading to computational and memory complexities of O(f s2). However, DGS mitigates this throughthe deployment of multiple softmax functions, each managing a segment of the state. With m softmaxfunctions, each addressing a subset of h = s/m rows, the computational and memory requirements areeffectively reduced to O(f h s), demonstrating the methods efficiency in optimizing both computationaland memory resources. Furthermore, one can easily fix h to a predetermined value and optimize the state sizes to be multiples of this parameter. Therefore, assuming a fixed h, the total computational complexity scaleslinearly with respect to the state size s. This property demonstrates a better scaling than backpropagation,which scales with s2. These factors collectively justify the selection of forward-mode AD for the differentiationprocess in the ER component of our architecture. The Jacobians updates were implemented manually usingPyTorch (Fey & Lenssen, 2019). In the NN component, number of learnable parameters varies based on model architecture, primarily involv-ing the networks weights. The parameters of the NN component are optimized using backpropagation, andto implement that we also leverage the functionalities of PyTorch. As a result, the architecture of the DGSmethod employs a mixed-mode AD approach, as illustrated in . Compared to the GS approach, allowing the key parameters (, , and W) to be learned from the data over-comes the limitations of separate tuning processes, thereby simplifying the training procedure. Additionally,this enables the use of vectorized forgetting coefficients instead of scalar ones, which significantly enhancesthe models expressivity.",
  "Mini-Batch Training": "To expedite the training process, we employ mini-batch training, wherein the input comprises a batch ofedges.In cases where a singular node appears multiple times within a single batch, each occurrence isassociated with the same prior node state, which represents the most recent state prior to the batchsexecution. This methodology implies that nodes contained within the same batch do not utilize the mostcurrent information due to the prohibition of intra-batch informational exchange. One can also implement abatch strategy similar to the one implemented by the Jodie method (Kumar et al., 2019), where batch sizeis dynamic and nodes only appear once in the same batch.",
  "Inference Process": "The inference phase is characterized by the absence of gradient computation, which simplifies the overallprocedure. In the streaming context, as elaborated in .1, the occurrence of an edge triggers an updatein the states of the nodes interconnected by this edge, employing Equation 2 for the update mechanism.Subsequently, these updated states are ingested by the NN component. The nature of the input to the NNcomponent is task-dependent: it may constitute a singular node state for node classification tasks, or theconcatenation of two node states for tasks such as link prediction.",
  "Mini-Batch Inference": "To accelerate inference in scenarios suitable for batch processing, we employ a mini-batch inference strategy.This approach updates the states of nodes or edges within each batch simultaneously. When a node appearsmultiple times within the same batch, as in the training phase, each instance is linked to the same priornode state, which is the most recent state before the batchs execution. Consequently, there is no exchangeof states within the batch. Note that this is optional and similarly to mini-batch procedure in training wecan leverage a different strategy. Following the parallel updates, the aggregated states are inputted into the neural network (NN) component.This step generates a batched output tailored to the task, whether it involves node classification, linkprediction, or any other relevant activity. It is important to note that the DGS method is fully online and supports both single-sample and mini-batchinference, offering flexibility depending on the scenario. Mini-batch inference, when suitable, improves bothtraining and inference speeds even further. In our experiments (Section ??), we utilize mini-batch inferenceto ensure comparability with other experimental setups used in state-of-the-art studies.",
  "Baselines": "We compare the performance of our method against several baselines. The first simple baseline, called Raw,trains a machine learning model using only raw edge features. Another baseline is Graph-Sprints (Eddinet al., 2023) a graph feature engineering method, which we refer to by GS. The GS baseline uses the same MLclassifier used by the Raw baseline but diverges in the features used for training GS employs Graph-Sprintsencoded features, whereas Raw employs the raw edge features. An additional baseline set comprises SOTA GNN methods, specifically TGN (Rossi et al., 2020). Our TGNimplementation, based on the default PyTorch Geometric implementation, differs from the original paper byrestricting within-batch neighbor sampling, for a more realistic scenario. For the node classification tasks, our TGN implementation diverges slightly from the default PyTorch Geo-metric implementation, which was originally implemented for link prediction, by updating the state of targetnodes with current edge features before classification. In contrast, for link prediction, this update occurs afterthe classification decision, aligning with the PyTorch Geometric implementation. These settings are typicalfor node classification and link prediction, respectively, and both GS and DGS follow the same setup1. Several TGN variants were used: TGN-attn, aligning with the original papers best variant, TGN-ID, asimplified version focusing solely on memory module embeddings, and Jodie, which utilizes a time projectionembedding with gated recurrent units. TGN-ID and Jodie baselines, which do not necessitate neighborsampling, were chosen for their lower-latency attributes compared to TGN-attn. All GNN baselines (TGN-ID, TGN-attn, and Jodie) used a node embedding size of 100.",
  "Optimization": "The hyperparameter optimization process utilizes Optuna (Akiba et al., 2019) for training 100 models. Initial70 trials are conducted through random sampling, followed by the application of the TPE sampler. Eachmodel incorporated an early stopping mechanism, triggered after 10 epochs without improvement. enumerates the hyperparameters and their respective ranges employed in the tuning process of DGS and thebaselines. Importantly, the state size for DGS is fixed to 100 in the node classification task, achieved by setting theproduct of the number of softmax functions and the number of rows per softmax to 100 (m h = 100).This aligns with the configurations of other GNN baseline models (TGN-ID, TGN-attn, and Jodie) toensure comparability. In the link prediction task, we set the DGS state size to 250 because a state size of100 was insufficient for achieving comparable performance. Despite this larger state size compared to theGNN baselines, the DGS method has, on average, 2.5 times fewer learnable parameters than the TGN-attnbaseline. Additionally, only 35% of the learnable parameters in DGS on average are attributed to the ERcomponent, with the remaining 65% belonging to the classification head (further detail in ).",
  "Datasets": "We leverage five different datasets, all CTDGs and labeled. Each dataset is split into train, validation, andtest sets respecting time (i.e., all events in the train are older than the events in validation, and all events invalidation are older than the events in the test set). Three of these datasets are public (Kumar et al., 2019)from the social and education domains. In these three datasets, we adopt the identical data partitioningstrategy employed by the baseline methods we compare against, which also utilized these datasets. Theother two datasets are real-world banking datasets from the AML domain. Due to privacy concerns, we cannot disclose the identity of the FIs nor provide exact details regarding the node features. We refer to thedatasets as FI-A and FI-B. The graphs in this use case are constructed by considering the accounts as nodesand the money transfers between accounts as edges. shows the details of all the used datasets. 1In the original GS paper, link prediction for the GS was performed using the same setup as node classification, i.e. updatingthe state before classification. We believe it is more fair to use the link prediction setup as all other models, hence our GS resultson link prediction tasks are not directly comparable to the original paper. Similarly, the TGN baselines in node classificationtasks used the link prediction setup in the original paper, hence those results are also not directly comparable here.",
  "Node classification": "In the node classification task, given the dataset characteristics detailed in , we address binaryclassification with class imbalance. To evaluate performance, we calculate the area under the ROC curve(ROC-AUC), referred to as AUC for brevity, by plotting the True Positive Rate (TPR) against the FalsePositive Rate (FPR) across various classification thresholds, and then computing the area under the curve.The AUC is calculated using the sklearn library in python. The results for node classification are detailed in , displaying the average test AUC std for theexternal datasets and the AUC for the AML datasets. To obtain these figures, we retrained the bestmodel identified through hyperparameter optimization across 10 different random seeds. It is important to note that the GNNs and GS baselines leverage the latest edge information, similar to theDGS method. This means they update the node state with the most recent information before classifyingthe node. We have highlighted the best and second-best performing models for each dataset. To provide an overview,we include a column showing the average rank, representing the mean ranking computed from all datasets.DGS achieves either the highest or the second-highest scores in four out of the five datasets. The exceptionis the Mooc dataset, where GNN baselines surpass our method. We did note that there is some overfittingof the GNN baselines. This is due to the extreme scarcity in positive labels, which resulted in the validationmetrics being badly correlated with the test metrics for these baselines. Counter-intuitively, although not reported here2, we observed that the performance of the GNN baselines im-proved when evaluated without leveraging the latest edge features, which could indicate a reduced overfittingto the validation dataset.",
  "Link Prediction": "For the link prediction task, the evaluation process generates n 1 negative edges for each positive edge,where n denotes the number of nodes (possible destinations) in the graph. We then measure the meanreciprocal rank (MRR), which indicates the average rank of the positive edge. An MRR of 50% impliesthat the correct edge was ranked second, while an MRR of 25% implies it was ranked third. Additionally,we measure Recall@10, which represents the percentage of actual positive edges ranked in the top 10 scoresfor every edge. We retrain the hyperparameter-optimized model using 10 random seeds and report the average test MRR standard deviation and Recall@10 standard deviation in . Evaluations were conducted in bothtransductive (T) and inductive (I) settings.The transductive setting involves predicting future links ofnodes that could be observed during training, while the inductive setting involves predictions for nodes notencountered during training.",
  "Inference Runtime": "DGS has a primary goal of achieving reduced inference times. Comparative latency assessments were con-ducted amongst DGS, Graph-Sprints, and baseline GNN models. These assessments involved processing200 batches, each containing 200 events, across distinct datasets (Wikipedia, Mooc, and Reddit) for nodeclassification task. The average times were computed over 10 iterations. Tests were performed on a Linux PCequipped with 24 Intel Xeon CPU cores (3.70GHz) and an NVIDIA GeForce RTX 2080 Ti GPU (11GB). Notethat all experiments, including those for link prediction and node classification mentioned in the previoussections, used the same machine. As depicted in , DGS exhibited significant speed advantages. On the Reddit dataset, it was morethan ten times faster than the TGN-attn GNN baseline. For the smaller datasets, this speed enhancementranged approximately between 5 and 6 times, while maintaining a competitive speed with the low latencyGS baseline. Notably, the runtime of DGS remained stable and was not influenced by the number of edges inthe graph, as demonstrated in . In the Wikipedia and Reddit datasets, DGS consistently took 0.24seconds. In contrast, TGN-attn exhibited a runtime increase from 1.2 seconds in the Wikipedia dataset toapproximately 3 seconds in the Reddit dataset. This stability suggests that we may obtain higher speed gainsin larger or denser graphs, especially considering that inference times in the TGN-attn baseline are impactedby the number of graph edges.Moreover, When benchmarked against other GNNs baselines (TGN-ID,Jodie), DGS consistently demonstrated significantly lower inference latency. In comparison to the GS framework, known for its low latency, DGS generally exhibited marginally superiorspeed, especially noticeable in the Wikipedia and Reddit datasets, with latencies of 0.24 versus 0.29 seconds.This performance gain can be attributed to the higher feature count in these datasets (172 features), whichpotentially increases the processing time for GS due to the elevated feature volume. In contrast, for theMooc dataset, which has only 7 edge features, GS showed a slight gain in speed (0.24 versus 0.28 seconds).",
  ": Trade-off between AUC and runtime": "function but transitions to vectorized parameters, and . This modification aims to explore the effects ofincreasing these parameters complexity on the performance of the model. The final variant, referred to asDGS, not only incorporates vectorized parameters and but also integrates a learnable embedding matrix.This approach aims to assess the impact of learnable feature embedding matrix. presents the results of the three variants for node classification across five different datasets. Itdisplays the average test AUC standard deviation for the external datasets and the AUC for the AMLdatasets. The DGS variant showes to be on average the best variant.",
  "Graph Representation Learning": "Most existing graph representation learning methods focus on static graphs, thereby neglecting temporaldynamics (Perozzi et al., 2014; Tang et al., 2015; Grover & Leskovec, 2016; Hamilton et al., 2017a; Yinget al., 2018).Dynamic graphs, which evolve over time, introduce additional complexities.A commonapproach is to use DTDGs by considering the dynamic graph a series of discrete snapshots and apply staticmethods (Sajjad et al., 2019), but this approach fails to capture the full spectrum of temporal dynamics. To address this limitation, more advanced techniques have been developed to better handle CTDGs. Thesemethods include incorporating time-aware features or inductive biases into the architecture(e.g., (Nguyenet al., 2018; Jin et al., 2019; Lee et al., 2020; Rossi et al., 2020)).",
  "Low-latency Graph Representation Learning": "This section reviews methods for low-latency graph representation learning. For example, APAN (Wanget al., 2021) aims to reduce inference latency by decoupling expensive graph operations from the infer-ence module, executing the costly k-hop message passing asynchronously. While APAN enhances inferenceefficiency, it may use outdated information due to its asynchronous updates, which could impact overallperformance. In contrast, our method, Deep-Graph-Sprints, addresses latency without compromising thefreshness of the information used. Furthermore, Liu et al. (2019) present a real-time algorithm for graph streams that updates node represen-tations based on the embeddings of 1-hop neighbors of a node of interest, and ignoring its attributes. Chu& Lin (2024) propose ETSMLP, a model that leverages an exponential smoothing technique to model long-term dependencies in sequence learning. However, ETSMLP is tailored specifically for sequence modelingand is not applicable to graph-based tasks out-of-the-box. Graph-Sprints (Eddin et al., 2023) offers a featureengineering approach that approximates random walks for low-latency graph feature extraction. Unlike ourapproach, Graph-Sprints requires extensive hand-crafting of features. In addition to inference optimization, several methods address the reduction of computational costs in GNNs.HashGNN(Wu et al., 2021) employs MinHash to generate node embeddings suitable for link prediction tasks,grouping similar nodes based on their hashed embeddings. Another approach, SGSketch(Yang et al., 2022),introduces a streaming node embedding framework that gradually forgets outdated edges, leading to speedimprovements. Unlike our approach, SGSketch primarily updates the adjacency matrix and focuses on thegraph structure rather than incorporating additional node or edge attributes.",
  "Conclusions": "CTDGs are essential for representing connected and evolving systems. The computational and memorydemands associated with performing lookups to sample multiple neighbors limit their feasibility in low-latency scenarios. In this paper, we introduce the real-time graph representation learning method for CTDGs, named DGS.This novel approach addresses the latency challenges associated with current deep learning methods. It alsoobviates the need for manual tuning and domain-specific expertise, which are prerequisites for traditionalfeature extraction methods. The architecture design makes the use of real-time recurrent learning (RTRL)",
  "George Karypis and Vipin Kumar. Metis: A software package for partitioning unstructured graphs, parti-tioning meshes, and computing fill-reducing orderings of sparse matrices. 1997": "Srijan Kumar, Xikun Zhang, and Jure Leskovec. Predicting dynamic embedding trajectory in temporalinteraction networks. In Proceedings of the 25th ACM SIGKDD international conference on Knowledgediscovery and data mining. ACM, 2019. John Boaz Lee, Giang Nguyen, Ryan A Rossi, Nesreen K Ahmed, Eunyee Koh, and Sungchul Kim. Dy-namic node embeddings from edge streams. IEEE Transactions on Emerging Topics in ComputationalIntelligence, 5(6):931946, 2020. Xi Liu, Ping-Chun Hsieh, Nick Duffield, Rui Chen, Muhe Xie, and Xidao Wen. Real-time streaming graphembedding through local actions. In Companion proceedings of the 2019 world wide web conference, pp.285293, 2019.",
  "Marvin Minsky. Steps toward artificial intelligence. Proceedings of the IRE, 49(1):830, 1961": "Giang Hoang Nguyen, John Boaz Lee, Ryan A Rossi, Nesreen K Ahmed, Eunyee Koh, and Sungchul Kim.Continuous-time dynamic network embeddings. In Companion proceedings of the the web conference 2018,pp. 969976, 2018. Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representations. InProceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,pp. 701710, 2014. Emanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, Federico Monti, and Michael Bronstein.Temporal graph networks for deep learning on dynamic graphs.In ICML 2020 Workshop on GraphRepresentation Learning, 2020.",
  "Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, and Kannan Achan. Inductive representationlearning on temporal graphs. arXiv preprint arXiv:2002.07962, 2020": "Dingqi Yang, Bingqing Qu, Jie Yang, Liang Wang, and Philippe Cudre-Mauroux. Streaming graph embed-dings via incremental neighborhood sketching. IEEE Transactions on Knowledge and Data Engineering,35(5):52965310, 2022. Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec. Graphconvolutional neural networks for web-scale recommender systems.In Proceedings of the 24th ACMSIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 974983, 2018.",
  "A.6Comparison of Learnable Parameter Counts": "provides a comprehensive comparison of the learnable parameter counts between DGS and TGN-attn for the link prediction task across inductive and transductive settings, underscoring differences in modelcomplexity. : Comparison of the number of learnable parameters between DGS and TGN-attn in the link pre-diction task, evaluated in both inductive (I) and transductive (T) settings. The ratio indicates the relativeproportion of parameters in TGN-attn compared to DGS."
}