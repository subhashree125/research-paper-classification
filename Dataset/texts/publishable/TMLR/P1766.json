{
  "Abstract": "Many deep learning methods are data-driven, often converging to local minima due tolimited training data. This situation poses a challenge in domains where acquiring adequatedata is difficult for model training or fine-tuning, such as generalized few-shot semanticsegmentation (GFSSeg) and monocular depth estimation (MDE). To this end, we proposea self-trained geometry regularization framework to enhance model training or fine-tuningin scenarios with limited training data using geometric knowledge. Specifically, we proposeto leverage low-level geometry information extracted from the training data and define anovel regularization term, which is a plug-and-play module jointly trained with the primarytask via multi-task learning. Our proposed regularization neither relies on extra manuallabels and data in training nor requires extra computation during the inference stage. Wedemonstrate the effectiveness of this regularization on GFSSeg and MDE tasks. Notably, itimproves the state-of-the-art GFSSeg by 5.61% and 4.26% mIoU of novel classes on PASCALand COCO in the 1-shot scenario. In MDE, it achieves a relative reduction of SILog errorby 16.6% and 9.4% for two recent methods in the KITTI dataset.",
  "Introduction": "The recent advancements in deep learning-based approaches have been significant and promising. Numerousmodern and sophisticated deep neural networks (Dosovitskiy et al., 2020; Liu et al., 2021; Oquab et al.,2023; Carion et al., 2020; Touvron et al., 2022) have been proposed to address a variety of tasks. These deeplearning models are mostly data-driven and require large amounts of training data (Zhou et al., 2017; Denget al., 2009; Russakovsky et al., 2015; Kuznetsova et al., 2020), to avoid the risk of converging to local minima.However, annotating large quantities of training samples in practice is exceedingly challenging, particularlyfor tasks involving dense predictions. For instance, in monocular depth estimation (MDE), collecting large-scale data with paired ground truth is costly (Ranftl et al., 2022; Bhat et al., 2023; Ranftl et al., 2021a;Piccinelli et al., 2024). Additionally, in generalized few-shot semantic segmentation (GFSSeg) (Tian et al.,",
  "Published in Transactions on Machine Learning Research (December/2024)": "Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network.In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 28812890, 2017. Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsingthrough ade20k dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition,pp. 633641, 2017. Shengjie Zhu, Garrick Brazil, and Xiaoming Liu. The edge of depth: Explicit constraints between segmenta-tion and depth. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,pp. 1311613125, 2020.",
  "The major contributions of this paper are:": "We propose a simple yet effective self-trained geometry regularization, which uses the accessiblelow-level geometry information to construct a regularization term via multi-task learning to traindense prediction tasks with less data. We propose a primary-to-auxiliary geometry aggregation module to account for the relationshipbetween the primary and auxiliary tasks.This design encourages the primary branch to gener-ate predictions that benefit the auxiliary task, which differs from post-processing fusion or featuresmoothing in previous methods.",
  "Dense Prediction Tasks with Less Data": "Dense prediction tasks refer to tasks where predictions are required for each pixel, such as depth esti-mation (Laina et al., 2016; Yuan et al., 2022; Patil et al., 2022; Shao et al., 2023a; Yang et al., 2024; Ranftlet al., 2020; Bhat et al., 2023; Li et al., 2023a; Kirillov et al., 2023), semantic image segmentation (Zhaoet al., 2017; Chen et al., 2017; Cheng et al., 2022; Xie et al., 2021; Li et al., 2023a; Isensee et al., 2021),etc. Recently, The Segment Anything model and Depth Anything model have shown that large-scale data isimportant in improving the performance of dense prediction tasks (Ranftl et al., 2021b; Kirillov et al., 2023;Bhat et al., 2023; Yang et al., 2024). However, acquiring large-scale datasets for certain tasks can be chal-lenging. For instance, around 63 million images are collected for training the Depth Anything model (Yanget al., 2024), and 12 million for the Segment Anything model (Kirillov et al., 2023). Training with suchlarge-scale data is resource-intensive. Instead of pursuing large-scale data, we aim to design accessible yeteffective geometry regularization for dense prediction tasks with less data. In this paper, we use generalizedfew-shot semantic segmentation (GFSSeg) and monocular depth estimation (MDE) as two case studies tovalidate our approach, where both tasks face data scarcity issues. Generalized few-shot semantic segmentation is a type of dense prediction task with less data (Tianet al., 2022; Myers-Dean et al., 2021; Liu et al., 2023a; 2024; 2022). These methods adapt models to novelclasses while maintaining performance on base classes, using only a few shots of data from the novel classes.Tian et al. (2022) propose context-aware prototype learning (CAPL), which mines contextual cues fromsupport and query samples to enrich the classifier for the novel class segmentation.Liu et al. (2023a)introduce projection onto orthogonal prototypes (POP) to generalize well on base classes and quickly adaptto new objects or instances. Hajimiri et al. (2023) propose leveraging the InfoMax principle to maximizeMutual Information (MI) between learned feature representations and predictions with an easily optimizedinference phase. We utilize two methods (CAPL and POP) as baselines to assess the effectiveness of ourmethod in GFSSeg. Monocular depth estimation is another example. Some approaches (Ranftl et al., 2020; Bhat et al.,2023; Yang et al., 2024; Piccinelli et al., 2024) aim to collect as much data as possible to train robustmodels. Ranftl et al. (2020) attempt to combine multiple datasets from different sources and propose arobust training objective to boost the depth estimation performance. Bhat et al. (2023) propose the firstapproach that combines both relative and metric depth estimations, resulting in a model with excellentgeneralization performance in MED while maintaining metric scale. Built upon (Bhat et al., 2023), Yanget al. (2024) extended the pretraining dataset to 62 million images. This model is then fine-tuned using asmaller dataset of NYUv2 or KITTI, achieving remarkable generalization performance. Since the data for",
  "Auxiliary Edge and Semantic Detection": "Semantic information has been utilized to improve the performance of main tasks. Ramirez et al. (2018)propose to leverage the semantics and geometry by enforcing spatial proximity between depth discontinuitiesand semantics for monocular depth estimation. Wu et al. (2019) introduce pyramid cost volumes to capturesemantic and multi-scale spatial information for semantic stereo-matching. Zhu et al. (2020) explore theconstraints from semantic segmentation to help unsupervised monocular depth estimation.Rahman &Fattah (2024) propose a depth semantics symbiosis module to achieve comprehensive mutual awarenessinformation for boosting depth estimation. While semantic labels are beneficial, acquiring large-scale labelscan be costly in many cases. Low-level structure information is more accessible to compute compared to semantics. For instance, itis easy to compute the Canny edge or Sobel edge using the famous Canny operator (Canny, 1986) and theSobel operator (Sobel & Feldman, 1973). Pretrained deep-learning models, such as SAM (Kirillov et al.,2023), can also be employed. Schenk & Fraundorfer (2017) leverage the combination of edge images anddepth maps for joint camera pose estimation. In some recent work, synthetic images are used to train deeplearning models, where the ground truths can be easily generated (Zhang et al., 2019b; DeTone et al., 2018).In this work, we explore using different low-level geometric information, such as keypoints and the edge,in the auxiliary task. The previously used edge is considered a special case. Our study demonstrates thatdiverse low-level geometries work similarly in improving the models.",
  "The Overall Architecture and Optimization Objective": "We propose a simple yet effective geometry regularization network (GRNet) to improve the model training orfine-tuning in dense prediction tasks with limited training data, specifically in generalized few-shot semanticsegmentation (GFSSeg) and monocular depth estimation (MDE) tasks. GRNet constructs a regularizationterm by learning from low-level keypoints or edge information via multi-task learning, as illustrated in. We denote the primary dense prediction task as the primary task and the regularization term asthe auxiliary task. Both tasks are trained jointly using multi-task learning. Our optimization objective is to use the auxiliary task to regularize the primary task, combining the lossesof both tasks through a weighted sum. Supervisory geometric information is obtained from hand-crafted or",
  "(b) The details of primary-to-auxiliary aggregation": ": The pipeline of the proposed auxiliary self-trained geometry regularization network (GRNet).(a) GRNet consists of a low-level geometry extractor, a primary-to-auxiliary aggregation module, and anauxiliary decoder. (b) The primary-to-auxiliary aggregation module first combines two inputs, feature bfrom the encoder backbone and output p of the primary task, followed by gate attention and softmax togenerate the correlation attention maps between primary and auxiliary tasks.",
  "Primary-to-Auxiliary Aggregation": "Unlike other methods that fuse the outputs of the low-level tasks for the primary dense prediction task, wepropose a regularization approach for the primary dense prediction task, as shown in (a). We proposea primary-to-auxiliary (P2A) aggregation module with soft attention to achieve a one-way information flowfrom the primary dense prediction branch to the auxiliary low-level regularization branch.(b)illustrates the detailed diagram of the proposed P2A aggregation module. Given feature b extracted fromthe backbone and output p from the primary decoder, we first concatenate them to obtain c = Concat(b, p).Then we define two functions to map c via two different spatial-wise gates Gb and Gp via two 1 1convolution. This results in:Ab = Gb(c), Ap = Gp(c).(3)",
  ".(6)": "This module is introduced to regularize the model training and prevent convergence to local minima. Itseamlessly integrates the existing methods in few-shot segmentation and monocular depth estimation withoutchanging the network structure of existing methods. As a result, this plug-and-play module is discarded aftertraining, incurring no additional computational cost during inference. It is important to emphasize that our method is conceptually distinct from many existing methods (Li et al.,2020; Lu et al., 2022; Li et al., 2024). Our method utilizes the output p of the primary task as one of theinputs of the auxiliary task. This design encourages the network to compute p in a way that benefits theauxiliary task, i.e., the network would compute p that leads to lower loss of the auxiliary task. This isequivalent to imposing a constraint on the output p such that it favours low-level geometry extraction inthe auxiliary task. In contrast, other methods often apply post-processing smoothing directly to the output.",
  "Choice of Low-level Geometry Information": "One important aspect of the method is the type of geometry information used to train the auxiliary decoder.Our study explores several options, including SIFT, SuperPoint and Canny edge. Previous studies (Songet al., 2020) often conjecture that joint edge detection provides information to smooth the output of theprimary task and improves the results. Some other methods (Shao et al., 2023b) use such information toiteratively update MDE. In GRNet, we find that the keypoints and edge work similarly to improve the resultsin GFSS and MDE tasks. This raises the question of the role of edge detection in previous methods. We haveconducted some experiments in our study in .2.3, and our experiments suggest that post-processingiterations could be eliminated if the proposed GRNet is used to regularize model training.",
  "Datasets and Evaluations": "Datasets. We use the PASCAL-5i dataset, built on the PASCAL VOC 2012 (Everingham et al.,2010). This dataset contains 12,031 images with high-quality pixel-level annotations of 20 classes,split into 10,582 training images and 1,449 validation images.Following the standard protocolin (Liu et al., 2023b; Tian et al., 2022), the 20 classes are evenly partitioned into four folds forcross-validation.Additionally, we evaluate the capability of our method on a more challengingCOCO-20i dataset, which includes 122,218 images with 80 object classes, with 82,081 images usedfor training and 40,137 for validation. Similarly, experiments on COCO-20i are also conducted withcross-validation on four folds (20 classes per fold). Evaluation Metrics. For both datasets, once the model is validated on a given fold, the classes inthis fold are treated as novel classes, and the classes in the other three folds plus background playthe role of base classes. The Intersection over Union (IoU) per class and mean IoU (mIoU) overthe base, novel, and all classes are computed. We follow the same settings as in (Tian et al., 2022;Myers-Dean et al., 2021; Liu et al., 2023a) to conduct four-fold cross-validation and calculate theaverage values. Additionally, each experiment is repeated with five different random seeds, and themean results are reported (Tian et al., 2022).",
  "Implementation Details": "Our GRNet is implemented on PyTorch. Following the training strategy in (Tian et al., 2022; Liu et al.,2023a), the mini-batch stochastic gradient descent with momentum 0.9 and weight decay 0.0001 is used tooptimize the model. During the base class learning, the initial learning rate is set to 0.01, which is annealeddown to zero following a poly policy whose power is fixed to 0.9. The batch size is set to 8 for bothdatasets, and the models are trained for 100 epochs for base class learning. For novel class updating, themodel is updated with a fixed learning rate of 0.01 for 500 epochs, and the batch size is set as 2 and 8 forPASCAL5i and COCO-20i, respectively.",
  "Comparison with state-of-the-art methods": "We integrate the proposed GRNet with the two latest GFSSeg methods, CAPL (Tian et al., 2022) andPOP (Liu et al., 2023a), to justify its effectiveness on PASCAL-5i and COCO-20i datasets. We denotethe method applying GRNet on baseline approach A as GRNet w A. We choose the SIFT keypoint asthe main geometry knowledge in our implementation, but the results using other low-level structures arealso reported. summarizes the mIoU performances of different settings (k-shot, k = 1, 5, 10) onthe PASCAL-5i dataset. Overall, the results across three different settings consistently indicate that theproposed GRNet can improve the performances of the novel classes. In particular, the mIoU performanceof novel class by GRNet with the CAPL outperforms the original CAPL model by 7.1%, 27.5% and 14.1%relatively in 1-shot, 5-shot and 10-shot, respectively, while the performances for base classes are maintainedor slightly improved. Similarly, the GRNet with POP outperforms the original POP by 15.8%, 2.2%, and1.8% for novel classes in 1-shot, 5-shot, and 10-shot, respectively. shows that GRNet with CAPL outperforms the baseline CAPL method on base and novel classesacross three different shot experiments. In contrast, the results of GRNet with POP show only a slight mIoUimprovement (0.02%) of base class on the 5-shot setting, and the performance is even lower in the 1-shot and10-shot settings. We think the reason is the different training strategies between CAPL and POP. Notably,",
  "POP (Liu et al., 2023a)54.7115.3144.9854.9029.9748.7555.0135.0550.08GRNet w POP 53.8119.5745.3654.1231.6249.2755.0636.1850.29": "GRNet causes a slight performance drop for base classes in POP (Liu et al., 2023a), but this does not happenin CAPL (Tian et al., 2022). This is due to the different training strategies between CAPL and POP. CAPLiteratively optimizes the models for the base classes and updates the novel classes in each epoch. POP trainsthe models for base classes first and then updates them for the novel classes without utilizing the base classdata during the update. details the performance comparisons of 1-shot, 5-shot, and 10-shot on COCO-20i dataset. Similarto the experimental results on the PASCAL-5i dataset, GRNet with POP surpasses the original POP by27.8%, 5.5%, and 3.2% relatively on 1-shot, 5-shot, and 10-shot settings. The results again empirically verifythe effectiveness of our method. shows three sample results for GFSSeg in PASCAL, by CAPL,POP and our methods under the 5-shot setting. As shown in the results, our methods achieve more accuratesegmentation. Next, we delve into the deeper insights revealed by the experimental results. When training data is limited orscarce in the few-shot segmentation task, the risk of overfitting increases as the model tends to optimize forthe limited training data, leading to poor generalization. While data augmentation can somewhat alleviatethis issue, there is still space for improvement. Our core idea is to introduce an extra branch as the low-levelgeometry regularization. This branch is included via multi-task learning. By including the low-vision taskin the few-shot semantic segmentation, we ask the network to optimize for both the primary and low-visiontasks simultaneously. and 2 present that our proposed GRNet algorithm enhances the performanceof novel classes across three different settings, demonstrating its effectiveness in reducing over-fitting andimproving generalization.As the training data for novel classes increases, the gain provided by GRNet",
  "p-value0.0320.1090.622": "To verify the effectiveness of different low-level geometry extractors, we compare the SIFT (), SuperPoint() and Canny edge () as low-level geometry in the GFSSeg. presents the performance comparisonsof different geometry regularizations on PASCAL-5i. In the GRNet with CAPL, the three different low-levelgeometries improve the performance of novel classes comparably. We conduct a statistic t-test to compareSIFT regularization against SuperPoint, Canny and the baseline without GRNet. shows the statistic t-test p values to validate the choice of regularizations. As we can see, there is asignificant difference between the SIFT regularized model and the baseline (p < 0.05). The different choicesof regularization terms do not lead to significant differences (p > 0.05), suggesting that the choice does notcritically impact the final performance.",
  "Ablation Studies": "We conduct ablation studies to evaluate the effectiveness of each proposed module. We use POP as thebaseline. The GRNet includes two modules. The first module is a simple multi-task learning of the primarytask and a low-level extraction task to detect SIFT, denoted as Multi-task. The second module is theprimary-to-auxiliary module, which accounts for the relationship between the two tasks. summarizesthe performance comparison when the two components are applied in GFSSeg. Specifically, in the case of",
  "Datasets": "We use the NYUv2 (Nathan Silberman & Fergus, 2012) and KITTI (Geiger et al., 2013) datasets to evaluatethe model generalization of the proposed GRNet in MDE. NYUv2 is an indoor dataset. We follow the officialtraining and testing split to evaluate our method, with 249 scenes used for training and 654 images from 215scenes for testing. KITTI is an outdoor dataset. The images are around 376 1241 resolution. We followthe experimental setting in (Yuan et al., 2022), which consists of 85,898 training images, 1000 validationimages and 500 test images. We report the results on the validation data.",
  "Evaluation Metrics": "Similar to previous work (Yuan et al., 2022; Shao et al., 2023b), we use the standard evaluation protocolsin MDE, e.g., square root of the scale-invariant logarithmic error (SILog), relative absolute error (Abs Rel),relative squared error (Sq Rel), root mean squared error (RMSE), log10 error (log10), and threshold accuracy( <1.25). We also report the computational complexity in GFLOPs.",
  "GRNet w NeWCRFs 0.0890.3140.0420.0390.92943.182GRNet w NeWCRFs 0.0880.3140.0410.0390.92943.182GRNet w NeWCRFs 0.0880.3140.0410.0380.92943.182": "Results on indoor scenes. We choose the recent NeWCRFs as our baseline and implement GRNet Onthe NYUv2 dataset for indoor scenes. We use three different geometries: SIFT, SuperPoint and Canny edge.Their results are denoted by , and respectively. shows that our proposed GRNet improvesthe depth estimation performance in the two main metrics, Abs Rel error and RMSE. Specifically, theAbs Rel error is reduced by 7.37% relatively from 0.095 to 0.088, and the RMSE error is reduced by 5.99%relatively from 0.334 to 0.314. We also compare our method with IEBins, which proposes the Iterative ElasticBins on top of NeWCRFs for MDE. shows that our GRNet with NeWCRFs achieves comparableperformance to IEBins but with a 56.4% reduction in GFLOPs. This shows that our method could be usedto replace the iterative elastic bins. visualizes the comparisons between NeWCRFs and our GRNetin MDE. It is also interesting to note that different choices of geometries improve the performance of MDE comparably,similar to that in GFSSeg. Previous work (Krishna & Vandrotti, 2023) hypothesized that the edges provideinformation to smooth the output of the segmentation or depth estimation for improved performance. How-ever, this cannot explain why the keypoints improve the performance comparable to the edge. Moreover,SuperPoint and SIFT differ from each other, while their improvements in the tasks here are also similar. Weconjecture that the terms from either edges or points play a role of regularization instead of smoothing.",
  "Conclusions": "In this paper, we propose a novel regularization term to improve the training of existing deep neural net-works, especially for dense prediction tasks with limited training data, such as generalized few-shot semanticsegmentation and monocular depth estimation. The proposed self-trained plug-and-play geometry regular-ization is discarded in the inference stage and would neither modify the network structure nor change thespeed. Experimental results on generalized few-shot semantic segmentation and monocular depth estimationmethods validate the proposed regularisation terms effectiveness and generalisation. One limitation of theapproach is the additional training time required.",
  "John Canny. A computational approach to edge detection. IEEE Transactions on Pattern Analysis andMachine Intelligence, PAMI-8(6):679698, 1986. doi: 10.1109/TPAMI.1986.4767851": "Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and SergeyZagoruyko. End-to-end object detection with transformers. In European conference on computer vision,pp. 213229. Springer, 2020. Mathilde Caron, Hugo Touvron, Ishan Misra, Herv Jgou, Julien Mairal, Piotr Bojanowski, and ArmandJoulin.Emerging properties in self-supervised vision transformers.In Proceedings of the IEEE/CVFinternational conference on computer vision, pp. 96509660, 2021. Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab:Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs.IEEE transactions on pattern analysis and machine intelligence, 40(4):834848, 2017. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastivelearning of visual representations. In International conference on machine learning, pp. 15971607. PMLR,2020.",
  "Timothe Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers.arXiv preprint arXiv:2309.16588, 2023": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchicalimage database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee,2009. Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superpoint: Self-supervised interest point de-tection and description. In Proceedings of the IEEE conference on computer vision and pattern recognitionworkshops, pp. 224236, 2018. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-terthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth16x16 words: Transformers for image recognition at scale. In ICLR, 2020.",
  "Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascalvisual object classes (voc) challenge. International journal of computer vision, 88:303338, 2010": "Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, and Dacheng Tao. Deep ordinal regres-sion network for monocular depth estimation. In Proceedings of the IEEE conference on computer visionand pattern recognition, pp. 20022011, 2018a. Huazhu Fu, Jun Cheng, Yanwu Xu, Changqing Zhang, Damon Wing Kee Wong, Jiang Liu, and XiaochunCao. Disc-aware ensemble network for glaucoma screening from fundus image. IEEE Transactions onMedical Imaging, 37(11):24932501, 2018b. doi: 10.1109/TMI.2018.2837012.",
  "Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun.Vision meets robotics: The kittidataset. The International Journal of Robotics Research, 32(11):12311237, 2013": "Jean-Bastien Grill, Florian Strub, Florent Altch, Corentin Tallec, Pierre Richemond, Elena Buchatskaya,Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap yourown latent-a new approach to self-supervised learning. Advances in neural information processing systems,33:2127121284, 2020. Sina Hajimiri, Malik Boudiaf, Ismail Ben Ayed, and Jose Dolz. A strong baseline for generalized few-shotsemantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pp. 1126911278, 2023. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervisedvisual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and patternrecognition, pp. 97299738, 2020. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollr, and Ross Girshick. Masked autoencodersare scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and patternrecognition, pp. 1600016009, 2022. Fabian Isensee, Paul F Jaeger, Simon AA Kohl, Jens Petersen, and Klaus H Maier-Hein.nnu-net: aself-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2):203211, 2021. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao,Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of theIEEE/CVF International Conference on Computer Vision, pp. 40154026, 2023.",
  "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-trainingwith frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023b": "Xiangtai Li, Houlong Zhao, Lei Han, Yunhai Tong, Shaohua Tan, and Kuiyuan Yang. Gated fully fusion forsemantic segmentation. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp.1141811425, 2020. Xiangtai Li, Jiangning Zhang, Yibo Yang, Guangliang Cheng, Kuiyuan Yang, Yunhai Tong, and DachengTao.Sfnet: Faster and accurate semantic segmentation via semantic flow.International Journal ofComputer Vision, 132(2):466489, 2024. Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollr.Focal loss for dense objectdetection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(2):318327, 2020. doi:10.1109/TPAMI.2018.2858826. Sun-Ao Liu, Yiheng Zhang, Zhaofan Qiu, Hongtao Xie, Yongdong Zhang, and Ting Yao. Learning orthogonalprototypes for generalized few-shot semantic segmentation. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pp. 1131911328, 2023a.",
  "Weide Liu, Chi Zhang, Henghui Ding, Tzu-Yi Hung, and Guosheng Lin. Few-shot segmentation with optimaltransport matching and message flow. IEEE Transactions on Multimedia, 25:51305141, 2022": "Weide Liu, Zhonghua Wu, Yang Zhao, Yuming Fang, Chuan-Sheng Foo, Jun Cheng, and Guosheng Lin.Harmonizing base and novel classes: A class-contrastive approach for generalized few-shot segmentation.International Journal of Computer Vision, 2023b. Weide Liu, Zhonghua Wu, Yang Zhao, Yuming Fang, Chuan-Sheng Foo, Jun Cheng, and Guosheng Lin.Harmonizing base and novel classes: A class-contrastive approach for generalized few-shot segmentation.International Journal of Computer Vision, 132(4):12771291, 2024. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swintransformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVFinternational conference on computer vision, pp. 1001210022, 2021.",
  "Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob Fergus. Indoor segmentation and support inferencefrom rgbd images. In ECCV, 2012": "Maxime Oquab, Timothe Darcet, Tho Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, PierreFernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visualfeatures without supervision. arXiv preprint arXiv:2304.07193, 2023. Vaishakh Patil, Christos Sakaridis, Alexander Liniger, and Luc Van Gool.P3depth: Monocular depthestimation with a piecewise planarity prior. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pp. 16101621, 2022. Luigi Piccinelli, Yung-Hsu Yang, Christos Sakaridis, Mattia Segu, Siyuan Li, Luc Van Gool, and Fisher Yu.Unidepth: Universal monocular metric depth estimation. In IEEE Conference on Computer Vision andPattern Recognition (CVPR), 2024. Md Awsafur Rahman and Shaikh Anowarul Fattah. Semi-supervised semantic depth estimation using sym-biotic transformer and nearfarmix augmentation. In Proceedings of the IEEE/CVF Winter Conference onApplications of Computer Vision, pp. 250259, 2024.",
  "Ren Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In Pro-ceedings of the IEEE/CVF international conference on computer vision, pp. 1217912188, 2021b": "Ren Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monoc-ular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE Transactions on PatternAnalysis and Machine Intelligence, 44(3), 2022. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, AndrejKarpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge.International journal of computer vision, 115:211252, 2015.",
  "Fabian Schenk and Friedrich Fraundorfer. Combining edge images and depth maps for robust visual odom-etry. In Proceedings 28th British Machine Vision Conference, pp. 112, 2017": "Shuwei Shao, Zhongcai Pei, Weihai Chen, Xingming Wu, and Zhengguo Li.Nddepth: Normal-distanceassisted monocular depth estimation. In Proceedings of the IEEE/CVF International Conference on Com-puter Vision, pp. 79317940, 2023a. Shuwei Shao, Zhongcai Pei, Xingming Wu, Zhong Liu, Weihai Chen, and Zhengguo Li. Iebins: Iterativeelastic bins for monocular depth estimation.In Advances in Neural Information Processing Systems(NeurIPS), 2023b.",
  "Wanjuan Su and Wenbing Tao. Efficient edge-preserving multi-view stereo network for depth estimation. InProceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 23482356, 2023": "Zhuotao Tian, Hengshuang Zhao, Michelle Shu, Zhicheng Yang, Ruiyu Li, and Jiaya Jia.Prior guidedfeature enrichment network for few-shot segmentation. IEEE transactions on pattern analysis and machineintelligence, 44(2):10501065, 2020. Zhuotao Tian, Xin Lai, Li Jiang, Shu Liu, Michelle Shu, Hengshuang Zhao, and Jiaya Jia. Generalizedfew-shot semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pp. 1156311572, 2022.",
  "Hugo Touvron, Matthieu Cord, and Herv Jgou. Deit iii: Revenge of the vit. In European conference oncomputer vision, pp. 516533. Springer, 2022": "Kaixin Wang, Jun Hao Liew, Yingtian Zou, Daquan Zhou, and Jiashi Feng. Panet: Few-shot image semanticsegmentation with prototype alignment. In proceedings of the IEEE/CVF international conference oncomputer vision, pp. 91979206, 2019. Zhenyao Wu, Xinyi Wu, Xiaoping Zhang, Song Wang, and Lili Ju. Semantic stereo matching with pyramidcost volumes. In 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 74837492,2019. doi: 10.1109/ICCV.2019.00758. Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer:Simple and efficient design for semantic segmentation with transformers. Advances in neural informationprocessing systems, 34:1207712090, 2021. Gangwei Xu, Xianqi Wang, Xiaohuan Ding, and Xin Yang. Iterative geometry encoding volume for stereomatching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.2191921928, 2023a. Zunnan Xu, Zhihong Chen, Yong Zhang, Yibing Song, Xiang Wan, and Guanbin Li. Bridging vision andlanguage encoders: Parameter-efficient tuning for referring image segmentation. In Proceedings of theIEEE/CVF International Conference on Computer Vision, pp. 1750317512, 2023b.",
  "Weihao Yuan, Xiaodong Gu, Zuozhuo Dai, Siyu Zhu, and Ping Tan. Neural window fully-connected crfs formonocular depth estimation. pp. 39163925, 2022": "Junming Zhang, Katherine A. Skinner, Ram Vasudevan, and Matthew Johnson-Roberson.Dispsegnet:Leveraging semantics for end-to-end learning of disparity estimation from stereo imagery. IEEE Roboticsand Automation Letters, 4:11621169, 2019a. Tianyang Zhang, Huazhu Fu, Yitian Zhao, Jun Cheng, Mengjie Guo, Zaiwang Gu, Bing Yang, Yuting Xiao,Shenghua Gao, and Jiang Liu. Skrgan: Sketching-rendering unconditional generative adversarial networksfor medical image synthesis.In Dinggang Shen, Tianming Liu, Terry M. Peters, Lawrence H. Staib,Caroline Essert, Sean Zhou, Pew-Thian Yap, and Ali Khan (eds.), Medical Image Computing and ComputerAssisted Intervention MICCAI 2019, pp. 777785, Cham, 2019b. Springer International Publishing."
}