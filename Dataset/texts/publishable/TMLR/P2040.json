{
  "Abstract": "Traditional deep learning models are trained and tested on relatively low-resolution images(< 300 px), and cannot be directly operated on large-scale images due to compute andmemory constraints. We propose Patch Gradient Descent (PatchGD), an effective learningstrategy that allows us to train the existing CNN and transformer architectures (herebyreferred to as deep learning models) on large-scale images in an end-to-end manner. PatchGDis based on the hypothesis that instead of performing gradient-based updates on an entireimage at once, it should be possible to achieve a good solution by performing model updateson only small parts of the image at a time, ensuring that the majority of it is covered over thecourse of iterations. PatchGD thus extensively enjoys better memory and compute efficiencywhen training models on large-scale images. PatchGD is thoroughly evaluated on PANDA,UltraMNIST, TCGA, and ImageNet datasets with ResNet50, MobileNetV2, ConvNeXtV2,and DeiT models under different memory constraints. Our evaluation clearly shows thatPatchGD is much more stable and efficient than the standard gradient-descent method inhandling large images, especially when the compute memory is limited. Code is available at",
  "Published in Transactions on Machine Learning Research (8/2024)": "conducted for different epsilon settings, image and patch sizes, and memory constraints. We found thatfor smaller patch sizes, employing gradient accumulation steps greater than 1 is essential, with significantgains observed as the patch size to image size ratio decreases. Despite this promising trend, remains ahyperparameter requiring further tuning. Moreover, exploring the nuanced relationship between accuracy andsteps is an essential aspect for future investigation in optimizing PatchGD. In case of UltraMNIST dataset at512 image size, best performance is observed at = 1 for a patch size of 256. For PANDA two variationswere tried for image size 512 and image size 4096 with best results obtained at 8 and 32 respectively.",
  "PatchGD (4 GB)": ": Performance comparison of standard CNNand PatchGD (ours) for the task of classification of Ul-traMNIST digits of size 512512 pixels using ResNet50model. Two different computational memory budgetsof 16 GB and 4GB are used, and it is demonstratedthat PatchGD is relatively stable for the chosen imagesize, even for very low memory compute. evolution of the associated models and their applications across a wide range of scientific domains, we referto the reviews presented in Khan et al. (2020); Li et al. (2021a); Alzubaidi et al. (2021); Khan et al. (2022);Shamshad et al. (2022). With the recent technological developments, very large images are obtained from domains like microscopy(Khater et al., 2020; Schermelleh et al., 2019), medical imaging (Aggarwal et al., 2021), and earth sciences(Huang et al., 2018; Amani et al., 2020), and the challenge of using deep learning models on big data toanalyze such images is immense. For example, images obtained from high-content nanoscopy 1 can be aslarge as 6000 6000 or even more (Villegas-Hernndez et al., 2022), with the smallest scale of features beingonly a few pixels in size. Clearly, processing these large images with such fine details prohibits the use of anyimage downsampling algorithm, and using existing models on such high-dimension images is computationallyinfeasible. Most prevailing deep learning models are trained on datasets such as ImageNet, which mainly compriseof low-resolution (< 500 pixels) images. Most research efforts have focused on achieving state-of-the-artperformance on these datasets. However, applying these models to high-resolution images results in aquadratic increase in activation size, requiring significantly more training compute and memory. Moreover,limited GPU memory makes it impractical to process such large images with such models. This paper introduces a novel training framework for deep learning models aimed at handling very largeimages. The definition of large images depends on the available computational memory for training. Forinstance, training a ResNet50 model with 10, 000 10, 000 size images would be challenging with a 48 GBGPU memory, but training the same model with 512 512 size images would be feasible with 12 GB GPUmemory. However, when limited to a 4 GB GPU memory, even 512 512 size images may be considered toolarge. illustrates the above issue on the task of classification of the UltraMNIST digits (Gupta et al.,2022) into one of the 10 predefined classes labeled from 0-9. More details on the UltraMNIST datasetand classification problem are provided in the supplementary material. The semantic relationship betweendifferent parts of the images and the large variation in spatial feature size makes this problem difficult fortraditional models, particularly when dealing with large image sizes or low processing memory. In this study,",
  "we focus on images of size 512 512 pixels and examine the problem under two computational memorybudgets: GPU memory limits of 4 GB and 16 GB": "For the base model, we use ResNet50 (He et al., 2016) and employ the standard training approach. Werefer to it as Gradient descent (GD). Note that the term GD is used here as a generic notation to refer tothe class of gradient-based optimizers popularly used in deep learning (such as SGD, SGD with momentum(Bengio et al., 2013) and Adam (Kingma and Ba, 2014), among others), and it is not necessarily restricted tostochastic gradient descent method. For the results demonstrated in , we used the Adam optimizer.We also present results using the proposed training method, called Patch Gradient Descent (PatchGD), whichis a scalable training approach for building neural networks with large images, low memory compute, or both. PatchGDs effectiveness is demonstrated in , where it outperforms GD in both 16 GB and 4 GBmemory limits. The performance difference is 4% at 16 GB but significantly increases to 13% at 4 GB,simulating real-world challenges with large images. Training a ResNet50 model with 512 512 images usingonly 4 GB memory leads to inferior performance, as shown in . However, PatchGD is stable evenat this low memory regime, and this can be attributed to its design which makes it invariant to image sizeto a large extent. We explain the method in more detail later and present experimental results on variousimage sizes, highlighting PatchGDs ability to adapt existing CNN models for large images with limited GPUmemory.",
  "Due to its inherent ability to work with small fractions of a given image, PatchGD is scalable onsmall GPUs, where training the original full-scale images is not possible": "PatchGD reinvents the existing training pipeline for deep learning models to large images in avery simplified manner and this makes it compatible with any existing CNN architecture or anyconventional gradient-based optimization method used in deep learning. Moreover, its simple designallows it to benefit from the pre-training of the standard CNNs on low-resolution data.",
  "Related Work": "This paper seeks to enhance the ability of existing deep-learning models to handle large images. Previousresearch in this direction is scarce, with most studies focusing on histopathological datasets, which are apopular source of large images. Many of these studies rely on pixel-level segmentation masks, which are notalways available. For instance, Iizuka et al. (2020); Liu et al. (2017) use patchwise segmentation masks toperform patch-level classification on whole slide images, and then apply an RNN to obtain the final imagelabel. Meanwhile, Braatz et al. (2022) uses goblet cell segmentation masks for patch-level feature extraction.However, these approaches require labeled segmentation data, are computationally expensive, have limitedfeature learning, and are more susceptible to error propagation. Another set of methods focuses on building a compressed latent representation of the large input imagesusing existing pre-trained models or unsupervised learning approaches. For example, Brancati et al. (2021)uses a model pre-trained on Imagenet to construct a latent block, which is further passed to an attentionhead to do the final classification. Other similar variants include using a U-net autoencoder to build thelatent features (Lai et al., 2022), using encoding strategies involving reconstruction error minimization andcontrastive learning (Tellez et al., 2018), and getting stronger latent representations through multi-tasklearning Tellez et al. (2020). A critical limitation of this class of methods is that the encoding network createdfrom unsupervised learning is not always a strong representative of the target task. There exist several methods that use pre-trained models derived from other tasks as feature extractors andthe output is then fed to a classifier. Example methods include using Cancer-Texture Network (CAT-Net)and Google Brain (GB) models as feature extractors (Kosaraju et al., 2022), or additionally using similar",
  "General description": "Patch Gradient Descent (PatchGD) is a novel approach for training deep learning models with high-resolutionimages. Its a modification of the standard feedforward-backpropagation method. PatchGD is built on thehypothesis that, instead of applying gradient-based updates to the entire image simultaneously, similar resultscan be achieved by updating the model in small image segments, while ensuring the full image coverage overmultiple iterations. Even if only a portion of the image is used in each iteration for gradient updates, themodel is still trainable end-to-end with PatchGD. presents the schematic representation of the PatchGD pipeline. The central idea behind PatchGDis to construct the Z block, which is a deep latent representation of the entire input image. Although onlya subset of the input is used to perform model updates, Z captures information about the entire image bycombining information from different parts of the image acquired from the previous update steps. aillustrates the use of the Z block, which is an encoding of an input image X using a model parameterizedby weights 1. The input image is divided into n n number of patches, and each patch is processedindependently using 1. The size of Z is always enforced to be n n s, such that each patch in the inputspace corresponds to the respective 1 1 s segment in the Z block. The filling of Z is carried out in multiple steps, with each step involving the sampling of k patches and theirpositions from X and feeding them to the model as a batch for processing. The output from the model alongwith the corresponding positions are then used to fill the respective parts of Z. After sampling all n npatches of X, the completely filled Z is obtained. PatchGD utilizes this concept of Z-filling during bothtraining and inference stages.",
  "minL(f(; X), y),(1)": "where X, y D represents the data samples used, and L() represents the loss function. Conventionally,this problem is solved using mini-batch gradient descent method where at every step, the model weights areupdated using the average of gradients computed over a batch of samples, denoted here as S. Based on this,the model update at the ithstep is",
  "d(i1)(2)": "where and B denote the learning rate and the size of the batch used, respectively. As can be seen in Eq. 2,if the size of image samples s S is very large, it will lead to large memory requirements for the respectiveactivations, and under limited compute availability, only small values of B, and sometimes not even a singlesample, fit into the GPU memory. This should clearly demonstrate the limitation of the gradient descentmethod when handling large images. This issue is alleviated by our PatchGD approach.",
  "d(i,j1) .(3)": "Here, i refers to a mini-batch iteration within a certain epoch. Further, j denotes the inner iterations, whereat every inner iteration, k patches are sampled from the each input image X X (denoted as PX,j) andthe gradient-based updates are performed as stated in Eq. 3. Note that for any iteration i, multiple inneriterations are run ensuring that the majority of samples from the full set of patches that are obtained fromthe tiling of X are explored. In Eq. 3, (i,0) denotes the initial model for the inner iterations on Si and is equal to (i1,), the final modelstate after inner iterations of patch-level updates using Si1. For a more detailed understanding of themodel update process, please see Algorithm 1. As described earlier, PatchGD uses an additional sub-networkthat looks at the full latent encoding Z for the input batch X. Thus, the parameter set is extended as = , where the base CNN model and the additional sub-network are f1 and g2, respectively. Algorithm 1 describes model training over one batch of B images, denoted as X RBMMC. As the firststep of the model training process, Z corresponding to X is initialized. The process of filling of Z is describedin Algorithm 2. For a patch indexed by position v and denoted as xv, the respective Z[v] is updated usingthe output obtained from f1. Note here that 1 is loaded from the last state obtained during the modelupdate on the previous batch of images. During the filling of Z, no gradients are stored for backpropagation. Next, the model update process is performed over a series of inner-iterations, where at every stepj {1, 2, . . . , }, k patches are sampled per image in X and the respective parts of Z are updated. Next,the partly updated Z is processed with the additional sub-network 2 to compute the class probabilitiesand the corresponding loss value. Based on the computed loss, gradients are backpropagated to performupdates of 1 and 2. Note that we control here the frequency of model updates in the inner iterationsthrough an additional term . Similar to how a batch size of 1 in mini-batch gradient descent introducesnoise and adversely affects the convergence process, we observed that gradient update per inner iterationleads to sometimes poor convergence. Thus, we introduce gradient accumulation over inner steps and",
  "GD3467.7GD* 33460.0PatchGD10474.8": "update the model accordingly. Gradients are allowed to backpropagate only through those parts of Z that areactive at the jth inner-iteration. During inference phase, Z is filled using the optimized f1 as described inAlgorithm 2, and then the filled version of Z is used to compute the class probabilities for input X using g2 .",
  "Experimental setup": "Datasets. We perform thorough evaluation on two datasets, UltraMNIST (Gupta et al., 2022) and PANDA(Bulten et al., 2022), and also conduct additional experiments using TCGA-NSCLC (Chen et al., 2022) andImageNet Deng et al. (2009) datasets. Details about the datasets are presented in the supplementary part ofthe paper. CNN models.We assess PatchGD on ResNet50 and MobileNetV2 architectures. ResNet50 serves as abackbone for diverse computer vision tasks, while MobileNetV2 is a lightweight architecture for edge devices.We also conduct experiments with ConvNextV2, a state-of-the-art vision model, as well as provide preliminaryresults for generative modeling. Implementation details. We employ consistent hyperparameters throughout our experiments and reportclassification accuracy for UltraMNIST and ImageNet tasks and additionally Quadratic Weighted Kappa(QWK)2 on the PANDA dataset. For TCGA-NSCLC, we comply with the previous baselines and report themean and standard deviation of AUC across a 10-fold cross-validation set. Both the baselines and PatchGDare implemented using PyTorch. We consider GPU memory constraints of 4GB, 16GB, and 48GB to simulatecommon limits and measure latency on an NVIDIA 40GB A100 GPU and an NVIDIA 24GB L4 GPU.Additional details are described in the supplementary material.",
  "Results": "UltraMNIST classification. The performance of PatchGD for UltraMNIST has already been shownin . More detailed results are presented in Tables 1 and 2. For both the architectures, PatchGDimproves over the standard gradient descent method (abbreviated as GD) by large margins. Our approachemploys an additional sub-network g2, and it can be argued that the gains reported in the paper are due toit. For this purpose, we extend the base CNN architectures used in GD and report the respective performancescores in Tables 1 and 2 as GD*. 3. For both architectures, PatchGD outperforms GD as well as GD* by large margins. For ResNet50, theperformance difference is even higher for a low memory constraint. At 4 GB, while GD seems unstable with",
  "HIPT Chen et al. (2022)34.80.388HIPT-L49.30.531ABNN Brancati et al. (2021)48.20.593C2C Sharma et al. (2021)50.90.668PatchGD59.70.730": "a performance dip of more than 11% compared to the 16 GB case, PatchGD is significantly more stable. ForMobileNetV2, the difference between PatchGD and GD is even higher at 16GB case, thereby clearly showingthat PatchGD blends well with even lightweight models such as MobileNetV2. For MobileNetV2, there is nodrop in model performance when going from 16 GB to 4 GB, which demonstrates that MobileNetV2 can workwell with GD even at low memory conditions. Nevertheless, PatchGD still performs significantly better. Theunderlying reason for this gain can partly be attributed to the fact that since PatchGD facilitates operatingwith partial images, the activations are small and more images per batch are permitted. We also observe thatthe performance scores of GD* are inferior compared to even GD. ResNet50 and MobilenetV2 are optimizedarchitectures and we speculate that the addition of plain convolutional layers in the head of the network isnot suited due to which the overall performance is adversely affected. Prostate Cancer Classification (PANDA). presents the results obtained on PANDA dataset forthree different image resolutions. For all experiments, we maximize the number of images used per batchwhile also ensuring that the memory constraint is not violated. For images of 512 512, we see that PatchGD,with patches of size 128 128, delivers approximately the same performance as GD (for both accuracy aswell as Quadratic Weighted Kappa (QWK) metric at 16 GB memory limit. However, reducing the patchsize and thus increasing the batch size leads to a very sharp gain in the scores of PatchGD. For a similarmemory constraint, when images of size 2048 2048 pixels are used, GD scores approximately 10% lowerwhile PatchGD shows a boost of 9% in accuracy. Two factors contribute to the performance gap between GD and PatchGD. Firstly, GD faces a bottleneckwith batch size due to increased activation size in higher-resolution images, allowing only 1 image per batch.Gradient accumulation and hierarchical training were explored but did not improve performance significantly.Increasing the memory limit helped mitigate the issue of using only 1 image per batch. Secondly, theoptimized receptive field of ResNet50 is not well-suited for higher-resolution images, resulting in suboptimalperformance. PatchGD demonstrates superior accuracy and QWK compared to GD on the PANDA datasetwhen handling large images end-to-end. In terms of inference latency, PatchGD performs comparably to GD.The smaller activations in PatchGD offset the slowness caused by patchwise image processing. PatchGDshows potential for real-time inference in applications requiring large image handling.",
  "-85.40-88.4025100-88.68-90.74100100-76.90-78.20500100-73.65-70.82": "For transformer backbones, we have observed that the performance of the model is better when the head isalso a transformer rather than a CNN model. presents a comparison of CNN and transformer headsfor the classification task on the PANDA dataset. For the transformer head, we use a single multi-headedself-attention layer with 2 heads each of 192 channels followed by a linear layer. The CNN head uses 3conv-relu-bn blocks with a kernel size of 3 3 and 256 channels followed by a linear layer. We consistentlysee that the transformer head works better. Handling natural images (ImageNet). To understand how PatchGD works with natural images, westudy its performance on ImageNet dataset for different choices of the number of classes as well as the numberof samples per class. This follows the results discussed in the earlier section. We conduct these experimentsusing DeiT-Tiny transformer architecture and the results are reported in . To study the effect of thenumber of samples, we fix classes to 25. Interestingly, we observe that PatchGD outperforms the standardtraining approach by around 2% accuracy, a significant improvement in the context of ImageNet training. We further examined the performance of PatchGD across different numbers of classes, keeping the numberof samples per class fixed at 100 (). Interestingly, PatchGD outperformed the baseline approachwhen dealing with fewer classes. However, when the number of classes increased to 500, the baseline methodperformed better. This discrepancy arises because, for low-resolution images such as those in the ImageNetdataset, the small information loss at the edges of the patches becomes significant when there are many classesand limited samples per class. Our initial findings suggest that this issue can be mitigated to some extent byusing overlapping patches, although this increases computational demands. Nonetheless, our observationsindicate that PatchGD is the preferred choice for natural images in low-data regimes. PatchGD vs. Activation Checkpointing vs. Activation Offloading. As has been described throughoutthis paper, PatchGD aims at better utilization of GPUs, by facilitating to training of deep learning modelswith larger images (leading to larger model activations) on smaller GPUs. Two other popular approachesaiming at fitting a larger model on smaller GPUs are activation checkpointing (Chen et al., 2016) and",
  "Baseline2048-4-4848.60.612PatchGD204812832CNN4848.90.589PatchGD204812832Transformer4857.40.702": "activation offloading (Rhu et al., 2016). presents a comparison of PatchGD with these methods. Wepresent the comparison of ResNet50 architecture on PANDA dataset at two different image resolutions on anNVIDIA 16 GB L4 graphics card. For gradient checkpointing, we employ chunk sizes of 4 and 6. PatchGD outperforms checkpointing and offloading approaches, particularly with 2K resolution images, wherethe margin of superiority is significantly larger. Under the selected memory constraint at this resolution,both baseline methods can only handle a maximum batch size of 4 per iteration, with activation offloadingmanaging only 2. In contrast, PatchGD can handle batch sizes of 14. For smaller images, all methodscan increase the batch size, but PatchGD still delivers the best performance. This clearly demonstratesthat PatchGD is more effective in utilizing GPU resources. Additionally, it is worth noting that PatchGD,checkpointing, and offloading are orthogonal methods and can be combined to fit even larger models onsmaller GPU resources.",
  "Additional study. In this section, we show some additional experiments to further prove the advantages ofPatchGD. Training recipes and hyperparameters are provided in the supplementary material": "Role of end-to-end training. shows that freezing the backbone leads to reduced performance,highlighting the key role of end-to-end training in PatchGD. Other existing methods can fine-tune the network : Performance scores on PANDA dataset(2048 2048) at 24 GB memory budget for differ-ent choices of the feature extractor: pretrained andfrozen on ImageNet, trained on PANDA using GDand frozen, and fully trainable.",
  "end-to-end, but only on low-resolution images, whereas PatchGD enjoys fully-trainable end-to-end trainingeven at higher resolutions": "Additional architectures.Beyond the experiment on TCGA-NSCLC task, we also conducted an additionalexperiment with ConvNext-V2, a state-of-the-art image classification model, on PANDA dataset and theresults are presented in ). PatchGD outperforms the baseline at higher resolution (2048) whileperforming competitively at a relatively low resolution too (512). This shows that PatchGD can takeadvantage of the higher representation power of newer CNN architectures. On attention-based head module for CNNs. We have shown in the paper the working of a CNN backbonewith a CNN head as well as a Transformer backbone with a Transformer head. Here, we study whetherusing a head with attention module could be beneficial for the learning of CNNs. For this purpose, weexperimented with ResNet50 backbone and PatchGD and we replaced the CNN head with an attention-basedMLP to see the effect. For the attention head, we employed a single-layer multi-head attention module with64 heads, and for each pixel of the latent corresponding to a token, we concatenated a trainable CLS tokenfor final classification. Additional details are presented in the supplementary material. Compared to thebase performance accuracy of 52.1% with a CNN head, the accuracy of the model with the attention headimproved to 53.6%. This clearly shows that using an attention module can help to enhance the performanceof PatchGD results. Further, we anticipate that for larger images, where the spatial size of the L1-block islarger, this improvement will be even more.",
  "Conclusions": "In this paper, we introduced Patch Gradient Descent (PatchGD), a novel training strategy for deep learningmodels that effectively handles large images even with limited GPU memory. PatchGD updates the modelusing partial image fractions, ensuring comprehensive context coverage over multiple steps. Through variousexperiments, we demonstrated the superior performance of PatchGD compared to standard gradient descent,both in handling large images and operating under low memory conditions. The presented method and",
  "Limitations": "While our numerical experiments have showcased the effectiveness of PatchGD, there are still limitations interms of comprehensively understanding its generalization and stability. Additionally, our methods relativeslowness compared to standard gradient descent is a minor drawback, particularly when real-time training iscrucial. However, this limitation does not affect the inference speed, making it a bottleneck only in specificscenarios prioritizing real-time training. Gradient bias in PatchGD. PatchGD introduces gradient bias in both the forward and backward passes,unlike methods such as activation checkpointing and activation offloading. During the forward pass, thebias arises because the classifier operates on stale z-vectors, which are derived from previous iterations.This results in suboptimal feature representations since the z-vectors do not accurately reflect the latestmodel updates. Unlike activation checkpointing or offloading, which recompute or store exact intermediateactivations, PatchGDs dependence on these delayed z-vectors can lead to discrepancies between the computedand true activations. In the backward pass, gradient bias occurs because some z-vectors do not propagate gradients. This incompletegradient flow results from PatchGDs strategy of updating only a subset of z-vectors during each iteration.Additionally, due to overlapping receptive fields, neighboring patches can influence these z-vectors, leading toan uneven gradient propagation and an approximation that deviates from the true gradient. To mitigate these biases, several strategies can be employed. Using smaller patch sizes reduces the forwardpass bias by ensuring that z-vectors are updated more frequently, thereby decreasing the staleness effect.Introducing overlapping patches helps in capturing more accurate gradients by minimizing boundary effectsand ensuring more uniform gradient propagation. Incorporating momentum in stochastic gradient descent(SGD) can help average out the bias over multiple iterations by leveraging historical gradient information tosmooth out the noise introduced by the gradient bias. Empirical evaluations show that while PatchGD offers significant memory savings, the introduced gradientbias results in noisier gradient updates. However, this bias does not significantly impact overall trainingperformance and convergence. The benefits of reduced memory usage and the ability to train larger modelswith PatchGD outweigh the impact of gradient bias. We acknowledge the presence of this bias and recommendfurther studies to quantify and refine these strategies, enhancing the effectiveness of PatchGD in traininglarge-scale models efficiently.",
  "Future work": "This paper has established the foundational concept of patch gradient descent to enable training CNNs usingvery large images and even when only limited GPU memory is available for training. The results as wellas insights presented in the paper open doors to several novel secondary research directions that could beinteresting in terms of improving the efficacy as well as the acceptance of the presented method in a broaderscientific community. We list some such directions here. Scaling to gigapixel images at small compute memory. An ambitious but very interesting applicationof PatchGD would be to be able to process gigapixel images with small GPU memory. We can clearlyenvision this with PatchGD but with additional work. One important development needed is toextend the PatchGD learning concept to multiple hierarchical Z blocks, thereby sampling patchesfrom the outer block to iteratively fill the information in the immediate inner Z block and so on. Enhanced receptive field. So far, PatchGD has been looked at only in the context of being able tohandle very large images. However, a different side of its use is that with almost the same architecture,it builds a smaller receptive build, thereby zooming in better. We speculate that in this context,",
  "Broader Impact": "The broader impact of this work lies particularly in its potential to extend the capability of deep learningmodels. By addressing the challenge of training models on large-scale images with limited computationalresources, our approach opens up opportunities for researchers and practitioners with constrained hardwaresetups to tackle complex problems in healthcare, agriculture, and environmental monitoring, where high-resolution images play a crucial role in decision-making processes. Moreover, our approach can contribute toreducing the environmental footprint of deep learning by enabling efficient training on low-power devices, thuspromoting sustainability in the development and deployment of deep learning models. In summary, our workhas the potential to empower diverse communities, drive sustainable development, and accelerate scientificprogress. It is essential to approach these advancements with a conscientious mindset, taking into accountthe broader societal impact and proactively working towards an inclusive and responsible deployment of deeplearning technologies. With our work, it is also important to address the potential risks and challenges. Issuesrelated to data privacy, bias, and fairness should be carefully addressed to prevent any unintended negativeconsequences. Additionally, the potential for misuse or malicious applications of deep learning models shouldbe acknowledged and proactively addressed through robust security measures and ethical guidelines.",
  "A. Khan, A. Sohai, U. Zahoora, and A. S. Qureshi. A survey of the recent architectures of deep convolutionalneural networks. Artificial Intelligence Review, 53:54555516, 2020": "Zewen Li, Fan Liu, Wenjie Yang, Shouheng Peng, and Jun Zhou. A survey of convolutional neural networks:Analysis, applications, and prospects. IEEE Transactions on Neural Networks and Learning Systems, pages121, 2021a. L. Alzubaidi, J. Zhang, A. J. Humaidi, A. Al-Dujaili, Y. Duan, ). Al-Shamma, J. Santamara, M. A. Fadhel,M. Al-Amidie, and L. Farhan. Review of deep learning: concepts, cnn architectures, challenges, applications,future directions. Journal of Big Data, 8, 2021.",
  "Ismail M Khater, Ivan Robert Nabi, and Ghassan Hamarneh. A review of super-resolution single-moleculelocalization microscopy cluster analysis and quantification methods. Patterns, 1(3):100038, 2020": "Lothar Schermelleh, Alexia Ferrand, Thomas Huser, Christian Eggeling, Markus Sauer, Oliver Biehlmaier,and Gregor PC Drummen. Super-resolution microscopy demystified. Nature cell biology, 21(1):7284, 2019. Ravi Aggarwal, Viknesh Sounderajah, Guy Martin, Daniel SW Ting, Alan Karthikesalingam, Dominic King,Hutan Ashrafian, and Ara Darzi. Diagnostic accuracy of deep learning in medical imaging: a systematicreview and meta-analysis. NPJ digital medicine, 4(1):65, 2021.",
  "Yanbo Huang, Zhong-xin Chen, YU Tao, Xiang-zhi Huang, and Xing-fa Gu. Agricultural remote sensing bigdata: Management and applications. Journal of Integrative Agriculture, 17(9):19151931, 2018": "Meisam Amani, Arsalan Ghorbanian, Seyed Ali Ahmadi, Mohammad Kakooei, Armin Moghimi, S MohammadMirmazloumi, Sayyed Hamed Alizadeh Moghaddam, Sahel Mahdavi, Masoud Ghahremanloo, SaeidParsian, et al. Google earth engine cloud computing platform for remote sensing big data applications:A comprehensive review. IEEE Journal of Selected Topics in Applied Earth Observations and RemoteSensing, 13:53265350, 2020.",
  "Deepak K. Gupta, Udbhav Bamba, Abhishek Thakur, Akash Gupta, Suraj Sharan, Ertugrul Demir, andDilip K. Prasad. Ultramnist classification: A benchmark to train cnns for very large images. arXiv, 2022": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770778, 2016. Yoshua Bengio, Nicolas Boulanger-Lewandowski, and Razvan Pascanu. Advances in optimizing recurrentnetworks. In 2013 IEEE international conference on acoustics, speech and signal processing, pages 86248628.IEEE, 2013.",
  "Diederik P Kingma and Jimmy Ba.Adam:A method for stochastic optimization.arXiv preprintarXiv:1412.6980, 2014": "Osamu Iizuka, Fahdi Kanavati, Kei Kato, Michael Rambeau, Koji Arihiro, and Masayuki Tsuneki. Deeplearning models for histopathological classification of gastric and colonic epithelial tumours. ScientificReports, 10(1):1504, Jan 2020. ISSN 2045-2322. doi: 10.1038/s41598-020-58467-9. URL Yun Liu, Krishna Gadepalli, Mohammad Norouzi, George E Dahl, Timo Kohlberger, Aleksey Boyko,Subhashini Venugopalan, Aleksei Timofeev, Philip Q Nelson, Greg S Corrado, et al. Detecting cancermetastases on gigapixel pathology images. arXiv preprint arXiv:1703.02442, 2017. Jon Braatz, Pranav Rajpurkar, Stephanie Zhang, Andrew Y Ng, and Jeanne Shen. Deep learning-based sparsewhole-slide image analysis for the diagnosis of gastric intestinal metaplasia. arXiv preprint arXiv:2201.01449,2022.",
  "Nadia Brancati, Giuseppe De Pietro, Daniele Riccio, and Maria Frucci. Gigapixel histopathological imageanalysis using attention-based neural networks. IEEE Access, 9:8755287562, 2021": "Zhi-Fei Lai, Gang Zhang, Xiao-Bo Zhang, and Hong-Tao Liu. High-resolution histopathological imageclassification model based on fused heterogeneous networks with self-supervised feature representation.BioMed Research International, 2022:8007713, Aug 2022. ISSN 2314-6133. doi: 10.1155/2022/8007713.URL David Tellez, Geert J. S. Litjens, Jeroen A. van der Laak, and Francesco Ciompi. Neural image compression forgigapixel histopathology image analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence,43:567578, 2018. David Tellez, Diederik Hppener, Cornelis Verhoef, Dirk Grnhagen, Pieter Nierop, Michal Drozdzal, JeroenLaak, and Francesco Ciompi. Extending unsupervised neural image compression with supervised multitasklearning. In Medical Imaging with Deep Learning, pages 770783. PMLR, 2020. Sai Chandra Kosaraju, Jeongyeon Park, Hyun Lee, Jung Yang, and Mingon Kang. Deep learning-basedframework for slide-based histopathological image analysis. Scientific Reports, 12, Nov 2022. doi: 10.1038/s41598-022-23166-0. URL Nikhil Naik, Ali Madani, Andre Esteva, Nitish Shirish Keskar, Michael F. Press, Daniel Ruderman, David B.Agus, and Richard Socher. Deep learning-enabled breast cancer hormonal receptor status determinationfrom base-level h&e stains.Nature Communications, 11(1):5727, Nov 2020.ISSN 2041-1723.doi:10.1038/s41467-020-19334-3. URL Gabriele Campanella, Matthew G Hanna, Luke Geneslaw, Allen Miraflor, Vitor Werneck Krauss Silva, Klaus JBusam, Edi Brogi, Victor E Reuter, David S Klimstra, and Thomas J Fuchs. Clinical-grade computationalpathology using weakly supervised deep learning on whole slide images. Nature medicine, 25(8):13011309,2019.",
  "Angelos Katharopoulos and Franois Fleuret. Processing megapixel images with deep attention-samplingmodels. In International Conference on Machine Learning, pages 32823291. PMLR, 2019": "Richard J. Chen, Chengkuan Chen, Yicong Li, Tiffany Y. Chen, Andrew D. Trister, Rahul G. Krishnan, andFaisal Mahmood. Scaling vision transformers to gigapixel images via hierarchical self-supervised learning.In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1614416155,June 2022. Paras Jain, Ajay Jain, Aniruddha Nrusimha, Amir Gholami, Pieter Abbeel, Joseph Gonzalez, Kurt Keutzer,and Ion Stoica. Checkmate: Breaking the memory wall with optimal tensor rematerialization. Proceedingsof Machine Learning and Systems, 2:497511, 2020.",
  "Marisa Kirisame, Steven Lyubomirsky, Altan Haan, Jennifer Brennan, Mike He, Jared Roesch, Tianqi Chen,and Zachary Tatlock. Dynamic tensor rematerialization. arXiv preprint arXiv:2006.09616, 2020": "Amir Gholami, Ariful Azad, Peter Jin, Kurt Keutzer, and Aydin Buluc. Integrated model, batch, anddomain parallelism in training neural networks. In Proceedings of the 30th on Symposium on Parallelismin Algorithms and Architectures, pages 7786, 2018. Nikoli Dryden, Naoya Maruyama, Tom Benson, Tim Moon, Marc Snir, and Brian Van Essen. Improvingstrong-scaling of cnn training by exploiting finer-grained parallelism. In 2019 IEEE International Paralleland Distributed Processing Symposium (IPDPS), pages 210220. IEEE, 2019. Yosuke Oyama, Naoya Maruyama, Nikoli Dryden, Erin McCarthy, Peter Harrington, Jan Balewski, SatoshiMatsuoka, Peter Nugent, and Brian Van Essen. The case for strong scaling in deep learning: Training large3d cnns with hybrid parallelism. IEEE Transactions on Parallel and Distributed Systems, 32(7):16411652,2020. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations towardtraining trillion parameter models. In SC20: International Conference for High Performance Computing,Networking, Storage and Analysis, pages 116. IEEE, 2020.",
  "Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost.arXiv preprint arXiv:1604.06174, 2016": "Minsoo Rhu, Natalia Gimelshein, Jason Clemons, Arslan Zulfiqar, and Stephen W Keckler. vdnn: Virtualizeddeep neural networks for scalable, memory-efficient neural network design. In 2016 49th Annual IEEE/ACMInternational Symposium on Microarchitecture (MICRO), pages 113. IEEE, 2016. Henggang Cui, Hao Zhang, Gregory R. Ganger, Phillip B. Gibbons, and Eric P. Xing. Geeps: scalabledeep learning on distributed gpus with a gpu-specialized parameter server. In Proceedings of the EleventhEuropean Conference on Computer Systems, EuroSys 16, New York, NY, USA, 2016. Association forComputing Machinery. ISBN 9781450342407. doi: 10.1145/2901318.2901323. URL Wouter Bulten, Kimmo Kartasalo, Po-Hsuan Cameron Chen, Peter Strm, Hans Pinckaers, Kunal Nagpal,Yuannan Cai, David F Steiner, Hester van Boven, Robert Vink, et al. Artificial intelligence for diagnosisand gleason grading of prostate cancer: the panda challenge. Nature medicine, 28(1):154163, 2022. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchicalimage database. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR), pages 248255. Ieee, 2009. Ming Y Lu, Drew FK Williamson, Tiffany Y Chen, Richard J Chen, Matteo Barbieri, and Faisal Mahmood.Data-efficient and weakly supervised computational pathology on whole-slide images. Nature BiomedicalEngineering, 5(6):555570, 2021. Yu Zhao, Fan Yang, Yuqi Fang, Hailing Liu, Niyun Zhou, Jun Zhang, Jiarui Sun, Sen Yang, Bjoern Menze,Xinjuan Fan, and Jianhua Yao. Predicting lymph node metastasis using histopathological images based onmultiple instance learning with deep graph convolution. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR), pages 48374846, June 2020. Bin Li, Yin Li, and Kevin W. Eliceiri. Dual-stream multiple instance learning network for whole slide imageclassification with self-supervised contrastive learning. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR), pages 1431814328, June 2021b. Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv Jgou.Training data-efficient image transformers & distillation through attention. In International conference onmachine learning, pages 1034710357. PMLR, 2021. Robert Geirhos, Jrn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge,and Felix A Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2(11):665673, 2020.",
  "A.1PANDA": "The Prostate cANcer graDe Assessment Challenge Bulten et al. (2022) consists of one of the largest publicallyavailable datasets for Histopathological images which scale to a very high resolution. It is important tomention that we do not make use of any masks as in other aforementioned approaches. Therefore, thecomplete task boils down to taking an input high-resolution image and then classifying them into 6 categoriesbased on the International Society of Urological Pathology (ISUP) grade groups. There are a total of 10.6Kimages which are split into train and test sets in the ratio 80:20.",
  "A.2UltraMNIST": "This is a synthetic dataset generated by making use of the MNIST digits. For constructing an image, 3-5digits are sampled such that the total sum of digits is less than 10. Thus an image can be assigned a labelcorresponding to the sum of the digits contained in the image. Each of the 10 classes from 0-9 has 1000samples making the dataset sufficiently large. Note that the variation used in this dataset is an adaptedversion of the original data presented in Gupta et al. (2022), with background noise removed so that anyshortcut learning is avoided Geirhos et al. (2020). Since the digits vary significantly in size and are placedfar from each other, this dataset fits well in terms of learning semantic coherence in a image. Moreover, itposes the challenge that downscaling the images leads to a significant loss of information. While even higherresolution could be chosen, we later demonstrate that the chosen image size is sufficient to demonstrate thesuperiority of PatchGD over the conventional gradient descent method.",
  "A.3TCGA": "The TCGA-NSCLC dataset, known as The Cancer Genome Atlas-Non-Small Cell Lung Cancer, encompassestwo distinct types of lung cancer: Lung Adenocarcinoma (LUAD), with 522 cases, and Lung Squamous CellCarcinoma (LUSC), with 504 cases, with a total number of image files 3220. The data was split in a stratifiedmanner using the patient cases, into train and test set in the ratio 80:20, making sure there is no data leakagefrom train to test. The whole slide images are used to evaluate the performance of baseline and PatchGD inclassifying the lung cancer subtypes.",
  "BTraining Methodology and Hyperparameters": "For Tables 1,2,3,5,7,8,9,10,11 presented in the main paper, all models are trained for 100 epochs with Adamoptimizer and a peak learning rate of 1e-3. A learning rate warm-up for 2 epochs starting from 0 and lineardecay for 98 epochs till half the peak learning rate was employed. The latent classification head consists of4 convolutional layers with 256 channels in each. We perform gradient accumulation over inner iterationsfor better convergence, in the case of PANDA. To verify if results are better, not because of an increase inparameters (coming from the classification head), baselines are also extended with a similar head. GD*, forMobileNetV2 on UltraMNIST, refers to the baseline extended with this head. In the case of low memory, as demonstrated in the UltraMNIST experiments, the original backbone architectureis trained separately for 100 epochs. This provides a better initialization for the backbone and is further usedin PatchGD as mentioned in Tables 1 and 2. For baseline in PANDA at 2048 resolution, another study involved gradient accumulation over images, whichwas done for the same number of images that can be fed when the percent sampling is 10% i.e. 14 times sincea 2048x2048 image with a patch size of 128 and percentage sampling of 10 percent can have a maximumbatch size of 14 under 16GB memory constraint. That is to say, the baseline can virtually process a batch of14 images. This, however, was not optimal and the peak accuracy reported was in the initial epochs dueto the loading of the pre-trained model on the lower resolution after which the metrics remained stagnant(accuracy: 32.11%, QWK:0.357).",
  ": Sample PANDA images along with their latent space Z. It can be seen that the latent space clearlyacts as a rich feature extractor": "For the ImageNet experiments in , we follow the exact training recipe as given in Touvron et al.(2021). This includes a 300 epoch training regime with cosine decay and a combination of multiple imageaugmentations4. Convergence of baseline models. For all the baseline experiments reported in the paper, we have alsoinvestigated extended training of the baseline to match the training time of the corresponding PatchGDexperiments. However, it has been consistently observed that the configurations reported in the paper arethe most optimal and the baseline converged within the initial 100 epochs for all the configurations. Thisclearly confirms that the gain reported by PatchGD is not due to the additional training time associatedwith this method.",
  "Resnet50PANDA48GB4096256856.9Resnet50PANDA48GB40962563259.7": "results using StyleGAN-2 on the CIFAR-10 dataset showed that our method generated patches of 16 16which were stitched together and analyzed by the discriminator, leading to a comparable FID score of 6.3to the standard GDs FID score of 6.1. We believe this small performance gap can be eliminated withhyperparameter optimization. We consider that the potential of PatchGD in generative modeling can bemaximized by generating large images with various semantic contexts, although this needs to be exploredfurther. PatchGD for segmentation. We discuss here how PatchGD can be used for tasks such as segmentation orany other encoder-decoder tasks We have discussed generative modeling already, and since the setup wouldbe something similar, we present here an understanding of how the PatchGD formulation would unfold fortasks such as segmentation. For the task of segmentation as well, we have two sets of weights 1 and 2 thatconstitute the encoder and the decoder, respectively. Here, the encoder generates a Z-block and the decoderis used to generate the segmentation map from the Z-block. Similar to the classification problem, PatchGDoperates on each image over a course of multiple inner iterations. At each inner iteration, patches are sampledfrom image x and accordingly passed through and the output is then used to update the respective parts ofZ. Further, k c-dimensional vectors are sampled from Z and passed through the decoder to generate maskpatches that are used to update parts of the segmentation map y, and the process is repeated. Note thatsimilar to Zfilling, this process also requires y-filling before the model updates of the encoder and decoderare performed over patches. For this purpose, we can first train a segmentation model on lower-resolutionimages of the chosen task and then use its encoder and decoder, and starting models for the PatchGD learningprocess.",
  "FApplications in Time Series Classification": "Extending the concept of PatchGD to the 1-dimensional case, we find the application in time series classification.For this task, we take the example of UCI Human Activity Recognition Dataset. A set of 9 inertial signalsat 128 unique time stamps are used to predict the action being executed (sitting, walking, etc.). For thebaseline model, we use a basic 1-d Convolutional Network with 64 kernels each of size 3 and a linear layerat the end which achieves an accuracy of 88.9%. The model is trained using Adam as an optimizer with aconstant learning rate of 1e-3 for 30 epochs with 32 batch size. The counterpart PatchGD-inspired approachinvolved the same 1-d convolutional network as the encoder with an intermediate latent vector, with othercommon hyperparameters being kept the same. The time series is broken into chunks temporally, each chunkbeing of length 16. Each inner iteration consists of sampling 25% of the total chunks with gradient updatesenabled. The model is updated at the final iteration. Impressively, the approach achieves similar accuracyof 88.5%. The results are promising and yet again demonstrate the wide application to other tasks wherePatchGD can be applied. Although this needs to be investigated further."
}