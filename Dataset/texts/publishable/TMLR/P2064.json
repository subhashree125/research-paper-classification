{
  "Abstract": "The predominant method for computing confidence intervals (CI) in few-shot learning (FSL)is based on sampling the tasks with replacement, i.e. allowing the same samples to appear inmultiple tasks. This makes the CI misleading in that it takes into account the randomnessof the sampler but not the data itself. To quantify the extent of this problem, we conducta comparative analysis between CIs computed with and without replacement. These re-veal a notable underestimation by the predominant method. This observation calls for areevaluation of how we interpret confidence intervals and the resulting conclusions in FSLcomparative studies. Our research demonstrates that the use of paired tests can partiallyaddress this issue. Additionally, we explore methods to further reduce the (size of the) CI bystrategically sampling tasks of a specific size. We also introduce a new optimized benchmark,which can be accessed at",
  "Introduction": "The recent surge of interest in few-shot learning (FSL), driven by its potential applications in many real-world scenarios, has led to a proliferation of new methods and novel experimental protocols (Sung et al.,2018; Snell et al., 2017; Bendou et al., 2022; Zhang et al., 2020). If in conventional machine learning it iscommon to benchmark methods using a fixed split into training and validation sets, FSL presents unique",
  "Published in Transactions on Machine Learning Research (09/2024)": "Jerzy Neyman. Outline of a theory of statistical estimation based on the classical theory of probability.Philosophical Transactions of the Royal Society of London. Series A, Mathematical and Physical Sciences,236(767):333380, 1937. Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes.In 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing, pp. 722729. IEEE,2008. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, GirishSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models fromnatural language supervision. In International conference on machine learning, pp. 87488763. PMLR,2021. Mengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B Tenenbaum, HugoLarochelle, and Richard S Zemel. Meta-learning for semi-supervised few-shot classification. arXiv preprintarXiv:1803.00676, 2018.",
  "FT+ 0 0+ 0 00 0 0": ": Comparison between different methods for few-shot classification. Each entry consists of three elements:[With Replacement (Closed), Without Replacement(Open), Paired Tests (PT)]. Symbols + and respec-tively indicate significant differences (positive and negative)between the row method and the column method while 0is non-conclusive. Results derive from the DTD test split(bottom left triangle) and Traffic Signs (top-right) split ofMetaDataset, with task sampling at 5 shots, 5 ways, and15 queries. Note the inversion in bold. NCC (NearestClass Centroid), FT (Fine-tune) with CLIP and DINO asfeature extractors. The purpose of this paper is to highlightthis crucial consideration when computing CIs.We propose strategies to address this issueand obtain meaningful comparisons while stillaccounting for the randomness of the data.These strategies rely on a) Paired Tests(PT), where methods are evaluated on thesame set of generated tasks, and b) adequatelysizing tasks. Throughout the paper, we focuson the specific case of few-shot classification invision, the most popular area of research in thefield of FSL. Our investigation using Open Confidence In-tervals (OCIs) can lead to conclusions that areinconsistent with those obtained using the clas-sical approach in the field of few-shot learn-ing.In particular, we find that some meth-ods previously reported as statistically signifi-cantly outperforming others are actually indis-tinguishable when using OCIs, and vice versa.In addition, we show cases where the use ofCCIs lead to (statistically significant) conclu-sions that are diametrically opposed to thoseobtained using PT. One such example is givenin , where we compare different methods for few-shot classification depending on their used featureextractor (here CLIP (Radford et al., 2021) or DINO (Caron et al., 2021)) and their adapting methods (hereLogistic Regression (LR), Nearest Class Centroid (NCC) or Fine-Tuning (FT)). In the table, we report threeconclusions for each pair of model and method combinations: the first is obtained from the methodologydescribed in Luo et al. (2023) where the authors used the predominant way to compute CIs, that is CCIs;the second OCIs, and the third is based on PT. A conclusion that the row method is better than the column",
  "The main contributions of this work are:": "We highlight the importance of considering data replacement when computing CIs for FSL methodscomparison. Our study illustrates the impact on CI ranges when transitioning from closed to openCIs on standard off-the-shelf vision datasets. By implementing paired evaluations, wherein multiple methods are compared on identically gener-ated task sets, we demonstrate the ability to reach conclusive comparisons more frequently thanwhen relying on simple performances with OCIs. We investigate how to optimize task generation from a given dataset, taking into account its size andnumber of classes, to reach small ranges of CIs and lead to more conclusive comparisons betweenmethods. The result is a benchmark that can be used for few-shot classification of images.",
  "Standard Evaluation and Notations": "The predominant method of evaluation in the field of few-shot classification is described in Algorithm 1. Afew-shot classification task T = (K, S, Q) comprises a set of classes K, a support set S = {Sc}cK and aquery set Q = {Qc}cK where Sc, Qc denote the sets of support and query examples for each class c K.Let K = |K| denote the number of ways (i.e. classes in a few-shot task), S = |Sc| the number of shots perclass, and Q = |Qc| the number of queries per class (for simplicity, we assume the classes to be balanced).Few-shot evaluation is typically performed by constructing many tasks from a larger evaluation dataset. Anevaluation dataset D = (C, X) comprises a set of classes C and examples for all classes X = {Xc}cC. LetC = |C| K denote the number of classes, and N = |Xc| S + Q the number of examples per class.As highlighted in the introduction, the rationale followed by the CI computation predominant method is toconsider D to be fixed and non-probabilistic.",
  ": end procedure": "The standard few-shot task sampler constructs T random tasks with K ways, S shots and Q queries from adataset D as outlined in Algorithm 1. This procedure introduces additional random variables (besides thedataset itself) in the selection of classes and examples. Let Tt = (Kt, St, Qt) for t = 1, . . . , T denote thesampling of tasks. The average accuracy on each task is obtained as:",
  "T.(3)": "Note that in the case of a very small number of tasks, Students distributions can be used instead. Also, wetook the example of 95% CIs, which is arbitrary but very common in the literature. For more generality, weconsider a probability plimit in the following for all theoretical considerations, and stick with the 95% valuefor experiments. Note that, as the number of tasks becomes larger, it is inevitable that many tasks will re-use examples, sincetasks are constructed independently with replacement. Therefore, as T becomes large, the variance of thesample mean will approach the conditional variance Var[ A | D], and the confidence interval will representthe likely range of outcomes if we were to repeat the experiment with a random set of tasks on the same",
  "Are OCIs larger than CCIs? An empirical study": "In contrast to Algorithm 1, the task sampling without replacement is presented in Algorithm 2 (see Ap-pendix). Note that we make an explicit use of a Students law estimator in this algorithm as it is expectedthat for some small datasets it can only generate but a few independent tasks. In the latter, the total numberof tasks T is determined directly by the sampling process, thanks to a specific stopping condition, based onthe exhaustion of the dataset. This is done in an effort to minimize the obtained CIs ranges. Since samplescannot be sampled twice, we can consider that the classes and examples are drawn IID from an underlyingdata distributions p(C) and p(X | C). This is why OCIs also account for the randomness of the data. In our experiments, we utilize datasets from the Metadataset Benchmark as referenced in Triantafillou et al.(2019). This benchmark comprises 10 datasets, out of which we employ 9, excluding Imagenet, to focuson cross-domain results in line with the recent trend in the literature (Zhou et al., 2022b). These includeOmniglot (handwritten characters), Aircraft, CUB (birds), DTD (textures), Fungi, VGG Flowers, TrafficSigns, Quickdraw (crowd-sourced drawings) and MSCOCO (common objects) (Lake et al., 2015; Maji et al.,2013; Wah et al., 2011; Cimpoi et al., 2014; Schroeder & Cui, 2018; Nilsback & Zisserman, 2008; Houbenet al., 2013; Jongejan et al., 2016; Lin et al., 2014). Luo et al. (2023) details few-shot accuracies for 2000 tasks with 5-shots, 5 ways, and 15 queries in a com-prehensive table covering various works on the Metadataset datasets. Our studys only difference lies in theadoption of the T = 600 setting, a more prevalent choice in existing literature. If CCIs are found to benarrower than OCIs with this smaller T, it will be even starker with T = 2000 tasks as shown in Equation 3.Our primary reference for methods and models is the comprehensive compilation provided by Luo et al.(2023), a foundational starting point for our experiments. Our findings are detailed in , showcasing results across different few-shot methods and datasets.Firstly, there is a noticeable homogeneity in CCIs, arising from the fixed number of tasks set at T = 600,which contrasts with the variability observed in OCIs. Interestingly, CCIs are substantialy narrower thanOCIs for small datasets such as Aircraft and DTD. Conversely, in the case of larger datasets like Quickdraw,CCIs become larger than OCIs due to T = 600 being insufficient to deplete the dataset. Indeed, Aircraftand DTDs test splits contain 1,500 and 840 samples respectively, whereas the test splits for MSCOCO andQuickdraw have much larger sizes of 152,000 and 7.7 million samples respectively. Across various datasets,models and methods, CCIs are on average 3.8 times larger than OCIs. These results highlight the imperativeneed for accurate interpretation of Confidence Intervals, given the dramatic differences between OCIs andCCIs ranges, that undoubtedly lead to disagreeing conclusions if misinterpreted. We also notice that for cases where methods reach accuracies near 100%, like adaptation methods usingCLIP (unlike those using DINO) on the CUB dataset, both types of CIs become narrower. This is due toaccuracy saturation at 100%, which reduces the standard deviation of accuracies.",
  "Impact on Conclusiveness": "First let us recall how confidence intervals are used to draw conclusions when comparing methods. Suppose wehave two variables of interest x1 and x2, with their corresponding plimit - confidence intervals (a generalizedversion of 95%-CIs) [x1 1, x1 + 1] and [x2 2, x2 + 2]. To draw conclusions about the fact x1 is smallerthan x2, we proceed as follows: if the two intervals do not intersect, and x1 + 1 < x2 2, then:",
  "where the (1 plimit)/2 part comes from the symmetry of the Gaussian distribution": "With this in mind, the results listed in can lead to different conclusions depending on whether CCIsor OCIs are used. For example, using the DTD dataset, CCIs lead to conclusive comparisons between bothbackbones and methods, whereas OCIs are inconclusive as they all intersect. Again, this should not be seenas a contradiction, but rather as having different paradigms about the comparisons. CCIs compare methodsif they were reused on the same data, whereas OCIs focus on the underlying distribution. Next, we proposetwo methods to improve conclusiveness when comparing methods, namely paired tests in and tasksizing in .",
  "Definitions": "As an effort to reduce the ranges of Open Confidence Intervals (OCIs), we propose to make use of pairedtests. Indeed, as we pointed out in the introduction, FSL tasks have a vast diversity in difficulty, lead-ing to a high variance in accuracy across tasks.It is noteworthy that a task deemed hard for methodA often aligns in difficulty for method B. This parallel in task difficulty across different methods was",
  "since Var[X Y ] = Var[X] + Var[Y ] 2 Cov(X, Y )": "The lower variance of t compared to At results ina correspondingly smaller confidence interval, as de-tailed in Equation 3. Consequently, this leads to sce-narios where two methods can exhibit significant dif-ferences when analyzed using paired testing despitethere being no significant differences when directlycomparing accuracies. We performed experimentscomparing various methods to fine-tuning (FT). Re-sults are shown in , where each line corre-sponds to a specific dataset and feature extractor and each column to a combination of an adaptationmethod and a feature extractor. Two conclusions are depicted, the first being based on directly comparingaccuracies, and the second using paired tests instead. Note that in all cases, we rely on OCIs, that is tosay sampling without replacements. Let us also notice that paired tests never lead to contradictory con-clusions with direct comparisons of accuracies, a direct consequence of previous remarks about the mean ofdifferences. Their difference lies in the ability to conclude or not. The fine-tuning adaptation method is selected as the baseline in , primarily due to its substantialcomputational cost. The cost of fine-tuning stems from the necessity to update the weights of the entirefeature extractor for each considered task. The objective is to determine if it surpasses more cost-effectivemethods in performance. In comparative analyses excluding the comparison between CLIP and DINO andfocusing only on the same feature extractor, it is frequently noted that fine-tuning either underperformsrelative to other methods or the results are inconclusive. Indeed, fine-tuning significantly outperforms othermethods in only 4 out of 36 cases, while underperforming in 14 cases. The outcomes in the remaining instancesremain inconclusive. Consequently, we conclude that fine-tuning, in light of its considerable computationaloverhead, is not particularly advantageous to consider. These results should be nuanced by the dependencyof fine-tuning on hyperparameters which makes any assertions about this method contingent upon a specificset of hyperparameters (Kumar et al., 2022). For each of the nine considered datasets, we have a total of 135 unique comparisons, taking into account onlydistinct pairs of (model, methods) across two models and three methods. We found that 57 comparisonswere conclusive using direct comparison with OCI while 94 were conclusive using paired tests. inthe Appendix illustrates this. illustrates, on the DTD and Traffic signs dataset, that the three different approaches for computingCIs discussed so far result in varied assessments of the significance of differences between two methods. Onall datasets, in 27 instances out of the 114 where the comparison with replacement was conclusive, that is 23% of such cases, a pattern emerges: a comparison initially classified as significantly different becomesnon-conclusive under sampling without replacement, and then conclusive again when a paired test is applied.On 12 instances, 11% of previously considered cases, conclusiveness is not confirmed by the paired test.",
  "VGG Flower0000000": ": Impact of Paired Testing on Significance Relative to Simple Comparison.In this analysis, wecompare the Finetune (FT) method against all other methods across both CLIP and DINO models. Theinitial character in each pair denotes the significance outcome determined without replacement, while thesubsequent character reflects the result derived with paired testing. Here, 0 represents a non-significantdifference, + indicates a significantly higher accuracy of the FT method, and conveys a significantly loweraccuracy. The FT columns comparing CLIP and DINO are opposites of each other. Particularly striking is one instance of inversion, where a method previously deemed significantly more accu-rate than another was found to be significantly less accurate using a paired test. This reversal was observedin the comparison of Fine-tuning on DINO versus NCC with CLIP features on the Traffic Signs dataset.This implies that a method can significantly outperform another on a specific dataset, yet significantly un-derperform when evaluated across the entire distribution. The dataset is thus a particular instance of datathat favors one method. This example powerfully exemplifies the need for clarity when interpreting CIs. A natural question that arises from previous considerations is that of how to size tasks when performingsampling without replacement, aiming to reduce the range of obtained CIs. In that matter, we consider thesize of the support set to be fixed at KS, leaving as the only free variable the number of queries per taskand per class Q. Indeed, increasing the number of queries will inevitably reduce the total number of taskswe can construct, as shown in the following equation. Assuming a balanced dataset, we can estimate thenumber of tasks T that can be sampled by exhausting the full dataset:",
  "If increasing Q reduces the number of tasks, it also changes": "Var(At) which is proportional to the CI. Assuch, there exists a trade-off between the number of queries and the feasible number of tasks that can begenerated to minimize OCIs for any given dataset. Intuitively, measuring A with a small T (and consequentlya high Q) results in extensive CI ranges, a phenomenon depicted in Equation 3. Conversely, measuring withQ = 1 may generate many tasks (large T) with an extremely high variance because the accuracy per classbecomes either 0% or 100%. In the following, we aim to identify the optimal number of queries, denoted Q,",
  "DataModel": ": Variance of the average accuracy vs.the number of queries with synthetic data. Thetwo classes are represented as 1D GaussiansN(1, 1) and N(1, 1). The size of the datasetis N = 1000 (500 samples per class). Tasks aresampled according to Algorithm 2. The numberof shots is set to 5. We fit this with the modeldescribed in Equation 10 and observe a strong fitof the model with our experiment.",
  "with , and also defined in the Appendix, and > 0": "These parameters are difficult to estimate in particularwhen dealing with real datasets and methods. If 0,then Var Ais decreasing as a function of Q since Q N. In the following, we focus only on cases where >0. This choice is supported by empirical evidence, whichwe will present later, indicating a U-shaped relationshipbetween the variance of A and Q for a certain range ofS. Assuming this, Var Areaches its minimum at Q =",
  "Effect of S and N on Q": "Given the definition of and obtained in Equation 10,we find that Q is an increasing function of S and a con-stant function of N. In this section, we show that theseresults are confirmed empirically on real datasets. We propose to study the aforementioned variance modelof A with respect to Q in a simplistic 1D representation ofsamples. In our model, two class distribution are repre-sented as two Gaussians (Ni = N(i, i) with i {1, 2}).We then sample an artificial balanced dataset of fixed sizeN. Next, we sample tasks within this artificial dataset until exhaustion with the procedure described in Algo-rithm 2 setting K = C = 2. Using the NCC classifier we obtain a set of accuracies on which we can computethe average accuracy A. This procedure consisting of instantiating the synthetic dataset from Gaussiansand measuring A is iterated. This yields a set of { Aj}j for a given set of parameters {S, Q, N, N1, N2} fromwhich we compute an empirical variance Var( A). In , we show the measured Var( A) vs Q. We use 1000 samples in the datasets, split between thetwo classes. We set the number of shots to S = 5. The model in Equation 10 is fitted very precisely. Thediscretisation effect seen at high Q is due to the low number of tasks. Next, we study the effect of S and Nand compare it to our experiments with synthetic data. Increasing S shifts the curves minimum from Q = 1 towards Q + as depicted in . This alignswith our models predictions. At S = 1, opting for Q = 1 effectively has two effects: (a) the high varianceof At due to small support and query sets increases the variance of A and (b) low Q allows a significantlylarger T, thus reducing the overall variance of A as shown in Equation 3. Conversely, for S 20, the settingboils down to classical transfer learning. Indeed, the narrowest CI is attained with one task with a largesupport and query set task. Finally, we find a third regime where, for a range of values of S, Q is nontrivial.It corresponds to what is shown in the S = 5 regime in . When increasing N, Q should not be affected according to Equation 10. However, we observe a hardlyperceptible shift of Q in when N is increased. We consider this effect to be sufficiently small andtherefore negligible. Next, we explore how these results extend to real-world datasets.",
  "S=1S=5S=20": ": Variance of the average accuracy A and number of tasks T across different settings of S and N,derived from synthetic datasets featuring two 1D Gaussian classes, N(1, 1) and N(1, 1). The left pair ofgraphs display results with a fixed number of shots (S = 5), while the right pair of graphs show results fora constant sample size in the synthetic dataset (N = 1000).",
  "Real Dataset Experiments": "We now shift our focus to the findings derived from real image datasets. These datasets are often unbalancedand exhibit a variety of class numbers. Our objective is to determine whether the earlier conclusions remainvalid in this context. First, we observe that Q does not depend on the size of datasets. However the size of the dataset scalesT which in turn scales CI95%. Similar to the results with synthetic data, we observe a discretisation of theconfidence interval in the high Q (low T) regime. We also observe a similar phenomenon with different regimes in 1, 5 and 10 shots in . Our analysissuggests that for tasks in the 1 shot regime, the best number of queries, Q, should be set to 1. For taskswith S = 5 and S = 10, the best values for Q are approximately Q = 5 and Q = 7, respectively. also clearly shows that the predominent 15 queries is not optimal to narrow the OCI. We also show similarvalues for Q using DINOv2 instead of CLIP in the Appendix ().",
  "Benchmark Proposal": "Building on previously obtained results, we propose a simple benchmark where Paired Tests are used andthe value of Q is chosen as the minimum found in the previous paragraph. Implicitly, we are assumingthat the minimum of s OCI is also reached at Q. More precisely, we are assuming the independenceof the covariance with respect to Q. These assumptions are backed by the improved number of conclusivecomparisons in when optimizing Q and using paired tests. Indeed, while PT yieled 94 conclusivecomparisions, PT with optimized Q yields a little more with 97 conclusive comparisons. We present our results with the baselines adaptation methods previously studied in using DINOv2as our baseline model. Our experiments consistently show that, for a given model, fine-tuning tends to beless effective than both logistic regression and the nearest class centroid methods. We also observe the choiceof model is primordial with a clear advantage of DINOv2 over CLIP and DINO on most datasets. Ourbenchmark, including the code, seed values, task descriptions, and accuracy results, is available for use.",
  "Related Work": "Few-Shot LearningSince the seminal works of Vinyals et al. (2016) and Snell et al. (2017), the fieldof few-shot learning has known many contributions. Most solutions rely on the use of a pretrained featureextractor, trained on a generic abundant dataset (Wang et al., 2020; Bendou et al.; Antoniou et al., 2018).The feature extractor can then be used as is on the target problem (Wang et al., 2019), or adapted before",
  "AircraftCUBDTDFungiVGG FlowerMSCOCO": ": (Left) Confidence Interval ranges (Right) Corresponding number of tasks generated. In all graphsthe x-axis is the number of queries Q. These results represent averages from multiple trials, with the numberof trials tailored according to the Task Count (T). Some curves are stopped before Q reaches 100 becauseof the number of samples per class. We do not show Omniglot and Quickdraw for visibility. classifying (Zhang et al., 2021).Most proposed methods differ in the way they combine the pretrainedfeature extractor and their adaptation to the target problem Luo et al. (2023). Over the years, multiplebenchmarks have been proposed, including MiniImageNet (Vinyals et al., 2016), Omniglot (Lake et al., 2015)and TieredImegenet (Ren et al., 2018). If initially, these benchmarks focused on the in-domain case, wherethe feature extractor is trained on disjoint classes from the target problem, but all drawn from the sameinitial dataset, the trend evolved with the introduction of Meta-dataset (Triantafillou et al., 2019) (MD)and later COOP (Zhou et al., 2022a), where the feature extractor is trained on a large generic dataset andapplied to various other domains, including fine-grain problems, embodying a cross-domain evaluation.",
  "Limitations": "A first limitation of our study we would like to point out is that for large datasets such as MSCOCO orQuickDraw, the predominant method of CI calculation leads to intervals that are actually larger than ourproposed OCI. As such, on such large datasets using CCI may not be an unfair approximation. Furthermore, our mathematical modeling only explains the origin of the minimum of CI with respect to Qbut does not provide a way to find it analytically since we cannot easily estimate , and in Equation 10.",
  "Conclusion": "In our study, we demonstrated the stark contrast between Open and Closed measurements of method accu-racy in Few-Shot Learning. Notably, OCIs take into account data randomness but are far wider than CCIs.We identified two major approaches that contribute to narrowing the OCIs and subsequently introduced abenchmark which uses these approaches. Our findings underscore the importance of using confidence inter-vals that account for data randomness in evaluations, a practice we advocate extending beyond classificationand vision to encompass all domains employing task-based few-shot learning assessments.",
  "Sarah Belia, Fiona Fidler, Jennifer Williams, and Geoff Cumming. Researchers misunderstand confidenceintervals and standard error bars. Psychological methods, 10(4):389, 2005": "Yassir Bendou, Yuqing Hu, Raphael Lafargue, Giulia Lioi, Bastien Pasdeloup, Stphane Pateux, and VincentGripon. Easy: Ensemble augmented-shot y-shaped learning: State-of-the-art few-shot classification withsimple ingredients. arxiv 2022. arXiv preprint arXiv:2201.09699. Yassir Bendou, Yuqing Hu, Raphael Lafargue, Giulia Lioi, Bastien Pasdeloup, Stphane Pateux, and VincentGripon. Easyensemble augmented-shot-y-shaped learning: State-of-the-art few-shot classification withsimple components. Journal of Imaging, 8(7):179, 2022.",
  "Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning throughprobabilistic program induction. Science, 350(6266):13321338, 2015": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollr, andC Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13thEuropean Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pp. 740755.Springer, 2014. Chenghao Liu, Zhihao Wang, Doyen Sahoo, Yuan Fang, Kun Zhang, and Steven CH Hoi. Adaptive tasksampling for meta-learning. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK,August 2328, 2020, Proceedings, Part XVIII 16, pp. 752769. Springer, 2020.",
  "Student. The probable error of a mean. Biometrika, 6(1):125, 1908": "Qianru Sun, Yaoyao Liu, Zhaozheng Chen, Tat-Seng Chua, and Bernt Schiele. Meta-transfer learning throughhard tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(3):14431456, 2020. Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales. Learningto compare: Relation network for few-shot learning. In Proceedings of the IEEE conference on computervision and pattern recognition, pp. 11991208, 2018.",
  "Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni. Generalizing from a few examples: Asurvey on few-shot learning. ACM computing surveys (csur), 53(3):134, 2020": "Chi Zhang, Yujun Cai, Guosheng Lin, and Chunhua Shen. Deepemd: Few-shot image classification withdifferentiable earth movers distance and structured classifiers. In Proceedings of the IEEE/CVF conferenceon computer vision and pattern recognition, pp. 1220312213, 2020. Renrui Zhang, Rongyao Fang, Wei Zhang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Qiao, and Hong-sheng Li. Tip-adapter: Training-free clip-adapter for better vision-language modeling. arXiv preprintarXiv:2111.03930, 2021.",
  "AircraftCUBDTDFungiOmniglotVGG FlowerMSCOCO": ": This figure shows that the value of Q is not dependent on the model used. This experiment isconducted using DINO v2 instead of CLIP. (Left) Confidence Interval ranges (Right) Corresponding numberof tasks generated. In all graphs the x-axis is the number of queries Q. These results represent averages frommultiple trials on DINO v2, with the number of trials tailored according to the Task Count (T). Some curvesare stopped before Q reaches 100 because of the number of samples per class. We do not show Omniglotand Quickdraw for visibility.",
  "Total number of comparisons": ": Effect of using paired tests comparisons between all possible pairs formed from combinations ofmodels and methods summed across datasets (with Omniglot removed for technical reasons) for 1, 5 and 10shots. The number of conclusive comparisons is counted on the Y-axis. When Q is not optimized, we useQ = 15. This histogram illustrates that paired test and the optimization of the size of tasks yields the maximumnumber of conclusive comparisons. Optimizing Q slightly improves the number of conclusive comparisonscompared to simple paired tests."
}