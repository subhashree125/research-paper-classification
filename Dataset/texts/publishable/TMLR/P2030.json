{
  "Abstract": "Image generation using diffusion can be controlled in multiple ways.In this paper, wesystematically analyze the equations of modern generative diffusion networks to proposea framework, called MDP, that explains the design space of suitable manipulations. Weidentify 5 different manipulations, including intermediate latent, conditional embedding,cross attention maps, guidance, and predicted noise. We analyze the corresponding param-eters of these manipulations and the manipulation schedule. We show that some previousediting methods fit nicely into our framework. Particularly, we identified one specific con-figuration as a new type of control by manipulating the predicted noise, which can performhigher-quality edits than previous work for a variety of local and global edits.",
  ": Our proposed manipulation of predicted noise, which fits into our proposed editing frameworkMDP, can do multiple local and global edits without training or finetuning": "Previously, image editing using GANs has achieved great success (Isola et al., 2017; Zhu et al., 2017; Choiet al., 2018; Wang et al., 2018; Huang et al., 2018; Kim et al., 2017; Bau et al., 2020).As large-scaletext-to-image datasets became available, text-guided image synthesis and editing has obtained increasingattention (Ramesh et al., 2021; Crowson et al., 2022; Ding et al., 2021; 2022; Gafni et al., 2022). Generativediffusion models Ho et al. (2020); Rombach et al. (2022); Saharia et al. (2022); Nichol et al. (2022); Ramesh",
  "Published in Transactions on Machine Learning Research (09/2024)": "We therefore suggest a tuning of the timesteps to inject the self-attention maps. The more timesteps to dothat manipulation, the more the edited image will look like the input image. In general, for global editing,where the changes from the input image to the edited image should be larger, a smaller TM can be consideredcompared to which for local editing. A general setting for TM to replace the cross attention map like 20 or40 is enough.",
  "Related Work": "Image diffusion models.In recent years, the interest in deep generative models has gained strong mo-mentum and many different methods have been proposed to create high-quality samples from a magnitudeof different data domains. One kind of architecture that currently stands out are diffusion models (Ho et al.,2020; Dhariwal & Nichol, 2021; Karras et al., 2022). In order to obtain better control over the denoising process and the generated content, several works pro-posed text-conditioned image synthesis using diffusion models and either CLIP guidance or classifier-freeguidance (Nichol et al., 2022; Ramesh et al., 2022; Saharia et al., 2022). Several approaches focus on different optimization strategies and either propose to generalize DDPMs via aclass of non-Markovian diffusion processes to trade off computation for sample quality (Song et al., 2021) orto do the diffusion process in the latent space of a pretrained autoencoder (Rombach et al., 2022) instead ofthe RGB pixel space. Image editing with diffusion models.InstructPix2Pix Brooks et al. (2022) can edit images by followinguser instructions. Paint by Example Yang et al. (2022) allows users to replace an object with a conditionalexample image. ControlNet Zhang & Agrawala (2023) trains a task-specific condition network on top ofa pre-trained diffusion model and supports various edits based on a specific training set. All these worksusually require re-designing of the model, collecting training data and a long training time to do imagetranslation. Blended DIffusion Avrahami et al. (2022b) and Blended Latent Diffusion Avrahami et al. (2023) adopt auser mask as input and design specific loss function to obtain good text-guided image editing results withinthe mask region. UniTune Kawar et al. (2022) and Imagic Kawar et al. (2022) can do realistic text-guidedimage editing by either just finetuning the diffusion models or the models and text embeddings together.However, because of the optimization process, it takes several minutes or more to generate a single image. Recently, there are many interesting works that only utilize a pretrained diffusion model to do text-guidedimage editing without any training or finetuning (Meng et al., 2021; Radford et al., 2021; Couairon et al.,2022; Liew et al., 2022; Hertz et al., 2022; Park et al., 2022; Parmar et al., 2023; Tumanyan et al., 2022).DiffEdit Couairon et al. (2022) automatically generates a mask by computing the differences between thenoisy latent generated from an input image and a conditional text prompt. However, because of its inpaint-ing nature, DiffEdit cannot perform global editing operations like converting a photo to an oil painting.MagicMix Liew et al. (2022) interpolates the noisy input latent and the denoised latent to mix the objectsthat have large semantic differences. Prompt-to-Prompt Hertz et al. (2022) proposes to manipulate thecross-attention maps corresponding to the changes between the input and guided text prompt. Concurrentwork pix2pix-zero Parmar et al. (2023) computes an editing direction to edit an image based on a providedprompt, combined with a cross-attention mechanism to preserve the layout of the input image. Compared to prior works, we propose a generalization that includes multiple previous approaches as specialcase. We also highlight a novel manipulation, that is suitable to better perform a wide range of local andglobal edits than previous methods. Our edits do not require masks for an input image. Our method ispurely based on a pre-trained diffusion model and does not require finetuning.",
  "Design Space for Manipulating the Diffusion Path": "We provide the details of sampling using diffusion model, image editing using diffusion model and inversionfor diffusion model in the Supplementary Materials. Here we introduce the design space of our framework.We abstract our framework to linear interpolation operations and no higher-order operations are exploredin this work. : We use the superscript (A) to represent outputs obtained from condition c(A) and (B) for conditionc(B). The edited outputs are represented with superscript (). is a number between 0 and 1. is usuallya positive real number.",
  "Pred. Noise Masking()t= M (A)t+ (1 M) ()t": "Simply switching to another condition during denoising is a very straightforward way to add new semanticsto the input image when doing text-guided image editing. We perform a simple experiment in toillustrate that the early timesteps in the diffusion process contribute to the layout, while the later timestepsare adding semantics and details to the image.Our goal is to systematically analyze the design spacefor manipulating the diffusion path over time to perform these types of edits. As result of our analysis,we propose MDP as generalized framework for text-guided image editing and show how concurrent andprevious methods fit our framework. A summary can be found in Tab. 1. All manipulations in our framework are time-dependent and have parameters, e.g. interpolation parameters.MDP allows the user to specify different types of manipulations and different manipulation parameters ateach timestep, which we call the manipulation schedule. Assume we have an image x(A)0as input, along with a condition c(A) that is used to generate x(A)0and a newcondition c(B). We first invert the image x(A)0to get an initial noise xT . We identify the following types ofmanipulations:",
  "where c()twill be used as the new condition that embeds the information from both condition c(A) and c(B)": "Cross attention.Recent condition diffusion models inject condition information c by cross attendingbetween c and image features. Prompt-to-Prompt Hertz et al. (2022) and Attend-and-Excite Chefer et al.(2023) find that modifying the cross attention maps can give interesting image editing effects. Here, we donot dig into details of how to manipulate the cross-attention maps. Instead, we simply write the manipulationin functional form M,",
  "()t=xt, c(A), t+ xt, c(A), t xt, , t,(5)": "where is a real number and denotes an empty condition. The is often called guidance scale. Theterm (xt, , t) can be seen as unconditional output of . Similar ideas are also used in Bansal et al.(2023); Liu et al. (2023); Brack et al. (2022) for image editing. This can be generalized to",
  "Settings": "Editing tasks.We describe a small taxonomy for image editing applications that we use to analyze ourframework and compare it to previous work.We divide all common image editing operations into twocategories: local editing and global editing. We provide the details in the Supplementary Materials. Our manipulations and baselines.Given a real input image x(A)0, we first use Null-text Inver-sion Mokady et al. (2022) together with a text-condition c(A) to obtain an initial noise tensor xT . Theinitial condition c(A) can be either an empty text, a manual prompt provided by the user or an captiongenerated by image captioning tools. Another given condition c(B) that is used to guide the editing processwould induce the generation of output image x(B)0starting from the same initial noise tensor xT . As we donot use a mask as an input, we only consider linear operations that do not involve a mask: MDP-xt: intermediate latent interpolation. We linearly interpolate the intermediate latents x(A)tand x(B)tusing equation 1. This manipulation differs from MagicMix Liew et al. (2022) by usingNull-text Inversion to obtain the initial noise tensor xT rather than directly adding noise.",
  "DiffEdit Couairon et al. (2022): intermediate latent interpolation. We also adopt DiffEdit as anothercompetitive baseline for more comparisons": "Comparing MDP-xt, MDP-t and P2P from their formulations, MDP-xt directly injects the intermediatelatent from the original image, which has the largest degree of preserving the original layout. As for MDP-t,which injects the predicted noise and can only partly affect the new intermediate latent; while for P2P, itmanipulates the attention maps which can only partly affect the new predicted noise. Therefore, P2P hasthe weakest degree of preserving the original layout. Due to the space limit, we show the qualitative resultsof DiffEdit in the Supplementary Materials. Manipulation schedule.We investigate how the methods work under a simple schedule using defaultsettings. We start the manipulation at step tmax and end at step tmin. The total number of timesteps ofa manipulation is denoted as TM = tmax tmin. We perform edits only during these TM steps. For theremaining steps, we simply use c(B) to denoise the noisy latent. We vary tmax and tmin and manually selectthe best result for each method. We fix the interpolation factors (guiding scale for MDP-) for the followingconcerns: (1) We observe that setting the manipulation schedule and interpolation factor can both adjustthe degree to which the layout is maintained in the edited image to some extent; (2) We want to find a gooddefault setting of interpolation factors for each manipulation; (3) When setting the interpolation factor = 0for MDP-, this manipulation is equal to MDP-c, which we want to avoid in order to show the characteristicof each manipulation. We therefore empirically fix the interpolation factor of MDP-xt, MDP-c, P2P, MDP-and MDP-t to be 0.7, 1, 1, -0.3, and 1, respectively. In general, tmax is ranging from 0 to 5 while TM is setto be around 20 for each manipulation. We tune the manipulation schedule for each manipulation to obtainthe desired editing result. We analyze and summarize how the methods work under various manipulationschedules in the Supplementary Materials. Implementation.For text-guided editing, we test all the methods using the publicly available latent diffu-sion model Stable Diffusion 1. For class-guided editing, we use the conditional latent diffusion model Rombachet al. (2022) trained on ImageNet Deng et al. (2009). We test our manipulations on one NVIDIA A100 GPU.As no training and finetuning is required, each manipulation can be generally done within 10 seconds. Asthe inversion method we use is built on top of the DDIM sampler Mokady et al. (2022), we also use DDIMsampler during the sampling process. However, our method can adopt other deterministic samplers. Evaluation.We provide both quantitative and qualitative comparison for evaluation. For quantitativeevaluation, We adopt three metrics to evaluate the results. We collected around 90 real-world images andgrouped them into different editing applications for evaluation. Part of the images examples are shown in and Supplementary Materials. We adopt LPIPS Zhang et al. (2018), CLIP score Hessel et al. (2022)and CLIP directional similarity Gal et al. (2021) for evaluation. Explanations of these metrics can be foundin the Supplementary Materials. Despite from these metrics, which do not necessarily align with the humanperspective, we conducted a user study to compare MDP-t against baseline Prompt-to-Prompt using figuresshown in the Supplementary Materials. We conducted the study on MTurk and asked the participants toevaluate based on three different aspects:",
  "Quantitative results": "We show the quantitative results in Tab. 2. Results show that MDP-t has the best consistency regardingthe changes between the input image and edited images and the changes between the input condition andnew condition. Although DiffEdit achieved the best LPIPS score, we argue that DiffEdit is an local editingmethod that only edits a localized region of an image. Therefore, the degree of changes in the edited imagesis usually lower. However, DiffEdit can fall short of global editing applications. We further argue that ahigher number of LPIPS or CLIP score alone does not necessarily mean a better quality, as identical imagescan obtain almost infinite LPIPS, while images that are edited too much can also obtain very high CLIPscore. Nevertheless, CLIP directional similarity, which takes input image, edited image and text prompt intoconsideration at the same time, plays a better role for accessing image-text alignment as well as preservingoriginal content. In this case, MDP-t demonstrated the best quantitative evaluation results. As those quantitative metrics cannot fully align with the human judgement, we also show the user studyresults. In total, we could acquire 60 workers from MTurk to participate in our study. In detail, the editedimages generated by MDP-t were preferred over P2P in 60.6 % of the cases w.r.t layout preservation ofthe input image, in 67.9 % of the cases w.r.t. general quality of the output and in 73.0 % of the cases w.r.timage-text alignment. We show more details of the user study in the Supplementary Materials.",
  "Manipulation starts from early to late": ": We change the background of the input image from grass to library. We show the results ofMDP-xt, MDP-c, and MDP-t from top to bottom. The manipulation range TM is set to be 10, 20, and 20respectively, which are the best settings for each manipulation. Manipulation t can do faithful edits acrossdifferent manipulation ranges while the other two methods fail.",
  "Local editing": "We show examples of local edits for changing object, adding object, removing object, changing attribute,and mixing objects in . The edits by MDP- are somewhat reasonable, but the overall layout is notwell preserved. In general, all the other manipulations can do the edits guided by the text prompt whilepreserving the background of the input image. As the initial diffusion generation steps contribute to thelayout of the generated image, we thus recommend to start the manipulation at the early diffusion stage,usually ts can range from 50 to 45, and TM can be ranging from 15 to 25. For MDP-xt and MDP-t, editingcan start at later steps, i.e. tmax can be chosen to be smaller, as more layout is preserved during editing.Additionally, we observe some interesting findings that Prompt-to-Prompt fails for edits that remove objects.For the application of mixing objects, as there is no universal standard of what the mixed object should looklike, we provide more examples generated by each method in the Supplementary Materials.",
  "Global editing": "We visualize the results in . While in local editing MDP-xt and MDP-c can yield good results, in globalediting they are either not able to do the edits, or the overall layout is changed too much. In contrast, Weobserve that MDP-t can produce very good results most of the time even when the strong baseline methodPrompt-to-Prompt (P2P) fails. Further, we compare the editing results for MDP-xt MDP-c and MDP-tunder different ranges to inject layout from the input image in . We observe that MDP-t can createmeaningful edits when the manipulation starts at early or later stages. We therefore strongly recommendusing MDP-t when doing global editing, as it is more stable and can provide a variety of meaningful results.Here we conjecture the reason why P2P fails to perform some edits or the edited images are lack of fine-grained details. As we analyze, injecting original attention maps has a weaker effect of preserving originallayout.Therefore, P2P needs to inject attention maps for more sampling timesteps to preserve layout.However, as we show in , semantics are formed in the later sampling timesteps, more original semanticsare also injected. That means in the failure cases of P2P, it either cannot preserve the original layout, or toomuch original semantics are leaking to the edited images. As for DiffEdit, as it is a mask-based method, itdoes not perform well in the global editing applications compared to the local editing applications. We alsoprovide more results in the Supplementary Materials.",
  "(c)(d)": ": Typical failure cases of MDP-t. In (a) MDP-t fails to preserve the layout of the input image; In(b) MDP-t fails to faithfully synthesize the yellow color. In (c) and (d), MDP-t fails to do the map-basedimage editing, where the color and texture of the input map has no semantic meaning.",
  "Broader impact": "Here, we discuss several potential impacts of our work in a broader context. Our paper uses publicly availableStable Diffusion as our diffusion model, which inherits the biases from its training dataset. The biases includebut are not limited to race, ethical groups, gender, socioeconomic status, and culture stereotypes. A morediverse and inclusive dataset can reduce the biases.Moreover, bias detection and correction tools areimportant to apply to the output of the diffusion models or be integrated into the generation process. Diffusion models can generate harmful content, such as deepfakes, misleading images, and inappropriatematerial. Implementing an effective filter is crucial for detecting and screening the outputs of these models.For certain types of content, like deepfakes, which may not be easily caught by standard filters, more advancedmethods should be employed to prevent misuse of the diffusion models. Since our framework can edit images based on text prompts, there is a risk that it could be used to modifyand distribute proprietary designs or artworks without permission, potentially violating copyright laws. Toprevent intellectual property rights infringement, it is necessary to develop tools for verifying the originalityof input images.",
  "Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusion-based semanticimage editing with mask guidance. arXiv preprint arXiv:2210.11427, 2022": "Katherine Crowson, Stella Biderman, Daniel Kornis, Dashiell Stander, Eric Hallahan, Louis Castricato, andEdward Raff. Vqgan-clip: Open domain image generation and editing with natural language guidance.In Computer VisionECCV 2022: 17th European Conference, Tel Aviv, Israel, October 2327, 2022,Proceedings, Part XXXVII, pp. 88105. Springer, 2022. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchicalimage database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee,2009. Prafulla Dhariwal and Alexander Quinn Nichol.Diffusion models beat gans on image synthe-sis.In MarcAurelio Ranzato,Alina Beygelzimer,Yann N. Dauphin,Percy Liang,and Jen-niferWortmanVaughan(eds.),Advances in Neural Information Processing Systems 34:An-nual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14,2021, virtual, pp. 87808794, 2021.URL",
  "Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-basedgenerative models. In Proc. NeurIPS, 2022": "Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and MichalIrani. Imagic: Text-based real image editing with diffusion models. arXiv preprint arXiv:2210.09276,2022. Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jung Kwon Lee, and Jiwon Kim. Learning to discover cross-domain relations with generative adversarial networks. In International conference on machine learning,pp. 18571865. PMLR, 2017.",
  "Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editingreal images using guided diffusion models. arXiv preprint arXiv:2211.09794, 2022": "Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew,Ilya Sutskever, and Mark Chen. GLIDE: Towards photorealistic image generation and editing with text-guided diffusion models. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, GangNiu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning,volume 162 of Proceedings of Machine Learning Research, pp. 1678416804. PMLR, 1723 Jul 2022. URL Dong Huk Park, Grace Luo, Clayton Toste, Samaneh Azadi, Xihui Liu, Maka Karalashvili, Anna Rohrbach,and Trevor Darrell. Shape-guided diffusion with inside-outside attention. arXiv preprint arXiv:2212.00210,2022.",
  "Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shotimage-to-image translation. arXiv preprint arXiv:2302.03027, 2023": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, GirishSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models fromnatural language supervision. In International conference on machine learning, pp. 87488763. PMLR,2021. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, andIlya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pp.88218831. PMLR, 2021. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditionalimage generation with CLIP latents. CoRR, abs/2204.06125, 2022. doi: 10.48550/arXiv.2204.06125. URL Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolutionimage synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), pp. 1068410695, June 2022. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar SeyedGhasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, JonathanHo, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deeplanguage understanding. CoRR, abs/2205.11487, 2022. doi: 10.48550/arXiv.2205.11487. URL Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In 9th InternationalConference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenRe-view.net, 2021. URL",
  "AMore results": "We show more comparisons between Prompt-to-Prompt and MDP-t for each kind of edits we describe inthe main paper. We show the edits for local editing in , 22, 23, 24, and 25, and for global editingin , 30, 31, and 32. We also show more intermediate results for mixing objects in . We showthat in many cases, MDP-t can perform as good as or better than Prompt-to-Prompt. Additionally, wealso show editing results of MDP-t using Stable Diffusion 2.1 2 as the base diffusion model in .Noted that all the input images we use here are synthesized from Stable Diffusion 2.1 instead of real images.We also provided comparisons between DiffEdit and MDP-t in . As an editing method that onlyedits local regions in the images, DiffEdit has difficulty to perform global editing applications.",
  "BTaxonomy of Image Editing Applications": "In this subsection we describe a small taxonomy for text-guided image editing applications that we use toanalyze our framework and compare it to previous work. To be noted that this taxonomy only includessome major image editing applications but is not an exhaustive one. Given either a real or synthetic imageas input, we edit the image following a condition such as a text prompt or a class label. We divide allcommon image editing operations into two categories: local editing and global editing. We further identifysub-categories for both local and global editing.",
  "CLIP score: It can be used to evaluate the similarity between the image-text pair. In this work weuse CLIP score to evaluate if the edited image is aligned with the new condition c(B) or not": "CLIP directional similarity: This metric is to evaluate if the changes in images align with the changesin text. In this work we adopt this metric to see if the changes in the edited image compared to theinput image align with the changes in the new condition c(B) compared to the input condition c(A).",
  "minEx0D,N(0,I),tU(1,T ) (xt, c, t)2 ,(10)": "where D is an image dataset (could be raw image pixels or latents obtained from an image autoencoder), xtis a noised version of the image x0, c is a conditional embedding (e.g., text, class label, image) and (, , )is a neural network parameterized by . After training, we can sample a new image given condition c withthe commonly used DDIM sampler,",
  "t1 (xt, c, t),(11)": "where f(xt, c, t) = xt1t(xt,c,t)t, t = (xt, c, t), and t is a noise schedule factor as in DDIM. Thesampling is to iteratively apply the above equation by giving an initial noise xT N(0, I). For brevity, weonly consider DDIM as our sampler. Image editing using pre-trained diffusion models.Our generalized editing framework MDP edits animage in its latent space also called noise space. Given an image x0 and the corresponding initial noise xT ,we modify xT or the generating process and our aim is to obtain a different x0 which suits our needs. Wesummarize the symbols used in our later discussions here. One step of diffusion sampling can be represented",
  "FAvailability of the initial condition": "In practice, when editing real images, users need to provide c(A) as the initial caption for the input image.This caption is used during the inversion of the real image and guides the synthesis of the edited image. A pre-trained image captioning tool, like BLIP2, can be used to generate a caption for the input image. However,the generated caption may not always be optimal for producing the best editing results. For instance, if wewant to edit an image of a forest in Spring to look like Winter, including the keyword Spring in c(A) canoffer more precise guidance for the diffusion models to edit from Spring to Winter. While it is possible toomit the keyword or even use an empty text prompt, we found that specifying the keyword explicitly oftenresults in more stable inversion and editing outcomes. However, an image captioner might not automaticallygenerate a caption containing this keyword. Since both the input condition c(A) and the new conditionc(B) are crucial for achieving satisfactory edits, future work could focus on developing efficient methods forgenerating captions tailored to text-guided image editing tasks.",
  "GUser study": "We show the interface of the user study presented to the participants in . We show the input imageand the editing prompt in the middle column, along with two edited images in the left and right column,respectively. One of the edited image is from MDP-t while another is from P2P. We randomly place eitherMDP-t on the left or right side. For each example, we ask the user three questions which correspond to theevaluation aspects we discuss in the main paper:",
  "HMore Analysis in the Design Space": "We observe that the manipulation schedule has a relatively larger influence on MDP-xt, MDP-c, and MDP-t. For MDP- the linear facfor (guidance scale) has a much more important effect towards the controllabilityof the edited image. For Prompt-to-Prompt (a manipulation for attention maps) the range of time stepsto inject the self-attention maps strongly determines what the edited image will look like. Therefore, forMDP-xt, MDP-c, and MDP-t, we explore four different manipulation schedules: constant, linear, cosine,and exponential (). Given an integer timestep t , we give the equations for the linear factor forconstant schedule (constt), linear schedule (lineart), cosine schedule (cost) and exponential schedule linearschedule (expt):",
  "(12)": "The results are shown in , 13, 14, 15, 16, 17. For MDP-, we vary the guidance scale from -0.7to 0.7 and show the results in . For Prompt-to-Prompt, we use the constant schedule to inject theattention maps and show the results in and . We only visualize one example for global editingas we show in the paper: given an airplane image input, we change its background from sky to airportrunway. We only show this as an example because this example is challenging so we can identify the abilityof different methods. For Prompt-to-Prompt, we show an additional example from local editing for a betterunderstanding of the method. We discuss the results in the following subsections. For the parameters in the design space for each manipulation, we show a general recommendation in Tab.3and Tab.4, which is summarized based on the examples we have tried. Note that this is only considereda recommendation for general cases. For specific applications (and input images), improved results can beobtained by fine-tuning the parameters.",
  "(b)": ": Linear schedule is shown in (a), while linear, cosine, and exponential schedule are shown in purple,orange, and green, respectively, in (b). For linear schedule, we fix the linear factor as 1.0 while varying tmaxand tmin by varying TM. For the other three schedules, we fix the tmax = 50 then vary scale factors and tmin.",
  ": The recommended parameters in the design space for global edits": "MDP-xt, MDP-c and MDP-tWhile in most of the local editing applications and some of the globalediting applications, MDP-xt can faithfully do the edits, for the example we show in the figures, all theexamples generated by MDP-xt under different schedules fail. The edited image preserves the overall layout ofthe input image, however, the new semantics fail to be injected. We conjecture that keeping the intermediatelatent from the path when generating the input image will impose a very strong condition to preserve theinformation from the input image. For the result in the main paper, we fix the linear factors for MDP-xtto be 0.7 under all the edits. However, for a clear comparison with other two similar methods MDP-c andMDP-t, we fix the linear factors as 1.0, which results in that the new condition has no influence whentmin t tmax. That is also the reason why we empirically set the linear factor of MDP-xt to be 0.7 for allthe results in the main paper, rather than 1.0 as the other two manipulations. On the other hand, MDP-c and MDP-t can inject the new semantics under the same schedules when linearfactors are all set to be 1.0 when tmax. For MDP-c, typically when tmax is smaller than 50 or TM is smallerthan 15, the new semantics can be injected into the edited image. However, the layout information from theinput image is also lost. For MDP-t, in terms of constant schedule, when TM is 25 and tmax is smaller than50, the edited image can both preserve the layout and incorporate the new semantics. Empirically, we observe that MDP-xt is the strongest one to inject layout information, then is MDP-t, whileMDP-c is the weakest one when using the same manipulation schedule. Theoretically, this observation isaligned with the diffusion generation formula:",
  "t1 (xt, c, t),(13)": "where f(xt, c, t) = xt1t(xt,c,t)t, t = (xt, c, t), and t is a noise schedule factor as in DDIM. Thesampling is to iteratively apply the above equation by giving an initial noise xT N(0, I). When settingthe linear factors as 1.0, MDP-xt directly replaces the intermediate latent from the path when generatingthe input image, while MDP-t replaces the predicted noise and keeps the previous intermediate latent whenapplying equation 13, and MDP-c only replaces the conditional embedding when predicting the noise. We also observe that when tmax = 50, i.e., the manipulation happens right at the start of the whole denoisingprocess, and the layout information can be injected more than when tmax is smaller. In addition, considering",
  "For the same manipulation schedule, MDP-xt injects layout information the strongest from the inputimage, then comes MDP-t, while MDP-c is the weakest one": "For MDP-xt, we recommend to set the linear factor smaller than 1.0 when using the constantschedule, or consider using schedule with decreasing linear factors during the generation process,such as linear, cosine, or exponential schedules. For MDP-c, we recommend using it only in localediting applications. For our proposed and highlighted MDP-t, we recommend to set tmax to bearound 44 to 50 while TM to be around 20 to 25. As the early stages during the diffusion generation process has a larger influence on the layout of thegenerated image, we recommend that for every kind of manipulation, we set the tmax to be largerthan 44.",
  "xt, c(A), t xt, c(B), t,(14)": "where R is called guidance scale. Intuitively, with equation 14, we want the output to have morecharacteristics from c(A) when generating using the new condition c(B) to preserve the layout. This is a bitdifferent from the original classifier free guidance Ho & Salimans (2022), where condition c(B) is an emptycondition and is a positive real number. In our specific image editing task, we find that when using apositive (e.g., the first row in ), the edited image will be too close to condition c(A) while losingboth the layout from the input image and the new semantics from condition c(B). Only when the guidancescale is a negative number, can the new semantics be added into the edited image. In this situation, theguidance becomes a linear interpolation operation:",
  "()t= xt, c(A), t+ (1 ) xt, c(B), t,(15)": "where = 1 is a linear factor from 0 to 1, which means both condition c(A) and condition c(B) contributepositively to the generation process. Overall, has a larger influence over the edited image than varying themanipulation timesteps. We empirically find that [0.6, 0.8] can obtain better results. Prompt-to-PromptWe refer to the official implementation3, which also utilizes Null-text InversionMokady et al. (2022) to invert the real image and uses DDIM as the sampler. We observe that insteadof replacing cross-attention maps, injecting self-attention maps has a bigger influence on the edited images.As in the effect of injecting self-attention maps is not obvious enough, we additionally show one localediting example in , where when fixing the timesteps to inject self-attention maps, the edited imagelook almost the same under different timesteps to inject cross attention maps; while under different timestepsto inject the self-attention maps, the generated images look more different."
}