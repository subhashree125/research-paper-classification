{
  "Abstract": "We propose NeuFace, a 3D face mesh pseudo annotation method on videos via neuralre-parameterized optimization. Despite the huge progress in 3D face reconstruction methods,generating reliable 3D face labels for in-the-wild dynamic videos remains challenging. UsingNeuFace optimization, we annotate the per-view/-frame accurate and consistent face mesheson large-scale face videos, called the NeuFace-dataset.We investigate how neural re-parameterization helps to reconstruct 3D facial geometries, well complying with input facialgestures and motions. By exploiting the naturalness and diversity of 3D faces in our dataset,we demonstrate the usefulness of our dataset for 3D face-related tasks: improving thereconstruction accuracy of an existing 3D face reconstruction model and learning 3D facialmotion prior. Project page:",
  "Introduction": "A comprehensive understanding of dynamic 3D human faces has been a long-standing problem in computervision and graphics. Reconstructing and generating dynamic 3D human faces are key components for diversetasks such as face recognition (Weyrauch et al., 2004; Blanz & Vetter, 2003), face forgery detection (Cozzolinoet al., 2021; Rssler et al., 2018; 2019), video face editing (B R et al., 2021; Kim et al., 2018; Tewari et al.,2020), facial motion or expression transfer (Thies et al., 2015; 2016a; 2018), XR applications (Elgharib et al.,2020; Wang et al., 2021; Richard et al., 2021; Saito et al., 2024; Sung-Bin et al., 2024a;b; EunGi et al., 2024),and human avatar generation (Raj et al., 2020; Ma et al., 2021; Youwang et al., 2022; 2024). Recent studies (Wood et al., 2021; 2022; Bae et al., 2023; Yeh et al., 2022) have shown that reliable datasetsof facial geometry, even synthetic or pseudo ones, can help achieve a comprehensive understanding of static",
  "NeuFaceCelebV-HQ3515,00065Wild8": "3D faces. However, there is currently a lack of reliable and large-scale datasets containing dynamic andnatural 3D facial motion annotations. The lack of such datasets becomes a bottleneck for studying inherentfacial motion dynamics or 3D face reconstruction tasks by restricting them to rely on weak supervision, e.g.,2D landmarks or segmentation maps. Accurately acquired 3D face video data may mitigate such issues buttypically requires intensive and time-consuming efforts with carefully calibrated multi-view cameras andcontrolled lighting conditions (Yoon et al., 2021; Joo et al., 2015; 2018; Cudeiro et al., 2019; Ranjan et al.,2018). Few seminal works (Fanelli et al., 2010; Ranjan et al., 2018; Cudeiro et al., 2019; Zielonka et al., 2022)take such effort to build 3D face video datasets. Despite significant efforts, the existing datasets obtainedfrom such restricted settings are limited in scale, scenarios, diversity of actor identity and expression, andnaturalness of facial motion (see ). In contrast to 3D, there are an incomparably large amount of 2D face video datasets available online (Wanget al., 2020; Nagrani et al., 2017; Chung et al., 2018; Zhu et al., 2022; Parkhi et al., 2015; Cao et al., 2018;Karras et al., 2019; Wang et al., 2021; 2019; Liu et al., 2015), which are captured in diverse in-the-wildenvironments but without 3D annotations. As successfully demonstrated in some 3D tasks (Fang et al., 2021;Bouazizi et al., 2021; Huang et al., 2022; Mller et al., 2021; Hassan et al., 2019; Bayer et al., 2016; Ng et al.,2022) as well as other analysis tasks (Miech et al., 2019; Nagrani et al., 2022; Lee et al., 2021), leveragingoff-the-shelf reconstruction models is a common practice to obtain pseudo ground-truth of such in-the-wildvideos that were already captured. They showed that high-quality and large-scale pseudo ground-truth issufficient to achieve the state-of-the-art at the time of their works. Similarly, a nave approach is to constructa large-scale 3D face video dataset by curating existing 2D video datasets and obtain 3D face annotationswith off-the-shelf face reconstruction models (Feng et al., 2021; Danecek et al., 2022). However, existing 3Dface reconstruction models have limitations for reconstructing temporally smooth or multi-view consistent 3Dface meshes from videos. This is because state-of-the-art face reconstruction models are typically trained onsingle-view static images only with 2D supervision; thus fail to extrapolate to faces having rare poses andyield jittered motion due to the per-frame independent inference. To address these difficulties, we propose NeuFace optimization, which reconstructs accurate and spatio-temporally consistent parametric 3D face meshes on videos. By re-parameterizing 3D face meshes with neuralnetwork parameters, NeuFace infuses spatio-temporal cues of dynamic face videos on 3D face reconstruction.NeuFace optimizes spatio-temporal consistency losses and the 2D landmark loss to acquire reliable face meshpseudo-labels for videos. Using this method, we create the NeuFace-dataset, the first large-scale, accurate and spatio-temporallyconsistent 3D face meshes for videos. Our dataset contains 3D face mesh pseudo-labels for large-scale,multi-view or in-the-wild 2D face videos, MEAD (Wang et al., 2020), VoxCeleb2 (Chung et al., 2018), andCelebV-HQ (Zhu et al., 2022), achieving about 1,000 times larger number of sequences than existing facialmotion capture datasets (see ). Our dataset inherits the benefits of the rich visual attributes inlarge-scale face videos, e.g., various races, appearances, backgrounds, natural facial motions, and expressions.We assess the fidelity of our dataset by investigating the cross-view vertex distance and the 3D motionstability index. We demonstrate that our dataset contains more spatio-temporally consistent and accurate3D meshes than the competing datasets built with strong baseline methods. To demonstrate the potential of",
  "Related work": "3D face datasets. To achieve a comprehensive understanding of dynamic 3D faces, large-scale in-the-wild3D face video datasets are essential. There exist large-scale 2D face datasets that provide expressive faceimages or videos (Wang et al., 2020; Nagrani et al., 2017; Chung et al., 2018; Zhu et al., 2022; Parkhiet al., 2015; Cao et al., 2018; Karras et al., 2019; Liu et al., 2015) with diverse attributes covering a widevariety of appearances, races, environments, scenarios, and emotions. However, most 2D face datasets do nothave corresponding 3D annotations, due to the difficulty of 3D face acquisition, especially for in-the-wildenvironments. Although some recent datasets (Yoon et al., 2021; Ranjan et al., 2018; Cudeiro et al., 2019;Zielonka et al., 2022; Wood et al., 2021) provide 3D face annotations with paired images or videos,1 they areacquired in the restricted and carefully controlled indoor capturing environment, e.g., laboratory, yieldingsmall scale, unnatural facial expressions and a limited variety of facial identities or features. Achievingin-the-wild naturalness and acquiring true 3D labels would be mutually exclusive in the real-world. Due tothe challenge of constructing a real-world 3D face dataset, FaceSynthetics (Wood et al., 2021) synthesizeslarge-scale synthetic face images and annotations derived from synthetic 3D faces, but limited in that theyonly publish images and 2D annotations without 3D annotations, which restrict 3D face video applications.In this work, we present the NeuFace-dataset, the first large-scale 3D face mesh pseudo-labels paired withthe existing in-the-wild 2D face video datasets, resolving the lack of the 3D face video datasets. 3D face reconstruction. To obtain reliable face meshes for large-scale face videos, we need accurate 3Dface reconstruction methods for videos. Reconstructing accurate 3D faces from limited visual cues, e.g., amonocular image, is an ill-posed problem. Model-based approaches have been the mainstream to mitigate theill-posedness and have advanced with the 3D Morphable Models (3DMMs) (Blanz & Vetter, 1999; Paysanet al., 2009; Li et al., 2017) and 3DMM-based reconstruction methods (Zollhfer et al., 2018; Egger et al.,2020; Feng et al., 2021; Danecek et al., 2022; Zielonka et al., 2022). 3D face reconstruction methods can be categorized into learning-based and optimization-based approaches.The learning-based approaches, e.g., (Feng et al., 2021; Danecek et al., 2022; Zielonka et al., 2022; Sanyalet al., 2019a; an Trn et al., 2016), use neural networks trained on large-scale face image datasets to regressthe 3DMM parameters from a single image. The optimization-based approaches (Blanz & Vetter, 2003; Huberet al., 2015; Chen et al., 2013; Wood et al., 2022; Thies et al., 2015; Gecer et al., 2019) optimize the 2Dlandmark or photometric losses with extra regularization terms directly over the 3DMM parameters. Given aspecific image, these methods overfit to 2D landmarks observations, thus showing better 2D landmark fitthan the learning-based methods. These approaches are suitable for our purpose in that we need accuratereconstruction that best fits each video. However, the regularization terms are typically hand-designed withprior assumptions that disregard the input image. These regularization terms often introduce mean shapebiases (Feng et al., 2021; Pavlakos et al., 2019; Bogo et al., 2016; Joo et al., 2020), due to their independenceto input data, which we call the data-independent prior. Also, balancing the losses and regularization isinherently cumbersome and may introduce initialization sensitivity and local minima issues (Joo et al., 2020;Pavlakos et al., 2019; Bogo et al., 2016; Choutas et al., 2020).",
  "Published in Transactions on Machine Learning Research (08/2024)": "(a)(b)Figure S2:FLAME fitting vs.NeuFace optimization. (a) NeuFace optimization obtains muchexpressive and pixel-level aligned meshes than the baseline FLAME fitting.(b) We observe NeuFaceoptimization induces richer and data-dependent gradients compared to the sparse gradients of baseline. mean shape-biased 3D faces. We explain such results in terms of the data-independency of the direct FLAMEoptimization. The baseline method requires several regularization terms based on the prior, pre-built fromexternal 3D datasets, which is data-independent (Eq. (5)). Such data-independent regularization encouragesthe optimized FLAME parameters to stay close to the mean of parameter distributions, regardless of thefacial characteristics on input images. Balancing such regularization terms with other losses is cumbersomeand prone to obtain mean shape faces. In contrast, recall that NeuFace re-parameterizes the FLAME parameters as the pre-trained neural network,such as DECA (Feng et al., 2021). Such re-parameterization allows NeuFace to update face meshes in an input-image-conditioned manner, called data-dependent mesh update (Sec. 3.3 in the main paper). Figure S2(b)visualizes the data-dependent gradient of our optimization. While the baseline shows similar gradient patternsthroughout three input images, NeuFace optimization produces diverse gradient maps according to inputimages. Such rich and data-dependent gradients consider high-dimensional visual features induced from theinput RGB values in the optimization process, yielding accurate and expressive 3D faces.",
  "Neural re-parameterization of 3D face meshes": "We use FLAME (Li et al., 2017), a renowned 3DMM, as a 3D face representation. 3D face mesh vertices M andfacial landmarks J for F frame videos can be acquired with the differentiable skinning: M, J=FLAME(r, , , ),where r R3, R12, R100 and R50 denote the head orientation, face poses, face shape andexpression coefficients, respectively. For simplicity, FLAME parameters can be represented as, =[r, , , ]. We further re-parameterize the FLAME parameters and weak perspective camera parametersp RF 3 for video frames {If}Ff=1, into a neural network, , with parameters w, i.e., [, p] = w({If}Ff=1).We use the pre-trained DECA (Feng et al., 2021) or EMOCA (Danecek et al., 2022) encoder for w.",
  "NF ,NVf=1,v=1(Jtf,v(w), ptf,v) jf,v1,(2)": "where (, ) denotes the weak perspective projection, and J(w) is the 3D landmark from w(). Eq. (2)computes the pixel distance between the pre-detected 2D facial landmarks j and the regressed and projected3D facial landmarks (J(w), p). j stays the same for the whole optimization. We use FAN (Bulat &Tzimiropoulos, 2017) to obtain j with human verification to reject the failure cases. Temporal consistency loss.Our temporal consistency loss reduces facial motion jitter caused by per-frame independent mesh regression on videos. Instead of a complicated Markov chain style loss, for eachiteration t, we first estimate latent target meshes that represent temporally smooth heads in Expectation step(E-step). Then, we simply maximize the likelihood of regressed meshes to its corresponding latent target inMaximization step (M-step). In E-step, we feed {If,v}NF ,NVf=1,v=1 into the network wt and obtain FLAME andcamera parameters, [t, pt]. For multiple frames in view v, we extract the head orientations rt:,v, from t andconvert it to the unit quaternion qt:,v. To generate the latent target, i.e., temporally smooth head orientationsqt:,v, we take the temporal moving average over qt:,v. In M-step, we compute the temporal consistency loss as:",
  "where q is the unit-quaternion representation of r. We empirically found that such simple consistency loss issufficient enough to obtain temporal smoothness while allowing more flexible expressions": ": Multi-view bootstrapping. Given initialmesh predictions for each view in frame f, we align andmerge the meshes depending on the confidence. The boot-strapped mesh serves as a target for computing Lmultiview. Multi-view consistency loss.Although theaforementioned L2D roughly guides the multi-viewconsistency of landmarks, it cannot guarantee theconsistency for off-landmark or invisible facial re-gions across views. Therefore, for multi-view cap-tured face videos (Wang et al., 2020), we leveragea simple principle to obtain consistent meshes overdifferent views: face geometry should be consis-tent across views at the same time. The goal is tobootstrap the per-view estimated noisy meshes byreferencing the visible, or highly confident facialregions across different views. Analogous to thetemporal consistency loss, in M-step, we computethe multi-view consistency loss as follows:",
  "NF ,NVf=1,v=1Mtf,v Mtf1,": "(4)where Mtf denotes the latent target mesh vertices estimated in E-step of each iteration. In E-step, givenvertices Mtf,: of multiple views in frame f, we interpret the vertex visibility as the per-vertex confidence.We assign the confidence score per each vertex by measuring the angle between the vertex normal and thecamera ray. We set the vertices as invisible if the angle is larger than the threshold a, and the vertex has adeeper depth than z, i.e., z < z. We empirically choose a = 72, z = 0.08. To obtain the latent targetmesh Mtf, we align per-view estimated meshes to the canonical view, and bootstrap the meshes by takingthe weighted average of Mtf,: depending on the confidence (see ). With this, Eq. (4) constrains thevertices of each view to be consistent with Mtf.",
  "Why is NeuFace optimization effective?": "Note that one can simply update FLAME parameters directly with the same loss in Eq. (1). Then, why dowe need neural re-parameterization of 3D face meshes? We claim such neural re-parameterization allowsdata-dependent mesh update, which the FLAME fitting cannot achieve. To support our claim, we analyze thebenefit of our optimization by comparing it with the solid baseline.",
  "2w(x) y22. Under someassumptions, gradient descent finds a global optimum in polynomial time with high probability": "Proposition 1 can be derived by simply re-compositing the results by Allen-Zhu et al. (2019). Its proof sketchcan be found in the appendix. This hints that our over-parameterization helps NeuFace optimization achieverobustness to local minima and avoid mean shape biases. To see how data-dependent gradient of NeuFace affects the mesh optimization, we visualize the absolutemagnitude of the back-propagated gradients of each method in . The baseline optimization produces asparse gradient map along the face landmarks, which disregards the pixel-level facial details, e.g., wrinkles orfacial boundaries. In contrast, NeuFace additionally induces the dense gradients over face surfaces, not justsparse landmarks, which are helpful for representing image-aligned and detailed facial expressions on meshes.Thanks to the rich gradient map, our method yields more expressive and accurately image-aligned meshesthan the baseline.",
  "How reliable is NeuFace optimization?": "Many recent face-related applications (Ng et al., 2022; Khakhulin et al., 2022; Feng et al., 2022) utilize apre-trained, off-the-shelf 3D face reconstruction model or the FLAME fitting (Eq. (5)) as a pseudo ground-truth annotator. Compared to such conventional face mesh annotation methods, we discuss how reliableNeuface optimization is. Specifically, we measure the vertex-level accuracy of the reconstructed face meshesby NeuFace optimization on the motion capture videos, VOCASET (Cudeiro et al., 2019).",
  ":Given the ground-truth meshes, ouroptimization reconstructs more vertex-level accu-rate meshes than the competing methods": "VOCASETisasmall-scalefacialmotioncapturedataset that provides registered ground-truth mesh se-quences. Given the ground-truth mesh sequences fromthe VOCASET, we evaluate the Mean-Per-Vertex-Error(MPVE) (Cho et al., 2022; Lin et al., 2021b;a) of facemeshes obtained by pre-trained DECA, FLAME fittingand our method. In , NeuFace optimization achievesmore vertex-level accurate meshes than other methods,i.e., lower MPVE. Note that FLAME fitting still achievescompetitive MPVE with ours, which shows that it is avalid, strong baseline. Such favorable mesh accuracy ofNeuFace optimization motivates us to leverage it as areliable face mesh annotator for large-scale face videos,and build the NeuFace-dataset.",
  "The NeuFace-dataset": "The NeuFace-dataset provides accurate and spatio-temporally consistent face meshes of existing large-scale2D face video datasets; MEAD (Wang et al., 2020), VoxCeleb2 (Chung et al., 2018), and CelebV-HQ (Zhuet al., 2022). Our datasets are denoted with NeuFace{} and summarized in . The NeuFace-dataset is,namely, the largest 3D face mesh pseudo-labeled dataset in terms of the scale, naturalness, and diversity offacial attributes, emotions, and backgrounds. Please refer to the appendix for the visualization of the dataset,acquisition method, and filtering details.",
  "Fidelity evaluation of the NeuFace-dataset": "We assess the fidelity of our dataset in terms of spatio-temporal consistency and landmark accuracy. Wemake competing datasets and compare the quality of the generated mesh annotations. First, we composethe strong baseline, Base-dataset, by fitting FLAME with Eq. (5). We also utilize pre-trained DECA andEMOCA as mesh annotators and built DECA-dataset and EMOCA-dataset, respectively. Finally, we buildtwo versions of our dataset, i.e., NeuFace-D, and NeuFace-E, where each dataset is generated via Eq. (1)with DECA and EMOCA for the neural re-parameterization w, respectively. Temporal consistency. We extend the Motion Stability Index (MSI) (Ling et al., 2022) to MSI3D andevaluate the temporal consistency of each dataset. MSI3D computes a reciprocal of the motion accelerationvariance of either 3D landmarks or vertices and quantifies facial motion stability for a given NF frame video,{If}NFf=1, as MSI3D({If}NFf=1)= 1",
  "i1": "(ai), where ai denotes the 3D motion acceleration of i-th 3D landmarksor vertices, () the temporal variance, and K the number of landmarks or vertices. If the mesh sequencehas small temporal jittering, i.e., low motion variance, it has a high MSI3D value. We compute MSI3D forlandmarks and vertices, i.e., MSIL3D and MSIV3D, respectively. shows the MSIL3D and MSIV3D averagedover the validation sets. For the VoxCeleb2 and CelebV-HQ splits, the NeuFace-D/E-dataset outperform theother datasets in both MSI3Ds. Remarkably, we have improvements on MSI3D more than 20 times in MEAD.We postulate that the multi-view consistency loss also strengthens the temporal consistency for MEAD. Inother words, our losses would be mutually helpful when jointly optimized. We discuss it through loss ablationstudies in the later section (Sec. 4.2). Multi-view consistency. We visualize the predicted meshes over different views in , where per-viewindependent estimations are presented, not a single merged one. We verify that the NeuFace-D-datasetcontains multi-view consistent meshes compared to the DECA-dataset, especially near the mouth region. Seeappendix for the comparison of the EMOCA-dataset and NeuFace-E-dataset. As a quantitative measure,we compute the cross-view vertex distance (CVD), i.e., the vertex distance between two different views, iand j, in the same frame f: Mf,iMf,j1. We compare the averaged CVD of all views in . CVD is",
  "DECA (Feng et al., 2021)0.2090.0110.0164.650.0280.0444.780.0120.0185.34": ": Effect of the Lmultiview. Byoptimizing our full objectives (Eq. (1)), thegenerated meshes are fitted to each of its 2Dlandmarks and are also multi-view consis-tent. Views are aligned for the visualization. : Comparisons of cross-view vertex dis-tance. We quantitatively show the multi-view consis-tency of our method by averaging the cross-view vertexdistance on the validation set. L- and R- denote Leftand Right, respectively, and 30 and 60 denote the viewangles which the video is captured from.",
  "only evaluated on the MEAD dataset, which is in a multi-camera setup. While the DECA-/EMOCA-datasetresults in high CVD, the NeuFace-dataset shows significantly lower CVD on overall views": "2D landmark accuracy. A trivial solution to obtain low CVD and high MSI3D is to regress the same meanface meshes across views and frames regardless of the input image. To verify such occurrence, we measure thelandmark accuracy of the regressed 2D facial landmarks using the normalized mean error (NME) (Sagonaset al., 2016). The NeuFace-D/E-dataset outperform the other datasets in NME, i.e., contain spatio-temporallyconsistent and accurately landmark-aligned meshes.",
  "Ablation on loss functions": "We analyze the effect of our proposed spatio-temporal consistency losses in the NeuFace optimization,Lmultiview, and Ltemporal. We evaluate the quality of the meshes obtained by optimizing each of the lossconfigurations. All the experiments are conducted on the same validation set as the . Only L2D. We start by optimizing the loss function with only 2D facial landmark re-projection error, L2D.As we optimize L2D, the obtained meshes achieve lower NME than DECA over the whole validation set (see). However, we observe that this may break both multi-view and temporal consistencies, degradingthe CVD and MSI compared to that of DECA. The qualitative results in also show that optimizingonly L2D reconstructs more expressive but inconsistent meshes over different views. Therefore, we proposeloss functions that can induce multi-view and temporal consistencies to the meshes during the optimization. L2D+Lmultiview. By optimizing Lmultiview along with L2D, we obtain significantly lower CVD than DECAand the meshes optimized with only L2D. We have not optimized any regularization term which inducestemporal consistency; thus, MSIs remain low. As discussed in Sec. 4.1, a trivial solution for achieving low",
  "Learning 3D human facial motion prior": "A facial motion prior is a versatile tool to understand how human faces move over time. It can generaterealistic motions or regularize temporal 3D reconstruction (Rempe et al., 2021). Unfortunately, the lack oflarge-scale 3D face video datasets makes learning facial motion prior infeasible. We tackle this by exploitingthe scale, diversity, and naturalness of the 3D facial motions in our dataset. Learning facial motion prior.We learn a 3D facial motion prior using HuMoR (Rempe et al., 2021)with simple modifications. HuMoR is a conditional VAE (Sohn et al., 2015) that learns the transitiondistribution of human body motion. We represent the state of a facial motion sequence as the combination",
  "NeuFaceVoxCeleb2LargeIn-the-wild31.3252.69": "of FLAME parameters and landmarks in the NeuFace-dataset and train the dedicated face motion prior,called HuMoR-Face. We train three motion prior models (HuMoR-Face) with different training datasets,i.e., VOCASET (Cudeiro et al., 2019), NeuFaceMEAD, and NeuFaceVoxCeleb2. Please refer to appendix andHuMoR (Rempe et al., 2021) for the details. Long-term face motion generation.We evaluate the validity and generative power of the learnedmotion prior by generating long-term 3D face motion sequences (10.0s). Long-term motions are generated byauto-regressive sampling from the learned prior, given only a starting frame as the condition (see ).VOCASET provides small-scale, in-the-lab captured meshes, thus limited in motion naturalness and facialdiversity. Accordingly, the HuMoR-Face trained with VOCASET fails to learn a valid human facial motionprior and generates unnatural motion. Using only the subset, NeuFaceMEAD, the long-term stability of headmotion has significantly enhanced. Please refer to the supplementary video for motion comparisons. We quantitatively evaluate HuMoR-Face models using two metrics: motion Frchet distance (FD) (Ng et al.,2022) and average pairwise distance (APD) (Aliakbarian et al., 2020; Rempe et al., 2021). FD measures thenaturalness like the FID score (Heusel et al., 2017) and APD measures the diversity of generated motions.For APD, we generate 50 long-term motions from the same initial state and compute the mean landmarkdistance between all pairs of samples. HuMoR-Face models trained with large-scale and diverse motions, i.e.,the NeuFace-dataset, show superior performance in naturalness and diversity (see ). Specifically, theHuMoR-Face trained with NeuFaceVoxCeleb2 shows substantial enhancement on APD. APD is not reported forthe HuMoR-Face trained with VOCASET, since the model fails to generate realistic motion.We attributesuch high-quality motion prior to the benefit of the NeuFace-dataset: large-scale facial motion annotations.Further, exploiting diverse in-the-wild, dynamic, and natural motion annotation from NeuFaceVoxCeleb2 helpsHuMoR-Face learn real-world motion prior and surprisingly generate much diverse and dynamic motions.",
  "Improving the 3D reconstruction accuracy": "Due to the absence of large-scale 3D face video datasets, existing face mesh regressor models utilize limitedvisual cues, such as 2D landmarks or segmentations. Thus, we utilize the NeuFace-dataset to add direct 3Dsupervision to enhance the performance of such a model. 3D supervision with the NeuFace-dataset. We implement the auxiliary 3D supervision as conventional3D vertex and landmark losses (Kolotouros et al., 2019; Cho et al., 2022; Lin et al., 2021b;a).Givenregressed and our annotated mesh vertices, M, M RNM3, and regressed and our annotated 3D landmarks,J, J RNJ3, the auxiliary 3D losses are computed as: LM3D=1",
  "NJ JJ2, where NM,NJ is the number of mesh vertices and landmarks, respectively": "Enhancement on 3D reconstruction accuracy.By fine-tuning DECA (Feng et al., 2021) using theimages of MEAD (Wang et al., 2020), VoxCeleb2 (Chung et al., 2018) and CelebV-HQ (Zhu et al., 2022), withand without our 3D supervision, we obtain DECANeuFace,3D and DECANeuFace,2D. Following the evaluationprotocol of the NoW benchmark (Sanyal et al., 2019a), we reconstruct 3D faces for the provided imagesvia each model and report the 3D reconstruction errors. In , our DECANeuFace,3D shows lower 3Dreconstruction error than DECAoriginal and DECANeuFace,2D.",
  "Discussion, Limitation and Conclusion": "We develop NeuFace, an optimization for generating accurate and spatio-temporally consistent 3D face meshpseudo-labels on videos with a provable optimal guarantee. Moreover, with the technique, we build theNeuFace-dataset, a large-scale 3D face meshes paired with in-the-wild 2D videos. We demonstrate the potential of the diversity and naturalness of our NeuFace-dataset as a training dataset tolearn generative 3D facial motion prior. Also, we improve the reconstruction accuracy of a de-facto standard3D face reconstruction model using our dataset. Note that the focus of our NeuFace optimization is to buildthe 3D face dataset for videos. The experiment for the static face reconstruction in was conducted todemonstrate the potential application of our method and dataset. Given our emphasis on video face data, ourmodel may not obtain the best accuracy for static face reconstruction. However, it is important to note thatour method still shows a marked improvement over existing data. This underscores the effectiveness of theNeuFace-dataset in enhancing 3D face reconstruction.We expect NeuFace to open up new opportunities byproviding large-scale, real-world 3D face video data, the NeuFace-dataset, as a reliable data curation method.The failure cases of NeuFace optimization could occur when the 2D video contains extreme degradations,e.g., motion blur, low resolution, extremely (>50%) occluded, so that the 2D keypoint detection fails. Pleasenote that when we construct the NeuFace-dataset, we tackle these cases with automatic filtering followed byhuman verification (will be discussed in appendix Sec. A), which guarantees the reliability of the dataset.Since 2D landmark human annotations are relatively cheaper than any other signals, we think using better2D landmarks can mitigate this limitation.",
  "Ethics Statement": "For face reconstruction tasks and datasets, the diversity of race or ethnicity, gender, appearance, and actions isan important topic to discuss (Wang et al., 2019; Zhu et al., 2022). Existing 3D face video datasets (Zielonkaet al., 2022; Ranjan et al., 2018; Cudeiro et al., 2019) typically have limited diversity regarding ethnicity,gender, appearance, and actions. Such 3D face datasets rarely provide video pairs, but with artificial facialmarkers attached to human faces and a small set of identities. On the other hand, our NeuFace-datasetmitigates such issues since our dataset is acquired on top of large-scale in-the-wild face video datasets,which typically rely on internet videos. Such video datasets are diverse in terms of ethnicity, gender, facialappearances, and actions when compared to the small/medium-scale 3D facial motion capture datasets. Sinceour dataset is acquired based on the existing public video datasets (Wang et al., 2020; Chung et al., 2018;Zhu et al., 2022), all the rights, licenses, and permissions follow the original datasets. Moreover, we willrelease the NeuFace-dataset by providing the reconstructed 3DMM parameters without the actual facial videoframes. NeuFace-dataset does not contain identity-specific metadata and facial texture maps. Nonetheless,per-identity shape coefficients can give a rough guide about human facial shape. Thus, we release our datasetfor research purposes only.",
  "We will make our code and data accessible to the public": "Manya V Afonso, Jos M Bioucas-Dias, and Mrio AT Figueiredo. An augmented lagrangian approach to theconstrained optimization formulation of imaging inverse problems. IEEE Transactions on Image Processing(TIP), 20(3):681695, 2010. 5 Sadegh Aliakbarian, Fatemeh Sadat Saleh, Mathieu Salzmann, Lars Petersson, and Stephen Gould. Astochastic conditioning scheme for diverse human motion prediction. In IEEE Conference on ComputerVision and Pattern Recognition (CVPR), 2020. 11",
  "Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.A convergence theory for deep learning via over-parameterization. In International Conference on Machine Learning (ICML), 2019. 4, 7, 24, 25, 26": "Anh Tu an Trn, Tal Hassner, Iacopo Masi, and Grard Medioni. Regressing robust and discriminative 3Dmorphable models with a very deep neural network. In IEEE Conference on Computer Vision and PatternRecognition (CVPR), 2016. 3 Mallikarjun B R, Ayush Tewari, Tae-Hyun Oh, Tim Weyrich, Bernd Bickel, Hans-Peter Seidel, HanspeterPfister, Wojciech Matusik, Mohamed Elgharib, and Christian Theobalt. Monocular reconstruction of neuralface reflectance fields. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 1 Gwangbin Bae, Martin de La Gorce, Tadas Baltruaitis, Charlie Hewitt, Dong Chen, Julien Valentin, RobertoCipolla, and Jingjing Shen. Digiface-1m: 1 million digital face images for face recognition. In IEEE WinterConf. on Applications of Computer Vision (WACV), 2023. 1",
  "Volker Blanz and Thomas Vetter. A morphable model for the synthesis of 3d faces. ACM Transactions onGraphics (SIGGRAPH), 1999. 3": "Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, and Michael J. Black.Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image. In EuropeanConference on Computer Vision (ECCV), 2016. 3, 23 Arij Bouazizi, Ulrich Kressel, and Vasileios Belagiannis. Learning temporal 3d human pose estimation withpseudo-labels. In IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS),2021. 2 Adrian Bulat and Georgios Tzimiropoulos.How far are we from solving the 2d & 3d face alignmentproblem?(and a dataset of 230,000 3d facial landmarks). In IEEE International Conference on ComputerVision (ICCV), 2017. 5, 20",
  "Chen Cao, Derek Bradley, Kun Zhou, and Thabo Beeler. Real-time high-fidelity facial performance capture.ACM Transactions on Graphics (SIGGRAPH), 34(4), 2015. 23": "Qiong Cao, Li Shen, Weidi Xie, Omkar M. Parkhi, and Andrew Zisserman. VGGFace2: A dataset forrecognising faces across pose and age.In International Conference on Automatic Face and GestureRecognition, 2018. 2, 3 Yen-Lin Chen, Hsiang-Tao Wu, Fuhao Shi, Xin Tong, and Jinxiang Chai. Accurate and robust 3d facialcapture using a single rgbd camera. In IEEE International Conference on Computer Vision (ICCV), 2013.3",
  "Yaim Cooper. Global minima of overparameterized neural networks. SIAM Journal on Mathematics of DataScience, 2021. 4": "Davide Cozzolino, Andreas Rssler, Justus Thies, Matthias Niener, and Luisa Verdoliva. Id-reveal: Identity-aware deepfake video detection. In IEEE International Conference on Computer Vision (ICCV), 2021.1 Daniel Cudeiro, Timo Bolkart, Cassidy Laidlaw, Anurag Ranjan, and Michael Black. Capture, learning,and synthesis of 3D speaking styles. In IEEE Conference on Computer Vision and Pattern Recognition(CVPR), 2019. 2, 3, 7, 11, 13 Radek Danecek, Michael J. Black, and Timo Bolkart. EMOCA: Emotion driven monocular face capture andanimation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 2, 3, 4, 5,20, 22",
  "Simon Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.Gradient descent provably optimizes over-parameterized neural networks. In International Conference on Machine Learning (ICML), 2019b. 4": "Bernhard Egger, William A. P. Smith, Ayush Tewari, Stefanie Wuhrer, Michael Zollhoefer, Thabo Beeler,Florian Bernard, Timo Bolkart, Adam Kortylewski, Sami Romdhani, Christian Theobalt, Volker Blanz,and Thomas Vetter. 3d morphable face modelspast, present, and future. ACM Transactions on Graphics(SIGGRAPH), 39(5), 2020. 3 Mohamed Elgharib, Mohit Mendiratta, Justus Thies, Matthias Niener, Hans-Peter Seidel, Ayush Tewari,Vladislav Golyanik, and Christian Theobalt. Egocentric videoconferencing. ACM Transactions on Graphics(SIGGRAPH), 39(6), 2020. 1 Han EunGi, Oh Hyun-Bin, Kim Sung-Bin, Corentin Nivelet Etcheberry, Suekyeong Nam, Janghoon Ju, andTae-Hyun Oh. Enhancing speech-driven 3d facial animation with audio-visual guidance from lip readingexpert. In INTERSPEECH, 2024. 1",
  "Gabriele Fanelli, Juergen Gall, Harald Romsdorfer, Thibaut Weise, and Luc Van Gool. A 3-d audio-visualcorpus of affective communication. IEEE Transactions on Multimedia, 12(6), 2010. 2": "Qi Fang, Qing Shuai, Junting Dong, Hujun Bao, and Xiaowei Zhou. Reconstructing 3d human pose bywatching humans in the mirror. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),2021. 2 Haiwen Feng, Timo Bolkart, Joachim Tesch, Michael J. Black, and Victoria Abrevaya. Towards raciallyunbiased skin tone estimation via scene disambiguation. In European Conference on Computer Vision(ECCV), 2022. 7 Yao Feng, Fan Wu, Xiaohu Shao, Yanfeng Wang, and Xi Zhou. Joint 3d face reconstruction and densealignment with position map regression network. In European Conference on Computer Vision (ECCV),2018. 12 Yao Feng, Haiwen Feng, Michael J. Black, and Timo Bolkart. Learning an animatable detailed 3D face modelfrom in-the-wild images. ACM Transactions on Graphics (SIGGRAPH), 40(8), 2021. 2, 3, 4, 5, 6, 9, 10,12, 20, 23, 24 Baris Gecer, Stylianos Ploumpis, Irene Kotsia, and Stefanos Zafeiriou. Ganfit: Generative adversarial networkfitting for high fidelity 3d face reconstruction. In IEEE Conference on Computer Vision and PatternRecognition (CVPR), 2019. 3 Philip-William Grassal, Malte Prinzler, Titus Leistner, Carsten Rother, Matthias Niener, and Justus Thies.Neural head avatars from monocular rgb videos. In IEEE Conference on Computer Vision and PatternRecognition (CVPR), 2022. 4",
  "Jianzhu Guo, Xiangyu Zhu, Yang Yang, Fan Yang, Zhen Lei, and Stan Z Li. Towards fast, accurate andstable 3d dense face alignment. In European Conference on Computer Vision (ECCV), 2020. 12": "Mohamed Hassan, Vasileios Choutas, Dimitrios Tzionas, and Michael J. Black. Resolving 3D human poseambiguities with 3D scene constraints. In IEEE International Conference on Computer Vision (ICCV),2019. 2 Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trainedby a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural InformationProcessing Systems (NeurIPS), 2017. 11 Chun-Hao P. Huang, Hongwei Yi, Markus Hschle, Matvey Safroshkin, Tsvetelina Alexiadis, Senya Polikovsky,Daniel Scharstein, and Michael J. Black. Capturing and inferring dense full-body human-scene contact. InIEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 2 Patrik Huber, Zhen-Hua Feng, William Christmas, Josef Kittler, and Matthias Rtsch. Fitting 3d morphableface models using local features. In IEEE International Conference on Image Processing (ICIP), 2015. 3",
  "Taras Khakhulin, Vanessa Sklyarova, Victor Lempitsky, and Egor Zakharov. Realistic one-shot mesh-basedhead avatars. In European Conference on Computer Vision (ECCV), 2022. 7": "Hyeongwoo Kim, Pablo Garrido, Ayush Tewari, Weipeng Xu, Justus Thies, Matthias Niessner, Patrick Prez,Christian Richardt, Michael Zollhfer, and Christian Theobalt. Deep video portraits. ACM Transactionson Graphics (SIGGRAPH), 37(4), 2018. 1 Nikos Kolotouros, Georgios Pavlakos, Michael J. Black, and Kostas Daniilidis. Learning to reconstruct 3dhuman pose and shape via model-fitting in the loop. In IEEE International Conference on ComputerVision (ICCV), 2019. 12, 23 Sangho Lee, Jiwan Chung, Youngjae Yu, Gunhee Kim, Thomas Breuel, Gal Chechik, and Yale Song.Acav100m: Automatic curation of large-scale datasets for audio-visual video representation learning. InIEEE International Conference on Computer Vision (ICCV), 2021. 2 Tianye Li, Timo Bolkart, Michael. J. Black, Hao Li, and Javier Romero. Learning a model of facial shape andexpression from 4D scans. ACM Transactions on Graphics (SIGGRAPH Asia), 36(6), 2017. 3, 4, 6, 20, 23",
  "Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou. Tang. Deep learning face attributes in the wild. In IEEEInternational Conference on Computer Vision (ICCV), 2015. 2, 3": "Shugao Ma, Tomas Simon, Jason Saragih, Dawei Wang, Yuecheng Li, Fernando De la Torre, and Yaser Sheikh.Pixel codec avatars. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 1 Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic.HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips. InIEEE International Conference on Computer Vision (ICCV), 2019. 2",
  "Omkar M. Parkhi, Andrea Vedaldi, and Andrew Zisserman. Deep face recognition. In British Machine VisionConference (BMVC), 2015. 2, 3": "Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas,and Michael J. Black. Expressive body capture: 3d hands, face, and body from a single image. In IEEEConference on Computer Vision and Pattern Recognition (CVPR), 2019. 3, 23 P. Paysan, R. Knothe, B. Amberg, S. Romdhani, and T. Vetter. A 3d face model for pose and illuminationinvariant face recognition. In Proceedings of the 6th IEEE International Conference on Advanced Videoand Signal based Surveillance (AVSS) for Security, Safety and Monitoring in Smart Environments, 2009. 3 Amit Raj, Michael Zollhoefer, Tomas Simon, Jason Saragih, Shunsuke Saito, James Hays, and StephenLombardi. Pva: Pixel-aligned volumetric avatars. In IEEE Conference on Computer Vision and PatternRecognition (CVPR), 2020. 1",
  "Anurag Ranjan, Timo Bolkart, Soubhik Sanyal, and Michael J. Black. Generating 3D faces using convolutionalmesh autoencoders. In European Conference on Computer Vision (ECCV), 2018. 2, 3, 13": "Davis Rempe, Tolga Birdal, Aaron Hertzmann, Jimei Yang, Srinath Sridhar, and Leonidas J. Guibas. Humor:3d human motion model for robust pose estimation. In IEEE International Conference on Computer Vision(ICCV), 2021. 10, 11 Alexander Richard, Michael Zollhfer, Yandong Wen, Fernando de la Torre, and Yaser Sheikh. Meshtalk: 3dface animation from speech using cross-modality disentanglement. In IEEE International Conference onComputer Vision (ICCV), 2021. 1 Andreas Rssler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, and Matthias Niener. Face-forensics: A large-scale video dataset for forgery detection in human faces. arXiv preprint arXiv:1803.09179,2018. 1 Andreas Rssler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, and Matthias Niener.FaceForensics++: Learning to detect manipulated facial images. In IEEE International Conference onComputer Vision (ICCV), 2019. 1 Christos Sagonas, Epameinondas Antonakos, Georgios Tzimiropoulos, Stefanos Zafeiriou, and Maja Pantic.300 faces in-the-wild challenge: Database and results. Image and Vision Computing (IMAVIS), 47, 2016. 9",
  "Mathieu Salzmann. Continuous inference in graphical models with polynomial energies. In IEEE Conferenceon Computer Vision and Pattern Recognition (CVPR), 2013. 5": "Soubhik Sanyal, Timo Bolkart, Haiwen Feng, and Michael Black. Learning to regress 3d face shape andexpression from an image without 3d supervision. In IEEE Conference on Computer Vision and PatternRecognition (CVPR), 2019a. 3, 12 Soubhik Sanyal, Timo Bolkart, Haiwen Feng, and Michael J Black. Learning to regress 3d face shape andexpression from an image without 3d supervision. In IEEE Conference on Computer Vision and PatternRecognition (CVPR), pp. 77637772, 2019b. 12",
  "Kihyuk Sohn, Xinchen Yan, and Honglak Lee.Learning structured output representation using deepconditional generative models. In Advances in Neural Information Processing Systems (NeurIPS), 2015. 10": "Kim Sung-Bin, Lee Chae-Yeon, Gihun Son, Oh Hyun-Bin, Janghoon Ju, Suekyeong Nam, and Tae-HyunOh. Multitalk: Enhancing 3d talking head generation across languages with multilingual video dataset. InINTERSPEECH, 2024a. 1 Kim Sung-Bin, Lee Hyun, Da Hye Hong, Suekyeong Nam, Janghoon Ju, and Tae-Hyun Oh. Laughtalk:Expressive 3d talking head generation with laughter. In IEEE Winter Conf. on Applications of ComputerVision (WACV), 2024b. 1 Ayush Tewari, Mohamed Elgharib, Mallikarjun BR, Florian Bernard, Hans-Peter Seidel, Patrick Prez,Michael Zllhofer, and Christian Theobalt. Pie: Portrait image embedding for semantic control. ACMTransactions on Graphics (SIGGRAPH Asia), 39(6), 2020. 1",
  "Justus Thies, M. Zollhfer, M. Niener, L. Valgaerts, M. Stamminger, and C. Theobalt. Real-time expressiontransfer for facial reenactment. ACM Transactions on Graphics (SIGGRAPH), 34(6), 2015. 1, 3": "Justus Thies, M. Zollhfer, M. Stamminger, C. Theobalt, and M. Niener. Face2face: Real-time face captureand reenactment of rgb videos. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),2016a. 1 Justus Thies, Michael Zollhofer, Marc Stamminger, Christian Theobalt, and Matthias Niener. Face2face:Real-time face capture and reenactment of rgb videos. In IEEE Conference on Computer Vision andPattern Recognition (CVPR), 2016b. 23 Justus Thies, Michael Zollhfer, Christian Theobalt, Marc Stamminger, and Matthias Niessner. Headon:Real-time reenactment of human portrait videos. ACM Transactions on Graphics (SIGGRAPH), 37(4), jul2018. ISSN 0730-0301. doi: 10.1145/3197517.3201350. 1 Anh Tuan Tran, Tal Hassner, Iacopo Masi, and Grard Medioni. Regressing robust and discriminative 3dmorphable models with a very deep neural network. In IEEE Conference on Computer Vision and PatternRecognition (CVPR), 2017. 12 Kaisiyuan Wang, Qianyi Wu, Linsen Song, Zhuoqian Yang, Wayne Wu, Chen Qian, Ran He, Yu Qiao, andChen Change Loy. Mead: A large-scale audio-visual dataset for emotional talking-face generation. InEuropean Conference on Computer Vision (ECCV), 2020. 2, 3, 5, 7, 9, 12, 13, 20 Mei Wang, Weihong Deng, Jiani Hu, Xunqiang Tao, and Yaohai Huang. Racial faces in the wild: Reducingracial bias by information maximization adaptation network.In IEEE International Conference onComputer Vision (ICCV), 2019. 2, 13",
  "Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu. One-shot free-view neural talking-head synthesis for videoconferencing. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 1, 2": "Benjamin Weyrauch, Bernd Heisele, Jennifer Huang, and Volker Blanz. Component-based face recognitionwith 3d morphable models. In IEEE Conference on Computer Vision and Pattern Recognition Workshops(CVPRW), 2004. 1 Erroll Wood, Tadas Baltrusaitis, Charlie Hewitt, Sebastian Dziadzio, Matthew Johnson, Virginia Estellers,Tom Cashman, and Jamie Shotton. Fake it till you make it: Face analysis in the wild using synthetic dataalone. In IEEE International Conference on Computer Vision (ICCV), 2021. 1, 3 Erroll Wood, Tadas Baltrusaitis, Charlie Hewitt, Matthew Johnson, Jingjing Shen, Nikola Milosavljevic,Daniel Wilde, Stephan Garbin, Chirag Raman, Jamie Shotton, Toby Sharp, Ivan Stojiljkovic, Tom Cashman,and Julien Valentin. 3d face reconstruction with dense landmarks. In European Conference on ComputerVision (ECCV), 2022. 1, 3, 6, 12",
  "We provide detailed configurations for implementations and experiments in the main paper": "NeuFace optimization details.NeuFace optimization is composed of a neural network w and theoptimizing part. For the network w, we use a pre-trained DECA (Feng et al., 2021) or a pre-trainedEMOCA (Danecek et al., 2022) encoder network. Overall optimization takes about 8 min. for 7 views, 120frames of videos, and about 2.5 min. for 1 view, 120 frames of videos. NeuFace-dataset acquisition. We provide reliable 3D face mesh annotations for large-scale face videodatasets: MEAD (Wang et al., 2020), VoxCeleb2 (Chung et al., 2018), and CelebV-HQ (Zhu et al., 2022).We optimize our full objective (Eq. (1)) to acquire FLAME meshes for the datasets with a multi-view camerasetup, e.g., MEAD. Otherwise, we optimize (Eq. (1)) with view=0. We automatically discard the sequencesif the optimization yields out-of-distribution shape parameters, i.e., the L2-norm of shape parameters deviateslargely from the pre-built distribution (Li et al., 2017), 2>1.0, or if the 2D landmark detector (Bulat &Tzimiropoulos, 2017) fails to capture the faces. After subsequent human verification, the NeuFace-datasetachieves <0.1% of failure rate on upon criteria, supporting the reliable quality of our dataset. NeuFace dataset includes 24 diverse emotion/expression categories. Compared to indoor motion capturedatasets (BIWI 3D, COMA, VOCASET), our dataset has a greater variety of emotion/expression categories.Moreover, our dataset contains 3D faces in natural talking scenarios, resulting in higher diversity within thesecategories. NeuFace-dataset contains at least 40 different facial attributes, including hairstyles and accessoriessuch as glasses. While traditional motion capture datasets do not disclose the number of facial attributes,their facial attribute diversity is incomparably limited since they contain a small number of identities, andindoor environments. Unlike traditional motion capture datasets, which are recorded in controlled indoorenvironments with fixed camera views and lighting, the NeuFace-dataset uses in-the-wild YouTube videos,providing a much greater diversity of backgrounds. Facial motion prior. As one of our datasets prominent applications, we introduced the learning of 3Dfacial motion prior, called HuMoR-Face (Sec.5.1 in the main paper). We first pre-process 3D face meshesin the NeuFace-dataset. We compute root orientation, face pose angles, 3D landmark positions, and theirvelocities, respectively. Then, we represent the state of a moving human face as x = [, , , J, J], where, denotes head root orientation and its velocity, denotes the FLAME face pose parameters, and J, Jdenotes facial joint and its velocity, respectively. The generative facial motion prior is trained to predict the facial motion state, xt+1, given the current statext as a condition. We consider the NeuFace holdout test split as the real motion distribution and compute theFD for the generated motions. We do not consider VOCASET as the real motion distribution for computingFD. It is limited in diversity and naturalness, which contradicts FDs purpose of measuring the naturalnessof generated motions. Our NeuFace holdout test split is much larger and more diverse than VOCASET. Fine-tuning face mesh regressor.As our datasets another application, we improve the accuracy ofthe pre-trained DECA model with our NeuFace-dataset and its 3D annotations. Specifically, we fine-tunethe pre-trained DECA parameters with our NeuFace-dataset and the auxiliary 3D supervisions proposedin the main paper (L795-797). During fine-tuning, we use an adjusted learning rate, 1 105, which is tentimes smaller than training DECA from scratch. Note that there exist the DECA-coarse model and theDECA-detail model. Unfortunately, there are known issues in reproducing DECA-detail due to the absenceof VGGFace2 and the training recipe (DECA GitHub issues: bit.ly/3jj2psn, bit.ly/3HIiVf0).",
  "BAblation on the design choices": "In our proposed NeuFace optimization, the temporal consistency loss employs a temporal moving averageto estimate latent target meshes that represent temporally smooth heads. To assess the effectiveness of thetemporal moving average operation, we conduct an ablation study using a median operation instead of theaverage on a subset of the MEAD dataset. In Table S1, both methodstemporal moving average and medianoperation in the temporal windowdemonstrated comparable performance in terms of NME and CVD, withno significant difference in the optimization results. This finding suggests that the choice of operation has anegligible impact on the optimization process. Although our proposed NeuFace optimization utilizes a simple combination of losses, we find that it issurprisingly effective for constructing accurate pseudo 3D face data. For example, we observe that the use ofphotometric loss (Lphoto) has a negligible effect, as demonstrated in Table S2 (a). In fact, Lphoto tends todegrade performance, with larger values of this loss leading to greater degradation. We postulate that thismight be due to the impact of self-shadows and non-Lambertian reflection caused by lighting and noise invideo data, which could interfere with robust optimization. Interestingly, we also reveal that without explicitregularization for shared identities across frames in videos, the identity codes tend to converge automatically,as shown in Table S2 (b). This finding suggests that adding shared identity regularization may be unnecessary.Despite the effectiveness of our proposed NeuFace optimization, we believe that exploring additional losses tofurther refine the optimization process could be a worthwhile future direction. We proposed an EM-style optimization for our NeuFace optimization. In each optimization step t, we computelatent target variables Mtf and qtf,v at E-step using multi-view bootstrapping or temporal smoothing. Thelatent target variables serve as the supervision for spatio-temporal consistency losses at M-step. Our EM-stylelosses are as follows (Eq. (3) and Eq. (4) in the main):",
  "CNeuFace optimization with EMOCA": "Recall that we can replace the neural parameterization of face meshes with another neural model. Specifically,we use EMOCA (Danecek et al., 2022), which is built upon DECA with an additional expression encoder.We change the neural network from DECA to EMOCA and optimize over it with our spatio-temporal andlandmark losses. In and Sec. 4 in the main paper, we discussed about the quantitative quality ofNeuFace-E-dataset. Figure S1: Multi-view consistency: EMOCA vs. NeuFace-E. We visualize the meshes obtained byEMOCA (Danecek et al., 2022) and NeuFace-E. By aligning the meshes to face the same direction, we canclearly notice that NeuFace-E obtains multi-view consistent meshes compared to EMOCA. Qualitatively, we visualize the rendered meshes over different views as in Fig. S1. NeuFace-E-dataset (3rdrow) contains multi-view consistent meshes over views compared to the meshes obtained by EMOCA inference(2nd row). Specifically, EMOCA produces huge discrepancies in mouth area across the views, while ourmethod is more consistent. Moreover, our method reconstructs more accurate meshes, especially for theshape of the nose and face contour.",
  "DNeuFace vs. Video 3D Face Tracking Method": "We verify the favorable quality of NeuFace-datasets by comparing it with the meshes obtained by the state-of-the-art method, MICA with a tracker (Zielonka et al., 2022) (MICA+T), among the existing methods (Caoet al., 2013; 2015; Thies et al., 2016b; Zielonka et al., 2022). MICA+T jointly optimizes 3DMM, cameras,and textures with landmark and photometric losses, and statistic regularizers. In Table S4, NeuFace performsbetter than MICA+T in CVD & NME with comparable MSI. Also, faster optimization makes NeuFacepreferable when annotating large-scale videos.",
  "EAnalysis on NeuFace": "In this section, we introduce and validate our design choices for NeuFace optimization, through analysis.Specifically, we build a strong baseline and support our choice of re-parameterized face mesh optimizationmethod for NeuFace in Sec. E.1. Next, we provide a proof sketch of the provable global minima convergenceof NeuFace optimization in Sec. E.2.",
  "E.1FLAME fitting vs. NeuFace optimization": "Recall that NeuFace re-parameterizes the 3DMM, i.e., FLAME (Li et al., 2017) to the neural parameters(represented as DECA (Feng et al., 2021)), then optimizes over them to obtain accurate 3D face meshesfor videos. Following the prior arts in the parametric human body reconstruction literature (Bogo et al.,2016; Pavlakos et al., 2019; Kolotouros et al., 2019), there exists a simple method to optimize the parametricmodel; 3DMM parameter fitting. Thus, we implement FLAME fitting as a solid baseline and compare thequantitative and qualitative results with NeuFace optimization to analyze and support our choice of neuralre-parameterization.",
  "[b, pb] = arg minb,pbL2D + tempLtemporal + viewLmultiview + rLr + L + L + L,": "where the losses L2D, Ltemporal, and Lmultiview are identical to the losses discussed in the main paper(Eqs. 2,3,4). We can obtain the initial FLAME parameters for the optimization in two ways: (1) initialize frommean parameters and (2) initialize from pre-trained DECA (Feng et al., 2021) predictions. We empiricallyfound that initialization with mean FLAME parameters frequently fails when the input images containextreme head poses. Thus, we choose to initialize FLAME parameters from the pre-trained DECA predictions,thus providing a plausible initialization for a fair comparison. Also, following the convention (Li et al., 2017),we optimize FLAME parameters in a coarse-to-fine manner. For the earlier stage, we fix FLAME parametersthat control local details, i.e., b, b, and b, and optimize the global head orientation, rb, and cameraparameters pb. Then we fix camera parameters and optimize other FLAME parameters jointly at a laterstage to fit the local details. Since we initialize FLAME parameters and camera parameters from the pre-trained DECA predictions,i.e., initial [b, pb] in Eq. (5). Accordingly, the meshes obtained by the FLAME fitting achieve betterspatio-temporal consistency and 2D landmark accuracy than the meshes obtained by a pre-trained DECAwithout any post-processing. Qualitative result. In Fig. S2(a), NeuFace-dataset contains much expressive and image-aligned meshes, e.g.,wrinkles and face boundaries. On the other hand, the meshes obtained by the direct FLAME optimization show",
  "E.2Proposition 1: Convergence Property to Global Minima": "Proposition 1 in this work is a straightforward variation of the main result of Allen-Zhu et al. (Allen-Zhu et al., 2019).Before providing the proof sketch of Proposition 1.We describe the assumptionsneeded to prove Proposition 1.For simplicity, we consider a simple l2-based regression loss and a L-layer fully connected ReLU network data consisting of vector pairs {(xi, y W() having a uniform weight width size of m. For the trainingi )}i[n], the network has the batched input of x={xi}i[n], where",
  "Assumption 1. Without loss of generality, i, xi = 1 and yi O(1)": "Assumption 2. The pretrained neural network weights W(0) are assumed to be started from the val-ues distributed normally, i.e., considered as a sample instance from a Gaussian distribution. Specifically,[W(0)l]i,j N(0, 2/row[Wl]) for l {1, , L 1} and [W(0)L ]i,j N(0, 1/row[WL]) for every (i, j), wherethe operator row[] returns the row size of the input matrix. Assumption 2 appears to be restrictive by those standard deviations, but it is not. The assumptions cover afairly broad range of weight distribution scenarios. For larger standard deviations, we can always set a smallnorm for xs in Assumption 1 without loss of generality, and vice versa.",
  "L(W)": "This lemma states the semi-smoothness property of the objective function L w.r.t. W() to take into accountnon-smoothness introduced by ReLU activation in W(). The semi-smoothness looks similar to the Lipschitz smoothness except for the first order term W2. Interestingly, when we increase m, the increasing rate ofthe first order term is much slower than that of the second order term; thus, the second order term becomesdominant compared to the first order one, and the semi-smoothness approaches closer to the Lipschitzsmoothness. This means that the neural network is smoother as m goes larger. Under the assumption that W(t) W(0)F is small (will be verified later), the next step is to combineLemma 2 with gradient descent to derive the loss-decrease guarantee. Denoting t = L(W(t)), the gradientdescent update rule is defined as: W(t+1) = W(t) t for a learning rate > 0. Then, from Lemma 2,putting W(t+1) = W + W and W(t) = W, i.e., W = t, we have",
  "Remark 1: Global optimality. In Proposition 1, we can set arbitrarily small. With a very small , itsuggests that a converged point W is a global minimum": "Remark 2: The radius condition between the initial weights and updated one.The spectralradius bounds for W W(0)F required in Lemmas 2 and 1 appear to be small, it is sufficiently largeenough to completely change the output of the model, considering the large width of size m and the standarddeviation1m of weight entries in Assumption 2. Remark 3: Other architectures. Allen-Zhu et al. (2019); Du et al. (2019a) present the recipes to convertthe L-layer fully connected networks to convolutional neural networks and to ResNet by sacrificing thecomplexity of proof. Thus, the conclusion of the provable guarantee does not change with such architecturalchanges. Thus, the architectures we experimented provably comply with the conclusion of Proposition 1 upto the choice of the parameters, i.e., global convergence. Remark 4: Other losses. In the above proof sketch, one of the important pieces is the semi-smoothness inLemma 2. While we discuss only with the simple l2 regression loss function, fortunately, the semi-smoothnessalready encompasses any choice of Lipschitz smooth cases for the loss functions. Thus, as long as the choiceof the loss function is Lipschitz smooth, the replacement of the loss function does not alter the conclusion ofProposition 1 even for non-convex losses except the choice of parameters. This hints that our choice of themulti-task loss in Eq. (1) provably complies with the conclusion of Proposition 1 except the choice of theparameters.",
  "FMore dataset samples": "We present more qualitative samples of NeuFace-dataset, with diverse identities and visual features (seeFig. S3). Since we cannot deliver expressive and temporally smooth facial motion in images, we stronglyrecommend seeing video visualizations for NeuFace-dataset, in the supplementary videos. Figure S3: NeuFace-dataset is the large-scale 3D face video dataset containing 3DMM annotations forfaces with diverse ethnicity, gender, emotions, and actions. See supplementary videos for the dynamic facevisualizations."
}