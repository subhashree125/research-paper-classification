{
  "Abstract": "Estimating heterogeneous treatment effects is important to tailor treatments to those indi-viduals who would most likely benefit. However, conditional average treatment effect pre-dictors may often be trained on one population but possibly deployed on different, possiblyunknown populations. We use methodology for learning multi-accurate predictors to post-process CATE T-learners (differenced regressions) to become robust to unknown covariateshifts at the time of deployment. The method works in general for pseudo-outcome regres-sion, such as the DR-learner. We show how this approach can combine (large) confoundedobservational and (smaller) randomized datasets by learning a confounded predictor from theobservational dataset, and auditing for multi-accuracy on the randomized controlled trial.We show improvements in bias and mean squared error in simulations with increasinglylarger covariate shift, and on a semi-synthetic case study of a parallel large observationalstudy and smaller randomized controlled experiment. Overall, we establish a connection be-tween methods developed for multi-distribution learning and achieve appealing desiderata(e.g. external validity) in causal inference and machine learning.",
  "Introduction": "Causal inference studies how to make the right decision, at the right time, for the right person. Extensiverecent literature on heterogeneous treatment effects, also called conditional average treatment effects (CATE),studies the estimation of personalized causal effects, rather than only population-level average treatmenteffects. Estimating CATE can inform better triage of resources to those who most benefit in healthcare,social services, e-commerce, and many other domains. In these consequential domains, many firms/decision-makers face treatment decisions where other firms alsoneed to make the same decision, although perhaps each with slightly different data distributions. For example,problems of clinical risk prediction, such as risk of a heart disease or medication treatment guidelines, are",
  "Authors listed in alphabetical order": "shared widely across hospitals, but each has its own distribution of patients in addition to idiosyncraticreporting, testing, and treatment patterns that can hinder external validity (Caruana et al., 2015). Indeed,off-the-shelf, relatively simple clinical risk calculators developed on one population are often broadly deployedas a decision support tool in many locations, without the ability to share the originating individual-leveldata, or with data drift over time. In social settings, the Arnold Public Safety Assessment (PSA), trainedon a proprietary dataset and used in hundreds of jurisdictions (Goel et al., 2021), is an example of a widelydeployed tool. Its accompanying decision-making matrix is another example of a treatment recommendationrule made more widely available (Laura and John Arnold Foundation, 2016), which can introduce disparitiesand poor treatment efficacy (Zhou, 2024). Other examples include the design of algorithmic profiling in activelabor market programs (Crpon and Van Den Berg, 2016; Bach et al., 2023; Krtner and Bonoli, 2023): manydifferent jurisdictions run different active labor market programs, and policymakers face questions about howto learn from what works elsewhere and how to scale up programs across heterogeneous locations. A key challenge in these settings is to certify valid predictive performance of personalized causal effectsfor unknown deployment settings, each of which could have a different target covariate distribution. Forexample, predictive risk calculators, such as those for chronic heart disease, learned on a specific populationmight induce biased estimation for different locales with different populations. As one example, the widelyused Framingham risk score overestimates risk for Asian populations (Badawy et al., 2022). This problemis not limited to earlier risk scores, but also affects modern ones: a sepsis predictive risk score provided byEpic, a major healthcare IT provider, fell short in a study of external validity on another population (Habibet al., 2021). External validity, generalizability, and transportability are also important questions for causal inference (Tip-ton, 2014; Tipton and Hartman, 2023; Bareinboim and Pearl, 2013). Heterogeneous causal effect estimatesmight also be similarly learned on one population, but made more widely available, hence vulnerable tounknown covariate shifts. Spini (2021) studies the potential impacts of shifts in population for generalizingresults from the Oregon Health Insurance Experiment, while Shyr et al. (2024) studies potential shifts ineffect heterogeneity across multiple cancer studies. On the other hand, we do want to leverage predictive information when it is available. How can we developmethods for heterogeneous treatment effect estimation so that a new hospital, without its own large databaseor in-house machine learning team, is still assured guarantees of low predictive bias on its own population,that might differ in unknown ways from a proprietary risk score that does not publish the original data?Importantly, while some methods guarantee validity of prediction models for a known target distribution,our previous examples highlight important cases when we want to guarantee low prediction bias under manyunknown target distributions. In this paper, we show how methods from multi-accurate learning (Hbert-Johnson et al., 2018; Kim et al.,2019) can endow conditional average treatment effect estimation with robustness to unknown covariate shifts.Indeed, the problem of confounding itself is a covariate shift problem, from the treated or control popula-tion to some target population (Johansson et al., 2022). Multi-accurate learning is a powerful and flexibleframework that, by ensuring low predictive bias over a test function class, is also robust to combinationsof these covariate shifts: those induced by confounding or unknown covariate shifts in the target popu-lation. Although multi-calibrated and accurate learning originated from fairness motivations for ensuringcalibration/low prediction bias over rich subgroups, in this work we show how the adversarial test functionsin the formulation also confer broad robustness against covariate shift. To highlight this flexibility, we usemulti-accurate calibration on an extremely small clinical trial to correct a predictor from an confoundedobservational study. Though there is extensive work on establishing external validity and transportability of causal effects, mostof this work assumes information about a target population. Drawing inspiration from Kim et al. (2022),which studied universal adaptability of averaged outcome estimators with bias robust to unknown covariateshifts, we learn CATE estimates that will maintain unbiased predictions under unknown target populations. Although causal inference and machine learning has witnessed significant methodological innovation eitherin orthogonal/statistical learning or other machine learning adaptations (Kennedy, 2023; Nie and Wager,2020; Chernozhukov et al., 2018; Wager and Athey, 2018; Shalit et al., 2017; Hill, 2011), to name just a few,multi-accurate learning (Kim et al., 2019) introduces a different methodological toolkit related to boost-ing/adversarial formulations of conditional moment conditions (Dikkala et al., 2020; Bennett and Kallus,2023; Ghassami et al., 2022). We conduct a thorough empirical study comparing finite- and large-sampleperformance of multi-accurate learning and other causal machine learning techniques more specifically tai-lored for causal structure. To summarize, we find that multi-accurate methods grant additional robustnessagainst unknown covariate shifts while being competitive with more advanced causal machine learning meth-ods in finite-samples. There is a robustness-efficiency tradeoff: the latter methods are designed to exploitin-distribution efficiency, which multi-accurate learning off-the-shelf does not. Nonetheless, our work con-nects these two previously unrelated lines of work and shows how multi-accurate learning off-the-shelf canaddress the problem of robust CATE estimation. Multi-accurate learning reduces prediction bias from modelmisspecification, just as is required for conditional average treatment effect estimation. In our thorough empirical study we find that our proposed multi-accurate T- and DR-learner performwell under unobserved covariate shift. Although our work does not suggest multi-accurate learning as areplacement for state-of-the-art causal machine-learning for in-distribution estimation, due to differencesin variance reduction or hyperparameter selection subroutines, it does provide evidence that could informfurther methodological improvements and variance reduction of multi-accurate learning for CATE estimation.In summary: Multi-accurate learning can be used off-the-shelf to post-process CATE estimates based ondifferenced outcome regressions to endow them with robustness to unknown covariate shift.Such post-processing can be appealing because it can work with only black-box access to predictors and original data.(Our earlier examples indicate many situations where only black-box access to predictors is available).Alternative approaches to robustness against unknown shifts, like distributionally robust optimization, couldchange the robust-optimal predictor to a risk-sensitive one rather than the true CATE, but multi-accuratelearning does not.",
  "The contributions of our work are the following": "We propose multi-accurate post-processing of differenced-regression (T-learner) and pseudo-outcome(the doubly-robust score in the DR-learner) based CATE estimation to obtain unbiased predictionon unknown deployment populations. This approach can also flexibly adapt to a variety of covariateshifts from confounding to adversarial/unknown shifts: we illustrate by postprocessing a CATEestimator that combines large observational/small randomized data.We theoretically establishidentification. In Proposition 3 we show that multi-accurate post-processing of simple CATE estimates (T-learner)with a richer test function class can approximate a less-multi-accurate/less-robust but more-advanced CATE estimator, i.e. a multi-accurate DR-learner under a simpler test function class.This establishes theoretically favorable estimation properties: that our method not only providesrobust bias control, but is also comparable to (weaker implementations of) CATE estimators thatpursue efficient estimation. This is an important contribution since the multi-accuracy frameworkalone only guarantees robust bias control. We show in extensive experiments with simulations and real-world observational and randomizeddata from the Womens Health Initiative how our approach achieves finite-sample gains in ensur-ing robust bias control (and correspondingly, MSE) under unknown distribution shifts. Hence weempirically verify our theoretical contributions. In , we introduce the formal problem setup. In we describe the background contextand most closely related work to our methodological developments (other related work is discussed in Sec-tion 6). We develop and analyze our methodology in . In , we provide extensive empirical",
  "{Y (1), Y (0)} T | X": "Assumption 1 is a generally untestable assumption that permits causal identification. For example, it holdsin randomized trials by design, and in observational studies if the observed covariates are fully informativeof selection into treatment. Later on, we will jointly consider access to both a large-scale observational study(with potential violations of unconfoundedness) and a small randomized trial.",
  "Assumption 3 (Bounded outcomes). We assume that |Y (1)|, |Y (0)| B": "Many prior works have also assumed bounded outcomes, including but not limited to Kennedy (2020);Dudk et al. (2014). Relaxing the assumption of bounded outcomes is a mild technicality: it is used inthe convergence analysis of multi-accuracy as a sufficient but not necessary condition and as a conditionfor Chernoff inequalities, which could be replaced with stronger concentration inequalities under weakerassumptions. See also Devroye and Wise (1980); Gayraud (1997) for canonical estimators for estimating thesupport of a distribution. Estimands.Thecommonestimandincausalinferenceisthe average treatment effect,(ATE)E [Y (1) Y (0)]. In regimes with posited heterogeneous treatment effects, that are predictable given co-variates X, a (functional) estimand of interest is the conditional average treatment effect (CATE)",
  "Unknown": "Setting 1: External Validity Setting 2: Observational + Randomized Data : Schematic of setting 1 (external shift), and setting 2 (learning from large observational and smallRCT data). We propose multi-accuracy (MC-Boost) auditing as an off-the-shelf procedure to improve thedownstream robustness of CATE learners to unknown covariate shifts in both settings.",
  "The bias is of course a component of the MSE: multi-accuracy methods provide guarantees on the absolutebias; later in we extensively empirically evaluate the mean squared error as well": "We write QX(x), PX, PX1, PX0 for the marginal distribution of X under Q, the marginal distribution of Xunder P, and PX1, PX0 under X | T = 1, X | T = 0 on the observed data, respectively. We also denoteEP [], EP1[], EP0[] to denote marginalization over X in the training data, X | T = 1, or X | T = 0,respectively. For brevity we write EQ[] to denote expectations under the unknown target distribution QX onX (which can be extended to accommodate shifts beyond the typical covariate shift assumption in a slightabuse of notation). For example, t arg ming EPXt[(Y g(X))2], e.g. by default, regression in each treatedarm minimizes the MSE under the covariate distribution of each treatment arm.",
  "We seek an estimator (X) with low bias under Q : |EQ[((X) (X))]|": "L1 is the set of valid covariate shift likelihood ratios, such that QX is absolutely continuous w.r.t PX1, i.e.the likelihood ratio is finite whenever PX1 has nonzero support (almost surely, with probability 1 underthe measure PX1), and marginalizes to 1. We will call this type of unknown deployment shift an externalshift. This is analogous to the unknown shift setting studied in Kim et al. (2022), as well as other literatureon unknown covariate shifts (Jeong and Namkoong, 2020; Subbaswamy et al., 2021; Hatt et al., 2021). Incontrast to an extensive literature on transportability and external validity, we focus on the case of a-prioriunknown deployment shifts. If suitably nonparametric CATE estimation indeed recovered the Bayes-optimal predictor in finite samples,there would be no issue of unknown deployment shifts. But because in finite samples it generally does not,modifying estimation to protect against unknown deployment shifts can protect against misspecification andfinite-sample issues. For example, misspecified CATE estimation is vulnerable to unknown covariate shift.Furthermore, the conventional mean-squared error MSE can be nonzero for the Bayes-optimal predictor1(X) = E[Y (1) | X]. If the conditional bias or variance in Y is heteroskedastic (i.e. varies in x), the predic-tion MSE changes as external shifts change the marginalizing covariate distribution. Under misspecificationand nontrivial residual variance of the Bayes regressor, different weighting functions can indeed change thepopulation minimizer (Shimodaira, 2000; Buja et al., 2019). Later on we will use multi-accurate learning topost-process CATE estimates to ensure robustness against covariate shifts represented by a function class oflikelihood ratios.",
  "Unobserved confounding: observational data with RCT": "We consider a different setting where unknown covariate shifts may arise: a large observational datasetand small randomized trial. The observational study may be subject to unobserved confounding. On theother hand, the sample size of the randomized data may be small, so that learning conditional causal effectssolely from randomized data is unsupported. This regime is common in clinical settings, such as the parallelWomens Health Initiative observational study and clinical trial (Machens and Schmidt-Gollwitzer, 2003);see also (Colnet et al., 2020; Yang et al., 2020) and (Bareinboim and Pearl, 2013) for identification resultsfor the related setting of data fusion. The data setting is as follows. The observational dataset may have been collected under unobserved con-founders, Dobs = (X, U, T, Y ), but we only observe Dobs = (X, T, Y ). Hence unbiased causal estimation isnot possible from the observational dataset alone. On the other hand, we also have a randomized controlledstudy, Drct = (Xr, Ur, Tr, Yr). We summarize our assumptions about the sample size, and shift regimes ofobservational/randomized data below. The punchline is that multi-accuracy provides robustness againstthese potentially unknown shifts, under assumptions of well-specification of the test function class. (Allshifts are in the causal, rather than anti-causal setting).",
  "In the below setting, we aim to learn a valid CATE estimator E[Y (1)Y (0) | X] for the covariate distributionof the observational study or additional unknown covariate shifts": "Setting 2 (Observational and randomized study). Assume Assumption 2. Suppose an observational datasetDobs, collected under violations of Assumption 1 (the observational data were collected under unobservedconfounders), and a randomized dataset Drct, where Assumption 1 holds (the randomized data are uncon-founded). For covariate shift, consider likelihood ratio functions with respect to a target population Q. Againwe seek an estimator (X) with low bias under Q : |EQ[((X) (X))]| .",
  "t(x) arg ming(x)G EP [(Y g(X))2 | T = t], t {0, 1}": "Although many other advanced machine learning and causal inference methods have been developed basedon advanced estimating equations (Nie and Wager, 2020; Kennedy, 2023; Oprescu et al., 2019; Wager andAthey, 2018; Semenova and Chernozhukov, 2021), or other machine-learning adaptations (Shalit et al., 2017;Shi et al., 2019), we will first instantiate our post-processing method with the T-learner and describe howit can be used with more advanced methods based on pseudo-outcome regression (such as the DR-learner;Kennedy 2020; Semenova and Chernozhukov 2021). Our meta-algorithm is based on post-processing CATE estimation (the T- or DR-learner) with algorithmsfor multi-calibration. Next, we describe multi-calibration and its prior use for universal adaptability. Universal Adaptability via Multicalibration.Recent work of Kim et al. (2022) introduced the conceptof universal adaptability. Much work on inference under (external) covariate shift assumes that the shift isknown at the time of estimation. Instead, universal adaptability builds a prediction function from a sourcedataset whose average outcome estimate incurs small bias on any downstream covariate distribution, withina broad class of unknown shifts.The work of Kim et al. (2022) establishes the feasibility of universaladaptability via a connection to the notions of multi-calibration/multi-accuracy, originally introduced in theliterature on algorithmic fairness (Hbert-Johnson et al., 2018). Following this line of research, we show howmulti-accuracy can be used, off-the-shelf, to address unknown (external and internal) shifts in the contextof CATE. The multi-calibration criterion was originally motivated to provide guarantees over a variety of subpopu-lations, such as valid calibration over arbitrary subgroups (Hbert-Johnson et al., 2018). The related, butsomewhat weaker, notion of multi-accuracy ensures low prediction bias within arbitrary subgroups (Kimet al., 2019). Throughout this paper, we focus on multi-accuracy (although analogous results hold for thestronger criterion of multi-calibration).",
  "Definition 1 (Multi-accuracy). For c(X) in a set of functions C, a predictor p : X [B, B] is (C, )multi-accurate ifmaxcC | E[(Y p(X))c(X)]| ,": "Interpretations depend on the specification of the function class C. When C is a class of subgroup indicatorfunctions, C = {I[x C]: C C}, with C a set of subsets of X, then the multi-accuracy criterion ensureslow prediction bias over a rich set of subpopulations. The class C could indicate sublevel sets of functionswith a finite VC-dimension. For example, if C is the space of all decision trees of depth 4, it has a finiteVC-dimension and can describe complex subpopulations. A growing line of work has developed algorithms with guarantees to (approximately) satisfy multi-calibrationand multi-accuracy criteria (Hbert-Johnson et al., 2018; Kim et al., 2019; Gopalan et al., 2022b; Pfistereret al., 2021) via boosting. When initialized from scratch, multi-calibration/accuracy can be viewed as alearning algorithm, but it can also be used to post-process a given predictor, as we do in this paper. Ourmeta-algorithms leverage these existing algorithms for obtaining multi-accurate predictors by post-processing. Specifically, we use the MCBoost algorithm (Pfisterer et al., 2021), pseudocode included in Algorithm 4.MCBoost (Pfisterer et al., 2021) takes as input a given initial predictor p, test-function class C, approxima-tion parameter , and post-processing datasets Dpost for calibration and validation. For historical reasons,because the codebase was developed initially for multi-calibration which requires discretization of the in-terval , we standardize the prediction range, under assumption 3. This pre-processing is completelywithout loss of generality. Later on in our meta-algorithms, to be concise we will refer to this as runningMCBoost(p, C, , Dpost). As a brief summary, MCBoost is a boosting procedure that proceeds via a seriesof auditing steps: given the initial predictor p, it then solves a least-squares problem over the calibrationdataset to find a c C that maximizes correlation with the residuals. The process iterates until the totalmiscalibration or accuracy error drops below a stopping criterion. Next, we discuss the auditing step of thisprocedure in more detail.",
  "|E[(Y p(X))c(X)]|": "In practice, evaluation of the multi-calibration or multi-accuracy criterion over discrete subgroups is im-plemented via regression, reminiscent of twicing (Tukey et al., 1977).Twicing is a one-step boostingprocedure: given a predictor function p(x), fit the residual errors c arg mincC E[((Y p(X)) c(X))2]and return p(x) + c(x) as the final prediction. (In the conditional moment literature, these audit test func-tions would be called instrument functions). That is, the algorithm often audits over real-valued functionsc(x): X R. These can be connected to the subpopulation motivation by viewing c(x): X as arelaxation of indicator functions; and real-valued functions as a rescaling of the former. (Later on, we willrelate the real-valued weight functions directly to IPW weight functions (i.e. Riesz representers) in causalinference estimators). The MCBoost algorithm incorporates these worst-case subgroup error functions to improve the predictor.Given a predictor pk(x) at some iteration k of the algorithm, the auditing step learns a test function c thatbest correlates with the residual function pk(x) y. For auditing over regression prediction functions, forstep k, this is a regression problem: the auditing step solves mincC E[((Y pk(X))c(X))2]. The predictoris then updated with a multiplicative-weights update based on the worst-case test function c(x), that is,the next predictor pk+1(x) is upweighted or downweighted based on the predicted error c(x), pk+1(x) ecc(x)/2pk(x), where c is the multi-accuracy error computed on a validation set, EV [c(X)(Y pk(X))]. Therefore the MCBoost procedure adjusts the original predictor based on the predicted errors (c(x)),adjusting more strongly in regions of higher error, as well as if the predictor incurs higher error on average.Auditing and postprocessing occurs in a different held-out dataset: we will refer to this as the post-processingdataset, including both calibration and validation sets. If the multi-accuracy criterion is not met for this testfunction c C, the algorithm takes a boosting step and adds a multiplicative update with this test function.If the multi-accuracy criterion is met, the algorithm terminates. Remark 1 (Relation to conditional moment restrictions). A reader in causal inference or econometrics maynotice connections to conditional moment formulations. We expect that our later analysis, which is focusedon multi-calibration/accuracy algorithms, also hold for adversarial formulations of conditional moments.(For example, Greenfeld and Shalit (2020) observes that adversarial moment conditions, in their case HSICfor independence of residuals, imply robustness to covariate shift). Recent advances in machine learning forconditional moment equations (Dikkala et al., 2020; Bennett and Kallus, 2023; Ghassami et al., 2022) typi-cally develop min-max estimation algorithms that are unstable in practice; besides, the theory of conditionalmoment restrictions typically identifies finite-dimensional parameters rather than entire functions like theCATE. (See for more extensive discussion of related work.) Multi-accuracy vs.multi-calibration.Throughout this paper, we have focused on a weaker multi-accuracy criterion (Kim et al., 2019), which is nonetheless sufficient for the robustness guarantees we seekfor external validity and robustness to shifts. On the other hand, the stronger criterion of multi-calibration issomewhat more often studied in the literature. In this paper, we do not investigate the calibration propertiesof heterogeneous treatment effects, focusing instead on robustness to external shift. On the other hand, theseare not completely unrelated: with slightly different notions, Wald et al. (2021) studies connections betweenmulti-domain calibration and out-of-distribution generalization (without a focus on causal estimation). Otherpapers have investigated calibration for heterogeneous treatment effects (Xu and Yadlowsky, 2022), includingorthogonalized isotonic regression (Van Der Laan et al., 2023).",
  "Method": "In this section, we develop and analyze our meta-algorithm that uses multi-accurate post-processing on theblack-box regression models for a T-learner of the CATE. We first describe the meta-algorithm. To justifyits use, we start with basic results focusing on ATE estimation. First we show how regression adjustmentwith multi-accurate outcome models approximates an augmented doubly-robust estimator (AIPW), in theabsence of unobserved confounding. (This relates to estimation properties of our final approach). Second,we discuss the target-independent identification properties of ATE estimation, under the potential presenceof unobserved confounding, which relates to our guarantees under unknown covariate shifts. Identification implies we can write our target causal estimand in terms of functionals of the observationaldistribution alone.Estimating the regression-adjustment-identified functional with multi-accurate post-processing will result in certain equivalences with other estimation approaches, such as double robustness,with corresponding function classes. We establish results for the different settings we consider by combiningthese properties. Wethendiscussthespecificsettingsofunknowncovariateshifts(externalvalidity),observa-tional/randomized data, and CATE estimation therein. Finally, a natural question remains: we establishedthe properties of a multi-accurate T-learner, but what about more complex approaches? We provide exten-sions to recently studied pseudo-outcome regressions and prove approximate equivalence of a multi-accurateT-learner with a weaker multi-accurate DR-learner, suggesting that expanding the test function class cancapture some of the improvements of a more complex estimation target.",
  "E[(X)] = E[1(X) 0(X)].(3)": "This represents estimating the ATE by imputing potential outcomes via regression: hence its name regressionadjustment in the statistics literature, the direct method in the computer science literature, etc. Notably,the estimator does not explicitly use propensity scores. Nonetheless, we show how the robust test functionsin multi-accurate auditing can nonetheless approximate inverse propensity score functions.",
  "Warmup: Multicalibration, universal adaptability, and the ATE": "We introduce properties of multi-calibration/multi-accuracy algorithms for estimation of the CATE and ATE.We begin by establishing that regression adjustment estimation with multi-accurate outcome predictors canapproximate an AIPW estimator with certain nuisance functions. Later on, we combine these estimationproperties with robust identification of the ATE and CATE under external covariate shifts. Robust estimation of the ATE under unconfoundedness:regression adjustment with suffi-ciently post-processed multi-accurate outcome predictors = doubly-robust estimation.As awarm-up, we first consider estimation when unconfoundedness holds. We show in the causal context, thatmulti-calibration/multi-accuracy can be viewed as finding a boosted predictor whose marginalization satisfiesestimating equations for the average treatment effect. In addition, multi-calibration/multi-accuracy as analgorithmic scheme expands the functional complexity of the original predictor it is initialized with. Inter-estingly, regression adjustment with a multi-calibrated/multi-accurate predictor approximates the doubly-robust estimator for the ATE, and hence is consistent if either the original predictor is well-specified, theinverse propensity score is within the auditor function class, or if the prediction function is within theexpanded function class output by multi-calibration/multi-accuracy. The doubly-robust augmented inverse-propensity weighting estimator (AIPW) is a canonical estimator high-lighting improved estimation opportunities for causal inference (Robins et al., 1994). It has the following",
  "et(X) (Y t(X)) + t(X)(4)": "It enjoys improved estimation properties, such as the mixed-bias property (only requiring one of outcome orpropensity model to be consistent for consistent estimation of the ATE) or rate double-robustness. The approximation relies on conducting multi-accurate learning with an auditor function class containingthe inverse propensity score. As a note, the below statements hold up to an additional misspecification error(as shown in Kim et al. (2022)). Because the auditor function class is typically large (i.e. contains functionsbeyond the inverse propensity score), this is a robust way to conduct doubly-robust estimation. Proposition 1 (Multi-accuracy implies robust estimation of the ATE via regression adjustment). SupposeAssumptions 1 to 3.Consider an auditor class H that is closed under affine transformation.Assumeunconfoundedness holds.Consider the estimator E[(X)] where (x) is the output of Algorithm 1 withauditor class H, approximation parameter , initial outcome model estimators 1(x), 0(x), and Dpost fromthe same distribution as the data. If at least one of the following is true: (1) the original outcome models 1(x), 0(x) are consistent estimators,(2) e1(X)1, (1 e1(X))1 H, or (3) if using multi-accuracy, the true 1(x), 0(x) are in the linear spanof G + conv(H), thenE [(X)] = E [1(X) 0(X)] = E[Y (1) Y (0)] + 2,",
  "i.e. we obtain 2-consistent estimation of the ATE": "Although identification of the ATE with doubly-robust estimator is standard, note that our estimator thatuses multi-accurate outcome models does not explicitly estimate the propensity scores. Nonetheless, thenovelty of Prop. 1 is that averaging multi-accurate outcome models, because multi-accuracy implies E[(Y t(X))h(X)] , h H leads to direct approximation of a doubly-robust estimator with the multi-accurateregression-based estimator E [1(X) 0(X)], under conditions on how the test function class H relates toinverse propensity score functions. Because this is a novel perspective, we establish it for ATE estimationfirst before showing how it applies to the robust covariate shifts that motivate our method. This proposition connects the use of multi-accurate estimation to doubly-robust estimates and thereforeestablishes variance reduction properties, which is important because the multi-accuracy criterion itself ischaracterized via bias reduction on subgroups alone, without directly discussing the mean-squared error orestimation variance. Although the beneficial estimation properties of AIPW are well-established (Robins et al., 1994; Kennedy,2016), recall that our multi-accurate T-learner was not explicitly constructed as an AIPW estimator. Itis therefore an example of an outcome-regression based approach that nonetheless satisfies doubly-robustestimating equation properties, in the spirit of (Bang and Robins, 2005), but algorithmically different. Thisadditional perspective on how multi-accurate learning may robustly approximate a doubly-robust estimatoris new in our paper, beyond previous analysis of universal adaptability. It also relates in spirit to recent workstudying connections between regression adjustment and IPW methods, as (Chattopadhyay and Zubizarreta,2023) does for linear regression.We build on this characterization in Proposition 3 to relate our laterdevelopments to doubly-robust CATE estimators. Robust target-independent identification and estimation of the ATE under universal adapt-ability.Next we show how estimation with multi-accurate learning can robustly identify and estimate theATE under potential violations of unconfoundedness. This is a re-interpretation of universal adaptabil-ity in Kim et al. (2022) which studied missing data under unknown shifts, which directly implies robustidentification for causal inference.",
  "W 1 (X) := E[1/e1(X, U) | X]": "(Note that 1/e1 = E[1/e1(X, U) | X] due to Jensens inequality). Robust identification of the ATE followsfrom Equation (5), i.e. that H contains (approximately) E[1/e1(X, U) | X]. Essentially, assuming that theauditor function class contains the (unknown) identifying weight, we can re-interpret the multi-accuratecriterion as an approximation of adversarial IPW. We summarize this in the following corollary. Corollary 1. Suppose Assumptions 2 and 3. Suppose that W 1 (X), W 0 (X) H. Run Algorithm 1 up to()-multiaccuracy on D (possibly with unobserved confounders) over auditor function class H and outcomefunction class G to obtain (X) = 1(X) 0(X). Then",
  "The result follows from the multi-accuracy criterion, which implies that |E[I[T = t]W t (X) (Y t(X))]| ,which obtains identification as in eq. (5) and the triangle inequality": "Of course, we have not gained identification for free: we cannot verify the assumption that W 1 (x), W 0 (x) Hfrom observational data alone, just as we cannot test the unconfoundedness assumption from data alone.However, multi-accuracy/multi-calibration methods already work with quite flexible function classes, whichcould be nonparametric (RKHS, etc). This is how multi-accuracy confers general robustness to distribution shift, whether from the data generatingprocess such as unobserved confounders, or from external covariate shifts at the time of deployment.",
  "Identification under Setting 1The robust identification argument for universal adaptability re-interprets the test functions c(X) C as potential adversarial likelihood ratios for distribution shift": "However, the same properties of multi-accuracy also imply robustness to external shift. In this subsection,we indeed suppose Assumption 1, unconfoundedness. Recall that our goal was to control the predictive biason a target covariate distribution QX, potentially unknown, |EQ[((X) (X))]|. Note that each of 1, 0are learned on a treatment conditional distribution, so we have that the valid likelihood ratio, which wedenote wt(x), is defined as:",
  "et(x)(7)": "Obtaining robust identification for a universally adaptable CATE function instead interprets adversarialtest functions as a product function class F = C H for both the subpopulations that identify CATE,and the adversarial likelihood ratio function. Our next proposition gives conditions on the weight functionswt H, t {0, 1} to satisfy robust CATE estimation under unknown covariate shifts. Proposition 2. Suppose Assumptions 1 to 3. Let C denote a test function class for subgroup membershipand H a test function class for likelihood ratios. Run Algorithm 1 for -multi-accurate postprocessing ofthe outcome models of a T-learner estimate. Then, for all target covariate distributions Q such that thelikelihood ratios w1, w0 H,",
  ": Return (x) = 1(x) 0(x)": "Because the guarantee holds for all functions f F, it holds for complex subpopulations c(X) and vacuouslikelihood ratios with h(X) = 1, as well as the inverse: complex h(X) and vacuous subgroups (i.e. c(x) = 1).Our assumption is that F is sufficiently well-specified to cover the product of these relevant functions, but weare generally agnostic as to the precise complexity of its constituent classes C, H. And, in practice, followingthe algorithmic implementation of MCBoost, we work with auditor function classes such as ridge regression,rather than direct products of subpopulations and other test functions. Observe that although similar arguments apply, obtaining conditional guarantees for CATE estimation re-quires a richer test function class than for universal adaptability of the ATE alone. This illustrates that thecase of learning CATE is indeed statistically harder than that of universal adaptability of the ATE. ForCATE estimation, we need to choose a richer auditor function class than we would for ATE estimation.",
  "Observational and randomized data (Setting 2)": "(Meta)-Algorithm.In this setting, we learn confounded outcome regressions from the observationaldata. We use the smaller randomized controlled trial data as post-processing datasets in MCBoost (theboosting paradigm for multi-calibrated and multi-accurate predictors).In Algorithm 2 we describe themeta-algorithm. Identification and estimation of CATE.Identification and estimation for the CATE follows by in-terpreting the auditing functions c(X) C as subpopulations. Achieving multi-accuracy on the RCT datahence identifies the CATE. That is, multi-accuracy assures us that",
  "and we can evaluate this criterion on the unconfounded RCT data. On the unconfounded RCT data, weindeed have that E[Y | X, T = t] = E[Y (t) | X] so that the T-learner identifies CATE": "The intuition for why our meta-algorithm improves upon directly running the T-learner on the randomizeddata alone is that we can learn a low-variance, high-bias (due to unobserved confounding) estimate of the trueoutcome model E[Y (t) | X] by outcome modeling on the observational data to obtain Eobs[Y | T = 1, X].On the other hand, although randomized data is available, the finite-sample estimate of Erct[Y | T = 1, X]can be high-variance (though unbiased) under Assumption 4. We do note that the analysis of the boostingalgorithm in Hbert-Johnson et al. (2018) is not tight enough to provably show faster convergence fromwarm-starting on the confounded regressions on the observational data, relative to multi-calibrating on therandomized data alone. However, we show benefits in later experiments. Identification of target-independent CATE.In complete analogy to the external shift setting, chang-ing our interpretation of the target functions allows us to infer robustness to external shifts. Multi-accuracyensures that, for all target covariate distributions QX such that the likelihood ratios wt(x) H, t {0, 1},",
  "Extension to CATE pseudo-outcome regression": "A natural question given our work on the T-learner is whether we can provide similar guarantees for anestimation-improved CATE learner, since the T-learner generally does not enjoy any improved estimationproperties in causal inference. The causal inference and machine learning literature has developed manyimproved orthogonal/semiparametrically efficient procedures such as (but not limited to) the R-learner (Nieand Wager, 2020), DR-learner (Kennedy, 2023), or other machine-learning adaptations (Wager and Athey,2018; Shalit et al., 2017). Namely, some CATE estimation procedures give a pseudo-outcome (O; e, ), where O denotes data tuples,i.e. O = (X, T, Y ), such that E[(O; e, ) | X] = (X). (It is designated as a pseudo-outcome becauseregressing upon it identifies the CATE or functional of interest, although it is not exactly an outcome itself).One such pseudo-outcome is the doubly-robust score. Pseudo-outcome regression of it as a CATE estimatorwas recently studied in Semenova and Chernozhukov (2021); Kennedy (2020).",
  "Next, we instantiate such a procedure when the pseudo-outcome is the doubly-robust score": "Multi-accurate DR-learner.We give the algorithm for obtaining a multi-accurate DR-learner estimatein Algorithm 3. To summarize: we do need four folds of data (D1a, D1b, D2, D3); the first three for sample-splitting of the nuisance estimates and pseudo-outcome evaluation and the last for validation/calibration forMCBoost. Estimate the nuisance functions on the first two folds D1a, D1b and on D2, evaluate the pseudo-outcome value (O; e, ) and regress (x) = En[ (O; e, ) | X = x]. Finally, we conduct post-processing viamulti-accurate learning upon the DR-learner estimate , to obtain a multi-accurate . Again we will interpret the input auditor function class F = C H as a product function class of subgroupenvelope functions c C and likelihood ratios H. (Likelihood ratios are assumed to transport from themarginal distribution of X to the new distribution). Then, (robust) identification of the predictions followsexactly as in Proposition 2. Proposition 1 establishes that under specification assumptions, the multi-accurate regression adjustmentestimator is (robustly) equivalent to the doubly-robust estimator up to approximation error, connectingmulti-calibration with doubly-robust estimation. This implies basic (robust) doubly-robust properties of themulti-accurate T-learner. We now strengthen this connection by showing that multi-accurate post-processingof the T-learner over a richer function class (containing the true propensity score, and additional functions)implies that t is also a multi-accurate estimate of the DR-learner over the additional functions.",
  "by the triangle inequality and multi-accuracy of t over the richer function class": "The interpretation is that post-processing a simple T-learner for multi-accuracy over a richer (yet well-specified) function class can approximate a DR-learner that was post-processed for multi-accuracy over aweaker function class. The population criterion for multi-accuracy confers some nonparametric robustnessto bias over the specified test function class. Although this is a different estimation approach than causalmachine learning estimates, we relate them formally here, and investigate empirically and thoroughly in. So, although multi-accurate post-processing of a T-learner appears on its face as a basic CATEestimator, in fact, the judicious choice of a richer function class for post-processing can approximate a moreadvanced estimator. Interestingly, concurrent with the preparation of this work, Bruns-Smith et al. (2023) study augmentedbalancing weights and find a certain target-independent property of the augmented estimator related to theuniversal adaptability of Kim et al. (2022). Studying connections further would be an interesting directionfor future work.",
  "Experiments": "We previously provided identification arguments and meta-algorithms for leveraging multi-accurate learningto learn CATE subject to unknown covariate shifts. To be sure, modern estimation of CATE prescribesnonparametric estimation that, in the infinite-data limit, is immune to external covariate shifts if CATEestimation recovers the Bayes-optimal predictor. Of course, real-world datasets are often smaller so that ourmethods can improve robustness in finite samples. To illustrate this, we conduct extensive empirical studies,testing our proposed multi-calibrated CATE estimation algorithms in comparison to other CATE learners insimulation scenarios that follow the previously introduced settings unknown deployment shifts (Setting 1)and observational data with RCT (Setting 2).",
  "Simulations": "For both settings, we simulate data according to pre-specified propensity score functions, true outcomefunctions and external shift functions, with different degrees of complexity. For the external shift setting(simulation 1a and 1b), we assume access to training data from an observational study without unobservedconfounding and a small auditing sample from the same distribution. The test data used for evaluation,however, is externally shifted by deliberately sampling with weights given by the shift function (and differentshift intensities) from the original distribution. In this setting, we implement two simulations that differ in thecomplexity of the true CATE and propensity score functions (simulation 1a: linear CATE, beta confounding,simulation 1b: full linear CATE, logistic confounding). In the joint observational/RCT setting, we assumeaccess to both a large observational training data set and a small RCT; and an external shift betweenboth data sources. We implement two simulations in this setting that differ in whether both data sources(simulation 2a: confounded observational data and RCT) or only the observational training data (simulation2b: total shift between observational data and RCT) are affected by unobserved confounding. The test dataused for evaluation follows the covariate distribution of the observational training data. In simulation 2a,the problem is that of covariate shift alone; while in simulation 2b, the underlying conditional model (trueE[Y (t) | X] also changes. Simulation 2b illustrates the usefulness of the framework to simultaneously handlea variety of shifts.We present the simulation framework and comparisons to a broader set of baselinemethods in supplementary material (D.2). Methods.In both simulation settings, we use causal forests (CForest-OS) and random forest-based T-learner (T-learner-OS) and DR-learner (DR-learner-OS) trained in the observational training data as bench-mark methods. T-learner-OS and DR-learner-OS also serve as the input for post-processing with MCBoostusing ridge regression in the auditing data in simulation 1a and 1b (T-learner-MC-Ridge, DR-learner-MC-Ridge). In simulation 2a and 2b, post-processing is implemented with ridge regression-based auditing inthe RCT data. In these simulations, we present causal forests trained in the RCT data (CForest-CT) as anadditional baseline. To prevent overfitting with small auditing data, we regularized multi-accuracy boostingusing small learning rates and a limited fixed number of boosting iterations (see in D.2). Comparing to T-learner-OS establishes the robustness benefits of our methods. On the other hand, our do-no-harm property holds with respect to the MSE of the best-in-class T-learner. Comparing to CForest-OSallows us to assess robustness-efficiency tradeoffs. CForest-OS is a representative state-of-the-art methodthat leverages the causal structure and modifies the estimation procedure of random forests; it is a verystrong comparison point, but also very data-hungry. In contrast, our post-processing approach does notmodify the estimation procedure. An interesting direction for future work is to achieve the robust biasguarantees of multi-calibration with other variance-reduced CATE estimators. Results.We evaluate the outlined methods with respect to MSE of the estimated CATE in the test datain (external shift) and (observational data with RCT), over different sizes of initial trainingdatasets and different intensities of covariate shift. The results of simulation 1a (a) highlight howpost-processing robustifies the initial T-learner and consistently improves over T-learner-OS in scenarioswith moderate and strong external shift. When the observational training data is small, the multi-accurateT-learner also outperforms causal forests in these scenarios. With small training data, we see similar im-provements of the multi-accurate DR-learner over DR-learner-OS. As the training data size increases, thenaive DR-learner becomes more competitive and post-processing yields smaller gains. In simulation 1b (b), the more complex CATE function leads to higher MSE overall, while thepreviously observed pattern persists: The multi-accurate T-learner consistently improves over the naive T-learner, particularly under distribution shift. Our approach, DR-learner-MC-Ridge, is best in settings withstrong unknown external shift and small dataset size: it then outperforms both T-learner and causal forest.Larger dataset sizes permit estimation over richer function classes and methods become asymptoticallyequivalent. We compare our approach to additional baselines, including shift-reweighted causal forests, T-and DR-learner in the supplementary material D.2.2. a shows that in simulation 2a, learning from both the observational training data and a smallRCT via multi-accuracy boosting is beneficial across scenarios.The multi-accurate T-learner and DR-learner considerably improve over T-learner-OS and DR-learner-OS and in particular T-learner-MC-Ridgeis competitive with CForest-OS. The improvement from multi-accuracy boosting can also be observed whenpost-processing was conducted with externally shifted RCT data and is similarly prominent for both T- andDR-learner in the total shift setting (b). In both simulations, learning directly in the RCT data(CForest-CT) is only a viable option in the absence of a shift in the covariate distribution in the evaluationdistribution (i.e. when only deploying on the smaller RCT population), and can incur considerable errorotherwise. For results of additional CATE estimation techniques, see supplementary material D.2.3.",
  "WHI data application": "We next present a case study that draws on data from the Womens Health Initiative (WHI) studies (Machensand Schmidt-Gollwitzer, 2003). The WHI includes a large observational study and clinical trial data to in-vestigate the effectiveness of hormone replacement therapy (HRT) in preventing the onset of chronic diseases.As the observational study has been suspected to suffer from various (unobserved) confounding phenomena(Kallus and Zhou, 2018), we study how utilizing both data sources in combination via multi-accuracy boost-ing compares to learning CATE from the observational or clinical trial data only. We focus on the effect ofHRT treatment on systolic blood pressure and use age and ethnicity as covariates. Implementation detailsand results with an extended set of covariates is presented in the supplementary material D.3. Methods and Results.We subsample the observational data to train causal forests (CForest-OS) andinitial T-learner (T-learner-OS) and DR-learner (DR-learner-OS). We further sample from the clinical trialdata to create CT training data with different sample sizes to post-process the initial T- and DR-learner withMCBoost using ridge regression (T-learner-MC-Ridge, DR-learner-MC-Ridge). We also train causal forestssolely on the CT training data as an additional (strong) baseline (CForest-CT). Another sample from theCT data serves as the test set, with which we infer the (unobserved) true CATE using elastic net-basedR-learner (Nie and Wager, 2020) and estimate the ATE as evaluation benchmarks. We evaluate bias of the estimated ATE and MSE of the estimated CATE in . a showshow post-processing an initial T- and DR-learner with CT data can reduce bias, even if the auditing data issmall. We similarly see improvements in MSE when comparing the multi-accurate learner to T-learner-OSand DR-learner-OS in b. T-learner-MC-Ridge additionally improves over CForest-OS. Training onlyin the CT data leads to ATE estimates with low bias, but the MSE of CForest-CT is not competitive whenmodel training is based on CT data with small sample sizes. Further results are presented in supplementarymaterial D.3. no shiftmoderate shiftstrong shift Train size MSE (CATE) CForestOS TlearnerOS TlearnerMCRidge DRlearnerOS DRlearnerMCRidge",
  "Related work: further discussion": "A popular approach for handling unknown shifts is to enforce robustness against a family of covariate shifts(e.g. unknown shifts parametrized by unknown covariate shift functions) (Liu and Ziebart, 2014; Wen et al.,2014; Chen et al., 2016). The goal is to find a robust hypothesis that maximizes the worst-case predictionrisk (for example, squared error) evaluated with respect to unknown shifts within some class of covariateshifts. Parametrizations include distributionally robust optimization or linear basis functions. The workof Greenfeld and Shalit (2020) is motivated differently and penalizes with a Hilbert-Schmidt IndependenceCriterion loss; they show this implies some robustness to covariate shift. While most of this work is in thegeneric prediction setting, recent work also assesses ATE under covariate shift via distributionally robustoptimization (Subbaswamy et al., 2021), use of the marginal sensitivity model for external shifts (Hatt et al.,2021), or variational characterizations of coherent risk measures (Jeong and Namkoong, 2020). Methodolog-ically, some of this work is similar to work in causal inference that studies unobserved confounding under thelens of robust optimization adversarial likelihood ratios over some ambiguity set (Kallus et al., 2018a; Kallusand Zhou, 2021; 2020; Dorn et al., 2021; Zhao et al., 2019; Bruns-Smith and Zhou, 2023; Yadlowsky et al.,2018; Tan, 2006; Bruns-Smith and Zhou, 2023). This highlights the broad simultaneous interpretations ofadversarial weight functions for handling unobserved confounding (in the generation of the data) in additionto robust adversarial covariate shifts (in the deployment of the predictor). The approach based on multi-accuracy boosting, although it can be stated as a similar optimization problemin the abstract, differs from the previously mentioned works in a few important ways: (1) boosting couplesthe functional complexity of the post-processed predictor and the covariate shift function, and (2) underwell-specification of the auditor function class and other conditions, boostings asymptotic limit is the Bayes- no shiftmoderate shiftstrong shift Train size MSE (CATE) CForestOS TlearnerOS TlearnerMCRidge DRlearnerOS DRlearnerMCRidge CForestCT",
  ": Average absolute bias and MSE by clinical trial sample size in WHI data application": "optimal predictor, whereas robust optimization changes the asymptotic limit: typically to a coherent riskmeasure. In this sense, we expect that approaches based on multi-accuracy are less conservative withindistribution. To the best of our knowledge, the only prior discussion of connections between boosting-stylealgorithms and distributionally robust optimization is Blanchet et al. (2019). Approaches based on multi-calibration or multi-accuracy inherently couple the specification of the (expanded)hypothesis class of multi-calibrated predictors along with the specification of covariate shift functions, i.e.the boosting-type algorithm returns a predictor in the sum class of the original predictor and the classes ofshifts. Approaches for robust covariate shift, to reduce the complexity of the adversary, require additional moment constraints satisfied by valid likelihood ratios, i.e specifying a sharp set C of only valid covariateshifts. In robust optimization-based approaches to covariate shift, the hypothesis class and class of weightfunctions can be independently varied. But for multi-calibration, restricting the auditor function classesalso simultaneously reduces the functional complexity of the hypothesis class of predictors. Distributionallyrobust objectives are equivalent to variance regularization or control of the tail risk, which couples statisticallymore difficult control of tail behavior with the control of ambiguous shift functions. Another importantpoint of difference is that the Bayes-optimal predictor satisfies the multi-accuracy criterion, while a Bayes-optimal predictor with heteroskedastic noise may not satisfy desiderata of uniform performance implied bydistributional robustness. For example, (Duchi and Namkoong, 2018, Example 2) discusses the exampleof linear well-specified models where the distributionally robust predictor coincides with the Bayes-optimalpredictor; but in cases of model misspecification/heteroskedastic noise, this may not be the case. We leavea finer-grained comparison for alternative work. Our discussion of the hybrid observational and randomized setting is more to highlight an off-the-shelfapplication of multi-accuracy, rather than the tightest analysis in this setting. Other works use more structureof this hybrid setting, or more heavily modify algorithms (i.e. learning shared representations) (Hatt et al.,2021; Yang et al., 2020; Kallus et al., 2018b); analogous adaptations with multi-accuracy are interestingdirections for future work.See also Bareinboim and Pearl (2013) for a survey on transportability andexternal validity, Colnet et al. (2020) for a survey on learning from observational and randomized data. Inthis literature, Wu and Yang (2023; 2022) among others consider reweighting towards shifted distributions.Other approaches include Cheng et al. (2023).",
  "Conclusion": "In this work, we connect multi-accurate learning and conditional average treatment effect (CATE) estimationand show how off-the-shelf multi-accurate learning can be used for CATE estimation that is robust tounknown covariate shift. Although we empirically compare to more state of the art causal machine learning,these methods were designed for different purposes. Important directions for future work include best-of-both-worlds guarantees on both robustness and efficiency by improving variance reduction propertiesof Multi-CATE. A finer-grained analysis of the statistical implications of algorithmic implementations ofboosting could also be relevant, in addition to improving hyperparameter tuning in the causal setting. Inour work, we focus on establishing robustness properties.",
  "Xiangli Chen, Mathew Monfort, Anqi Liu, and Brian D Ziebart.Robust covariate shift regression.InArtificial Intelligence and Statistics, pages 12701279. PMLR, 2016": "Yuwen Cheng, Lili Wu, and Shu Yang. Enhancing treatment effect estimation: A model robust approachintegrating randomized experiments and external controls using the double penalty integration estimator.In Uncertainty in Artificial Intelligence, pages 381390. PMLR, 2023. Victor Chernozhukov, Mert Demirer, Esther Duflo, and Ivan Fernandez-Val. Generic machine learning infer-ence on heterogeneous treatment effects in randomized experiments, with an application to immunizationin india. Technical report, National Bureau of Economic Research, 2018. Bndicte Colnet, Imke Mayer, Guanhua Chen, Awa Dieng, Ruohong Li, Gal Varoquaux, Jean-PhilippeVert, Julie Josse, and Shu Yang. Causal inference methods for combining randomized trials and observa-tional studies: a review. arXiv preprint arXiv:2011.08047, 2020.",
  "Ghislaine Gayraud. Estimation of functionals of density support. Mathematical Methods of Statistics, 6(1):2646, 1997": "AmirEmad Ghassami, Andrew Ying, Ilya Shpitser, and Eric Tchetgen Tchetgen. Minimax kernel machinelearning for a class of doubly robust functionals with application to proximal causal inference. In Inter-national conference on artificial intelligence and statistics, pages 72107239. PMLR, 2022. Sharad Goel, Ravi Shroff, Jennifer Skeem, and Christopher Slobogin. The accuracy, equity, and jurisprudenceof criminal risk assessment. In Research handbook on big data law, pages 928. Edward Elgar Publishing,2021.",
  "Sookyo Jeong and Hongseok Namkoong. Robust causal inference under covariate shift via worst-case sub-population treatment effects. In Conference on Learning Theory, pages 20792084. PMLR, 2020": "Fredrik D Johansson, Uri Shalit, Nathan Kallus, and David Sontag. Generalization bounds and represen-tation learning for estimation of potential outcomes and causal effects.Journal of Machine LearningResearch, 23(166):150, 2022. Nathan Kallus and Angela Zhou.Confounding-robust policy improvement.In S. Bengio, H. Wallach,H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural InformationProcessing Systems, volume 31. Curran Associates, Inc., 2018. URL",
  "Edward H Kennedy. Towards optimal doubly robust estimation of heterogeneous causal effects. ElectronicJournal of Statistics, 17(2):30083049, 2023": "Michael P Kim, Amirata Ghorbani, and James Zou. Multiaccuracy: Black-box post-processing for fairnessin classification. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, pages247254, 2019. Michael P Kim, Christoph Kern, Shafi Goldwasser, Frauke Kreuter, and Omer Reingold. Universal adapt-ability: Target-independent inference that competes with propensity scoring. Proceedings of the NationalAcademy of Sciences, 119(4), 2022. John Krtner and Giuliano Bonoli. Predictive algorithms in the delivery of public employment services. InHandbook of Labour Market Policy in Advanced Democracies, pages 387398. Edward Elgar Publishing,2023.",
  "John Wilder Tukey et al. Exploratory data analysis, volume 2. Springer, 1977": "Lars Van Der Laan, Ernesto Ulloa-Prez, Marco Carone, and Alex Luedtke. Causal isotonic calibration forheterogeneous treatment effects. In International Conference on Machine Learning, pages 3483134854.PMLR, 2023. Stefan Wager and Susan Athey. Estimation and inference of heterogeneous treatment effects using randomforests. Journal of the American Statistical Association, 113(523):12281242, 2018. doi: 10.1080/01621459.2017.1319839.",
  "Yoav Wald, Amir Feder, Daniel Greenfeld, and Uri Shalit. On calibration and out-of-domain generalization.Advances in neural information processing systems, 34:22152227, 2021": "Junfeng Wen, Chun-Nam Yu, and Russell Greiner.Robust learning under uncertain test distributions:Relating covariate shift to model misspecification.In International Conference on Machine Learning,pages 631639. PMLR, 2014. Marvin N. Wright and Andreas Ziegler. ranger: A fast implementation of random forests for high dimensionaldata in C++ and R. Journal of Statistical Software, 77(1):117, 2017. doi: 10.18637/jss.v077.i01. Lili Wu and Shu Yang. Integrative r-learner of heterogeneous treatment effects combining experimental andobservational studies. In Conference on Causal Learning and Reasoning, pages 904926. PMLR, 2022.",
  "Yizhe Xu and Steve Yadlowsky. Calibration error for heterogeneous treatment effects. In InternationalConference on Artificial Intelligence and Statistics, pages 92809303. PMLR, 2022": "Steve Yadlowsky, Hongseok Namkoong, Sanjay Basu, John Duchi, and Lu Tian. Bounds on the conditionaland average treatment effect with unobserved confounding factors. arXiv preprint arXiv:1808.09521, 2018. Shu Yang, Chenyin Gao, Donglin Zeng, and Xiaofei Wang. Elastic integrative analysis of randomized trialand real-world data for treatment heterogeneity estimation. arXiv preprint arXiv:2005.10579, 2020. Qingyuan Zhao, Dylan S Small, and Bhaswar B Bhattacharya. Sensitivity analysis for inverse probabilityweighting estimators via the percentile bootstrap.Journal of the Royal Statistical Society: Series B(Statistical Methodology), 81(4):735761, 2019.",
  "Notation/ObjectDescription": "XCovariatesTTreatment, T 0, 1Y (T)Potential outcomesYObserved outcome, Y = Y (T)UUnobserved confounderset(x)Propensity score, P(T = t | X = x)t(x)Outcome regression, E[Y | X = x, T = t](x)Conditional average treatment effect (CATE), E[Y (1) Y (0) | X = x]CClass of subsets of XF, G, HFunction classesMulti-accuracy parameterDobsObservational datasetDrctRandomized controlled trial dataset(x)Estimated CATE function(x)Multi-accurate/calibrated CATE estimator",
  "BDetails on algorithms": "For completeness we describe the MCBoost algorithm for multi-calibration. See (Hbert-Johnson et al.,2018; Kim et al., 2019; 2022; Pfisterer et al., 2021) for more details, including theoretical analysis andimplementation details. We describe the algorithm for a generic (x, y) dataset (without reference to causalinference). See (Kim et al., 2019) for more details on the variant that achieves multi-accuracy (althoughideas at a high level are similar.) The key inputs include a regression algorithm for the boosting procedure, approximation parameter whichis a stopping condition (although in practice a finite limit on the number of iterations is used), and avalidation/calibration set. When developing methods for Setting 1 (unknown covariate shifts), the calibrationand validation set are drawn from the observational distribution. Our method for Setting 2 uses the (assumedsmall) RCT data as calibration/validations sets.",
  "Proof of Proposition 1. (1a) Suppose 1(x), 0(x) are consistent estimators and e H. Then Equation (5)immediately implies -consistency": "(1b) Suppose 1(x), 0(x) are consistent estimators but e / H. If 1(x), 0(x) are consistent, they willasymptotically satisfy the multi-calibrated or multi-accurate criterion. See Hbert-Johnson et al. (2018)for related do-no-harm properties in this setting. Let t = E[Y | X, T = t] denote the true conditionalexpectation; it satisfies t arg min E[(Y t(X))2 | T = t] and that E[Y t (X) | T = 1, X] = 0, a.s.Hence f(X) F, E[(Y t (X))f(X) | T = 1] = 0. Therefore t (X) is feasible.Since the additiveiterates of boosting approaches like MCBoost for multi-accuracy are commutative, (Gopalan et al., 2022a)characterizes multi-accuracy via a global optimization of squared loss over additive basis functions of H.Since t (X) is a optimal solution for the unconstrained problem, and feasible for the constrained problem,it is also optimal for the constrained problem.",
  "We provide code of the simulation studies and the real data application for replication purposes in thefollowing public OSF repository:": "Data preparations, model training and evaluation are conducted in R (3.6.3) (R Core Team, 2020) usingthe packages ranger (0.13.1) (Wright and Ziegler, 2017), grf (2.0.2) (Tibshirani et al., 2021) and rlearner(1.1.0) (Nie and Wager, 2020). The simulation studies heavily draw on the causal experiment simulator ofthe causalToolbox (0.0.2.000) (Knzel et al., 2019) package. In all experiments, (initial) T-learner and DR-learner are post-processed using the MCBoost algorithmas implemented in the mcboost (0.4.2) (Pfisterer et al., 2021) package. More concretely, we make use ofboosting for degree-2 multi-calibration, a (slightly) stronger notion than multi-accuracy, but computationallyless demanding than full multi-calibration Gopalan et al. (2022b). The hyperparameter settings used forpost-processing are listed as part of the following detailed presentation of the experiments ( and 13).",
  "and used to simulate externally shifted observational data Dosshift or shifted randomized controltrial (RCT) data, Drct (where e1(X) = 0.5), depending on the simulation scenario": "We vary the shift intensity s {0, 0.25, . . . , 2} and training set size {500, 2000, 3500, 5000}, and run experi-ments for each combination 25 times. The size of the (audit/RCT) data used for multi-calibration boosting(500 observations) and the (test) data used for model evaluation (5000 observations) is fixed. EvaluationWe compare and evaluate various techniques with respect to bias in ATE and MSE in CATEestimation. Bias is assessed based on the true ATE and the average of the estimated (x) in the test data.",
  "D.2.2External Shift": "In this initial setting, we simulate data that emulates an observational study with (observable) confounding.We additionally consider an external shift between the observational data that is available for initial modeltraining, Dos, and the distribution of the test (or deployment) data, Dosshift. We further assume access toan auditing sample from the original training distribution. The task is to estimate the true CATE functionas evaluated the shifted test set, using models that either learned in the observational training data only ormade additional use of the auditing data.",
  "EvaluationWe evaluate bias in ATE and MSE in CATE estimation in the externally shifted test data": "ResultsWe show the bias of the estimated average treatment effect (ATE) by shift intensity (columnpanels) and training set size (row panels) for each CATE estimation method in (see also which includes quantifications of the distribution shifts using KL divergence). The results show that inthe present setting all methods are able to produce unbiased estimates of the ATE in the non-shifted testdata (first column). Introducing an external shift (second and third column), however, incurs bias across allmethods with the shift-reweighted causal forest and shift-reweighted T-learner performing best. The ridgeregression-based multi-accurate DR- and T-learner perform best among the shift-blind methods that had noaccess to the shifted test distribution. We show the corresponding results for the MSE of the CATE estimation by shift intensity and trainingset size in (and ). In the present setting, causal forest achieve the smallest MSE in thenon-shifted test data as well as in settings with large initial training data (first column and third and lastrow). With increasing shift, however, ridge-based multi-accurate T-learner perform best in settings withsmall to moderately sized training data (upper right quadrant). no shiftmoderate shiftstrong shift 151050510 151050510 151050510 Bias (ATE) CForestOS CForestwOS SlearnerOS SlearnerwOS DRlearnerOS DRlearnerMCRidge DRlearnerMCTree TlearnerOS TlearnerwOS TlearnerMCRidge TlearnerMCTree : Bias of ATE estimation by shift intensity and training set size for different CATE estimationmethods (Simulation 1a (external shift, linear CATE, beta confounding)). The distribution of bias scoresover simulation runs is shown. Given an external shift between training and test data, DR-learner-MC-Ridgeand T-learner-MC-Ridge perform best among the shift-blind methods that had no access to the shifted targetdistribution. no shiftmoderate shiftstrong shift MSE (CATE) CForestOS CForestwOS SlearnerOS SlearnerwOS DRlearnerOS DRlearnerMCRidge DRlearnerMCTree TlearnerOS TlearnerwOS TlearnerMCRidge TlearnerMCTree : MSE of CATE estimation by shift intensity and training set size for different estimation methods(Simulation 1a (external shift, linear CATE, beta confounding)).The distribution of MSE scores oversimulation runs is shown. T-learner-MC-Ridge performs best in settings with small to moderately sizedtraining data and shifted test data.",
  "EvaluationBias in ATE and MSE in CATE estimation is evaluated in the externally shifted test data": "ResultsThe results for bias of the ATE estimation in (and including KL divergence) showthat in absence of external shift (first column), causal forest-based estimators perform best and are able toachieve unbiasedness. Introducing an external shift between the observational training and test data (secondand third columns) amplifies bias such that only the shift-reweighted causal forest is able to approximate thetrue ATE on average, given sufficient training data. The ridge regression-based multi-accurate DR-learnerimprove over the initial DR-learner and are competitive with shift-reweighted causal forest in settings withsmall initial training data and strong shift. Again note that, in contrast to the shift-reweighted methods,the multi-accurate learner had no access to the shifted test distribution during model training. (and ) shows results for the MSE of the estimated CATE. The ridge-based multi-accurateDR-learner consistently improve over the initial DR-learner and achieve the lowest MSE among all methodsin the initial, non-shifted setting. With increasing shift, ridge-based multi-accurate and shift-reweightedlearner perform well in settings with small to moderately sized training data. Causal forest is competitivein all settings and particularly as the training set size increases. no shiftmoderate shiftstrong shift 2010010 2010010 2010010 Bias (ATE) CForestOS CForestwOS SlearnerOS SlearnerwOS DRlearnerOS DRlearnerMCRidge DRlearnerMCTree TlearnerOS TlearnerwOS TlearnerMCRidge TlearnerMCTree : Bias of ATE estimation by shift intensity and training set size for different CATE estimationmethods (Simulation 1b (external shift, full linear CATE, logistic confounding)). The distribution of biasscores over simulation runs is shown. Given an external shift between training and test data, DR-learner-MC-Ridge performs best among the shift-blind methods, particularly in settings with small initial trainingdata. no shiftmoderate shiftstrong shift MSE (CATE) CForestOS CForestwOS SlearnerOS SlearnerwOS DRlearnerOS DRlearnerMCRidge DRlearnerMCTree TlearnerOS TlearnerwOS TlearnerMCRidge TlearnerMCTree : MSE of CATE estimation by shift intensity and training set size for different estimation methods(Simulation 1b (external shift, full linear CATE, logistic confounding)). The distribution of MSE scoresover simulation runs is shown. DR-learner-MC-Ridge and T-learner-MC-Ridge consistently improve overDR-learner-OS and T-learner-OS. DR-learner-MC-Ridge performs best overall in settings with small tomoderately sized training data.",
  "D.2.3Observational study and RCT": "We simulate a setting in which we have access to training data from an observational study (OS) and froma small randomized control trial (RCT). We consider a covariate/ external shift between the observationalstudy, Dos, and the RCT, Drct. We further assume unobserved confounding either in both data sources(D.2.3) or in the observational training data only (D.2.3). The task is to estimate the true CATE usingmodels that learned either in the observational (training) data or in the RCT, or by using both data sourcesin combination.",
  "(T-learner-wCT) T-learner using random forest trained in the shift-reweighted randomized controltrial": "EvaluationWe evaluate bias in ATE and MSE in CATE estimation on a test set drawn from the ob-servational data. In calculating the true ATE and (x), we marginalize over U and compute E[Yi(1)|X] =Yi(1) + 3 E[U|Xi] and E[Yi(0)|X] = Yi(0) E[U|Xi]. ResultsWe plot the bias of the estimated ATE for each method by shift intensity (column panels) andtraining set size (row panels) in (see also including KL divergence). In the absence ofcovariate shift (first column), naive learning in the observational data results in biased estimates of the ATE.Utilizing both data sources in combination via multi-calibration boosting allows to improve over the initialDR- and T-learner.Introducing a covariate shift between the observational data and the RCT (secondcolumn) degenerates the performance of the RCT-based estimators and the best results are achieved bymulti-accurate DR- and T-learner, especially for strong shifts (third column). Results for the MSE of the estimated CATE are shown in (). In the absence of covariateshift (first column), the RCT-based estimators outperform the estimators that learned from the observationaldata. Introducing a shift between the observational study and the RCT (second and third column) increasesthe MSE of the RCT-based learners considerably such that the best results can now be observed for the tree-based multi-accurate T-learner, followed by the ridge regression-based multi-accurate T-learner and causalforests learned in the observational data. no shiftmoderate shiftstrong shift 201001020 201001020 201001020 Bias (ATE) CForestOS SlearnerOS DRlearnerOS DRlearnerMCRidge DRlearnerMCTree TlearnerOS TlearnerMCRidge TlearnerMCTree CForestCT CForestwCT SlearnerCT SlearnerwCT DRlearnerCT TlearnerCT TlearnerwCT : Bias of ATE estimation by shift intensity and training set size for different CATE estimationmethods (Simulation 2a (confounded observational data and RCT)). The distribution of bias scores oversimulation runs is plotted. Given moderate to strong covariate shift between the observational data andRCT, multi-accurate learner achieve the best results. no shiftmoderate shiftstrong shift MSE (CATE) CForestOS SlearnerOS DRlearnerOS DRlearnerMCRidge DRlearnerMCTree TlearnerOS TlearnerMCRidge TlearnerMCTree CForestCT CForestwCT SlearnerCT SlearnerwCT DRlearnerCT TlearnerCT TlearnerwCT : MSE of CATE estimation by shift intensity and training set size for different estimation methods(Simulation 2a (confounded observational data and RCT)). The distribution of MSE scores over simulationruns is shown. T-learner-MC-Tree outperform other methods in settings with shifted RCT data.",
  "P(Yobs = y | X, A) = P(Yrct = y | X, A), y": "The difference between Assumption 6 and Assumption 5 is whether we allow the marginal distribution of Uto shift. Assumption 5 is a conditional model invariance assumption between the data-generating processand the RCT. A sufficient condition for this to hold is that P(Uobs) = P(Urct) and the invariant conditionalprobability assumption above. On the other hand, the total shift of Assumption 6 could arise from shifts inthe distribution of U. Both of these are additional covariate shifts.",
  "+ e(2(x20.5)+(x30.5))": "EvaluationWe evaluate bias in ATE and MSE in CATE estimation on a test set that follows the covariatedistribution of the observational data, Dos. However, in constructing the true ATE and (x) we use 0(x)and 1(x) as specified in the RCT, i.e. without unobserved confounders. ResultsThe bias of the estimated ATE for each method by shift intensity (column panels) and trainingset size (row panels) is presented in (and including KL divergence).The first setof results (first column) indicate that under unobserved confounding in the observational data only andwithout external covariate shift, RCT-based estimators are, as expected, unbiased. The multi-accurate DR-and T-learner that draw on both data sources are able to reduce the bias of the naive DR- and T-learner. Asthe external shift between the observational data and the RCT increases (second and third column), learningonly on the RCT incurs bias and shift-reweighted methods as well as (tree-based) multi-accurate DR- andT-learner achieve the best results. The results for the MSE of the CATE are shown in (). Similar to the results for bias, theRCT-based estimators perform best under the no external covariate shift setting (first column). Among theestimators based on the observational data, the ridge- and tree-based multi-accurate T-learner and causalforests perform best. Tree-based post-processing performs best among all methods in scenarios with strongcovariate shift (third column). no shiftmoderate shiftstrong shift 201001020 201001020 201001020 Bias (ATE) CForestOS SlearnerOS DRlearnerOS DRlearnerMCRidge DRlearnerMCTree TlearnerOS TlearnerMCRidge TlearnerMCTree CForestCT CForestwCT SlearnerCT SlearnerwCT DRlearnerCT TlearnerCT TlearnerwCT : Bias of ATE estimation by shift intensity and training set size for different CATE estimationmethods (Simulation 2b (total shift between observational data and RCT)). The distribution of bias scoresover simulation runs is shown. As the external shift between the observational data and the RCT increases,multi-accurate DR-learner and T-learner-MC-Tree are competitive with shift-reweighted learning. no shiftmoderate shiftstrong shift MSE (CATE) CForestOS SlearnerOS DRlearnerOS DRlearnerMCRidge DRlearnerMCTree TlearnerOS TlearnerMCRidge TlearnerMCTree CForestCT CForestwCT SlearnerCT SlearnerwCT DRlearnerCT TlearnerCT TlearnerwCT : MSE of CATE estimation by shift intensity and training set size for different estimation methods(Simulation 2b (total shift between observational data and RCT)). The distribution of MSE scores oversimulation runs is shown. T-learner-MC-Tree performs best among all methods in scenarios with strongcovariate shift. : Bias of ATE estimation by shift intensity and training set size for different CATE estimationmethods, averaged over simulation runs (Simulation 1a (external shift, linear CATE, beta confounding)).For each setting, method achieving best performance printed in bold (second best in italic).",
  "D.3WHI Data Application": "DataWe consider a case study using clinical trial and observational data from the Womens Health Ini-tiative (Machens and Schmidt-Gollwitzer, 2003). A focus of this study was to investigate the effectiveness ofhormone replacement therapy (HRT) treatment in preventing the onset of chronic (cardiovascular) diseases.As the observational study and clinical trial data led to conflicting findings, the WHI study has become aprime example of how confounding in observational data can introduce bias and, in this case, suggest overlyoptimistic results (for more detail, see Kallus and Zhou 2018). In this setting, we study how multi-accurateCATE estimators that are warm-started with observational data and have access to small samples fromthe clinical trial compare to estimators that draw on either observational or clinical trial data only. We aim to assess the effect of HRT treatment on systolic blood pressure as a major risk factor for cardio-vascular diseases. We estimate the CATE with respect to two sets of covariates a small (age, ethnicity)and an extended set (age, ethnicity, number of cigarettes per day, systolic blood pressure baseline, diastolicblood pressure baseline, BMI baseline; see ). In our application setting, we start with the observational study (OS) (52,335 observations) and draw arandom 50% sample that serves as observational training data for (naive) CATE estimation. We split theclinical trial data (14,531 observations) into an initial 50% training set and a 50% test set.The initialtraining set is used to draw further random samples of size {250, 500, 750, 1000, 1250, 1500} that serve asclinical trial (CT) training data. For each CT training set size, sampling is repeated 25 times.",
  "(T-learner-OS) T-learner using regression forest to learn separate outcome models for treated anduntreated in the training set of the observational data": "We estimate DR-learner and T-learner using multi-calibration boosting with samples of clinical trial data.The MCBoost hyperparameter settings are shown in .Hyperparameters of the baseline CATElearner are listed in . (DR-learner-MC-Ridge) DR-learner using regression forest in the training set of the observationaldata is post-processed with MCBoost using with ridge regression in the training set of the clinicaltrial data. (DR-learner-MC-Tree) DR-learner using regression forest in the training set of the observationaldata is post-processed with MCBoost using with decision trees in the training set of the clinical trialdata. (T-learner-MC-Ridge) T-learner using regression forest in the training set of the observationaldata is post-processed with MCBoost using with ridge regression in the training set of the clinicaltrial data. (T-learner-MC-Tree) T-learner using regression forest in the training set of the observational datais post-processed with MCBoost using with decision trees in the training set of the clinical trial data.",
  "(x)": "In evaluating MSE, we use the estimated CATE function, (x), based on learners that had privileged accessto the clinical trial test data (XRF, RL, TL) as a substitute for the true (x) and evaluate against (x) ofthe CATE estimation methods outlined above (using the observational and/or clinical trial training dataonly).",
  "( (x) (x))2": "Resultsa (small set of covariates) and b (extended set) show the bias of the estimatedATE for each method by clinical trial training set size. As expected, learning in the clinical trial trainingdata allows for unbiased estimation of the ATE as shown by the three CT-based methods in both settings.These estimates, however, come with high variability if the CT training data is small. Learning solely in theobservational data incurs bias in ATE estimation, particularly in settings where the CATE learner only haveaccess to a small set of covariates (a). In this case, post-processing with clinical trial data improvesupon the initial T-learner. Given an extended set of covariates the bias of the observational data-basedmethods decreases and post-processing is less effective (b). We evaluate the MSE of the estimated CATE in (small set of covariates) and (extendedset) by clinical trial training set size against the three approximations of the true CATE that are based onthe clinical trial test data. The observational data-based methods generally outperform the CT-based CATEestimates, indicating that the small clinical trial training sets on their own are not sufficient for accurateCATE estimation (comparing a to 14b). Post-processing the initial T-learner via multi-calibrationboosting with clinical trial data allows to achieve the smallest MSE for most CT training set sizes and trueCATE estimation techniques in the limited covariate setting (a). As the observational data-basedCATE learner achieve low MSE with the extended set of covariates, post-processing shows no improvementin this case (a)."
}