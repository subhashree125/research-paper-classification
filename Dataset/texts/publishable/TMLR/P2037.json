{
  "Abstract": "Building a single generalist agent with strong zero-shot capability has recently sparked significantadvancements. However, extending this capability to multi-agent decision making scenarios presentschallenges. Most current works struggle with zero-shot transfer, due to two challenges particular tothe multi-agent settings: (a) a mismatch between centralized training and decentralized execution;and (b) difficulties in creating generalizable representations across diverse tasks due to varying agentnumbers and action spaces. To overcome these challenges, we propose a Mask-Based collaborativelearning framework for Multi-Agent decision making (MaskMA). Firstly, we randomly mask partof the units and collaboratively learn the policies of unmasked units to handle the mismatch. Inaddition, MaskMA integrates a generalizable action representation by dividing the action space intointrinsic actions solely related to the unit itself and interactive actions involving interactions withother units. This flexibility allows MaskMA to tackle tasks with varying agent numbers and thusdifferent action spaces. Extensive experiments in SMAC reveal MaskMA, with a single model trainedon 11 training maps, can achieve an impressive 77.8% average zero-shot win rate on 60 unseen testmaps by decentralized execution, while also performing effectively on other types of downstreamtasks (e.g., varied policies collaboration, ally malfunction, and ad hoc team play).",
  "Introduction": "The powerful transformer-based (Vaswani et al., 2017) generalist models (Ouyang et al., 2022; Touvron et al., 2023;Brown et al., 2020; Ramesh et al., 2022) have brought artificial intelligence techniques to the daily life of people.The reinforcement learning community has also witnessed a growing amount of successes in transformer-basedmodels (Chen et al., 2021; Carroll et al., 2022; Liu et al., 2022; Janner et al., 2022; Meng et al., 2023). However, mostof these existing generalist models either fail to complete unseen tasks effectively or fail to incorporate the multi-agentreality of decision-making. A natural follow-up question is: how can one build a generalist model for multi-agentdecision-making that is capable of zero-shot transfer to new tasks (e.g., different maps and varying numbers of agentsin SMAC (Samvelyan et al., 2019)). Compared to single-agent scenarios, directly utilizing transformers for centralized training in multi-agent settingsencounters two primary challenges. (a) A mismatch between centralized training and decentralized execution. Multi-agent decision-making typically follows centralized training with a decentralized execution (Gronauer & Diepold,2022) approach. Most existing methods (Wen et al., 2022b; Tseng et al., 2022b) treat multi-agent decision-makingas a sequence modeling problem and directly employ transformer architectures. However, the centralized trainingphase of transformers utilizing all units as inputs mismatches with the decentralized execution phase where eachagents perception is limited to only nearby units. This mismatch significantly reduces performance. (b) Generalization",
  "Published in Transactions on Machine Learning Research (09/2024)": "Muning Wen, Jakub Kuba, Runji Lin, Weinan Zhang, Ying Wen, Jun Wang, and Yaodong Yang.Multi-agentreinforcement learning is a sequence modeling problem. Advances in Neural Information Processing Systems, 35:1650916521, 2022b. Jiachen Yang, Tarik Dzanic, Brenden Petersen, Jun Kudo, Ketan Mittal, Vladimir Tomov, Jean-Sylvain Camier, TuoZhao, Hongyuan Zha, Tzanio Kolev, et al. Reinforcement learning for adaptive mesh refinement. In InternationalConference on Artificial Intelligence and Statistics, pp. 59976014. PMLR, 2023. Fuxiang Zhang, Chengxing Jia, Yi-Chen Li, Lei Yuan, Yang Yu, and Zongzhang Zhang. Discovering generalizablemulti-agent coordination skills from multi-task offline data. In The Eleventh International Conference on LearningRepresentations, 2023.",
  "Win Rate": "Training MapsTesting Maps MaskMAMADT : Win rate on training and testing maps. The blue line separates the 11 training maps on the left from the 60 testing mapson the right, where the performance on the testing maps is the zero-shot performance. The orange line demonstrates the substantialperformance advantage of MaskMA over MADT (Multi-Agent Decision Transformer (Meng et al., 2023)). problems caused by varying numbers of agents and actions. Downstream tasks have different numbers of agents,resulting in varying action spaces. Meng et al. (2023) takes a large action space and mutes the unavailable actions usingan action mask to handle the variable action spaces. However, this method suffers from poor generalization because thesame component of the action vector represents different physical meanings in tasks with different numbers of agents. To address these challenges, we propose introduce two scalable techniques: a Mask-based Training Strategy (MTS) anda Generalizable Action Representation (GAR). The two techniques form the basis of a new mask-based collaborativelearning framework for multi-agent decision-making, named MaskMA. To address the mismatch challenge, weincorporate random masking into the attention matrix of the transformer, effectively reconciling the discrepancy betweencentralized training and partial observations in decentralized execution, thus bolstering the models generalizationcapabilities. To handle the generalization challenge, MaskMA integrates GAR by categorizing actions into two types:intrinsic actions, which are solely related to the unit itself, and interactive actions, which involve interactions with otherunits. GAR predicts one units interactive action from its own features together with other units features, instead ofonly relying on its own features, allowing MaskMA to generalize to diverse tasks with varying agent numbers andaction spaces. We evaluate MaskMAs performance using the StarCraft Multi-Agent Challenge (SMAC) benchmark. To validate thezero-shot performance, we evaluate the win rate in a challenging setting, using only 11 maps for training and 60 unseenmaps for testing. Extensive experiments demonstrate that our model significantly outperforms the previous state-of-the-art methods in zero-shot scenarios. We also introduce various downstream tasks to further verify the strong robustgeneralization ability of MaskMA, including varied policies collaboration, ally malfunction, and ad hoc team play.MaskMA is the first approach that achieves strong notable zero-shot capability for multi-agent decision-making (e.g.,78% win rate in SMAC). We hope this work lays the groundwork for further advancements in multi-agent fundamentalmodels, with potential applications across a wide range of domains. Our main contributions are summarized as threefolds: We introduce a novel mask-based collaborative learning framework, MaskMA, for multi-agent decision-making. Thisframework pretrains a transformer architecture with a Mask-based Training Strategy (MTS) and a GeneralizableAction Representation (GAR). We set up a challenging zero-shot setting for general models in multi-agent scenarios based on the SMAC, i.e.,training on only 11 maps and testing on 60 different maps and three downstream tasks including varied policiescollaboration, ally malfunction, and ad hoc team play. To our best knowledge, MaskMA is the first general model for multi-agent decision-making with notable zero-shotperformance. With only 11 training maps, our MaskMA has achieved an impressive average zero-shot win rate of77.8% on 60 unseen test maps, representing a 78% improvement relative to the baseline method.",
  "Related Work": "Masked Training in Single-agent Decision Making.DT (Chen et al., 2021) casts the reinforcement learning as asequence modeling problem conditioned on return-to-go, using a transformer to generate optimal action. MaskDP (Liuet al., 2022) utilizes autoencoders on state-action trajectories, learning the environments dynamics by masking andreconstructing states and actions. Uni[MASK] (Carroll et al., 2022) expresses various tasks as distinct masking schemesin sequence modeling, using a single model trained with randomly sampled maskings. In this paper, we explore thedesign of sequences in multi-agent decision making and how it can be made compatible with the mask-based trainingstrategy. Multi-agent Decision Making as Sequence Modeling.MADT (Meng et al., 2023) introduces Decision Trans-former (Chen et al., 2021) into multi-agent reinforcement learning, significantly improving sample efficiency andachieving strong performance in SMAC. MAT (Wen et al., 2022a) leverages an encoder-decoder architecture, incorpo-rating the multi-agent advantage decomposition theorem to reduce the joint policy search problem into a sequentialdecision-making process. Tseng et al. (2022a) utilize the Transformer architecture and propose a method that identifiesand recombines optimal behaviors through a teacher policy. ODIS (Zhang et al., 2023) trains a state encoder andan action decoder to extract task-invariant coordination skills from offline multi-task data. In contrast, our proposedMaskMA adapts the Transformer architecture to multi-agent decision making by designing a sequence of inputs andoutputs for a generalizable action representation. This approach offers broad generalizability across varying actions andvarious downstream tasks. Action Representation.Recent works have explored semantic action in decision-making, especially in multi-agentenvironments. ASN (Wang et al., 2020) focuses on modeling the effects of actions by encoding the semantics of actionsto understand the consequences of agent actions and improve coordination among agents. UPDeT (Hu et al., 2021)employs a policy decoupling mechanism that separates the learning of local policies for individual agents from thecoordination among agents using transformers. Yang et al. (2023) proposes a graph network to handle variable actionspaces, applying reinforcement learning to improve finite element (Brenner, 2008) methods. Our MaskMA emphasizessequence modeling, masking strategies, and generalizable action representation, which is similar to Yang et al. (2023),to train a generalist model in the multi-agent field. This model can be applied to unseen maps and a wide range ofdownstream tasks, involving varied policy collaboration, ally malfunction, and ad hoc team play.",
  "Multi-Agent Preliminaries": "In multi-agent scenarios, states are only partially observable, i.e., each agent has only limited sight and can only observepart of other units (e.g., enemies). Therefore, a cooperative multi-agent task is generally defined as a decentralizedpartially observable Markov decision process (Oliehoek & Amato, 2015), denoted as G =< S, U, A, P, O, r, >.Here S is the global state space, and U {ui}Ni=1 is the set of N units, where the first M units are controllableand the rest N M units are uncontrollable, often subsumed as part of the environment. A = Mi=1 Ai is theaction space for controllable units. At time step t, each agent ui {ui}Mi=1 selects an action ai Ai, forming ajoint action a A. The joint action a at state s S triggers a transition of G, subject to the transition functionP (s | s, a) : S A S . Meanwhile, a shared reward function r (s, a) : S A R is employed, with denoting the discount factor. The global state at the t-th time step is defined as st = (st1, st2, ..., stN), where each sti exclusively represents the state ofunit ui, and does not include any information about other units. The local observation oti for each unit ui is composedof the states of all units visible to it at the t-th step, expressed as oti = {sti | i pti}, where pti indicates the indexes of",
  "+1": ": MaskMA employs the transformer architecture combined with generalizable action representation and then trained througha mask-based training strategy. It effectively generalizes skills and knowledge from training maps into various downstream tasks,including unseen maps, varied policies collaboration, ally malfunction, and ad hoc team play. units within uis observation range. Actions are categorized into two types: intrinsic actions, which are solely related tothe unit itself, and interactive actions, which involve interactions with other units.",
  "Multi-Task Imitation Learning": "Since agents have to learn from a broad range of tasks before they can show good transfer to new tasks, we formulate theproblem as multi-task imitation learning. Formally, we assume access to a dataset of passively logged expert trajectoriesD = {i = {(st, at)}Tt=1} from some training tasks: T p(T ), where p(T ) is the task distribution. After imitatingthe expert demonstrations through supervised learning from D, we then test the agents performance on new tasksdrawn from p(T ). However, vanilla-supervised learning fails to produce good transfer in the multi-agent settings due tothe mismatch between centralized pretraining and decentralized execution as well as the fact that the varying number ofagents may lead to completely new observation and action space that agents have never seen before. Then, we presentour strategy to solve these issues.",
  "Mask-Based Training Strategy": "To handle the mismatch between centralized training and decentralized execution, we propose to randomly mask someparts of the units in the environment and collaboratively learn the execution policies of other unmasked units. We utilize a standard causal transformer with only encoder layers as our model backbone, with the context length equalto L. At the t-th timestep, the input comprises the recent L global states of N agents, expressed as (stL+1, , st),resulting in total L N tokens, including all agents in the entire sequence, both controllable and uncontrollable. Thenwe harness the capabilities of the attention matrix to realize mask-based collaborative learning. The shape of thesemask matrices is (LN LN), corresponding to L N input tokens. During the training phase, we meticulouslycraft the attention matrix m1 to exhibit a causal structure along the timestep dimension, while maintaining a non-causal configuration within each discrete timestep. This design is illustrated in the left part of . Based on apredetermined mask ratio, we then generate the final attention matrix m2 for training (as depicted in the right part of) through a process of random sampling, wherein specific elements are set to zero. For evaluation, we can efficiently shift between centralized and decentralized execution by adjusting the attention maskmatrix. For decentralized execution, we modify the attention matrix to ensure that each agent focuses solely on thesurrounding agents during the self-attention process. In contrast, for centralized execution, we simply set the attentionmatrix to be equivalent to m1.",
  "Generalizable Action Representation": "As agents involve interactive actions among other units in most multi-agent tasks (e.g., the healing allies and attackingenemies in SMAC), their action space grows with the number of units. To address the challenge posed by variablenumbers of agents, we introduce the concept of Generalizable Action Representation (GAR), which is similar to Yanget al. (2023). illustrates the process through which the final action ati of a specific agent is determined by GAR.",
  "Mask": ": Visualization of the attention matrix in MTS. Left: The attention matrix displays a causal structure along the timestepdimension, complemented by a non-causal configuration within each discrete timestep. Right: The final attention matrix used fortraining is obtained by randomly masking elements of the left attention matrix. Consider an interaction between the executor unit ui and the receiver unit uj, we define the embedding of this interactionas hti,j = hti htj, where hti and htj are the output embedding of ui and uj from the transformer encoder respectively, and denotes the concatenation operation. For interactive actions, we compute the logits asFC(hti,j)Nj=1, wherethe fully connected layer FC has an output shape of one. In contrast, the logits for intrinsic actions are obtained usingFC(hti), with the output shape of FC corresponding to the number of intrinsic actions. These logits, encompassingboth interactive and intrinsic actions, are then combined and input into a softmax function to determine the final action.Please refer to the pseudocode in the Appendix A.4 for the practical implementation.",
  "j=1CE(Aij, Pij),": "where CE(a, p) is the cross-entropy between the ground truth action and the predicted action probability distribution.Here, M is the number of controllable agents, and L represents the context length. Note that for all agents in an entireinput L N, we only consider the loss for the M controllable agents.",
  "Experiments": "In this section, we design experiments to evaluate the following features of MaskMA. (1) Zero-shot Ability. We conductexperiments on SMAC using only 11 maps for training and up to 60 maps for testing, assessing the models abilityto generalize to unseen scenarios. In SMAC tasks, agents must adeptly execute a set of skills such as alternating fire,kiting, focus fire, and positioning to secure victory. These attributes make zero-shot transfer profoundly challenging.(2) Effectiveness of MTS and GAR for different multi-agent tasks. We conduct ablation studies to figure out (3)Generalization of MaskMA to downstream tasks. We evaluate the models performance on various downstream tasks,such as varied policies collaboration, ally malfunction, and ad hoc team play. This helps us understand how the learnedskills and strategies can be effectively adapted to different situations. Setup.In SMAC (Samvelyan et al., 2019), players control ally units in StarCraft using cooperative micro-tricks todefeat enemy units with built-in rules, and the state of unit si includes unit type, position, health, shield, and so on.Our approach differs from existing methods that only consider grouped scenarios, such as Easy, Hard, and Super-Hardmaps. Instead, we extend the multi-agent decision-making tasks by combining different units with varying numbers.We include three races: Protoss (colossus, zealot, stalker), Terran (marauder, marine, and medivac), and Zerg (baneling,zergling, and hydralisk). Note that since StarCraft II does not allow units from different races to be on the same team,we have designed our experiments within this constraint. Firstly, we collect expert trajectories as offline datasets from",
  "Average19.20 2.0490.91 3.5651.78 7.2789.35 3.92": "the 11 training maps by utilizing the expert policies trained with a strong RL method named ACE (Li et al., 2022).This yields 11 offline datasets, most of which contain 10k episodes with an average return exceeding 18. Then, weemploy different methods to pretrain on the offline dataset and evaluate their zero-shot capabilities on 60 generated testmaps. As shown in , we run 32 test episodes to obtain the win rate and report the average win rate as well as thestandard deviation across 4 seeds. BaselineWe take the MADT (Meng et al., 2023) as our baseline for comparison which utilizes a causal transformerto consider the history of local observation and action for an agent. MADT transforms multi-agent pretraining data intosingle-agent pretraining data and trains each agents policy independently, therefore adopting a decentralized trainingand decentralized execution setting. Specifically, the input to each agents policy is its own observation. Such anindependent learning pipeline leads to an increase in computational complexity of ON 3w.r.t. agent numbers N. Ourexperiments of MaskMA and baseline MADT utilize the same hyperparameters which are detailed in the Appendix.",
  "Performance on Training Maps": "Performance on the training domain is the most basic ability of one model, thus we first assess MaskMA and thebaseline method on datasets including 11 training maps. As shown in , MaskMA achieves a 90.91% averagewin rate in 11 maps for Centralized Execution setting, while MADT can not be applied in this setting. Moreover,MaskMA surpasses MADT by a significant margin on Decentralized Execution setting (89.35% v.s. 51.78%). In somechallenging maps like 3s5z_vs_3s6z, the advantage of our MaskMA is even much larger (85.16% v.s. 14.84%). One keyobservation is that MaskMA consistently performs well in both Centralized Training & Centralized Execution (CTCE)and Centralized Training & Decentralized Execution (CTDE) settings, highlighting its flexibility and adaptability invarious execution paradigms. a represents the curve of the average training win rate of MaskMA and thebaseline method in 11 training maps on the Decentralized Execution setting. MaskMA significantly outperforms thebaseline and achieves more than 80% win rate in most maps within 0.5M training steps, showing the robustness andefficiency of MaskMA.",
  "MaskMA as Excellent Zero-shot Learners": "Zero-shot generalization ability is the core motivation of our design, so we also evaluate the models ability to generalizeto extensive unseen scenarios without any retraining. Specifically, we evaluate our MaskMA and MADT on the 60unseen testing maps. shows that MaskMA outperforms MADT in zero-shot scenarios by a large margin in",
  "Performance on Downstream Tasks": "Evaluating MaskMAs generalization ability to downstream tasks can help us understand whether the learned skills andstrategies could be effectively adapted to different situations. In this section, we provide various downstream tasks toassess MaskMAs robust generalization, including varied policies of collaboration, ally malfunction, and ad hoc teamplay. Varied Policies Collaboration.In this task, some agents are controlled by our trained policy (i.e., controllableagents), while others are controlled by an external baseline policy (i.e., uncontrollable agents). This task requires ourtrained policy to have excellent collaborative ability to cooperate with external policies. We conducted simulationsin the 8m_vs_9m map, where our team controlled 8 marines to defeat 9 enemy marines. The uncontrollable agentsare controlled by an external baseline policy with an average win rate of 41.41%. As shown in , MaskMAexhibits seamless collaboration with uncontrollable agents under different scenarios. MaskMA dynamically adapts tothe strategies of other players and effectively coordinates actions. Specially, with only 50% controllable agents, the winrate has been improved significantly by nearly two times (79.69% v.s. 41.41%). : Win rate on varied policies collaboration task with 8m_vs_9m map. The uncontrollable agents are controlled by apolicy with 41.41% average win rate (i.e., the win rate with 0% controllable agents). MaskMA demonstrates excellent collaborativeperformance in diverse scenarios with varying proportion of uncontrollable agents.",
  "Win Rate41.41 6.0062.50 7.3379.69 5.1889.84 2.5986.72 4.06": "Ally Malfunction.In this task, one ally marine may become disabled or die due to external factors during execution.The time of ally marine malfunction is a changeable parameter. MaskMA is requested to handle such situationsgracefully by redistributing tasks among the remaining agents while maintaining overall performance. Simulationsare conducted in the 8m_vs_9m map, where we control 8 marines against 9 enemy marines. The average win ratewithout ally malfunction is 86.72%. As given in , MaskMA exhibits robustness and adaptability in the face ofunexpected ally malfunction.",
  "Win Rate86.72 4.061.56 1.5637.5 6.9971.09 6.7786.72 2.59": "Ad Hoc Team Play.In this task, agents need to quickly form a team with one new ally agent during the execution ofthe task. The challenge lies in the ability of the model to incorporate this new agent into the team and allow them tocontribute effectively without disrupting the ongoing strategy. Simulations are conducted in the 7m_vs_9m map (i.e.,our 7 marines to defeat 9 enemy marines), and the average win rate without extra ally marine is 0%. As shown in ,MaskMA demonstrates excellent performance in ad hoc team play scenarios, adjusting its strategy to accommodate newagents and ensuring a cohesive team performance. We provide visualizations of MaskMAs behavior on ad hoc teamplay in . More results are presented in the Appendix. : Win rate on ad hoc team play task with 7m_vs_9m map. Marine Inclusion Time\" indicates the time of adding an extraally marine during an episode. For example, 0.2 represents adding one ally marine at 1/5th of the episode. None signifies theoriginal 7m_vs_9m setup without any extra ally marine.",
  "(d)": ": Visualization of Ad Hoc Team Play on 7m_vs_9m with Marine Inclusion Time = 0.8. This experiment demonstrates thatwhen new Marines are added near the end of an episode, MaskMA still can quickly incorporate them into the team and enable themto contribute effectively. (a) Initial distribution of agents positions. (b) Prior to the addition of the new Marine, our team is leftwith only three severely wounded agents, on the brink of defeat. (c) The new agent (indicated by the red arrow) joins our team andimmediately engages the enemy. (d) With the assistance of the newly added agent, our team successfully defeats the enemy.",
  "We perform extensive ablation studies to verify the effectiveness of proposed components and hyper-parameter choices": "Generalizable Action Representation (GAR).We compare GAR module to an alternative action representationmethod, i.e., aligning the maximum action length with a specific action mask for each task. As shown in ,removing GAR leads to significant performance degradation (#2 v.s. #4) on the testing maps, emphasizing its importancein improving the models generalization capabilities. Mask-based Training Strategy (MTS).The ablation results are shown in . The alternative method of MTSis centralized training without masking, which naturally excels in the Centralized Execution setting (#3). However,the model without MTS struggles to generalize to Decentralized Execution setting, exhibiting significant performancedegradation (#3 v.s. #4). The MTS employed in MaskMA, while posing a challenging training task, helps the modellearn permanent representations stable representations across different scenarios and observation range. Intuitively, therandom mask ratio aligns with the inference process, where the number of enemies and allies gradually increases in anagents local observation due to cooperative micro-operations, e.g., positioning, kiting, and focusing fire.",
  "(c)": ": (a) Comparison of learning curve with the win rate on training maps in Decentralized Execution setting. MaskMAconsistently outperforms MADT on average win rate. (b) Ablation on timestep with the win rate on training maps in theDecentralized Execution setting. MaskMA performs better with a longer timestep. (c) Ablation on the number of training mapswith the win rate on unseen maps in the Decentralized Execution setting. With the increasing of training maps (especially from 5 to8), the models performance on various unseen maps improves, indicating better generalization ability. : Ablation over mask-based training strategy (MTS) and generalizable action representation (GAR). The results are theperformance on training maps. The baseline is a naive transformer. Each row adds a new component to the baseline, showcasing howeach modification affects the performance.",
  "3.358.03 1.44239.49 3.0539.91 3.97391.26 4.2141.55 4.38490.91 3.5689.35 3.92": "Mask Ratio in MTS.We explore different types of masks for training, including a set of fixed mask ratios, local mask,and random mask ratios chosen from for units at each time step. We provide mask ratio analysis in . Theresults show that as the masking ratio increases, the performance on the training maps increases in the DecentralizedExecution setting while decreasing in the Centralized Execution setting. This suggests that an appropriate masking ratiohelps strike a balance between learning useful representations and maintaining adaptability to dynamic scenarios in theagents local observation. In conclusion, a random mask ratio with the range of is a simple yet effective way tobalance the two settings, which could combine the advantages of various ratio masks. This approach allows MaskMAto demonstrate strong performance in both centralized and decentralized settings while maintaining the adaptability andgeneralization necessary for complex multi-agent tasks. Timestep Length.We conduct experiments with different timestep length to study the access to previous states.As shown in b, MaskMAs performance is better when using a longer timestep length. One hypothesis isthat the POMDP property of the SMAC environment necessitates that policies in SMAC take into account sufficienthistorical information to make informed decisions. Considering the balance between performance and efficiency, weselect timestep K as 10 as our default hyper-parameter. This choice allows MaskMA to leverage enough historicalinformation to make well-informed decisions while maintaining a reasonable level of computational complexity. Number of Training Maps.c demonstrates the influences of the number of training maps. As the number oftraining maps increases, the win rate on the testing maps also improves, indicating that the model is better equippedto tackle new situations. A marked uptick in win rate is observed when the map count rises from 5 to 8, underliningthe value of training the model across varied settings. This trend in MaskMA offers exciting prospects for multi-agentdecision-making. It implies that by augmenting the count of training maps or integrating richer, more intricate trainingscenarios, the model can bolster its adaptability and generalization skills.",
  "Decentralized Execution83.59 8.0841.55 4.3858.03 5.7071.52 4.2382.03 5.0189.35 3.92": "Training Cost and Parameter Count.MaskMA processes the inputs of all agents concurrently, achieving a notabledegree of parallelism superior to MADT, which transforms multi-agent training data into single-agent data. Conse-quently, MaskMA is considerably more time-efficient than MADT when trained over identical epochs. Specifically,training 2M iterations on one A100 GPU with the same model architecture and the same 190K training data, MaskMAneeds only 31 hours, whereas MADT requires 70 hours. Also, the parameter counts for both models are nearly identicalsince they share the same transformer architecture and the only difference lies in the final fully connected (FC) layer foraction output.",
  "Discussion": "We have introduced MaskMA, a mask-based collaborative learning framework for multi-agent decision-making, to tacklethe challenges of zero-shot generalization in the multi-agent decision-making field. MaskMA consists of a transformerarchitecture, a mask-based training strategy, and a generalizable action representation. Extensive experiments on SMACdataset demonstrate the effectiveness of our MaskMA in terms of zero-shot performance and adaptability to variousdownstream tasks, such as varied policies collaboration, ally malfunction, and ad hoc team play. Limitations & Future WorkOur current study primarily relies on expert datasets. Exploring how to utilize low-quality demonstration datasets to develop a generalist agent with robust zero-shot capabilities presents a promisingresearch direction. Furthermore, extending the maskMA approach to create a single generalist agent capable ofgeneralizing across different types of environments, such as entirely different games, is also a promising avenue forfuture research.",
  "Susanne C Brenner. The mathematical theory of finite element methods. Springer, 2008": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neuralinformation processing systems, 33:18771901, 2020. Micah Carroll, Orr Paradise, Jessy Lin, Raluca Georgescu, Mingfei Sun, David Bignell, Stephanie Milani, KatjaHofmann, Matthew Hausknecht, Anca Dragan, et al. Uni [mask]: Unified inference in sequential decision problems.Advances in neural information processing systems, 35:3536535378, 2022. Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas,and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neuralinformation processing systems, 34:1508415097, 2021.",
  "Frans A Oliehoek and Christopher Amato. A concise introduction to decentralized pomdps, 2015": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, SandhiniAgarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback.Advances in Neural Information Processing Systems, 35:2773027744, 2022. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, ZemingLin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library.Advances in neural information processing systems, 32:80268037, 2019.",
  "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional imagegeneration with clip latents. arXiv preprint arXiv:2204.06125, 2022": "Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas Nardelli, Tim GJ Rudner,Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The starcraft multi-agent challenge. InProceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems, pp. 21862188,2019. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, BaptisteRozire, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, andGuillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Wei-Cheng Tseng, Tsun-Hsuan Johnson Wang, Yen-Chen Lin, and Phillip Isola. Offline multi-agent reinforcementlearning with knowledge distillation. Advances in Neural Information Processing Systems, 35:226237, 2022a. Wei-Cheng Tseng, Tsun-Hsuan Johnson Wang, Yen-Chen Lin, and Phillip Isola. Offline multi-agent reinforcementlearning with knowledge distillation. Advances in Neural Information Processing Systems, 35:226237, 2022b. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser, and IlliaPolosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Weixun Wang, Tianpei Yang, Yong Liu, Jianye Hao, Xiaotian Hao, Yujing Hu, Yingfeng Chen, Changjie Fan, andYang Gao. Action semantics network: Considering the effects of actions in multiagent systems. In InternationalConference on Learning Representations, 2020. Muning Wen, Jakub Kuba, Runji Lin, Weinan Zhang, Ying Wen, Jun Wang, and Yaodong Yang.Multi-agentreinforcement learning is a sequence modeling problem. Advances in Neural Information Processing Systems, 35:1650916521, 2022a.",
  "A.5State of units": "In the original StarCraft II Multi-Agent Challenge (SMAC) setting, the length of the observation feature fluctuatesin accordance with the number of agents. To enhance generalization, MaskMA directly utilizes each units state asinput to the transformer architecture. As depicted in , within the SMAC context, each units state comprises 42elements, constructed from nine distinct sections. Specifically, the unit type section, with a length of 10, represents thenine unit types along with an additional reserved type.",
  "B.2Comparition of pretraining dataset size": "The scale of the pretraining dataset significantly impacts the eventual performance. In the multi-agent StarCraft IIenvironment, SMAC, we investigated the optimal size for the pretraining dataset. We take the 3s_vs_5z map as anexample and solely use the pretraining dataset of this map to train MaskMA, and then test it on the same map. Asillustrated in , a dataset encompassing 1k episodes was found insufficient, leading to a progressive decline inwin rates. In contrast, a dataset comprising 50k episodes demonstrated exceptional performance. Specifically, for the3s_vs_5z and MMM2 maps, a pretraining dataset containing 50k episodes proved appropriate. For the remaining ninemaps, a dataset consisting of 10k episodes was found to be suitable. 0.00.51.01.52.00 Training Iterations (M) Win Rate % Episode Number = 50kEpisode Number = 10k",
  "(b)": ": Teammate Malfunction on 8m_vs_9m with Marine Malfunction Time = 0.6. The agent within the red circle suddenlymalfunctions in the middle of the episode, remaining stationary and taking no actions. (a): The agent within the red circle startsmalfunctioning. (b): Despite the malfunctioning teammate, other agents continue to collaborate effectively and eventually succeed ineliminating the enemy.",
  "Centralized ExecutionDecentralized ExecutionDecentralized Execution": "3s_vs_5z85.94 3.4982.81 7.8173.44 3.493s5z98.44 1.5699.22 1.3515.62 6.991c3s5z94.53 4.0695.31 1.5654.69 8.413s5z_vs_3s6z85.94 6.4485.16 5.5814.84 9.975m_vs_6m86.72 1.3584.38 4.9485.94 5.188m_vs_9m88.28 6.0086.72 4.0687.50 2.21MMM292.97 2.5986.72 4.6262.50 11.692c_vs_64zg99.22 1.3592.97 2.5934.38 9.11corridor96.88 3.8394.53 2.5921.88 11.486h_vs_8z75.00 5.8576.56 6.4427.34 6.77bane_vs_bane96.09 2.5998.44 1.5691.41 4.62 2s_vs_1z100.000.00100.000.0070.31 10.003z_vs_3s71.88 3.8353.91 9.470.00 0.001c2s44.53 5.5841.41 6.392.34 2.594z_vs_3s99.22 1.3597.66 1.354.69 2.714m99.22 1.35100.000.0028.91 6.001c3s54.69 3.4948.44 4.6917.19 4.695m_vs_4m100.000.00100.000.00100.000.005m100.000.00100.000.0098.44 1.562s3z10.94 5.188.59 2.590.00 0.001s4z_vs_1ma4m91.41 4.6288.59 3.4135.16 3.413s2z_vs_2s3z9.38 5.8511.72 2.590.00 0.002s3z_vs_1ma2m2me92.97 4.0682.03 14.0435.16 9.732s3z_vs_1ma3m1me90.62 3.1293.75 3.8337.50 7.971c3s1z_vs_1ma4m100.000.0099.22 1.3598.44 1.561s4z_vs_2ma3m83.59 7.1277.66 3.4173.44 6.441c3s1z_vs_2s3z100.000.00100.000.0078.12 5.852s3z_vs_1h4zg96.88 3.1296.09 3.4171.88 7.163s2z_vs_2h3zg95.31 5.1893.75 4.4275.00 3.831c4z_vs_2s3z100.000.00100.000.0071.88 6.632ma3m_vs_2ma2m1me46.09 12.5739.06 9.240.00 0.002s3z_vs_1b1h3zg100.000.0096.88 2.2167.19 4.061s4z_vs_5z3.12 2.215.47 4.060.00 0.001c1s3z_vs_1c4z64.06 7.1676.56 5.6335.94 6.446m97.66 2.5999.22 1.3547.66 5.121s3z_vs_5b5zg93.75 3.8387.50 2.2158.59 1.358z_vs_6h93.75 2.2198.44 1.5638.28 4.062s5z17.97 4.067.03 3.410.78 1.351c3s1z_vs_1b1h8zg100.000.00100.000.0057.81 0.001c3s1z_vs_1h9zg100.000.00100.000.0060.94 0.002c1s2z_vs_1h9zg100.000.00100.000.0049.22 0.003ma6m1me_vs_3h2zg100.000.00100.000.0023.44 0.003s2z_vs_1b1h8zg97.66 1.3585.94 4.6952.34 2.595ma4m1me_vs_1b3h1zg100.000.00100.000.0098.44 2.714ma5m1me_vs_5zg100.000.00100.000.0085.94 8.121s5z_vs_4ma6m39.06 10.0016.41 3.414.69 4.694ma7m_vs_2s3z98.44 1.56100.000.0035.16 13.072ma9m_vs_3s2z89.06 2.7180.47 6.0051.56 9.505s2z_vs_2ma8m48.44 9.2429.69 11.168.59 4.062s5z_vs_9m1me99.22 1.3599.22 1.3546.09 4.941c3s_vs_3h10zg85.94 5.6389.84 5.5843.75 4.943s7z_vs_5ma2m3me99.22 1.3596.88 3.8360.94 2.591c3s6z58.59 7.1261.72 7.7714.84 6.772c2s6z_vs_1c3s6z100.000.00100.000.0097.66 1.351c2s7z25.00 9.6325.78 5.127.81 3.491c4s5z_vs_5ma3m2me100.000.0083.59 8.0884.38 0.0010s1z_vs_1b2h7zg96.09 4.0696.88 2.2168.75 1.5610m_vs_11m79.69 6.4478.91 7.450.00 0.001b10zg85.16 5.5889.06 7.1660.16 5.122b10zg92.19 4.6998.44 1.5630.47 5.581c_vs_32zg59.38 9.3855.47 7.771.56 2.7132zg_vs_1c92.97 3.4194.53 3.4153.91 3.121b20zg74.22 10.9182.81 2.7166.41 7.452b20zg100.000.0097.66 1.3571.09 4.943b20zg96.09 3.4188.28 4.6285.94 4.6916z_vs_6h24zg97.66 2.5997.66 2.5971.09 4.065b20zg99.22 1.35100.000.0032.03 2.5964zg_vs_2c55.47 6.3956.25 2.2148.44 8.411c8z_vs_64zg89.06 7.1685.94 5.182.34 2.591c8z_vs_2b64zg61.72 7.7765.62 5.850.00 0.001c8z_vs_5b64zg6.25 2.214.69 3.490.78 1.35"
}