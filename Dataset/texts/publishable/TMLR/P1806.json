{
  "Abstract": "Time series data analysis is a critical component in various domains such as finance, healthcare,and meteorology. Despite the progress in deep learning for time series analysis, there remainsa challenge in addressing the non-stationary nature of time series data. Most of the existingmodels, which are built on the assumption of constant statistical properties over time,often struggle to capture the temporal dynamics in realistic time series and result in biasand error in time series analysis. This paper introduces the Adaptive Wavelet Network(AdaWaveNet), a novel approach that employs Adaptive Wavelet Transformation for multi-scale analysis of non-stationary time series data. AdaWaveNet designed a lifting scheme-based wavelet decomposition and construction mechanism for adaptive and learnable wavelettransforms, which offers enhanced flexibility and robustness in analysis. We conduct extensiveexperiments on 10 datasets across 3 different tasks, including forecasting, imputation, anda newly established super-resolution task. The evaluations demonstrate the effectivenessof AdaWaveNet over existing methods in all three tasks, which illustrates its potential invarious real-world applications. The code implemented for the AdaWaveNet is available at",
  "Introduction": "Time series data, extensively encountered in various domains, including finance, healthcare, and meteorology,require effective analytical methodologies (Esling & Agon, 2012). Therefore, understanding and analyzingtime series data has triggered substantial interest in various real-world applications. Recently, the rapiddevelopment of deep learning has significantly transformed the landscape of time series analysis. Theseadvancements have triggered breakthroughs in various applications, including forecasting (Zhou et al., 2021;2022b; Wu et al., 2021), imputation (Wu et al., 2022; Xu et al., 2022), and anomaly detection (Blzquez-Garcaet al., 2021; Li & Jung, 2023). However, even with the promising performances of the aforementioned methods, a notable limitation in thisresearch area is the inadequate focus on the non-stationary nature of time series data. Non-stationarity, withits evolving statistical properties and time-dependent patterns, poses a significant challenge for traditionaldeep learning models (Hyndman & Athanasopoulos, 2018; Shumway et al., 2017). The constantly shiftingnature of time series data can make it difficult for the aforementioned methods to fully capture its dynamicpatterns, which potentially leads to inaccurate analysis.",
  "Published in Transactions on Machine Learning Research (11/2024)": "0.5 0.0 0.5 1.0 1.5 2.0 2.5 AdaWaveNet GroundTruthPrediction 0.5 0.0 0.5 1.0 1.5 2.0 iTransformer GroundTruthPrediction 0.5 0.0 0.5 1.0 1.5 2.0 FreTS GroundTruthPrediction 0.5 0.0 0.5 1.0 1.5 2.0 PatchTST GroundTruthPrediction 0.5 0.0 0.5 1.0 1.5 2.0 TimesNet GroundTruthPrediction 0.5 0.0 0.5 1.0 1.5 2.0 DLinear GroundTruthPrediction 0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Nonstationary_Transformer GroundTruthPrediction 0.5 0.0 0.5 1.0 1.5 2.0 FiLM GroundTruthPrediction FEDformer GroundTruthPrediction",
  "Our contribution can be summarized as:": "We introduce AdaWaveNet, a novel architecture designed to address the challenges posed bynon-stationary time series data through an adaptive, multi-scale approach. Differing from priordecomposition-based or fixed-parameter methods, AdaWaveNet leverages a learnable wavelet trans-formation via the lifting scheme, which enables dynamic adaptation to evolving statistical propertieswithin the data. While we leverage time-series decomposition module that separates trend andseasonal components following previous methods, the adaptability is further enhanced by a groupedlinear module, which uniquely captures channel-specific dependencies. We establish a new benchmark for super-resolution in the field of time series data. This benchmarkaims to enhance the quality of data obtained from under-sampled sequences and improve the overallefficiency and effectiveness of time series data monitoring and analysis. Our extensive evaluations demonstrate that AdaWaveNet outperforms existing methods in forecastingand super-resolution tasks. These results suggest the capability of the proposed method for diversereal-world applications.",
  "Time Series Analysis with Deep Learning": "The evolution of deep learning has significantly impacted temporal modeling and time series analysis.Recurrent neural networks (RNNs), such as those based on Long Short-Term Memory (LSTM) (Hochreiter& Schmidhuber, 1997; Siami-Namini et al., 2019), are designed to capture temporal dependencies throughinternal states. Multi-Layer Perceptrons (MLPs) (Zeng et al., 2023; Li et al., 2023) have shown effectiveness intemporal modeling by processing point-wise projections of sequences. Convolutional Neural Networks (CNNs)(Lea et al., 2017; Liu et al., 2022a) excel in extracting hierarchical features and detecting complex patterns,leveraging their strength in spatial and temporal data processing. Moreover, the Transformer variantshave demonstrated remarkable results in time series applications by capturing long-range dependencies andprocessing entire sequences efficiently (Zhou et al., 2022b; Liu et al., 2023a; 2022b; Zhang & Yan, 2022).Nevertheless, in real-world applications, these well-proven structures may struggle with non-stationarity intime series data because their learned patterns and dependencies are based on the assumption of consistentstatistical properties. On the other hand, most of the realistic temporal data, which is non-stationary withdynamics over time, violates the consistent assumption. These methods have been developed and applied to various tasks, including forecasting (Zhou et al., 2021;2022b; Wu et al., 2021), imputation (Wu et al., 2022; Xu et al., 2022), and anomaly detection (Blzquez-Garcaet al., 2021; Li & Jung, 2023). However, the field of super-resolution in time series analysis remains relatively",
  "Decomposition-Based Methods": "Decomposition-based methods have emerged as promising techniques for time series analysis by separatingdata into interpretable components such as trend and residual terms, which enhances the ability to captureboth short-term and long-term patterns (Wu et al., 2021; Wang et al., 2024; Zhu et al., 2023; Cao et al.,2023; Hu et al., 2023). For instance, DRCNN (Zhu et al., 2023) uses a multi-resolution approach withconvolutional kernels to improve forecasting accuracy across different time scales. Similarly, MICN (Wanget al., 2023) combines multi-scale local and global context modeling through convolution and isometricconvolutions, achieving significant improvements in long-term forecasting tasks. TEMPO (Cao et al., 2023)extends Transformer-based models by introducing a prompt-based generative pre-training architecture thatdecomposes time series into trend, seasonal, and residual components and establishes a foundational modelfor time series forecasting. Despite the success of the aforementioned decomposition-based methods, they have limitations in handlingthe dynamic and evolving nature of non-stationary time series data. For instance, DRCNNs focus on fixeddecomposition into residual and trend terms may fail to capture complex temporal shifts over time, whileMICN, though addressing both short-term and long-term patterns, relies on pre-defined convolutional kernelsthat lack adaptability to dynamically changing signals. To leverage the benefits of decomposition whileincorporating adaptability, our proposed AdaWaveNet introduces a novel adaptive wavelet-based liftingscheme that dynamically learns wavelet coefficients through end-to-end training.",
  "Non-Stationarity-Enhanced Models": "Recent developments in time series analysis have started addressing non-stationarity issues (Liu et al.,2022b; Zhou et al., 2022b; Liu et al., 2023b;c). For instance, Liu et al. (2022b) introduced Non-stationaryTransformers with strategies like Series Stationarization and De-stationary Attention to standardize signalstatistics over time. Zhou et al. (2022b) employed frequency domain-enhanced attentions in Transformers,incorporating Fourier and wavelet transform-based techniques. Additionally, Liu et al. (2023b) integratedKoopman operator theory for analyzing non-linear dynamical systems by transforming signals into a linear,high-dimensional space. While these methods effectively model stationarity, they exhibit limitations suchas the need for manually tuned filters in FEDformer and extensive computations for long-term signals orhigh-dimensional projections. Recent advancements in adaptive wavelet-based models, such as the Adaptive Multi-Scale Wavelet NeuralNetwork (AMSW-NN) (Ouyang et al., 2021), have demonstrated the effectiveness of combining multi-scaleconvolutional neural networks with depthwise convolutions. However, AMSW-NN primarily focuses ongenerating candidate frequency decompositions without explicitly considering inter-channel relationships ordynamic adaptability at different scales. To address these gaps, we propose the Adaptive Wavelet Network(AdaWaveNet). AdaWaveNet utilizes a novel lifting scheme that learns adaptive wavelet coefficients throughend-to-end training and incorporates a grouped linear module for efficient trend processing. This dual-residualapproach dynamically captures both fine-grained and broad patterns, which enhances adaptability andperformance across diverse time series datasets.",
  "Background": "Non-stationary time series data often contain transient behaviors and changing frequencies, which makestraditional methods that assume constant statistical properties less effective. Wavelet analysis addresses thisby enabling multi-resolution decomposition, where each scale handles different frequency bands over time.For instance, high-frequency components can capture rapid changes or anomalies that occur over a short",
  "Wavelet Transform": "The wavelet transform is a versatile tool for analyzing signals across multiple scales, simultaneously capturingboth frequency and temporal information.Unlike the Fourier transform, which focuses exclusively onthe frequency domain, the wavelet transform enables a time-frequency analysis by decomposing a signalinto components that reveal its structure at different resolutions. This ability is crucial for handling non-stationary time series, which often contain short-term fluctuations (high-frequency) alongside long-termtrends (low-frequency). Given a signal f, the discrete wavelet transform decomposes it into the followingform:f(x) =",
  "In the equation, i,j(x) = 2ix jrepresents the wavelets at different scales i and translations j, withw = f, (x) denoting the wavelet coefficients": "The wavelet transformation provides an extensive capability for analyzing the changing dynamics andnon-stationarity in time series data, which shows advances compared to the Fourier transforms frequency-centric approach. Traditional wavelet bases, such as Haar (Haar, 1909), Daubechies (Daubechies, 1988),and Biorthogonal (Cohen et al., 1992) wavelets, have been widely used in various real-world applications.However, the selection of an optimal wavelet basis remains a challenge, as the effectiveness of different basescan vary considerably depending on the specific characteristics and structure of the real-world time seriesdata being analyzed.",
  "Lifting Scheme": "The lifting scheme, also known as the second-generation wavelet approach, was introduced by Sweldens (1998)to enhance the flexibility and adaptability of wavelet transforms. Differing from traditional wavelets builtfrom dilations and translations of a single function, the lifting scheme constructs wavelets in the spatialdomain using a series of simple operations. This approach preserves essential wavelet properties while offeringgreater adaptability to specific signal characteristics. The scheme processes an input signal x to segregate it into approximation (c) and detail (d) sub-bands,which achieves multi-resolution analysis through three main stages: split, update, and predict. This methodenables the creation of custom wavelets that can be more efficiently computed and better adapted to irregulardata structures or non-standard sampling grids. By doing so, the lifting scheme provides an efficient routeto achieve similar mathematical properties and practical results as traditional wavelets, which makes itparticularly valuable for various signal processing applications.",
  "Split: The input signal is divided into two non-overlapping components: the even (xe) and odd (xo)components, denoted as xe[n] = x[2n] and xo[n] = x[2n + 1]": "Update: This stage separates the signal in the frequency domain to generate the approximation c. An updateoperator U() is applied to a sequence of neighboring odd polyphase samples, yielding c[n] = xe[n]+U(xLUo [n]). Predict: Given the correlation between xe and xo, a predictor P() is developed for one partition based onthe other. The detail sub-band d, is computed as the prediction residual d[n] = xo[n] P(cLP [n]). The lifting scheme improves the flexibility of wavelet transformations by allowing for a data-driven adaptationof wavelet coefficients, which makes it more suitable for analyzing non-periodic and intricate signals frequently",
  "Adaptive Wavelet Network": "We propose an Adaptive Wavelet Network (AdaWaveNet), which comprises a time series decompositionmodule, stacked adaptive wavelet (AdaWave) blocks based on the lifting scheme (Sweldens, 1998), and agrouped linear module. illustrates the overall framework of our method. We denote the inputsequence as xinput RCL and the target model output as xpred RCLp. The target output can representvarious terms depending on the task, such as future sequences for the forecasting task or completed signalsfor the imputation task. The decomposition module processes the time series data into seasonal (xs) andtrend (xtrend) components. The AdaWave blocks then transform xs into a low-rank approximation xls andwavelet coefficients cl at different levels l. The channel-wise attention layer models the intermediate xls acrosschannels to predict the targeted low-rank approximation xls. We reconstruct the predicted seasonal phasexs from cl and xls using inverse adaptive wavelet (InvAdaWave) blocks. The trend component xtrend oftenexhibits alignment issues and discrepancies across variates, and we employ a grouped linear module thatapplies distinct linear heads to different channel groups, to enhance the quality of trend phase predictions.The networks final output is the sum of xs and xtrend. AdaWaveNet offers several advantages, including multi-scale processing to mitigate non-stationary issuesand a data-driven approach to learn wavelet coefficients through the lifting scheme adaptively.Thisadaptability is a key aspect of our proposed method. Additionally, the AdaWave and InvAdaWave blocks,based on convolutional layers, provide computational efficiency compared to the prior self-attention-basedimplementation of the wavelet transform (Zhou et al., 2022b). The grouped linear module further improvesthe modeling of the trend component, which tackles the discrepancies across channels or signal variates.",
  "xinput = xs + xtrend(2)": "In this equation, the trend component (xtrend) shows the overall direction and long-term movements of thedata over time. It is often calculated using a moving average, which smooths out short-term fluctuations tohighlight longer-term trends. The seasonal component (xs) captures repeating patterns that occur within aspecific period and helps identify systematic variations in the data. To perform the decomposition, we first apply a moving average to the time series to estimate the trendcomponent. Once the trend is identified, we subtract it from the original time series to isolate the seasonalcomponent. By separating the data in this way, we can better understand and analyze the different scales ofvariation present in the time series. This decomposition is the first step in our multi-scale analysis framework.",
  "Adaptive Wavelet Block": "Given the seasonal component xs of a time series, we apply a Lifting Wavelet Transform (LWT) usingConvolutional Neural Networks (CNNs) to refine features at various levels of granularity iteratively. Theseprocesses transform the seasonal component at level l 1 into a more refined level l, denoted as xls, andgenerate the corresponding detail coefficients, cl.",
  "Layers": ": Illustration of the AdaWaveNet framework for time series analysis. The input sequence xinputundergoes decomposition into trend (xtrend) and seasonal (xs) components. The trend component is processedthrough a clustering algorithm followed by a grouped linear module to produce a refined trend predictionxtrend. Concurrently, the seasonal component is processed through stacked AdaWave blocks, employing indexsplitting, convolutional layers, and a channel-wise attention layer to capture multi-scale features and generatea low-rank approximation xls. This is followed by inverse AdaWave blocks that reconstruct the seasonalprediction xs. The final predicted output xpred is obtained by summing the predicted seasonal and trendcomponents.",
  "where denotes the convolution operation, Wle and Wlo are the convolutional filter weights, and ble and bloare the biases for the even and odd components at level l, respectively": "In the prediction step, we use the even-indexed components to estimate the odd-indexed components. This isbecause, in many signals, there is a smooth transition between consecutive samples. The prediction step aimsto estimate the finer details of the signal (odd indexed components) using a smoothed version (even indexedcomponents) to capture the high-frequency variations by computing the detail coefficients cl:",
  "Channel-wise Attention for Approximation Projection": "To enhance the feature representation of the seasonal component xls specifically at the final level of theAdaWave blocks, a self-attention (SA) mechanism is employed, inspired by Liu et al. (2023a). The self-attention structure is applied to xNs , where N denotes the final level of decomposition. This computation afterthe last layer of decomposition ensures efficient and focused refinement of the feature map of the low-rankapproximation of the seasonal component, as the length of the sequences after N blocks of AdaWave blocksbecomes (L/(2N)). The channel-wise attention mechanism operates on the channels of xNs to refine itsapproximation, to project the processed seasonal component onto the targeted sequences as xNs . Importantly,during this process, the detail coefficients (cN) remain unchanged. Focusing the channel-wise attention mechanism at the final decomposition layer is both computationallyefficient and effective in capturing the essential characteristics of the time series. It allows the model toemphasize the global contextual information, which is crucial for the accurate representations of the seasonalcomponents in complex time series data.",
  "Grouped Linear Module": "We process the trend component xtrend using a two-step approach: clustering the channels and applyingdistinct linear projections based on the clustering labels. This method effectively captures and models thelong-term progression of the trend component. In the first step, we group the channels of xtrend using a clustering algorithm, specifically K-means. Thisclustering step helps to identify patterns and group similar channels together. For example, in the diagramas in , channels are clustered into different groups according to their characteristics. This step iscrucial for understanding the temporal patterns within the trend data that might indicate different statesacross variates.",
  ": Comparison of model performances in forecasting, imputation, and super-resolution": "Following the clustering, each group of channels is connected with a linear projection layer. This step ensuresthat the model can apply specific linear projections to different clusters of the trend data. This design aimsto help the model accurately capture different patterns within the trend data. This dual approach of clustering followed by grouped linear transformations is particularly effective in dealingwith complex time series data. By initially clustering the trend components into different groups, the modelcan recognize and adapt to different underlying patterns. Subsequent application of distinct linear projectionsto each cluster further enhances the models ability to represent and forecast the trend component accurately.",
  "We conduct experiments on different time series analysis tasks to evaluate the proposed AdaWaveNet, includingforecasting, imputation, and the newly proposed super-resolution tasks": "AdaWaveNet is extensively benchmarked against established models from recent literature. For modelsrelated to wavelet and frequency domain enhancements, comparisons include FreTS (Yi et al., 2023), FiLM(Zhou et al., 2022a), TimesNet (Wu et al., 2022), and FEDformer (Zhou et al., 2022b). Additionally, modelspreviously recognized for state-of-the-art (SOTA) performances, such as iTransformer (Liu et al., 2023a),DLinear (Zeng et al., 2023), and PatchTST (Nie et al., 2022), are included as experimental baselines. TheNon-stationary Transformer (Stationary) (Liu et al., 2022b), known for addressing non-stationary issues intime series, is also featured for comparison. presents the aggregated results across the forecasting, imputation, and super-resolution tasks. Theresults indicate that our proposed AdaWaveNet method achieves SOTA performance in all three areas oftime series analysis.",
  "Time Series Forecasting": "Forecasting is essential in time series applications, such as weather, traffic, exchange rate, etc. In this section,we conduct extensive experiments to evaluate the forecasting performance of the proposed AdaWaveNet onvarying-domain datasets. Following prior studies such as iTransformer (Liu et al., 2023a) and TimesNet(Wu et al., 2022), we adopt a long-term forecasting setting with datasets including traffic, electricity (ECL),exchange rate, weather, solar energy, and electricity transformer temperature (ETT). For each dataset, weset the predicting length Lp with {96, 192, 336, 720} with the inputting observation length equal to thepredicting length. The performances of evaluations are shown in . The proposed AdaWaveNet achieves promisingperformances in both MSE and MAE across various datasets. In particular, compared to the prior frequencydomain-enhanced methods, AdaWaveNet outperforms 7.7% in MSE and 6.2% in MAE when compared to thebest performances in all these methods. For the SOTA methods such as iTransformer (Liu et al., 2023a) and",
  "Time Series Imputation": "During time series data collection, various factors can disrupt continuous observation and monitoring andlead to missing values within the dataset. For example, factors such as the malfunction of the devices andinterference of signals can all trigger data quality issues, e.g., missing observation. Therefore, in this study,we also investigate the capability of the proposed method in time series imputation tasks. Following theevaluation strategy as Wu et al. (2022), we conduct extensive experiments on the datasets with controlledmasking rates under a random missing setting. We further extend the experimental strategy by introducingthe extended missingness. In (Wu et al., 2022), the missingness was crafted by randomly masking timestampswith a certain ratio that simulates sporadic data loss; whereas the extended missingness masks contiguoussubsequences of the original signals across all channels, which emulates the prolonged periods of missing data.Refer to Appendix A.2 for the detailed descriptions of two masking methods. In this experiment, we examinethe proposed method and the baselines on the weather and electricity data. Also, considering the commonmissingness of sensing data of the wearable sensors Xu et al. (2022), the experiments cover biobehavioraldatasets, including 12-lead electrocardiogram (ECG) from the PTB-XL dataset Wagner et al. (2020), andelectroencephalogram (EEG) from the Sleep-EDFE dataset Kemp et al. (2000). shows the imputation performance for each dataset. The observed results show the two differentmasking strategies create different levels of challenges for the imputation tasks. The extended masking taskintroduces larger errors for all the models. For both imputations, our proposed method shows competitiveperformances on the PTB-XL and Sleep-EDFE datasets, where the sequence lengths are significantly longerthan the ECL and weather datasets. TimesNet (Wu et al., 2022) outperforms all the other models in ECLand Weather random imputation tasks; whereas AdaWaveNet shows substantial improvement compared tothe other baselines on ECL and Weather data for the random masked imputation task. Visualization ofimputed examples can be found in Appendix C.2.",
  "Avg.0.0900.1180.1140.1500.097 0.1270.114 0.1460.128 0.1720.102 0.1280.1300.1720.112 0.1480.1250.172": ": Super-resolution task. Super-resolution upsampling ratios are set at {2, 5, 10}. Experimentally,sequence lengths are fixed at 200 for ETTm1, ETTh1, and Traffic datasets, and at 1000, 3000, and 960 forPTB-XL, Sleep-EDFE, and CLAS datasets, respectively. Evaluation metrics include Mean Squared Error(MSE) and Mean Absolute Error (MAE), with all results being averages over 4 masking ratios. The lowestMSE is highlighted in bold red, while the second lowest is underlined in blue.",
  "Time Series Super-resolution": "This study introduces a benchmark for super-resolution in time series data, which can be crucial for enhancingthe detail and quality of data, particularly when dealing with under-sampled or low-resolution datasets.The objective of the super-resolution task is to reconstruct the original high-resolution signal from thedown-sampled version and increase the temporal resolution by reconstructing the data at finer granularities.The super-resolution task differs from traditional imputation in that it focuses on upsampling the temporalresolution rather than simply filling in missing values. Specifically, it requires the model to infer the higher-frequency components that are missing due to down-sampling. For instance, reconstructing a 100 Hz ECGsignal from 10, 20, 50 Hz inputs requires generating fine-grained details that are not directly observable inthe low-resolution input. In this context, we conduct super-resolution experiments on the ETT, traffic, ECG (PTB-XL), EEG (Sleep-EDFE), and electrodermal activity (EDA) datasets from the CLAS study (Markova et al., 2019). We evaluatethe performance of various models, including AdaWaveNet, by upscaling the temporal resolution at ratios of 2,5, 10. Our results demonstrate that AdaWaveNet excels at reconstructing high-resolution signals, capturingboth low- and high-frequency components, which leads to superior performance compared to other baselinemodels. shows the evaluation results for the super-resolution task. The proposed method shows competitiveresults compared to the baseline methods, especially on datasets such as Traffic, Sleep-EDFE, and CLAS. Wecredit the observed improvement to the multi-scale analysis inside the model architecture. In this experiment,we find the frequency domain enhanced methods including FreTS, TimesNet, and FEDformer generally providebetter results in strongly periodic data. For example, the first three places of averaged super-resolutionreconstruction performances on ECG signals in the PTB-XL dataset are TimesNet, AdaWaveNet, andFreTS, respectively. The channel-wise attention also brings advantages to models, such as AdaWaveNet andiTransformer, in reconstructing the high-dimensional traffic data.",
  "Ablation Study": "An ablation study was conducted to assess the contribution of individual components within the proposedAdaWaveNet framework to its overall performance. This involved evaluating the model on forecasting tasksand extended imputation tasks with the omission of specific components: the Grouped Linear Module,reversible instance normalization (RevIn) as described by (Kim et al., 2021), channel-wise Attention inspiredby Liu et al. (2023a), and the AdaWaveNet and InvAdaWaveNet blocks. The results are presented in , which indicates that the AWB component significantly enhances performance on the ECL and PTB-XLdatasets for imputation tasks. Meanwhile, CWA shows notable efficacy in forecasting tasks. The GL modulealso demonstrates improved results on the PTB-XL dataset during imputation.",
  "Stationary": "2.9 GB, 43.7 ms DLinear 1.2 GB, 30.1 ms TimesNet 16.8 GB, 4471.2ms FreTS 22.5 GB, 389.6 ms iTransformer7.5 GB, 140.7 ms AdaWaveNet7.7 GB, 168.0 ms 5 GB20 GB40 GB Memory FootprintTrafficETTh1 Training Time (ms / iter) 0.25 0.325 0.4 FEDformer 2.2 GB, 226.0 msFiLM 1.7 GB, 129.4ms PatchTST 1.8 GB, 32.7ms",
  "Memory Footprint": "MSE iTransformer1.4 GB, 8.9 ms : Comparison of model efficiency on the ETTh1 and Traffic datasets. The y-axis represents theaveraged mean squared error performances across the forecasting and super-resolution tasks; whereas thex-axis represents the model training time for each iteration. The input length is 192 with a batch size of 16.",
  "Efficiency Analysis": "Efficiency is one of the important concerns in time series analysis. Therefore, we evaluate the efficiency of theproposed method and compare the results with all the baseline models we used in the main experiments.Controlling the input length of the sequences as 96 time points, we select the data samples from the ETTh1and Traffic sets, as they have the least (7) and the most (862) numbers of variates. The environment used inthis evaluation is AWS g5 instances with Nvidia A10 GPUs. The batch size is fixed as 16. Following theprevious studies (Wu et al., 2022; Liu et al., 2023a), we evaluate the efficiency of the models in aspects ofperformances (MSE), training speed, and memory footprints. The comparison results shown in illustrate that AdaWaveNet achieves state-of-the-art performancewhile maintaining reasonable model efficiency. The DLinear model exhibits the smallest memory footprintand fastest training time among the evaluated models. AdaWaveNet demonstrates comparable training speedand memory usage to the iTransformer; while achieving significant savings in memory and training timecompared to established approaches such as PatchTST.",
  "Wavelet Decomposition": "illustrates the wavelet decomposition process employed by AdaWaveNet in a time series forecastingtask. The decomposition splits the seasonal component into layers of approximations and detail coefficients.A channel-wise attention mechanism is then applied to forecast future sequence approximations. Eachdecomposition level incorporates a residual connection that integrates the reconstructed signals from theInvAdaWave blocks. This example illustrates the multi-scale analytical capability of the proposed method,which enables the model to extract features across various granularities. Moreover, the flexibility introducedin the learnable CNN kernels empowers the AdaWave and InvAdaWave blocks to dynamically adapt tovarying input data.",
  "Model Complexity": "AdaWaveNet incorporates effective components, such as the grouped linear module and cross-channel attentionmechanisms, to model dependencies across similar variates in the trend phase. It also introduces multi-scalecapabilities through the AdaWave blocks. Despite its promising performance, AdaWaveNet is less efficientcompared to simpler MLP-based models such as DLinear Zeng et al. (2023), which potentially limits itsapplicability in environments where computational resources are constrained or real-time analysis is required.",
  "Linear": "InvAdaWave Block InvAdaWave Block : An example of the wavelet decomposition and projection in time series forecasting on ETTh1data with two layers of AdaWave and InvAdaWave blocks. xls and xls represents the the approximation atwavelet level l of the input and target sequences, respectively. c is the wavelet coefficients decomposed by theAdaWave blocks.",
  "Generalization to Different Signal Types": "AdaWaveNet demonstrates robust performance in forecasting tasks with data types such as weather andsolar, as well as in extended imputation tasks with electricity and EEG data. However, its capacity togeneralize across the full spectrum of time series data and various tasks has room to be further improved.For example, in tasks involving random imputation of electricity and weather datasets, TimesNet exhibitssuperior performance. Similarly, iTransformer outperforms AdaWaveNet in traffic forecasting tasks.",
  "Conclusion and Future Work": "This paper presented AdaWaveNet, a novel and efficient architecture for addressing the challenges of non-stationarity in time series data analysis. Through the integration of adaptive wavelet transformations,AdaWaveNet demonstrates the advantages of modeling multi-scale data representations in time series data.Our extensive experiments are conducted on 10 datasets for forecasting, imputation, and the newly establishedsuper-resolution tasks, and the results indicate the effectiveness of our approach. Future efforts will focus on exploring and expanding AdaWaveNet and its multi-scale analysis framework to abroader range of tasks, such as classification and anomaly detection. Also, we aim to explore the possibilityof using multi-scale analysis in large-scale pre-training models.",
  "Rob J Hyndman and George Athanasopoulos. Forecasting: principles and practice. OTexts, 2018": "Bob Kemp, Aeilko H Zwinderman, Bert Tuk, Hilbert AC Kamphuisen, and Josefien JL Oberye. Analysis of asleep-dependent neuronal feedback loop: the slow-wave microcontinuity of the eeg. IEEE Transactions onBiomedical Engineering, 47(9):11851194, 2000. Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and Jaegul Choo. Reversible instancenormalization for accurate time-series forecasting against distribution shift. In International Conference onLearning Representations, 2021. Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term temporalpatterns with deep neural networks. In The 41st international ACM SIGIR conference on research &development in information retrieval, pp. 95104, 2018. Colin Lea, Michael D Flynn, Rene Vidal, Austin Reiter, and Gregory D Hager. Temporal convolutionalnetworks for action segmentation and detection. In proceedings of the IEEE Conference on ComputerVision and Pattern Recognition, pp. 156165, 2017.",
  "Zhe Li, Shiyi Qi, Yiduo Li, and Zenglin Xu. Revisiting long-term time series forecasting: An investigation onlinear mapping. arXiv preprint arXiv:2305.10721, 2023": "Minhao Liu, Ailing Zeng, Muxi Chen, Zhijian Xu, Qiuxia Lai, Lingna Ma, and Qiang Xu. Scinet: Timeseries modeling and forecasting with sample convolution and interaction. Advances in Neural InformationProcessing Systems, 35:58165828, 2022a. Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Non-stationary transformers: Exploring thestationarity in time series forecasting. Advances in Neural Information Processing Systems, 35:98819893,2022b. Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long. itransformer:Inverted transformers are effective for time series forecasting. arXiv preprint arXiv:2310.06625, 2023a.",
  "Robert H Shumway, David S Stoffer, and David S Stoffer. Time series analysis and its applications, volume 3.Springer, 2017": "Sima Siami-Namini, Neda Tavakoli, and Akbar Siami Namin. The performance of lstm and bilstm inforecasting time series. In 2019 IEEE International conference on big data (Big Data), pp. 32853292.IEEE, 2019. Akara Supratak and Yike Guo. Tinysleepnet: An efficient deep learning model for sleep stage scoring basedon raw single-channel eeg. In 2020 42nd Annual International Conference of the IEEE Engineering inMedicine & Biology Society (EMBC), pp. 641644. IEEE, 2020.",
  "Wim Sweldens.The lifting scheme: A construction of second generation wavelets.SIAM journal onmathematical analysis, 29(2):511546, 1998": "Patrick Wagner, Nils Strodthoff, Ralf-Dieter Bousseljot, Dieter Kreiseler, Fatima I Lunze, Wojciech Samek,and Tobias Schaeffter. Ptb-xl, a large publicly available electrocardiography dataset. Scientific data, 7(1):154, 2020. Huiqiang Wang, Jian Peng, Feihu Huang, Jince Wang, Junhui Chen, and Yifei Xiao. Micn: Multi-scale localand global context modeling for long-term series forecasting. In The eleventh international conference onlearning representations, 2023. Shiyu Wang, Haixu Wu, Xiaoming Shi, Tengge Hu, Huakun Luo, Lintao Ma, James Y Zhang, and Jun Zhou.Timemixer: Decomposable multiscale mixing for time series forecasting. arXiv preprint arXiv:2405.14616,2024.",
  "Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet: Temporal2d-variation modeling for general time series analysis. arXiv preprint arXiv:2210.02186, 2022": "Maxwell Xu, Alexander Moreno, Supriya Nagesh, Varol Aydemir, David Wetter, Santosh Kumar, and James MRehg. Pulseimpute: A novel benchmark task for pulsative physiological signal imputation. Advances inNeural Information Processing Systems, 35:2687426888, 2022. Kun Yi, Qi Zhang, Wei Fan, Shoujin Wang, Pengyang Wang, Hui He, Defu Lian, Ning An, Longbing Cao,and Zhendong Niu. Frequency-domain mlps are more effective learners in time series forecasting. arXivpreprint arXiv:2311.06184, 2023. Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecasting? InProceedings of the AAAI conference on artificial intelligence, volume 37, pp. 1112111128, 2023.",
  "There are 10 datasets used in this study, and the overall summary of the datasets information can be seen in. Also, the descriptions of each set are introduced in this section": "ETT (Electricity Transformer Temperature): The ETT dataset (Zhou et al., 2021) includes detailedrecords of electricity transformer temperatures and corresponding electricity consumption data. This high-resolution dataset, sampled every 15 minutes, covers two years. It provides insights into the relationshipbetween transformer temperatures and electricity demand, crucial for maintaining efficient and safe operationsof power systems. In our research, we utilize the ETT dataset to forecast short-term electricity demand andtransformer temperatures. Electricity Load Diagrams: The Electricity Load Diagrams dataset, sourced from the UCI Machine Learn-ing Repository (Asuncion & Newman, 2007), comprises electricity consumption data recorded at 15-minute",
  "A.2Imputation Settings": "In the context of time series imputation, masking refers to the process of artificially removing or concealingparts of the data to simulate missing values, which the model then attempts to impute. Two distinct maskingapproaches are commonly employed: random masking and extended masking. Random Masking: This method involves randomly selecting points or segments within the time seriesdata and setting them as missing or masked. The randomness of this approach is intended to simulate datamissingness that occurs sporadically and without a specific pattern, which is a common scenario in real-worlddatasets. However, this method may not adequately represent scenarios where data is missing for extendedperiods, which is also a practical occurrence. Extended Masking: To address the limitations of random masking, we introduce the extended maskingapproach. This technique creates artificial gaps by masking contiguous segments of the time series. Extendedmasking is designed to simulate scenarios where data might be missing due to prolonged outages or systematicissues, providing a more challenging and realistic task for the imputation model to tackle.",
  "A.3.2Hyperparameter Setting": "In this section, we list some essential hyperparameters used in this study. For the design of AdaWave blocks,we adjust the depth of transformations in a range of 1 to 5; whereas the kernel size of the utilized convolutionalkernels is adjusted based on the datasets. The number of clusters used in the grouped linear model variesfrom 1 to 9 depending on the channels of signals. Also, we adjust the learning rate for each set of experimentsto achieve better convergent performances. Further, to determine the number of clusters in the proposedgroup linear module, we conduct extensive experiments as shown in . The detailed hyperparametersof each dataset are listed in .",
  "Avg.0.0690.0880.0800.1030.0700.0890.0690.0870.0820.103": "where f1 and f2 are the low and high frequencies respectively, controls the transient decay, representsthe linear trend, and (t) is Gaussian noise. This formulation captures fundamental non-stationary behaviorswhile maintaining interpretability. We first generate signals with a regular low-frequency component (5 Hz)and a high-frequency component (50 Hz) as the simple signal. Further, for increased complexity, we introducetraffic-like signals that incorporate daily and weekly cyclical patterns to mimic real-world traffic fluctuations:",
  "Similarly, electricity consumption-mimic signals simulate seasonal and daily variations typical of power usagedata:selectricity(t) = 5 sin(2t) + 2 sin(4t) + 3 sin(2365t) + 2t + (t)(16)": "Both complex models include trend components and noise to simulate realistic non stationary data. Fur-thermore, we implement optional variance shifts and step changes to test model robustness against abruptsignal changes. The variance shift is modeled as an increase in the amplitude of (t) at a specific time point,while the step change is represented by a sudden additive constant to the signal. Based on the three settings -including simple, traffic, and electricity - we further adjust (1) the variance shift values to be 1 and 2, formoderate and large shifts, (2) the step changes as -0.5 or 0.5 to indicate the negative and positive changes.The synthetic signals are visualized in . In total, for each signal, 1024 data points are generated,where the first 512 timestamps are used in training models. In the evaluation stage, we compare modelpredictions with a denoised signal, i.e., sin(2 5t) + sin(2 50t) e50(t0.5)2 + t. Also, the variance shiftsand step changes are only applied in the test phases. The performance of various models, including AdaWaveNet, DLinear, TimesNet, FreTS, PatchTST, andiTransformer, was evaluated on this synthetic non-stationary time series. summarizes the resultsof this case study. Further, we showcase two examples, including Simple and Traffic with variance shiftand step changes, in and 8. With the lowest MSE and MAE among all models, AdaWaveNetspredictions closely align with the ground truth, effectively capturing both the low-frequency sinusoidalcomponent and the high-frequency transient component. In contrast, other models struggle to varying degreesin accurately predicting the non-stationary signal. DLinear shows significant deviation from the ground truth,particularly in capturing the high-frequency components, which results in high MSE and MAE. FreTS andiTransformer demonstrate moderate performance but fail to capture the finer details of the signal, particularlythe high-frequency components.",
  "Amplitude": "(d-4) Approximation of Seasonal (Level 3) : Multi-level decomposition of a non-stationary time series using AdaWaveNet. The top plot showsthe input signal, followed by four levels of approximations (left column) and their corresponding waveletcoefficients (right column). Each level represents a coarser approximation of the original signal, with thecoefficients capturing the details lost between successive approximation levels.",
  ": Visualization of a forecasting task on traffic dataset. The length of both the input sequence andforecasting sequence is 96": "illustrates the multi-scale decomposition capabilities of AdaWaveNet when processing the generatednon-stationary time series. The approximation at each level represents a progressively coarser version of theoriginal signal, which effectively separates low-frequency trends from high-frequency details. Concurrently,the wavelet coefficients encode the fine-grained information lost between successive approximation levels. Thishierarchical decomposition allows AdaWaveNet to adapt to the signals changing characteristics at multiplescales. For instance, the Level 0 approximation retains much of the original signals structure, while Levels 1,2, and 3 focus on broader trends. This adaptive multi-scale approach enables the proposed model to learnboth local fluctuations and overarching patterns in non-stationary time series.",
  "C.1Forecasting": "presents a comparative example of forecasting future traffic volumes using various models. Thefigure reveals a notable disparity between past sequences and predicted sequences for this particular variate.The visual results indicate that AdaWaveNet yields the most accurate forecast in this instance. Additionally,the iTransformer model also performs commendably, which suggests that its channel-wise attention mechanismis particularly useful for analyzing past traffic patterns for prediction purposes.",
  "C.2.1Random Masking": "We provide an example of imputation with the random masking method. The proposed AdaWaveNet andall the other baseline methods. As shown in , AdaWaveNet, alongside baseline methods suchas PatchTST, TimesNet, and Nonstationary-Transformer, is capable of capturing the temporal dynamics.Notably, AdaWaveNet shows superior performance in imputing fine-grained details, effectively handling boththe seasonality during peak phases and the underlying trends in flatter regions.",
  "C.2.2Extended Masking": "We provide an example of imputation with the extended masking method. The proposed AdaWaveNet and allthe other baseline methods. Shown as in , the proposed method exhibits a close approximation tothe ground truth, which indicates a higher predictive accuracy within this interval. The consistency across themodels outside the masked region implies a shared ability to capture the temporal dynamics in non-maskedintervals; while the differences within the masked region highlight the distinct predictive capabilities andpotential overfitting issues of the individual models.",
  "C.3Super-resolution": "The visualization presents the results of a super-resolution task on time series data, specifically forecastingtraffic volume. Each subplot represents the performance of a different model: AdaWaveNet, iTransformer,FreTS, TimesNet, DLinear, PatchTST, Non-stationary Transformer, FiLM, and FEDformer. In each plot,three lines are denoted as the Ground Truth (blue line), which is the actual high-resolution data; thePrediction (orange line), which is the models predicted high-resolution data; and the Low-resolution Input(gray line), which serves as the models input data and represents the downsampled or coarse version of theGround Truth.",
  ": Visualization of an extended imputation task on ETTh1 dataset. The sequence length is 96 andthe masked ratio is 0.25": "The predictions of AdaWaveNet closely follow the Ground Truth across the entire sequence. The fidelityof AdaWaveNets prediction to the Ground Truth, particularly in capturing the peaks and troughs of thetraffic volume, showcases the models capability in the super-resolution task. The granularity of details inthe prediction suggests that AdaWaveNet effectively upsamples the low-resolution input and reconstructsnuanced and accurate traffic data."
}