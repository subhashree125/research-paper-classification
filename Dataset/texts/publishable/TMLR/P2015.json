{
  "Abstract": "Operator learning has been increasingly adopted in scientific and engineering applications,many of which require calibrated uncertainty quantification. Since the output of operatorlearning is a continuous function, quantifying uncertainty simultaneously at all points in thedomain is challenging. Current methods consider calibration at a single point or over onescalar function or make strong assumptions such as Gaussianity. We propose a risk-controllingquantile neural operator, a distribution-free, finite-sample functional calibration conformalprediction method. We provide a theoretical calibration guarantee on the coverage rate,defined as the expected percentage of points on the function domain whose true value lieswithin the predicted uncertainty ball. Empirical results on a 2D Darcy flow and a 3D carsurface pressure prediction task validate our theoretical results, demonstrating calibratedcoverage and efficient uncertainty bands outperforming baseline methods. In particular, onthe 3D problem, our method is the only one that meets the target calibration percentage(percentage of test samples for which the uncertainty estimates are calibrated) of 98%.Code is available at module).",
  "Introduction": "Neural operators have been increasingly adopted to solve partial differential equations (PDE), demonstratinga significant speedup over traditional numerical methods. Their applications span various scientific andengineering domains, including weather forecasting (Pathak et al., 2022), carbon capture (Wen et al., 2023),automotive design (Li et al., 2023), and nuclear fusion(Gopakumar et al., 2023) to name a few. Onekey challenge in their adoption in real-world scenarios lies in uncertainty quantificationthe capabilityto estimate how uncertain the model output is. Applications like plasma evolution prediction for nuclearfusion (Gopakumar et al., 2023) and pressure field prediction for automotive design (Li et al., 2023) aresafety-critical and thus require reliable quantification of model uncertainty. Applications such as extremeweather forecastingKurth et al. (2023); Lam et al. (2023); Persson & Grazzini (2007), carbon capture andstorage (Wen et al., 2023) affect high-impact decision-making, which also requires uncertainty quantification.Existing methods for uncertainty quantification face challenges in three key areas, viz., lack of function-spacecoverage, distribution-free calibration guarantee, and scalability. Function-space formulation: Neural operators provide function-space solutions that can be evaluated atany point in the domain. The necessary formulation of uncertainty for neural operators requires to be on",
  "Simultaneous Point-wise Uncertainty (blue dotted)": ": Overall schematic of UQNO, a risk-controlling quantile neural operator. In operator learning, thelearned neural operator outputs a function (red dots sampled at grid points). We train a residual operator withgeneralized quantile loss and then calibrate with conformal prediction, which yields simultaneous pointwiseuncertainty estimates with a PAC guarantee on calibration coveragethe expected percentage of true value(black) that lies within our predicted uncertainty bands (green=upper bound and yellow=lower bound). Inthis example, since the output is 1D, we output a pointwise uncertainty band. In higher dimensions, weoutput a pointwise heterogeneous uncertainty ball. the function space, which provides simultaneous uncertainty estimation for all points. For example, in anautomotive design setting that uses neural operators to predict surface pressure Li et al. (2023), simultaneousuncertainty estimates on the whole surface can inform designers of structured error around the front faceridge, as shown in , whereas a single-point or aggregate measure of uncertainty cannot convey suchinformation. Uncertainty quantification in function spaces is more challenging than naively combining point predictions inthe function space. Even if we obtain point-wise guarantees on calibration with probability 1p, the standardunion bound leads to a loose bound on the probability of simultaneous calibrations. Prior work focuses onsingle-point uncertainty estimation (Guo et al., 2023), and theoretical developments investigate uncertaintyquantification in the function space, yet focus on transformed formulations such as functional projection orscalar function properties such as pseudo-density or loss (Lei et al., 2015; Benitez et al., 2023). Distribution-free calibration guarantee: Classical deep learning uncertainty quantification methodssuch as MCDropout (Gal & Ghahramani, 2016) or ensemble learning (Maddox et al., 2019) do not providecalibration guarantees. These methods output mean and variance estimates under Gaussian assumptions,and the results are generally heuristic. Recent works on uncertainty quantification for operator learning areeither heuristic (Guo et al., 2023; Akhare et al., 2023; Nehme et al., 2023), or rely on Gaussian assumptionsand approximations that may not hold in real-world settings (Magnani et al., 2022). Heuristic uncertaintyestimation is insufficient for safety-critical or high-impact applications. We need a method with a rigorouscalibration guarantee that works in the wild\", where distributional assumptions might be broken. Sincereal-world applications have finite data, we also want our calibration guarantee to be finite-sample ratherthan asymptotic. Scalability:Frameworks such as Bayesian inference are principled, yet challenging to scale up. For example,Meng et al. (2022) considers problems with per-function samples on the order of hundreds, yet many practicalapplications are magnitudes larger in scale, e.g., 7.2 million mesh points in fluid dynamics Li et al. (2023),creating computational challenges for such methods. We present an uncertainty-quantified neural operator (UQNO) framework that simultaneously addresses thesechallenges by leveraging the conformal prediction framework. UQNO is a distribution-free and finite-samplefunctionally calibrated conformal prediction method built on the framework of risk-controlling quantileoperator learning that leverages the classical conformal prediction principle (Lei et al., 2015; Angelopoulos &Bates, 2021). A high-level schematic of our method is shown in . We develop a generalized quantileloss extended to the function space to train a neural operator that takes a function as input and outputsa heuristic uncertainty band that can be queried at any point. Then, we provide the conformal prediction",
  "framework to calibrate the risk-controlling prediction set for operator learning. This framework enablesfunction prediction along with point-wise calibrated coverage uncertainty": "We demonstrate that our method satisfies the desired calibration guarantee while providing efficient uncertaintybands, outperforming baselines in a 2D Darcy flow problem, and a 3D automotive surface pressure fieldprediction. In the 2D Darcy flow problem, our method provides uncertainty bands that are 1.52x tighter thanMCDropout and 76.1x tighter than Laplace approximation while satisfying the target calibration percentageof 98%. In the 3D car surface pressure field prediction problem, our method is the only method that satisfiesthe target calibration percentage of 98%. Our main contributions are as follows:",
  "G = minG E(a,u)l(G(a), u)(1)": "where a A = A(D, Rda) is the input function, u U = U(D, Rdu) is the output function, D Rd iscompact and A, U are separable Banach spaces. We aim to learn the operator G which minimizes expectedloss l : U U R over the distribution of input-output functions. The point estimate of operator G : A Udoes not give information of uncertainty beyond its empirical loss on available data. With uncertaintyquantification, we want to output an uncertainty set instead of a point estimate. We formulate the uncertaintyquantification problem as finding",
  "Published in Transactions on Machine Learning Research (08/2024)": "Author ContributionsZ. Ma came up with the method, developed the theory, motivated the problem, and performed the experiments.D. Pitt merged the project code with the neural operator library and reproduced the results. K. Azizzadenesheliand A. Anandkumar advised on the overall direction of the project. AcknowledgmentsZ. Ma is supported by the Kortschak Scholarship. A.Anandkumar is supported by the Bren Named Chair,Schmidt AI 2050 Senior fellow, and ONR (MURI grant N00014-18-12624). The authors thank Jean Kossaififor several helpful discussions regarding code reproducibility. The authors thanks Julius Berner, Zongyi Li,and Nikola Kovachki for helpful discussions. Niccol Ajroldi, Jacopo Diquigiovanni, Matteo Fontana, and Simone Vantini. Conformal prediction bands fortwo-dimensional functional time series. Computational Statistics & Data Analysis, pp. 107821, 2023.",
  "C(a)(x) = {p Rdu: p G(a)(x)2 E(a)(x)}(4)": "For a given function a and given point x, the set defined above is the set of points in the co-domain ofoutput function u whose distance (measured by 2-norm) from the base operator prediction is no greaterthan E(a)(x), i.e. the true value lies within a ball centered at the base operator prediction with a radiusE(a)(x) scaled by . In our implementation, E is parameterized by a neural operator as described in .1.",
  "Pre-Calibration Quantile Neural Operator": "We use state-of-the-art neural operator architectures to parameterize E in in equation 4Fourier NeuralOperator (FNO) (Li et al., 2021) for the 2D Darcy problem and its variant Geometric-Informed NeuralOperator (Li et al., 2023) for the 3D pressure field prediction problem. We generalize quantile loss (Koenker,2005; Chung et al., 2021) to the operator learning setting. Similar to the canonical quantile loss formulation,this loss penalizes out-of-band distance weighted by 1 and in-band distance weighted by , and thusencourages the model to output a value close to the 1 percentile of the error magnitude seen duringtraining. E is trained with the loss defined below:",
  "Calibration via Split Conformal Prediction": "We leverage conformal prediction as an overall framework (Vovk et al., 2005), which provides a finite-sample, distribution-free confidence set for a scalar-value prediction problem, utilizing calibration set that isexchangeable with test data. Split conformal prediction is used to avoid re-calibration for each new test datapoint Vovk et al. (2020). Note that we do not require training data to come from the same distribution. Under this framework, given training set Dtrain = {(ati, uti)}, a calibration set Dcal = {(aci, uci)|i = 1, ...n},and nonconformity score function s : (A, U) R, under the assumption that the calibration set and testset are exchangeable, the possible outcomes which the 1 quantile of si obtained on the calibrationset provides a valid estimate of the 1 quantile on the test set, we have the following: Given trainingset Dtrain = {(ati, uti)}, calibration set Dcal = {(aci, uci)|i = 1, ...n}, we can define any nonconformity scorefunction s : (A, U) R and calculate the score for all samples in the calibration set, i.e. si = s(aci, uci). i.e.,let",
  ": Predict: given a test input function a and any x Rd, output uncertainty ball B(G(a)(x), E(a)(x)),which is a (, ) risk-controlling prediction equation 3": "This method has three advantages: (1) Heteroscedasticity: The base uncertainty estimator is parametrizedas a neural operator Li et al. (2021; 2023), which can predict higher uncertainty for input samples thatare dissimilar to training input. (2) Simultaneous pointwise prediction: Simultaneous prediction ofuncertainty estimates for all points on the domain allows for a structural understanding of error, as visualizedin Figures 2 and 5. This is not possible in prior methods such as Guo et al. (2023) and Benitez et al. (2023).(3) Controllablility: We note that both the domain coverage threshold and function-level coverage (equation 3) are user-specified. This is demonstrated empirically in Figures 3 and 4. We note that thecalibration guarantee of our method is an upper bound over the true calibration, i.e., the actual coverage isguaranteed to be no less than the target coverage. The sparser our sample points are (on the domain), themore conservative our band becomes. Our formulation naturally extends to scenarios where data is in various discretizations/irregular grid geome-tries. Different discretizations is regarded as different approximations of continuous functions that satisfyexchangeability. The theoretical derivation is presented in .2.",
  "UQNO0.0006298.6%0.00045 99.8%2 hours0.55077 98.2%0.40018 100%6 hours": "Laplace approximation Magnani et al. (2022): which takes point predictions as the Maximum a posteriori(MAP) estimator and does local Laplace approximation by leveraging the Hessian of the last layer weights. Notethat bandwidth is not defined for this method since its predicted uncertainty subspace is unbounded. Neural posterior principal components Nehme et al. (2023): which trains the model to not only predict apoint estimate but also the first k principal components of residual, thus outputting an uncertainty subspace\"based on these components. We focus on two aggregate metrics: (1) Calibration percentage: the percentage of functions in the test setthat satisfy the target threshold. For each test sample, satisfying the target threshold\" means over 1 of uniformly-sampled points on the function domain lie within our predicted uncertainty sets, as describedin Equation 4. Note that calibration percentage is an aggregate metric defined on the whole test set. Fora single instance, we also define coverage\" as the percentage of uniformly sampled points on the functiondomain that lie within predicted uncertainty sets for a specific function. This metric is used in single-instancevisualizations, as shown in Figures 2, 5. (2) Bandwidth: the average predicted uncertainty ball radiusaveraged across all sampled points on the domain. This metric shows how efficient a method istrivially, aninfinitely large uncertainty prediction will always have perfect calibration percentage, yet is uninformativesince the balls are not efficient. We demonstrate that our method provides good calibration and tight bandwidth, compared to baselines. TheDarcy flow task illustrates a data-rich setting with 5000 data points, whereas the car pressure prediction taskrepresents a data-scarce setting with a 500-sample training set for a 3D problem. provides statisticsof bandwidth and calibration percentage percentage for all methods on both tasks. We see our methodconsistently outputs the most efficient uncertainty bands that satisfy the calibration targets. In the data-richsetting of Darcy flow, we achieve up to 1.53x efficiency improvement in band tightness. More notably, in thedata-scarce problem of 3D car pressure prediction, all other baselines fail to provide calibrated uncertaintyestimates in both high-domain-threshold and low-domain-threshold settings.",
  "Laplace Approximation": "Coverage 1.000 0.0005 0.0010 0.0015 0.0020 : Uncertainty quantification comparison across methods on 2D Darcy flow problem. The leftmostheatmap plots true error. The top panels show the predicted pointwise uncertainty, and the bottom panelsshow the coverage (i.e. true error less than predicted error) for each point on the domainyellow points arecovered by our predicted uncertainty bands, and purple points are uncovered. The coverage percentage foreach method is shown above the bottom panels. Our method of UQNO predicts uncertainty that correspondswell with true error while providing 99.1% domain coverage. MCDropout does not capture the uncertainregion well. Laplace approximation greatly overestimates uncertaintyon the scale of 50 larger than trueerror, and thus appears all yellow in the heatmap.",
  "u(x) = 0x = (0, 1)2(8)": "This is a data-rich scenario with 5000 total training data and 421 421 resolution, for which we obtain theground truth from prior work Li et al. (2021). We fix the same Fourier Neural Operator architecture for allmethods. For UQNO, we split the training set in half for training the base and the quantile model. provides a visualization of domain-level uncertainty predictions and coverage for one function sample forUQNO (our method), MCDropout, and Laplace approximation. UQNO captures the error structure well andprovides good domain-level coverage. In addition to coverage, UQNO also provides a tight band, as shown in Figures 3 and 7. Among the methodsthat satisfy calibration (green shaded), our method (purple dot) consistently provides the lowest bandwidth.We also plot the average bandwidth against calibration percentage for different methods in . UQNOprovides the tightest band among methods while providing a calibration percentage of 98.8%. We note thatthe neural posterior principal components method does not provide a finite band since it outputs a subspace,and thus, bandwidth does not apply. Nevertheless, we take the percentage of error variance falling in thepredicted subspace as its calibration percentage to compare with the other methods. We see the varianceratio falling into the dominant linear subspace is low (15%), suggesting the nonlinearity of the problem. Wefurther demonstrate the flexibility of UQNO by showing calibration results at different target calibrationpercentages (1 in Equation 3) and domain coverage thresholds (1 in Equation 3). There is a trade-offbetween the tightness of the uncertainty band and calibration percentageintuitively, an infinitely largeuncertainty set has a perfect calibration percentage yet is uninformative. By varying the domain threshold (in Equation 3) and target percentage ( in Equation 3), we calibrate UQNO to provide different guarantees. demonstrates the flexibility of our method by trading off band tightness with calibration percentage.Intuitively, wider bands correspond to a higher calibration percentage. We also show the efficacy of the calibration percentage guarantee in . The empirical results agree withour theoretical guarantee that the true calibration percentage should be no less than the expected calibrationpercentage of 1 as in Equation 3 for various domain threshold values. We note that by definition, ourmethod is conservative bound due to the finite resolution of data samples (the t term in Equation 7).",
  "Bandwidth": "alpha=2%alpha=4%alpha=6%alpha=8%alpha=10% : Bandwidth vs. calibration percentage trade-off of UQNO on 2D Darcy flow. Each curve shows thebandwidth vs. calibration percentage trade-off for a fixed domain threshold ( in Equation 3), demonstratingthe flexibility to achieve a higher calibration percentage with wider bands. Overall bandwidth increases asthe domain threshold becomes more stringent (smaller ).",
  "D Car Surface Pressure Prediction": "This task aims to learn the solution operator for 3D computational fluid dynamics (CFD) simulations of theNavier-Stokes equation for car shapes from Umetani & Bickel (2018) modified from the Shape-Net datasetChang et al. (2015) car category. The car surface is represented as a 3D mesh of 3586 points, and we obtainground-truth time-averaged pressure fields by solving the Reynolds-averaged NavierStokes (RANS) equationwith a finite element solver Zienkiewicz et al. (2014) with Reynolds number 5x106 and inlet velocity 72km/has in Li et al. (2023). This is a data-scarce setting with only 500 total training samples on a 3D mesh of 3586points. We have access to the signed distance function (SDF) and point-cloud representations as input andaim to predict the pressure at every mesh point on the car surface. compares UQNO, MCDropout,and Laplace approximation on a sample car shape and shows UQNO to capture the error structure andprovides good coverage of 99.7%, albeit being conservative (over-estimates uncertainty).",
  "Coverage=0.9997": ": Uncertainty quantification comparison across methods on 3D car pressure prediction problem. Theleftmost heatmap plots true error. The top panels show the predicted pointwise uncertainty, and the bottompanels show the coverage (i.e. true error less than predicted error) for each point on the domainyellow pointsare covered by our predicted uncertainty bands, and purple points are uncovered. The coverage percentagefor each method is shown above the bottom panels. Our method, although conservative (over-estimatesuncertainty), is able to capture the error pattern on both the top and bottom of the front face of the car.Due to the conservative nature of our calibration procedure, our method satisfies the coverage threshold of> 98% (actual 99.7%). MCDropout, although on the same scale as true error, misses the top error regionand fails to meet the coverage threshold. Laplace approximation greatly over-estimates the error and doesnot capture the error structure well. 0.880.900.920.940.960.981.00 Target calibration percentage 0.88 0.90 0.92 0.94 0.96 0.98 1.00 Actual calibration percentage alpha=2%alpha=4%alpha=6%alpha=8%alpha=10%",
  "C : A U,U = U(D, B(Rdu))(9)": "where B(Rdu) is the Borel set on Rdu. C evaluated at any function a A gives a correspondence (Aliprantis &Border, 2006) (D, B(Rdu), C(a)) which maps any point x D to a prediction set on Rdu, the co-domain of u.To obtain uncertainty quantification with conformal guarantee, we want to construct a (, )-risk-controllingconfidence set satisfies:",
  "Mixed Discretization": "For the fixed discretization case, instead of sampling m points from the domain for each function in calibrationset, we assume mi points are sampled for each input-output function pair (ai, ui). We assume test sample(a, u) has discretization m. The underlying assumption is that even though our query discretization changes,the functions are still exchangeable. Thus, our main analysis still holds, and we just need to account fordifferent discretization errors caused by the varying sampling resolution. The definition of nonconformityscore is discretization-dependent, i.e.",
  "Related Work": "Conformal Prediction: The conformal prediction framework was introduced in the 2000s (Vovk et al.,2005) to provide distribution-free and finite-sample uncertainty estimates for scalar values. It has subsequentlybeen extended to settings such as regression (Romano et al., 2019), time-series forecasting (Ajroldi et al.,2023), imaging (Angelopoulos et al., 2022), and functional data analysis (Lei et al., 2015; Diquigiovanniet al., 2022; 2021). In this work, we provide a conformal prediction framework for operator learning, and ourmain distinction from prior work is heteroscedasticity on function spaces. Methods based on modulation orpseudo-density (Lei et al., 2015) may limit learning statistical error patterns across samples and do not adaptuncertainty estimates based on each test sample. We provide heteroscedastic estimates by combining theconformal prediction framework with a base quantile neural operator. Approximate Gaussian Methods: Many existing methods for uncertainty rely on Gaussian assumptionsor approximations that result in heuristic, rather than rigorous uncertainty estimates. For example, Magnaniet al. (2022) allows for sampling from a Gaussian process to approximate the posterior operator via Laplaceapproximation, which we have shown to overestimate errors. Akhare et al. (2023) similarly relies on Gaussianassumption and over-estimates uncertainty in empirical studies. (Zou et al., 2023) also uses Gaussianassumption to learn a pointwise noise variance as a heuristic, which does not give a coverage guarantee. Byleveraging the conformal prediction framework, we obtain principled uncertainty estimates. Data Uncertainty: Bayesian methods (Zou et al., 2023; Meng et al., 2022) provide the advantage ofdecomposing uncertainty sources, yet face challenges in scaling up to high-dimension problems. Nehmeet al. (2023) studies data uncertainty by learning low-dimensional latent representations of error providesinterpretability, but similar faces challenges when an error does not lie in a low-dimensional manifold as",
  "problem dimension grows. Our method focuses on model uncertainty and is complementary to these works.How to incorporate uncertainty decomposition into our method remains an open area for future work": "Uncertainty in Operator Learning: As operator learning starts to gain popularity, various works startto explore how to formulate uncertainty in this setting. Guo et al. (2023) explores variance estimation butonly provides standard deviation under Gaussian assumption and is constrained to single-point uncertaintyestimates. Benitez et al. (2023) approaches uncertainty estimation from a statistical learning lens, and focuseson loss. We provide a bound simultaneously for all points on the domain that is able to capture errorstructure.",
  "Conclusion": "We show a method of risk-controlling neural operator, which leverages conformal prediction and quantile lossto provide principled uncertainty estimates simultaneously for all points on the domain. We show empiricallyon a 2D Darcy flow problem and a 3D car pressure prediction problem that our method consistently providesthe tightest band while satisfying target calibration percentage among various uncertainty quantificationmethods. Notably, in the 3D problem, our method is the only method that satisfies target calibrationpercentage of 98%. We also show that our method helps reveal interpretable error structures via visualization.In addition to empirical results, we also provide proof that shows the calibration percentage guarantee of ourmethod. While our method is the first introduction of conformal prediction to operator learning, we note that, ourmethod is prescriptive and comes with a principled calibration guarantee, we still rely on a good heuristicuncertainty estimator (E in equation 4) to obtain tight uncertainty balls. In the worst case, if the heuristicestimator E is completely uninformative, our method will yield very wide uncertainty bands. In this work,we focused on expectation over the domain, which is an average, as a measure of risk. Often, other notionsof risks, including the worst-case formulation, may be of interest that we leave for future work. Thereare still many open problems in extending principled uncertainty quantification to scientific ML problemsinvolving PDEs. For example, uncertainty decomposition, better representations of error structure, andefficient sampling are all important directions for future work.",
  "Anastasios N Angelopoulos and Stephen Bates. A gentle introduction to conformal prediction and distribution-free uncertainty quantification. arXiv preprint arXiv:2107.07511, 2021": "Anastasios N Angelopoulos, Amit Pal Kohli, Stephen Bates, Michael Jordan, Jitendra Malik, Thayer Alshaabi,Srigokul Upadhyayula, and Yaniv Romano. Image-to-image regression with distribution-free uncertaintyquantification and applications in imaging. In International Conference on Machine Learning, pp. 717730.PMLR, 2022. Stephen Bates, Anastasios Angelopoulos, Lihua Lei, Jitendra Malik, and Michael Jordan. Distribution-free,risk-controlling prediction sets. J. ACM, 68(6), sep 2021. ISSN 0004-5411. doi: 10.1145/3478535. URL J. Antonio Lara Benitez, Takashi Furuya, Florian Faucher, Anastasis Kratsios, Xavier Tricoche, and Maarten V.de Hoop. Out-of-distributional risk bounds for neural operators with applications to the helmholtz equation,2023. Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, SilvioSavarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu. Shapenet: Aninformation-rich 3d model repository, 2015. Youngseog Chung, Willie Neiswanger, Ian Char, and Jeff Schneider. Beyond pinball loss: Quantile methods forcalibrated uncertainty quantification. Advances in Neural Information Processing Systems, 34:1097110984,2021. Jacopo Diquigiovanni, Matteo Fontana, and Simone Vantini. The importance of being a band: Finite-sampleexact distribution-free prediction sets for functional data. arXiv preprint arXiv:2102.06746, 2021.",
  "Roger Koenker. Quantile regression, volume 38. Cambridge university press, 2005": "Thorsten Kurth, Shashank Subramanian, Peter Harrington, Jaideep Pathak, Morteza Mardani, DavidHall, Andrea Miele, Karthik Kashinath, and Anima Anandkumar. Fourcastnet: Accelerating globalhigh-resolution weather forecasting using adaptive fourier neural operators. In Proceedings of the Platformfor Advanced Scientific Computing Conference, PASC 23, New York, NY, USA, 2023. Association forComputing Machinery. ISBN 9798400701900. doi: 10.1145/3592979.3593412. URL Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertaintyestimation using deep ensembles. Advances in neural information processing systems, 30, 2017. Remi Lam, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger, Meire Fortunato, Ferran Alet,Suman Ravuri, Timo Ewalds, Zach Eaton-Rosen, Weihua Hu, Alexander Merose, Stephan Hoyer, GeorgeHolland, Oriol Vinyals, Jacklynn Stott, Alexander Pritzel, Shakir Mohamed, and Peter Battaglia. Learningskillful medium-range global weather forecasting. Science, 382(6677):14161421, 2023. doi: 10.1126/science.adi2336. URL",
  "Jing Lei, Alessandro Rinaldo, and Larry Wasserman. A conformal prediction approach to explore functionaldata. Annals of Mathematics and Artificial Intelligence, 74:2943, 2015": "Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart,and Anima Anandkumar. Neural operator: Graph kernel network for partial differential equations. arXivpreprint arXiv:2003.03485, 2020. Zongyi Li, Nikola Borislavov Kovachki, Kamyar Azizzadenesheli, Burigede liu, Kaushik Bhattacharya, AndrewStuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. InInternational Conference on Learning Representations, 2021. URL Zongyi Li, Nikola Borislavov Kovachki, Chris Choy, Boyi Li, Jean Kossaifi, Shourya Prakash Otta, Moham-mad Amin Nabian, Maximilian Stadler, Christian Hundt, Kamyar Azizzadenesheli, and Anima Anandkumar.Geometry-informed neural operator for large-scale 3d pdes, 2023. Wesley J Maddox, Pavel Izmailov, Timur Garipov, Dmitry P Vetrov, and Andrew Gordon Wilson. A simplebaseline for bayesian uncertainty in deep learning. Advances in neural information processing systems, 32,2019. Emilia Magnani, Nicholas Krmer, Runa Eschenhagen, Lorenzo Rosasco, and Philipp Hennig. Approximatebayesian neural operators: Uncertainty quantification for parametric pdes. arXiv preprint arXiv:2208.01565,2022. Xuhui Meng, Liu Yang, Zhiping Mao, Jos del guila Ferrandis, and George Em Karniadakis. Learningfunctional priors and posteriors from data and physics. Journal of Computational Physics, 457:111073,May 2022. ISSN 0021-9991. doi: 10.1016/j.jcp.2022.111073. URL",
  "Vladimir Vovk, Ivan Petej, Ilia Nouretdinov, Valery Manokhin, and Alexander Gammerman. Computationallyefficient versions of conformal predictive distributions. Neurocomputing, 397:292308, 2020": "Gege Wen, Zongyi Li, Qirui Long, Kamyar Azizzadenesheli, Anima Anandkumar, and Sally M Benson.Real-time high-resolution co 2 geological storage prediction using nested fourier neural operators. Energy& Environmental Science, 16(4):17321741, 2023. O.C. Zienkiewicz, R.L. Taylor, and P. Nithiarasu. The finite element method for fluid dynamics. In The FiniteElement Method for Fluid Dynamics (Seventh Edition), pp. xxxvxxxvi. Butterworth-Heinemann, Oxford,seventh edition edition, 2014. ISBN 978-1-85617-635-4. doi: URL"
}