{
  "Abstract": "We consider the problem of estimating probability density functions based on sample data,using a finite mixture of densities from some component class. To this end, we introduce theh-lifted KullbackLeibler (KL) divergence as a generalization of the standard KL divergenceand a criterion for conducting risk minimization. Under a compact support assumption, weprove an O(1/n) bound on the expected estimation error when using the h-lifted KLdivergence, which extends the results of Rakhlin et al. (2005, ESAIM: Probability andStatistics, Vol.9) and Li & Barron (1999, Advances in Neural Information ProcessingSystems, Vol. 12) to permit the risk bounding of density functions that are not strictlypositive. We develop a procedure for the computation of the corresponding maximum h-lifted likelihood estimators (h-MLLEs) using the Majorization-Maximization framework andprovide experimental results in support of our theoretical bounds.",
  "Introduction": "Let (, A, P) be an abstract probability space and let X : X be a random variable taking valuesin the measurable space (X, F), where X is a compact metric space equipped with its Borel -algebra F.Suppose that we observe an independent and identically distributed (i.i.d.) sample of random variablesXn = (Xi)i[n], where [n] = {1, . . . , n}, and that each Xi arises from the same data generating process asX, characterized by the probability measure F on (X, F), with density function f = dF/d, for some-finite . In this work, we are concerned with the estimating f via a data dependent double-index sequenceof estimators (fk,n)k,nN, where",
  "for each k, n N, and whereP = (; ) : X R0 | Rd,(1)": "k = (1, . . . , k, 1, . . . , k), and d N. To ensure the measurability and existence of various optima, weshall assume that is Carathodory in the sense that (; ) is (X, F)-measurable for each , and (X; )is continuous for each X X. In the definition above, we can identify the set Ck = cok (P) as the set of density functions that can bewritten as a convex combination of k elements of P, where P is often called the space of component densityfunctions. We then interpret Ck as the class of k-component finite mixtures of densities of class P, as studied,for example, by McLachlan & Peel (2004); Nguyen et al. (2020; 2022b).",
  ",": "where (Zi)i[n] are independent and identically distributed samples from a distribution with density (f+h)/2.This sampling can be performed by choosing Xi with probability 1/2 or Yi with probability 1/2 for eachi [n], where Xi is an observation from the generative model f and Yi is an independent sample fromthe auxiliary density h. Although the modified estimator, based on the bound from Rakhlin et al. (2005),attains equivalent convergence rates, it inefficiently utilizes observed data, as 50% of the data is replacedby simulated samples Yi. In contrast, our h-MLLE estimator maximally utilizes all available data whileachieving the same bound.",
  "E f fk,n22, infqC f q22, c11k + c21n,": "c1, c2 > 0, without the lower bound assumption on f, fk above, even permitting X to be unbounded. Viathe main results of Naito & Eguchi (2013), the bound above can be generalized to the U-divergences, whichincludes the special L2() norm distance as a special case. On the one hand, the sequence of MLEs required for the results of Li & Barron (1999) and Rakhlin et al.(2005) are typically computable, for example, via the usual expectationmaximization approach (cf. McLach-lan & Peel 2004, Ch. 2). This contrasts with the computation of least-squares density estimators of form(3), which requires evaluations of the typically intractable integral expressions fk (; k)22. However, theleast-squares approach of Klemel (2007) permits the analysis using families P of usual interest, such asnormal distributions and beta distributions, the latter of which being compactly supported but having den-sities that cannot be bounded away from zero without restrictions, and thus do not satisfy the regularityconditions of Li & Barron (1999) and Rakhlin et al. (2005).",
  "Main contributions": "We propose the following h-lifted KL divergence, as a generalization of the standard KL divergence to addressthe computationally tractable estimation of density functions which do not satisfy the regularity conditionsof Li & Barron (1999) and Rakhlin et al. (2005). The use of the h-lifted KL divergence has the possibilityto advance theories based on the standard KL divergence in statistical machine learning. To this end, leth : X R0 be a function in L1(), and define the h-lifted KL divergence by:",
  "In the sequel, we shall show that KLh is a Bregman divergence on the space of probability density functions,as per Csiszr (1995)": "Assume that h is a probability density function, and let Yn = (Yi)i[n] be a an i.i.d. sample, independent ofXn, where each Yi : X is a random variable with probability measure on (X, F), characterized by thedensity h with respect to . Then, for each k and n, let fk,n be defined via the maximum h-lifted likelihoodestimator (h-MLLE; see Appendix B for further discussion) fk,n = fk (; k,n), where",
  "for some constants c1, c2 > 0, without requiring the strict positivity assumption that f, fk a > 0": "This result is a compromise between the works of Li & Barron (1999) and Rakhlin et al. (2005), and Klemel(2007), as it applies to a broader space of component densities P, and because the required h-MLLEs (5) canbe efficiently computed via minorizationmaximization (MM) algorithms (see e.g., Lange 2016). We shalldiscuss this assertion in .",
  "Relevant literature": "Our work largely follows the approach of Li & Barron (1999), which was extended upon by Rakhlin et al.(2005) and Klemel (2007). All three texts use approaches based on the availability of greedy algorithmsfor maximizing convex functions with convex functional domains. In this work, we shall make use of the",
  "Published in Transactions on Machine Learning Research (11/2024)": "by other iterative methods, such as gradient descent or line-search based quasi-Newton methods. Quadraticconvergence rates near k can be achieved with a Newton method, though this forfeits the monotonicity(or stability) of the MM algorithm, as it is well-known that even in convex settings, Newtons method candiverge if the initialization is not properly handled. An additional advantage of the MM algorithm over Newtons method is its capacity to decompose theoriginal objective into a sum of functions where each component of k = (1, . . . , k, 1, . . . , k) is separablewithin the summation.In other words, we can independently optimize functions that depend only onsubsets of parameters, either (1, . . . , k) or each j for j = 1, . . . , k, thereby simplifying the iterativecomputation. This characteristic is noted after Equation 9 in the main text. Such decomposition can leadto computational efficiency by avoiding the need to compute the Hessian matrix for Newtons method orapproximations required by quasi-Newton methods. Specifically, in cases involving mixtures of exponentialfamily distributions such as the beta distributions discussed in .2, each parameter-separated problembecomes a strictly concave maximization problem, which can be efficiently solved (see Proposition 3.10 inSundberg, 2019).",
  "Organization of paper": "The manuscript is organized as follows. In , we formally define the h-lifted KL divergence as aBregman divergence and establish several of its properties. In , we present new risk bounds for theh-lifted KL divergence of the form (2). In , we discuss the computation of the h-lifted likelihoodestimator in the form of (5), followed by empirical results illustrating the convergence of (2) with respectto both k and n. Additional insights and technical results are provided in the Appendices at the end of themanuscript.",
  ". Linearity: dc11+c22(p, q) = c1d1(p, q) + c2d2(p, q) for c1, c2 0": "The properties for Bregman divergences between scalars can be extended to density functions and otherfunctional spaces, as established in Frigyik et al. (2008) and Stummer & Vajda (2012), for example. We alsodirect the interested reader to the works of Pardo (2006), Basu et al. (2011), and Amari (2016). The class of h-lifted KL divergences constitute a generalization of the usual KL divergence and are a subset ofthe Bregman divergences over the space of density functions that are considered by Csiszr (1995). Namely,let P be a convex set of probability densities with respect to the measure on X. The Bregman divergenceD : P P [0, ) between densities p, q P can be constructed as follows:",
  "Advantages of the h-lifted KL divergence": "When the standard KL divergence is employed in the density estimation problem, it is common to restrictconsideration of density functions to those bounded away from zero by some positive constant. That is, onetypically considers the smaller class of so-called admissible target densities P P (cf. Meir & Zeevi, 1997),whereP = {(; ) P | (; ) > 0} . Without this restriction, the standard KL divergence can be unbounded, even for functions with boundedL1 norms. For example, let p and q be densities of beta distributions on the support X = . That is,suppose that p, q Pbeta, respectively characterized by parameters p = (ap, bp) and q = (aq, bq), where",
  "where : R>0 R is the digamma function. Next, suppose that ap = bq and aq = bp = 1, which leads tothe simplificationKL (p || q) = (ap 1) { (ap) (1)}": "Since is strictly increasing, we observe that the right-hand side diverges as ap .Thus, the KLdivergence between beta distributions is unbounded. The h-lifted KL divergence in contrast does not sufferfrom this problem, and does not require the restriction to P. This allows us to consider cases where p, q Pare not bounded away from 0, as per the following result.",
  "Proof. See Appendix C.3": "Remark 4. Proposition 2 highlights the benefit of the h-lifted KL divergence being bounded for all continuousdensities, unlike the standard KL divergence, while satisfying a relationship similar to that between theKL divergence and the L2 norm distance. Moreover, the first inequality of Proposition 3 is a Pinsker-likerelationship between the h-lifted KL divergence and the total variation distance TV(f, g) = 1",
  "Main results": "Here we provide explicit statements regarding the convergence rates claimed in (6) via Theorem 5 andCorollary 6, which are proved in Appendix A.2. We assume that f is bounded above by some constant cand that the lifting function h is bounded above and below by constants a and b, respectively. Theorem 5. Let h be a positive density satisfying 0 < a h(x) b, for all x X. For any target densityf satisfying 0 f(x) c, for all x X and where fk,n is the minimizer of KLh over k-component mixtures,the following inequality holds:",
  "where c1 and c2 are positive constants": "Remark 7. Our results are applicable to any compact metric space X, with used in the experimentalsetup in .2 as a simple and tractable example to illustrate key aspects of our theory.There isno issue in generalizing to X = [m, m]d for m > 0 and d N, or more abstractly, to any compact subsetX Rd. Additionally, X could even be taken as a functional compact space, though establishing compactnessand constructing appropriate component classes P over such spaces to achieve small approximation errorsKLh (f || C) is an approximation theoretic task that falls outside the scope of our work. From the proof of Theorem 5 in Appendix A.2, it is clear that the dimensionality of X only influences ourbound through the complexity of the class P, specifically, the constant c0 log1/2 N(P, /2, )d, whichremains independent of both n and k.Here, N(P, , ) is the -covering number of P.In fact, theconstant with respect to k (u1 in Theorem 5) is entirely unaffected by the dimensionality of X. Thus, therates of our bound on the expected h-lifted KL divergence are dimension-independent and hold even when Xis infinite-dimensional, as long as there exists a class P such that c0 log1/2 N(P, /2, )d is finite.",
  "Numerical experiments": "Here we discuss the computability and computation of KLh estimation problems and provide empiricalevidence towards the rates obtained in Theorem 5.Specifically, we seek to develop a methodology forcomputing h-MLLEs, and to use numerical experiments to demonstrate that the sequence of expected h-lifted KL divergences between some density f and a sequence of k-component mixture densities from asuitable class P, estimated using n observations does indeed decrease at rates proportional to 1/k and 1/n,as k and n increase.",
  "(ii) Qn (k, k) Lh,n (k),": "for each k, k k. In this context, given a fixed k, the minorizer Qn (, k) should possess propertiesthat simplify it compared to the original objective Lh,n. These properties should make the minorizer moretractable and might include features such as parametric separability, differentiability, convexity, amongothers. In order to build an appropriate minorizer for Lh,n, we make use of the so-called Jensens inequality minorizer,as detailed in Lange (2016, Sec. 4.3), applied to the logarithm function. This construction results in aminorizer of the form",
  "(s)k= arg maxkkQnk, (s1)k,(9)": "for each s > 0, and where (0)kis user chosen and is typically referred to as the initialization of thealgorithm.Notice that for each s, (9) is a simpler optimization problem than (5).Writing (s)k=(s)1 , . . . , (s)k , (s)1 , . . . , (s)k, we observe that (9) simplifies to the separated expressions:",
  "sN": "Of course, we can provide stronger guarantees under additional assumptions. Namely, assume that (iii)k k, where k is an open set in a finite dimensional Euclidean space on which Lh,n and Qn (, k) isdifferentiable, for each k k. Then, under assumptions (i)(iii) regarding Lh,n and Qn, and due to thecompactness of k and the continuity of Qn on k k, Razaviyayn et al. (2013, Cor. 1) implies that(s)k",
  ", 3": "5, and f2(x) = 0 when x = 1/2, and hence neitherdensities are bounded away from 0, on X. Thus, the theory of Rakhlin et al. (2005) cannot be applied toprovide bounds for the expected KL divergence between MLEs of beta mixtures and the pair of targets. Wevisualize f1 and f2 in . 0.00.20.40.60.81.0 0.00.51.01.52.0 x density",
  ": Simulation target densities f1 (solid line) and f2 (dashed line)": "To observe the rate of decrease of the h-lifted KL divergence between the targets and respective sequencesof h-MLLEs, we conduct two experiments E1 and E2. In E1, our target density is set to f1 and h1 = (; 1/2, 1/2). For each n 210, . . . , 215and k {2, . . . , 8}, we independently simulate Xn and Yn witheach Xi and Yi (i [n]), i.i.d., from the distributions characterized by f1 and h1, respectively. In E2, wetarget f2 with h-MLLEs over the same ranges of k and n, but with h2 = (; 1, 1)the density of the uniformdistribution. For each k and n, we simulate Xn and Yn, respectively, from distributions characterized by f2and h2. In both experiments, we simulate r = 50 replicates of each (k, n)-scenario and compute the correspondingh-MLLEs, (fk,n,l)l[r], using the previously described MM algorithm. For each l [r], we compute thecorresponding negative log h-lifted likelihood between the target f and fk,n,l:",
  "Results": "We report the estimates along with 95% asymptotic confidence intervals for the parameters of (10) for E1and E2 in . Plots of the average negative log h-lifted likelihood values by sample sizes n and numbersof components k are provided in . :Estimates of parameters for fitted relationships (with 95% confidence intervals) between negativelog h-lifted likelihood values, sample size and number of mixture components for experiments E1 and E2.",
  "Est.1.471.496.754.361.0795% CI(1.48, 1.47)(0.58, 2.41)(2.17, 11.32)(3.91, 4.81)(0.97, 1.16)": "From , we observe that E [Kk,n,l] decreases with both n and k in both simulations, and that the ratesat which the averages decrease are faster than anticipated by Theorem 5, with respect to both n and k. Wecan visually confirm the decreases in the estimate of E [Kk,n,l] via . In both E1 and E2, the rateof decrease over the assessed range of n is approximately proportional to 1/n, as opposed to the anticipatedrate of 1/n, whereas the rate of decrease in k is far larger, at approximately 1/k1.87 for E1 and 1/k4.36 forE2. These observations provide empirical evidence towards the fact that the rate of decrease of E [Kk,n,l] is atleast 1/k and 1/n, respectively, for k and n, at least over the simulation scenarios. These fast rates of fitover small values of n and k may be indicative of a diminishing returns of fit phenomenon, as discussed inCadez & Smyth (2000) or the so-called elbow phenomenon (see, e.g., Ritter 2014, Sec. 4.2), whereupon therate of decrease in average loss for small values of k is fast and becomes slower as k increases, convergingto some asymptotic rate. This is also the reason why we do not include the outcomes when k = 1, as thedrop in E [Kk,n,l] between k = 1 and k = 2 is so dramatic that it makes our simulated data ill-fitted by anymodel of form (10). As such, we do not view Theorem 5 as being pessimistic in light of these phenomena,as it applies uniformly over all values of k and n.",
  "Conclusion": "The estimation of probability densities using finite mixtures from some base class P appears often in machinelearning and statistical inference as a natural method for modelling underlying data generating processes.In this work, we pursue novel generalization bounds for such mixture estimators. To this end, we introducethe family of h-lifted KL divergences for densities on compact supports, within the family of Bregmandivergences, which correspond to risk functions that can be bounded, even when densities in the class P arenot bounded away from zero, unlike the standard KL divergence. Unlike the least-squares loss, the corresponding maximum h-likelihood estimation problem can be computedvia an MM algorithm, mirroring the availability of EM algorithms for the maximum likelihood problemcorresponding to the KL divergence. Along with our derivations of generalization bounds that achieve thesame rates as the best-known bounds for the KL divergence and least square loss, we also provide numericalevidence towards the correctness of these bounds in the case when P corresponds to beta densities. Aside from beta distributions, mixture densities on compact supports that can be analysed under our frame-work appear frequently in the literature.For supports on compact Euclidean subset, examples includemixtures of Dirichlet distributions (Fan et al., 2012) and bivariate binomial distributions (Papageorgiou &David, 1994). Alternatively, one can consider distributions on compact Euclidean manifolds, such as mix-tures of Kent (Peel et al., 2001) distributions and von MisesFisher distributions (Banerjee et al., 2005,Ng & Kwong, 2022). We defer investigating the practical performance of the maximum h-lifted likelihoodestimators and accompanying theory for such models to future work. We express sincere gratitude to the Reviewers and Action Editor for their valuable feedback, which hashelped to improve the quality of this paper. Hien Duy Nguyen and TrungTin Nguyen acknowledge fundingfrom the Australian Research Council grant DP230100905.",
  "Cathy Maugis-Rabusseau and Bertrand Michel. Adaptive density estimation for clustering with gaussianmixtures. ESAIM: Probability and Statistics, 17:698724, 2013": "Colin McDiarmid. On the method of bounded differences. In J.Editor Siemons (ed.), Surveys in Combi-natorics, 1989: Invited Papers at the Twelfth British Combinatorial Conference, London MathematicalSociety Lecture Note Series, pp. 148188. Cambridge University Press, 1989. Colin McDiarmid. Concentration. In Michel Habib, Colin McDiarmid, Jorge Ramirez-Alfonsin, and BruceReed (eds.), Probabilistic Methods for Algorithmic Discrete Mathematics, pp. 195248. Springer BerlinHeidelberg, Berlin, Heidelberg, 1998.",
  "Tin Lok James Ng and Kwok-Kun Kwong. Universal approximation on the hypersphere. Communicationsin Statistics-Theory and Methods, 51:86948704, 2022": "Hien D Nguyen. An introduction to majorization-minimization algorithms for machine learning and statisticalestimation. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 7(2):e1198, 2017. Hien D Nguyen, TrungTin Nguyen, Faicel Chamroukhi, and Geoffrey J McLachlan.Approximations ofconditional probability density functions in Lebesgue spaces via mixture of experts models. Journal ofStatistical Distributions and Applications, 8(1):13, August 2021. Hien D Nguyen, Florence Forbes, Gersende Fort, and Olivier Capp. An online minorization-maximizationalgorithm. In Proceedings of the 17th Conference of the International Federation of Classification Societies,2022a. TrungTin Nguyen, Hien D Nguyen, Faicel Chamroukhi, and Geoffrey J McLachlan. Approximation by finitemixtures of continuous density functions that vanish at infinity.Cogent Mathematics & Statistics, 7:1750861, 2020. TrungTin Nguyen, Faicel Chamroukhi, Hien D Nguyen, and Geoffrey J McLachlan.Approximation ofprobability density functions via location-scale finite mixtures in Lebesgue spaces. Communications inStatistics - Theory and Methods, pp. 112, 2022b. TrungTin Nguyen, Hien D Nguyen, Faicel Chamroukhi, and Florence Forbes. A non-asymptotic approachfor model selection via penalization in high-dimensional mixture of experts models. Electronic Journal ofStatistics, 16(2):47424822, 2022c.",
  "for each k N, where (fk,n)kN are h-MLLEs defined via (5)": "As is common in many statistical learning/uniform convergence results (e.g., Bartlett & Mendelson, 2002,Koltchinskii & Panchenko, 2004), we employ the use of Rademacher processes and associated bounds. Let(i)i[n] be i.i.d. Rademacher random variables, that is P(i = 1) = P(i = 1) = 1/2, that are independentof (Xi)i[n]. The Rademacher process, indexed by a class of real measurable functions S, is defined as thequantity",
  "for s S. The Rademacher complexity of the class S is given by Rn(S) = E supsS|Rn(s)|": "In the subsequent section, we make use of the following result regarding the supremum of convex functions:Lemma 11 (Rockafellar, 1997, Thm. 32.2). Let be a convex function on a linear space T , and let S Tbe an arbitrary subset. Then,suppS (p) =suppco(S) (p) . In particular, we use the fact that since a linear functional of convex combinations achieves its maximumvalue at vertices, the Rademacher complexity of S is equal to the Rademacher complexity of co(S) (seeLemma 21). We consequently obtain the following result.Lemma 12. Let (i)i[n] be i.i.d. Rademacher random variables, independent of (Xi)i[n] and P be defined asabove. The sets C and P will have equal complexity, Rn(C) = Rn(P), and the supremum of the Rademacherprocess indexed by C is equal to the supremum on the basis functions of P:",
  "A.2Proofs": "We first present a result establishing a uniform concentration bound for the h-lifted log-likelihood ratios,which is instrumental in the proof of Theorem 5. Our proofs broadly follow the structure of Rakhlin et al.(2005), modified as needed for the use of KLh. Assume that 0 (; ) < c for some c R>0. For brevity, we adopt the notation: T(g)C = supgC|T(g)|.Theorem 13. Let X1, . . . , Xn be an i.i.d. sample of size n drawn from a fixed density f such that 0 f(x) c for all x X, and let h be a positive density with 0 < a h(x) b for all x X. Then, for eacht > 0, with probability at least 1 et,1n",
  "where c1, c2, and c3 are constants that depend on some or all of a, b, and c": "Remark 17. The approximation error characterises the suitability of the class C, i.e., how well functionsin C are able to estimate a target f which does not necessarily lie in C. The estimation error characterisesthe error arising from the estimation of the target f on the basis of the finite sample of size n.",
  "B.1Elementary derivation": "From Equation 4, we observe that if X arises from a measure with density f, and if we aim to approximatef with a density g cok(P) that minimizes the h-lifted KL divergence KLh with respect to f, then we candefine an approximator (referred to as the minimum h-lifted KL approximator) as",
  "t/n), the combined bound in the proof of Theorem 5 in Appendix A.2 is also of this order,as required": "Finally, to obtain the additional samples Yn = (Yi)i[n], we simply simulate Yn from the data-generatingprocess defined by h. Since we can choose h freely, selecting an h that facilitates easy simulation (e.g., huniform over X, which remains bounded away from zero on a compact set) is advisable for satisfying therequirements of our theorems.",
  "However, as highlighted in the foundational works of Li & Barron (1999) and Rakhlin et al. (2005), controllingthe expected riskEKLf || fk,n KL (f || C)": "requires that f for some strictly positive constant > 0. This requirement excludes many interestingdensity functions as targets, including those that vanish at the boundaries of X, such as the (; 1/2, 1/2)distribution, or those that vanish in the interior of X, such as examples f1 and f2 in .2. Consequently,the condition f is restrictive and often impractical in many data analysis settings.",
  "Using a sample Xn generated from the distribution given by f, the first term of the objective can beapproximated by 1": "nni=1 g(Xi), which is relatively simple. However, the second term involves an intractableintegral that cannot be approximated by Monte Carlo sampling from a fixed generative distribution, as itdepends on the optimization argument g. Thus, unlike the h-MLLE, it is not feasible to reduce this intractableintegral to a sample average, which implies the need for a numerical approximation in practice. This can be",
  "Ef fk,n2 infgC f g2": "can be bounded, as shown in the works of Klemel (2007) and Klemel (2009), even when minX f = 0. Incomparison with the minimum L2 estimator and the MLE, we observe that the h-MLLE allows risk boundingfor targets f not bounded from below (i.e., minX f = 0), without requiring intractable integral expressions.The h-MLLE achieves the beneficial properties of both the MLE and minimum L2 estimators, which is thefocus of our work. Other divergences and risk minimization schemes for estimating f, such as -likelihoods and Lq likelihood,could also be considered. The Lq likelihood, for instance, provides a maximizing estimator with a simplesample average expression, similar to the MLE and h-MLLE. However, it lacks a characterization in terms ofa proper divergence function, such as the KL divergence, h-lifted KL divergence, or L2 norm. Consequently,this estimator is often inconsistent, as observed in Ferrari & Yang (2010) and Qin & Priebe (2013). Thesestudies show that the Lq likelihood estimator may not converge meaningfully to f, even when f cok(P),for any fixed q R>0 \\ {1}, unless a sequence of maximum Lq likelihood estimators is constructed with qdepending on n and approaching 1 to approximate the MLE. Thus, the maximum Lq likelihood estimatordoes not yield the type of risk bound we require. Similarly, with the -likelihood (or density power divergence), the situation is comparable to that of theminimum L2 norm estimator, where the sample-based estimator involves an intractable integral that cannotbe approximated through SAA. Specifically, the minimum -likelihood estimator is defined as (cf. Basuet al., 1998):",
  "Xg1+d": "for > 0, which closely resembles the form of the minimum L2 estimator. Hence, the limitations of theminimum L2 estimator apply here as well, although a risk bound with respect to the -likelihood divergencecould theoretically be obtained if the computational challenges are disregarded.In .3, we citeadditional estimators based on various divergences and modified likelihoods. Nevertheless, in each case, oneof the limitations discussed here will apply.",
  "B.3Selection of the lifting density function h": "The choice of h is entirely independent of the data. In fact, h can be any density with respect to , satisfying0 < a h(x) b < for every x X. Beyond this requirement, our theoretical framework remainsunaffected by the specific choice of h. In , we explore cases where h is uniform and non-uniform,demonstrating convergence in both k and n that aligns with the predictions of Theorem 5. For practicalimplementation, as discussed in Appendix B.1, h serves as the sampling distribution for the sample averageapproximation (SAA) of the intractable integral Eh log{g + h}. Given its role as a sampling distribution, itis advantageous to select a form for h that is easy to sample from. In many applications, we find that theuniform distribution over X is an optimal choice for h, as it meets the bounding conditions. We observe that although calibrating h does not improve the rate, it does influence the constants in theupper bound of the final equation of Equation 19. Specifically, for each t > 0, with probability at least1 et,",
  "B.4Discussions regarding the sharpness of the obtained risk bound": "Similar to the role of Gaussian mixtures as the archetypal class of mixture models for Euclidean spaces, betamixtures represent the archetypal class of mixture models on the compact interval , as established inthe studies by Ghosal (2001); Petrone (1999). Just as Gaussian mixtures can approximate any continuousdensity on X = Rd to an arbitrary level of accuracy in the Lp-norm (Nguyen et al., 2020; 2021; 2022b),mixtures of beta distributions can similarly approximate any continuous density on X = with respectto the supremum norm (Ghosal, 2001; Petrone, 1999; Petrone & Wasserman, 2002). We will leverage thisproperty in the following discussion. Assuming the target f is within the closure of our mixture class C (i.e., KLh (f || C) = 0), setting kn = O(n)achieves a convergence rate in expected KLh of O(1/n) for the mixture maximum h-lifted likelihoodestimator (h-MLLE) fkn,n. An interesting question is whether this rate is tight and not overly conservative,given the observed rates in . We aim to investigate this question by discussing a lower bound for theestimation problem.",
  "sincesupx|f(x) g(x)| L2(f, g) KLh (f || g) ,": "for any 0 < h, with the second inequality due to Proposition 3. Thus, for a compact parameter space defining P, we assume KLh (f || C) = 0. Consequently, the rate of O(n1/4) for expected total variationdistance is achievable in the beta mixture model setting. This convergence is uniform in the sense that",
  "The target f2 from our simulations in belongs to the class of Lipschitz targets, and thus theimproved lower bound rate of O(n1/3) from Devroye & Lugosi (2001) applies. We can compare this with": "nb2 for Experiment 2 in , yielding an empirical rate in n of O(n1.03), with an exponent between1.07 and 0.98 (95% confidence), over the range n {210, . . . , 215}. Clearly, this observed rate is fasterthan the lower bound rate of O(n1/3), indicating that the faster rates observed in are due to smallvalues of n and k. As n increases, the rate must eventually decelerate to at least O(n1/3) when the targetf is Lipschitz on X, which is only marginally faster than our guaranteed rate of O(n1/4). Demonstratingthat O(n1/4) is minimax optimal for certain target classes f is a complex task, left for future exploration. Lastly, we note that our discussions in this section implies that the h-MLLE provides an effective and geneticmethod for obtaining estimators with total variation guarantees, which complements the comprehensivestudies on the topic presented in Devroye & Gyrfi (1985) and Devroye & Lugosi (2001).",
  "B.6Comparison of the MM algorithm and the EM algorithm": "Since the risk functional is not a log-likelihood, a straightforward EM approach cannot be used to computethe h-MLLE. However, by interpreting KLh as a loss between the target mixture (f +h)/2 and the estimator(fk,n + h)/2, an EM algorithm can be constructed using the standard admixture framework (see Lange,2013, .5). Remarkably, the EM algorithm for estimating (fk,n + h)/2, has the same form as ourMM algorithm, which leverages Jensens inequality (cf. Lange, 2013, .3). In fact, the majorizer inany EM algorithm results directly from Jensens inequality (see Lange, 2013, .2), making our MMalgorithm in .1 no more complex than an EM approach for mixture models. Beyond the EM and MM methods, no other standard algorithms typically address the generic estimation ofa k-component mixture model in cok(P) for a given parametric class P. Since our MM algorithm followsa structure nearly identical to the EM algorithm for the MLE of this problem, it has comparable iterativecomplexity. Notably, per iteration, the MM approach requires additional evaluations for both Xn and Yn,and for g(Xi) and h(Xi), so it requires a constant multiple of evaluations compared to EM, depending onwhether h is a uniform distribution or otherwise (typically by a factor of 2 or 4).",
  "B.7Non-convex optimization": "We note that the h-MLLE problem (and the corresponding MLE) are non-convex optimization problems.This implies that, aside from global optimization methods, no iterative algorithmwhether gradient-basedmethods like gradient descent, coordinate descent, mirror descent, or momentum-based variantscan beguaranteed to find a global optimum. Likewise, second-order techniques such as Newton and quasi-Newtonmethods also cannot be expected to locate the global solution. In non-convex scenarios, the primary assurancethat can be offered is convergence to a critical point of the objective function. In our case, this assurance isachieved by applying Corollary 1 from Razaviyayn et al. (2013), as discussed in .1. Notably, thisconvergence guarantee is consistent with that provided by other iterative approaches, such as EM, gradientdescent, or Newtons method. Additionally, it may be valuable to examine whether specific convergence rates can be ensured when thealgorithms iterates approach a neighborhood around a critical value. In the context of the MM algorithm, wecan affirmatively answer this question: since the h-MLLE objective is twice continuously differentiable withrespect to the parameter k, it satisfies the local convergence conditions outlined in Lange (2016, Proposition7.2.2). This result implies that if (s)klies within a sufficiently close neighborhood of a local minimizer k,the MM algorithm converges linearly to k. This behavior aligns with the convergence guarantees offered",
  "C.2Proof of Proposition 2": "Let f = f +h and g = g+h. Since h is positive, there exists some g such that g = infxX {g(x) + h(x)} > 0.Similarly, since X is compact, there exists some positive f such that 0 < f = supxX {f(x) + h(x)} < .Define M = supxX log{ f(x)/g(x)}. Then M < , and",
  "Then, Rn(A) = Rn(A), i.e., both A and A have the same Rademacher complexity": "Lemma 22 (van de Geer, 2016, Thm. 16.2). Let (Xi)i[n] be non-random elements of X and let F be aclass of real-valued functions on X. If i : R R, i [n], are functions vanishing at zero that satisfy forall u, v R, |i(u) i(v)| |u v|, then we have"
}