{
  "Abstract": "One of the most striking findings in modern research on large language models (LLMs) isthat scaling up compute during training leads to better results. However, less attentionhas been given to the benefits of scaling compute during inference. This survey focuseson these inference-time approaches. We explore three areas under a unified mathemati-cal formalism: token-level generation algorithms, meta-generation algorithms, and efficientgeneration. Token-level generation algorithms, often called decoding algorithms, operateby sampling a single token at a time or constructing a token-level search space and thenselecting an output. These methods typically assume access to a language models logits,next-token distributions, or probability scores. Meta-generation algorithms work on partialor full sequences, incorporating domain knowledge, enabling backtracking, and integratingexternal information. Efficient generation methods aim to reduce token costs and improvethe speed of generation. Our survey unifies perspectives from three research communities:traditional natural language processing, modern LLMs, and machine learning systems.",
  "Preliminaries4": "2.1The users goal in generation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .52.2The modeling problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .6",
  "Token-level generation algorithms7": "3.1MAP decoding algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .73.2Sampling and adapters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .93.3Token-level sampling adapters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .93.4Controlled generation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .103.5Constrained decoding. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .12",
  "Meta-generation algorithms13": "4.1Chained meta-generators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .134.2Parallel meta-generators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .154.3Step-level search algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .194.4Refinement algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .21",
  "Incorporating external information23": "5.1Multiple models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .235.2External environment information. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .24",
  "Token cost and performance analysis24": "6.1Token budget . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .256.2Scaling the token budget to improve performance . . . . . . . . . . . . . . . . . . . . . . . . .266.3Minimizing the token budget. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .276.4Compute optimal inference. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .276.5Dependence on the underlying generator(s). . . . . . . . . . . . . . . . . . . . . . . . . . . .27",
  "Speeding up generation27": "7.1Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .287.2Speeding up the generator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .297.3Speeding up meta-generation algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .317.4Libraries and tools for fast generation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .32",
  "Generator": ": Generation algorithms produce output text using a language model. Meta-generation algorithmsare programs that interleave calls to generation algorithms with control flow and external information,yielding text. Our survey covers generation algorithms and their goals (3), meta-generation patterns (4)and sources of external information (5), and efficiency in terms of token cost (6) and speed (7).",
  "Introduction": "One of the most striking findings in modern research on large language models (LLMs) is that, given amodel and dataset of sufficient scale, scaling up the compute used at training time leads to better final results(Kaplan et al., 2020; Hoffmann et al., 2022). However, there is another, lesser-mentioned scaling phenomenon,where adopting more sophisticated methods or scaling compute at inference time (Jones, 2021) can resultin substantially better outputs from LLMs. This survey focuses on these approaches by exploring threeconnected themes: token-level generation algorithms, meta-generation algorithms, and efficient generation. Token-level generation algorithms, often called decoding algorithms, have a rich history in natural languageprocessing, ranging from classical greedy decoding and beam search to modern sampling algorithms such asnucleus (Holtzman et al., 2020) and -sampling (Hewitt et al., 2022). These methods operate by samplingone token at a time or constructing a token-level search space. They assume varying levels of access to alanguage models internals, such as logits, next-token distributions, or probability scores. Recently there has been growing interest in meta-generation algorithmsalgorithms that operate on partialor full sequences, and treat the LLM as a black box that is called as part of a larger generation program(; Khattab et al. (2022); Dohan et al. (2022); Schlag et al. (2023)). For example, a meta-generationalgorithm for solving a math problem might generate multiple solution paths, evaluate the solutions with acalculator, then select the most common answer. Meta-generators can increase the compute resources devotedto generation by making multiple model calls, augmenting the model with search algorithms (Yao et al., 2023;Madaan et al., 2023), or incorporating external data sources. Doing so has seen success in improving taskperformance (e.g., problem solving (Lewkowycz et al., 2022)) and steering the output distribution (e.g., withhuman preferences (Stiennon et al., 2020)), and may offer a way to overcome limitations of standard LLMssuch as error accumulation (Dziri et al., 2023) and computational capacity (Merrill & Sabharwal, 2024).Moreover, meta-generation research is widely accessible, as it often only requires black-box LLM access. Finally, generation needs to be fast and cost-effective. Fast generation becomes increasingly challenging asmodels grow in size, while cost becomes a critical factor in meta-generation algorithms that call models manytimes. On the other hand, meta-generation algorithms open new kinds of shared computation that can beleveraged for improved efficiency. As a result, there is growing interest in efficient generation algorithmsthat speed up generation and reduce token costs by drawing on ideas from machine learning systems and",
  "related areas. Efficient generation in turn expands the frontier of algorithms that are feasible to experimentwith and develop, leading to a virtuous cycle of algorithmic development": "Our survey provides a unified treatment of these three themes: token-level generation algorithms, meta-generation algorithms, and techniques for making generation fast and cost-effective.We integrate ideasfrom traditional natural language processing, modern LLMs, and machine learning systems, and presenta mathematical formalism that includes both classical generation algorithms and modern meta-generators.This unified view is particularly important as the field expands. For example, practitioners working on novelmeta-generation algorithms may benefit from learning about the historical context of generation algorithmsor practical efficiency constraints, while researchers interested in efficiency may benefit from learning aboutmajor algorithmic patterns. More broadly, we aim to promote further research on inference-time approaches. Comparison to existing surveys.Several prior surveys have focused on training-time methods for bettertext generation (Li et al., 2021; Lu et al., 2018). Wiher et al. (2022) presents a detailed analysis of a smallerset of decoding strategies, while Zarrie et al. (2021) spotlight token-level methods, with a particular focus onconsiderations for encoder-decoder models. In parallel, several surveys have addressed prompting and relatedmethods (Liu et al., 2021b; Sahoo et al., 2024), though these works do not address token-level methods.Recent surveys have also considered strategies for speeding up inference (Chitty-Venkata et al., 2023; Miaoet al., 2023a; Khoshnoodi et al., 2024; Wang et al., 2024a). However, these works focus primarily on token-level generation, not meta-generation; as a result, the discussion of inference-time compute-performancetradeoffs is limited. Our survey unifies and draws connections across these three areas. Finally, Xiao et al.(2023) focus on non-autoregressive generation, while our survey focuses on autoregressive generation. Roadmap.This paper provides a survey of algorithms for token-level generation, meta-generation, andefficient generation, summarized in . First, we consider why we use generation algorithms at all.Generally, a users intent is to surface a high-quality output from the model, which we formalize and discussin 2. Readers who would like to review terminology or follow the mathematical formulation of the survey indepth should start in this section. Next, we discuss token-level generation algorithms in detail in 3. Mostalgorithms referred to as decoding algorithms in the literature are covered in this section. We discuss thesemethods theoretical motivation, practical impact, commonalities, and provide a unified frame for discussion.These methods generally require some degree of access to the models internals. A growing set of methods operate over partial or full sequences rather than individual tokens. These meta-generation algorithms have emerged from several communitites, including researchers interested in designingnew decoding algorithms or prompting methods, as well as researchers interested in language model alignmentand reasoning. Works from these communities often have different motivations and use different terminology.We present a unified picture in 4, classifying them according to their programmatic structure (e.g., parallelgeneration, search, or refinement), and discussing their motivations. In addition to wanting a high-quality output, we often care about the efficiency of generation. We considertwo definitions of efficient generation. In 6 we consider the token cost of generation algorithms, which isespecially relevant for studying cost-performance tradeoffs as the amount of computation allocated to gener-ation is scaled up, and for those using API-access models that charge by the token. In 7, we discuss methodsfor speeding up generation primarily from a systems perspective, where access to the model weights is as-sumed and latency and throughput are the key considerations. In this section, we draw upon work primarilyfrom the machine learning systems (MLSys) community. The section serves as both an introduction to thisarea for machine learning researchers whose work does not focus on systems, and a practical exploration oftools for speeding up generation. We include a review of libraries that implement the described techniques.",
  "SymbolNameExplanation/example": "pModel distributionThe conditional distribution defined by an LM with parameters sModel logit functionAssigns LM scores to tokens. Normalizing gives p.pTraining distributionThe target distribution for which an LM was trained.AAcceptability functionTakes a set S of outputs, assigns an acceptability score.rReward functionProxy for A.vScoring functionProxy for r. Also: value function, learned verifier, reward model.gA generatorE.g., a sampling algorithm for an LM, or a refinement algorithm.g(|x)Generator distributionThe distribution obtained by applying a generator to an input x.qTarget distributionE.g., the distribution of utterances in the style of a helpful assistant.fDeterministic functionE.g., a deterministic generator such as greedy decoding, or a calculator.A generators parametersE.g., temperature, number of samples.dDistance functionMeasures distance, e.g., KL divergence, between two distributions.YOutput spaceThe set of possible LM outputs, i.e., strings.VVocabularyThe set of tokens that make up sequences in Y.P(Y)Probability distributionsThe set of probability distributions over outputs.ytCurrent tokenThe token at index t in a sequence y Y.y<tPrefix, or contextThe tokens preceeding index t in a sequence y Y.",
  "The users goal in generation": "When a user is generating outputs with a language model, it may be with one or more goals in mind.The user may want output that is as high quality as possible for some notion of quality, such as a correctanswer to a math problem or a factual and well-written summary. The user may want multiple outputs,such as alternative solutions to a problem or multiple summaries to read through and synthesize. In general,users now access language models through general-purpose text-in text-out APIs, making it impossible toenumerate all of the specific use cases or goals that a user might have. As a result, to formalize an overall goal for generation, we will need to take a fairly general perspective. Weassume that the user has some underlying measure of acceptability for any set S of outputs, A(S) R. Forexample, a single sequence set may have high acceptability if it represents a correct solution to a problem,while in a different context a set S may have high acceptability if it balances some notion of diversity withsome notion of quality. The acceptability scores, when normalized, form a probability distribution that wecall the target distribution q,",
  "q(S) A(S).(1)": "Next, we treat generating outputs with a language model as sampling from a generator S g that producesa set of sequences each time it is called. Finally, we assume that a user wants the distribution of outputsfrom the generator to be close to the distribution of their acceptability scores according to some proximitymeasurement d between distributions. An ideal generator g would thus satisfy:",
  "arg mingd(q, g).(2)": "In practice, we typically do not know how to measure the users acceptability nor their desired notion ofproximity, let alone how to design a generator that is guaranteed to produce outputs with high acceptability.At a high level, the remainder of this survey can be seen as surveying ways to design generators that optimizesome proxy of acceptability in an efficient way. For example, some algorithms will try to produce a singleoutput that is acceptable with a language models probability as a proxy of acceptability. Other algorithms",
  "The modeling problem": "Language models.Let p be a language model that approximates the distribution p, denoted p p.We consider autoregressive language models p(y|x) = Tt=1 p(yt|y<t, x), where y is a sequence of tokensfrom vocabulary V. Each conditional distribution is of the form, p(|y<t, x) = exp(s(|y<t, x))/Z, wheres(|y<t, x) R|V| are referred to as logits and Z = |V|i=1 exp (s(|y<t, x))i. We henceforth refer to a modelof this form as simply a language model (LM) for brevity. A generation model associated with a language model p is a function g : X P(Y)that maps an input x X, a model p with , and any additional parameters to a probabilitydistribution over outputs, g(y|x; p, ) P(Y). Calculating the probability distribution over outputs g(y|x; p, ) P(Y) is in most situations analyticallyintractable. One can use the generation algorithm in order to obtain independent or dependent samplesfrom g(y|x; p, ) P(Y); we refer to this process as generating, y g(y|x; p, )). We will also refer tog as a generator, and the distribution obtained by applying g to an input as a generation distribution.Generation algorithms may be deterministic or stochastic. We denote generating from a deterministicgenerator as y = f(x; p, ), where f is the deterministic generator. While methods to maximize a scoringfunction are often deterministic and methods for generating sets of outputs are often stochastic, in practiceeach kind of method can be used toward either goal. Let us now return to the general goal of generation that we formulated above. For notational simplicity, letus consider generating a single sequence, i.e. S = {y}. In practice, we can design a generation algorithm tomaximize some proxy r() of acceptability:",
  "arg maxgr (q(|x), g(|x; p, )) ,(3)": "where r : P(Y) P(Y) R is some reward function between distributions. We group generation methodsinto 3 categories: methods for maximization (2.2.1), sampling from the model (2.2.2), and sampling froma target distribution (2.2.3). These are special cases of (3). For maximization, q v for a scoring functionv, and we have r(q, g) = Eygq(y|x). For sampling from a target distribution, r is a divergence betweenthe target distribution and the generation distribution.",
  "Token-level generation algorithms": "In this section, we discuss representative methods that operate on the token-level (e.g., by sampling a singletoken at a time, or constructing a token-level search space and then selecting an output at the end). Methods in this category generally assume access to a language models logits, next-token distributions, orprobability scores. These methods will later be treated as black boxes that are called by meta-generators.",
  "y1 = arg maxy1Vp(y1|x), , yT = arg maxyT Vp(yT |y<T , x).(10)": "Despite its naive approximation, greedy decoding is a widely-used generation algorithm. For instance, it isused in Googles Gemini report (Gemini Team et al., 2023), and is available on typical language model APIs. Other MAP decoding algorithms.Several algorithms have been designed that typically return betterapproximations (i.e., more probable sequences) than greedy decoding. In the context of neural sequence-to-sequence models, beam search (Graves, 2012; Sutskever et al., 2014) is a widely-studied MAP decodingalgorithm. It maintains a data structure of multiple prefixes y<t at each generation step, expands each prefixwith each possible next-token, y<t yt, scores each expanded prefix with p(y<t yt|x), and retains the top-Kexpanded prefixes for the next iteration. This can be seen as a generalization of greedy decoding, whichexpands only a single prefix. In practice, beam search has been shown to improve upon greedy decoding interms of downstream task performance in many settings (e.g., Sutskever et al. (2014); Freitag & Al-Onaizan(2017); Kulikov et al. (2019)). It has several variations and generalizations that we will return to when wetake the perspective of generation as search (4.3). Although the space of possible outputs is extremely large,it is sometimes possible to find an exact MAP solution (i.e., a sequence that maximizes (7)). For instance,Stahlberg & Byrne (2019) combine elements of beam search and depth-first search to perform exact searchwith machine translation models, which was improved upon in Stahlberg et al. (2022). Pitfalls of MAP decoding.Despite its popularity, several studies suggest that the MAP decoding objec-tive is not desirable (Meister et al., 2020). Empirically, MAP decoding has a tendency to produce degenerateresults. For example, Koehn & Knowles (2017) found that wide beam search (which approaches exact MAPdecoding in the limit) degrades neural machine translation (NMT) outputs by favoring shorter outputs. Infact, Stahlberg & Byrne (2019) found that exact MAP decoding often returned the empty sequence in NMT.Length normalization (e.g., dividing the log-probability of the sequence by its length) can mitigate MAPdecodings tendency to favor shorter sequences (see Murray & Chiang, 2018), but this is only a heuristic anddoes not fully counteract degradation for the largest beam sizes (Koehn & Knowles, 2017). ApproximateMAP decoding, e.g., greedy, can also fail by getting trapped in repetitive sequences (Holtzman et al., 2020;Welleck et al., 2020; Eikema & Aziz, 2020). There are several explanations for degenerate behavior in MAP decoding, a phenomenon known as theinadequacy of the mode (Eikema, 2024). Some studies attribute degenerative phenomena in MAP decodingto the tendency of the most likely generations to accumulate so little probability that the mode becomesarbitrary due to small errors in probability estimation (Eikema & Aziz, 2020; Stahlberg et al., 2022). Inan alternative explanation, Meister et al. (2023b) use information-theoretic analysis to show that MAPdecoding generations often fall outside of the typical set of sequences in the language models distribution.To illustrate how this occurs, consider that the most probable outcome of 100 flips of a slightly biased coin(with 0.51 probability of heads, 0.49 probability of tails) is a sequence of 100 heads. However, this resultwould be atypical; a close-to-even mix of heads and tails would be more typical (Dieleman, 2020). Unreasonable effectiveness of approximate MAP decoding.Despite the drawbacks of MAP decod-ing, rough approximations of MAP decoding remain popular in the forms of greedy decoding and narrowbeam search. For example, Shi et al. (2024a) study Llama 2 language models and find that beam search per-forms well on input-output tasks such as translation. Meister et al. (2020) hypothesize that these decodingmethods are effective because they inadvertently enforce information-theoretic patterns that are characteris-tic of human text. On the other hand, the MAP objective can limit diversity, and beam search in particularcan incur a large computational cost. Increasingly, practitioners using an inference library or language modelAPI may instead turn to algorithms that efficiently sample from the model,2 which we describe next.",
  "yt p(|y<t, x),(12)": "where y0 is a given starting token, and the algorithm terminates upon reaching a particular token or a givenlength. The result is mathematically equivalent to sampling a sequence y directly from p( | x), and isknown as ancestral sampling. Other algorithms such as speculative sampling (Leviathan et al., 2022) aim tosample from p more efficiently, which we will discuss in more detail later in the review (7). Sampling, MAP, and the diversity-coherence trade-off.Ancestral sampling avoids many of thedegenerate behaviors of MAP decoding, such as repetition traps, and introduces more diversity into LMgenerations. However, ancestral sampling can suffer from incoherence, i.e., over-sampling highly-unlikelytokens due to model error (Zhang et al., 2021). Hewitt et al. (2022) hypothesize that this occurs becauseperplexity-based loss functions encourage language models to over-estimate the probability of unlikely to-kens to avoid large loss penalties (a behavior called mode-seeking). Alternatively, Finlayson et al. (2024a)hypothesize that constraints imposed by the LMs output layer, i.e., the softmax bottleneck (Yang et al.,2018), cause model errors, and propose a method, basis-aware truncation (BAT), to avoid these errors. Balancing the diversity-coherence tradeoff.Several decoding strategies attempt to balance thediversity-coherence tradeoff by interpolating between greedy and ancestral sampling.These include nu-cleus (Holtzman et al., 2020), top-k (Fan et al., 2018), and - and -sampling (Hewitt et al., 2022), which usevarious heuristics to choose a threshold at each time step and only sample tokens with probability greaterthan the threshold. Another approach, temperature sampling (Ackley et al., 1985; Hinton et al., 2015), scalesthe LM logits to interpolate between greedy sampling and uniform sampling (setting all token probabilitiesequal), which can be useful when one wants more diversity than ancestral sampling offers.",
  "MethodPurposeAdapterExtrinsic": "Ancestral samplingy pTemperature sampling y q(p)RescaleGreedy decodingy max pArgmax (temperature 0)Top-k sampling y q(p)Truncation (top-k)Nucleus sampling y q(p)Truncation (cumulative prob.)Typical sampling y q(p)Truncation (entropy)Epsilon sampling y q(p)Truncation (probability) sampling y q(p)Truncation (prob. and entropy)Mirostat decoding Target perplexityTruncation (adaptive top-k)Basis-aware sampling y q(p)Truncation (linear program)LP SolverContrastive decoding y q(p)log p log p and truncationModel pDExperts y q(|x, c) p (p+/p)Models p+, pInference-time adapters y q r(y) (p p)Model pProxy tuning y q(|x, c) p (p+/p)Models p+, p",
  ": Survey of token-level generation. r(y) is a scalar reward function. c is a control attribute. Extrinsicrefers to a model or solver separate from the underlying language model p": "the language models next-token distributions. In practice, next-token distributions are increasingly notprovided by common generation APIs, both for practical reasons and for security (Finlayson et al., 2024b;Carlini et al., 2024). Instead, token-level algorithms are often implemented by the API provider, and usedby setting hyperparameters (e.g., setting a temperature ). Adapters for statistical control.Several decoding methods use sampling adapters to control the statis-tical and information-theoretic properties of model outputs and align them with those of human text. Theseinclude locally typical sampling (Meister et al., 2023b), which aims to sample from the LM distributions typ-ical set (MacKay, 2004); and mirostat sampling (Basu et al., 2021), which attempts to match the perplexityof the generated text to the expected perplexity under Zipfs law (Zipf, 1999; Powers, 1998). Intriguingly, Shiet al. (2024a) evaluate Llama 2 models with a variety of adapters (temperature, top-k, top-p, , Mirostat,and typical sampling), and find no definitive best method for the evaluated open-ended text generation tasks.Furthermore, temperature sampling usually outperformed the other adapters in input-output tasks such ascode generation and translation. In general, which adapter to use remains an open question. Autoregression and lookahead adapters.Token-level algorithms generate from left-to-right, meaningthat they generate each token without knowing the eventual identity of tokens to the right. Several algorithmshave incorporated various heuristic scores v(yt) that adjust the next-token distribution using informationfrom potential future tokens. This includes explicitly generating several tokens ahead (e.g., Lu et al. (2022);Leviathan et al. (2022)), or learning a function v(yt) that predicts a property of a full sequence (e.g., itsstyle score or correctness) (Yang & Klein, 2021). Doing so can aid in satisfying sequence-level criteria. Distribution adjustment with another language model.Some algorithms adjust the next-tokendistribution using another language model. This can arise from several motivations, including removingabnormalities in the models next-token distributions (Li et al., 2023a), speeding up generation (Leviathanet al., 2022), or shifting the generation distribution to one with a property (e.g., a style) (Liu et al., 2021a).",
  "Published in Transactions on Machine Learning Research (11/2024)": "Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik RNarasimhan. Tree of thoughts: Deliberate problem solving with large language models. In Thirty-seventhConference on Neural Information Processing Systems, 2023. URL Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. Orca: A distributedserving system for Transformer-Based generative models.In 16th USENIX Symposium on OperatingSystems Design and Implementation (OSDI 22), pp. 521538. USENIX Association, 2022. ISBN 978-1-939133-28-1. URL",
  "q p(y|x)p(a|x, y),(16)": "where is a hyperparameter assigning more weight to the classifier at higher values of . Various generationalgorithms have been developed for this purpose, such as approximations based on reweighting next-tokendistributions with other language models (Liu et al., 2021a), reweighting with a learned classifier that approx-imates the sequence-level classification p(a|y<t, x) p(a|y, x) (Yang & Klein, 2021), or additional trainingto sample from q (Khalifa et al., 2021; Hu et al., 2024; Zhao et al., 2024a). Indicator.A special case is c(y) indicating whether y falls into a target set Y x , such as the set of correctsolutions to a reasoning problem, or sequences that have desired keywords. The goal is then to sample from:",
  "q p(y|x)I[y Y x ],(17)": "where I[y Y x ] is 0 when y Y x and 1 when y Y x . Various generation algorithms incorporate a learnedverifier v(x, y) I[y Y x ] to aid in achieving this goal (Cobbe et al., 2021; Lightman et al., 2024), ordesign beam search heuristics for the case of desired keywords (Hokamp & Liu, 2017; Lu et al., 2022).",
  "where R interpolates between sampling from p ( ) and maximizing reward ( 0)": "A notable example is aligning the distribution of generated text with a distribution of text preferred byhumans (Ziegler et al., 2020; Stiennon et al., 2020; Ouyang et al., 2022). One way of operationalizing thisproblem is as one of finding a policy that balances maximizing a reward r(x, y) that quantifies humanpreferences with generating sequences that are probable under a pretrained model p:",
  "r(x, y),(20)": "where Z(x) is a normalization factor. One strategy for sampling from q is updating p with reinforcementlearning, then using ancestral sampling. This strategy is referred to as reinforcement learning from humanfeedback (Askell et al., 2021). Later, we will discuss meta-generation algorithms for addressing this problem. A second approach is to re-weight each next-token distribution during autoregressive sampling. For example,reward-augmented decoding (Deng & Raffel, 2023) assumes access to a reward function r(yt, x) that assignsa scalar reward to partial generations yt. It re-weights the tokens with the top-k next-token probabilitiesusing the reward, and samples from the re-weighted distribution. That is,",
  "Constrained decoding": "Often it is desirable to generate tokens that satisfy some constraint. These constraints may be structural, e.g.,satisfying a JSON schema; or lexical, i.e., containing specific words. In contrast to controlled generation,constrained generation mandates that the output conforms to specific, formal criteria.Several methodsfrom controlled generation can be employed to encourage LMs to conform to constraints, but here we focuson methods that strictly enforce such constraints. Navely, one can strictly enforce constraints via rejectionsampling (4.2.3), though there is no guarantee that this method will terminate. In this section, we thereforefocus on efficient methods for constrained decoding.",
  "Parser-based decoding for structural constraints": "Structural constraints often take the form of a grammar. A simple example of this would be to constrain thelanguage model output to valid US phone numbers. Another would be to constrain outputs that conform toa specific JSON schema. Structural constrained decoding methods enforce such constraints by pairing thelanguage model with a parser. The parser filters next-token candidates by allowing only valid continuations.The central challenges of parser-based constrained decoding are efficiently identifying valid next-tokens withthe parser and interfacing the parser with a sub-word tokenizer. Formal languages and parser efficiency.The efficiency of a parser at identifying valid next-tokensdepends on the complexity of the formalism used to specify the set of valid outputs. Regular expression(RegEx)-based parsers; e.g., Guidance (Lundberg et al., 2024), LMQL (Beurer-Kellner et al., 2022), and Out-lines (Willard & Louf, 2023); can be implemented as finite-state automata, meaning that these can recognizevalid continuations in constant time and space. More complex constraints, such as JSON specifications, re-quire more sophisticated, computationally expensive parsers. Though JSON is technically not a context-freegrammar (CFG), several CFG-based parsers including PICARD (Scholak et al., 2021), GCD (Geng et al.,2023), Synchromesh (Poesia et al., 2022), and Domino (Beurer-Kellner et al., 2024) can be used to enforcevalid JSON outputs. Template speedupsParser-based constrained decoding methods can accelerate decoding, i.e., when thereis only a single valid next token, that token can be immediately accepted without doing a forward pass withthe model. Highly templated RegEx-based methods benefit most from this property (Lundberg et al., 2024). Token healing.One drawback of constrained decoding is that constraints may cause the language modelto exhibit unusual behaviors due to the template or grammar forcing an unnatural token boundary. Severaldecoding methods deal with this issue by employing a method called token healing (Lundberg, 2023). Tokenhealing rolls back the tokenizer to the penultimate token then chooses the next token with the constraintthat it must have the remaining characters as a prefix (see for an example). This method uses aprefix tree to track valid continuations. Minimally invasive constrained decoding.Constrained LLM performance can also be limited by tem-plates that are too inflexible. Over-constrained templates can prevent the model from generating outputsthat would otherwise be valid. (Beurer-Kellner et al., 2024) call this property the invasiveness of a con-strained decoding method, and refer to a method as being minimally invasive if every valid output thatthe model can generate can still be generated under constrained decoding. They link the invasiveness of",
  "Lexically constrained decoding": "It is often desirable to constrain language model outputs to contain or not contain specific words. LLMinference APIs often allow users to specify token ban lists or logit bias to discourage the model from out-putting specific words, but forcing LLMs to output specific words is more challenging. Lexically constraineddecoding often employs search (Hokamp & Liu, 2017) to find likely generations that satisfy the constraints.Some methods seek to improve this search, such as through gradient guidance (Kumar et al., 2022). For more complex lexical constraints, NeuroLogic decoding (Lu et al., 2021) allows users to specify con-straints as a logical formula in conjunctive normal form (CNF) then enforces these constraints via a modifiedbeam search, e.g., (foodfoods)(tabletables) specifies that the generation must contain either food orfoods and either table or tables. Followup work (Lu et al., 2022) adds a lookahead to this approach. In summary, we have seen several strategies for constructing a token-level search space and adjusting thenext-token distributions of a model during sampling. Next, we will treat these algorithms as black-boxesthat can be used to generate partial or full sequences, and survey algorithms that construct search spaceson the (partial-)sequence level or operate by drawing multiple samples.",
  "Meta-generation algorithms": "Some generation algorithms have the distinctive property of requiring access to a separate generation sub-routine. For instance, best-of-N calls a generator to sample N sequences from the language model. Thissub-generator is interchangable; it can be freely chosen from top-k, temperature sampling, or any othersequence generator. We coin the term meta-generation to describe algorithms that call sub-generators, i.e.,",
  "y g(|x, {g1, . . . , gG}, ),(23)": "where g defines the strategy used by the meta-generator, {g1, . . . , gG} are sub-generators, and is a genericparameter for any other inputs (such as verifier or retrieval models) and hyperparameters (such as the numberof tokens to generate). Since a meta-generator is itself a generation algorithm, i.e., a function that mapsinputs to a distribution over outputs (2.2), we will freely use g() for either a meta-generator or a token-levelgenerator. We will often hide the parameters {g1, . . . , gG} or make other parameters explicit based on thecontext. We identify four common strategies among meta-generators. In particular, we find that they canbe classified into the categories of chained, parallel, step-level, and refinement-based meta-generators.",
  ": Three meta-generation patterns": "where f() is a deterministic generator, and the prompt z is a sequence of tokens that specifies the desiredbehavior through a natural language instruction or input-output examples (Brown et al., 2020; Ouyang et al.,2022). For instance, given z = multiply the two numbers and x = 1432 293, we can generate an outputy that contains an (attempted) solution. It is natural to compose the generator call with other operations,such as composing a generator that outputs Python code with a function that executes Python code.",
  "f(x; p, (f1, f2, f3)),(26)": "i.e., a mapping from an input x, model p, and other parameters , to an output (here, contains thegeneration algorithms f1, f2, f3), or in general, a distribution over outputs g(y|x, p, ). In general, we canview calls to generation algorithms as steps in a program whose execution yields a generated output. Wecan view the program f(x; p, F), which calls generation algorithms f F, as a meta-generation algorithm. Related ideas appear in the literature under various names, including the programmatic view in Demonstrate-Search-Predict (DSP) and DSPy (Khattab et al., 2022; 2024), language model cascades (Dohan et al., 2022),LLM program (Schlag et al., 2023), and recently, scaffolding program (Zelikman et al., 2024b). We introducethe term meta-generation as an abstraction that is agnostic to the implementation of the underlying generatormodel(s) (which need not be LLMs), and to clarify the connection with other generation algorithms. Problem decomposition.A variety of algorithms have adopted the chain pattern in order to decomposean input-output problem into multiple steps, with each step implemented by a language model or exter-nal function. As a motivating example, Chain-of-Thought (Wei et al., 2022) decomposes generation intogenerating a chain-of-thought followed by generating an answer i.e.,",
  "z g(z|x), y g(y|x, z),(27)": "where g is a generator, x is an input, z is an intermediate sequence (a chain-of-thought), and y is ananswer. It is instructive to view a variety of methods as generalizing this two-part decomposition to multipleintermediate sequences, z1, z2, . . ., that involve calls to generators or external functions (Dohan et al., 2022).For instance, least-to-most prompting (Zhou et al., 2023a) first calls a generator to decompose a problem intosub-questions and then consecutively calls a generator to answer each sub-question, while Self-Ask (Presset al., 2023) additionally calls a search engine after generating each sub-question. Both of these are specialcases of Demonstrate-Search-Predict (DSP) programs (Khattab et al., 2022). A wide range of methods canbe seen as constructing alternative chained meta-generators, ranging from System 2 Attention (Weston &Sukhbaatar, 2023), which rewrites an input prior to generation to help the model refrain from attendingto irrelevant information, to methods that decompose formal proof generation (Jiang et al., 2023). Moregenerally, a number of tools such as LangChain (Chase, 2022) and MiniChain (Rush, 2023) provide domain-specific languages for declaring and executing chains involving prompted language models.",
  "Parallel meta-generators": "Another pattern is to generate multiple trajectories in parallel, then merge the resulting terminal states toarrive at a final generated sequence. For instance, various sequence-level generation algorithms generate anN-best list {y(n)}Nn=1 g, then apply an aggregation function h(y(1), . . . , y(N)) to arrive at a final generatedsequence. The N-best list of sequences might come from sampled generations, a beam search algorithm, orany other generator y g that generates full sequences. We discuss aggregation functions that rerank (4.2.1)or transform (4.2.2) the N-best list, then discuss sequence-level statistical rejection sampling (4.2.3). presents a brief summary of algorithms from the classes that we discuss.",
  "Reranking algorithms": "Reranking (or rescoring) is a classical approach (Collins, 2000; Huang & Chiang, 2007) originally developedfor parsing and automatic speech recognition to achieve a trade-off between the computational complexityof MAP decoding and its tendency to rule out good hypotheses. A reranking algorithm orders an N-bestlist with a reranking function h(y(1), . . . , y(N)) (y(1), . . . , y(N)), then selects the top-k ranked sequences.Reranking has recently found new applications in text generation (e.g., Cobbe et al. (2021); Stiennon et al.(2020); Krishna et al. (2022); Ni et al. (2023); Lightman et al. (2024)) by using various reranking functionsand various sources of data to learn the reranking functions. A simple and effective method is best-of-N.",
  "where each y(n) is a generated sequence": "Best-of-N can be performed with any algorithm that can be used to generate a list of N sequences, includingtemperature sampling, beam search, Viterbi decoding, or many others. In the context of language modeling,best-of-N was developed for parsing (Charniak & Johnson, 2005; Pauls & Klein, 2009), and traditionallyinvolved modifying a decoding algorithm originally developed to find the top-1 hypothesis so that it obtainsthe top-N highest scoring decodings. An attractive property is that Best-of-N usually incurs only a linearincrease in computational complexity compared to top-1 decoding. In the context of LLMs, best-of-N isamenable to black-box generators (e.g., accessed via an API), since it does not require knowledge of thegenerator for populating the N-best list. Modern instances of best-of-N use learned scoring functions thatare often themselves parameterized by LLMs. We discuss examples from reasoning and preference alignment. Best-of-N in reasoning.In some settings the goal is to generate correct sequences, such as a correctsolution to a mathematical problem or a program that passes test cases. A common approach in these casesis to learn a verifier v(x, y) that predicts the probability that an output y is correct, and use itwithin Best-of-N. Doing so has seen success in mathematical reasoning (e.g., Cobbe et al. (2021); Uesato",
  "r(x, y).(29)": "When a single high-reward sequence is desired (e.g., at low values of ), a natural strategy is to use best-of-Nwith a learned approximation of the reward, v(x, y), as the scoring function. In practice, this strategy isan effective alternative to reinforcement learning from human feedback (RLHF) methods (Gao et al., 2022;Beirami et al., 2024). For example, AlpacaFarm (Dubois et al., 2023) found that Best-of-1024 with a human-preference reward model was competitive with more standard decoding methods with a model trained usingRLHF. A potential benefit is that Best-of-N does not require updating the model ps parameters, at theexpense of generation-time compute. Best-of-N depends on the quality of the reward function, which is typically a learned function v(x, y),termed a reward model. It can suffer from reward over-optimizationi.e., returning an undesired sequencethat nevertheless receives high reward.Specifically, suppose that q(y|x) v(x, y), where v perfectlycaptures the desired outcome of generation. Best-of-N at high values of N can be seen as approximating:",
  "where yn g. In practice, the learned model v typically does not match v, especially on out-of-distributionsequences, so best-of-N may find sequences that overoptimize the reward (Gao et al., 2022)": "Noisy-channel reranking in Neural Machine Translation.A wide range of reranking methods pre-cede the era of large language models. A classic approach is a noisy-channel model (Brown et al., 1993).Noisy-channel means that the observed output from the system (e.g., a machine translation system) isdistorted by some unknown noise pattern (i.e., noisy channel). If we consider p(y|x) as the probabilityof the translation y of the source language text x, then Bayes rule suggests the following relationship:p(y|x) p(x|y)p(y), where p(x|y) is a channel model, and p(y) is the target language LM.",
  "Transformation algorithms": "In contrast to reranking elements of the N-best list, other algorithms transform the list into a new sequencewhich might not be part of the N-best list itself.For instance, mathematical question answering is anexample of a task where the potential outputs (answers to math questions) are produced as part of muchlonger decoded sequences from the LLM. In other cases we might draft N summaries, then synthesize theminto a new, final summary. This requires a transformation of the N summaries rather than a simple reranking.",
  "where u : Y Y R. A dependence on p is introduced in specific instances of MBR decoding algorithms": "The MBR objective is motivated by decision theory. Intuitively, it can be thought of as seeking an outputwith highest average similarity, as measured by the utility function u, to other candidates, particularlythose assigned high probability under p. Various algorithms provide approximate solutions to the Minimum Bayes Risk (MBR) objective.Theytypically consist of providing a utility u(, ) R, populating a hypothesis set Yh using a generator, andpopulating a evidence set Ye to estimate the risk of each hypothesis:",
  "yYeu(y, y).(37)": "The hypothesis set is typically akin to an N-best set, populated by calling a generator {y(n)}Nn=1 g(|x).Simple strategies sample from the model, y(n) p. Others take the best-k outputs from a ranked listof generations, or use more sophisticated strategies such as iteratively adding hypotheses or transformingthem (Gonzlez-Rubio et al., 2011; Gonzlez-Rubio & Casacuberta, 2013). Freitag et al. (2023) investigatethe impact of the underlying sampling strategy, finding variation across strategies, with epsilon samplingperforming best for machine translation. The evidence set is typically sampled from a generator, or set tothe hypothesis set to save on computation. Finally, the metric impacts performance. For example, MBRwith a metric tends to inflate performance on that metric, sometimes by gaming it (Freitag et al., 2023).",
  "Known upper bound distribution MpUn-normalized distribution qReject regionAccept region": ": In rejection sampling, the aim is to sample from a distribution q whose normalizing constant isunknown. To do so, use a known distribution p that serves as an upper bound for the unknown distributionwhen scaled by a constant, i.e., for some constant M and all values y, Mp(y) q(y).Next, obtain asample y p and accept this sample with probability q(y)/Mp(y)), otherwise reject the sample and repeatthe process. This is equivalent to sampling from q. MBR methods have a rich history in the machine translation and speech recognition literature (Goel et al.,2004; Heigold et al., 2005; GOEL, 2003; Kingsbury et al., 2012; Eikema & Aziz, 2020), and have also beenapplied across other tasks (Shi et al., 2022; Suzgun et al., 2023). Interestingly, Bertsch et al. (2023) showthat self-consistency and other voting techniques are special cases of MBR. For example, weighted voting(and as a special case, voting) corresponds to a utility that checks if two answers match,",
  "u(y, y) = I [a = a] v(y),(38)": "where y = (z, a), y = (z, a), and v is the weighted voting scoring function. In general, there are severalother dimensions along which MBR methods are categorized. We refer the reader to Bertsch et al. (2023)for a further in-depth study and a taxonomy of MBR methods. Generate-and-transform.In general, we can view the algorithms above as first generating an N bestlist, followed by transforming the N best list using a transformation h(y(1), . . . , y(N)), such as voting orone that internally estimates risk. Rather than hand-designing the transformation, recent methods exploreusing language models themselves. For instance, universal self-consistency (Chen et al., 2023c) prompts alanguage model to generate a final sequence given the N-best list, which can avoid the aforementioned issueof parsing sequences into an answer. Branch-solve-merge (Saha et al., 2023) transforms an input into Ndifferent prompts, generates with those prompts, then merges the results by prompting a language model.",
  "Sequence-level rejection sampling": "Previously we discussed the goal of designing a generation algorithm that samples from a target distributionq (2.2.3). A related pattern is using a stochastic sequence generator y g to sample from q using rejectionsampling. This involves sampling multiple sequences from g and is thus akin to a parallel meta-generator. Specifically, statistical rejection sampling is a technique for sampling from a target distribution q with anunknown normalizing constant. This is accomplished by first sampling from a known distribution y gwhich serves as an upper bound for q, (e.g., for some constant M, Mg(y) q(y)), then accepting thesample with probability q(y)/Mg(y). illustrates this process. Rejection sampling is a useful toolfor sampling from a specified target distribution over an intractably large support, e.g., the set of sequences. One example of sequence-level rejection sampling for LMs is sampling valid JSON strings from an LM. Thespace of valid JSON strings is infinite and the normalizing factor is unknown, but we can sample from thisdistibution by first sampling from the LM distribution p, then rejecting any string that is not valid JSON.Here, the un-normalized distribution we are sampling from is",
  "y Bon(p, g, N, v),(39)": "where Bon means generating N sequences y(1), . . . , y(N) g, then selecting the sequence with the highestscore v. This idea has been termed the best-of-N policy (Stiennon et al., 2020; Gao et al., 2022). Interestingly,Gao et al. (2022) find that the best-of-N policy may give similar reward maximization to reinforcementlearning, though with a different pattern of divergence from the underlying language model. Their analysisuses an analytical expression for the KL divergence from (Stiennon et al., 2020), DKL (Bonp) log N (N 1)/N. Beirami et al. (2024) show that this expression is an upper bound on the actual KL divergenceand propose an estimator that empirically provides a tighter approximation. Finally, y Bon can be understood as internally performing rejection sampling (Stiennon et al., 2020). Werefer the reader to Liu et al. (2024c) for a more detailed discussion of this connection, as well as an improvedalgorithm that builds on the connection between rejection sampling and best-of-N. Pseudo-rejection sampling.Several decoding methods employ various forms of pseudo-rejection sam-pling. One example of this is Li et al. (2024a), where the authors sample a set of k outputs from the LM,compute the value of each of these outputs, and then sample from the output set by interpreting thevalues as logits. As k tends toward infinity, this method approaches sampling from the value function witha regularization term that keeps the distribution close to the LM distribution. Another method based ona similar construct is the one of Zhao et al. (2024b). Such methods can also be interpreted as samplingimportance resampling, in the spirit of sequential Monte-Carlo sampling algorithms (Douc et al., 2014).Pseudo-rejection sampling is often employed when the prerequisites for rejection sampling are not met, forinstance when there is no known upper bound on the target distribution.",
  "Step-level search algorithms": "Next, we discuss meta-generation algorithms that implement classical search algorithms by calling generators.To introduce these, it is helpful to view generation as navigating a state space s S by taking actions a Ausing a generator, and receiving new states from an environment E : S A P(S), yielding a trajectory(s0, a1, s1, . . . , aT , sT ). The start state s0 contains the input to the generation algorithm, i.e. x s0, whilethe terminal state contains the output of the generation algorithm, i.e. y sT . Generation consists ofrunning the resulting process until reaching a terminal state.",
  "yt = arg maxytVp(yt|y<t, x),(40)": "for t = 1, . . . , T. The search perspective interprets this as taking next-token actions yt given states (x, y<t), agenerator that selects the most probable next-token from p, and an environment that appends a next-tokento form a state (x, y<t yt). Since greedy decoding is an approximate MAP decoding algorithm, it aims toend in a state that maximizes p(y|x). In other cases the environment is less trivial, such as those involvingcode execution and visual observations (Shinn et al., 2023; Zhou et al., 2023b). Many algorithms can berecovered by varying the states, actions, environment, and/or generator. In particular, reasoning tasks such as mathematical problem solving or theorem proving have served as atestbed for developing step-level search algorithms. In these tasks, the final output (a solution or a proof)naturally decomposes into steps, y = (y1, . . . , yT ), where each yt is itself a sequence of tokens. One can thenconsider a partial solution y<t as the state st, and generating a next-step yt as the action. The environmentappends the next-step to the partial solution, y<t yt. There is also a natural notion of success (i.e., acorrect answer, a valid proof), leading to the idea of a value function v(st) that is used to predictwhether a solution-so-far will eventually be correct (or in general, predict the expected reward of the state). Several algorithms maintain a queue of states that contain partially generated outputs, and iteratively selectstates for exploration.Exploring a state involves expanding the states partial output and scoring the",
  "MethodSearchStateGenerationValue v(st)Tasks": "gpt-f Proof Search Best-firstProof-so-farProof steplog pFormal provinggpt-f +outcome Best-firstProof-so-farProof stepv E(success)Formal provingProofsize Search Best-firstProof-so-farProof stepv E(length)Formal proving Stepwise++ BeamProof-so-farProof steplog p + n-gramsInformal provingSelf-Evaluation BeamSteps-so-farReasoning steplog p + LLMMulti-step correctnessReward Balanced Search BFS-likeSteps-so-farReasoning stepv E(correct)Multi-step correctnessTree-of-Thought BFS/DFSSteps-so-farGeneration stepPrompted LLMMulti-step generationGraph-of-Thought BFS/DFSSteps-so-farGeneration stepPrompted LLMMulti-step generation HyperTree Proof Search MCTSProof-so-farProof stepv E(success)Formal provingAlphaLLM MCTSSteps-so-farReasoning stepsv E(correct)Multi-step correctnessReasoning via Planning MCTSSteps-so-farGeneration stepPrompted LLMMulti-step generation",
  ": Survey of step-level search methods": "expanded output with a value function v(st). The scores are then used to prune or prioritize states for thenext iteration. Conceptually, step-level search is typically a tree search, consisting of states as nodes andactions plus environment transitions as edges. Although the algorithms below typically contain domain-agnostic ideas, we will ground the discussion below by discussing reasoning tasks as the running examples. Warmup: token-level beam search.Traditional beam search (Graves, 2012; Sutskever et al., 2014)maintains a queue of prefixes {yk<t}Kk=1 termed a beam, expands each prefix using each possible next-token,{yk<t yt | k {1, . . . , K}, yt V}, scores each expanded prefix using log p(yk<t yt|x), and prunes thequeue by keeping only the top-K scored expansions for the next iteration. In this case, the value functionis v(y<t yt) = log p(y<t yt|x), and it is used to prune states by selecting the top-K expanded prefixes.Traditional beam search operates on the token-level, using the specific strategy of expanding each possiblenext-token, which assumes access to primitive operations (e.g., next-token distributions). Partial sequence expansion.We can consider higher-level algorithms that operate on the partial se-quence (i.e., step) level rather than the token level, and call an arbitrary generator to expand states, e.g.,{y(k)t}Kk=1 g(|st). For example, stepwise beam search (Welleck et al., 2022; Xie et al., 2023) performs abeam search over steps of mathematical problems or proofs. Tree-of-thoughts (Yao et al., 2023) considers abeam search over generated steps that include additional thought sequences. Potential benefits of partialsequence expansion over traditional beam search include efficiency due to executing the value function lessoften, and not requiring access to all of a models next-token probabilities. Alternate search strategy.Another axis of variation is the underlying search strategy. Beam search isa pruned breadth-first search, which has been used with contemporary LLMs in methods such as stepwisebeam search (Welleck et al., 2022), tree-of-thoughts (Yao et al., 2023), and recently in Snell et al. (2024).However, other search algorithms are available, such as best-first search, used in the context of formaltheorem proving (Polu & Sutskever, 2020; Polu et al., 2023; Yang et al., 2023a). Formal theorem proving hasa natural decomposition of outputs (i.e., a proof) into steps (termed tactics), which has historically madeit a fruitful testbed for more advanced search algorithms. For example, HyperTree Proof Search (Lampleet al., 2022) draws on Monte-Carlo Tree Search (MCTS)a method that was central to the impactfulGo playing system AlphaGo (Silver et al., 2016; 2017)which prioritizes states according to a confidencebound and scores states by rolling out trajectories. Recently, similar ideas have been adapted to other LLMgeneration tasks. For instance, Reasoning via Planning (Hao et al., 2023) and ThoughtSculpt (Chi et al.,2024) incorporate MCTS selection and rollouts for several tasks. However, there are a few key differencesbetween environments in which MCTS has excelled (such as Go) and general-purpose language generation.For example, MCTS explores based on visit counts in the search tree, which can require a large number ofsimulations (for instance, AlphaGo Zero used 1,600 simulations per move (Silver et al., 2017)). This can beprohibitively expensive with large language models. Second, Go has a natural decomposition into steps, anda reliable terminal reward (i.e., win vs. lose) that can aid in training a value function. Alternate value functions.Another axis of variation is the choice of value function. For instance, intraditional beam search, a value function can be manually designed to score candidates (y<tyt) more highly",
  "y(t) g(y|x, y(<t), z(t)).(43)": "Intuitively, the refiner generates a revised output y(t) given previous versions y(<t) and extra informationz(t), such as feedback or environment observations. The algorithm alternates between receiving information,z h, and refining, y g, until a stopping condition is met. Refinement algorithms vary based on choice ofinitial generator, the refiner, the content and source of extra information z, and the stopping condition. Learned refiners.Self-correction, introduced by Welleck et al. (2023), provides a recipe for training arefiner model p(y(t)|x, y(t1), z(t)) which iteratively refines an output to improve the score from a rewardfunction r(x, y) using (z(t), y(t1), y(t)) examples collected from model trajectories.Here z is either 0/1(apply the refiner or do not apply the refiner) or a feedback string. z is assumed to be given to the systemat generation time, which is a limitation for some tasks (e.g., we often do not know whether a mathematicalsolution should be revised). GLoRe (Havrilla et al., 2024) relaxes this limitation by training a verifier todetermine whether to apply the refiner, and to localize per-step errors.",
  "y(t) gy|Prefine(x, y(<t), z(t)),(44)": "where g is a generation algorithm that involves prompting a model p with a prompt Prefine(x, y(<t), z(t)),as introduced in Self-Refine (Madaan et al., 2023) and Reflexion (Shinn et al., 2023). This allows the initialgenerator and the refiner to share a single language model that is not necessarily tuned for a specific task.",
  "z(t) hz|Pfeedback(x, y(<t), z(<t)).(45)": "This feedback is often also termed critique (Matiana et al., 2021; Castricato et al., 2022; Bai et al., 2022;Saunders et al., 2022).Self-Refine (Madaan et al., 2023) shares across the feedback provider, refiner,and initial generator, yielding a refinement algorithm given only a model p and 3 prompts.Similarly,Reflexion (Shinn et al., 2023) uses a prompted feedback provider. In these cases, the feedback is termedself-feedback or self-reflection.",
  "meaning that each iteration refines based on new environment information (e.g., code execution results)": "Reflexion (Shinn et al., 2023) adopts this perspective of generation as a trajectory involving an environment,with the refiner akin to an actor. The idea has since been adapted to digital agents (Kim et al., 2023; Panet al., 2024), code (Chen et al., 2024b; Shi et al., 2024b), and other environments (Pan et al., 2023). Does refinement work?Notice that a refinement algorithm is a 3-tuple (g0, h, g). Intuitively, if theinformation source h adds new information beyond that contained in the initial generator g0(|p) to therefinement algorithm, it is plausible that a refinement algorithm can outperform the initial generator alone.Hence, in our discussion we distinguish between algorithms that receive information external to the generatorat inference time, and those that do not receive external information at inference time (termed extrinsic andintrinsic refinement or self-correction, respectively (Huang et al., 2024; Kumar et al., 2024)). For extrinsic refinement, it is plausible that there are information sources which add new information beyondthat in g0(|p), and hence lead to a potential gain with refinement.For instance, if z h contains acompiler error, test case results, or an image of webpage after it is clicked, the refiner plausibly receivesnew information. Similarly, if z h represents feedback, and the feedback comes from a source outside ofthe model p (e.g., a human, a model with additional parameters, supervision, or a different objective), thefeedback function may be expected to add new information beyond that in the initial generator. Indeed,refinement has seen success in code generation, where a code interpreter or compiler provides feedback(e.g., Chen et al. (2023d)), or when a retriever or larger model (Olausson et al., 2024) provide feedback. Ina similar vein, Reflexion (Shinn et al., 2023) finds that without code execution, refinement yields little tono gain on code generation. What factors contribute to the potential improvements, aside from the generalnotion of adding new information? Taking a compiler as an example, the compiler can potentially (i)evaluate correctness, (ii) localize errors, and (iii) provide other semantic information related to an error(e.g., an error type). A parallel method such as Best-of-N (see 4.2) can only use this information to preferone sequence over another. A refiner, in contrast, can implement an arbitrary transformation that acts uponthe information. That said, fully understanding the factors that enable refinement remains an open question. For intrinsic refinement, we first consider refinement algorithms that prompt a single model for initial gen-eration, feedback, and refinement (e.g., Madaan et al. (2023)). In these cases, the efficacy of refinement hasbeen mixed. For example, Huang et al. (2024) and Tyen et al. (2024) find that such methods often fail toimprove on reasoning tasks. The failure likely stems from the difficulty of a model evaluating the correctnessof its own outputs (Huang et al., 2024). Similarly, Havrilla et al. (2024) observe that even learned verifierstrained to predict correctness can have high false positive rates that trigger spurious refinements. In thesecases, we can view the information source as being too noisy for the refiner to reliably act upon. Finally, we consider intrinsic algorithms that aim to train a refiner.The key difference with promptedmethods is that the refiner can receive new information through the training procedure. For example, codecorrectness might be used as a reward signal to train a refiner that maps buggy code to correct (high reward)code. Since the refiner is typically a generic sequence-to-sequence model, the key question is whether such arefiner can be trained in practice. Self-corrective learning (Welleck et al., 2023) framed training as generating(x, y, y) examples online with the initial generator, refiner, and a reward function, updating the refiner byfine-tuning on the examples, then repeating the process. However, recently Kumar et al. (2024) identifieda behavior collapse phenomenon, in which the refiner learns to ignore the intermediate output y.This",
  "Incorporating external information": "Next, we consider what kinds of information a generation or meta-generation algorithm incorporates outsideof the language model, such as other models or tools.Algorithms use external information by callingoperations beyond primitive operations from p (e.g., those from another model), or through assumptionson the inputs or outputs. We comment on common patterns related to incorporating external information.",
  "Multiple models": "A variety of generation algorithms incorporate multiple models. More formally, recall that in (2.2) wedefined a generation algorithm as a function that maps an input x, model p, and other inputs to adistribution g(y|x; p, ). A generator uses multiple models if contains other models (e.g., an additionallanguage model), and operations from the model are used in the algorithm. In this sense, the external modelscan add new information beyond that contained in p to the generator. Small and large language models.A notable pattern is using a small language model to either adjusta models distribution or to speed up generation. Lu et al. (2023) train a small model p with reinforcementlearning such that it adjusts the next-token probabilities of a larger model p to maximize a reward function.The models are combined into a token-level product of experts (Liu et al., 2021a),",
  "Z p(yt|y<t, x)p(yt|y<t, x),(48)": "where p is a separate language model, R, and Z R is a normalization constant. Liu et al. (2024a)adopt a similar idea but with supervised finetuning of p. In order to amplify the improvement of a large,strong model over a small, weak one, contrastive decoding (Li et al., 2023a) defines a scoring function forbeam search that returns the difference between the likelihood under the model p with that of a smallerlanguage model p,",
  "s(y<t yt) = log p(yt|y<t) log p(yt|y<t),(49)": "along with a truncation criterion that sets the score to zero for some tokens. Intuitively, the smaller modeloften has larger model errors on unfavorable tokens (e.g., assigning more probability to tokens leading torepetition or incoherence compared to p). Assuming there is a nontrivial difference in probability assignedto these tokens, the score will reduce their prevalence in generated texts. Finally, speculative decoding (Leviathan et al., 2022) is motivated by speeding up generation, which we willdiscuss further in (7). It uses a small draft model to propose generations that are verified or rejected inparallel by the larger model p, hence speeding up generation when the rejection rate is not too high. Scalar feedback models.A common pattern is learning a verifier model v(x, y) that predictsthe probability that a generation is correct (Cobbe et al., 2021). The verifier can be used to select outputsin best-of-N, or for weighted majority voting (Li et al., 2023b). This pattern is particularly suitable formathematical problem solving and code generation (Ni et al., 2023), which have well-defined notions ofcorrectness.Several works have iterated on the verifier models design and learning procedure.Uesatoet al. (2022) show that a verifier trained to predict the correctness of each step in an output (termed aprocess-based verifier) can outperform a verifier trained to predict the correctness of a full solution (termedan outcome-based verifier), and Lightman et al. (2024) obtain new human annotations for a process-basedverifier. Math Shepherd (Wang et al., 2023a) propose a method for obtaining supervision from generations.",
  "More generally, generation algorithms can incorporate information from an external environment": "Calling an external tool.Certain functionality such as reliably performing a calculation or a web searchmay either be outside of a models capabilities or inefficient to perform with the language model. A naturalalternative is to issue a call to an external routine that performs the functionality at generation time. One way to do this is through special tokens that denote a call to the routine, followed by replacing theprefix with the result. For instance, suppose the preceding tokens y<t include [CALC]4+4[/CALC]. Then atstep t of a token-level decoding algorithm, a calculator would be called on the query 4+4, and in subsequentsteps, the prefix yt would contain the result 8, along with possible reformatting (e.g., removing [CALC]). A second common use of an external routine is as a verifier following the generation of a full sequence. Forinstance, in language-model based theorem proving the proof assistant is used to verify or reject generatedproofs, while in code generation it is common to execute test cases. More generally, the notion of tooluse (i.e., calling external programs) is now widespread, and has been incorporated into libraries such asLangChain (Chase, 2022) and products. Refer to Wang et al. (2024b) for further discussion. Receiving observations from an environment.The search perspective framed generation as a sequen-tial decision making process that involves observations from an environment (4.3). A notable applicationarea is code generation, which has natural environment information (e.g., interpreters, compilers).Forinstance, Lever (Ni et al., 2023) feeds execution results into a reward model used for best-of-N, while Self-Debugging (Chen et al., 2024b) incorporates error messages into refinement. A recent line of work tailorsgeneration algorithms to language-conditioned digital agentsmodels that operate on diverse observationspaces X such as images of web pages, and output sequences y representing actionsincluding variants ofrefinement (Shinn et al., 2023) combined with learned evaluators (Pan et al., 2024).",
  "Token cost and performance analysis": "A natural question is the cost of executing a given meta-generator, and its relationship with performance.There are several ways to measure cost, including the number of tokens generated, the overall compute usedduring generation, or the runtime. In some cases, we would like to design an algorithm that improves aswe add more cost, such as improving problem solving ability by generating more tokens. In other cases, wewould like to minimize the cost at a fixed level of performance.",
  "Refinement (general)Tin (1 + Nr)T (1 + Nr)Nr CzNr, CzRefinement (self-feedback) Tin + (2Tin + T) NrT + 2T NrNr": ": Token budget for representative algorithms from each meta-generation class. Reranking. Tinand T are the number of input tokens and output tokens for each call to the generator, respectively. Forsimplicity, we assume the number of input and output tokens is constant across calls to the generator. Csrefers to the number of tokens required to call a scoring model (e.g., a prompted LLM) on an input andoutput sequence. LLM scorer refers to prompting a LLM with an input and output, and generating a scalarscore (assumed to be 1 token). Transformation. Ct refers to the number of tokens required to call atransformation function (e.g., a prompted LLM) on N sequences. Step-level search. Ts is the number ofoutput tokens in a step, with S the maximum number of steps, such that Ts S T. Nb is the number ofcandidates to keep after pruning (e.g., beam size), and Ne is the number of expansions per iteration. Weassume the cost of the scorer is equal to the cost of scoring a full sequence (Cs). Refinement. Nr is thenumber of refinement iterations. Cz refers to the number of tokens required to obtain external informationduring a refinement iteration.",
  "Token budget": "Meta-generators consist of calling generators, which leads to costs associated with generating tokens. Forinstance, common APIs charge by the number of tokens in the input prompt and the number of output tokens.In general, meta-generators incur token costs from input tokens, output tokens, and external information. For instance, a reranker that generates N sequences incurs a cost of Tin N input tokens, T N outputtokens, and N Cs tokens to run the scoring model, where Cs is the token cost of calling the scoring modelon one sequence. When the scoring model is implemented by prompting an LLM and generating a scalarquality score (assumed to cost 1 token), the external information cost is N (Tin + T + 1). showsthe token budget for representative algorithms from each meta-generation class. Step-level vs. sequence-level search.Consider solving a mathematical problem by generating a solu-tion that consists of multiple steps. Two strategies for doing so are (1) generating one step at a time usinga step-level search algorithm, or (2) generating full solutions in a transformation or re-ranking algorithm.In this case, we can assume that T = Ts S, i.e., the total number of tokens in a solution (T) equals thenumber of tokens in a step (Ts) times the number of steps (S). We can then use to reason aboutwhen step-level search can cost fewer tokens than sequence-level search. From , we see that step-level methods incur a cost from generating output tokens that depends on thepruning parameter Nb, the number of expansions per iteration Ne, and the number of iterations S. Assumingthat Ts S = T, step-level search has fewer output tokens than sequence-level search when Nb Ne < N. Forexample, under these assumptions step-level beam with a beam size of 16 and 64 expansions per iterationhas the same number of output tokens as best-of-1024, while lowering the expansions per iteration to 32would be half the output token cost compared to best-of-1024. On the other hand, shows that step-level search calls the scoring model more often than sequence-level search methods. For instance, when Nb Ne = N, step-level beam search calls the scoring modelN S times compared to N times with reranking. Therefore, one must also account for potential token costsassociated with external information (e.g., sequence scores) when comparing meta-generator token budgets.",
  "Scaling the token budget to improve performance": "In various reasoning-related tasks such as mathematical problem solving, it has been widely observed thatgeneration algorithms which generate multiple sequences and choose among the sequences (e.g., best-of-N, majority voting) can outperform generation algorithms that generate a single sequence (e.g., greedydecoding) (Cobbe et al., 2021; Wang et al., 2023b; Azerbayev et al., 2024; Lightman et al., 2024; Wang et al.,2023a; Sun et al., 2024a).",
  ": Plot from Sun et al. (2024a). Scaling behav-ior of three meta-generators in the number of samplesN on mathematical problem solving (MATH500)": "shows a plot from Sun et al. (2024a)that compares the relationship between the gen-eration budget (in units of sequences) with threesequence-level approaches on the MATH500 bench-mark (Lightman et al., 2024). The plot shows thatthese algorithms can improve monotonically by in-creasing the generation budget. Moreover, each al-gorithm has a different improvement as a functionof the generation budget. For instance, at a budgetof 1024 sequences, weighted voting is preferred tomajority voting or best-of-N in terms of task per-formance. Recently, Chen et al. (2024a) found thatsome models can have a non-monotonic relationshipbetween generation budget and voting performance. The idea of increasing the generation budget to im-prove performance has appeared in many applica-tions.For instance, AlphaCode (Li et al., 2022)generates up to a million sampled programs that arethen filtered using heuristics and execution results.In theorem proving, Draft-Sketch-Prove (Jiang et al., 2023) leverage the proof checker at generation time bygenerating and checking many formal proof candidates, resulting in a monotonically increasing percentageof proven theorems as a function of the budget. More formally, let q(y|x) 1 if y is correct, and 0 otherwise, where correctness may mean a correct solutionto a mathematical problem, a valid proof, a program that passes test cases, etc. Then the goal of generationis y = arg maxyY q(y|x). Since the space of solutions Y is too large, a meta-generator can approximate itby calling a generator multiple times,",
  "arg maxyny1,...,yN q(yn|x),(51)": "where yn q(|x, p). It is clear that performance should improve as N increases, so long as the generatorq assigns probability mass to correct solutions. However, in practice we do not have access to q at testtime, so different meta-generators approximate (51), e.g. with a learned verifier v(x, y), or with a votingalgorithm. The plot above shows that different approximations have different levels of effectiveness.",
  "Minimizing the token budget": "A complementary direction is minimizing the generation budget to achieve a given level of performance.One direction is to route generations to progressively more costly models. For instance, FrugalGPT (Chenet al., 2023b) first generates with a cheap model, then uses a learned scoring function to determine whetherto generate again with a more expensive model, leading to significant cost reductions over calling GPT-4 intheir experimental setting. Another direction is leveraging properties of specific meta-generation algorithmsto reduce the number of calls. Aggarwal et al. (2023) propose to stop sampling in majority voting uponconverging to a majority.",
  "Compute optimal inference": "When choosing or designing a meta-generator, a key consideration is the cost of the meta-generator neededto achieve a given level of performance. For example, running Monte Carlo Tree Search may give goodtask performance, but not if we only allow it to generate the same amount of tokens as parallel sampling.Relatedly, Kapoor et al. (2024b;a) argue that performance comparisons of meta-generation algorithms mustbe performed with respect to token budget and monetary cost, and that in some cases simple meta-generationbaselines can provide a pareto-optimal cost-performance tradeoff compared to complex algorithms. Wu et al. (2024) formalize these tradeoffs in terms of compute-optimal inference: the problem of choosinga model size, number of generated tokens, and meta-generation algorithm that minimizes error subject to acompute budget. Specifically, let the error rate E(M, T; g) be a function of the number of model parametersM, the number of generated tokens T, and the generator g. The goal is to minimize E under the constraintc(M, T, g) = C, where c(M, T, g) is the compute used during inference, measured in floating-point operations:",
  "(Mopt(C), Topt(C), gopt) =arg minM,T,g s.t. c(M,T ;g)=CE(M, T, g),(52)": "where Mopt(C), Topt(C), gopt denotes the choice of model size, generated tokens, and meta-generator thatachieves the lowest error with compute budget C.Wu et al. (2024) study tradeoffs between best-of-N,majority voting, and tree search variants, finding that sampling more tokens from a smaller model often hadbetter cost-performance tradeoffs compared to using a larger model at a given compute budget. Moreover,Monte-Carlo tree search often had worse cost-performance tradeoffs than the other meta-generators. Snellet al. (2024) study similar cost-performance trade-offs between best-of-N, step-level search, and refinement.",
  "Dependence on the underlying generator(s)": "The defining property of meta-generators is that they rely on calling other generation algorithms. Hence asecond natural question is to what degree their performance depends on the underlying generation algorithms. Sampling parameters.Chen et al. (2021) found that the optimal temperature in best-of-N was dependenton N for code generation with the Codex model, with higher temperatures returning better performancefor higher N. Many prior studies use temperatures or sampling parameters that are either unexplainedor ad-hoc. For instance, Minerva (Lewkowycz et al., 2022) uses majority voting with temperature 0.6 andnucleus sampling p = 0.95. These settings have propagated into subsequent studies (Azerbayev et al., 2024). For some classes of meta-generators such as minimum Bayes risk (4.2.2), the effect of sampling parameters isrelatively well-studied. For example, Freitag et al. (2023) investigate the impact of the underlying samplingstrategy in MBR, finding variation across strategies, with epsilon sampling performing best for translation.",
  "aspect of efficiency: the speed of (meta-)generation. Speed is an inherent concern of almost any practicalapplication of generation algorithms: users typically want outputs quickly": "Meta-generators in particular raise demands for fast generation, since they often involve generating manysequences and coordinating multiple components.For example, the meta-generators shown in require generating and scoring 1024 sequences. There are at least two high-level strategies one can taketo speed up generation: (1) speeding up the generation of each individual sequence, and (2) leveragingstructure that comes from multiple generator calls, such as shared partial outputs or the structure of theoverall meta-generation program. We will consider both of these below. Before we start, it is worth noting two points. First, the notion of speeding up itself needs to be mademore precise and measurable. To that end we provide background on the notions of latency, throughput,and the idea that speed is often dependent on the hardware environment in which a meta-generator is run. Second, the topics in this section are part of a rich, rapidly evolving research field that ranges from machinelearning systems to programming language design.It goes without saying that our survey here merelyscratches the surface. We focus our discussion on introducing key ideas, and on examining the interactionbetween the design space of (meta-)generation algorithms and generation speed.",
  "Background": "Goals of speeding up generation.Speeding up generation requires balancing between three high-levelmetrics: (1) latency, the time it takes to generate a single output; (2) throughput, the rate at which outputscan be produced; and (3) quality, measures of model quality such as loss or downstream task metrics. Forinstance, one might change the generation algorithm in a way that speeds up a single generation (improvinglatency), but removes the ability to generate outputs in parallel (degrading throughput). Other cases suchas reducing the precision of model weights may improve latency and throughput, but degrade the modelstask performance. Ideally, we would like to reduce latency, increase throughput, and maintain quality.",
  "Y = ReLU(X)(54)": "must read X from memory, compute ReLU(X) on-chip, and write the result out to memory. However,these two operations have very different arithmetic intensities, defined as the ratio of compute (in FLOP)to unit of memory read or written. This results in (53), for large enough B, C, being compute-bound(bottlenecked by the rate at which operations can be performed) while (54) for large X is memory-bound(bottlenecked by the speed of reading inputs and writing outputs to memory). Thus, reducing the quantity of operations performed (in FLOP) for a given step may not always proportion-ately transfer to an equivalent real-world speedup or cost reduction. This is exacerbated by the propertiesof recent acceleratorsGPUs and TPUs are heavily specialized for matrix multiplication and other high-arithmetic intensity, heavily parallelizable workloads (NVIDIA, 2017; 2020). For example, the H100 canperform up to 989.4 TFLOP/s in BF16 within a dense matrix multiplication using Tensor Cores, but only133.8 TFLOP/s of BF16 arithmetic (NVIDIA, 2022).This specializationand the fact that naive at-tempts to optimize performance oblivious to which operations may be the key bottlenecks may not achievethe anticipated gainsimplies that hardware-aware optimization is a key viewpoint to take when seekingspeedy generation. Algorithmic and architectural co-design with the hardware (Dao et al., 2022; Dao, 2023;Anthony et al., 2024) has yielded some of the most significant speed gains in recent years, in contrast to",
  ": Outline of classes of techniques for speeding up a single generation call": "approaches seeking to minimize theoretical complexity that are disconnected from the hardware level. Onthe flip side, however, Hooker (2020) discuss the notion of the hardware lotterythe idea that co-design ofnovel techniques creates adverse selection effects, where research ideas off the beaten path are dispreferredbecause they interact less well with existing hardware.",
  "p(|y<t, x) = softmax(s(|y<t, x)).(55)": "Performing this step returns two outputs: p(|y<t, x), the probability distribution over immediate nexttokens following (y<t, x) that we have discussed previously, and a state Sy<t,x created as a byproduct ofprocessing y<t, x. For a Transformer (Vaswani et al., 2017) Sy<t,x is produced by retaining all keys andvalues from timesteps up to yt1 within the attention for each layer.3 This is termed the Key-Value (KV)Cache produced by attention at each layer. At this step, we may sample a next-token yt from p(|y<t, x). Subsequently, to generate additional new tokens we may perform any number of decoding steps, whereeach step selects a token from a next-token distribution. For example, the t+1th step selects a token using:",
  "p(|y<t+1, x, Syt,x) = softmax(s(|y<t+1, x, Syt,x)).(56)": "Here, we feed the state Syt,x into the model, representing the already-processed sequence. Each decodingstep saves on computations that are cached in the state, such as the attention keys and values from thepreceding steps. After selecting the t + 1th token, the state is updated to Syt+1,x. These decoding stepsmay be repeated until we have finished generating a sequence. One can accelerate a single generation from an LM by speeding up the time taken per step, such as througharchitectural modifications, model compression, hardware-aware implementation decisions, or by clever par-allelization during autoregressive generation. We discuss each of these in the following paragraphs. Architectural modifications.One strategy is to modify the model architecture. For example, multi-query (Shazeer, 2019) and grouped-query (Ainslie et al., 2023a) attention propose the use of fewer key andvalue heads in transformers attention, leading to reduced KV Cache sizes. Smaller KV Cache sizes canlower memory bandwidth demands, or provide the ability to process larger batches concurrently at a time byenabling more requests to be stored in GPU memory. Similarly, DeepSeek-AI (2024) propose multi-headedlatent attention, attempting to retain the reduced KV Cache of GQA while improving model quality. TheO(t2) complexity of attention (O(t) for each decoding step) can slow generation down as sequences becomelonger, so another option is to forego the transformer architecture or its attention layer altogether. Forexample, traditional recurrent language models (Elman, 1990; Mikolov et al., 2010) compute a next-tokendistribution by maintaining a hidden state, leading to a O(t) time and space complexity (O(1) per step).",
  "Hardware-aware implementation.A number of optimizations may be performed without modifyingthe model architecture or what operations must be performed, simply how they are performed": "For instance, Flash Attention (Dao et al., 2022; Dao, 2023) famously overcomes the O(t2) space complexityof self-attention by adapting the algorithm proposed by Rabe & Staats (2022) for computing self-attentionbased on online softmax (Milakov & Gimelshein, 2018; Jang et al., 2019) and blockwise computation, cruciallywithout changing the output of the attention mechanism, simply its mapping to hardware. Similarly, FlashDecoding (Dao et al., 2023) accelerates the attention operation during decoding by adding extra parallelismover the sequence dimension, allowing the GPU to be fully saturated even for small query and batch sizes, butonly changing the order and mapping of operations on-device, not the end result (up to numeric precision). Numerous software tools (Tillet et al. (2019); PyTorch (2023); Thakkar et al. (2023), inter alia) can enablefast decoding and efficient low-level implementation in practice. Overall, while architectural modificationsto the model itself can increase the ceiling on generation speed, effective implementation is key for achievingperformance anywhere near this ceiling on current accelerators. Parallelization across time.Rather than speeding up the core next-token operation, the draft-then-verify (also called speculative sampling or speculative decoding) pattern leverages clever parallelizationduring autoregressive generation. Draft-then-verify consists of generating proposed next-tokens with a fastmethod (e.g., a smaller model), computing next-token distributions given the proposed tokens in parallel,and either keeping or rejecting the proposed tokens. For example, previously we briefly referred to speculative sampling (Leviathan et al., 2022; Chen et al.,2023a). This method assumes a language model p(yt|y<t) and an efficient draft model q(yt|y<t). At a givenstep t, it generates a continuation yt, yt+1, . . . , yt+k using q, then computes the next token distributionsp(yt|y<t), . . . , p(yt+k|y<t+k) in parallel. Finally, it processes each proposed token, keeping it if q(yt|y<t) p(yt|y<t), and rejecting it when q(yt|y<t) > p(yt|y<t) with probability p(yt|y<t)/q(yt|y<t) or if apreceding token was rejected. Intuitively, as long as (i) generating with q() is much faster than computingthe distributions with p in sequence, and (ii) the rejection rate is not too high, then speculative samplingwill speed up generation without affecting the original models output distribution or quality. Several methods iterate on ideas underlying speculative sampling, including guessing and verifying a treeof proposed tokens (Miao et al., 2023b; Sun et al., 2024b; Chen et al., 2024c), using alternative proposalmodels q (Miao et al., 2023b; Cai et al., 2024), using prompt n-grams as proposals (Yang et al., 2023b), orgenerating in parallel and reusing the generated n-grams as proposals (Fu et al., 2024). Interestingly, manyspeculative sampling approaches which require a separate draft model q() require more total FLOP in orderto generate a given sequence (Chen et al., 2023a; Leviathan et al., 2023; Fu et al., 2024). However, becausethe decoding step is typically memory-bound, the increased parallelism afforded is more than sufficient toprovide substantial generation speedups.",
  "Leveraging shared prefixes": "The repeated model generation calls that occur in meta-generation algorithms crucially often share sim-ilarities in input. Most importantly, they often share prefixes across generation calls.This provides anopportunity to save on computation and dramatically speed up generation throughput. KV Cache and state reuse.In typical transformers, because the KV Cache is updated by appendingthe keys and values of a new token to the cache, the KV Cache for shared prefixes can be prefilled onlya single time and reused across generation calls that share this input prefix. For example, in parallel meta-generation algorithms (4.2) such as Best-of-N, when producing an N-best list {y(n)}Nn=1 g, generatingeach candidate yi requires a prefill step computing Sx in order to sample yi1 and each successive token inyi. Simply computing Sx once and reusing it when sampling each yi can save significant computation andtime, in effect reducing the input token count for such algorithms by a factor of N (). Making such state sharing efficient requires careful handling of the state in memory, but can significantlyspeed up throughput by allowing more outputs to be processed at a time as a result of lightened GPUmemory requirements (Kwon et al., 2023). It can also be generalized beyond a single prefix being shared(Zheng et al., 2023) in order to handle branching, complex trees of already-processed inputs. Later work hasshown that redundant computation can be eliminated even further, allowing specific speed optimizations inthe presence of shared prefixes (Juravsky et al., 2024). KV Cache and state compression.A complementary line of work approaches the challenge of handlingreused model states or KV Caches efficiently by compressing them, reducing the storage required. Gisting(Mu et al., 2023) and other related techniques (Chevalier et al., 2023; Ge et al., 2024b) tackle the sub-problemof long, frequently-recurring input prompts by learning to produce a series of soft tokens (trained tokenembeddings) which compress a given large input prompt into a much smaller, more compact state. Thesemethods can be viewed as a generalization of prefix tuning or prompt tuning (Li & Liang, 2021; Lester et al.,2021). Other methods explore the shortening of KV Caches via determining which items to retain or evictfrom the input prompt, or at each step whether to append new keys and values to the cache (Ge et al., 2024a;Liu et al., 2023; Zhang et al., 2023; Li et al., 2024b; Nawrot et al., 2024; Raposo et al., 2024; Xiao et al.,2024b). Much like model weights, the KV Cache can also be compressed via reducing its storage precision,such as via quantization (Sheng et al., 2023; Lin et al., 2024b; Zhao et al., 2024c; Zirui Liu et al., 2023).Thus, the memory bandwidth cost of loading the KV Cache from memory is reduced, and more tokenscaches can be fit onto GPU memory. However, these compression techniques can lose model quality.",
  "Libraries and tools for fast generation": "We briefly note a few useful libraries and tools for fast and efficient generation, although the space ofuseful tools and libraries is in particular especially subject to fast change. vLLM (Kwon et al., 2023) is ahighly popular library that introduced PagedAttention and implements a number of up-to-date optimizationsfor fast generation, including continuous batching, prefix caching and reuse, various model and KV cachequantization techniques, speculative decoding, and more. TensorRT-LLM is another highly efficient LLMserving library. Especially relevant to this survey, SGLang (Zheng et al., 2023) builds on vLLM to providea domain-specific language optimized for meta-generation. GPT-Fast (PyTorch, 2023) provides a minimal implementation of latency-constrained fast decoding in Py-Torch, and is designed to be useful for prototyping new ideas and to demonstrate the ease of optimizinglow-latency unbatched decoding workloads using simple tools such as torch.compile. For end users, especially those without easy access to data center-grade or high-end consumer-grade GPUs,a number of libraries also implement fast decoding on CPU, which presents its own set of challenges notfully explored in this paper. Libraries such as Llama.cpp4 are popular for consumers, and libraries such asDeepSpeed-Inference (Aminabadi et al., 2022) or PowerInfer (Song et al., 2023; Xue et al., 2024) exploreoptimizations when offloading activations or parameters to slower-access storage or CPU RAM, which requiresystems considerations beyond those discussed for the more typical homogenous accelerator setting.",
  "Discussion: why use sophisticated generation algorithms?": "Finally, we return to the question that we posed in the introduction: why are sophisticated generationalgorithms needed at all?For example, we might imagine that simply sampling once from the modelsunmodified output distribution, y p(y|x) is sufficient. We offer some takeaways based on our survey. Takeaway 1: iron out degeneracies in the learned distribution.In 3.2 and 3.3 we discussedintroducing token-level adapter algorithms to avoid errors in the models distribution (for instance, when thelearned model assigns too much probability to sequences that have low or zero probability under the truedistribution). At a qualitative level, examples encountered in practice include ancestral sampling resultingin incoherent sequences. At another extreme, MAP decoding algorithms (3.1) can result in unnaturallyrepetitive sequences that are nevertheless assigned high probability by the model, or even empty sequences.These degeneracies motivate the use of generation algorithms with alternative goals, such as minimizing BayesRisk (4.2.2), or the use of a token-level adapter. In these cases, generation algorithms offer a mechanismfor modifying the resulting generation distribution to remove these errors. Moving forward. As models improve, will generation algorithms for these cases still be needed? Since theaforementioned errors stem from a model imperfection, it is plausible that future models will not have theseimperfections. Moreover, taking simplicity of the overall generation system as an objective to strive for, wemight explicitly strive to ultimately remove the generation algorithm for these cases. On the other hand,",
  "Conclusion": "We surveyed generation algorithms for language models. We motivated generation algorithms, formalizedtheir goals, and provided a unified treatment of three themes: token-level generation algorithms, meta-generation algorithms, and efficient generation. Our survey brings together past research from the decoding,LLM reasoning, and machine learning systems communities, and identifies directions for future work. We thank Akari Asai, Rob Brekelmans, Yejin Choi, Saibo Geng, Konstantin Golobokov, Shibo Hao, OmarKhattab, Taehyeon Kim, Chufan Shi, Aditi Raghunathan, Sasha Rush, Peter West, Mert Yuksekgonul, andthe TMLR reviewers for helpful discussions.This work was partially supported by NSF DMS-2134012,CCF-2019844, IARPA 2022-22072200003. SW would like to thank Convergent Research. Part of this workwas done while Z. Harchaoui was visiting the Simons Institute for the Theory of Computing.",
  "DavidH.Ackley,GeoffreyE.Hinton,andTerrenceJ.Sejnowski.Alearningalgorithmforboltzmann machines.Cognitive Science, 9(1):147169, 1985.ISSN 0364-0213.doi: URL": "Pranjal Aggarwal, Aman Madaan, Yiming Yang, and Mausam.Lets sample step by step: Adaptive-consistency for efficient reasoning and coding with llms. In Conference on Empirical Methods in NaturalLanguage Processing, 2023. URL Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sang-hai. GQA: Training generalized multi-query transformer models from multi-head checkpoints. In HoudaBouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methodsin Natural Language Processing, pp. 48954901. Association for Computational Linguistics, 2023a. doi:10.18653/v1/2023.emnlp-main.298. URL",
  "Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrn, and Sumit Sanghai.Gqa: Training generalized multi-query transformer models from multi-head checkpoints, 2023b": "Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia Zhang, Ammar Ahmad Awan, Cheng Li, Du Li,Elton Zheng, Jeff Rasley, Shaden Smith, Olatunji Ruwase, and Yuxiong He. Deepspeed inference: Enablingefficient inference of transformer models at unprecedented scale, 2022. Quentin Anthony, Jacob Hatef, Deepak Narayanan, Stella Biderman, Stas Bekman, Junqi Yin, Aamir Shafi,Hari Subramoni, and Dhabaleswar Panda. The case for co-designing model architectures with hardware,2024.",
  "Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian L. Croci, Bo Li, Martin Jaggi, Dan Alistarh, TorstenHoefler, and James Hensman. Quarot: Outlier-free 4-bit inference in rotated llms, 2024": "Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, NicholasJoseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, JacksonKernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish,Chris Olah, and Jared Kaplan. A general language assistant as a laboratory for alignment, 2021. Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen Marcus McAleer, Al-bert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck.Llemma:An open language modelfor mathematics.In The Twelfth International Conference on Learning Representations, 2024.URL Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, John Kernion, Andy Jones, A. Chen,Anna Goldie, Azalia Mirhoseini, C. McKinnon, Carol Chen, Catherine Olsson, C. Olah, Danny Hernandez,Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, E. Perez, Jamie Kerr, J. Mueller, Jeff Ladish,J. Landau, Kamal Ndousse, Kamile Lukoiute, Liane Lovitt, Michael Sellitto, Nelson Elhage, NicholasSchiefer, Noemi Mercado, Nova Dassarma, R. Lasenby, Robin Larson, Sam Ringer, Scott Johnston,Shauna Kravec, S. E. Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly,T. Henighan, Tristan Hume, Sam Bowman, Zac Hatfield-Dodds, Benjamin Mann, Dario Amodei, NicholasJoseph, Sam McCandlish, Tom B. Brown, and Jared Kaplan. Constitutional ai: Harmlessness from aifeedback. ArXiv preprint, abs/2212.08073, 2022. URL Sourya Basu, Govardana Sachitanandam Ramachandran, Nitish Shirish Keskar, and Lav R. Varshney. Miro-stat: a neural text decoding algorithm that directly controls perplexity. In 9th International Conferenceon Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.URL",
  "Peter Bickel and Kjell Doksum. Mathematical Statistics: Basic Ideas and Selected Topics., volume 56. 1977.doi: 10.2307/2286373": "Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. The mathematicsof statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263311, 1993.URL Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, JeffreyWu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Ben-jamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, andDario Amodei.Language models are few-shot learners.In Hugo Larochelle, MarcAurelio Ranzato,Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Pro-cessing Systems 33:Annual Conference on Neural Information Processing Systems 2020, NeurIPS2020, December 6-12, 2020, virtual, 2020.URL Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming Chen, and Tri Dao.Medusa: Simple llm inference acceleration framework with multiple decoding heads. arXiv preprint arXiv:2401.10774, 2024. Nicholas Carlini, Daniel Paleka, Krishnamurthy Dvijotham, Thomas Steinke, Jonathan Hayase, A. F.Cooper, Katherine Lee, Matthew Jagielski, Milad Nasr, Arthur Conmy, Eric Wallace, David Rolnick,and Florian Tramr. Stealing part of a production language model. ArXiv preprint, abs/2403.06634, 2024.URL Louis Castricato, Alexander Havrilla, Shahbuland Matiana, Michael Pieler, Anbang Ye, Ian Yang, SpencerFrazier, and Mark Riedl. Robust preference learning for storytelling via contrastive reinforcement learning,2022. Kai-Wei Chang, Akshay Krishnamurthy, Alekh Agarwal, Hal Daum III, and John Langford. Learning tosearch better than your teacher. In Francis R. Bach and David M. Blei (eds.), Proceedings of the 32ndInternational Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, volume 37 ofJMLR Workshop and Conference Proceedings, pp. 20582066. JMLR.org, 2015. URL Eugene Charniak and Mark Johnson. Coarse-to-fine n-best parsing and MaxEnt discriminative reranking.In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL05), pp.173180. Association for Computational Linguistics, 2005. doi: 10.3115/1219840.1219862. URL",
  "Lingjiao Chen, Jared Quincy Davis, Boris Hanin, Peter Bailis, Ion Stoica, Matei Zaharia, and James Zou.Are more llm calls all you need? towards scaling laws of compound inference systems, 2024a": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harrison Edwards,Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov,Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, S. Gray, Nick Ryder, Mikhail Pavlov, AletheaPower, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, F. Such, D. Cummings,Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William H. Guss, Alex Nichol,Igor Babuschkin, S. Balaji, Shantanu Jain, A. Carr, J. Leike, Joshua Achiam, Vedant Misra, EvanMorikawa, Alec Radford, M. Knight, Miles Brundage, Mira Murati, Katie Mayer, P. Welinder, Bob Mc-Grew, Dario Amodei, Sam McCandlish, I. Sutskever, and Wojciech Zaremba. Evaluating large languagemodels trained on code. ArXiv preprint, abs/2107.03374, 2021. URL Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao, Pengcheng Yin, Sushant Prakash, CharlesSutton, Xuezhi Wang, and Denny Zhou. Universal self-consistency for large language model generation,2023c.",
  "Krishna Teja Chitty-Venkata, Sparsh Mittal, Murali Emani, Venkatram Vishwanath, and Arun K. Somani. Asurvey of techniques for optimizing transformer inference, 2023. URL": "Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, MatthiasPlappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Trainingverifiers to solve math word problems. ArXiv preprint, abs/2110.14168, 2021. URL Michael Collins. Discriminative reranking for natural language parsing. In Pat Langley (ed.), Proceedingsof the Seventeenth International Conference on Machine Learning (ICML 2000), Stanford University,Stanford, CA, USA, June 29 - July 2, 2000, pp. 175182. Morgan Kaufmann, 2000.",
  "DeepSeek-AI. Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model, 2024": "Haikang Deng and Colin Raffel. Reward-augmented decoding: Efficient controlled text generation with a uni-directional reward model. In Proceedings of the 2023 Conference on Empirical Methods in Natural LanguageProcessing. Association for Computational Linguistics, 2023. doi: 10.18653/v1/2023.emnlp-main.721. URL Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. GPT3.int8(): 8-bit matrix multiplicationfor transformers at scale. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.),Advances in Neural Information Processing Systems, 2022.URL Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos,Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. Spqr: A sparse-quantized representation fornear-lossless llm weight compression, 2023.",
  "Randal Douc, Eric Moulines, and David Stoffer. Nonlinear time series: Theory, methods and applicationswith R examples. CRC press, 2014": "Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, PercyLiang, and Tatsunori B. Hashimoto. Alpacafarm: A simulation framework for methods that learn fromhuman feedback, 2023. Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Sean Welleck,Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D. Hwang, Soumya Sanyal, Xiang Ren, AllysonEttinger, Zaid Harchaoui, and Yejin Choi. Faith and fate: Limits of transformers on compositionality. InThirty-seventh Conference on Neural Information Processing Systems, 2023. URL Bryan Eikema.The effect of generalisation on the inadequacy of the mode.In Ral Vzquez, HandeCelikkanat, Dennis Ulmer, Jrg Tiedemann, Swabha Swayamdipta, Wilker Aziz, Barbara Plank, JorisBaan, and Marie-Catherine de Marneffe (eds.), Proceedings of the 1st Workshop on Uncertainty-AwareNLP (UncertaiNLP 2024), pp. 8792. Association for Computational Linguistics, 2024.URL Bryan Eikema and Wilker Aziz. Is MAP decoding all you need? the inadequacy of the mode in neuralmachine translation. In Proceedings of the 28th International Conference on Computational Linguistics,pp. 45064520. International Committee on Computational Linguistics, 2020.doi: 10.18653/v1/2020.coling-main.398. URL",
  "Jeffrey L. Elman.Finding structure in time.Cogn. Sci., 14:179211, 1990.URL": "Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceedings of the56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.889898. Association for Computational Linguistics, 2018.doi: 10.18653/v1/P18-1082.URL Matthew Finlayson, John Hewitt, Alexander Koller, Swabha Swayamdipta, and Ashish Sabharwal. Clos-ing the curious case of neural text degeneration. In The Twelfth International Conference on LearningRepresentations, 2024a. URL",
  "Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot,2023": "Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. OPTQ: Accurate quantization for gen-erative pre-trained transformers. In The Eleventh International Conference on Learning Representations,2023. URL Markus Freitag and Yaser Al-Onaizan. Beam search strategies for neural machine translation. In Proceed-ings of the First Workshop on Neural Machine Translation, pp. 5660. Association for ComputationalLinguistics, 2017. doi: 10.18653/v1/W17-3207. URL Markus Freitag, Behrooz Ghorbani, and Patrick Fernandes. Epsilon sampling rocks: Investigating samplingstrategies for minimum Bayes risk decoding for machine translation. In Houda Bouamor, Juan Pino, andKalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 91989209. Association for Computational Linguistics, 2023. doi: 10.18653/v1/2023.findings-emnlp.617. URL",
  "Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. ArXiv preprint,abs/2312.00752, 2023. URL": "Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, and Zhiting Hu. Reasoning withlanguage model is planning with world model. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.),Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 81548173, Singapore, December 2023. Association for Computational Linguistics.doi: 10.18653/v1/2023.emnlp-main.507. URL Shibo Hao, Yi Gu, Haotian Luo, Tianyang Liu, Xiyan Shao, Xinyuan Wang, Shuhua Xie, Haodi Ma, AdithyaSamavedhi, Qiyue Gao, et al. Llm reasoners: New evaluation, library, and analysis of step-by-step reason-ing with large language models. arXiv preprint arXiv:2404.05221, 2024. Alex Havrilla, Sharath Chandra Raparthy, Christoforus Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravin-skyi, Eric Hambro, and Roberta Railneau. Glore: When, where, and how to improve llm reasoning viaglobal and local refinements. ArXiv preprint, abs/2402.10963, 2024. URL",
  "Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. volumeabs/1503.02531, 2015. URL": "Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,Diego de las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland,Katherine Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, KarenSimonyan, Erich Elsen, Oriol Vinyals, Jack William Rae, and Laurent Sifre. An empirical analysis ofcompute-optimal large language model training.In Alice H. Oh, Alekh Agarwal, Danielle Belgrave,and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022.URL Chris Hokamp and Qun Liu. Lexically constrained decoding for sequence generation using grid beam search.In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume1: Long Papers), pp. 15351546. Association for Computational Linguistics, 2017. doi: 10.18653/v1/P17-1141. URL",
  "Sara Hooker. The hardware lottery, 2020": "Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W. Mahoney, Yakun Sophia Shao, KurtKeutzer, and Amir Gholami. Kvquant: Towards 10 million context length llm inference with kv cachequantization, 2024. Edward J Hu, Moksh Jain, Eric Elmoznino, Younesse Kaddar, Guillaume Lajoie, Yoshua Bengio, and Niko-lay Malkin.Amortizing intractable inference in large language models.In The Twelfth InternationalConference on Learning Representations, 2024. URL Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and DennyZhou. Large language models cannot self-correct reasoning yet. In The Twelfth International Conferenceon Learning Representations, 2024. URL Liang Huang and David Chiang. Forest rescoring: Faster decoding with integrated language models. InProceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pp. 144151.Association for Computational Linguistics, 2007. URL DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, and Behnam Neyshabur.Block-recurrenttransformers. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances inNeural Information Processing Systems, 2022. URL Hanhwi Jang, Joonsung Kim, Jae-Eon Jo, Jaewon Lee, and Jangwoo Kim. Mnnfast: a fast and scalablesystem architecture for memory-augmented neural networks.In Proceedings of the 46th InternationalSymposium on Computer Architecture, ISCA 19, pp. 250263. Association for Computing Machinery, 2019.ISBN 9781450366694. doi: 10.1145/3307650.3322214. URL Albert Qiaochu Jiang, Sean Welleck, Jin Peng Zhou, Timothee Lacroix, Jiacheng Liu, Wenda Li, MatejaJamnik, Guillaume Lample, and Yuhuai Wu. Draft, sketch, and prove: Guiding formal theorem proverswith informal proofs. In The Eleventh International Conference on Learning Representations, 2023. URL",
  "Sayash Kapoor, Benedikt Stroebl, Zachary S. Siegel, Nitya Nadgir, and Arvind Narayanan. Ai agent bench-marks that matter. Manuscript submitted for publication, 2024b": "Muhammad Khalifa, Hady Elsahar, and Marc Dymetman. A distributional approach to controlled textgeneration.In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event,Austria, May 3-7, 2021. OpenReview.net, 2021. URL Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and MateiZaharia. Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensiveNLP. arXiv preprint arXiv:2212.14024, 2022.",
  "Brian Kingsbury, Tara N. Sainath, and Hagen Soltau. Scalable minimum bayes risk training of deep neuralnetwork acoustic models using distributed hessian-free optimization. In Interspeech, 2012. URL": "Philipp Koehn and Rebecca Knowles. Six challenges for neural machine translation. In Proceedings of theFirst Workshop on Neural Machine Translation, pp. 2839. Association for Computational Linguistics,2017. doi: 10.18653/v1/W17-3204. URL Tomasz Korbak, Hady Elsahar, Germn Kruszewski, and Marc Dymetman. Controlling conditional lan-guage models without catastrophic forgetting. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, CsabaSzepesvri, Gang Niu, and Sivan Sabato (eds.), International Conference on Machine Learning, ICML2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Re-search, pp. 1149911528. PMLR, 2022a. URL Tomasz Korbak, Hady ElSahar, Germn Kruszewski, and Marc Dymetman. On reinforcement learning anddistribution matching for fine-tuning language models with no catastrophic forgetting. ArXiv preprint,abs/2206.00761, 2022b. URL Tomasz Korbak, Ethan Perez, and Christopher Buckley. RL with KL penalties is better viewed as Bayesianinference. In Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 10831091. As-sociation for Computational Linguistics, 2022c. URL Kalpesh Krishna, Yapei Chang, John Wieting, and Mohit Iyyer. RankGen: Improving text generation withlarge ranking models. In Proceedings of the 2022 Conference on Empirical Methods in Natural LanguageProcessing, pp. 199232. Association for Computational Linguistics, 2022. URL Ilia Kulikov, Alexander Miller, Kyunghyun Cho, and Jason Weston. Importance of search and evaluationstrategies in neural dialogue modeling. In Proceedings of the 12th International Conference on NaturalLanguage Generation, pp. 7687. Association for Computational Linguistics, 2019. doi: 10.18653/v1/W19-8609. URL Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D Co-Reyes, Avi Singh, Kate Baumli, ShariqIqbal, Colton Bishop, Rebecca Roelofs, Lei M Zhang, Kay McKinney, Disha Shrivastava, Cosmin Paduraru,George Tucker, Doina Precup, Feryal Behbahani, and Aleksandra Faust. Training language models to self-correct via reinforcement learning, 2024. URL",
  "Sachin Kumar, Biswajit Paria, and Yulia Tsvetkov. Gradient-based constrained sampling from languagemodels. In Conference on Empirical Methods in Natural Language Processing, 2022. URL": "Shankar Kumar and William Byrne. Minimum Bayes-risk decoding for statistical machine translation. InProceedings of the Human Language Technology Conference of the North American Chapter of the Asso-ciation for Computational Linguistics: HLT-NAACL 2004, pp. 169176. Association for ComputationalLinguistics, 2004. URL Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gon-zalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving withpagedattention, 2023. Guillaume Lample, Timothee Lacroix, Marie anne Lachaux, Aurelien Rodriguez, Amaury Hayat, ThibautLavril, Gabriel Ebner, and Xavier Martinet. Hypertree proof search for neural theorem proving. In Alice H.Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural InformationProcessing Systems, 2022. URL",
  "Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decod-ing, 2023": "AitorLewkowycz,AndersJohanAndreassen,DavidDohan,EthanDyer,HenrykMichalewski,Vinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu,Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with lan-guage models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advancesin Neural Information Processing Systems, 2022. URL Junyi Li, Tianyi Tang, Wayne Xin Zhao, and Ji-Rong Wen. Pretrained language model for text generation:A survey. In Zhi-Hua Zhou (ed.), Proceedings of the Thirtieth International Joint Conference on ArtificialIntelligence, IJCAI-21, pp. 44924499. International Joint Conferences on Artificial Intelligence Organi-zation, 8 2021. doi: 10.24963/ijcai.2021/612. URL SurveyTrack.",
  "Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, PatrickLewis, and Deming Chen. Snapkv: Llm knows what you are looking for before generation, 2024b": "Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rmi Leblond, Tom Ec-cles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Mas-son dAutume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, AlexeyCherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nandode Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with alphacode.Science, 378(6624):10921097, 2022. doi: 10.1126/science.abq1158. URL Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, ShakedMeirom, Yonatan Belinkov, Shai Shalev-Shwartz, Omri Abend, Raz Alon, Tomer Asida, Amir Bergman,Roman Glozman, Michael Gokhman, Avashalom Manevich, Nir Ratner, Noam Rozen, Erez Shwartz, MorZusman, and Yoav Shoham. Jamba: A hybrid transformer-mamba language model, 2024. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike,John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth InternationalConference on Learning Representations, 2024. URL Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao,Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for llm compressionand acceleration. In MLSys, 2024a.",
  "Yujun Lin, Haotian Tang, Shang Yang, Zhekai Zhang, Guangxuan Xiao, Chuang Gan, and Song Han. Qserve:W4a8kv4 quantization and system co-design for efficient llm serving, 2024b": "Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A. Smith, and YejinChoi. DExperts: Decoding-time controlled text generation with experts and anti-experts. In Proceedingsof the 59th Annual Meeting of the Association for Computational Linguistics and the 11th InternationalJoint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 66916706. Associationfor Computational Linguistics, 2021a. doi: 10.18653/v1/2021.acl-long.522. URL",
  "Alisa Liu, Xiaochuang Han, Yizhong Wang, Yulia Tsvetkov, Yejin Choi, and Noah A. Smith. Tuning languagemodels by proxy, 2024a": "Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru, Yejin Choi, Hannaneh Hajishirzi, and Asli Celikyilmaz.Dont throw away your value model! generating more preferable text with value-guided monte-carlo treesearch decoding.In First Conference on Language Modeling, 2024b.URL Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train,prompt, and predict: A systematic survey of prompting methods in natural language processing, 2021b.URL",
  "Sidi Lu, Yaoming Zhu, Weinan Zhang, Jun Wang, and Yong Yu. Neural text generation: Past, present andbeyond, 2018. URL": "Ximing Lu, Peter West, Rowan Zellers, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. NeuroLogicdecoding: (un)supervised neural text generation with predicate logic constraints. In Kristina Toutanova,Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell,Tanmoy Chakraborty, and Yichao Zhou (eds.), Proceedings of the 2021 Conference of the North AmericanChapter of the Association for Computational Linguistics: Human Language Technologies, pp. 42884299,Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.339.URL Ximing Lu, Sean Welleck, Peter West, Liwei Jiang, Jungo Kasai, Daniel Khashabi, Ronan Le Bras, LianhuiQin, Youngjae Yu, Rowan Zellers, Noah A. Smith, and Yejin Choi. NeuroLogic a*esque decoding: Con-strained text generation with lookahead heuristics. In Proceedings of the 2022 Conference of the NorthAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies, pp.780799. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.naacl-main.57. URL Ximing Lu, Faeze Brahman, Peter West, Jaehun Jung, Khyathi Chandu, Abhilasha Ravichander, Prithvi-raj Ammanabrolu, Liwei Jiang, Sahana Ramnath, Nouha Dziri, Jillian Fisher, Bill Lin, Skyler Hallinan,Lianhui Qin, Xiang Ren, Sean Welleck, and Yejin Choi. Inference-time policy adapters (IPA): Tailor-ing extreme-scale LMs without fine-tuning.In Houda Bouamor, Juan Pino, and Kalika Bali (eds.),Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 68636883. Association for Computational Linguistics, 2023.doi: 10.18653/v1/2023.emnlp-main.424.URL",
  "Shahbuland Matiana, JR Smith, Ryan Teehan, Louis Castricato, Stella Biderman, Leo Gao, and SpencerFrazier. Cut the carp: Fishing for zero-shot story evaluation, 2021": "Clara Meister, Ryan Cotterell, and Tim Vieira. If beam search is the answer, what was the question? InProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),pp. 21732185. Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.emnlp-main.170.URL Clara Meister, Tiago Pimentel, Luca Malagutti, Ethan Wilcox, and Ryan Cotterell.On the efficacy ofsampling adapters. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings ofthe 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.",
  "William Merrill and Ashish Sabharwal. The expressive power of transformers with chain of thought. InThe Twelfth International Conference on Learning Representations, 2024.URL": "Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Hongyi Jin, Tianqi Chen, and Zhihao Jia.Towards efficient generative large language model serving: A survey from algorithms to systems, 2023a.URL Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae Ying Yee Wong, ZhuomingChen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao Jia. Specinfer: Accelerating generative llm servingwith speculative inference and token tree verification.ArXiv preprint, abs/2305.09781, 2023b.URL",
  "Piotr Nawrot, Adrian acucki, Marcin Chochowski, David Tarjan, and Edoardo M. Ponti. Dynamic memorycompression: Retrofitting llms for accelerated inference, 2024": "Nathan Ng, Kyra Yee, Alexei Baevski, Myle Ott, Michael Auli, and Sergey Edunov.Facebook FAIRsWMT19 news translation task submission. In Proceedings of the Fourth Conference on Machine Trans-lation (Volume 2: Shared Task Papers, Day 1), pp. 314319. Association for Computational Linguistics,2019. doi: 10.18653/v1/W19-5333. URL Ansong Ni, Srini Iyer, Dragomir Radev, Ves Stoyanov, Wen-tau Yih, Sida I Wang, and Xi Victoria Lin. Lever:Learning to verify language-to-code generation with execution. In Proceedings of the 40th InternationalConference on Machine Learning (ICML23), 2023.",
  "NVIDIA. Nvidia H100 tensor core GPU architecture, 2022": "Franz Josef Och and Hermann Ney. Discriminative training and maximum entropy models for statisticalmachine translation. In Proceedings of the 40th Annual Meeting of the Association for ComputationalLinguistics, pp. 295302. Association for Computational Linguistics, 2002. doi: 10.3115/1073083.1073133.URL Theo X. Olausson, Jeevana Priya Inala, Chenglong Wang, Jianfeng Gao, and Armando Solar-Lezama. Isself-repair a silver bullet for code generation?In The Twelfth International Conference on LearningRepresentations, 2024. URL",
  "Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, and Alane Suhr. Autonomous evaluationand refinement of digital agents, 2024": "Liangming Pan, Michael Stephen Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William YangWang. Automatically correcting large language models: Surveying the landscape of diverse self-correctionstrategies. ArXiv preprint, abs/2308.03188, 2023. URL Adam Pauls and Dan Klein. K-best A* parsing. In Proceedings of the Joint Conference of the 47th AnnualMeeting of the ACL and the 4th International Joint Conference on Natural Language Processing of theAFNLP, pp. 958966. Association for Computational Linguistics, 2009. URL Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao,Xin Cheng, Michael Chung, Leon Derczynski, Xingjian Du, Matteo Grella, Kranthi Gv, Xuzheng He,Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartomiej Koptyra, Hayden Lau, Ji-aju Lin, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Jo-han Wind, Stanisaw Woniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu. RWKV:Reinventing RNNs for the transformer era.In Houda Bouamor, Juan Pino, and Kalika Bali (eds.),Findings of the Association for Computational Linguistics:EMNLP 2023, pp. 1404814077. Associ-ation for Computational Linguistics, 2023.doi:10.18653/v1/2023.findings-emnlp.936.URL Gabriel Poesia, Oleksandr Polozov, Vu Le, A. Tiwari, Gustavo Soares, Christopher Meek, and Sumit Gulwani.Synchromesh: Reliable code generation from pre-trained language models. ArXiv, abs/2201.11227, 2022.",
  "Alexander Rush. MiniChain, 2023. URL": "Jon Saad-Falcon, Adrian Gamarra Lafuente, Shlok Natarajan, Nahum Maru, Hristo Todorov, Etash Guha,E. Kelly Buchanan, Mayee Chen, Neel Guha, Christopher R, and Azalia Mirhoseini. Archon: An archi-tecture search framework for inference-time techniques, 2024. URL Swarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit Bansal, Jason Weston, and Xian Li. Branch-solve-merge improves large language model evaluation and generation. ArXiv preprint, abs/2310.15123, 2023.URL",
  "Noam Shazeer. Fast transformer decoding: One write-head is all you need, 2019": "Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y. Fu, Zhiqiang Xie, BeidiChen, Clark Barrett, Joseph E. Gonzalez, Percy Liang, Christopher R, Ion Stoica, and Ce Zhang. Flexgen:High-throughput generative inference of large language models with a single gpu, 2023. Chufan Shi, Haoran Yang, Deng Cai, Zhisong Zhang, Yifan Wang, Yujiu Yang, and Wai Lam. A thoroughexamination of decoding methods in the era of LLMs. In Yaser Al-Onaizan, Mohit Bansal, and Yun-NungChen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing,pp. 86018629, Miami, Florida, USA, November 2024a. Association for Computational Linguistics. URL Freda Shi, Daniel Fried, Marjan Ghazvininejad, Luke Zettlemoyer, and Sida I. Wang. Natural languageto code translation with execution.In Proceedings of the 2022 Conference on Empirical Methods inNatural Language Processing, pp. 35333546. Association for Computational Linguistics, 2022.URL",
  "Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao.Reflexion: Language agents with verbal reinforcement learning, 2023": "David Silver, Aja Huang, Christopher J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche,Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Do-minik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, KorayKavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of go with deep neural networksand tree search. Nature, 529:484503, 2016. URL",
  "Yixin Song, Zeyu Mi, Haotong Xie, and Haibo Chen. Powerinfer: Fast large language model serving with aconsumer-grade gpu, 2023": "Felix Stahlberg and Bill Byrne.On NMT search errors and model errors: Cat got your tongue?InProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9thInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 33563362. As-sociation for Computational Linguistics, 2019. doi: 10.18653/v1/D19-1331. URL Felix Stahlberg, Ilia Kulikov, and Shankar Kumar. Uncertainty determines the adequacy of the mode andthe tractability of decoding in sequence-to-sequence models. In Proceedings of the 60th Annual Meeting ofthe Association for Computational Linguistics (Volume 1: Long Papers), pp. 86348645. Association forComputational Linguistics, 2022. doi: 10.18653/v1/2022.acl-long.591. URL Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, DarioAmodei, and Paul Christiano. Learning to summarize from human feedback. In Proceedings of the 34thInternational Conference on Neural Information Processing Systems, NIPS20. Curran Associates Inc.,2020. ISBN 9781713829546.",
  "Ziteng Sun, Ananda Theertha Suresh, Jae Hun Ro, Ahmad Beirami, Himanshu Jain, and Felix Yu. Spectr:Fast speculative decoding via optimal transport, 2024b": "Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. InZoubin Ghahramani, Max Welling, Corinna Cortes, Neil D. Lawrence, and Kilian Q. Weinberger (eds.),Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Pro-cessing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pp. 31043112, 2014. URL Mirac Suzgun, Luke Melas-Kyriazi, and Dan Jurafsky.Follow the wisdom of the crowd: Effective textgeneration via minimum Bayes risk decoding.In Anna Rogers, Jordan Boyd-Graber, and NaoakiOkazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, pp. 42654293.Association for Computational Linguistics, 2023. doi: 10.18653/v1/2023.findings-acl.262. URL Vijay Thakkar, Pradeep Ramani, Cris Cecka, Aniket Shivam, Honghao Lu, Ethan Yan, Jack Kosaian,Mark Hoemmen, Haicheng Wu, Andrew Kerr, Matt Nicely, Duane Merrill, Dustyn Blasig, Fengqi Qiao,Piotr Majcher, Paul Springer, Markus Hohnerbach, Jin Wang, and Manish Gupta. Cutlass, 2023. URL",
  "Gladys Tyen, Hassan Mansoor, Victor Crbune, Peter Chen, and Tony Mak. Llms cannot find reasoningerrors, but can correct them given the error location, 2024": "Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell,Geoffrey Irving, and Irina Higgins. Solving math word problems with process- and outcome-based feedback,2022. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio,Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in NeuralInformation Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017,December 4-9, 2017, Long Beach, CA, USA, pp. 59986008, 2017. URL",
  "Giorgos Vernikos and Andrei Popescu-Belis. Dont rank, combine! combining machine translation hypothesesusing quality estimation, 2024": "Peiyi Wang, Lei Li, Zhihong Shao, R. X. Xu, Damai Dai, Yifei Li, Deli Chen, Y.Wu, and Zhifang Sui.Math-shepherd:Verify and reinforce llms step-by-step without human annotations.ArXiv preprint,abs/2312.08935, 2023a. URL Wenxiao Wang, Wei Chen, Yicong Luo, Yongliu Long, Zhengkai Lin, Liye Zhang, Binbin Lin, Deng Cai, andXiaofei He. Model compression and efficient inference for large language models: A survey, 2024a. URL Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery,and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The EleventhInternational Conference on Learning Representations, 2023b. URL",
  "Zhiruo Wang, Zhoujun Cheng, Hao Zhu, Daniel Fried, and Graham Neubig. What are tools anyway? asurvey from the language model perspective, 2024b": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh,Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information ProcessingSystems, 2022. URL Sean Welleck, Ilia Kulikov, Jaedeok Kim, Richard Yuanzhe Pang, and Kyunghyun Cho. Consistency of arecurrent language model with respect to incomplete decoding. In Proceedings of the 2020 Conference onEmpirical Methods in Natural Language Processing (EMNLP), pp. 55535568. Association for Computa-tional Linguistics, 2020. doi: 10.18653/v1/2020.emnlp-main.448. URL",
  "Zhenliang Xue, Yixin Song, Zeyu Mi, Le Chen, Yubin Xia, and Haibo Chen.Powerinfer-2: Fast largelanguage model inference on a smartphone, 2024": "Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan Prenger,and Anima Anandkumar. LeanDojo: Theorem proving with retrieval-augmented language models. InNeural Information Processing Systems (NeurIPS), 2023a. Kevin Yang and Dan Klein. FUDGE: Controlled text generation with future discriminators. In Proceedingsof the 2021 Conference of the North American Chapter of the Association for Computational Linguistics:Human Language Technologies, pp. 35113535. Association for Computational Linguistics, 2021.doi:10.18653/v1/2021.naacl-main.276. URL Nan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin Jiang, Linjun Yang, Rangan Majumder, and Furu Wei.Inference with reference: Lossless acceleration of large language models. ArXiv preprint, abs/2304.04487,2023b. URL",
  "Stephen Zhao, Rob Brekelmans, Alireza Makhzani, and Roger Grosse. Probabilistic inference in languagemodels via twisted sequential monte carlo, 2024b. URL": "Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy,Tianqi Chen, and Baris Kasikci. Atom: Low-bit quantization for efficient and accurate llm serving, 2024c. Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Jeff Huang, Chuyue Sun, Cody Hao Yu, Shiyi Cao, ChristosKozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. Efficiently programming largelanguage models using sglang, 2023. Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang.Distserve: Disaggregating prefill and decoding for goodput-optimized large language model serving, 2024. Denny Zhou, Nathanael Schrli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, ClaireCui, Olivier Bousquet, Quoc V Le, and Ed H. Chi. Least-to-most prompting enables complex reasoningin large language models. In The Eleventh International Conference on Learning Representations, 2023a.URL Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk,Daniel Fried, Uri Alon, et al. Webarena: A realistic web environment for building autonomous agents.ArXiv preprint, abs/2307.13854, 2023b. URL"
}