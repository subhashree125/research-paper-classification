{
  "Abstract": "Recent studies reveal that deep representation learning models without proper regularizationcan suffer from the dimensional collapse problem, i.e., representation vectors span over a lowerdimensional space. In the domain of graph deep representation learning, the phenomenonthat the node representations are indistinguishable and even shrink to a constant vector iscalled oversmoothing. Based on the analysis of the rank of node representations, we findthat representation oversmoothing and dimensional collapse are highly related to each otherin deep graph neural networks, and the oversmoothing problem can be interpreted by thedimensional collapse of the node representation matrix. Then, to address the dimensionalcollapse and the oversmoothing together in deep graph neural networks, we first find vanillaresidual connections and contrastive learning producing sub-optimal outcomes by ignoringthe structured constraints of graph data. Motivated by this, we propose a novel graphneural network named DrGNN to alleviate the oversmoothing issue from the perspective ofaddressing dimensional collapse. Specifically, in DrGNN, we design a topology-preservingresidual connection for graph neural networks to force the low-rank of hidden representationsclose to the full-rank input features. Also, we propose the structure-guided contrastivelearning to ensure only close neighbors who share similar local connections can have similarrepresentations. Empirical experiments on multiple real-world datasets demonstrate thatDrGNN outperforms state-of-the-art deep graph representation baseline algorithms. The codeof our method is available at the GitHub link:",
  "Introduction": "Representation learning models have achieved outstanding performance for various application domains byoutputting informative hidden representations, such as computer vision and natural language processing.Recent studies (Hua et al., 2021; Jing et al., 2022; Guo et al., 2023) show that the deep representation learningmodels without proper regularization tend to produce representations that collapse along certain directions,known as the dimensional collapse, which can be further interpreted by the visualization of the singularityranking of the matrices of representations (Hua et al., 2021).",
  "Published in Transactions on Machine Learning Research (10/2024)": "As shown in , our DrGNN achieves the second place performance in terms of node classificationaccuracy, among other state-of-the-art graph neural network methods. Also, we can observe that stackingmore layers can increase the performance of DrGNN, because multi-hop neighbor information is aggregatedfor the message passing in heterphilous graphs. The reason why DrGNN can not achieve the best is that,although stacking layers can aggregate information from multi-hop neighbors, the loss SCL is still regulatingthat close neighbors should share similar representations, which is generally based on the homophilouscondition for more common settings. Note that our design of DrGNN is not solely for heterphilous graphsbut for how to stack layers wisely for upgrading performance when the stacking operation is necessary andunavoidable. Heterophily is not the only reason for stacking graph neural layers; at least, the reasoning canalso originate from missing features and labels, as shown above, where we need to stack more graph neurallayers to mitigate the missing features by incorporating more neighbors.",
  "(a)(b)": ": (a). A toy example on the Cora dataset demonstrates the rank deficiency, where GCN is chosen asthe backbone, the number of layers is set to 64, and the dimension of representation is 100. WGResNet andSCL are two major components of our proposed DrGNN, which are model-agnostic and can be added onGCN for example. (b) The visualization about how vanilla residual connections of neural layers turn thelow-rank representation Z into a full-rank representation Z. However, stacking more graph neural layers is indispensable in some cases (e.g., heterophily, long-rangeinteraction, etc.). For example, for a certain query node, the close neighbors can have missing features andlabels; then, we may need the information from distant neighbors to represent the query node, as two casestudies show in .2. According to the message passing of graph neural networks (Xu et al., 2018),adding one graph neural network layer equals aggregating information from one-hop neighbors, such that weneed to stack multiple graph neural network layers. Thus, how to stack more graph neural networklayers to maintain and even boost the representation power motivates our paper. Addressing the dimensional collapse problem for deep neural networks, previous study of residual connec-tions (He et al., 2016) among neural layers can be an effective manner, i.e., it has been discovered thatresidual connections across neural network layers force the low-rank of hidden representations close to thefull-rank input features (Jing et al., 2022), as shown in (b). The residual connections pave theway for eliminating the dimensional collapse for deep neural networks and indicate the de-oversmoothingprobability for deep graph neural networks, but for non-IID graph data, the vanilla residual connectionsignore the topological assumption of homophilous graph data that closer neighbors are more important duringthe embedding process. Formally, adding vanilla residual connection can induce the drawback of what we callthe shading neighbors effects, i.e., close neighbors become less important during the neural representationprocess. The details are discussed in .2.",
  ": An Arbitrary GNN Backbone with the Proposed DrGNN": "Moreover, for the oversmoothing phenomenon in the graph representation learning domain, the direct resultis that individual representations are indistinguishable. Hence, contrastive learning serves as a viable solution,but the existing work (Guo et al., 2023) simply introduces vanilla contrastive loss as a regularization whilefailing to consider the topological relationship of positive and negative pairs. Facing the latent dimensional collapse problem (i.e., non-sufficient singular values of covariance matrix ofrepresentations) and observable oversmoothing problem (i.e., non-discriminative node embedding vectors),we propose two effective solutions, i.e., Weight-Decaying Graph Residual Connection (WG-ResNet) andStructure-Guided Contrastive Learning (SCL) for deep graph neural networks. In brief, WG-ResNet appliesweighted residual connections to preserve the input graph topology, and SCL also weighs different positiveand negative pairs based on their topological relations. Besides the theoretical analysis, a visualization ofSCL and WG-ResNet in addressing dimensional collapse is also shown in (a). Moreover, it can beobserved that SCL itself can alleviate the dimensional collapse to some extent, i.e., alleviating oversmoothingby contrastive learning can alleviate the dimensional collapse, which suggests the close relationship betweendimensional collapse and oversmoothing. Hence, we propose an end-to-end graph neural network model DrGNN, which encloses SCL and WG-ResNetin a model-agnostic manner to help arbitrary graph neural networks go deeper effectively compared to state-of-the-art baselines, supported by theoretical and empirical analysis. Furthermore, we designed extensiveablation studies to show that SCL and WG-ResNet both contribute to boosting the representation power,and their combination can reach optimal results in terms of node classification.",
  "The Proposed DrGNN": "In this section, we begin with the overview of DrGNN and then provide the details of the Weight-decayingGraph Residual Connection (WG-ResNet) and Structure-guided Contrastive Learning (SCL). We formalizethe problem of graph embedding within the context of an undirected graph G = (V, E, X), where V consistsof n nodes, E consists of m edges, X Rnd denotes the feature matrix and d is the feature dimension. Welet A Rnn denote the adjacency matrix and denote Ai Rn as the adjacency vector for node vi. Hi Rh",
  "Overview": "The overview of our proposed DrGNN is shown in and DrGNN consists of two parts, includingthe graph architecture WG-ResNet and contrastive loss SCL. Specifically, the green dash line stands forWG-ResNet, where H(l) at the l-th layer will be adjusted by its second last layer H(l2) and the first layerH(1) with proper weights. The red dash line in stands for SCL, where we sample positive nodepairs and negative node pairs based on the input graph topology such that the hidden representations of",
  "Weight-Decaying Graph Residual Connection (WG-ResNet)": "As shown in (b), the vanilla residual connections (e.g., ResNet (He et al., 2016)) have the potentialto alleviate the dimensional collapse of deep neural networks. But for deep graph neural networks, we discoverthat simply adding residual connections leads to the sub-optimal solution. As the way of ResNet stackinggraph neural network layers, the importance of close neighbors features gradually decreases during the GNNinformation aggregation process, and the faraway neighbor information becomes dominant. More concretelyspeaking, because the residual connection of ResNet connects the current neural layer with its second lastlayer, taking graph convolutional network (GCN) (Kipf & Welling, 2017) as an example, the vanilla residualconnection of ResNet is expressed as follows.",
  "A D 1": "2 and A = A + I, where D is the degree matrix. Withoutloss of generality, we can assume the last layer of GNNs l is divisible by 2. Then, just by extending thedangling H(lt) iteratively (e.g., first substituting H(l2) with its previous residual blocks), the above Eq. 2for the standard residual connection (i.e., the way of ResNet (He et al., 2016)) could be rewritten as follows.",
  "+ RELU( AH(i)W (i)) + + RELU( AH(1)W (1))Information aggregated from the nearest neighbors(3)": "According to (Xu et al., 2019), stacking l layers in GNNs and obtaining H(l) can be interpreted as aggregatinginformation from l-hop neighbors for node hidden representations. As shown in Eq. 3, when we stack morelayers in GNNs, the information collected from faraway neighbors becomes dominant (as there are more termsregarding the information from faraway neighbors), and dilutes the information collected from the nearestneighbors (e.g., 1-hop or 2-hop neighbors). This phenomenon contradicts the general intuition on homophilousgraph that the close neighbors of a node carry the most important information, and the importance degradeswith faraway neighbors. Formally, we describe this phenomenon as shading neighbors effect when stackinggraph neural layers, as the importance of the nearest neighbors is diminishing. As the enlarging the field ofinformation aggregation, this way cannot essentially get rid of the overshooting problem. We empiricallydemonstrate that the shading neighbors effect degrades GNN performance in downstream tasks in Section F:Specifically, we show that (1) vanilla residual connection of ResNet exhibits the shading neighbors effect ingraph representation learning; (2) jumping knowledge (Xu et al., 2018) can be a viable solution to mitigatethe shading neighbors effect to some extent; (3) our proposed WG-ResNet achieves the best effectiveness inaddressing the shading neighbors effect. To formally introduce our proposed generic graph architecture, i.e., Weight-Decaying Graph ResidualConnection (WG-ResNet), we first introduce the formulation and then provide insights regarding why itcan work. Specifically, our WG-ResNet introduces the layer similarity and weight decaying factor as follows.",
  "H(1)i H(l)i measures the similarity between the l-th layer and the 1-st layer,": "and we use the exponential function e() to map the cosine similarity ranging from to [e1, e1] toavoid the negative similarity weights. Moreover, the term el/ is the decaying factor to further adjust thesimilarity weight of H(l), where is a constant hyperparameter. Our introduced similarity ecos(H(1),H(l))l/ is not fixed but dynamic during each training iteration toreflect the optimized similarity of different layers, which also expands the hypothesis space of deeper GNNs.More importantly, the introduced decaying factor el/ aims to mitigate the shading neighbor effect bystrengthening layer-wise dependency and preserving topology information in deeper GNNs. As is a positiveconstant, the value of el/ is decreasing as l increases, such that the later stacked layers become lessinfluential than the previously stacked ones due to the decaying weight. In contrast, without the decayingfactor, the layer-wise weights remain independent, and the shading neighbors effect persists. Moreover, wevisualize the layer-wise weight distribution of different residual connection methods and their effectivenessin Appendix B. From another perspective, the hyperparameter of the decaying factor actually controlsthe number of effective neural layers in deeper GNNs, i.e., after the effective layer, stacking more layers justmaintains the performance or brings marginal upgrade. We find its value directly related to the diameter ofthe input graph, and this finding is quite important to avoid heavy hyperparameter tuning. The detaileddiscussion can be found in .4.",
  "Structure-Guided Contrastive Learning (SCL)": "According to (Hua et al., 2021; Jing et al., 2022), contrastive representation learning methods show success inpreventing dimensional collapse for image recognition. For graph data, the contrastive methods are able toconstruct the positive and negative sets and minimize the similarity of the negative pairs (Wang & Isola,2020; Guo et al., 2023). However, simply adopting the idea of contrastive regularization in deep graph neuralnetworks could not fully alleviate the oversmoothing issue due to ignoring the topological relation of non-IIDgraph data. To address this issue with the geometry consideration, we propose the Structure-guidedContrastive Learning (SCL) as follows.Intuitively, for an anchor node vi, its positive sample (i.e.,connected node) should have close node representations, and its negative sample (i.e., disconnected node)should have discriminative node representations. Moreover, we propose and to distinguish the importanceof positive and negative samples based on the input structure.",
  "ab": "||a||||b|| ),dist() is a distance measurement function (e.g., hamming distance (Norouzi et al., 2012)), Ni is the setcontaining one-hop neighbors of node vi, Ni is the complement of the set Ni, m is the number of edges and nis the number of vertices. Moreover, (vi, vj) iterates over all connected edges in the input graph, and (vi,vk) iterates over all disconnected edges in the input graph. The intuition of Eq. 5 is to maximize the similarity of the representations of the positive pairs and tominimize the similarity of the representations of the negative pairs, such that the node representations becomediscriminative. In which process, some previous research works (Perozzi et al., 2014; Grover & Leskovec, 2016;Le, 2021) would first assume that the importance of each edge is identical. However, such an assumption doesnot always get satisfied in many applications (Velickovic et al., 2017; Faisal et al., 2015). To address this issue,we reweigh the importance of edges by considering the graph topological structure via importance scores and . Therefore, for a positive pair, if two nodes have similar topological structures (e.g., ego-networks), theimportance score of this node pair should be large; but for a negative pair, if two nodes also have similartopological structures, the importance score of this negative pair should be small.",
  "E( Ppos(E) log(D(E)) + Pneg(E) log(1 D(E)))dE": "where D(E) = f(zi, zj) is the discriminator of GAN with edge E = (vi, vj) being connected or not, by noderepresentations zi and zj. Probabilities Ppos(E) = Ppos(E) and Pneg(E) = Pneg(E) are defined above.(Proof in Appendix C) According to Proposition 2.1, the proposed LSCL can be interpreted as distinguishing the existence of acertain edge based on the representation vectors of two nodes. Next, we then derive how this design helps, asa regularizer, to alleviate the oversmoothing problem (Rusch et al., 2023) (i.e., hidden representation vectorsof different nodes are similar given a selected distance function) based on positive and negative samples. Theorem 2.2. Given an optimized discriminator D(E), to achieve the optimization process of Proposi-tion 2.1, regularization term LSCL enforces that connected nodes have different representation vectors thandisconnected nodes. The theoretical optimum1 of D(E) isPpos(E)",
  "Experiment Setup": "Datasets. Cora (Lu & Getoor, 2003) dataset is a citation network consisting of 5,429 edges and 2,708scientific publications from 7 classes. The edge in the graph represents the citation of one paper by another.CiteSeer (Lu & Getoor, 2003) dataset consists of 3,327 scientific publications which could be categorized into6 classes, and this citation network has 9,228 edges. PubMed (Namata et al., 2012) is a citation networkconsisting of 88,651 edges and 19,717 scientific publications from 3 classes. Reddit (Hamilton et al., 2017b)",
  "When do we need more layers of graph neural networks?": "Case 1: Missing Features. We first consider a scenario where some attribute values are missing in the inputgraph. In such cases, shallow GNNs may not perform well because they cannot gather enough informationfrom close neighbors due to the presence of numerous missing values. By increasing the number of layers,GNNs can gather more information and capture latent knowledge to compensate for missing features. To verifythis, we conducted the following experiment: we randomly masked p% of attributes (i.e., setting the maskedattributes to be 0), gradually increased the number of layers, and recorded the accuracy for each setting. Inthis case study, we selected the number of layers from the set {2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30, 40, 50, 60},and the backbone model used was GCN. For a fair comparison, we added residual connection of ResNet (Heet al., 2016) to baselines if adding it could enhance the baselines performance, denoted as (+RC). Werepeated the experiments five times and recorded the mean accuracy and standard deviation.",
  "DrGNN0.6524 0.0087200.6169 0.0063600.5576 0.007050": "From , we find that when the missing rate is 25%, shallow GCN with residual connections has enoughcapacity to achieve the best performance. Given that, our proposed method further improves the performanceby more than 3.83% on the CiterSeer dataset and 4.08% on the Cora dataset by stacking a few layers.However, when we increase the missing rate to 50% and 75%, we observe that most methods tend to achievethe best performance by stacking more layers. Specifically, PairNorm achieves the best performance at 10layers when 25% features are missing, while it has the best performance at 40 layers when 75% features",
  "(b)": ": A Toy Example to Demonstrate the Benefit of Deeper GNN Models: (a) Two groups of nodesin the semi-supervised setting. Stars are labeled, dots are unlabeled, and the diamond is the query node.Euclidean distance between two nodes determines the edge connection. (b) Comparison of node classificationaccuracy between shallow and deeper GNN models using data on the left. The deeper GNNs are realized byDrGNN with the corresponding same backbones. Case 3: Heterophilous Graphs. In this section, we evaluate the performance of DrGNN on a largeheterophilous graph (i.e., arXiv-year) from the benchmark (Lim et al., 2021). In brief, heterophily means theconnected two nodes do not share the same label.",
  "Effectiveness Analysis of DrGNN": "Motivated by the above subsection, we then aim to answer the question: whether our DrGNN can perform ifstacking more layers is inevitable? Therefore, we prepare different deep settings to test if DrGNN can alwaysoutperform baseline methods. In , for a fair comparison, the backbone model for all methods we used in these experiments is GCN,and we set the dimension of the hidden layer to 50 and vary the number of hidden layers from 2 to 16, 32,and 64 for all methods. Additionally, in , we test DrGNN on the large-scale dataset OGB-arXiv,where we fix the feature dimension of the hidden layer as 100, set the total iteration to 3000, and chooseGCN as the backbone model. Due to memory limitations, we set the number of layers to 2, 10, and 20. Theexperiments are repeated 5 times, and we record the mean accuracy as well as the standard deviation.",
  "DrGNN0.7369 0.00140.7386 0.00060.7401 0.0009": "robustly even if the GNN becomes deep, and the performance gain is obvious. For example, at layer 16, inthe Cora dataset, DrGNN can achieve 80.02% accuracy. Additional comparison with RevGCN-Deep andEGNN can be found in Appendix E. To echo with and , we visualize the corresponding number of the nonzero singular valueson those datasets in . In , taking Cora and OGB-arXiv as examples, we observe that (1)PairNorm and ContraNorm begin to suffer from the dimensional collapse issue on both datasets when thenumber of layers is greater than 10; (2) Dropedge, DGN, and GCNII perform well on the small dataset butfail to preserve the full-rank representation on the large dataset; (3) node representations by DrGNN arefull-rank on both datasets, indicating that DrGNN effectively alleviates the dimensional collapse.",
  ": The x-axis is the number of layers and the y-axis is the number of the non-zero singular values ofthe covariance matrix of the node representations by different methods": "Although the above results have proven that our DrGNN can outperform robustly in different deep settings,we also observe that there seems to be a peak in terms of the number of graph neural network layers. Inother words, after a certain depth, the performance gain seems marginal or missing. For example, in theCora dataset, the performance of our DrGNN increases from 77.68% to 80.02% from 2 layers to 16 layers.However, the performances are maintained when stacking to the 32nd layer or the 64th layer. Therefore, wemay ask if there are a number of effective neural layers? and if the number exists, how can wefind it before the training to avoid the heavy hyperparameter tuning process? Luckily, we foundthe answer in the following subsection.",
  "Number of Effective Layers in Deep Graph Neural Networks": "We conduct the hyperparameter analysis of DrGNN, regarding in the weight decaying function of Eq. 4.For example, when = 10, the decaying factor for the 10-th layer is 0.3679 (i.e., e1); but for the 30-thlayer, it is 0.0049 (i.e., e3). This decay limits the effective information aggregation scope of deeper GNNsbecause the later stacked layers will become significantly less important. Based on this controlling propertyof , a natural follow-up question is whether its value depends on the property of input graphs. Basically, itsuggests that initially stacking a few layers is effective, but more deeper layers do not help (or do not helpmuch) boost performance. Interestingly, through our experiments, we find that the optimal is very close to the diameter ofinput graphs (if it is connected) or the largest component (if it does not have many separatecomponents). This observation verifies our conjecture regarding the property of (i.e., it controls thenumber of effective layers or the number of hops during the message passing aggregation schema of GNNs).Hence, the value of can be searched around the diameter of the input graph.",
  ": Hyperparameter Analysis, i.e., vs Node Classification Accuracy on Four Datasets": "In details, to analyze the hyperparameter , we fix the feature dimension of the hidden layer to be 50, thetotal iteration is set to be 3000, the number of layers is set to be 60, the sampling batch size for DrGNN is10, and GCN is chosen as the backbone model. The experiment is repeated five times for each configuration.In each sub-figure of , the x-axis is the value of , and the y-axis is the accuracy of 60-layer GCN inthe above setting. Specifically, we find that the optimal = 20 on the Cora dataset, the optimal = 10 on the CiteSeer dataset,the optimal = 18 on the PubMed dataset, and the optimal = 20 on the Reddit dataset. Then, naturalquestions to ask are (1) what determines the optimal value of in different datasets? (2) can we gather someheuristics to narrow down the hyperparameter search space to efficiently establish effective GNNs? Thus, we provide our discovery. In Eq. 4, we have analyzed that the decaying factor controls the number ofeffective layers in deeper GNNs by introducing the layer-wise dependency. It means that larger slows downthe weight decay and gives considerably large weights to more layers such that they can be effective, andthe information aggregation scope of GNN extends as more multi-hop neighbor features are collected andaggregated. In graph theory, diameter represents the scope of the graph, which is the largest value of theshortest path between any node pairs in the graph. Therefore, the optimal should be restricted by theinput graph, i.e., being close to the input graph diameter.",
  "Diameter of the Graph (or the Largest Component)19281817": "Interestingly, our experiments reflect this observation. Combining the optimal in and the diameterin , for connected graphs PubMed and Reddit, the optimal is very close to the graph diameter. Thisalso happens to Cora (even though Cora is not connected), because the number of components is not large.As for CiteSeer, the optimal is less than the diameter of its largest component. A possible reason is thatCiteSeer has many (i.e., 438) small components, which shrinks the information propagation scope, such thatwe do not need to stack many layers and we do not need to enlarge to the largest diameter (i.e., 28). In",
  "Different Backbones of DrGNN": "Here, we show the performance of our proposed DrGNN cooperating with different backbone models (e.g.,GAT (Velickovic et al., 2018) and GraphSage (Hamilton et al., 2017a)). In , we set the numbers ofthe hidden layers as 60 for all methods and the dimension of the hidden layer as 50. The total number oftraining iterations is 1500.",
  ": Accuracy of Different Backbone Models with 64 Hidden Layers on Four Datasets": "By observation, we find that both GAT and GraphSage tend to have worse performance when the architecturebecomes deeper, and our proposed method DrGNN greatly boosts the performance by 40%-60% on averageover four datasets. Specifically, compared with the vanilla GraphSage, our DrGNN boosts its performance by43% on the CiteSeer dataset and more than 67% on the Reddit dataset.",
  "Discussion and Limitation": "The major assumption of our proposed method is that the close neighbors of a node carry the most importantinformation, and the importance degrades with faraway neighbors. This assumption is mainly based onthe characteristics of homophilous graphs. In the experiments, we show the great performance of DrGNNin several homophilous graphs. Despite the limited assumption on the heterophilous graph, our proposedmethod can still achieve competitive performance compared to the state-of-the-art methods, as shown in.",
  "Conclusion": "In this paper, we focus on building deeper graph neural networks to effectively model graph data andillustrate the oversmoothing cause from the perspective of dimensional collapse. To this end, we first provideinsights regarding why the vanilla residual connection of ResNet is not best suited for many deeper graphneural network solutions, i.e., the shading neighbors effect. Then, we propose a new residual architecture,Weight-decaying Graph Residual Connection (WG-ResNet), to alleviate this effect. In addition, we proposea Structure-guided Contrastive Learning (SCL) to alleviate the problem from another viewpoint, wherewe utilize graph topological information, pull the representations of connected node pairs closer, and pushremote node pairs farther apart via contrastive learning regularization. Combining WG-ResNet with SCL, anend-to-end model DrGNN is proposed for deep graph neural networks. We provide the theoretical analysis ofour proposed method and demonstrate the effectiveness of DrGNN by extensive experiment comparing withstate-of-the-art methods. This work was supported by National Science Foundation under Award No. IIS-2117902, and the U.S.Department of Homeland Security under Grant Award Number, 17STQAC00001-08-01. The views andconclusions are those of the authors and should not be interpreted as representing the official policies of thefunding agencies or the government.",
  "John C. Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning andstochastic optimization. JMLR, 2011": "S. M. Faisal, Georgios Tziantzioulis, Ali Murat Gok, Nikolaos Hardavellas, Seda Ogrenci Memik, andSrinivasan Parthasarathy. Edge importance identification for energy efficient graph processing. In 2015IEEE International Conference on Big Data (IEEE BigData 2015), Santa Clara, CA, USA, October 29 -November 1, 2015, pp. 347354. IEEE Computer Society, 2015. Dongqi Fu, Liri Fang, Ross Maciejewski, Vetle I. Torvik, and Jingrui He. Meta-learned metrics over multi-evolution temporal graphs. In Aidong Zhang and Huzefa Rangwala (eds.), KDD 22: The 28th ACMSIGKDD Conference on Knowledge Discovery and Data Mining, Washington, DC, USA, August 14 - 18,2022, pp. 367377. ACM, 2022. doi: 10.1145/3534678.3539313. URL Dongqi Fu, Zhigang Hua, Yan Xie, Jin Fang, Si Zhang, Kaan Sancak, Hao Wu, Andrey Malevich, JingruiHe, and Bo Long. Vcr-graphormer: A mini-batch graph transformer via virtual connections. CoRR,abs/2403.16030, 2024a. doi: 10.48550/ARXIV.2403.16030. URL Dongqi Fu, Yada Zhu, Hanghang Tong, Kommy Weldemariam, Onkar Bhardwaj, and Jingrui He. Gen-erating fine-grained causality in climate time series data for forecasting and anomaly detection. CoRR,abs/2408.04254, 2024b. doi: 10.48550/ARXIV.2408.04254. URL",
  "Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C.Courville, and Yoshua Bengio. Generative adversarial networks. CoRR, abs/1406.2661, 2014. URL": "Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Balaji Krishnapuram,Mohak Shah, Alexander J. Smola, Charu C. Aggarwal, Dou Shen, and Rajeev Rastogi (eds.), Proceedingsof the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, SanFrancisco, CA, USA, August 13-17, 2016, pp. 855864. ACM, 2016. Xiaojun Guo, Yifei Wang, Tianqi Du, and Yisen Wang. Contranorm: A contrastive learning perspective onoversmoothing and beyond. In The Eleventh International Conference on Learning Representations, ICLR2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL",
  "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. InCVPR, 2016": "Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, andJure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. In Hugo Larochelle,MarcAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in NeuralInformation Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020,NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL",
  "Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In AAAI, 2018": "Derek Lim, Felix Hohne, Xiuyu Li, Sijia Linda Huang, Vaishnavi Gupta, Omkar Bhalerao, and Ser Nam Lim.Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods. Advancesin Neural Information Processing Systems, 34:2088720902, 2021. Qing Lu and Lise Getoor. Link-based classification. In Tom Fawcett and Nina Mishra (eds.), Machine Learning,Proceedings of the Twentieth International Conference (ICML 2003), August 21-24, 2003, Washington,DC, USA, pp. 496503. AAAI Press, 2003. Galileo Namata, Ben London, Lise Getoor, Bert Huang, and U Edu. Query-driven active surveying forcollective classification. In 10th International Workshop on Mining and Learning with Graphs, volume 8,2012.",
  "Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node classification.In ICLR, 2020": "Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: online learning of social representations. InSofus A. Macskassy, Claudia Perlich, Jure Leskovec, Wei Wang, and Rayid Ghani (eds.), The 20th ACMSIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 14, New York, NY,USA - August 24 - 27, 2014, pp. 701710. ACM, 2014.",
  "Kuansan Wang, Zhihong Shen, Chiyuan Huang, Chieh-Han Wu, Yuxiao Dong, and Anshul Kanakia. Microsoftacademic graph: When experts are not enough. Quantitative Science Studies, 1(1):396413, 2020": "Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment anduniformity on the hypersphere. In Proceedings of the 37th International Conference on Machine Learning,ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp.99299939. PMLR, 2020. Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka.Representation learning on graphs with jumping knowledge networks. In Jennifer G. Dy and AndreasKrause (eds.), Proceedings of the 35th International Conference on Machine Learning, ICML 2018,Stockholmsmssan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine LearningResearch, pp. 54495458. PMLR, 2018.",
  "ihi": "where hi is the node representation for node i and covariance(h) = 0 indicates that the learned representationis indistinguishable and the deep model suffers from an oversmoothing issue. Notice that the dimensionalcollapse is observed when the covariance matrix of the node representations is not full-rank (i.e., the numberof non-zero singular values is less than the dimension of the node representation in ).Whencovariance(h) = 0, it also indicates that hi = hj = h for all i and j, and the rank of the covariance matrix ofthe node representation matrix is 0 (While a large value of covariance(h) does not mean that the performanceis ). In other words, the graph model suffers from complete collapse, where all node representations shrink toa single point. Thus, we could see that the oversmoothing issue is highly related to dimensional collapse.",
  ": Weight Visualization. The y-axis represents the weight of each layer, and the x-axis represents theindex of each layer, in deeper models": "number of layers is set to be 60; the sampling batch size for DrGNN is 10; GCN is chosen as the backbonemodel; is set to be 20. In , The x-axis is the index of each layer, and the y-axis is the weight foreach layer. DrGNN-S removes the similarity measurement ecos(H(1), H(l)) in Eq. 5 and DrGNN-D removes thedecaying weight factor and only keeps the exponential cosine similarity ecos(H(1), H(l)) to measure the weightfor each layer. DrGNN-S achieves the simplified WG-ResNet in DrGNN, which removes the exponentialcosine similarity ecos(H(1), H(l)) in DrGNN. By observation, we find that (1) ResNet sets the weight of eachlayer to be 1, which easily leads to shading neighbors effect when stacking more layers, because the farawayneighbor information becomes more dominant in the GCN information aggregation; (2) without weightdecaying factor, the weight for each layer in DrGNN-D fluctuates because they are randomly independent.More specially, the weights for the last several layers (e.g., L=58 or L=60) are larger than the weights for thefirst several layers, which contradicts the intuition that the first several layers should be more important thanthe last several layers; (3) the weights for each layer in both DrGNN and DrGNN-S reduce as the number oflayers increase, which suggests that both of them could address the shading neighbors effect to some extents;(4) combining the results from , DrGNN achieves better performance than DrGNN-S, as it imposeslarger weights to the first several layers, which verifies that the learnable similarity sim(H(1), H(l)) achievesbetter performance with the enlarged hypothesis space for neural networks.",
  "Therefore, towards D(E), D(E) is maximizing the conditional log-likelihood P(y = 1|E), where y = 1indicates edge E is a positive (i.e., exiting) edge": "In other words, the discriminator D(E) is defined on a node pair in terms of their representation vectors,i.e., D(E) is able to distinguish whether a node pair is positive or not, then the corresponding hiddenrepresentations of different nodes are different. For example, for a positive (i.e., existing) edge (vi, vj) and anegative (i.e., non-existing) edge (vi, vk), f(zi, zj) is maximized and f(zi, zk) is minimized, then zj and zkare different. Back to Proposition 2.1, node vectors zj and zk are obtained through . Thats why optimizingloss LSCL can alleviate the oversmoothing problem, according to the definition of oversmoothing (Rusch et al.,2023), the final layer node representation vectors are similar given a certain threshold and a certain similarityfunction, and the common functions can be Dirichlet energy and mean-average distance.",
  "DReproducibility": "For a fair comparison, we set the dropout rate to 0.5, the weight decay rate to 0.0005, and the total numberof iterations to 1500 for all baseline methods in and ; if not specialized, GCN is chosen as thebackbone, and the dimension of each layer is set to 50 for all the graph neural network baseline methods.",
  "EAdditional Effectiveness Analysis": "We conduct the additional experiments by comparing our proposed method with RevGCN-Deep (Li et al.,2021) and EGNN (Zhou et al., 2021). We set the number of layers for all baseline methods to 60 for Cora,Citeseer, PubMed, and Reddit. For the OGB-arXiv dataset, we set the number of layers to 10 for all methods.",
  "FAblation Study of DrGNN": "Here, we conduct the ablation study to show the effectiveness and irreplaceability of WG-ResNet and SCL interms of node classification in . In this experiment, we fix the total iteration set as 3000, and GCN ischosen as the backbone model. For the Cora dataset, the feature dimension of the hidden layer is 50 and thenumber of layers is 64; for the OGB-arXiv dataset the feature dimension of the hidden layer is 100 and thenumber of layers is 20. In , DrGNN-T removes SCL, DrGNN-D removes the weight decaying factorin WG-ResNet and DrGNN-JK replaces the WG-ResNet by Jumping Knowledge (Xu et al., 2018).",
  "DrGNN0.8022 0.00610.6685 0.00660.8109 0.00330.9721 0.00110.7401 0.0009": "In , we have the following observations (1) comparing DrGNN with DrGNN-T, we find that DrGNNboosts the performance by 1.84% on the Cora dataset after adding SCL, which demonstrates the effectivenessof SCL to alleviate the oversmoothing issue; (2) DrGNN outperforms DrGNN-D on Cora dataset by 5.61%,which shows that DrGNN could alleviate the shading neighbors effect by adding the weight decaying factor;(3) comparing DrGNN with DrGNN-JK, we verify that our proposed WG-ResNet is more effective thanDrGNN-JK. Besides, one drawback of jumping knowledge is its high memory required as the number oflayers increases, while our proposed WG-ResNet doesnt; (4) DrGNN outperforms GCN (+RC) by morethan 7.7% on the Cora dataset and 2.6% on the OGB-arXiv dataset, which indicates that WG-ResNet couldalleviate the shading neighbors effect. Also, we conduct an additional ablation study to evaluate the performance of each component (i.e., the cosinesimilarity function in Eq. 4 and SCL formulated in Eq. 5). In , DrGNN-S denotes the variant ofDrGNN after removing the cosine similarity function in Eq. 4 and SCL denotes the variant of DrGNN byremoving the WG-ResNet. We have the following observations: (1). DrGNN outperforms DrGNN-S on mostdatasets except PubMed, which suggests that introducing the layer similarity (i.e., cosine similarity between",
  ": Hyperparameter Analysis, i.e., vs Node Classification Accuracy": "To analyze the hyperparameter in DrGNN, we fix the feature dimension of the hidden layer to be 50, thetotal iteration is set to be 3000, the number of layers is set to be 60, the sampling batch size for DrGNNis 10, GCN is chosen as the backbone model, and the dataset is Cora. We gradually increase the value of and record the accuracy. The experiment is repeated five times in each setting. In , the x-axisis and the y-axis is the accuracy score. By observation, when = 1, the performance is worst and theperformance begins to increase by decreasing the value of . It achieves the best accuracy when = 0.03.The performance starts to decrease again if we further decrease the value of . We conjecture that when is large, it will dominate the overall objective function, thus jeopardizing the classification performance.Besides, the performance also decreases if we set the value of to be a small number (i.e., = 0.001). Inaddition, comparing with the performance without using SCL regularization (i.e., = 0), our proposedmethod with = 0.03 can boost the performance by more than 1.8%, which demonstrates that our proposedSCL alleviates the issue of oversmoothing to some extent.",
  "HEfficiency Analysis": "Here, we conduct an efficiency analysis regarding our proposed method in the Cora dataset. We fix thefeature dimension of the hidden layer to be 50, the total iteration is set to be 1500, the sampling batch size forDrGNN and DrGNN-S is 10, and GCN is chosen as the backbone model. We gradually increase the numberof layers and record the running time. In , the x-axis is the number of layers and the y-axis is the running time in seconds. We observe thatthe running time of both DrGNN and DrGNN-S is linearly proportional to the number of layers. Comparingthe running time of DrGNN, the running time of DrGNN-S is further reduced after the weighting functionin DrGNN (e.g., sim()) is replaced by a constant.",
  "ISampling Method for SCL": "To realize SCL expressed in Eq. 5, we need to get the positive nodes vj and negative nodes vk towards theselected central node vi. To avoid iterating over all existing nodes or randomly sampling several nodes, wepropose to sample positive nodes vj and negative nodes vk from the star subgraph Si of the central node vi.Moreover, to make the sampling scalable and to reduce the search space of negative nodes, we propose abatch sampling method.",
  ": Batch Sampling. Each star node in the figure corresponds to node vi in Eq. 5": "As shown in , the batch size is controlled by the number of central nodes (i.e., star nodes in thefigure). For each central node, the positive nodes are those 1-hop neighbors, and the negative nodes consistof unreachable nodes. In our batch sampling, we strictly constrain that the positive nodes are only fromthe 1-hop neighborhood for the following three reasons: (1) they are efficient to be accessed; (2) consideringall k-hop neighbors as positive will enlarge the scope of positive nodes and further decrease the intimacyof the directly connected nodes; (3) 1-hop positive nodes in the star subgraph can preserve enough usefulinformation, compared to the positive nodes from the whole graph. For the third point, we prove it throughthe graph influence loss (Huang & Zitnik, 2020) in Proposition I.1, and the formal definition of graph influenceloss is given in the following paragraph after Proposition I.1. Proposition I.1 (Bounded Graph Influence Loss for Sampling Positive Pairs Locally). Taking GCN asan example of GNN, the graph influence loss R(vc) on node vc w.r.t positive nodes from the wholegraph against positive nodes from the 1-hop neighborhood star subgraph is bounded by R(vc) (n dc)",
  "(D PGM)| P|": "where is a constant, D PGM is the geometric mean of the degree of nodes sitting in path P, and P is the pathfrom the positive node vs to the center node vc that could generate the maximal multiplication of normalizededge weight, | P| denotes the number of nodes in path P.",
  "(DPiGM)| Pi|": "Specifically, the graph influence loss (Huang & Zitnik, 2020) R(vc) can be expressed as R(vc) = IG(vc)IL(vc),which is determined by the global graph influence on vc (i.e., IG(vc)) and the star subgraph influence on vc(i.e., IL(vc)). Then, to compute the graph influence IG(vc), we need to compute the node influence of eachnode vj to node vc, where node vj is reachable from node vc. Based on the final output node representationvectors, the node influence is expressed as Ivc,vj = h()c/h()j, and the norm can be any subordinatenorm (Wang & Leskovec, 2020). Then, IG(vc) is computed by the L1-norm of the following vector, i.e.,IG(vc) = [Ivc,v1, Ivc,v2, . . . , Ivc,vn]1. Similarly, we can compute the star subgraph influence IL(vc) on nodevc. The only difference is that we collect each reachable node vj in the star subgraph L (i.e., 1-hop neighborsof vc). Overall, in Proposition I.1, we show why positive pairs can be locally sampled with the support fromgraph influence loss of a node representation vector output by the GCN final layer."
}