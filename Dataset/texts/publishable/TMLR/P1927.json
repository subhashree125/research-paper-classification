{
  "Abstract": "In terms of accuracy, Graph Neural Networks (GNNs) are the best architectural choice forthe node classification task. Their drawback in real-world deployment is the latency thatemerges from the neighbourhood processing operation. One solution to the latency issue isto perform knowledge distillation from a trained GNN to a Multi-Layer Perceptron (MLP),where the MLP processes only the features of the node being classified (and possibly somepre-computed structural information). However, the performance of such MLPs in bothtransductive and inductive settings remains inconsistent for existing knowledge distillationtechniques. We propose to address the performance concerns by using a specially-designedstudent model instead of an MLP. Our model, named Routing-by-Memory (RbM), is aform of Mixture-of-Experts (MoE), with a design that enforces expert specialization. Byencouraging each expert to specialize on a certain region on the hidden representation space,we demonstrate experimentally that it is possible to derive considerably more consistentperformance across multiple datasets.Code available at",
  "Introduction": "Graphs can be used to encode the dependencies between data samples. The impressive performance of GraphNeural Networks (GNNs) shows that taking into account the structural information increases the quality ofprediction on tasks like product prediction on co-purchasing graphs or paper category prediction on citationgraphs (Kipf & Welling, 2016; Hamilton et al., 2017). However, despite the potential accuracy improvementsof GNNs, multi-layer perceptrons (MLPs) remain preferable to graph neural networks for many large-scaleindustrial applications. This is due to the fundamental inefficiency of GNNs, with scalability limitationsmaking deployment challenging (Zhang et al., 2020a; Jia et al., 2020; Zheng et al., 2022). GNNs operatein layers, and each layer requires the processing of a neighbourhood of the node in order to compute theprediction. For example, evaluating the prediction for a single node with an L-layer GNN requires processingat least every node in the L-hop neighbourhood. For real-world graphs, involving millions of nodes, the L-hopneighbourhood can be very large, leading to resource intensive operations (Jin et al., 2021). Even if we onlysample a subset of the neighbours at each layer, the receptive field for the node can grow very rapidly. Thiscan lead to a latency performance bottleneck in high-load systems. This is especially the case in situationswhere the graph is distributed over multiple servers. Fetching the entire L-hop neighbourhood can resultin greatly increased latency (Zheng et al., 2022). By contrast, forming a prediction for a single node withan L-layer MLP requires processing only the features of that node. This naturally eliminates the latencyproblems associated with additional node fetching. Therefore it can be scaled and deployed efficiently, andparallelization is straightforward.",
  "Published in Transactions on Machine Learning Research (10/2024)": "Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: A next-generation hyperparameter optimization framework.In Proc. Int. Conf. Knowl. Discovery and DataMining, 2019. Zewen Chi, Li Dong, Shaohan Huang, Damai Dai, Shuming Ma, Barun Patra, Saksham Singhal, PayalBajaj, Xia Song, Xian-Ling Mao, Heyan Huang, and Furu Wei. On the Representation Collapse of SparseMixture of Experts. In Adv. Neural Inf. Process. Syst., 2022.",
  "Ours is the first work to propose the use of a student Mixture-of-Experts (MoE) model for the distillationof a GNN": "We propose a Routing-by-Memory (RbM) model that differs from, and outperforms, a standard sparseMoE. We introduce important adaptations to a routing system previously proposed by Zhang et al.(2021a) for routing to a single expert in a computer vision setting. We allow for routing to multipleexperts and use a different distance.",
  "GNN-to-MLP Knowledge Distillation": "Knowledge distillation from a Graph Neural Network (GNN) into a Multi-Layer Perceptron (MLP) promotesinference efficiency by avoiding the aggregation over neighbourhood nodes. Yang et al. (2021) presented oneof the first distillation attempts, employing a student model that combines label propagation with an MLPacting on the features. Although label propagation is a lightweight form of aggregation, it is still reliant onthe graph, so the overall speed-up in inference time is not dramatic. The GLNN by Zhang et al. (2021b) introduces knowledge distillation to an MLP without any aggregationover the graph nodes. The student is trained with node content features as input and soft labels, generated bya pretrained GNN, which acts as the teacher. Tian et al. (2022) show that soft labels alone are not enough toachieve consistent performance due to noise injected by the teacher GNN. The presented NOSMOG approachincorporates a set of techniques to be used on top of knowledge distillation to help the student MLP to betterapproximate the graph-based information. It includes explicitly encoded position features generated usingDeepWalk (Perozzi et al., 2014). NOSMOG also introduces a representational similarity distillation, whichstrives to encourage the student MLP to preserve the similarities between node representations that areobserved in the teacher GNN. Adversarial attack perturbations are also applied in order to ensure that thestudent model is more stable. While all the introduced techniques provide some improvement, the positionalencoding is responsible for the vast majority of the performance gain.",
  "Mixture-of-Experts": "A Sparse Mixture-of-Experts (MoE) model is a weighted combination of similarly structured models withdynamically computed weights (Shazeer et al., 2016; Gross et al., 2017; Zhang et al., 2021a; Li et al., 2022;Dryden & Hoefler, 2022; Chi et al., 2022; Komatsuzaki et al., 2022; Pavlitska et al., 2023). For any sample,only a small portion of the experts have non-zero weights. This allows us to increase the model learningcapacity without significantly inflating the processing costs (Fedus et al., 2022). An MoE can also be used asa layer inside a larger model (Qu et al., 2022; Yan & Li, 2023). In most implementations, the computation ofthe expert weights, referred to as routing, is perforrmed by a separate neural network (a policy network) ofa size comparable to an expert. Based on the MoE input, it produces weights for the experts. Because boththe policy network and expert networks are trained simultaneously, there is a danger of under-utilization,and consequently under-training, of some experts (Krishnamurthy et al., 2023). This is commonly mitigatedby injecting noise and designing loss functions that even out the utilization. An alternative routing scheme, designed to prevent routing inconsistencies, involves pairing each expertnetwork with an embedding vector. Instead of predicting the expert weights directly, the policy networkthen aims to project the input sample into the embedding space. The weights are derived from the distancesto the expert embeddings (Gross et al., 2017; Zhang et al., 2021a; Chi et al., 2022; Li et al., 2022; Yan & Li,2023; Qu et al., 2022). Initially, the Euclidean distance was used to route to the single closest expert (Grosset al., 2017; Zhang et al., 2021a). However, this causes computational stability issues when multiple expertsare used (Qu et al., 2022). Using dot-product similarity (Lample et al., 2019; Fedus et al., 2022) insteadof Euclidean distance typically leads to representation collapse. Chi et al. (2022) used cosine similarity toavoid the collapse, and this is currently used in many implementations (Li et al., 2022; Yan & Li, 2023).Zhang et al. (2021a) propose moving the expert embeddings into the layers input space in order to encourageexpert specialization. In their scheme, the input vector is always routed to a single expert, the one withthe closest embedding vector. Each expert embedding is updated by calculating a moving average over theinput embedding vectors that are routed to that expert. This leads to each expert specialising on the areaof the input space around its embedding. Knowledge distillation into a Mixture-of-Experts has not been intensively studied. In tangentially relatedwork, Zuo et al. (2022) study the distillation of language models and incorporate the MoE structure intopre-trained models for fine-tuning. Komatsuzaki et al. (2022) explore the task of upgrading a pre-traineddense model into a larger, sparsely-activated MoE.",
  "Background": "We denote a graph by G = (V, E, X), where V is a set of N nodes, E is a set of edges between nodes,and X RNd represents a matrix with each row being a vector of d node features associated with thecorresponding node. For the node classification task, with C classes, we use a label matrix, Y {0, 1}NC,with each row containing a one-hot encoded class label. The superscript L denotes the labelled nodes of thegraph and the superscript U denotes unlabelled nodes, i.e., VL, XL, YL are, respectively, the labeled nodes,their node features, and the one-hot class labels.",
  "(a)(b)": ": (a) An overview of the overall training framework. A teacher GNN is trained on the graph andprovides targets for Knowledge Distillation (KD) (9) and Knowledge-Aware Reliable Distillation (KRD) (12)losses. A Mixture-of-Experts student is trained on the node features and positional encoding (see .2).(b) We use three additional losses to adjust the internal representations of the model, as the embeddingswe use for routing (see .1). We provide schematic representations of these losses to aid intuitiveunderstanding. Commitment loss (6) pulls representations closer to embeddings (highlighted in blue). Self-similarity loss (7) prevents collapse of representations.Load balance loss (8) helps to move borderlinerepresentations towards embedding of the less populated experts.",
  "Methodology": "We now introduce our distillation approach, which uses a Mixture-of-Experts (MoE) model. The methodstarts with the training of a teacher GNN. The teacher model is used to produce soft-labels for the knowledgedistillation (see .2). The knowledge distillation setup uses a combination of reliable sampling andpositional encoding. Our student model is a Routing-by-Memory model, with a special routing procedurethat enforces expert specialisation (see .1). .3 describes the expert initialization procedure. provides an illustration of the overall training framework. A teacher GNN is trained on the graphand provides soft targets for Knowledge Distillation (KD) (9) and Knowledge-Aware Reliable Distillation(KRD) (12) losses. The student model is trained with the node features and positional encodings as inputs.We introduce spatial routing by memory, so at each layer, each expert is represented by an embedding inthe same space as the input representations for that layer. Representations are then routed to the closestexperts. Three types of loss terms, discussed in more detail below, are used to encourage effective learningof the expert embeddings and the hidden representations. (b) depicts the goals of these losses. Acommitment loss pulls representations closer to embeddings, encouraging specialization of experts. A self-similarity loss pushes the expert embeddings apart and prevents the collapse of the representations. A loadbalance loss strives to achieve more equal utilization of experts by moving representations that are almostequidistant to two or more experts closer to the less-utilized experts.",
  "i=1G(hl1)ifi(hl1),(1)": "where G : Rd E is a policy network that produces routing coefficients, fi : Rd Rd is an expert,E is the number of experts, and hl1 Rd is the input hidden representation of the datapoint, x Rd,emerging from the (l1)-th layer. For the first layer, the hidden representation is the input, i.e., h0 = x. Li et al. (2022) introduce a routing scheme that uses a set of embeddings, QMoE REde, with eachembedding being associated with a particular expert. The weight assigned to each expert for a given input",
  ".(2)": "Here W Rded is a trainable projection matrix, and the operation Topk() is a one-hot embedding thatsets all elements in the output vector to zero except for the elements with the largest k values. Chi et al.(2022) demonstrate that using a policy network of this form results in a more even distribution of tokenprojections over the projection space, leading to improved performance. While Li et al. (2022) provide analysis demonstrating that GMoE() provides a degree of specialization forexpert networks, we found it insufficient (see the experimental results in .5). We therefore enforceexperts local specialization by setting the expert embeddings QRbM REd to vectors in the input space,rather than projecting to a separate space. We achieve this by setting:",
  ",(3)": "where sg[] is a stop gradient function. In our approach, each expert embedding vector is positioned at thecenter of an area in the representation space that the expert is specialising on. We use cosine similarity andinterpolation between multiple experts (see ). As every embedding vector in our approach can be interpreted as a centre of a cluster, we do not updateit with gradient descent, but instead use a direct approach, calculating a moving average over the inputbatches. We evaluate:",
  "i=1GRbM(h)ifi(exp(atti) h),(5)": "where s R is a learnable output scaler, atti Rd is a learnable input attention vector, and denoteselement-wise multiplication. Note that the attention is applied only to the expert input and not to therouter input. This routing approach is inspired by the technique presented by Zhang et al. (2021a); weextend it to perform top-k rather than top-1 routing and employ cosine similarity instead of Euclideandistance.",
  "Previous training examples": ": A simplified example of cosine routing (3).Three experts are present in total (E = 3).Twoexperts are used at a time (k = 2), and thus the twoexperts with closest embeddings are used. Arrowsshow expert embeddings on the unit circle. Pointsare representations of the previously routed trainingexamples (see equation 4).",
  "2021b). While the supervised part of the loss uses only the labeled part of the graph, the KL-divergence iscomputed over the entire set of nodes": "We use positional encodings generated by DeepWalk (Perozzi et al., 2014) in our distillation procedure.Before running the student training we learn positional encodings by running the DeepWalk algorithm onthe input graph. The positional encodings are based solely on the graph structure, and are concatenatedwith the node features when nodes are processed by the student model. In order to additionally leverage the graph structure, we employ the knowledge-based sampling technique ofKRD (Wu et al., 2023). KRD is based on the principle of aligning the representation of a target node withthose of reliable nodes in its neighbourhood. To accomplish this, Wu et al. (2023) introduce a measure of re-liability, which is based on identifying nodes whose teacher GNN representations do not change substantiallywhen the features in the graph are subjected to perturbation. The reliability metric for node j is:",
  "E XN(X,I)H(yj) H(yj)2 ,(10)": "where N(, I) is Gaussian noise with diagonal covariance matrix, with as the diagonal elements. H()denotes the entropy, and yj and yj are the predicted class distributions for node j with and withoutperturbation, respectively. This metric is based on the principle that more reliable teacher soft labels aremore robust to feature perturbations.",
  ",(11)": "where max = arg maxiV i, and is a learnable power parameter. To learn , we follow the procedureof (Wu et al., 2023). During training of the overall model, we strive to match the class distribution predictedby the student yv with the teacher-provided soft labels of the sampled nodes. We therefore include a lossterm:LossKRD = 1",
  "vVEuN(v)up(u|u,)KL(yv, yu),(12)": "where, for each node v, the expectation is with respect to a distribution p(u|u, ), which is proportionalto p(u|u, ) defined in equation 11, for the nodes u in the neighborhood of v, and zero elsewhere. Thehyperparameter is the same as in equation 9. Our model consist of L MoE layers (see ). The training objective is a combination of the cross-entropy loss, the distillation loss, the KRD loss, and embedding losses for every MoE layer of the model.With a model with L MoE layers, we introduce weights , , and to determine the influences of theassociated embedding losses:",
  "MoE initialization": "In most implementations, the embeddings of the experts are initialised randomly (Chi et al., 2022; Li et al.,2022; Yan & Li, 2023). In our approach, we desire more informative initialisation, because embeddings areoperating in the input space. We therefore apply a pretraining stage, following Zhang et al. (2021a). Wepretrain the model for several epochs, routing all input vectors to the first expert. Subsequently, we clonethe parameters of the first expert to all other experts. We reset the optimiser state after pretraining. Wecollect the inputs for each MoE layer and apply L2 normalization. We then apply K-means clustering, withthe number of clusters equal to the number of experts. We initialise the embeddings with the cluster centers. Operating directly in the space of the hidden representations not only removes the superfluous parametersof the projection matrix from the model, but it also makes initialization of the experts much easier. Thishelps to avoid representation collapse that can be caused by randomly initialized projection matrix.",
  "Experimental setting": "Datasets.To conduct our experiments we use nine real-world datasets: Cora (Sen et al., 2008), Cite-seer (Giles et al., 1998), Pubmed (McCallum et al., 2000), Amazon-Photo, Amazon-Computers, Academic-CS, Academic-Physics (Shchur et al., 2018), OGB-ArXive and OGB-Products (Hu et al., 2020). For theCora, Citeseer, and Pubmed datasets, we follow the data splitting strategy specified by Kipf & Welling(2016). For the Amazon-Photo, Amazon-Computers, Academic-CS, Academic-Physics, we follow the proce-dure employed by Zhang et al. (2021b), Tian et al. (2022) and Wu et al. (2023). We randomly split the datainto train/val/test subsets. Each random seed corresponds to a different data split. For the OGB-ArXive andOGB-Products we use the public data splits provided by Hu et al. (2020). Dataset statistics are provided in. For the Amazon-Photo, Amazon-Computers, Academic-CS, Academic-Physics, OGB-ArXive andOGB-Products datasets, we use batched updates due to the large number of nodes and edges. When presenting and discussing results, we divide the datasets into large, medium and small categories,according to the number of training nodes available. Our method focuses on large and medium-sized datasets.In general, more complicated architectures and distillation procedures struggle to achieve performance gainson small datasets, as demonstrated by Zhang et al. (2021b). Baselines. We compare to three node classification baselines that use GNN-to-MLP knowledge distillation:NOSMOG (Tian et al., 2022), KRD (Wu et al., 2023) and GLNN (Zhang et al., 2021b). All baselines arereproduced using provided official code and hyperparameters.1 By default, we use GraphSAGE (Hamiltonet al., 2017) as the teacher model, in order to facilitate comparison with previous work. We do, however,examine how the method performs with other teacher models. We also compare to CoHOp (Winter et al.,2024), a baseline method that does not employ a teacher, but has a higher inference cost.In order tocompare results with a similar parameter count, we provide four parameter-inflated baselines: NOSMOG+,KRD+, GLNN+ (Zhang et al., 2021b) and CoHOp+. For these baselines, we increase the number of studentparameters to be 8 times the teacher size, following Zhang et al. (2021b).2 We conduct 10 runs for our methodand each baseline, with the same sequence of seeds for all methods. Model. Our Routing-by-Memory models have the same number of layers as the teacher. Every expert is alinear layer with the same size as the corresponding layer of the teacher. We use up to 8 experts for RbM(the exact number for each dataset is selected using the validation set). We use the same number of expertsfor all the RbM layers inside the model. Three experts are active at a time. RbM routing is sensitive todropout, so we avoid the application of dropout directly before the RbM layers. In our model, we applydropout before the input of the expert for each layer except the first one. We use a linear annealing schedulefor the updating of the expert embeddings (see Appendix G). Evaluation protocol. We report the mean and standard deviation of accuracy for ten separate runs withdifferent random seeds. We use the same sequences of seeds reported as Tian et al. (2022) and Wu et al.(2023). We use validation data to select the optimal model. The hyperparameter selection procedure isdescribed in Appendix C. We measure model performance using test data. We conduct our experiments in two settings: transductive (trans) and inductive (ind). For the transductivesetting we train the model on the full sets of nodes, V, and edges, E. Classification loss is only computedover XL and Y L, but soft labels for KL-divergence and KRD losses are computed on the full sets of Xand E. In the transductive setting we evaluate the model over XU and Y U. For the inductive setting, wesplit the unlabeled nodes, VU, into a set of observed nodes, VUobs, and a set of inductive nodes, VUind, byrandomly selecting 20% of the nodes as the inductive subset, following the procedure of Tian et al. (2022) 1We were not able to reproduce reported NOSMOG results on the OGB-Products dataset for the inductive setting usingthe official code; for this dataset and setting we provide results using our implementation of NOSMOG.2We are not able to provide results for KRD+ on OGB-ArXive as it requires more than 32 GB of VRAM to run.",
  "Performance comparison": "We compare our method to GLNN, KRD, NOSMOG and CoHOp baselines. Results are presented in for GraphSAGE as the teacher, and in for more advanced teacher GNNs. We use RevGNN-Wide (Liet al., 2021) and DRGAT (Zhang et al., 2023) as the advanced teachers, because they are among the bestperforming GNN models for the larger OGB datasets.",
  "We make the following observations:": "1. shows that RbM consistently ranks first or second for the medium and large datasets. It can besuccessfully applied to small datasets but without meaningful performance gains. RbM outperforms allthe baselines on medium sized datasets. It is outperformed only by CoHOp on the large datasets. Wediscuss this in .4, but for now, we note that CoHOp employs computationally burdensome labelpropagation. 2. KRD, NOSMOG, and RbM often outperform the teacher model. This has been observed previously (Wuet al., 2023; Tian et al., 2022). Distillation can lead to better generalization and renders the predictionarchitecture less susceptible to spurious edges. In addition, the students form predictions using both nodefeatures and structural information (via positional encoding or DeepWalk), whereas the teacher focusesprimarily on the features (the impact of graph structure is much less direct, arising from message passing). 3. Although GLNN performs reasonably well for small and medium datasets, with accuracy close to that ofthe teacher, it struggles with the large OGB datasets. The node feature information is highly informativefor the small/medium datasets. In contrast, for the large datasets, with sparser labelling, the accessto graph information is important. This is achieved by neighbourhood aggregation for the teacher, andpositional encoding for the students. 4. indicates that better teachers lead to improved performance of the distilled models (except forGLNN). The proposed method demonstrates similar outperformance with respect to the baselines. Thedistilled models do not outperform the more advanced teachers that can more effectively incorporategraph information. In order to demonstrate that our approach leverages additional parameters better than the baselines, weconduct experiments with parameter-inflated baselines (see ). These expanded baselines are denotedGLNN+, KRD+, NOSMOG+ and CoHOp+. In this experiment every baseline has the number of parametersincreased 8 times the teacher size, as in Zhang et al. (2021b) and Tian et al. (2022). The size of every hiddenlayer is increased, but other parameters are not changed. CoHOp does not use a teacher; we increase thenumber of parameters by a factor of 8 compared to the model in the original paper. indicates thata larger number of parameters can improve performance for some cases, but we do not observe a consistentperformance improvement for any baseline. Even when there is performance improvement, RbM remains thebest-performing algorithm in 9 out of the 12 settings for medium and large datasets. We apply the Skillings-",
  "tran0.14200.00000.94700.48940.66960.9980": "Mack test to assess whether the RbM outperformance is statistically significant. For all experimental settings,p-values are smaller than 0.02, and in most cases are smaller than 0.005, indicating statistical significance. Since we perform pretraining and clustering in our model initialization, there is some additional computa-tional overhead compared to training parameter-inflated baselines. In our experiments, pretraining does notexceed 7% of the total number of training epochs on average (in our implementation we use early stop, sothe number of epochs can vary). The clustering overhead is effectively insignificant. On our setup, it takes80 seconds to cluster all required embeddings for RbM on OGB-ArXive, one of the larger datasets. : Accuracy for advanced teacher models. Performance with the GraphSAGE teacher is provided forreference. OGB-ArXive dataset is used in transductive setting for the experiments. Scores are statisticallysignificant under the Skillings-Mack test with significance level of 5% (p = 0.0193)",
  "Comparing with ensemble and vanilla MoE": "To additionally explore whether our approach is an efficient mechanism for exploiting additional parameters,we construct two baselines: a soft-voting ensemble of MLPs and a vanilla MoE. The soft-voting ensembleconsists of several MLP students with the same structure, but different random initializations. The three-MLP ensemble has the same inference cost as active experts in the Mixture of Experts, and the eight-MLPensemble has the same parameter count as all experts. Appendix F provides extensive analysis on parameternumber and inference complexity. The vanilla Mixture of Experts model is structured in the same way as theproposed RbM model, but it uses a routing scheme where the policy network and embeddings are initializedrandomly and updated with backpropagation (Chi et al., 2022; Li et al., 2022; Yan & Li, 2023). For all",
  "tran0.07750.18340.99000.17500.20161.0000": "models, each sample is routed to three experts. We pretrain the vanilla Mixture of Experts model for severalepochs, routing all inputs to the first expert. The weights of the pretrained first expert are then cloned tothe other experts. Embeddings are not updated during the pretrain stage. shows that RbM outperforms the soft-voting ensemble and vanilla MoE baselines in 10 out of 12of the settings for the medium and large datasets. The vanilla MoE outperforms the ensemble of MLPs formost settings (8 out of 12) on the medium and large datasets. The vanilla MoE has fewer hyperparametersthan RbM, and thus it may be preferable if there is a need to reduce the tuning overhead.",
  "Ablation study, label propagation, and number of experts": "Loss terms.We now examine whether each component of the equation 13 is important for achievingbetter performance. Our model includes three additional loss terms for each RbM layer (see equation 13):commitment loss (6), self-similarity loss (7), and load balance loss (8). In order to conduct the ablationstudy we remove each component individually. None of the other hyperparameters is changed. The resultsare presented in , and show that removing any of the loss components reduces performance. Theperformance deterioration is relatively small for each dataset, but it is observed for every setting and everyloss. It is not clear that any of the three loss terms is the most important. Any combination that includestwo loss terms outperforms (by a small margin) the baseline with all three removed for all medium and largedatasets, indicating that all three loss terms contribute. Label propagation. By default, we use DeepWalk positional encoding as an additional set of features.RbM is fully compatible with additional positional information that can be extracted from the graph. As anexample, demonstrates that including the label propagation information from CoHOp can increasethe performance of RbM on the datasets where a considerable portion of the nodes are labeled (OGB-ArXive,OGB-Products). Generating the label propagation information requires propagating and averaging one-hotencoded training labels from a 10-hop neighbourhood around each nodes. In an inductive setting, this must",
  "be conducted for each new node, and it can become a time bottleneck due to the overhead of fetching nodelabels from memory": "Number of experts. During our experiments we use the same number of experts for all RbM layers inorder to reduce the number of hyperparameters. We found that there is an optimal number of experts forRbM for each dataset that can be identified using validation data (from the range [3, . . . , 8]). Appendix Dprovides results depicting an example of how performance varies as the total number of experts is changed.",
  ": Analysis of hidden representation for RbM (b) and its projection for MoE (a). Each point representan instance from the Academic-Physics dataset in transductive setting": "In order to analyse the routing spatial structure qualitatively, we utilise the T-SNE (Van der Maaten &Hinton, 2008), with perplexity of 30 and PCA initialization, to produce a 2-d visualizations of a routerembedding space for RbM and a vanilla MoE in . These correspond to the hidden representationh for RbM (see equation 3) and the linear projection of the hidden representation, Wh, for the MoE (seeequation 2). We trained both MoE and RbM models on the Academic-Physics dataset in the transductivesetting with the same teacher and selected the router of the last layer to produce the representation. In, the hidden representations are colored according to the labels (on the left) and according to thetop-score expert (on the right). shows that MoE experts mix and distribute datapoints of the sameclasses between different experts, while RbM experts have a clear specialization.",
  "Conclusion and Future work": "In this paper we focused on the task of distillation from a graph neural network and introduced RbM, aMixture of Experts model that encourages strong expert specialization at the routing level. We establishedhow parameter inflation can positively affect the performance and showed practical application of MoE in theknowledge distillation domain. Our approach outperforms existing baselines on most medium-size or largedatasets. The key innovations of our approach is in embeddings to be the part of the hidden representationspace. We used additional losses hidden space is forced into the shape of multiple elliptical clusters, which couldbe too restrictive and thus suboptimal. In order to reduce number of hyperparameres we assumed all layerof RbM to have the same number of experts and thus clustered into the same number of clusters. Selectinga suitable number of experts for the layer automatically can improve the performance of the overall model.Both off these assumptions led to the model being more sensitive to the number of experts than the plainMoE. We leave those direction of improvement as a future work. In addition to that a a future directioncan be about the application of the MoE in graph domain. This work uses a concatenation of positional andfeature vector for both routing and selected experts processing, however an alternative approach can be toroute with positional vector while feature vector is supplied to the selected expert.",
  "Wentao Zhang, Xupeng Miao, Yingxia Shao, Jiawei Jiang, Lei Chen, Olivier Ruas, and Bin Cui. ReliableData Distillation on Graph Convolutional Network. In Proc. Int. Conf. Manage. Data, 2020b": "Chenguang Zheng, Hongzhi Chen, Yuxuan Cheng, Zhezheng Song, Yifan Wu, Changji Li, James Cheng,Hao Yang, and Shuai Zhang. ByteGNN: Efficient Graph Neural Network Training at Large Scale. In Proc.Conf. Very Large Data Bases, 2022. Wenqing Zheng, Edward W Huang, Nikhil Rao, Sumeet Katariya, Zhangyang Wang, and Karthik Subbian.Cold Brew: Distilling Graph Node Representations with Incomplete or Missing Neighborhoods. In Proc.Int. Conf. Learn. Representations, 2021. Simiao Zuo, Qingru Zhang, Chen Liang, Pengcheng He, Tuo Zhao, and Weizhu Chen. MoEBERT: fromBERT to Mixture-of-Experts via Importance-guided Adaptation. In Proc. Conf. North Am. Chapter ofthe Assoc. Comput. Linguistics: Human Lang. Technol., 2022.",
  "BHardware specification": "Our experiments were conducted using an NVIDIA Tesla V100 GPU with 32GB of memory. The machinehas an Intel Xeon Gold 6140 CPU with clock frequency of 2.30GHz and total thread count of 36.Allcomputations, with exception of the clustering, were executed on the GPU. For Cora, Citeseer, PubMed,Amazon-Comp, Amazon-Photo and Academic-CS datasets we executed five parallel runs simultaneously.Each parallel run was allocated 6GB of GPU memory and 5 threads for the clustering. For Academic-Phy,OGB-ArXive, OGB-Products we executed only one run at a time with 32GB of GPU memory and 10 threadsfor clustering. We were unable to run KRD+ on our setup as it requires more than 32GB of memory.",
  "CHyperparameter tuning": "We use Ray Tune (Liaw et al., 2018) to tune model hyperparameters. Specifically, we use the Optuna searchalgorithm Akiba et al. (2019). We sample 200 hyperparameter configurations for small and medium datasetsand 80 for the large datasets. We tuned the following model structure hyperparameters: (i) dropout rate wasselected from [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6] and applied to all dropout layers in the model; (ii) total numberof experts was selected from . In addition to the structure hyperparameters, we selected thefollowing training hyperparameters: (i) learning rate for Adam optimizer (Kingma & Ba, 2014) was chosen",
  ": Test set accuracy with respect to the number of experts/clusters for RbM on OGB-ArXive dataset.The optimal number of clusters (5) is clearly identifiable in both transductive and inductive cases": "from [0.01, 0.005, 0.001]; (ii) weight of the commitment loss (6) from the range [0.0, 0.1]; (iii) weights and of the the load-balancing loss (8) and self-similarity loss (7) correspondingly from the range [0.0, 0.05]. Comparing to a distillation setup with MLP student, our training setup introduces three loss hyperparameters(see equation (13)) and an embedding update hyperparameter (see equation (4)).Due to the usage ofhyperparameter sampling, these additional hyperparameters do not increase the search time.",
  "DSelecting the number of experts": "As discussed in the main text, the performance of RbM does vary as the total number of experts is changed.During our experiments we use the same number of experts/clusters for all RbM layers in order to reducethe number of hyperparameters. We found that there is an optimal number of experts for RbM that can beidentified for each dataset using the validation dataset (from the range [3, . . . , 8]). shows how the accuracy depends on the number of experts for the OGB-ArXive dataset. Theseresults are for the test set, but similar results are observed for the validation set, thus allowing selectionof the best model. The optimal number of experts can be clearly identified for both the transductive andinductive settings (in the depicted case, 5 for each).",
  "EGCN Teacher Model": "We investigate whether our model is compatible with an alternative teacher GNN and demonstrates the sameadvantages over the baselines. The main paper provides results for GraphSAGE as the teacher, togetherwith some results for more advanced GNN teachers. provides additional results for experimentsin a transductive setting with GCN (Kipf & Welling, 2016) as the teacher. We use the originally reportedparameter settings of the baselines for this experiment. These were selected for a GraphSAGE teacher, so itis possible that parameter tuning could improve performance.",
  "MLPF(N + 1)F(N + 1)O(FN)EnsembleEaF(N + 1)EaF(N + 1)O(EaFN)MoEF(EN + E + 1) + H(F + E)F(EaN + Ea + 1) + H(F + E)O(EaFN + EH + FH)RbMEF(N + 1) + EFEaF(N + 1) + EFO(EaFN + EF)": "the parameter count and computational complexity of a single layer for MLP, MoE and RbM. We also contrastthis with an ensemble of MLPs. The feature size of the input vector is denoted by F. We assume that MLPlayer is a linear layer of projection size N and a bias. We denote the total number of experts in MoE andRbM by E, and the number of active experts during inference by Ea E. Note that E F and E N.We set the number of MLPs in the ensemble to be equal to the number of active experts. Each expertof MoE or RbM layer is a linear layer of projection size N and a bias. Routing is conducted accordingto equation 3 for RbM and equation 2 for the MoE. During the inference, routing procedure is conductedto descide which experts to run, therefore active parameter counter and time complexity both contain Edimensionality. MoE routing embeddings have internal size of H. Note that H E and it is common forintermediate MoE layers to have H = N. For the RbM layer we utilise a trainable constant attention thatmultiplies each feature with a scalar. In the ensemble input is always routed to all the members, thus allparameters of the ensemble are contributing to computation complexity. From one can see that MoE and RbM have some additional complexity comparing to the ensembleof MLPs that comes from the router. Thus if ensemble is expected to be Ea times slower that MLP student,RbM is expected to be Ea + 1 times slower. However, RbM is faster than MoE that utilises projection intoembedding space while RbM uses embeddings that are in input space.",
  "tran72.480.1372.480.1372.460.2272.440.2072.340.25": "but vary the annealing parameter . From , we can observe how for larger values, there is a minorperformance drop, but generally RbM is relatively insensitive to the annealing schedule. We assume thatlarge values of potentially lead to the algorithm becoming stuck at poorer expert embeddings. Empirically,we observe that, even with = 0, the expert embeddings converge sufficiently such that all representationsare consistently routed to exactly the same experts in successive epochs."
}