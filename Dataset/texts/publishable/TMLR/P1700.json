{
  "Abstract": "Collusion rings pose a significant threat to peer review. In these rings, reviewers who arealso authors coordinate to manipulate paper assignments, often by strategically bidding oneach others papers. A promising solution is to detect collusion through these manipulatedbids, enabling conferences to take appropriate action. However, while methods exist fordetecting other types of fraud, no research has yet shown that identifying collusion rings isfeasible. In this work, we consider the question of whether it is feasible to detect collusion rings fromthe paper bidding. We conduct an empirical analysis of two realistic conference biddingdatasets and evaluate existing algorithms for fraud detection in other applications. We findthat collusion rings can achieve considerable success at manipulating the paper assignmentwhile remaining hidden from detection: for example, in one dataset, undetected colludersare able to achieve assignment to up to 30% of the papers authored by other colluders.In addition, when 10 colluders bid on all of each others papers, no detection algorithmoutputs a group of reviewers with more than 31% overlap with the true colluders. Theseresults suggest that collusion cannot be effectively detected from the bidding using popularexisting tools, demonstrating the need to develop more complex detection algorithms as wellas those that leverage additional metadata (e.g., reviewer-paper text-similarity scores).",
  "Introduction": "In scientific peer review, researchers are called upon to evaluate the work of other researchers in the samescientific community and provide a recommendation as to whether each work should be published. Withinthe field of computer science, academic conferences are some of the primary venues of publication. Theseconferences receive a large pool of paper submissions from authors, and they rely on a peer review processto accurately assess the quality of each submission and determine which to accept or reject. The resultingacceptance decisions can have substantial impacts on the careers of the authors, as publication of a paperin a competitive computer science conference is often used as an indication of research quality. Thus, it isvital that the peer review processes of these conferences are fair to the authors. However, the perceived prestige of a conference publication can incentivize authors and reviewers to act inunethical ways. One such form of unethical behavior that has occurred in computer science conferences is",
  "Published in Transactions on Machine Learning Research (12/2024)": "2468 10 12 14 16 18 20 22 24 26 28 30 32 34number of reviewers (k) 1.0 0.8 0.6 0.4 0.2 bid density ( ) 0.38.00 0.38.00 0.38.00 0.37.00 0.38.00 0.38.00 0.37.00 0.59.04 1.00.00 1.00.00 1.00.00 1.00.00 1.00.00 1.00.00 1.00.00 1.00.00 1.00.00 0.38.00 0.38.00 0.38.00 0.38.00 0.37.00 0.37.00 0.38.00 0.37.00 0.38.02 0.78.02 0.81.00 0.82.00 0.82.00 0.82.00 0.82.00 0.82.00 0.82.00 0.38.00 0.38.00 0.38.00 0.38.00 0.38.00 0.38.00 0.37.00 0.37.00 0.37.01 0.37.01 0.36.01 0.37.01 0.57.02 0.64.00 0.63.00 0.63.00 0.64.00 0.38.00 0.38.00 0.38.00 0.38.00 0.38.00 0.38.00 0.38.00 0.37.01 0.38.00 0.37.00 0.36.01 0.36.01 0.37.00 0.36.01 0.36.01 0.35.01 0.35.01 0.38.00 0.38.00 0.38.00 0.38.00 0.38.00 0.38.00 0.38.00 0.38.00 0.38.00 0.38.00 0.38.00 0.37.00 0.38.00 0.37.00 0.37.00 0.38.00 0.37.01",
  "Once assigned, the colluders can give positive reviews to and push for the acceptance of other colluderspapers": "Approaches to addressing academic fraud can be divided into mitigation-based and detection-based meth-ods (Wilson, 2020). Several past works have proposed modifications to paper assignment algorithms thataim at mitigating the impact of malicious bidding (Guo et al., 2018; Jecmen et al., 2020; Wu et al., 2021;Boehmer et al., 2022; Jecmen et al., 2022). However, such approaches necessarily come with a cost in termsof assignment quality, as they ignore some aspects of reviewer preferences expressed through bids. Moreover,they do not identify which (if any) individuals were engaged in collusion. In contrast, detection methods havereceived little attention in prior work. Ideally, an accurate detection method could identify any colluders sothat the conference can intervene to stop them. For example, a conference could decide to block the assign-ment of detected colluders to each others papers, potentially leading to higher-quality assignments amongthe honest reviewers as compared to mitigation methods. Reliable detection methods could also allow theconference organizers to take more serious action against the colluding individuals, perhaps after a manualinvestigation provides further evidence. In the words of Littman (2021):",
  "Better paper-assignment technology would help close one loophole that is being exploited.But, without better investigative tools, we may never be able to hold the colluders to account": "However, careless deployment of detection algorithms may result in false positives: honest reviewers andauthors being falsely identified as colluders. This is a serious danger in scientific peer review, since falsely-accused reviewers and authors may have their reputations tarnished or their papers unfairly rejected. Es-tablishing effective methods to detect collusion rings is thus a critical path for research on this problem. A long line of research has applied anomaly-detection techniques to successfully detect various forms of fraudin other settings. Many of these settings involve a network of people in which the fraud appears as a set ofanomalous interactions: for example, auction fraud on online platforms, fraudulent financial transactions,fake reviews on sites such as Amazon, and fake accounts on social media (Akoglu et al., 2015). This raisesan important question of whether these techniques can be similarly applied to effectively detect fraudulentinteractions in the context of paper bidding for peer review. However, there is a vast range of approaches todetection in the research literature that could potentially be applied to this problem. To facilitate a thoroughanalysis, our work focuses on methods that attempt to detect collusion using only the paper bidding andauthorship data (i.e., the data that directly reflects the strategic bidding of colluders). Our focus on biddingis additionally motivated by the fact that real-world investigations of collusion often begin with analysis ofthe bidding data. Furthermore, many conferences use bidding-based heuristics to mitigate collusion (e.g.,ignoring the bids of reviewers with too few positive bids; Jecmen et al. 2022). Concretely, we consider thequestion: is it possible to effectively detect collusion rings from only the bids and authorships?Answering this question alone requires an extensive empirical analysis. However, resolving this questionprovides important direction for future research on detecting collusion rings.Our work does not attempt toinvestigate various other questions of interest to the problem of collusion detection: for example, comparingthe effectiveness of detection- and mitigation-based approaches, conducting a game-theoretical analysis of",
  "Overall, our results suggest that collusion rings cannot be effectively detected from only the bidding andauthorship data using popular existing algorithms. Our findings include the following:": "Using only the bidding and authorship data, all detection algorithms we analyze fail to detect someinjected collusion rings that are larger than any honest-reviewer groups with a similar density ofbidding. For example, when the collusion ring consists of 10 reviewers who bid on all of each othersauthored papers, the output of the best-performing detection algorithm across both datasets hasonly a 31% overlap (Jaccard similarity) with the true colluders on average. Colluders are able to achieve assignment to a substantial fraction of the papers authored by othercolluders while avoiding detection by all analyzed algorithms (30% and 24% in each of the twodatasets).",
  "Related Work": "Recent research has looked at improving various aspects of the peer review process (Shah, 2022). In thisline of work, various solutions have been proposed to the problem of reviewer-author collusion that weconsider (Jecmen et al., 2022).Jecmen et al. (2020) propose an algorithm for randomizing the paperassignment such that the probability of a colluder achieving assignment to another colluders paper is limited.Wu et al. (2021) propose learning a model of reviewer preferences from the bidding and using this modelto determine the paper assignment, thereby reducing the influence of collusive bidding.This work alsoprovides a semi-synthetic paper bidding dataset, which we use in our analysis. The works (Guo et al., 2018;Boehmer et al., 2022) consider the problem of finding a cycle-free paper assignment. In the 2021 AAAIConference on Artificial Intelligence, a large AI conference, several precautions were taken to address theproblem of collusion: two-cycles in the paper assignment were disallowed and a constraint on the countries ofthe reviewers assigned to the same paper was added (Leyton-Brown et al., 2022). Notably, these solutions allaim to mitigate the impact of collusion rings on the assignment, rather than detecting them outright. Whilethe present work focuses on detecting attacks in the paper bidding phase, other works show that attacks onthe text-similarity algorithms used by conferences can be effective (Markwood et al., 2017; Tran & Jaiswal,2019; Eisenhofer et al., 2023). Most relevant to our work is the prior work (Jecmen et al., 2023), which observes paper bidding in a mockconference setting and collects data on the strategies employed by participants acting as colluding reviewers.In contrast, rather than attempting to precisely simulate the behavior of colluding reviewers, we consider anintentionally simplified setting where malicious reviewer behavior can be easily parameterized, allowing us tosweep out a wide range of potential colluder strategies. We note that the most common colluder strategiesobserved by Jecmen et al. (2023) consist of injecting dense bidding between colluders, as is done by thestrategies considered in our analysis. Jecmen et al. (2023) also evaluate the performance of very simpledetection methods on the collected dataset, while we consider a set of more complex detection methodsbased on dense-subgraph discovery. Outside of the setting of scientific peer review, similar problems of fraud have been studied in the anomalydetection literature. In online platforms such as Amazon or Yelp, several methods have been proposed todetect products or sellers who purchase fraudulent reviews from users (Akoglu et al., 2013; Eswaran et al.,2017; Kumar et al., 2018). Other works propose density-based methods for detecting groups of fraudstersin these and similar online network settings (e.g., fraudulent transactions on eBay or fake followers onTwitter) (Pandit et al., 2007; Prakash et al., 2010; Hooi et al., 2016); we evaluate the performance of (Hooiet al., 2016) in our analysis. However, our setting (scientific peer review) is distinct from these online platformsettings in a few ways. Most significantly, our work focuses on collusion in the paper bidding phase, wherethe objective of colluders is to achieve a desired outcome in the subsequent paper assignment; in contrast, thefraudulent reviews are themselves the objective of fraudsters on (e.g.) Amazon. As a result, the incentivesfor peer-review colluders are not the same as those of frausters in the other settings: for example, sinceeach reviewer is assigned to review a limited number of papers, camouflaging by bidding positively onnon-colluder papers may result in those papers being assigned instead of the targeted papers. In addition,fraudulent interactions on online platforms are often provided by large numbers of fake accounts specificallyused for fraud; since making fake accounts on common peer-review platforms is non-trivial, interactionsbetween colluders in peer review may be more often done under their real identities, leading to differentpatterns of behavior. Numerous works have proposed other methods for graph-based anomaly detection,including those that address online spam, cyber-attacks, and other forms of fraud (Akoglu et al., 2015).Generic approaches for dense-subgraph detection (Goldberg, 1984; Tsourakakis et al., 2013; Hooi et al.,2020) can also be applied to detect anomalies in graphs; we evaluate these methods in our analysis. Aside from reviewer-author collusion rings, reviewers can attempt to dishonestly improve the chances ofacceptance of their own papers in other ways. One such form of malicious behavior occurs when reviewersprovide negative reviews to the papers they are assigned, thereby improving the relative standing of theirown authored work. In contrast to the issue of collusion rings, which require the cooperation of multiplereviewers, this sort of strategic reviewing can be done by each reviewer alone. In a controlled experiment,Balietti et al. (2016) found that people do exhibit strategic reviewing in a competitive peer evaluation setting.",
  "Setting": "We consider a conference peer review setting with a set of submitted papers P and a set of reviewers R.Conferences generally recruit the authors of the submitted papers to serve as reviewers, along with external,non-author reviewers. The authorship set A R P contains all pairs (r, p) such that reviewer r authoredpaper p. Additionally, the conflict-of-interest set C R P contains all pairs (r, p) such that reviewer r hasa conflict-of-interest with paper p and should not be assigned to review it (A C). Once submissions arereceived, the conference asks each reviewer to indicate their level of interest on each submitted paper viapaper bidding. While conferences often allow each reviewer to choose from a number of discrete levels (e.g.,Eager, Willing, Not willing), we consider a simplified setting with binary bids (positive or neutral);note that allowing additional levels of bids can only give colluders more flexibility to manipulate the paperassignment. The bid set B R P contains all pairs (r, p) such that reviewer r bid positively on paper p(B C = ). The conference also computes text-similarity scores between each reviewer and paper using afunction T : R P , where T(r, p) indicates the level of similarity between the text of paper p andthe text of the past work of reviewer r. The conference then computes the paper assignment in the following manner. First, the conference computessimilarity scores between each reviewer and paper using a function S : R P . In our experiments,we use S(r, p) = 1 2T(r, p)2I[(r,p)B] based on the function used in the 2016 Conference on Neural InformationProcessing Systems (NeurIPS), a large machine-learning conference (Shah et al., 2018).The conferencethen computes an assignment of papers to reviewers such that the total similarity of the assigned pairs ismaximized, subject to constraints that (i) each paper is assigned to exactly 3 reviewers, (ii) each reviewer isassigned at most 6 papers, and (iii) no reviewer-paper pairs in C are assigned. While we use the stated valuesin our experiments, the exact reviewer and paper loads vary between conferences. The maximum-similarityassignment can be computed efficiently as a min-cost flow or as a linear program. This framework for paperassignment has been used in numerous conferences and venues (Shah, 2022).",
  "We next provide details regarding the two datasets that we analyze in this work": "The first dataset, which we refer to as AAMAS, contains a subset of the real bidding from the 2021International Conference on Autonomous Agents and Multiagent Systems, an AI conference. This dataset ispublicly available from PrefLib (Mattei & Walsh, 2013) and contains de-identified bids from reviewers thatdid not opt-out from data collection. In this conference, bids were selected from {Yes, Maybe, Conflict,No response}; we consider Yes and Maybe responses to be positive bids and include those reviewer-paper pairs in B. We set C as the set of all reviewer-paper pairs with Conflict bids. Since the dataset doesnot include authorship information, we reconstruct the authorships A by subsampling 3 conflicts-of-interestuniformly at random for each paper. The resulting dataset has 526 papers and 596 reviewers, 398 of whomauthored at least one paper. The dataset also does not contain text-similarity scores T(r, p). We generatesynthetic text-similarity scores using the procedure described in Appendix A, based on the text-similaritiesfrom the 2018 International Conference on Learning Representations reconstructed by Xu et al. (2019). Our second dataset, which we refer to as S2ORC, is the semi-synthetic dataset constructed and madepublicly available by Wu et al. (2021).This dataset contains synthetic bids between a large subset ofpublished computer science papers and authors from the Semantic Scholar Open Research Corpus (Ammar",
  "Problem Statement": "Our goal is to detect collusion rings. We suppose that there exists a group of colluding reviewers M R whotry to manipulate the paper assignment by altering their bids, with the aim of being assigned to review thepapers authored by other members of the colluding group. The objective of a collusion-detection algorithmis to output M given the set of bids B, along with the authorships A and the other conflicts C. Note that thetext-similarities T(r, p) cannot be used for detection in our analysiswe consider the problem of detectionusing only the bidding itself and the authorships. As our original datasets do not contain colluding reviewers, we inject collusion into the datasets by choosinga group of reviewers to be the colluders M and modifying the bids of these reviewers (i.e., adding or removingelements of MP to/from B). The strongest form of collusion would be to add bids between all reviewers inM and all papers authored by other reviewers in M, while additionally removing all other bids by reviewersin M. However, malicious reviewers may not want to perform such an obvious manipulation out of fearof being caught. Due to the lack of concrete evidence on colluding groups and the exact bidding strategythat they might employ, we consider a wide range of possible colluding groups parameterized by both a sizeparameter (i.e., the number of colluding reviewers) and a density parameter (roughly corresponding to theattack strength). As there are some modeling choices involved in concretely defining the bidding density parameter, we con-sider two different notions of density, corresponding to two different graph representations of the biddingrelationships between the reviewers of the conference. The first representation () is a unipartitegraph in which vertices represent reviewers. The second representation () is a bipartite graph inwhich vertices represent both reviewers and papers. We motivate and define each of these formulations intheir respective sections. For each graph formulation, we investigate the following three high-level researchquestions:",
  "(a) Exact counts, AAMAS": "number of reviewers (k) 1.0 0.9 0.8 0.7 0.6 0.5 edge density ( ) 1652 1555 12348825202185460000 1652 1555 4225 2976 4679 1.2E4 9596 1.1E4 4597 2474 1512331 1652 6015 1.2E4 1.9E4 5.3E4 1.7E5 3.4E5 4.8E5 1.5E58973670 1652 6015 4.3E4 2.5E5 8.3E5 2.2E6 5.6E6 7.7E4 8.7E417800 1652 3.4E4 1.2E5 2.5E6 1.2E7 2.7E7 1.9E6 2.9E4 2.7E4000 1.9E4 1.7E5 2.6E6 1.9E7 1.2E8 1.0E7 5.0E5 8.5E5 3297000",
  "(d) Groups found by greedy peeling, S2ORC": ": Exact counts of honest-reviewer groups with varying size and edge density (Figures 1a-1b), and thesize and edge density of honest-reviewer groups found by a heuristic method (Figures 1c-1d). In Figures 1a-1b, values in cells marked with represent lower bounds since exact counts were infeasible to compute;the vertical axis represents lower bounds on the edge density. In Figures 1c-1d, each point corresponds toan existing honest-reviewer group (found by a greedy peeling method), and the shaded area indicates theregion in which there exists at least one honest-reviewer group. The vertical axis represents exact values forthe edge density. Note that the vertical axis does not start at 0 for easier comparison with other figures. least one paper authored by reviewer r2. Informally, the density of a group of reviewers in this graph is thefraction of possible edges present between the reviewers. We formalize these definitions shortly. In ,we consider an alternative, bipartite graph representation of the bidding. Our motivation for considering this graph representation is that it most naturally represents the reciprocalbehavior expected of a collusion ring. Several existing works (Guo et al., 2018; Boehmer et al., 2022; Leyton-Brown et al., 2022) on reviewer-author collusion consider a similar reviewer-to-reviewer graph, aiming toprevent cycles (i.e., rings) between reviewers in the paper assignment. For our purposes, the notion ofdensity implied by this graph has several useful properties. First, malicious reviewers may not attempt tomanipulate the assignment of every paper they author. For example, each colluder might have one bad paperthat they want their fellow colluders to review (e.g., because it is lower quality) and many good papers notinvolved in the manipulation. Even if each colluder authors many good papers, collusion would still appearas a dense subgraph. Additionally, collusion rings are based on a quid-pro-quo arrangement between thecolluders, where each colluder needs help getting some papers accepted to the conference. Each colludertherefore should receive some benefit from the other colluders, reflected as density. Formally, we denote the (directed) graph by G1 = (V1, E1), where V1 = R and E1 = pP{(r1, r2) R2 :(r1, p) B (r2, p) A}. Given this graph representation, we characterize a potential group of colludingreviewers S R in terms of two parameters. The first parameter kS = |S| is the size of the group. Thesecond parameter S is the edge density of the group, defined as follows. For an arbitrary graph G = (V, E)and any subset of vertices V V, we use E[V] to denote the set of edges in the subgraph induced by V.We then define the edge density of S to be S = |E[S]|",
  "Honest-Reviewer Groups (Q1)": "First, we count the number of honest-reviewer groups in G2 for each value of size k and bid density . Weconsider only reviewers that have authored at least one paper. Since the bid density for a group is naturallylower than the edge density in general, we consider values of bid density ranging between 0.2 and 1.0. The counts are shown in . Due to the more complicated definition of bid density, we cannot efficientlyprune branches of the search tree and must enumerate all|R|kreviewer subsets. As a result, the range offeasible k are significantly more limited than in .1: counts for k 6 for AAMAS and k 5 forS2ORC could not be completed in 24 hours. In S2ORC, we see that numerous honest-reviewer groups doexist at every feasible setting, up to = 1.0 and k = 4. Honest-reviewer groups are less prevalent in AAMAS,but still exist at k = 5 and 0.8. Like the figures in .1, these figures may be independently usefulto conferences as indicators of the number of false positives for any heuristic defenses against collusion theyuse. As in .1, we also run a greedy peeling method to heuristically find high-density reviewer groups oflarger sizes. However, this method performs very poorly and so we relegate the results to Appendix B.",
  "Detection Algorithm Evaluation (Q2)": "Next, we evaluate the performance of a variety of dense-subgraph discovery algorithms at detecting injectedcollusion in G2, including an influential fraud-detection algorithm for the online review setting. Specifically,we consider the following algorithms: DSD, OQC-Greedy, OQC-Local, and TellTail: These algorithms are introduced in .2. Asinput to each algorithm, we provide an unattributed bipartite graph G = (V2, E) with the samevertex set as G2. We consider two variants for the edge set of the input graph: the edge set consistsof only the bid edges E = E(B)2, and the edge set consists of the union of the bid and authorshipedges E = E(B)2 E(A)2. In the OQC algorithms, we use the original objective function for undirectedgraphs f(S) = |E[S]| (1/3)|S|2. Fraudar (Hooi et al., 2016):This algorithm is designed to detect fake product reviews (e.g., onAmazon/Yelp) or fake followers (e.g., on Twitter) from a bipartite graph of users and products G =(V, E), where E contains edges from users to products. Denote by deg(v) the degree of a vertex v inG. The returned subset of vertices S V is chosen to approximately maximize the objective functionf(S) =1|S| (u,v)E[S] (log(5 + deg(v)))1. This objective is camouflage-resistent, meaning thatfraudulent users cannot affect their own objective value by adding extra edges to honest products.In our setting, we consider the reviewers to be users and papers to be products. We again inputan unattributed bipartite graph with the same vertex set as G2 and two variants for the edge set:E = E(B)2and E = E(B)2 E(A)2.",
  "connectivity between the chosen subset and the rest of the graph": "DSD, OQC-Greedy, and OQC-Local operate on pre-specified notions of density, without considering thesparsity of subgraphs in the input graph. In contrast, TellTail defines density in a data-driven fashion byidentifying the distribution of masses that subgraphs of certain sizes follow, arguing that other notions ofdensity tend to be biased towards larger subgraphs. TellTail was also shown to detect fraudulent followerson Twitter. These algorithms cover a representative set of the landscape of dense-subgraph mining solutions. We evaluate the detection algorithms against the following collusion model. For each setting of (k, ), wechoose a subset of k reviewers uniformly at random from among those reviewers that authored at least onepaper. We then add edges uniformly at random between colluding reviewers until the subgraph has edgedensity at least . This modified graph is then passed as input to each detection algorithm. We repeat thisprocedure for 50 trials for each setting. We report the mean Jaccard similarity between the set of injectedcolluders and the set of reviewers output by the detection algorithms. For the detection algorithms that use local search to optimize their objective (OQC-Local and TellTail), wereturn the result with highest objective value over 11 initializations. The first run is initialized accordingto the triangle-counting heuristic suggested by Tsourakakis et al. (2013), and the remaining 10 runs areinitialized uniformly at random. We find that the detection performance of OQC-Local is significantly betterwhen only the heuristic initialization is used, since the random initializations often resulted in output witha higher objective value but lower overlap with the true colluders; thus, we show the results with heuristicinitialization only in this section and defer the results with all initializations to Appendix B. However,the poor performance of OQC-Local when all initializations are used indicates that the objective value ofOQC-Local is misaligned with the detection objective. The results for DSD, OQC-Local, and TellTail are shown in . Results for OQC-Greedy are shown inAppendix B as it generally performs worse than OQC-Local. In both datasets, DSD performs very poorly;this is because it consistently returns a overly large subset of reviewers. On the AAMAS dataset, OQC-Localdoes somewhat better, achieving Jaccard similarity above 0.5 for values of k 26 and 0.9. TellTailperforms by far the best, detecting colluders consistently for moderate-to-large values of (k, ). However,there exists a wide range of settings in which all algorithms fail to detect injected colluders: for example,reviewer groups with 8 k 12 and = 1.0 do not exist in the honest bidding and yet are still not detectedby any algorithm. On the S2ORC dataset, OQC-Local and TellTail appear to perform equally well. Thereis a similar region in which honest-reviewer groups do not exist and yet detection fails to identify the correctgroup with high probability (e.g., 10 k 12 with = 1.0), albeit smaller than in the AAMAS dataset.",
  "(a) DSD, AAMAS": "2468 10 12 14 16 18 20 22 24 26 28 30 32 34number of reviewers (k) 1.0 0.9 0.8 0.7 0.6 0.5 edge density ( ) 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.89.04 1.00.00 1.00.00 1.00.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.04.00 0.04.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.90.00 0.90.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.04.00 0.04.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.04.00 0.04.00 0.04.00 0.04.00 0.04.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.04.00 0.04.00 0.04.00 0.04.00 0.04.00 0.04.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.05.00 0.04.00 0.04.00 0.04.00 0.04.00",
  "(b) DSD, S2ORC": "2468 10 12 14 16 18 20 22 24 26 28 30 32 34number of reviewers (k) 1.0 0.9 0.8 0.7 0.6 0.5 edge density ( ) 0.27.00 0.27.00 0.27.00 0.27.00 0.27.00 0.27.00 0.27.00 0.27.01 0.31.03 0.53.05 0.51.05 0.63.05 0.74.05 0.82.04 0.82.04 0.91.03 0.88.03 0.27.00 0.27.00 0.27.00 0.27.00 0.27.00 0.27.00 0.27.00 0.27.00 0.26.00 0.33.03 0.34.03 0.42.04 0.51.04 0.59.04 0.71.04 0.72.04 0.75.03 0.27.00 0.27.00 0.27.00 0.27.00 0.27.00 0.27.00 0.27.00 0.27.00 0.27.00 0.26.01 0.25.01 0.25.01 0.34.03 0.40.04 0.43.04 0.49.04 0.51.04 0.27.00 0.27.00 0.27.00 0.27.00 0.27.00 0.28.00 0.27.00 0.27.00 0.27.00 0.28.00 0.27.00 0.26.01 0.25.01 0.26.01 0.28.02 0.29.02 0.32.02 0.27.00 0.27.00 0.27.00 0.27.00 0.28.00 0.27.00 0.27.00 0.27.00 0.27.00 0.27.00 0.27.00 0.27.00 0.26.00 0.27.01 0.26.01 0.25.01 0.24.01 0.27.00 0.27.00 0.27.00 0.27.00 0.27.00 0.27.00 0.27.00 0.27.00 0.27.00 0.27.00 0.27.00 0.27.00 0.27.00 0.27.00 0.27.00 0.26.00 0.26.01",
  "(c) OQC-Local, AAMAS": "2468 10 12 14 16 18 20 22 24 26 28 30 32 34number of reviewers (k) 1.0 0.9 0.8 0.7 0.6 0.5 edge density ( ) 0.42.00 0.42.00 0.42.00 0.42.00 0.43.01 0.55.04 0.78.05 0.90.03 0.94.03 1.00.00 1.00.00 1.00.00 1.00.00 1.00.00 1.00.00 1.00.00 1.00.00 0.42.00 0.42.00 0.42.00 0.42.00 0.42.00 0.43.01 0.56.03 0.72.04 0.78.03 0.87.02 0.86.02 0.89.01 0.89.01 0.90.00 0.90.00 0.90.00 0.90.00 0.42.00 0.42.00 0.42.00 0.42.00 0.42.01 0.42.00 0.43.01 0.45.03 0.61.03 0.68.03 0.74.02 0.76.02 0.77.02 0.80.00 0.77.02 0.80.00 0.80.00 0.42.00 0.42.00 0.42.00 0.42.00 0.42.00 0.42.00 0.42.01 0.43.01 0.46.02 0.46.03 0.52.03 0.50.03 0.54.03 0.64.02 0.62.02 0.67.02 0.67.02 0.42.00 0.42.00 0.42.00 0.42.00 0.42.00 0.42.00 0.42.00 0.42.01 0.42.00 0.43.01 0.42.01 0.39.02 0.42.02 0.42.03 0.40.02 0.49.02 0.49.02 0.42.00 0.42.00 0.42.00 0.42.00 0.42.00 0.42.00 0.42.00 0.42.00 0.42.00 0.41.01 0.41.01 0.41.01 0.40.01 0.41.01 0.39.01 0.37.02 0.34.02",
  "(d) OQC-Local, S2ORC": "2468 10 12 14 16 18 20 22 24 26 28 30 32 34number of reviewers (k) 1.0 0.9 0.8 0.7 0.6 0.5 edge density ( ) 0.57.00 0.57.00 0.57.00 0.57.00 0.57.00 0.57.00 0.93.02 0.98.01 1.00.00 1.00.00 1.00.00 1.00.00 1.00.00 1.00.00 1.00.00 1.00.00 1.00.00 0.57.00 0.57.00 0.57.00 0.57.00 0.57.00 0.57.00 0.57.00 0.68.02 0.90.00 0.90.00 0.90.00 0.89.01 0.90.00 0.90.00 0.90.00 0.90.00 0.90.00 0.58.00 0.57.00 0.57.00 0.57.00 0.57.00 0.57.00 0.57.00 0.57.00 0.57.00 0.63.01 0.79.01 0.80.00 0.80.00 0.80.00 0.80.00 0.80.00 0.80.00 0.57.00 0.57.00 0.58.00 0.57.00 0.57.00 0.57.00 0.57.00 0.57.00 0.57.00 0.57.00 0.57.00 0.58.00 0.58.01 0.67.01 0.70.00 0.70.00 0.70.00 0.57.00 0.57.00 0.57.00 0.58.00 0.58.00 0.57.00 0.58.00 0.57.00 0.57.00 0.57.00 0.57.00 0.57.00 0.57.00 0.57.00 0.57.00 0.57.00 0.57.00 0.57.00 0.57.00 0.57.00 0.57.00 0.57.00 0.58.00 0.57.00 0.57.00 0.57.00 0.57.00 0.57.00 0.57.00 0.57.00 0.57.00 0.57.00 0.58.00 0.57.00",
  "(e) TellTail, AAMAS": "2468 10 12 14 16 18 20 22 24 26 28 30 32 34number of reviewers (k) 1.0 0.9 0.8 0.7 0.6 0.5 edge density ( ) 0.84.00 0.84.00 0.84.00 0.84.00 0.79.02 0.70.04 0.82.04 0.95.02 0.99.01 0.99.01 1.00.00 1.00.00 1.00.00 1.00.00 1.00.00 1.00.00 1.00.00 0.84.00 0.84.00 0.84.00 0.84.00 0.82.01 0.76.03 0.78.03 0.70.03 0.86.02 0.89.01 0.88.01 0.89.01 0.90.00 0.90.00 0.90.00 0.90.00 0.90.00 0.84.00 0.84.00 0.84.00 0.84.00 0.84.00 0.82.01 0.75.03 0.60.04 0.64.03 0.71.02 0.75.02 0.79.01 0.78.01 0.80.00 0.80.00 0.80.00 0.80.00 0.84.00 0.84.00 0.84.00 0.84.00 0.84.00 0.84.00 0.84.00 0.80.02 0.68.04 0.47.04 0.55.03 0.52.03 0.60.02 0.66.01 0.68.01 0.69.01 0.70.00 0.84.00 0.84.00 0.84.00 0.84.00 0.84.00 0.84.00 0.84.00 0.84.00 0.82.01 0.75.03 0.68.03 0.55.04 0.45.04 0.45.03 0.51.02 0.55.02 0.60.02 0.84.00 0.84.00 0.84.00 0.84.00 0.84.00 0.84.00 0.84.00 0.84.00 0.84.00 0.82.01 0.84.00 0.82.01 0.73.03 0.68.04 0.55.04 0.56.04 0.38.04",
  "Manipulation Success Evaluation (Q3)": "Finally, we evaluate the success of the colluders at manipulating the assignment as a function of the parame-ters (k, ). For each setting of these parameters, we inject a group of colluders as described in the precedingsection and compute a paper assignment using the procedure detailed in .1. As in .3, we",
  "(a) Fraction of target papers with a colluder assigned,AAMAS": "2468 10 12 14 16 18 20 22 24 26 28 30 32 34number of reviewers (k) 1.0 0.8 0.6 0.4 0.2 bid density ( ) 0.03.01 0.08.02 0.12.01 0.14.01 0.16.01 0.19.01 0.20.01 0.22.01 0.22.01 0.24.01 0.26.01 0.28.01 0.27.01 0.29.01 0.29.01 0.31.01 0.30.01 0.02.01 0.07.01 0.12.01 0.14.01 0.16.02 0.18.01 0.20.01 0.20.01 0.21.01 0.21.01 0.25.01 0.27.01 0.26.01 0.28.01 0.28.01 0.29.01 0.29.01 0.02.01 0.06.01 0.07.01 0.13.01 0.14.01 0.15.01 0.17.01 0.19.01 0.19.01 0.22.01 0.22.01 0.23.01 0.25.01 0.24.01 0.26.01 0.26.01 0.27.01 0.04.02 0.04.01 0.06.01 0.08.01 0.09.01 0.11.01 0.13.01 0.14.01 0.17.01 0.17.01 0.18.01 0.20.01 0.22.01 0.20.01 0.21.01 0.23.01 0.22.01 0.02.01 0.01.01 0.04.01 0.06.01 0.06.01 0.07.01 0.09.01 0.10.01 0.11.01 0.11.01 0.13.01 0.13.01 0.15.01 0.14.01 0.16.01 0.16.01 0.16.01",
  "(b) Fraction of target papers with a colluder assigned,S2ORC": "2468 10 12 14 16 18 20 22 24 26 28 30 32 34number of reviewers (k) 1.0 0.8 0.6 0.4 0.2 bid density ( ) 0.09.03 0.23.03 0.31.03 0.41.02 0.43.02 0.48.02 0.52.02 0.56.02 0.55.02 0.61.02 0.60.02 0.65.01 0.64.01 0.65.01 0.67.01 0.71.01 0.72.01 0.12.03 0.21.03 0.31.03 0.34.02 0.44.02 0.46.02 0.50.02 0.54.02 0.52.02 0.59.02 0.63.01 0.60.01 0.62.02 0.63.02 0.67.01 0.69.01 0.68.01 0.08.03 0.18.03 0.27.03 0.30.02 0.36.02 0.45.02 0.44.02 0.48.02 0.48.02 0.51.02 0.54.01 0.55.02 0.59.01 0.62.01 0.62.01 0.63.01 0.66.01 0.10.03 0.14.03 0.20.02 0.28.03 0.32.02 0.34.02 0.36.02 0.43.02 0.50.02 0.45.02 0.48.01 0.52.01 0.55.01 0.55.01 0.59.01 0.58.01 0.59.01 0.04.02 0.13.02 0.15.02 0.18.02 0.25.02 0.31.02 0.32.02 0.32.02 0.35.02 0.40.01 0.40.01 0.42.02 0.47.02 0.48.01 0.48.01 0.49.01 0.53.01",
  "(c) Fraction of colluders with a colluder assigned,AAMAS": "2468 10 12 14 16 18 20 22 24 26 28 30 32 34number of reviewers (k) 1.0 0.8 0.6 0.4 0.2 bid density ( ) 0.05.02 0.17.03 0.23.02 0.24.02 0.28.02 0.31.02 0.33.02 0.33.02 0.34.01 0.37.01 0.39.01 0.43.01 0.42.01 0.41.01 0.43.01 0.47.01 0.44.01 0.04.02 0.13.02 0.21.02 0.23.02 0.27.02 0.28.02 0.32.02 0.34.01 0.34.02 0.33.01 0.39.02 0.42.01 0.42.01 0.42.01 0.43.01 0.43.01 0.44.01 0.02.01 0.09.02 0.13.02 0.22.02 0.24.02 0.24.02 0.27.02 0.32.01 0.31.02 0.33.02 0.36.01 0.36.01 0.38.01 0.38.01 0.39.02 0.41.01 0.41.01 0.06.02 0.08.02 0.09.01 0.14.02 0.16.02 0.20.02 0.22.02 0.25.02 0.28.02 0.27.01 0.28.01 0.33.01 0.33.02 0.30.01 0.33.01 0.36.01 0.35.01 0.03.02 0.04.01 0.07.01 0.10.02 0.10.01 0.14.01 0.17.01 0.17.01 0.19.02 0.19.01 0.21.01 0.22.01 0.26.01 0.23.01 0.27.01 0.28.01 0.27.01",
  ": Success of colluders in terms of k and . Values indicate the mean for each metric along withstandard errors": "evaluate the success of the colluders in two ways: the fraction of colluder-authored papers with at least onecolluder assigned, and the fraction of colluders with at least one colluder assigned to one of their authoredpapers. We conduct 50 trials for each setting of the parameters and report the mean for each of the twometrics. The results are shown in . On AAMAS, we see that at (k = 16, = 0.8) where no detection algorithmwas able to detect colluders with high probability, the colluders can successfully achieve assignments to 30%of their target papers and 54% of the colluders. Similar results are seen elsewhere on the frontier of theundetected region. On S2ORC, although the detection algorithms performed poorly, the success values arealso lower than in the corresponding case on AAMAS. Still, in the cell (k = 26, = 0.8) where the detectionalgorithms performed poorly, colluders can achieve assignment to 26% of target papers and 42% of colluders,a sizeable influence on the paper assignment. We note that these success rates are quite similar to thosefound in .3 for the unipartite graph setting, which may provide some indication that the abilityof undetected colluders to manipulate the assignment is robust to the exact graph representation. Overall,colluders are still able exert influence over the acceptance decisions for a non-trivial fraction of their ownpapers while avoiding detection.",
  "Bipartite Bidding Graph": "In this section, we represent the reviewer bidding data as a bipartite graph, in which one set of verticescorresponds to reviewers and the other set of vertices corresponds to papers. This graph contains threetypes of undirected edges between a reviewer r and a paper p: bid edges, indicating that reviewer r bidpositively on paper p; authorship edges, indicating that reviewer r authored paper p; and conflict-of-interestedges, indicating that reviewer r has a conflict-of-interest with paper p but did not author it. Our motivationfor considering this graph is that it directly represents all data that the detection algorithms have access to. Formally, we denote this graph as G2 = (V2, (E(B)2, E(A)2, E(C)2)), where V2 = R P is the vertex set andE(B)2= B, E(A)2= A, and E(C)2= C \\ A are the sets of bid, authorship, and conflict edges respectively. As in, we characterize a (potentially colluding) group of reviewers S R in terms of a size parameterkS = |S| and a density parameter S. In G2, we consider a new notion of density, which we call bid density.For any subset of reviewers R R, define P[R] = rR{p P : (r, p) A} to be the subset of papersauthored by at least one reviewer in R. The bid density of S is then defined as the total number of bidsmade by reviewers in S on papers in P[S], divided by the maximum possible number of bids by reviewersin S on papers in P[S]:",
  "(a) OQC-Greedy, AAMAS": "2468 10 12 14 16 18 20 22 24 26 28 30 32 34number of reviewers (k) 1.0 0.8 0.6 0.4 0.2 bid density ( ) 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 1.00.00 1.00.00 1.00.00 1.00.00 1.00.00 1.00.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.53.03 0.78.01 0.81.00 0.81.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00 0.41.00",
  "(b) OQC-Greedy, S2ORC": "2468 10 12 14 16 18 20 22 24 26 28 30 32 34number of reviewers (k) 1.0 0.8 0.6 0.4 0.2 bid density ( ) 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.14.00 0.51.06 0.96.02 1.00.00 1.00.00 1.00.00 1.00.00 1.00.00 1.00.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.12.00 0.12.00 0.13.00 0.14.00 0.14.00 0.26.03 0.61.04 0.79.02 0.81.00 0.82.00 0.82.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.12.00 0.10.01 0.09.01 0.10.01 0.12.00 0.14.00 0.14.00 0.15.00 0.18.01 0.28.03 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.12.00 0.10.01 0.08.01 0.07.00 0.07.00 0.08.00 0.08.01 0.09.01 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.11.00 0.09.01",
  "(c) Fraudar, AAMAS": "2468 10 12 14 16 18 20 22 24 26 28 30 32 34number of reviewers (k) 1.0 0.8 0.6 0.4 0.2 bid density ( ) 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.09.02 0.55.06 0.97.02 1.00.00 1.00.00 1.00.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.11.02 0.36.03 0.67.03 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00 0.02.00",
  "(d) Fraudar, S2ORC": "2468 10 12 14 16 18 20 22 24 26 28 30 32 34number of reviewers (k) 1.0 0.8 0.6 0.4 0.2 bid density ( ) 0.47.00 0.47.00 0.47.00 0.49.01 0.59.02 0.67.02 0.70.02 0.68.02 0.76.02 0.75.02 0.77.01 0.79.01 0.77.01 0.79.01 0.79.01 0.77.01 0.78.01 0.47.00 0.47.00 0.47.00 0.48.01 0.50.01 0.56.02 0.61.02 0.61.02 0.67.02 0.68.01 0.69.01 0.69.01 0.70.01 0.70.01 0.70.01 0.71.01 0.71.01 0.47.00 0.47.00 0.47.00 0.48.00 0.48.00 0.47.00 0.50.01 0.52.01 0.54.01 0.54.01 0.56.01 0.57.01 0.57.01 0.57.01 0.58.01 0.59.01 0.61.00 0.47.00 0.47.00 0.47.00 0.48.00 0.47.00 0.47.00 0.47.00 0.46.00 0.47.00 0.47.00 0.47.00 0.47.00 0.45.00 0.45.00 0.45.00 0.44.00 0.44.00 0.48.00 0.47.00 0.47.00 0.48.00 0.47.00 0.47.00 0.47.00 0.47.00 0.47.00 0.47.00 0.47.00 0.47.00 0.47.00 0.47.00 0.47.00 0.47.00 0.47.00",
  "(e) OQC-Specialized, AAMAS": "2468 10 12 14 16 18 20 22 24 26 28 30 32 34number of reviewers (k) 1.0 0.8 0.6 0.4 0.2 bid density ( ) 0.57.00 0.57.00 0.57.00 0.58.00 0.58.00 0.57.01 0.58.01 0.59.01 0.63.02 0.61.01 0.62.01 0.65.02 0.63.01 0.65.02 0.64.02 0.67.02 0.66.02 0.57.00 0.57.00 0.57.00 0.58.00 0.57.00 0.57.00 0.57.00 0.57.00 0.58.01 0.59.01 0.59.01 0.61.01 0.59.01 0.60.01 0.61.01 0.61.01 0.62.01 0.57.00 0.58.00 0.58.00 0.57.00 0.57.00 0.58.00 0.58.00 0.57.00 0.57.00 0.57.00 0.57.00 0.57.01 0.58.00 0.57.00 0.57.00 0.58.00 0.57.00 0.57.00 0.58.00 0.58.00 0.57.00 0.57.00 0.58.00 0.58.00 0.57.00 0.57.00 0.58.00 0.57.00 0.56.00 0.57.01 0.56.01 0.58.00 0.57.00 0.57.00 0.57.00 0.57.00 0.57.00 0.57.00 0.58.00 0.58.00 0.57.00 0.57.00 0.58.00 0.57.00 0.57.00 0.57.00 0.57.00 0.57.00 0.57.00 0.57.00 0.57.01",
  "|E(A)2[S P[S]]| |E(C)2[S P[S]]|": "Analogous to the concept of edge surplus that motivates the optimal quasi-clique objective func-tion, this corresponds to the number of excess bids above the expectation if each bid is madeindependently with probability . We use local search to optimize this objective. As in the otherOQC algorithms, we set = 1/3.",
  "Since we aim to detect a group of colluding reviewers, we discard any paper vertices in the subset output byeach algorithm, considering only the output reviewer vertices as the detected reviewers": "In addition to the algorithms from .2, Fraudar is an influential fraud-detection algorithm thatleverages a density-based signal for detection. It was designed to detect fraudulent reviews on platforms likeAmazon, a similar problem to our own. OQC-Specialized was not proposed in prior work, but naturallygeneralizes the objective function of OQC-Local to operate directly on G2. We now describe the collusion model against which we evaluate the detection algorithms. For each valueof (k, ), we inject a group of colluding reviewers into G2 as follows. We first choose a set of k reviewers,denoted by M, uniformly at random from among all reviewers who have authored at least one paper. Wethen choose a reviewer-paper pair uniformly at random from M P[M] and add this pair to E(B)2if thereis no existing edge of any type between this pair. We repeat this until the bid density is at least . Thismodified graph is then used to construct the input to each detection algorithm. We repeat this procedure for50 trials for each setting of parameters. We report the mean Jaccard similarity between the set of injectedcolluders and the output set of reviewers from the detection algorithms. As in .2, for each local search algorithm (OQC-Local, TellTail, OQC-Specialized), we return theresult with highest objective value over 11 initializations. The first initial subset is chosen according to ageneralization of the heuristic suggested by Tsourakakis et al. (2013). For each vertex, we count the numberof bid-author-bid-author cycles that this vertex participates in and divide by the total bid- and authorship-degree of the vertex; we then choose the initial subset to be the maximum vertex according to this metricalong with all of its neighbors. The remaining 10 runs are initialized uniformly at random. We show results for the OQC-Greedy, Fraudar, and OQC-Specialized algorithms in . Results for theremaining algorithms, which generally performed worse, are in Appendix B. For all algorithms (other thanOQC-Specialized), we find that the performance is very similar when the input edge set is E(B)2and whenit is E(B)2 E(A)2; thus, all results shown are for the case with edge set E(B)2.On AAMAS, we see that bothOQC-Greedy and Fraudar are perform very well for high values of (k, ) (e.g., k 20, 0.8). However,both of these algorithms fail to detect colluders entirely at more moderate values of k (e.g., k = 14, = 1.0).OQC-Specialized achieves some success for a much wider range of parameters, but fails to achieve Jaccardsimilarities above 0.9 even for extreme values of (k, ); this may indicate that the true colluding groupis not a local optimum for this algorithms objective function. On S2ORC, performance of all algorithmsis significantly worse. OQC-Greedy and Fraudar still identify the colluding groups for extreme values of(k, ), but OQC-Specialized fails to consistently identify the colluders for any parameter values. Overall,there exists a significant region of moderate parameter values at which no algorithm achieves good detectionperformance.",
  "Discussion": "We provide an empirical exploration of the problem of detecting reviewer-author collusion rings from manip-ulated bidding, framing the problem as a dense-subgraph discovery problem in two different graph represen-tations. Overall, we find that within our parametrized model of colluder behavior, malicious reviewers canmanipulate the paper assignment to moderate success while remaining within typical or difficult-to-detectlevels of density in the bidding graph (using popular existing anomaly-detection algorithms). This providesevidence to support the conclusion that malicious reviewer behavior cannot be effectively detected using justthe bidding data. While our analysis cannot conclusively prove that bidding data is insufficient to detectcollusion, our methodology thoroughly explores this problem from several different analyses of realistic con-ference data. Our work considers only a specific parametrized model of collusive behavior in which colludersreceive reciprocal benefits within the conference in question. However, our results indicate that even thisbehavior cannot be detected. In reality, colluders may be more sophisticated in their attempts to avoiddetection, making detection more challenging.",
  "Leman Akoglu, Hanghang Tong, and Danai Koutra. Graph based anomaly detection and description: Asurvey. Data Mining and Knowledge Discovery, 29(3):626688, 2015": "Noga Alon, Felix Fischer, Ariel Procaccia, and Moshe Tennenholtz. Sum of us: Strategyproof selection fromthe selectors. In Conference on Theoretical Aspects of Rationality and Knowledge, pp. 101110. ACM,2011. Waleed Ammar, Dirk Groeneveld, Chandra Bhagavatula, Iz Beltagy, Miles Crawford, Doug Downey, JasonDunkelberger, Ahmed Elgohary, Sergey Feldman, Vu Ha, et al. Construction of the literature graph inSemantic Scholar. arXiv preprint arXiv:1805.02262, 2018.",
  "Andrew V. Goldberg.Finding a maximum density subgraph. University of California, Berkeley EECSDepartment Technical Report, 1984": "Longhua Guo, Jie Wu, Wei Chang, Jun Wu, and Jianhua Li. K-loop free assignment in conference reviewsystems. In 2018 International Conference on Computing, Networking and Communications (ICNC), pp.542547. IEEE, 2018. Bryan Hooi, Hyun Ah Song, Alex Beutel, Neil Shah, Kijung Shin, and Christos Faloutsos. Fraudar: Boundinggraph fraud in the face of camouflage. In Proceedings of the 22nd ACM SIGKDD International Conferenceon Knowledge Discovery and Data Mining, pp. 895904, 2016. Bryan Hooi, Kijung Shin, Hemank Lamba, and Christos Faloutsos. TellTail: Fast scoring and detectionof dense subgraphs. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp.41504157, 2020. Steven Jecmen, Hanrui Zhang, Ryan Liu, Nihar Shah, Vincent Conitzer, and Fei Fang. Mitigating manip-ulation in peer review via randomized reviewer assignments. Advances in Neural Information ProcessingSystems, 33:1253312545, 2020.",
  "Ivan Stelmakh, John Wieting, Graham Neubig, and Nihar B Shah. A gold standard dataset for the reviewerassignment problem. arXiv preprint arXiv:2303.16750, 2023": "Dat Tran and Chetan Jaiswal. PDFPhantom: Exploiting PDF attacks against academic conferences papersubmission process with counterattack. In 2019 IEEE 10th Annual Ubiquitous Computing, Electronics &Mobile Communication Conference (UEMCON), pp. 07360743. IEEE, 2019. Charalampos Tsourakakis, Francesco Bonchi, Aristides Gionis, Francesco Gullo, and Maria Tsiarli. Denserthan the densest subgraph: Extracting optimal quasi-cliques with quality guarantees. In Proceedings ofthe 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 104112,2013.",
  "Paul Wilson. Academic fraud: Solving the crisis in modern academia. Exchanges: The InterdisciplinaryResearch Journal, 7(3):1444, 2020": "Ruihan Wu, Chuan Guo, Felix Wu, Rahul Kidambi, Laurens Van Der Maaten, and Kilian Weinberger.Making paper reviewing robust to bid manipulation attacks. In International Conference on MachineLearning, pp. 1124011250. PMLR, 2021. Yichong Xu, Han Zhao, Xiaofei Shi, and Nihar B. Shah.On strategyproof conference peer review.InProceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, pp. 616622,2019. doi: 10.24963/IJCAI.2019/87. URL",
  "AText Similarity Details": "In this section, we provide more details about the text-similarity scores in our datasets, introduced in.2. We synthetically generated the AAMAS text-similarity scores to supplement the original biddingdataset. The S2ORC text-similarity scores were included in the original dataset by Wu et al. (2021), andwere computed by running the widely-used TPMS algorithm on the abstracts of the reviewers past papersand the paper in question. shows the CDFs of the text-similarity scores among the reviewer-paperpairs that did not have conflicts-of-interest. Since our work analyzes the effect of bids on the paper assignment, it is important to validate that our textsimilarity scores have a realistic level of agreement with the bids. We do this by comparing to the results ofStelmakh et al. (2023), who evaluate the accuracy of commonly-used text-similarity algorithms at predictingreviewer expertise using a ground-truth dataset. Specifically, Stelmakh et al. (2023) evaluate the accuracy of",
  "BOmitted Experimental Results": "In , we provide evaluations of detection algorithms on G1 omitted from .2. This includes theOQC-Greedy algorithm, as well as the OQC-Local algorithm with all initializations (detailed in .2).These algorithms generally perform worse than those with results shown in .2. In , we show the size and bid density of honest-reviewer groups detected by a greedy peeling methodon G2 omitted from .1. In this method, we begin with the entire set of reviewers. At each iteration,we greedily remove the reviewer whose removal results in the highest bid density within the set of remainingreviewers. We then plot the bid density for each group size across the iterations. Clearly, this methodperforms very poorly at finding groups of high density.",
  "(c) OQC-Local (including random initializations),AAMAS": "2468 10 12 14 16 18 20 22 24 26 28 30 32 34number of reviewers (k) 1.00.90.80.70.60.5 edge density ( ) 0.00.00 0.00.00 0.00.00 0.01.00 0.01.00 0.01.00 0.03.02 0.07.03 0.43.07 0.37.07 1.00.00 1.00.00 1.00.00 1.00.00 1.00.00 1.00.00 1.00.00 0.00.00 0.00.00 0.00.00 0.01.00 0.01.00 0.01.00 0.03.02 0.07.03 0.15.05 0.41.07 0.35.07 1.00.00 1.00.00 1.00.00 1.00.00 1.00.00 1.00.00 0.00.00 0.00.00 0.00.00 0.01.00 0.01.00 0.01.00 0.01.00 0.01.00 0.05.03 0.13.05 0.29.06 0.47.07 1.00.00 0.98.02 0.98.02 1.00.00 1.00.00 0.00.00 0.00.00 0.01.00 0.00.00 0.01.00 0.01.00 0.01.00 0.01.00 0.01.00 0.09.04 0.17.05 0.25.06 0.27.06 0.90.04 0.88.05 0.92.04 0.94.03 0.00.00 0.00.00 0.01.00 0.01.00 0.01.00 0.01.00 0.01.00 0.01.00 0.01.00 0.01.00 0.03.02 0.03.02 0.13.05 0.29.06 0.23.06 0.69.07 0.73.06 0.00.00 0.00.00 0.00.00 0.00.00 0.01.00 0.01.00 0.01.00 0.01.00 0.01.00 0.01.00 0.01.00 0.01.00 0.01.00 0.04.02 0.05.03 0.04.02 0.05.03",
  "(d) OQC-Local (including random initializations),S2ORC": ": Performance of additional detection algorithms on G1. Values indicate the mean Jaccard similaritybetween the true set of colluders and the algorithm output, along with standard errors.Higher valuescorrespond to better detection performance. 2468 10 12 14 16 18 20 22 24 26 28 30 32 34 0.0 0.2 0.4 0.6 0.8 1.0 bid density ( )",
  "CDensity of Detected Groups": "One question relevant to the analyses in Sections 4.2 and 5.2 is why the detection algorithms fail to detectthe true colluders. Specifically, one may be interested in distinguishing the scenarios in which the detectionalgorithms output a group of reviewers less-dense than the true colluders (indicating that the colluders couldpotentially be detected if a better algorithm was developed) from the scenarios in which the detection algo-rithms output a group of reviewers with higher-density (indicating that it will be difficult for any algorithmto detect the colluders). In the following plots, we show the mean (edge or bid) density of the groups outputby the detection algorithms when colluding groups of varying size and density are injected. Note that thedetection algorithms are not restricted to outputting groups of the same size as the injected colluders. The results for all algorithms are shown in (unipartite setting) and (bipartite setting).Overall, we generally see that in the (lower-left) region of the plots in which the algorithms fail to detectthe true colluders (according to Figures 2 and 5), the algorithms are instead detecting reviewer groups withlow density. This indicates that their objective may not be aligned well with the profile of the true colludersin this region. However, in the unipartite setting for the S2ORC dataset, TellTail detects a highly-densegroup of honest reviewers when the colluding group is small and less-dense. This provides some additional",
  "DExperimental Results with Variant Datasets": "As we detail in .2, the datasets we analyze in this work are semi-synthetic. The AAMAS datasetdoes not contain authorships, and so we construct authorships by subsampling three conflicts-of-interestfor each paper. The S2ORC dataset contains bids that were synthetically constructed by Wu et al. (2021)based on bidding statistics from NeurIPS 2016. In this section, we vary these synthetic aspects and conductadditional experiments on these variant datasets. In the variant AAMAS dataset, we construct authorships as follows. For each paper, instead of subsamplingthree conflicts-of-interest uniformly at random, we choose an integer from {1, . . . , 5} uniformly at randomand subsample that many conflicts-of-interest to use as the authorships. In the variant S2ORC dataset,we modify the original bids by removing 15,000 positive bids (over 10% of the positive bids) uniformly atrandom and adding positive bids among 15,000 non-bidding pairs uniformly at random. The results of the experiments on these variant datasets are shown in Figures 13-14 (unipartite setting)and Figures 15-16 (bipartite setting). In general, we see that the results are very similar to those obtainedwith the original datasets; for comparison, see Figures 2-3, Figures 5-6, and Figures 8-10. Notably, in theunipartite setting, the TellTail, OQC-Greedy, and OQC-Local detection algorithms perform slightly betteron both variant datasets than on the original datasets. In the middle region where k = 15 and 0.8, thesealgorithms are able to achieve moderate-to-high success rates, unlike on the original datasets.",
  "(f) TellTail, S2ORC": "2468 10 12 14 16 18 20 22 24 26 28 30 32 34number of reviewers (k) 1.0 0.9 0.8 0.7 0.6 0.5 edge density ( ) 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.52.03 0.64.04 0.72.04 0.85.03 0.89.03 0.90.03 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.49.01 0.54.02 0.74.03 0.79.03 0.87.01 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.49.01 0.67.02 0.75.02 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00",
  "(g) OQC-Greedy, AAMAS": "2468 10 12 14 16 18 20 22 24 26 28 30 32 34number of reviewers (k) 1.0 0.9 0.8 0.7 0.6 0.5 edge density ( ) 0.52.00 0.52.00 0.52.00 0.52.00 0.52.00 0.52.00 0.52.00 0.52.00 0.92.03 0.99.01 1.00.00 1.00.00 1.00.00 1.00.00 1.00.00 1.00.00 1.00.00 0.52.00 0.52.00 0.52.00 0.52.00 0.52.00 0.52.00 0.52.00 0.52.00 0.54.01 0.88.01 0.89.01 0.90.00 0.90.00 0.90.00 0.90.00 0.90.00 0.90.00 0.52.00 0.52.00 0.52.00 0.52.00 0.52.00 0.52.00 0.52.00 0.52.00 0.52.00 0.52.00 0.67.02 0.80.01 0.80.00 0.80.00 0.80.00 0.80.00 0.80.00 0.52.00 0.52.00 0.52.00 0.52.00 0.52.00 0.52.00 0.52.00 0.52.00 0.52.00 0.52.00 0.52.00 0.55.01 0.65.01 0.70.00 0.70.00 0.70.00 0.70.00 0.52.00 0.52.00 0.52.00 0.52.00 0.52.00 0.52.00 0.52.00 0.52.00 0.52.00 0.52.00 0.52.00 0.52.00 0.52.00 0.53.00 0.59.01 0.61.00 0.60.00 0.52.00 0.52.00 0.52.00 0.52.00 0.52.00 0.52.00 0.52.00 0.52.00 0.52.00 0.52.00 0.52.00 0.52.00 0.52.00 0.52.00 0.52.00 0.52.00 0.52.00",
  "(h) OQC-Greedy, S2ORC": "2468 10 12 14 16 18 20 22 24 26 28 30 32 34number of reviewers (k) 1.0 0.9 0.8 0.7 0.6 0.5 edge density ( ) 0.47.00 0.47.00 0.47.00 0.47.00 0.48.00 0.48.00 0.47.00 0.48.00 0.48.00 0.47.00 0.48.00 0.47.00 0.47.00 0.47.00 0.81.04 0.93.02 0.97.01 0.47.00 0.47.00 0.47.00 0.48.00 0.48.00 0.47.00 0.48.00 0.48.00 0.47.00 0.47.00 0.48.00 0.48.00 0.48.00 0.47.00 0.47.00 0.66.03 0.85.02 0.47.00 0.48.00 0.47.00 0.47.00 0.47.00 0.47.00 0.47.00 0.47.00 0.47.00 0.48.00 0.47.00 0.48.00 0.47.00 0.47.00 0.48.00 0.48.00 0.48.01 0.47.00 0.47.00 0.47.00 0.47.00 0.47.00 0.47.00 0.47.00 0.47.00 0.47.00 0.47.00 0.48.00 0.47.00 0.48.00 0.48.00 0.48.00 0.48.00 0.47.00 0.47.00 0.47.00 0.47.00 0.47.00 0.47.00 0.47.00 0.47.00 0.47.00 0.48.00 0.48.00 0.47.00 0.48.00 0.47.00 0.47.00 0.48.00 0.48.00 0.48.00 0.47.00 0.47.00 0.47.00 0.48.00 0.48.00 0.48.00 0.48.00 0.48.00 0.47.00 0.47.00 0.47.00 0.48.00 0.47.00 0.47.00 0.48.00 0.48.00 0.48.00",
  "(i) OQC-Local (including random initializations),AAMAS": "2468 10 12 14 16 18 20 22 24 26 28 30 32 34number of reviewers (k) 1.0 0.9 0.8 0.7 0.6 0.5 edge density ( ) 0.51.00 0.50.00 0.50.00 0.50.00 0.51.00 0.51.00 0.52.01 0.54.02 0.72.03 0.69.03 1.00.00 1.00.00 1.00.00 1.00.00 1.00.00 1.00.00 1.00.00 0.50.00 0.50.00 0.51.00 0.51.00 0.51.00 0.50.00 0.52.01 0.54.01 0.57.02 0.67.03 0.65.03 0.90.00 0.89.01 0.90.00 0.90.00 0.90.00 0.90.00 0.50.00 0.50.00 0.50.00 0.51.00 0.51.00 0.51.00 0.50.00 0.51.00 0.52.01 0.55.01 0.60.02 0.65.02 0.80.00 0.79.01 0.79.01 0.80.00 0.80.00 0.51.00 0.50.00 0.51.00 0.50.00 0.51.00 0.50.00 0.50.00 0.51.00 0.51.00 0.52.01 0.54.01 0.56.01 0.57.01 0.68.01 0.67.01 0.68.01 0.69.01 0.52.00 0.50.00 0.51.00 0.51.00 0.51.00 0.51.00 0.51.00 0.50.00 0.51.00 0.51.00 0.51.00 0.51.00 0.52.00 0.54.01 0.53.01 0.57.01 0.58.01 0.50.00 0.51.00 0.51.00 0.50.00 0.50.00 0.51.00 0.50.00 0.50.00 0.50.00 0.50.00 0.50.00 0.51.00 0.50.00 0.51.00 0.51.00 0.51.00 0.51.00",
  "(f) OQC-Specialized, S2ORC": "2468 10 12 14 16 18 20 22 24 26 28 30 32 34number of reviewers (k) 1.0 0.8 0.6 0.4 0.2 bid density ( ) 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.14.00 0.17.02 0.59.06 0.96.02 1.00.00 1.00.00 0.99.00 0.99.00 1.00.00 1.00.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.14.00 0.15.00 0.28.03 0.69.03 0.81.00 0.81.00 0.81.00 0.81.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.14.00 0.15.00 0.16.00 0.21.02 0.35.03 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.12.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00 0.13.00",
  "(g) DSD, AAMAS": "2468 10 12 14 16 18 20 22 24 26 28 30 32 34number of reviewers (k) 1.0 0.8 0.6 0.4 0.2 bid density ( ) 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.33.01 0.27.02 0.50.06 0.91.04 0.99.01 1.00.00 1.00.00 1.00.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.35.01 0.25.02 0.23.02 0.38.04 0.73.03 0.80.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.01 0.36.01 0.36.00 0.34.01 0.26.02 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00 0.36.00",
  "(h) DSD, S2ORC": "2468 10 12 14 16 18 20 22 24 26 28 30 32 34number of reviewers (k) 1.0 0.8 0.6 0.4 0.2 bid density ( ) 0.28.01 0.30.01 0.30.01 0.28.01 0.36.03 0.43.04 0.51.04 0.51.05 0.81.04 0.61.05 0.79.04 0.84.03 0.88.03 0.90.02 0.84.04 0.95.01 0.99.01 0.30.01 0.28.01 0.29.01 0.31.01 0.29.01 0.32.02 0.37.03 0.48.03 0.38.03 0.49.03 0.58.04 0.51.04 0.53.04 0.60.03 0.65.03 0.67.03 0.72.03 0.28.01 0.29.01 0.27.01 0.27.01 0.28.01 0.29.01 0.28.01 0.28.01 0.32.01 0.31.01 0.34.02 0.32.02 0.35.02 0.38.02 0.39.02 0.39.02 0.45.02 0.30.01 0.28.01 0.28.01 0.29.01 0.29.01 0.28.01 0.29.01 0.28.01 0.28.01 0.29.01 0.28.01 0.29.01 0.28.01 0.29.01 0.29.01 0.28.01 0.28.01 0.30.01 0.29.01 0.27.01 0.29.01 0.28.01 0.28.01 0.27.01 0.28.01 0.27.01 0.28.01 0.29.01 0.28.01 0.27.01 0.28.01 0.30.01 0.29.01 0.28.01",
  "(i) OQC-Local, AAMAS": "2468 10 12 14 16 18 20 22 24 26 28 30 32 34number of reviewers (k) 1.0 0.8 0.6 0.4 0.2 bid density ( ) 0.29.02 0.31.01 0.30.01 0.29.02 0.29.02 0.31.02 0.28.02 0.32.03 0.33.03 0.31.02 0.36.03 0.35.03 0.41.04 0.37.04 0.33.03 0.40.04 0.38.04 0.30.02 0.29.02 0.31.01 0.29.02 0.29.02 0.30.02 0.30.02 0.30.02 0.28.02 0.30.01 0.28.02 0.28.02 0.27.02 0.33.02 0.29.02 0.31.03 0.36.03 0.31.02 0.29.02 0.28.01 0.30.01 0.30.02 0.25.01 0.27.02 0.27.02 0.32.02 0.29.02 0.30.02 0.31.02 0.25.02 0.26.02 0.29.02 0.28.02 0.28.02 0.29.01 0.29.02 0.27.02 0.28.02 0.27.02 0.28.01 0.29.01 0.28.02 0.29.02 0.29.02 0.27.02 0.31.02 0.26.02 0.31.01 0.30.01 0.30.02 0.27.02 0.29.02 0.28.02 0.31.02 0.27.02 0.28.02 0.26.02 0.27.02 0.29.02 0.31.01 0.28.02 0.28.02 0.28.02 0.27.02 0.28.02 0.28.02 0.30.02 0.29.02",
  "(j) OQC-Local, S2ORC": "2468 10 12 14 16 18 20 22 24 26 28 30 32 34number of reviewers (k) 1.0 0.8 0.6 0.4 0.2 bid density ( ) 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.21.03 0.85.05 1.00.00 1.00.00 1.00.00 1.00.00 1.00.00 1.00.00 1.00.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.20.02 0.59.04 0.76.03 0.81.01 0.82.00 0.82.00 0.82.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.17.01 0.35.03 0.61.02 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00 0.16.00",
  "(k) TellTail, AAMAS": "2468 10 12 14 16 18 20 22 24 26 28 30 32 34number of reviewers (k) 1.0 0.8 0.6 0.4 0.2 bid density ( ) 0.08.00 0.07.00 0.08.00 0.07.00 0.08.00 0.07.00 0.08.00 0.08.00 0.07.00 0.08.00 0.08.00 0.09.00 0.12.03 0.15.03 0.17.03 0.32.06 0.34.05 0.07.00 0.08.00 0.07.00 0.08.00 0.08.00 0.08.00 0.07.00 0.08.00 0.08.00 0.07.00 0.08.00 0.08.00 0.08.00 0.08.00 0.08.00 0.13.03 0.18.03 0.08.00 0.07.00 0.08.00 0.08.00 0.08.00 0.07.00 0.08.00 0.08.00 0.07.00 0.08.00 0.08.00 0.08.00 0.08.00 0.07.00 0.07.00 0.08.00 0.08.00 0.08.00 0.08.00 0.08.00 0.07.00 0.08.00 0.08.00 0.08.00 0.07.00 0.07.00 0.08.01 0.07.00 0.08.00 0.07.00 0.08.00 0.08.00 0.07.00 0.07.00 0.08.00 0.08.00 0.08.00 0.08.00 0.07.00 0.08.00 0.08.00 0.08.00 0.08.00 0.07.00 0.07.00 0.07.00 0.07.00 0.07.00 0.07.00 0.08.00 0.07.00"
}